{
  "article_text": [
    "hidden markov models are widely used in statistics ; see @xcite for a recent overview .",
    "an hmm  is a pair of discrete - time stochastic processes , @xmath9 and @xmath10 , where @xmath11 is unobserved and @xmath12 is observed .",
    "the hidden process @xmath13 is a  markov chain with initial density @xmath14 at time @xmath15 and transition density @xmath16 , i.e.@xmath17 where @xmath18 and @xmath19 is a dominating @xmath20finite measure .",
    "each observation @xmath21 is conditionally independent of other variables given @xmath22 and its conditional distribution is specified by a density @xmath23 , i.e.@xmath24 with @xmath25 and @xmath26 is a dominating @xmath20finite measure .",
    "we remark that @xmath27 , @xmath28 and @xmath29 may depend upon time - independent parameters , which we term static parameters .",
    "a variety of inference and estimation tasks for hmms involve the computation of the smoothing functional @xmath30 @xmath31 \\label{eq : exp_interest}\\ ] ] where @xmath32 , @xmath33 , @xmath34 , @xmath35 with @xmath36 and the expectation is w.r.t .  the joint smoothing distribution .",
    "for example in the cases @xmath37 and @xmath38 , approximates the posterior mean and first - order auto - covariance .",
    "when the hmm includes unknown static parameters , expectations of additive functionals play a central role in expectation - maximisation ( em ) algorithms and the calculation of score vectors ; see @xcite for some discussion .    in practice ,",
    "the expectation in can rarely be computed exactly and one resorts to the use of numerical integration techniques such as smc ; see @xcite for a recent overview .",
    "these methods typically rely on the ability to evaluate pointwise the conditional density @xmath39 .",
    "the methods we consider address the problem of obtaining an approximation of without performing any such evaluations , in a principled manner admitting control of the error in approximation .",
    "the motivations for avoiding such evaluations are as follows .",
    "firstly , for some models , @xmath39 may simply not have a closed form expression .",
    "secondly , in some situations evaluation of @xmath39 may be very expensive .",
    "the general technique we consider may be interpreted as an instance of abc .",
    "a recent review of this class of methods can be found in @xcite . in the context of hmms ,",
    "abc has been considered by @xcite , see also @xcite .",
    "the approximation ( given in section [ sec : abc_approx ] ) has been introduced by @xcite and still requires numerical ( e.g.  smc ) methods to fit them .",
    "alternative ideas include nonparametric filtering @xcite and the related convolution particle filter @xcite ; see @xcite for some discussion relative to abc . as noted by @xcite , in the scenario where there is a fixed amount of data and a fixed number of simulated samples , there is a distinct lack of theoretical results which quantify the combined abc and numerical ( smc ) errors ; we provide some of the first results in this context ( see @xcite for other results ) .",
    "our main objectives are to    1 .",
    "investigate , both theoretically and empirically , the error associated with the approximation scheme we propose 2 .",
    "demonstrate how this scheme can be used to perform smoothing and to estimate static parameters in hmms from a batch of data    regarding point 1 . , the error has two components .",
    "the first component arises from the introduction of an auxiliary hmm , incorporating auxiliary variables valued in the same space as the observations @xmath40 .",
    "smoothing expectations under this auxiliary model , which we write as @xmath41 $ ] , may be taken as approximations of ; the degree of approximation is controlled through a parameter @xmath1 and the error should disappear as @xmath42 . in turn , expectations under this auxiliary model admit efficient numerical approximation using smc techniques , without evaluation of @xmath39 .",
    "the second component of the overall error arises from this monte carlo scheme , and is controlled through a sample size parameter @xmath43 ; the error disappears as @xmath44 .",
    "it is noted that the smc method adopted is the forward only smoothing implementation of the forward - filtering backward smoothing ( ffbs ) method @xcite in @xcite ; this is currently one of the most accurate methods for smc approximation of smoothed additive functionals .",
    "we will write the smc estimate of the smoothing expectation under the auxiliary hmm as @xmath45 $ ] and , similarly , we may denote the smc estimate of the smoothing expectation under the original hmm - as @xmath46 $ ] . in the numerical studies in section [ sec : numerics ] , the errors associated with these smc estimates will be denoted as @xmath47 and @xmath48 , respectively .",
    "the overall error associated with the smc estimate of the smoothing expectation under the auxiliary hmm may be decomposed as @xmath49-\\xi_n[\\epsilon , n,\\mathcal{v}_n , y_{0:n}]=&\\mathbb{e}[\\mathcal{v}_n(x_{0:n})|y_{0:n}]-\\mathbb{e}^{\\epsilon}[\\mathcal{v}_n(x_{0:n})|y_{0:n}]\\nonumber\\\\ + & \\mathbb{e}^{\\epsilon}[\\mathcal{v}_n(x_{0:n})|y_{0:n}]-\\xi_n[\\epsilon , n,\\mathcal{v}_n , y_{0:n}].\\label{intro : err : decomp}\\end{aligned}\\ ] ] the first difference on the right of is a deterministic error , the second difference is a stochastic error . we will provide theoretical analysis of these two error terms which shows how the interplay between @xmath1 , @xmath3 and @xmath43 controls the overall quality of the approximation .",
    "these theoretical results , to an extent , are also studied from an empirical perspective .    regarding point 2 .",
    ", we show how the approximation scheme can be incorporated into a particle mcmc scheme in order to estimate static parameters of the hmm .",
    "particle mcmc uses smc techniques to generate proposals , for example , associated to hidden states of the hmm . here",
    ", we use the particle marginal metropolis - hastings algorithm in @xcite to sample from the abc approximation of the hmm , with a prior placed upon the unknown static - parameters . in contexts where additive functions of the hidden state are also of interest ( as above ) , we use the forward - only smoothing technique mentioned above , to use all the simulated samples from the smc proposal ( which is not typically adopted ) .",
    "a similar idea has been adopted by @xcite , except using an additional ` backward - pass ' in the ffbs algorithm , which is not needed .    in summary ,",
    "our main contributions are to :    * quantify , in terms of @xmath1 and @xmath3 , the error @xmath50-\\mathbb{e}^{\\epsilon}[\\mathcal{v}_n(x_{0:n})|y_{0:n}]$ ] - henceforth referred to as the _ abc error _ * quantify , in terms of @xmath1 , @xmath43 and @xmath3 , the error @xmath51-\\xi_n[\\epsilon , n,\\mathcal{v}_n , y_{0:n}]$ ] - henceforth referred to as the _ smc error _ * provide empirical evidence which illustrates some of these theoretical findings    this paper is structured as follows . in section [ sec : approach ] we discuss the abc approximation and we characterize the abc error . in section [ sec : simulation ] , smc and mcmc simulation techniques for targeting the ( abc ) smoother are detailed , and the estimation of static parameters in a bayesian manner is also considered . in section [ sec : theory ] our main theoretical result is given , which combines the abc and smc errors discussed above . in section [ sec : numerics ] some numerical studies are presented . in section [ sec : summary ]",
    "the article is concluded .",
    "the proofs are in the appendices .      given a measurable space @xmath52 , let @xmath53 be a @xmath20finite measure , @xmath54 be a non - negative kernel and @xmath55 a measurable function .",
    "the conventions @xmath56 , @xmath57 , @xmath58 are used .",
    "in addition , let @xmath59 and let @xmath60 be the banach space of bounded and measurable functions on @xmath61 endowed with the norm @xmath62 . for two probability measures @xmath63 the total variation distance is @xmath64 .",
    "for a markov kernel @xmath54 the dobrushin coefficient is @xmath65 . given a probability space @xmath66 ,",
    "we write @xmath67^{1/p}$ ] for the @xmath68 norm under @xmath69 . for a set @xmath70",
    ", @xmath71 is the indicator function .",
    "the joint smoothing density is @xmath72 where we will suppress the dependence on the data on the l.h.s . in most scenarios of practical interest",
    ", one can not calculate this density pointwise , or compute expectations w.r.t .  the density . as a result",
    ", a numerical approximation of , via advanced computational tools , is required .",
    "this problem is further exacerbated when the density @xmath39 is intractable or very expensive to calculate .",
    "that is , one can not evaluate it pointwise and there is no unbiased estimate available .",
    "however , we will assume throughout that one can sample from the associated distribution , for any @xmath73 .",
    "it is remarked that this latter condition is not completely necessary for any of the subsequent ideas that will appear ( see @xcite ) , but , it will facilitate a more compact exposition .    to introduce ideas ,",
    "let us momentarily step away from the setting of hmm s ; suppose , one is given observations @xmath74 associated to some intractable likelihood @xmath75 with @xmath76 an unknown parameter .",
    "then , bayesian inference associated to the posterior @xmath77 is typically not feasible even using advanced computational tools ; see @xcite . to deal with this issue",
    ", abc draws inference from the following modified posterior density on @xmath78 @xmath79 with @xmath0 a tolerance level and @xmath80 corresponds to some pseudo - observations .",
    "the set @xmath81 is defined as follows @xmath82 where @xmath83 represents some summary statistics and @xmath84 a distance metric .    as noted by @xcite ,",
    "given an appropriate structure for the likelihood ( such as i.i.d .",
    "data ) one can often achieve a more accurate approximation by removing the summary statistics and focusing upon the probabilistic structure of the likelihood .",
    "returning now to our setting of the hmm specified in section 1 , following @xcite we consider the abc approximation of the joint smoothing density , for @xmath0 : @xmath85 \\eta_0(x_0)\\prod_{i=1}^n f(x_{i-1},x_i)}{\\int_{\\mathbb{r}^{(n+1)d_x}}[\\prod_{i=0}^n \\int_{\\mathbb{r}^{d_y}}\\phi(\\frac{u - y_i}{\\epsilon})g(x_i , u)du ] \\eta_0(x_0)\\prod_{i=1}^n f(x_{i-1},x_i ) dx_{0:n } } \\label{eq : abc_smooth}\\ ] ] where @xmath86 is a potential which can take the form @xmath87 or a probability density function .",
    "this particular abc approximation maintains the markovian structure of the model , which will help to facilitate computational algorithms .",
    "in particular , in order to approximate @xmath88 $ ] where @xmath89 $ ] is an expectation w.r.t .  the abc smoothing distribution , one will still need to resort to numerical methods such as smc and mcmc .    from the perspective of a theoretical justification ,",
    "results on the consistency in estimation ( from both classical and bayesian perspectives ) of static parameters , associated to the abc approximation , as @xmath3 grows can be found in @xcite .",
    "there is an intrinsic asymptotic bias , but this bias can be removed by using a noisy version of abc ; see @xcite for further details . in this article",
    "we do not address the idea of noisy abc .      in order to ascertain the potential of using an abc approximation of hmms , when considering smoothed additive functionals , we investigate the abc error .",
    "the analysis that follows will concentrate on the scenario in which the abc kernel is _ not _ the indicator function .",
    "in particular , this allows the inclusion of ( a[hyp : a ] ) ( [ hyp : smooth_abc ] ) , below , which will facilitate the theoretical analysis of the smc error of the abc smoother , in section [ sec : theory ] .",
    "we remark that the below result for the abc bias can also be established when @xmath86 is an indicator function with some minor modifications to the proof .    in the subsequent analysis",
    "we adopt the following assumption .",
    "[ hyp : a ]    1 .",
    "[ hyp : density_control]there exists a @xmath90 such that for each @xmath91 and @xmath92 @xmath93 2 .",
    "there exists a @xmath94 such that for every @xmath95 @xmath96 with @xmath97 the @xmath98norm .",
    "[ hyp : smooth_abc ] there exist functions @xmath99 such that for any @xmath100 , @xmath101 @xmath102 with @xmath103 monotonically decreasing .",
    "in addition @xmath104 with @xmath105 .",
    "[ hyp : function ] the analysis is associated to additive functionals @xmath106 : @xmath107 with @xmath108 .",
    "the assumptions are rather strong and will typically only hold when the observations and hidden states lie on a compact state spaces , they are however , quite typically of assumptions employed in the analysis of smc and related approximation methods . of these assumptions , perhaps ( a[hyp : a]-[hyp : smooth_abc ] ) should be discussed . it can be verified when : @xmath109 it is remarked that in practice , one selects @xmath110 , so this is not such a demanding assumption .",
    "we have the following result , whose proof is in appendix [ app : abc_error ] .",
    "[ theo : abc_error ] assume ( a[hyp : a ] ) .",
    "then there exist a @xmath111 such that for any @xmath0 , @xmath112 , @xmath113 , @xmath114-\\mathbb{e}^{\\epsilon}[\\mathcal{v}_n(x_{0:n})|y_{0:n}]| \\leq c \\epsilon ( n+1)\\ ] ] where @xmath115 $ ] and @xmath89 $ ] are the expectation w.r.t .  the joint smoothing and abc smoothing distribution .",
    "the result establishes that the abc error does not grow any faster than linearly in time or @xmath1 .",
    "this is important , as it is known that the smc error when estimating @xmath51 $ ] also grows at most linearly in time @xcite . as a result ,",
    "the overall error as the time parameter increases will not necessarily be dominated by one source of error ( smc or abc ) .",
    "this suggests that an abc approximation can perform reasonably well in general .",
    "in the context of hmms , smc algorithms approximate @xmath116 recursively by propagating a collection of properly weighted samples , called particles , using a combination of importance sampling and resampling steps . for the importance sampling part of the algorithm at each step @xmath3 of the algorithm we will use general proposal ( markov ) kernels",
    "@xmath117 which possess normalizing constants that do not depend on the simulated paths",
    ". a typical smc algorithm is given below ( we assume it terminates at time @xmath118 ) :    * \\0 .",
    "initialisation : set @xmath119 ; for @xmath120 sample @xmath121 and compute @xmath122 with @xmath123 . * \\1 .",
    "decide whether or not to resample , and if this is performed , set all weights @xmath124 to @xmath125 .",
    "proceed to step 2 . * \\2 .",
    "set @xmath126 , if @xmath127 stop , else ; for @xmath120 sample @xmath128 , compute @xmath129 and set @xmath130 and return to the start of step 1 .",
    "if one chooses to implement smc without resampling steps , i.e.  to perform sequential importance sampling , as time progresses , the variance of the weights @xmath131 typically increases .",
    "this has been commonly referred to as the _ weight _ degeneracy property . to counter this resampling",
    "is used : the particles are sampled with replacement , according to the normalized weights @xmath132 given by @xmath133 and then each @xmath134 is reset to @xmath125 .",
    "we remark that more efficient alternatives are possible ; see e.g.  @xcite .",
    "if one resamples too often , the simulated past of the path of each particle will be very similar to each other .",
    "this has been documented as the _ path _ degeneracy problem .",
    "a common remedy was to resample only when an appropriate criterion drops beneath or goes above some threshold . in the former case ,",
    "a common criterion is the effective sample size @xmath135 @xcite .",
    "this approach , however , does not ultimately solve the path degeneracy problem .",
    "path degeneracy has been a long standing bottleneck when static parameters @xmath136 are estimated online using smc methods by augmenting them with the latent state ; see @xcite .",
    "considering the central limit theorem ( clt ) associated to the smc estimate of @xmath50 $ ] : @xmath137 it is remarked that the issue of path degeneracy leads , under very strong conditions on the hmm , to an asymptotic variance in this clt that grows quadratically in @xmath3 ; see @xcite .",
    "suppose one resamples , multinomially , at every iteration , except when @xmath138 .",
    "denote the resampled index of the ancestor of particle @xmath139 at time @xmath3 by @xmath140 ; this is a random variable chosen with probability @xmath141 .",
    "furthermore the joint density of the sampled particles and the resampled indices is @xmath142 where the complete genealogy of ancestors is denoted as @xmath143 and the randomly simulated values of the state as @xmath144 .",
    "together they form the following smc approximations for @xmath145@xmath146 and an approximation of the normalizing constant @xmath147 the complete ancestral genealogy at each time can always traced back by defining an ancestry sequence @xmath148 for every @xmath120 and @xmath149 , whose elements are given by the backward recursion @xmath150 where @xmath151 .",
    "this interpretation of smc approximations was introduced in @xcite and will be used later together with @xmath152 for describing pmcmc .      due to the path degeneracy effect , one does not want to use the smc approximation @xmath153 to perform smoothing .",
    "one potential solution to this issue is the forward filtering backward smoothing algorithm and in particular the forward only implementation of it in @xcite .",
    "that is , the ffbs algorithm includes a backward simulation step , which is eliminated in @xcite .",
    "we consider the smc approximation of the expectation @xmath50 $ ] where @xmath154 .",
    "the construction of the procedure is as follows .",
    "it is first noted that @xmath31 = \\int v_n(x_n ) \\hat{\\eta}_n(x_{0:n } ) dx_{0:n}\\ ] ] with , for @xmath112 @xmath155 @xmath156 , where @xmath157 , then one can establish , e.g.  @xcite , that @xmath158 \\hat{\\eta}_n(x_{n-1}|x_n)dx_{n-1}\\ ] ] where @xmath159 .",
    "these recursions lead to the following idea .",
    "given the current particle approximation of the marginal of @xmath160 , @xmath161 and of @xmath162 ( write this @xmath163 ) , one performs the following : update the smc approximation as in section [ sec : smc_desc ] and set @xmath164 } { \\sum_{j=1}^n \\bar{w}_{n-1}^j f(x_{n-1}^j , x_n^i ) } \\quad i\\in\\{1,\\dots , n\\}\\label{eq : v_update}\\ ] ] with @xmath165 .",
    "then the smc approximation of @xmath50 $ ] , is exactly : @xmath166 it is apparent that the computational cost of this recursion is @xmath167 per - time step . for functions such as @xmath154",
    ", it has been seen that , under some assumptions , the asymptotic variance in the clt associated to grows at most linearly in @xmath3 .",
    "this is in contrast to growing quadratically at least quadratically in @xmath3 , under similar assumptions , for the standard smc estimate ; see @xcite and also @xcite for additional theoretical analysis .",
    "if one can not or does not want to compute @xmath39 , then the algorithm described in section [ sec : smc_desc ] can seldom be implemented . in such contexts , we can easily use an smc algorithm to approximate the @xmath168 in .",
    "for example , in @xcite , at time 0 , one samples the signal from @xmath27 and the pseudo observations from the likelihood to yield an incremental weight @xmath169 where @xmath170 is the pseudo observation at time 0 . at subsequent",
    "time - points one can sample from the signal transition and likelihood to obtain @xmath171 .",
    "the selection of @xmath1 can be adaptive and different proposals ( other than the state - dynamics ) can be adopted ; we refer to @xcite for some discussion .",
    "it is remarked that a drawback of the algorithm is that when @xmath172 grows with @xmath173 fixed , one can not expect the algorithm to work well for every @xmath1 ; typically one must increase @xmath1 to yield reasonable algorithmic results and this is at the cost of increasing the bias ( see theorem [ theo : abc_error ] ) . to maintain @xmath1 at a reasonable level",
    ", one must consider more advanced strategies which are not investigated here .    in scenarios",
    "where @xmath174 , a potentially better procedure is to use the rejection kernel in @xcite ( note this differs from the ideas of @xcite ) . in this case , one initializes the smc algorithm as above .",
    "however , at subsequent time - points , @xmath112 , one uses the kernel @xmath175\\sum_{j=1}^n \\frac{g_{n-1}(u_{n-1}^{j } ) } { \\sum_{l=1}^n g_{n-1}(u_{n-1}^{l } ) } h_n((u_{n-1}^{j},x_{n-1}^{j}),(u_{n}^{i},x_{n}^{i } ) ) .",
    "\\label{eq : rej_kernel}\\end{aligned}\\ ] ] in this case , at any given time - step , we will only resample those particles which have @xmath176 .",
    "it has been shown by @xcite that this kernel produces a lower asymptotic variance in the clt than an algorithm which resamples at every time step .",
    "it will be of interest to see if this advantage is realized when @xmath43 is finite , especially versus the dynamic resampling that is mentioned in section [ sec : smc_desc ] .",
    "this particular smc approach is termed ` rejection smc ' ( rsmc ) throughout the article .    when using smc for the abc approximation of @xmath50 $ ] , the procedure in section [ sec : fos ] can be followed with only modifications in notations and state - spaces .      in this section",
    "we consider the scenario where one has unknown static parameters @xmath177 associated to the hmm .",
    "we concentrate upon batch inference .",
    "particle markov chain monte carlo methods are mcmc algorithms operating on an extended state - space and targeting an extended distribution over the random variables appearing in the smc algorithm . as in standard",
    "mcmc the idea is to run an ergodic markov chain to obtain samples from the distribution of interest .",
    "the difference lies in the fact that , due to using an smc approximation to generate a proposal , the invariant distribution of the simulated chain is defined on an extended state space , with an appropriate marginal being the distribution that we are interested in sampling from in the first place .",
    "we will present the particle marginal metropolis - hastings ( pmmh ) algorithm of @xcite .",
    "the pmmh algorithm can sample from the target distribution @xmath178 \\pi(\\theta ) \\label{eq : joint_post}\\ ] ] where @xmath179 is the prior on @xmath136 .",
    "we concentrate upon the presentation in the scenario that one is interested in the original hmm ; the abc extension is simple and just uses the smc procedures described in section [ sec : smc_abc ] instead of those at the start of section [ sec : smc_desc ] .",
    "note also , that the algorithm is given when using an smc algorithm that resamples at each time - step ; a dynamic resampling schedule can also be used .",
    "the pmmh algorithm proceeds as follows :    * \\0 . set @xmath180 .",
    "sample @xmath181 from ( which now depends upon @xmath136 ) .",
    "sample @xmath182 from @xmath183 and and compute @xmath184 as in .",
    "store @xmath185 . set @xmath186 * \\1 .",
    "propose a new @xmath187 from a candidate @xmath188 and @xmath189 and @xmath190 as in step 0 .",
    "accept or reject this as the new state of the chain with probability @xmath191 if we accept , set @xmath192 , otherwise + @xmath193 @xmath194 .",
    "set @xmath195 and return to 1 .",
    "in @xcite it is shown that the sequence @xmath196 provides an approximation of , for any @xmath197 .",
    "if one is interested in approximating , say @xmath198 as noted by @xcite , the ffbs estimate can be used , based upon the smc at each time - step , by simply extending the definition of @xmath199 in to include @xmath136 ( c.f .  ):",
    "@xmath200 where the first summation is over @xmath201 iterations of the pmmh algorithm .",
    "trivially , one can extend this to the case where only a forward pass as in section [ sec : fos ] is used .",
    "a critical point is despite the improvement in the smc estimation , whether this is necessarily reasonable given the increase in computational cost and the iterative nature of the mcmc ; especially in an abc context , which is presently not known to our knowledge .",
    "we remark again , that any of the smc for abc algorithms mentioned in section [ sec : smc_abc ] can be adopted , when considering the abc approximation of ; whether using dynamic resampling or the kernel the estimate of the normalizing constant is unbiased - see @xcite for why this is of interest .",
    "we consider the error in estimation of smoothed additive functionals , when using an abc approximation of the hmm .",
    "this is in the scenario where one does not need to estimate static parameters .",
    "recall that we have already considered the abc error in theorem [ theo : abc_error ] ; the main objective is to present a result with regards to the smc error and the overall effect on the approximation of @xmath50 $ ] .",
    "we will use ( a[hyp : a ] ) which only applies in the scenario where one uses a kernel density in the abc approximation ( i.e.  not an indicator function ) .",
    "in addition , the smc algorithm samples from the transition density of the state , with multinomial resampling at every time step ( that is , rsmc is not considered ) . these hypotheses can be removed with a more technical proof .",
    "in addition , we condition upon the data and do not treat the randomness of these quantities .",
    "we simply assume that we are given a data set and do not address the issue of whether they may , or may not originate from a hmm .",
    "below the @xmath202norm is associated to the random process generated by the smc algorithm .",
    "we also use the abuse of notation @xmath203 , @xmath204 , to represent the incremental weights of the smc algorithm ( that is as described in section [ sec : smc_abc ] ) .",
    "note that , in comparison to , we resample at every time - point , so we can use the incremental weights in the estimate , instead of the normalized weights",
    ". note that @xmath205 = \\sum_{i=1}^n\\frac{g_{n,\\epsilon}(x_n^i)}{\\sum_{j=1}^n g_{n,\\epsilon}(x_n^j ) } v_{n,\\epsilon}^n(x_n^i ) \\label{eq : smc_est}\\ ] ] where @xmath45 $ ] is the quantity that we discussed in section [ sec : intro ] ; from herein we use the r.h.s .",
    "of to denote the smc estimate .",
    "[ theo : main_theorem ] assume ( a[hyp : a ] ) .",
    "there exist a @xmath111 and for any @xmath206 there exist a @xmath207 such that for any @xmath0 , @xmath197 , @xmath112 and @xmath113 : @xmath208\\bigg\\|_p \\leq ( n+1)\\bigg[\\frac{a_p\\overline{\\delta}(\\epsilon)}{\\sqrt{n } } + c\\epsilon\\bigg]\\ ] ] where @xmath209 , with @xmath210 as in ( a[hyp : a ] ) ( [ hyp : smooth_abc ] ) and @xmath115 $ ] is the expectation w.r.t .  the joint smoothing distribution .",
    "after adding and subtracting @xmath51 $ ] one can apply minkowski followed by theorem [ theo : smc_error ] and theorem [ theo : abc_error ] to conclude .",
    "[ rem : error_avg ] the bound is decomposed into two sources of error . for the smc approximation",
    ", the error tends to decrease as @xmath1 grows as one would expect .",
    "conversely , the abc error term @xmath211-\\mathbb{e}[\\mathcal{v}_n(x_{0:n})|y_{0:n}]|$ ] grows as @xmath1 grows .",
    "both error rates increase at most linearly with the time parameter .",
    "when @xmath212 , one can remove the linear decay in the bias term .",
    "in addition , with @xmath43 fixed , the smc error can be shown to _ decrease _ with @xmath3 ; see @xcite .",
    "our numerical studies are implemented on the following hmm also considered in , for example , @xcite .",
    "we take @xmath213 and the model is : @xmath214 with @xmath215 and independently @xmath216 and @xmath217 with @xmath218 the @xmath219-dimensional zero vector , @xmath220 the @xmath221 identity matrix and @xmath222 the @xmath219-dimensional normal distribution of mean @xmath53 and covariance matrix @xmath223 .",
    "whilst the conditional density of the observations given the state is not intractable , it will facilitate an investigation into the accuracy of abc . in this scenario",
    "one can obtain an approximation of the ` correct ' answers using smc / pmcmc with many particles / iterations .",
    "the objective of our numerical study , for smoothing is to consider the accuracy of abc , when only considering forward only smoothing ( the performance of forward only smoothing relative to using the path of particles has been studied elsewhere - for example @xcite ) .",
    "we also want to investigate the worth of rsmc in the abc context ; recall the asymptotic improvements predicted in @xcite . along the way we also consider the issue of the dimension of the hmm and the utility of using abc in high - dimensions . finally , the time dependence of the errors are presented , to allow some investigation into theorem [ theo : main_theorem ] . when considering pmcmc , we are concerned with both the accuracy of abc for batch static parameter estimation and the worth of including forward only smoothing as a ` post - processing ' of the mcmc output .",
    "we consider estimating the expected mean state over the observation period @xmath224 $ ] ; i.e.  @xmath225 , @xmath226 .",
    "we set @xmath227 and @xmath228 .",
    "the data are simulated from the true model with the given parameter values . to obtain a true answer with which to understand the accuracy of the methods we investigate , we use",
    "the mean estimate obtained over 50 implementations of the forward smoothing procedure , targeting the exact model , with @xmath229 particles .",
    "the algorithms for smc ( that is , approximating the exact model ) and smc abc are run for 10 different values of @xmath230 which are labelled @xmath231 in the figures . the smc abc approach , i.e.  that dynamically resamples , does so when the effective sample size drops below @xmath232 . for the smc ,",
    "smc abc and rsmc ( which targets the abc approximation ) the hidden state dynamics are used as proposals .",
    "we also run the algorithms for @xmath233 which will also allow us to assess the accuracy of smc for an abc hmm in ` high ' dimensions .",
    "to investigate the accuracy of abc , we compute a true value for @xmath50 $ ] , as discussed above , and then average the @xmath234error of the estimate @xmath45 $ ] , calculated with respect to the computed true value , across its dimension , i.e.  @xmath235-\\xi_{n}[\\epsilon , n,\\mathcal{v}_{n},y_{0:n}]\\right|$ ] , with @xmath97 the @xmath236-distance .",
    "an smc procedure targeting the exact hmm is also run , to provide some benchmark performance ; the corresponding error , @xmath48 , is similarly calculated as the dimension - averaged @xmath234error of the smc estimate @xmath46 $ ] with respect to the same true value as above .",
    "all results are averaged over 50 independent runs .    for the abc specification , we set @xmath237 ; this will allow us to easily understand the impact of the rsmc . in the implementations of the two smc abc schemes described in section",
    "[ sec : smc_abc ] @xmath1 is set to be the smallest obtainable in a preliminary set of runs .",
    "that is , the smallest @xmath1 for which the weights do not become zero at any time - point .    to conclude the numerical study",
    ", we consider the time - dependence of the bias .",
    "we consider the smc and abc using only forward only smoothing as the time parameter increases from 10 , 20,@xmath238 , 100 , @xmath233 .",
    "the smc algorithm is run with @xmath239 .",
    "rsmc is not considered .",
    "the exact and abc forward smoothing errors , @xmath48 and @xmath47 respectively , are presented in figure [ fig : forwardsmoothingerrors_smcvsabc ] ; the mean errors obtained across the 50 runs are displayed , along with their standard errors . under the abc hmm , as one would expect , in almost all of the plots the accuracy of the estimate @xmath47 can not improve with increasing @xmath43 ( as the bias persists ) , but the variability of the estimates falls - i.e.  the smc component of @xmath47 is being controlled . in the plots for @xmath240 ,",
    "the exact implementation outperforms , as one would expect , the abc approximation in terms of accuracy .",
    "this is illustrated by the means and standard errors of the smoothing errors @xmath48 being smaller than those of the abc smoothing errors @xmath47 for a vast majority of the values of @xmath43 .",
    "interestingly , as the dimension increases ( @xmath241 ) , the abc estimates appear to be _ more _ accurate than their smc counterparts ( at least for this function ) .",
    "one might explain this as follows .",
    "for smc in high - dimensions , one often requires @xmath242 ( @xmath243 ) for some stability , but this is not the case for abc - see @xcite and the references therein .",
    "these ( empirical ) results suggest that abc is a viable approximation technique in higher - dimensions , where it can be difficult to find smc techniques that always work well .    ) obtained over 50 independent implementations of the forward smoothing procedure targeting the true hmm ( @xmath48 , black ) and its abc approximation ( @xmath47 , red ) .",
    "the horizontal axis represents the 10 different values of @xmath43 for which we ran both algorithms.,width=529,height=755 ]    ) obtained over 50 independent implementations of the forward smoothing smc ( @xmath47 , red ) and rsmc ( @xmath244 , blue ) procedures targeting the abc approximation of the smoothing distribution.,width=529,height=755 ]    ) obtained over 50 independent implementations of the forward smoothing smc ( @xmath47 , red ) and rsmc ( @xmath244 , blue ) procedures targeting the abc approximation of the smoothing distribution .",
    "these standard errors are also displayed in figure [ fig : forwardsmoothingerrors_abcsmcvsabcrsmc].,width=529,height=755 ]    in figures [ fig : forwardsmoothingerrors_abcsmcvsabcrsmc ] and [ fig : forwardsmoothingstderrors_abcsmcvsabcrsmc ] we compare the performance of smc and rsmc for performing estimation of the smoothing expectation under the abc hmm .",
    "figure [ fig : forwardsmoothingerrors_abcsmcvsabcrsmc ] presents the mean and standard errors of the abc smoothing errors that correspond to estimates calculated using the smc method and the rsmc method ; distinction is made through a further subscript , with the abc smoothing errors corresponding to the rsmc estimates being denoted @xmath244 .",
    "for clarity of presentation , we only display the results of 7 of the 10 values of @xmath43 which were run . from figure [ fig : forwardsmoothingerrors_abcsmcvsabcrsmc ] , it is noted that the accuracy of the smc and rsmc procedures for performing abc forward smoothing are very comparable , with the rsmc estimates even appearing to offer a marginal improvement over the smc estimates in terms of mean smoothing error .",
    "the standard errors of @xmath47 and @xmath244 are more clearly presented in figure [ fig : forwardsmoothingstderrors_abcsmcvsabcrsmc ] .",
    "this figure shows that , under the abc hmm , the variability of the rsmc procedure seems to be slightly less than that of the smc procedure , especially as @xmath43 is allowed to grow .",
    "in addition , the observed run times for the abc forward smoothing procedure were consistently lower when using the rsmc method against the smc abc approach .",
    "these results suggest , at least under the criteria considered , that the use of rsmc would not only be a viable alternative , but it could be preferable to using smc with dynamic resampling .",
    "this is when using forward smoothing to perform inference with respect to the abc approximation of the hmm .    in figure",
    "[ fig : time ] we consider the time dependence of the error @xmath47 associated with the smc method applied to the abc hmm .",
    "we can observe that in this scenario , there is not any obvious increase in the overall error @xmath47 , with time , for this particular estimate associated to the smoothing distribution .",
    "this is consistent with our theoretical results which illustrate that the error does not grow any worse than linearly with time .",
    "as expected , on the basis of the results above , the quality of the smc approximation appears to deteriorate ( for @xmath43 fixed ) as the dimension grows , but such a deterioration is less obvious for the abc approximation .    ,",
    "@xmath245 ) obtained over 50 independent implementations of the forward smoothing smc ( @xmath48 , black ) and abc ( @xmath47 , red ) procedures targeting the true and abc approximation of the smoothing distribution respectively.,width=529,height=755 ]        as for the smoothing , we estimate the expected mean state over the observation period @xmath224 $ ] as well as estimating the static parameters @xmath246 , with priors as in @xcite .",
    "we set @xmath247 throughout and the data are the same as for the smoothing experiment ( when @xmath247 ) . to obtain a proxy for the true value we ran a pmmh algorithm as described in @xcite for 50000 iterations with 20000 particles ( no forward smoothing ) and averaged the results over 50 runs .",
    "when no forward smoothing is used , only the selected particle ( see section [ sec : pmcmc ] ) is used for the estimate of a smoothed additive functional .",
    "the abc approximation was as for the smoothing example ( that is , the function @xmath237 ) . to allow direct comparison to running an exact pmmh algorithm ( that is , one which uses a dynamic resampling smc algorithm on the true hmm ) we only adopt an smc abc algorithm ,",
    "i.e. we do not consider the use of rsmc here .",
    "for the smc and smc abc the hidden state dynamics are used as proposals .",
    "the pmmh proposal on the parameters is as in @xcite .",
    "we run the algorithms for 50000 iterations with a 10000 iteration burn in .",
    "in addition , 5 different values of @xmath43 are considered @xmath248 for the forward only smoothing approaches . in comparing to pmmh algorithms that do not use all the particles ( and hence the computational cost of the smc algorithm",
    "is @xmath249 ) a number of particles with similar computational costs are run ; these were @xmath250 .",
    "as with smc smoothing , the accuracy of the pmmh procedures in estimating the smoothing expectation @xmath50 $ ] is measured using @xmath48 and @xmath47 .",
    "as above , these errors are calculated as the ( dimension - averaged ) @xmath234errors of the pmmh estimates under the exact and abc hmm , respectively .",
    "all results are repeated over 50 independent runs .",
    "our results are displayed in figures [ fig : pmmh_smooth]-[fig : pmmh_sigy ] . in figure",
    "[ fig : pmmh_smooth ] , we can observe the accuracy of pmmh estimation of the smoothed additive functional , using smc updates both with and without forward smoothing , under both the exact hmm and its abc approximation . here",
    "we observe the expected pattern ; the use of forward only smoothing in the pmmh update scheme significantly enhances estimative accuracy for roughly the same computational cost - the accuracy is better and the variance lower . when using forward smoothing in the smc update mechanism , we further observe that the abc hmm can be targeted with reasonable accuracy .",
    "consider the effect of increasing @xmath43 on the errors in figure [ fig : pmmh_smooth ] .",
    "interestingly , the improvement in estimation is more evident when using forward only smoothing , even though one expects a pmmh algorithm with more particles to mix better ( see e.g.  @xcite ) and thus the estimation to be most likely improved .    in terms of the estimation of parameters , we consider figures [ fig : pmmh_sigx ] and [ fig : pmmh_sigy ] . here ,",
    "we are mainly concerned with the quality of parameter estimation under the abc hmm without forward smoothing - the forward smoothing can not contribute anything to parameter estimation here .",
    "the accuracy of the abc is , in general quite biased by up - to 40% of the parameter values .",
    "the variance is also quite substantial relative to the exact approach .",
    "obtained over 50 independent implementations .",
    "the pmcmc with exact smc is in black , the abc in red .",
    "the dotted lines indicate the usage of forward only smoothing .",
    "the dotted horizontal line is the estimated true value.,width=529,height=755 ]     obtained over 50 independent implementations .",
    "the pmcmc with exact smc is in black , the abc in red .",
    "the dotted lines indicate the usage of forward only smoothing .",
    "the dotted horizontal line is the estimated true value.,width=529,height=755 ]      on the basis of our numerical study we can tentatively conclude the following",
    ". for smoothing :    * in higher dimensions , abc , in terms of accuracy , is competitive with using standard smc ( even if the model is analytically tractable ) ; * for abc with @xmath86 specified as an indicator function , one would prefer to use the rsmc procedure over an smc procedure with dynamic resampling .    for the use of pmmh for performing batch parameter estimation",
    ", it would appear that , for moderate length time series , using forward only smoothing is not necessarily useful . if one is interested in the estimation of smoothed additive functionals , however , the use of forward smoothing can provide significant improvements ( for the same computational cost ) in estimative accuracy when compared to the pmmh procedure in @xcite .",
    "the abc procedure produces parameter estimates which are perhaps more biased than estimation of smoothed additive functionals , but this is also linked to the fact that the estimation method used is focussed on the latter quantities . these conclusions ,",
    "of course , can not be comprehensive as they are model and quantity ( w.r.t .",
    "estimation ) dependent .",
    "however , we have seen similar trends in different examples or different parameter settings for the same model .",
    "in this article we have investigated smoothing and static parameter estimation for hmms with intractable likelihoods . we have constructed smc and pmcmc based - solutions for abc approximations and investigated the bias associated to our procedure .",
    "there are several extensions to the work that has been considered here . from the perspective of parameter estimation ,",
    "we have only considered batch estimation by using pmcmc . in many practical problems ,",
    "one is often interested in performing statistical inference as samples arrive online .",
    "we are currently investigating methodology for this problem in @xcite and the theoretical and empirical work here is of great relevance in these latter ideas ; in particular when applying the online em algorithm as considered in @xcite . in this article",
    "we have focussed upon the forward - only smoothing technique in @xcite , however , this is not the only possibility ; one can also investigate the ideas in @xcite in the context of abc .",
    "in particular , the relative performance of these procedures is of interest .",
    "the first author was supported by an epsrc grant .",
    "the second author was supported by an moe grant .",
    "the second and fourth authors acknowledge assistance from the lms research in pairs grant .",
    "the second and third authors acknowledge assistance from an epsrc platform grant ep / i019111/1 .",
    "we give an analysis of the self - normalized estimate that was not the objective in @xcite and is explicit in @xmath1 . to that end , we introduce the following notations , to keep a consistent notation with @xcite on which our analysis relies .",
    "we set : @xmath251 to avoid notational overload , we will simply write @xmath252 , @xmath253 and @xmath254 , @xmath255 ( despite the independence of @xmath256 on only @xmath257 ) .",
    "we consider the approximation of the path measure @xmath258 , which is by definition : @xmath259 \\mathcal{v}_n(x_{0:n } ) \\eta_0(x_0 ) \\prod_{p=1}^n h(x_{p-1},x_p ) dx_{0:p}\\ ] ] where @xmath260f(x_n ) \\eta_0(x_0 ) \\prod_{p=1}^n h(x_{p-1},x_p ) dx_{0:p}\\ ] ] and @xmath55 .",
    "recall for additive functionals @xmath261 @xmath262 with @xmath263 ; c.f .  .",
    "we remind the reader that @xmath264 is a function on @xmath257 only .",
    "[ theo : smc_error ] assume ( a[hyp : a ] ) .",
    "then for any @xmath265 , there exist a @xmath266 such that for any @xmath0 , @xmath197 , @xmath112 , @xmath113 : @xmath267 \\bigg\\|_p \\leq \\frac{a_p(n+1)\\overline{\\delta}(\\epsilon)}{\\sqrt{n}}\\ ] ] where @xmath209 , with @xmath210 as in ( a[hyp : a ] ) ( [ hyp : smooth_abc ] ) and @xmath89 $ ] is the expectation w.r.t .",
    "the joint abc smoothing distribution .",
    "consider the decomposition : @xmath268 @xmath269 @xmath270 where we recall the normalized @xmath271time marginal @xmath272 .",
    "note that @xmath273 \\mathcal{v}_n(x_{0:n } ) \\eta_0(x_0 ) \\prod_{p=1}^n h(x_{p-1},x_p)dx_{0:p}\\\\ & = &   \\mathbb{e}^{\\epsilon}[\\mathcal{v}_n(x_{0:n})|y_{0:n}]\\end{aligned}\\ ] ] which is the quantity of interest .    to consider an @xmath202analysis ,",
    "we can split the two terms in via minkowski .",
    "we consider the first term : @xmath274 now , one has : @xmath275 .",
    "this is proved by induction , the initialization with @xmath119 being obvious , assuming for @xmath276 , one has : @xmath277 and one easily concludes .",
    "thus , it follows : @xmath278 @xmath279 by theorem 7.4.4 of @xcite that there exist some @xmath207 such that @xmath280 hence @xmath281 for some @xmath207 that does not depend upon @xmath3 or @xmath1 . to deal with the second term in one",
    "can use lemma [ lem : updated_lp_bound ] along with ( a[hyp : a ] ) ( [ hyp : smooth_abc ] ) to conclude .",
    "we provide a proof of lemma [ lem : updated_lp_bound ] . to that end , introduce the operator : @xmath282 where @xmath283 where all the conventions of @xcite are preserved . in the empirical measure in the final line ,",
    "one considers the mutated particles .",
    "we also use the convention @xmath284 .",
    "note for the backward kernel , when considering the filter @xmath285 one can write @xmath286    [ lem : updated_lp_bound ] assume ( a[hyp : a ] ) .",
    "then for any @xmath265 there exist a @xmath266 such that for any @xmath0 , @xmath197 , @xmath112 , @xmath113 : @xmath287 where @xmath288 , with @xmath289 as in ( a[hyp : a ] ) ( [ hyp : smooth_abc ] ) .",
    "the proof of this result follows the proof of theorem 3.2 of @xcite .",
    "the complications are the control of the oscillations of @xmath290 with a different function as well as obtaining a rate w.r.t .",
    "it is remarked that the application of the kintchine - type inequality in theorem 3.2 of @xcite will not add any dependence upon @xmath1 .",
    "using the definition of @xmath291 one has @xmath292 thus it follows that : @xmath293 where @xmath294 and @xmath295 . to deal with oscillations of @xmath296 we consider both sums separately .",
    "we being with the first term on the r.h.s .  of .",
    "in particular the difference with arguments @xmath297 and @xmath298 : @xmath299 then , treating the summands , one has @xmath300 + \\ ] ] @xmath301\\ ] ] which is clearly upper - bounded by @xmath302 .",
    "\\label{eq : bound1}\\ ] ]    now consider the second term on the r.h.s .  of .",
    "in particular , for each summand , one has after subtracting the function in @xmath298 from that in @xmath297 @xmath303 @xmath304\\ ] ] dealing with the two terms separately , one can easily show that the first term is upper - bounded by @xmath305 where @xmath306 . similarly using trivial manipulations",
    ", one can also show that the second term is upper - bounded by same term , yielding the upper - bound @xmath307    combining the bounds - one can deduce that : @xmath308 \\\\ & & +   4\\|g_{n,\\epsilon}\\|\\sum_{q=0}^{p-1}\\|v_q\\|b_{q , n,\\epsilon}^2 \\beta(s_{p , q,\\epsilon}).\\end{aligned}\\ ] ] thus , one has , via the proof of theorem 3.2 of @xcite : @xmath309 where @xmath310 +   4\\|g_{n,\\epsilon}\\|\\sum_{q=0}^{p-1}\\|v_q\\|b_{q , n,\\epsilon}^2 \\beta(s_{p , q,\\epsilon})\\bigg].\\ ] ]    to complete the proof we need to consider the term @xmath311 , to that end , we quote the following bounds which follow from ( a[hyp : a ] ) ; see @xcite and the citations therein for details : @xmath312 first consider the expression @xmath313\\ ] ] which is upper - bounded by @xmath314\\ ] ] with @xmath315 as in ( a[hyp : a ] ) ( [ hyp : function ] ) . by standard manipulations ,",
    "this is upper - bounded by @xmath316 for @xmath317 that does not depend upon @xmath3 or @xmath1 .",
    "second the expression @xmath318 which is upper - bounded by @xmath319 again , by standard manipulations one can upper - bound this latter expression by @xmath320 for @xmath317 that does not depend upon @xmath3 or @xmath1 ; we can now conclude .",
    "below we will repeatedly apply theorem 2 of @xcite . this",
    "can also be established under ( a[hyp : a ] ) for smoothed abc ; the proof is omitted and follows the description in @xcite .",
    "recall that the abc approximation of the joint smoothing density is : @xmath85 \\eta_0(x_0)\\prod_{i=1}^n f(x_{i-1},x_i)}{\\int_{\\mathbb{r}^{(n+1)d_x}}[\\prod_{i=0}^n \\int_{\\mathbb{r}^{d_y}}\\phi(\\frac{u - y_i}{\\epsilon})g(x_i , u)du ] \\eta_0(x_0)\\prod_{i=1}^n f(x_{i-1},x_i ) dx_{0:n}}.\\ ] ] it is stressed that the analysis here is performed by integrating the auxiliary data , whilst the smc analysis works on the joint space of auxiliary data and hidden state .",
    "we have @xmath114-\\mathbb{e}^{\\epsilon}[\\mathcal{v}_n(x_{0:n})|y_{0:n}]| = |\\sum_{p=0}^n \\int v_p(x_p)[\\hat{\\eta}_n(x_{p : n})-\\hat{\\eta}_{n,\\epsilon}(x_{p : n})]dx_{p : n}|\\ ] ] where @xmath321 and @xmath322 are the abc and true smoothers . using the backward representation of the smoothers ,",
    "one has the decomposition of the r.h.s . :",
    "@xmath323dx_{p : n}| =      one can now adopt a telescoping sum decomposition for each summand of the r.h.s .  of : @xmath327(m_{n - s :",
    "p+1}(v_p))\\bigg ) + [ \\hat{\\eta}_n -\\hat{\\eta}_{n,\\epsilon}](m_{n : p+1,\\epsilon}(v_p))\\ ] ] for the second term , one can use theorem 2 of @xcite . thus concentrating on the summands in the first term we have @xmath328(m_{n - s : p+1}(v_p ) ) & \\leq & \\|\\hat{\\eta}_n m_{n : n - s,\\epsilon}[m_{n - s+1}-m_{n - s+1,\\epsilon}]\\|_{tv}\\times\\\\ & & \\big(\\prod_{q = n - s}^{p+1 } \\beta(m_{q})\\big ) \\textrm{osc}(v_p)\\end{aligned}\\ ] ] via ( a[hyp : a ] ) ( [ hyp : density_control ] ) , ( [ hyp : function ] ) and lemma [ prop : backward_control ]",
    "it clearly follows that there exist a @xmath317 and @xmath329 which do not depend upon @xmath3 , @xmath113 @xmath1 such that @xmath330(m_{n - s : p+1}(v_p ) )   \\leq c \\epsilon \\xi^{p+s+2-n}.\\ ] ] as a result , we have @xmath114-\\mathbb{e}^{\\epsilon}[\\mathcal{v}_n(x_{0:n})|y_{0:n}]| \\leq c\\sum_{p=0}^n\\bigg[\\big(\\sum_{s=1}^{n - p}\\xi^{p+s+2-n}\\big ) + 1\\bigg]\\epsilon\\ ] ] for @xmath317 that does not depend upon @xmath3 , @xmath113 @xmath1 .",
    "elementary manipulations allow us to conclude .",
    "[ prop : backward_control ] assume ( a[hyp : a ] ) .",
    "then there exist a @xmath111 such that for any @xmath331 @xmath0 , @xmath113 and @xmath332 we have @xmath333\\varphi(z)| \\leq c \\epsilon\\ ] ] where @xmath334 and @xmath335 are defined in - and @xmath336 and @xmath337 are the abc and true filters .",
    "we have the decomposition : @xmath338\\varphi(z ) =   \\int \\varphi(z)f(z , x)\\bigg[\\frac{\\hat{\\eta}_{k,\\epsilon}(z ) - \\hat{\\eta}_k(z)}{\\int \\hat{\\eta}_{k,\\epsilon}(u)f(u , x ) du}\\ ] ] @xmath339f(u , x)du } { \\int \\hat{\\eta}_{k,\\epsilon}(u)f(x , u)du\\int \\hat{\\eta}_k(u)f(u , x ) du}\\bigg\\}\\bigg ] dz\\ ] ] where we have suppressed the data from the notation .    dealing with the first part",
    ", we have for some @xmath340 that does not depend upon @xmath341 or @xmath113 @xmath342dz \\leq   c \\epsilon \\|\\varphi\\|\\ ] ] where we have used ( a[hyp : a ] ) ( [ hyp : density_control ] ) in the denominator and to control @xmath343 as well as theorem 2 of @xcite .",
    "now , for the second part @xmath344f(u , x)du } { \\int \\hat{\\eta}_{k,\\epsilon}(u)f(x , u)du\\int \\hat{\\eta}_k(u)f(u , x ) du}\\bigg\\}\\bigg ] dz   \\leq \\|\\varphi\\| c \\epsilon\\ ] ] where , again ( a[hyp : a ] ) ( [ hyp : density_control ] ) has been applied along with theorem 2 of @xcite and @xmath340 does not depend upon @xmath341 or @xmath113 . using the uniformity in @xmath297 of the above bounds allows us to conclude .",
    ", s. j. , doucet , a. & west , m. ( 2004 ) .",
    "monte carlo smoothing for nonlinear time series .",
    "_ j. amer .",
    "_ , * 99 * , 156 - 168 . , a. , singh , s. , martin , j. , & mccoy , e.  ( 2012 ) . filtering via approximate bayesian computation .",
    "_ statist . comp _",
    "( to appear ) ."
  ],
  "abstract_text": [
    "<S> we consider a method for approximate inference in hidden markov models ( hmms ) . </S>",
    "<S> the method circumvents the need to evaluate conditional densities of observations given the hidden states . </S>",
    "<S> it may be considered an instance of approximate bayesian computation ( abc ) and it involves the introduction of auxiliary variables valued in the same space as the observations . </S>",
    "<S> the quality of the approximation may be controlled to arbitrary precision through a parameter @xmath0 . </S>",
    "<S> we provide theoretical results which quantify , in terms of @xmath1 , the abc error in approximation of expectations of additive functionals with respect to the smoothing distributions . under regularity assumptions , </S>",
    "<S> this error is @xmath2 , where @xmath3 is the number of time steps over which smoothing is performed . for numerical implementation </S>",
    "<S> we adopt the forward - only sequential monte carlo ( smc ) scheme of @xcite and quantify the combined error from the abc and smc approximations . </S>",
    "<S> this forms some of the first quantitative results for abc methods which jointly treat the abc and simulation errors , with a finite number of data and simulated samples . </S>",
    "<S> when the hmm has unknown static parameters , we consider particle markov chain monte carlo @xcite ( pmcmc ) methods for batch statistical inference . + </S>",
    "<S> * key - words * : smoothing , hidden markov models , approximate bayesian computation , sequential monte carlo , markov chain monte carlo . +    * approximate bayesian computation for smoothing *    by james s. martin@xmath4 , ajay jasra@xmath5 , sumeetpal s. singh@xmath6 , nick whiteley@xmath7 & emma mccoy@xmath8    @xmath4australian school of business , university of new south wales , sydney , 2052 , aus . </S>",
    "<S> + e-mail:`james.martin04@ic.ac.uk ` + @xmath5department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`staja@nus.edu.sg ` + @xmath6department of engineering , university of cambridge , cambridge , cb2 1pz , uk . </S>",
    "<S> + e-mail:`sss40@cam.ac.uk ` </S>",
    "<S> + @xmath7department of mathematics , university of bristol , bristol , bs8 1tw , uk . </S>",
    "<S> + e-mail:`nick.whiteley@bristol.ac.uk ` + @xmath8department of mathematics , imperial college london , london , sw7 2az , uk . </S>",
    "<S> + e-mail:`e.mccoy@ic.ac.uk ` </S>"
  ]
}