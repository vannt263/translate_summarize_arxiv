{
  "article_text": [
    "the optimization of a cost function which has a number of local minima is a relevant subject in all fields of science and engineering . in particular ,",
    "most of machine learning problems are stated like oftenly complex , optimization tasks @xcite . a common setup consist in the definition of appropriate families of models that should be selected from data .",
    "the selection step involves the optimization of a certain cost or likelihood function , which is usually defined on a high dimensional parameter space . in other approaches to learning , like bayesian inference @xcite , the entire landscape generated by the optimization problem associated with a set of models together with the data and",
    "the cost function is relevant .",
    "other areas in which global optimization plays a prominent role include operations research @xcite , optimal design in engineenered systems @xcite and many other important applications .",
    "stochastic strategies for optimization are essential to many of the heuristic techniques used to deal with complex , unstructured global optimization problems .",
    "methods like simulated annealing @xcite and evolutionary population based algorithms @xcite , have proven to be valuable tools , capable of give good quality solutions at a relatively small computational effort . in population",
    "based optimization , search space is explored through the evolution of finite populations of points .",
    "the population alternates periods of self  adaptation , in which particular regions of the search space are explored in an intensive manner , and periods of diversification in which solutions incorporate the gained information about the global landscape .",
    "there is a large amount of evidence that indicates that some exponents of population based algorithms are among the most efficient global optimization techniques in terms of computational cost and reliability .",
    "these methods , however are purely heuristic and convergence to global optima is not guaranteed .",
    "simulated annealing on the other hand , is a method that statistically assures global optimality , but in a limit that is very difficult to acomplish in practice . in simulated annealing a single particle explores the solution space through a diffusive process . in order to guarantee global optimality , the `` temperature '' that characterize the diffusion",
    "should be lowered according to a logarithmic schedule @xcite .",
    "this condition imply very long computation times .    in this contribution the convergence properties of an estimation procedure for the stationary density of a general class of stochastic search processes , recently introduced by the author @xcite ,",
    "is explored . by the estimation procedure",
    ", promising regions of the search space can be defined on a probabilistic basis .",
    "this information can then be used in connection with a locally adaptive stochastic or deterministic algorithm .",
    "preliminary applications of this density estimation method in the improvement of nonlinear optimization algorithms can be found in @xcite .",
    "theoretical aspects on the foundations of the method , its links to statistical mechanics and possible use of the density estimation procedure as a general diversification mechanism are discussed in @xcite . in the next section",
    "we give a brief account of the basic elements of our stationary density estimation algorithm .",
    "thereafter , theoretical and empirical evidence on the convergence of the density estimation is given .",
    "besides global optimization , the density estimation approach may provide a novel technique for maximum likelihood estimation and bayesian inference .",
    "this possibility , in the context of artificial neural network training , is outlined in section [ bayes ] .",
    "final conclusions and remarks are presented in section [ conclusion ] .",
    "we now proceed with a brief account of the stationary density estimation procedure on which the present work is based .",
    "consider the minimization of a cost function of the form @xmath0 with a search space defined over @xmath1 .",
    "a stochastic search process for this problem is modeled by    @xmath2    where @xmath3 is an additive noise with zero mean .",
    "equation ( [ langevin ] ) , known as langevin equation in the statistical physics literature @xcite , captures the essential properties of a general stochastic search . in particular , the gradient term gives a mechanism for local adaptation , while the noise term provides a basic diversification strategy .",
    "equation ( [ langevin ] ) can be interpreted as an overdamped nonlinear dynamical system composed by @xmath4 interacting particles in the presence of additive white noise .",
    "the stationary density estimation is based on an analogy with this physical system , considering reflecting boundary conditions .",
    "it follows that the stationary conditional density for particle @xmath5 satisfy the linear partial differential equation ,    @xmath6    which is a one dimensional fokker ",
    "planck equation . an important consequence of eq .",
    "( [ sfp ] ) is that the marginal @xmath7 can be sampled by drawing points from the conditional @xmath8 via a gibbs sampling @xcite . due to the linearity of the fokker ",
    "planck equation , a particular form of gibbs sampling can be constructed , such that its not only possible to sample the marginal density , but to give an approximate analytical expression for it . from eq .",
    "( [ sfp ] ) follows a linear second order differential equation for the cumulative distribution @xmath9 ,    @xmath10    the boundary conditions @xmath11 , @xmath12 came from the fact that the densities are normalized over the search space .",
    "random deviates can be drawn from the density @xmath8 by the inversion method @xcite , based on the fact that @xmath13 is an uniformly distributed random variable in the interval @xmath14 $ ] . viewed as a function of the random variable @xmath15",
    ", @xmath16 can be approximated through a linear combination of functions from a complete set that satisfy the boundary conditions in the interval of interest ,    @xmath17    choosing for instance , a basis in which @xmath18 , the @xmath19 coefficients are uniquely defined by the evaluation of eq .",
    "( [ sfpm ] ) in @xmath20 interior points . in this way",
    ", the approximation of @xmath13 is performed by solving a set of @xmath19 linear algebraic equations , involving @xmath20 evaluations of the derivative of @xmath21 .",
    "the basic sampling procedure , that we will call here stationary fokker  planck ( sfp ) sampling , is based on the iteration of the following steps :    * 1 ) * fix the variables @xmath22 and approximate @xmath16 by the use of formulas ( [ sfpm ] ) and ( [ set ] ) .    *",
    "2 ) * by the use of @xmath23 construct a lookup table in order to generate a deviate @xmath24 drawn from the stationary distribution @xmath25 .",
    "* 3 ) * update @xmath26 and repeat the procedure for a new variable @xmath27 .",
    "an algorithm for the automatic learning of the equilibrium distribution of the diffusive search process described by eq .",
    "( [ langevin ] ) can be based on the iteration of the three steps of the sfp sampling .",
    "a convergent representation for @xmath7 is obtained after taking the average of the coefficients @xmath28 s in the expansion ( [ set ] ) over the iterations . in order to see this , consider the expressions for the marginal density and the conditional distribution ,    @xmath29    @xmath30    from the last two equations follow that the marginal @xmath31 is given by the expected value of the conditional @xmath32 over the set @xmath33 ,    @xmath34 .\\end{aligned}\\ ] ]    all the information on the set @xmath33 is stored in the coefficients of the expansion ( [ set ] ) . therefore    @xmath35    where the brackets represent the average over the iterations of the sfp sampling .",
    "the marginals @xmath36 give the probability that a diffusive particle be at any region @xmath37 inside the search interval @xmath38 $ ] , under the action of the cost function .",
    "convergence of the stationary density estimation procedure depends on :    \\i ) the existence of the stationary state .",
    "\\ii ) convergence of the sfp sampling .",
    "conditions for the existence of the stationary state for general multi  dimensional fokker ",
    "planck equations can be found in @xcite . for our",
    "particular reflecting boundary case , in which the cost function and the diffusion coefficient do not depend on time , the basic requirement is the absence of singularities in the cost function .    by the evaluation of eq .",
    "( [ setav ] ) at each iteration of a sfp sampling the stationary density associated with the stochastic search can be estimated , and the accuracy of the estimate improves over time .",
    "we call this procedure a stationary fokker  planck learning ( sfpl ) of a density .",
    "the convergence of the sfpl follows from the convergence of gibbs sampling .",
    "it is known that under general conditions a gibbs sampling displays geometric convergence @xcite .",
    "fast convergence is an important feature for the practical value of sfpl like a diversification mechanism in optimization problems .",
    "the rigorous study of the links between the geometric convergence conditions ( stated in @xcite as conditions on the kernel in a markov chain ) with sfpl applied on several classes of optimization problems , should be a relevant research topic . at this point ,",
    "some numerical experimentation on the convergence of sfpl is presented .    in",
    "what follows , the specific form of the expansion ( [ set ] )    @xmath39    is used .",
    "the estimation algorithm converges in one iteration for separable problems .",
    "a separable function is given by a linear combination of terms , where each term involves a single variable .",
    "separable problems generate an uncoupled dynamics of the stochastic search described by eq .",
    "( [ langevin ] ) .",
    "this behavior is illustrated by the minimization of the michalewicz s function , a common test function for global optimization algorithms @xcite .",
    "the michalewicz s function in a two dimensional search space is written as    @xmath40    the search space is @xmath41 .",
    "the michalewicz s function is interesting as a test function because for large values of @xmath42 the local behavior of the function gives little information on the location of the global minimum .",
    "for @xmath43 the global minimum of the two dimensional michalewicz s function has been estimated has @xmath44 and is roughly located around the point @xmath45 , as can be seen by plotting the function .",
    "the partial derivatives of function ( [ micha ] ) with @xmath43 , have been evaluated for each variable at @xmath20 equidistant points separated by intervals of size @xmath46 .",
    "the resulting algebraic linear system has been solved by the lu decomposition algorithm @xcite . in fig .",
    "( [ fig1 ] ) , fig .",
    "( [ fig2 ] ) and fig .",
    "( [ fig3 ] ) the functions @xmath47 and @xmath48 and their associated probability densities are shown .",
    "the densities have been estimated after a single iteration of sfpl .",
    "the densities @xmath49 and @xmath50 are straightforwardly calculated by taking the corresponding derivatives . in fig .",
    "( [ fig1 ] ) a case with @xmath51 and @xmath52 is considered , while in fig .",
    "( [ fig2 ] ) @xmath51 and @xmath53 . in fig .",
    "( [ fig3 ] ) a smaller randomness parameter is considered ( @xmath54 ) , using @xmath55 .",
    "notice that even when @xmath56 is high enough to allow an approximation of @xmath13 with the use of very few evaluations of the derivatives , the resulting densities will give populations that represent the cost function landscape remarkably better than those that would be obtained by uniform deviates .",
    "the asymptotic convergence properties of the sfpl are now experimentally studied on the xor optimization problem ,    @xmath57 ^{-1 }              \\right\\rbrace^2 \\\\",
    "\\nonumber      + \\left\\lbrace 1 -                   \\left [ 1 + exp\\left (                       - \\frac{x_7 } { 1 + exp ( - x_2 - x_5 ) }                      - \\frac{x_8 } { 1 + exp ( - x_4 - x_6 ) }                      - x_9                         \\right )                   \\right ] ^{-1 }              \\right\\rbrace^2 \\end{aligned}\\ ] ]    the xor function is an archetypical example that displays many of the features encountered in the optimization tasks that arise in machine learning .",
    "this is a case with multiple local minima @xcite and strong nonlinear interactions between decision variables . in the experiment reported in fig .",
    "[ fig : obxor ] and fig .",
    "[ fig : conxor ] , two independent trajectories are followed over successive iterations .",
    "the parameters of the sfp sampler are @xmath58 and @xmath59 . in fig .",
    "[ fig : obxor ] are reported the cost function values at the coordinates in which the marginals are maximum . for each trajectory ,",
    "an initial point is uniformly drawn from the search space . as can be seen ,",
    "both trajectories converge to a similarly small value of the objective function .",
    "the average cost function value , which is estimated by the evaluation of the cost function on @xmath60 points uniformly distributed over the search space , is @xmath61 .",
    "after @xmath62 iterations , the differences between both trajectories are around @xmath63 of the average cost function value . moreover , the differences in objective value of the trajectories with respect to a putative global optimum    @xmath64    is @xmath65 of the average cost after the iteration @xmath62 .",
    "the putative global optimum in the search interval has been found by performing local search via steepest descent from a population of points draw from the estimated density .",
    "evaluation of @xmath13 and @xmath66 by one iteration of the sfpl algorithm for the michalewicz s function , using @xmath52 and @xmath51 . despite the very low number of gradient evaluations used ,",
    "the algorithm is capable to find a probability structure that is consistent with the global properties of the cost function.,title=\"fig : \" ] 1.0 cm   evaluation of @xmath13 and @xmath66 by one iteration of the sfpl algorithm for the michalewicz s function , using @xmath52 and @xmath51 . despite the very low number of gradient evaluations used ,",
    "the algorithm is capable to find a probability structure that is consistent with the global properties of the cost function.,title=\"fig : \" ]     the same case reported in fig .",
    "( [ fig1 ] ) , but using @xmath53.,title=\"fig : \" ] 1.0 cm   the same case reported in fig .",
    "( [ fig1 ] ) , but using @xmath53.,title=\"fig : \" ]     evaluation of @xmath13 and @xmath66 by one iteration of the sfpl algorithm for the michalewicz s function . in this case @xmath55 and @xmath54 . with the increment in precision and the reduction of the randomness parameter",
    ", sfpl finds a probability density that is sharply peaked around the global minimum .",
    "notice that the computational effort is still small , involving only @xmath67 evaluations of the gradient.,title=\"fig : \" ] 1.0 cm   evaluation of @xmath13 and @xmath66 by one iteration of the sfpl algorithm for the michalewicz s function . in this case",
    "@xmath55 and @xmath54 . with the increment in precision and the reduction of the randomness parameter",
    ", sfpl finds a probability density that is sharply peaked around the global minimum .",
    "notice that the computational effort is still small , involving only @xmath67 evaluations of the gradient.,title=\"fig : \" ]        in order to check statistical convergence , the following measures are introduced ,    @xmath68    where the brackets in this case represent statistical moments of the estimated marginals . under the expansion ( [ set2 ] ) , all the necessary integrals",
    "are easily performed analytically .     iterations .",
    "asymptotically the distance behave like a power law characterized by @xmath69 , where @xmath70 is the number of iterations .",
    ", title=\"fig : \" ] 1.0 cm    iterations .",
    "asymptotically the distance behave like a power law characterized by @xmath69 , where @xmath70 is the number of iterations .",
    ", title=\"fig : \" ]    in the first graph of fig .",
    "[ fig : conxor ] , the evolution over iterations of the sfp sampler of @xmath71 and @xmath72 for two arbitrary and independent trajectories is plotted .",
    "a very fast convergence in the measure @xmath72 is evident .",
    "the measure @xmath71 is further studied in the second graph of fig .",
    "[ fig : conxor ] , where the difference on that measure among the two trajectories is followed over iterations .",
    "the convergence is consistent with a geometric behavior over the first ( @xmath73 ) iterations and shows an asymptotic power law rate .",
    "besides its applicability like a diversification strategy for local search algorithms , the fast convergence of the sfpl could be fruitful to give efficient aproaches to inference , for instance in the training of neural networks . from the point of view of statistical inference , the uncertainty about unknown parameters of a learning machine is characterized by a posterior density for the parameters given the observed data @xcite .",
    "the prediction of new data is then performed either by the maximization of this posterior ( maximum likelihood estimation ) or by an ensemble average over the posterior distribution ( bayesian inference ) . to be specific ,",
    "suposse a _ system _ which generates an output @xmath74 given an input @xmath75 , such that the data is described by a distribution with first moment @xmath76 = f(x , w)$ ] and diagonal covariance matrix @xmath77 .",
    "the problem is to estimate @xmath78 from a given set of observations @xmath79 .",
    "the parameters could be , for instance , different neural network weights and architectures .",
    "the observed data defines an evidence for the different ensemble members , given by the posterior @xmath80 . in maximum likelihood estimation , training consists on finding a single set of optimal parameters that maximize @xmath80 .",
    "bayesian inference , on the other hand , is based on the fact that the estimator of @xmath78 that minimizes the expected squared error under the posterior is given by @xcite    @xmath81    so training is done by estimating this ensemble average .    in the sfpl framework proposed here",
    ", priors are always given by uniform densities .",
    "this choice involves very few prior assumptions , regarding the assignment of reasonable intervals on which the components of @xmath82 lie . under an uniform prior , and if the data present in the sample has been independently drawn , it turns out that the posterior is given by    @xmath83    where @xmath84 and @xmath21 is the given loss function .",
    "the sfpl algorithm can be therefore directly applied in order to learn the marginals @xmath85 of the posterior ( [ posterior ] ) . by construction",
    ", these marginals will be properly normalized .",
    "it is now argued that sfpl can be used to efficiently perform maximum likelihood and bayesian training .",
    "consider again the xor example .",
    "the associated density has been estimated assuming a prior density for each parameter @xmath86 over the interval @xmath87 $ ] .",
    "the posterior density , on the other hand , is a consequence of the cost function given the set of training data . in section [ converge ]",
    "it has been shown that for nonseparable nonlinear cost functions like in the xor case , sfpl converges to a correct estimation of the marginal densities @xmath88 .",
    "therefore , the maximization of the likelihood is reduced to @xmath4 line maximizations , where @xmath4 is the number of weights to be estimated .",
    "the advantage of this procedure in comparision with the direct maximization of @xmath80 is evident . on the other hand ,",
    "the sfp sampler itself is designed as a generator of deviates that are drawn from the stationary density . the average ( [ av ] ) can be approximated by    @xmath89    without the need of direct simulations of the stochastic search , which is necessary in most of other techniques @xcite .    in fig .",
    "[ fig : weightdensity ] is shown the behavior of @xmath88 for a particular weight as the sample size increases .",
    "the parameters @xmath59 and @xmath58 are fixed .",
    "the two dotted lines correspond to cases with sample sizes of one and two , with inputs @xmath90 and @xmath91 respectively .",
    "the resulting densities are almost flat in both situations .",
    "the dashed line corresponds to a sample size of three .",
    "the sample points are @xmath92 . in this case",
    "the sample is large enough to give a sufficient evidence to favor a particular region of the parameter domain .",
    "the solid line corresponds to the situation in which all the four points of the data set are used for training .",
    "the resulting density is the sharpest .",
    "the parameter @xmath56 is proportional to the noise strength in the stochastic search .",
    "it can be selected on the basis of a desired computational effort , as discussed in @xcite .",
    "figure [ fig : weightdensity ] indicates that at a fixed noise level @xmath56 , an increase of evidence imply a decrease on the uncertainty of the weights .",
    "this finding agrees with what is expected from the known theory of the statistical mechanics of neural networks @xcite , according to which the weight fluctuations decay as the data sample grows .    , a bias of one of the neurons in the hidden layer ) of the ann model for the xor problem .",
    "the dotted lines correspond to cases with sample sizes of one and two .",
    "the dashed line is for the density that results from a sample of size three while the case for a sample size of four is given by the solid line . ]",
    "the performance of maximum likelihood and bayesian training is reported in fig .",
    "[ fig : train ] , using the complete sample for the inference of the weights .",
    "the standard deviation of the error of the networks instantiated at the inferred weigths is reported at each iteration .",
    "the solid line corresponds to maximum likelihood training , which is essentially the same calculation already reported on fig .",
    "[ fig : obxor ] .",
    "the performance is very similar for bayesian training , which corresponds to the dashed line .",
    "the estimation of the average ( [ av2 ] ) as been performed by evaluating the neural network on a weight vector drawn by the sfp sampler at each iteration . in this way ,",
    "the number of terms in the sum of eq .",
    "( [ av2 ] ) is equal to the number of iterations of the sfp sampler",
    ".     sample points . ]    a larger example involving noisy data is now presented .",
    "consider the `` robot arm problem '' , a benchmark that has already been used in the context of bayesian inference @xcite .",
    "the data set is generated by the following dynamical model for a robot arm    @xmath93    the inputs @xmath94 and @xmath95 represent joint angles while the outputs @xmath96 and @xmath97 give the resulting arm positions . following the experimental setup proposed in @xcite , the inputs are uniformly distributed in the intervals @xmath98 \\cup [ 0.453 , 1.932]$ ] , @xmath99 $ ] .",
    "the noise terms @xmath100 and @xmath101 are gaussian and white with standard deviation of @xmath102 .",
    "a sample of @xmath103 points is generated using these prescriptions .",
    "a neural network with one hidden layer consisting on @xmath104 hyperbolic tangent activation functions is trained on the generated sample using sfp learning , considering a squared error loss function .",
    "the same priors are assigned to all of the weights : uniform distributions in the interval @xmath105 $ ] .",
    "the average absolute difference in training errors for two independent trajectories is shown on fig .",
    "[ fig : robot ] for a case in which @xmath106 , @xmath107 and @xmath108 iterations . taking into account that the expected equilibrium square error is @xmath109 , it turns out that the differences between both trajectories are of the same order of magnitude as the expected equilibrium error in about @xmath110 iterations . during the course of the total of @xmath111 iterations of sfp of one of the trajectories , the network as been evaluated in the test input @xmath112 .",
    "the resulting bayesian prediction is shown on fig .",
    "[ fig : histogram1 ] in the form of a pair of histograms .",
    "the output that would be given by the exact model ( [ robot ] ) in the absence of noise is @xmath113 .",
    "the bayesian prediction given by sfpl has its mean at @xmath114 with a standard deviation of @xmath115 for each variable .",
    "therefore the output given by the underlying model is contained in the @xmath116 confidence interval around the bayes expectation .",
    "consistent predictions can also be obtained under less precision and data . in fig .",
    "[ fig : histogram2 ] are shown the histograms obtained for a case with a sample size of @xmath117 points , @xmath118 and @xmath119 .",
    "altough the bayes prediction is more uncertain , it still is statistically consistent with the underlying process .    .",
    "the sfpl parameters are @xmath106 , @xmath107 and @xmath108 , using @xmath103 sample points . ]     but with sfpl parameters given by @xmath118 , @xmath119 , @xmath120 and sample size of @xmath117 points . ]    in the robot arm example the number of gradient evaluations needed to approximately converge to the equilibrium density in the @xmath121 case was about @xmath122 .",
    "this seems competitive with respect to previous approaches , like the hybrid monte carlo strategy introduced by neal .",
    "the reader is referred to neal s book @xcite in order to find a very detailed application of hybrid monte carlo to the robot arm problem .",
    "an additional advantage of the sfpl method lies on the fact that explicit expressions for the parameter s densities are obtained .",
    "much more detailed experimentation is under current development .",
    "additional studies regarding issues like generalization of more complex ann models under a limited amount of data , in the spirit of the general framework for bayesian learning @xcite , is currently a work in progress by the author .",
    "theoretical and empirical evidence for the characterization of the convergence of the density estimation of stochastic search processes by the method of stationary fokker ",
    "planck learning as been presented . in the context of nonlinear optimization problems , the procedure turns out to converge in one iteration for separable problems and displays fast convergence for nonseparable cost functions .",
    "the possible applications of stationary fokker  planck learning in the development of efficient and reliable maximum likelihood and bayesian ann training techniques have been outlined .",
    "this work was partially supported by the national council of science and technology of mexico under grant conacyt j45702-a .",
    "a. berrones , generating random deviates consistent with the long term behavior of stochastic search processes in global optimization , in : proc .",
    "iwann 2007 , lecture notes in computer science , vol .",
    "4507 ( springer , berlin , 2007 ) 1 - 8 .",
    "d. pe~ na , r. snchez , a. berrones , stationary fokker ",
    "planck learning for the optimization of parameters in nonlinear models , in : proc .",
    "micai 2007 , lecture notes in computer science vol .",
    "4827 , ( springer , berlin , 2007 ) 94 - 104 ."
  ],
  "abstract_text": [
    "<S> the convergence properties of the stationary fokker - planck algorithm for the estimation of the asymptotic density of stochastic search processes is studied . </S>",
    "<S> theoretical and empirical arguments for the characterization of convergence of the estimation in the case of separable and nonseparable nonlinear optimization problems are given . </S>",
    "<S> some implications of the convergence of stationary fokker - planck learning for the inference of parameters in artificial neural network models are outlined .    </S>",
    "<S> heuristics , optimization , stochastic search , statistical mechanics </S>"
  ]
}