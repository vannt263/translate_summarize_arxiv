{
  "article_text": [
    "this paper focuses on dimension reduction in @xmath0 , in particular , on the method based on _ cauchy random projections _",
    "@xcite , which is special case of _ linear random projections_.    the idea of _ linear random projections _ is to multiply the original data matrix @xmath11 with a random projection matrix @xmath12 , resulting in a projected matrix @xmath13 . if @xmath14 , then it should be much more efficient to compute certain summary statistics ( e.g. , pairwise distances ) from @xmath15 as opposed to @xmath5 .",
    "moreover , @xmath15 may be small enough to reside in physical memory while @xmath5 is often too large to fit in the main memory .",
    "the choice of the random projection matrix @xmath16 depends on which norm we would like to work with .",
    "@xcite proposed constructing @xmath16 from i.i.d .",
    "samples of @xmath17-stable distributions , for dimension reduction in @xmath18 ( @xmath19 ) . in the stable distribution family @xcite ,",
    "normal is 2-stable and cauchy is 1-stable .",
    "thus , we will call random projections for @xmath9 and @xmath0 , _ normal random projections _ and _ cauchy random projections _ , respectively .    in _ normal random projections _",
    "@xcite , we can estimate the original pairwise @xmath9 distances of @xmath5 directly using the corresponding @xmath9 distances of @xmath15 ( up to a normalizing constant ) .",
    "furthermore , the johnson - lindenstrauss ( jl ) lemma @xcite provides the performance guarantee .",
    "we will review _ normal random projections _ in more detail in section [ sec_intr_rp ] .    for _ cauchy random projections _",
    ", we should not use the @xmath0 distance in @xmath15 to approximate the original @xmath0 distance in @xmath5 , as the cauchy distribution does not even have a finite first moment .",
    "the impossibility results @xcite have proved that one can not hope to recover the @xmath0 distance using linear projections and linear estimators ( e.g. , sample mean ) , without incurring large errors .",
    "fortunately , the impossibility results do not rule out nonlinear estimators , which may be still useful in certain applications in data stream computation , information retrieval , learning , and data mining .",
    "@xcite proposed using the sample median ( instead of the sample mean ) in _ cauchy random projections _ and described its application in data stream computation . in this study , we provide three types of nonlinear estimators : the bias - corrected sample median estimator , the bias - corrected geometric mean estimator , and the bias - corrected maximum likelihood estimator .",
    "the sample median estimator and the geometric mean estimator are asymptotically equivalent ( i.e. , both are about @xmath10 efficient as the maximum likelihood estimator ) , but the latter is more accurate at small sample size @xmath8 . furthermore , we derive explicit tail bounds for the bias - corrected geometric mean estimator and establish an analog of the jl lemma for dimension reduction in @xmath0 .",
    "this analog of the jl lemma for @xmath0 is weaker than the classical jl lemma for @xmath9 , as the geometric mean estimator is a non - convex norm and hence is not a metric .",
    "many efficient algorithms , such as some sub - linear time ( using super - linear memory ) nearest neighbor algorithms @xcite , rely on the metric properties ( e.g. , the triangle inequality ) .",
    "nevertheless , nonlinear estimators may be still useful in important scenarios .    * _ estimating @xmath0 distances online _",
    "+ the original data matrix @xmath11 requires @xmath20 storage space ; and hence it is often too large for physical memory .",
    "the storage cost of all pairwise distances is @xmath21 , which may be also too large for the memory .",
    "for example , in information retrieval , @xmath22 could be the total number of word types or documents at web scale .",
    "to avoid page fault , it may be more efficient to estimate the distances on the fly from the projected data matrix @xmath15 in the memory . * _ computing all pairwise @xmath0 distances _ + in distance - based clustering and classification applications , we need to compute all pairwise distances in @xmath5 , at the cost of time @xmath23 . using _",
    "cauchy random projections _",
    ", the cost can be reduced to @xmath24 .",
    "because @xmath14 , the savings could be enormous . * _ linear scan nearest neighbor searching _",
    "+ we can always search for the nearest neighbors by linear scans .",
    "when working with the projected data matrix @xmath15 ( which is in the memory ) , the cost of searching for the nearest neighbor for one data point is time @xmath25 , which may be still significantly faster than the sub - linear algorithms working with the original data matrix @xmath5 ( which is often on the disk ) .",
    "we briefly comment on _ coordinate sampling _",
    ", another strategy for dimension reduction . given a data matrix @xmath26 , one can randomly sample @xmath8 columns from @xmath5 and estimate the summary statistics ( including @xmath0 and @xmath9 distances ) . despite its simplicity , there are two major disadvantages in coordinate sampling .",
    "first , there is no performance guarantee . for heavy - tailed data",
    ", we may have to choose @xmath8 very large in order to achieve sufficient accuracy .",
    "second , large datasets are often highly sparse , for example , text data @xcite and market - basket data @xcite . @xcite and @xcite",
    "provide an alternative coordinate sampling strategy , called _ conditional random sampling ( crs ) _ , suitable for sparse data .",
    "for non - sparse data , however , methods based on _ linear random projections _ are superior .",
    "the rest of the paper is organized as follows .",
    "section [ sec_intr_rp ] reviews _ linear random projections_. section [ sec_results ] summarizes the main results for three types of nonlinear estimators .",
    "section [ sec_median ] presents the sample median estimators .",
    "section [ sec_gm ] concerns the geometric mean estimators .",
    "section [ sec_mle ] is devoted to the maximum likelihood estimators .",
    "section [ sec_conclusion ] concludes the paper .",
    "we give a review on _ linear random projections _ , including _",
    "normal _ and _ cauchy random projections_.    denote the original data matrix by @xmath27 , i.e. , @xmath22 data points in @xmath28 dimensions .",
    "let @xmath29 be the @xmath30th row of @xmath5 .",
    "let @xmath31 be a random matrix whose entries are i.i.d",
    ". samples of some random variable . the projected data matrix @xmath13 .",
    "denote the entries of @xmath16 by @xmath32 and let @xmath33 be the @xmath30th row of @xmath15 .",
    "then @xmath34 , with entries @xmath35 , i.i.d .",
    "@xmath36 to @xmath8 , where @xmath37 is the @xmath38th column of @xmath16 .    for simplicity , we focus on the leading two rows , @xmath39 and @xmath40 , in @xmath5 , and the leading two rows , @xmath41 and @xmath42 , in @xmath15 .",
    "define @xmath43 to be @xmath44    if we sample @xmath45 i.i.d . from a _ stable distribution _",
    "@xcite , then @xmath46 s are also i.i.d .",
    "samples of the same stable distribution with a different scale parameter . in the family of stable distributions ,",
    "normal and cauchy are two important special cases .",
    "when @xmath45 is sampled from the standard normal , i.e. , @xmath47 , i.i.d . , then @xmath48 because a weighted sum of normals is also normal .",
    "denote the squared @xmath9 distance between @xmath39 and @xmath40 by @xmath49 .",
    "we can estimate @xmath50 from the sample squared @xmath9 distance : @xmath51 it is easy to show that ( e.g. , @xcite ) @xmath52    we would like to bound the error probability @xmath53 by @xmath54 . since there are in total @xmath55 pairs among @xmath22 data points , we need to bound the tail probabilities simultaneously for all pairs . by the bonferroni union bound , it suffices if @xmath56    using ( [ eqn_normal_tail ] ) , it suffices if @xmath57    therefore , we obtain one version of the jl lemma :     _ if @xmath58 , then with probability at least @xmath59 , the squared @xmath9 distance between any pair of data points ( among @xmath22 data points ) can be approximated within @xmath60 fraction of the truth , using the squared @xmath9 distance of the projected data after normal random projections .",
    "_    many versions of the jl lemma have been proved @xcite .",
    "note that we do not have to use @xmath61 for dimension reduction in @xmath9 .",
    "for example , we can sample @xmath45 from some _ sub - gaussian distributions _ @xcite , in particular , the following _ sparse projection distribution _ : @xmath62    when @xmath63 , @xcite proved the jl lemma for the above sparse projection , which can also be shown by sub - gaussian analysis @xcite .",
    "recently , @xcite proposed _ very sparse random projections _ using @xmath64 in ( [ eqn_subg_rji ] ) , based on two practical considerations :",
    "* @xmath28 should be very large , otherwise there would be no need for dimension reduction .",
    "* the original @xmath9 distance should make engineering sense , in that the second ( or higher ) moments should be bounded ( otherwise various _ term - weighting _ schemes will be applied ) .",
    "based on these two practical assumptions , the projected data are asymptotically normal at a fast rate of convergence when @xmath64 . of course , _ very sparse random projections _ do not have worst case performance guarantees .      in _ cauchy random projections _ , we sample @xmath45 i.i.d . from the standard cauchy distribution ,",
    "i.e. , @xmath65 . by the 1-stability of cauchy @xcite ,",
    "we know that @xmath66 that is , the projected differences @xmath67 are also cauchy random variables with the scale parameter being the @xmath0 distance , @xmath68 , in the original space .",
    "recall that a cauchy random variable @xmath69 has the density @xmath70    the easiest way to see the 1-stability is via the characteristic function , @xmath71 for @xmath72 , @xmath73 , ... , @xmath74 ,",
    "@xmath75 , and any constants @xmath76 , @xmath77 , ... , @xmath78 .    therefore , in _ cauchy random projections _",
    ", the problem boils down to estimating the cauchy scale parameter of @xmath79 from @xmath8 i.i.d .",
    "samples @xmath80 .",
    "unfortunately , unlike in _ normal random projections _ , we can no longer estimate @xmath81 from the sample mean ( i.e. , @xmath82 ) because @xmath83 .",
    "although the impossibility results @xcite have ruled out estimators that are metrics , there is enough information to recover @xmath81 from @xmath8 samples @xmath43 , with a high accuracy .",
    "for example , @xcite proposed using the sample median as an estimator .",
    "the problem with the sample median estimator is the inaccuracy at small @xmath8 and the difficulty in deriving explicit tail bounds needed for determining the sample size @xmath8 . +",
    "this study focuses on deriving better estimators and explicit tail bounds for _ cauchy random projections_. our main results are summarized in the next section , before we present the detailed derivations .",
    "casual readers may skip these derivations after section [ sec_results ] .",
    "we propose three types of nonlinear estimators : the bias - corrected sample median estimator ( @xmath84 ) , the bias - corrected geometric mean estimator ( @xmath85 ) , and the bias - corrected maximum likelihood estimator ( @xmath86 ) . @xmath84 and @xmath85 are asymptotically equivalent but the latter is more accurate at small sample size @xmath8 . in addition , we derive explicit tail bounds for @xmath85 , from which an analog of the johnson - lindenstrauss ( jl ) lemma for dimension reduction in @xmath0 follows .",
    "asymptotically , both @xmath84 and @xmath85 are @xmath87 efficient compared to the maximum likelihood estimator @xmath86 .",
    "we propose accurate approximations to the distribution and tail bounds of @xmath86 , while the exact closed - form answers are not attainable .",
    "denoted by @xmath84 , the bias - corrected sample median estimator is @xmath88 where @xmath89    here , for convenience , we only consider @xmath90 , @xmath91 = 1 , 2 , 3 , ...    some key properties of @xmath84 :    * @xmath92 , i.e , @xmath84 is unbiased . * when @xmath93 , the variance of @xmath84 is @xmath94 @xmath95 if @xmath96 .",
    "* as @xmath97 , @xmath84 converges to a normal in distribution @xmath98      denoted by @xmath85 , the bias - corrected geometric mean estimator is defined as @xmath99    important properties of @xmath85 include :    * this estimator is a non - convex norm , i.e. , the @xmath18 norm with @xmath100 . *",
    "it is unbiased , i.e. , @xmath101 . *",
    "its variance is ( for @xmath102 ) @xmath103 * for @xmath104 , its tail bounds can be represented in exponential forms @xmath105 * these exponential tail bounds yield an analog of the johnson - lindenstrauss ( jl ) lemma for dimension reduction in @xmath0 : +   _ if @xmath106 , then with probability at least @xmath59 , one can recover the original @xmath0 distance between any pair of data points ( among all @xmath22 data points ) within @xmath107 ( @xmath108 ) fraction of the truth , using @xmath85 , i.e. , @xmath109 . _      denoted by @xmath86 , the bias - corrected maximum likelihood estimator is @xmath110 where @xmath111 solves a nonlinear mle equation @xmath112    some properties of @xmath86 :    * it is nearly unbiased , @xmath113 . *",
    "its asymptotic variance is @xmath114 i.e. , @xmath115 , @xmath116 , as @xmath117 .",
    "( @xmath118 ) * its distribution can be accurately approximated by an inverse gaussian , at least in the small deviation range .",
    "based on the inverse gaussian approximation , we suggest the following approximate tail bound @xmath119 which has been verified by simulations for the tail probability @xmath120 range .",
    "recall in cauchy random projections , @xmath121 , we denote the leading two rows in @xmath5 by @xmath39 , @xmath40 @xmath122 , and the leading two rows in @xmath15 by @xmath41 , @xmath42 @xmath123 .",
    "our goal is to estimate the @xmath0 distance @xmath124 from @xmath43 , @xmath125 , i.i.d .",
    "it is easy to show ( e.g. , @xcite ) that the population median of @xmath126 is @xmath81 .",
    "therefore , it is natural to consider estimating @xmath81 from the sample median , @xmath127    as illustrated in the following lemma ( proved in appendix [ app_proof_lem_me ] ) , the sample median estimator , @xmath128 , is asymptotically unbiased and normal . for small samples ( e.g. , @xmath129 ) , however , @xmath128 is severely biased .",
    "[ lem_me ] the sample median estimator , @xmath128 , defined in ( [ eqn_def_me ] ) , is asymptotically unbiased and normal @xmath130 when @xmath90 , @xmath91 = 1 , 2 , 3 , ... , the @xmath131 moment of @xmath128 can be represented as @xmath132 if @xmath133 , then @xmath134 .",
    "+   +    for simplicity , we only consider @xmath90 when evaluating @xmath135 .",
    "once we know @xmath136 , we can remove the bias of @xmath128 using @xmath137 where the bias correction factor @xmath138 is @xmath139    @xmath138 can be numerically evaluated and tabulated , at least for small @xmath8 . as an infinite sum .",
    "note that @xmath140 , @xmath141 , is the probability density of a beta distribution @xmath142 .",
    "] obviously , @xmath84 is unbiased , i.e. , @xmath92",
    ". its variance would be @xmath143    of course , @xmath85 and @xmath144 are asymptotically equivalent , i.e. , @xmath145 .",
    "figure [ fig_bme ] plots @xmath138 as a function of @xmath8 , indicating that @xmath128 is severely biased when @xmath129 .",
    "when @xmath146 , the bias becomes negligible .",
    "note that , because @xmath147 , the bias correction not only removes the bias of @xmath128 but also reduces its variance .",
    "the sample median is a special case of sample quantile estimators @xcite .",
    "for example , one version of the quantile estimators given by @xcite would be @xmath148 where @xmath149 and @xmath150 are the .75 and .25 sample quantiles of @xmath151 , respectively .",
    "our simulations indicate that @xmath128 actually slightly outperforms @xmath152 .",
    "this is not surprising .",
    "@xmath152 works for any cauchy distribution whose location parameter does not have to be zero , while @xmath128 takes advantage of the fact that the cauchy location parameter is always zero in our case .",
    "this section derives estimators based on the geometric mean , which are more accurate than the sample median estimators .",
    "the geometric mean estimators allow us to derive tail bounds in explicit forms and ( consequently ) an analog of the johnson - lindenstrauss ( jl ) lemma for dimension reduction in @xmath0 .",
    "recall , our goal is to estimate @xmath81 from @xmath8 i.i.d .",
    "samples @xmath153 . to help derive the geometric mean estimators , we first study two nonlinear estimators based on the fractional moment , i.e. , @xmath154 ( @xmath155 ) and the logarithmic moment , i.e , @xmath156 , respectively , as presented in lemma [ lem_d_log ] .",
    "see the proof in appendix [ app_proof_lem_d_log ] .",
    "[ lem_d_log ] assume @xmath157 .",
    "then @xmath158 from which we can derive two biased estimators of @xmath81 from @xmath8 i.i.d .",
    "samples @xmath80 : @xmath159 whose variances are , respectively , @xmath160    the term @xmath161 decreases with decreasing @xmath162 , reaching a limit @xmath163 in other words , the variance of @xmath164 converges to that of @xmath165 as @xmath162 approaches zero . +    note that @xmath165 can in fact be written as the _ geometric mean _ : @xmath166    @xmath164 is a non - convex norm ( @xmath167 ) because @xmath168 .",
    "@xmath144 is also a non - convex norm ( the @xmath167 norm as @xmath169 ) .",
    "both @xmath164 and @xmath144 do not satisfy the triangle inequality .",
    "we propose @xmath85 , the bias - corrected geometric mean estimator .",
    "lemma [ lem_d_gm ] derives the moments of @xmath85 , proved in appendix [ app_proof_lem_d_gm ] .",
    "[ lem_d_gm ] @xmath99 is unbiased , with the variance ( valid when @xmath102 ) @xmath170    the third and fourth central moments are ( for @xmath171 and @xmath172 , respectively ) @xmath173 +    the higher ( third or fourth ) moments may be useful for approximating the distribution of @xmath85 . in section [ sec_mle ] , we will show how to approximate the distribution of the maximum likelihood estimator by matching the first four moments ( in the leading terms ) .",
    "we could apply the similar technique to approximate @xmath85 .",
    "fortunately , we do not have to do so because we are able to derive the exact tail bounds of @xmath85 in lemma [ lem_d_gm_tail ] , which is proved in appendix [ app_proof_lem_d_gm_tail ] .",
    "[ lem_d_gm_tail ] @xmath174 where @xmath175 @xmath176 where @xmath177    by restricting @xmath178 , the tail bounds can be written in exponential forms : @xmath179 +    an analog of the jl bound for @xmath0 follows from the exponential tail bounds ( [ eqn_exp_right ] ) and ( [ eqn_exp_left ] ) .    [ lem_jl_l1 ] using @xmath85 with @xmath180 , then with probability at least @xmath59 , the @xmath0 distance , @xmath81 , between any pair of data points ( among @xmath22 data points ) , can be estimated with errors bounded by @xmath181 , i.e. , @xmath182 .",
    "* remarks on lemma [ lem_jl_l1 ] * : ( 1 ) we can replace the constant `` 8 '' in lemma [ lem_jl_l1 ] with better ( i.e. , smaller ) constants for specific values of @xmath183 .",
    "for example , if @xmath184 , we can replace `` 8 '' by `` 5 '' .",
    "see the proof of lemma [ lem_d_gm_tail ] .",
    "( 2 ) this lemma is weaker than the classical jl lemma for dimension reduction in @xmath9 as reviewed in section 2.1 .",
    "the classical jl lemma for @xmath9 ensures that the @xmath9 inter - point distances of the projected data points are close enough to the original @xmath9 distances , while lemma [ lem_jl_l1 ] merely says that the projected data points contain enough information to reconstruct the original @xmath0 distances . on the other hand ,",
    "the geometric mean estimator is a non - convex norm ; and therefore it does contain some information about the geometry .",
    "we leave it for future work to explore the possibility of developing efficient algorithms using the geometric mean estimator .",
    "+ figure [ fig_hist_d_gm ] presents the simulated histograms of @xmath85 for @xmath185 , with @xmath186 and @xmath187 .",
    "the histograms reveal some characteristics shared by the maximum likelihood estimator we will discuss in the next section :    * supported on @xmath188 , @xmath85 is positively skewed . *",
    "the distribution of @xmath85 is still `` heavy - tailed . '' however , in the region not too far from the mean , the distribution of @xmath85 may be well captured by a gamma ( or a generalized gamma ) distribution . for large @xmath8 ,",
    "even a normal approximation may suffice .",
    "figure [ fig_gm_vs_me ] compares @xmath85 with the sample median estimators @xmath128 and @xmath84 , in terms of the mean square errors .",
    "@xmath85 is considerably more accurate than @xmath128 at small @xmath8 .",
    "the bias correction significantly reduces the mean square errors of @xmath128 .",
    "this section is devoted to analyzing the maximum likelihood estimators ( mle ) , which are `` asymptotically optimum . '' in comparisons , the sample median estimators and geometric mean estimators are not optimum .",
    "our contribution in this section includes the higher - order analysis for the bias and moments and accurate closed - from approximations to the distribution of the mle .",
    "the method of maximum likelihood is widely used .",
    "for example , @xcite applied the maximum likelihood method to _ normal random projections _ and provided an improved estimator of the @xmath9 distance by taking advantage of the marginal information .",
    "the cauchy distribution is often considered a `` challenging '' example because of the `` multiple roots '' problem when estimating the location parameter @xcite . in our case , since the location parameter is always zero , much of the difficulty is avoided .",
    "recall our goal is to estimate @xmath81 from @xmath8 i.i.d .",
    "samples @xmath189 .",
    "the @xmath190 joint likelihood of @xmath43 is @xmath191 whose first and second derivatives ( w.r.t .",
    "@xmath81 ) are @xmath192    the maximum likelihood estimator of @xmath81 , denoted by @xmath111 , is the solution to @xmath193 , i.e. , @xmath194 because @xmath195 , @xmath111 indeed maximizes the joint likelihood and is the only solution to the mle equation ( [ eqn_mle ] ) . solving ( [ eqn_mle ] )",
    "numerically is not difficult ( e.g. , a few iterations using the newton s method ) . for a better accuracy ,",
    "we recommend the following bias - corrected estimator : @xmath196    lemma [ lem_mle_asymp ] concerns the asymptotic moments of @xmath111 and @xmath86 , proved in appendix [ app_proof_lem_asymp ] .",
    "[ lem_mle_asymp ] both @xmath111 and @xmath86 are asymptotically unbiased and normal .",
    "the first four moments of @xmath111 are @xmath197 the first four moments of @xmath86 are @xmath198 +    the order @xmath199 term of the variance , i.e. , @xmath200 , is known , e.g. , @xcite .",
    "we derive the bias - corrected estimator , @xmath86 , and the higher order moments using stochastic taylor expansions @xcite .",
    "we will propose an inverse gaussian distribution to approximate the distribution of @xmath86 , by matching the first four moments ( at least in the leading terms ) .",
    "the maximum likelihood estimators are tested on msn web crawl data , a term - by - document matrix with @xmath201 web pages .",
    "we conduct cauchy random projections and estimate the @xmath0 distances between words . in this experiment",
    ", we compare the empirical and ( asymptotic ) theoretical moments , using one pair of words .",
    "figure [ fig_bias_var ] illustrates that the bias correction is effective and these ( asymptotic ) formulas for the first four moments of @xmath86 in lemma [ lem_mle_asymp ] are accurate , especially when @xmath202 .",
    "theoretical analysis on the exact distribution of a maximum likelihood estimator is difficult .",
    ", @xmath203 , ... , @xmath204 , the distribution of @xmath111 can be exactly characterized @xcite .",
    "@xcite studied the conditional confidence interval of the mle .",
    "later , @xcite proposed the normal approximation to the exact conditional confidence interval and showed that it was superior to the unconditional normality approximation .",
    "unfortunately , we can not take advantage of the conditional analysis because our goal is to determine the sample size @xmath8 before seeing any samples . ] in statistics , the standard approach is to assume normality , which , however , is quite inaccurate . the so - called _ edgeworth expansion _",
    "improves the normal approximation by matching higher moments @xcite .",
    "for example , if we approximate the distribution of @xmath86 using an edgeworth expansion by matching the first four moments of @xmath86 derived in lemma [ lem_mle_asymp ] , then the errors will be on the order of @xmath205",
    ". however , edgeworth expansions have some well - known drawbacks .",
    "the resultant expressions are quite sophisticated .",
    "they are not accurate at the tails .",
    "it is possible that the approximate probability has values below zero .",
    "also , edgeworth expansions consider the support is @xmath206 , while @xmath86 is non - negative .",
    "we propose approximating the distributions of @xmath86 directly using some well - studied common distributions .",
    "we will first consider a gamma distribution with the same first two ( asymptotic ) moments of @xmath86 .",
    "that is , the gamma distribution will be asymptotically equivalent to the normal approximation .",
    "while a normal has zero third central moment , a gamma has nonzero third central moment .",
    "this , to an extent , speeds up the rate of convergence .",
    "another important reason why a gamma is more accurate is because it has the same support as @xmath86 , i.e. , @xmath188 .",
    "we will furthermore consider a _",
    "generalized gamma _ distribution , which allows us to match the first three ( asymptotic ) moments of @xmath86 .",
    "interestingly , in this case , the generalized gamma approximation turns out to be an inverse gaussian distribution , which has a closed - form probability density .",
    "more interestingly , this inverse gaussian distribution also matches the fourth central moment of @xmath86 in the @xmath207 term and almost in the @xmath208 term . by simulations ,",
    "the inverse gaussian approximation is highly accurate .",
    "note that , since we are interested in the very small ( e.g. , @xmath209 ) tail probability range , @xmath205 is not too meaningful .",
    "for example , @xmath210 if @xmath211",
    ". therefore , we will have to rely on simulations to assess the accuracy of the approximations . on the other hand",
    ", an upper bound may hold exactly ( verified by simulations ) even if it is based on an approximate distribution .",
    "as the related work , @xcite applied gamma and generalized gamma approximations to model the performance measure distribution in some wireless communication channels using random matrix theory and produced accurate results in evaluating the error probabilities .",
    "the gamma approximation is an obvious improvement over the normal approximation .",
    ", the resultant estimator of the squared @xmath9 distance has a chi - squared distribution ( e.g. , ( * ? ? ?",
    "* lemma 1.3 ) ) , which is a special case of gamma . ] a gamma distribution , @xmath212 , has two parameters , @xmath213 and @xmath214 , which can be determined by matching the first two ( asymptotic ) moments of @xmath86 .",
    "that is , we assume that @xmath215 , with @xmath216    assuming a gamma distribution , it is easy to obtain the following chernoff bounds ; and we then choose @xmath217 that minimizes the upper bound . ] : @xmath218 where we use @xmath219 to indicate that these inequalities are based on an approximate distribution .",
    "note that the distribution of @xmath220 ( and hence @xmath221 ) is only a function of @xmath8 as shown in @xcite .",
    "therefore , we can evaluate the accuracy of the gamma approximation by simulations with @xmath222 , as presented in figure [ fig_gamma_tail ] .",
    "figure [ fig_gamma_tail](a ) shows that both the gamma and normal approximations are fairly accurate when the tail probability @xmath223 ; and the gamma approximation is obviously better .",
    "figure [ fig_gamma_tail](b ) compares the empirical tail probabilities with the gamma chernoff upper bound ( [ eqn_gamma_right])+([eqn_gamma_left ] ) , indicating that these bounds are reliable , when the tail probability @xmath224 .",
    "the distribution of @xmath86 can be well approximated by an inverse gaussian distribution , which is a special case of the three - parameter generalized gamma distribution @xcite , denoted by @xmath225 .",
    "note that the usual gamma distribution is a special case with @xmath226 .",
    "if @xmath227 , then the first three moments are @xmath228    we can approximate the distribution of @xmath86 by matching the first three moments , i.e. , @xmath229 from which we obtain @xmath230 taking only the leading term for @xmath231 , the generalized gamma approximation of @xmath86 would be @xmath232    in general , a generalized gamma distribution does not have a closed - form density function although it always has a closed - from moment generating function . in our case , ( [ eqn_ig ] )",
    "is actually an inverse gaussian distribution , which has a closed - form density function . assuming @xmath233 , with parameters @xmath213 and @xmath214 defined in ( [ eqn_ig_parameters ] ) , the moment generating function ( mgf )",
    ", the probability density function ( pdf ) , and cumulative density function ( cdf ) would be ( * ? ? ?",
    "* chapter 2 ) @xcite @xmath234 where @xmath235 is the standard normal cdf , i.e. , @xmath236 .",
    "here we use @xmath237 to indicate that these equalities are based on an approximate distribution .",
    "assuming @xmath238 , then the fourth central moment should be @xmath239    lemma [ lem_mle_asymp ] has shown the true asymptotic fourth central moment : @xmath240 that is , the inverse gaussian approximation matches not only the leading term , @xmath241 , but also almost the higher order term , @xmath242 , of the true asymptotic fourth moment of @xmath86 .",
    "assuming @xmath243 , the tail probability of @xmath86 can be expressed as @xmath244    assuming @xmath243 , it is easy to show the following chernoff bounds : @xmath245    to see ( [ eqn_ig_left ] ) .",
    "assume @xmath246 .",
    "then , using the chernoff inequality : @xmath247 whose minimum is @xmath248 , attained at @xmath249",
    ". we can similarly show ( [ eqn_ig_right ] ) .",
    "+ combining ( [ eqn_ig_left ] ) and ( [ eqn_ig_right ] ) yields a symmetric bound @xmath250    figure [ fig_ig_tail ] compares the inverse gaussian approximation with the same simulations as presented in figure [ fig_gamma_tail ] , indicating that the inverse gaussian approximation is highly accurate .",
    "when the tail probability @xmath251 , we can treat the inverse gaussian as the exact distribution of @xmath86 .",
    "the chernoff upper bounds for the inverse gaussian are always reliable in our simulation range ( the tail probability @xmath120 ) .",
    "it is well - known that the @xmath0 distance is far more robust than the @xmath9 distance against `` outliers . ''",
    "there are numerous success stories of using the @xmath0 distance , e.g. , lasso @xcite , lars @xcite , 1-norm svm @xcite , and laplacian radial basis kernel @xcite .",
    "dimension reduction in the @xmath0 norm , however , has been proved _ impossible _",
    "if we use _ linear random projections _ and _ linear estimators_. in this study , we propose three types of nonlinear estimators for _ cauchy random projections _ : the bias - corrected sample median estimator , the bias - corrected geometric mean estimator , and the bias - corrected maximum likelihood estimator .",
    "our theoretical analysis has shown that these nonlinear estimators can accurately recover the original @xmath0 distance , even though none of them can be a metric .",
    "the bias - corrected sample median estimator and the bias - corrected geometric mean estimator are asymptotically equivalent but the latter is more accurate at small sample size .",
    "we have derived explicit tail bounds for the bias - corrected geometric mean estimator and have expressed the tail bounds in exponential forms . using these tail bounds ,",
    "we have established an analog of the johnson - lindenstrauss ( jl ) lemma for dimension reduction in @xmath0 , which is weaker than the classical jl lemma for dimension reduction in @xmath9 .",
    "we conduct theoretic analysis on the bias - corrected maximum likelihood estimator ( mle ) , which is `` asymptotically optimum . ''",
    "both the sample median estimator and the geometric mean estimator are about @xmath10 efficient as the mle .",
    "we propose approximating its distribution by an inverse gaussian , which has the same support and matches the leading terms of the first four moments of the proposed estimator .",
    "approximate tail bounds have been provide based on the inverse gaussian approximation",
    ". verified by simulations , these approximate tail bounds hold at least in the @xmath252 tail probability range .    although these nonlinear estimators are not metrics , they are still useful for certain applications in ( e.g. , ) data stream computation , information retrieval , learning and data mining , whenever the goal is to compute the @xmath0 distances efficiently using a small storage space",
    ".    the geometric mean estimator is a non - convex norm ( i.e. , the @xmath18 norm as @xmath100 ) ; and therefore it does contain some information about the geometry .",
    "it may be still possible to develop certain efficient algorithms using the geometric mean estimator by avoiding the non - convexity .",
    "we leave this for future work .",
    "we are grateful to piotr indyk and assaf naor for the very constructive comments on various versions of this manuscript .",
    "we thank dimitris achlioptas , christopher burges , moses charikar , jerome friedman , tze l. lai , art b. owen , john platt , joseph romano , tim roughgarden , yiyuan she , and guenther walther for helpful conversations or suggesting relevant references .",
    "we also thank silvia ferrari and gauss cordeiro for clarifying some parts of their papers .",
    "trevor hastie was partially supported by grant dms-0505676 from the national science foundation , and grant 2r01 ca 72028 - 07 from the national institutes of health .",
    "59 natexlab#1#1url # 1#1    dimitris achlioptas .",
    "database - friendly random projections : with binary coins . , 660 ( 4):0 671687 , 2003 .    charu  c. aggarwal and joel  l. wolf . a new method for similarity indexing of market basket data . in _ proc .",
    "of sigmod _ , pages 407418 , philadelphia , pa , 1999 .",
    "nir ailon and bernard chazelle .",
    "approximate nearest neighbors and the fast transform . in _ proc . of stoc _ ,",
    "pages 557563 , seattle , wa , 2006 .",
    "charles antle and lee bain . a property of maximum likelihood estimators of location and scale parameters . , 110 ( 2):0 251253 , 1969 .",
    "rosa arriaga and santosh vempala .",
    "an algorithmic theory of learning : robust concepts and random projection . in _ proc . of focs _ , pages 616623 , new york , 1999 .",
    "rosa arriaga and santosh vempala .",
    "an algorithmic theory of learning : robust concepts and random projection .",
    ", 630 ( 2):0 161182 , 2006 .",
    "v.  d. barnett .",
    "evaluation of the maximum - likelihood estimator where the likelihood equation has multiple roots .",
    ", 530 ( 1/2):0 151165 , 1966",
    ".    m.  s. bartlett .",
    "approximate confidence intervals , . , 400 ( 3/4):0 306317 , 1953 .",
    "r.  n. bhattacharya and j.  k. ghosh . on the validity of the formal expansion . , 60 ( 2):0 434451 , 1978 .    bo  brinkman and mose charikar .",
    "on the impossibility of dimension reduction in @xmath0 . in _ proc . of focs _ , pages 514523 , cambridge , ma , 2003",
    "bo  brinkman and mose charikar .",
    "on the impossibility of dimension reduction in @xmath0 . , 520 ( 2):0 766788 , 2005 .    olivier chapelle , patrick haffner , and vladimir  n. vapnik .",
    "support vector machines for histogram - based image classification . , 100 ( 5):0 10551064 , 1999",
    ".    herman chernoff .",
    "a measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations .",
    ", 230 ( 4):0 493507 , 1952 .    raj  s. chhikara and j.  leroy folks . .",
    "marcel dekker , inc , new york , 1989 .",
    "francisco jose de .",
    "a. cysneiros , sylvio jose  p.",
    "dos santos , and gass  m. cordeiro .",
    "skewness and kurtosis for maximum likelihood estimator in one - parameter exponential family models . , 150 ( 1):0 85105 , 2001 .",
    "sanjoy dasgupta and anupam gupta .",
    "an elementary proof of a theorem of .",
    ", 220 ( 1):0 60  65 , 2003",
    ".    inderjit  s. dhillon and dharmendra  s. modha .",
    "concept decompositions for large sparse text data using clustering . , 420 ( 1 - 2):0 143175 , 2001 .    bradley efron , trevor hastie , iain johnstone , and robert tibshirani . least angle regression . , 320 ( 2):0 407499 , 2004 .",
    "eugene  f. fama and richard roll .",
    "some properties of symmetric stable distributions . , 630 ( 323):0 817836 , 1968 .    eugene  f. fama and richard roll .",
    "parameter estimates for symmetric stable distributions . , 660 ( 334):0 331338 , 1971 .",
    "william feller . .",
    "john wiley & sons , new york , ny , second edition , 1971 .",
    "marin ferecatu , michel crucianu , and nozha boujemaa .",
    "retrieval of difficult image classes using svd - based relevance feedback . in _ prof . of multimedia information retrieval _ , pages 2330 , new york , ny , 2004 .",
    "silvia l.  p. ferrari , denise  a. botter , gauss  m. cordeiro , and francisco cribari - neto .",
    "second and third order bias reduction for one - parameter family models .",
    ", 30:0 339345 , 1996 .",
    "r.  a. fisher .",
    "two new properties of mathematical likelihood . , 1440 ( 852):0 285307 , 1934 .",
    "p.  frankl and h.  maehara .",
    "the lemma and the sphericity of some graphs .",
    ", 440 ( 3):0 355362 , 1987 .    hans  u. gerber . from the generalized gamma to the generalized negative binomial distribution .",
    ", 100 ( 4):0 303309 , 1991 .",
    "i.  s. gradshteyn and i.  m. ryzhik . .",
    "academic press , new york , fifth edition , 1994 .",
    "gerald haas , lee bain , and charles antle .",
    "inferences for the cauchy distribution based on maximum likelihood estimation . , 570 ( 2):0 403408 , 1970 .",
    "david  v. hinkley .",
    "likelihood inference about location and scale parameters .",
    ", 650 ( 2):0 253261 , 1978 .",
    "p.  hougaard .",
    "survival models for heterogeneous populations derived from stable distributions . , 730 ( 2):0 387396 , 1986 .",
    "piotr indyk .",
    "stable distributions , pseudorandom generators , embeddings and data stream computation . in _ focs _ , pages 189197 , redondo beach , ca , 2000 .",
    "piotr indyk .",
    "algorithmic applications of low - distortion geometric embeddings . in _ proc . of focs _ , pages 1033 , las vegas , nv , 2001 .",
    "piotr indyk and rajeev motwani .",
    "approximate nearest neighbors : towards removing the curse of dimensionality . in _ proc . of stoc _ , pages 604613 , dallas , tx , 1998 .",
    "piotr indyk and assaf naor .",
    "nearest neighbor preserving embeddings . , 2006",
    ".    jens  ledet jensen . .",
    "oxford university press , new york , 1995 .",
    "w.  b. johnson and j.  lindenstrauss .",
    "extensions of mapping into space . , 26:0 189206 , 1984 .",
    "j.  f. lawless .",
    "conditional confidence interval procedures for the location and scale parameters of the cauchy and logistic distributions . , 590 ( 2):0 377386 , 1972 .",
    "james  r. lee and assaf naor . embedding the diamond graph in @xmath18 and dimension reduction in @xmath0 . , 140 ( 4):0 745747 , 2004 .    ping li and kenneth  w. church . using sketches to estimate two - way and multi - way associations",
    ". technical report tr-2005 - 115 , microsoft research , ( a shorter version is available at www.stanford.edu/@xmath253pingli98/publications/report_sketch.pdf ) , redmond , wa , september 2005 .",
    "ping li , kenneth  w. church , and trevor  j. hastie .",
    "conditional random sampling : a sketched - based sampling technique for sparse data .",
    "technical report , department of statistics , stanford university ( www.stanford.edu/~pingli98/publications/crs_tr.pdf ) , 2006 .",
    "ping li , trevor  j. hastie , and kenneth  w. church .",
    "improving random projections using marginal information . in _ proc .",
    "of colt _ , pittsburgh , pa , 2006 .",
    "ping li , trevor  j. hastie , and kenneth  w. church .",
    "sub - gaussian random projections .",
    "technical report , department of statistics , stanford university ( www.stanford.edu/~pingli98/report/subg_rp.pdf ) , 2006 .",
    "ping li , trevor  j. hastie , and kenneth  w. church .",
    "very sparse random projections . in _ proc . of kdd _ , philadelphia , pa , 2006 .",
    "ping li , debashis paul , ravi narasimhan , and john cioffi . on the distribution of for the receiver and performance analysis . , 520 ( 1):0 271286 , 2006 .",
    "gabor lugosi .",
    "concentration - of - measure inequalities . , 2004 .",
    "j.  huston mcculloch .",
    "simple consistent estimators of stable distribution parameters . , 150 ( 4):0 11091136 , 1986 .",
    "thomas  k. philips and randolph nelson .",
    "the moment bound is tighter than chernoff s bound for positive tail probabilities .",
    ", 490 ( 2):0 175178 , 1995 .",
    "v.  seshadri . .",
    "oxford university press inc .",
    ", new york , 1993 .",
    "thomas  a. severini . .",
    "oxford university press , new york , 2000 .",
    "gregory shakhnarovich , trevor darrell , and piotr indyk , editors . . the mit press , cambridge , ma , 2005 .",
    "jun shao . .",
    "springer , new york , ny , second edition , 2003 .    l.  r. shenton and k.  bowman .",
    "higher moments of a maximum - likelihood estimate . , 250 ( 2):0 305317 , 1963 .",
    "alexander strehl and joydeep ghosh .",
    "a scalable approach to balanced , high - dimensional clustering of market - baskets . in _ proc . of hipc _ , pages 525536 , bangalore , india , 2000 .",
    "robert tibshirani .",
    "regression shrinkage and selection via the lasso .",
    ", 580 ( 1):0 267288 , 1996 .",
    "m.  c.  k. tweedie . statistical properties of inverse gaussian distributions . . , 280 ( 2):0 362377 , 1957 .",
    "m.  c.  k. tweedie . statistical properties of inverse gaussian distributions . . , 280 ( 3):0 696705 , 1957 .",
    "santosh vempala . .",
    "american mathematical society , providence , ri , 2004 .",
    "ji  zhu , saharon rosset , trevor hastie , and robert tibshirani . 1-norm support vector machines . in",
    ", 2003 .",
    "v.  m. zolotarev . .",
    "american mathematical society , providence , ri , 1986 .",
    "assume @xmath157 . the probability density function ( pdf ) and",
    "the cumulative density function ( cdf ) of @xmath254 would be @xmath255                                                    for @xmath303 , we can prove an exponential bound for @xmath304 .",
    "first of all , note that we do not have to choose the optimum @xmath297 . by the taylor expansion , for small @xmath183",
    ", @xmath305 can be well approximated by @xmath306          it is easy to see that as @xmath312 , @xmath313 .",
    "figure [ fig_gm_constant](a ) illustrates that it suffices to let @xmath314 , which can be numerically verified .",
    "this is why the last step in ( [ eqn_proof_right ] ) holds .",
    "of course , we can get a better constant if ( e.g. , ) @xmath315 .            in order to attain @xmath326",
    ", we have to restrict @xmath8 to be larger than a certain value . for no particular reason ,",
    "we like to express the restriction as @xmath327 , for some constant @xmath328 .",
    "we find @xmath329 suffices , although readers can verify that a slightly better ( smaller ) restriction would be @xmath330 .",
    "general formulas for the bias and higher moments of the mle are available in @xcite .",
    "we need to evaluate the expressions in @xcite , involving tedious algebra : @xmath340}{2k\\text{i}^2 } + o\\left(\\frac{1}{k^2}\\right ) \\\\ & \\text{var}\\left(\\hat{d}_{mle}\\right ) = \\frac{1}{k\\text{i } } + \\frac{1}{k^2}\\left(-\\frac{1}{\\text{i}}+\\frac{[1 ^ 4]-[1 ^ 22]-[13]}{\\text{i}^3 } + \\frac{3.5[12]^2-[1 ^ 3]^2}{\\text{i}^4}\\right ) +   o\\left(\\frac{1}{k^3}\\right ) \\\\ &",
    "\\text{e}\\left(\\hat{d}_{mle}-\\text{e}\\left(\\hat{d}_{mle}\\right)\\right)^3 =   \\frac{[1 ^ 3]-3[12]}{k^2\\text{i}^2}+o\\left(\\frac{1}{k^3}\\right ) \\\\\\notag & \\text{e}\\left(\\hat{d}_{mle}-\\text{e}\\left(\\hat{d}_{mle}\\right)\\right)^4=   \\frac{3}{k^2\\text{i}^2 } + \\frac{1}{k^3}\\left(-\\frac{9}{\\text{i}^2}+    \\frac{7[1 ^ 4 ] - 6[1 ^ 22]-10[13]}{\\text{i}^4}\\right)\\\\ & \\hspace{1.7 in } + \\frac{1}{k^3}\\left(\\frac{-6[1 ^ 3]^2 - 12[1 ^ 3][12]+45[12]^2}{\\text{i}^5}\\right)+o\\left(\\frac{1}{k^4}\\right),\\end{aligned}\\ ] ] where , after re - formatting , @xmath341 = \\text{e}(l^\\prime)^3 +   \\text{e}(l^\\prime l^{\\prime\\prime } ) , \\hspace{0.3 in } [ 1 ^ 4 ] = \\text{e}(l^\\prime)^4 , \\hspace{0.3 in } [ 1 ^ 22 ] = \\text{e}(l^{\\prime\\prime}(l^\\prime)^2 ) +   \\text{e}(l^{\\prime})^4 , \\\\ & [ 13 ] = \\text{e}(l^\\prime)^4 + 3\\text{e}(l^{\\prime\\prime}(l^\\prime)^2 )   + \\text{e}(l^\\prime l^{\\prime\\prime\\prime } ) , \\hspace{0.3 in } [ 1 ^ 3]=\\text{e}(l^\\prime)^3.\\end{aligned}\\ ] ]      without giving the detail , we report @xmath343 hence @xmath344 = -\\frac{1}{2}\\frac{1}{d^3 } , \\hspace{0.25 in } [ 1 ^ 4 ] = \\frac{3}{8}\\frac{1}{d^4 } , \\hspace{0.25in}[1 ^ 22 ] = \\frac{1}{4}\\frac{1}{d^4 } , \\hspace{0.25in}[13 ] = \\frac{3}{4}\\frac{1}{d^4 } , \\hspace{0.25in}[1 ^ 3 ] = 0.\\end{aligned}\\ ] ]      because @xmath111 has @xmath199 bias , we recommend the bias - corrected estimator @xmath346 whose first four moments are @xmath347 by brute - force algebra . first , it is obvious that @xmath348 then @xmath349"
  ],
  "abstract_text": [
    "<S> for _ , is available as a technical report in stanford statistics achive ( report no . </S>",
    "<S> 2006 - 04 , june , 2006 ) . ] </S>",
    "<S> dimension reduction in @xmath0 , the method of _ cauchy random projections _ multiplies the original data matrix @xmath1 with a random matrix @xmath2 ( @xmath3 ) whose entries are i.i.d </S>",
    "<S> . samples of the standard cauchy @xmath4 . </S>",
    "<S> because of the impossibility results , one can not hope to recover the pairwise @xmath0 distances in @xmath5 from @xmath6 , using linear estimators without incurring large errors . however , nonlinear estimators are still useful for certain applications in data stream computation , information retrieval , learning , and data mining .    </S>",
    "<S> we propose three types of nonlinear estimators : the bias - corrected sample median estimator , the bias - corrected geometric mean estimator , and the bias - corrected maximum likelihood estimator . </S>",
    "<S> the sample median estimator and the geometric mean estimator are asymptotically ( as @xmath7 ) equivalent but the latter is more accurate at small @xmath8 . </S>",
    "<S> we derive explicit tail bounds for the geometric mean estimator and establish an analog of the johnson - lindenstrauss ( jl ) lemma for dimension reduction in @xmath0 , which is weaker than the classical jl lemma for dimension reduction in @xmath9 .    </S>",
    "<S> asymptotically , both the sample median estimator and the geometric mean estimators are about @xmath10 efficient compared to the maximum likelihood estimator ( mle ) . </S>",
    "<S> we analyze the moments of the mle and propose approximating the distribution of the mle by an inverse gaussian .    </S>",
    "<S> * keywords : * dimension reduction , @xmath0 norm , cauchy random projections , jl bound </S>"
  ]
}