{
  "article_text": [
    "supervised learning is one of the central problems in machine learning which aims at learning an input - output relationship from given input - output paired data samples .",
    "although many methods were proposed to perform supervised learning , they often work poorly when the input variables have high dimensionality . such a situation",
    "is commonly referred to as the _ curse of dimensionality _",
    "@xcite , and a common approach to mitigate the curse of dimensionality is to preprocess the input variables by _ dimension reduction _ @xcite .",
    "a typical goal of dimension reduction in supervised learning is to find a low - dimensional subspace of the input space such that the projected input variables preserve maximal information about the output variables .",
    "thus , a successive supervised learning method can use the low - dimensional projection of the input variables to learn the input - output relationship with a minimal loss of information .",
    "the purpose of this paper is to develop a novel supervised dimension reduction method .",
    "the dependence maximization approach solves the supervised dimension reduction problem through maximizing a statistical dependence measure between projected input variables and output variables . _ mutual information _ ( mi ) is a well - known tool for measuring statistical dependency between random variables @xcite .",
    "mi is well - studied and many methods were proposed to estimate mi from data . a notable method is the _ maximum likelihood mi _",
    "( mlmi ) @xcite , which does not require any assumption on the data distribution and can perform model selection via cross - validation . for these reasons",
    ", mlmi seems to be an appealing tool for supervised dimension reduction .",
    "however , mi is defined based on the _ kullback - leibler _ divergence @xcite , which is known to be sensitive to outliers @xcite .",
    "hence , mi is not an appropriate tool when it is applied on a dataset containing outliers .",
    "_ quadratic mi _",
    "( qmi ) is a variant of mi @xcite . unlike mi , qmi is defined based on the @xmath0 distance .",
    "a notable advantage of the @xmath0 distance over the kl divergence is that the @xmath0 distance is more robust against outliers @xcite .",
    "moreover , a computationally efficient method to estimate qmi from data , called _ least - squares qmi _ ( lsqmi ) @xcite , was proposed recently .",
    "lsqmi does not require any assumption on the data distribution and it can perform model selection via cross - validation . for these reasons , developing a supervised dimension reduction method based on lsqmi is more promising .",
    "an approach to use lsqmi for supervised dimension reduction is to firstly estimate qmi between projected input variables and output variables by lsqmi , and then search for a subspace which maximizes the estimated qmi by a nonlinear optimization method such as gradient ascent .",
    "however , the essential quantity of the subspace search is the derivative of qmi w.r.t .",
    "the subspace , not qmi itself .",
    "thus , lsqmi may not be an appropriate tool for developing supervised dimension reduction methods since the derivative of an accurate qmi estimator is not necessarily an accurate estimator of the derivative of qmi .    to cope with the above problem , in this paper",
    ", we propose a novel method to _ directly _ estimate the derivative of qmi without estimating qmi itself .",
    "the proposed method has the following advantageous properties : it does not require any assumption on the data distribution , the estimator can be computed analytically , and the tuning parameters can be objectively chosen by cross - validation .",
    "we show through experiments that the proposed direct estimator of the derivative of qmi is more accurate than the derivative of the estimated qmi .",
    "then we develop a fixed - point iteration which efficiently uses the proposed estimator of the derivative of qmi to perform supervised dimension reduction .",
    "finally , we demonstrate the usefulness of the proposed supervised dimension reduction method through experiments and show that the proposed method is more robust against outliers than existing methods .",
    "the organization of this paper is as follows .",
    "we firstly formulate the supervised dimension reduction problem and review some of existing methods in section  [ section : sdr ] , then we give an overview of qmi and review some of qmi estimators in section  [ section : qmi ] .",
    "the details of the proposed derivative estimator are given in section  [ section : lsqmid ] .",
    "then in section  [ section : sdr_lsqmid ] we develop a supervised dimension reduction algorithm based on the proposed derivative estimator .",
    "the experiment results are given in section  [ section : experiments ] .",
    "the conclusion of this paper is given in section  [ section : conclusion ] .",
    "in this section , we firstly formulate the supervised dimension reduction problem .",
    "then we briefly review existing supervised dimension reduction methods and discuss their problems .",
    "let @xmath1 and @xmath2 be the input domain and output domain with dimensionality @xmath3 and @xmath4 , respectively , and @xmath5 be a joint probability density on @xmath6 .",
    "firstly , assume that we are given an input - output paired data set @xmath7 , where each data sample is drawn independently from the joint density : @xmath8    next , let @xmath9 be an orthonormal matrix with a known constant @xmath10 , where @xmath11 denotes the @xmath12-by-@xmath12 identity matrix and @xmath13 denotes the matrix transpose .",
    "then assume that there exists a @xmath12-dimensional subspace in @xmath14 spanned by the rows of @xmath15 such that the projection of @xmath16 onto this subspace denoted by  @xmath17   preserves the maximal information about @xmath18 of @xmath16 .",
    "that is , we can substitute @xmath16 by @xmath19 with a minimal loss of information about @xmath18 .",
    "we refer to the problem of estimating @xmath15 from the given data as _ supervised dimension reduction_. below , we review some of the existing supervised dimension reduction methods .      sliced inverse regression ( sir )",
    "@xcite is a well known supervised dimension reduction method .",
    "sir formulates supervised dimension reduction as a problem of finding @xmath15 which makes @xmath16 and @xmath20 conditionally independent given @xmath19 : @xmath21    the key principal of sir lies on the following equality is standardized so that @xmath22 = 0 $ ] and @xmath23 = \\boldsymbol{i}_{d_{\\boldsymbol{\\mathrm{z}}}}$ ] ] @xmath24 & = a_0 + \\sum_{i=1}^{d_{\\boldsymbol{\\mathrm{z } } } }      a_i \\boldsymbol{w}_i^\\top { \\boldsymbol{x } } ,    \\label{sir_condition}\\end{aligned}\\ ] ] where @xmath25 denotes the conditional expectation and @xmath26 denotes the @xmath27-th row of @xmath15 .",
    "the importance of this equality is that if the equality holds for any @xmath28 and some constants @xmath29 , then the inverse regression curve @xmath30 $ ] lies on the space spanned by @xmath15 which satisfies eq .. based on this fact , sir estimates @xmath15 as follows .",
    "first , the range of @xmath20 is sliced into multiple slices . then @xmath30 $ ]",
    "is estimated as the mean of @xmath16 for each slice of @xmath20 .",
    "finally , @xmath15 is obtained as the @xmath12 largest principal components of the covariance matrix of the means .",
    "the significant advantages of sir are its simplicity and scalability to large datasets",
    ". however , sir relies on the equality in eq . which typically requires that @xmath31 is an elliptically symmetric distribution such as gaussian",
    "this is restrictive and thus the practical usefulness of sir is limited .",
    "the _ minimum average variance estimation based on the conditional density functions _ ( dmave ) @xcite is a supervised dimension method which does not require any assumption on the data distribution and is more practical compared to sir .",
    "briefly speaking , dmave aims to find a matrix @xmath15 which yields an accurate non - parametric estimation of the conditional density @xmath32 .",
    "the essential part of dmave is the following model : @xmath33 where @xmath34 denotes a symmetric kernel function with bandwidth @xmath35 , @xmath36 denotes a conditional expectation of @xmath37 given @xmath19 , and @xmath38 $ ] with @xmath39 = 0 $ ] .",
    "an important property of this model is that @xmath40 when @xmath41 as @xmath42 .",
    "then , dmave estimates @xmath36 by a local linear smoother @xcite .",
    "more specifically , a local linear smoother of @xmath43 is given by @xmath44 where @xmath45 is an arbitrary point close to @xmath46 , and @xmath47 and @xmath48 are parameters .",
    "based on this local linear smoother , dmave solves the following minimization problem : @xmath49 ^ 2 k_h({\\boldsymbol{x}}_i , { \\boldsymbol{x}}_j ) , \\label{dmave_obj}\\end{aligned}\\ ] ] where @xmath50 is a symmetric kernel function with bandwidth @xmath51 .",
    "the function @xmath52 is a trimming function which is evaluated as zero when the densities of @xmath16 or @xmath20 are lower than some threshold . a solution to this minimization problem",
    "is obtained by alternatively solving quadratic programming problems for @xmath15 , and @xmath53 until convergence .",
    "the main advantage of dmave is that it does not require any assumption on the data distribution .",
    "however , a significant disadvantage of dmave is that there is no systematic method to choose the kernel bandwidths and the trimming threshold . in practice",
    ", dmave uses a bandwidth selection method based on the _ normal - reference _ rule of the non - parametric conditional density estimation @xcite , and a fixed trimming threshold .",
    "although this model selection strategy works reasonably well in general , it does not always guarantee good performance on all kind of datasets .",
    "another disadvantage of dmave is that the optimization problem in eq",
    ". may have many local solutions . to cope with this problem , dmave proposed to use a supervised dimension reduction method called the _ outer product of gradient based on conditional density functions _ ( dopg )",
    "@xcite to obtain a good initial solution .",
    "thus , dmave may not perform well if dopg fails to provide a good initial solution .",
    "another supervised dimension reduction method which does not require any assumption on the data distribution is _ kernel dimension reduction _ ( kdr ) @xcite .",
    "unlike dmave which focuses on the conditional density , kdr aims to find a matrix @xmath15 which satisfies the conditional independence in eq .. the key idea of kdr is to evaluate the conditional independence through a conditional covariance operator over reproducing kernel hilbert spaces ( rkhss ) @xcite .    throughout this subsection , we use @xmath54 to denote an rkhs of functions on the domain @xmath55 equipped with reproducing kernel @xmath56 : @xmath57 for @xmath58 and @xmath59 .",
    "the rkhss of functions on domains @xmath60 and @xmath61 are also defined similarly as @xmath62 and @xmath63 , respectively .",
    "the _ cross - covariance operator _",
    "@xmath64 : @xmath65 satisfies the following equality for all @xmath58 and @xmath66 : @xmath67   - \\mathbb{e}_{{\\boldsymbol{z } } } \\left [ f({\\boldsymbol{z}})\\right ]    \\mathbb{e}_{{\\boldsymbol{y } } } \\left [ g({\\boldsymbol{y}})\\right],\\end{aligned}\\ ] ] where @xmath68 , @xmath69 , and @xmath70 denotes expectations over densities @xmath71 , @xmath72 , and @xmath73 , respectively .",
    "then , the _ conditional covariance operator _ can be defined using cross - covariance operators as @xmath74 where it is assumed that @xmath75 always exists .",
    "the importance of the conditional covariance operator in supervised dimension reduction lies in the following relations : @xmath76 where the inequality refers to the partial order of self - adjoint operators , and @xmath77 these relations mean that the conditional independence can be achieved by finding a matrix @xmath15 which minimizes @xmath78 in the partial order of self - adjoint operators .",
    "based on this fact , kdr solves the following minimization problem : @xmath79 , \\label{kdr_obj}\\end{aligned}\\ ] ] where @xmath80 denotes a regularization parameter , @xmath81 and @xmath82 denotes centered gram matrices with the kernels @xmath56 and @xmath83 , respectively , and @xmath84 $ ] denotes the trace of an operator .",
    "a solution to this minimization problem is obtained by a gradient descent method .",
    "kdr does not require any assumption on the data distribution and was shown to work well on various regression and classification tasks @xcite .",
    "however , kdr has two disadvantages in practice .",
    "the first disadvantage of kdr is that even though the kernel parameters and the regularization parameter can heavily affect the performance , there seems to be no justifiable model selection method to choose these parameters so far .",
    "although it is always possible to choose these tuning parameters based on a criterion of a successive supervised learning method with cross - validation , this approach results in a nested loop of model selection for both kdr itself and the successive supervised learning method .",
    "moreover , this approach makes supervised dimension reduction depends on the successive supervised learning method which is unfavorable in practice .",
    "the second disadvantage is that the optimization problem in eq .",
    "is non - convex and may have many local solutions .",
    "thus , if the initial solution is not properly chosen , the performance of kdr may be unreliable .",
    "a simple approach to cope with this problem is to choose the best solution with cross - validation based on the successive supervised learning method , but this approach makes supervised dimension reduction depends on the successive supervised learning method and is unfavorable .",
    "a more sophisticated approach was considered in @xcite which proposed to use a solution of a supervised dimension reduction method called _ gradient - based kernel dimension reduction _",
    "( gkdr ) as an initial solution for kdr .",
    "however , it is not guarantee that gkdr always provide a good initial solution for kdr .",
    "the _ least - squares dimension reduction _ ( lsdr ) @xcite is another supervised dimension reduction method which does not require any assumption on the data distribution .",
    "similarly to kdr , lsdr aims to find a matrix @xmath15 which satisfies the conditional independence in eq .. however , instead of the conditional covariance operators , lsdr evaluates the conditional independence through a statistical dependence measure .",
    "lsdr utilizes a statistical dependence measure called _ squared - loss mutual information _ ( smi ) .",
    "smi between random variables @xmath19 and @xmath18 is defined as @xmath85 @xmath86 is always non - negative and equals to zero if and only if @xmath19 and @xmath18 are statistically independent , i.e. , @xmath87 .",
    "the important properties of smi in supervised dimension reduction are the following relations : @xmath88 and @xmath89 thus , the conditional independence can be achieved by finding a matrix @xmath15 which maximizes @xmath86 . since @xmath86 is typically unknown , it is estimated by the _",
    "least - squares mutual information _",
    "@xcite method which directly estimates the density ratio @xmath90 without performing any density estimation .",
    "then , lsdr solves the following maximization problem : @xmath91 where @xmath92 denotes the estimated smi .",
    "the solution to this maximization problem is obtained by a gradient ascent method .",
    "note that this maximization problem is non - convex and may have many local solutions .",
    "lsdr does not require any assumption on the data distribution , similarly to dmave and kdr .",
    "however , the significant advantage of lsdr over dmave and kdr is that lsdr can perform model selection via cross - validation and avoid a poor local solution without requiring any successive supervised learning method .",
    "this is a favorable property as a supervised dimension reduction method .",
    "however , a disadvantage of lsdr is that the density ratio function @xmath90 can be highly fluctuated , especially when the data contains outliers .",
    "since it is typically difficult to accurately estimate a highly fluctuated function , lsdr could be unreliable in the presence of outliers .",
    "next , we consider a supervised dimension reduction approach based on quadratic mutual information which can overcome the disadvantages of the existing methods .",
    "in this section , we briefly introduce quadratic mutual information and discuss how it can be used to perform robust supervised dimension reduction .",
    "_ quadratic mutual information _ ( qmi ) is a measure for statistical dependency between random variables @xcite , and is defined as @xmath93 @xmath94 is always non - negative and equals to zero if and only if @xmath19 and @xmath18 are statistically independent , i.e. , @xmath87 .",
    "such a property of qmi is similar to that of the ordinary _ mutual information _ ( mi ) , which is defined as @xmath95 the essential difference between qmi and mi is the discrepancy measure .",
    "@xmath94 is the @xmath0 distance between @xmath71 and @xmath96 , while @xmath97 is the _ kullback - leibler _ ( kl ) divergence @xcite .",
    "mi has been studied and applied to many data analysis tasks @xcite . moreover ,",
    "an efficient method to estimate mi from data is also available @xcite .",
    "however , mi is not always the optimal choice for measuring statistical dependence because it is not robust against outliers .",
    "an intuitive explanation is that mi contains the log function and the density ratio : the value of logarithm can be highly sharp near zero , and density ratio can be highly fluctuated and diverge to infinity .",
    "thus , the value of mi tends to be unstable and unreliable in the presence of outliers .",
    "in contrast , qmi does not contain the log function and the density ratio , and thus qmi should be more robust against outliers than mi .",
    "another explanation of the robustness of qmi and mi can be understood based on their discrepancy measures .",
    "both @xmath0 distance ( qmi ) and kl divergence ( mi ) can be regarded as members of a more general divergence class called the _ density power _",
    "divergence @xcite : @xmath98 where @xmath99 . based on this divergence class , the @xmath0 distance and the kl divergence can be obtained by setting @xmath100 and @xmath101 , respectively . as discussed in @xcite",
    ", the parameter @xmath102 controls the robustness against outliers of the divergence , where a large value of @xmath102 indicates high robustness .",
    "this means that the @xmath0 distance ( @xmath100 ) is more robust against outliers than the kl divergence ( @xmath101 ) .    in supervised dimension reduction ,",
    "robustness against outliers is an important requirement because outliers often make supervised dimension reduction methods to work poorly .",
    "thus , developing a supervised dimension reduction method based on qmi is an attractive approach since qmi is robust against outliers .",
    "this qmi - based supervised dimension reduction method is performed by finding a matrix @xmath103 which maximizes @xmath104 : @xmath105 the motivation is that , if @xmath104 is maximized then @xmath19 and @xmath18 are maximally dependent on each other , and thus we may disregard @xmath16 with a minimal loss of information about @xmath18 .",
    "since @xmath104 is typically unknown , it needs to be estimated from data .",
    "below , we review existing qmi estimation methods and then discuss a weakness of performing supervised dimension reduction using these qmi estimation methods .",
    "we review two qmi estimation methods which estimate @xmath94 from the given data .",
    "the first method estimates qmi through density estimation , and the second method estimates qmi through density difference estimation .",
    "expanding eq . allows us to express @xmath94 as @xmath106 a naive approach to estimate @xmath94 is to separately estimate the unknown densities @xmath71 , @xmath72 , and @xmath73 by density estimation methods such as _ kernel density estimation _ ( kde ) @xcite , and then plug the estimates into eq ..    following this approach , the kde - based qmi estimator has been studied and applied to many problems such as _ feature extraction for classification _",
    "@xcite , _ blind source separation _",
    "@xcite , and _ image registration _ @xcite .",
    "although this density estimation based approach was shown to work well , accurately estimating densities for high - dimensional data is known to be one of the most challenging tasks @xcite .",
    "moreover , the densities contained in eq . are estimated independently without regarding the accuracy of the qmi estimator .",
    "thus , even if each density is accurately estimated , the qmi estimator obtained from these density estimates does not necessarily give an accurate qmi .",
    "an approach to mitigate this problem is to consider density estimators which their combination minimizes the estimation error of qmi .",
    "although this approach shows better performance than the independent density estimation approach , it still performs poorly in high - dimensional problems @xcite .      to avoid the separate density estimation , an alternative method called _ least - squares qmi _",
    "( lsqmi ) @xcite was proposed .",
    "below , we briefly review the lsqmi method .",
    "first , notice that @xmath94 can be expressed in term of the density difference as @xmath107 where @xmath108 the key idea of lsqmi is to directly estimate the density difference @xmath109 without going through any density estimation by the procedure of the _ least - squares density difference _ @xcite .",
    "letting @xmath110 be a model of the density difference , lsqmi learns @xmath110 so that it is fitted to the true density difference under the squared loss : @xmath111 by expanding the integrand , we obtain @xmath112 since the last term is a constant w.r.t .",
    "the model @xmath110 , we omit it and obtain the following criterion : @xmath113 then , the density difference estimator @xmath114 is obtained as the solution of the following minimization problem : @xmath115 .",
    "\\label{lsqmi_min}\\end{aligned}\\ ] ]    the solution of the minimization problem in eq . depends on the choice of the model @xmath110 .",
    "lsqmi employs the following linear - in - parameter model @xmath116 where @xmath117 is a parameter vector and @xmath118 is a basis function vector . for this model ,",
    "finding the solution of eq .",
    "is equivalent to solving @xmath119,\\end{aligned}\\ ] ] where @xmath120 by approximating the expectation over the densities @xmath71 , @xmath72 , and @xmath73 with sample averages , we obtain the following empirical minimization problem @xmath121,\\end{aligned}\\ ] ] where @xmath122 is the sample approximation of eq . : @xmath123 by including the @xmath0 regularization term , we obtain @xmath124,\\end{aligned}\\ ] ] where @xmath125 is the regularization parameter .",
    "then , the solution is obtained analytically as @xmath126 therefore , the density difference estimator is obtained as @xmath127 finally , qmi estimator is obtained by substituting the density difference estimator into eq .. a direct substitution yields two possible qmi estimators : @xmath128 however , it was shown in @xcite that a linear combination of the two estimators defined as @xmath129 provides smaller bias and is a more appropriate qmi estimator .    as shown above",
    ", lsqmi avoids multiple - step density estimation by directly estimating the density difference contained in qmi .",
    "it was shown that such direct estimation procedure tends to be more accurate than multiple - step estimation @xcite .",
    "moreover , lsqmi is able to objectively choose the tuning parameter contained in the basis function @xmath118 and the regularization parameter @xmath130 based on cross - validation .",
    "this property allows lsqmi to solve challenging tasks such as _ clustering _",
    "@xcite and _ unsupervised dimension reduction _ @xcite in an objective way .      given an efficient qmi estimation method such as lsqmi",
    ", supervised dimension reduction can be performed by finding a matrix @xmath103 defined as @xmath131 a straightforward approach to solving eq .",
    "is to perform the gradient ascent : @xmath132 where @xmath133 denotes the step size .",
    "the update formula means that the essential point of the qmi - based supervised dimension reduction method is not the accuracy of the qmi estimator , but the accuracy of the estimator of the derivative of the qmi .",
    "thus , the existing lsqmi - based approach which first estimates qmi and then compute the derivatives of the qmi estimator is not necessarily appropriate since an accurate estimator of qmi does not necessarily mean that its derivative is an accurate estimator of the derivative of qmi .",
    "next , we describe our proposed method which overcomes this problem .",
    "to cope with the weakness of the qmi estimation methods when performing supervised dimension reduction , we propose to _ directly _ estimate the derivative of qmi without estimating qmi itself .      from eq",
    ", the derivative of the @xmath134 w.r.t .  the @xmath135-th element of @xmath15 can be expressed by instead of @xmath134 when we consider its derivative for notational convenience .",
    "however , they still represent the qmi between random variables @xmath19 and @xmath18 . ]",
    "@xmath136 where in the second line we assume that the order of the derivative and the integration is interchangeable . by approximating the expectations over the densities @xmath71 , @xmath72 , and @xmath73 with sample averages ,",
    "we obtain an approximation of the derivative of qmi as @xmath137 note that since @xmath138 , we have that @xmath139 is the @xmath12-dimensional vector with zero everywhere except at the @xmath140-th dimension which has value @xmath141 .",
    "hence , eq . can be simplified as @xmath142 this means that the derivative of @xmath94 w.r.t .",
    "@xmath15 can be obtained once we know the derivatives of the density difference w.r.t .",
    "@xmath143 for all @xmath144 .",
    "however , these derivatives are often unknown and need to be estimated from data .",
    "below , we first discuss existing approaches and their drawbacks .",
    "then we propose our approach which can overcome the drawbacks .",
    "our current goal is to obtain the derivative of the density difference w.r.t .",
    "@xmath143 which can be rewritten as @xmath145 all terms in eq .",
    "are unknown in practice and need to be estimated from data .",
    "there are three existing approaches to estimate them .",
    "( a ) density estimation : :     +    separately estimate the densities    @xmath71 ,    @xmath72 , and    @xmath73 by , e.g. , _ kernel density    estimation_. then estimate the right - hand side of eq . as",
    "@xmath146    where @xmath147 ,    @xmath148 , and    @xmath149 denote the estimated    densities .",
    "( b ) density derivative estimation : :     +    estimate the density @xmath73 by e.g. , kernel    density estimation .",
    "next , separately estimate the densities derivative    @xmath150    and    @xmath151    by , e.g. , the method of _ mean integrated square error for derivatives _",
    "@xcite , which can estimate the density derivative without estimating    the density itself .",
    "then estimate the right - hand side of eq . as    @xmath152    where @xmath149 denotes the    estimated density , and    @xmath153    and    @xmath154    denote the ( directly ) estimated density derivatives .",
    "( c ) density difference estimation : :     +    estimate the density difference    @xmath109 by e.g. ,    _ least - squares density difference _",
    "@xcite , which can estimate the    density difference without estimating the densities themselves . then    estimate the left - hand side of eq . as @xmath155    where @xmath156    denotes the ( directly ) estimated density difference .",
    "the problem of approaches ( a ) and ( b ) is that they involve multiple estimation steps where some quantities are estimated first and then they are plugged into eq .. such multiple - step methods are not appropriate since each estimated quantity is obtained without regarding the others and the succeeding plug - in step using these estimates can magnify the estimation error contained in each estimated quantity .    on the other hand , approach ( c ) seems more promising than the previous two approaches since there is only one estimated quantity @xmath109 .",
    "however , it is still not the optimal approach due to the fact that an accurate estimator of the density difference does not necessarily means that its derivative is an accurate estimator of the derivative of the density difference .    to avoid the above problems , we propose a new approach which directly estimates the derivative of the density difference .",
    "we propose to estimate the derivative of the density difference w.r.t .",
    "@xmath143 using a model @xmath157 : @xmath158 the model @xmath157 is learned so that it is fitted to its corresponding derivative under the square loss : @xmath159 by expanding the square , we obtain @xmath160 since the last term is a constant w.r.t .",
    "the model @xmath157 , we omit it and obtain the following criterion : @xmath161 the second term is intractable due to the unknown derivative of the density difference . to make this term tractable",
    ", we use _ integration by parts _ @xcite to obtain the following : @xmath162_{z^{(\\ell ) }   = -\\infty}^{z^{(\\ell ) } = \\infty } { \\mathrm{d}\\boldsymbol{z}}_{\\backslash z^{(\\ell ) } } { \\mathrm{d}\\boldsymbol{y}}\\notag \\\\ & \\quad\\quad = \\iint f({\\boldsymbol{z}},{\\boldsymbol{y } } ) \\frac { \\partial g_{\\ell}({\\boldsymbol{z}},{\\boldsymbol{y}})}{\\partial z^{(\\ell ) } } { \\mathrm{d}\\boldsymbol{z}}{\\mathrm{d}\\boldsymbol{y}}+ \\iint g_{\\ell}({\\boldsymbol{z}},{\\boldsymbol{y } } ) \\frac { \\partial f({\\boldsymbol{z}},{\\boldsymbol{y}})}{\\partial z^{(\\ell ) } } { \\mathrm{d}\\boldsymbol{z}}{\\mathrm{d}\\boldsymbol{y } } , \\label{int_by_parts}\\end{aligned}\\ ] ] where @xmath163 denotes an integration over @xmath19 except for the @xmath140-th element . here",
    ", we require @xmath164 _ { z^{(\\ell ) } = -\\infty}^{z^{(\\ell ) } = \\infty } = 0 , \\label{assumption}\\end{aligned}\\ ] ] which is a mild assumption since the tails of the density difference @xmath165 often vanish when @xmath143 approaches infinity .",
    "applying the assumption to the left - hand side of eq . allows us to express eq . as @xmath166",
    "then , the estimator @xmath167 is obtained as a solution of the following minimization problem : @xmath168 .",
    "\\label{g_min}\\end{aligned}\\ ] ]    the solution of eq . depends on the choice of the model .",
    "let us employ the following linear - in - parameter model as @xmath157 : @xmath169 where @xmath170 is a parameter vector and @xmath171 is a basis function vector whose practical choice will be discussed later in detail . for this model , finding the solution of eq .",
    "is equivalent to solving @xmath172 ,   \\label{j_l_theta}\\end{aligned}\\ ] ] where we define @xmath173 by approximating the expectation over the densities @xmath71 , @xmath72 , and @xmath73 with sample averages , we obtain the following empirical minimization problem : @xmath174 , \\end{aligned}\\ ] ] where @xmath175 is the sample approximation of eq . :",
    "@xmath176 by including the @xmath0 regularization term to control the model complexity , we obtain @xmath177 , \\end{aligned}\\ ] ] where @xmath178 denotes the regularization parameter .",
    "this minimization problem is convex w.r.t .",
    "the parameter @xmath170 , and the solution can be obtained analytically as @xmath179 where @xmath180 denotes the identity matrix .",
    "finally , the estimator of the derivative of the density difference is obtained by substituting the solution into the model eq . as @xmath181    using this solution , an estimator of the derivative of qmi can be directly obtained by substituting eq . into eq .",
    "as @xmath182 we call this method the _ least - squares qmi derivative _ ( lsqmid ) .      as basis function @xmath171",
    ", we propose to use @xmath183^\\top,\\end{aligned}\\ ] ] where @xmath184 .",
    "first , let us define the @xmath185-th gaussian function as @xmath186 where @xmath187 and @xmath188 denote gaussian centers chosen randomly from the data samples @xmath189 , and @xmath190 denotes the gaussian width",
    ". we may use different gaussian widths for @xmath19 and @xmath18 , but this approach significantly increases the computation time for model selection which will be discussed in section  [ cross_validation ] . in our implementation , we standardize each dimension of @xmath16 and @xmath18 to have unit variance and zero mean , and then use the common gaussian width for both @xmath19 and @xmath18 .",
    "we also set @xmath191 in the experiments .",
    "based on the above gaussian function , we propose to use the following function as the @xmath185-th basis for the @xmath140-th model of the derivative of the density difference : @xmath192 this function is the derivative of the @xmath185-th gaussian basis function w.r.t .",
    "a benefit of this basis function design is that the integral appeared in @xmath193 can be computed analytically . through some simple calculation , we obtain the @xmath194-th element of @xmath193 as follows : @xmath195    as discussed in section  [ section : sdr_lsqmid ] , this basis function choice has further benefits when we develop a supervised dimension reduction method .",
    "the practical performance of the lsqmid method depends on the choice of the gaussian width @xmath190 and the regularization parameter @xmath196 included in the estimator @xmath167 .",
    "these tuning parameters can be objectively chosen by the @xmath197-fold cross - validation ( cv ) procedure which is described below .    1 .",
    "divide the training data @xmath7 into @xmath197 disjoint subsets @xmath198 with approximately the same size . in the experiments , we choose @xmath199 .",
    "2 .   for each candidate @xmath200 and each subset @xmath201 , compute a solution @xmath202 by eq . with the candidate @xmath203 and samples from @xmath204 ( i.e. , all data samples except samples in @xmath201 ) .",
    "3 .   compute the cv score of each candidate pair @xmath203 by @xmath205,\\end{aligned}\\ ] ] where @xmath206 denotes @xmath175 computed from the candidate @xmath203 and samples in @xmath201 .",
    "4 .   choose the tuning parameter pair such that it minimizes the cv score as @xmath207",
    "in this section , we propose a supervised dimension reduction method based on the proposed lsqmid estimator .",
    "recall that our goal in supervised dimension reduction is to find the matrix @xmath103 : @xmath208 a straightforward approach to find a solution of eq . using the proposed method is to perform gradient ascent as @xmath209 where @xmath210 denotes the step size .",
    "it is known that choosing a good step size is a difficult task in practice @xcite .",
    "_ line search _ is an algorithm to choose a good step size by finding a step size which satisfies certain conditions such as the _ armijo rule _ @xcite .",
    "however , these conditions often require access to the objective value @xmath211 which is unavailable in our current setup since the qmi derivative is directly estimated without estimating qmi . thus",
    ", if we want to perform line search , qmi needs to be estimated separately .",
    "however , this is problematic since the estimation of the derivative of the qmi and the estimation of the qmi are performed independently without regard to the other , and thus they may not be consistent . for example , the gradient @xmath212 , which is supposed to be an ascent direction , may be regarded as a descent direction on the surface of the estimated qmi .",
    "for such a case , the step size chosen by any line search algorithm is unreliable and the resulting @xmath15 may not be a good solution .",
    "below , we consider two approaches which can cope with this problem .      to avoid separate qmi estimation , we consider an approximated qmi which is obtained as a by - product of the proposed method . recall that the proposed method models the derivative of the density difference as @xmath213 this means that the density difference can be approximated by @xmath214 where @xmath215 is an unknown quantity which is a constant w.r.t .",
    "@xmath143 .    in a special case",
    "where @xmath216 , we can use eq . to obtain a proper approximator of @xmath94 in a similar fashion to the lsqmi method . to verify this ,",
    "let us substitute eq . into one of the @xmath217 in eq .",
    "to obtain @xmath218 where the last line follows from @xmath219 by approximating the expectation with sample averages , we obtain a qmi approximator as @xmath220    the main advantage of using @xmath221 is that it is obtained from the derivative estimation , and thus should be consistent with the estimated derivative .",
    "this allows us to perform line search for the gradient ascent in a consistent manner .",
    "we may further improve the optimization procedure by considering an optimization problem over the _ grassmann manifold _ :",
    "@xmath222 where @xmath223 is defined as @xmath224 that is , @xmath223 is a set of @xmath12-by-@xmath3 orthonormal matrices whose rows span the same subspace .",
    "this manifold optimization is more efficient than the original optimization since every step of the optimization always satisfies the orthonormal constraint , and we no longer need to perform orthonormalization .",
    "more details of manifold optimization can be found in @xcite .    although the qmi approximation in eq . allows us to choose step size by line search in a consistent manner , such an approximation is unavailable when @xmath225 .",
    "next , we consider an alternative optimization strategy which does not require an access to the qmi value .",
    "to avoid the problem of choosing the step size which requires an access to the qmi value , we propose to use a fixed - point iteration for finding a solution of eq .. note that from the first order optimality condition , a solution @xmath103 is a stationary point which satisfies @xmath226 where @xmath227 denotes @xmath12-by-@xmath3 zero matrix . by using the proposed basis function in eq . , eq .",
    "can be expressed as @xmath228 where we define @xmath229 with @xmath230 be the column vector of length @xmath231 consisting of the @xmath140-th dimension over all @xmath187 and the symbol @xmath232 represents the element - wise vector product .",
    "then , an approximated solution may be obtained by finding @xmath233 for all @xmath135 such that the left - hand side of eq .",
    "this optimization strategy results in a fixed - point iteration for each dimension of @xmath15 : @xmath234 finally , we orthonormalize the solution after each iteration as @xmath235 in practice , we perform this orthonormalization only every several iterations for computational efficiency .",
    "note that the optimization problem in eq .",
    "is non - convex and may have many local solutions . to avoid obtaining a poor local optimal solution",
    ", we perform the optimization starting from several initial guesses and choose the solution which gives the maximum estimated qmi as the final solution .",
    "in this section , we demonstrate the usefulness of the proposed method through experiments .",
    "firstly , we perform the following experiment to illustrate the usefulness of the proposed method in term of the qmi derivative estimation .",
    "let @xmath236 denotes the gaussian distribution with mean @xmath237 and covariance @xmath238 .",
    "then , for @xmath239 , we generate a dataset @xmath240 and a matrix @xmath15 as follows : @xmath241 where @xmath242 denotes a zero vector of length 2 .",
    "thus we have @xmath243 .",
    "the goal is to estimate @xmath244 at different value of @xmath245 .",
    "note that @xmath94 is maximized at @xmath246 , i.e. , @xmath247 .",
    "figure  [ fig_qmi_illust_avg ] shows the averaged value over 20 experiment trials of the estimated @xmath94 by lsqmi .",
    "the vertical axis indicates the value of the estimated qmi and the horizontal axis indicates value of @xmath248 $ ] .",
    "we use @xmath249 and @xmath250 for estimating qmi and denote the results by lsqmi(3000 ) and lsqmi(100 ) , respectively .",
    "we perform cross validation at @xmath246 and use the chosen tuning parameters for all values of @xmath245 .",
    "the result shows that lsqmi accurately estimates @xmath94 when the sample size is large .",
    "however , when the sample size is small , the estimated @xmath94 has high fluctuation .",
    "figure  [ fig_dqmi_illust_avg ] shows the averaged value over 20 experiment trials of the derivative of @xmath94 w.r.t .",
    "@xmath245 computed by lsqmi(3000 ) , lsqmi(100 ) , and the proposed method with @xmath250 which is denoted by lsqmid(100 ) . for the proposed method , we perform cross validation at @xmath246 and use the chosen tuning parameters for all values of @xmath245 .",
    "the result shows that lsqmid(100 ) gives a smoother estimate than lsqmi(100 ) which has high fluctuation .",
    "to further explain the cause of the fluctuation of lsqmi(100 ) , we plot experiment results of 4 trials in figure  [ fig_qmi_dqmi_illust ] , where the left column corresponds to the value of the estimated @xmath94 while the right column corresponds to the value of the estimated derivative of @xmath94 w.r.t .  @xmath245 .",
    "these results show that for lsqmi(100 ) , a small fluctuation in the estimated qmi can cause a large fluctuation in the estimated derivative of qmi . on the other hand , lsqmid directly estimates the derivative of qmi and thus does not suffer from this problem .",
    "0.50   and the estimated derivative of @xmath94 w.r.t .",
    "@xmath245 over 20 experiment trials.,title=\"fig:\",scaledwidth=100.0% ] [ fig_qmi_illust_avg ]    0.50   and the estimated derivative of @xmath94 w.r.t .",
    "@xmath245 over 20 experiment trials.,title=\"fig:\",scaledwidth=100.0% ] [ fig_dqmi_illust_avg ]    0.50       [ fig_qmi_illust ]    0.50       [ fig_dqmi_illust ]      next , we evaluate the usefulness of the proposed method in supervised dimension reduction using artificial datasets .",
    "firstly , let @xmath251 denote the uniform distribution over an interval @xmath252 $ ] , @xmath253 denote the gamma distribution with shape parameter @xmath254 and scale parameter @xmath231 , and @xmath255 denote the laplace distribution with mean @xmath254 and scale parameter @xmath231 .",
    "then we consider the input @xmath16 with @xmath256 , the output @xmath20 with @xmath257 , and the optimal matrix @xmath258 ( including their rotations ) as follows :    dataset a : : :    for @xmath259 , we use    @xmath260 dataset b : : :    for @xmath261 and    @xmath262 , we use @xmath263 dataset c : : :    for @xmath261 and    @xmath262 , we use @xmath264 dataset d : : :    for @xmath265 and    @xmath262 , we use @xmath266    for the datasets * a * , * b * , and * c * , @xmath267 is an additive gamma noise , while for the datasets * d * , @xmath267 is a multiplicative gaussian noise .",
    "figure  [ fig_art_data ] shows the plot of these datasets ( after standardization ) .",
    "note the presence of outliers in the datasets .",
    "0.50     0.50     0.50     0.50     to estimate @xmath15 from @xmath268 , we execute the following methods :    lsqmid : : :    the proposed method . supervised dimension reduction is performed by    maximizing @xmath94 where the derivative of    @xmath94 is estimated by the proposed method .    the solution @xmath269 is obtained by    fixed - point iteration .",
    "lsqmi : : :    supervised dimension reduction is performed by maximizing    @xmath94 where @xmath94    is estimated by lsqmi and the derivative of    @xmath94 w.r.t .  @xmath15    is computed from the qmi estimator . the solution    @xmath269 is obtained by gradient    ascent with linesearch over the grassmann manifold .",
    "lsdr : : :    supervised dimension reduction is performed by maximizing    @xmath86 . the solution    @xmath269 is obtained by gradient    ascent with linesearch over the grassmann manifold .",
    "dmave : : :    supervised dimension reduction is performed by minimizing an error of    the local linear smoother of the conditional density    @xmath32 . the solution    @xmath269 is obtained by alternatively    solving quadratic programming problems .",
    "kdr : : :    supervised dimension reduction is performed by minimizing the trace of    the conditional covariance operator    @xmath78 .    the solution @xmath269 is obtained by    gradient descent with linesearch over the stiefel    manifold .    for methods which require initial solutions , i.e. , lsqmid , lsqmi , and lsdr",
    ", we randomly generate 10 orthonormal matrices and use them as the initial solutions . for dmave and kdr , we use a solution obtained by dopg and gkdr , respectively , as the initial solution .",
    "finally , the obtained solution @xmath269 is evaluated by the dimension reduction error defined as @xmath270 where @xmath271 denotes the frobenius norm of a matrix .",
    "table  [ table_art ] shows the mean and standard error over 30 experiment trials of the dimension reduction error on the artificial datasets with different sample size .",
    "the results show that the proposed method works well overall .",
    "lsdr performs well especially for dataset * a * and * b*. kdr also performs well overall .",
    "however , its performance is quite unstable for dataset * b * , which can be seen by relatively large standard errors .",
    "this is because gkdr might provide a poor initial solution to kdr in some experiment trials , which makes kdr fails to find a good solution .",
    "on the other hand , both lsqmi and dmave do not perform well overall .",
    "lsqmi tends to be unstable and works very poorly especially when the sample size is small , except for dataset * d*. the cause of this failure could be the high fluctuation of the derivative of qmi by lsqmi , as shown previously in the illustrative experiment .",
    "although the solution of dmave is quite stable , its performance is not overall comparable to the other methods .",
    "this is because the model selection strategy in dmave did not perform well for these datasets .    [",
    "cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     [ table_bench ]      finally , we evaluate the proposed method in supervised dimension reduction on uci benchmark datasets @xcite . for all datasets , we append the original input @xmath16 with noise features of dimensionality 5 . more specifically , for the original input @xmath16 with dimensionality @xmath3 , we consider the augmented input @xmath272 with dimensionality @xmath273 as @xmath274 where @xmath275 for @xmath276 .",
    "then we use the paired data @xmath277 to perform experiments .",
    "we randomly choose @xmath278 samples for training purposes , and use the rest @xmath279 for testing purposes .",
    "we execute the supervised dimension reduction methods with target dimensionality @xmath280 to obtain solutions @xmath269 .",
    "then we train a kernel ridge regressor   @xmath281   with the gaussian kernel where the tuning parameters are chosen by 5-fold cross - validation .",
    "finally , we evaluate the regressor by the _ root mean squared error _ ( rmse ) : @xmath282    table  [ table_bench ] shows the rmse averaged over 30 trials for the benchmark experiments .",
    "it shows that the proposed method performs well overall on all datasets .",
    "lsqmi performs very well for the ` fertility ' and ` bike ' datasets , but its performance is quite poor for the other datasets . in contrast",
    ", dmave performs very well especially for the ` concrete ' dataset where it gives the best solutions for all value of @xmath12 .",
    "however , its performance is quite poor for the ` fertility ' and ` bike ' datasets .",
    "both lsdr and kdr do not perform well on these datasets .",
    "we proposed a novel supervised dimension reduction method based on efficient maximization of quadratic mutual information ( qmi ) .",
    "our key idea was to _ directly _ estimate the derivative of qmi without estimating qmi itself .",
    "we firstly developed a method to directly estimate the derivative of qmi , and then developed fixed - point iteration which efficiently uses the derivative estimator to find a maximizer of qmi .",
    "in addition to the robustness against outliers thanks to the property of qmi , the proposed method is widely applicable because it does not require any assumption on the data distribution and tuning parameters can be objectively chosen via cross - validation .",
    "the experiment results on artificial and benchmark datasets showed that the proposed method is promising .",
    "atif , j. , ripoche , x. , coussinet , c. , and osorio , a. ( 2003 ) .",
    "non rigid medical image registration based on the maximization of quadratic mutual information . in _ proceedings of bioengineering conference , 2003 ieee 29th annual _ , pages 7172 .",
    "sasaki , h. , noh , y. , and sugiyama , m. ( 2015 ) .",
    "direct density - derivative estimation and its application in kl - divergence approximation . in _ proceedings of the eighteenth international conference on artificial intelligence and statistics , aistats 2015 ,",
    "san diego , california , usa , may 9 - 12 , 2015_.            suzuki , t. , sugiyama , m. , sese , j. , and kanamori , t. ( 2008 ) . approximating mutual information by maximum likelihood density ratio estimation . in _",
    "third workshop on new challenges for feature selection in data mining and knowledge discovery , fsdm 2008 , held at ecml - pkdd 2008 , antwerp , belgium , september 15 , 2008 _ , pages 520 ."
  ],
  "abstract_text": [
    "<S> a typical goal of supervised dimension reduction is to find a low - dimensional subspace of the input space such that the projected input variables preserve maximal information about the output variables . </S>",
    "<S> the dependence maximization approach solves the supervised dimension reduction problem through maximizing a statistical dependence between projected input variables and output variables . </S>",
    "<S> a well - known statistical dependence measure is mutual information ( mi ) which is based on the kullback - leibler ( kl ) divergence . </S>",
    "<S> however , it is known that the kl divergence is sensitive to outliers . on the other hand , quadratic mi ( qmi ) </S>",
    "<S> is a variant of mi based on the @xmath0 distance which is more robust against outliers than the kl divergence , and a computationally efficient method to estimate qmi from data , called least - squares qmi ( lsqmi ) , has been proposed recently . for these reasons , developing a supervised dimension reduction method based on lsqmi </S>",
    "<S> seems promising . </S>",
    "<S> however , not qmi itself , but the derivative of qmi is needed for subspace search in supervised dimension reduction , and the derivative of an accurate qmi estimator is not necessarily a good estimator of the derivative of qmi . in this paper , we propose to directly estimate the derivative of qmi without estimating qmi itself . </S>",
    "<S> we show that the direct estimation of the derivative of qmi is more accurate than the derivative of the estimated qmi . </S>",
    "<S> finally , we develop a supervised dimension reduction algorithm which efficiently uses the proposed derivative estimator , and demonstrate through experiments that the proposed method is more robust against outliers than existing methods . </S>"
  ]
}