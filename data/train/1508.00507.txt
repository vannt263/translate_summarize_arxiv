{
  "article_text": [
    "in weakly supervised learning paradigms , a learner receives a training set consisting of a limited amount of labelled data as well as a fairly larger amount of unlabelled data . without loss of generality , assume that we have a dataset consisting of bags of instances with binary labels that are assigned based on the existence of a specific object of interest .",
    "each bag consists of instances .",
    "bags , as well as instances can be labelled positive or negative .",
    "a positive label means that the corresponding bag ( instance ) contains ( is ) the object , whereas a negative label signifies that the bag ( instance ) does not contain ( is not ) the object .",
    "labels are provided for the bags only .",
    "if a bag is labelled negative , then it is fully labelled as all the instances of the bag definitely do not represent the object of interest , and therefore all the instances or parts of the bag are labelled .",
    "a positive - labelled bag is considered fully labelled if the instances representing the object of interest are indicated in the bag .",
    "a bag can be positive - labelled but no information is provided about which instances represent the object in the bag or , put in other words , instance labels are treated as latent variables .",
    "datasets containing the latter kind of bag represent an example of datasets handled by weakly supervised learning paradigms . with all instances of positive - labelled bags",
    "being unlabelled , performance of a classifier will undoubtedly be far from optimal .",
    "the aim of the weakly supervised learning approach introduced in this study , is to weakly annotate the unlabelled instances so that performance of a classifier applied subsequently on the data can be improved .",
    "weakly supervised learning paradigms are recently gaining further significance in machine learning due to the increasing importance of learning from unlabelled data .",
    "obtaining fully labelled data is apparently very useful but is error - prone and , more importantly , expensive to obtain @xcite , and this is why weakly supervised learning paradigms are crucial .    a real - world challenge that lends very well to the application of",
    "weakly supervised learning paradigms where datasets have a bags - of - instances form is the problem of muscle classification based on electromyographic ( emg ) signals . here ,",
    "the datasets consist of sets of electromyographic ( emg ) signals that represent a particular muscle , with each emg signal comprising of emg signal contributions from the different components that make up the muscle .",
    "each component of a muscle is referred to as a motor unit ( mu ) and the emg contribution of each component is referred to as a motor unit potential train ( mupt ) .",
    "each mu has one label out of three ; normal , myopathic or neurogenic .",
    "the same three labels are used to label each muscle . in the training dataset , there are feature values for each component of the muscle , i.e.  for each mu , while labels are available only for each muscle as a whole . for two of the three labels ( myopathic and neurogenic , and",
    "it will be referred to both of them together here as disordered ) , a muscle that is `` myopathic '' or `` neurogenic '' contains both normal and myopathic or neurogenic components , respectively . on the other hand , in a `` normal '' muscle ,",
    "all components are normal .",
    "this can be seen as a multiclass version of the bags - of - instances example described earlier .",
    "rather than having two labels ; one containing fully labelled instances and another containing unlabelled instances , there are three labels , one containing fully labelled instances and two containing unlabelled instances . instances of normal muscles are fully labelled because they are all labelled normal , while instances of both myopathic and neurogenic muscles are unlabelled because it is not known which of them are disordered and which are normal . therefore",
    ", the normal label in muscles is equivalent to the negative label in bags while both the myopathic and neurogenic labels in muscles are equivalent to the positive label in bags .",
    "there are numerous examples of learning algorithms where unlabelled or weakly labelled data are utilised @xcite .",
    "also , @xcite provide another example where they exploit weakly annotated web images to build a weakly supervised object classifier .",
    "in addition to object recognition in images , weakly supervised learning has other applications in computer vision .",
    "for example , @xcite perform learning based on weakly labelled videos and fully labelled images in order to detect objects from web videos .",
    "other examples of weakly supervised learning algorithms that are applied on videos include @xcite and @xcite .",
    "multiple - instance learning ( mil ) algorithms are closely related to weakly supervised data . in mil",
    ", training instances are grouped together in bags .",
    "each bag has a label associated to it .",
    "each instance belongs to only one bag .",
    "each instance has a label associated to it that can be the same or different from the label of the bag it belongs to .",
    "instance labels are not observed .",
    "@xcite train an mil discriminative classifier on weakly annotated image data .",
    "@xcite represents another example of an mil algorithm applied on weakly annotated data , while @xcite , @xcite and @xcite are examples of mil - based approaches applied on weakly labelled images for the purpose of object recognition .",
    "the main objective of this study is to turn a training dataset consisting of a limited number of labelled instances and a larger number of unlabelled instances into a larger annotated training dataset consisting of weakly labelled instances , which are those that were unlabelled , and strongly labelled instances , for the sake of improving classification performance .",
    "this is to be achieved via the proposed weakly supervised learning approach .",
    "unlabelled data are weakly annotated by applying a spectral graph - theoretic grouping strategy that makes use of strongly labelled instances as well as similarity among instances in order to assign weak labels to the unlabelled instances .",
    "spectral graph - theoretic grouping is based on similarity graph models .",
    "in addition to the similarity graph models in the literature , two new similarity graph models are introduced in this study .",
    "weakly labelled and strongly labelled instances form a larger annotated training dataset . by utilizing the proposed spectral grouping strategy to facilitate for",
    "weakly supervised learning , one can obtain a larger annotated training dataset composed of strongly and weakly labelled data that should consequently lead to an improved classification performance compared to the case when only strongly labelled data , or strongly labelled data and unlabelled data , are used .",
    "this study serves as a methodological guide to other weakly supervised learning paradigms , and is not limited to the aforementioned emg muscle classification example .",
    "in addition to being a part of the proposed weakly supervised learning paradigm , performance of the new similarity graph models is tested separately on three benchmark datasets , abalone , swiss banknotes and segmentation .",
    "the abalone dataset consists of abalone s physical measurements ( input ) , and it is required to predict the abalone s age ( output ) from these measurements .",
    "features of the swiss banknotes dataset are explanatory variables describing characteristics of swiss banknotes and the goal is to decide whether or not each banknote is genuine @xcite .",
    "the segmentation dataset contains images , each representing one out of seven outdoor objects , i.e.  grass , path , window , cement , foliage , sky or brickface @xcite .",
    "the main contribution of this study is the introduction of a new weakly supervised learning approach based on spectral graph - theoretic grouping .",
    "the approach mainly targets datasets of the bags - of - instances form , like the one shown in figure  [ fig : one](a ) .",
    "instances belonging to the green labelled bags ( e.g.  the bag at top left ) are labelled , but instances belonging to blue and red ( e.g.  the other two ) bags are unlabelled . for instances of each of the latter two bag labels , instances are grouped together in one spectral graph .",
    "more specifically , all instances of blue labelled bags are grouped together in one similarity graph , which is graph1 in figure  [ fig : one](b ) whereas graph2 in figure  [ fig : one](b ) contains all instances of red labelled bags .",
    "spectral graph - theoretic grouping is performed by first constructing similarity graph models and then performing spectral grouping ( figure  [ fig : one](b ) ) . as per this step ,",
    "two similarity graph models are proposed . using the groups resulting from spectral grouping along with the strongly labelled instances associated within the spectral groups ,",
    "unlabelled instances are weakly annotated and the result is figure  [ fig : one](c ) .",
    "the assumption is that across all blue ( resp .",
    "red ) bags , the total number of green instances is less than the total number of blue ( resp .",
    "red ) instances .",
    "thus , the group with greater cardinality is assigned the blue ( resp .",
    "red ) label .",
    "the premise is that by applying an efficient grouping strategy on nodes of each similarity graph , we can weakly ( but reliably ) annotate the involved instances . by doing so",
    ", we construct a larger annotated training set .",
    "finally a weakly supervised classifier exploits the whole dataset consisting of strongly labelled data and weakly labelled data ( figure  [ fig : one](d ) ) . in this study , we will not only introduce a new weakly supervised learning approach , but will also investigate and devise a number of similarity graph models and study their effect on the ability to obtain reliable weak annotations for unlabelled instances .",
    "graph - theoretic grouping has been studied before in the literature for a number of different applications .",
    "@xcite used spectral graph - theoretic grouping in an image segmentation application .",
    "they developed spectral groups which were based on using a small number of samples and extrapolating so that the computational requirements are reduced .",
    "@xcite developed a graph - theoretic grouping algorithm that was used for image grouping .",
    "they grouped images based on the observation that visually similar images are also similar in the feature space because they have similar feature vectors . @xcite",
    "represent one further example of a graph - theoretic grouping algorithm where they develop an algorithm for image segmentation .",
    "they performed grouping by building an undirected graph using data instances , then forming mutually exclusive subgraphs by gradually removing arcs according to a certain criterion . here , we make use of graph - theoretic grouping for a completely different purpose : for improved weakly annotation of unlabelled data for weakly supervised learning .    one strategy of weakly annotating unlabelled data is to apply a grouping technique so that each part of the unlabelled data can be related to a group , which in turn is assigned a certain label depending on the inherent relationships between the strongly labelled part of the data and the unlabelled part . due to the fact that spectral grouping acts only on the unlabelled part of the data , the introduced spectral graph - theoretic grouping strategies are applicable on fully unlabelled datasets because they practically act on the unlabelled part of the data , as long as other grouping issues ( e.g. number of groups ) can be handled via the problem assumptions .",
    "for the emg datasets , instances belonging to each disordered type of bag ( myopathic or neurogenic ) are known to be either normal or of the same disorder .",
    "therefore , there are two groups . for the benchmark datasets used",
    ", the number of groups is known . as a preliminary phase of this study , spectral grouping as well as other grouping algorithms",
    "were applied to the emg data .",
    "normalised spectral graph - theoretic grouping according to shi and malik @xcite performed slightly better than other spectral graph - theoretic grouping algorithms which in turn performed better than other grouping algorithms .",
    "however , the improvement provided by shi and malik s normalised spectral graph - theoretic grouping was not considerable .",
    "after a careful inspection of the reason why shi and malik s normalised spectral graph - theoretic grouping does not perform better than it does , it turned out to be the fact that all similarity graph models used before in the spectral clustering literature do not capture well the pairwise similarities between instances .",
    "it is worth noting that there are two main normalised spectral graph - theoretic grouping algorithms ; one is according to shi and malik @xcite and the other is according to ng et al .",
    "for the sake of simplicity , the former is shortly referred to in this study as normalised spectral graph - theoretic grouping , unless stated otherwise .",
    "the first step of the proposed weakly supervised learning approach is to perform spectral graph - theoretic grouping on the unlabelled part of the dataset .",
    "spectral graph - theoretic grouping in turn begins by forming similarity graph model(s ) of unlabelled data . in the literature , there are several popular similarity graph models that transform a given set @xmath0 of data instances with pairwise similarities @xmath1 or pairwise distances @xmath2 into a graph . when constructing a similarity graph model",
    "the goal is to model the local neighbourhood relationships between data instances .",
    "the following is a list of the main similarity graph models in the literature .",
    "* @xmath3-neighbourhood graph : * instances that have pairwise distances among each other less than @xmath3 are connected while the instances with pairwise distances greater than or equal to @xmath3 are not .",
    "weights are considered to be at the same scale of distances ; which is at most @xmath3 @xcite .",
    "therefore an @xmath3-neighbourhood graph is unweighted .    * @xmath4-nearest neighbour graph * : an instance @xmath5 is connected to an instance @xmath6 if @xmath6 is among the @xmath4-nearest neighbours of @xmath5 .",
    "the neighbourhood relationship is not symmetric and due to that the resulting graph is a directed graph .",
    "therefore , the graph should be transformed into an undirected graph .",
    "one way of transforming it into an undirected graph is by connecting two instances @xmath5 and @xmath6 if @xmath6 is among the neighbours of @xmath5 `` or ''",
    "@xmath5 is among the neighbours of @xmath6 .",
    "the resulting undirected graph is referred to as the @xmath4-nearest neighbour graph ( or symmetric @xmath4-nearest neighbour graph @xcite ) .",
    "another way is to connect two instances @xmath5 and @xmath6 if @xmath6 is among the neighbours of @xmath5 `` and '' @xmath5 is among the neighbours of @xmath6 . the resulting undirected graph in this case",
    "is referred to as the mutual @xmath4-nearest neighbour graph .",
    "after building the similarity graph model in both cases , edges of the graph are weighted by measuring the similarity of the respective vertices @xcite .",
    "* fully connected graph : * all instances are connected to one another ; in other words all instances are considered `` similar '' to one another .",
    "the edges are weighted by @xmath1 .",
    "the graph is useful only when local neighbourhoods can be modelled by the similarity function because this is the only way by which the fully connected graph can represent the local neighbourhood relationships @xcite .",
    "the gaussian similarity function @xmath7 is an example of this kind of similarity function .",
    "the parameter @xmath8 of the gaussian similarity function controls the width of the neighbourhoods .",
    "the parameter @xmath8 acts like the parameter @xmath3 in the construction of the @xmath3-neighbourhood graph @xcite .",
    "emg datasets provide examples of datasets where neither an @xmath3-neighbourhood graph , a @xmath4-nearest neighbour graph nor a fully connected graph can capture properly the similarities between the data instances especially when there are different densities within the same dataset . to address this issue ,",
    "we propose two new similarity graph models aimed at providing greater robustness in handling different data densities within a dataset .",
    "they are referred to as probabilistic threshold graph and probabilistic criterion graph .",
    "one of the main advantages of the proposed probabilistic threshold and probabilistic criterion graphs is that they do not have a problem in dealing with instances in different scales .",
    "this means that unlike the @xmath3-neighbourhood graph which does not connect instances belonging to the same scale when a dataset is on different scales , and unlike the @xmath4-nearest neighbour graph which , in the same case , would connect instances on different scales , the proposed similarity graph models can connect instances within regions of constant density when data is on different scales .",
    "the mutual @xmath4-nearest neighbour graph can at times act on different scales but setting the parameter @xmath4 in this case usually is a problem because , first finding the optimal @xmath4 value for a certain dataset is tricky and second and more importantly , one dataset can have the optimal value of @xmath4 that does not mix the data scales with one another on one part of the dataset different from the optimal value of @xmath4 on another part of the same dataset .",
    "+ in the proposed probabilistic threshold graph , a parameter @xmath9 is used as a threshold on the similarity values .",
    "similarity values greater than or equal to @xmath9 are kept while similarity values smaller than @xmath9 are assigned using a truncated gaussian distribution with mean = @xmath9 and standard deviation = @xmath8 .",
    "another parameter @xmath10 is used to decide the final similarity values as illustrated in the probabilistic thresholding similarity graph model section .",
    "as shown in equations  [ eq : one ] @xmath11  [ eq : two ] , initial values of similarity , which are compared with @xmath9 are normalised based on the summation of distances from a certain instance .",
    "this leads to the fact that the thresholding applied here is relative to the data and does not depend on absolute values as is the case with @xmath3 in the @xmath3-neighbourhood graph and @xmath4 in the @xmath4-nearest neighbour graph .",
    "nonetheless @xmath9 and @xmath10 still provide a hard thresholding on the similarity values and therefore these parameters affect the similarity values a great deal . in order to alleviate this effect of hard thresholding , a similarity graph model that is based on a probabilistic acceptance criterion",
    "is proposed . as illustrated in the probabilistic acceptance criterion similarity graph model section ,",
    "similarity values are assigned either from @xmath12 or to @xmath13 based on a probability value and there is no hard threshold under which similarity values are directly assigned a value of @xmath13 .",
    "the proposed similarity graph models can be formally described as follows :      for each vertex @xmath14 , distances between a vertex @xmath14 and all other vertices ; @xmath15 are calculated first as euclidean distances .",
    "initial similarity values are subsequently calculated as a function of the distances by equation  [ eq : one ] .",
    "@xmath16    similarity values greater than or equal to @xmath9 are kept , while the rest of similarity values are assigned using a truncated gaussian distribution with mean = @xmath9 and standard deviation = @xmath8 .",
    "@xmath17 is a smoothing parameter that controls the normalised similarity values by tuning the ratio given to each distance . in the limit @xmath18 ,",
    "distances are assigned equal weight and @xmath9 acts the same way as @xmath4 in the @xmath4-nearest neighbour similarity graph model . on the other hand , the larger the absolute value of @xmath17 the larger the weight assigned to the smallest distance and the smaller the weight assigned to the rest .",
    "@xmath19 is used in all the experiments , because it keeps a convenient number of distances greater than or equal to @xmath9 .",
    "also , experiments were applied using @xmath20 , and the highest classification accuracy has always been obtained with @xmath19 .    if @xmath21 , @xmath1 is used to represent the respective edge weight on the similarity graph model . for the interval @xmath22",
    ", values of @xmath12 are used to decide the final value of @xmath1 as follows .",
    "if a weight value generated by @xmath23 is smaller than a certain small threshold value @xmath10 , then the respective similarity value is set to @xmath13 , otherwise the similarity value is set to the generated weight . in summary , define @xmath24 as @xmath25 , where @xmath26 is defined in equation  [ eq : two ] .",
    "then , similarity values greater than or equal to @xmath9 are taken as they are , similarity values smaller than @xmath24 are set to @xmath13 , and for the interval @xmath27 similarity values are assigned by @xmath12 , as shown in equation  [ eq : two ] .",
    "similarity graph models constructed by probabilistic thresholding are referred to as probabilistic threshold graphs .",
    "@xmath28 based on comprehensive cross - validation , the optimal values of @xmath9 and @xmath8 are obtained .",
    "the parameter @xmath8 controls the width of the neighbourhoods for instances farther than @xmath9 . in the limit @xmath29 ,",
    "such neighbourhoods are assigned a weight value of @xmath13 .",
    "the smaller the value of @xmath8 , the more sparse the similarity graph model .      distances and corresponding initial similarity values are calculated the same way as in thresholding .",
    "similarity values that are greater than or equal to @xmath9 are again kept as they are while a truncated gaussian distribution with mean = @xmath9 and standard deviation = @xmath8 is utilised as follows in order to calculate similarity values smaller than @xmath9 .",
    "the weight values resulting from @xmath23 are accepted as they are into the neighbourhood with a probability based on the generated weight and therefore a stochastic acceptance criterion , that does not require a threshold , is provided . to sum it up , similarity values greater than or equal to @xmath9",
    "are taken as they are , while for interval @xmath22 similarity values are obtained either by @xmath12 , with a probability based on the weight generated from @xmath23 , or set to 0 otherwise , as displayed in equation  [ eq : three ] .",
    "similarity graph models constructed by probabilistic acceptance criterion are referred to as probabilistic criterion graphs . @xmath30",
    "both neighbourhood relationships of the proposed similarity graph models are turned into symmetric neighbourhoods in a fashion similar to the @xmath4-nearest neighbour graph ; either by assigning the maximum value out of @xmath31 @xmath32 to both of them or by taking the minimum value out of these two values to be their updated symmetric similarity value .",
    "+ examples where the advantages of the proposed probabilistic threshold and probabilistic criterion graphs are clear , usually relate to groups that have irregular shapes . for example",
    ", figure  [ fig : two ] shows a toy dataset representing a pattern that takes place quite often in the emg datasets as well as other datasets where there are two or more ( two in the case of figure  [ fig : two ] ) irregular groups in the data .",
    "a matlab gui , which was developed by @xcite , is tailored in order to show the figures used throughout this illustrative example .",
    "figure  [ fig : three ] shows how the instances are connected when an @xmath3-neighbourhood graph is used with values of @xmath3 equal to @xmath33 and @xmath34 .",
    "when @xmath3 is less than the former , number of groups or connected components is @xmath35 5 whereas number of groups is always 1 for values of @xmath3 greater than the latter .",
    "in figure  [ fig : three](a ) , @xmath36 is the value of @xmath3 that resulted from leave - one - out cross - validation on this small dataset and it leads to @xmath37 groups as it loosely or never connects instances belonging to the same correct group ( a correct group refers to a group in figure  [ fig : two ] ) .",
    "bigger values of @xmath3 , like @xmath38 in figure  [ fig : three](b ) , overconnects instances belonging to the two different correct groups .    figure  [ fig : four ] shows the similarity graph models when a symmetric @xmath4-nearest neighbour graph is used with values of @xmath4 equal to @xmath39 , @xmath40 , @xmath41 and @xmath42 .",
    "the number of groups is always @xmath39 for values of @xmath4 greater than 4 .",
    "no value of @xmath4 made a symmetric @xmath4-nearest neighbour graph get the correct groups .",
    "the value of @xmath4 resulting from leave - one - out cross - validation is @xmath43 and it connects instances belonging to the two different correct groups .",
    "figure  [ fig : five ] shows the similarity graph models when a mutual @xmath4-nearest neighbour graph is used with values of @xmath4 equal to @xmath42 , @xmath37 , @xmath44 and @xmath45 .",
    "the number of groups is always @xmath39 for values of @xmath4 greater than 7 .",
    "no value of @xmath4 made a mutual @xmath4-nearest neighbour graph get the correct groups .",
    "the value of @xmath4 resulting from leave - one - out cross - validation is @xmath46 and even if it is a better fit than both the @xmath3-neighbourhood graph and the symmetric @xmath4-nearest neighbour graph , the two disconnected components on the right side of figure  [ fig : five](a ) should have been connected as one group because , as per figure  [ fig : two ] , they belong to the same correct group .",
    "the same goes for the group on the right side along with the one in the middle of figure  [ fig : five](a ) .",
    "figure  [ fig : six ] shows the similarity graph model resulting when a probabilistic threshold graph is used with a value of weight threshold @xmath9 equal to @xmath47 which is the value resulting from applying leave - one - out cross - validation on this illustrative dataset .",
    "the probabilistic threshold graph is the only similarity graph model that leads to the correct groups because the values that @xmath9 are compared to are normalised values representing the distance between a certain instance and another divided by summation of distances between the former to all instances of the dataset .",
    "this normalization leads to a similarity graph model that not only depends on absolute values of parameters but is also heavily impacted by relative weights where a distance from a certain instance to another is taken into consideration relative to other distances from the former instance to all others .",
    "formal notations of a general form of grouping and graph grouping are presented , followed by three equations ( equations  [ eq : four ] ,  [ eq : five ] @xmath11  [ eq : six ] ) presenting the main graph laplacian matrices in the literature .",
    "[ [ grouping - input ] ] grouping input + + + + + + + + + + + + + +    the learner receives a set @xmath48 of @xmath49 i.i.d .  instances where each instance has @xmath50 features . even if it is not always the case , but let s assume another number @xmath4 is given , representing the number of groups",
    "this is in line with this study .",
    "[ [ grouping - output ] ] grouping output + + + + + + + + + + + + + + +    the learner is required to return a partition of the @xmath49 instances into @xmath4 disjoint subsets @xmath51 , where @xmath52 @xcite .",
    "a good partitioning should minimise pairwise distances among instances of the same subset and maximise pairwise distances among instances of different subsets , so that subsets are homogeneous and well separated , respectively .    [",
    "[ graph - theoretic - grouping ] ] graph - theoretic grouping + + + + + + + + + + + + + + + + + + + + + + + +    the learner is required to return a partition of a graph into disjoint subsets , or groups of vertices , where edges between vertices of different groups have weights that are as low as possible ( well separated groups ) and edges between vertices within the same group have weights that are as high as possible ( homogeneous groups ) .",
    "let @xmath53 be the degree matrix and @xmath54 be the edge weight matrix of the similarity graph .",
    "the unnormalised graph laplacian is equal to the following : @xmath55 there are two ways by which a normalised graph laplacian can be calculated @xcite , which are as follows : @xmath56 or @xmath57    a normalised graph laplacian is calculated in all the spectral graph - theoretic grouping algorithms performed in this study via equation  [ eq : six ] .",
    "the following is a description of the spectral graph - theoretic grouping algorithm used in the experiments :    [ [ normalised - spectral - graph - theoretic - grouping - according - to - shi - and - malik ] ] normalised spectral graph - theoretic grouping according to shi and malik @xcite + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    * construct a similarity graph model @xmath58 as one of the models described in the similarity graph models section or one of the two introduced similarity graph models . *",
    "calculate the normalised laplacian @xmath59 . *",
    "calculate the first @xmath4 eigenvectors @xmath60 of @xmath61 .",
    "@xmath4 is the number of groups . in our study , @xmath62 .",
    "* let @xmath63 be a matrix whose columns are @xmath60 * let @xmath64 be the @xmath65 row of @xmath66 . * group @xmath67 , @xmath68 into subsets @xmath69 using k - means",
    "the proposed weakly supervised learning approach mainly consists of a spectral graph - theoretic grouping strategy , which in turn is based on similarity graph models , and a subsequent weak classifier .",
    "therefore , investigating the performance of similarity graph models is crucial to the performance evaluation of the weakly supervised learning approach .",
    "datasets used in the evaluation of the proposed weakly supervised learning approach as a whole are real - world emg datasets , whilst datasets used in a separate evaluation of the introduced similarity graph models are three benchmark datasets , abalone , swiss banknotes and segmentation .",
    "there is no ground truth labelling available for the unlabelled instances of the emg datasets .",
    "the main purpose of weakly annotating unlabelled instances is to improve the performance of the subsequent weak classifier .",
    "this means that accuracy of the weak classifier is the main metric for measuring the quality of the weak annotation .",
    "still , we present the results of an internal evaluation metric of grouping in order to demonstrate the quality of the grouping strategy in a generic sense .",
    "davies - bouldin index is used as an internal evaluation measure for the emg datasets .    for a @xmath40-group problem like the emg grouping",
    "( see the methods section ) , davies - bouldin can be calculated as follows : @xmath70 where @xmath71 is the centroid of group @xmath72 , and @xmath73 is the average distance of all instances of group @xmath72 to centroid @xmath74 , and @xmath75 is the distance between the two centroids @xcite .",
    "as the numerator expresses the compactness of the groups of a grouping result ( intra - group distance ) and the denominator expresses the separation among the groups ( inter - group distance ) , the smaller the value of davies - bouldin index , the better the corresponding grouping .",
    "values of davies - bouldin index do not depend on number of groupings nor the grouping algorithm in use @xcite .",
    "another advantage of davies - bouldin index is that it has a better time complexity than most other internal evaluation measures of grouping@xcite .",
    "benchmark grouping datasets used in the experiments of this study have their ground truth labels available .",
    "ground truth labels were never used in the learning process by any means .",
    "f1 score is used as an external evaluation measure for these datasets where ground truth labels are available .",
    "f1 score is a grouping external evaluation measure that weights the recall by a parameter @xmath76 @xcite .",
    "precision is @xmath77 and recall is @xmath78 .",
    "f1 score is calculated by : @xmath79 here we use @xmath80 .",
    "therefore , f1 score used here is the harmonic mean of precision and recall : @xmath81 best value of f1 score is @xmath39 or @xmath82 and worst value is @xmath13 . in this range ,",
    "the larger the value of f1 score , the better the corresponding grouping result .    results are divided into spectral graph - theoretic results , which are based on similarity graph models , and weakly supervised classification results .",
    "the former represent the content of the analysis of similarity graph models section which provides an analysis of similarity graph models , two of which are proposed in this study .",
    "the second part of the results compares between weakly supervised classifiers and the corresponding fully supervised classifier ( the weakly supervised classifier vs.  fully supervised classifier section ) .",
    "regarding the emg datasets , the parts used of every emg dataset in the similarity graph models represent the unlabelled parts .",
    "the outcome of each spectral graph - theoretic grouping consists of two groups .",
    "the labelled part of every emg dataset is used to annotate the groups because the group with a smaller number of elements is assigned the same label ( normal ) as the labelled instances while the group with more elements is assigned the disordered label .",
    "this assumption is based on the structure of a disordered muscle , which typically contains more disordered mus than normal mus .",
    "the elements which get annotated by the spectral grouping represent weakly labelled data that can be later used as further annotated training data for classification .    for the other three datasets ,",
    "it is an ordinary spectral grouping problem .",
    "grouping is applied on all the data as there is no labelled part of the data .",
    "spectral grouping is evaluated on its own in this section and then its impact on classification performance is evaluated in the weakly supervised classifier vs.  fully supervised classifier section . as a part of the spectral graph - theoretic grouping evaluation , the proposed probabilistic threshold and probabilistic criterion similarity graph models ( see the methods section ) are evaluated based on grouping evaluation measures by comparing them to other similarity graph models existent in the literature .    in table",
    "[ tab : one ] , values of the evaluation measures are shown for the following datasets :    * abalone @xcite dataset : 4177 instances , 9 features in 10 groups . * swiss banknotes @xcite dataset : 200 instances , 6 features in 2 groups .",
    "* segmentation @xcite dataset : 2310 instances , 19 features in 7 groups . *",
    "emg myopathic upper leg dataset ( myo upper leg ) : 557 instances , 8 features in 2 groups .",
    "* emg neurogenic upper leg dataset ( neuro upper leg ) : 356 instances , 8 features in 2 groups .",
    "* emg myopathic lower leg dataset ( myo lower leg ) : 583 instances , 8 features in 2 groups . * emg neurogenic lower leg dataset ( neuro lower leg ) : 444 instances , 8 features in 2 groups .",
    "the similarity graph models experimented are :    * probabilistic threshold : optimal values of @xmath9 and @xmath8 are obtained by cross - validation ( see the methods section for a detailed illustration of the probabilistic threshold graph ) .",
    "there are two versions of the probabilistic threshold graph as per how to transform the similarity matrix into a symmetric matrix : * * prob .",
    "threshold min .",
    ": assign the minimum value out of @xmath83 to both of them * * prob .",
    "threshold max : assign the maximum value out of @xmath83 to both of them * probabilistic criterion : optimal values of @xmath9 and @xmath8 are obtained by cross - validation ( see the methods section for a detailed illustration of the probabilistic criterion graph ) .",
    "there are two versions of the probabilistic criterion graph as per how to transform the similarity matrix into a symmetric matrix : * * prob .",
    "acceptance min .",
    ": assign the minimum value out of @xmath83 to both of them * * prob .",
    "acceptance max : assign the maximum value out of @xmath83 to both of them * @xmath3-neighbourhood : optimal value of @xmath3 is obtained by cross - validation . *",
    "@xmath4-nearest neighbour : optimal value of @xmath4 is obtained by cross - validation .",
    "* mutual @xmath4-nearest neighbour : optimal value of @xmath4 is obtained by cross - validation .",
    "* fully connected graph : optimal value of @xmath8 is obtained by cross - validation .    the first 3 datasets are publicly available datasets that have been used in grouping before .",
    "the latter 4 datasets represent real emg datasets .",
    "these datasets were acquired from upper leg and lower leg recordings and each of them contains two types of instances ; normal as well as disordered ( myopathic or neurogenic ) .",
    "in fact there are 2 rather than 4 emg datasets as the myopathic and neurogenic upper leg datasets represent bags of the same dataset ( the same goes for the lower leg dataset ) but they are shown as two different datasets here because they are treated separately as far as spectral graph - theoretic grouping and its evaluation are concerned . in the weakly supervised classifier vs.  fully supervised classifier section , where weakly supervised classification is applied , weakly annotated data of both upper leg datasets are being processed together along with the strongly labelled instances of the upper leg ( resp .",
    "lower leg ) dataset .",
    "all emg data were collected under irb approval and were de - identified .    as mentioned earlier , for the f1 score , the greater the value the more accurate the similarity graph model , while for davies - bouldin index , the smaller the value the better ( higher similarity within a group and lower similarity between groups ) the similarity graph model . as can be seen in table  [ tab : one ] and in figure  [ fig : seven ] and figure  [ fig : eight ] , with the proposed probabilistic similarity graph models , grouping results are better , or at least as good as , the other similarity graph models . figure  [ fig : eight ] shows the improvement achieved by using any of the four proposed similarity graph models ( minimum and maximum graphs of probabilistic threshold , and minimum and maximum graphs of probabilistic criterion ) over the other similarity graph models in comparison as the davies - bouldin index values are clearly better with the former .",
    "the same conclusion is shown in case of the segmentation dataset ( by far the largest out of the 3 datasets ) in figure  [ fig : seven ] .",
    "for the abalone dataset in figure  [ fig : seven ] , the probabilistic threshold maximum graph leads to the best result as its f1 score value is slightly better than the one achieved by constructing the probabilistic criterion maximum graph as well as the k - nearest neighbour graph .",
    "all graphs are nearly equally good for the banknotes dataset displayed in figure  [ fig : seven ] .",
    "one other advantage of the proposed similarity graph models lies in the fact they do not depend on distance among the instances , location of the instances nor on a number of neighbours specified a priori that can perform well at some part of the dataset but not on another part of the same dataset due to , for example , having a dataset containing different densities within it .",
    "results show that both the probabilistic threshold and the probabilistic criterion graphs lead to very similar grouping results among themselves as shown by the values of the validity indices .",
    "the former leads to better results in case of the abalone dataset and the minimum graph of the segmentation dataset , while the latter leads to slightly better results in case of the maximum graph of the segmentation dataset . for the rest of the datasets , results are quite similar among the two proposed similarity graph models .    on the other hand , for some datasets",
    "minimum similarity graph models lead to better results than maximum similarity graph models and vice versa on the other datasets .",
    "this suggests that the choice between the minimum and maximum similarity graph models for the same model ( probabilistic threshold or probabilistic criterion ) should depend on the value of the group validity index resulting from cross - validation .",
    "now that the annotated training data is larger due to the addition of weakly labelled instances that got annotated by spectral grouping , we want to evaluate the significance of the weak labelling procedure with respect to classification performance on the emg datasets . results of the classification are evaluated before and after weak labelling , i.e.  with a fully supervised classifier and with a weak classifier that uses weakly labelled data resulting from spectral grouping along with strongly labelled data .",
    "results are presented using three different classifiers .",
    "the classifications algorithms used are logistic regression , @xmath4-nearest neighbour and quadratic discriminant analysis ( qda ) .",
    "[ [ logistic - regression ] ] logistic regression + + + + + + + + + + + + + + + + + + +    assuming @xmath84 refers to a @xmath50-dimensional feature vector of an instance @xmath85 , the utilised logistic regression classifier assumes the following : @xmath86 } + \\beta_{i[1:p ] } f } , i < 3.\\ ] ]    [ [ k - nearest - neighbour ] ] @xmath4-nearest neighbour + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    @xmath4-nearest neighbour classifiers usually perform well in cases where the decision boundary is complex and there is enough data to train . here",
    ", the value of @xmath87 is obtained by cross - validation .    [",
    "[ quadratic - discriminant - analysis - qda ] ] quadratic discriminant analysis ( qda ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    qda assumes a gaussian distribution of each label and assigns to an instance the label with a greater posterior probability .",
    "parameters of the gaussian distribution of each class are estimated from the training data by a maximum likelihood estimate ( mle ) .",
    "a posterior probability refers to @xmath88 , and the assigned label is obtained by @xmath89 .",
    "+ in the fully supervised classifier , unlabelled instances are dealt with as follows .",
    "an instance belonging to a myopathic or a neurogenic bag is assumed to be myopathic or neurogenic respectively .",
    "this assumption is not accurate because myopathic and neurogenic bags contain normal instances .",
    "therefore , it is easy to see that performance of the fully supervised classifier would severely suffer due to this assumption , which is demonstrated by the results . on the other hand ,",
    "the weak classifier exploits weak labels assigned to each of the previously unlabelled instances by the spectral grouping procedure .",
    "the upper leg dataset has @xmath90 labelled instances and @xmath91 unlabelled instances .",
    "the fully supervised classifier assigns the respective bag label to each unlabelled instance . on the other hand , the weakly supervised classifier processes @xmath91 weakly labelled instances as well as @xmath90 strongly labelled instances",
    "the lower leg dataset has @xmath92 labelled instances and @xmath93 unlabelled instances .",
    "therefore , the weakly supervised classifier processes @xmath93 weakly labelled instances as well as @xmath92 strongly labelled instances .    leave - one - out cross - validation is implemented by setting instances belonging to a single muscle as test data while training on instances of the rest of the muscles , then repeating this process for every muscle",
    ". it can be more precisely referred to as leave - one - muscle - out cross - validation as far as this study is concerned .",
    "overall muscle classification accuracy is the main metric used to evaluate the classification performance .",
    "table  [ tab : two ] shows the results of the fully supervised and weakly supervised classifiers .",
    "every weak classifier is named after the spectral graph - theoretic grouping strategy pursued but three different classification algorithms are utilised with each .",
    "results show that , using a weak classifier , muscle classification accuracy significantly improves compared to the fully supervised classifier .",
    "logistic regression performs slightly better compared to both k - nearest neighbours and qda , but the difference is not huge .",
    "this shows that if the training instance annotation process is properly performed , the classification results are stable and not algorithm - dependent .",
    "a weakly supervised learning paradigm is introduced .",
    "the goal is to improve classification performance by first weakly annotating unlabelled samples of a training dataset using a spectral graph - theoretic grouping strategy , then using the weakly annotated data along with the strongly labelled data to construct a larger annotated training set to be used in classification .",
    "spectral graph - theoretic grouping exploits similarity among data instances as well as the relationship between unlabelled and strongly labelled data instances , by constructing similarity graph models to weakly annotate unlabelled data instances .",
    "two new similarity graph models , which provide greater robustness in handling different data densities within a dataset , are introduced .",
    "afterwards , a classifier learns from the weakly as well as strongly labelled data .",
    "results show that performance of the resulting weakly supervised classifier as a whole is better than its counterpart fully supervised classifier on the emg datasets .",
    "also , results of experiments performed on benchmark and emg datasets show that the spectral graph - theoretic grouping strategy based on the introduced similarity graph models leads to grouping results better than or on a par with similarity graph models in the literature .",
    "the proposed spectral graph - theoretic grouping strategy for weakly supervised learning provided improved performance compared to its fully supervised learning counterpart primarily due to the fact that such an approach can obtain a reliable set of weakly labelled data that , when augmented with strongly labeled data to form a larger annotated training dataset , can be used to train a classifier with stronger classification performance than can be achieved using only strongly labelled data , or strongly labelled data and unlabelled data .",
    "furthermore , the proposed similarity graph models led to improved results due primarily to the flexibility of the models that can adapt to the underlying data when compared to existing graph models which require more strict and require more rigid parameter optimization .",
    "30 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    galleguillos , c. , babenko , b. , rabinovich , a. @xmath11 belongie , s. weakly supervised object recognition and localization with stable segmentations .",
    "european conf . comp . vision ( eccv ) _ * 1 , * 193207 ( 2008 ) .    arora , h. , loeff , n. , forsyth , d. @xmath11 ahuja , n. unsupervised segmentation of objects using efficient learning . _ comp .",
    "vision and patt .",
    "( cvpr ) _ * 1 , * 17 ( 2007 ) .",
    "chum , o. @xmath11 zisserman , a. an exemplar model for learning object classes . _ comp .",
    "vision and patt .",
    "( cvpr ) _ * 2 , * 18 ( 2007 ) .",
    "lee , y. @xmath11 grauman , k. learning the easy things first : self - paced visual category discovery . _ comp . vision and patt .",
    "( cvpr ) _ * 1 , * 17211728 ( 2011 ) .    winn , j. @xmath11 jojic , n. locus : learning object classes with unsupervised segmentation .",
    "comp . vision ( iccv ) _ * 1 , * 756763 ( 2005 ) .    crandall , d. j. @xmath11 huttenlocher , d. weakly supervised learning of part - based spatial models for visual object recognition . _",
    "european conf . comp .",
    "vision ( eccv ) _ * 1 , * 1629 ( 2006 )",
    ".    bergamo , a. @xmath11 torresani , l. exploiting weakly - labeled web images to improve object classification : a domain adaptation approach .",
    "neural inf . process . syst .",
    "( nips ) _ * 22 , * 181189 ( 2010 ) .",
    "prest , a. , leistner , c. , civera , j. , schmid , c. @xmath11 ferrari , v. learning object class detectors from weakly annotated video . _ comp . vision and patt .",
    "( cvpr ) _ * 2 , * 32823289 ( 2012 ) .",
    "ali , k. , hasler , d. @xmath11 fleuret , f. flowboost - appearance learning from sparsly labeled video . _ comp . vision and patt .",
    "( cvpr ) _ * 1 , * 14331440 ( 2011 ) .",
    "leistner , c. , godec , m. , schulter , s. , saffari , a. @xmath11 bischof , h. improving classifiers with weakly - related videos . _ comp .",
    "vision and patt .",
    "( cvpr ) _ * 1 , * 27532760 ( 2011 ) .    andrews , s. , tsochantaridis , i. @xmath11 hofmann , t. support vector machines for multiple - instance learning .",
    "neural inf . process .",
    "( nips ) _ * 15 , * 561568 ( 2003 ) .",
    "blaschko , m. , vedaldi , a. @xmath11 zisserman , a. simultaneous object detection and ranking with weak supervision .",
    "neural inf . process .",
    "( nips ) _ * 22 , * 141148 ( 2010 ) .    vezhnevets , a. @xmath11 buhmann , j. towards weakly supervised semantic segmentation by means of multiple instance and multitask learning .",
    "_ comp . vision and patt .",
    "( cvpr ) _ * 1 , * 32493256 ( 2010 ) .    vijayanarasimhan , s. @xmath11 grauman , k. keywords to visual categories : multiple - instance learning for weakly supervised object categorization .",
    "_ comp . vision and patt .",
    "( cvpr ) _ * 1 , * 18 ( 2008 ) .    flury , b. @xmath11 riedwyl , h. _ angewandte multivariate statistik _ ( mit press , cambridge , 1983 ) .    ung , a. k. , xu , x. @xmath11 ooi , b. c. curler : finding and visualizing nonlinear correlation clusters .",
    "conf . on manag . of data ( acm sigmod ) _ * 8 , * 467478 ( 2005 ) .",
    "fowlkes , c. , belongie , s. , chung , f. @xmath11 malik , j. spectral grouping using the nystrm method .",
    "_ ieee trans . patt .",
    "* 26 , * 214225 ( 2004 ) .",
    "aksoy , s. @xmath11 haralick , r. m. graph-theoretic clustering for image grouping and retrieval .",
    "_ comp . vision and patt .",
    "( cvpr ) _ * 1 , * 8186 ( 1999 ) .",
    "wu , z. @xmath11 leahy , r. an optimal graph theoretic approach to data clustering : theory and its application to image segmentation .",
    "_ ieee trans .",
    "_ * 15 , * 11011113 ( 1993 )",
    ".    shi , j. @xmath11 malik , j. normalized cuts and image segmentation .",
    "_ ieee trans .",
    ". intell . _ * 22 , * 888905 ( 2000 ) .",
    "ng , a. , jordan , m. @xmath11 weiss , y. on spectral clustering : analysis and an algorithm .",
    "neural inf . process",
    "( nips ) _ * 14 , * 849856 ( 2002 )",
    ".    von luxburg , u. a tutorial on spectral clustering .",
    "_ stat . and computing _ * 17 , * 395416 ( 2007 ) .",
    "maier , m. , hein , m. @xmath11 von luxburg , u. cluster identification in nearest - neighbor graphs .",
    "theory ( alt ) _ * 1 , * 196210 ( 2007 ) .",
    "hein , m. @xmath11 von luxburg , u. similarity graphs in machine learning .",
    "mpi biological cybernetics _ * 1 , * 11 ( 2007 ) .",
    "ben - david , s. , von luxburg , u. @xmath11 pal , d. a sober look at clustering stability .",
    "theory ( colt ) _ * 19 , * 519 ( 2006 ) .    chung , f. _ spectral graph theory _ ( ams press , brooklyn , 1997 ) .",
    "davies , d. l. @xmath11 bouldin , d. w. a cluster separation measure .",
    "_ ieee trans .",
    "* 2 , * 224227 ( 1979 ) .",
    "petrovic , s. a comparison between the silhouette index and the davies - bouldin index in labelling ids clusters .",
    "_ nordic workshop on secure it systems _ * 11 , * 5364 ( 2006 ) .",
    "van rijsbergen , c. j. _ information retrieval _",
    "( butterworth , oxford , 1979 ) .",
    "snelson , e. , rasmussen , c. e. @xmath11 ghahramani , z. warped gaussian processes .",
    "neural inf . process .",
    "( nips ) _ * 15 , * 281288 ( 2003 ) .",
    "likas , a. , vlassis , n. @xmath11 verbeek , j. j. the global k - means clustering algorithm .",
    "recog . _ * 36 , * 451461 ( 2003 ) .",
    "this research was undertaken , in part , thanks to funding from the canada research chairs program .",
    "the study was also funded by the natural sciences and engineering research council ( nserc ) of canada and the ontario ministry of economic development and innovation .",
    "ta and aw were involved in designing the study and performing performance analysis .",
    "ta and aw were involved in the writing and editing .",
    "ds was involved in the editing .",
    "all authors reviewed the manuscript .",
    "competing financial interest : all authors in this study have no competing financial interests .      :",
    "a 26-instance 2-label dataset is used to test different similarity graph models .",
    "instances are coloured according to their labels , i.e.  blue instances are labelled blue whereas red instances are labelled red.[fig : two ] ]     as a result of applying an @xmath3-neighbourhood graph .",
    "this technique fails to identify the 2 groups of @xmath94 .",
    "* a. * @xmath95 leads to @xmath37 groups as it loosely or never connects instances belonging to the same correct group . *",
    "b. * @xmath96 leads to @xmath39 group as it overconnects instances belonging to the two different correct groups.[fig : three ] ]     as a result of applying a symmetric @xmath4-nearest neighbour graph .",
    "this technique fails to identify the 2 groups of @xmath94 .",
    "* a. * @xmath97 leads to @xmath98 groups as it hardly connects more than few instances in each correct group .",
    "* b. * @xmath99 leads to @xmath41 groups one of them ( the middle group ) containing instances belonging to both correct groups . *",
    "c. * @xmath43 leads to @xmath39 group as it overconnects instances belonging to the two different correct groups . *",
    "d. * @xmath46 leads to @xmath39 group as it overconnects instances belonging to the two different correct groups.[fig : four ] ]     as a result of applying a mutual @xmath4-nearest neighbour graph .",
    "this technique fails to identify the 2 groups of @xmath94 .",
    "* a. * @xmath46 leads to @xmath42 groups as it does not connect all instances of each correct group . *",
    "b. * @xmath100 leads to @xmath41 groups as it does not connect all instances of the right side correct group . *",
    "c. * @xmath101 leads to @xmath39 group as it overconnects instances belonging to the two different correct groups . *",
    "d. * @xmath102 leads to @xmath39 group as it overconnects instances belonging to the two different correct groups.[fig : five ] ]     as a result of applying a probabilistic threshold graph with @xmath103 .",
    "this similarity graph model manages to correctly identify the 2 groups of @xmath94 as it can connect instances belonging to different densities .",
    "it bases its decision whether or not to connect instances based on a weight value .",
    "the values that each weight value is compared to are normalised values representing the distance between a certain instance and another divided by summation of distances between the former and all instances of the dataset .",
    "this normalization leads to a similarity graph model ( both the probabilistic threshold and probabilistic criterion graphs ) that not only depends on absolute values of parameters but is also heavily impacted by relative weight values where a certain distance value from a certain instance to another is taken into consideration only with relative to another distance from the former instance to a third instance.[fig : six ] ]"
  ],
  "abstract_text": [
    "<S> in this study , a spectral graph - theoretic grouping strategy for weakly supervised classification is introduced , where a limited number of labelled samples and a larger set of unlabelled samples are used to construct a larger annotated training set composed of strongly labelled and weakly labelled samples . </S>",
    "<S> the inherent relationship between the set of strongly labelled samples and the set of unlabelled samples is established via spectral grouping , with the unlabelled samples subsequently weakly annotated based on the strongly labelled samples within the associated spectral groups . </S>",
    "<S> a number of similarity graph models for spectral grouping , including two new similarity graph models introduced in this study , are explored to investigate their performance in the context of weakly supervised classification in handling different types of data . </S>",
    "<S> experimental results using benchmark datasets as well as real emg datasets demonstrate that the proposed approach to weakly supervised classification can provide noticeable improvements in classification performance , and that the proposed similarity graph models can lead to ultimate learning results that are either better than or on a par with existing similarity graph models in the context of spectral grouping for weakly supervised classification . </S>"
  ]
}