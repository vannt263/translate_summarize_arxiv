{
  "article_text": [
    "in the recent decades , there have been many quantum algorithms proposed for the problems inefficient to solve on classical computers .",
    "the quantum phase estimation algorithm @xcite is a leading illustration of such algorithms .",
    "it is used for quantum simulations to estimate the eigenenergy corresponding to a given approximate eigenvector of the unitary evolution operator of a quantum hamiltonian .",
    "furthermore , with different settings , it has been adapted as a sub frame of many quantum algorithms applied to wide variety of applications in different fields ( see the review article ref.@xcite and the references therein ) . however , the success of the algorithm partly depends on the pre - existing approximate eigenvector .",
    "thus , it may fail to output the right eigenvalue in the cases where the approximation to the corresponding eigenvector is not good enough .",
    "this hinders the uses of the algorithm when there is not enough prior knowledge to procure a good approximation to the eigenvector .",
    "a symmetric matrix is called non - negative if all of its elements are greater than or equal to zero .",
    "it is irreducible if the corresponding graph is strongly connected or if it is not reducible to a block - diagonal form by any column - row permutation . due to perron - frobenius theorem @xcite ,",
    "an irreducible non - negative matrix have a positive eigenvector ( all elements are positive ) with an associated positive principal eigenvalue whose magnitude is greater than the rest of the eigenvalues .",
    "these matrices have been studied extensively @xcite , the distribution of the coefficients of their principal eigenvectors have been related to the matrix elements @xcite , and the sum of the coefficients have been shown to be related to the number of walks in molecular graphs @xcite .",
    "the phase estimation algorithm ( pea ) mainly uses two quantum registers:@xmath0 initially set to zero state and @xmath1 holding an initial approximate eigenvector . in this article",
    ", we show that for irreducible non - negative matrices , one can obtain the principal eigenvalue by using an equal superposition input state in pea : i.e. , instead of an approximate eigenvector , initial value of @xmath2 is set to @xmath3 and then put into equal superposition state by applying hadamard gates . in the output of the algorithm , this generates each one of the eigenvalues with the probability determined by the normalized sum of the coefficients of the associated eigenvector . in addition , because all eigenvectors but the principal one include positive - negative elements , we show that in most of the random cases the probability to see principal eigenvalue in the output is much larger than the others . in also some cases ,",
    "we show that first applying hadamard gates to the second register in the output ( or measuring second register in the hadamard basis ) and then measuring the first register when the measurement outcome of the second register is in @xmath4 state increase the success probability further in the output .",
    "eigenvector - coefficients of stochastic matrices sum to zero for all but the principal eigenvector .",
    "therefore , for these matrices one can generate the eigenvalue in the phase estimation algorithm with probability one @xcite . since any given symmetric irreducible non - negative matrix can be converted into a stochastic matrix by a diagonal scaling matrix ; using the closeness of the matrix to a stochastic matrix , we also show that the success probability of the algorithm can be predicted beforehand .",
    "we finally compare the estimated success probabilities with the computed ones for random symmetric matrices and 3-local hamiltonians of different dimensions with non - negative off - diagonal elements .    in the following sections",
    ", we shall first discuss pea in detailed steps , then draw the estimates for the success probability and finally show the possible applications of the algorithm .",
    "the phase estimation algorithm ( pea ) in general sense finds the value of @xmath5 for a given approximate eigenvector @xmath6 in the eigenvalue equation @xmath7 .",
    "the algorithm mainly uses two quantum registers : viz .",
    "@xmath0 initially set to zero state and @xmath1 holding an approximate eigenstate of a unitary matrix @xmath8 .",
    "the first operation in the algorithm puts @xmath9 into the equal superposition state . in this setting , a sequence of operators , @xmath10 , controlled by the @xmath11th qubit of @xmath0 are applied to @xmath1 . here",
    ", @xmath12 and @xmath13 is the number of qubits in @xmath9 and also determines the precision of the output .",
    "these sequential operations generate the quantum fourier transform @xmath14 of the phase on @xmath0 .",
    "therefore , the application of the inverse quantum fourier transform @xmath15 turns the value of @xmath0 into the binary value of the phase .",
    "consequently , one measures @xmath9 to obtain the phase . here ,",
    "if the unitary operator @xmath8 is the time evolution operator of a quantum hamiltonian @xmath16 , i.e. @xmath17 ; then one also obtains the eigenenergy of that hamiltonian .    for a symmetric irreducible non - negative matrix @xmath16 of order @xmath18 with ordered eigenvalues",
    "as @xmath19 and associated eigenvectors @xmath20 , it is known that @xmath21 is the only eigenvector with all positive coefficients .",
    "assume unitary operator @xmath17 and its powers @xmath10 with @xmath22 are readily available for pea . to estimate the value of @xmath23 on the first register , in our setting",
    ", we simply modify the conventional phase estimation algorithm in the following way :    * instead of an approximate eigenvector , initial value of @xmath2 is set to @xmath3 and then put into equal superposition state by applying hadamard gates . *",
    "( * optional * ) in the output , we change the basis of the second register by applying the hadamard gates .",
    "then , if the measurement of the second register is equal to @xmath3 state , then the principal eigenvalue is estimated on the first register .",
    "this modification increases the success probability of the algorithm further when the probability of measuring the principal eigenvalue is higher then the other eigenvalues .    in the following subsection",
    ", we shall describe the phase estimation algorithm in steps , also shown in fig.[fig : peageneral ] , to show how the above modifications have an impact on the algorithm .      here",
    ", the algorithm is described in details by showing how the quantum state changes in each step :    * the system is prepared in a way so that it includes two register : viz . , @xmath24 and @xmath2 with @xmath13 and @xmath25 number of qubits , respectively . *",
    "both registers are initialized into zero state : @xmath26=@xmath24@xmath2=@xmath3@xmath3 .",
    "* then , the hadamard operators are applied to both registers : @xmath27 * as in the customary phase estimation algorithm ; the unitary evolution operators @xmath10 controlled by the @xmath11th qubit of @xmath24 are applied to @xmath2 , and finally the inverse of the quantum fourier transform ( @xmath28 ) is applied to @xmath9 .",
    "+ at this point of the algorithm , we have a quantum state in which @xmath24 and @xmath2 hold the superposition of the eigenvalues and the associated eigenvectors , respectively : @xmath29 here , the amplitude @xmath30 is related to the angle between the equal superposition state and the eigenvector @xmath31 and can be described as the normalized sum of the coefficients of the eigenvectors since @xmath2 was @xmath32 at the beginning : @xmath33 if @xmath2 is written in the hadamard basis , @xmath34 , where @xmath35 ; then the following quantum state is obtained : @xmath36 where @xmath37s are new coefficients .",
    "now , since there is only one eigenvector , viz .",
    "@xmath38 , with all positive real elements , in the hadamard basis , we can expect this positive eigenvector to be the closest state to @xmath39 . * to revert the above state back to the standard basis , the hadamard operator @xmath40 is again applied to @xmath2 : @xmath41 * @xmath2 is measured in the standard basis . as a result , for @xmath2= @xmath42 , the system collapses to the state where the phase associated with the positive eigenvector is expected to be highly dominant in the first register .",
    "* in the final step , the first register is measured to obtain the phase @xmath23 and get the eigenvalue of @xmath16 .",
    "these steps also drawn in fig.[fig : peageneral ] .",
    "+      it is easy to see that the success probability of the algorithm without the optional hadamard gates on @xmath2 in the output is determined from the normalized sum of the coefficients of the eigenvectors given in eq.([eq : normalizedsum ] ) . therefore , the success probability for the dominant eigenvalue is @xmath43 ( note that the coefficients of @xmath38 are all positive , hence , @xmath44 . ) .    with the hadamard gates in the output , we first find the probability to measure @xmath2 in @xmath3 state and then find the probability to see the dominant eigenvalue in @xmath24 knowing @xmath45 : as obvious in eq.([eqfinal ] ) , the success probability of getting @xmath3 is @xmath46 .",
    "in addition , after measuring @xmath3 on @xmath2 , the probability to measure @xmath47 on @xmath24 is @xmath48 .",
    "since we apply an equal superposition as an input , the component of an eigenvector in this direction determines the probability to get the corresponding eigenvalue on @xmath24 .",
    "more formally , after the application of @xmath49 , we get @xmath50 , where @xmath51 . if @xmath2 of the state in eq.([eqh ] ) is measured in the hadamard basis , the probability to see @xmath52 is the sum of the amount of the components of the eigenvectors in the direction of the initial vector : @xmath53 this is also equal to @xmath54 , i.e. the probability of measuring @xmath2 in @xmath55 state at the end .",
    "@xmath56 takes the smallest value only when all @xmath57s are equal to @xmath58 .",
    "moreover , when @xmath2=@xmath4 , the probability to see @xmath47 on @xmath24 is : @xmath59    although we have drawn the equalities for probabilities , without knowing the eigenvectors , it is not possible to compute exact @xmath57s and so @xmath60 and @xmath56 .",
    "therefore , we shall try to estimate them : since @xmath61 is the normalized sum of the vector elements , it is easy to see that @xmath62 , where the equality occurs only when the magnitude of the eigenvectors are the columns of the identity matrix .",
    "a similar observation is also made in ref.@xcite where the principal eigenvector is found for a given eigenvalue equal to 1 .    here",
    ", we will relate the matrix @xmath16 to a stochastic matrix in order to develop some intuition for the estimation of the probabilities .",
    "it is known that stochastic matrices have only one eigenvector with coefficients summing to a nonzero value .",
    "a matrix can be made stochastic by a column wise scaling @xmath63 , where @xmath64 is a diagonal matrix whose diagonal elements are the inverses of the sums of the columns of @xmath16 .",
    "the closeness of the matrix @xmath16 to @xmath63 may provide a prediction to estimate the success behavior of the phase estimation algorithm with an initial superposition state .",
    "the closeness of @xmath16 to @xmath63 can be defined by @xmath65 where @xmath66 is any matrix norm .",
    "this also defines a perturbation error .",
    "if we normalize this error term by @xmath67 , we get the relative error : @xmath68 where @xmath69 is an identity matrix .",
    "we can also look at the relative error in the inverses : @xmath70    since the matrix @xmath64 also changes the eigenvector elements ; when the variance of the diagonal elements of @xmath64 is small , we can expect the eigenvectors of @xmath16 to show the similar behaviors as of @xmath63 and have one eigenvector whose elements sum to a much greater value than the others .",
    "when @xmath71 or @xmath72 , left and right principal eigenvectors of symmetric @xmath16 are the same and have the elements equal to @xmath73 in the normalized space . therefore , the variances @xmath74 and @xmath75 of the diagonal elements of @xmath76 and @xmath64 , the column sums of @xmath16 and their inverses , can somehow describe how much the elements of the principal eigenvector deviate from @xmath73 which is also the expectation value for randomly generated normalized uniform @xmath77 numbers . using these intuitions",
    ", we define the estimate @xmath56 as : @xmath78 with @xmath79    in addition , @xmath80 for stochastic matrices and so for @xmath16 when @xmath71 .",
    "since the any deviation of the elements of the principal eigenvector from @xmath73 would change the equality @xmath80 , we multiply variance by @xmath77 to get total deviation from 1 .",
    "therefore , we define the estimate @xmath43 as : @xmath81    note that @xmath82 and @xmath83 are only predictions and may fail to provide good estimates for @xmath84 and @xmath85 , respectively , for particularly some structured matrices and matrices whose eigenvectors are close to the columns of an identity matrix . for instance , consider the following matrix : @xmath86 the eigenvalues of this matrix are @xmath87 and @xmath88 , respectively , associated with the following eigenvectors : @xmath89 as computed from the above , the magnitudes of the sums of the components of the eigenvectors are very close to each other : @xmath90 and @xmath91 , respectively . in addition , the second eigenvector , not associated to the principal eigenvalue , has the largest sum .",
    "also note that one can also predict @xmath84 from the distribution of the matrix elements .",
    "for instance @xcite , @xmath92 where @xmath93s are the nonzero matrix elements of @xmath16 . however , these bounds are generally not tight enough to give good estimates in many cases .    in sec.[sec : numericsimulations ] , we shall show the comparisons of these estimates with the computed probabilities for random matrices .",
    "in the previous sections , we have showed the positive eigenpair of an irreducible symmetric non - negative matrix can be obtained by using an initial equal superposition state when the estimated probabilities high .",
    "this eliminates the necessity of knowing an initial approximate eigenvector in the applications of the phase estimation algorithm to the problems involving non - negative matrices .",
    "irreducible non - negative matrices are known to have positive eigenpairs due to perron - frobenious theorem .",
    "there is also more general but the same statements indicated in this theorem for compact operators @xcite .",
    "one may also apply permutation matrices or splitting techniques to convert the interested matrix to a non - negative one or to increase the expected probability outcome when the probability is low than an acceptable value .    since a wide variety of problems in science can be represented by non - negative matrices , the phase estimation algorithm can be applied without an initial approximate eigenvector to these problems . in the following subsections , we shall consider two important classes among these problems : viz .",
    ", the k - local hamiltonian problem and the stochastic processes .",
    "one of the applications of the non - negative matrices can be found in quantum mechanics where such matrices called stoquastic matrices @xcite : i.e. , matrices with non - negative off - diagonal elements .",
    "if a hamiltonian operator @xmath16 , a hermitian operator acting on @xmath94 , can be expressed as a sum of local hamiltonians representing the interactions between at most @xmath95 qubits , then it is called @xmath95-local @xmath25-qubit hamiltonian .",
    "more formally , @xmath96 . finding the lowest eigenvalue of @xmath16 defines an eigenvalue problem which is so called local hamiltonian problem .",
    "this eigenvalue problem can be also described as a decision problem to decide whether the ground state energy of @xmath97 is at most @xmath98 or at least @xmath99 for given constants @xmath98 and @xmath99 such that @xmath100 and @xmath101 .",
    "this problem has been shown to be @xmath102-complete for @xmath103 @xcite ( for k=2 , it is qma - complete only when both negative and positive signs in the hamiltonian are allowed @xcite ) .",
    "for the local hamiltonians with positive off - diagonal matrix elements in the standard basis , the matrix elements of the corresponding gibbs density matrix , @xmath104 , are all positive for any @xmath105 . due to the theorem given above , the ground state energy of @xmath16 with positive off - diagonal elements can be associated with the eigenvector whose coefficients are all positive . because one can associate a probability distribution to the ground state , the nature of these hamiltonians are considered similar to stochastic processes .",
    "thus , the hamiltonians of these types are called _ stoquastic _ @xcite.(note that the hamiltonian is not a stochastic matrix whose rows and columns are sum to one and the principal eigenvalue is one associated with an eigenvector of all ones . )",
    "the phase estimation algorithm can find the principal eigenpair of a stochastic matrix with probability one @xcite since the sum of the coefficients is one for the principal eigenvector and zero for the rest of the eigenvectors .",
    "however , the stochastic processes are generally defined by non - hermitian matrices , which are also widely seen in quantum physics : the hamiltonian of a closed system in equilibrium must be hermitian to have real energy values .",
    "however , nonequilibrium processes and open systems connected to reservoirs can be only described by non - hermitian models @xcite .",
    "any non - hermitian matrix @xmath16 can be decomposed into a hermitian and a skew - hermitian parts as : @xmath106 where @xmath107 describes the conjugate transpose of @xmath16 . here ,",
    "@xmath108 and @xmath109 define the nearest hermitian and skew hermitian matrices to @xmath16 , respectively @xcite.therefore , the matrix @xmath97 can be used as an approximation to @xmath16 in the simulation .",
    "a matrix @xmath16 is called normal if @xmath110 .",
    "when @xmath16 is a normal , it is easy to see that @xmath97 and @xmath111 also commute : @xmath112=hs - hs=0 $ ] .",
    "moreover , because of the symmetry , all the eigenvalues of @xmath97 are real ; and because of the skew - symmetry , all the eigenvalues of @xmath111 involve only imaginary parts .",
    "since @xmath113 , the eigenvectors of @xmath97 and @xmath111 are the same .",
    "in addition , the imaginary part of the eigenvalues of @xmath16 are equal to the eigenvalues of @xmath111 and the real parts are to the eigenvalues of @xmath97 .",
    "hence , using @xmath114 and @xmath115 in the simulation , one can simulate the non - hermitian operator @xmath16 on quantum computers by using separate two registers to obtain the imaginary and the real parts of the eigenvalue",
    ". however , in the stochastic cases , if @xmath16 is normal , it must be also doubly stochastic : i.e. , its left and right principal eigenvectors are already known to be a vector of all ones with the eigenvalue one .",
    "therefore , instead of an approximate normal matrix , we can use the closest hermitian matrix defined above as @xmath108 .    in the non - stochastic cases , one can also try to instead find the closest normal matrix by following the jacobi algorithm which attempts to diagonalize the matrix by using rotation matrices ( givens rotations ) .",
    "and converges to a matrix so called @xmath116 .",
    "the best closest matrix in the frobenius norm is then defined by the sum of the diagonal elements of @xmath116 and the rotation matrices used in the algorithm @xcite .",
    "one can also procure a quantum circuit to implement this closest matrix by mapping rotation matrices to quantum gates as done in @xcite .",
    "however , since this algorithm is based on the eigenvalue decomposition , the eigenvalues ( the diagonal elements of @xmath116 , and the eigenvectors ( the combination of the rotation matrices ) are already generated for the found normal matrix .",
    "in this section , we compare the estimated @xmath43 , @xmath60 , @xmath56 : i.e. @xmath82 , @xmath117 , @xmath83 , respectively , with the computed probabilities @xmath43 , @xmath60 , @xmath56 for the randomly generated matrices described below .      in matlab",
    ", we generate a random symmetric matrix with non - negative off - diagonal elements as follows : first a random diagonal matrix and a strictly upper triangular sparse non - negative matrix are generated .",
    "then , these matrices are combined to have a symmetric matrix with non - negative off - diagonal elements :    .... % matlab code to generate a random symmetric matrix % with nonnegative off - diagonal elements r = triu(sprand(n , n,0.5),1 ) ; % n is the dimension ,   % and 0.5 is the sparsity of the matrix .",
    "l = randn(n,1 ) ; h = r + r ' + diag(l ) ; ....    fig.[fig : randommatrices ] shows the comparisons of the estimated and computed probabilities for matrices of different dimensions generated by following the above code . as shown in the figure , the estimated and computed probabilities are almost the same and very high ( close to 1 ) .",
    "furthermore , the success probability , @xmath60 , with the hadamard gates applied to @xmath2 in the end is almost one and higher than the probability , @xmath43 , without the hadamard gates .      as our first example",
    ", the following hamiltonian is employed : @xmath118 with @xmath119 and @xmath120 . here ,",
    "@xmath121 and @xmath122 are the pauli spin operators . choosing @xmath123 and @xmath124",
    "randomly , 50 numbers of random matrices are generated for each 9 , 10 , 11 , 12 , and 13 qubit systems and show the estimated and computed probabilities in fig.[fig : randomlocalh1 ] .    in the second example ,",
    "1-body and 2-body interaction terms are also included separately in the hamiltonian : @xmath125 with @xmath126 and @xmath127 . as done for eq.([eq : localh1 ] , choosing the coefficients randomly , hamiltonians of different sizes are generated .",
    "the results are shown in fig.[fig : randomlocalh2 ] . as seen in the figure ,",
    "the estimation is not as good as the cases in fig.[fig : randommatrices ] and fig.[fig : randomlocalh1 ] .",
    "this is because the ratio defined in eq.([eq : ratioofelements ] ) is generally higher for the matrices generated from eq.([eq : localh2 ] ) in comparison to the ones from eq.([eq : localh1 ] ) .",
    "in this paper , we have showed the positive eigenpair of an irreducible symmetric non - negative matrix can be obtained when the estimated probabilities high .",
    "this eliminates the necessity of knowing an initial approximate eigenvector in the applications of the phase estimation algorithm to a wide variety of problems involving non - negative matrices . here",
    ", we also have discussed how to apply it to local hamiltonian problems and stochastic processes and showed the comparisons of the estimated success probabilities with the computed ones for random symmetric matrices and 3-local hamiltonians .",
    "20ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ]  + 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty link:\\doibase 10.1103/physrevlett.83.5162 [ * * ,   ( ) ] @noop * * ( ) @noop * * ,   ( ) @noop _ _ ,  vol .",
    "( ,  ) @noop _ _ ,  vol .",
    "( ,  ) @noop * * ( ) @noop * * ,   ( ) link:\\doibase 10.1021/ci000149u [ * * ,   ( ) ] link:\\doibase 10.1007/s11128 - 014 - 0818 - 7 [ * * ,   ( ) ] @noop * * ,   ( ) @noop _ _ , vol .",
    "( , )  chap .",
    "http://dl.acm.org/citation.cfm?id=2011772.2011773 [ * * ,   ( ) ] link:\\doibase 10.1137/08072689x [ * * ,   ( ) ] ,   @noop * * ,   ( ) link:\\doibase 10.1103/physreva.81.032331 [ * * ,   ( ) ] @noop * * ,",
    "( ) http://www.jstor.org/stable/2690338 [ * * ,   ( ) ] in  @noop _ _  ( ,  )  pp .",
    "@noop * * ,   ( ) @noop * * , ( )"
  ],
  "abstract_text": [
    "<S> quantum phase estimation algorithm has been successfully adapted as a sub frame of many other algorithms applied to a wide variety of applications in different fields . </S>",
    "<S> however , the requirement of a good approximate eigenvector given as an input to the algorithm hinders the application of the algorithm to the problems where we do not have any prior knowledge about the eigenvector .    in this paper , we show that the principal eigenvalue of an irreducible non - negative operator can be determined by using an equal superposition initial state in the phase estimation algorithm . </S>",
    "<S> this removes the necessity of the existence of an initial good approximate eigenvector . </S>",
    "<S> moreover , we show that the success probability of the algorithm is related to the closeness of the operator to a stochastic matrix . </S>",
    "<S> therefore , we draw an estimate for the success probability by using the variance of the column sums of the operator . </S>",
    "<S> this provides a priori information which can be used to know the success probability of the algorithm beforehand for the non - negative matrices and apply the algorithm only in cases when the estimated probability reasonably high . </S>",
    "<S> finally , we discuss the possible applications and show the results for random symmetric matrices and 3-local hamiltonians with non - negative off - diagonal elements . </S>"
  ]
}