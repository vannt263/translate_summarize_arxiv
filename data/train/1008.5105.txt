{
  "article_text": [
    "at an intuitive level , at least for a limited class of indexing schemes the geometric and probabilistic origin of the curse of dimensionality is quite transparent .",
    "let @xmath1 denote a similarity workload , where @xmath2 is a metric on a domain @xmath3 and @xmath4 is a finite subset of @xmath3 ( dataset ) .",
    "let us say we are interested in indexing into @xmath5 for deterministic , exact range queries . a traditional `` distance - based '' indexing scheme , stripped down to the bone , consists of a family of real - valued functions @xmath6",
    ", @xmath7 on @xmath3 , either fully or partially defined , which satisfy the @xmath0-lipschitz property : @xmath8 ( for example , a pivot - based indexing scheme will be using distance functions @xmath9 to the pivots @xmath10 . ) given a range query @xmath11 , where @xmath12 and @xmath13 , the algorithm chooses recursively a sequence of indices @xmath14 , where each @xmath15 is determined by the values @xmath16 , @xmath17 .",
    "the functions @xmath6 serve to discard those datapoints which can not possibly answer the query .",
    "namely , if @xmath18 , then , by the @xmath0-lipschitz property of @xmath6 , one has @xmath19 , and so the point @xmath20 is irrelevant and need not be considered ( figure [ fig : discarding ] ) .",
    "[ 0.25 ] can be discarded .",
    "[ fig : discarding],title=\"fig : \" ]    after the calculation terminates , the algorithm returns all points which can not be discarded , and checks each one of them against the condition @xmath21 .    next come two standard observations about high - dimensional data . the first one , known as the `` empty space paradox , '' asserts that the average distance @xmath22 to the nearest neighbour approaches the average distance @xmath23 between two datapoints as the dimension @xmath24 goes to infinity , provided the number of datapoints , @xmath25 , grows subexponentially in @xmath24 .",
    "figure [ fig : nn_dist ] , where we illustrate the point with a constant number of points ( @xmath26 and @xmath27 ) , and the distances are normalized so that the _ characteristic size _ of the gaussian space @xmath28 , @xmath29 is one .",
    "the second observation is that the histograms of values of common @xmath0-lipschitz functions on high - dimensional data are concentrated near their mean ( or median ) values .",
    "this effect is already pronounced in moderate dimensions such as @xmath30 in figure [ fig : hist_14gauss ] . here",
    "the function is a distance to a randomly chosen pivot @xmath31 , and assuming the query point @xmath32 is at a distance @xmath33 from @xmath31 , only the points outside of the region marked by vertical bars can be discarded .",
    "the two properties combined imply that as @xmath34 , fewer and fewer datapoints can be discarded for an average range query , and the performance of an indexing scheme degrades rapidly .",
    "this mechanism has been discussed repeatedly , e.g. @xcite , pp . 3537 , @xcite , @xcite , p. 487",
    ", to mention just a few sources .",
    "to make this idea yield rigorous lower performance bounds , one needs to guarantee first that _ every _ histogram of distances of @xmath0-lipschitz functions used to build an indexing scheme for a given domain @xmath3 is highly concentrated . in other words , if @xmath35 denotes a class of @xmath0-lipschitz functions from which we can choose the @xmath6 , then we want a low uniform upper bound on the variances of @xmath36 .",
    "results of this type are indeed well - known for a variety of geometric objects and are referred jointly as the _ phenomenon of concentration of measure _ @xcite .",
    "next problem is , how to link the concentration of functions @xmath37 with regard to the presumed _ underlying distribution _",
    "@xmath38 on the domain @xmath3 to concentration with regard to the _ empirical measure _",
    "@xmath39 supported on the dataset @xmath4 ( this was essentially a criticism of @xcite made in @xcite ) ?",
    "here one needs the machinery of _ statistical learning theory _ of vapnik and chervonenkis @xcite , which can guarantee such results provided the class @xmath35 has low combinatorial complexity ( e.g. , a finite vc dimension ) . this way , one obtains @xmath40 lower bounds for the pivot table expected average performance @xcite , as well as superpolynomial in @xmath24 lower bounds for metric trees @xcite .",
    "_ approximate nn queries _ @xcite",
    "seem to be in some sense free from the curse of dimensionality .",
    "in fact , the concentration of measure becomes a positive force here , and we will try to explain why , using the example of random projections in the hamming cube ( the approach of kushilevitz , ostrovsky and rabani @xcite ) , as well as the euclidean space ( the johnson ",
    "lindenstrauss lemma @xcite ) .",
    "getting back to exact search , the _ curse of dimensionality conjecture _",
    "@xcite calls for a general statement about lower bounds , which would apply across the entire range of all possible indexing schemes .",
    "the conjecture is still open even for the hamming cube @xmath41 , and we discuss it briefly .",
    "we conclude the article with a few remarks on the notion of intrinsic dimensionality of data , on a black - box search model of krauthgamer and lee @xcite , as well as on a spatial approximation algorithm based on delaunay graphs @xcite .",
    "informally , the phenomenon can be stated as follows :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ on a typical `` high - dimensional '' structure @xmath3 , every @xmath0-lipschitz function @xmath42 $ ] has small variation . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    usually , however , concentration is being dealt with using a different dispersion parameter .",
    "we proceed to precise definitions .",
    "let a metric space @xmath43 carry a probability measure @xmath38 .",
    "such an object is called a _",
    "metric space with measure_. one defines the _ concentration function _",
    "@xmath44 of @xmath3 by setting @xmath45 and , for @xmath13 , @xmath46    the value @xmath47 gives a uniform upper bound on the measure of the complement to the @xmath48-neighbourhood @xmath49 of every subset @xmath50 of measure @xmath51 , cf .",
    "[ fig : illconc1 ] .",
    "[ 0.3 ] .,title=\"fig : \" ]    on a typical high - dimensional geometric object @xmath3 the function @xmath52 drops off steeply near zero . for regular geometric objects such as hamming cubes , euclidean unit spheres and so on",
    ", one can usually derive gaussian upper bounds of the form @xmath53 where @xmath24 is the dimension parameter .",
    "for example , the hamming cube @xmath54 with the normalized hamming metric and uniform measure satisfies a chernoff bound @xmath55 ( obtained by combining harper s isoperimetric inequality , see e.g. @xcite , with the classical chernoff bound , cf .",
    "@xcite , 2.2.1 ) .",
    "see figure [ fig : hamming100 ] .",
    "it follows easily that for every real - valued @xmath0-lipschitz function @xmath37 on @xmath3 and for each @xmath56 one has @xmath57 where @xmath58 is the median value of @xmath37 , that is , a ( generally non - unique ) real number with the property that for a randomly drawn @xmath59 the probabilities of the events @xmath60 $ ] and @xmath61 $ ] are at least @xmath62 each .",
    "one can further derive uniform upper bounds in terms of @xmath44 on the variances of @xmath0-lipschitz functions on @xmath3 with values in a bounded interval .",
    "the concentration phenomenon admits the following illustration .",
    "draw @xmath63 points randomly from a high - dimensional geometric object such as the unit cube @xmath64 centred at the origin , choose a random orthogonal projection onto a two - dimensional subspace , and project both the cube and the chosen points on this subspace .",
    "the points will concentrate near the centre , the more so the higher the dimension @xmath24 is , as seen in figure [ fig : cpts300 ] for the values of dimension @xmath65 and @xmath66 .",
    "the red outline is the two - dimensional projection of the cube .",
    "another noteworthy concequence of concentration is that the shape of the random projection of the cube is getting ever more similar to a disk as @xmath34 .",
    "in fact , for @xmath67 the only visual difference one can spot between a random projection of a unit cube and that of a unit sphere , is the scale of the two projections : the diameter of a unit @xmath24-cube is @xmath68 .",
    "this illustrates an interesting feature of geometry of high dimensions : many high - dimensional objects look essentially the same to a low - dimensional observer .",
    "for instance , a certain precise version of this statement holds true for _ all _ convex bodies , as recently proved by klartag @xcite . for this reason , for an asymptotic study of performance of indexing schemes",
    "when @xmath34 the choice of a particular family of domains ( euclidean spheres , balls , cubes , hamming cubes ) does not matter that much .    among the books treating the concentration phenomenon , @xcite is the most reader - friendly , @xcite most comprehensive , and @xcite contains a wealth of ideas .",
    "see also a survey @xcite .",
    "let us agree on the following four assumptions on the similarity workload :      the metric domain @xmath43 is equipped with a probability measure @xmath38 , and datapoints are drawn from @xmath3 in an i.i.d .",
    "fashion following the distribution @xmath38 .",
    "( this is the model used in @xcite , which of course agrees with the traditional statistical approach to data modelling . )",
    "the distance @xmath2 on the domain is normalized so that the characteristic size of @xmath3 is constant : @xmath69    ( every domain can be renormalized in the above fashion unless the expected distance between the two points is infinite , which does not appear to be a realistic assumption anyway . )      @xmath3 has `` intrinsic dimension @xmath24 '' in the sense that the concentration function of the metric space with measure @xmath70 admits a gaussian upper bound @xmath71    ( such an approach to intrinsic dimensionality is developed in @xcite . )      the number @xmath25 of datapoints grows faster than any polynomial function in @xmath24 , but slower than any exponential function in @xmath24 : @xmath72 ( this is a standard assumption in the asymptotic analysis of indexing schemes for similarity search , cf .",
    "an example of such a rate of growth is @xmath73 . )",
    "note that randomly drawing a single dataset @xmath74 with @xmath25 points amounts to randomly drawing a single point in the @xmath25-th power of the domain , @xmath75 , equipped with the product probability measure @xmath76 . in order to perform asymptotic analysis of indexing scheme performance",
    ", we will in fact be choosing an _ infinite sequence _ of datasets @xmath77 , @xmath78 .",
    "this is equivalent to drawing a single point @xmath79 ( _ sample path _ ) in the infinite product @xmath80 with regard to the corresponding infinite product of probability measures : @xmath81 when talking about _ confidence , _ we will mean the product probability in the above infinite product space .",
    "specifically , a statement @xmath82 parametrized by the dimension @xmath24 and taking as a variable the sample path @xmath79 _ occurs with _",
    "( _ asymptotically _ ) _ high confidence _ if for every @xmath83 there is @xmath84 so that @xmath85>1-\\delta\\ ] ] whenever @xmath86 .    at the same time",
    ", in order to keep the notation simple , we will suppress the dimension index @xmath24 and talk just of a single domain @xmath3 and a dataset @xmath74 .",
    "denote @xmath87 the nearest neighbour distance function on @xmath3 , given by @xmath88 .",
    "[ th : empty ] under our standing assumptions on the workload , for every @xmath13 one has with asymptotically high confidence that for all points @xmath89 except for a set of measure @xmath90 @xmath91    the result applies to the hamming cube , the euclidean cube , the euclidean space with gaussian measure , the euclidean ball , etc . as a byproduct of the technique , one obtains :    [ p : simplex ] under the same assumptions , for every @xmath13 the pairwise distances between datapoints of @xmath4 are all in the range @xmath92 with asymptotically high confidence .    for constant @xmath93 and the case of a euclidean domain the result",
    "was established in @xcite .",
    "our proofs can be found in appendix a.",
    "let @xmath94 denote a collection of subsets of the domain @xmath3 .",
    "the vc dimension is an important measure of combinatorial complexity of @xmath94 .",
    "a finite set @xmath95 is _ shattered _ by @xmath94 if every subset @xmath96 can be `` carved out '' of @xmath50 with the help of a suitable element @xmath97 of @xmath94 : @xmath98    [ 0.25 ] shattered by a class @xmath94.,title=\"fig : \" ]    the _ vc dimension _ of @xmath94 , denoted @xmath99 , is the supremum of cardinalities of all finite subsets of the domain which are shattered by @xmath94",
    ". here are some classical examples .",
    "l|l family of sets & vc dimension + intervals in @xmath100 & @xmath101 + half - spaces in @xmath102 & @xmath103 + euclidean balls in @xmath102 & @xmath103 + parallelepipeds in @xmath102 & @xmath104 + convex polygons in @xmath102 & @xmath105 + any family with @xmath25 sets & @xmath106 + hamming balls in @xmath54 & @xmath107 +   + @xmath108 & c + @xmath109 & @xmath110 + @xmath111-fold intersections & @xmath112 +  of members of @xmath94 & +    proofs can be found e.g. in @xcite , ch .",
    "4 .    estimating the vc dimension of a particular family of sets is often a non - trivial task .",
    "for example , the value of this parameter does not seem to be known for the collection of all cubes in @xmath102 with sides parallel to the coordinate hyperplanes .",
    "more generally , it is tempting to conjecture that the vc dimension of the family of all balls ( either open or closed ) in a banach space of finite dimension @xmath24 equals @xmath103 , but the author is unaware of any results beyond the euclidean case .",
    "recall that the _ borel sigma - algebra _ of subsets of a metric space @xmath3 is the smallest family closed under countable intersections and complements and containing all open balls .",
    "elements of the borel sigma - algebra are called simply _ borel subsets_.",
    "we will restrict our attention to those families @xmath94 whose elements are borel subsets of @xmath3 .",
    "this assumption guarantees that the value @xmath113 is well - defined for every probability measure @xmath38 on @xmath3 .",
    "the _ empirical measure _ of @xmath114 with regard to a finite sample @xmath115 is just the normalized counting measure @xmath116 the vc dimension of @xmath94 is finite if and only if , with high confidence , the empirical measures of every @xmath114 converge uniformly to the true value @xmath113 as the sample size @xmath25 goes to infinity , no matter what the underlying measure @xmath38 is .",
    "here is a more exact formulation .",
    "a class @xmath94 has the property of _ uniform convergence of empirical measures _ , or is a _ uniform glivenko  cantelli class , _ if there is a function @xmath117 ( _ sample complexity _ of the class ) so that , given a desired precision value @xmath13 and a risk level @xmath83 , whenever @xmath118 , one has @xmath119 here @xmath120 denotes the family of all probability measures on @xmath3 .",
    "we quote the following as stated in @xcite , theorem 7.8 .",
    "[ th : ugc ] a concept class @xmath94 is uniform glivenko ",
    "cantelli if and only if @xmath121 , in which case @xmath122    one of the components of the proof is the concentration of measure in the hamming cube @xmath41 .",
    "let us remark that similar results can be stated and proved for _ function classes , _ that is , collections @xmath35 of functions from the domain @xmath3 to the interval @xmath123 $ ] .",
    "the role of vc dimension is taken over by other combinatorial parameters , such as the _ fat shattering dimension_. we will not enter into details .    among a great selection of books treating vc theory , let us mention encyclopaedic sources @xcite and @xcite , a classical monograph @xcite , and a lighter , but very well - written @xcite .",
    "let @xmath124 be a similarity workload , @xmath125 a metric space , and @xmath126 a @xmath0-lipschitz function .",
    "if queries in @xmath125 are easier to process than in @xmath3 , then it makes sense , given a range query @xmath11 in @xmath127 , to run a @xmath128 range query in @xmath129 , retrieving all datapoints @xmath20 with @xmath130 within the distance of @xmath131 of @xmath132 , and then check them against the condition @xmath133 .",
    "the @xmath0-lipschitz property of @xmath37 guarantees that no true hits will be missed .    in this way",
    ", the function @xmath37 can be viewed as a _ projective reduction _ of the exact similarity search problem to the new workload @xmath129 .",
    "this viewpoint is developed in some detail e.g. in @xcite .",
    "access overhead _ of the reduction @xmath37 is defined as @xmath134 this simple and well - known idea on its own can be surprisingly efficient , cf .",
    "@xcite .",
    "every finite collection @xmath136 of 1-lipschitz functions on @xmath43 defines a @xmath0-lipschitz mapping @xmath137 from @xmath3 to @xmath135 via the formula @xmath138 here @xmath135 is the vector space @xmath139 equipped with the norm @xmath140 .",
    "if the @xmath6 are distance functions from pivot points @xmath10 , the resulting mapping @xmath37 is of the form @xmath141 in @xcite , it was suggested to use a reduction of this form in case where the distance computations in @xmath3 are so expensive that even a simple sequential scan of the image @xmath142 in @xmath143 is computationally cheaper .",
    "this idea was analyzed for more general similarity measures than metrics in @xcite . by combining it with other access methods on the space @xmath143 ,",
    "further new indexing methods have been developed , see e.g. @xcite .",
    "a @xmath144-nn similarity query is processed in @xmath145 in time @xmath146 here the first term stands for the calculation of @xmath111 distances from a query point @xmath32 to the pivots and @xmath147 is the processing time of a rectangular query in @xmath135 , while the latter expression lists the number of distance computations in @xmath3 needed to separate false hits from @xmath111 true positives .",
    "a classical paper on optimizing the pivot selection is @xcite .",
    "our next result ( a slightly corrected version of the main theorem in ) is valid not only for the hamming cube which is a testbed for asymptotic analysis of performance of indexing schemes , but also for the euclidean space @xmath102 with the gaussian measure , the cube @xmath123^d$ ] , and so forth .",
    "[ th : pivotcurse ] in addition to the assumptions of subs .",
    "[ ss : assumptions ] , suppose also that the vc dimension of the family of all balls in @xmath3 is @xmath148 . any pivot table with @xmath149 pivots",
    "will return an expected average number of @xmath150 datapoints .",
    "consequently , the average total complexity of the performance of any pivot table for the resulting workload is @xmath151 .",
    "assume the number of pivots @xmath111 is @xmath152 .",
    "let @xmath153 denote the median value of the function @xmath87 , so that for at least half query points @xmath32 the distance to the nn in @xmath4 is @xmath154 .",
    "for each pivot @xmath155 , @xmath156 , denote @xmath157 the median value of the distance function @xmath9 . because of concentration , the mesure of the spherical shell @xmath158 is @xmath159 , and the complement to the intersection , @xmath160 , of all @xmath111 shells has measure @xmath161 since @xmath25 is subexponential in @xmath24 .",
    "[ 0.25],title=\"fig : \" ]    thus , among all @xmath111-fold intersections of spherical shells ( figure [ fig : pivots ] ) , we have found a giant one , whose @xmath38-measure is nearly one .    to assure that this intersection contains an accordingly high proportion of datapoints , consult the table in subs .",
    "[ ss : vc ] to deduce that the family of all @xmath111-fold intersections of spherical shells in @xmath3 has vc dimension not exceeding @xmath162 . by theorem [ th : ugc ] , the empirical measure @xmath163 approaches @xmath164 and therefore @xmath0 with high confidence as @xmath34 .",
    "the measure of the set @xmath165 of query points @xmath166 whose distance to the nearest neighbour in @xmath4 is greater than or equal to @xmath153 is at least @xmath167 . for every non - empty range query @xmath11 where @xmath168",
    ", all datapoints belonging to @xmath169 , that is , most datapoints of @xmath4 , have to be returned .",
    "this gives an expected average total complexity @xmath150 under our assumption on the number of pivots .",
    "notice that we allow the pivots @xmath155 to be arbitrary points of the domain @xmath3 .",
    "if we require that pivots be chosen from the dataset @xmath4 , then the set @xmath169 in the above proof will with high confidence contain @xmath170 datapoints by theorem [ th : empty ] and proposition [ p : simplex ] , and we obtain ( without using vc theory ) :    under the assumptions of subs .",
    "[ ss : assumptions ] , if all pivots @xmath155 belong to the dataset @xmath4 , then the expected total complexity of the performance of the resulting pivot table is @xmath171 .",
    "the above lower bounds agree with an exponential in @xmath24 upper bound of @xmath172 derived in the influential paper @xcite , theorem 3 within a similar model , with no restriction on a number @xmath25 of datapoints , and with @xmath24 a dimension parameter defined by a certain measure distribution density condition verified , e.g. by the hamming cube @xmath54 or the euclidean sphere @xmath173 . here",
    "@xmath174 is a constant depending on @xmath3 , the smallest distortion parameter of a @xmath0-lipschitz embedding @xmath175 : @xmath176 @xmath177 however , the usefulness of the result is limited because of an imprecise claim ( _ loc .",
    "_ , example 1 ) that for a bounded subset @xmath4 of @xmath178 there always exists a 1-lipschitz function @xmath179 having distortion @xmath180 . in fact , an optimal constant here is on the order @xmath181 ( see [ s : distortion ] ) . as a result , the query performance estimate for the euclidean domains made in remark after the main theorem 3 , _",
    "loc.cit._ , becomes superexponential in @xmath24 and thus meaningless .",
    "this misconception has led to some further confusion , cf .",
    "remarks made in @xcite ( p. 2358",
    ", end of first paragraph on the r.h.s . , and at the beginning of section 5 ) .",
    "for a finite rooted tree @xmath182 we denote @xmath183 the set of leaves of @xmath182 and @xmath184 the set of inner nodes .",
    "the symbol @xmath185 will denote the root node of @xmath182 .",
    "let @xmath35 be a class of @xmath0-lipschitz functions on @xmath3 ( possibly partially defined ) .",
    "a _ metric tree _ ( _ of type @xmath35 _ ) for a workload @xmath124 is a hierarchical indexing structure consisting of + @xmath186 a finite binary rooted tree @xmath187 , + @xmath186 an assignment of a function @xmath188 ( a _ pruning , _ or _ decision function _ ) to every inner node @xmath189 , and + @xmath186 a collection of subsets @xmath190 , @xmath191 ( _ bins _ ) , covering the dataset : @xmath192 .",
    "since we assume that the tree @xmath182 is binary , it can be identified with a sub - tree of the prefix tree , that is , a subset of binary strings @xmath193 , @xmath194 , where @xmath195 for all @xmath196 . at each inner node",
    "@xmath197 the value of the pruning function @xmath198 at the query center @xmath199 is evaluated . the condition @xmath200 gurantees that the child node @xmath201 need not be visited , because all elements @xmath20 of the bins indexed with the descendants of @xmath202 are at a distance @xmath203 from @xmath199 .",
    "indeed , assuming @xmath204 , one has @xmath205 similarly , if @xmath206 , then the node @xmath207 can be pruned , because no bin labelled with descendants of @xmath208 can possibly contain a point within the range @xmath131 from @xmath199 .",
    "however , if @xmath209 $ ] , then no pruning is possible and both children nodes of @xmath210 have to be visited",
    ". the search branches out . in the presence of concentration , the amount of branching is considerable , and results in dimensionality curse .",
    "the m - tree @xcite is by now a classical example of a metric tree .",
    "however , metric tree - type indexing schemes are very numerous , cf . sections 2.1 - 2.4 in @xcite or section 4.5 in @xcite .      for a function @xmath37 and a real number @xmath210 , denote @xmath211 .    [",
    "th : mtree ] in addition to the assumptions of subs .",
    "[ ss : assumptions ] , let @xmath35 be a class of @xmath0-lipschitz functions on the domain @xmath3 such that the vc dimension of the family of sets @xmath212 , @xmath36 , @xmath213 is @xmath214 .",
    "then the expected average performance of every metric tree indexing structure of type @xmath35 is superpolynomial in @xmath24 .    that the above combinatorial assumption on the class @xmath35 is sensible , follows from a theorem of goldberg and jerrum @xcite .",
    "consider a parametrized class @xmath215 for some @xmath216-valued function @xmath37 .",
    "suppose that , for each input @xmath217 , there is an algorithm that computes @xmath218 , and this computation takes no more than @xmath210 operations of the following types : + @xmath186 the arithmetic operations @xmath219 and @xmath220 on real numbers , + @xmath186 jumps conditioned on @xmath221 , @xmath222 , @xmath223 , @xmath224 , @xmath225 , and @xmath226 comparisons of real numbers , and + @xmath186 output @xmath227 or @xmath0",
    ".    then @xmath228 .",
    "essentially , the above result states that a class of binary functions that can be computed in polynomial time taking a parameter value of polynomial length will have a polynomial vc dimension .",
    ".3 cm on the proof of theorem [ th : mtree ] .",
    "( for details , see @xcite . ) suppose the conclusion is false , and fix a particular @xmath229 rate , @xmath230 , bounding from above the performance of a metric tree on any sample path .",
    "as the total content of bins @xmath231 indexed with strings @xmath210 of length exceeding the rate @xmath230 has to be asymptotically negligible , we can assume without loss in generality that the indexing tree has depth @xmath230 .    without loss in generality",
    "every bin can be replaced with an intersection of a family of sets of the form @xmath212 , @xmath36 and their complements .",
    "this provides a @xmath229 upper bound on the vc dimension on the family of all possible bins .",
    "with high confidence , a bin of a large measure will contain many data points , contradicting the @xmath229 performance bound .",
    "this leads to conclude that measures of bins can not be too skewed .",
    "now concentration of measure is used to prove that at least @xmath232 bins @xmath231 have size so large that the @xmath233-neighbourhood of @xmath231 has almost full measure .",
    "one deduces further that query centres @xmath32 whose @xmath234-neighbourhood meets at least @xmath235 bins have measure @xmath236 .",
    "processing a nearest neighbour query with such a centre @xmath32 requires accessing all of these bins , let even to verify that some of them are empty .",
    "this leads to a contradiction with the assumed uniform performance bound on the algorithm .",
    "of course the above are just particular results only applicable to specific indexing schemes .",
    "if one wants to validate the curse of dimensionality once and for all , here is an interesting open problem .",
    "let @xmath4 be a dataset with @xmath25 points in the hamming cube @xmath54 .",
    "suppose @xmath237 and @xmath238 .",
    "then any data structure for exact nearest neighbour search in @xmath4 , with @xmath239 query time , must use @xmath240 space .",
    "the data structure and algorithm are understood in the sense of the _ cell probe model _ of computation ( cf .",
    "@xcite ) .      in the context of similarity search",
    ", the model can be described as follows .",
    "an abstract indexing structure for a domain @xmath3 consists of + @xmath186 a collection of cells @xmath241 , indexed with a set @xmath242 , + @xmath186 a dictionary @xmath243 over an alphabet @xmath244 , viewed as a rooted prefix tree , + @xmath186 a computable mapping @xmath245 from @xmath182 to @xmath242 ( _ cell selector _ ) , and + @xmath186 a computable function @xmath246 ( either partially or fully ) defined on @xmath247 and taking values in @xmath5 .    for a @xmath248",
    ", one can think of each @xmath198 as a function defined on a subset of @xmath3 and taking a @xmath249-bit string @xmath250 as a parameter , except if @xmath251 is the root .",
    "a value @xmath252 is a child @xmath253 of the node @xmath210 .    for every @xmath196 ,",
    "the cell @xmath254 can hold a @xmath249-bit string .",
    "sometimes @xmath249 is regarded as constant , but often it is assumed that @xmath255 , so that a cell corresponding to a leaf node can store a pointer to a datapoint @xmath256 . occasionally the nearest neighbour problem is replaced with a weaker _ decision version _ ( known as _ near neighbour problem _ ) , whereby a range parameter @xmath257 is fixed and the algorithm is expected to tell whether there is an @xmath256 at a distance @xmath258 from the query point .",
    "in such a case , a leaf node cell @xmath254 will hold a single bit ( a `` yes '' or `` no '' answer ) .    building the data structure at the preprocessing stage , given a dataset @xmath4 , consists in storing in every node cell a @xmath249-bit string .",
    "a memory image of the indexing structure @xmath259 is created when the algorithm is initialized .",
    "given a query point @xmath12 , the prefix tree @xmath243 is traversed down to the leaf level beginning with the root . at the inner node @xmath210 ,",
    "the content @xmath250 of the cell @xmath260 is read and passed on to the function @xmath198 as a parameter .",
    "the computed value @xmath261 indicates a child of @xmath210 to follow at the next step . when a leaf @xmath262 is reached , the algorithm halts and returns the contents of @xmath263 .",
    "the query time is the length of the branch traversed , or equivalently the number of cell probes during the execution of the algorithm .",
    "the space requirement of the model is the total number of cells , @xmath264 .",
    "the cell probe model is very liberal , as the cost of computing the values of @xmath37 is disregarded .",
    "for this reason , any lower bound obtained under the cell probe will likely hold under any other model of computation .",
    "the best lower bound currently known is @xmath265 , where @xmath253 is the number of cells used by the data structure @xcite . in particular , this implies the earlier bound @xmath266 for polynomial space data structures @xcite , as well as the bound @xmath267 for near linear space ( namely @xmath268 ) .",
    "approximate nearest neighbour search @xcite is often said to be free from the curse of dimensionality , and the reason is that the ( dimensionality ) reduction maps @xmath37 used in indexing are no longer @xmath0-lipschitz .",
    "rather , they are what may be called `` probably approximately @xmath0-lipschitz '' , and sometimes only on a certain distance scale .",
    "such maps no longer exhibit a strong concentration around their means .",
    "the price to pay is that we may lose some relevant datapoints , as some distances are typically getting distorted , and so such maps can not be used for exact nn search .",
    "think of the hamming cube @xmath54 as the set of all binary functions in the space @xmath269)$ ] , where @xmath270=\\{1,2,\\ldots , d\\}$ ] supports a uniform measure .",
    "in other words , we normalize the hamming distance @xmath271 by multiplying it by @xmath272 .",
    "of course such a normalization has no effect on similarity search .",
    "if the dataset @xmath273 contains @xmath25 points , then the vc dimension of @xmath4 , viewed as a concept class on @xmath274 , does not exceed @xmath275 . according to the uniform glivenko  cantelli theorem [ th : ugc ] , if @xmath276 coordinates of the hamming cube",
    "are chosen at random , then with high confidence the restriction mapping from @xmath4 to the hamming cube @xmath277 ( under its own normalized hamming distance ) preserves the pairwise distances to within @xmath278 .",
    "figure [ fig : distortion_h500 ] .",
    "the error of @xmath279 is additive rather than multiplicative , so the random sampling of the coordinates is only appropriate for ann search in the range on the order of @xmath280 .",
    "the construction has to be generalized for all possible ranges @xmath281 .",
    "such a generalization was developed in @xcite .",
    "projecting on a randomly sampled subset of @xmath111 coordinates of the hamming cube essentially amounts to a linear transformation @xmath282 , where @xmath50 is a @xmath283 matrix with i.i.d .",
    "bernoulli entries assuming values @xmath0 and @xmath227 with probabilities @xmath272 and @xmath284 , respectively . ( the operations are carried @xmath285 . )",
    "one of the key observations of @xcite  in the form given to it in @xcite , 7.2  is that if the probability @xmath272 is replaced with @xmath286 , then a random linear transformation @xmath287 , under a suitable normalization , preverves distances on the scale @xmath288 , @xmath281 , to within an additive error @xmath289 , and on a larger scale  away from it .",
    "since the new cube only contains @xmath290 points , a hash table storing nearest neighbours , together with the reduction map @xmath37 , produces an indexing scheme for @xmath147-range search taking space polynomial in @xmath25 and answering @xmath291-approximate queries in time @xmath276 .",
    "another discovery of @xcite is that if on every scale @xmath147 one employs a sufficiently large series of independent projections onto @xmath111-cubes , then with high confidence one can assure that _ every _ ann query  as opposed to _ most _ ann queries  will be answered correctly .",
    "finally , a separate indexing scheme is constructed for every range @xmath147 .",
    "the overall space requirement is still polynomial in @xmath25 , and the running time of the algorithm is @xmath292 .",
    "let @xmath293 denote the euclidean sphere of unit radius in the space @xmath102 .",
    "the projection @xmath294 on the first coordinate is a @xmath0-lipschitz function .",
    "for all pairs of points @xmath295 , one has @xmath296 , and for exactly one pair of antipodal points the equality is achieved .",
    "now let @xmath295 be drawn at random .",
    "what is the expected value of the distortion of distances @xmath297 ?",
    "figure [ fig : lischitz100sph ] shows that for a vast majority of pairs of points , the projection distorts distances by the factor @xmath298 .",
    "a geometric explanation , at least at an intuitive level , is simple .",
    "two randomly chosen points on the high - dimensional sphere , because of concentration of measure , are at a distance @xmath299 from each other . at the same time , half of the points of the sphere project on the interval of length @xmath300 , and",
    "so are contained in the equatorial region ( figure [ fig : projection ] ) .",
    "[ 0.25]to the geometry of random projections.,title=\"fig : \" ]    it follows that the expected absolute value of the norm of a projection of a given point @xmath20 in a random direction is of the order @xmath298 .",
    "now let @xmath115 be a finite subset of points of the sphere .",
    "denote @xmath301 the set of all vectors of the form @xmath302 whose length is normalized to one .",
    "each @xmath303 can be identified with the function @xmath304 on the unit sphere .",
    "if we now think of @xmath173 as the domain ( consisting of one - dimensional projections ) , then @xmath301 plays the role of a finite _ function class_. just like for finite concept classes , the combinatorial dimension of @xmath301 is of the order @xmath305 , and so , by vc theory , the empirical mean on a random sample of @xmath306 directions will estimate the expectations of all @xmath307 , @xmath303 to within a factor of @xmath131 with high confidence . a small number of randomly chosen directions are likely to be nearly pairwise orthogonal because of concentration , so we can instead choose an orthogonal projection to a randomly chosen space of dimension @xmath308 . since the projection is a linear map , we get the same estimate , but with a _ multiplicative error @xmath131 _",
    ", for all pairwise distances between the points of @xmath4 . it remains to work out the meaning of the empirical mean in the above setting in order to obtain the following famous result .",
    "let @xmath309 be a real number , and @xmath310 be a set of @xmath25 points in @xmath311 .",
    "let @xmath111 be an integer with @xmath312 , where @xmath97 is a sufficiently large absolute constant .",
    "then there is a mapping @xmath313 such that @xmath314 @xmath315 for all @xmath316 .",
    "moreover , as @xmath37 , one can with high confidence choose a suitably renormalized random projection from @xmath311 to a @xmath111-dimensional euclidean subspace .",
    "an even simpler proof using concentration can be found in @xcite , section 15.2 , and an up - to - date survey of the lemma , in @xcite .",
    "the normalized projection is not quite as good as a genuine @xmath0-lipschitz map , because the distortion of a distance can exceed one , and on rare occasions very considerably . yet , as a reduction mapping for _ approximate nn search , _ the projection map is quite ok . and its histogram is concentrated _ no more_. this explains the efficiency of the random projection method for _ approximate _ nn search .",
    "combined with a suitable indexing scheme in a lower - dimensional space @xmath139 , or rather a collection of such schemes , the random projection method leads to an efficient indexing scheme for an @xmath291-approximate nn search ( indyk and motwani @xcite ) .",
    "the articles @xcite and @xcite have appeared independently and at about the same time , and afterwards the dimensionality reduction methods have been shown @xcite to be near optimal in the cell probe model .",
    "merits of asymptotic analysis of indexing algorithms using artificial datasets sampled from theoretical high - dimensional distributions should be clear from @xcite . at the same time",
    ", it is an often held belief that the real data does not have very high intrinsic dimension .",
    "this corresponds to the existence of @xmath0-lipschitz functions that are highly dissipating .",
    "figure [ fig : density_best ] shows the distance distribution to the points of the sisap benchmark dataset of nasa images @xmath317 of @xmath318 vectors in a @xmath319-dimensional euclidean space @xcite from a highly dissipating pivot , selected from a gaussian cloud around @xmath4 with standard deviation on the order of the tolerance range @xmath320 retrieving on average @xmath321 of data .",
    "this has to be compared to figure [ fig : hist_14gauss ] .    of a great variety of approaches to intrinsic dimension @xcite , at least two specifically measure the amount of concentration in data .",
    "the first one is the intrinsic dimension by chvez _",
    "@xcite @xmath322 the second is the concentration dimension , studied within an axiomatic approach of @xcite : @xmath323 ^ 2}.\\ ] ] ( in both cases we assume that @xmath324 . )",
    "the value ( [ eq : concdim ] ) is convenient for asymptotic analysis in the spirit of this paper , but is nearly impossible to estimate for a given dataset . on the other hand , ( [ eq : int ] ) is readily calculated by sampling ( e.g. @xmath325 for nasa images ) and forms a good statistical estimator for the dimension of the hypothetical underlying measure @xmath38 in the most ( only ? ) interesting case where metric balls have low vc dimension .",
    "the shortcoming of ( [ eq : int ] ) is that the parameter estimates the concentration / dissipation behaviour of a _ typical _ pivot distance function , while it is a few most dissipating pivots that really matter for indexing .",
    "one may envisage the emergence of further concepts of intrinsic dimension in the same spirit , such as the _ local dimension _ of ollivier @xcite , definition 3 .",
    "the _ black box model _ of similarity search was studied by krauthgamer and lee @xcite .",
    "given a metric space ( instance ) @xmath326 , a query is a one - point metric space extension @xmath327 , where the distances @xmath328 , @xmath256 are accessible via the distance oracle .",
    "each @xmath328 can be evaluated in constant ( unit ) time .",
    "a preprocessing phase is allowed , under the condition that an indexing scheme occupies @xmath329 space .",
    "the efficiency of an algorithm for ( exact or approximate ) similarity search is estimated as a number of calls to the distance oracle necessary to answer a query .    this is a `` black box model '' in the sense that , formally speaking , there is no obvious domain ( though we will see shortly that the domain is a well - defined separable metric case , and the setting is , in fact , classical ) .",
    "a remarkable feature of the model is that the problem of characterizing workloads admitting approximate nn queries in terms of an intrinsic dimension parameter receives a complete answer .",
    "recall that the _ assouad _ ( or _ doubling _ ) _ dimension _ of a metric space @xmath326 is the minimum value @xmath330 such that every set @xmath50 in @xmath4 can be covered by @xmath331 balls of half the diameter of @xmath50 .",
    "( the _ diameter _ of a set @xmath50 is the supremum of @xmath332 , @xmath333 . )",
    "denote this parameter by @xmath334 .",
    "a metric space @xmath326 admits an algorithm requirying @xmath329 space and taking @xmath335 time to answer a @xmath336-approximate nearest neighbour query , where @xmath337 , if and only if @xmath338    here we will show that , on the contrary , an _ exact _ nn search in this context exhibits the curse of dimensionality even if the metric space @xmath326 is contained in the unit interval @xmath123 $ ] with the usual distance . with this purpose , we first convert the black box model into a conventional setting of searching in a metric domain .    the _ universal urysohn metric space , _ @xmath339 , @xcite is a complete separable metric space uniquely defined by the _",
    "one - point extension property : _ suppose @xmath4 is a finite subset of @xmath339 and @xmath32 a one - point metric space extension of @xmath4 . then @xmath339 contains a point @xmath340 so that the distances from @xmath32 and from @xmath340 to any point @xmath256 are the same .    [ 0.25 ] one - point extension property.,title=\"fig : \" ]    an equivalent definition is that if @xmath341 is finite and @xmath342 satisfes @xmath343 for all @xmath344 , then there is @xmath345 with @xmath346 for all @xmath256 .",
    "( the functions satisfying ( [ eq : flood ] ) are called _ kattov functions_. )    this remarkable object has recently received plenty of attention in metric geometry . it is a _ random , _ or _ generic , _ metric space , in a sense that by equipping the integers with a randomly chosen metric @xmath2 and taking a completion , one obtains @xmath339 almost surely @xcite . the space @xmath339 contains an isometric copy of every separable metric space @xmath3 . for this reason",
    ", one can use @xmath339 as a `` universal domain , '' and the black - box model can be restated as a classical similarity search problem in the domain @xmath347 .",
    "let @xmath4 be a finite metric space .",
    "denote @xmath93 .",
    "then any deterministic algorithm for exact similarity search in @xmath4 within the black box model will take the worst case time @xmath25 .",
    "the result is true for simple information - theoretic reasons .",
    "we will produce for every @xmath348 a query @xmath349 with a uniquely defined nearest neighbour in @xmath4 which can not be answered in time @xmath111 .    without loss in generality",
    ", we can assume that @xmath350 .",
    "let initially @xmath32 be a query having the property that @xmath351 for all @xmath256 .",
    "suppose that the algorithm has made @xmath348 calls to the distance oracle .",
    "denote @xmath352 the points whose distance to @xmath32 has been accessed . since @xmath353 for all @xmath354 , the algorithm clearly can not halt at this stage .",
    "let @xmath165 be the set of all @xmath355 with @xmath356 for all @xmath156 .",
    "since the algorithm is deterministic , we can replace @xmath32 with any @xmath357 , and the sequence of executed calls to the oracle up until the step @xmath111 will be the same .",
    "now denote @xmath358 and fix an @xmath359 .",
    "the function @xmath360 is kattov , and thus it is the distance function from some @xmath349 .",
    "clearly , @xmath361 , and @xmath349 admits a unique nearest neighbour in @xmath4 , namely @xmath362 .",
    "thus , the search can not be concluded in @xmath111 steps even if it started with the well - defined query @xmath349 .",
    "if one requires the queries to follow the same underlying distribution as datapoints , the problem becomes more subtle , and we do not know the answer .      here is an example of an indexing scheme for exact similarity search which is still `` distance - based '' but of a rather different type from either pivots or metric trees .",
    "the _ voronoi cell _ @xmath363 of a datapoint @xmath256 in a metric domain @xmath3 consists of all points @xmath12 having @xmath20 as the nearest neighbour .",
    "the _ delaunay graph _",
    "has @xmath4 as the set of vertices , with @xmath364 being adjacent if their voronoi cells intersect .",
    "suppose the domain has the property that every two points @xmath365 can be joined by a shortest geodesic path , not necessarily unique .",
    "( all the domains previously considered in this article are such , including even the urysohn space . )",
    "then for any @xmath366 and @xmath256 , either @xmath20 is the nearest neighbour to @xmath32 , or else one of the datapoints @xmath367 delaunay - adjacent to @xmath20 is strictly closer to @xmath32 than @xmath20 is .",
    "( proof : start moving along a shortest geodesic from @xmath20 towards @xmath32 , cf . figure [ fig : delaunay8 ] , and use the triangle inequality . )",
    "[ 0.25 ]    this observation turns the delaunay graph of @xmath4 in @xmath3 into an indexing scheme for exact nearest neighbour search . denote @xmath368 the list of points adjacent to each @xmath256 . given a query @xmath32 ,",
    "start with an arbitrary @xmath369 , and find @xmath370 if @xmath371 , move to @xmath372 and repeat the procedure .",
    "once @xmath373 , the algorithm halts and returns @xmath374 .",
    "this algorithm , already mentioned in @xcite , was studied for general metric spaces by navarro @xcite .",
    "see also @xcite , 4.1.6 .    in order for the algorithm to be efficient",
    ", the average vertex degree of the delaunay graph has to be small .",
    "navarro had observed ( _ loc .",
    ", _ theorem 1 ) that this is not the case in general metric spaces .",
    "specifically , he proved that for every two elements @xmath375 there exists a finite metric space @xmath376 containing @xmath4 as a subspace in which @xmath375 are connected in the delaunay graph of @xmath4 . the result by navarro translates immediately into :    let @xmath4 be a finite metric subspace of the universal urysohn space @xmath339 .",
    "then every two elements @xmath375 are adjacent in the delaunay graph of @xmath4 in @xmath339 .    in fact , the same remains true in less exotic situations , as one can deduce from proposition [ p : simplex ] that if @xmath3 be either @xmath311 , or the sphere @xmath377 , or the hamming cube , then under the assumptions of subs .",
    "[ ss : assumptions ] the delaunay graph of @xmath4 is , with high confidence , a complete graph on @xmath25 vertices .    thus , the indexing scheme in question still suffers from the curse of dimensionality because of concentration of measure considerations , but the argument seems to be of a different nature from that either for pivots or for trees .",
    "what would a common proof for all three types of schemes look like ?",
    "this highlights the difficulty of obtaining in a uniform way lower bounds for all possible `` distance - based '' indexing schemes ( after they are formalized in a suitable way ) , not to mention an even more general setting of the cell probe model for all possible indexing schemes .",
    "this having said , for real data the complexity of the delaunay graph is lower than in an artificial asymptotic setting , and voronoi diagrams are being successfully used for data mining algorithms in high dimensions , cf .",
    "@xcite .",
    "in fact , it would be interesting to investigate the performance of the spatial approximation algorithm in hyperbolic metric spaces . recall that a metric space @xmath4 in which every two points @xmath364 can be joined by a geodesic segment @xmath378 $ ] is _ hyperbolic _ ( in the sense of rips ) @xcite if there exists a @xmath83 so that every geodesic triangle is _ @xmath379-thin : _ each side @xmath378 $ ] is contained in the @xmath379-neighbourhood of the two other sides , @xmath380 $ ] and @xmath381 $ ] ( figure [ fig : geodesic ] ) .    [ 0.25 ] a @xmath379-thin geodesic triangle.,title=\"fig : \" ]    alain connes has conjectured in @xcite , pp .",
    "138141 that a long - term human memory is organized as a hyperbolic simplicial complex , where a search is performed in a manner similar to the above .",
    "without loss in generality , normalize the observable diameter of @xmath3 to one .",
    "let @xmath89 .",
    "the distance function @xmath382 is @xmath0-lipschitz and so concentrates around its median value , @xmath383 .",
    "the resulting function @xmath384 , @xmath385 is also @xmath0-lipschitz , and concentrates around its median , @xmath386 .",
    "it is easy to check that under our assumptions , the difference between the mean and the median of every @xmath0-lipschitz function @xmath37 on @xmath3 converges to zero as @xmath387 ( uniformly in @xmath37 ) .",
    "thus , without a loss in generality , we can assume that , with high confidence , @xmath388 as @xmath34 .",
    "notice that the above argument concerns the _ domain _ and not a particular _",
    "dataset_.    to prove proposition [ p : simplex ] , fix @xmath13 and sample an instance of data , @xmath4 . with confidence @xmath389 ,",
    "one has @xmath390 for all @xmath256 .",
    "moreover , since the datapoints are sampled in an i.i.d .",
    "fashion , by the union bound one has with confidence @xmath391 that @xmath392 for every pair @xmath344 . since @xmath93 is subexponential in @xmath24 , the statement follows .    to prove theorem [ th : empty ] , again fix @xmath13 . denote @xmath153 the median value of the function @xmath87 .",
    "suppose @xmath393 .",
    "proceed to a subsequence of domains and find @xmath394 with @xmath395 for all @xmath24 .",
    "the probability that @xmath383 deviates from @xmath386 by more than @xmath396 is exponentially small in @xmath24 .",
    "since @xmath93 only grows subexponentially in @xmath24 , with confidence @xmath397 one has for every @xmath256 : @xmath398 now we use a technical observation from @xcite : if @xmath95 is such that @xmath399 for some @xmath394 , then @xmath400 .",
    "it follows that @xmath401 and therefore @xmath402 which contradicts the definition of @xmath153 .",
    "this implies : @xmath403 .    to establish the converse inequality @xmath404 ,",
    "recall that a ball of radius @xmath383 centred at @xmath405 has measure @xmath51 , and so we have an obvious estimate @xmath406 .",
    "the rest follows from concentration of the function @xmath407 around one .",
    "* lemma [ eq : c ] * _ fix @xmath111 .",
    "let @xmath409 be a constant having the property that for every @xmath24 and each bounded subset @xmath4 of @xmath178 there exists a 1-lipschitz function @xmath410 having distortion @xmath174 : for all @xmath344 , @xmath411 then @xmath412 , that is , @xmath181 with a constant depending on @xmath111 . _ + the proof consists of a series of statements .      for every @xmath414 , choose a function @xmath415 from the closed @xmath25-ball @xmath416 in @xmath178 to the @xmath25-ball in @xmath417 with distortion @xmath174 .",
    "the banach space ultrapowers of both participating spaces formed with regard to a non - principal ultrafilter on the integers ( see e.g. page 55 in @xcite ) are isometric , respectively , to @xmath178 and @xmath417 , because the spaces in question are finite - dimensional .",
    "the family of @xmath0-lipschitz functions @xmath418 determines in a standard way a @xmath0-lipschitz function , @xmath37 , from @xmath178 to @xmath419 , with the property ( [ eq : c ] ) being preserved .",
    "choose @xmath37 as in 1 . according to the rademacher theorem ( cf . a discussion and references on p. 42 in @xcite ) , @xmath37 is differentiable almost everywhere with regard to the lebesgue measure .",
    "the differential of @xmath37 at any point , which we denote @xmath182 , is a linear operator of norm one having property ( [ eq : c ] ) .",
    "in particular it is injective ( though of course not onto ) , and the inverse has norm @xmath420 .",
    "recall that the _ multiplicative banach ",
    "mazur distance _ between two normed spaces @xmath421 and @xmath422 of the same dimension is the infimum of all numbers @xmath423 , where @xmath182 ranges over all isomorphisms between @xmath421 and @xmath422 .",
    "( see @xcite , p. 3 , and @xcite , 7.2 ) . from the previous observation ,",
    "we conclude :        there is a projection @xmath31 from @xmath424 having @xmath427 as its kernel and such that @xmath428 and @xmath429 ( combine @xcite , corollary on page 209 , with a classical result of kadec and snobar on projection constants , cf .",
    "@xcite , p. 71 ) .",
    "the banach - mazur distance between @xmath430 and any other @xmath111-dimensional normed space , including the kernel of @xmath31 , is @xmath431 .",
    "choose an isomorphism @xmath169 realizing this distance , then it is easy to verify that @xmath432 realizes the distance @xmath426 between @xmath425 and @xmath424 .",
    "p. ciaccia , m. patella and p. zezula .",
    ": an efficient access method for similarity search in metric spaces . in _ proceedings of 23rd international conference on very large data bases ( vldb97 ) , ( athens , greece ) _",
    ", 426435 , 1997 .",
    "nearest neighbours in high - dimensional spaces , in : j.e .",
    "goodman , j. orourke , eds .",
    ", _ handbook of discrete and computational geometry _ ,",
    "chapman and hall / crc , boca raton  london  new york  washington , d.c .",
    "877892 , 2004 .",
    "m. patella and p. ciaccia .",
    "the many facets of approximate similarity search . invited paper , in : _ proc .",
    "first int .",
    "workshop on similarity search and applications ( sisap 2008 ) , _ cancun , mxico , pp .",
    "1021 , 2008 ."
  ],
  "abstract_text": [
    "<S> degrading performance of indexing schemes for exact similarity search in high dimensions has long since been linked to histograms of distributions of distances and other @xmath0-lipschitz functions getting concentrated . </S>",
    "<S> we discuss this observation in the framework of the phenomenon of concentration of measure on the structures of high dimension and the vapnik - chervonenkis theory of statistical learning .    </S>",
    "<S> exact similarity search , indexing schemes , curse of dimensionality , lipschitz functions , concentration of measure , uniform glivenko  cantelli theorem , pivot tables , metric trees 68p10 , 68p20 , 68q87 </S>"
  ]
}