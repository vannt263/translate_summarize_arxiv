{
  "article_text": [
    "the numerical computation of reachable sets is a crucial topic in nonlinear control theory and the quantification of deterministic uncertainty in dynamical systems .",
    "collision avoidance of manned and unmanned vehicles is one particular application that currently attracts a lot of attention ( see e.g.  @xcite and the references therein ) .",
    "standard techniques such as the set - valued euler method @xcite evolve a grid - based approximation of the reachable set along the relevant time interval .",
    "they are typically very slow , because there is a high degree of redundancy in the computations they carry out .",
    "recently , a version of the set - valued euler method was presented in @xcite that tracks the boundaries of the reachable sets and uses only the boundaries of the right - hand side of the differential inclusion . with this approach ,",
    "the complexity of the euler scheme is reduced drastically in the low - dimensional setting , but only marginally in higher dimensions .",
    "the dfog optimal control algorithm @xcite , which will be discussed in more detail in section [ section : dfog ] , is another recent attempt to reduce the proportion of irrelevant computations .",
    "every point of a grid in the relevant region of the phase space is projected to the reachable set by solving a mayer problem . from this data",
    ", one can derive  at least theoretically  an accurate description of the reachable set .",
    "in contrast to traditional methods , there is no guarantee that the numerical optimisation routine finds a global minimum , and therefore , the algorithm is , strictly speaking , unstable .",
    "numerical studies , however , support the usefulness of this method .    in this paper , we propose a new approach to the calculation and representation of a reachable set approximation , motivated as an extension to the dfog algorithm .",
    "the extension consists of using the results of these optimal control problems to search for a function in a particular function space , so that the reachable set is represented as a sublevel set of this function .",
    "the function space under consideration is a reproducing kernel hilbert space ( rkhs ) , and the algorithm to search for this function is a modified support vector machine ( svm ) algorithm .",
    "our algorithm has the advantage that it is robust to a small number of errors made by the optimisation routines from the dfog method .",
    "in addition , the function used for the reachable set approximation has a sparse representation in terms of the optimal control results , and the algorithm focuses on information provided by points that are close to the boundary of the reachable set .",
    "in the following , we give a condensed overview over basic properties of reachable sets ( see section [ sec : reachable ] ) , the currently most common numerical methods for approximating them ( see section [ sec : runge : kutta ] ) and the dfog method ( see section [ section : dfog ] ) , which is the basis of our new method .",
    "we recall some standard definitions with regard to set representations .",
    "let @xmath0 be compact sets , and @xmath1 .",
    "the distance of a point @xmath2 to the set @xmath3 is defined by @xmath4 for any @xmath5 , the @xmath6-neighbourhood of @xmath3 is the set @xmath7 the projection of @xmath2 to @xmath3 is the set of points in @xmath3 that realise the infimum distance to @xmath2 , i.e. @xmath8 the hausdorff semi - distance between sets @xmath3 and @xmath9 is given by @xmath10 and the hausdorff distance between @xmath3 and @xmath9 is given by @xmath11    throughout this paper , the symbol @xmath12 denotes the euclidean norm .",
    "the symbols @xmath13 , @xmath14 etc .  denote the corresponding concepts based on the maximum norm .",
    "let @xmath15 be a nonempty convex and compact subset of @xmath16 and @xmath17,\\mathbb{r}^d):u(t)\\in u \\text { for almost all } t\\in [ t_0,t]\\big\\}\\ ] ] for fixed times @xmath18 .",
    "we consider the nonlinear control problem    [ gesamt ] @xmath19    for some @xmath20 , where holds for almost every @xmath21 $ ] and @xmath22,\\mathbb{r}^d)$ ] is absolutely continuous .",
    "we are interested in the _ reachable set _ at time @xmath23 , given by @xmath24 problem is equivalent to the differential inclusion    [ eqn : controlprobleminclusion ] @xmath25    with valid for almost all @xmath26 $ ] , and @xmath27 .",
    "reachable sets of nonlinear control systems , or , equivalently , nonlinear differential inclusions , are , in general , nonconvex .",
    "it is , however , well - known , that they enjoy several favourable properties under mild assumptions imposed on the right - hand side ( see e.g.  ( * ? ? ?",
    "* corollary 7.1 ) ) :    let @xmath28\\times\\r^d\\rightrightarrows\\r^d$ ] have closed and convex images , and assume that    * the mapping @xmath29 is measurable for all @xmath30 , * the mapping @xmath31 is upper semicontinuous for all @xmath21 $ ] , * there exists @xmath32)$ ] such that @xmath33 for all @xmath21 $ ] and @xmath30 .",
    "then the mapping @xmath34 is upper semicontinuous , and the reachable set @xmath35 is nonempty and compact for all @xmath36 .",
    "reachable sets may be approximated numerically using set - valued runge - kutta methods . given a time discretisation @xmath37 with @xmath38 and @xmath39 ,",
    "the iterations @xmath40 with initial value @xmath41 from and @xmath42 with a suitable increment function @xmath43\\times\\r^d\\rightrightarrows\\r^d$ ] define the trajectories and the reachable sets of the numerical scheme .",
    "the simplest example of such a runge - kutta scheme is the set - valued euler method with increment function @xmath44 , which has been studied in @xcite and several other contributions .",
    "the central result for our purposes is published in @xcite .",
    "[ semi : discrete : euler ] let @xmath45 be lipschitz continuous w.r.t .",
    "@xmath46 with convex and compact values .",
    "then the reachable sets of the euler scheme satisfy @xmath47    in practice , to compute approximations of the reachable set it is necessary to spatially discretise these schemes .",
    "the most natural approach is to introduce a grid @xmath48 in the phase space @xmath49 with grid size @xmath50 , and to define a fully discretized scheme with trajectories @xmath51 yielding discrete reachable sets    @xmath52    the blowup of the images is necessary in order to obtain subsets of the grid that are close to the original euler sets in hausdorff distance .",
    "error estimates corresponding to spatial discretisation have been studied in @xcite .",
    "some results are subsumed in the following statement .",
    "[ fully : discrete : thm ] let @xmath45 be lipschitz continuous w.r.t .",
    "@xmath46 with convex and compact values .",
    "then the reachable sets of the fully discrete euler scheme satisfy @xmath53    the error term @xmath54 forces the user to choose a very fine spatial discretisation , causing a high computational complexity , which is worsened by the high level of redundancy incurred by computing parts of the reachable set over and over again in the union .",
    "higher - order runge  kutta methods are practically infeasible when directly transferred from ordinary differential equations to inclusions , and hence do not seem to be a cure for the complexity problem , because the computational costs of a successive evaluation of multifunctions undo all positive effects of the higher order time - discretisation .",
    "therefore , euler s method has been the main focus of study in this area .",
    "a method that is based on the euler scheme and tracks the boundary instead of the complete reachable set for reducing the computational cost has been studied in @xcite .",
    "variations of the implicit euler scheme that are superior to the explicit euler scheme when applied to stiff differential inclusions have been analysed in @xcite and @xcite .      in this section",
    "we review a different approach that has been recently proposed by baier , gerdts and xausa @xcite , and which they call the dfog method ( short for _ distance fields on grids _ ) .",
    "exploratory work has been published earlier in @xcite and @xcite .",
    "this method exploits the representation @xmath55 of a closed set @xmath3 as the complement of the union of all open balls contained in @xmath56 .",
    "note that @xmath57 implies @xmath58 .",
    "[ dfogmethod ] let @xmath20 and @xmath59 be as in .",
    "let @xmath50 , let @xmath60 be a bounded region with @xmath61 , and define a grid @xmath62 .",
    "for every @xmath63 , solve the optimisation problem @xmath64 over trajectories @xmath65 of the runge - kutta scheme .",
    "let @xmath66 be a solution to this problem , and denote @xmath67 and @xmath68 .",
    "then the set @xmath69 is an approximation to the reachable set @xmath35 .",
    "algorithm [ dfogmethod ] requires the solution of gobal optimisation problems in a very high - dimensional state space , to which standard tools for local optimisation such as the sqp method may be applied ( see for example @xcite ) .",
    "due to the high state - space dimension , it is impossible to use global optimisation routines . therefore , one potential pitfall is the existence of local minima leading to incorrect results for @xmath70 and @xmath71 .",
    "this means that the representation may potentially cut away large parts of the reachable set .",
    "one heuristic solution to this problem offered in @xcite is ` ball - checking ' : for any @xmath72 , check whether the computed @xmath70 and @xmath73 satisfy @xmath74 . in that case",
    ", the optimisation routine failed to compute @xmath73 properly , so that the ball @xmath75 is incorrect and must be ignored .",
    "however , this strategy does not necessarily detect erroneous results .",
    "the following statement is a consequence of theorem [ semi : discrete : euler ] and the representation .",
    "[ dfog : estimate ] let @xmath45 be lipschitz continuous w.r.t .",
    "@xmath46 with convex and compact values .",
    "if the optimisation problems in algorithm [ dfogmethod ] are solved correctly , then @xmath76    in contrast to theorem [ fully : discrete : thm ] , the error estimate does not contain the critical term @xmath54 , which indicates that the dfog method should be substantially faster than the euler scheme . nevertheless , it is difficult to compare the performance of both schemes on a formal level , because their design and their behaviour are too different .",
    "the dfog method can be accelerated using the method of maximal gains published in @xcite , which does not use a spatial a grid , but chooses optimal test points with respect to all information that is available at runtime .",
    "in this section , we develop step by step the algorithm the present paper is concerned with .",
    "as the dfog method , it requires the results of the optimal control routine given in algorithm [ dfogmethod ] , but it uses the well - known svm algorithm from machine learning to represent the approximation of the reachable set as a sublevel set of a smooth function chosen from a reproducing kernel hilbert space ( rkhs ) .",
    "this representation provides a smoother boundary for the reachable set approximation and some robustness against a small number of errors corresponding to the global optimisation routine finding local minima .",
    "in section [ sec : rkhs ] , we give a brief introduction to reproducing kernel hilbert spaces , and in section [ sec : svm ] , we adapt the classical svm algorithm to our particular problem . in section [ sec : incremental ] , we discuss the possibility of an adaptive enlargement of the dataset and computational implications . a brief comment on the ball - checking procedure in the svm context is given in section [ sec : ball : check ] .",
    "we will define our rkhs in terms of a mercer kernel .",
    "let @xmath77 .",
    "a mercer kernel is a function @xmath78",
    "satisfying    * @xmath79 for all @xmath80 , * @xmath81 for any @xmath82 , any @xmath83 and any @xmath84 .    typical examples of kernel functions include the gaussian kernel @xmath85 and the _ _ degree-__@xmath86 polynomial kernel @xmath87    given a kernel function , we define @xmath88 .",
    "the following theorem states how a mercer kernel uniquely defines a reproducing kernel hilbert space @xcite :    [ thm : moore - aronszajn ] given a mercer kernel @xmath89 , there exists a unique hilbert space @xmath90 of functions on @xmath91 with associated inner product @xmath92 satisfying the following conditions :    * @xmath93 for all @xmath94 , * @xmath95 is dense in @xmath90 , * @xmath96 for all @xmath97 and all @xmath94 .",
    "the inner product in the rkhs is defined by @xmath98 and extending linearly .",
    "@xmath90 is then taken as the completion of the linear span of @xmath99 with respect to this inner product .",
    "the third property in theorem [ thm : moore - aronszajn ] is the _ reproducing property_.    in this paper , we choose to work with the rkhs corresponding to the gaussian kernel , although the algorithm is also viable with other choices of kernels .",
    "the gaussian rkhs has been well studied and is a very rich function space to work in , which is illustrated by the following result from @xcite .",
    "let @xmath77 be compact .",
    "then the gaussian rkhs @xmath90 on @xmath91 is dense in the space @xmath100 of continuous functions on @xmath91 .    for a detailed coverage of the gaussian rkhs",
    ", we refer to @xcite . in practice",
    ", we will be working with finite dimensional rkhs , and in particular , the set @xmath91 will be chosen according to the grid points and results from the dfog optimal control method .",
    "support vector machines ( svms ) are well - known supervised learning algorithms frequently used for classification problems , a common task in machine learning problems . the soft - margin svm algorithm was first proposed by cortes and vapnik @xcite and is now a popular choice for machine learning problems .",
    "applications of the svm algorithm include handwriting recognition , image classification and text categorisation @xcite .",
    "we apply the svm algorithm to a labelled training set @xmath101 , which contains all relevant information encoded in the output data @xmath102 of the dfog method , in order to recognise the shape of the reachable set . the training set @xmath101 and index sets @xmath103 , @xmath104 and @xmath105 that partition @xmath101 into interior , exterior and boundary points of the reachable set , respectively , are constructed as follows .",
    "[ construct : training : set ] first run algorithm [ dfogmethod ] to obtain the data @xmath102 .",
    "set @xmath106 and @xmath107 .",
    "fix @xmath108 .",
    "+ for @xmath109 + @xmath110 then + @xmath111 + @xmath112 , @xmath113 + @xmath114 +   + @xmath111 , @xmath115 + @xmath116 , @xmath117 , @xmath118 + @xmath119 .",
    "+   + end    the idea behind this algorithm is simple . for any @xmath109 , the fact that @xmath120 implies @xmath121 . if , on the other hand , @xmath122 , then @xmath123 and @xmath124 , assuming no error has been made in the global optimisation routine",
    "this way , algorithm [ construct : training : set ] constructs a training set @xmath125 of points @xmath126 with index set partitions @xmath103 , @xmath104 and @xmath105 .",
    "the small parameter @xmath108 is introduced to compensate for numerical precision errors . by construction",
    ", we have @xmath127 .",
    "the support vector machine algorithm is designed to find a function from an rkhs ( along with its sublevel set ) which best fits a labelled training set such as this , in the sense of minimising a suitable loss function .",
    "however , the context here differs from the usual setting in which the svm is applied ( to a set of randomly generated data potentially subject to noise ) in two main ways .",
    "firstly , the training set is not just labelled according to whether a sample point belongs to the reachable set or not , but also has the possible label of being on the boundary , as indicated by the three index sets @xmath103 , @xmath104 and @xmath105 .",
    "secondly , the standard soft - margin svm classifier allows for statistical errors in the labelling of data . here , only specific errors can occur : a point that is labelled as an interior or boundary point must belong to the reachable set as the optimal control routine finds an admissible path to reach that point",
    "so we do not want to allow a point with index @xmath128 to be on the exterior of our reachable set approximation .",
    "however , a point labelled as an exterior point could well belong to the reachable if the optimal control routine failed to find the global minimum .",
    "we present the following adapted svm algorithm in order to account for these differences :    [ adaptedsvm ] first run algorithm [ construct : training : set ] to obtain the set @xmath129 and index set partitions @xmath103 , @xmath104 and @xmath105 .",
    "fix regularisation parameters @xmath130 , and let @xmath131 be a mercer kernel with corresponding finite - dimensional rkhs @xmath90 on @xmath132 .",
    "we search for a function @xmath133 in @xmath90 by solving the following optimisation problem over the optimisation variables @xmath134 : @xmath135 the approximation of the reachable set @xmath35 is given by @xmath136    the labelled training set generated by algorithm [ construct : training : set ] , which contains all available knowledge about the reachable set , is incorporated in constraints , and .",
    "the constraint ensures that the function value is at least @xmath137 on the points that are labelled as interior points .",
    "note that there is no slack variable appearing in this constraint , according to our observation that points labelled as interior points must lie within the reachable set .",
    "in contrast , contains the non - negative slack variable @xmath138 ( see also ) , which allows for the possibility of an error being made on a point labelled as an exterior point .",
    "where the slack variable is zero , the function value is less than or equal to @xmath139 on exterior points .",
    "the constraint tries to place boundary points on the level set @xmath140 . here",
    "the non - negativity condition follows front the fact that points labelled as boundary points are the endpoints of orbits of and so can not be on the exterior of the reachable set .",
    "the first term of the cost function controls the complexity of the function @xmath97 ( and hence the sub level set ) to avoid overfitting the training set @xmath101 .",
    "this is contrasted with the following two terms , which control the penalty due to errors in classification .",
    "this bias - variance trade - off is managed through the regularisation coefficients @xmath141 and @xmath142 .",
    "as these coefficients approach infinity , the function @xmath143 is allowed to become more and more complex , and the solution to the optimisation problem approaches the hard - margin solution where no errors are permitted on the training set .    the optimisation problem",
    " is a convex optimization problem , and in particular all the constraints are affine . in this case",
    "slater s theorem guarantees strong duality if the problem is feasible @xcite . in the case of the gaussian kernel",
    ", feasibility is guaranteed by the following theorem @xcite .",
    "[ thm : gaussianfullrank ] let @xmath144 be distinct points , and @xmath145 .",
    "the matrix @xmath146 given by @xmath147 has full rank .",
    "therefore , algorithm [ adaptedsvm ] can be recast into the dual problem using the kkt conditions .",
    "this is the problem that is generally solved in practice .",
    "we introduce the variables @xmath148 , @xmath149 by defining @xmath150 for @xmath128 and @xmath151 for @xmath152 .    under the same conditions as in algorithm [ adaptedsvm ] ,",
    "solve the following minimisation problem over the variables @xmath153 : @xmath154    the solution to the problem  provides the function @xmath133 , where the @xmath155 are given by @xmath156 for @xmath157 .",
    "the points @xmath158 for which the corresponding constraint , or are strictly satisfied are called the _ support vectors _ in the literature . for the support vectors the corresponding constraints , and are satisfied as equalities , and in addition @xmath138 or @xmath159 is equal to zero .",
    "the offset @xmath160 can therefore be computed from , and for the support vectors .",
    "accordingly , points @xmath158 ( @xmath152 ) for which @xmath161 and points @xmath158 ( @xmath162 ) for which @xmath163 are the so - called",
    "_ error vectors_. these are the points for which @xmath138 and @xmath159 may be nonzero , and for which the reachable set approximation may misclassify . a boundary point @xmath158 for which @xmath164 will still be classified as being in the reachable set , but will not be on the boundary of the set approximation .",
    "however an exterior point @xmath158 for which @xmath165 will be misclassified by . if @xmath166 then @xmath158 will be still be on the exterior of @xmath167 but will be inside the ` margin ' @xmath168 and so it is still called an error vector .",
    "finally , points @xmath158 @xmath169 for which @xmath170 are _ ignored vectors_. it is not hard to see that these points have no influence on the solution to the above optimisation problem , and could as well have been left out of the data set .",
    "in addition , the property of being an ignored vector is robust with respect to perturbation of the support and error vectors .",
    "note that the set of boundary points @xmath158 ( @xmath162 ) by definition does not contain any ignored vectors , since the property that @xmath170 is not robust with respect to such a perturbation due to .",
    "roughly speaking , points that are far away from the boundary of the reachable set ( and are correctly classified ) will be ignored vectors .",
    "however it is not practically possible to tell in advance which data points will be ignored vectors , or even if ignored vectors will remain ignored with the addition of new points .",
    "it is possible to increase the accuracy of the svm approximation step by step , until a desired precision is reached . in that case , the optimisation problem ",
    "( or  ) needs to be solved after each addition of a batch of new points .",
    "this optimisation problem runs over all points in the training set , so as this set becomes larger , this may become costly .",
    "fortunately it is possible to solve the svm optimisation problem by means of incremental updates @xcite .",
    "this procedure consists of deriving equations to keep the kkt conditions satisfied , as a new dual variable @xmath171 is incremented from zero .",
    "the procedure ends when a new point becomes either a support vector or error vector . for details ,",
    "we refer to @xcite . here",
    ", we outline the procedure for our adapted version of the svm algorithm .",
    "the cost function in the dual formulation of the optimisation problem may trivially be rewritten in the more convenient form @xmath172 retaining the constraints  , with the offset @xmath160 re - introduced as a lagrange multiplier . the necessary and sufficient kkt conditions for this problem",
    "may be written as follows : @xmath173 @xmath174    the conditions  are satisfied with equality for the support vectors . given",
    "a new labelled point @xmath175 with dual variable @xmath176 initially set to zero , we need to ensure that these equality conditions ( as well as ) continue to be satisfied for the support vectors as we increment @xmath176 from zero .",
    "following @xcite , we define the _ coefficient sensitivities _",
    "@xmath177 by @xmath178 = - \\left [ \\begin{matrix } 0 & y_{s_1 } & \\cdots & y_{s_{\\mathcal{n}(s)}}\\\\ y_{s_1 } & q_{s_1s_1 } & \\cdots & q_{s_1s_{\\mathcal{n}(s)}}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ y_{s_{\\mathcal{n}(s ) } } & q_{s_{\\mathcal{n}(s)}s_1 } & \\cdots & q_{s_{\\mathcal{n}(s)}s_{\\mathcal{n}(s ) } } \\end{matrix } \\right]^{-1 } \\left [ \\begin{matrix } y_c\\\\ q_{s_1c}\\\\ \\vdots",
    "\\\\ q_{s_{\\mathcal{n}(s)}c } \\end{matrix }   \\right]\\ ] ] where @xmath179 and @xmath180 is the index set corresponding to the support vectors ( @xmath181 is the number of support vectors ) . we define @xmath182 for indices @xmath183 corresponding to ignored and error vectors .",
    "then , the kkt conditions  will continue to be satisfied as @xmath176 is incremented from zero provided the existing dual coefficients are also incremented according to @xmath184 where @xmath185 is a small increment in @xmath176 .",
    "the _ margin sensitivities _",
    "@xmath186 are likewise defined by @xmath187 and give the variation of the margins @xmath188 in  : @xmath189 equations  ensure that @xmath190 for support vectors .",
    "now , for each new point @xmath175 the corresponding @xmath188 is first computed .",
    "if the new point automatically satisfies the kkt conditions then it is an ignored vector and @xmath176 is left at zero .",
    "otherwise we use , and to compute the largest possible increment of @xmath176 so that  continue to be satisfied , at which point either @xmath191 becomes a support or error vector , or else another point in the data set migrates between the sets of support , error or ignored vectors",
    ". then the coefficient and margin sensitivities must be recomputed and the procedure continues .",
    "the procedure outlined in the previous can also naturally be reversed in order to remove a point from the training set . in practical implementation",
    "we have included the ball checking routine as described in the end of section [ section : dfog ] in which case it is sometimes necessary to remove an exterior point from the data set .",
    "note however that in these cases the svm algorithm allows to keep the corresponding boundary point in the data set .    in practice ,",
    "where the optimisation routine fails to find the global minimum , it is usually the case that the computed optimal trajectory still terminates at a boundary point of the reachable set .",
    "however , note that the svm algorithm does allow for points in the index set @xmath105 to actually be interior points . in this case , some information on the reachable set is still retained in the case of an error due to the global optimisation routine of the dfog method .",
    "we illustrate the qualities of our method by applying it to two examples from the literature . in each example",
    "we compare its performance in the reachable set representation @xmath192 with that of the dfog method and its reachable set approximation @xmath193 as given in .",
    "the following example is taken from @xcite as a model system that exhibits convexity of the reachable set for small times , but nonconvexity for larger times .",
    "both the dfog and svm methods work for either case .",
    "we consider the two - dimensional control system @xmath194\\ , .",
    "\\label{eqn : bilinear5}\\end{aligned}\\ ] ]    we are interested in approximating the reachable set @xmath195 for @xmath196 .",
    "the reachable set is shown in figure [ fig : bilineardfog ] . in this computation",
    "the time interval @xmath197 $ ] has been discretised with @xmath198 steps .",
    "we note that the error due to time discretisation is the same for both the dfog and modified svm methods .",
    "this is because both methods use the same time discretisation in the constraints for the optimal control problem from algorithm [ dfogmethod ] .",
    "the difference between the two methods is the spatial representation of the reachable set . in order to compare the methods we leave the time discretisation at @xmath198 and vary the spatial grid size @xmath199 .     for the bilinear control system.,scaledwidth=70.0% ]",
    "figure [ fig : bilineardfog_svm ] shows successive approximations of both the dfog and svm methods for the reachable set @xmath200 , where @xmath201 and @xmath198 is fixed . for both algorithms , the set of grid points @xmath202 was defined as a restriction of @xmath203 , and the approximations are made for varying spatial discretizations @xmath199 independently . in this example",
    "there are very few errors made by the global optimisation routine .",
    "the hausdorff distances calculated between the true reachable set and the numerical approximations from both the dfog and svm methods are shown in figure [ fig : blcs_hd ] .    [ cols=\"^,^,^ \" , ]         [ fig : nlcs_hd ]    0.28    0.28    0.28    0.28    0.28    0.28    0.28    0.28    0.28",
    "the modified support vector machine algorithm provides an alternative representation of the reachable set , based on the results gained from a set of global optimisation problems provided by the dfog algorithm .",
    "this new approach has the advantage that it is robust to a small number global optimisation errors , and appears to benefit from faster convergence for particular examples .",
    "several specialised algorithms exist for efficiently solving the standard svm optimisation problem @xcite , which could be adapted to the modified svm algorithm we have presented here .",
    "the global optimal control problems are also particularly expensive when the dimension of the control variable is large , or when a fine time discretisation is used",
    ". therefore for many real - world problems only few optimal control problems can be solved in practice . in these cases where relatively few data from the optimal control routine are available , the svm algorithm performs significantly better .",
    "in addition , the sublevel set representation of the svm approach is more handy for many applications than the dfog representation .    as is always the case with algorithms of this type , there are several parameters in the algorithm that need to be tuned for optimal performance .",
    "the tolerance @xmath204 as described in section  [ sec : svm ] is important to distinguish interior and exterior points , and affects the approximation for both the dfog and svm algorithms . within the svm algorithm , the parameters @xmath141 and @xmath142 control the regularity of the solution as described earlier . also for radial basis functions such as the gaussian kernel used in our examples ,",
    "the scaling parameter @xmath205 is an additional parameter , related to the regularisation parameters @xmath141 and @xmath142 .",
    "these parameters can be chosen using standard validation techniques such as hold - out testing ( see @xcite ) , but the precise effect of these parameters on the regularity of the solution ( and how the parameters relate to each other ) is not yet well understood .",
    "a final important problem is that of choosing the best points on which to run the global optimisation routine in order to improve the current approximation . in our problem setting , we are in the fortunate position of being able to choose any point to run the algorithm on at each step .",
    "this is in contrast to many applications of the support vector machine , where the data is randomly generated from an unknown underlying distribution . in our example applications",
    "we have run the algorithm on a regular grid , however it is clear that this is not the optimal strategy .",
    "the question of how to choose the best point is likely to be related to problem of understanding the effect of the parameters in the algorithm , and again is a worthwhile subject of future work . the framework provided in this paper to incrementally update the svm algorithm is also a precursor to such a strategy .",
    "a further benefit of our proposed methodology is that it may also in principle be used to compute invariant sets for random dynamical systems @xcite , as well as invariant sets for control systems @xcite .",
    "r.  baier and m.  gerdts , _ a computational method for non - convex reachable sets using optimal control _ , proceedings of the european control conference ( ecc ) 2009 , budapest ( hungary ) , august 2326 , 2009 ( budapest ) , euca , 2009 , pp .",
    "97102 .",
    "to3em , _ making large - scale support vector machine learning practical _ , in advances in kernel methods - support vector learning , cambridge ma : mit press , schlkopf , burges and smola , eds . , 1998 , 169184 .",
    "micchelli , _ algebraic aspects of interpolation _ , approximation theory ( new orleans , la . , 1986 ) , proceedings of symposia in applied mathematics , vol .",
    "36 , american mathematical society , providence , ri , 1986 , pp .",
    "81102 .",
    "minh , _ some properties of gaussian reproducing kernel hilbert spaces and their implications for function approximation and learning theory _ , constructive approximation * 32 * ( 2010 ) , no .  2 , 307338 .",
    "platt , _ fast training of support vector machines using sequential minimum optimization _ , in advances in kernel methods - support vector learning , cambridge ma : mit press , schlkopf , burges and smola , eds . , 1998 , 185208 .",
    "i.  steinwart , d.  hush , and c.  scovel , _ an explicit description of the reproducing kernel hilbert spaces of gaussian rbf kernels _ , ieee transactions on information theory * 52 * ( 2006 ) , no .  10 , 46354643 ."
  ],
  "abstract_text": [
    "<S> we propose and discuss a new computational method for the numerical approximation of reachable sets for nonlinear control systems . </S>",
    "<S> it is based on the support vector machine algorithm and represents the set approximation as a sublevel set of a function chosen in a reproducing kernel hilbert space . in some sense </S>",
    "<S> , the method can be considered as an extension to the optimal control algorithm approach recently developed by baier , gerdts and xausa . </S>",
    "<S> the convergence of the method is illustrated numerically for several examples . </S>"
  ]
}