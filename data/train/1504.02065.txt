{
  "article_text": [
    "the opening quotes set up the frame in which this paper has been written : in the sciences we always deal with uncertainties ; being in condition on uncertainty we can only state ` somehow ' how much we believe something ; in order to do that we need to build up probabilistic models based on good sense .",
    "for example , if we are uncertain about the value we are going to _ read on _ an instrument , we can make probabilistic assessments about it .",
    "but in general our interest is the _ numerical value of a physics quantity_. we are usually in great condition of uncertainty before the measurement , but we still remain with some degree of uncertainty after the measurement has been performed .",
    "models enter in the construction of the the causal network which connects physics quantities to what we can observe on the instruments .",
    "they are also important because it is convenient to use , whenever it is possible , probability distributions , instead than to assign individual probabilities to each individual ` value ' ( after suitable discretization ) that a physics quantity might assume .",
    "as we know , there are good reasons why in many cases the gaussian distribution ( or _ normal _ distribution ) offers a _",
    "reasonable _ and _ convenient _ description of the probability that the quantity of interest lies within some bounds .",
    "but it is important to remember that ,  @xcite when he derived the famous distribution for the measurement errors , one should not take literally the fact that the variable appearing in the formula can range from minus infinite to plus infinite : an apple can not have infinite mass , or a negative one !    sticking hereafter to gaussian distributions , it is clear that if we are only interested to the probability density function ( pdf ) of a variable at the time , we can only describe our uncertainty about that quantity , and nothing more .",
    "the game becomes interesting when we study the joint distribution of several variables , because this is the way we can learn about some of them assuming the values of the others .",
    "for example , if we assume the joint pdf @xmath0 of variables @xmath1 and @xmath2 under the state of information @xmath3 ( on which we ground our assumptions ) , we can evaluate @xmath4 , that is the pdf adding the extra condition @xmath5 , which is usually not the same as @xmath6 , that is the pdf of @xmath1 for any value @xmath2 might assume .",
    "is called _ marginal _ , although there is never special about this name , since all distributions of a single variable can be thought as being ` marginal ' to all other possible quantities which we are not interested about .",
    "@xmath4 is instead ` called ' _ conditional _ , although it is a matter of fact that distributions are conditional to a given state of information , here indicated by @xmath3 . note that throughout this paper will shall use the same symbol @xmath7 for all pdf s , as it is customary among physicists  i have met mathematics oriented guys getting mad by the equation @xmath8 because , they say , `` the three functions can not be the same '' ... ]    let us take for example the three diagrams of fig .",
    "[ fig : modelli_base ]    to which we give a physical interpretation :    1 .   in the diagram on the left",
    "the variable @xmath1 might represent the numerical value of a physics quantity , on which we are in condition on uncertainty , modelled by @xmath9 where @xmath10 and @xmath11 are suitable parameters to state our ` ignorance ' about @xmath1 ( ` complete ignorance ' , if it does ever exist , is recovered in the limit @xmath12 ) .",
    "instead , @xmath2 is then what we read on an instrument when we apply it to @xmath1 .",
    "that is , even if we knew @xmath1 , we are still uncertain about what we can read on the instrument , as it is well understood .",
    "modelling this uncertainty by a normal distribution we have , for any value of @xmath1 @xmath13 where @xmath14 is a compact symbol for @xmath15 and which is in general different from @xmath16 .",
    "in fact our uncertainty about @xmath2 ( for any possible value of @xmath1 ) must be larger than that about @xmath1 itself , for obvious reasons ",
    "we shall see later the details .",
    "2 .   in the diagram on the center",
    "@xmath17 might represent a second observation done _ independently _ applying in general a second ( possibly different ) instrument to the identical value @xmath1 .",
    "this means that @xmath18 and @xmath19 are independent , although @xmath2 and @xmath17 are , as we shall see .",
    "3 .   in the diagram on the right @xmath17",
    "is the observation read on the instrument applies to @xmath1 , but possibly influenced by @xmath2 , that might then represent a kind of _ systematics_.    note , how it has been precisely stated , that @xmath2 of the first and of the second diagrams , as well as @xmath17 of the other two , are the _ readings _ on the instruments and the result of the measurement !",
    "this is because by `` result of the measurement '' we mean statements about the quantity of interest and not about the quantities read on the instruments ( think for example at the an experiment measuring the higgs boson mass , making use of the information recorded by the detector ! ) . in this case",
    "the `` result of the measurement '' would be @xmath20 where * data * stands for the set of observed variables",
    ".    the diagrams of the figure can be complicated , using sets of data , with systematics effects common to observations in each subset .",
    "the aim of this paper is to help in developing some intuition of what is going on in problems of this kind , with the only simplification that all pdf s of interest are normal .",
    "we assume that the reader is familiar with some basic concepts related to _ uncertain numbers _ and _ uncertain vectors _ , usually met under the name of `` random variables '' .",
    "@xmath21 : + @xmath22 \\ ] ]    with @xmath23 & = & \\mu \\\\",
    "\\mbox{var}[x ] & = & \\sigma^2 \\\\",
    "\\sigma[x ] = \\sqrt{\\mbox{var}[x ] } & = & \\sigma\\,.\\end{aligned}\\ ] ] ( we remind that in most physics applications @xmath24 simply means @xmath25 . )    in the r language  @xcite there are functions ( dnorm ( ) , pnorm ( ) and qnorm ( ) , respectively ) to calculate the pdf , the cumulative function , usually indicated with `` @xmath26 '' , as well as its inverse , as shown in the following , self explaining examples ( ` @xmath27 ' is the r console prompt ) : +   + [ 1 ]  0.3989423 +   + [ 1 ]  0.3989423 +   + [ 1 ]  0.5 +   + [ 1 ]  0.6826895 +   + [ 1 ]  5 +   + [ 1 ]  inf +   + [ 1 ]  -inf + note the capability of the language to handle infinities , as it can be cross checked by +   + [ 1 ]  1 + and here are the instructions to produce the plots of figure [ fig : gaussian_f - f ] .      the joint distribution of a bivariate normal distribution is given by @xmath28           \\right\\ } \\",
    ", , \\label{eq : bivar}\\end{aligned}\\ ] ] where @xmath29 & = & \\mu_i \\\\ \\mbox{var}[x_i ] & = & \\sigma_i^2 \\\\",
    "\\sigma[x_i ] \\equiv \\sqrt{\\mbox{var}[x_i ] } & = & \\sigma_i\\\\ \\rho_{12}&= & \\frac{\\mbox{cov}[x_1,x_2]}{\\sigma_1\\,\\sigma_2}\\ , , \\end{aligned}\\ ] ] with variances and covariances forming the _ covariance matrix _ @xmath30 & \\mbox{cov}[x_1,x_2 ] \\\\               & \\\\",
    "\\mbox{cov}[x_1,x_2 ] & \\mbox{var}[x_2 ]               \\end{array }        \\!\\right ) =        \\left(\\!\\begin{array}{cc }              \\sigma_1 ^ 2 & \\rho_{12}\\,\\sigma_1\\,\\sigma_2 \\\\               & \\\\",
    "\\rho_{12}\\,\\sigma_1\\,\\sigma_2 &   \\sigma_2 ^ 2              \\end{array }        \\!\\right ) \\end{aligned}\\ ] ] the bivariate pdf ( [ eq : bivar ] ) can be rewritten in a compact form as @xmath31              \\ , ,   \\label{eq : normale_multivariata_gen}\\end{aligned}\\ ] ] where @xmath32 stands for det(@xmath33 ) .",
    "this expression is valid for any number @xmath34 of variables and it turns , , into @xmath35\\ , .",
    "\\label{eq : normale_multivariata_ind}\\ ] ] ( for an extensive , although mathematically oriented treatise on multivariate distribution see ref .",
    "@xcite , freely available online . )",
    "functions to calculate multivariate normal pdf s , as well as cumulative functions and random generators are provided in r via the package mnormt that needs first to be installed issuing +   + and then loaded by the command +   + then we have to define the values of the parameters and built up the vector of the central values and the covariance matrix .",
    "here is an example : +   +   +   +   +   +   + then we can evaluate the joint pdf in a point @xmath36 , e.g. +   + [ 1 ]  0.1645734 + or we can evaluate @xmath37 , or @xmath38 , respectively , with +   + [ 1 ]  0.140636 + and +   + [ 1 ]  0.3524164 +      if we like to visualize the joint distribution we need a 3d graphical package , for example rgl or plot3d .",
    "we need to evaluate the joint pdf on a grid of values ` @xmath39 ' and ` @xmath40 ' and provide them to the suited function . here",
    "are the instructions that use the persp3d ( ) of the rgl package : +   +   +   +   +   +   + after the plot is shown in the graphics window , the window can be enlarged and the plot rotated at wish .",
    "figure [ fig : normale_bivariata ] shows in the upper two plots two views of the same distribution .",
    "+    here are also the instructions to use plot3d ( ) : +   +   +   +   + the result is shown in the lower plot of fig .",
    "[ fig : normale_bivariata ] .    another convenient and often used representation of normal bivariates is to draw iso - pdf contours , i.e. lines in correspondence of the points in the plane @xmath36 such as @xmath41 .",
    "this requires that the _ quadratic form _ at the exponent of eq .",
    "( [ eq : bivar ] ) [ that is what is written in general as @xmath42 has a fixed value .",
    "in the two dimensional case of eq .",
    "( [ eq : bivar ] ) we recognize the expression of an ellipse .",
    "we have in r the convenient package ellipse to evaluate the points of such an ellipse , given the vector of expected values , the covariance matrix and the probability that a point falls inside it .",
    "here is the script that applies the function to the same bivariate normal of fig .",
    "[ fig : normale_bivariata ] , thus producing the contour plots of fig .  [ fig : normale_bivariata_ellipse ] :    the probability to find a point inside the ellipse contour is defined by the argument level .",
    "the ellipses drawn with solid lines define , in order of size , 50% , 90% and 99% contours . for comparison",
    "there are also the contours at 68.3% , 95.5% and 99.73% , which define the _ highly confusing _",
    "1-@xmath43 , 2-@xmath43 and 3-@xmath43 contours .",
    "indeed , the probability that each of the variable falls in the interval of @xmath44\\pm k\\,\\sigma[x_i]$ ] has little to do with these ellipses . if we are interested to the probability that a point falls in a rectangles defined by @xmath45\\pm k\\,\\sigma[x_1]\\ , \\ & \\,\\mbox{e}[x_2]\\pm k\\,\\sigma[x_2])$ ] the probability needs to be calculated making the integral of the joint distribution inside the rectangle ( some of these rectangles are shown in fig .",
    "[ fig : normale_bivariata_ellipse ] by the dotted lines , that indicate 1-@xmath43 , 2-@xmath43 and 3-@xmath43 bound in the individual variable ) .",
    "let us see how to evaluate in r the probability that a point falls in a rectangle , making use of the cumulative probability function pmnorm ( ) . in fact the probability in a rectangle is related to the cumulative distribution by the following relation + @xmath46   & = &    p[\\ , ( x_1 \\le x_{1_m})\\ , \\&\\ , ( x_2 \\le x_{2_m } ) ]   \\nonumber \\\\ & &   - p[\\ , ( x_1 \\le x_{1_m})\\ , \\&\\ , ( x_2 \\le x_{2_m } ) ]   \\nonumber \\\\ & &      - p[\\ , ( x_1 \\le x_{1_m})\\ , \\&\\ , ( x_2 \\le x_{2_m } ) ]   \\nonumber \\\\ & &    +   p[\\ , ( x_1 \\le x_{1_m})\\ , \\&\\ , ( x_2 \\le x_{2_1 } ) ] \\ , , \\hspace{0.9cm}\\end{aligned}\\ ] ] +   + that can be implemented in an r function : for example  51313 ] +   + [ 1 ]  0.5138685 +   + [ 1 ]  0.5138685 + as a cross check , let us calculate the probabilities in strips of plus / minus one standard deviations around the averages ( the ` strips ' provide a good intuition of what a ` marginal ' is ) : +   + [ 1 ]  0.6826895 +   + [ 1 ]  0.6826895      a nice feature of the multivariate normal distribution is that if we are just interested to a subset of variables alone , neglecting which value the other ones can take ( ` marginalizing ' ) , we just drop from @xmath47 and from @xmath48 the uninteresting values , or the relative rows and columns , respectively .",
    "for example , if we have  see subsection [ sss : syst_x3 ] ",
    "@xmath49 marginalizing over the second variable ( i.e. being only interested in the first and the third ) we obtain @xmath50 here is a function that returns expected values and variance of the multivariate ` marginal '    .... marginal.norm < - function(mu , v , x.m ) {    # x.m is a vector with logical values ( or non zero ) indicating    # the elements on which to marginalise ( the others are 0 , na or false )    x.m[is.na(xm ) ] < - false    v < - which ( as.logical(x.m ) )    list(mu = mu[v ] , v = v[v , v ] ) } ....    ( note how the function has been written in a very compact form , exploiting some peculiarities of the r language",
    ". in particular , the elements of x.m to which we are interested can be true , or can be a numeric value different from zero ; the others can be false , 0 or na . )      a different problem is the pdf of one of variables , say @xmath1 , for a given value of the other .",
    "this is not as straightforward as the marginal ( and for this reason in this subsection we only consider the bivariate case ) .",
    "fortunately the distribution is still a gaussian , with _ shifted central value _ and _ squeezed width _ : @xmath51 i.e. @xmath52 & = & \\mu_1 + \\rho_{12}\\,\\frac{\\sigma_1}{\\sigma_2}\\ , ( x_2-\\mu_2 )   \\label{eq : x1_cond1_e } \\\\",
    "\\mbox{var}[x_1 ] & = & \\sigma_1 ^ 2\\cdot(1-\\rho_{12}^2 ) \\label{eq : x1_cond1_var } \\\\",
    "\\sigma[x_1 ] & = & \\sigma_1 \\cdot\\sqrt{1-\\rho_{12}^2}\\ , .",
    "\\end{aligned}\\ ] ] and , by symmetry , @xmath53 mnemonic rules to remember eqs .",
    "( [ eq : x1_cond1_e ] ) and ( [ eq : x1_cond1_var ] ) are    * the shift of the expected value depends linearly on the correlation coefficient as well on the difference between the value of the conditionand ( @xmath54 ) and its expected value ( @xmath55 ) ; the ratio @xmath56 can be seen as a minimal dimensional factor in order to get a quantity that has the same dimensions of @xmath57 ( remember that @xmath1 and @xmath2 have in general different physical dimensions ) ; * the variance is reduced by a factor which depends on the absolute value of the correlation coefficient , but not on its sign .",
    "in particular it goes to zero if @xmath58 , limit in which the two quantities become linear dependent , while it does not change if @xmath59 , since the two variables become independent and they can not effect each other .",
    "( in general independence implies @xmath60 . for the normal bivariate it is also true the other way around . )",
    "an example of a bivariate distribution ( from @xcite , with @xmath61 and @xmath54 indicated as customary with @xmath39 and @xmath40 ) is given in fig .",
    "[ fig : bivar ] , which shows also the marginals and some conditionals .      as an exercise ,",
    "lets prove ( [ eq : x1_cond1 ] ) , with the purpose of show some useful tricks to simplify the calculations .",
    "if we take literally the rule to evaluate @xmath62 knowing that @xmath0 is given by ( [ eq : bivar ] ) we need to calculate @xmath63 the trick is to make the calculations neglecting all irrelevant multiplicative factors , starting from the whole denominator @xmath64 , which given @xmath5 ( whatever its value might be ! ) .    here",
    "are the details ( note that additive terms in the exponential are factors in the function of interest!):}$ ] , then it is also proportional to @xmath65 } =   \\exp{\\left[-h^2\\,\\left(x- \\left(-\\frac{\\alpha}{2}\\right ) \\right)^2\\right]}\\,,\\ ] ] that is a gaussian with @xmath66 and @xmath67 . ]",
    "@xmath68           \\right\\ }   \\nonumber \\\\ \\!\\ ! & \\propto&\\!\\ !   \\exp \\left\\ {    -\\frac{1}{2\\,(1-\\rho_{12}^2)\\,\\sigma_1 ^ 2 }     \\left [ ( x_1-\\mu_1)^2     - 2\\,\\rho_{12}\\,\\frac{\\sigma_1}{\\sigma_2}(x_1-\\mu_1)(x_2-\\mu_2 )                   \\right ]   \\right\\ }   \\nonumber   \\\\ \\!\\ ! & \\propto&\\!\\ !   \\exp \\left\\ {    -\\frac{1}{2\\,(1-\\rho_{12}^2)\\,\\sigma_1 ^ 2 }     \\left [ x_1 ^ 2 - 2\\,\\mu_1x_1 + \\mu_1 ^ 2     - 2\\,\\rho_{12}\\,\\frac{\\sigma_1}{\\sigma_2}\\,(x_2-\\mu_2)\\,x_1                   \\right ]   \\right\\ } \\ , , \\nonumber \\\\ \\!\\ ! & \\propto&\\!\\ !   \\exp \\left\\ {    -\\frac{1}{2\\,(1-\\rho_{12}^2)\\,\\sigma_1 ^ 2 }     \\left [ x_1 ^ 2 - 2\\,x_1\\ , [ \\mu_1     + \\rho_{12}\\,\\frac{\\sigma_1}{\\sigma_2}\\,(x_2-\\mu_2 ) ]                   \\right ]   \\right\\ } \\nonumber\\\\ \\!\\ ! & \\propto&\\!\\ !   \\exp \\left\\ {    -\\frac{1}{2\\,(1-\\rho_{12}^2)\\,\\sigma_1 ^ 2 }     \\left [ x_1 ^ 2 - 2\\,x_1\\ , [ \\mu_1     + \\rho_{12}\\,\\frac{\\sigma_1}{\\sigma_2}\\,(x_2-\\mu_2 ) ]               + [ \\mu_1       + \\rho_{12}\\,\\frac{\\sigma_1}{\\sigma_2}\\,(x_2-\\mu_2)]^2    \\right ]   \\right\\ }   \\nonumber   \\nonumber\\\\ \\!\\ ! & \\propto&\\!\\ !   \\exp \\left\\ {    -\\frac{1}{2\\,(1-\\rho_{12}^2)\\,\\sigma_1 ^ 2 }     \\left ( x_1   - [ \\mu_1       + \\rho_{12}\\,\\frac{\\sigma_1}{\\sigma_2}\\,(x_2-\\mu_2 ) ]    \\right)^2   \\right\\ }   \\end{aligned}\\ ] ] in which we recognize a gaussian with expected value @xmath69 and standard deviation @xmath70 ( and therefore the normalization factor can be obtained without any calculation ) .",
    "linear transformations of variables are important because there are several practical problems to which they apply .",
    "there are also other cases in which the transformation is not rigorously linear , but it can be still approximately linearized in the region of interest , where the probability mass is concentrated .",
    "there are well known theorems that relate expected values and covariance matrix of the _ input quantities _ to expected values and covariance matrix of the _",
    "output quantities_. the most famous case is when a single output quantity @xmath71 depends on several variables @xmath72 .",
    "so , given @xmath73 there is a relation which always holds , no matter if the @xmath74 are independent or not and whichever are the pdf s which describe them : @xmath75 & = & \\sum_i c_i\\ , \\mbox{e}[x_i]\\,.\\end{aligned}\\ ] ] in the special case that the @xmath74 are also * independent * , we have @xmath76 & = & \\sum_i c_i^2\\ , \\mbox{var}[x_i].\\end{aligned}\\ ] ] instead it is not always simple to calculate the pdf of @xmath71 in the most general case .",
    "there are however two remarkable cases , which we assume known and just recall them here , in which is @xmath71 is normally distributed :    1 .   * linear combinations of normally distributed variables *",
    "are still normal ; 2 .",
    "the * central limit theorem * states that if we have ` many ' that goes to infinity '' ! some practice",
    "is then needed to judge when it is large enough  often @xmath34 around 10 is can be considered ` large ' , in other cases even @xmath77 is not enough ! ( think of one million of variables described by a poisson distribution with @xmath78 . ) ] * independent variables * their linear combination is normally distributed with variance equal to @xmath79 $ ] if none of the non - normal components dominates the overall variance , i.e. if @xmath80 \\ll         \\sum_i c_i^2\\ , \\mbox{var}[x_i]$ ] , where @xmath81 denotes any of those non - normal components .    since in this paper",
    "we only stick to normal pdf s , the only task will be to evaluate the covariance matrix of the set of variables of interest , depending on the problem .    the general transformation from @xmath34 input variables to @xmath82 output variable",
    "is given by @xmath83 or , in a compact form that use the _ transformation matrix _ @xmath84 , whose elements are the @xmath85 , @xmath86 expected value and covariance matrix of the output quantities are given by @xmath87 & = &   { \\mbox{\\boldmath$c$}}\\ , \\mbox{e}[{\\mbox{\\boldmath$x$ } } ] \\\\ { \\mbox{\\boldmath$v$}}_y & = & { \\mbox{\\boldmath$c$}}\\,v_x\\,{\\mbox{\\boldmath$c$}}^t\\end{aligned}\\ ] ] for example , if @xmath88 , with @xmath89 , @xmath90 and @xmath91 , and the transformation rule is given by @xmath92 i.e. @xmath93 we get in r : .",
    "but it has a third parameter fun which which it is possible to evaluate different function on the ` grid ' defined by the cartesian product of the two vector .",
    "try for example +   +   +   + ] let us get a visual representation of the probability distribution of @xmath72 and @xmath94 using this time , instead of iso - pdf ellipses , points in the @xmath95 plane produced by the random generator provided by the package mnormt ( see result in fig .",
    "[ fig : esempio_tras.eps ] ) :      instead , a less known rule is that which gives the covariance matrix of a conditional distribution with a number of variables above two .",
    "for example we might have 5 variables @xmath96 and could be interested in the expected values and the covariance matrix of @xmath97 , given @xmath98 .",
    "problems of this kind might look a mere mathematical curiosity , but they are indeed important to understand how we learn from data and we make probabilistic predictions using probability theory .    compact formulae to solve this problems can be found in ref .  @xcite .",
    "if we partition @xmath99 and @xmath33 into the the subsets of variable on which we want to condition and the other ones , i.e. @xmath100 the result is @xmath101   & = & { \\mbox{\\boldmath$\\mu$}}_1 + { \\mbox{\\boldmath$v$}}_{12}\\,{\\mbox{\\boldmath$v$}}_{22}^{-1}\\,({\\mbox{\\boldmath$a$}}-{\\mbox{\\boldmath$\\mu$}}_2 ) \\label{eq : eaton_e } \\\\ & & \\nonumber \\\\ { \\mbox{\\boldmath$v$}}\\left[\\left.{\\mbox{\\boldmath$x$}}_1\\right|_{{\\mbox{\\boldmath$x$}}_2}\\right ] & = &   { \\mbox{\\boldmath$v$}}_{11 } - { \\mbox{\\boldmath$v$}}_{12}\\,{\\mbox{\\boldmath$v$}}_{22}^{-1}\\,{\\mbox{\\boldmath$v$}}_{21}\\label{eq : eaton_v}\\end{aligned}\\ ] ] ( and analogous formulae for @xmath102 $ ] and @xmath103 $ ] . )    in the case of a bivariate distributions we recover easily eqs .",
    "( [ eq : x1_cond1_e])-([eq : x1_cond1_var ] ) , as it follows .    expected value : : :    @xmath104 is the off - diagonal term    @xmath105 , while    @xmath106 is equal to    @xmath107 .",
    "( [ eq : eaton_e ] ) becomes then    @xmath108 & = &             \\mu_1 + \\rho_{12}\\,\\sigma_1\\,\\sigma_2\\ , \\frac{1}{\\sigma_2 ^ 2 }                    \\,(a - \\mu_2 ) \\nonumber \\\\     & = &     \\mu_1 + \\rho_{12}\\,\\frac{\\sigma_1}{\\sigma_2 }                    \\,(a - \\mu_2 )          \\end{aligned}\\ ] ] variance : : :    the remaining two terms of interest are also very simple :    @xmath109 is @xmath110 ,    while @xmath111 , equal to    @xmath104 , is    @xmath105 .",
    "it follows    @xmath112 & = & \\sigma_1 ^ 2 -                        \\rho_{12}\\,\\sigma_1\\,\\sigma_2\\ , \\frac{1}{\\sigma_2 ^ 2 }                      \\ ,   \\rho_{12}\\,\\sigma_1\\,\\sigma_2 \\nonumber \\\\       & = &   \\sigma_1 ^ 2 - \\rho_{12}^2\\,\\sigma_1 ^ 2 \\nonumber \\\\      & = &   \\sigma_1 ^ 2\\,(1-\\rho_{12}^2 ) .",
    "\\end{aligned}\\ ] ]    * note * that , while the conditioned expected value depends on the conditionand vector @xmath113 , the conditioned variance .",
    "at this point , having set up all our tools , here is the r function which implements the above formulae : of sec .  [",
    "sec : fits ] , a more numerically stable way to invert a matrix in r would be using the choleski decomposition , but for the purpose of this note the difference is slightly appreciable . ]",
    "the conditionand vector @xmath114 has to contain numbers in the positions corresponding to the variables on which we want to condition , and na , that is ` not available ' or ` unknown ' , in the others , as we shall see in the examples .",
    "the code of parameter full is to return the vector of expectation and the covariance having the initial dimensionality .",
    "the expectation of the variable used as condition is the condition itself .",
    "all elements of the covariance matrix related to conditionals are instead zero , and the utility of this convention will be clear going through the examples .",
    "let us try with a simple case of two normal quantities @xmath88 of section [ ss : linearcombinations ] .",
    "the question is how our uncertainty on @xmath115 change if _ we assume _ @xmath116 : the effect of the conditions to shift the expected value of @xmath115 from 2 to 1.68 and to squeeze its _ standard uncertainty _ to 0.12 .",
    "if we provide our result in the conventional form `` expected value @xmath117 standard uncertainty '' , the assumption ( or ` knowledge ' ) @xmath118 updates our ` knowledge ' about @xmath1 from ` @xmath119 ' to ` @xmath120 ' .",
    "let us go back to the first diagram of fig .",
    "[ fig : modelli_base ] , that we repeat here for convenience :    this diagram describes the situation in which we have the physical quantity @xmath1 , that is a parameter of our physical model of reality , and the reading on an instrument , @xmath2 , _ caused _ by @xmath1 .    the instrument has been well calibrate , such to give @xmath2 around @xmath1 , but it is not perfect , as usual . in other words , even if we knew exactly the value @xmath61 we were not sure about the value @xmath54 we would read . for simplicity ,",
    "let us model this uncertainty by a normal distribution , i.e. @xmath121 but we usually do not know @xmath1 , and therefore we are even more uncertain about what we shall read on the instrument .",
    "in fact we are dealing with a joint distribution describing the joint uncertainty about the two quantities , that is @xmath122 our knowledge about @xmath2 will be given , instead , by @xmath123 , a distribution characterized by @xmath124 \\ne \\mbox{var}[\\left.x_2\\right|_{x_1}]$ ] .",
    "it is convenient to model our uncertainty about @xmath1 with a normal distribution , with a standard deviation @xmath11 much larger than @xmath14  if we make a measurement we want to gain knowledge about that quantity !  and centered around the values we roughly expect .    in order to simplify the calculations , in the exercise that follows",
    "let us assume that @xmath1 is centered around zero .",
    "we shall see later how to get rid of this limitation .",
    "the joint distribution @xmath125 is then given by @xmath126\\times \\frac{1}{\\sqrt{2\\,\\pi}\\,\\sigma_1}\\ ,",
    "\\exp\\left[-\\frac{x_1 ^ 2}{2\\,\\sigma_1 ^ 2}\\right ] \\label{eq : joint_x1x1}\\end{aligned}\\ ] ] as an exercise , let us see how to evaluate @xmath125 .",
    "the trick , already applied before , is to manipulate the terms in the exponent in order to recover a well known pattern . here",
    "are the details , starting from ( [ eq : joint_x1x1 ] ) rewritten dropping all irrelevant factors : @xmath127\\\\ & \\propto & \\exp\\left [ -\\frac{1}{2}\\left (                         \\frac{x_2 ^ 2 - 2\\,x_1x_2+x_1 ^ 2}{\\sigma_{2|1}^2 }                        + \\frac{x_1 ^ 2}{\\sigma_1 ^ 2 }                          \\right)\\right ] \\\\ & \\propto & \\exp\\left [   -\\frac{1}{2}\\left (            \\frac{x_2 ^ 2}{\\sigma_{2|1}^2 }            -\\frac{2\\,x_1x_2}{\\sigma_{2|1}^2 }           + x_1 ^ 2\\cdot\\left(\\frac{1}{\\sigma_{2|1}^2 } + \\frac{1}{\\sigma_{1}^2}\\right )            \\right)\\right ] \\\\ & \\propto & \\exp\\left [   -\\frac{1}{2}\\left (            \\frac{x_2 ^ 2}{\\sigma_{2|1}^2 }            -\\frac{2\\,x_1x_2}{\\sigma_{2|1}^2 }           + x_1 ^ 2\\cdot\\frac{\\sigma_{2|1}^2+\\sigma_{1}^2 }                          { \\sigma_{2|1}^2\\cdot\\sigma_{1}^2 }           \\right)\\right ] \\\\ & \\propto & \\exp\\left [   -\\frac{1}{2}\\,\\frac{\\sigma_{2|1}^2+\\sigma_{1}^2 }                                           { \\sigma_{2|1}^2 }            \\,\\left (            \\frac{x_2 ^ 2}{\\sigma_{2|1}^2+\\sigma_1 ^ 2 }            -\\frac{2\\,x_1x_2}{\\sigma_{2|1}^2+\\sigma_1 ^ 2 }           + \\frac{x_1 ^ 2}{\\sigma_{1}^2 }           \\right)\\right ] \\\\ & \\propto & \\exp\\left [   -\\frac{1}{2}\\,\\frac{1}{\\frac{\\sigma_{2|1}^2 }                                              { \\sigma_{2|1}^2+\\sigma_{1}^2 }                                           }            \\,\\left (            \\frac{x_2 ^ 2}{\\sigma_{2|1}^2+\\sigma_1 ^ 2 }            -\\frac{2\\,x_1x_2}{\\sigma_{2|1}^2+\\sigma_1",
    "^ 2 }           + \\frac{x_1 ^ 2}{\\sigma_{1}^2 }           \\right)\\right]\\end{aligned}\\ ] ] in this expression we recognize a bivariate distribution centered around @xmath128 , provided we interpret @xmath129 and after having checked the consistency of the terms multiplying @xmath130 .",
    "indeed we have @xmath131 and then the second term within parenthesis can be rewritten as @xmath132 then @xmath133\\end{aligned}\\ ] ] is definitively a bivariate normal distribution with @xmath134 as a cross check , let us evaluate expected value and variance of @xmath2 if we assume a certain value of @xmath1 , for example @xmath135 : @xmath136 & = & 0 +   \\frac{\\sigma_1 ^ 2}{\\sigma_1 ^ 2}\\cdot(x_1 - 0 )                                    = x_1\\\\ \\mbox{var}[\\left.x_2\\right|_{x_1=x_1 } ] & = &   \\sigma_1 ^ 2 + \\sigma_{2|1}^2    -    \\frac{\\sigma_1 ^ 2}{\\sigma_1 ^ 2 } \\,\\sigma_1 ^ 2 =   \\sigma_{2|1}^2 \\,,\\end{aligned}\\ ] ] as it should be : provided we know the value of @xmath1 our expectation of @xmath2 is around its value , with standard uncertainty @xmath14 .",
    "more interesting is the other way around , that is indeed the purpose of the experiment : how our knowledge about @xmath1 is modified by @xmath5 : @xmath137 & = & 0 +   \\frac{\\sigma_1 ^ 2}{\\sigma_1 ^ 2+\\sigma_{2|1}^2}\\cdot(x_2 - 0 )                      = x_2 \\cdot \\frac{1}{1+\\sigma_{2|1}^2/\\sigma_1 ^ 2 } \\label{eq : e_x1|x2 } \\\\",
    "\\mbox{var}[\\left.x_1\\right|_{x_2=x_2 } ] & = &   \\sigma_1 ^ 2     -    \\frac{\\sigma_1 ^ 2}{\\sigma_1 ^ 2 + \\sigma_{2|1}^2 } \\,\\sigma_1 ^ 2   = \\sigma_{1|2}^2   \\cdot \\frac{1}{1+\\sigma_{2|1}^2/\\sigma_1 ^ 2 } \\",
    ", , \\label{eq : var_x1|x2}\\end{aligned}\\ ] ] contrary to the first case , this second result is initially not very intuitive : the expected value of @xmath1 is not exactly equal to the ` observed ' value @xmath54 , unless @xmath11 , that models our _ prior standard uncertainty _ about @xmath1 , is much larger than the experimental resolution @xmath14 .",
    "similarly , the _ final standard uncertainty _ is in general a smaller than @xmath14 , unless , again , @xmath138.[multiblock footnote omitted ] although initially surprising , these result are in qualitative agreement with the good sense of experienced physicists  @xcite .",
    "the next step is to see what happens when we are in the conditions to make several _ independent measurements _ on the same quantity @xmath1 , possibly with different instruments , each one characterized by a conditional standard uncertainty @xmath139 and perfectly calibrated , that is @xmath140 = x_1 $ ] .",
    "the situation can be illustrated with the diagram at the center of fig .",
    "[ fig : modelli_base ] , reported here for convenience , extended to other observations :    we have learned that if we are able to build up the covariance matrix of the joint distribution @xmath141 the problem is readily solved , at least in the normal approximations we are using throughout the paper .    in principle",
    "we should repeat the previous exercise to evaluate , sticking to the first two observations @xmath54 and @xmath142 , @xmath143 where in the last step we have made explicit that @xmath144 does not depend on @xmath2 , once @xmath1 is known .",
    "but this does not implies that @xmath2 and @xmath17 are independent , as we shall see later ! they are simply _ conditionally independent _ , i.e. independent under the ( to be meant in general as an _ hypothesis _ ) that @xmath1 has a precisely known value .    in reality",
    "we do not need to go through a similar derivation , that indeed _ was just an exercise_. the easy solution arises , going back to the previous case , noting that the _ observation _",
    "@xmath145 is the sum of the value of the physics quantity @xmath146 and the instrumental _ error _ @xmath147 ( a ` noise ' , as you might like to see it ) , i.e. @xmath148 with @xmath147 modelled , as usual , by a normal distribution , that is , in general @xmath149 the general uncertain vector @xmath72 will be then @xmath150 , as clarified by the following diagram :    these are then the first terms the transformation rules : @xmath151 from which the calculation of the covariance matrix is straightforward :    * the @xmath152-th element diagonal is given by the variance of @xmath74 , that is @xmath110 , @xmath153 , and so on ; * the off - diagonal elements are all equal to @xmath110 , because the only element in common in all linear combinations is @xmath146 .    hence here is the covariance matrix of interest : @xmath154      at this point , instead of trying to get analytic formulae for all conditional probabilities of interest , we prefer to use the properties of the multivariate normal distribution implemented in the function norm.mult.cond ( ) seen before . and ,",
    "since the game is now automatic , we enlarge our space to 6 variables , @xmath1 for the ` true value ' and @xmath2-@xmath155 for four possible ` readings ' .",
    "although it is not any longer needed , we still set out prior central value about @xmath1 around @xmath156 , which is equivalent to set to 0 all expected values .",
    "( for didactic purposes we have set @xmath11 _ only _ 10 larger than the experimental resolutions @xmath139 , as we shall discuss commenting the results . ) here is the r code , with some comments : as we can see , all variables are correlated !",
    "the reason is very simple : any precise information we get about one of them changes the pdf of all others . in physics terms",
    ", a reading on a instrument changes our opinion about the value of the quantity of interest as well as of all other readings we have not yet done ( or we not yet aware of their values  in probability theory what matters is not time ordering , but ignorance ) .",
    "let us now see what happens if we condition on a * precise value of the true value * @xmath157 , for example : as we see , the expected values are all equal , @xmath1 is not longer uncertain , and all other variables become _ independent _ , more precisely `` conditional independent ''    let s now see what happens if we condition instead on the * observation * : the ` measurement ' has had the effect of changing all our expectations , becoming all ` practically equal ' to the observed value of @xmath158 .",
    "but the uncertainties about the possible future observations are different than that of the true value @xmath1 .",
    "they are in fact larger by a factor @xmath159 ( see also fig .  [",
    "fig : norm_mult_cond_exp_x2 ] ) .",
    "the reason is that @xmath2 and @xmath17 ( i.e. @xmath160 and @xmath161 ) and all other possible readings @xmath162 , @xmath163 and @xmath164 ` communicate ' each other via @xmath1 : their uncertainty is than the combination ( quadratic combination ! ) of that assigned to @xmath1 and that of the readings @xmath74 if we new exactly @xmath1 ( that is @xmath165 ) .",
    "let us see if we add * another observation * , e.g. , that is we recondition now simultaneously on @xmath166 and @xmath167 as we can see , after the second observation the expected values are * practically * equal to 1.5 , average between the two readings .",
    "the uncertainty about the true value has decreased by a factor 1.41 , that is @xmath159 , while the uncertainties about the forecasting decrease only by a factor 1.15 , going from 1.41 to 1.22 .",
    "this latter number can be understood as @xmath168 , as it will be justified in a while .",
    "let us see what happens if we suppose that also @xmath1 is precisely known , namely @xmath169 ( different from @xmath170 previously used , not only `` just to change '' but also to use a value different from that of @xmath2 and @xmath17 ) : if @xmath1 is perfectly known the observations @xmath2 and @xmath17 are irrelevant , as it has to be . and",
    "@xmath17 are ` very far ' from @xmath1 ? in this simple model",
    "we are using , there is little to do , because any observation from minus infinite to plus infinite is never incompatible with a any gaussian .",
    "but we know by experience that something strange might be happened .",
    "this case we need to put in mathematical form the model we have in mind . ]    finally , going back to the physical case of interest , in which @xmath1 is unknown , let us add a * third observation * , e.g. as we can see , the value of @xmath1 is with very good approximation the average of the three observations , that is 1 , with a the standard uncertainty decreasing with @xmath171 , passing from 1.00 to 0.71 to 0.58 .",
    "this is because the three pieces of information enter with the same weight , since @xmath139 , related to the ` precision of the instrument ' , is the same in all cases and equal to 1 .",
    "as far as the prediction of future observations , obviously they must be centered around the value we believe @xmath1 is , at the best of our knowledge , a value which changes with the observations . as far as uncertainty and correlation coefficient are concerned , they decrease as follows ( starting from the very beginning , before any observation ) :    standard uncertainty : : :    10.05 , 1.41 , 1.22 , 1.15 . +    we can see that they are a quadratic combination of the uncertainty    with which we know @xmath1 and that with which we expect the    observation given a precise value of @xmath1 .",
    "if we indicate    the state of information at time @xmath172 as    @xmath173 , the rule is @xmath174 & = & \\mbox{var}[x_1\\,|\\,i(t ) ] + \\sigma_{i|1}^2\\ ,",
    ".          \\end{aligned}\\ ] ] asymptotically , when after many measurements the    determination of @xmath1 is very accurate , it only remains    @xmath175 , as it has to be .",
    "correlation coefficient : : :    0.990 , 0.50 , 0.33 , 0.25 .",
    "+    it is initially very high because any new observation changes    dramatically our expectation about the others .",
    "but then , when we have    already made several observations , a new one has only very little    effect on our forecasting .",
    "asymptotically , when we have made a very    large number of observations and @xmath1 is very well    ` determined ' , all future observations become essentially    `` conditionally independent '' .      at this point",
    "the game can be continued with different options .",
    "one has only to re - build the initial covariance matrix and play changing the conditions .",
    "an interesting exercise is certainly that of increasing @xmath11 , for example to 100 , i.e. 100 times large than the ` precision ' of our instrument , or even 1000 , to see how our conclusions change .",
    "the result will be that true value and future measurements are ` practically ' only determined by the observations .",
    "it could also interesting to see what happens if the different observations come from instruments having different precisions .",
    "finally , one could produce a ( relatively ) large random sample of observations measuring the same true value .",
    "being @xmath82 the number of observations , the dimensionality of our problem will be @xmath176 , because we have to add  obviously  @xmath1 and we want to have at least two future observations ( @xmath177 and @xmath177 ) in order to check their degree of correlation . here is the r session in which we have been playing with a sample of 100 observations ( for obvious reasons we shall focus only on the uncertain variables , i.e. @xmath178 , @xmath179 and @xmath180 ) : expected values of the true value and of the future measurements are now equal to the average of the sample , with excellent approximation .",
    "this is due to the fact that the initial uncertainty of 10 is in this case much larger than the final one of 0.10 .",
    "this value is indeed equal to @xmath181 , the famous standard deviation of the mean .",
    "this means that the standard deviation of the sample , that is +   + [ 1 ]  0.08263812 + is not used .",
    "this is not a surprise , since in our model @xmath139 are assumed to be perfectly known.s is not any longer linear , thus going beyond the purpose of this note . ]",
    "we see that the uncertainty on the future observations is a bit larger than that on the true value , as it must be .",
    "this is because they depends on the uncertain value of the true value and the experimental resolution , combining in quadrature ( @xmath182 ) .",
    "the correlations become small , in particular those among the future observations , which practically become ` conditionally independent ' .",
    "indeed , the covariance matrix is that shown in eq .",
    "( [ eq : covmatx1-x2_9 ] ) , with @xmath11 replaced by @xmath183 ( what matters is the uncertainty about @xmath1 , not its source ! ) .",
    "let us now move to the second diagram of fig .",
    "[ fig : modelli_base ] , which we repeat her for convenience , in    which @xmath17 is caused by both @xmath1 and @xmath2 : this diagram can model the presence of a systematic effect , because we expect that the possible values of @xmath17 are caused by both @xmath1 and @xmath2 , and it will be then influenced by how uncertain is the quantity @xmath2 that acts as a systematic .",
    "the simplest case of systematic effect is an additive one , of unknown value , but with expected value 0 ( the instrument has been calibrated ` at the best ' ! ) and a standard uncertainty @xmath184 .",
    "needless to say , we also model this uncertainty with a normal distribution , with much simplification in the calculations ( and also because this is often the case ) .",
    "the model can be extended to several observations , as shown in the left diagram of fig .",
    "[ fig : modelli_syst ] . in the figure",
    "it is also shown a different interpretation of the effect of the systematic error , which is very close to the physicist intuition .",
    "the observations @xmath17 , @xmath185 and @xmath186 are normally distributed around a kind of ` virtual state ' @xmath187 determined by the _",
    "unknown _ true value @xmath1 and the _ unknown _ offset @xmath2 , i.e. the true ` zero ' of the instrument . the transformation rule to build the initial covariance matrix will be then , starting from symbols that have a physical meaning [ value @xmath146 , ` zero ' @xmath188 , and the others as in eqs .",
    "( [ eq : x3-o2e2])-([eq : x3-o2e2 ] ] @xmath189 the calculation of the variances is trivial .",
    "as far as the covariances we have @xmath190 & = & 0   \\\\ \\mbox{cov}[x_1 , x_i ] & = & \\sigma_1 ^ 2\\hspace{1.4cm}(i > 2 ) \\\\",
    "\\mbox{cov}[x_2 , x_i ] & = & \\sigma_2 ^ 2\\hspace{1.4cm}(i > 2 ) \\\\",
    "\\mbox{cov}[x_i , x_j ] & = & \\sigma_1 ^ 2 + \\sigma_2 ^ 2       \\hspace{0.5cm}(i>2,\\ j>2)\\end{aligned}\\ ] ] this is then the covariance matrix of interest , limited to the five variables shown in the figure ( and than it is easy to continue ) : @xmath191 where @xmath192 stands for @xmath193 , i.e. @xmath194 , and so on , later also indicated with the short hand @xmath195 .",
    "from such an interesting matrix we can expect interesting results , useful to _ train our intuition_. but before analyzing some cases , as done in the previous section , let us make the exercise to build up the covariance matrix in a different way .",
    "the transformation rules ( [ eq : y1_x1])-([eq : y5_x5 ] ) can be rewritten using the transformation matrix @xmath196 to be applied to the diagonal matrix of the independent variables , @xmath197 applying then the transformation rule of the covariance matrix we reobtain the above result  an implementation in r will be shown in the next subsection .",
    "let set up the covariance matrix for 5 possible ` observations ' let us also show the * alternative way to build up the covariance matrix *    .... > c < - matrix(rep(0 , n*n ) , c(n , n ) )               # transf .",
    "matrix > c[,1 ] < - c(1 , 0 , rep(1 , n-2 ) ) > c[,2 ] < - c(0 , rep(1 , n-1 ) ) > diag(c ) < - rep(1 , n ) > c       [ , 1 ] [ , 2 ] [ , 3 ] [ , 4 ] [ , 5 ] [ , 6 ] [ , 7 ] [ 1 , ]     1     0     0     0     0     0     0 [ 2 , ]     0     1     0     0     0     0     0 [ 3 , ]     1     1     1     0     0     0     0 [ 4 , ]     1     1     0     1     0     0     0 [ 5 , ]     1     1     0     0     1     0     0 [ 6 , ]     1     1     0     0     0     1     0 [ 7 , ]     1     1     0     0     0     0     1 > v0 < - matrix(rep(0 , n*n ) , c(n , n ) )      # initial diagonal matrix > diag(v0 ) < -   sigma^2 > ( v < - c % * % v0 % * % t(c ) )             # joint covariance matrix       [ , 1 ] [ , 2 ] [ , 3 ] [ , 4 ] [ , 5 ] [ , 6 ] [ , 7 ] [ 1 , ]   100     0   100   100   100   100   100 [ 2 , ]     0     1     1     1     1     1     1 [ 3 , ]   100     1   102   101   101   101   101 [ 4 , ]   100     1   101   102   101   101   101 [ 5 , ]   100     1   101   101   102   101   101 [ 6 , ]   100     1   101   101   101   102   101 [ 7 , ]   100     1   101   101   101   101   102 ....    as we see the result is identical to that obtained setting the elements ` by hand ' .    then let us now repeat the steps previously followed without systematic offset .      .... > ( mu.c < - c(2 , rep(na , n-1 ) ) ) [ 1 ]   2 na na na na na na > ( out < - norm.mult.cond(mu , v , mu.c ) ) $ mu [ 1 ] 2 0 2 2 2 2 2    $ v       [ , 1 ] [ , 2 ] [ , 3 ] [ , 4 ] [ , 5 ] [ , 6 ] [ , 7 ] [ 1 , ]     0     0     0     0     0     0     0 [ 2 , ]     0     1     1     1     1     1     1 [ 3 , ]     0     1     2     1     1     1     1 [ 4 , ]     0     1     1     2     1     1     1 [ 5 , ]     0     1     1     1     2     1     1 [ 6 , ]     0     1     1     1     1     2     1 [ 7 , ]     0     1     1     1     1     1     2 > ( out.s < - sqrt(diag(out$v ) ) ) [ 1 ] 0.000000 1.000000 1.414214 1.414214 1.414214",
    "1.414214 1.414214 > round ( out$v / outer(out.s , out.s ) , 3 )        [ , 1 ]   [ , 2 ]   [ , 3 ]   [ , 4 ]   [ , 5 ]   [ , 6 ]   [ , 7 ] [ 1 , ]   nan    nan    nan    nan    nan    nan    nan [ 2 , ]   nan 1.000 0.707 0.707 0.707 0.707 0.707 [ 3 , ]   nan 0.707 1.000 0.500 0.500 0.500 0.500 [ 4 , ]   nan 0.707 0.500 1.000 0.500 0.500 0.500 [ 5 , ]   nan 0.707 0.500 0.500 1.000 0.500 0.500 [ 6 , ]   nan 0.707 0.500 0.500 0.500 1.000 0.500 [ 7 , ]   nan 0.707 0.500 0.500 0.500 0.500 1.000 ....    the condition on the ` true value ' changes the values of the observables to its value , but it does not affect the offset , which has a role in the uncertainty of the future observations as well in their correlation .",
    "in fact , contrary to the case see in the previous section without uncertain offset , they are not any longer independent .",
    "they would become independent if also the offset were known ( try for example with `` mu.c < - c(2 , 0 , rep(na , n-2 ) ) '' to see the difference , or even better with `` mu.c < - c(2 , 1 , rep(na , n-2 ) ) '' ) .      ....",
    "> ( mu.c < - c(na , na , 2 , rep(na , n-3 ) ) ) [ 1 ] na na   2 na na na na > out < - norm.mult.cond(mu , v , mu.c ) > round ( out$mu , 4 ) [ 1 ] 1.9608 0.0196 2.0000 1.9804 1.9804",
    "1.9804 1.9804 > round ( out$v , 4 )          [ , 1 ]     [ , 2 ] [ , 3 ]    [ , 4 ]    [ , 5 ]    [ , 6 ]    [ , 7 ] [ 1 , ]   1.9608 -0.9804     0 0.9804 0.9804 0.9804 0.9804 [ 2 , ] -0.9804   0.9902     0 0.0098 0.0098 0.0098 0.0098 [ 3 , ]   0.0000   0.0000     0 0.0000 0.0000 0.0000 0.0000 [ 4 , ]   0.9804   0.0098     0 1.9902 0.9902 0.9902 0.9902 [ 5 , ]   0.9804   0.0098     0 0.9902 1.9902 0.9902 0.9902 [ 6 , ]   0.9804   0.0098     0 0.9902 0.9902 1.9902 0.9902 [ 7 , ]   0.9804   0.0098     0 0.9902 0.9902",
    "0.9902 1.9902 > round ( out.s < - sqrt(diag(out$v ) ) , 4 )   [ 1 ] 1.4003 0.9951 0.0000 1.4107 1.4107",
    "1.4107 1.4107 > round ( out$v / outer(out.s , out.s ) , 3 )         [ , 1 ]    [ , 2 ] [ , 3 ]   [ , 4 ]   [ , 5 ]   [ , 6 ]   [ , 7 ] [ 1 , ]   1.000 -0.704   nan 0.496 0.496 0.496 0.496 [ 2 , ] -0.704   1.000   nan 0.007 0.007 0.007 0.007 [ 3 , ]     nan     nan   nan    nan    nan    nan    nan [ 4 , ]   0.496   0.007   nan 1.000 0.498 0.498 0.498 [ 5 , ]   0.496   0.007   nan 0.498 1.000 0.498 0.498 [ 6 , ]   0.496   0.007   nan 0.498 0.498 1.000 0.498 [ 7 , ]   0.496   0.007   nan 0.498 0.498 0.498 1.000 ....    to understand the result we need to compare it with the case without uncertainty uncertainty . in that case",
    "we had @xmath199 .",
    "now we have @xmath200 .",
    "the difference , although practically irrelevant , is conceptually important .",
    "it is indeed equal to the expected value of the offset ( precisely @xmath201 ) .",
    "this is because the role of the observation is to give us an information about @xmath202 , sum of the true value and the offset .",
    "the fact that we use the observations to update our knowledge on the true value is simply because the offset is a priori better known that the true value , as it is well understood by experienced physicists : if the calibration is poor the instrument can not be used for ` measurements ' .",
    "note also the correlation that now appears between @xmath1 and @xmath2 , and in particular its negative sign : the value of the true value could increase at the ` expenses ' of the offset , and the other way around .      ....",
    "> ( mu.c < - c(na , na , 2 , 1 , rep(na , n-4 ) ) ) [ 1 ] na na   2   1 na na na > out < - norm.mult.cond(mu , v , mu.c )   > round ( out$mu , 4 ) [ 1 ] 1.4778 0.0148 2.0000 1.0000 1.4926",
    "1.4926 1.4926 > round ( out$v , 4 )               [ , 1 ]     [ , 2 ] [ , 3 ] [ , 4 ]    [ , 5 ]    [ , 6 ]    [ , 7 ] [ 1 , ]   1.4778 -0.9852     0     0 0.4926 0.4926 0.4926 [ 2 , ] -0.9852   0.9901     0     0 0.0049 0.0049 0.0049 [ 3 , ]   0.0000   0.0000     0     0 0.0000 0.0000 0.0000 [ 4 , ]   0.0000   0.0000     0     0 0.0000 0.0000 0.0000 [ 5 , ]   0.4926   0.0049     0",
    "0 1.4975 0.4975 0.4975 [ 6 , ]   0.4926   0.0049     0",
    "0 0.4975 1.4975 0.4975 [ 7 , ]   0.4926   0.0049     0     0 0.4975 0.4975 1.4975 > round ( out.s < - sqrt(diag(out$v ) ) , 4 )   [ 1 ] 1.2157 0.9951 0.0000 0.0000 1.2237 1.2237 1.2237",
    "> round ( out$v / outer(out.s , out.s ) , 3 )          [ , 1 ]    [ , 2 ] [ , 3 ] [ , 4 ]   [ , 5 ]   [ , 6 ]   [ , 7 ] [ 1 , ]   1.000 -0.814   nan   nan 0.331 0.331 0.331 [ 2 , ] -0.814   1.000   nan   nan 0.004 0.004 0.004 [ 3 , ]     nan     nan   nan   nan    nan    nan    nan [ 4 , ]     nan     nan   nan   nan    nan    nan    nan [ 5 , ]   0.331   0.004   nan   nan 1.000 0.332 0.332",
    "[ 6 , ]   0.331   0.004   nan   nan 0.332 1.000 0.332 [ 7 , ]",
    "0.331   0.004   nan   nan 0.332 0.332 1.000 ....    the only new effect we observe is the increase ( in module ) of the correlation coefficient between true value and offset .",
    "this is due to the fact that the increased number of observation has increased the constrain between the two quantities",
    ". it will increase more if we use further observations , for example conditioning on \" mu.c < - c(na , na , 2 , 1 , 1.5 , 2.2 , 0.5 )  , or decreasing the standard deviations @xmath195 .",
    "for example if we set all @xmath195 to 0.1 , the same conditioning on @xmath17 and @xmath17 would produce a correlation coefficient of @xmath204 .",
    "asymptotically there will be a deterministic constrain between @xmath1 and @xmath2 of the kind @xmath205 , and the two variables become _ logically dependent_.      what happens if we instead fix the value of the true value and some values of the observables ?",
    "in this case we update our information on the offset .",
    "let us see the case in which we fix the value of the true value at 2 , and the average of the two observations at 1.5 .    ....",
    "> ( mu.c < - c(2 , na , 2 , 1 , rep(na , n-4 ) ) ) [ 1 ]   2 na   2   1 na na na > out < - norm.mult.cond(mu , v , mu.c )   > round ( out$mu , 4 ) [ 1 ]   2.0000 -0.3333   2.0000   1.0000   1.6667   1.6667",
    "1.6667 > round ( out$v , 4 )          [ , 1 ]    [ , 2 ] [ , 3 ] [ , 4 ]    [ , 5 ]    [ , 6 ]    [ , 7 ] [ 1 , ]     0 0.0000     0     0 0.0000 0.0000 0.0000 [ 2 , ]     0 0.3333     0     0 0.3333 0.3333 0.3333 [ 3 , ]     0 0.0000     0     0 0.0000 0.0000 0.0000 [ 4 , ]     0 0.0000     0     0 0.0000 0.0000 0.0000 [ 5 , ]     0 0.3333     0     0 1.3333 0.3333 0.3333 [ 6 , ]     0 0.3333     0     0 0.3333 1.3333 0.3333 [ 7 , ]     0 0.3333",
    "0     0 0.3333 0.3333 1.3333 > round ( out.s < - sqrt(diag(out$v ) ) , 4 )   [ 1 ] 0.0000 0.5774 0.0000 0.0000 1.1547",
    "1.1547 1.1547 > round ( out$v / outer(out.s , out.s ) , 3 )        [ , 1 ] [ , 2 ] [ , 3 ] [ , 4 ] [ , 5 ] [ , 6 ] [ , 7 ] [ 1 , ]   nan   nan   nan   nan   nan   nan   nan [ 2 , ]   nan   1.0   nan   nan 0.50 0.50 0.50 [ 3 , ]   nan   nan   nan   nan   nan   nan   nan [ 4 , ]   nan   nan   nan   nan   nan   nan   nan [ 5 , ]   nan   0.5   nan   nan 1.00 0.25 0.25 [ 6 , ]   nan   0.5   nan   nan 0.25 1.00 0.25 [ 7 , ]   nan   0.5   nan   nan 0.25 0.25 1.00 ....    as a result , the expected value of the offset becomes @xmath206 , with a standard deviation of 0.58 , against the ( possible ) intuitive guess of @xmath207 ( i.e @xmath208 ) with a standard uncertainty of 0.71 ( i.e. @xmath209 ) .",
    "the reason is that our prior knowledge on the offset had a standard uncertainty of 1 , that has to be taken into account .",
    "indeed it can be easily checked that the ` intuitive ' result would have been recovered if we had a very large uncertainty ( @xmath210 ) .",
    "in fact @xmath206 is the weighted average of the initial value 0 and @xmath207 , with weights equal to @xmath211 and @xmath158 .",
    "the reason is that the result based on reconditioning provides automatically the rule of the weighted average with ` inverse of the variances ' , where the ` variance ' associated to @xmath207 would be that obtained if the prior knowledge on the offset was irrelevant ( i.e. @xmath210 ) .",
    "another interesting issue , very common in experimental physics , is when we make several measurements on homogeneous quantities using the same instrument that , as all instruments , has unavoidable uncertainty in the calibration .",
    "the situation is sketched in the diagrams of fig .",
    "[ fig : x1x2x3-x4x5x6x7 ] , drawn with the different symbols used in the text : @xmath1 and @xmath2 are the true values ; @xmath17 the common offset ; @xmath185 and @xmath186 the independent readings when the instrument is applied to @xmath1 ; @xmath155 and @xmath212 the independent readings when the instrument is applied to @xmath2 .",
    "from this model we can easily build the transformation matrix @xmath84 @xmath213 ( for example it says that row 6 depends on @xmath2 , @xmath17 and @xmath214 ) . applying it to the starting diagonal matrix ( @xmath215 , @xmath216 and @xmath188 are _ initially independent _ ; the various errors @xmath217-@xmath218 are independent ) we get the covariance matrix of the joint multivariate normal of interest : @xmath219 this is a very interesting covariance matrix and we leave the reader the pleasure of exploiting all possibilities . here",
    "we only show a numerical example , with parameters similar to the ones used before for a better understanding , and just discuss a single case of conditioning .    ....",
    "> n=7 ; mux1=0 ; sigmax1=10 ; mux2=0 ; sigmax2=10 ;   # set parameters > muz=0 ; sigmaz=1       > mu",
    "< - c(mux1 , mux2 , muz , rep(mux1+muz,2 ) , rep(mux2+muz,2 ) ) # set expected values > ( sigma < - c(sigmax1 , sigmax1 , sigmaz , rep(1 , n-3 ) ) )    # standard deviations    [ 1 ] 10 10   1   1   1   1   1 > c < - matrix(rep(0 , n*n ) , c(n , n ) )       # tranformation matrix > diag(c ) < - rep(1 , n ) >",
    "c[4 , ] < - c(1 , 0 , 1 , 1 , 0 , 0 , 0 ) > c[5 , ] < - c(1 , 0 , 1 , 0 , 1 , 0 , 0 ) > c[6 , ] < - c(0 , 1 , 1 , 0 , 0 , 1 , 0 ) > c[7 , ] < - c(0 , 1 , 1 , 0 , 0 , 0 , 1 ) > c       [ , 1 ] [ , 2 ] [ , 3 ] [ , 4 ] [ , 5 ] [ , 6 ] [ , 7 ] [ 1 , ]     1     0     0     0     0     0     0 [ 2 , ]     0     1     0     0     0     0     0 [ 3 , ]     0     0     1     0     0     0     0 [ 4 , ]     1     0     1     1     0     0     0 [ 5 , ]     1     0     1     0     1     0     0 [ 6 , ]     0     1     1     0     0     1     0 [ 7 , ]     0     1     1     0     0     0     1 > v0 < - matrix(rep(0 , n*n ) , c(n , n ) )      # covariance matrix     > diag(v0 ) < -   sigma^2 > v < - c % * % v0 % * % t(c ) > v       [ , 1 ] [ , 2 ] [ , 3 ] [ , 4 ] [ , 5 ] [ , 6 ] [ , 7 ] [ 1 , ]   100     0     0   100   100     0     0 [ 2 , ]     0   100     0     0     0   100   100 [ 3 , ]     0     0     1     1     1     1     1 [ 4 , ]   100     0     1   102   101     1     1 [ 5 , ]   100     0     1   101   102     1     1 [ 6 , ]     0   100     1     1     1   102   101 [ 7 , ]     0   100     1     1     1   101   102 > su < - sqrt(diag(v ) )                    # standard uncertainties > round ( v / outer(su , su ) , 4 )              # correlation matrix         [ , 1 ]    [ , 2 ]   [ , 3 ]    [ , 4 ]    [ , 5 ]    [ , 6 ]    [ , 7 ] [ 1 , ] 1.0000 0.0000 0.000 0.9901 0.9901 0.0000 0.0000 [ 2 , ] 0.0000 1.0000 0.000 0.0000 0.0000 0.9901 0.9901 [ 3 , ] 0.0000 0.0000 1.000 0.0990 0.0990 0.0990 0.0990 [ 4 , ] 0.9901 0.0000 0.099 1.0000 0.9902 0.0098 0.0098 [ 5 , ] 0.9901 0.0000 0.099 0.9902 1.0000 0.0098 0.0098 [ 6 , ] 0.0000 0.9901 0.099 0.0098 0.0098 1.0000 0.9902 [ 7 , ] 0.0000 0.9901 0.099 0.0098 0.0098 0.9902 1.0000 ....    now let us assume we have applied our instrument once on @xmath1 and once on @xmath2 , obtaining the readings @xmath203 and @xmath220 , respectively . here is how our knowledge is updated : as expected , @xmath203 sets essentially to 1 the true value @xmath1 and the ` future '  or not yet known !",
    " reading @xmath186 .",
    "similarly , @xmath155 sets essentially to 2 @xmath2 and @xmath212 .",
    "( the difference from the exact value of 1 and 2 , respectively , is due  let us repeat it once again  to the fact that we use , for didactic purposes , initial standard uncertainties @xmath11 and @xmath184 ` relatively small ' , while the uncertainty on the common offset is ` relatively large ' . )",
    "the most interest part of the result is the @xmath221 upper left part of the resulting correlation matrix , which we repeat here : @xmath222 as we have learned in the previous section , the value of the offset gets anticorrelated to the true values .",
    "moreover the two true values get * positively correlated * , as expected : a part of our uncertainty on them is due the imprecise knowledge of the offset , which then affects both values in the same direction .",
    "often our inferences and forecasting are based on averages , instead than on individual values .",
    "it is rather understood that in gaussian samples the inference on the gaussian ` @xmath223 ' is the same if we use the mean rather than the detailed information , due to the so called property of ` statistical sufficiency ' .",
    "it is instead less clear what we should expect for a next mean , based on a sample of the same size of the first one .",
    "for example , very often one ears and reads[multiblock footnote omitted ] something like `` if we have got a mean ( @xmath224 ) and then imagine to repeat a large number of independent samples of the same size ( @xmath34 ) of the ` past ' one , then we expect about in the interval @xmath225 '' , that is @xmath226 a statement * wrong * by a factor @xmath209 in the size of the interval ( or , equivalently , about 30% in the value of expected frequency , that should be 52% ) as we shall see in a while ( see also ref .",
    "@xcite ) . in order to do this ,",
    "let us play with our tool , building the minimal model to observe the effect of interest .",
    "figure [ fig : x1 - 2medie ]    shows the value of a quantity ( @xmath1 ) and two samples , each of three observations ( @xmath2-@xmath185 and @xmath186-@xmath212 ) , whose mean values are @xmath227 and @xmath228 ( the dashed arrows indicate that the links are _ deterministic _ instead than probabilistic , since",
    "an arithmetic mean is univocally determined by the values of the sample ) .",
    "we only need to write down the transformation rules to get @xmath227 and @xmath228 in order to build up the extra rows of the transformation matrix .",
    "they are : @xmath229 from which it follows @xmath230 here is our implementation in r , where we have now used @xmath231 , much larger than the ` experimental resolution ' of 1 .",
    "this choice makes , for the numerical values of the observations we shall use , the prior on @xmath1 practically irrelevant ( the effect on the expected values is of the order @xmath232 ) so that we can better focus on other effects : as we see , all quantities are now highly correlated . in particular , it is interesting to see how @xmath227 and @xmath228 are correlated with @xmath1 , with any observation of the first sample ( @xmath2-@xmath185 ) and with any observation of the second sample ( @xmath186-@xmath212 ) .",
    "let us now fix @xmath1 at our usual value of @xmath158 : as we already know , all observations become conditionally independent with expected value 2 and standard uncertainty 1 .",
    "the averages are also expected to be around 2 , but with smaller uncertainty , namely @xmath233 and , obviously , the averages are only correlated with each observation of their own sample .",
    "for example , for @xmath2 and @xmath227 , we have , starting from the transformation rules ( [ eq : x2-o1e1 ] ) and ( [ eq : y8-mean1 ] ) , we get , being @xmath1 certain , @xmath234 & = &    \\frac{1}{3}\\times \\sigma_{e_1}^2 \\\\                  & = &   \\frac{1}{3}\\times 1 \\approx   0.33\\,,\\end{aligned}\\ ] ] and then @xmath235 & = & \\frac { \\mbox{cov}[x_2 , x_8 ] }                          { \\sigma[x_2]\\cdot \\sigma[x_8 ]   } \\\\   & = & \\frac{1/3}{1\\times 1/\\sqrt{3 } } = \\sqrt{3 } \\approx 0.5774\\,,\\end{aligned}\\ ] ] that is exactly what we can read in the r output .",
    "let us now see what happens if we get informed about a mean value , e.g. @xmath236 .",
    "the knowledge about the first average constrains @xmath1 to @xmath237 , that is @xmath238 , while the expectations about the next average is @xmath239 , that is @xmath240 .",
    "the future observations are instead expected to be @xmath241 , where the standard uncertainty comes from @xmath242 , quadratic combination of the uncertainty about @xmath1 and that of any of the future observations around @xmath1 .    and ,",
    "as expected , there are correlations among all values which are still uncertain , with the _ exception _ of @xmath1 with @xmath2 , @xmath17 and @xmath185 ( the observations of the first sample ) .",
    "this on a first sight is not very intuitive .",
    "the reason is that @xmath1 is fully determined by the average @xmath227 , and therefore our knowledge about it can not change if we are informed about the individual values of the measurements , as we shall see in the next subsection .",
    "remaining on the values of the first sample , their expected value is exactly 2 , instead than @xmath243 , a difference absolutely negligible in practice , but very interesting indeed to understand the flow of the probabilistic updates .",
    "their values depend only on the average , and on the prior about @xmath1 .",
    "their uncertainty is the same as the uncertainty on the future average ( @xmath244 ) , although not easy to understand at an intuitive level . easier to understand are their mutual anticorrelations , since their linear combination @xmath227 ( their mean value ) is fixed .      in order to better understand the role of the mean in the inference ,",
    "let us assume we also know the value of one of the three observations contributing to it , for example @xmath167 .    ....",
    "> ( mu.c < - c(na , 1 , rep(na , m-4 ) , 2 , na ) )           # first mean ( x8 ) = 2 ;   x2=1   [ 1 ] na   1 na na na na na   2 na > out < - norm.mult.cond(mu , v , mu.c , check = false ) > round(out$mu , 4 ) [ 1 ] 1.9999 1.0000 2.5000 2.5000 1.9999 1.9999 1.9999",
    "2.0000 1.9999 > round (   out.s < - sqrt(diag(out$v ) ) , 4 ) [ 1 ] 0.5773 0.0000 0.7071 0.7071 1.1547 1.1547 1.1547 0.0000 0.8165 > round(out$v , 4 )                         [ , 1 ] [ , 2 ] [ , 3 ] [ , 4 ]    [ , 5 ]    [ , 6 ]    [ , 7 ] [ , 8 ]    [ , 9 ]   [ 1 , ] 0.3333     0   0.0   0.0 0.3333 0.3333 0.3333",
    "0 0.3333   [ 2 , ] 0.0000     0   0.0   0.0 0.0000 0.0000 0.0000",
    "0 0.0000   [ 3 , ] 0.0000     0   0.5 -0.5 0.0000 0.0000 0.0000",
    "0 0.0000   [ 4 , ] 0.0000     0 -0.5   0.5 0.0000 0.0000 0.0000     0 0.0000   [ 5 , ] 0.3333     0   0.0   0.0 1.3333 0.3333 0.3333",
    "0 0.6667   [ 6 , ] 0.3333     0   0.0   0.0 0.3333 1.3333 0.3333",
    "0 0.6667   [ 7 , ] 0.3333     0   0.0   0.0 0.3333 0.3333 1.3333",
    "0 0.6667   [ 8 , ] 0.0000     0   0.0   0.0 0.0000 0.0000 0.0000     0 0.0000   [ 9 , ] 0.3333     0   0.0   0.0 0.6667 0.6667 0.6667     0 0.6667 >   round (   out$v / outer(out.s , out.s ) , 4 )          [ , 1 ] [ , 2 ] [ , 3 ] [ , 4 ]    [ , 5 ]    [ , 6 ]    [ , 7 ] [ , 8 ]    [ , 9 ]   [ 1 , ] 1.0000   nan     0",
    "0 0.5000 0.5000 0.5000   nan 0.7071   [ 2 , ]     nan   nan   nan   nan     nan     nan     nan   nan     nan   [ 3 , ] 0.0000   nan     1    -1 0.0000",
    "0.0000 0.0000   nan 0.0000   [ 4 , ] 0.0000   nan    -1     1 0.0000 0.0000 0.0000   nan 0.0000   [ 5 , ] 0.5000   nan     0     0 1.0000 0.2500 0.2500   nan 0.7071   [ 6 , ] 0.5000   nan     0",
    "0 0.2500 1.0000 0.2500   nan 0.7071   [ 7 , ] 0.5000   nan     0     0 0.2500 0.2500 1.0000   nan 0.7071   [ 8 , ]     nan   nan   nan   nan     nan     nan     nan   nan     nan   [ 9 , ] 0.7071   nan     0     0 0.7071 0.7071 0.7071   nan 1.0000 ....    as we can see , the inference about @xmath1 does not change . as a consequence , also the expectations about the future observations",
    "are not affected by this extra piece of informations .",
    "instead , we change our knowledge about @xmath17 and @xmath185 , whose expected values become 2.5 , in order to compensate @xmath167 [ i.e @xmath245 and they are fully anticorrelated , as more or less expected .",
    "another important issue is how the knowledge that the some quantities are intrinsically correlated changes the inference .",
    "cases of this kind happen when several quantities are related by a deterministic relation , and a well understood case is when measuring the internal angles of a triangles in a flat space .",
    "just to focus on a numerical example , let us imagine the individual angles to be determined , starting from very vague priors as @xmath246 the measurements can be independent , as we have supposed ( let us forget the case of measurements with common systematics in order to focus on the effect of the constrain ) , but nevertheless the relation @xmath247 will make the results correlated .",
    "the graphical model is represented in figure [ fig : triangolo ]    with the extra node @xmath212 representing the sum of the angles and related to @xmath1 , @xmath248 and @xmath17 by deterministic links ( dashed arrows ) .",
    "being the case rather simple , especially if all uncertainties are equal , let us make the exercise of going through the exact solution .",
    "indicating the angles all together with the variable @xmath249 , whose expected value is @xmath250 =   \\{a=58^\\circ , b=73^\\circ , c=54^\\circ\\}$ ] .",
    "the covariance matrix is diagonal with all terms equal to @xmath251 .",
    "we make then the transformation to @xmath252 , where @xmath253 , and then condition on @xmath254 .",
    "the transformation matrix is then @xmath255 from which we obtain @xmath256 conditioning on @xmath254 , that is @xmath257 , using eqs .",
    "( [ eq : eaton_e ] ) and ( [ eq : eaton_v ] ) , we get @xmath258 with @xmath259 . in practice",
    "the resulting rule is the most naive one could imagine : subtract to each value one third of the excess of their sum above @xmath260 .",
    "( if you think that this rule is to simplistic , the reason might be that your model of uncertainty in this kind of measurements is different than that used here , implying for example scale type errors .",
    "but this kind of errors are beyond the aim of this note , because they imply non - linear transformations . )",
    "this is the conditioned covariance matrix @xmath261 written in a form that highlights the correlation matrix .",
    "the result is finally @xmath262 and similar expression for @xmath263 and @xmath264 , thus yielding @xmath265 with @xmath266 .      in analogy of what we have previously done in several cases ,",
    "we start from independent quantities @xmath215 , @xmath216 , @xmath267 , @xmath268 , @xmath269 and @xmath214 . for the true values of the angle",
    "we choose a flat prior , modelled with a gaussian : it is just a trick to have a pdf that is practically flat between 0 and @xmath260 .",
    "the trick allows us to use the normal multivariate formulae of reconditioning .",
    "obviously , one has to check that the final results are consistent with our assumptions and that the tails of the gaussian posterior distributions are harmless , as it is the case in our example .",
    "] of central value ( all values in degrees ) @xmath270 and @xmath271 .",
    "the expected values of the fluctuations of the observations around the true values are instead 0 , with standard deviations equal to the experimental resolutions , called sigma.gonio in the code , so that it can be changed at wish .",
    "the transformation rules are @xmath272 from which we get the transformation matrix @xmath273 here is the r code to calculate the expected values and covariance matrix of the three angles :    .... mu.priors < - rep(60 , 3 ) ; sigma.priors < - rep(1000 , 3 )      # priors sigma.gonio < - c(2 , 2 , 2 )                # experimental resolutions m=6 ; mu0 < - c(mu.priors , rep(0 , 3 ) )   sigma",
    "< - c(sigma.priors , sigma.gonio ) v0 < - matrix(rep(0 , m*m ) , c(m , m ) )        # diagonal matrix   diag(v0 ) < - sigma^2     c < - matrix(rep(0 , m*m ) , c(m , m ) )         # tranformation matrix diag(c ) < - 1 for(i in 1:3 ) c[3+i , i ] < - 1 c < - rbind(c , c ( rep(1 , 3 ) , rep(0,3 ) ) )    v < - c % * % v0 % * % t(c )                   # transformed matrix mu < - as.vector(c % * % mu0 )               # expected values    out < - norm.mult.cond(mu , v , c(na , na , na , 58 , 73 , 54 , 180 ) ) angles < - marginal.norm(out$mu , out$v , rep(1,3 ) )   ....    and these are , finally , the results , shown as an r session :    .... > angles$mu [ 1 ] 56.33335 71.33329 52.33336 > ( sigma.angles < - sqrt(diag(angles$v ) ) ) [ 1 ] 1.63299 1.63299 1.63299 > ( corr < - angles$v / outer(sigma.angles , sigma.angles ) )             [ , 1 ]        [ , 2 ]        [ , 3 ] [ 1 , ]   1.0000000 -0.4999976 -0.4999976 [ 2 , ] -0.4999976   1.0000000 -0.4999976 [ 3 , ] -0.4999976 -0.4999976   1.0000000 ....    as we see , we get the same results obtained above , with the advantage that we can now change the experimental resolutions of the individual measurement . or we can modify the model , in order to include the effect of common systematic , though limited to offset type , exercise left to the reader .",
    "as a last example , let us see how _",
    "simple _ fits can be described in terms of conditioned normal multivariates . `",
    "simple ' does not mean here linear fits , because even a realistic model to fit a straight line through data points is not that ` simple ' , if we are interested to infer also the standard deviations(s ) describing the errors and we consider errors on both axes ( see ref .",
    "@xcite , from which fig .",
    "[ fig : bn1 ] has been taken ) .",
    "on the other hand , also fitting high order polynomials can be considered ` simple ' , under the same assumptions .",
    "the meaning of fig .",
    "[ fig : bn1 ] is that for each data point we have three uncertain quantities : the true value of @xmath39 ( `` @xmath274 '' ) , the observed @xmath275 and the observed @xmath276 , while the true value @xmath274 is deterministically related to @xmath274 and to the model parameters .",
    "so , for @xmath34 data points the ` really ' dimensionality of the problem ( i.e. not taking into account the @xmath277 ) is @xmath278 , where @xmath279 is the number of parameters .",
    "the inference on the parameters @xmath280 is the performed conditioning on all @xmath281 and @xmath282 and marginalizing on @xmath283 .    a usual simplification is to ignore the errors on the @xmath39 values , making then @xmath277 deterministically on @xmath275 and @xmath280 . or , if we like , we can see each @xmath276 caused by the corresponding @xmath275 and the set of parameters @xmath280 . assuming a normal error distribution with standard deviations",
    ", linear and quadratic models can be described as @xmath284 and then expanded to all possible models of the kind @xmath285 where @xmath286 , @xmath287 and so on are mathematical functions of @xmath275 not containing free parameters . ,",
    "@xmath288 and @xmath289 ; @xmath290 , @xmath291 and @xmath292 . ]",
    "it is then rather clear that under these assumptions the problem can be treated using the properties of the multivariate normal distributions .    the general model of fig .",
    "[ fig : bn1 ] becomes , for the first three data points , that of fig .",
    "[ fig : linear_fit ] .",
    "the variables of our problem are then , indicating them with @xmath293 @xmath294 and so on .",
    "our usual transformation matrix transformation for the case of three data points is then ] and so on for higher order polynomials . ]",
    "@xmath295      as numeric example let us consider ( see fig .",
    "[ fig : linear_fit_plot_0 ] ) the * five * @xmath39 values x < - 2:6 , in correspondence of which we have ` observed ' the @xmath40 values y < - c(7.0 , 9.5 , 11.8 , 12.9 , 14.8 ) , in fact simulated by the command +   + our true values of the parameters are indeed @xmath296 and @xmath297 , while the standard deviations describing the errors @xmath147 are all equal to 0.5 . moreover we consider another * three * @xmath39 values ( 0 , 8 and 10 ) , about which we are interested in making predictions .",
    "they are indicated in fig .",
    "[ fig : linear_fit_plot_0 ] by vertical dashed lines .    having",
    "set up the problem , here is how we construct the initial diagonal matrix in @xmath298 , assigning very ` uninformative priors ' to the fit parameters . and",
    "@xmath299 , uncorrelated .",
    "not that if the standard deviations of the priors are ` quite large ' then numerical instabilities arise because the results depend on the sum of very large numbers with small ones ( the most sensitive of the two is @xmath300 which starts to create problems above 600 , while @xmath301 ) is quite harmful up to more than 2000 .",
    "[ fn : numerical ] ]    .... x < - 2:6                              # x ( ' predictors ' )",
    "y < - c(7.0 ,   9.5 , 11.8 , 12.9 , 14.8 )   #   observed y x.f < - c(8 , 10 , 0 )                    # x of new ( ' future ' ) measurements    cm.priors < - c(0,0 ) ; sigma.priors < - c(100,100 )    # priors about c and m sy < - 0.5             # standard deviation of y values n.points < - 8         # number of points   ( 5 data + 3 predictions ) mu0 < - c(cm.priors , rep(0,n.points ) ) sigma",
    "< - c(sigma.priors , rep(sy , n.points ) ) m < - n.points + 2     # dimensionality of the problem ( points + parameters ) v0 < - matrix(rep(0 , m*m ) , c(m , m ) )        # diagonal matrix diag(v0 ) < - sigma^2    ....    then we build up the transformation matrix @xmath302 and calculate the covariance matrix of the * ten * quantities of the problem ( 2 parameters , 5 data points and 3 points about which we want to make predictions ) :    .... c < - matrix(rep(0 , m*m ) , c(m , m ) )         # tranformation matrix diag(c ) < - 1 c[3:m , 1 ]",
    "< - 1 c[3:m , 2 ] < - c(x , x.f )     v < - c % * % v0 % * % t(c )                   # transformed matrix mu < - as.vector(c % * % mu0 )               # expected values ....    here",
    "are the quantities of interest    .... > sigma   [ 1 ] 1e+03 1e+03 5e-01 5e-01 5e-01 5e-01 5e-01 5e-01 5e-01 5e-01 > c        [ , 1 ] [ , 2 ] [ , 3 ] [ , 4 ] [ , 5 ] [ , 6 ] [ , 7 ] [ , 8 ] [ , 9 ] [ , 10 ]   [ 1 , ]     1     0     0     0     0     0     0     0     0      0   [ 2 , ]     0     1     0     0     0     0     0     0     0      0   [ 3 , ]     1     2     1     0     0     0     0     0     0      0   [ 4 , ]     1     3     0     1     0     0     0     0     0      0   [ 5 , ]     1     4     0     0     1     0     0     0     0      0   [ 6 , ]     1     5     0     0     0     1     0     0     0      0   [ 7 , ]     1     6     0     0     0     0     1     0     0      0   [ 8 , ]     1     8     0     0     0     0     0     1     0      0   [ 9 , ]     1    10     0     0     0     0     0     0     1      0 [ 10 , ]     1     0     0     0     0     0     0     0     0      1 > mu   [ 1 ] 0 0 0 0 0 0 0 0 0 0 > sigma.v < - sqrt(diag(v ) ) > round ( v /outer(sigma.v , sigma.v ) , 4 )          [ , 1 ]    [ , 2 ]    [ , 3 ]    [ , 4 ]    [ , 5 ]    [ , 6 ]    [ , 7 ]    [ , 8 ]    [ , 9 ]   [ , 10 ]   [ 1 , ] 1.0000 0.0000 0.4472 0.3162 0.2425 0.1961 0.1644 0.1240 0.0995 1.0000   [ 2 , ] 0.0000 1.0000 0.8944 0.9487 0.9701 0.9806 0.9864 0.9923 0.9950 0.0000   [ 3 , ] 0.4472 0.8944 1.0000 0.9899 0.9762 0.9648 0.9558 0.9430 0.9345 0.4472   [ 4 , ] 0.3162 0.9487 0.9899 1.0000 0.9971 0.9923 0.9878 0.9806 0.9754",
    "0.3162   [ 5 , ] 0.2425 0.9701 0.9762 0.9971 1.0000 0.9989 0.9968 0.9927 0.9895",
    "0.2425   [ 6 , ] 0.1961 0.9806 0.9648 0.9923 0.9989 1.0000 0.9995 0.9973 0.9952",
    "0.1961   [ 7 , ] 0.1644 0.9864 0.9558 0.9878 0.9968 0.9995 1.0000 0.9992 0.9979 0.1644",
    "[ 8 , ] 0.1240 0.9923 0.9430 0.9806 0.9927 0.9973 0.9992 1.0000 0.9997 0.1240",
    "[ 9 , ] 0.0995 0.9950 0.9345 0.9754 0.9895 0.9952 0.9979 0.9997 1.0000 0.0995 [ 10 , ] 1.0000 0.0000 0.4472 0.3162 0.2425 0.1961 0.1644 0.1240 0.0995 1.0000 ....    where the last output shows the initial correlation matrix .",
    "all variables are correlated , with some exceptions . in fact",
    "intercept and slope are nt , as it should be , and the prediction at @xmath303 ( i.e. @xmath304 ) has zero correlation with the slope ( its value is not influenced by the slope ) , while it is 100% correlated with the intercept .    the inference on the model parameters is finally obtained conditioning on the observed values of @xmath40 ( this time we use the parameter full = false to avoid large outputs ) : and out$v[1,2 ] , respectively equal to @xmath305 and @xmath306 , would become identical and equal to @xmath307 .",
    "nevertheless since this check has been done only at this stage of the paper and being the result absolutely negligible , the original matrix inversion function solve ( ) has been used also through all this section . [",
    "fn : choleski ] ]    .... > ( out < - norm.mult.cond(mu , v , c(na , na , y , na , na , na ) , full = false ) ) $ mu [ 1 ]   3.599857   1.900031 18.800107 22.600169   3.599857    $ v              [ , 1 ]         [ , 2 ]         [ , 3 ]        [ , 4 ]         [ , 5 ] [ 1 , ]   0.44997876 -0.09999521 -0.34998295 -0.5499734   0.44997876 [ 2 , ] -0.09999524   0.02499899   0.09999666   0.1499946 -0.09999524 [ 3 , ] -0.34998318   0.09999669   0.69999034   0.6499837 -0.34998318 [ 4 , ] -0.54997366   0.14999467   0.64998368   1.1999730 -0.54997366 [ 5 , ]   0.44997876 -0.09999521 -0.34998295 -0.5499734   0.69997876 ....    from which we extract standard uncertainties and correlation coefficient :    .... >   ( sigmas < - sqrt ( diag(out$v ) )   ) 1 ] 0.6708046 0.1581107 0.8366543 1.0954328 0.8366473 >   ( corr < - out$v / outer(sigmas , sigmas ) )             [ , 1 ]        [ , 2 ]        [ , 3 ]        [ , 4 ]        [ , 5 ] [ 1 , ]   1.0000000 -0.9428053 -0.6235982 -0.7484450   0.8017770 [ 2 , ] -0.9428055   1.0000000   0.7559242   0.8660217 -0.7559198 [ 3 , ] -0.6235986   0.7559244   1.0000000   0.7092032 -0.4999870 [ 4 , ] -0.7484454   0.8660219   0.7092032   1.0000000 -0.6000863 [ 5 , ]   0.8017770 -0.7559195 -0.4999867 -0.6000860   1.0000000 ....    our resulting parametric inference on intercept and slope is then @xmath308 with the correlation coefficient far from being negligible , and in fact crucial when we want to evaluate other quantities that depend on @xmath309 and @xmath82 , as we shall see in a while .",
    "we can check our result , at least as far expectations are concerned , against what we obtain using the r function lm ( ) , based on ` least squares ' :    .... > lm(y ~ x )    call : lm(formula = y ~ x )    coefficients : ( intercept )             x            3.6           1.9    ....    the data points , together with the best fit line and the intercept are reported in fig .",
    "[ fig : linear_fit_plot ] .",
    "the expectations about the future measurements are instead @xmath310 with interesting correlations :    * @xmath311 = 0.71 $ ] , positive and quite high , because their are `` on the same side '' of the experimental points and quite close to each other : due to the uncertainty about the slope they could be both smaller or larger than expected . *",
    "@xmath312 = -0.50 $ ] , @xmath313 = -0.60 $ ] negative for the opposite reason , and in absolute value increasing with the distance .",
    "note how the uncertainty on @xmath314 and @xmath304 are the same , because the corresponding @xmath39 values ( 8 and 0 , respectively ) are equally distant , from the barycenter the data along the @xmath39 axis .",
    "instead , @xmath315 is different from @xmath316 because _ they are not the same thing _ ( ! ) : the uncertainty is a parameter of the model , while @xmath317 is what we would measure at @xmath303 on the base of the information provided by the previous measurements ( and our assumptions about the model ) .",
    "the expected @xmath40 s at different values of @xmath39 are simply the values @xmath320 calculated at different @xmath39 , as it is easy to check +   + [ 1 ]  18.800001 22.600001 3.599999 + more intriguing are the uncertainties .",
    "indeed they get a contribution from the uncertainty of the true value @xmath318 and that due to the experimental error around it .    as far as the true values , in our simplified model they are given by @xmath321 , which we can rewrite in matrix form as @xmath322 here are then their expected values and covariance matrix directly in r    .... > ( c.mu.f < - cbind(rep(1,3 ) , x.f ) )         x.f [ 1 , ] 1    8 [ 2 , ] 1   10 [ 3 , ] 1    0 ....    .... >   ( mu.f < - as.vector ( c.mu.f % * % out$mu[1:2 ] ) ) [ 1 ] 18.800107 22.600169   3.599857 > ( v.mu.f < - c.mu.f % * % out$v[1:2,1:2 ] % * % t(c.mu.f ) )             [ , 1 ]        [ , 2 ]        [ , 3 ] [ 1 , ]   0.4499903   0.6499837 -0.3499832 [ 2 , ]   0.6499836   0.9499730 -0.5499737 [ 3 , ] -0.3499829 -0.5499734   0.4499788 >   ( sigma.mu.f < - sqrt(diag(v.mu.f ) ) ) [ 1 ] 0.6708132 0.9746656 0.6708046 ....    we have then @xmath323 with @xmath324 exactly equal to the intercept . and",
    "again , the uncertainties on @xmath325 and @xmath326 are the same , and then equal to @xmath316.[multiblock footnote omitted ]    the reason while the uncertainties about @xmath319 are larger than those of @xmath318 , for the same @xmath39 , is also easy to understand . to the uncertainty about the true value we have to add that due to the experimental error . and in a linear model like ours the two contributions add in quadrature , as it easy to check    while @xmath325 and @xmath326 have the standard uncertainty slightly smaller than those of the corresponding @xmath327 and @xmath328 . to obtain these latter standard uncertainties",
    "it is enough to add quadratically the standard deviation of the experimental error :    .... >   sqrt(sigma.mu.f^2 + sy^2 ) [ 1 ] 0.8366542 1.0954328 0.8366473 ....    the effect of the experimental errors is also to dilute the correlations , which among the true values are    ....",
    ">   v.mu.f / outer(sigma.mu.f , sigma.mu.f )             [ , 1 ]        [ , 2 ]        [ , 3 ] [ 1 , ]   1.0000000   0.9941347 -0.7777671 [ 2 , ]   0.9941347   1.0000000 -0.8411826 [ 3 , ] -0.7777666 -0.8411821   1.0000000 ....      also in this case one do several other instructive test , which we read to the reader .",
    "here is a partial list .    *",
    "impose a precise value for the intercept and the slope , to see how the other parameter changes .",
    "this can be done , for example for the intercept ( @xmath296 ) with the following conditioning : +   + ( quick test : what would you expect for @xmath304 ? )",
    "* do the same test , but using the previous out$v and out$mu .",
    "* use some informative priors for @xmath309 , @xmath82 or both . *",
    "make a new fit on another 5 data points generated from the same model , using as priors for @xmath309 and @xmath82 the result of the previous inference ( including the correlation ! ) .",
    "* make a global fit on the 10 data points of the two datasets ( starting from uninformative priors ) and compare with the result of the two inferences in sequence .    then is the question of estimating the common standard deviation of the model from the data . as told above , this can not be done with the tools we are playing in this paper because the problem is not linear .",
    "certainly a rough estimate can be done by the residuals , but if the number of data points is ` small ' the uncertainty on the estimated sigma do not only affect this parameter , but also the joint pdf of @xmath309 and @xmath82 , which is longer normal bivariate ( with consequences on the pdf s of the previsions ) .",
    "the problem has to be solved using a model without short cuts and making the integrals numerically or by markov chain monte carlo , issues which are beyond the aim of this paper .",
    "( and pay attention to covariance matrices obtained by linearization!@xcite )",
    "let us take again the diagrams ( ` _ graphs _ ' ) which describe two observations from the same true value and one observation resulting from a true value and a systematic effect .",
    "they are show again in fig .  [",
    "fig : propagation_evidence ] ,          it follows @xmath332 and hence @xmath333 & = &    \\left(\\!\\ ! \\begin{array}{cc }",
    "\\frac{\\sigma_1 ^ 2\\sigma_{2|1}^2}{\\sigma_1 ^ 2 + \\sigma_{2|1}^2 } &   0 \\\\   0   & \\sigma_{3|2}^2     \\end{array } \\!\\!\\right)\\end{aligned}\\ ] ] as expected , the exercise shows that @xmath1 and @xmath17 become now independent and the uncertainty about @xmath17 is simply @xmath334 .",
    "and also in @xmath335 $ ] we recognize a familiar pattern ( see also footnote [ fn : media_pesata ] ) : @xmath336 } & = &   \\frac{1}{\\sigma_1 ^ 2 } + \\frac{1}{\\sigma_{2|1}^2}\\,.\\end{aligned}\\ ] ]      c.f .",
    "`` theoria motus corporum coelestium in sectionibus conicis solem ambientum '' _ , hamburg 1809 , n.i 172179 ; reprinted in werke , vol .",
    "7 ( gota , gttingen , 1871 ) , pp 225234 .",
    "+ ( see e.g. in http://www.roma1.infn.it/~dagos/history/gaussmotus/index.html )"
  ],
  "abstract_text": [
    "<S> the properties of the normal distribution under linear transformation , as well the easy way to compute the covariance matrix of marginals and conditionals , offer a unique opportunity to get an insight about several aspects of uncertainties in measurements . </S>",
    "<S> the way to build the overall covariance matrix in a few , but conceptually relevant cases is illustrated : several observations made with ( possibly ) different instruments measuring the same quantity ; effect of systematics ( although limited to _ offset _ , in order to stick to linear models ) on the determination of the ` true value ' , as well in the prediction of future observations ; correlations which arise when different quantities are measured with the same instrument affected by an offset uncertainty ; inferences and predictions based on averages ; inference about constrained values ; fits under some assumptions ( linear models with known standard deviations ) . </S>",
    "<S> many numerical examples are provided , exploiting the ability of the r language to handle large matrices and to produce high quality plots . </S>",
    "<S> some of the results are framed in the general problem of ` propagation of evidence ' , crucial in analyzing graphical models of knowledge .    </S>",
    "<S> # 1    # 1#2 \\{#2 } # 1 </S>"
  ]
}