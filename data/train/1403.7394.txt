{
  "article_text": [
    "in 2010 , big data was growing at 2.5 quintillion  @xcite bytes per day .",
    "this overwhelming volume , velocity , and variety of data can be attributed to the ubiquitously spread sensors , perpetual streams of user - generated content on the web , and increased usage of social media platforms  twitter alone produces 12 terabytes of tweets every day .",
    "the sustained financial health of the world s leading corporations is intimately tied to their ability to sift , correlate , and ascertain actionable intelligence from big data in a timely manner .",
    "these immense computational requirements have created a heavy demand for advanced analytics methodologies which leverage the latest in distributed , fault - tolerant parallel computing architectures . among a variety of choices ,",
    "mapreduce has emerged as one of the leading parallelization strategies , with its adoption rapidly increasing due to the availability of robust open source distributions such as apache hadoop  @xcite . in the present work ,",
    "we develop a novel mapreduce implementation of a fairly recent clustering approach @xcite , and demonstrate its favorable performance for big data analytics .",
    "clustering techniques are at the heart of many analytics solutions .",
    "they provide an unsupervised solution to aggregate similar data patterns , which is key to discovering meaningful insights and latent trends .",
    "this becomes even more necessary , but exponentially more difficult , for the big data scenario .",
    "many clustering solutions rely on user input specifying the number of cluster centers ( e.g. k - means clustering  @xcite or gaussian mixture models  @xcite ) , and biasedly group the data into these desired number of categories .",
    "frey et al .",
    "@xcite introduced an exemplar - based clustering approach called .",
    "as an exemplar - based clustering approach , the technique does not seek to find a mean for each cluster center , instead certain representative data points are selected as the exemplars of the clustered subgroups . the technique is built on a message passing framework where data points `` talk '' to each other to determine the most likely exemplar and automatically determine the clusters , _",
    "i.e. _  there is no need to specify the number of clusters a priori .",
    "the sole input is the pairwise similarities between all data points under consideration for clustering  making it ideally suited for a variety of data types ( categorical , numerical , textual , etc . ) .",
    "a recent extension of the clustering algorithm is  @xcite , which groups and stacks data in a tiered manner . only requires the number of hierarchy levels as input and the communication between data points occurs both within a single layer and up and down the hierarchy . to date , and have been mainly relegated to smaller , manageable quantities of data due to the prohibitive quadratic run time complexity .",
    "our investigations will demonstrate an effective parallelization strategy for using the mapreduce framework , for the first time enabling applications of these powerful techniques on big data problems .",
    "first introduced by google  @xcite , the mapreduce framework is a programming paradigm designed to facilitate the distribution of computations on a cluster of computers .",
    "the ability to distribute processes in a manner that takes the computations to the data is key when mitigating the computational cost of working with extremely large data sets .",
    "the parallel programming model depends on a mapper phase that uses key - value identifiers for the distribution of data and subsequent independent executions to generate intermediate results .",
    "these are then gathered by a reducing phase to produce the final output key - value pairing .",
    "this simple , yet widely applicable parallelization philosophy has empowered many to take machine learning algorithms previously demonstrated only on `` toy data '' and scale them to enterprise - level processing  @xcite . in this same vein , we adopted the most popular open source implementation of the mapreduce programming model , apache hadoop , to develop the _ first ever _ parallelized extension of , which we refer to as .",
    "this allows efficient fault - tolerant clustering of big data , and more importantly , improves the run time complexity to potentially linear time ( given enough machines ) .      to handle the explosion of available data , there is now a vast amount of research in computational frameworks to efficiently manage and analyze these massive information quantities . here",
    "we focus on the mapreduce framework  @xcite for faster and more efficient data mining , covering the most relevant to our approach .    among the state of the art mapreduce clustering algorithms",
    "is , which was implemented by nair et al .",
    "@xcite . uses cloud computing to handle large data sets , while supporting top to bottom hierarchies or a bottom to top approach .",
    "since it derives its roots from k - means , requires one to specify the number of clusters .",
    "our implementation does not require presetting the number of required clusters ; it instead organically and objectively discovers the data partitions .    in wu et al .",
    "@xcite , the authors propose to parallelize on the mapreduce framework to cluster large scale e - learning resources .",
    "the parallelization happens on the individual message level of the algorithm .",
    "we perform a similar parallelization but significantly go beyond and allow for hierarchical clustering , which enables a deeper understanding of the data s semantic relationships .",
    "in addition , our development is designed to work on a variety of data sources ; thus , our experiments will showcase results on multiple data modalities , including images and numerical data , as shown in   [ sec : experimental - results ] .",
    "the rest of this paper is organized as follows . in the next section ,  [ sec : hap ] ,",
    "we detail the non - parallel algorithm . ",
    "[ sec : mapreduce ] discusses the mapreduce paradigm and the implementation details for these algorithms .",
    "the experimental validations provided in  [ sec : experimental - results ] demonstrate our favorable performance against another clustering algorithm which is readily available in the open source project apache mahout  @xcite .",
    "finally , we conclude with a summary of our efforts and future recommendations .",
    "is a clustering algorithm introduced by frey et al.@xcite motivated by the simple fact that given pairwise similarities between all input data , one would like to partition the set to maximize the similarity between every data point and its cluster s exemplar . recall that an exemplar is an actual data point that has been selected as the cluster center . as we will briefly discuss",
    ", these ideas can be represented as an algorithm in a message passing framework . in the landscape of clustering methodologies , which includes such staples as k - means @xcite , k - medoids @xcite , and gaussian mixture models @xcite",
    ", predominantly all methods require the user to input the desired number of cluster centers .",
    "avoids this artificial segmentation by allowing the data points to communicate amongst themselves and organically give rise to a partitioning of the data . in many applications",
    "an exemplar - based clustering technique gives each cluster a more representative and meaningful prototype for the center , versus a fabricated mean .",
    ", introduced by @xcite , extends to allow tiered clustering of the data .",
    "the algorithm starts by assuming that all data points are potential exemplars .",
    "each data point is viewed as a node in a network connected to other nodes by arcs such that the weight of the arcs @xmath0 describes how similar the data point with index @xmath1 is to the data point with index @xmath2 .",
    "takes as input this similarity matrix where the entries are the negative real valued weights of the arcs . having the similarity matrix as the main input versus",
    "the data patterns themselves provides an additional layer of abstraction  one that allows seamless application of the same clustering algorithm regardless of the data modality ( _ e.g._text , images , general features , etc . ) .",
    "the similarity can be designed to be a true metric on the feature space of interest or a more general non - metric @xcite .",
    "the negative of the squared euclidean distance if often used as a metric for the similarities .",
    "the diagonal values of the similarity matrix , @xmath3 , are referred to as the preferences which specify how much a data point j wants to be an exemplar . since the similarity matrix entries are all negative values , @xmath4",
    ", @xmath5 implies data point @xmath2 has high preference of being an exemplar and @xmath6 implies it has very low preference . in some cases , as in @xcite , the preference values are set using some prior knowledge ; for example , uniformly setting them to the average of the maximum and minimum values of @xmath0 , or by setting them to random negative constants . through empirical verification , we experienced better performance with randomizing the preferences and adopt this approach for most of our experiments .",
    "once the similarity matrix is provided to the algorithm , the network of nodes ( data points ) recursively transmits two kinds of intra - level messages between each node until a good set of exemplars is chosen .",
    "the first message is known as the responsibility message and the second as the availability message .",
    "the responsibility messages , @xmath7 , are sent at level @xmath8 from data point @xmath1 to data point @xmath2 portraying how suitable node @xmath1 thinks node @xmath2 is to be its exemplar .",
    "similarly , availability messages , @xmath9 , are sent at level @xmath8 from data point @xmath2 to @xmath1 , indicating how available @xmath2 is to be an exemplar for data point @xmath1 . the responsibility and availability update equations",
    "are given in eq .",
    "[ eq : respupdatehap ] and eq .",
    "[ eq : availupdatehap ] , respectively .",
    "@xmath10\\label{eq : respupdatehap}\\\\ \\alpha_{ij}^{l < l } & \\leftarrow & \\min\\big\\{0,c_{j}^{l}+\\phi_{j}^{l}+\\rho_{jj}^{l}+\\sum_{\\mathclap{ks.t.k\\notin\\{i , j\\}}}\\max\\{0,\\rho_{kj}^{l}\\}\\big\\}\\label{eq : availupdatehap}\\\\ \\alpha_{jj}^{l < l } & \\leftarrow & c_{j}^{l}+\\phi_{j}^{l}+\\sum_{ks.t.k\\neq j}\\max\\{0,\\rho_{kj}^{l}\\}\\label{eq : self - availabilityhap}\\end{aligned}\\ ] ] where @xmath11 is the number of levels defined by the user and @xmath12 .",
    "[ eq : self - availabilityhap ] is the self - availability equation which reflects the accumulated positive evidence that @xmath2 can be an exemplar .",
    "the self - responsibility messages are updated the same way as the responsibility messages . to avoid numerical oscillation ,",
    "the responsibility and availability messages are dampened by @xmath13 at every level @xmath8",
    ".    also introduces two inter - level messages .",
    "these messages are denoted by @xmath14 in eq .",
    "[ eq : tau ] , which receives messages from the lower level and @xmath15 in eq .",
    "[ eq : phi ] , which receives messages from the upper level . at every level ,",
    "the cluster preference @xmath16 is updated using eq .",
    "[ eq : exemplarlevelupdate ] .",
    "@xmath17    a variety of strategies can be employed to update the similarity matrix @xmath18 to vary level - wise .",
    "we have achieved good results by simply taking into consideration the cluster relationship of the previous level : @xmath19\\label{eq : similaritylevelupdate}\\ ] ] where @xmath20 is a constant value within [ 0,1 ] .",
    "this updates the relation between data points in level @xmath21 by negatively increasing the similarity between points that belong to different clusters in level @xmath8 and enforces the similarity between points that fall under the same cluster in level @xmath8 .",
    "after all messages have been sent and received , the cluster assignments are chosen , at every level , based on the maximum sum of the availability and responsibility messages as in eq .",
    "[ eq : exemplarpositionlevelupdate ] .",
    "these cluster assignments can be used to extract the list of exemplars .",
    "@xmath22 these net message exchanges seek to maximize the cost of correctly labeling a point as an exemplar and gathering its representative members ( a cluster ) . in algorithm 1",
    ", we detail the pseudo - code implementation of . given this description of , we now proceed to discuss mapreduce and our novel parallelization strategy .",
    "* input : * similarity ( s ) , levels ( l ) , iterations , and @xmath23 * initialize : * @xmath24 , @xmath25 , @xmath26 , @xmath27 , @xmath28 , @xmath29 update @xmath30 ( eq .  [ eq : respupdatehap ] ) & dampen @xmath31 update @xmath32",
    "[ eq : availupdatehap ] & [ eq : self - availabilityhap ] ) & dampen @xmath33 update @xmath34 , @xmath35 & @xmath36 ( eq .  [ eq : tau ] , [ eq : phi ] & [ eq : exemplarlevelupdate ] ) * optional * update @xmath37 ( eq .  [ eq : similaritylevelupdate ] ) update @xmath38 ( [ eq : exemplarpositionlevelupdate ] ) [ alg : hap ]",
    "the mapreduce programming model  @xcite is an abstract programming paradigm independent of any language that allows the processing workload of the implemented algorithm to be balanced over separate nodes within a computer cluster .",
    "our overarching mapreduce approach for was motivated by viewing the major update equations for ( see algorithm 1 ) as tensorial mathematical constructs  @xcite .",
    "one can simply view these tensorial constructs as two or three dimensional matrices .",
    "the algorithm can be parallelized because all the updates to the various tensors require only a subset of the information provided .",
    "therefore , the updates can be split up into different jobs and each job will receive the subset of data needed to evaluate the update .    to achieve a balance between computational partitioning and efficient formatting for data representation on the ,",
    "all the data is constructed as three dimensional tensors . in support of the fault tolerance aspect of mapreduce",
    ", it is important to retain a copy at all times of the @xmath39 , @xmath40 , @xmath41 , @xmath42 , @xmath14 , and @xmath15 tensors .",
    "( recall @xmath39 , @xmath40 , @xmath41 , and @xmath42 refer to the similarity , availability , responsibility , and cluster preferences , respectively . ) to this end , even those tensors not required by a job must be passed directly through to the next job . for the @xmath39 , @xmath40 , and @xmath41 tensors ,",
    "the dimensions represent the nodes , the exemplars , and the levels .",
    "since there are @xmath43 nodes , @xmath43 possible exemplars , and @xmath11 levels , these tensors contain @xmath44 values . for the @xmath42 , @xmath14 , and @xmath15 tensors ,",
    "the first two dimensions represent the index and level and the depth dimension has length one . since there are @xmath43 indices and @xmath11 levels , these tensors contain @xmath45 values . in the sequel , for the @xmath39 , @xmath40 , and @xmath41 tensors",
    ", the node dimension will be iterated by @xmath1 , the exemplar dimension will be iterated by @xmath2 , and the level dimension iterated by @xmath8 .",
    "as for the @xmath42 , @xmath14 , and @xmath15 tensors , the index dimensions will be iterated by both @xmath1 and @xmath2 .    with these underlying structures ,",
    "data must be deconstructed and represented as ( key , value ) pairs for use in the mapreduce framework .",
    "there are two formats for storing the information : node - based and exemplar - based formatting . in the _ node - based format _",
    ", the keys are string tuples , @xmath46 , where @xmath1 represents the node , @xmath8 represents the level , and @xmath47 represents the tensor @xmath48 .",
    "the values , represented by @xmath49 , are the vectors for the @xmath50 node of the matrix on the @xmath51 level of the tensor . in the _ exemplar - based format _",
    ", the keys are string tuples , @xmath52 , where @xmath2 represents the exemplar , @xmath8 represents the level , and @xmath47 represents the tensor .",
    "the values , represented by @xmath49 , are the vectors for the @xmath53 exemplar of the matrix on the @xmath51 level of the tensor . with",
    "the data thus represented , mapreduce jobs must be constructed to manipulate the information using the given equations .    in our parallelization scheme ,",
    "is broken down into three separate mapreduce jobs .",
    "the first job handles updating @xmath14 , @xmath42 , and @xmath41 .",
    "the second job handles updating @xmath15 and @xmath40 .",
    "these first two jobs loop for a set number of iterations . at the end of the iterations ,",
    "the final job extracts the cluster assignments on each level .",
    "due to dependencies set out in the equations , the @xmath41 update must occur first .",
    "therefore , @xmath14 and @xmath42 are not updated during the first iteration . in all other iterations",
    "they occur before the responsibility update . at the start of each iteration",
    ", the data will be in exemplar - based format .",
    "after the first job , the data will have switched to node - based format .",
    "the second job converts the data back to exemplar - based format to begin a new iteration or to be used as input to the final job .",
    "see fig .",
    "[ fig : parallelization - scheme ] for a visual representation of the parallelization scheme .",
    "the figure represents what happens to the data during either of the first two jobs .",
    "the tensors have been stacked to show how the indices line up .",
    "the yellow strips on the left represent information being passed to one mapper , one strip per mapper .",
    "the focus of each mapper is on providing the reducers with the necessary information .",
    "the subsequent focus of each reducer is on performing the tensor updates as defined in the equations .",
    "as the data comes out of the job , the switch between exemplar - based and node - based formats can be easily seen .",
    "the output is now ready for use by the next job , which will follow a similar flow .",
    "the following sections will provide in - depth explanations of each mapreduce job .",
    "parallelization scheme , scaledwidth=30.0% ]      this job takes as input the exemplar - based representation of the data and outputs the node - based representation of the data with updated values . in the first iteration , @xmath14 and @xmath42 are not updated due to previously mentioned dependencies . in this mapreduce job",
    ", the mapper deconstructs the exemplar - based vectors into node - based values for the reducer to reconstruct node - based vectors .",
    "each mapper receives a key describing a unique @xmath52 combination and a value with the corresponding vector .",
    "the indices of the vector represent the nodes ; thus , the mapper iterates over the vector with @xmath1 .",
    "each reducer receives a key describing a unique @xmath54 combination and a list of values which will be used to reconstruct the 6 node - based vectors , the 2 node - based vectors from the level below and the 2 special diagonal vectors .",
    "the indices of the constructed vector represent the exemplars so the reducer iterates over the vector with @xmath2 .",
    "this job takes as input the node - based representation of the data and outputs the exemplar - based representation of the data with updated values . in this mapreduce job",
    ", the mapper deconstructs the node - based vectors into exemplar - based values for the reducer to reconstruct exemplar - based vectors .",
    "each mapper receives a key describing a unique @xmath46 combination and a value with the corresponding vector .",
    "the mapper iterates over the vector with @xmath2 .",
    "each reducer receives a key describing a unique @xmath55 combination and a list of values which will be used to reconstruct the 6 exemplar - based vectors and the 2 node - based vectors from the level above .",
    "the indices of the constructed vector represent the nodes so the reducer iterates over the vector with @xmath1 .",
    "this job takes as input the exemplar - based representation of the data and outputs the cluster assignments . in this mapreduce job",
    ", the mapper deconstructs the exemplar - based vectors into node - based values for the reducer to reconstruct node - based vectors .",
    "each mapper receives a key describing a unique @xmath52 combination and a value with the corresponding vector .",
    "the mapper iterates over the vector with @xmath1 .",
    "since this is the last step , only the required information has to pass to the reducer and the other information can be neglected .",
    "each reducer receives a key describing a unique @xmath54 combination and a list of values which will be used to reconstruct the 2 node - based vectors and the 2 special diagonal vectors .",
    "the reducer iterates over the vector with @xmath2 .",
    "a standard sequential implementation must necessarily have a runtime complexity of @xmath56 where @xmath57 represents the number of algorithmic iterations , run either as a hard limit or until convergence is reached , @xmath11 represents the number of output levels requested , and @xmath43 represents the cardinality of the input data set , such that the size of @xmath39 is @xmath58 .",
    "the runtime complexity is a direct result of iterating over all three dimensions of the tensors for each iteration . by implementing the algorithms in the mapreduce framework ,",
    "we are able to achieve superior runtime complexity . under mapreduce",
    ", the runtime complexity reduces to a linear relationship with the data , assuming the total number of on the cluster , @xmath59 , scales to @xmath45 , _",
    "i.e. _  @xmath60 as @xmath61 . in , @xmath59 can only scale up to a maximum of @xmath45 because @xmath59 is limited to the number of tasks that can be evaluated at the same time . in this case , it is limited to the minimum of the @xmath62 mapper tasks and the @xmath45 reducer tasks , where the constant factor six represents the number of tensor identifications introduced into the algorithm , namely @xmath63 , and @xmath42 .",
    "[ cols=\"^,^ \" , ]     for the `` buttons '' image , the number of levels was set to @xmath64 .",
    "the top right image is the lowest level where the pixels were grouped into 154 clusters .",
    "the bottom left image is the second level where the pixels were grouped into 25 clusters . finally , the bottom right image is the highest level where the pixels were grouped into 11 clusters .",
    "the highest level of the hierarchy appears fuzzier than the original image due to similar colors clustering underneath a single exemplar .      in order to test the scalability of the algorithm with respect to speed",
    ", we use the data set `` aggregation ''  @xcite , which is a shape set composed of 788 two - dimensional points .",
    "the purpose of these tests was to observe trends in algorithm runtime as cluster computing power increased , as well as to determine the benefits of running in a distributed environment as compared to an undistributed environment ( a single - machine hadoop cluster ) .",
    "hadoop clusters were provided using to dynamically create clusters of standard instances .",
    "cluster computing power was scaled both by increasing the number of within a cluster and by provisioning more powerful .",
    "the two instance types used are : ( 1 ) the m1.small , which has 1.7 gb of memory and is considered to have 1 with 160 gb of instance storage and a 32-bit architecture , and ( 2 ) the m1.xlarge , which has 15 gb of memory , 8 , 1,690 gb of instance storage , and a 64-bit architecture . the single - machine hadoop cluster utilized to simulate an undistributed environment has 8 gb of memory , 8 , 40 gb of machine storage , and a 64-bit architecture .    for comparison to another state - of - the - art mapreduce clustering methodology ,",
    "our algorithm was benchmarked against .",
    "due to its inherently parallel design , immediately begins to benefit from being placed in a distributed environment .",
    "represented by a solid blue line in fig .",
    "[ fig : speed ] , runtime decreases by 64% , from 320 minutes to 115 minutes , when cluster computational power is increased by just 4 additional . eventually reaches the threshold of a linear relationship with the size of the input data at a runtime of around 20 minutes , which is a 94% decrease from the single cluster .",
    "furthermore , at its best , performs 66% faster in a distributed environment than the undistributed environment which is represented by the blue dotted line in fig .",
    "[ fig : speed ] .",
    "in contrast , the mahout hk - means algorithm used in this experimentation , indicated by the solid green line in fig .",
    "[ fig : speed ] , is not parallelized to the extent of .",
    "each single iteration of k - means is structured under mahout to distribute over a hadoop cluster , but the hierarchical `` top down '' structure requires iterative executions of k - means for each level .",
    "this lack of an overall parallelization scheme results in reduced performance at scale than .",
    "hk - means runtime initially increases by 8.5% when is increased from 1 to 10 due to hadoop cluster overhead , including network latency and i / o time .",
    "however , at 10 , overcomes this overhead and begins to benefit from the mapreduce parallelization scheme .",
    "this results in an eventual 16% runtime decrease between 1 and 80 , at which point eventually reaches a linear relationship with the data at a runtime around 225 minutes . unlike , hk - means never surpasses its undistributed runtime threshold of 146 minutes indicated by the green dotted line in fig .  [ fig : speed ] .",
    "finally , at its best , runs 90% slower than , requiring 226 minutes of execution compared to s 23 minutes . with significantly faster runtimes ,",
    "still posts purity levels competitive with hk - means , shown in fig .",
    "[ fig : purity ] .",
    "this combination of speed and high performance is ideal for processing big data in a large - scale cloud computing environment .",
    "time vs. number of ec2 cpus .",
    "our better utilizes available compute resources to significantly improve runtime . ]",
    "the need for efficient and high performing data analysis frameworks remain paramount to the big data community .",
    "the clustering algorithm is rapidly becoming a favorite amongst data scientists due to its high quality grouping capabilities , while requiring minimal user specified parameters .",
    "recently , a multilayer structured version of the algorithm , , was introduced to automatically extract tiered aggregations inherent in many data sets .",
    "is modeled as a message - based network that allows communication between nodes and between levels in the hierarchy , and mitigates many of the biases that arise in techniques that require one to input the number of clusters . in the present work ,",
    "we have developed the first ever extension , , to address the big data problem  demonstrating an efficient parallel implementation using mapreduce that directly improves the runtime complexity from quadratic to linear .",
    "the novel tensor - based partitioning scheme allows for parallel message updates and utilizes a consistent data representation that is leveraged by map and reduce tasks .",
    "our approach seamlessly allows us to cluster a variety of data modalities , which we experimentally showcased on data sets ranging from synthetic numerical points to imagery .",
    "our analysis and computational performance is competitive with the state - of - the - art in mapreduce clustering techniques .",
    "the authors acknowledge partial support from nsf grant no . 1263011 .",
    "any opinions , findings , and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nsf ."
  ],
  "abstract_text": [
    "<S> the accelerated evolution and explosion of the internet and social media is generating voluminous quantities of data ( on zettabyte scales ) . </S>",
    "<S> paramount amongst the desires to manipulate and extract actionable intelligence from vast big data volumes is the need for scalable , performance - conscious analytics algorithms . to directly address this need , </S>",
    "<S> we propose a novel mapreduce implementation of the exemplar - based clustering algorithm known as affinity propagation . </S>",
    "<S> our parallelization strategy extends to the multilevel hierarchical affinity propagation algorithm and enables tiered aggregation of unstructured data with minimal free parameters , in principle requiring only a similarity measure between data points . </S>",
    "<S> we detail the linear run - time complexity of our approach , overcoming the limiting quadratic complexity of the original algorithm . </S>",
    "<S> experimental validation of our clustering methodology on a variety of synthetic and real data sets ( _ e.g. _  images and point data ) demonstrates our competitiveness against other state - of - the - art mapreduce clustering techniques . </S>"
  ]
}