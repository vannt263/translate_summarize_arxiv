{
  "article_text": [
    "the _ partition function _ is of great importance in statistical physics since most of the thermodynamic variables of a system can be expressed in terms of this quantity or its derivatives .",
    "this quantity also plays an important role in many other contexts , including artificial intelligence , combinatorial enumeration , approximate inference , and parameter estimation . in general , the exact calculation of the partition function is computationally intractable therefore finding low - complexity estimates and bounds is desirable .    in  @xcite , we proposed upper and lower bounds on the partition function that depend on the partition function of any sub - junction tree of a given junction graph representing the inference problem . in",
    "@xcite a greedy algorithm that gives low - complexity upper and lower bounds on the partition function was proposed .",
    "an inequality was proved that compares the lower bounds calculated from different sub - junction trees based on their entropies  ( * ? ? ?",
    "* theorem 2 ) .    in this paper , we study the properties of the minimum entropy sub - junction tree and will extend the results of  ( * ? ? ?",
    "* theorem 2 ) by stating new theorems and corollaries .",
    "we prove that there is an upper bound on how much any other lower bound can be better than the one obtained from the minimum entropy sub - tree .",
    "we also show that the probability distributions over the sub - tree that gives the best lower bound and the one with the minimum entropy are close in divergence .",
    "suppose a global function defined over several random variables , e.g. a probability mass function , factors as a product of a series of non - negative local kernels , each kernel defined over a subset of the set of all random variables .",
    "the goal is to compute the normalization constant and the marginals of the global function according to those subsets .",
    "more formally , consider a set @xmath0 of @xmath1 discrete random variables taking their values in a finite set @xmath2 .",
    "let @xmath3 represent the possible realizations of @xmath4 and let @xmath5 stand for @xmath6 .",
    "suppose @xmath7 are subsets of @xmath8 and @xmath9 is a collection of subsets of the indices of the random variables @xmath10 through @xmath11 .",
    "let us also suppose that @xmath12 , the joint probability mass function , factors into product of finite and non - negative local kernels as @xmath13 where each local kernel @xmath14 is a function of the variables whose indices appear in @xmath15 , and @xmath16 is the partition function , also known as the _ global normalization constant _ whose role is simply normalizing the probability distribution .    in a _",
    "probabilistic inference _ problem , we are interested in computing @xmath16 and the marginal densities @xmath17 , which are defined as @xmath18",
    "graphical models use graphs to represent and manipulate joint probability distributions . an efficient way to solve a probabilistic inference problem is to represent it with a graphical model and use a message passing algorithm on this model .",
    "there are many graphical models in the literature such as junction graphs , markov random fields , and ( forney - style ) factor graphs . in this paper",
    "we focus on graphical models defined in terms of _ junction graphs_. our results can be easily expressed with other graphical models . _",
    "definition 1 : _ a junction graph is an undirected graph @xmath19 where each vertex and each edge have labels , denoted by @xmath20 , and @xmath21 respectively . the labels on the edges must be a subset of the labels of their corresponding vertices .",
    "furthermore , the induced subgraph consisting only of the vertices and edges which contain a particular label , must be a tree .",
    "we say that @xmath19 is a junction graph for the inference problem defined by @xmath22 , if @xmath23 . for any probabilistic inference a junction graph representation always exists .    _ the generalized distributive law ( gdl ) _ is an iterative message passing algorithm , described by its",
    "_ messages _ and _ beliefs _ , to solve the probabilistic inference problem on a junction graph .",
    "it operates by passing messages along the edges of a junction graph , see @xcite , @xcite .",
    "the message sent from a vertex @xmath24 , to another vertex @xmath25 , is a function of the variables whose indices are on @xmath26 , the edge between @xmath25 and @xmath24 , and is denoted by @xmath27 .",
    "the beliefs on vertices and edges are denoted by @xmath28 and @xmath29 , respectively .",
    "the messages and the beliefs are computed as @xmath30    @xmath31    where @xmath32 denotes the neighbors of @xmath25 ; @xmath33 and @xmath34 are the local normalizing constants .",
    "[ th : gdl ] on a junction tree the beliefs converge to the exact local marginal probabilities after a finite number of steps  ( * ? ? ?",
    "* theorem 3.1 ) .",
    "if @xmath35 is a tree , @xmath12 defined by ( [ eq : jointp ] ) factors as follows , see  @xcite @xmath36    in this case , the entropy of the global distribution decomposes as the sum of the entropies of the vertices minus the sum of the entropies on the edges .",
    "similarly , the global normalization constant @xmath16 can be expressed in terms of the local normalization constants as follows @xmath37    therefore if @xmath35 is a tree , there is an efficient algorithm to compute @xmath16 , the marginals of @xmath12 , and the entropy of @xmath12 .",
    "if @xmath35 is not a tree , the above algorithm is not guaranteed to give the exact solution or even to converge , although empirically it performs very well .",
    "new theoretical results show that there is a connection between message passing algorithms and certain approximations to the energy function in statistical mechanics .",
    "the idea is that having plausible approximations to the energy function gives hope that the minimizing arguments are also reasonable approximations to the exact marginals , see  @xcite .",
    "see also  @xcite for some new results regarding the partition function and loop series .",
    "for a general junction graph , calculating the partition function , @xmath16 , through a straightforward manner as expressed in ( [ eq : zz ] ) , needs a sum with an exponential number of terms .",
    "therefore it is desirable to have bounds on @xmath16 which can be obtained with low complexity , see  @xcite .    according to ( [ eq : decompz ] ) , on a junction tree",
    "the partition function can be computed efficiently . in this section",
    "we derive lower bounds on @xmath16 which depend on the partition function of @xmath38 a sub - junction tree of @xmath35 , see  @xcite .",
    "consider a probabilistic inference problem defined by @xmath39 . also consider @xmath40 , a subset of @xmath22 that has a junction tree representation .",
    "if @xmath41 denotes the global probability distribution and @xmath42 the partition function constant on @xmath38 , we can rewrite @xmath12 defined in ( [ eq : jointp ] ) as follows    @xmath43    take logarithm of both sides of ( [ eq : rewrite ] ) , multiply by @xmath44 , and sum over @xmath5 .",
    "@xmath45 by rearranging ( [ eq : ub4 ] ) we obtain @xmath46 hence the following @xmath47 if we denote the lower bound obtained using @xmath48 , i.e. the left hand side of ( [ eq : ub6 ] ) , by @xmath49 , the following theorem holds .",
    "see  ( * ? ? ?",
    "* theorem 2 ) .",
    "[ th : twotrees ] consider @xmath50 and @xmath51 , subsets of @xmath22 with junction tree representations .",
    "also suppose that @xmath52 and @xmath53 denote the global probability distributions , and @xmath54 and @xmath55 the partition functions over @xmath50 and @xmath51 respectively . without loss of generality suppose @xmath56 , then the following inequality holds @xmath57 here @xmath58 and @xmath59 denote the global probability distributions on @xmath60 and @xmath61 respectively .     via @xmath62 or via @xmath63,width=132 ]    _ corollary 1 : _ in the case that @xmath64 , namely when the junction graph decomposes into two junction trees ( for example this can be the case when we choose a sub - tree in a graph with only one cycle ) , if @xmath56 the bound in ( [ eq : compare ] ) simplifies to @xmath65 note that in general @xmath66 is not symmetrical therefore equation ( [ eq : compare2 ] ) does not tell whether one bound is better than the other . according to ( [ eq : ub5 ] ) and the definition of @xmath49 , we can see that @xmath67 therefore we can rewrite ( [ eq : compare2 ] ) as @xmath68 in other words , the distance from @xmath63 to @xmath69 via @xmath62 is shorter than the distance from @xmath62 to @xmath69 via @xmath63 .",
    "see fig .",
    "[ fig : qpq ] . note that in general @xmath66 does not satisfy the triangular inequality therefore equation ( [ eq : compare3 ] ) does not tell whether one bound is better than the other either .    clearly , the distance from @xmath62 to @xmath69 is also shorter than the distance from @xmath62 to @xmath69 via @xmath63 .",
    "@xmath70    _ corollary 2 : _ consider a subset @xmath71 of @xmath72 with junction tree representation .",
    "also suppose that @xmath73 , the probability distribution over @xmath74 , has the smallest entropy among all the probability distributions on sub - trees .",
    "then for any subset @xmath75 of @xmath72 with tree representation the following inequality holds @xmath76   +    according to theorem  [ th : twotrees ] @xmath77 and hence the following @xmath78    in other words , the lower bound obtained from any sub - tree can not be better than the lower bound obtained from the minimum entropy sub - tree by more than @xmath79 , a value that does not depend on @xmath48 .",
    "this gives us a quality guarantee for the lower bound obtained from the minimum entropy sub - tree  @xcite .",
    "[ th : thbs ] consider subsets @xmath71 and @xmath80 of @xmath72 with junction tree representations .",
    "also suppose that @xmath73 , the probability distribution over @xmath71 , has the smallest entropy and @xmath81 , the probability distribution over @xmath80 , gives the best lower bound , then the following inequality holds @xmath82     +    since @xmath83 according to theorem  [ th : twotrees ] we can write @xmath84 since @xmath81 gives the best lower bound @xmath85 the proof would be clear by adding equations ( [ eq : th51 ] ) and ( [ eq : th52 ] ) .",
    "theorem  [ th : thbs ] gives us another quality guarantee regarding the minimum entropy sub - tree ( which is the least random , least uncertain , and most biased sub - tree ) .",
    "this theorem shows that the probability distribution on the tree that gives the best bound and the minimum entropy distribution are close , where closeness is measured by divergence .",
    "the upper bound is @xmath79 which does not depend on @xmath81  @xcite .",
    "see fig .",
    "[ fig : dqq ] .    _ corollary 3 : _ in the case that @xmath86 we have the following inequality @xmath87    _ remark 1 : _ in almost all the theorems and corollaries , we insisted that the subsets of @xmath72 have junction - tree representations .",
    "this assumption can be relaxed and the theorems and corollaries would still be valid for the sub - graphs . however , having a junction tree representation makes the computation of the entropy and the partition function easier ( using gdl or any other iterative message passing algorithm ) .     and @xmath73,width=139 ]",
    "in this paper , we extended some of our previous results on bounding the partition function . in the case",
    "that the graph decomposes into two sub - trees we derived a number of divergence inequalities concerning the global probability distribution and the probability distributions on the sub - trees .",
    "we showed that the minimum entropy sub - tree has some optimality properties , namely the lower bound obtained from this tree can not be far from the lower bound obtained from any other sub - tree and the probability distribution on this tree and the probability distribution on the tree that gives the best lower bound are close where the divergence is the measure of closeness .",
    "the first author wishes to thank j. dauwels and n. macris for their helpful comments ."
  ],
  "abstract_text": [
    "<S> computing the partition function and the marginals of a global probability distribution are two important issues in any probabilistic inference problem . in a previous work , we presented sub - tree based upper and lower bounds on the partition function of a given probabilistic inference problem . using the entropies of the sub - trees we proved an inequality that compares the lower bounds obtained from different sub - trees . in this paper </S>",
    "<S> we investigate the properties of one specific lower bound , namely the lower bound computed by the minimum entropy sub - tree . </S>",
    "<S> we also investigate the relationship between the minimum entropy sub - tree and the sub - tree that gives the best lower bound . </S>"
  ]
}