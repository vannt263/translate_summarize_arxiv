{
  "article_text": [
    "over the last decade matrix valued functions of matrices have become an essential tool in a variety of sub - fields of science and engineering @xcite . an important application for matrix functions in the field of lattice qcd",
    "is the so - called overlap dirac operator , which is a discretisation of the dirac operator that respects the properly defined lattice chiral symmetry ( ginsparg - wilson relations ) and is free of doublers .",
    "therefore the overlap dirac operator is well suited for the non - perturbative study of strongly interacting chiral fermions . at finite chemical potential @xmath3",
    "the overlap dirac operator is defined as @xcite @xmath4 \\right),\\ ] ] where @xmath5 , @xmath6 is the wilson ",
    "dirac operator at chemical potential @xmath3 , @xmath7 is the matrix sign function and @xmath8 stands for the lattice spacing .",
    "an explicit form of @xmath6 is given in [ ap : wd_operator ] .    at finite chemical potential @xmath9",
    "is a non - hermitian matrix with complex eigenvalues . since the size of the linear space on which @xmath9 is defined is typically very large ( @xmath10 ) , it is not feasible to evaluate the matrix sign function exactly . while at @xmath11 one can efficiently approximate the sign function of the hermitian operator @xmath9 by polynomials or rational functions @xcite , at nonzero @xmath3 the operator @xmath9 becomes non - hermitian and such approximations typically become inefficient .",
    "however , having in mind that in practice the sign function @xmath12 $ ] is applied to some source vector , one can still construct an efficient polynomial approximation for each particular source vector by using krylov subspace methods , such as krylov  ritz - type approximations .",
    "one of the practical krylov  ritz - type approximations which are suitable for the finite - density overlap dirac operator is the two - sided lanczos ( tsl ) algorithm , developed in @xcite .",
    "the efficiency of the tsl approximation can be further improved by using a nested version of the algorithm @xcite .",
    "many practical tasks within lattice qcd simulations require the calculation of the derivatives of the lattice dirac operator with respect to the gauge fields or some other external parameters .",
    "for example , conserved lattice vector currents and fermionic force terms in hybrid monte - carlo simulations involve the derivatives of the dirac operator with respect to abelian or non - abelian gauge fields . also ,",
    "electric charge susceptibilities which are used to quantify electric charge fluctuations in quark  gluon plasma involve derivatives of the dirac operator with respect to the chemical potential .    while explicit expressions for the derivatives of source - independent approximations of matrix functions are well known and are routinely used in practical lattice qcd simulations , differentiating the implicit source - dependent approximation appears to be a more subtle problem . in principle the algorithms for taking numerical derivatives of scalar functions , like the finite difference method or algorithmic differentiation , can be generalised to matrix functions and to matrix function approximation algorithms .",
    "it is easy to combine an approximation with the finite difference method , but finite differences are very sensitive to round - off errors and it is often not possible to reach the desired precision in the derivative using this method . for algorithmic differentiation the situation is more complicated .",
    "depending on the approximation method used it might not be immediately clear how to apply algorithmic differentiation .",
    "even if algorithmic differentiation can be implemented for the approximation method this might lead to a numerically unstable algorithm .",
    "such a behaviour was observed when the tsl approximation was used in conjunction with algorithmic differentiation  @xcite .    in this paper , we propose and test a practical numerically stable method which makes it possible to compute derivatives of implicit approximations of matrix functions to high precision .",
    "the main motivation for this work is the need to take derivatives of the overlap dirac operator in order to compute conserved currents on the lattice .",
    "the structure of the paper is the following : in section [ sec : mat_func ] we state some general theorems about matrix functions and their derivatives , which provide the basis for our numerical method . in section",
    "[ sec : deflation ] we discuss how the calculation of the derivatives of matrix functions can be made more efficient by deflating a number of small eigenvalues of the matrix which is the argument of the function being differentiated .",
    "the deflation is designed with the matrix sign function and the tsl approximation in mind .",
    "nevertheless we want to emphasise that the method is very general and can be applied to other matrix functions and different matrix function approximation schemes . in section [ sec : results ] we demonstrate how the method can be used in practice .",
    "first we discuss the efficiency and convergence of the tsl approximation .",
    "after that , as a test case , we compute @xmath13 lattice vector currents , which involve the derivatives of the overlap dirac operator with respect to background abelian gauge fields , and demonstrate that they are conserved .",
    "finally we summarise and discuss the advantages and disadvantages of our method in section [ sec : discussion ] .",
    "detailed calculations and derivations as well as a pseudo code implementation of the method can be found in the appendices .",
    "for completeness we start this section with a brief review of matrix functions .",
    "let the function be defined on the spectrum of a matrix @xmath14 . there exist several equivalent ways to define the generalisation of @xmath15 to a matrix function @xmath16@xcite .",
    "for the purpose of this paper the most useful definition is via the jordan canonical form . a well known theorem states that any matrix @xmath17 can be written in the jordan canonical form @xmath18 where every jordan block @xmath19 corresponds to an eigenvalue @xmath20 of @xmath21 and has the form @xmath22 with @xmath23 .",
    "the jordan matrix @xmath24 is unique up to permutations of the blocks but the transformation matrix @xmath25 is not .",
    "using the jordan canonical form the matrix function can be defined as @xcite @xmath26 the function of the jordan blocks is given by @xmath27 note that this definition requires the existence of the derivatives @xmath28 for @xmath29 . if @xmath21 is diagonalisable every jordan block has size one and equation  ( [ eq : jordan_function ] ) reduces to the so - called spectral form @xmath30 which does not depend on the derivatives of @xmath15 .",
    "for instance , the matrix sign function @xmath31 $ ] in ( [ eq : overlap ] ) is defined by @xmath32 .",
    "finding the jordan normal form ( or the spectral decomposition ) of a matrix is a computationally expensive task and takes @xmath33 operations . for large matrices",
    "it is therefore not feasible to compute the matrix function exactly . in practical calculations",
    "it is often sufficient to know the result @xmath34 of the action of the matrix function on a vector and it is not necessary to explicitly compute the matrix @xmath35 .",
    "a variety of different methods have been developed to efficiently calculate an approximation of @xmath34 ( see chapter 13 of @xcite for an overview ) .",
    "for non - hermitian matrices @xmath21 one typically constructs the approximation to @xmath36 using the krylov subspaces spanned by the krylov vectors @xmath37 and @xmath38 , @xmath39 .",
    "this implies that the approximation depends both on the matrix @xmath21 itself and on the source vector @xmath40 .",
    "a practical example of such an approximation is the tsl algorithm of @xcite .",
    "let us now assume that the matrix @xmath41 depends on some parameter @xmath42 .",
    "in this work we are interested in the calculation of the derivative @xmath43 where @xmath44 and the source vector @xmath40 is assumed to be independent of @xmath42 .",
    "the practical method for calculating @xmath45 which we propose here is based on the following theorem @xcite :    [ th : mathias ] let @xmath46 be differentiable at @xmath47 and assume that the spectrum of @xmath48 is contained in an open subset @xmath49 for all @xmath42 in some neighbourhood of @xmath50 .",
    "let @xmath15 be @xmath51 times continuously differentiable on  @xmath52 .",
    "then @xmath53    theorem [ th : mathias ] relates the derivative of a matrix function @xmath54 to the function of a block matrix @xmath55 , and is in fact closely related to the formula ( [ eq : jordan_function ] ) .",
    "it is remarkable that one does not need to know the explicit form of @xmath56 to compute the derivative of @xmath15 .",
    "this comes at the cost of evaluating the function @xmath15 for the matrix @xmath57 that has twice the dimension of @xmath21 and one needs to know the derivative @xmath58 . in practice @xmath58",
    "is usually known analytically or can be computed to high precision .",
    "moreover the block matrix @xmath57 is very sparse and one only needs to store @xmath21 and @xmath58 .",
    "thus theorem [ th : mathias ] makes it possible to efficiently calculate the derivative of a matrix function .    using theorem [ th : mathias ] it is straightforward to compute the action of the derivative of a matrix function on a vector : @xmath59 now one can use any matrix function approximation method to compute an approximation to the function @xmath60 in equation ( [ eq : func_derivative_1 ] ) . to be specific",
    "we will use the tsl approximation for the rest of this paper , but any other approximation scheme can be used instead .",
    "the convergence properties of the tsl approximation crucially depend on the spectrum of the matrix to which it is applied . in [ ap : prop_mathias ] we show that the spectrum of eigenvalues of @xmath57 is identical to the spectrum of @xmath21 .",
    "often the efficiency of the tsl approximation can be greatly improved by deflating a small number of eigenvalues .",
    "one might for example deflate the eigenvalues that are close to a pole of the function . for the matrix sign function it is advantageous to deflate the eigenvalues smallest in absolute value@xcite .",
    "the standard deflation method relies on the diagonalisability of the matrix @xmath21 .",
    "it is then straightforward to project out the eigenvectors corresponding to the deflated eigenvalues .",
    "however the following theorem states that , in general , the matrix @xmath57 is not diagonalisable :    [ th : diag ] let @xmath21 be a diagonalisable matrix . if @xmath61 for at least one eigenvalue @xmath20 of @xmath21 then the matrix @xmath57 is not diagonalisable",
    ". if @xmath21 has no degenerate eigenvalues and @xmath61 for all @xmath62 then every jordan block in the jordan normal form of @xmath57 is of size two , i.e. @xmath63    the proof of this theorem is somewhat technical and the main steps of the proof can be found in .",
    "it follows directly from theorem [ th : diag ] that a matrix of the form @xmath57 does not possess a full basis of eigenvectors and it is therefore not immediately clear how to apply deflation to the numerical evaluation of equation  ( [ eq : func_derivative_1 ] ) .    in the following we",
    "develop a deflation method that is based on the jordan normal form of @xmath57 . since @xmath64 is the jordan matrix of @xmath57 there exists an invertible matrix @xmath65 such that @xmath66 . using theorem [ th : diag ]",
    "it is possible to derive an analytic expression for @xmath65 and @xmath67 in terms of the eigenvalues and left and right eigenvectors of @xmath21 and their derivatives .",
    "after some algebraic manipulations that are summarised in [ ap : jordan_decomp ] we get    @xmath68    where we used the eigenvalues @xmath20 and left ( right ) eigenvectors @xmath69 @xmath70 of @xmath21 .",
    "the matrix @xmath67 consists of @xmath71-dimensional row vectors , i.e. @xmath72 analogously @xmath65 is made up by @xmath71-dimensional column vectors . with the help of the matrices @xmath65 and @xmath67",
    "we can define a generalised deflation .",
    "let @xmath73 be the columns of @xmath65 and @xmath74 the rows of @xmath67 , then @xmath75 .",
    "we define the projectors @xmath76 and @xmath77 , such that for every vector @xmath78 we have @xmath79 now we have all the necessary ingredients to define a deflation algorithm for the matrix @xmath57 .",
    "say we want to deflate the first @xmath80 eigenvalues of @xmath57 ( corresponding to the first @xmath81 eigenvalues @xmath82 of @xmath21 ) . to calculate @xmath60 we split the evaluation of the function into two parts : @xmath83 where we used equation ( [ eq : matrix_func ] ) for the first term on the right side",
    ". to compute the function of the jordan matrix we only need to consider the function for each jordan block . applying equation ( [ eq : jordan_function ] ) to the @xmath84-th block of @xmath64 yields",
    ": @xmath85 since the block structure of @xmath64 is preserved by the function and the @xmath73 and @xmath74 are biorthogonal one finds : @xmath86\\end{gathered}\\ ] ] it is not necessary to compute the full transformation matrices @xmath65 and @xmath67 since the rows and columns needed to evaluate equation ( [ eq : defl_res_1 ] ) are known analytically in terms of the left and right eigenvectors of @xmath21 .",
    "the latter can be efficiently computed using the arnoldi algorithm ( we have used the implementation @xcite ) . in practical calculations one chooses @xmath87 and splits the calculation of @xmath60 in the following way @xmath88}_{\\text{exact } }    \\end{split}\\ ] ] this looks very similar to the standard deflation formula for diagonalisable matrices .",
    "the difference is that now , because the jordan blocks have size two , there is an additional `` mixing term '' proportional to @xmath89 .",
    "for the sign function equation  ( [ eq : final_res ] ) becomes even simpler .",
    "the sign function is piece - wise constant and for realistic gauge field configurations the eigenvalues of @xmath9 do not cross the discontinuity line @xmath90 when an external parameter is varied .",
    "therefore the derivative of the sign function @xmath91 is identically zero and the mixing term is absent in the deflation",
    ". a detailed explicit expression for the deflated derivative of the matrix sign function and the corresponding pseudo - code can be found in [ ap : efficient_defl ] .",
    "we note that in hybrid monte - carlo simulations with dynamical overlap fermions it is possible that an eigenvalue @xmath20 of @xmath9 crosses the discontinuity of the sign function at @xmath92 , which corresponds to a change of the topological charge .",
    "the derivative of @xmath93 in the last term in ( [ eq : final_res ] ) then becomes singular .",
    "however , in practice such singularities in the fermionic force are typically avoided by modifying the molecular dynamics process in the vicinity of the singularity , for example by using the transmission - reflection step of @xcite .",
    "for the numerical tests we have used quenched @xmath0 gauge configurations generated with the tadpole - improved lscher ",
    "weisz action  @xcite .",
    "the wilson  dirac operator with a background abelian gauge field and finite quark chemical potential @xmath3 is described in [ ap : wd_operator ] .",
    "there we also give an analytic expression for the derivative @xmath94 of the wilson  dirac operator with respect to the external lattice gauge field @xmath95 .    before we discuss our results we give a detailed description of our numerical setup .",
    "the results in this section were obtained using a nested version of the tsl algorithm  @xcite .",
    "we found that the main performance gains are already achieved with a single nesting step and that further nesting does not significantly improve the efficiency of the algorithm .",
    "therefore for all our calculations we used only one level of nesting . in this case one has to choose two parameters for the lanczos approximation , the sizes of the inner and outer krylov subspace . for the tsl method",
    "it is not known how to compute a priori error estimates . in particular",
    "it is not possible to estimate which values for the krylov subspace size parameters are necessary to reach a given precision in the calculation .",
    "an a  posteriori estimate for the numerical error @xmath96 can be computed using the identity @xmath97 : @xmath98 where @xmath99 and the factor two was added because we have to apply the tsl approximation twice to compute the square of the sign function .",
    "similarly , in order to estimate the error @xmath100 in the calculation of the derivative of the sign function we use the same formula as ( [ eq : error_est ] ) where @xmath21 is replaced by @xmath57 and the vector @xmath78 is replaced by the sparse vector @xmath101 , i.e    @xmath102    we note that the square of the sign of the block matrix @xmath57 is given by @xmath103 and the error estimate for the derivative as defined in equation ( [ eq : error_est ] ) contains the anti - commutator @xmath104 of the sign function and its derivative : @xmath105 note that the first summand in ( [ eq : ba_error_est2 ] ) is in general not equal to @xmath106 , because the optimal polynomials approximating the sign functions of @xmath21 and @xmath57 are in general different",
    ". since the square of the sign function is the identity the anti - commutator @xmath107 should vanish and its deviation from zero can be used as an indicator of the precision with which the derivative @xmath108 is calculated @xcite .",
    "we found numerically that @xmath100 gives a better estimate for the true error of the derivative and moreover it is easier to compute than the anti - commutator .    during production",
    "runs it is not feasible to check the error for every source vector @xmath78 . in general the optimal subspace sizes",
    "will depend on the matrix and on the source vector .",
    "if deflation techniques are used , however , the performance critical components of the source vector are projected out and treated exactly",
    ". then it is reasonable to assume that for a given matrix the optimal parameter values will depend only weakly on the source vector .",
    "this suggests that it is possible to find a set of optimal parameters that will give the desired error for any vector . in order to find these optimal parameters we perform a `` tuning run '' for every gauge configuration :    * select a target precision @xmath109 ( typically @xmath110 ) . *",
    "select a trial vector @xmath111 . to compare different parameter sets",
    "the trial vector should be the same throughout the tuning run .",
    "additionally it should be a good representation of the vectors that will be used in the production runs .",
    "here we use @xmath112 to estimate the error @xmath113 of @xmath114 and the sparse vector @xmath115 to estimate the error @xmath100 of @xmath116 . *",
    "choose a set of trial parameters for the outer ( @xmath117 ) and inner ( @xmath118 ) krylov subspace size @xmath119 .",
    "for every @xmath120 compute @xmath121 and the cpu time @xmath122 the tsl approximation took . *",
    "sort out all @xmath123 for which @xmath124 * from the remaining parameter sets @xmath125 choose the one with the smallest @xmath126 * save the optimal parameters @xmath127 and @xmath128 for use in the production runs    of course there is no guaranty that for some vector other than the trial vector @xmath111 the parameters @xmath127 and @xmath128 found in the tuning run will give an error smaller than @xmath109 .",
    "one way to validate the results from the tuning run is to perform a cross check :    * use @xmath127 and @xmath128 to compute the error for @xmath81 random vectors ( in practice , @xmath129 ) * check if the maximal error for all random vectors is smaller than @xmath109 .",
    "if the maximal error is found to be too large one can restart the tuning run with a different trial vector to get better estimates for the optimal parameters .",
    "we found that in most cases the error for the random vectors has the same order of magnitude as the error for the trial vector .",
    "as a rule of thumb if one wants to achieve a precision @xmath109 one should use the target precision @xmath130 for the trial run .",
    "the tsl algorithm implicitly generates a polynomial approximation of the sign function .",
    "the highest order of the approximation polynomial is given by the size of the outer krylov subspace .",
    "another way to test the validity of the optimal parameter estimate obtained in the trial run is to compare the results of the tsl approximation to some other method . to this end",
    "we have set @xmath11 and compared the relative error of the tsl approximation with the error of the minmax polynomial approximation @xcite , which works well when @xmath9 is a hermitian operator .    in figure [ fig : comp_minmax ] we compare the error of the tsl approximation for a given outer krylov subspace size with the minmax polynomial result . the error estimate ( [ eq : error_est ] ) for @xmath131 is computed for 10 random vectors on 20 different gauge configurations of size @xmath132 ( corresponding to a matrix of dimension @xmath133 ) with @xmath134 . for these parameters",
    "the spectrum of @xmath9 has only a small gap around the line @xmath135 , which makes it more difficult to approximate the matrix sign function with a polynomial of low order . to improve both the tsl and the minmax approximation we deflated the @xmath136 eigenvalues with the smallest absolute value in our calculations .",
    "the mean error is computed by averaging over random vectors and gauge configurations and we plot the mean error as a function of the maximal power of @xmath9 in the approximating polynomial ( for the nested tsl algorithm , this is the size of the outer krylov subspace ) .",
    "the results are shown in figure [ fig : comp_minmax ] .",
    "we find that at fixed polynomial degree the error for the tsl method is smaller than the error of the minmax polynomial by almost an order of magnitude .",
    "this is to be expected , since the minmax polynomial tries to minimise the maximal error over all vectors , while the tsl method constructs a different and optimised polynomial for every source vectors . for large matrices",
    "the main computational cost comes from matrix vector products and the overhead of different algorithms for finding the optimal coefficients of the approximating polynomial becomes negligible .",
    "the minmax polynomial method generates a polynomial in @xmath137 , while the tsl method constructs the krylov subspaces for both @xmath9 and @xmath138 .",
    "moreover , since storing all the krylov vectors requires very large ram memory and is hardly feasible in practice , we have used a two - pass version of the tsl method .",
    "the first pass is used to find the coefficients of the optimal approximating polynomial , and the second pass to calculate this polynomial with known coefficients .",
    "because of the twice larger number of vectors used to construct the krylov subspace and the need to calculate these vectors twice , for a given order of the approximating polynomial the minmax approach is roughly four times faster .",
    "the cpu time of the two algorithms is compared in figure  [ fig : comp_minmax ] .     on an intel^^ core^tm^ i5 - 3470 cpu , with multi - core openblas",
    "@xcite used for basic linear algebra .",
    "as expected the tsl is slower than the minmax polynomial approach by a factor of four .",
    ", title=\"fig : \" ]    on an intel^^ core^tm^ i5 - 3470 cpu , with multi - core openblas @xcite used for basic linear algebra .",
    "as expected the tsl is slower than the minmax polynomial approach by a factor of four .",
    ", title=\"fig : \" ]    our numerical test for @xmath132 configurations shows that the error of the tsl method does not strongly depend on the source vector and that it is almost an order of magnitude smaller that one would expect from a naive comparison with other approximation methods . for our production runs we therefore use a single tuning run to find an estimate for the optimal parameters .",
    "the parameter set found in this way is then used for all further calculations .      after tuning the tsl approximation for the sign function @xmath131",
    ", we are now ready to apply it to the block matrix @xmath139 .",
    "motivated by the practical calculations of conserved vector currents on the lattice , we assume that the parameter @xmath42 is the abelian lattice gauge field @xmath95 on the link which goes in the direction @xmath3 from the lattice site @xmath140 .    in figure [",
    "fig : krylov_comp ] we show typical results for the error of @xmath131 and @xmath141 at @xmath142 as a function of the outer krylov subspace size for gauge configurations ranging in size from @xmath143 ( @xmath144 ) to @xmath1 ( @xmath145 ) .",
    "the benefit of the deflation is clearly visible . in the computation of the derivatives the deflation process has an additional positive side effect . to evaluate derivatives we have to take a sparse input vector .",
    "moreover the derivative matrix in lattice qcd is very sparse , since the change of a single link influences only two lattice sides .",
    "therefore the vectors generated by the tsl algorithm have a sparse upper part and the krylov subspace does not efficiently approximate the full space . by projecting out the vectors corresponding to the eigenvalues near zero the sparse pattern of the source vector is destroyed and the krylov subspace has a more general form which positively influences the convergence rate of the method .",
    "figure [ fig : krylov_comp ] shows that the method scales very well with the lattice volume . for the configuration with @xmath146 and @xmath147 the temperature is already above the deconfinement transition temperature for the lscher ",
    "weisz action  @xcite , hence there is already a large gap in the spectrum of @xmath9 and the deflation has only a minor effect on the efficiency of the tsl method .    .",
    "( un-)deflated results are marked by ( circular ) triangular symbols .",
    "the data points are connected with lines to guide the eye . a solid line stands for the sign function results while a broken line indicates the results for the derivative .",
    "clockwise from top left the results are shown for @xmath148 , @xmath149 , @xmath1 and @xmath2 lattices . for @xmath148 , @xmath149 , @xmath1 lattices",
    "we have used @xmath134 ( @xmath150 ) and for @xmath2 lattice we have used @xmath147 ( @xmath151 ) .",
    "the inner krylov subspace size is set to @xmath152 in all plots .",
    "for the deflation of the sign function we use the @xmath153 eigenvalues with the smallest magnitude . to deflate the derivative , we have used two eigenvalues for @xmath148 , @xmath149 and @xmath2 lattices and six eigenvalues for @xmath1 . ]    an interesting question is how the optimal krylov subspace size for a given error depends on the chemical potential . as the chemical potential",
    "is increased the operator @xmath9 deviates more and more from a hermitian matrix and one expects that a larger krylov subspace size is necessary to achieve a given accuracy .",
    "we indeed observe this behaviour and figure [ fig : krylov_comp_mu ] shows the results for the error at different values of @xmath3 . from the plots we can estimate that at @xmath142 an outer krylov subspace size of 500 is sufficient to obtain an error of @xmath154 for the deflated derivative for the @xmath155 configuration . for @xmath156 one",
    "has to use a subspace size of roughly 600 for the same error . at @xmath157",
    "it seems that the error can be achieved with a subspace size of around 650 .",
    "going from zero chemical potential to a finite value of @xmath156 makes it necessary to increase the krylov subspace size by about @xmath158 . once the chemical potential is switched on , however , the increase in the krylov subspace size is not that dramatic . between @xmath156 and the rather large value @xmath159",
    "the increase in the krylov subspace size is roughly @xmath160 .     and @xmath161 on the left and @xmath162 on the right .",
    "bottom : @xmath163 and @xmath156 on the left and @xmath157 on the right .",
    "all other parameters are the same as for figure [ fig : krylov_comp ] . ]    for the @xmath146 configuration switching on a small chemical potential only has a negligible effect on the error for a given krylov subspace size . if @xmath162 one has to take a krylov subspace size of approximately 350 to get an error of @xmath154 .",
    "compared to the krylov subspace size of 280 for @xmath142 this is an increase of about @xmath164 .      as a further practice - oriented test of our method",
    ", we now consider the divergence of the @xmath13 vector current @xmath165 at a randomly chosen lattice site @xmath140 for a fixed gauge field configuration which was randomly selected from an ensemble of equilibrium gauge field configurations .",
    "for a fixed gauge field configuration , the current @xmath166 flowing in the direction @xmath3 from the lattice site @xmath140 is given by @xmath167 invariance of the dirac operator under the gauge transformations @xmath168 demands that the divergence of the vector current at a given lattice site vanishes .",
    "for small lattice sizes the trace in ( [ eq : lat_vec_cur ] ) can be calculated exactly , but for larger volumes this is no longer feasible .",
    "we use stochastic estimators with @xmath169-noise @xcite to compute an approximation of the trace for all currents in ( [ eq : divergence_def ] ) .",
    "the error bars on our results are given by the standard estimate for the error of the sample mean .    as a further check we additionally compute the `` total current '' at a lattice site , which is just the sum over incoming and outgoing currents ( replacing the minus sign in ( [ eq : divergence_def ] ) with a plus ) .",
    "this quantity has no physical meaning and can take any value . if the total current is not zero an exact cancellation is necessary to achieve a vanishing divergence . finding numerically that the divergence vanishes even when the total current does not can be seen as an additional cross check",
    "figure  [ 3x4b8.1_divtot ] shows the results for the divergence and the total current at finite @xmath3 for two configurations of different size . for both configurations",
    "we find that the total current has a finite value but the divergence vanishes as the number of stochastic estimators is increased .     and",
    "@xmath157 , plotted as functions of the number of stochastic estimators . on the left @xmath170 .",
    "for this small configuration the exact values can be computed and are indicated by solid blue lines . on the right @xmath171 .",
    "here it is not feasible to compute the exact values .",
    "the results for the total current are shifted by 100 estimators to the right for better visibility and the blue line marks zero.,title=\"fig : \" ]   and @xmath157 , plotted as functions of the number of stochastic estimators . on the left @xmath170 . for this small configuration",
    "the exact values can be computed and are indicated by solid blue lines .",
    "on the right @xmath171 .",
    "here it is not feasible to compute the exact values .",
    "the results for the total current are shifted by 100 estimators to the right for better visibility and the blue line marks zero.,title=\"fig : \" ]    results for larger configurations are shown in figure [ fig : large_divtot ] . computing a large number of stochastic estimators is very expensive even for relatively small lattice sizes . for the larger lattices we therefore studied the current conservation only for the case @xmath172 .",
    "we found that the divergence of the current as well as the total current is very small for both configurations . to see a clear separation between the divergence and the total current",
    "one would have to significantly increase the number of stochastic estimators .",
    "the value of the divergence is in both cases consistent with zero and we emphasise that the error bars clearly show the expected inverse square root dependence on the number of estimators .     of stochastic estimators .",
    "left @xmath173 with @xmath147 , right @xmath174 with @xmath134 .",
    "the chemical potential is @xmath172 for both plots .",
    "the results for the total current are shifted by 50 estimators to the right for better visibility .",
    "the straight blue line marks zero and the curved blue lines depicting @xmath175 are drawn to visualise the error dependence on the number of estimators.,title=\"fig : \" ]   of stochastic estimators .",
    "left @xmath173 with @xmath147 , right @xmath174 with @xmath134 .",
    "the chemical potential is @xmath172 for both plots .",
    "the results for the total current are shifted by 50 estimators to the right for better visibility .",
    "the straight blue line marks zero and the curved blue lines depicting @xmath175 are drawn to visualise the error dependence on the number of estimators.,title=\"fig : \" ]",
    "in this work we have proposed and tested an efficient numerical method to compute derivatives of matrix functions of general complex matrices . in particular we have shown that our method in combination with the tsl approximation can be used to compute derivatives of the matrix sign function with high precision .",
    "when one computes an approximation of a matrix function it is often possible to improve the efficiency of the algorithm by deflating a relatively small number of eigenvalues and treating them exactly .",
    "for this reason we have also presented a generalised deflation method that does not depend on the diagonalisability of the matrix .    in practical calculations",
    "it is important to know ( an estimate for ) the error of an algorithm .",
    "we give a heuristic for tuning the parameters of our method to a given error .",
    "to check how reliable the error estimate is we computed the error for a variety of random source vectors on a number of medium sized ( @xmath132 ) gauge configurations . as a cross check we compared the errors of the tsl approximation with the errors obtained by a minmax polynomial approximation .",
    "we found that our heuristic works very well and gives a good error estimate .",
    "the error of the tsl method is consistently lower than the error of the minmax polynomial by almost an order of magnitude .",
    "the tsl method constructs two krylov subspaces and we have to use a double - pass version of the algorithm because of memory limitations , which makes the tsl method about a factor of four slower than the minmax polynomial for a given order of the interpolating polynomial .",
    "we note that our findings suggest that a lanczos type algorithm that is optimised for hermitian matrices could outperform the minmax polynomial approximation .",
    "the lanczos method for hermitian matrices constructs only one krylov subspace size , which reduces the computational effort and the memory requirements by a factor two .",
    "therefore a single - pass version of the lanczos algorithm could be feasible even for relatively large hermitian matrices . at a given precision",
    ", such an algorithm would be considerably faster than the minmax approximation .",
    "we have tested our method for the derivatives of the finite - density overlap dirac operator on different gauge configurations and for different values of the chemical potential @xmath3 .",
    "the higher the value of @xmath3 the more the operator @xmath9 differs from a hermitian matrix .",
    "we find that our method performs very well even for large values of @xmath3 up to @xmath176 in physical units and that the krylov subspace size necessary to achieve a given error does not strongly depend on @xmath3 .",
    "for all configurations we compared the deflated and the undeflated version of the algorithm . with our choice of parameters in the confining phase the operator @xmath9 in general only has a rather small gap in the spectrum around the discontinuity line @xmath135 and deflation greatly improves the performance of the method .",
    "on the other hand we find numerically that in the deconfinement phase the gap in the spectrum of @xmath9 is larger and there are no eigenvalues with real part close to zero . in this case",
    "the deflation has only a minor effect .    in the deflated version of our method a large fraction of the overall cpu time",
    "is spent for the inversion of the matrices @xmath177 in equation  ( [ eq : c1i_2 ] ) .",
    "all the inversions act on the same source vector and if it is possible to use a multi - shift algorithm for the inversions the performance of the method could be significantly improved .",
    "we tried to use the bicgstab algorithm to compute @xmath178 , since multi - shift versions of this algorithm exist .",
    "it turned out , however , that this approach is numerically unstable and in the end we computed the inverse by applying the cg algorithm to the matrix @xmath179 . thus finding a suitable multi - shift inverter is one of the possible ways to speed up our algorithm .",
    "equivalents of theorem [ th : mathias ] exist also for higher order derivatives of matrix functions @xcite and in principle it is straightforward to generalise our method for higher derivatives . to compute the @xmath180-th derivative of a function of a @xmath181-dimensional matrix one has to construct an upper - triangular block matrix of dimension @xmath182 .",
    "in particular , second - order derivatives are required for the calculation of current  current and charge  charge correlators , from which one can extract electric conductivity and charge diffusion rate . however , in the case of higher derivative the expressions for the deflated derivatives become extremely complicated .",
    "it seems thus that the only practical way to calculate higher derivatives is to avoid deflation , which necessarily involves restriction to small lattice sizes or to high temperatures .",
    "an important application of our method and the main motivation for this work is the computation of conserved currents for the finite - density overlap dirac operator in lattice qcd . conserved currents and current - current correlators are important observables for the study of anomalous transport effects such as the chiral magnetic @xcite or the chiral separation @xcite effects . these dissipation - less parity - odd transport effects , which originate from the chiral anomaly , have recently become the focus of intensive studies in both high - energy and solid - state physics . while the anomalous transport for free chiral fermions is well understood , there are still many interesting open questions for strongly interacting fermions . with our method",
    "it is possible to efficiently study anomalous transport for strongly interacting chiral fermions on the lattice at finite quark chemical potential .",
    "we thank andreas frommer for pointing us to theorem [ th : mathias ] , and jacques bloch and simon heybrock for useful discussions of the lanczos approximation .",
    "this work was supported by the s.  kowalevskaja award from the alexander von humboldt foundation .",
    "the wilson  dirac operator @xmath6 for a single quark flavour at finite quark chemical potential @xmath3 and in the presence of a background @xmath183 gauge field can be written as @xmath184 with @xmath185 the @xmath186 are ( dynamical ) lattice gauge fields and the factors @xmath187 describe the ( background ) lattice gauge fields corresponding to the external abelian gauge field @xmath188 .",
    "we set the lattice spacing to one and define the hopping parameter @xmath189 , where @xmath190 is the wilson mass term .",
    "the matrices @xmath191 are the euclidean dirac matrices .    computing the derivative of the wilson ",
    "dirac operator with respect to the abelian gauge field is straightforward and the result reads as    @xmath192",
    "in this appendix we formally define the notion of the derivative of eigenvectors and summarise some useful results .",
    "let @xmath48 be a diagonalisable matrix which depends on a parameter @xmath42 and has eigenvalues @xmath20 and ( left ) eigenvectors ( @xmath69 ) @xmath193 . for brevity",
    "we assume that @xmath21 has no degenerate eigenvalues , i.e. @xmath194 if @xmath195 .",
    "for the wilson  dirac operator on real lattice qcd configurations , this is usually the case , since configurations with degenerate eigenvalues form a set of measure zero in the space of all gauge field configurations .    if @xmath196 is an eigenvector of @xmath21 so is @xmath197 for any @xmath198 .",
    "the direction of an eigenvector is fixed , but its norm and phase are not . in practice",
    "a common choice is to fix the eigenvectors by requiring that the left and right eigenvectors are bi - orthonormal : @xmath199 in general @xmath200 can be any non - vanishing differentiable function @xmath201 .",
    "the freedom in choosing @xmath200 leads to a freedom in the norm and the direction of the derivative of the eigenvector , since @xmath202 this means the derivative of an eigenvector is fixed only up to a multiplication with a scalar and the addition of any vector from @xmath203 .",
    "therefore it is necessary to specify which one of all this possible derivatives is used in a certain calculation . requiring the normalisation ( [ eq : eignorm ] ) only leads to the restriction @xmath204 and does not fully fix the derivatives of the eigenvectors . throughout this paper",
    "we will therefore employ the additional constraint @xmath205 so that the derivative of an eigenvector is well defined .",
    "it is always possible to choose the eigenvectors such that ( [ eq : eig_deriv_constr ] ) is fulfilled .",
    "note that condition ( [ eq : eignorm ] ) does not fix the norm of the vectors @xmath196 and @xmath206 .",
    "suppose we found vectors that obey ( [ eq : eignorm ] ) .",
    "then @xmath207 , where @xmath208 is either zero or purely imaginary .",
    "if we now define @xmath209 and @xmath210 we find that @xmath211 and @xmath212 .",
    "using the definitions above we will now derive some useful relations .",
    "we start with the eigenvalue equation @xmath213 taking the derivative on both sides yields @xmath214 multiplying from the left by @xmath206 gives the following relation for the derivative of the eigenvalue : @xmath215 to derive a similar result for the derivative of the eigenvectors multiply ( [ eq : eig_deriv ] ) from the left by @xmath69 for some @xmath216 . with the normalisation ( [ eq : eignorm ] )",
    "this gives @xmath217 therefore the following equation holds for @xmath216 : @xmath218 multiplying this equation by @xmath193 from the left and summing over @xmath216 yields @xmath219 where we used ( [ eq : eig_deriv_constr ] ) and the identity @xmath220 to make the replacement @xmath221 . similarly we obtain for the derivative of the left eigenvectors @xmath222",
    "the convergence properties of matrix function approximation methods in general depend on the spectrum of the matrix .",
    "it is therefore important to know the spectrum of @xmath57 .",
    "let @xmath57 be defined as in theorem [ th : mathias ] and let @xmath20 , @xmath223 be the eigenvalues of @xmath21 . since @xmath57 is an upper block matrix @xmath224 . from this",
    "it immediately follows that the eigenvalues of @xmath57 are degenerate and identical to the @xmath20 .",
    "a matrix is diagonalisable if and only if its minimal polynomial is a product of _ distinct _ linear factors .",
    "we will now discuss the outline of a proof of theorem [ th : diag ] , which states that in general the matrix @xmath57 is not diagonalisable . in the following",
    "we assume that @xmath21 has no degenerate eigenvalues to simplify the argumentation .",
    "the generalisation to the case of degenerate eigenvalues is possible but a bit more involved . for every eigenvalue @xmath20 of @xmath21",
    "we define the two vectors @xmath225 it is easy to convince oneself that these vectors are linearly independent and that @xmath226 is an eigenvector of @xmath57 to the eigenvalue @xmath20 . for @xmath227",
    "we have @xmath228 the vector @xmath227 is an eigenvector of @xmath57 only if @xmath229 vanishes . if @xmath230 we find @xmath231 and therefore @xmath227 is a generalised eigenvector of rank two corresponding to the eigenvalue @xmath20 . from this and the fact that the algebraic multiplicity of the eigenvalue @xmath20 of @xmath57 is two",
    "it immediately follows that the multiplicity of @xmath20 in the minimal polynomial of @xmath57 is also two .",
    "this proves the first part of theorem  [ th : diag ] .    to see that the second part of theorem [ th : diag ] is true ,",
    "note that for every eigenvalue of @xmath21 we have at least one jordan block .",
    "moreover the size of the largest jordan block belonging to an eigenvalue @xmath20 is the multiplicity of the eigenvalue in the minimal polynomial .",
    "therefore if the eigenvalues are all pairwise distinct there are at least @xmath181 jordan blocks of size @xmath232 and since the dimension of @xmath57 is @xmath71 this proves the theorem .",
    "the aim of this appendix is to derive the analytic form of the jordan decomposition of @xmath57 in terms of the eigenvectors and eigenvalues of @xmath21 . in practice one",
    "never encounters a matrix @xmath21 with degenerate eigenvalues .",
    "moreover if one or more of the derivatives of the eigenvalues of @xmath21 vanishes the jordan matrix of @xmath57 only becomes simpler .",
    "therefore in this paper we assume that every jordan block of @xmath57 is of size two.the generalisation to the case where some of the jordan blocks have size one is straightforward .    if @xmath64 is the jordan normal form of the matrix @xmath57 there exists an invertible matrix @xmath65 such that @xmath233 , i. e. @xmath234 the exact form of the jordan matrix @xmath64 follows form theorem [ th : diag ] and can be exploited to compute the transformation matrix @xmath65 . in bra ",
    "ket notation the transformation matrix reads as @xmath235 .",
    "evaluating the right hand side of equation ( [ eq : jordan ] ) yields @xmath236 combining equations ( [ eq : jordan ] ) and ( [ eq : jordan_rhs1 ] ) leads to @xmath181 coupled equations @xmath237 where @xmath238 .",
    "equation ( [ eq : eigenvalue ] ) is just the eigenvalue equation for the matrix @xmath57 and is easy to solve .",
    "assume that @xmath196 is an eigenvector of @xmath21 to the eigenvalue @xmath239 , then it is straightforward to show that @xmath240 is an eigenvector of @xmath57 to the same eigenvalue .    using the solution of equation ( [ eq : eigenvalue ] ) and defining @xmath241 equation ( [ eq : coupled ] )",
    "can be written as @xmath242 which simplifies to    @xmath243    equation in the system ( [ eq : sol3 ] ) is again the eigenvalue equation for @xmath21 and the solution is simply @xmath244 , where @xmath245 is a finite complex number . using this result equation ( ) becomes @xmath246    let @xmath247 denote the set of left eigenvectors of @xmath21 , i.e. @xmath248 and assume the normalisation @xmath249 . then @xmath250 is well defined . multiplying both sides of equation ( [ eq : sol4 ] ) by @xmath251 yields @xmath252",
    "the term @xmath253 on the right hand side of equation ( [ eq : sol5 ] ) is a projector to the space @xmath254 and using equation ( [ eq : eigenvec_deriv_3 ] ) one finds that the right hand side is equal to @xmath255 .",
    "it follows that the projection of @xmath256 is equal to @xmath257 and therefore @xmath258 where @xmath259 is a complex number .",
    "note that @xmath196 is in the kernel of @xmath260 , which means we can add any scalar multiple of @xmath196 to the solution @xmath256 of equation ( ) and get another solution .",
    "exploiting this freedom it is possible to set @xmath261 . to compute the value of @xmath262 simply multiply equation ( ) by @xmath206 from the left .",
    "this annihilates the first term on the left hand side and we obtain @xmath263 applying equation ( [ eq : eigval_der ] ) gives @xmath264 .",
    "putting everything together yields an analytic expression for the columns of the matrix @xmath65 : @xmath265 to compute the columns of @xmath67 we start with the equation @xmath266 which follows directly from equation ( [ eq : jordan ] ) .",
    "it turns out that it is advantageous to consider the transpose of this equation @xmath267 where @xmath268 is introduced to simplify the notation .",
    "the right hand side of equation ( [ eq : inverse_2 ] ) can be written as @xmath269 the next steps are analogous to the derivation of the columns of @xmath65 above .",
    "let @xmath270 be the @xmath271th column of @xmath272 , then ( [ eq : jordan_rhs ] ) is equivalent to @xmath181 systems of two equations : @xmath273 equation ( [ eq : inv_eigenvalue ] ) is an eigenvalue equation and the solution is @xmath274 , where @xmath275 is the eigenvector of @xmath276 to the eigenvalue @xmath239 and @xmath277 is a scalar constant that will be fixed later by requiring @xmath278 .    with the notation @xmath279 we can rewrite equation ( [ eq : inv_coupled ] ) as a system of two equations : @xmath280 mimicking the steps used to solve the system ( [ eq : sol3 ] ) one finds the following expressions for the columns of @xmath272 : @xmath281 the rows of the inverse transformation matrix @xmath282 follow immediately from equation ( [ eq : inv_final ] ) : @xmath283 where we used the fact that @xmath284 .    to find the values of the complex constants @xmath277 one has to evaluate the product @xmath285 . in this computation one",
    "encounters only four different types of bra ",
    "ket products , which are all shown in the following matrix product : @xmath286 the entry below the diagonal is trivially zero and the super - diagonal entry vanishes because @xmath287 . with",
    "the choice @xmath288 the right hand side of equation ( [ eq : inv_prod ] ) becomes the unit matrix and the explicit form of the transformation matrices is given by equations ( [ eq : xinv_final ] ) and ( [ eq : x_final ] ) in the main text .",
    "in this appendix an efficient algorithm for the deflation of derivatives of the sign function is developed .",
    "let @xmath193 and @xmath69 be the left and right eigenvectors of @xmath21 to the eigenvalue @xmath289 , respectively . considering equation  ( [ eq : func_derivative_1 ] ) it is tempting to exploit the sparsity of @xmath290 to simplify the deflation calculations .",
    "however it turns out that it is necessary to define the deflation for general @xmath291 .",
    "the reason is that the most convenient way to estimate the error of the tsl approximation is via equation  ( [ eq : error_est ] ) , i.e. we apply the tsl approximation twice and measure the deviation from unity .",
    "therefore we need a deflation method that works with general input vectors . in analogy to equation  ( [ eq : final_res ] )",
    "we then get @xmath292 + \\begin{pmatrix } { \\ensuremath{{\\ensuremath{\\left|{\\ensuremath{\\partial_{t}}}r_{i}\\right\\rangle } } } } \\\\ { \\ensuremath{{\\ensuremath{\\left|r_{i}\\right\\rangle } } } } \\end{pmatrix } { \\ensuremath{\\left\\langle\\left.l_i\\right| x_2      \\right\\rangle } } \\right\\}}_{\\text{{\\uppercase\\expandafter{\\romannumeral1 } } } } \\\\",
    "\\label{eq : array_app_defl_1 } & & + \\underbrace{\\operatorname{sgn}({\\ensuremath{\\mathcal{b}}})\\bar{\\mathcal{p}}_{2k}\\begin{pmatrix } { \\ensuremath{\\left|x_1\\right\\rangle } } \\\\ { \\ensuremath{\\left|x_2\\right\\rangle } }    \\end{pmatrix}}_{\\text{{\\uppercase\\expandafter{\\romannumeral2}}}},\\end{aligned}\\ ] ] where we used the fact that @xmath293 since the sign function is piece - wise constant and introduced the notation @xmath294 .",
    "let us now investigate part   of ( [ eq : array_app_defl_1 ] ) .",
    "@xmath295}_{c_{1i } } + \\begin{pmatrix } { \\ensuremath{{\\ensuremath{\\left|{\\ensuremath{\\partial_{t}}}r_{i}\\right\\rangle } } } } \\\\ { \\ensuremath{{\\ensuremath{\\left|r_{i}\\right\\rangle } } } } \\end{pmatrix } \\underbrace{{\\ensuremath{\\left\\langle\\left.l_i\\right| x_2      \\right\\rangle}}}_{c_{2i } } \\right\\}\\end{aligned}\\ ] ] in a practical calculation we are interested in finding an efficient way to compute the coefficients @xmath296 and @xmath297 .",
    "note that these coefficients are proportional to the scalar products of @xmath298 with the odd and even rows of the matrix @xmath67 respectively .",
    "the same coefficients appear in the projection @xmath299 , which is needed to compute part .",
    "apart from @xmath296 and @xmath297 the only non - trivial part of the deflation is the computation of @xmath300 .",
    "as we mentioned earlier , the left and right eigenvectors @xmath69 and @xmath193 can be computed with routines .",
    "the first part of the coefficients @xmath296 is again a scalar product .",
    "the computation of the second part is more involved .",
    "first we use equation [ eq : eigenvec_deriv_left ] to get rid of the derivative of the eigenvector : @xmath301 computing all the eigenvectors of @xmath21 is in general way too expensive and only the first @xmath180 eigenvectors are known explicitly .",
    "the trick now is to use the identity @xmath302 , where @xmath303 .",
    "note that the inverse of @xmath304 is well defined for @xmath305 and we can write @xmath306 in practical applications the matrix @xmath307 is sparse and its inverse can be computed very efficiently with iterative methods . since the vector @xmath308 in equation ( [ eq : c1i_2 ] ) is the same for all @xmath309 with @xmath310 it is in principle possible to use a multi - shift inversion algorithm to compute the inversions . in practice , however , we found that the numerical inversion with the ( multi - shift ) bicgstab algorithm was unstable . to avoid stability issues we used the cg algorithm to find the inverse of the hermitian matrix @xmath311 , from which it is straightforward to compute the inverse of @xmath312 .",
    "the vector @xmath193 lies in the kernel of the matrix @xmath312 . for this reason and because of numerical errors the vector @xmath313 can have non - zero components in @xmath193 direction .",
    "remember that we normalised the derivative of the eigenvectors such that @xmath314 . in order to enforce this normalisation in a numerical calculation",
    "we have to project out the spurious @xmath193 component . to this end",
    "we define the projection operator @xmath315 in the following way @xmath316 with this operator we can now write down the final equation for @xmath317 that can be used in numerical calculations @xmath318 analogously one can use equation ( [ eq : eigenvec_deriv_3 ] ) to derive the following formula for the derivative of the right eigenvectors @xmath319 we now have all the parts needed for an efficient computation of the deflation .",
    "the whole computation is summarised in the following code listing :    //",
    "compute @xmath320 // with deflation // compute @xmath300 @xmath321 @xmath322 @xmath323 // compute @xmath297 @xmath324 = { \\ensuremath{\\left\\langle\\left.l_i\\right| x_2      \\right\\rangle}}$ ] // compute @xmath296 , the exact part and the projection of @xmath40 @xmath325 @xmath326 @xmath327 @xmath328 @xmath329{\\ensuremath{{\\ensuremath{\\left|r_{j}\\right\\rangle}}}}}{\\lambda_i-\\lambda_j}$ ] @xmath330 = { \\ensuremath{{\\ensuremath{\\left\\langlel_{i}\\right|}}}}\\left(({\\ensuremath{\\partial_{t}}}a ) { \\ensuremath{\\left|v\\right\\rangle } } + { \\ensuremath{\\left|x_1\\right\\rangle}}\\right)$ ] // exact part of output @xmath331 @xmath332{\\ensuremath{{\\ensuremath{\\left|r_{i}\\right\\rangle}}}}+c2[i]{\\ensuremath{{\\ensuremath{\\left|{\\ensuremath{\\partial_{t}}}r_{i}\\right\\rangle}}}}\\right)$ ] @xmath333{\\ensuremath{{\\ensuremath{\\left|r_{i}\\right\\rangle}}}}\\right ) $ ] //",
    "projection of input vector @xmath334{\\ensuremath{{\\ensuremath{\\left|r_{i}\\right\\rangle}}}}+c2[i]{\\ensuremath{{\\ensuremath{\\left|{\\ensuremath{\\partial_{t}}}r_{i}\\right\\rangle}}}}\\right)$ ] @xmath335{\\ensuremath{{\\ensuremath{\\left|r_{i}\\right\\rangle}}}}\\right ) $ ] //",
    "exact part plus tsl approximation of projected part @xmath336 @xmath337              j.  van  den eshof , a.  frommer , t.  lippert , k.  schilling , h.  a. van  der vorst , numerical methods for the qcd overlap operator : i. sign - function and error bounds , comput.phys.commun . 146 ( 2002 ) 203224 .",
    "http://arxiv.org/abs/hep-lat/0202025 [ ] .",
    "j.  c.  r. bloch , t.  breu , a.  frommer , s.  heybrock , k.  schfer , t.  wettig , short - recurrence krylov subspace methods for the overlap dirac operator at nonzero chemical potential , comput.phys.commun . 181",
    "( 2010 ) 13781387 .",
    "http://arxiv.org/abs/0910.1048 [ ] .",
    "m.  puhr , p.  v. buividovich , a method to calculate conserved currents and fermionic force for the lanczos approximation to the overlap dirac operator , pos lat2014 ( 2014 ) 047 .",
    "http://arxiv.org/abs/1411.0477 [ ] .",
    "r.  b.  lehoucq , d.  c.  sorensen , c.  yang , http://epubs.siam.org/doi/book/10.1137/1.9780898719628[arpack users guide : solution of large - scale eigenvalue problems with implicitly restarted arnoldi methods , siam , 1998 . ]",
    "n.  cundy , s.  krieg , g.  arnold , a.  frommer , t.  lippert , k.  schilling , numerical methods for the qcd overlap operator iv : hybrid monte carlo , comput.phys.commun . 180 ( 2009 ) 2654 .",
    "http://arxiv.org/abs/hep-lat/0502007 [ ] ."
  ],
  "abstract_text": [
    "<S> we present a method for the numerical calculation of derivatives of functions of general complex matrices . </S>",
    "<S> the method can be used in combination with any algorithm that evaluates or approximates the desired matrix function , in particular with implicit krylov  ritz - type approximations . </S>",
    "<S> an important use case for the method is the evaluation of the overlap dirac operator in lattice quantum chromodynamics ( qcd ) at finite chemical potential , which requires the application of the sign function of a non - hermitian matrix to some source vector . </S>",
    "<S> while the sign function of non - hermitian matrices in practice can not be efficiently approximated with source - independent polynomials or rational functions , sufficiently good approximating polynomials can still be constructed for each particular source vector . </S>",
    "<S> our method allows for an efficient calculation of the derivatives of such implicit approximations with respect to the gauge field or other external parameters , which is necessary for the calculation of conserved lattice currents or the fermionic force in hybrid monte - carlo or langevin simulations . </S>",
    "<S> we also give an explicit deflation prescription for the case when one knows several eigenvalues and eigenvectors of the matrix being the argument of the differentiated function . </S>",
    "<S> we test the method for the two - sided lanczos approximation of the finite - density overlap dirac operator on realistic @xmath0 gauge field configurations on lattices with sizes as large as @xmath1 and @xmath2 .    </S>",
    "<S> chiral lattice fermions , finite density qcd , krylov subspace methods , numerical differentiation </S>"
  ]
}