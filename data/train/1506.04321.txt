{
  "article_text": [
    "the statistical theory of undirected graphical models for continuous variables is usually based on the assumption of multivariate normality . in practice",
    ", data may deviate from the normal model in various ways .",
    "outliers and heavy tails pose a problem of particular gravity : they frequently occur , and the normal likelihood methods , such as the sample covariance matrix , are very susceptible to them .",
    "our objective is to deal with heavy - tailed data and to safeguard graphical modelling against the impact of faulty outliers .",
    "we restrict our attention to the case where we have only continuous variables and only undirected edges .",
    "joint multivariate normality is often assumed in this situation , and the statistical methodology is called gaussian graphical modelling .",
    "we propose the class of elliptical distributions as a more general model and call our approach elliptical graphical modelling .",
    "the lack of robustness of gaussian graphical modelling has been noted by several authors .",
    "four proposals of robust approaches to gaussian graphical modelling are known to us : @xcite and @xcite suggest replacing the sample covariance matrix by the reweighted minimum covariance determinant estimator . @xcite",
    "propose an alternative m - type estimation , and finegold & drton ( arxiv:1009.3669 ) consider robustified versions of the graphical lasso by @xcite .",
    "this article delivers a systematic treatment of the plug - in approach used in the first two references .",
    "we show that the sample covariance matrix may be replaced by any affine equivariant , root-@xmath0-consistent estimator . as long as ellipticity can be assumed ,",
    "the classical gaussian graphical modelling tools can be employed with simple adjustments .",
    "thus the data analyst is free to choose the appropriate estimator , delivering the degree of robustness necessary for the data situation at hand . in order to reduce the search space , graphical modelling",
    "is often restricted to decomposable graphical models , which allow better interpretability , cf .",
    "* chapter  12 ) , but are also easier to handle mathematically . for conciseness",
    "we restrict our derivations to decomposable models .",
    "we close this section by introducing some mathematical notation . depending on the context ,",
    "the symbol @xmath1 means distributed as or asymptotically equivalent .",
    "finite index sets are denoted by small greek letters .",
    "subvectors and submatrices are referenced by subscripts , e.g.  for @xmath2 the @xmath3 matrix @xmath4 is obtained from @xmath5 by deleting all rows that are not in @xmath6 and all columns that are not in @xmath7 .",
    "similarly , the @xmath8 matrix @xmath9 is obtained from @xmath5 by putting all rows not in @xmath6 and all columns not in @xmath7 to zero .",
    "we view this matrix operation as two operations performed sequentially : first @xmath10 extracting the submatrix and then @xmath11 writing it back on a blank matrix at the coordinates specified by @xmath6 and @xmath7 .",
    "of course , the latter is not well defined without the former , but this allows us to write @xmath12 , for example .",
    "subscripts have priority over superscripts , @xmath13 stands for @xmath14 .",
    "let @xmath15 and @xmath16 be the sets of all symmetric , respectively positive definite @xmath8 matrices , and define @xmath17 as the diagonal matrix having the same diagonal as @xmath18 .",
    "the kronecker product @xmath19 of two matrices @xmath20 is defined as the @xmath21 matrix with entry @xmath22 at position @xmath23 .",
    "let @xmath24 be the unit vectors in @xmath25 and @xmath26 the @xmath27-vector consisting only of ones .",
    "define the matrices : @xmath28 where @xmath29 denotes the @xmath21 identity matrix ; @xmath30 is also called the commutation matrix .",
    "finally , let @xmath31 be the @xmath32-vector obtained by stacking the columns of @xmath33 from left to right underneath each other .",
    "more on these concepts and their properties can be found in @xcite .",
    "we introduce elliptical graphical models in analogy to gaussian graphical models . for details on the latter see @xcite , @xcite , @xcite or @xcite .",
    "consider the class @xmath34 of all continuous , elliptical distributions on @xmath25 .",
    "a continuous distribution @xmath35 on @xmath36 is said to be elliptical if it has a density @xmath37 of the form @xmath38 for some @xmath39 and symmetric , positive definite @xmath8 matrix @xmath40 .",
    "we call @xmath5 the shape matrix of @xmath35 , and denote the class of all continuous elliptical distributions on @xmath36 with the parameters @xmath41 and @xmath5 by @xmath42 . a continuous distribution on @xmath25 is called spherical if @xmath5 is proportional to the identity matrix .",
    "the shape matrix @xmath5 is unique only up to scale , that is , @xmath43 for any @xmath44 .",
    "several forms of standardization have been suggested in the literature . @xcite",
    "argues for @xmath45 . for our considerations",
    "the standardization of @xmath5 is irrelevant , and we understand the shape of an elliptical distribution as an equivalence class of positive definite random matrices being proportional to each other and call any matrix @xmath40 satisfying ( [ density ] ) for a suitable function @xmath46 a shape matrix of @xmath35 .",
    "we likewise view its inverse @xmath47 , which we call a pseudo concentration matrix of @xmath35 .",
    "furthermore let @xmath48 and @xmath49 .",
    "the function @xmath50 is invariant to scale changes , i.e. , @xmath51 is a uniquely defined parameter of @xmath52",
    ". the diagonal elements of @xmath51 are equal to @xmath53 . if the second - order moments of @xmath54 exist , then @xmath55 is proportional to @xmath5 .",
    "consequently , the element @xmath56 of @xmath51 at position @xmath57 is the partial correlation of @xmath58 and @xmath59 given the other components of @xmath60 ( * ? ? ?",
    "* chapter  5 ) .",
    "we call @xmath51 the generalized partial correlation matrix of @xmath35 and refer to it as partial correlation matrix for brevity .",
    "the qualitative information of @xmath51 can be coded in an undirected graph @xmath61 , where @xmath62 is the vertex set and @xmath63 the edge set , in the following way : the variables @xmath64 are the vertices , and an edge is drawn between @xmath58 and @xmath59 if and only if @xmath65 @xmath66 .",
    "the graph @xmath67 thus obtained is called the generalized partial correlation graph of @xmath35 .",
    "formally we set @xmath68 and write the elements of @xmath63 as unordered pairs @xmath69 @xmath66 .",
    "the global and the local markov property with respect to any generalized partial correlation graph @xmath67 are equivalent for any @xmath70 without any moment assumptions @xcite .",
    "let @xmath71 be the subset of @xmath16 consisting of all positive definite matrices with zero entries at the positions specified by the graph @xmath61 , i.e. , @xmath72 and define @xmath73 to be the elliptical graphical model induced by @xmath67 .",
    "we call the model @xmath74 decomposable if @xmath67 is decomposable , i.e. , if it possesses no chordless cycle of length greater than three . for alternative characterizations and properties of decomposable graphs see e.g.  ( * ? ? ?",
    "* chapter  2 ) .    in the remainder of this section",
    "we discuss the interpretation of an absent edge in the partial correlation graph of @xmath70 .",
    "let us assume that the second - order moments of @xmath75 are finite .",
    "the partial uncorrelatedness of , say , @xmath76 and @xmath77 given @xmath78 , i.e. , @xmath79 , is to be understood as linear independence of @xmath76 and @xmath77 after the common linear effects of @xmath78 have been removed .",
    "a relation of similar type is conditional independence : roughly , @xmath76 and @xmath77 are conditionally independent given @xmath78 , if the conditional distribution of @xmath80 is a product measure for almost all values of the conditioning variable @xmath81 . in comparison to partial correlation we understand conditional independence as complete independence of @xmath76 and @xmath77 after the removal of all common effects of @xmath78 .",
    "another related term is conditional uncorrelatedness : the conditional distribution of @xmath80 given @xmath81 has correlation zero for almost all values of @xmath81 .",
    "there is an important qualitative difference between partial and conditional correlation : the former is a real value , the latter a function of the conditioning variable .",
    "all marginal and conditional distributions of elliptical distributions are again elliptical ( * ? ? ?",
    "* section  2.6 ) . hence partial uncorrelatedness implies conditional uncorrelatedness @xcite , and @xmath79 means linear independence of @xmath76 and @xmath77 after all common effects of @xmath78 have been removed .",
    "however , the only spherical distributions with independent margins are gaussian distributions , cf .",
    "thus contrary to gaussian graphical models a missing edge in the partial correlation graph of an elliptical distribution can in general not be interpreted as conditional independence .",
    "it appears , that by going from the normal to the elliptical model , the gain in generality is paid by a loss in the strength of inference . but",
    "this loss is illusory . from a data modelling perspective",
    "the conditional independence interpretation of partial uncorrelatedness under normality is an assumption , not a conclusion . by modelling multivariate data by a joint gaussian distribution one",
    "models the linear dependencies and assumes that there are no other than linear associations among the variables . by fitting an appropriate non - gaussian model one may still model the linear dependencies and allow non - linear dependencies .",
    "using semiparametric models embodies this idea : the aspects of interest , in our case linear dependencies , are modelled parametrically , whereas other aspects remain unspecified .",
    "of course , non - normal data need not be elliptical . any relevant data feature , such as non - linearities , anomalous values , etc .",
    ", is of potential interest and should be analysed .",
    "if the data , say , contains strong quadratic interactions , models that incorporate them should be used , as it is described e.g.  in ( * ? ? ?",
    "* section 2.10 ) .",
    "we address primarily the situation where the essential structure of the data is captured by an ellipse , and the linear interactions are the prominent ones . in any case , a robust analysis of the linear effects , as proposed here , is a suitable starting point of any subsequent tests for potential non - linear effects .",
    "an important initial step towards elliptical graphical modelling is the unconstrained estimation of @xmath51 .",
    "unconstrained , since we do not assume a graphical model to hold , not forcing any constraints on @xmath51 .",
    "we will consider estimators of the type @xmath82 , where @xmath83 is a suitable estimator of a multiple of @xmath5 , therefore start by considering shape estimators @xmath83 .",
    "let @xmath84 be independent and identically distributed random vectors sampled from an elliptical distribution @xmath85 .",
    "depending on the context , @xmath86 may denote the @xmath87th @xmath27-dimensional observation or the @xmath87th component of the vector @xmath60 .",
    "furthermore let @xmath88 be the @xmath89 data matrix and @xmath90 be a scatter estimator .",
    "the symbol @xmath83 may have two meanings : a function on the sample space , or as abbreviation for @xmath91 , a random variable .",
    "we use the term scatter estimator for any symmetric matrix - valued estimator that gives some information about the spread of the data .",
    "we call @xmath83 affine pseudo - equivariant , if it satisfies [ ape ] _ n(_n a^t + 1_n b^t ) a _",
    "n(_n ) a^t for all @xmath92 and full rank @xmath33 .",
    "this is a generalization of the strict affine equivariance for scatter estimators , which is obtained if ( [ ape ] ) is satisfied with equality .",
    "we use this weaker condition since overall scale is irrelevant for partial correlations , and we want to include estimators which only estimate shape , but not scale , and do not satisfy strict affine equivariance .",
    "examples are given in section  [ sec : examples ] .",
    "@xcite shows that , if a strictly affine equivariant scatter estimator is evaluated at an elliptical distribution , its first two moments , if existent , have a common structure .",
    "if the proportionality factor in ( [ ape ] ) is not random , the same holds true for pseudo - equivariant scatter estimators .",
    "the following condition is therefore natural for affine pseudo - equivariant estimators at elliptical distributions @xmath35 , and many shape estimators have been shown to satisfy it under suitable additional conditions on @xmath35 , see also the examples in section  [ sec : examples ] .",
    "[ asyms ] the estimator @xmath83 converges in probability to @xmath93 for some @xmath94 , and there exist @xmath95 and @xmath96 such that @xmath97 in distribution as @xmath98 , where @xmath99 .",
    "the scalars @xmath100 and @xmath101 depend on the estimator @xmath83 , the dimension @xmath27 and the function @xmath46 , but are constant with respect to the shape @xmath5 .",
    "we have the following implication for the derived estimators @xmath102 and @xmath103 . [ prop1 ] if @xmath83 satisfies assumption [ asyms ] , then with @xmath47 ,    a.   [ asymk ] @xmath104 + in distribution as @xmath98 , where @xmath105 , and b.   [ asymp ] @xmath106 + in distribution as @xmath98 with @xmath107 .",
    "an important aspect of proposition [ prop1 ] is that under ellipticity the asymptotic covariance matrices of partial correlation estimators @xmath108 derived from affine equivariant shape estimators @xmath83 are proportional to each other .",
    "in this section we treat the estimation of @xmath51 under a given graphical model @xmath74 specified by the graph @xmath61 , i.e. , estimating @xmath51 with zero - entries .",
    "a crude approach is to put the concerning elements in an unconstrained estimate @xmath109 to zero , but this generally destroys the positive definiteness of the estimate .",
    "we define the function @xmath110 by [ h_g ]     ( a_g)_i , j = a_i , j & ( \\{i , j } e   i = j ) , +   ( a_g^-1)_i , j = 0 & ( \\{i , j } e ,  i j ) , +    where @xmath111 are the elements of @xmath112 .",
    "a unique and positive definite solution @xmath113 of ( [ h_g ] ) exists for any positive definite @xmath112 .",
    "the positive definiteness of @xmath112 is sufficient but not necessary .",
    "for details see @xcite .",
    "since we mainly deal with asymptotics , and shape estimators @xmath83 are usually almost surely positive definite at continuous distributions for sufficiently large @xmath0 , we assume positive definiteness for simplicity s sake .",
    "let @xmath61 be a decomposable graph with cliques @xmath114 ( @xmath115 ) , and define the sequence @xmath116 of successive intersections by @xmath117 we assume that the ordering @xmath118 is such that the cliques form a perfect sequence , i.e. , for all @xmath119 there is a @xmath120 such that @xmath121 .",
    "it is always possible to arrange the cliques of a decomposable graph in a perfect sequence ( * ? ? ?",
    "2.17 ) . for notational convenience",
    "we let @xmath122 then @xmath123 allows the following explicit formulation for decomposable g , @xmath124 we will use this representation of @xmath125 to further analyse the properties of the estimators @xmath126 , @xmath127 and @xmath128 for a decomposable graph @xmath67 . using the notation @xmath129 , @xmath130 , @xmath131 and @xmath132 we have the following result about the asymptotic distribution .",
    "it is not assumed that the true shape @xmath5 fits the model @xmath67 .",
    "[ prop2 ] if @xmath83 fulfils assumption [ asyms ] and @xmath67 is decomposable , then    a.   [ asymkg ] @xmath133  in distribution + as @xmath98 with  @xmath134 , b.   [ asymsg ] @xmath135   in distribution as @xmath98 + with @xmath136 , c.   [ asympg ] @xmath137  in distribution as @xmath98 , where + @xmath138 with @xmath139 as in proposition [ prop1 ] ( [ asymp ] ) .",
    "if the true shape @xmath5 satisfies the graph @xmath67 , the expressions for the asymptotic variances simplify .",
    "[ cor1 ] if @xmath83 satisfies assumption [ asyms ] with @xmath140 for a decomposable graph @xmath67 , then the assertions of proposition [ prop2 ] are true with    a.   [ asymkg2 ] @xmath141 , b.   [ asymsg2 ] @xmath142 and c.   [ asympg2 ] @xmath143 .",
    "an essential tool of most model selection procedures is to test if a model under consideration fits the data and to compare the fit of two nested models . on the set @xmath144 of the positions of a @xmath8 matrix we declare a strict ordering @xmath145 by @xmath146 for any subset @xmath147 , where @xmath148 ( @xmath149 ) and @xmath150 , define the matrix @xmath151 as follows : each line consists of exactly one entry 1 and zeros otherwise .",
    "the 1-entry in line @xmath87 is in column @xmath152 .",
    "thus @xmath153 picks the elements of @xmath112 at positions specified by @xmath154 in the order they appear in @xmath31 . for a graph @xmath155 with @xmath156",
    "let @xmath157 i.e. , the set @xmath158 gathers all sub - diagonal zero - positions that @xmath67 enforces on a concentration matrix .",
    "thus @xmath159 is equivalent to @xmath160 .",
    "now let @xmath161 and @xmath162 be two decomposable graphs with @xmath62 as above and @xmath163 , or equivalently , @xmath164",
    ". for notational convenience let @xmath165 furthermore @xmath166 an intuitive approach to testing @xmath167 against the broader model @xmath168 is to reject @xmath167 in favour of @xmath168 , if all entries at positions in @xmath169 of an estimate @xmath170 of @xmath51 under @xmath168 are close to zero .",
    "for example , a sum of suitably weighted squared entries of @xmath170 , such as @xmath171 below , is a possible test statistic .",
    "let @xmath172 for invertible @xmath5 the matrix @xmath173 has rank @xmath174 , which can be deduced from the inverse function theorem .",
    "then @xmath175 is of full rank , and the probability that the wald - type test statistic @xmath176 exists tends to 1 as @xmath98 .",
    "proposition [ propwald ] describes the asymptotic behaviour of @xmath171 under the null hypothesis that @xmath167 is true , part ( [ propwald1 ] ) , and under a local alternative , part ( [ propwald2 ] ) . [ propwald ] let @xmath167 , @xmath168 be as above and @xmath84 independent and identically distributed random variables with @xmath177 .",
    "let @xmath83 be an affine pseudo - equivariant scatter estimator such that @xmath91 satisfies assumption [ asyms ] .",
    "a.   [ propwald1 ] then @xmath178 in distribution as @xmath98 .",
    "b.   [ propwald2 ] for @xmath179 let @xmath180 be distributed as @xmath181 , thus @xmath182 , where the sequence @xmath183 is such that @xmath184 exists .",
    "if , for each @xmath185 , @xmath83 is applied to @xmath186 , then , as @xmath98 , [ wald.alt ] _",
    "n(g_0,g_1 ) _ 1 ^2_q_0,1\\ { _ 1 ^ -1(b , s ) } in distribution , where @xmath187    we have some remarks .    a.   we define the non - centrality parameter of the @xmath188 distribution @xmath189 as @xmath190 . b.   we require @xmath83 to be affine pseudo - equivariant to ensure that the convergence of @xmath191 for @xmath98 is uniform in @xmath192 . c.   in part ( [ propwald2 ] ) of proposition [ propwald ] we do not require the sequence of alternatives to lie in the model @xmath168 , i.e. , that @xmath193 , as it is not necessary for the convergence ( [ wald.alt ] ) to hold . when choosing a model by forward selection one usually compares two wrong models , so it is of interest to know the behaviour of @xmath171 also if @xmath168 is not true .",
    "a difficulty with the test in proposition [ propwald ] is the complicated formulation of @xmath171 .",
    "the classical test in gaussian graphical models is the deviance test .",
    "the next proposition gives the analogue for elliptical graphical modelling .",
    "it treats parts ( [ propwald1 ] ) and ( [ propwald2 ] ) of the previous proposition simultaneously .",
    "[ propdeviance ] let @xmath167 , @xmath168 be as above and @xmath83 a sequence of almost surely positive definite random @xmath8 matrices , for which @xmath194 converges in distribution to a non - degenerate limit for some @xmath195 with @xmath196 . then",
    ", as @xmath98 , @xmath197 if the larger model @xmath168 is the saturated model , then proposition [ propdeviance ] is a corollary of theorem 2 in @xcite .",
    "we extend tyler s result to two nested models .",
    "[ cordev ] both assertions ( [ propwald1 ] ) and ( [ propwald2 ] ) of proposition [ propwald ] remain true , if @xmath171 is replaced by @xmath198 .",
    "there are many affine equivariant , robust estimators , see , for example , @xcite or @xcite .",
    "the comparison of asymptotic properties of such estimators in the elliptical model reduces to a comparison of the respective values of the scalars @xmath100 and @xmath101 . of course , the sample covariance matrix is affine equivariant .",
    "the following can be found in @xcite .",
    "[ asymsigma ] if @xmath84 are independent and identically distributed with distribution @xmath52 and @xmath199 , then @xmath200 fulfils assumption [ asyms ] with @xmath201 and @xmath202 , where @xmath203 is the excess kurtosis of any component of @xmath76 .",
    "proposition [ asymsigma ] indicates the inappropriateness of the sample covariance matrix for heavy - tailed distributions : its asymptotic distribution depends on the kurtosis , which is large at heavy - tailed distributions , rendering the estimator inefficient .",
    "an alternative is tyler s m - estimator , which is defined as the solution @xmath204 of @xmath205 that satisfies @xmath206 .",
    "existence , uniqueness and asymptotic properties are treated in @xcite , where the following result is proven .",
    "[ asymtyler ] if @xmath84 are independent and identically distributed with distribution @xmath52 , furthermore @xmath207 and @xmath208 , then @xmath209 fulfils assumption [ asyms ] with @xmath210 and @xmath211 .",
    "we have the following remarks .",
    "a.   in proposition [ asymsigma ] the scalars @xmath100 and @xmath101 are constant , irrespective of the function @xmath46 , i.e. , the tyler matrix is asymptotically distribution - free within the elliptical model .",
    "hence , when carrying out any of the tests from section [ sec : testing ] , @xmath100 does not need to be estimated .",
    "b.   tyler s matrix can cope with arbitrarily heavy tails .",
    "the assumption of finite second moments is only required for location estimation by the mean .",
    "it may be replaced by any root-@xmath0-consistent location estimator , for instance the hettmansperger  randles ( 2002 ) median .",
    "the inverse moment condition @xmath208 is fairly mild : for @xmath212 it is fulfilled if @xmath46 has no singularity at @xmath213 . c.   the estimator @xmath209 is affine pseudo - equivariant and gives information only about the shape but none about the scale .",
    "other such estimators are oja sign and rank covariance matrices @xcite .",
    "the popular reweighted minimum covariance determinant estimator @xcite is highly robust and affine equivariant and has previously been proposed in the context of graphical modelling @xcite .",
    "it is defined as follows .",
    "a subset @xmath214 of size @xmath215 , where @xmath216 is fixed , is determined such that @xmath217 is minimal with @xmath218 the mean @xmath219 and covariance matrix @xmath220 computed from this minimizing subsample are called the raw minimum covariance determinant location and scatter estimates .",
    "the scatter part is scaled to achieve consistency for the covariance at the gaussian distribution .",
    "based on the raw estimates a reweighted scatter estimator @xmath221 is computed from the whole sample : @xmath222 where @xmath223 if @xmath224 and zero otherwise , and @xmath6 is a small rejection probability , e.g.  @xmath225 .",
    "the reweighted covariance estimate is again scaled , but since this is not necessary for our applications we omit the details .",
    "we present the results of a simulation study comparing several estimators .",
    "we repeatedly sample 100 independent observations of a 5-dimensional distribution .",
    "we use the same shape matrix throughout , with equal diagonal elements and the partial correlation structure represented by the graph in figure [ figure1 ] .",
    "we let the tail behaviour vary , using the normal distribution and several members of the @xmath226 family to generate heavier tails @xcite .",
    "the index @xmath227 denotes the degrees of freedom .",
    "the moments of @xmath226 are finite up to order @xmath228 .",
    "we may talk of a fixed shape of the @xmath226 distribution , since @xmath46 is specified .",
    "for @xmath229 , its covariance matrix is @xmath230 , and , for @xmath231 , the excess kurtosis of each component is @xmath232 .",
    "propositions [ asymsigma ] and [ asymtyler ] imply that the tyler matrix is asymptotically more efficient than the sample covariance matrix at @xmath226 if @xmath233 . for each distribution considered we generate 2000 samples , compute the estimates described in section [ sec : examples ] and , based on each estimate , select a model .",
    "we use a simple one - step model selection procedure , that allows us to concentrate on the effects of the different estimators . for each pair",
    "@xmath69 we test the model with all edges but @xmath69 against the saturated model , and exclude the edge @xmath69 if the test accepts the smaller model .",
    "the significance level @xmath225 is an ad hoc choice . in our simulations",
    "the wald - type test statistic @xmath234 and the deviance test statistic @xmath235 showed a very similar behaviour .",
    "tables [ table1 ] and [ table2 ] report the results of the deviance test .",
    ".one - step model selection based on @xmath236 or @xmath237 [ table1 ] [ cols=\"^,^,>,>,>,>\",options=\"header \" , ]     @xmath238 with finite - sample correction ; @xmath239 @xcite",
    "as a very simple and efficient technique to safeguard graphical modelling of continuous data against the impact of heavy tails , non - normality in general and , to some degree , also faulty outliers we recommend the use of tyler s estimator in place of the empirical covariance matrix .",
    "the gain in robustness comes at a very moderate loss in efficiency , which becomes smaller with increasing dimension , and a justifiable increase in computing time .",
    "@xcite report average computing times on a 2.83 ghz intel core2 cpu for @xmath240 and @xmath241 of less than a second for the tyler matrix , compared to less than three seconds for the reweighted minimum covariance determinant estimator .",
    "moreover , our approach allows the use of any affine pseudo - equivariant , root-@xmath0-consistent estimator @xmath83 in an analogous way .",
    "assumption [ asyms ] is the important prerequisite on @xmath83 , and our results also apply to estimators that are asymptotically affine equivariant , like the rank - based estimation technique of @xcite .",
    "a problem that has not been addressed in this article is the accuracy of the asymptotic approximations for small to moderate sample sizes , in particular , to what extent it depends upon the ratio @xmath242 .",
    "this question splits into two parts .",
    "the first is an evaluation of the finite - sample properties of the affine pseudo - equivariant scatter estimators",
    ". these may be very different and do not allow a unified treatment .",
    "very little seems to be known theoretically , either on the exact distribution of most robust scatter estimators or the rate of convergence to the gaussian limit . however , there is strong empirical evidence that tyler s estimator has excellent small - sample properties . in all our simulations",
    "the difference in the empirical distributions of any univariate function of the sample covariance matrix @xmath243 at normality and the tyler matrix @xmath209 at any elliptical distribution , is fully expressed by the asymptotic scaling factor @xmath244 , see also ( * ? ? ?",
    "* figure 2 ) .",
    "moreover , it is known that @xmath209 behaves similarly to @xmath243 when @xmath27 and @xmath0 grow large simultaneously @xcite .",
    "the second task is then , given the small - sample properties of the estimators , to assess the accuracy of the asymptotic @xmath188 distributions of the tests .",
    "this question is of relevance also in classical graphical modelling , where it has been noted that the deviance test statistic may substantially differ from its @xmath188 limit for small @xmath0 . improved small - sample approximations",
    "have been proposed @xcite , but also the exact distribution of the deviance test statistic is known for decomposable models , cf .",
    "* sections 5.2.2 and 5.3.3 ) .",
    "our simulations indicate that finite - sample correction techniques used in gaussian graphical modelling may be put to good use also under ellipticity by applying it in an analogous way to tyler s estimator .",
    "the main limitation of the affine equivariant approach is that it does not provide a solution in the @xmath245 situation or allow a simple transfer of standard techniques , like regularization , that are used in gaussian graphical modelling .",
    "any affine equivariant , robust estimator requires more than @xmath246 data points , because the only affine equivariant scatter estimator in the @xmath247 situation is the sample covariance estimator @xcite . dropping",
    "the affine equivariance property is inevitable for robust , high - dimensional graphical modelling .",
    "this research was supported by the german research foundation .",
    "the authors gratefully acknowledge the assistance of alexander drre in preparing the figure and the simulations and thank the referees , the associate editor and the editor for their helpful comments and suggestions .",
    "the proofs repeatedly apply the delta method to functions mapping matrices to matrices .",
    "we define the derivative of such a function , say , @xmath248 at point @xmath60 as the derivative of @xmath249 with respect to @xmath250 and denote its jacobian at point @xmath60 , which is of size @xmath21 , by @xmath251 .",
    "the symmetry of the argument poses a technical difficulty : there are @xmath252 rather than @xmath32 variables , and the function @xmath46 must be viewed as a function from @xmath253 to @xmath254 in order to define a derivative . to deal with this issue",
    "we compute the jacobian of @xmath46 interpreted as a function from @xmath254 to @xmath254 and post - multiply it by @xmath255 .",
    "this is justified by the chain rule applied to @xmath256 , where @xmath257 duplicates the off - diagonal elements and @xmath258 .",
    "the derivatives below contain the right - multiplied @xmath255 depending on whether we view the function as defined on @xmath15 or on @xmath254 .",
    "the textbook @xcite covers most of the tools of the proofs , in particular calculation rules concerning the @xmath259 operator , the kronecker product and derivatives of matrix functions .",
    "we repeatedly use the following without reference .",
    "@xmath260 for matrices @xmath261 @xcite .",
    "let @xmath262 denote matrix inversion .",
    "its jacobian matrix is @xcite @xmath263      part ( [ asymp ] ) : we have @xmath264 with @xmath265 .",
    "we need to compute the derivative of @xmath266 in order to apply the delta method .",
    "we start by considering @xmath267 .",
    "its jacobian matrix @xmath268 is obtained by elementwise differentiation . applying the multiplication rule to @xmath269 yields [ th ] ( a ) = - m_p \\{(a ) a_d}j_p  -  a_da_d . by the delta method",
    ", @xmath270 converges in distribution to a @xmath32-dimensional normal distribution with mean zero and covariance matrix @xmath271 which reduces to the expression given in proposition [ prop1 ] .",
    "in particular , @xmath101 vanishes , since @xmath272 .",
    "this is generally true for scale - invariant function @xmath266 .",
    "part ( [ asymkg ] ) : since @xmath273 with @xmath274 we want to compute the derivative of @xmath275 .",
    "let @xmath276 for any subset @xmath277 .",
    "the mapping @xmath278 is a composition of @xmath279 , @xmath280 and @xmath11 .",
    "we obtain by the chain rule @xmath281 then @xmath282 is shown to have the form given in proposition [ prop2 ] ( [ asymkg ] ) by noting that @xmath283 .",
    "this holds true because @xmath284 which is a consequence of the inversion formula for partitioned matrices .",
    "let @xmath288 be such that @xmath289 and write short @xmath290 for @xmath291 .",
    "it suffices to show that @xmath292 .",
    "proposition [ prop2 ] ( [ asymsg ] ) in connection with proposition [ asymsigma ] identifies the left - hand side as the asymptotic covariance of @xmath293 , where @xmath236 is the sample covariance matrix , at the normal distribution with covariance @xmath5 .",
    "formula ( 5.50 ) in @xcite identifies the same quantity as the right - hand side .    in the proofs of proposition [ propwald ] and corollary [ cordev ] we use the following lemma .",
    "[ lemma1 ] let @xmath294 and @xmath295 , @xmath296 , be as in proposition [ propwald ] and @xmath83 a shape estimator such that @xmath91 satisfies assumption [ asyms ] .",
    "assume furthermore that there is a continuously differentiable function @xmath297 with @xmath298 such that @xmath83 satisfies [ ape.xi ] _",
    "n(_n a^t + 1_n b^t ) = ( aa^t ) a _ n(_n ) a^t for any data matrix @xmath299 , @xmath92 and full rank matrix @xmath33 .",
    "then @xmath300 in distribution as @xmath98 , where @xmath301 is as in proposition [ propwald ] and @xmath302 .",
    "the proof of lemma [ lemma1 ] follows by straightforward calculations and is omitted .",
    "the constant @xmath303 is identified by means of the first order taylor expansion of @xmath304 around @xmath305 .",
    "part ( [ propwald1 ] ) follows by standard arguments from the asymptotic normality of the estimator @xmath306 . for any affine pseudo - equivariant estimator @xmath83 the rescaled estimator @xmath307 satisfies",
    "( [ ape.xi ] ) , and the value of the test statistic @xmath308 is the same , if computed from @xmath83 or @xmath309 .",
    "applying lemma [ lemma1 ] to @xmath309 we deduce part ( [ propwald2 ] ) for analogously to part ( [ propwald1 ] ) .    towards the proof of proposition [ propdeviance ] we state lemmas [ lemmaminimizer ] to [ lemmaderivatives ] .",
    "for @xmath310 let @xmath311 : @xmath312 .",
    "from the theory of gaussian graphical models we know that for any graph @xmath67 and @xmath310 the matrix @xmath313 is the unique solution of the constrained optimization problem [ cop1 ] f_a(b ) q_d(g)h(b ) = 0 ,  b ^+_p , because @xmath113 is the maximum likelihood estimate of the covariance matrix under the model @xmath67 at the normal distribution , if @xmath112 is the observed sample covariance , cf .",
    "now with the notation of section [ sec : testing ] let @xmath314 , @xmath315 and @xmath316 .      by ( [ cop1 ] ) and ( [ h_g ] )",
    ", @xmath318 uniquely solves the constrained optimization problem [ cop3 ] f_a_g_1(b ) h_0(b ) = 0 ,  b ^+_p .",
    "the restriction @xmath319 is equivalent to @xmath320 , and any matrix @xmath301 with @xmath321 can be written as @xmath322 for some @xmath323 .",
    "thus @xmath324 and @xmath325 are equal , and so are the solution sets of the constrained optimization problems ( [ cop3 ] ) and [ cop4 ] f_a_g_1(b ) b",
    ". thus @xmath318 uniquely solves ( [ cop4 ] ) , and all matrices @xmath323 with @xmath326 , among them @xmath318 , solve ( [ cop2 ] ) . the next two lemmas are stated without proof .",
    "expressions ( [ derivative2 ] ) can be deduced from the proofs of propositions [ prop1 ] and [ prop2 ] , and ( [ derivative1 ] ) can be assembled from the derivatives given in @xcite .",
    "[ lemmaae ] let @xmath327 be continuously differentiable and @xmath167 , @xmath168 as in section [ sec : testing ] .",
    "let furthermore @xmath83 be a sequence of almost surely positive definite random @xmath8 matrices , for which @xmath194 converges in distribution for some @xmath195 with @xmath196 . then for @xmath98 @xmath328          applying lemma [ lemmaae ] to @xmath336 and using ( [ derivative2 ] ) we find further @xmath337 and from ( [ schritt1 ] ) [ schritt2 ] _",
    "n(g_0,g_1 )  ~   \\ { ( _ g_1 - _ g_0 ) } ^t m_p _",
    "g_1(_g_0 ) ( _ g_1 - _ g_0 ) , n . next we introduce the lagrange multiplier @xcite .",
    "since @xmath338 solves the constrained optimization problem ( [ cop2 ] ) with @xmath339 , there exists a vector @xmath340 such that @xmath341 which transforms to @xmath342 ,   cf .",
    "lemma [ lemmaderivatives ] .",
    "+ we left - multiply both sides by @xmath343 and solve for @xmath344 .",
    "@xmath345 we substitute the right - hand side for the left - hand side of this equation in ( [ schritt2 ] ) , apply again lemma [ lemmaae ] , this time to @xmath346 , which leads to @xmath347 and obtain @xmath348 finally @xmath349 as @xmath98 , since both sides converge to @xmath173 .",
    "part ( 1 ) is straightforward . for part ( 2 )",
    "we take , as in proposition [ propwald ] , the detour via @xmath307 and make use of lemma [ lemma1 ] to ensure that @xmath350 meets the assumptions of proposition [ propdeviance ] .",
    "d.  vogel and r.  fried . on robust gaussian graphical modelling .",
    "in l.  devroye , b.  karaszen , m.  kohler , and r.  korn , editors , _ recent developments in applied probability and statistics .",
    "dedicated to the memory of jrgen lehn .",
    "_ , pages 155182 .",
    "berlin , heidelberg : springer - verlag , 2010 .",
    "d.  vogel , a.  drre , and r.  fried .",
    "elliptical graphical modeling in higher dimensions . in _ proceedings of international biosignal processing conference , july 14 - 16 , 2010 , berlin , germany .",
    "_ , pages 15 , 2010 .",
    "robust location and scatter estimators in multivariate analysis . in j.",
    "fan and h.  koul , editors , _ frontiers in statistics .",
    "dedicated to peter john bickel on honor of his 65th birthday _ , pages 467490 .",
    "london : imperial college press , 2006 ."
  ],
  "abstract_text": [
    "<S> we propose elliptical graphical models based on conditional uncorrelatedness as a generalization of gaussian graphical models by letting the population distribution be elliptical instead of normal , allowing the fitting of data with arbitrarily heavy tails . </S>",
    "<S> we study the class of proportionally affine equivariant scatter estimators and show how they can be used to perform elliptical graphical modelling , leading to a new class of partial correlation estimators and analogues of the classical deviance test . </S>",
    "<S> general expressions for the asymptotic variance of partial correlation estimators , unconstrained and under decomposable models , are given , and the asymptotic chi square approximation of the pseudo - deviance test statistic is proved . </S>",
    "<S> the feasibility of our approach is demonstrated by a simulation study , using , among others , tyler s scatter estimator , which is distribution - free within the elliptical model . </S>",
    "<S> our approach provides a robustification of gaussian graphical modelling . </S>",
    "<S> the latter is likelihood - based and known to be very sensitive to model misspecification and outlying observations . </S>"
  ]
}