{
  "article_text": [
    "semantic object parsing , which refers to segmenting an image region of an object into several semantic parts , enables the computer to understand the contents of an image in detail , as illustrated in figure  [ fig : task ] .",
    "it helps many higher - level computer vision applications , such as image - to - caption generation  @xcite , clothes recognition and retrieval  @xcite , person re - identification  @xcite and human behavior analysis  @xcite .",
    "recently , many research works  @xcite@xcite@xcite@xcite@xcite have been devoted to exploring various convolutional neural networks ( cnn ) based models for semantic object parsing , due to their excellent performance in image classification  @xcite , object segmentation  @xcite@xcite and part localization  @xcite .",
    "however , the classification of each pixel position by cnns can only leverage very local information from limited neighboring context , depicted by small convolutional filters .",
    "intuitively , larger local context and global perspective of the whole image are very critical cues to recognize each semantic part in the image .",
    "for instance , in terms of local context , visually similar regions can be predicted as  left - leg \" or ",
    "right - leg \" depending on their specific locations and neighboring semantic parts (  left - shoes \" or ",
    "right - shoes \" ) , especially for regions with two crossed legs .",
    "similarly , the regions of  tail \" and  leg \" can be distinguished by the spatial layouts relative to the region of  body \" . in terms of global perspective , distinguishing  skirt \" from  dress \" or ",
    "pants \" needs the guidance from the prediction on other semantic regions such as  upper - clothes \" or  legs \" .",
    "previous works often resort to some post - processing techniques to separately address these complex contextual dependencies , such as , super - pixel smoothing  @xcite , mean field approximation  @xcite and conditional random field  @xcite@xcite@xcite .",
    "they improve accuracy through carefully designed processing on the predicted confidence maps instead of explicitly increasing the discriminative capability of visual features and networks .",
    "these separate steps often make feature learning inefficient and result in suboptimal prediction for pixel - wise object parsing .        instead of employing separate processing steps , this work aims to explicitly increase the capabilities of features and networks in an end - to - end learning process .",
    "one key bottleneck to increase the network capability is the long - chain problem in deep cnn structures , that is , information from previous computations rapidly attenuates as it progresses through the chain .",
    "similar problem exists when recurrent neural networks are applied to long sequential data .",
    "lstm recurrent neural networks  @xcite were originally introduced to address this problem , which utilize the memory cells to process sequential data with complex and sequentialized interdependencies by independently reading , writing and forgetting some information .",
    "it could be similarly extended to image analysis .",
    "particularly , the emergence of grid lstm  @xcite allows for multi - dimensional spatial communications .",
    "our work builds on grid lstm  @xcite and proposes a novel local - global lstm ( lg - lstm ) for cnn - based semantic object parsing in order to simultaneously model global and local contextual information for improving the network capability .",
    "the proposed lg - lstm layers are appended to the intermediate convolutional layers in a fully convolutional neural network  @xcite to enhance visual features by seamlessly incorporating long - distance and short - distance dependencies .",
    "the hidden cells in lg - lstm serve as the enhanced features , and the memory cells serve as the intrinsic states that recurrently remember all previous interactions of all positions in each layer .    to incorporate local guidance , in each lg - lstm layer ,",
    "the features at each position are influenced by the hidden cells at that position in the previous lg - lstm layer ( depth dimension ) as well as the hidden cells from eight neighboring positions ( spatial dimensions ) .",
    "the depth lstm along the depth dimension is used to communicate information directly from one layer to the next while the spatial lstms along spatial dimensions allow for the complex spatial interactions and memorize previous contextual dependencies .",
    "individual memory cells for each position are used for each of the dimensions to capture the diverse spatial layouts of semantic parts in different images .",
    "moreover , to further incorporate global guidance , the whole hidden cell maps obtained from the previous lg - lstm layer are split into nine grids , with each grid covering one part of the whole image .",
    "then the max - pooling over each grid selects discriminative features as global hidden cells , which are used to guide the prediction on each position . in this way",
    ", the global contextual information can thus be conveniently harnessed together with the local spatial dependencies from neighboring positions to improve the network capability .    by stacking multiple lg - lstm layers and sequentially performing learning and inference ,",
    "the prediction of each position is implicitly influenced by the continuously updated global contextual information about the image and the local contextual information of neighboring regions in an end - to - end way . during the training phase , to keep the network invariant to spatial transformations , all gate weights for the spatial lstms for local contextual interactions are shared across different positions in the same lg - lstm layer , and the weights for the spatial lstms and the depth lstm are also shared across different lg - lstm layers .",
    "our main contributions can be summarized in four aspects .",
    "1 ) the proposed lg - lstm exploits local contextual information to guide the feature learning of each position by using eight spatial lstms and one depth lstm .",
    "2 ) the global hidden cells are posed as the input states of each position to leverage long - distance spatial dependencies of the whole image .",
    "3 ) the stacked lg - lstm layers allow long - range contextual interactions benefiting from the memorization of previous dependencies in all positions .",
    "4 ) the proposed lg - lstm layers are incorporated into fully convolutional networks to enable an end - to - end feature learning over all positions .",
    "we conduct comprehensive evaluations and comparisons on the horse - cow parsing dataset  @xcite , and two human parsing datasets ( atr dataset  @xcite and fashionista dataset  @xcite ) .",
    "experimental results demonstrate that our architecture significantly outperforms previously published methods for object parsing .",
    "* semantic object parsing : * there have been increasing research works in the semantic object parsing problem including the general object parsing  @xcite@xcite@xcite@xcite@xcite and human parsing  @xcite@xcite@xcite@xcite@xcite@xcite .",
    "recent state - of - the - art approaches often rely on deep convolutional neural networks along with advanced network architectures  @xcite@xcite@xcite . instead of learning features only from local convolutional kernels as in these previous methods",
    ", we solve this problem through incorporating the novel lg - lstm layers into cnns to capture both long - distance and short - distance spatial dependencies .",
    "in addition , the adoption of hidden and memory cells in lstms makes it possible to memorize the previous contextual interactions from local neighboring positions and the whole image in previous lg - lstm layers .",
    "it should be noted that while  @xcite models mean - field approximate inference as recurrent networks , it can only refine results based on predicted pixel - wise confidence maps .",
    "in contrast , our lg - lstm layers can progressively improve visual features to directly boost the performance of object parsing .",
    "* lstm on image processing : * lstm networks have been successfully applied to many tasks such as hand - writing recognition  @xcite , machine translation  @xcite and image - to - caption generation  @xcite . they have been further extended to multi - dimensional learning and applied to image processing tasks  @xcite  @xcite such as biomedical image segmentation  @xcite , person detection  @xcite and scene labeling  @xcite . most recently , grid lstm  @xcite extended lstm cells to allow the multi - dimensional communication across the lstm cells , and the stacked lstm and multi - dimensional lstm  @xcite can be regarded as special cases of grid lstm .",
    "the proposed lg - lstm architecture in this work is extended from grid lstm and adapted to the complex semantic object parsing task . instead of pure local factorized lstms in  @xcite  @xcite ,",
    "the features of each position in the proposed lg - lstm are influenced by the short - distance dependencies as well as the long - distance global information from the whole image .",
    "most of previous works verified the capability of grid lstm on very simple data ( simple digital images , graphical or texture data ) , while we focus particularly on the higher - level object parsing task . the closest work to our method is the scene labeling approach proposed in  @xcite , where 2d lstm cells are performed on the non - overlapping image patches .",
    "however , our architecture differs from that work in that we employ eight spatial lstms and one depth lstm on each pixel , and learn distinct gate weights for different lstms by considering different spatial interdependencies .",
    "in addition , global hidden cells are also incorporated as the inputs for different lstms in each lg - lstm layer .",
    "the proposed lg - lstm aims to generate the pixel - wise semantic labeling for each image .",
    "as illustrated in figure  [ fig : framework ] , the input image is first passed through a stack of convolutional layers to generate a set of convolutional feature maps",
    ". then the transition layer adapts convolutional feature maps into the inputs of lg - lstm layers , which are fed into the first lg - lstm layer .",
    "the lg - lstm layers is able to memorize long - period of the context information from local neighboring positions and global view of the image .",
    "more details about the lg - lstm layers are presented in section  [ sec : lstm ] . after each lg - lstm layer ,",
    "one feed - forward convolutional layer with @xmath0 filters generates the @xmath1 confidence maps based on these improved features .",
    "the individual cross - entropy loss function over all pixels is used after each feed - forward convolutional layer in order to train each lg - lstm layer . finally , after the last lg - lstm layer , the @xmath1 confidence maps for @xmath1 labels ( including background ) are inferred by the last feed - forward layer to produce the final object parsing result",
    ".        * transition layer . * to make sure the number of the input states for the first lg - lstm layer is consistent with that of the following lg - lstm layers so that they can share all gate weights , the feature maps from convolutional layers are first adapted by the transition layer and then fed into the lg - lstm layers .",
    "the transition layer uses the same number of lstms as in lg - lstm layers , and passes the convolutional features of each position into these lstms to generate individual hidden cells .",
    "these resulting hidden cells are then used to construct input states for the first lg - lstm layer .",
    "these weight matrices in the transition layer are not shared with those of the lg - lstm layers because their dimensions of input states are not consistent . in the initialization",
    ", all memory cells are set as zeros for all positions , following the practical settings used in pedestrian detection  @xcite .",
    "the updated memory cells from the transition layer can then be shared with lg - lstm layers , which enables lg - lstm layers to memorize feature representations obtained from convolutional layers .      in this section ,",
    "we describe the novel lg - lstm layer tailored for semantic object parsing .",
    "to be self - contained , we first recall the standard lstm recurrent neural network  @xcite and then describe the proposed lg - lstm layers .      the lstm network  @xcite easily memorizes the long - period interdependencies in sequential data . in image understanding , this temporal dependency learning can be conveniently converted to the spatial domain .",
    "the stacked layers of feature maps also enable the memorization of previous states at each pixel position ( referred as depth dimension in this work ) .",
    "each lstm accepts the previous input @xmath2 and determines the current states that comprises the hidden cells @xmath3 and the memory cells @xmath4 , where @xmath5 is the output number .",
    "following  @xcite , the lstm network consists of four gates : the input gate @xmath6 , the forget gate @xmath7 , the memory gate @xmath8 and the output gate @xmath9 .",
    "the @xmath10 are the corresponding recurrent gate weight matrices .",
    "suppose , @xmath11 is the concatenation of the input @xmath2 and the previous states @xmath12 .",
    "the hidden and memory cells can be updated as    @xmath13 where @xmath14 is the logistic sigmoid function , and @xmath15 indicates a pointwise product .",
    "let @xmath16 denote the concatenation of four weight matrices .",
    "following  @xcite , we use the function @xmath17 to shorten eqn .",
    "( [ eq : lstm ] ) as    @xmath18    the mechanism acts as a memory and implicit attention system , whereby the information from the previous inputs can be written to the memory cells and used to communicate with sequential inputs .",
    "grid lstm  @xcite extended one - dimensional lstm to cells that are arranged in a multi - dimension grid .",
    "inspired by the design of grid lstm , we propose the multi - dimensional local - global lstm to adapt to higher - level image processing .",
    "the features of each position depend on the local short - distance and global long - distance information .",
    "local information propagated from neighboring pixels can help retain the short - distance contextual interactions ( object boundaries ) while global information obtained from the whole feature maps can provide the long - distance contextual guidance ( global spatial layouts of semantic parts ) to boost feature prediction of each position .",
    "* local hidden cells .",
    "* in terms of local interactions , as illustrated in figure  [ fig : lstm ] , the feature prediction of each position @xmath19 takes in @xmath20 hidden cells from @xmath21 local neighboring pixels from @xmath21 spatial lstms and the hidden cells from one depth lstm .",
    "intuitively , the depth lstm can help track the previous information at each position using the memory cells benefited from the lstm mechanism , like in the one - dimension lstm .",
    "each spatial lstm computes the propagated hidden cells starting from each position to its corresponding spatial direction , as illustrated by the green arrows in figure  [ fig : lstm ] .",
    "thus , each position can provide distinct guidance to each spatial direction by employing distinct spatial lstms , which take the spatial layouts and interactions into account for feature prediction .",
    "let @xmath22 denote the hidden cells propagated from the corresponding neighboring position to a specific pixel @xmath19 along the @xmath23-th spatial dimension by the @xmath23-th spatial lstm , obtained from the @xmath24-th layer .",
    "the @xmath25 indicates the hidden cell computed by the depth lstm on the position @xmath19 using the weights updated in the @xmath24-th layer .    * global hidden cells . * in terms of global interaction , the feature prediction of each position",
    "@xmath19 also takes the global hidden cells generated based on the whole hidden cell maps from the previous lg - lstm layer as inputs .",
    "specifically , the whole hidden cell maps are constructed by the hidden cells of the depth lstm , i.e. @xmath26 of all positions , which represents the enhanced features of each position .",
    "as shown in figure  [ fig : global ] , we partition the whole hidden cell maps into nine grids and then the global max - pooling within each grid is performed , resulting in @xmath27 hidden cell maps . the hidden cells for each grid can thus capture the global information of each grid .",
    "such partition and max - pooling is perform over all the @xmath5 channels , forming @xmath28 hidden cells .",
    "we denote the global hidden cells obtained from the @xmath24-th lg - lstm layer as @xmath29 .",
    "the global max pooling of hidden cells enables the model to seamlessly incorporate global and local information based on previous hidden cells in the previous lg - lstm layer .",
    "* lg - lstm layer . *",
    "given the global hidden cells @xmath30 , the local hidden cells @xmath31 from @xmath21 spatial lstms and @xmath26 from one depth lstm for each position @xmath19 , the input states @xmath32 fed into the @xmath33-th lg - lstm layer at each position @xmath19 can be computed as    @xmath34^t .",
    "\\label{eq : hiddeninput}\\ ] ]    denote memory cells of all @xmath21 spatial dimensions for each position @xmath19 as @xmath35 in the @xmath24-th lstm layer and those of depth dimension as @xmath36 .",
    "extended from grid lstm  @xcite , the new hidden cells and memory cells of each position @xmath19 for all @xmath37 dimensions are calculated as    @xmath38 where @xmath39 represents the hidden cells propagated from the position @xmath19 to the @xmath23-th spatial direction , which are used by its neighboring positions to generate their input hidden cells in the next layer .",
    "note that @xmath39 can be distinguished from the @xmath40 by the different starting points and directions for information propagation .",
    "each position thus has @xmath21 sides of incoming hidden cells and @xmath21 sides of outgoing hidden cells for incorporating complex local interactions .",
    "although the same @xmath41 for each position is applied for all lstms , the distinct memory cells for each position used in different lstms enable individual information propagation from @xmath21-sides .",
    "these lstm functions are operated on all positions to produce the whole hidden cell maps . to keep invariance along different spatial dimensions ,",
    "the weight matrices @xmath42 of @xmath21 spatial lstms are shared . by sequentially stacking several lstm layers , the receptive field of each position",
    "can be considerably increased to sense a much larger contextual region .",
    "in addition , long - distance information can also be effectively captured by using the global hidden cells .",
    "the input features fed into feed - forward convolutional layers are computed as @xmath41 for each position .",
    "* dataset : * we evaluate the performance of lg - lstm architecture for semantic object parsing on the horse - cow parsing dataset  @xcite and two human parsing datasets , atr dataset  @xcite and fashionista dataset  @xcite",
    ".    * horse - cow parsing dataset  @xcite . *",
    "the horse - cow parsing dataset is a part segmentation benchmark introduced in  @xcite . for each class ,",
    "most observable instances from pascal voc 2010 benchmark  @xcite are manually selected , including 294 training images and 227 testing images .",
    "each image pixel is elaborately labeled as one of the four part classes , including head , leg , tail and body .",
    "following the experiment protocols in  @xcite and  @xcite , we use all the training images to learn the model and test every image given the object class . the standard intersection over union ( iou ) criterion and pixel - wise accuracy",
    "are adopted for evaluation .",
    "we compare the results of our lg - lstm with three state - of - the - art methods , including the compositional - based method (  sps \" )  @xcite , the hypercolumn (  hc \" )  @xcite and the most recent method (  joint \" )  @xcite .",
    "* atr dataset  @xcite and fashionista dataset  @xcite .",
    "* human parsing aims to predict every pixel of each image with 18 labels : face , sunglass , hat , scarf , hair , upper - clothes , left - arm , right - arm , belt , pants , left - leg , right - leg , skirt , left - shoe , right - shoe , bag , dress and null .",
    "originally , 7,700 images are included in the atr dataset  @xcite , with 6,000 for training , 1,000 for testing and 700 for validation .",
    "10,000 real - world human pictures are further collected by  @xcite to cover images with more challenging poses , occlusion and clothes variations .",
    "we follow the training and testing settings used in  @xcite .",
    "the fashionista dataset contains 685 images , among which 229 images are used for testing and the rest for training .",
    "we use the same evaluation metrics as in  @xcite  @xcite  @xcite , including accuracy , average precision , average recall , and average f-1 score .",
    "we compare the results of our lg - lstm with five recent state - of - the - art approaches  @xcite  @xcite  @xcite  @xcite  @xcite .",
    ".comparison of object parsing performance with three state - of - the - art methods over the horse - cow object parsing dataset  @xcite .",
    "we report the iou accuracies on background class , each part class and foreground class . [",
    "cols=\"^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]      we further evaluate different network settings to verify the effectiveness of the important components in our lg - lstm architecture , presented in table  [ tab : ablation ] .    * comparison with using convolutional layers * : to strictly evaluate the effectiveness of using the proposed lg - lstm layers , we report the performance of purely using convolutional layers , i.e. ,  vgg16 \" , indicating the results of the basic network architecture we use with one extra feed - forward convolution layer with @xmath43 filters attached to output pixel - wise confidence maps . by comparing",
    " vgg16 \" with  lg - lstm \" , @xmath44 improvement in iou on horse class can be observed , which demonstrates the superiority of using more lg - lstm layers to jointly address the long - distance and short - distance contextual information .",
    "note that , since the lstms are deployed in each position in each lg - lstm layer and more parameters are introduced for model learning , one possible alternative solution is just using more convolutional layers instead of lg - lstm layers .",
    "we thus report the performance of using more convolutional layers on the basic network structure , i.e.  vgg16 extra conv \" . to make fair comparison with our usage of five lg - lstm layers ,",
    "five extra convolutional layers are utilized containing @xmath45 convolutional filters with size @xmath46 in each convolutional layer , because nine lstms are used in lg - lstm layers and each of them has 64 hidden cell outputs . compared with ",
    "lg - lstm \" , the  vgg16 extra conv \" decreases the mean iou by @xmath47 and @xmath48 on horse and cow classes , respectively .",
    "it speaks well for the superiority of using lg - lstm layers to harness complex long - distances patterns and memorize long - period hidden states over purely convolutional layers .    * local connections in lg - lstm",
    "* : note that in lg - lstm , we use eight spatial neighboring connections to capture local contextual information for each position . to further validate the effectiveness of lg - lstm",
    ", we also report the performance of using two local connections and four local connections , i.e.  lg - lstm local_2 \" and  lg - lstm local_4 \" . for  lg - lstm local_2 \"",
    ", the top and left neighboring positions with respect to each position are used . for  lg - lstm local_4 \" , the local information is propagated from four neighboring positions in top , left , top - left and top - right directions .",
    "it can be observed that our lg - lstm architecture (  lg - lstm \" ) significantly outperforms  lg - lstm local_2 \" and  lg - lstm local_4 \" by @xmath49 and @xmath50 in iou on cow class , respectively .",
    "eight spatial connections with neighboring pixels for each position enable lg - lstm to capture richer information from neighboring context , which are more informative than other limited local spatial connections . as illustrated in the first row of figure  [ fig : humanparsing ] , lg - lstm gives more consistent parsing results by incorporating sufficient local connections while co - cnn produces many fragments of semantic regions .",
    "* global connections in lg - lstm * :  lg - lstm w / o global \" in table  [ tab : ablation ] indicates the results without using global hidden cells as the lstm inputs for each position .",
    "compared with ",
    "lg - lstm \" , @xmath51 and @xmath52 decreases in iou occur with ",
    "lg - lstm w / o global \" on horse and cow classes , respectively .",
    "this demonstrates well the effectiveness of global contextual information for inferring the prediction of each position .",
    "these global features provide an overview perspective of the image to guide the pixel - wise labeling .",
    "as shown in the second row of figure  [ fig : humanparsing ] , by gathering global information from the whole image , lg - lstm successfully distinguishes the combination of upper - clothes and skirt with dress while co - cnn often confuses them only from local cues .",
    "the qualitative comparisons of parsing results on horse - cow dataset and atr dataset are visualized in figure  [ fig : parsingresults ] and figure  [ fig : humanparsing ] , respectively . because the previous state - of - the - art methods do not publish their codes , we only compare our method with the  vgg16 \" on horse - cow parsing dataset . as can be observed from these visualized comparisons , our lg - lstm architecture outputs more semantically meaningful and precise predictions than  vgg16 \" and  co - cnn \" despite the existence of large appearance and position variations .",
    "for example , the small regions ( tails ) can be successfully segmented out by lg - lstm from neighboring similar semantic regions ( body and legs ) on the horse - cow dataset .",
    "lg - lstm can successfully handle the confusing labels such as skirt vs dress and legs vs pants on the human parsing dataset .",
    "the regions with similar appearances can be recognized and separated by the guidance from global contextual information , while the local boundaries for different semantic regions are preserved well by using local connections .",
    "in this work , we proposed a novel local - global lstm architecture for semantic object parsing .",
    "the lg - lstm layers jointly capture the long - distance and short - distance spatial dependencies by using global hidden cells from the whole maps and local hidden cells from eight spatial dimensions and one depth dimension .",
    "extensive results on three public datasets clearly demonstrated the effectiveness of the proposed lg - lstm in generating pixel - wise semantic labeling . in the future , we will explore how to develop a pure lg - lstm network architecture where all convolutional layers are replaced with well designed lg - lstm layers .",
    "it will produce more complex neural units in each layer , which can hierarchically exploit local and global connections of the whole image , and enables to remember long - period hidden states to capture complex visual patterns ."
  ],
  "abstract_text": [
    "<S> semantic object parsing is a fundamental task for understanding objects in detail in computer vision community , where incorporating multi - level contextual information is critical for achieving such fine - grained pixel - level recognition . </S>",
    "<S> prior methods often leverage the contextual information through post - processing predicted confidence maps . in this work , </S>",
    "<S> we propose a novel deep local - global long short - term memory ( lg - lstm ) architecture to seamlessly incorporate short - distance and long - distance spatial dependencies into the feature learning over all pixel positions . in each lg - lstm layer , local guidance from neighboring positions and global guidance from the whole image are imposed on each position to better exploit complex local and global contextual information . </S>",
    "<S> individual lstms for distinct spatial dimensions are also utilized to intrinsically capture various spatial layouts of semantic parts in the images , yielding distinct hidden and memory cells of each position for each dimension . in our parsing approach , </S>",
    "<S> several lg - lstm layers are stacked and appended to the intermediate convolutional layers to directly enhance visual features , allowing network parameters to be learned in an end - to - end way . </S>",
    "<S> the long chains of sequential computation by stacked lg - lstm layers also enable each pixel to sense a much larger region for inference benefiting from the memorization of previous dependencies in all positions along all dimensions . </S>",
    "<S> comprehensive evaluations on three public datasets well demonstrate the significant superiority of our lg - lstm over other state - of - the - art methods . </S>"
  ]
}