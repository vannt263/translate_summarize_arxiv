{
  "article_text": [
    "in a typical free - recall experiment , subjects are presented with a list of words , and then requested to recall them in any order .",
    "some of the main phenomena observed are :    \\(1 ) power - law scaling : the number of retrieved items scales like a power law of the number of items in the list ( murray et al . , 1976 ) ;",
    "\\(2 ) recency and contiguity effects : recall usually begins from the end of the list , and items contiguous within the list tend to be recalled contiguously ( murdock , 1962 ; kahana , 1996 ) ;    \\(3 ) temporal asymmetry , i.e. the tendency to recall items in forward order ( already reported in ebbinghaus , 1913 ) ;    \\(4 ) the classical word - length effect : lists composed of shorter words are easier to recall than lists of longer words ( baddeley et al . , 1975 ) ;    \\(5 ) the inverse word - length effect : within a `` mixed '' list ( containing words of various lengths ) longer words are easier to recall than shorter words ; so far , this effect has only been reported in one large database ( katkov et al . , 2014 ) .",
    "the power - law scaling of retrieval ( with an exponent close to @xmath2 ) has been understood successfully by romani et al .",
    "( 2013 ) through an argument based on a graph representation of attractors .",
    "if attractors are depicted as nodes on a complete graph and retrieval occurs through a random walk on the graph , a realistic scaling exponent can be calculated by assuming that recall terminates when the path self - intersects .",
    "romani et al .",
    "supported this finding through a network model of the long - term neural representation of items , but their graph argument applies well to both long - term and episodic memory .    the main assumption of romani et al .",
    "is that each word can be represented as a single attractor .",
    "a depiction of words as individual nodes in a graph , however , stands in sharp contrast to our current understanding of recency and contiguity effects , which is vehicled by `` contextual '' theories of episodic memory such as the temporal context model of howard and kahana ( 2002 ) .",
    "these approaches have proven conclusively that the recall process does not retrieve a given word per se , but rather a memory of the event corresponding to the word s presentation .",
    "since any word may have in principle multiple meanings , its relevant meaning depends on the context in which it has been presented , and the word s recall is mediated by the retrieval of that specific meaning .",
    "this is shown in graph form in fig . 1 .",
    "this standard picture is complicated , however , by the systematic occurrence of temporal asymmetry in the data .",
    "following the literature , let us call `` lag '' the difference between the serial positions of two consecutively recalled words ; for example , if the @xmath3th word in the list is remembered right after the @xmath4th , the lag is @xmath5 .",
    "lags are more often positive than negative , meaning that forward transitions are preferred ; yet , this is due almost entirely to the contribution from contiguous transitions ( @xmath6 ) , since forward contiguous transitions ( @xmath7 ) follow an entirely different statistics than transitions with other lags .",
    "it has been suggested ( kahan and caplan , 2002 ) that a `` discontinuity '' exists between two different types of recall process .",
    "indeed , this may be seen from fig .",
    "2 , where only the peak for sequential transitions seems to differentiate the forward and backward curves . we are therefore required to account for the existence of two possible recall mechanisms : the associative recall of fig . 1 and the sequential recall that gives rise to the peak at @xmath7 . a graph representation of the latter is shown in fig .",
    "3 , where the system revists in chronological order the intermediate nodes between each pair of consecutive memories .    why does the system opt sometimes for one procedure , sometimes for the other ? in section v",
    ", we will show how this flexibility in the choice of a retrieval mechanism may facilitate the recall process , allowing the system to optimize its strategy on the basis of the particular task at hand .",
    "the graph model invoked by romani et al . does not differentiate between different types of verbal items , as those authors were interested in the total number of retrieved items .",
    "hence , all words are represented as equivalent nodes .",
    "differentiating between the topological properties of different attractors becomes important , however , to study effects that depend on the properties of individual words .",
    "according to recent fmri measurements , one word property on which neural response exhibits a strong statistical dependence is semantic variability , or `` polysemy '' ( musz and thompson - schill , 2015 ) .",
    "the polysemy of a word is the degree of dependence of the word s meaning on context ( nerlich et al . , 2003 ) .",
    "between the words `` lion '' and `` lioness '' ( a classic example ) the word `` lion '' is considered `` more '' polysemous because it has two potential meanings ( a male lion , or a lion of unspecified gender ) whereas `` lioness '' has just one .",
    "context is needed to discern whether `` lion '' is referring to a generic member of the species or to a male ( fig .",
    "4 ) .    in all languages where studies are available ( zipf , 1949 ; guiter , 1974",
    "; sambor , 1984 ; rother , 1994 ) data show a negative correlation between a word s polysemy and its length .",
    "( a waring distribution seems to fit best this dependence , see rensinghoff and nemcov , 2010 ) . hence , we can rely on the fact that longer words differ from shorter ones by how strongly their meaning depends on context .",
    "the meaning we associate to a short word may change widely depending on where the word appears , whereas a long word is likely to evoke the very same image , or idea , within any context .",
    "such a statement is translated into graph form by representing every short word with several semantic nodes , each linked to another node in the semantic graph , and every long word with a single node ( a fixed meaning ) linked by multiple edges to other nodes in the graph , as in fig .",
    "5 .    for the sake of simplicity",
    ", we will allow for the existence of only two word lengths .",
    "nodes that represent anything but the words we are interested in ( including semantic states that are not verbalizable ) may be thought of as a semantic reservoir ( a large random graph , which in the crudest approximation would be the complete graph ) .",
    "we have thus a treatable graph representation of the current state of knowledge on the problem .",
    "the one thing we lack is the relation between the number @xmath8 of nodes corresponding to a short word and the number @xmath9 of edges linking a long - word node to the reservoir .",
    "this relation , as we will show , is easily obtained from our statistical knowledge on the frequency of words in a typical corpus .",
    "suppose verbal output is produced by random - walking ergodically on the semantic graph and uttering or writing the word associated to every verbalizable node met along the way . for @xmath10 , the frequency of short words in the resulting corpus will be identical to the frequency of long words .",
    "indeed , if the reservoir is invariant under node permutations , the two steps a trajectory needs in order to touch a verbal node and re - enter the reservoir are statistically equivalent whether the reservoir is re - entered through the same node or through another one . hence , short and long words affect identically the random walk , through their number of nodes and edges respectively .",
    "but the frequency of an average word is monotonously decreasing as a function of the word s length ( at least in the languages where data are available , see strauss et al . , 2006 ) .",
    "we must conclude that @xmath11 .",
    "consider the presentation stage of a free - recall experiment .",
    "we want to compute the distance the system travels between the presentation of two consecutive words . from the structure of the semantic graph",
    ", it can be seen that this distance is only a function of the length of the second word .",
    "we will call @xmath12 the probability of finding any of @xmath13 nodes in the reservoir in a time @xmath14 , starting from a random node of the reservoir .",
    "the explicit form of the function @xmath15 will not be needed for our predictions .",
    "suppose an average node in the reservoir is connected to @xmath16 nodes .",
    "in considering random walks through the reservoir , we can neglect the presence of verbal nodes other than the destination node .",
    "the probability that the random walk will take a time @xmath14 to find any of the nodes representing a given word is    @xmath17    for short and long words respectively .",
    "these may also be regarded as the distributions of the distance travelled by the system in order to recognize a short or long word during presentation .    during sequential recall ( as per fig .",
    "3 ) the distance traveled between two consecutive recalls is the same that was travelled during presentation , so @xmath18 and @xmath19 . during associative recall ,",
    "semantic space is re - explored through a new random walk ; hence , the probability that a given memory will be reached associatively , if starting from another word - memory , is    @xmath20    for short and long words respectively",
    ".    it would be rigorous to assume that the retrieval process terminates when the path finds twice the same word , as in romani et al ( 2013 ) ; our final results wo nt be affected , however , by choosing a simpler termination mechanism , where the search is abandoned after a maximal time @xmath21 has elapsed .",
    "if we define @xmath22 , the total recall probabilities are    @xmath23    and using the fact that @xmath24 is a monotonously increasing function , we find @xmath25 , @xmath26 .",
    "in addition , we obtain two predictions on the recall probabilities of either type as functions of length :    @xmath27",
    "i will test these inequalities against data from peers ( penn electrophysiology of encoding and retrieval study ) , a large study conducted at the university of pennsylania . to obtain a larger database ,",
    "i have collapsed the data described in lohnas et al .",
    "( 2015 ) with those described in healey and kahana ( 2016 ) summing up to a total of @xmath28 free - recall trials , all with lists of @xmath29 words .",
    "the wordpool from which the lists were assembled contains @xmath30 words of up to @xmath31 syllables . since only four @xmath32",
    "five - syllable words are present , and a single @xmath31-syllable word ( `` encyclopedia '' ) , the statistics for these two lengths may not be representative .",
    "6 shows the level of asymmetry for transitions to words of different lengths , as a function of the length of the word of arrival .",
    "the dependence on word length is virtually suppressed if we only consider noncontiguous recall , and is prominent if we include contiguous recall ( backward and forward ) into the count .",
    "moreover , the advantage for forward transitions is more prominent for shorter words , in accordance with the direction of inequality ( [ seqprediction ] ) .",
    "it must be ascertained now if the source of this inequality is that forward transitions are facilitated or that backward transitions are impaired .",
    "7 shows the probability of contiguous transitions to individual words ( blue dots ) and its average over all words with the same length ( red lines ) . for backward contiguous transitions ,",
    "the transition probability is nearly independent on word length ( with a p - value @xmath33 ) . for forward contiguous ( i.e. , sequential ) transitions ,",
    "the correlation coefficient is @xmath34 ( @xmath35 ) .",
    "hence , the probability of sequential recall tends to decrease as a function of word length , as predicted by formula ( [ seqprediction ] ) .",
    "8 offers an instructive overview of the transition probabilities , by displaying the fraction of transitions with a given lag to words of each given length .",
    "the peak at lag @xmath7 signals the action of sequential recall , without which the plot would be fairly symmetric .",
    "it can also be seen that the sequential recall probability is a sharply decreasing function of the number of syllables . moreover",
    ", only sequential transitions display a monotonous dependence on word length .",
    "we would like to test prediction ( [ assprediction ] ) , according to which the recall probability in the associative mode is an increasing function of length .",
    "if true , this would be a leading contribution to the inversion of the word length effect mentioned above . in fig .",
    "8 , however , the probabilities of associative transitions show no definite dependence on length , partly because they have been normalized by the total recall probability for each word .    to test the inequality , we integrate over all transitions with lags different from @xmath36 , which yields the plot in fig .",
    "data for individual words are shown as blue dots , together with an average over all words with the same length ( red line ) .",
    "the correlation coefficient is found to be @xmath37 ( @xmath38 . the mean probability of associative recall increases monotonically with the number of syllables , in agreement with the model .    a note on our method of analysis .",
    "we have regarded all the @xmath7 data as representive of sequential recall , but technically this is only correct if the @xmath7 peak in fig . 1 is much higher than the @xmath39 peak .",
    "if so , the consecutive recall of two consecutive memories is nearly always the result of a sequential procedure . in general , however , an associative random walk may also chance consecutively upon two consecutive memories .",
    "a way of keeping this into account is by exploiting the temporal symmetry of associative recall and subtracting a second copy of the @xmath39 data from the @xmath7 data . subtracting the left - hand panel of fig .",
    "7 from the right - hand panel leaves the decreasing trend unaltered , increasing @xmath40 to @xmath41 , with @xmath42 . as for fig .",
    "9 , adding a second copy of the @xmath39 data leaves the increasing trend unaltered , only reducing @xmath43 to @xmath44 ( @xmath38 .",
    "formulas ( [ seqprediction ] , [ assprediction ] ) refer to the average recall probabilities for individual words of a given length .",
    "can we use what we have learned on the recall of individual words to explain global properties of list recall ?    in actual data , fully sequential recall is almost never observed .",
    "the reason is easy to understand : sequential recall entails the retrieval of a larger amount of short - term information than associative retrieval , because the latter can rely almost entirely on long - term associations .",
    "hence , sequential recall is more costly by a factor @xmath45 in terms of reliance on the short - term memory store , where @xmath45 is the average length of the trajectory between two word - recognition events during presentation .    to keep this into account",
    ", we must multiply sequential recall probabilities by a reducing factor @xmath46 that tends to @xmath47 in the limit @xmath48 and to @xmath49 in the limit @xmath50 .",
    "formula ( [ exes ] ) becomes    @xmath51    suppose now the list contains a fraction @xmath52 of long words , and the system decides to opt for sequential retrieval a fraction @xmath53 of the times .",
    "the probability of recovering a long or short word in a time @xmath54 is @xmath55 that is , @xmath56 x_m \\end{aligned}\\ ] ]    from which we will proceed to compute the optimal value of @xmath53 .",
    "the average probability of recovering a word is    @xmath57 \\gamma \\big\\ } \\alpha\\ ] ]    and this is maximized by    @xmath58    where @xmath59 is a threshold that exists whenever @xmath8 is large enough .",
    "the optimal probability is then    @xmath60    thus the recall probability , plotted as a function of the average word length in the list , dissociates into a sequential and an associative regime .",
    "the derivative @xmath61 starts out negative and changes sign at @xmath62 , where @xmath63 .",
    "as long as @xmath8 is large enough ( @xmath64 ) , we are going to have @xmath65 , so the standard word - length effect emerges . on the other hand",
    ", we can see from eq .",
    "( [ prob ] ) that @xmath66 if @xmath67 , where @xmath68 , which is always true if @xmath69 , hence we have @xmath70 and the inverse word - length effect is always present in the associative regime .",
    "thus , in spite of its simplicity , the semantic - graph model offers an explation of how the two contradictory word - length effects ( classical and inverse ) may coexist .",
    "our mind has two retrieval strategies at its disposal , and decides which one to adopt as a result of its awareness on the average length of the words to be recalled . while the strategy that works best for short words is on the whole more efficient ,",
    "lists of longer words call for a retrieval procedure that leaves short words behind .    in peers data ,",
    "the average word length per list has a standard deviation @xmath71 and a mean @xmath72 such that @xmath73 , too small to allow for a test of this hypothesis .",
    "a conclusive test may come , however , from re - analyzing data already available from experiments performed thus far on pure lists .",
    "a topological argument has been presented for the structure of semantic space in the presence of words of different lengths .",
    "the argument is grounded in the negative correlation between a word s length and its polysemy , well documented in linguistics .",
    "the resulting graph structure has been applied to the modeling of free - recall experiments , resulting in predictions on the comparative values of recall probabilities . in particular ,",
    "associative recall was found to favor longer words whereas sequential recall was found to favor shorter word .",
    "these predictions were verified through an analysis of data from the peers experiment of lohnas et al .",
    "( 2015 ) and healey and kahana ( 2016 ) , shedding light on the reported inversion of the word - length effect for mixed lists .",
    "the argument has been further applied to the prediction of global properties of list recall .",
    "this has led to a possible explanation for the classical word - length effect based on the optimization of retrieval strategies .",
    "none of the results we have derived is impaired by regarding the semantic reservoir as a complete graph . in future extensions of this work , however , most problems will require keeping into account the lesser connectivity of an actual semantic reservoir , which effectively introduces a metric structure on nodes , some pairs of words being more closely connected than others .",
    "this may help us to understand , through topological arguments , such phenomena as the difference in contextual information rate between short and long words , reported by piantadosi et al .",
    ", 2011 . for low enough connectivity ,",
    "moreover , contiguous recall may favor short words even in a purely associative regime ( as can be argued from the case where the reservoir is a linear graph ) .    within the principles we have demonstrated , the generalization to multiple word lengths is straightforward : every word of length @xmath74 should be represented by @xmath75 nodes with @xmath76 edges each , with @xmath77 and @xmath78 .",
    "the statistics of associative retrieval on such a graph will be treated in detail elsewhere .",
    "another open problem is how to encode the different degree of semantic invariance of individual words , regardless of length .",
    "if we attribute two topological parameters @xmath79 to each word in a real vocabulary , how do these parameters relate mathematically to properties of the word measurable within corpora ( statistical correlations , information rate , conditional entropy ) ? by studying the ideal random walk that drives language production on the graph , we may be able to quantify the connection between the linguistic observables and the `` hidden '' topological parameters of the model .",
    "a final , crucial challenge is to find neural networks from which attractor structures such as the semantic graph we have inferred would emerge  together with search patterns as the ones we have described .",
    "this would vastly deepen our understanding of the links between cognition and neural processing .",
    "i would like to thank michael j. kahana , and his university of pennsylvania research group , for generously sharing the data obtained in their laboratory .",
    "baddeley a.d . ,",
    "thomson n. , buchanan m. , 1975 . word length and the structure of short - term memory .",
    "journal of verbal learning and verbal behavior , 14:575 - 589 ."
  ],
  "abstract_text": [
    "<S> a topological argument is presented concering the structure of semantic space , based on the negative correlation between polysemy and word length . </S>",
    "<S> the resulting graph structure is applied to the modeling of free - recall experiments , resulting in predictions on the comparative values of recall probabilities . </S>",
    "<S> associative recall is found to favor longer words whereas sequential recall is found to favor shorter words . </S>",
    "<S> data from the peers experiments of lohnas et al . </S>",
    "<S> ( 2015 ) and healey and kahana ( 2016 ) confirm both predictons , with correlation coefficients @xmath0 and @xmath1 . </S>",
    "<S> the argument is then applied to predicting global properties of list recall , which leads to a novel explanation for the word - length effect based on the optimization of retrieval strategies . </S>"
  ]
}