{
  "article_text": [
    "in information retrieval ( ir ) , term frequency is a fundamental and important component of a ranking model .",
    "intuitively , the larger the term frequency of a query word in a document , the more likely the document is to be about the query topic , and thus , the document should have a higher relevance score . in practice , however , documents are of various lengths , and the simple approach of preferring documents with higher term frequency could easily result in an excessive preference for long documents . to use",
    "the term frequency in a fairer approach , _ normalization _ of the term frequency has been extensively investigated by researchers .    with regard to the normalization problem ,",
    "robertson and walker introduced the verbosity and the scope hypotheses , which state that document length is mainly determined by two factors  _ verbosity _ and _ scope _  as follows @xcite :    \\1 ) * verbosity hypothesis * : `` _ _ some authors are simply more verbose , using more words to say the same thing _ _ @xcite . ''",
    "\\2 ) * scope hypothesis * : `` _ _ some authors have more to say :",
    "they may write a single document containing or covering more ground _ _ @xcite . ''    in this paper , we focus on the difference between the effect of the verbosity and the scope on the term frequency of a single word .",
    "verbosity , as the name implies , is related to the burstiness of term frequency , which helps an already mentioned word in a document get a higher frequency .",
    "even if a word has a low term frequency in normal verbosity , its term frequency could increase significantly when the document has high verbosity .",
    "on the other hand , scope mostly involves the creation of a new word , rather than boosting the term frequency .",
    "broadening the scope of a document would help unseen words in a normal document get non - zero frequencies .",
    "however , these non - zero frequencies might not be high .",
    "therefore , verbosity leads to a significant increase in term frequency , whereas scope leads to a rather limited increase in term frequency .",
    "in other words , the scope of a document only helps the occurrence of a new word , and the term frequency of the word is mostly governed by the verbosity of the document .    despite this difference between verbosity and scope , standard",
    "normalization is a length - driven approach , i.e. , it is based only on the document length , without distinguishing between verbosity and scope . as a result",
    ", it may suffer from _ insufficient _ penalization of a _ verbose _ document whose length is increased mainly by high verbosity , and _ excessive _ penalization of a _ broad _ document whose length is mainly derived from the broad scope .    in the light of this addressed difference , this paper argues that verbosity and scope should be normalized separately by employing different penalization functions . to achieve this",
    ", we propose a two - stage normalization approach .",
    "we first perform _ verbosity normalization _ for each document by linearly dividing the term frequency by the verbosity , thus obtaining a _ verbosity - normalized document representation_. we then perform _ scope normalization _ , in which an existing retrieval model is applied to this verbosity - normalized document representation .",
    "the final model obtained is called a _",
    "verbosity - normalized _ ( vn ) _ retrieval model_.    furthermore , we examine whether the proposed vn retrieval model resulting from two - stage normalization performs the desired separate penalizations . toward this end , we first select three popular retrieval models  the okapi model @xcite , the dirichlet - prior ( dp ) smoothed language model @xcite , and the markov random field ( mrf ) model @xcite  and then perform _ comparative axiomatic analysis _ of the original and the vn retrieval models , under the setting of the axiomatic framework introduced in @xcite .",
    "the analysis results confirm that the vn model indeed performs the desired separate normalizations , i.e. , a _ strict _ penalization of verbosity - increased documents and a _ relaxed _ penalization of scope - broadened documents .",
    "the results of experiments carried out on standard trec test collections show that the vn retrieval models are significantly better than the original models .",
    "the experimental results support our motivating argument that the verbosity and scope should be handled separately using different penalization functions .",
    "the remainder of this paper is organized as follows .",
    "section [ section_related_work ] describes previous studies .",
    "section [ section_two_stage_normalization ] describes the proposed two - stage normalization approach and presents the vn retrieval models for dp , okapi , and mrf .",
    "section [ section_comparative_axiomatic_analysis ] presents the main results of the analysis of retrieval models under standard length normalization constraints .",
    "sections [ section_experiments ] , [ section_experimental_results ] , [ section_application_lower_bounding_term_freq_normalization ] present the experimental setting and results",
    ". sections [ section_conclusion ] concludes .",
    "recognized that simply dividing the term frequency by the document length leads to the over - penalization problem in long documents . to overcome this problem , they proposed _ pivoted normalization _ ,",
    "in which a pivoted length is used to normalize the term frequency by adding a constant pivot factor ( i.e. , average document length ) to the original document length .",
    "pivoted normalization had originally been introduced in okapi s model @xcite , before it was formalized and named by . because pivoted normalization yields successful results , it has been explicitly adopted by other retrieval models , such as the inquery system @xcite .",
    "a similar relaxed type of normalization has been commonly used in more recent retrieval models  normalization 2 in the divergence from randomness ( dfr ) retrieval framework @xcite and the smoothed document length in dp @xcite .",
    "formally and mathematically defined ir heuristics , drawn from ranking characteristics most commonly used by existing retrieval models , thereby proposing a novel direction for an axiomatic approach to ir .",
    "the retrieval heuristics defined in the axiomatic approach have been used to define a new retrieval model inductively @xcite and to restrict the search space for automatically learning a retrieval function @xcite .",
    "in addition to original constraints , some studies have explored new constraints including : semantic term matching constraints @xcite , the proximity - based matching constraint @xcite , the burstness - based normalization constraint @xcite , the document frequency constraint for pseudo - relevance feedback @xcite , the feedback term weight constraints for pseudo - relevance feedback @xcite , and the translation probability constraints for translation language models @xcite . with regard to the length normalization problem ,",
    "defined three length normalization constraints ( referred to as lnc1 , lnc2 , and tf - lnc ) , demonstrating analytically that popular retrieval models satisfy all these normalization constraints at least for content - bearing words .",
    "our argument that different normalization functions should be used for verbosity and scope was also proposed by @xcite , in a more restricted manner , as follows : `` _ _ the verbose hypothesis suggests that we should simply normalize term frequencies by dividing by document length , while the scope hypothesis , on the other hand , suggests the opposite _ _ @xcite . ''",
    "that is , they suggest that a retrieval function does not necessarily penalize a long document when it has a broad scope .",
    "a similar argument was also made by @xcite .",
    "our suggestion , however , is that we still need the penalization for scope , but in a much more relaxed manner . in this sense",
    ", our argument can be regarded as a generalization of the previous arguments .    to the best of our knowledge ,",
    "one of the first approaches for two - stage normalization is _ pivoted unique normalization _ , suggested by @xcite . in their approach , the term frequency is first normalized on the basis of a nonlinear function by using the average term frequency ( which corresponds to verbosity normalization ) , and the normalized term frequency is then further divided by a pivoted unique length ( which corresponds to scope normalization ) .",
    "however , it remains unclear how their approach can be generalized to other retrieval models .",
    "going beyond the aforementioned existing works , we propose a generalized two - stage normalization approach , arguing more clearly that the term frequency should be penalized differently , depending on whether a document is long because of the verbosity or the scope .",
    "our approach is not limited to a specific retrieval model or a specific measure of the verbosity or scope .",
    "we also analytically present the retrieval heuristics realized by two - stage normalization , by performing a comparative axiomatic analysis under the setting of standard normalization constraints suggested by ,    interestingly , passage retrieval can also be viewed as two - stage normalization @xcite . because scopes are more similar in passages themselves than in documents , using passages itself can be considered as a type of scope normalization .",
    "thereafter , applying an existing retrieval method to score each passage corresponds to verbosity normalization .",
    "recently , and observed that when documents are extremely long , the score gap calculated as the difference between scores when a query term is present and when it is absent in a document , could be infinitely close to zero or negative . as a result ,",
    "extremely long documents tend to be overly penalized . to ensure a desirable score gap between documents that match and do not match a query term , proposed _ lower - bounding _ term frequency normalization , which can be described as follows : ( 1 ) a _ pseudo score gap _ between documents that match and do not match a query term is newly introduced as a document - independent factor .",
    "( 2 ) for each query term , the pseudo score gap is added to the original document score only when the document matches the query term , whereas the original document score is left unchanged for a document that does not match the query term . importantly ,",
    "closely examined the underlying principles of their proposed normalization , after which they proposed the constraints * lb1 * and * lb2 * as extensions of the existing formal heuristics used in @xcite . according to their axiomatic analysis , all modified retrieval functions proposed in @xcite unconditionally or more easily satisfy the lower bounds ( lbs ) without violating the original constraints of @xcite , whereas existing functions do not satisfy the lbs .",
    "experiment results showed that all modified retrieval functions showed statistically significant improvements , especially for verbose queries .",
    "in contrast to our work , the lower - bounding normalization proposed in @xcite uses only the document length .",
    "however , in our case , we distinguish the verbosity of the document from the scope .",
    "in addition , the new constraints used in @xcite are complementary to the existing length normalization constraints ( lncs ) , whereas our work emphasizes the need to pursue a new generation of lncs .      in @xcite , the initial form of the two - stage normalization approach",
    "was presented to modify language modeling approaches by introducing the _ pseudo document model_. however , @xcite were not aware of the importance of the pseudo document model as a generalized solution for handling the addressed problem .",
    "in addition , @xcite suggested a rather harsh retrieval constraint called tnc , which is too strong to be satisfied by even their own proposed method .",
    "given the previous presentation of @xcite , it thus remained unclear how the presented normalization yields some of the reported improved performances , and how it can be generalized to other retrieval models .",
    "building on our prior work , novel contributions of this paper are listed in the following :    * _ generalized _ two - stage normalization ( section [ section_two_stage_normalization ] ) , which was not explicitly argued and not fully formalized in @xcite . with the explicit formulation , we now correctly understand vn - dp as a specific instance of two stage normalization . *",
    "extensions to other models  okapi and mrf ( section [ section_two_stage_normalization ] ) , as a result of the proposed generalized normalization * analytically capturing retrieval heuristics of two - stage normalization by performing _ comparative axiomatic analysis _ ( section [ section_comparative_axiomatic_analysis ] & appendices c , d , and e ) under the standard constraint setting of @xcite * _ lengthpower _ as a novel scope measure ( section [ section_two_stage_normalization ] ) . using lengthpower , we have an unified view of language modeling approaches by considering both jm and dp as special cases of vn - dp . * _ comparison with lower bounding term frequency normalization _ ( section [ section_application_lower_bounding_term_freq_normalization ] )",
    "in this section , we describe our proposed two - stage normalization in detail , and apply it to the dp , okapi , and mrf approaches , as case studies .",
    "the following are notations commonly used in this paper .",
    "+ @xmath0 @xmath1 : @xmath2 , set of all words + @xmath0 @xmath3 : number of documents in a given collection + @xmath0 @xmath4 : a given collection , consisting of @xmath5",
    ". often , we also use @xmath4 to refer to the concatenated representations of all documents in @xmath4 .",
    "+ @xmath0 @xmath6 : document frequency of @xmath7 + @xmath0 @xmath8 ( or @xmath9 ) : a given document ( or a query ) + @xmath0 @xmath10 ( or @xmath11 ) : term frequency of word @xmath7 in document @xmath8 ( or query @xmath9 ) + @xmath0 @xmath12 : term frequency of word @xmath7 in collection @xmath4 defined by @xmath13 + @xmath0 @xmath14 : term discrimination value of @xmath7 such as idf + @xmath0 @xmath15 : length of document @xmath8 , defined by @xmath16 + @xmath0 @xmath17 : length of collection @xmath4 , defined by @xmath18 ( for brevity of notation , @xmath4 is either the set of documents or the concatenated representation of documents , depending on context ) + @xmath0 @xmath19 : scope of document @xmath8 ( @xmath20 ) + @xmath0 @xmath21 : verbosity of document @xmath8 + @xmath0 @xmath22 : average length , verbosity , and scope , respectively , of documents in the collection . motivated by the verbosity and the scope hypotheses",
    ", we first assume that the document length is decomposed into the verbosity and the scope , thereby providing the following simplified formula : @xmath23 as a result , we can formulate @xmath21 in terms of @xmath19 and @xmath15 as follows : @xmath24 the derivation of eq .",
    "( [ eq_def_verbosity ] ) is presented in appendix a.    in verbosity normalization , the original term frequency is normalized by dividing it by the verbosity of the document . to formally describe verbosity normalization ,",
    "let @xmath25 be a _",
    "verbosity normalization operator _ ; @xmath26 , the _ verbosity - normalized document representation _ of @xmath8 , which is the document transformed by applying the operator @xmath25 to all words in a document @xmath8 ; and @xmath27 , the _ verbosity - normalized term frequency _ of word @xmath7 .",
    "then , verbosity normalization refers to the process of obtaining @xmath27 for word @xmath7 , using the following formula : @xmath28 where @xmath29 is a verbosity scaling parameter . by substituting eq .",
    "( [ eq_def_verbosity ] ) into eq .",
    "( [ eq_def_verbosity_normalized_tf ] ) , @xmath30 becomes @xmath31 the resulting normalized term frequency is not only inversely proportional to the document length but is also proportional to the scope of the document .      for scope normalization",
    ", we need to consider a more relaxed function than that for verbosity normalization .",
    "we first note that the scope of an original document is the verbosity - normalized length of the document , as follows : @xmath32 furthermore , existing retrieval models perform a type of relaxed normalization by using their pivoted length or smoothed length .",
    "thus , instead of developing a new function , we perform scope normalization by straightforwardly applying an existing retrieval model to the verbosity - normalized document representation @xmath26 . formally , let @xmath33 be the original retrieval function that gives a score to @xmath8 , for query @xmath9 .",
    "applying two - stage normalization to @xmath33 gives @xmath34 , which is obtained by replacing @xmath10 used in all terms in @xmath33 with @xmath30 for all documents in the collection .",
    "we call @xmath34 a * vn * ( verbosity - normalized ) * retrieval model * or a * vn scoring function*.      in this section , we present the application of two - stage normalization to the dp , okapi , and mrf approaches .",
    "dp performs bayesian smoothing on a multinomial language model @xcite , for which the conjugate prior is the dirichlet distribution with the following parameters : @xmath35 the bayesian priors using the parameters of eq .",
    "( [ eq_dirichlet_prior ] ) give the following smoothed model of document @xmath8 : @xmath36 and the following scoring function for a given query @xmath9 @xcite : @xmath37 the vn model @xmath34 is assumed to employ the following _ document - specific _ conjugate prior : @xmath38 in other words , the more verbose @xmath8 is , the larger is the prior probability used . a detailed justification for eq .",
    "( [ eq_dirichletvn_prior ] ) is presented in appendix b. these modified bayesian priors using the parameters of eq .",
    "( [ eq_dirichletvn_prior ] ) give the following smoothed model : @xmath39 we simply use @xmath40 in eq .",
    "( [ eq_def_verbosity_normalized_tf ] ) , because the scaling parameter @xmath29 of @xmath30 is absorbed into the smoothing parameter @xmath41 .",
    "then , eq . ( [ eq_dirichletvn_smoothed_model ] ) becomes @xmath42 eq .",
    "( [ eq_dirichletvn_smoothed_model_using_operator ] ) is the same as the equation obtained by replacing @xmath10 with @xmath30 .",
    "( [ eq_dirichletvn_smoothed_model ] ) , the resulting retrieval function is given as @xmath43 which is called * * vn - dp**.      okapi s bm25 retrieval formula , as presented by @xcite , is @xmath44 where the term frequency component @xmath45 is @xmath46 here , @xmath47 , @xmath48 , and @xmath49 are constants . in the vn model ,",
    "the idf part is not changed ; however , @xmath45 is modified to @xmath50 obtained by replacing @xmath10 with @xmath30 , as follows : @xmath51 as in the case of dp , we assume the scale parameter @xmath29 to be 1 , because it is absorbed into @xmath47 , resulting in the following final form : @xmath52 the modified okapi function by using @xmath50 for @xmath45 is called * vn - okapi*.      mrfs are undirected graphical models that are used to define joint distributions over a set of random variables .",
    "the use of mrfs for ir was suggested by @xcite , going beyond the simplistic bag of words assumption , by explicitly modeling the term dependency among query words .",
    "thus far , three different variants of the mrf model have been suggested according to the type of dependency assumed among query words  full independence , sequence dependence , and full dependence .",
    "this paper focuses on _ sequence dependence _ , which has been widely used in many recent works @xcite , because of its good balance between effectiveness and efficiency .    to formally present the ranking function of the sequential dependence ,",
    "suppose that @xmath9 is a sequence of @xmath53 terms @xmath54 .",
    "according to the original framework , the relevance score of a document @xmath8 is given by @xcite @xmath55 where we have the constraint @xmath56 = 1 , and @xmath57 , @xmath58 and @xmath59 are called the _ feature functions _ of the term , _ ordered phrase _ , and _ unordered phrases _ , respectively .",
    "table [ tbl_mrf_feature_fuction ] presents the definition of each feature function @xcite .",
    "following the original framework @xcite , we assume that @xmath60 , @xmath61 , and @xmath62 are the same , i.e. , @xmath63 , unless otherwise stated .",
    "we refer to the retrieval function in eq .",
    "( [ eq_score_mrf_model ] ) as * mrf*.    to derive a vn retrieval model @xmath34 for mrf , we replace the original term frequencies with the verbosity normalized ones . for this purpose , let @xmath64 and @xmath65 be _ vn ordered _ and _ unordered phrase term frequencies _ for @xmath66 , respectively .",
    "similar to the definition of vn term frequency in eq .",
    "( [ eq_def_verbosity_normalized_tf ] ) , these vn phrase term frequencies are defined as follows : @xmath67    @xmath68    furthermore , let @xmath69 , @xmath70 , and @xmath71 be _ vn feature functions _ that correspond to original feature functions . table [ tbl_vn_mrf_feature_fuction ] describes the definition of each vn feature function .    as in the case of vn - dp , @xmath29",
    "is assumed to be 1 in all vn feature functions , because it is absorbed to @xmath60 , @xmath61 , or @xmath62 .",
    "finally , we obtain the scoring function for the vn model @xmath34 of mrf as follows : @xmath72 the mrf model using eq .",
    "( [ eq_score_vn_mrf_model ] ) is referred to as * vn - mrf*.      the remaining problem is how to compute the scope of a document @xmath19 . in this study , we adopt three different approaches  length power , the number of unique terms , and entropy power .      as mentioned in the introduction , according to the scope hypothesis , the document length is affected by the scope : the broader the scope of a document , the longer the document is , when its verbosity is assumed to be fixed .",
    "therefore , the document length could possibly be used as a scope measure according to the scope hypothesis . to derive such a length - based measure ,",
    "suppose that the scope of a document is a function of document length , i.e. , @xmath73 .",
    "many variants exist for such a function ; however , the verbosity and the scope hypotheses help us restrict the possible space for @xmath74 , given the following two necessary constraints :    @xmath0 * sc1 * : scope @xmath74 is a non - decreasing function of @xmath15 .",
    "+ @xmath0 * sc2 * : verbosity @xmath75 is a non - decreasing function of @xmath15 .",
    "+ to obtain such a scope measure that would satisfy both sc1 and sc2 , we use heap s law , which is given as follows @xcite relationship to the document length . because the number of unique terms can be used as a scope measure to indicate how broad the topic of the document is , as presented in section [ subsec_uniqlength ] , we use the formula of the heaps law to approximately predict the number of unique terms using only the document length . ] : @xmath76 where @xmath77 is an additional constant , containing the additional parameter @xmath78 . here",
    ", we assume that @xmath78 is absorbed in @xmath29 . ] .",
    "the possible range of @xmath77 is @xmath79 , from sc1 and sc2 .",
    "otherwise , @xmath19 ( or @xmath21 ) violates sc1 ( or sc2 ) if @xmath80 ( or @xmath81 ) . this length - based scope",
    "measure @xmath82 exactly degenerates into the original unnormalized representation , as a special case , when @xmath77 = 1 and @xmath29 = 1 , in which case @xmath82 = @xmath15 , @xmath21 = 1 , and @xmath83 .",
    "the scope measure using @xmath82 is called * lengthpower * in this paper .",
    "another useful scope measure is the number of unique terms @xmath84 , defined as @xmath85 .",
    "this is reasonable , because a different topic is described using a domain - specific vocabulary or named entities .",
    "the more unique terms used in a document , the larger is the scope of the document .",
    "the scope measure @xmath84 is referred to as * uniqlength * in this paper .",
    "the third scope measure is an entropy - based metric .",
    "previously , the entropy of a document was used to define the homogeneous measure of a document @xcite , which corresponds to the opposite concept of scope .",
    "another entropy - based metric is the entropy power defined by the exponential of the entropy , which was initially exploited in @xcite to construct the document structure .",
    "we compared the entropy with the entropy power in our preliminary experiments and found that the latter outperformed the former because of its similarity to document length or the number of unique terms .",
    "thus , we choose entropy power as our entropy - based metric , and it is defined as follows : @xmath86 where @xmath87 is defined by @xmath88 , which is the maximum likelihood estimation ( mle ) of the document language model for @xmath8 .",
    "the scope measure @xmath89 is called * entropypower * in this paper .",
    "in order to analytically check how differently the vn method satisfies retrieval constraints as compared to the corresponding original model , we present a comparative axiomatic analysis performed under the retrieval constraints introduced by .      as in the approach of @xcite",
    ", we divide the six standard constraints into two different sets  _ form constraints _",
    "( i.e. , tfc1 , tfc2 , and tdc in @xcite ) and _ normalization constraints _ ( i.e. , lnc1 , lnc2 , and tf - lnc in @xcite ) .",
    "the form constraints specify the desirable restrictions on the `` curve '' of a scoring function . formally , suppose that @xmath9 consists of a single word @xmath7 and @xmath34 is formulated by @xmath90 , where @xmath91 is @xmath10 and @xmath92 is @xmath14 . then , tfc1 , tfc2 , and tdc @xcite correspond to , respectively : @xmath93 it can be easily shown that tfcs and tdc are satisfied for all three normalized functions .",
    "this is a natural result , because our normalization only linearly transforms the term frequency and retains the original model , without any change to the basic concepts of the original model .",
    "the normalization constraints describe the necessary properties of a retrieval model for the case in which document - specific quantities such as length , verbosity , and scope are different across documents . according to @xcite",
    ", each normalization constraint can be equivalently described by how the score of a document changes after applying a perturbation operator to the document .",
    "we introduce three perturbation operators called * pan * , * pls * , and * par * that correspond to lnc1 , lnc2 , and tf - lnc , respectively , as follows :    \\1 ) * pan * ( * * p**erturbation of * * a**dding * * n**oise words ) : pan is an operator for adding noise terms , denoted by @xmath94 . given @xmath8 , @xmath95 is obtained by adding @xmath96 noise words @xmath97 to @xmath8 , i.e. , @xmath95 = @xmath98 , where @xmath99 .",
    "when @xmath100 = @xmath101 , @xmath102 and @xmath103 for all @xmath104 .",
    "\\2 ) * pls * ( * * p**erturbation of * * l**ength * * s**caling ) : pls is a length scaling operator , denoted by @xmath105 .",
    "given @xmath8 , @xmath106 is obtained by concatenating * all query words * in @xmath8 @xmath96 times and by scaling the length of @xmath8 up to @xmath96 times .",
    "= @xmath108 , @xmath109 and @xmath110 for all @xmath111 .",
    "note that the concatenation is only applied to query words , not necessarily to non - query words .",
    "the non - query words in @xmath8 might ( or might not ) be kept in @xmath106 . in the extreme case , all the non - query words do not appear in @xmath106 , being replaced with other non - query words .",
    "\\3 ) * par * ( * * p**erturbation of * * a**dding * * r**elevant words ) : par is an operator for adding a single relevant word , denoted by @xmath112 . given @xmath8 , @xmath113",
    "is obtained by appending a single query word @xmath111 , i.e. , @xmath113 = @xmath114 ( i.e. , the attached number of @xmath7 is @xmath96 ) @xmath96 times .",
    "= @xmath115 , @xmath116 , @xmath117 for a given single word @xmath111 , and @xmath118 for all @xmath119 .",
    "+ here , @xmath96 is a perturbation parameter .",
    "lnc1 , lnc2 , and tf - lnc can now be equivalently described as follows : * lnc1 * : if @xmath100 = @xmath101 , @xmath120 for @xmath121 .    *",
    "lnc2 * : if @xmath107 = @xmath108 , @xmath122 for @xmath123 .",
    "* tf - lnc * : let @xmath9 = @xmath124 be a query with only one term @xmath7 .",
    "if @xmath107 = @xmath115 , @xmath125 for @xmath121 .",
    "+ the perturbation operator pls for lnc2 is slightly different from the original version of lnc2 @xcite . in the original version , @xmath107 is fully copied to @xmath100 , making them identical . in our pls",
    ", only query words are concatenated @xmath96 times to @xmath107 , and no further assumption is made about non - query words . therefore",
    ", pls is the generalized version of the original operator , including the original version as a special case .",
    "this generalization does not cause any inconsistency in the known analysis results of lnc2 ; the analysis results reported in @xcite for lnc2 are also still consistently accepted with our pls operator . to see the difference more clearly , algorithm [ alg_pls_operator ] summarizes the detailed description of our pls operator .",
    "step 1 ) given @xmath8 , we apply the original pls operator of @xcite to @xmath8 to obtain @xmath126 ; @xmath126 is obtained by simply concatenating all words in @xmath8 @xmath96 times step 2 ) given @xmath126 , @xmath106 is obtained after applying the following procedure : let @xmath126 be @xmath127 initialize @xmath106 as an empty document .",
    "@xmath128 @xmath129 ( @xmath130 ) where @xmath131 is randomly chosen from @xmath132 .        before presenting our analysis results of the three normalization constraints",
    ", we make the following assumption :    * @xmath133 : for any query word @xmath111 , @xmath7 is assumed to be a content - bearing word ( i.e. , @xmath134 , and @xmath135 for any document @xmath8 in the collection ) .",
    "empirically , @xmath133 holds well in usual cases when we filter out stopwords .",
    "table [ tbl_c1_test ] lists the percentage of @xmath133 being satisfied using all non - stopwords in all queries from three different collections and three query types .",
    "( refer to section [ section_data_set ] for a description of the collections and query types . )",
    "as shown in table [ tbl_c1_test ] , @xmath135 is satisfied in more than 98% of the documents for all query words in robust , more than 95% in wt10 g , and more than about 93% in gov2 . the condition @xmath134 is satisfied for more than 98% of the query terms .",
    "there exist necessary conditions common for all vn retrieval models under @xmath133 to be satisfied for each normalization constraint .",
    "table [ tbl_analysis_result ] summarizes the analysis results of the general and the special cases of scope using lengthpower and uniqlength for vn retrieval models , relative to the original models .",
    "table [ tbl_analysis_result ] uses the notations introduced by @xcite , where `` yes '' and `` @xmath136 '' indicate that the corresponding model satisfies the particular constraint in the absence of conditions and under particular conditions , respectively .",
    "the specific conditions are    @xmath0 @xmath137 : @xmath138 + @xmath0 @xmath139 : @xmath140 + @xmath0 @xmath141 : @xmath142 + @xmath0 @xmath143 : @xmath144 + where @xmath137 , @xmath141 and @xmath143 are sufficient but not necessary conditions to satisfy the particular constraint . some derivations of the conditions",
    "are given in appendix c - e .",
    "as shown in table [ tbl_analysis_result ] , an original method satisfies all three constraints unconditionally under @xmath133 according to @xcite , whereas a vn method requires additional conditions that depend on the choice of scope measure .",
    "an exceptional case is lengthpower , in which all constraints are satisfied unconditionally .    among the three constraints ,",
    "tf - lnc is satisfied under lengthpower and uniqlength , the detailed proofs of which are presented in appendix d. under entropypower , tf - lnc is satisfied for almost all query words in our test collection , as shown in table [ tbl_c4_test ] ; @xmath143 is satisfied in more than 99.9% of the documents for all query words in robust , wt10 g and gov2",
    ". therefore , we do not explore tf - lnc further in this paper .      in this section , we discuss the retrieval behaviors entailed from the vn method in the cases of uniqlength and entropypower , with respect to the original method . in our discussion ,",
    "pan and pls are further divided into two different types ",
    "v - type and s - type  which refer to _ verbosity - increasing _ and _ scope - broadening _ perturbations , respectively .",
    "the definitions of these types of operators are as follows :    1 .   * v - type perturbation * : the operator @xmath145 is called _ v - type _ if the perturbation does not _ increase _ the scope of the document , i.e. , if @xmath107 = @xmath146 and @xmath147 is v - type , @xmath148 .",
    "* s - type perturbation * : the operator @xmath145 is called _ s - type _ if the perturbation does not _ decrease _ the scope of the document , i.e. , if @xmath107 = @xmath146 and @xmath147 is s - type , @xmath140    we then reexamine how the original and vn models satisfy lncs on v - type and s - type pan and pls .",
    "the notable result is that @xmath137 and @xmath139 correspond to a relaxed penalization of a scope - broadened document , and a strict penalization of a verbosity - increased document , respectively .",
    "first , we present the first heuristic h1 and discuss its derivation from @xmath137 :      _ the vn retrieval method performs a relaxed penalization of a scope - broadened document after performing pan .",
    "( from lnc1 ) _    to derive h1 , we divide pan into v - pan and s - pan .",
    "* v - pan * denotes _ verbosity - increasing _ pan , where @xmath96 added noise words are covered by the original scope of the document . *",
    "s - pan * denotes the _ scope - broadening _ pan , where @xmath96 added noise words describe _ new _ contents that are not covered by the scope of the original document . in terms of v - type and s - type , v - pan and s - pan can be defined as follows :    1 .   *",
    "v - pan * : v - pan is a specific type of pan , being v - type .",
    "* s - pan * : s - pan is a specific type of pan , being s - type .",
    "suppose that @xmath107 and @xmath149 are the given documents for lnc1 .",
    "then , we can show that the vn model often does _ not _ penalize @xmath100 for s - pan ; instead , it penalizes @xmath100 for v - pan . on the other hand , the original model _ always _",
    "penalizes @xmath100 for both s - pan and v - pan .",
    "thus , the vn model imposes a type of relaxed penalization to _ scope - broadened _ documents after pan , with respect to the original model .",
    "equivalently , the heuristic h1 can be rewritten in the form of a retrieval constraint as follows :    * h1-lnc * : if @xmath100 = @xmath101 and @xmath94 is s - pan , @xmath150 for @xmath121 with the following condition @xmath151 and @xmath152 , for vn - okapi and vn - dp , respectively :    @xmath0 @xmath151 : @xmath153 @xmath0 @xmath152 : @xmath154 where @xmath155 is additionally assumed in @xmath152 , assuming an extreme case where the query is a very highly topical , i.e. , @xmath156 or @xmath157 ) , @xmath152 is simplified as : @xmath158 ] .",
    "compared to lnc1 , the consequence part of h1-lnc conditionally entails the negation of lnc1 , implying that the vn model often _ prefers _ some of the scope - broadened documents resulting from s - pan , although the original model does not . ) and eq .",
    "( [ eq_c6_original ] ) , when @xmath159 is sufficiently large , @xmath151 ( or @xmath152 ) can be satisfied both for vn - okapi and vn - dp .",
    "this case can appear if @xmath160 is large and @xmath161 is small , but it is not always true .",
    "otherwise , @xmath151 ( or @xmath152 ) can be satisfied , according to the choice of a retrieval parameter value or a term discrimination value of a query word ; for vn - okapi , @xmath151 is satisfied if @xmath49 is sufficiently large ; for vn - dp , @xmath152 is satisfied if @xmath41 is sufficiently large and the query word is highly topical . ]",
    "@xmath0 _ example of h1 _ : here , we present examples of s - pan and v - pan .",
    "suppose that we use uniqlength as the scope measure and a document consisting of passages that are disjoint in scope .",
    "formally , let @xmath162 , @xmath163 , and @xmath91 be passages , and assume that @xmath162 , @xmath163 , and @xmath91 have no common or overlapping content , where @xmath162 denotes a relevant passage and @xmath163 and @xmath91 are non - relevant , i.e. , @xmath164 @xmath165 0 , @xmath166 = @xmath167 = 0 for query word @xmath111 .",
    "examples of s - pan and v - pan are as follows :    * example of s - pan : * +    * example of v - pan : * +    for both examples , the query relevant content is not changed after pan . because these two examples are pan examples , the original method always prefers @xmath107 to @xmath100 , irrespective of the pan type . however , @xmath137 is satisfied only for v - pan , because @xmath168 = @xmath161 and @xmath138 , and not clearly for s - pan , because @xmath169 is plausible due to @xmath170 .",
    "therefore , the vn method prefers @xmath100 in v - pan and not always in s - pan .",
    "@xmath0 _ derivation of h1 _ : to show how the vn model behaves differently toward v - pan and s - pan , we first rewrite @xmath137 by @xmath171 @xmath172 @xmath173 , implying that the scope of the new document @xmath100 must not be increased considerably after performing pan .    _",
    "i ) case : v - pan _    first , v - pan does not increase @xmath168 according to the definition of a v - type perturbation , resulting in @xmath174 . as a result",
    ", it is clear that @xmath137 is always true for v - pan , finally making lnc1 true .",
    "thus , for v - pan , there is no difference between the original and the vn models in satisfying lnc1 .",
    "for example , suppose that we use uniqlength as the scope measure and consider a given v - pan in which all @xmath96 words already occur in @xmath107 .",
    "in this case , @xmath175 because no new words occur in @xmath100 .",
    "thus , @xmath137 is equivalent to @xmath171= @xmath176 , which is true irrespective of @xmath96 .    _",
    "ii ) case : s - pan _",
    "second , s - pan increases the scope after performing pan according to the definition of an s - type perturbation , resulting in @xmath177 .",
    "therefore , @xmath137 is not always true .",
    "for example , suppose that we use uniqlength as the scope measure again , and consider a given s - pan in which all @xmath96 words are new and different from each other .",
    "in this case , @xmath168 = @xmath178 , and @xmath137 is equivalent to @xmath179 ; however , @xmath137 is usually not satisfied because @xmath180 .",
    "instead , it often satisfies the _ negation _ of lnc1 .",
    "consider the same s - pan example in which all @xmath96 words are different from each other and assume that we use vn - dp as an example retrieval model .",
    "@xmath152 is then equivalent to : @xmath181    there exist a number of situations in which @xmath152 is true according to eq .",
    "( [ eq_c6_simplified_example ] ) ( i.e. , @xmath160 is sufficiently large , or @xmath96 added words are highly topical ( @xmath182 ) and @xmath41 is reasonably large ) .",
    "therefore , for s - pan , the vn model often does not satisfy lnc1 , in contrast to the original model that always satisfies lnc1 @xmath183 .",
    "next , we present the heuristic h2 and discuss its derivation from @xmath139 :      _ the vn retrieval method imposes a strict penalization of a verbosity - increased document after performing pls ( from lnc2 ) . _    as was performed on pan , we divide pls into v - pls and s - pls . *",
    "v - pls * denotes _ verbosity - increasing _ pls , where the non - query words after pls is performed are covered by the original scope of the document , thereby increasing verbosity . *",
    "s - pls * denotes the _ scope - broadening _ pls , where the non - query words after pls is performed introduce _ new _ contents that are not covered by the scope of the original document . in terms of v - type and s - type , v - pls and s - pls",
    "can be defined as follows :    1 .   * v - pls * : v - pls is a specific type of pls , being v - type .",
    "* s - pls * : s - pls is a specific type of pls , being s - type",
    ".    given two documents @xmath184 and @xmath100 for lnc2 , from the definition of @xmath139 (",
    "i.e. , @xmath140 ) , lnc2 is satisfied only if the scope of the original document increases after pls is performed .",
    "therefore , the vn model _ prefers _ ( or does not penalize ) only @xmath107 for s - pls because it increases the scope ; instead , it _ penalizes _",
    "@xmath107 for v - pls , which decreases the scope . as such",
    ", the vn model imposes a strict penalization of a verbosity - increased document after pls .",
    "equivalently , the heuristic h2 can be rewritten in a form of retrieval constraint as follows :    * h2-lnc * : if @xmath107 = @xmath108 and @xmath105 is v - pls , @xmath185 for @xmath123 .",
    "compared to lnc2 , the consequence part of h2-lnc is the negation of that of lnc2 , implying that the vn model _",
    "always _ penalizes verbosity - increased documents resulting from s - pls , although the original model does _ not _",
    "( i.e , prefers them ) .",
    "@xmath0 _ example of h2 _ : we present examples of s - pls and v - pls .",
    "suppose that we use uniqlength as the scope measure and a document consisting of passages that are disjoint in scope .",
    "formally , let @xmath162 , @xmath163 , @xmath91 , and @xmath186 be passages with equal length ( i.e. , @xmath187 = @xmath188 = @xmath189 = @xmath190 ) and unit scope ( i.e. , @xmath191 = @xmath192 = @xmath193 = @xmath194 = 1 ) , and assume that @xmath162 , @xmath163 , @xmath91 , and @xmath186 have no common or overlapping content , where @xmath162 denotes a relevant passage and @xmath163 , @xmath91 , and @xmath186 are non - relevant , i.e. , @xmath195 , @xmath196 = @xmath197 = @xmath198 = 0 for query word @xmath111 .",
    "examples of s - pls and v - pls are as follows :    * example of s - pls : * +    * example of v - pls : * +    for both examples , @xmath199 = @xmath200 , @xmath201 = @xmath202 for @xmath111 , i.e. , the query - relevant content is copied twice .",
    "the example of s - pls introduces two _ new _ non - relevant passages @xmath203 and @xmath204 that are not given in @xmath100 , whereas the example of v - pls does not introduce any new passage but only repeats the previously mentioned non - relevant passage @xmath163 .",
    "because these two examples are pls examples , the original method always prefers @xmath107 to @xmath100 , irrespective of the pls type .",
    "however , @xmath139 is satisfied only for s - pls and not for v - pls , from @xmath205 in s - pls and @xmath206 in v - pls",
    ". therefore , the vn method prefers @xmath107 only in s - pls and not in v - pls .",
    "@xmath0 _ derivation of h2 _ : from the definitions of v - type and s - type perturbations , it is trivial to show that @xmath139 is true for v - pls but false for s - pls .",
    "therefore , for s - pls , the vn model does not satisfy lnc2 , in contrast to the original model which always satisfies lnc2 @xmath183 .      table [ tbl_analysis_result_of_four_perturbations ] summaries the normalization behaviors of the original and vn models in response to s - pan , v - pan , s - pls , and v - pls .    for v - pan and s - pls",
    ", the vn model leads to the same normalization heuristics as those of the original model . for s - pan and v - pls",
    ", however , the normalization behaviors are completely different between the original and the vn models ; for s - pan , the vn model often does _ not _ penalize the new document , whereas the original model _",
    "always _ penalizes it ; for v - pls , the vn model _ always _ penalizes the new document , whereas the original model does _ not _ penalize it .",
    "overall , the normalization heuristics entailed from the vn model are dependent on whether a perturbation is v - type or s - type .",
    "for a v - type perturbation , the vn model imposes a strict penalization of a verbosity - increased document ( i.e. , entailing h1 ) , irrespective of pan or pls . for an s - type perturbation ,",
    "the vn model unlikely penalizes a scope - broadened document ( i.e. , entailing h2 ) . on the other hand ,",
    "the normalization heuristics of the original model are dependent on whether a perturbation is pan or pls , not on whether it is v - type or s - type .",
    "for pan , the original model penalizes a new document , irrespective of whether the document is verbosity - increased or scope - broadened . for pls",
    ", it does not penalize the new document .",
    "for evaluation , we used three different standard trec collections  robust , wt10 g , and gov2 . table [",
    "tbl_collection_statistic ] lists the basic statistics of each test collection , where _ numdocs _ is the number of documents , _ numwords _ is the total number of word occurrences in each collection , _ topicset _ is the range of topic numbers used for training and testing , and _ avg _ of @xmath15 , @xmath89 , and @xmath21 indicates the average length , entropy power , and verbosity .",
    "] , respectively , in a given collection .",
    "@xmath207 is the corresponding _ coefficient of variance _ , which is defined as the ratio of the standard deviation to the mean .",
    "the interesting statistic is @xmath207 of @xmath21 , which indicates the differences among the verbosities of documents in a collection .",
    "robust has the most similar verbosities across documents , whereas gov2 has the most different verbosities .",
    "this is because many documents in robust are newspaper documents , for example , from financial times and los angeles times , which are more homogeneous collections .",
    "in contrast , the web documents in gov2 are more heterogeneous .",
    "for full definition of verbosity , suppose that @xmath208 is the total number of _ topics _ in a collection , and @xmath19 is the number of topics mentioned in @xmath8 ( or the _ expected _ number of topics in @xmath8 ) . here",
    ", we assume that the topic is _ countable _ , which may refer to an individual word or a concept . given document @xmath8",
    ", we first define the _ topic - specific verbosity _ of document @xmath8 , noted @xmath209 , which is the sum of frequencies of all words which belong to @xmath210 : @xmath211 where @xmath212 is the posterior probability that @xmath7 comes from @xmath210 ( i.e. , @xmath213 ) .    under eq .",
    "( [ eq_appendix_topic_specific_verbosity ] ) , we readily show that @xmath209 is the length of the passages in @xmath8 which belong to @xmath210 .    by the definition",
    "above , we further show that the following equality holds : @xmath214    to simplify eq .",
    "( [ eq_appendix_length_in_terms_of_topic_specific_verbosity ] ) , note that @xmath215 for most topics , as documents usually cover only a few topics .",
    "let @xmath216 be indexes of topics appearing in @xmath8 where @xmath217 .",
    "then , @xmath15 is reformulated as @xmath218    now , @xmath21 , the verbosity of document @xmath8 , is defined as the average of all _ per - topic _ verbosities computed for all @xmath19 topics appearing in @xmath8 , which is given by : @xmath219    thus , the average verbosity is the document length divided by the number of topics @xmath19 , which exactly replicates eq .",
    "( [ eq_def_verbosity ] ) . given eq .",
    "( [ eq_appendix_def_average_verbosity ] ) , we only require @xmath19 , without need to estimate @xmath209 which are usually unseen and hard to compute .",
    "the use of a document - specific conjugate prior for vn - dp ( i.e. , eq . ( [ eq_dirichletvn_prior ] ) ) is derived from the verbosity hypothesis . for convenience of discussion , suppose that @xmath220 indicates the unseen frequency of @xmath7 in @xmath8 . generally , smoothing uses @xmath221 defined as @xmath222 as a count of @xmath7 in @xmath8 , thereby estimating @xmath223 as @xmath224 .    in dp",
    ", it is assumed that the _ pseudo length _",
    "@xmath41 is distributed over unseen words according to @xmath225 , resulting in @xmath220 = @xmath226 . in our case , however , because document length is decomposed to verbosity and scope , we need to introduce two pseudo factors for unseen words : verbosity and scope . formally , let @xmath227 and @xmath228 be the verbosity and scope of an unseen part of @xmath8 , respectively . just like the formula of frequencies of seen words , which is given by @xmath10 = @xmath229 ,",
    "frequencies of unseen words are formulated as @xmath220 = @xmath230 .    to determine @xmath227 and @xmath228",
    ", we use the following assumptions :    \\1 . _ verbosity of an unseen part : given a document , the verbosity of unseen passages ( i.e. , consisting of all unseen words ) is the same as verbosity of the document . _  the assumption is due to the verbosity hypothesis ; @xmath10 is mostly governed by @xmath21 . just as the verbosity hypothesis is applied to seen words ,",
    "we apply the verbosity hypothesis to unseen words .",
    "this results in @xmath220 , the frequencies of unseen words , which should also be governed by @xmath21 .",
    "_ scope of an unseen part : given a document , the unseen scope of passages ( i.e. , consisting of all unseen words ) is independent of the scope of the document . _  unlike verbosity , we do not make a document - specific setting for the scope , as the relation between the unseen scope of @xmath8 and @xmath21 is not very clear .    under these assumptions",
    ", we have @xmath227 = @xmath21 , and @xmath228 = @xmath41 , thus resulting in @xmath231 , which leads to our use of a document - specific prior in eq .",
    "( [ eq_dirichletvn_prior ] ) .    therefore , the difference in formulating @xmath220 between dp and vn - dp results from whether or not we use the verbosity hypothesis for determining frequencies of unseen words .",
    "in this appendix , we briefly summarize the derivations of @xmath137 , @xmath139 , and @xmath141 for okapi and dp , where @xmath137 and @xmath141 are necessary but not sufficient for satisfying the particular constraint .",
    "let @xmath107 and @xmath100 be two given documents for lncs and tf - lnc and @xmath232 be @xmath233 .",
    "all our derivations start from the inequality of @xmath234 ( or @xmath235 ) . for vn - dp , @xmath236 is equivalent to @xmath237    for vn - okapi , @xmath234 is equivalent to @xmath238      we first show the derivation of the conditions for lnc1 under vn - dp and vn - okapi . for the sake of convenience ,",
    "we introduce the variables @xmath53 and @xmath239 that are defined as @xmath53 = @xmath240 and @xmath239 = @xmath241 , respectively . according to the definition of pan",
    ", we have the following relation between @xmath161 and @xmath168 : @xmath242 below , we summarize the derivation for each case of vn - dp and vn - okapi .",
    "+      we first simplify eq .",
    "( [ eq_appendix_score_difference_original_form_vn_dp ] ) by replacing @xmath168 with the terms @xmath53 , @xmath239 , and @xmath161 using eq .",
    "( [ eq_appendix_lnc1_relation_s1_s2 ] ) , as follows : @xmath243    first , when @xmath244 , it is easily shown that eq .",
    "( [ eq_appendix_lnc1_equivalent_vn_dp_starting ] ) holds .",
    "thus , we do not consider the equality case of @xmath133 to simplify eq .",
    "( [ eq_appendix_lnc1_equivalent_vn_dp_starting ] ) .    under the remaining cases of @xmath133 ( i.e. , @xmath245 ) , eq . ( [ eq_appendix_lnc1_equivalent_vn_dp_starting ] )",
    "is equivalent to @xmath246 under @xmath245 , the right - hand side of eq .",
    "( [ eq_appendix_lnc1_equivalent_vn_dp ] ) is a decreasing function with respect to @xmath247 , with the upper bound when @xmath247 = 0 . after replacing the right - hand side with this upper bound , the necessary condition for eq .",
    "( [ eq_appendix_lnc1_equivalent_vn_dp ] ) is simplified to : @xmath248 which is satisfied if @xmath137 is true , regardless of the choice of parameter @xmath41 .",
    "as in the case of vn - dp , we replace @xmath161 with the terms @xmath53 and @xmath239 based on eq .",
    "( [ eq_appendix_lnc1_relation_s1_s2 ] ) , simplifying eq .",
    "( [ eq_appendix_score_difference_original_form_vn_okapi ] ) to @xmath249    first , when @xmath250 , it is clear that eq .",
    "( [ eq_appendix_lnc1_equivalent_vn_okapi_starting ] ) holds .",
    "second , when @xmath251 ( under @xmath133 ) , eq .",
    "( [ eq_appendix_lnc1_equivalent_vn_okapi_starting ] ) is further rewritten as @xmath252 which is equivalent to @xmath253 thus , lnc1 is satisfied if @xmath137 is true .",
    "we could straight forwardly derive that lnc2 is equivalent to @xmath139 . to simplify the notation for the derivation ,",
    "we introduce @xmath254 = @xmath255 = @xmath256 ; the equality holds because of the characteristic of pls .",
    "we summarize the derivation of @xmath139 for each of vn - dp and vn - okapi . +      for vn - dp , eq .",
    "( [ eq_appendix_score_difference_original_form_vn_dp ] ) is simplified to @xmath257 it is trivial to show that the necessary and sufficient condition for lnc2 is @xmath139 , if @xmath133 holds :      for vn - okapi , eq .",
    "( [ eq_appendix_score_difference_original_form_vn_okapi ] ) is simplified to @xmath258 using @xmath259 from @xmath133 , eq .",
    "( [ eq_appendix_lnc2_equivalent_vn_okapi_starting ] ) is equivalent to @xmath139 .",
    "+      for the sake of convenience , we introduce variables @xmath260 and @xmath261 by putting @xmath260 = @xmath262 and @xmath261 = @xmath263 . according to the definition of par ,    @xmath264    below",
    ", we summarize the derivation for each of vn - dp and vn - okapi .",
    "+      we first simplify eq .",
    "( [ eq_appendix_score_difference_original_form_vn_dp ] ) by replacing @xmath161 with the terms @xmath260 , @xmath261 , and @xmath168 , as follows : @xmath265    when @xmath266 , it is easily shown that eq .",
    "( [ eq_appendix_tf_lnc_equivalent_vn_dp_starting ] ) holds .    when @xmath267 , in the remaining cases of @xmath133 , eq .",
    "( [ eq_appendix_tf_lnc_equivalent_vn_dp_starting ] ) is equivalent to @xmath268    for @xmath267 , the right - hand side of eq .",
    "( [ eq_appendix_tf_lnc_equivalent_vn_dp_derived ] ) is an increasing function with respect to @xmath247 , with the lower bound when @xmath247 = 0 .",
    "in addition , we can further lower the bound by eliminating @xmath269 because it is a positive value .",
    "thus , we obtain the following necessary condition for eq .",
    "( [ eq_appendix_tf_lnc_equivalent_vn_dp_derived ] ) : @xmath270 which is equivalent to @xmath141 ) can contain the equality condition , because @xmath271 ( even when @xmath272 ) ] .",
    "+      as in the case of vn - dp , we replace @xmath161 in the terms @xmath260 , @xmath261 , and @xmath168 using eq .",
    "( [ eq_appendix_tf_lnc_relation_s1_s2 ] ) simplifying eq .",
    "( [ eq_appendix_score_difference_original_form_vn_okapi ] ) to @xmath273    under @xmath133 , because @xmath259 , eq . ( [ eq_appendix_tf_lnc_equivalent_vn_okapi_starting ] ) is equivalent to @xmath274 a lower bound for the right - hand side eq .",
    "( [ eq_appendix_tf_lnc_equivalent_vn_okapi_derived ] ) is obtained by eliminating ( 1-@xmath275 ) , which is a positive value .",
    "therefore , the necessary condition for eq .",
    "( [ eq_appendix_tf_lnc_equivalent_vn_okapi_derived ] ) becomes @xmath276 which is equivalent to @xmath141 . )",
    "can allow the equality condition . ]",
    "tf - lnc is true when using uniqlength and lengthpower . to prove this ,",
    "let @xmath277 be @xmath278 .",
    "first , when @xmath279 , it is equivalent to @xmath280 , and thus , @xmath141 is satisfied . otherwise ( i.e. , @xmath281 ) , @xmath141 is equivalent to @xmath282 the term on the right - hand side of eq .",
    "( [ eq_c3_equivalent ] ) is a decreasing function with respect to @xmath277 . for the cases of uniqlength and lengthpower , @xmath283 , regardless of @xmath96 ; therefore , eq . ( [ eq_c3_equivalent ] ) is satisfied if @xmath284 ( i.e. , obtained by using @xmath285 in the right - hand side of eq .",
    "( [ eq_c3_equivalent ] ) ) , which is true for all cases :",
    "in this appendix , for vn models using entropypower , we show that tf - lnc is satisfied if @xmath143 is true . to prove this ,",
    "let @xmath277 be @xmath286 .",
    "when @xmath287 , it is equivalent to @xmath280 , and thus , @xmath141 is satisfied .",
    "otherwise , @xmath141 is equivalent to : .",
    "@xmath288            from the definition of tf - lnc , it is clear that once eq .",
    "( [ eq_condition_tf_lnc_entropypower ] ) holds for @xmath96 = 1 , then eq . ( [ eq_condition_tf_lnc_entropypower ] ) also holds for every @xmath96 . therefore , here",
    ", we only consider @xmath295 .",
    "when @xmath296 , eq .",
    "( [ eq_condition_tf_lnc_entropypower ] ) is further simplified to : @xmath297 which leads to :      applying @xmath299 to the first term of the left - hand side of eq .",
    "( [ eq_condition_tf_lnc_entropypower_simplified2 ] ) , we obtain the following sufficient condition for eq ( [ eq_condition_tf_lnc_entropypower_simplified2 ] ) : @xmath300 which is rewritten as @xmath301"
  ],
  "abstract_text": [
    "<S> the standard approach for term frequency normalization is based only on the document length . </S>",
    "<S> however , it does not distinguish the verbosity from the scope , these being the two main factors determining the document length . </S>",
    "<S> because the verbosity and scope have largely different effects on the increase in term frequency , the standard approach can easily suffer from insufficient or excessive penalization depending on the specific type of long document . to overcome these problems , this paper proposes two - stage normalization by performing verbosity and scope normalization separately , and by employing different penalization functions . in verbosity </S>",
    "<S> normalization , each document is pre - normalized by dividing the term frequency by the verbosity of the document . in scope </S>",
    "<S> normalization , an existing retrieval model is applied in a straightforward manner to the pre - normalized document , finally leading us to formulate our proposed verbosity normalized ( vn ) retrieval model . </S>",
    "<S> experimental results carried out on standard trec collections demonstrate that the vn model leads to marginal but statistically significant improvements over standard retrieval models .    </S>",
    "<S> [ retrieval models ]    author e - mail : nash@bufs.ac.kr + this work was partly supported by the it r&d program of msip / keit . [ 10041807 , development of original software technology for automatic speech translation with performance 90% for tour / international event focused on multilingual expansibility and based on knowledge learning ] and by the research grant of the busan university of foreign studies in 2014 .    </S>",
    "<S> # 1*_#1 _ * </S>"
  ]
}