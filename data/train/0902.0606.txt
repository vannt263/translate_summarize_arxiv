{
  "article_text": [
    "even in the era of the information technology revolution , language remains the most powerful and sophisticated communication system in the history of civilization  @xcite .",
    "its understanding requires an interdisciplinary approach and has broad conceptual and practical implications .",
    "it involves a number of disciplines ; from computer science , where natural language processing  @xcite seeks to model language computationally , to cognitive science , that tries to understand our intelligence with linguistics as one of its key contributing disciplines  @xcite .",
    "after speech , written text is probably the most fundamental manifestation of human language .",
    "nowadays , electronic and information technology media offer the opportunity of recording and accessing easily huge amounts of documents that can be analyzed in quest for some of the signatures of human communication . as a first step ,",
    "statistical patterns in written text can be detected as a trace of the mental processes we use in communication .",
    "it has been realized that various universal regularities characterize text from different domains and languages .",
    "the best - known is zipf s law on the distribution of word frequencies  @xcite , according to which the frequency of terms in a collection decreases inversely to the rank of the terms .",
    "zipf s law has been found to apply to collections of written documents in virtually all languages .",
    "other notable universal regularities of text are heaps law  @xcite , according to which vocabulary size grows slowly with document size , i.e. as a sublinear function of the number of words ; and the bursty nature of words  @xcite , making a word more likely to reappear in a document if it has already appeared , compared to its overall frequency across the collection .",
    "understanding the structure of written text is key to a broad range of critical applications such as web search  @xcite ( and the booming business of online advertising ) , literature mining  @xcite , topic detection  @xcite , and security  @xcite .",
    "thus , it is not surprising that researchers in linguistics , information and cognitive science , machine learning , and complex systems are coming together to model how universal text properties emerge .",
    "different models have been proposed that are able to predict each of the universal properties outlined above",
    ". however , no single model of text generation explains all of them together .",
    "furthermore , no model has been used to interpret or predict the empirical distributions of text similarity between documents in a collection  @xcite .        in this paper",
    "we present a model that generates collections of documents consistently with all of the above statistical features of textual corpora , and validate it against large and diverse web datasets .",
    "we go beyond the global level of zipf s law , which we take for granted , and focus on general correlation signatures within and across documents .",
    "these correlation patterns , manifesting themselves as burstiness and similarity , are destroyed when the words in a collection are reshuffled , even while the global word frequencies are preserved . therefore the correlations are not simply explained by zipf s law , and are directly related to the global organization and topicality of the corpora .",
    "the aim of our model is not to reproduce the microscopic patterns of occurrence of individual words , but rather to provide a stylized generative mechanism to interpret their emergence in statistical terms .",
    "consequently , our main assumption is a global distribution of word probabilities ; we do not need to fit a large number of parameters to the data , in contrast to parametric models proposed to describe the bursty nature or topicality of text  @xcite . in our model , each document is derived by a local ranking of dynamically reordered words , and different documents are related by sharing subsets of these rankings that represent emerging topics .",
    "our analysis shows that the statistical structure of text collections , including their level of topicality , can be derived from such a simple ranking mechanism .",
    "ranking is an alternative to preferential attachment for explaining scale invariance  @xcite and has been used to explain the emergent topology of complex information , technological , and social networks  @xcite .",
    "the present results suggest that it may also shed light on cognitive processes such as text generation and the collective mechanisms we use to organize and store information .",
    "we have selected three very diverse public datasets , from topically focused to broad coverage , to illustrate the statistical regularities of text and validate our model .",
    "the first corpus is the industry sector database ( is ) , a collection of corporate web pages organized into categories or sectors .",
    "the second dataset is a sample of the open directory ( odp ) , a collection of web pages classified into a large hierarchical taxonomy by volunteer editors .",
    "the third corpus is a random sample of topic pages from the english wikipedia ( wiki ) , a popular collaborative encyclopedia which also is comprised of millions of online entries .",
    "( see appendix a for details . )",
    "we measured the statistical regularities mentioned above in our datasets and the empirical results are shown in fig .  1 .",
    "the distributions of document length for all three collections is very well approximated by a log - normal , with different first and second moment parameters ( see table  1 and fig .  1 in appendix a ) .",
    "according to zipf s law  @xcite , the global frequency @xmath0 of terms in a collection decreases roughly inversely to their rank @xmath1 : @xmath2 or , in other words , the distribution of the frequency @xmath0 is well approximated by a power law @xmath3 with exponent @xmath4 .",
    "this regularity has been found to apply to collections of written documents in virtually all languages , including the datasets used here ( fig .",
    "heaps law  @xcite describes the sublinear growth of vocabulary size ( number of unique words ) @xmath5 as a function of the size of a document ( number of words ) @xmath6 ( fig .",
    "this regularity has also been observed in different languages , and the behavior has been interpreted as a power law @xmath7 with @xmath8 , although the exponent @xmath9 between 0.4 and 0.6 is language - dependent  @xcite .",
    "burstiness is the tendency of some words to occur clustered together in individual documents , so that a term is more likely to reappear in a document where it has appeared before  @xcite .",
    "this property is more evident among rare words , which are more likely to be topical .",
    "following elkan  @xcite , the bursty nature of words can be illustrated by dividing words into classes according to their global frequency ( e.g. , common vs. rare ) .",
    "for words in each class , we plot in fig .",
    "1c the fraction @xmath10 of documents in which these words occur with frequency @xmath11 .",
    "we compare the distribution @xmath10 of common and rare terms with those predicted by the null independence hypothesis , that generates documents whose length is drawn from the same lognormal distribution as the empirical data ( see table  1 and fig .  1 in appendix a ) by drawing words independently at random from the global zipf frequency distribution ( fig .",
    "1a ) . as compared to the reference of such a _ zipf model _",
    ", rare terms are much more likely to cluster in specific documents and not to appear evenly distributed in the collection , so that ordering principles beyond those responsible for zipf s law have to be at play .",
    "another signature of text collections , which is more telling about topicality , is the distribution of lexical similarity across pairs of documents . in information retrieval and text mining ,",
    "documents are typically represented as term vectors  @xcite .",
    "each element of a vector represents the weight of the corresponding term in the document .",
    "there are various vector representations according to different weighting schemes . here",
    ", we focus on the simplest scheme , in which a weight is simply the frequency of the term in the document .",
    "the similarity between two documents is given by the cosine between the two vectors : @xmath12 , where @xmath13 is the weight of term @xmath14 in document @xmath15 .",
    "it has been observed that for documents sampled from the odp , the distribution of cosine similarity based on term frequency vectors is concentrated around zero and decays in a roughly exponential fashion for @xmath16  @xcite .",
    "figure  1d shows that different collections yield different similarity profiles , however they all tend to be more skewed toward small similarity values than predicted by the zipf model .    modeling how these properties emerge from simple rules is central to an understanding of human language and related cognitive processes .",
    "our understanding , however , is far from definitive .",
    "first , because the empirical observations are open to different interpretations . as an example",
    ", much has been written about the debate between simon and mandelbrot around different interpretations of zipf s law ( see www.nslij-genetics.org/wli/zipf for a historical review of the debate ) .",
    "second , and perhaps more importantly , no single model of text generation explains all of the above observations simultaneously .",
    "third , models at hand are usually based on heuristic methods rather than on basic principles and general mechanisms that could explain linguistic processes as emergent phenomena .    in the remainder of this paper",
    ", we focus on burstiness and similarity distributions .",
    "regarding similarity , little attention has been given to its empirical distribution and , to the best of our knowledge , no model has been put forth to explain its profile . regarding text burstiness ,",
    "on the other hand , several models have been proposed including the two - poisson model  @xcite , the poisson zero - inflated mixture model  @xcite , katz k - mixture model  @xcite , and a gap - based variation of bayes model  @xcite .",
    "another line of generative models extends the simple multinomial family with increasingly complex views of topics .",
    "examples include probabilistic latent semantic indexing  @xcite , latent dirichlet allocation ( lda )  @xcite , and pachinko allocation  @xcite .",
    "these models assume a set of topics , each typically described by a multinomial distribution over words .",
    "each document is then generated from some mixture of these topics . in lda , for example",
    ", the parameters of the mixture are drawn from a dirichlet distribution , independently for each document .",
    "each word in a document is generated by drawing a topic from the mixture and then the term from its corresponding word distribution .",
    "a variety of techniques have been developed to estimate from data the parameters that characterize the many distributions involved in the generative process  @xcite .",
    "although the above models were mainly developed for subject classification , they have also been used to investigate burstiness since bursty words can characterize the topic of a document  @xcite .",
    "the very large numbers of free parameters associated with individual terms , topics , and/or their mixtures grant the above models great descriptive power . however , their cognitive plausibility is problematic .",
    "our aim here is instead to produce a simpler , more plausible mechanism compatible with the high - level statistical regularities associated with _ both _",
    "burstiness and similarity distributions , without regard for explicit topic modeling .",
    "two basic mechanisms , reordering and memory , can explain burstiness and similarity consistently with zipf s law .",
    "we show this by proposing a generative model that incorporates these processes to produce collections of documents characterized by the observed statistical regularities .",
    "each document is derived by a local ranking of words that reorganizes according to the changing word frequencies as the document grows , and different documents are related by sharing subsets of these rankings that represent emerging topics . with just the main assumptions of the global distribution of word probabilities and document sizes and a single tunable parameter measuring the topicality of the collection , we are able to generate synthetic corpora that re - create faithfully the features of our web datasets .",
    "next , we describe two variations of the model , one without memory and the second with a memory mechanism that captures topicality .          in our model ,",
    "@xmath17 documents are generated drawing word instances repeatedly with replacement from a vocabulary of @xmath18 words .",
    "the document lengths in number of words are drawn from a lognormal distribution .",
    "the parameters @xmath17 , @xmath18 and the lognormal mean and variance are derived empirically from each dataset ( see table  1 in appendix a ) .",
    "we further assume that at any step of the generation process , word probabilities follow a zipf distribution @xmath19 \\propto r(t)^{-1}$ ] where @xmath20 is the rank of term @xmath14 .",
    "however , rather than keeping a fixed ranking , we imagine that words are sorted dynamically during the generation of each document according to the number of times they have already occurred .",
    "words and ranks are thus decoupled : at different times , a word can have different ranks and a position in the ranking can be occupied by different words .",
    "the idea is that as the topicality of a document emerges through its content , topical words will be more likely to reoccur within the same document .",
    "this idea is incorporated into the model as a frequency bias favoring words that occur early in the document .    in the first version of the model ,",
    "each document is produced independently of each other . before each new document",
    "is generated , words are sorted according to an initial global ranking , which remains fixed for all documents .",
    "this ranking @xmath21 is also used to break ties during the generation of documents , mong words with the same occurrence counts .",
    "the algorithm corresponding to this dynamic ranking model is illustrated in fig .  2 and detailed in appendix c.    when a sufficiently large number of documents is generated , the measured frequency of a word @xmath14 over the entire corpus approaches the zipf distribution @xmath22^{-1}$ ] , ensuring the self consistency of the model .",
    "we numerically simulated the dynamic ranking model for each dataset . a direct comparison with the empirical burstiness curves shown in fig .",
    "1c can be found in fig .",
    "the excellent agreement suggests that the dynamic ranking process is sufficient for producing the right amount of correlations inside documents needed to realistically account for the burstiness effect .",
    "heaps law can be derived analytically from our model ( see appendix b ) . assuming a zipf s law with a tail of the form @xmath23 where @xmath24 , the solution is @xmath25 and we recover heaps sublinear growth with @xmath26 for large @xmath6 . according to the yule - simon model  @xcite , which interprets zipf s law through a preferential attachment process",
    ", the rank distribution should have a tail with exponent @xmath24 .",
    "this is confirmed empirically in many english collections ; for example our odp and wikipedia datasets yield zipfian tails with @xmath27 between 3/2 and 2 .",
    "our model predicts that in these cases heaps growth should be well approximated by a power law with exponent @xmath9 between 1/2 and 2/3 , closely matching those reported for the english language  @xcite .",
    "simulations using the empirically derived @xmath28 for each dataset display growth trends for large @xmath6 that are in good agreement with the empirical behavior ( fig .",
    "3b ) .",
    "the agreement between empirical data and simulations of the model with respect to the similarity distributions gets worse for those datasets that are more topically focused . a new mechanism is needed to account for topical correlations between documents .",
    "the model in the previous section generates collections of independent text documents , with specific but uncorrelated topics captured by the bursty terms . for each new document ,",
    "the rank of each word @xmath14 is initialized to its original value @xmath29 so that each document has no bias toward any particular topic .",
    "the synthetic corpora which result display broad coverage .",
    "however , real corpora may cover more or less specific topics .",
    "the stronger the semantic relationship between documents , the higher the likelihood they share common words .",
    "such collection topicality needs to be taken into account to accurately reproduce the distribution of text similarity between documents .    to incorporate topical correlations into our model ,",
    "we introduce a memory effect connecting word frequencies across different documents .",
    "generative models with memory have already been proposed to explain heaps law  @xcite . in our algorithm",
    "( see fig .  2 and appendix c ) we replace the initialization step so that a portion of the initial ranking of the terms in each document is inherited from the previously generated document . in particular ,",
    "the counts of the @xmath30 top - ranked words are preserved while all the others are reset to zero .",
    "the rank @xmath31 is drawn from an exponential distribution @xmath32 where @xmath33 is a probability parameter that models the lexical diversity of the collection and @xmath30 has expected value @xmath34 , which can be interpreted as the collection s shared topicality .",
    "this variation of the model does not interfere with the reranking mechanism described in the previous section , so that the burstiness effect is preserved .",
    "the idea is to interpolate between two extreme cases .",
    "the case @xmath35 , in which counts are never reset , converges to the null zipf model .",
    "all documents share the same general terms , modeling a collection of unspecific documents . here",
    "we expect a high similarity in spite of the independence among documents , because the words in all documents are drawn from the identical zipf distribution .",
    "the other extreme case , @xmath36 , reduces to the original model , where all the counts are always initialized to zero before starting a document . in this case , the bursty words are numerous but not the same across different documents , modeling a situation in which each document is very specific but there is no shared topic across documents . intermediate cases",
    "@xmath37 allow us to model correlations across documents not only due to the common general terms , but also to topical ( bursty ) terms .",
    "we simulated the dynamic ranking model with memory under the same conditions corresponding to our datasets , but additionally fitting the parameter @xmath33 to match the empirical similarity distributions .",
    "the comparisons are shown in fig",
    "the similarity distribution for the odp is best reproduced for @xmath36 , in accordance to the fact that this collection is overwhelmingly composed of very specific documents spanning all topics .",
    "in such a situation , the original model accurately reproduces the high diversity among document topics and there is no memory need .",
    "in contrast , wikipedia topic pages use a homogenous vocabulary due to their strict encyclopedic style and the social consensus mechanism driving the generation of content .",
    "this is reflected in the value @xmath38 , corresponding to an average of @xmath39 common words whose frequencies are correlated across successive pairs of documents .",
    "the industry sector dataset provides us with an intermediate case in which pages deal with more focused , but semantically related topics .",
    "the best fit of the similarity distribution is obtained for @xmath40 .    with the fitted values for the shared topicality parameter @xmath33 , the agreement between model and empirical similarity data in fig .",
    "3c is excellent over a broad range of similarity values . to better illustrate the significance of this result ,",
    "let us compare it with the prediction of a simple topic model .",
    "for this purpose one must have a priori knowledge of a set of topics to be used for generating the documents .",
    "the is dataset lends itself to this analysis because the pages are classified into twelve disjoint industry sectors , which can naturally be interpreted as unmixed topics . for each topic @xmath41 , we measured the frequency of each term @xmath14 and used it as a probability @xmath42 in a multinomial distribution .",
    "we generated the documents for each topic using the actual empirical values for the number of documents in the topic and the number of words in each document .",
    "as shown in fig .",
    "3c , the resulting similarity distribution is better than that of the zipf model ( where we assume a single global distribution ) , however the prediction is not nearly as good as that of our model .",
    "our model only requires a single free parameter @xmath33 plus the global ( zipfian ) distribution of word probabilities , which determines the initial ranking .",
    "conversely , for the topic model we must have or fit the frequency distribution @xmath42 over all terms for each topic , which implies an extraordinary increase in the number of free parameters since , apart from potential differences in the functional forms , each distribution would rank the terms in a different order .",
    "aside from complexity issues , the ability to recover similarities suggests that the dynamic ranking model , though not as well informed as the topic model on the distributions of the specific topics , better captures word correlations .",
    "topics emerge as a consequence of the correlations between bursty terms across documents as determined by @xmath33 , but it is not necessary to predefine explicitly the number of topics or their distributions as other models require .",
    "our results show that key regularities of written text beyond zipf s law , namely burstiness , topicality and their interrelation , can be accounted for on the basis of two simple mechanisms , namely frequency ranking with dynamic reordering and memory accross documents , and can be modeled with an essentially parameter - free algorithm .",
    "the rank based approach is in line with other recent models in which ranking has been used to explain the emergent topology of complex information , technological , and social networks  @xcite .",
    "it is not the first time that a generative model for text has walked parallel paths with models of network growth .",
    "a remarkable example is the yule - simon model of text generation  @xcite , which was later rediscovered in the context of citation analysis  @xcite , and has recently found broad popularity in the complex networks literature  @xcite .",
    "our approach applies to datasets where the temporal sequence of documents is not important , but burstiness has also been studied in contexts where time is a critical component  @xcite , and even in human languages evolution  @xcite .",
    "further investigations in relation to topicality could attempt to explicitly demonstrate the role of the topicality correlation parameter by looking at the hierarchical structure of content classifications .",
    "subsets of increasingly specific topics of the whole collection could be extracted to study how the parameter @xmath33 changes and how it is related to external categorizations .",
    "the proposed model can also be used to study the coevolution of content and citation structure in the scientific literature , social media such as the wikipedia , and the web at large  @xcite .    from a broader perspective",
    ", it seems natural that models of text generation should be based on similar cognitive mechanisms as models of human text processing since text production is a translation of semantic concepts in the brain into external lexical representations .",
    "indeed , our model s connection between frequency ranking and burstiness of words provides a way to relate two key mechanisms adopted in modeling how humans process the lexicon : rank frequency  @xcite and context diversity  @xcite .",
    "the latter , measured by the number of documents that contain a word , is related to burstiness since given a term s overall collection frequency , higher burstiness implies lower context diversity .",
    "while tracking frequencies is a significant cognitive burden , our model suggests that simply recognizing that a term occurs more often than another in the first few lines of a document would suffice for detecting bursty words from their ranking and consequently the topic of the text .    in summary , a picture of how language structure and topicality emerge in written text as complex phenomena can shed light into the collective cognitive processes we use to organize and store information , and find broad practical applications , for instance , in topic detection , literature analysis , and web mining .",
    "we thank charles elkan , rich shiffrin , michael jones , alessandro vespignani , dragomir radev , vittorio loreto , ciro cattuto , and marin bogu for useful discussions .",
    "jacob ratkiewicz provided assistance in gathering and analyzing the wikipedia dataset . this work was supported in part by a lagrange senior fellowship of the crt foundation to fm , by the institute for scientific interchange foundation , the epfl laboratory of statistical biophysics , and the indiana university school of informatics ; m.  a.  s. acknowledges support by dges grant no .",
    "fis2007 - 66485-c02 - 01 .",
    "we use three different datasets .",
    "the industry sector database ( is ) is a collection of almost 10,000 corporate web pages organized into 12 categories or sectors .",
    "the second dataset is a sample of the open directory ( dmoz.org , odp ) , a collection of web pages classified into a large hierarchical taxonomy by volunteer editors . while the full odp includes millions of pages , our collection comprises of approximately 150,000 pages , sampled uniformly from all top - level categories and crawled from the web .",
    "the third corpus is a random sample of 100,000 topic pages from the english wikipedia ( en.wikipedia.org , wiki ) , a popular collaborative encyclopedia which also is comprised of millions of online entries .    ) .,scaledwidth=45.0% ]    these english text collections are derived from public data and are publicly available ( is dataset is available at www.cs.umass.edu/~mccallum/code-data.html , odp and wikipedia corpora available upon request ) ; have been used in several previous studies , allowing a cross check of our results ; and are large enough for our purposes without being computationally unmanageable .",
    "the datasets are however very diverse in a number of ways .",
    "the is corpus is relatively small and topically focused , while odp and wikipedia are larger and have broader coverage , as reflected in their vocabulary sizes . is documents represent corporate content , while many web pages in the odp collection are individually authored .",
    "wikipedia topics are collaboratively edited and thus represent the consensus of a community . in spite of such differences , the distributions of document length for all three collections",
    "are very well approximated by log - normals shown in fig .",
    "[ lognormal ] , with different first and second moment parameters .",
    "table  [ tabledb ] summarizes the main statistical features of the three collections . before our analysis ,",
    "all documents in each collection have been parsed to extract the text ( removing html markup ) and syntactic variations of words have been conflated using standard stemming techniques  @xcite .",
    ".statistics for the different document collections .",
    "@xmath18 stands for vocabulary size , @xmath17 for the number of documents containing at least one word ( in parenthesis the number of empty documents in the collection ) , @xmath43 for the average size of documents in number of unique words , and @xmath44 and @xmath45 for the average and variance of document size in number of words . for each collection ,",
    "the distribution of document size is very well fitted by a lognormal with parameters @xmath46 and @xmath47 . [ cols=\"<,^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ tabledb ]",
    "the probability @xmath48 to find @xmath5 different words in a document of size @xmath6 satisfies the following discrete master equation : @xmath49 where @xmath50 , and @xmath28 is the zipf probability associated with rank @xmath1 .",
    "the tail of the zipfian rank distribution is critical because the words not yet observed occupy the ranks at the bottom of the frequency distribution ( @xmath51 ) , and their cumulative probability is therefore @xmath52 . multiplying both sides of eq .",
    "[ master ] by @xmath53 and summing over @xmath5 leads to a relation between the expected values @xmath54 $ ] of the number of different words for document sizes @xmath55 and @xmath6 : @xmath56=e[w(n)]+e [ 1-f(w(n ) ) ] \\label{relation}\\ ] ] where the second term in the _ r.h.s . _",
    "states that the probability to observe a new word ( when @xmath5 different words are already present in the document ) is the cumulative probability of words with frequency ranking larger than @xmath5 .",
    "neglecting fluctuations and taking the continuous limit , eq .",
    "[ relation ] leads to @xmath57 eq .",
    "[ eq : heap ] can be integrated numerically using the actual @xmath28 from the data .",
    "alternatively , eq .  [ eq : heap ] can be solved analytically for special cases ( see main text ) .",
    "the dynamic ranking model is implemented by the following algorithm :    `  ` + ` vocabulary :  ` @xmath58 + ` initial  ranking :  ` @xmath59 + ` repeat  until  ` @xmath17 `  documents  are  generated : ` + `  initialize  term  counts  to  ` @xmath60 `  ( * ) ` + `  draw  ` @xmath61 `  from  lognormal(`@xmath62 ` ) ` + `  repeat  until  ` @xmath61 `  terms  are  generated : ` + `  sort  terms  to  obtain  new  rank  ` @xmath20 + `  according  to  ` @xmath63 `  ( break  ties  by  ` @xmath64 ` ) ` + `  select  term  ` @xmath14 `  with  probability  ` @xmath65 + `  add  ` @xmath14 `  to  current  document ` + `  ` @xmath66 + `  end  of  document ` + ` end  of  collection `    the document initialization step ( line marked with an asterisk in above pseudocode ) is altered in the more general , memory version of the model ( see main text ) .",
    "in particular we set to zero the counts @xmath63 not of all terms , but only of terms @xmath14 such that @xmath67 .",
    "the rank @xmath31 is drawn from an exponential distribution @xmath32 where @xmath33 is a probability parameter that measures the lexical diversity of the collection and @xmath30 has expected value @xmath34 . in simpler terms , the counts of the @xmath30 top - ranked words are preserved while all the others are reset to zero .",
    "algorithmically , terms are sorted by counts so that the top - ranked term @xmath14 ( @xmath68 ) has the highest @xmath63 .",
    "we iterate over the ranks @xmath1 , flipping a biased coin for each term .",
    "as long as the coin returns false ( probability @xmath69 ) , we preserve @xmath70 . as soon as the coin returns true ( probability @xmath33 ) , say for the term @xmath71 , we reset all the counts for this and the following terms : @xmath72 .",
    "the special case @xmath36 reverts to the original , memory - less model ; all counts are reset to zero and each document restarts from the global zipfian ranking @xmath21 .",
    "the special case @xmath35 is equivalent to the zipf null model as the term counts are never reset and thus rapidly converge to the global zipfian frequency distribution ."
  ],
  "abstract_text": [
    "<S> human language , the most powerful communication system in history , is closely associated with cognition . </S>",
    "<S> written text is one of the fundamental manifestations of language , and the study of its universal regularities can give clues about how our brains process information and how we , as a society , organize and share it . </S>",
    "<S> still , only classical patterns such as zipf s law have been explored in depth . </S>",
    "<S> in contrast , other basic properties like the existence of bursts of rare words in specific documents , the topical organization of collections , or the sublinear growth of vocabulary size with the length of a document , have only been studied one by one and mainly applying heuristic methodologies rather than basic principles and general mechanisms . as a consequence , </S>",
    "<S> there is a lack of understanding of linguistic processes as complex emergent phenomena . </S>",
    "<S> beyond zipf s law for word frequencies , here we focus on heaps law , burstiness , and the topicality of document collections , which encode correlations within and across documents absent in random null models . </S>",
    "<S> we introduce and validate a generative model that explains the simultaneous emergence of all these patterns from simple rules . as a result </S>",
    "<S> , we find a connection between the bursty nature of rare words and the topical organization of texts and identify dynamic word ranking and memory across documents as key mechanisms explaining the non trivial organization of written text . </S>",
    "<S> our research can have broad implications and practical applications in computer science , cognitive science , and linguistics . </S>"
  ]
}