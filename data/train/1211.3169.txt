{
  "article_text": [
    "this review deals with the analysis of influences that one system , be it physical , economical , biological or social , for example , can exert over another .",
    "in several scientific fields , the finding of the influence network between different systems is crucial . as examples",
    ", we can think of gene influence networks @xcite , relations between economical variables @xcite , communication between neurons or the flow of information between different brain regions @xcite , or the human influence on the earth climate @xcite , and many others .",
    "the context studied in this report is illustrated in figure [ network : fig ] .",
    "for a given system , we have at disposal a number of different measurements . in neuroscience",
    ", these can be local field potentials recorded in the brain of an animal ; in solar physics , these can be solar indices measured by sensors onboard some satellite ; in the study of turbulent fluids , these can be the velocity measured at different scales in the fluid ( or can be as in the figure , the wavelet analysis of the velocity at different scales ) . for these different examples ,",
    "the aim is to find dependencies between the different measurements , and if possible , to give a direction to the dependence . in neuroscience",
    ", this will allow to understand how information flows between different areas of the brain ; in solar physics , this will allow to understand the links between indices and their influence on the total solar irradiance received on earth ; in the study of turbulence , this can confirm the directional cascade of energy from large down to small scales .    in a graphical modeling approach",
    ", each signal is associated to a particular node of a graph , and dependence are represented by edges , directed if a directional dependence exists .",
    "the questions addressed in this paper concern the assessment of directional dependence between signals , and thus concern the inference problem of estimating the edge set in the graph of signals considered .",
    "climatology and neuroscience were already given as examples by norbert wiener in 1956 @xcite , a paper which inspired econometrist clive granger to develop what is now termed granger causality @xcite .",
    "wiener proposed in this paper that a signal @xmath0 causes another time series @xmath1 , if the past of @xmath0 has a strictly positive influence on the quality of prediction of @xmath1 .",
    "let us quote wiener @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  as an application of this , let us consider the case where @xmath2 represents the temperature at 9 a.m. in boston and @xmath3 represents the temperature at the same time in albany .",
    "we generally suppose that weather moves from west to east with the rotation of the earth ; the two quantities @xmath4 and its correlate in the other direction will enable us to make a precise statement containing some if this content and then verify whether this statement is true or not . or again , in the study of brain waves we may be able to obtain electroencephalograms more or less corresponding to electrical activity in different part of the brain . here",
    "the study of coefficients of causality running both ways and of their analogues for sets of more than two functions @xmath5 may be useful in determining what part of the brain is driving what other part of the brain _ in its normal activity_. \" _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in a wide sense , granger causality can be summed up as a theoretical framework based on conditional independence to assess directional dependencies between time series . it is interesting to note that norbert wiener influenced granger causality , as well as another field dedicated to the analysis of dependencies : information theory .",
    "information theory has led to the definition of quantities that measure the uncertainty on variables using probabilistic concepts .",
    "furthermore , this has led to the definition of measures of dependence based on the decrease in uncertainty relating to one variable after observing another one .",
    "usual information theory is , however , symmetrical .",
    "for example , the well - known mutual information rate between two stationary time series is symmetrical under an exchange of the two signals : the mutual information assesses the undirectional dependence .",
    "directional dependence analysis viewed as an information - theoretic problem requires the breaking of the usual symmetry of information theory .",
    "this was realized in the 1960 s and early 1970 s by hans marko , a german professor of communication .",
    "he developed the bidirectional information theory in the markov case @xcite .",
    "this theory was later generalized by james massey and gerhard kramer , to what we may now call directed information theory @xcite .",
    "_ it is the aim of this report to review the conceptual and theoretical links between granger causality and directed information theory . _",
    "many information - theoretic tools have been designed for the practical implementation of granger causality ideas .",
    "we will not show all of the different measures proposed , because they are almost always particular cases of the measures issued from directed information theory .",
    "furthermore , some measures might have been proposed in different fields ( and/or at different periods of time ) and have received different names .",
    "we will only consider the well - accepted names .",
    "this is the case , for example , of ` transfer entropy ' , as coined by schreiber in 2000 @xcite , but which appeared earlier under different names , in different fields , and might be considered under slightly different hypotheses .",
    "prior to developing a unified view of the links between granger causality and information theory , we will provide a survey of the literature , concentrating on studies where information theory and granger causality are jointly presented .",
    "furthermore , we will not review any practical aspects , nor any detailed applications . in this spirit , this report is different from @xcite , which concentrated on the estimation of information quantities , and where the review is restricted to transfer entropy . for reviews on the analysis of dependencies between systems and for applications of granger causality in neuroscience",
    ", we refer to @xcite .",
    "we will mention however some important practical points in our conclusions , where we will also discuss some current and future directions of research in the field .",
    "we will not debate the meaning of causality or causation .",
    "we instead refer to @xcite .",
    "however , we must emphasize that granger causality actually measures a statistical dependence between the past of a process and the present of another . in this respect ,",
    "the word causality in granger causality takes on the usual meaning that a cause occurs _",
    "prior _ its effect .",
    "however , nothing in the definitions that we will recall precludes that signal @xmath0 can simultaneously be granger caused by @xmath1 and be a cause of @xmath1 ! this lies in the very close connection between granger causality and the feedback between times series .",
    "granger causality is based on the usual concept of conditioning in probability theory , whereas approaches developed for example in @xcite relied on causal calculus and the concept of intervention . in this spirit , intervention is closer to experimental sciences , where we imagine that we can really , for example , freeze some system and measure the influence of this action on another process .",
    "it is now well - known that causality in the sense of between random variables can be inferred unambiguously only in restricted cases , such as directed acyclic graph models @xcite . in the granger causality context , there is no such ambiguity and restriction .      in his nobel prize lecture in 2003 ,",
    "clive w. granger mentioned that in 1959 , denis gabor pointed out the work of wiener to him , as a hint to solve some of the difficulties he met in his work .",
    "norbert wiener s paper is about the theory of prediction @xcite . at the end of his paper , wiener proposed that prediction theory could be used to define causality between time series .",
    "granger further developed this idea , and came up with a definition of causality and testing procedures @xcite .    in these studies",
    ", the essential stones were laid .",
    "granger s causality states that a cause must occur before the effect , and that causality is relative to the knowledge that is available .",
    "this last statement deserves some comment .",
    "when testing for causality of one variable on another , it is assumed that the cause has information about the effect that is unique to it ; _ i.e. _ this information is unknown to any other variable . obviously , this can not be verified for variables that are not known .",
    "therefore , the conclusion drawn in a causal testing procedure is relative to the set of measurements that are available .",
    "a conclusion reached based on a set of measurements can be altered if new measurements are taken into account .",
    "mention of information theory is also present in the studies of granger . in the restricted case of two gaussian signals ,",
    "granger already noted the link between what he called the ` causality indices ' and the mutual information ( eq .",
    "5.4 in @xcite ) .",
    "furthermore , he already foresaw the generalization to the multivariate case , as he wrote in the same paper :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` in the case of @xmath6 variables , similar equations exist if coherence is replaced by partial coherence , and a new concept of partial information is introduced . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    granger s paper in 1969 does not contain much new information , but rather , it gives a refined presentation of the concepts .    during the 1970 s , some studies , _",
    "@xcite , appeared that generalized along some of the directions granger s work , and related some of the applications to economics . in the early 1980 s",
    ", several studies were published that established the now accepted definitions of granger causality @xcite .",
    "these are natural extensions of the ideas built upon prediction , and they rely on conditional independence . finally , the recent studies of dalhaus and eichler allowed the definitions of granger causality graphs @xcite .",
    "these studies provide a counterpart of graphical models of multivariate random variables to multivariable stochastic processes .    in two studies published in 1982 and 1984 @xcite , geweke , another econometrician , set up a full treatment of granger causality testing for the gaussian case , which included the idea of feedback and instantaneous coupling . in @xcite ,",
    "the study was restricted to the link between two time series ( possibly multidimensional ) . in this study ,",
    "geweke defined an index of causality from @xmath0 to @xmath1 ; it is the logarithm of the _ ratio _ of the asymptotic mean square error when predicting @xmath1 from its past only , to the asymptotic mean square error when predicting @xmath1 from its past and from the past of @xmath0 .",
    "geweke also defined the same kind of index for instantaneous coupling , and showed , remarkably , that the mutual information rate between @xmath0 and @xmath1 decomposes as the sum of the indices of causality from @xmath0 to @xmath1 and from @xmath1 to @xmath0 with the index of instantaneous coupling .",
    "this decomposition was shown in the gaussian case , and it remains valid in any case when the indices of causality are replaced by transfer entropy rates , and the instantaneous coupling index is replaced by an instantaneous information exchange rate .",
    "this link between granger causality and directed information theory was further supported by @xcite ( without mention of instantaneous coupling in @xcite ) , and the generalization to the nongaussian case by @xcite ( see also @xcite for related results ) .",
    "however , _",
    "prior _ to these recent studies , the generalization of geweke s idea to some general setting was reported in 1987 , in econometry by gouriroux _",
    "@xcite , and in engineering by rissannen&wax @xcite .",
    "gouriroux and his co - workers considered a joint markovian representation of the signals , and worked in a decision - theoretic framework .",
    "they defined a sequence of nested hypotheses , whether causality was true or not , instantaneous coupling was present or not .",
    "they then worked out the decision statistics using the kullback approach to decision theory @xcite , in which discrepancies between hypotheses are measured according to the kullback divergence between the probability measures under the hypotheses involved . in this setting , the decomposition obtained by geweke in the gaussian case was evidently generalised . in @xcite ,",
    "the approach taken was closer to geweke s study , and it relied on system identification , in which the complexity of the model was taken into account .",
    "the probability measures were parameterized , and an information measure that jointly assessed the estimation procedure and the complexity of the model was used when predicting a signal .",
    "this allowed geweke s result to be extended to nonlinear modeling ( and hence the nongaussian case ) , and provided an information - theoretic interpretation of the tests .",
    "once again , the same kind of decomposition of dependence was obtained by these authors .",
    "we will see in section [ infotheory : sec ] that the decomposition holds due to kramers causal conditioning .",
    "these studies were limited to the bivariate case @xcite .    in the late 1990 s , some studies began to develop in the physics community on influences between dynamical systems . a first route was taken that followed the ideas of dynamic system studies for the prediction of chaotic systems . to determine if one signal influenced another , the idea was to consider each of the signals as measured states of two different dynamic systems , and then to study the master - slave relationships between these two systems ( for examples , see @xcite ) .",
    "the dynamics of the systems was built using phase space reconstruction @xcite .",
    "the influence of one system on another was then defined by making a prediction of the dynamics in the reconstructed phase space of one of the processes . to our knowledge , the setting was restricted to the bivariate case .",
    "a second route , which was also restricted to the bivariate case , was taken and relied on information - theoretic tools .",
    "the main contributions were from palu and schreiber @xcite , with further developments appearing some years later @xcite . in these studies ,",
    "the influence of one process on the other was measured by the discrepancy between the probability measures under the hypotheses of influence or no influence . naturally , the measures defined very much resembled the measures proposed by gouriroux _ et",
    "@xcite , and used the concept of conditional mutual information . the measure to assess whether one signal influences the other was termed _",
    "transfer entropy _ by schreiber .",
    "its definition was proposed under a markovian assumption , as was exactly done in @xcite .",
    "the presentation by palu @xcite was more direct and was not based on a decision - theoretic idea .",
    "the measure defined is , however , equivalent to the transfer entropy .",
    "interestingly , palu noted in this 2001 paper the closeness of the approach to granger causality , as per the quotation :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` the [ latter ] measure can also be understood as an information theoretic formulation of the granger causality concept . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    note that most of these studies considered bivariate analysis , with the notable exception of @xcite , in which the presence of side information ( other measured time series ) was explicitely considered .    in parallel with these studies ,",
    "many others were dedicated to the implementation of granger causality testing in fields as diverse as climatology ( with applications to the controversial questions of global warming ) and neuroscience ; see @xcite , to cite but a few .    in a very different field ,",
    "information theory , the problem of feedback has lead to many questions since the 1950 s .",
    "we will not review or cite anything on the problem created by feedback in information theory as this is not within the scope of the present study , but some information can be found in @xcite .",
    "instead , we will concentrate on studies that are directly related to the subject of this review .",
    "a major breakthrough was achieved by james massey in 1990 in a short conference paper @xcite . following the ( lost ? ) ideas of marko on bidirectional information theory that were developed in the markovian case @xcite , massey re - examined the usual definition of what is called a discrete memoryless channel in information theory , and he showed that the usual definition based on some probabilistic assumptions prohibited the use of feedback .",
    "he then clarified the definition of memory and feedback in a communication channel .",
    "as a consequence , he showed that in a general channel used with feedback , the usual definition of capacity that relies on mutual information was not adequate .",
    "instead , the right measure was shown to be _ directed information _ , an asymmetrical measure of the flow of information .",
    "these ideas were further examined by kramer , who introduced the concept of causal conditioning , and who developed the first applications of directed information theory to communication in networks @xcite .",
    "after some years , the importance of causal conditioning for the analysis of communication in systems with feedback was realized .",
    "many studies were then dedicated to the analysis of the capacity of channels with feedback and the dual problem of rate - distortion theory @xcite . due to the rapid development in the study of networks ( _ e.g. , _ social networks , neural networks ) and of the afferent connectivity problem ,",
    "more recently many authors made connections between information theory and granger causality @xcite .",
    "some of these studies were restricted to the gaussian case , and to the bivariate case .",
    "most of these studies did not tackle the problem of instantaneous coupling .",
    "furthermore , several authors realized the importance of directed information theory to assess the circulation of information in networks @xcite .",
    "tools from directed information theory appear as natural measures to assess granger causality . although granger causality can be considered as a powerful theoretical framework to study influences between signals mathematically , directed information theory provides the measures to test theoretical assertions practically . as already mentioned , these measures are transfer entropy ( and its conditional versions ) , which assesses the dynamical part of granger causality , and instantaneous information exchange ( and its conditional versions ) , which assesses instantaneous coupling .",
    "this review is structured here as follows .",
    "we will first give an overview of the definitions of granger causality .",
    "these are presented in a multivariate setting .",
    "we go gradually from weak definitions based on prediction , to strong definitions based on conditional independence .",
    "the problem of instantaneous coupling is then discussed , and we show that there are two possible definitions for it . causality graphs ( after eichler @xcite ) provide particular reasons to prefer one of these definitions .",
    "section [ infotheory : sec ] introduces an analysis granger causality from an information - theoretic perspective .",
    "we insist on the concept of causal conditioning , which is at the root of the relationship studied .",
    "section [ links : sec ] then highlights the links . here",
    ", we first restate the definitions of granger causality using concepts from directed information theory . then from of a different point of view , we show how conceptual inference approaches lead to the measures defined in directed information theory .",
    "the review then closes with a discussion of some of the aspects that we do not present here intentionally , and on some lines for further research .",
    "all of the random variables , vectors and signals considered here are defined in a common probability space @xmath7 .",
    "they take values either in @xmath8 or @xmath9 , @xmath10 being some strictly positive integer , or they can even take discrete values . as we concentrate on conceptual aspects rather than technical aspects , we assume that the variables considered are well behaved. in particular , we assume finiteness of moments of sufficient order .",
    "we assume that continuously valued variables have a measure that is absolutely continuous with respect to the lebesgue measure of the space considered .",
    "hence , the existence of probability density functions is assumed .",
    "limits are supposed to exist when needed .",
    "all of the processes considered in this report are assumed to be stationary .",
    "we work with discrete time .",
    "a signal will generically be denoted as @xmath11 .",
    "this notation stands also for the value of the signal at time @xmath12 . the collection of successive samples of the signal",
    ", @xmath13 will be denoted as @xmath14 . often , an initial time will be assumed .",
    "this can be 0 , 1 , or @xmath15 . in any case",
    ", if we collect all of the sample of the signals from the initial time up to time @xmath16 , we will suppress the lower index and write this collection as @xmath17 .",
    "when dealing with multivariate signals , we use a graph - theoretic notation .",
    "this will simplify some connections with graphical modeling .",
    "let @xmath18 be an index set of finite cardinality @xmath19 .",
    "@xmath20 is a @xmath10-dimensional discrete time stationary multivariate process for the probability space considered . for @xmath21",
    ", @xmath22 is the corresponding component of @xmath23 .",
    "likewise , for any subset @xmath24 , @xmath25 is the corresponding multivariate process @xmath26 .",
    "we say that subsets @xmath27 form a partition of @xmath18 if they are disjoint and if @xmath28 . the information obtained by observing @xmath25 up to time @xmath12",
    "is resumed by the filtration generated by @xmath29 .",
    "this is denoted as @xmath30 .",
    "furthermore , we will often identify @xmath25 with @xmath31 in the discussion .",
    "the probability density functions ( p.d.f . ) or probability mass functions ( p.m.f ) will be denoted by the same notation as @xmath32 .",
    "the conditional p.d.f . and p.m.f .",
    "are written as @xmath33 .",
    "the expected value is denoted as @xmath34,e_x[.]$ ] or @xmath35 $ ] if we want to specify which variable is averaged , or under which probability measure the expected value is evaluated .",
    "independence between random variables and vectors @xmath0 and @xmath1 will be denoted as @xmath36 , while conditional independence given @xmath37 will be written as @xmath38 .",
    "the early definitions followed the ideas of wiener : a signal @xmath0 causes a signal @xmath1 if the past of @xmath0 helps in the prediction of @xmath1 .",
    "implementing this idea requires the performing of the prediction and the quantification of its quality .",
    "this leads to a weak , but operational , form of the definitions of granger causality .",
    "the idea of improving a prediction is generalized by encoding it into conditional dependence or independence .",
    "consider a cost function @xmath39 ( @xmath12 is some appropriate dimension ) , and the associated risk @xmath40 $ ] , where @xmath41 stands for an error term .",
    "let a predictor of @xmath42 be defined formally as @xmath43 , where @xmath31 and @xmath44 are subsets of @xmath18 , and @xmath5 is a function between appropriate spaces , chosen to minimize the risk with @xmath45 .",
    "solvability may be granted if @xmath5 is restricted to an element of a given class of functions , such as the set of linear functions .",
    "let @xmath46 be such a function class .",
    "define : @xmath47 \\label{risk : eq}\\end{aligned}\\ ] ] @xmath48 is therefore the optimal risk when making a one - step - ahead prediction of the multivariate signal @xmath49 from the past samples of the multivariate signal @xmath25 .",
    "we are now ready to measure the influence of the past of a process on the prediction of another .",
    "to be relatively general and to prepare comments on the structure of the graph , this can be done for subsets of @xmath18 .",
    "we thus choose @xmath31 and @xmath44 to be two disjoint subsets of @xmath18 , and we define @xmath50 ( we use @xmath51 to mean substraction of a set ) .",
    "we study causality from @xmath25 to @xmath49 by measuring the decrease in the quality of the prediction of @xmath42 when excluding the past of @xmath25 .",
    "let @xmath52 be the optimal risk obtained for the prediction of @xmath49 from the past of all of the signals grouped in @xmath23 .",
    "this risk is compared to @xmath53 , where the past of @xmath25 is omitted .",
    "then , for the usual costs functions , we have necessarily : @xmath54 a natural first definition for granger causality is :    @xmath25 granger does not cause @xmath49 relative to @xmath18 if and only if @xmath55    this definition of granger causality depends on the cost @xmath56 chosen as well as on the class @xmath46 of the functions considered . usually , a quadratic cost function is chosen , for its simplicity and for its evident physical interpretation ( a measure of the power of the error ) .",
    "the choice of the class of functions @xmath46 is crucial .",
    "the result of the causality test in definition 1 can change when the class is changed .",
    "consider the very simple example of @xmath57 , where @xmath58 and @xmath59 are gaussian independent and identically distributed ( i.i.d . )",
    "sequences that are independent of each other .",
    "the covariance between @xmath60 and @xmath58 is zero , and using the quadratic loss and the class of linear functions , we conclude that @xmath1 does not granger cause @xmath0 , because using a linear function of @xmath61 to predict @xmath0 would lead to the same minimal risk as using a linear function of @xmath62 only .",
    "however , @xmath58 obviously causes @xmath62 , but in a nonlinear setting .",
    "the definition is given using the negative of the proposition . if by using the positive way ,",
    "@xmath63 , granger proposes to say that @xmath25 is a _ prima facie _ cause of @xmath49 relative to @xmath18 , _ prima facie _ can be translated as at a first glance. this is used to insist that if @xmath18 is enlarged by including other measurements , then the conclusion might be changed .",
    "this can be seen as redundant with the mention of the relativity to the observation set @xmath18 , and we therefore do not use this terminology . however , a mention of the relativity to @xmath18 must be used , as modification of this set can alter the conclusion .",
    "a very simple example of this situation is the chain @xmath64 , where , for example , @xmath62 is an i.i.d .",
    "sequence , @xmath65 , @xmath66 , @xmath67 being independent i.i.d . sequences .",
    "relative to @xmath68 , @xmath0 causes @xmath37 if we use the quadratic loss and linear functions of the past samples of @xmath0 ( note here that the predictor @xmath69 must be a function of not only @xmath62 , but also of @xmath70 ) .",
    "however , if we include the past samples of @xmath1 and @xmath71 , then the quality of the prediction of @xmath37 does not deteriorate if we do not use past samples of @xmath0 .",
    "therefore , @xmath0 does not cause @xmath37 relative to @xmath71 .",
    "the advantage of the prediction - based definition is that is leads to operational tests .",
    "if the quadratic loss is chosen , working in a parameterized class of functions , such as linear filters or volterra filters , or even working in reproducing kernel hilbert spaces , allows the implementation of the definition @xcite . in such cases ,",
    "the test needed can be evaluated efficiently from the data . from a theoretical point of view",
    ", the quadratic loss can be used to find the optimal function in a much wider class of functions : the measurable functions . in this class , the optimal function for the quadratic loss is widely known to be the conditional expectation @xcite . when predicting @xmath49 from the whole observation set @xmath18 , the optimal predictor is written as @xmath72 $ ] .",
    "likewise , elimination of @xmath31 from @xmath18 to study its influence on @xmath44 leads to the predictor @xmath73 $ ] , where @xmath74 .",
    "these estimators are of little use , because they are too difficult , or even impossible , to compute .",
    "however , they highlight the important of conditional distributions @xmath75 and @xmath76 in the problem of testing whether @xmath25 granger causes @xmath49 relative to @xmath18 or not .",
    "the optimal predictors studied above are equal if the conditional probability distributions @xmath75 and @xmath76 are equal .",
    "these distributions are identical if and only if @xmath77 and @xmath78 are independent conditionally to @xmath79 .",
    "a natural extension of definition 1 relies on the use of conditional independence .",
    "once again , let @xmath80 be a partition of @xmath18 .",
    "@xmath25 does not granger cause @xmath49 relative to @xmath18 if and only if @xmath81    this definition means that conditionally to the past of @xmath82 , the past of @xmath25 does not bring more information about @xmath77 than is contained in the past of @xmath49 .",
    "definition 2 is far more general than definition 1 .",
    "if @xmath25 does not granger cause @xmath49 relatively to @xmath18 in the sense of definition 1 , it also does not in the sense of definition 2 .",
    "then , definition 2 does not rely on any function class and on any cost function .",
    "however , it lacks an inherent operational character : the tools to evaluate conditional independence remain to be defined .",
    "the assessment of conditional independence can be achieved using measures of conditional independence , and some of these measures will be the cornerstone to link directed information theory and granger causality .    note also that the concept of causality in this definition is again a relative concept , and that adding or deleting data from the observation set @xmath18 might modify the conclusions .",
    "the definitions given so far concern the influence of the past of one process on the present of another one .",
    "this is one reason that justifies the use of the term causality , when the definitions are actually based on statistical dependence . for an extensive discussion on the differences between causality and statistical dependence",
    ", we refer to @xcite .",
    "there is another influence between the processes that is not taken into account by definitions 1 and 2 .",
    "this influence is referred to as instantaneous causality @xcite .",
    "however , we will use our preferred term of instantaneous coupling , specifically to insist that it is not equivalent to a causal link _ per se _ , but actually a statistical dependence relationship . the term contemporaneous conditional independence that is used in @xcite could also be chosen .",
    "instantaneous coupling measures the common information between @xmath83 and @xmath77 that is not shared with their past . a definition of instantaneous coupling",
    "might then be that @xmath83 and @xmath77 are not instantaneously coupled if @xmath84 .",
    "this definition makes perfect sense if the observation set is reduced to @xmath31 and @xmath44 , a situation we refer to as the bivariate case .",
    "however , in general , there is also side information @xmath85 , and the definition must include this knowledge .",
    "however , this presence of side information then leads to two possible definitions of instantaneous coupling .",
    "@xmath25 and @xmath49 are not conditionally instantaneously coupled relative to @xmath18 if and only if @xmath86 , where @xmath87 is a partition of @xmath18 .",
    "the second possibility is the following :    @xmath25 and @xmath49 are not instantaneously coupled relative to @xmath18 if and only if @xmath88    note that definitions 3 and 4 are symmetrical in @xmath31 and @xmath44 ( the application of bayes theorem ) .",
    "the difference between definitions 3 and 4 resides in the conditioning on @xmath89 instead of @xmath90 .",
    "if the side information up to time @xmath16 is considered only as in definition 4 , the instantaneous dependence or independence is not conditional on the presence of the remaining nodes in @xmath85 .",
    "thus , this coupling is a bivariate instantaneous coupling : it does measure instantaneous dependence ( or independence between @xmath31 and @xmath44 ) without considering the possible instantaneous coupling between either @xmath31 and @xmath85 or @xmath91 and @xmath85 .",
    "thus , instantaneous coupling found with definition 4 between @xmath31 and @xmath44 does not preclude the possibility that the coupling is actually due to couplings between @xmath31 and @xmath85 and/or @xmath44 and @xmath85 .",
    "inclusion of all of the information up to time @xmath92 in the conditioning variables allows the dependence or independence to be tested between @xmath83 and @xmath77 _ conditionally _ to @xmath93 .",
    "we end up here with the same differences as those between correlation and partial correlation , or dependence and conditional independence for random variables . in graphical modeling , the usual graphs are based on conditional independence between variables @xcite .",
    "these conditional independence graphs are preferred to independence graphs because of their geometrical properties ( _ e.g. , _ d - separation , @xcite ) , which match the markov properties possibly present in the multivariate distribution they represent . from a physical point of view , conditional independence might be preferable , specifically to eliminate false coupling due to third parties . in this respect ,",
    "conditional independence is not the panacea , as independent variables can be conditionally dependent .",
    "the well - known example is the conditional coupling of independent @xmath0 and @xmath1 by their addition . indeed , even if independent , @xmath0 and @xmath1 are conditionally dependent to @xmath94 .",
    "granger causality graphs were defined and studied in @xcite .",
    "a causality graph is a mixed graph @xmath95 that encodes granger causality relationships between the components of @xmath23 .",
    "the vertex set @xmath18 stores the indexes of the components of @xmath23 .",
    "@xmath96 is a set of directed edges beween vertices .",
    "a directed edge from @xmath97 to @xmath98 is equivalent to `` @xmath22 granger causes @xmath99 relatively to @xmath18 '' .",
    "@xmath100 is a set of undirected edges .",
    "an undirected edge between @xmath22 and @xmath99 is equivalent to `` @xmath22 and @xmath99 are ( conditionally if def.4 adopted ) instantaneously coupled '' .",
    "interestingly , a granger causality graph may have markov properties ( as in usual graphical models ) reflecting a particular ( spatial ) structure of the joint probability distribution of the whole process @xmath101 @xcite .",
    "a taxonomy of markov properties : local , global , block recursive is studied in @xcite , and equivalence between these properties is put forward .",
    "more interestingly , these properties are linked with topological properties of the graph . therefore ,",
    "structural properties of the graphs are equivalent to a particular factorization of the joint probability of the multivariate process",
    ". we will not continue on this subject here , but this must be known since it paves the way to more efficient inference methods for granger graphical modeling of multivariate processes ( see a first step in this direction in @xcite ) .",
    "directed information theory is a recent extension of information theory , even if its roots go back to the 1960 s and 1970 s and the studies of marko @xcite .",
    "the developments began in the late 1990 s , after the _ impetus _ given by james massey in 1990 @xcite .",
    "the basic theory was then extended by gerhard kramer @xcite , and then further developed by many authors @xcite to cite a few .",
    "we provide here a short review of the essentials of directed information theory .",
    "we will , moreover , adopt a presentation close to the spirit of granger causality to highlight the links between granger causality and information theory .",
    "we begin by recalling some basics from information theory .",
    "then , we describe the information - theoretic approach to study directional dependence between stochastic processes , first in the bivariate case , and then , from section [ sideinfo : ssec ] , for networks , _",
    "i.e. , _ the multivariate case .      let @xmath102 $ ] be the entropy of a random vector @xmath78 , the density of which is @xmath103 .",
    "let the conditional entropy be defined as @xmath104 $ ] .",
    "the mutual information @xmath105 between @xmath78 and @xmath106 is defined as @xcite : @xmath107 where @xmath108 $ ] is the kulback - leibler divergence .",
    "@xmath109 is 0 if and only if @xmath110 , and it is positive otherwise .",
    "the mutual information effectively measures independence since it is 0 if and only if @xmath78 and @xmath106 are independent random vectors . as @xmath111",
    ", mutual information can not handle directional dependence .",
    "let @xmath112 be a third time series .",
    "it might be a multivariate process that accounts for side information ( all of the available observations , but @xmath78 and @xmath106 ) . to account for @xmath112 ,",
    "the conditional mutual information is introduced : @xmath113 \\\\          & = & d_{kl}\\big(p(x_a^n , y_b^n , x_c^n ) ||",
    "p(x_a^n | x_c^n ) p(y_b^n| x_c^n ) p(x_c^n ) \\big)\\end{aligned}\\ ] ] @xmath114 is zero if and only if @xmath115 and @xmath116 are independent _ conditionally _ to @xmath112 .",
    "stated differently , conditional mutual information measures the divergence between the actual observations and those which would be observed under the markov assumption @xmath117 .",
    "arrows can be misleading here , as by reversibility of markov chains , the equality above holds also for @xmath118 .",
    "this emphasizes how mutual information can not provide answers to the information flow directivity problem .",
    "the dependence between the components of the stochastic process @xmath23 is encoded in the full generality by the joint probability distributions @xmath119 .",
    "if @xmath18 is partitioned into subsets @xmath27 , studying dependencies between @xmath31 and @xmath44 then requires that @xmath119 is factorized into terms where @xmath25 and @xmath49 appear .",
    "for example , as @xmath120 , we can factorize the probability distribution as @xmath121 , which appears to emphasize a link from @xmath31 to @xmath44 .",
    "two problems appear , however : first , the presence of @xmath85 perturbs the analysis ( more than this , @xmath31 and @xmath85 have a symmetrical role here ) ; secondly , the factorization does not take into account the arrow of time , as the conditioning is considered over the whole observations up to time @xmath16 .",
    "marginalizing @xmath82 out makes it possible to work directly on @xmath122 .",
    "however , this eliminates all of the dependence between @xmath31 and @xmath44 that might exist _ via _",
    "@xmath85 , and therefore this might lead to an incorrect assessment of the dependence . as for granger causality , this means that dependence analysis is relative to the observation set .",
    "restricting the study to @xmath31 and @xmath44 is what we referred to as the bivariate case , and this allows the basic ideas to be studied .",
    "we will therefore present directed information first in the bivariate case , and then turn to the full multivariate case .",
    "the second problem is at the root of the measure of directional dependence between stochastic processes . assuming that @xmath123 and @xmath42 are linked by some physical ( _ e.g. , _ biological , economical ) system , it is natural to postulate that their dependence is constrained by causality : if @xmath124 , then an event occurring at some time in @xmath31 will influence @xmath44 later on",
    "let us come back to the simple factorization above for the bivariate case .",
    "we have @xmath125 , and furthermore : @xmath126 where for @xmath127 , the first term is @xmath128 .",
    "the conditional distribution quantifies a directional dependence from @xmath31 to @xmath44 , but it lacks the causality property mentioned above , as @xmath129 quantifies the influence of the whole observation @xmath78 ( past and future of @xmath130 ) on the present @xmath131 knowing its past @xmath132 .",
    "the causality principle would require the restriction of the _ prior _ time @xmath130 to the past of @xmath31 only .",
    "kramer defined causal conditioning precisely in this sense @xcite . modifying eq .",
    "( [ distcond : eq ] ) accordingly , we end up we the definition of the causal conditional probability distribution : @xmath133 remarkably this provides an alternative factorization of the joint probability . as noted by massey @xcite , @xmath134 can then be factorized as stands for the delayed collections of samples of @xmath49 .",
    "if the time origin is finite , 0 or 1 , the first element of the list @xmath135 should be understood as a wild card @xmath136 which does not influence the conditioning . ] : @xmath137 assuming that @xmath25 is the input of a system that creates @xmath49 , @xmath138 characterizes the feedback in the system : each of the factors controls the probability of the input @xmath25 at time @xmath130 conditionally to its past and to the past values of the output @xmath49 .",
    "likewise , the term @xmath139 characterizes the direct ( or feedforward ) link in the system .",
    "several interesting simple cases occur :    * in the absence of feedback in the link from @xmath31 to @xmath44 , there is the following : @xmath140 or equivalently , in terms of entropies , @xmath141 and as a consequence : @xmath142 * likewise , if there is only a feedback term , then @xmath143 and then : @xmath144 * if the link is memoryless , _",
    "i.e. , _ the output @xmath49 does not depend on the past , then : @xmath145    these results allow the question of whether @xmath25 influences @xmath49 to be addressed .",
    "if it does , then the joint distribution has the factorization of eq .",
    "( [ factorisation : eq ] ) .",
    "however , if @xmath25 does not influence @xmath49 , then @xmath146 , and the factorization of the joint probability distribution simplifies to @xmath147 .",
    "kullback divergence between the probability distributions for each case generalizes the definition of mutual information to the directional mutual information : @xmath148 this quantity measures the loss of information when it is incorrectly assumed that @xmath25 does not influence @xmath49 .",
    "this was called _ directed information _ by massey @xcite .",
    "expanding the kullback divergence allows different forms for the directed information to be obtained : @xmath149 where we define the ` causal conditional entropy ' : @xmath150 \\\\ & = & \\sum_{i=1}^n h\\big ( x_b(i ) \\big| x_b^{i-1 } , x_a^i \\big ) \\end{aligned}\\ ] ] note that causal conditioning might involve more than one process .",
    "this leads to the defining of the causal conditional directed information as : @xmath151    the basic properties of the directed information were studied by massey and kramer @xcite , and some are recalled below . as a kullback divergence ,",
    "the directed information is always positive or zero .",
    "then , simple algebraic manipulation allows the decomposition to be obtained : @xmath152 eq .",
    "( [ decompdi : eq ] ) is fundamental , as it shows how mutual information splits into the sum of a feedforward information flow @xmath153 and a feedback information flow @xmath154 . in the absence of feedback , @xmath155 and @xmath156 .",
    "( [ decompdi : eq ] ) allows the conclusion that the mutual information is always greater than the directed information , as @xmath157 is always positive or zero ( as directed information ) .",
    "it is zero if and only if : @xmath158 or equivalently : @xmath159 this situation corresponds to the absence of feedback in the link @xmath124 , whence the fundamental result that the directed information and the mutual information are equal if the channel is free of feedback .",
    "this result implies that mutual information over - estimates the directed information between two processes in the presence of feedback .",
    "this was thoroughly studied in @xcite , in a communication - theoretic framework .    the decomposition of eq .",
    "( [ decompdi : eq ] ) is surprising , as it shows that the mutual information is not the sum of the directed information flowing in both directions .",
    "instead , the following decomposition holds : @xmath160 where : @xmath161 this demonstrates that @xmath162 is symmetrical , but is in general not equal to the mutual information , except if and only if @xmath163 . as the term in the sum is the mutual information between the present samples of the two processes conditioned on their joint past values , this measure is a measure of instantaneous dependence .",
    "it is indeed symmetrical in @xmath31 and @xmath44 .",
    "the term @xmath164 will thus be named the _ instantaneous information exchange _ between @xmath25 and @xmath49 , and will hereafter be denoted as @xmath165 . like directed information ,",
    "conditional forms of the instantaneous information exchange can be defined , as for example : @xmath166 which quantifies an instantaneous information exchange between @xmath31 and @xmath44 causally conditionally to @xmath85 .",
    "entropy and mutual information in general increase linearly with the length @xmath16 of the recorded time series .",
    "shannon s information rate for stochastic processes compensates for the linear growth by considering @xmath167 ( if the limit exists ) , where @xmath168 denotes any information measure on the sample @xmath17 of length @xmath16 .    for the important class of stationary processes ( see _ e.g. , _",
    "@xcite ) , the entropy rate turns out to be the limit of the conditional entropy : @xmath169 kramer generalized this result for causal conditional entropies @xcite , thus defining the directed information rate for stationary processes as : @xmath170 this result holds also for the instantaneous information exchange rate . note",
    "that the proof of the result relies on the positivity of the entropy for discrete valued stochastic processes .",
    "for continously valued processes , for which the entropy can be negative , the proof is more involved and requires the methods developed in @xcite , and see also @xcite .",
    "as introduced by schreiber in @xcite , _ transfer entropy _ evaluates the deviation of the observed data from a model , assuming the following joint markov property : @xmath171 this leads to the following definition : @xmath172\\end{aligned}\\ ] ] then @xmath173 if and only if eq .",
    "( [ eq : jointmarkov ] ) is satisfied .",
    "although in the original definition , the past of @xmath0 in the conditioning might begin at a different time @xmath174 , for practical reasons @xmath175 is considered .",
    "actually , no _ a priori _ information is available about possible delays , and setting @xmath175 allows the transfer entropy to be compared with the directed information .    by expressing the transfer entropy as a difference of conditional entropies",
    ", we get : @xmath176 for @xmath177 and choosing 1 as the time origin , the identity @xmath178 leads to : @xmath179 for stationary processes , letting @xmath180 and provided the limits exist , for the rates , we obtain : @xmath181 transfer entropy is the part of the directed information that measures the influence of the past of @xmath25 on the present of @xmath49 .",
    "however it does not take into account the possible instantaneous dependence of one time series on another , which is handled by directed information .",
    "moreover , as defined by schreiber in @xcite , only @xmath182 is considered in @xmath183 , instead of its sum over @xmath130 in the directed information .",
    "thus stationarity is implicitly assumed and the transfer entropy has the same meaning as a rate . a sum over delays was considered by palu as a means of reducing errors when estimating the measure @xcite . summing over @xmath16 in eq .",
    "( [ eq : schreibert ] ) , the following decomposition of the directed information is obtained : @xmath184 eq .",
    "( [ dirinfodecomp : eq ] ) establishes that the influence of one process on another can be decomposed into two terms that account for the past and for the instantaneous contributions .",
    "moreover , this explains the presence of the term @xmath185 in the r.h.s .",
    "( [ sums2di : eq ] ) : instantaneous information exchange is counted twice in the l.h.s .",
    "terms @xmath162 , but only once in the mutual information @xmath186 .",
    "this allows eq .",
    "( [ sums2di : eq ] ) to be written in a slightly different form , as : @xmath187 which is very appealing , as it shows how dependence as measured by mutual information decomposes as the sum of the measures of directional dependences and the measure of instantaneous coupling .      the preceding developments aimed at the proposing of definitions of the information flow between @xmath25 and @xmath49 ; however , whenever @xmath31 and @xmath44 are connected to other parts of the network , the flow of information between @xmath31 and @xmath44 might be mediated by other members of the network .",
    "time series observed on nodes other than @xmath31 and @xmath44 are hereafter referred to as side information .",
    "the available side information at time @xmath16 is denoted as @xmath112 , with @xmath27 forming a partition of @xmath18 .",
    "then , depending on the type of conditioning ( usual or causal ) two approaches are possible .",
    "usual conditioning considers directed information from @xmath31 to @xmath44 that is conditioned on the whole observation @xmath112 . however , this leads to the consideration of causal flows from @xmath31 to @xmath44 that possibly include a flow that goes from @xmath31 to @xmath44 _ via _ @xmath85 in the future !",
    "thus , an alternate definition for conditioning is required .",
    "this is given by the definition of eq .",
    "( [ causaldi1:eq ] ) of the causal conditional directed information : @xmath188    does the causal conditional directed information decompose as the sum of a causal conditional transfer entropy and a causal conditional instantaneous information exchange , as it does in the bivariate case ? applying twice the chain rule for conditional mutual information , we obtain : @xmath189 in this equation , @xmath190 is termed the causal conditional transfer entropy. this measures the flow of information from @xmath31 to @xmath44 by taking into account a possible route _ via _ @xmath85 . if the flow of information from @xmath31 to @xmath44 is entirely relayed by @xmath85 , the causal conditional transfer entropy is zero . in this situation ,",
    "the usual transfer entropy is not zero , indicating the existence of a flow from @xmath31 to @xmath44 .",
    "conditioning on @xmath85 allows the examination of whether the route goes through @xmath85 . the term : @xmath191 is the causal conditional information exchange. this measures the conditional instantaneous coupling between @xmath31 and @xmath44 .",
    "the term @xmath192 emphasizes the difference between the bivariate and the multivariate cases .",
    "this extra term measures an instantaneous coupling and is defined by : @xmath193 an alternate decomposition to eq .",
    "( [ decomp : eq ] ) is : @xmath194",
    "which emphasizes that the extra term comes from : @xmath195 this demonstrates that the definition of the conditional transfer entropy requires conditioning on the past of @xmath85 . if not , the extra term appears and accounts for instantaneous information exchanges between @xmath85 and @xmath44 , due to the addition of the term @xmath196 in the conditioning .",
    "this extra term highlights the difference between the two different natures of instantaneous coupling . the first term",
    ", @xmath197 describes the intrinsic coupling in the sense that it does not depend on parties other than @xmath85 and @xmath44 . the second coupling term , @xmath198 is relative to the extrinsic coupling , as it measures the instantaneous coupling at time @xmath130 that is created by variables other than @xmath44 and @xmath85 .",
    "as discussed in section [ instantcoupl : ssec ] , the second definition for instantaneous coupling considers conditioning on the past of the side information _",
    "only_. causally conditioning on @xmath199 does not modify the results of the bivariate case . in particular , we still get the elegant decomposition : @xmath200 and therefore , the decomposition of eq .",
    "( [ sums2diinst : eq ] ) is generalized to : @xmath201 where : @xmath202 is the causally conditioned mutual information .    finally , let us consider that for jointly stationary times series , the causal directed information rate is defined similarly to the bivariate case , as : @xmath203    in this section we have emphasized on kramer s causal conditioning , both for the definition of directed information and for taking into account side information .",
    "we have also shown that schreiber s transfer entropy is that part of the directed information that is dedicated to the strict sense of causal information flow ( not accounting for simultaneous coupling ) .",
    "the next section more explicitely revisits the links between granger causality and directed information theory .",
    "granger causality in its probabilistic form is not operational . in practical situations , for assessing granger causality between time series",
    ", we can not use the definition directly .",
    "we have to define dedicated tools to assess the conditional independence .",
    "we use this inference framework to show the links between information theory and granger causality .",
    "we begin by re - expressing granger causality definitions in terms of some measures that arise from directed information theory .",
    "therefore , in an inference problem , these measures can be used as tools for inference .",
    "however , we show in the following sections that these measures naturally emerge from the more usual statistical inference strategies . in the following , and as above",
    ", we use the same partitioning of @xmath18 into the union of disjoint subsets of @xmath31 , @xmath44 and @xmath85 .",
    "as anticipated in the presentation of directed information , there are profound links between granger causality and directed information measures .",
    "granger causality relies on conditional independence , and it can also be defined using measures of conditional independence .",
    "information - theoretic measures appear as natural candidates .",
    "recall that two random elements are independent if and only if their mutual information is zero .",
    "moreover , two random elements are independent conditionally to a third one if and only if the conditional mutual information is zero",
    ". we can reconsider definitions 2 , 3 and 4 and recast them in term of information - theoretic measures .",
    "definition 2 stated that @xmath25 does not granger cause @xmath49 relative to @xmath18 if and only if @xmath204 .",
    "this can be alternatively rephrased into :    @xmath25 does not granger cause @xmath49 relative to @xmath18 if and only if @xmath205    since @xmath206 is equivalent to @xmath207 .",
    "otherwise stated , the transfer entropy from @xmath31 to @xmath44 causally conditioned on @xmath85 is zero if and only if @xmath31 does not granger cause @xmath44 relative to @xmath18 .",
    "this shows that causal conditional transfer entropy can be used to assess granger causality .",
    "likewise , we can give alternative definitions of instantaneous coupling .",
    "@xmath25 and @xmath49 are not conditionally instantaneously coupled relative to @xmath18 if and only if @xmath208 ,    or if and only if the instantaneous information exchange causally conditioned on @xmath85 is zero .",
    "the second possible definition of instantaneous coupling is equivalent to :    @xmath25 and @xmath49 are not instantaneously coupled relative to @xmath18 if and only if @xmath209 ,    or if and only if the instantaneous information exchange causally conditioned on the past of @xmath85 is zero .",
    "note that in the bivariate case only ( when @xmath85 is not taken into account ) , the directed information @xmath210 summarizes both the granger causality and the coupling , as it decomposes as the sum of the transfer entropy @xmath211 and the instantaneous information exchange @xmath212 .",
    "we consider the practical problem of inferring the graph of dependence between the components of a multivariate process .",
    "let us assume that we have measured a multivariate process @xmath213 for @xmath214 .",
    "we want to study the dependence between each pair of components ( granger causality and instantaneous coupling between any pair of components relative to @xmath18 ) .",
    "we can use the result of the preceding section to evaluate the directed information measures on the data .",
    "when studying the influence from any subset @xmath31 to any subset @xmath44 , if the measures are zero , then there is no causality ( or no coupling ) ; if they are strictly positive , then @xmath31 granger causes @xmath44 relative to @xmath18 ( or @xmath31 and @xmath44 are coupled relative to @xmath18 ) .",
    "this point of view has been adopted in many of the studies that we have already referred to ( _ e.g. _ @xcite ) , and it relies on estimating the measures from the data .",
    "we will not review the estimation problem here .",
    "however , it is interesting to examine more traditional frameworks for testing granger causality , and to examine how directed information theory naturally emerges from these frameworks . to begin with",
    ", we show how the measures defined emerge from a binary hypothesis - testing view of granger causality inference .",
    "we then turn to prediction and model - based approaches .",
    "we will review how geweke s measures of granger causality in the gaussian case are equivalent to directed information measures .",
    "we will then present a more general case adopted by @xcite and based on a model of the data .",
    "in the inference problem , we want to determine whether or not @xmath25 granger causes ( is coupled with ) or not @xmath49 relative to @xmath18 .",
    "this can be formulated as a binary hypothesis testing problem . for inferring dependencies between @xmath31 and @xmath44 relative to @xmath18 , we can state the problem as follows .",
    "assume we observe @xmath215 .",
    "then , we want to test : @xmath25 does not granger cause @xmath49 , against @xmath25 causes @xmath49 ; and @xmath25 and @xmath49 are instantaneously coupled against ` @xmath25 are @xmath49 not instantaneously coupled ' .",
    "we will refer to the first test as the granger causality test , and to the second one , as the instantaneous coupling test .",
    "in the bivariate case , for which the granger causality test indicates : @xmath216 this leads to the testing of different functional forms of the conditional densities of @xmath131 given the past of @xmath25 .",
    "the likelihood of the observation under @xmath217 is the full joint probability @xmath218 .",
    "under @xmath219 we have @xmath220 and the likelihood reduces to @xmath221 . the log likelihood",
    "_ ratio _ for the test is : @xmath222 for example , in the case where the multivariate process is a positive harris recurrent markov chain @xcite , the law of large numbers applies and we have under hypothesis @xmath217 : @xmath223 where @xmath224 is the transfer entropy rate .",
    "thus from a practical point of view , as the amount of data increases , we expect the log likelihood _",
    "ratio _ to be close to the transfer entropy rate ( under @xmath217 ) .",
    "turning the point of view , this can justify the use of an estimated transfer entropy to assess granger causality . under @xmath219",
    ", @xmath225 converges to @xmath226 , which can be termed ` the lautum transfer entropy rate ' that extends the ` lautum directed information ' defined in @xcite .",
    "directed information can be viewed as a measure of the loss of information when assuming @xmath25 does not causally influence @xmath49 when it actually does . likewise , ` lautum directed information ' measures the loss of information when assuming @xmath25 does causally influence @xmath49 , when actually it does not .    for testing instantaneous coupling , we will use the following : @xmath227 where under @xmath219 , there is no coupling .",
    "then , under @xmath217 and some hypothesis on the data , the likelihood ratio converges almost surely to the information exchange rate @xmath228",
    ".    a related encouraging result due to @xcite is the emergence of the directed information in the false - alarm probability error rate . merging the two tests ( [ test1:eq]),([test2:eq ] ) , _",
    "i.e. , _ testing both for causality and coupling , or neither , the test is written as : @xmath229 among the tests with a probability of miss @xmath230 that is lower than some positive value @xmath231 , the best probability of false alarm @xmath232 follows @xmath233 when @xmath183 is large .",
    "for the case studied here , this is the so - called stein lemma @xcite .    in the multivariate case , there is no such result in the literature .",
    "an extension is proposed here .",
    "however , this is restricted to the case of instantaneously _ uncoupled _ time series .",
    "thus , we assume for the end of this subsection that : @xmath234 which means that there is no instantaneous exchange of information between the three subsets that form a partition of @xmath18 . this assumption has held in most of the recent studies that have applied granger causality tests",
    ". it is , however , unrealistic in applications where the dynamics of the processes involved are faster than the sampling period adopted ( see @xcite for a discussion in econometry ) .",
    "consider now the problem of testing granger causality of @xmath31 on @xmath44 relative to @xmath18 .",
    "the binary hypothesis test is given by : @xmath235 the log likelihood _",
    "ratio _ reads as : @xmath236 again , by assuming that the law of large numbers applies , we can conclude that under @xmath217 @xmath237 this means that the causal conditional transfer entropy rate is the limit of the log likelihood _",
    "ratio _ as the amount of data increases .",
    "following definition 1 and focusing on the quadratic risk @xmath238 $ ] , geweke introduced the following indices for the study of gaussian stationary processes @xcite :",
    "@xmath239 geweke demonstrated the efficiency of these indices for testing granger causality and instantaneous coupling ( bivariate and multivariate cases ) .",
    "furthermore , in the bivariate case , geweke showed that : @xmath240 where @xmath241 is the mutual information rate .",
    "this relationship that was already sketched out in @xcite , is nothing but eq .",
    "( [ sums2diinst : eq ] ) .",
    "indeed , in the gaussian case , @xmath242 and @xmath243 stem from the knowledge that the entropy rate of a gaussian stationary process is the logarithm of the asymptotic power of the one - step - ahead prediction @xcite .",
    "likewise , we can show that @xmath244 and @xmath245 holds .",
    "in the multivariate case , conditioning on the past of the side information , _",
    "i.e. _ @xmath199 , in the definition of @xmath246 , a decomposition analagous to eq .",
    "( [ gewedecomp : eq ] ) holds , and is exactly that given by eq .",
    "( [ sums2diinstcond : eq ] ) .      in a more general framework",
    ", we examine how a model - based approach can be used to test for granger causality , and how directed information comes into play .",
    "let us consider a rather general model in which @xmath247 is a multivariate markovian process that statisfies : @xmath248 where @xmath249 is a function belonging to some functional class @xmath46 , and where @xmath250 is a multivariate i.i.d .",
    "sequence , the components of which are not necessarily mutually independent .",
    "function @xmath251 might ( or might not ) dependon @xmath252 , a multidimensional parameter .",
    "this general model includes as a particular case , linear multivariate autoregressive with moving average ( arma ) models , and nonlinear arma models ; @xmath251 can also stand for a function belonging to some reproducing kernel hilbert space , which can be estimated from the data @xcite . using the partition @xmath27 ,",
    "this model can be written equivalently as : @xmath253 where the functions @xmath254 are the corresponding components of @xmath255 .",
    "this relation can be used for inference in a parametric setting : the functional form is assumed to be known and the determination of the function is replaced by the estimation of the parameters @xmath256 .",
    "this can also be used in a nonparametric setting , in which case the function @xmath5 is searched for in an appropriate functional space , such as an rkhs associated to a kernel @xcite .    in any case , for studying the influence of @xmath25 to @xmath49 relative to @xmath18 , two models are required for @xmath49 : one in which @xmath49 explicitly depends on @xmath25 , and the other one in which @xmath49 does not depend on @xmath25 . in the parametric",
    "setting , the two models can be merged into a single model , in such a way that some components of the parameter @xmath257 are , or not , zero , which dependis whether @xmath31 causes @xmath44 or not .",
    "the procedure then consists of testing nullity ( or not ) of these components . in the linear gaussian case ,",
    "this leads to the geweke indices discussed above . in the nonlinear ( nongaussian ) case",
    ", the geweke indices can be used to evaluate the prediction in some classes of nonlinear models ( in the minimum mean square error sense ) . in this latter case , the decomposition of the mutual information , eq .",
    "( [ gewedecomp : eq ] ) , has no reason to remain valid .",
    "another approach base relies on directly modeling the probability measures .",
    "this approach has been used recently to model spiking neurons and to infer granger causality between several neurons working in the class of generalized linear models @xcite .",
    "interestingly , the approach has been used either to estimate the directed information @xcite or to design a likelihood ratio test @xcite .",
    "suppose we wish to test whether @xmath25 granger causes @xmath49 relative to @xmath18 as a binary hypothesis problem , as in section [ binaryhyp : ssec ] .",
    "forgetting the problem of instantaneous coupling , the problem is then to choose between the hypotheses : @xmath258 where the existence of causality is entirely reflected into the parameter @xmath252 . to be more precise ,",
    "@xmath259 should be seen as a restriction of @xmath260 when its components linked to @xmath25 are set to zero . as a simple example using the model approach discussed above , consider the simple linear gaussian model @xmath261 where @xmath262 is an i.i.d .",
    "gaussian sequence , and @xmath263 are multivariate impulse responses of appropriate dimensions .",
    "define @xmath264 and @xmath265 .",
    "testing for granger causality is then equivalent to testing @xmath266 ; furthermore , the likelihood _ ratio _ can be implemented due to the gaussian assumption .",
    "the example developed in @xcite , assumes that the probability that neuron @xmath98 ( @xmath267 ) sends a message at time @xmath268 ( @xmath269 ) to its connected neighbors is given by the conditional probability @xmath270 where @xmath271 is some decision function , the output of which belongs to @xmath272 $ ] , @xmath31 represents the subset of neurons that can send information to @xmath98 , and @xmath273 represents external inputs to @xmath98 . defining this probability for all @xmath274",
    "completely specifies the behavior of the neural network @xmath18 .",
    "the problem is a composite hypothesis testing problem , in which parameters defining the likelihoods have to be estimated .",
    "it is known that tere is no definitive answer to this problem @xcite .",
    "an approach that relies on an estimation of the parameters using maximum likelihood can be used .",
    "letting @xmath275 be the space where parameter @xmath252 is searched for and @xmath276 the subspace where @xmath259 lives , then the generalized loglikelihood _ ratio _ test reads : @xmath277 where @xmath278 denotes the maximum likelihood estimator of @xmath252 under hypothesis @xmath130 .",
    "in the linear gaussian case , we will recover exactly the measures developed by geweke . in a more general case , and as illustrated in section [ binaryhyp : ssec ] , as the the maximum likelihood estimates are efficient , we can conjecture that the generalized log likelihood _ ratio _ will converge to the causal conditional transfer entropy rate if sufficiently relevant conditions are imposed on the models ( _ e.g. , _ markov processes with recurrent properties ) .",
    "this approach was described in @xcite in the bivariate case .",
    "granger causality was developed originally in econometrics , and it is now transdisciplinary , with the literature on the subject is widely dispersed . we have tried here to sum up the profound links that exist between granger causality and directed information theory .",
    "the key ingredients to build these links are conditional independence and the recently introduced causal conditioning .",
    "we have eluded the important question of how to practically use the definitions and measures presented here . some of the measures can be used and implemented easily , especially in the linear gaussian case . in a more general case , different approaches can be taken .",
    "the information - theoretic measures can be estimated , or the prediction can be explicitly carried out and the residuals used to assess causality .",
    "many studies have been carried out over the last 20 years on the problem of estimation of information - theoretic measures .",
    "we refer to @xcite for information on the different ways to estimate information measures .",
    "recent studies into the estimation of entropy and/or information measures are @xcite .",
    "the recent report by @xcite extensively details and applies transfer entropy in neuroscience using @xmath12-nearest neighbors type of estimators .",
    "concerning the applications , important reviews include @xcite , where some of the ideas discussed here are also mentioned , and where practicalities such as the use of surrogate data , for example , are extensively discussed .",
    "applications for neuroscience are discussed in @xcite .",
    "information - theoretic measures of conditional independence based on kullback divergence were chosen here to illustrate the links between granger causality and ( usual ) directed information theory . other type of divergence could have been chosen ; metrics in probability space could also be useful in the assessing of conditional independence . as an illustration , we refer to the study of fukumizu and co - workers @xcite , where conditional independence was evaluated using the hilbert - schmidt norm of an operator between reproducing kernel hilbert spaces .",
    "the operator generalizes the partial covariance between two random vectors given a third one , and is called the conditional covariance operator .",
    "furthermore , the hilbert - schmidt norm of conditional covariance operator can be efficiently estimated from data . a related approach is also detailed in @xcite .",
    "many important directions can be followed .",
    "causality between nonstationary processes has rarely been considered ( see however @xcite for an _ ad - hoc _ approach in neuroscience ) .",
    "a very promising methodology is to adopt a graphical modeling way of thinking .",
    "the result of @xcite on the structural properties of markov - granger causality graphs can be used to identify such graphs from real datasets . a first step in this direction",
    "was proposed by @xcite .",
    "assuming that the network under study is a network of sparsely connected nodes and that some markov properties hold , efficient estimation procedures can be designed , as is the case in usual graphical modeling .",
    "p.o.a . is supported by a marie curie international outgoing fellowship from the european community .",
    "p.  o. amblard and o.  j.  j. michel .",
    "sur diffrentes mesures de dpendance causales entre signaux al atoires ( on different measures of causal dependencies between random signals ) . in _ proc .",
    "gretsi , dijon , france , sept .",
    "_ , 2009 .",
    "m.  kaminski , m.  ding , w.  truccolo , and s.  bressler .",
    "evaluating causal relations in neural systems : granger causality , directed transfer functions and statistical assessment of significance .",
    ", 85:145157 , 2001 .                                                                          c.  j. quinn and t. p. coleman and n. kiyavash and n.  g.hastopoulos .",
    "estimating the directed information to infer causal relationships in ensemble neural spike train recordings , journal of computational neuroscience , 30 : 1744 , 2011"
  ],
  "abstract_text": [
    "<S> this report reviews the conceptual and theoretical links between granger causality and directed information theory . </S>",
    "<S> we begin with a short historical tour of granger causality , concentrating on its closeness to information theory . </S>",
    "<S> the definitions of granger causality based on prediction are recalled , and the importance of the observation set is discussed . </S>",
    "<S> we present the definitions based on conditional independence . the notion of instantaneous coupling </S>",
    "<S> is included in the definitions . </S>",
    "<S> the concept of granger causality graphs is discussed . </S>",
    "<S> we present directed information theory from the perspective of studies of causal influences between stochastic processes . </S>",
    "<S> causal conditioning appears to be the cornerstone for the relation between information theory and granger causality . in the bivariate case , </S>",
    "<S> the fundamental measure is the directed information , which decomposes as the sum of the transfer entropies and a term quantifying instantaneous coupling . </S>",
    "<S> we show the decomposition of the mutual information into the sums of the transfer entropies and the instantaneous coupling measure , a relation known for the linear gaussian case . </S>",
    "<S> we study the multivariate case , showing that the useful decomposition is blurred by instantaneous coupling . </S>",
    "<S> the links are further developed by studying how measures based on directed information theory naturally emerge from granger causality inference frameworks as hypothesis testing .    * </S>",
    "<S> keyword : * granger causality , transfer entropy , information theory , causal conditioning , conditional independence </S>"
  ]
}