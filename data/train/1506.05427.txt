{
  "article_text": [
    "since its birthdate in 1989 , with the publication of carver mead s book @xcite , the field of _ neuromorphic engineering _ aims at embodying computational principles operating in the nervous system into analog vlsi electronic devices . in a way , this endeavour may be seen as one modern instance of an over three centuries - long attempt to map forms of intelligent behavior onto a physical substrate reflecting the best technology of the day @xcite .",
    "the additional twist of neuromorphic engineering is a case for a direct mapping of the dynamics of neurons and synapses onto the physics of corresponding analog circuits .",
    "initial success was mostly in emulating sensory functions ( e.g. visual or auditory perception ) , and important developments are still ongoing in this area @xcite .",
    "however , it soon became clear that the agenda should include serious efforts to emulate , along with such implementations , elements of information processing downstream sensory stages , with the ultimate goal of approaching cognitive functions .",
    "to make progress in this direction , beyond special - purpose solutions for specific functions , it seems important to identify neural circuitry implementing basic , and hopefully generic , dynamic building blocks , to provide reusable computational primitives , possibly subserving many types of information processing ; in fact , this is both a theoretical quest and an item in the agenda of neuromorphic science .",
    "steps in this direction have been taken recently in @xcite , where ` soft winner - take - all ' subnetworks provide reliable generic elements to compose finite - states machine capable of context - dependent computation .",
    "a review of the electronic circuits involved in such implementations is given in @xcite .",
    "in recurrent neural populations , synaptic self - excitation can support attractor dynamics , point attractors in the simplest instance on which our approach is based .",
    "point attractors are stable configurations of the network dynamics ; from any configuration inside the ` basin ' of one attractor state , the dynamics brings the network towards that attractor , where it remains ( possibly up to fluctuations if noise is present ) .",
    "in a system possessing several point attractor states , the dynamic correspondence between each attractor and its basin implements naturally an associative memory , the initial state within the basin being a metaphor of an initial stimulus , eliciting ( even if removed afterwards ) the retrieval of an associated prototypical information ( memory ) . for a given network size and connectivity graph , the set of available attractor states",
    "is determined by the matrix of synaptic efficacies weighing the links of the graph ; _ learning _ memories is implemented through stimulus - specific changes in the synaptic matrix @xcite .    the attractor - basin correspondence implements dimensionality reduction .",
    "besides , the stimulus - selective self - sustained neural activity following the removal of stimulus that elicited it , can act as a carrier of selective information across time intervals of unconstrained duration , only limited - in the absence of other intervening stimuli , by the stability of the attractor state against fluctuations @xcite . in a previous paper @xcite we demonstrated attractor dynamics in a neuromorphic chip , where synaptic efficacies were chosen and fixed so as to support the desired attractor states .    if attractor dynamics is to be considered as an interesting generic element of computation and representation for neuromorphic systems , we must address the question of how it can autonomously emerge from the ongoing stimulus - driven neural dynamics and the ensuing synaptic plasticity ; this we do in the present work .    to date ,",
    "sparse theoretical efforts have been devoted in this direction ( see @xcite ) , and to our knowledge this has been never undertaken in a neuromorphic chip . here , in line with our previous papers @xcite , and consistently with the above principles , we focus on the autonomous formation of attractor states as associative memories of simple visual objects .",
    "our setting is simple , in that our vlsi network learns two relatively simple , and non - overlapped , visual objects .",
    "still , it is complex , in that learning is effected autonomously ( that is , without a supervised mechanism to monitor errors and instruct synaptic changes ) ; synapses change under the local ( in space and time ) guidance of the spiking activities of the neurons they connect , which in turn change their response to stimuli and their average activity because of synaptic modifications .",
    "such a dynamic loop makes the combined dynamics of neurons and synapses during learning quite complex , and controlling it a tricky business ; even more so in a neuromorphic analog chip , with the implied heterogeneities , mismatches and the like .",
    "to gain predictive control on the chips learning dynamics , we first characterize the single - neuron input - output gain function .",
    "then , we use the mean - field theory of recurrent neural networks as a compass to navigate the parameters space of a population of neurons endowed with massive positive feedback and predict attractor states . finally we measure the rates of change ( potentiation or depression ) of the hebbian , stochastic synapses as a function of the pre- and post - synaptic neural activities .",
    "these three characterization measures let us choose the correct settings for a successful learning trajectory .",
    "we then proceed with experiments on the autonomous learning capabilities of the system and finally , we test the attractor property of the developed internal representations of the learnt stimuli , by checking that when presented with a degraded version of such stimuli the network dynamically reconstructs the complete representation .    to our knowledge",
    "this is the first demonstration of a vlsi neuromorphic system implementing online , autonomous learning .",
    "the neuromorphic system ( fig .",
    "1 ) is composed of a silicon retina @xcite and of two identical reconfigurable neural chips . visual stimuli are displayed on a screen ; the silicon retina captures the dynamic contrast of the stimuli and outputs spike sequences .",
    "those spikes are fed into the recurrent learning network implemented on the two neural chips .",
    "the network spiking activity is streamed to a pc for analysis . as shown in fig 1 the recurrent network",
    "consists of 196 _ excitatory _ and 43 _ inhibitory _ neurons physically distributed over the two identical chips .",
    "retina pixels have been grouped into a grid of 14x14 macro - pixels , each one generating convergent output to a single excitatory neuron ( see also s.i .",
    "the recurrent synapses between excitatory neurons are plastic : their efficacy can change depending on the ongoing spiking activity ( see below ) .",
    "` learning ' is the stimulus - specific synaptic change induced by the repeated presentation of stimuli and the ensuing neural activity .",
    "excitatory synapses are binary , in the sense that only two state of efficacy are allowed , a _ potentiated _ and a _ depressed _",
    "learning manifests itself as a sequence of transitions between these two states : as usual we name ` ltd ' , long - term depression the transition from a _ potentiated _ to a _ depressed _ state , and ` ltp ' , long - term potentiation the _ depressed _ @xmath0",
    "_ potentiated _ transition .",
    "recurrent synaptic connections are random and sparse ( see fig .",
    "1 ) . randomness and sparseness , together with average low values of synaptic efficacy , guarantee low correlations among neuronal activity and , simultaneously , ensure a mean homogeneous input to all neurons . under these conditions",
    "we can approximate the on - chip network behavior with mean - field theory equations ( see s.i . ) and use predictive theory - inspired tools ( i.e. the effective transfer function as explained below ) to tune the system parameters .",
    "moreover , the homogeneous connectivity is an ideally unbiased initial condition to test the effect of learning : we expect that synaptic plasticity will cluster the connectivity structure in a stimulus dependent manner .",
    "we provide here a brief descritpion of the neural chips already described in @xcite : the chips are composed each of @xmath1 integrate - and - fire ( if ) neurons and @xmath2 hebbian plastic bistable reconfigurable synapses .",
    "neurons and synapses are designed as mixed signal analog / digital circuits ; the internal dynamics of every element is implemented in continuous - time analog circuitry while communication among neurons relies on digital pulses representing the spikes .",
    "the entire chip works asynchronously and in real - time and does not necessitate any clocks . every neuron and synapse is implemented in silicon with a dedicated circuit , in this way we are able to exploit in parallel all the resources without relying on complex multiplexing schemes .",
    "hence the top - level view of the chip is an ordered matrix of 128x128 synaptic circuits connected to an array of 128 neurons .",
    "the drawback of the simplicity of the analog implementation is the unavoidable presence of mismatch deriving from the fabrication process .",
    "it causes distributions of the parameters among nominal identical circuits : one of the challenges faced in this work is to gain control over a network of mismatched elements.the analog synaptic circuit implements a plasticity model proposed in @xcite , to which we refer the reader for details ( see also s.i . , section 3 ) ; a slightly different version was previously implemented in @xcite . as already mentioned",
    "the synapse is binary , i.e. it has two levels of efficacy @xmath3 and @xmath4  _ potentiated _ and _ depressed _ which are stable in the absence of pre- and post - synaptic neural activity ; ` efficacy ' is the amount of change in the membrane potential of the post - synaptic neuron per pre - synaptic spike .",
    "plasticity is driven by neural spikes consistently with a rate - based hebbian paradigm , and the changes of efficacy in each synapse are stochastic because of the irregularity of neuronal spikes in time .",
    "the synaptic connectivity is fully configurable , up to all - to - all connectivity .",
    "each synapse can be set to be excitatory / inhibitory , and can be configured to connect two neurons on the same chip , or on different chips ( including the retina ) , or to accept synthetic spikes from a pc .",
    "the communication of spikes from the retina to the neural chips , between the neural chips , and to the pc is based on the parallel asynchronous address - event - representation ( aer ) @xcite and it is managed by a custom pci - aer board @xcite , which also implements inter - chips communication .",
    "pixels , outputs spikes to two neural chips configured to host a recurrent network with a population @xmath5 of @xmath6 excitatory neurons and an inhibitory population @xmath7 of @xmath8 neurons ( network s architecture sketched on top ) .",
    "sparse connections , shown with a solid line , are generated randomly : connectivity levels , i.e. the probability of synaptic contact , are @xmath9 for @xmath5 recurrent connection , @xmath10 for @xmath7 to @xmath5 connection , @xmath11 for @xmath5 to @xmath7 and 0.02 from retina to @xmath7 connection .",
    "the retina field of view is divided into @xmath12 macropixels , each projecting onto a single neuron of @xmath5 .",
    "the network s spiking activity is monitored by a pc.,width=453 ]    [ fig : system ]      as remarked in the introduction , the successful autonomous , unsupervised development of associative representations of stimuli in the form of attractors of the network s dynamics can be challenged in many ways by the interplay between neural and synaptic dynamics . because of this , and given the large network parameter space , setting neural and synaptic parameters by trial and error is not a viable route . to gain control over the system we proceed in three steps : 1 )",
    "we tune neuronal parameters by measuring the input - output response function of a single - neuron 2 ) then , to set the synaptic efficacy values , we characterize the response of a recurrent sub - population of neurons connected by non - plastic synapses , 3 ) finally , to choose the settings for the plasticity circuitry , we measure the ltd and ltp probability of the on - chip synapses when subjected to controlled neuronal activity . for the first and third measures we developed ad - hoc experimental procedures ( described respectively in the second section of the s.i . and in the next section ) . to study the response of the recurrent neuronal sub - populations we faced challenges related to the characterization of a system endowed with a massive positive feedback and embedded in a larger network",
    "relying on predictions based on theoretical models and mapping the derived parameters onto the hardware is not viable : on one side it requires lengthy calibration procedures and on the other side , anyhow , it is prone to fail due to unavoidable differences between the models assumptions and analog circuital behavior , which easily became critical in presence of a massive positive feedback . as an alternative",
    ", we used the method introduced and validated in @xcite to identify regions in the parameter space compatible with the desired network behavior : the coexistence of ( two , in our case ) stable , stimulus - selective collective attractor states ; in the si , section 2 , we briefly summarize the procedure , and describe the results for the present system .",
    "the method instantiates the dimensional reduction of the mean - field theory for a multi - population network ( proposed in @xcite ) in a self - consistent procedure on chip , allowing us to estimate the ` effective transfer function ' ( etf ) of a population of neurons of interest ( one selective excitatory population in our case ) . from an engineering point of view",
    "the etf can be seen as the open - loop transfer function of the sub - population of interest , taking into account the feedback provided by the rest of the network . as detailed in the s.i .",
    "the etf allows us to predict , within certain approximations , the mean firing rate of attractor states of a single sub - population for different levels of its recurrent synaptic potentiation .",
    "since learning implies selective changes in the ratio of the potentiated / depressed synapses in the network , the study of etf for the sub - populations which will be affected by the stimuli offers a predictive tool for expected learning histories .",
    "the analysis based on the etf gives an estimate of the expected changes in the average firing rates of the network s attractor states , as the ratio of potentiated / depressed synapses changes as a result of learning . in turn ,",
    "expected rates of synaptic changes evolve depending on changes in the network s populations average activities .",
    "it is therefore relevant to derive an estimate of the ltp and ltd transition probabilities as a function of the pre- and post - synaptic firing rates ( @xmath13 ) ; the procedure is described in materials and methods section .",
    ", on the y axis the post - synaptic neuronal firing rate @xmath14 .",
    "details on the experimental procedure are reported in methods , width=377 ]    [ fig : ltp_ltd ]    the contour plots in fig.2 , panel b confirm that the silicon synapse implements , as desired , hebbian ltp , ( its probability increases as @xmath15 and @xmath14 both increase ) and heterosynaptic ltd ( i.e. , synaptic depression probability increases with increasing @xmath15 , and only occurs for low @xmath14 ) .",
    "the joint information from the analysis of the etf and the ltp / ltd transition probabilities allowed us to approximately predict the ` working point ' of the plastic synapses at successive stages of learning ( the prediction is expected to be more reliable for slow learning , such that the network evolves through quasi - equilibrium states , which are difficult to obtain for a small and heterogeneous network like the one on chip ) .",
    "such knowledge allows to promote ` learning trajectories ' with balanced ltp / ltd changes , which is another important stability factor @xcite .",
    "two visual stimuli were repeatedly presented on a lcd screen , acquired in real time by the silicon retina , and mapped onto the recurrent network distributed on two neural chips ; our goal was to achieve autonomous associative learning leading to the formation of stimulus - selective attractor states as internal representations ( ` memories ' ) of the stimuli .",
    "hence , after learning , for every stimulus we expect a specific network response that should persist even after the removal of the stimulus .    visual stimuli , ( a ` happy ' and a ` sad ' face shown in panel a of fig.3 ) , are orthogonal ( zero overlap ) and their coding level ( fraction of activated macro - pixels , see s.i .",
    ") is fixed at about @xmath16 ; hence each stimulus activates about 5460 retina neurons and , correspondingly , about 65 excitatory neurons .",
    "the implications of this for more realistic situations are discussed in the discussion section .",
    "learning proceeds as a sequence of transitions between the @xmath3 and @xmath4 synaptic states , depending on the activity of pre- and post - synaptic neurons induced by stimuli , as explained below .",
    "notice that no explicit control is imposed on synaptic dynamics depending on the network being stimulated or not , and no distinction is made between ` learning ' and ` retrieval ' phases ; the network just evolves based on the incoming flow of stimuli , and its own feedback : our system s learning is completely autonomous and does not require any supervision .    according to the hebbian learning that synapses implement ( as from fig.2 ) , each stimulus presentation is expected to provoke changes in a fraction of synapses as follows ( we remind that learning is stochastic , to the extent that neural activities are ) : ltp in synapses connecting neurons activated at high rates by the same stimulus ; ltd in synapses connecting neurons activated by different stimuli , or connecting neurons activated by stimuli to neurons never activated by any stimuli ( we named them the ` background ' neurons ) ; statistically little or no changes in synapses connecting pairs of ` background ' neurons .    in the above scenario , because of autonomous learning , excitatory neurons get partitioned in three populations ( two selective to stimuli , and the background ) , which both under stimulation and in the absence of it show an evolving pattern of relatively stable firing rates ; as remarked in the introduction , during this unsupervised evolution the network could well drift to undifferentiated high activity or quiescence states .",
    "balance between ltp and ltd is essential to guarantee a successful learning trajectory @xcite .",
    "ltp should eventually grow high enough to support stable selective attractor states ; however , if this is not counterbalanced by ltd along the way , and especially in the early stages of learning , the learning trajectory can easily lead the network to globally unstable states ; knowledge of the ltp / ltd probabilities of fig.2 is important to obtain robust learning trajectories .",
    "this is where the predictive power of the etf , and knowledge ltp / ltd curves , come into play , hinting at safe paths in the large parameter space .",
    "our main results are described in fig.3 , which illustrates a typical successful ` learning history ' , and in fig.4 , which describes the underlying evolving micro - structure of the synaptic matrix .",
    "panels a , b , c of fig.3 describe respectively the sequence of stimuli , the average firing rates of the two selective populations and a two - dimensional representation of the network s output to match the representation of stimuli .",
    "it is seen that : 1 ) initially , each stimulus provokes a response in its target neural population ( which , through the induced inhibitory activation essentially silence the other populations , up to small noise ) ; this activity is rapidly extinguished when the stimulus is removed , and no noticeable activity is present during the inter - stimulus interval 2 ) during each stimulation , a fraction of synapses changes , according to the ( stochastic ) scheme already explained , and this is reflected in the slow increase in the firing rates under stimulation 3 ) after many repeated stimulations , the build - up of synaptic self - excitation determines the appearance of a high - activity ( meta)stable state for each of the two selective sub - populations ( the equivalent of the etf passing from 1 to 2 stable fixed points in fig 3 in s.i . ) , and after the stimulus is released , the corresponding selective sub - population stays in a self - sustained state of elevated activity : the attractor state propagating the memory of the last stimulus across the inter - stimulus interval ; sometimes this reverberant state is destabilized by the next incoming stimulus , while in other cases it decays spontaneously , due to finite - size fluctuation . correspondingly , in a phase of mature learning , the output the of the network during the inter - stimulus interval well matches the input stimuli .",
    "[ fig : learningpsth ]    a sample history of the synaptic changes underlying the learning history of fig.3 is given in fig.4 , panels a , b. panel a illustrates the time course of the fraction of synapses in the potentiated state , for the different synaptic groups ( identified by colour ) ; synapses are inspected every 2 presentations of the same stimulus ( 4 stimulations in the alternate sequence ) . consistently with expectation : synapses connecting neurons affected by the same stimulus get potentiated , actually approaching saturation , such that the average potentiation level remain essentially stable for @xmath17s ; ltd is visible in the initial stage of learning ; synapses connecting neurons in the background stay essentially unaffected .",
    "panel b shows the hamming distance between the network synaptic matrices sampled as in panel a. we see here that the seemingly steady situation reached in panel a for the selective ` sad to sad ' and ` happy to happy ' synapses actually conceals a dynamic balance ; for this to happen , again , ltp and ltd probabilities should be properly balanced .",
    "the evolution of the synaptic matrix is further illustrated in panels c  e of fig.4 .",
    "the pictures are , from top to bottom , snapshots of the synaptic matrix taken at the beginning of learning , after 30 seconds , ( 2 presentations of each stimulus ) , and after 300 seconds ( 40 presentations ) .",
    "the snapshot in panel c just reflects the random choice of @xmath18 potentiated synapses as the initial condition of the network ; in panel d we see that the synaptic matrix getting the expected structure , for the chosen index labeling : two blocks of mainly potentiated synapses ( ltp ) , corresponding to synapses connecting neurons responsive to the same stimulus ( remember that the stimuli are orthogonal , hence the non - overlapping ` potentiated squares ' ) ; ` whitened ' blocks of depressed synapses ( ltd ) connecting neurons responsive to different stimuli , or those connecting neurons responsive to either stimuli to background neurons ; one strip on the extreme right , of synapses connecting neurons in the background , which stay essentially unaffected .",
    "such features get further sharpened in the mature learning phase of panel e.      in order to check that the developed selective , self - sustaining states of elevated activity are indeed attractors of the network dynamics , 1 ) we observe their persistence after removal of the stimulus and 2 ) we check that the network is able to perform _ error correction _ of a degraded stimulus : in a mature stage of learning , for initial conditions ( stimuli ) close to one of the ` memories ' , the network spontaneously relaxes to the corresponding attractor state .",
    "this we show in panel d of fig 3 , in which the stimulus is degraded by removing @xmath19 of its active pixels .",
    "the network quickly reconstructs the complete , learnt memory , thanks to the selective feedback .",
    "we remark that systematically exploring the error - correction ability , by varying the amount of degradation of the stimuli , would allow to give an estimate of the _ basin of attraction _ of the attractor states .",
    "indeed , this we showed in @xcite ( in which the selective synaptic structure was imposed and not self - generated ) .",
    "we demonstrated a neuromorphic network of spiking neurons and plastic , hebbian , spike - driven synapses which autonomously develops attractor representations of real visual stimuli acquired by a silicon retina .",
    "attractor dynamics results in stimulus - selective , elevated activity after removal of the stimulus , and error correction properties .",
    "detailed inspiration from mean - field theory is used to implement on chip a search strategy in the large parameter space of the network , which works at the level of the network s input - output gain function , without having to delve into the single circuits level .",
    "[ fig : syn ]    within the limitations of the chosen scenario , which we discuss in the following , what we achieved takes a step along a most needed line of development , if neuromorphic systems are ultimately to act as autonomous and adaptive dynamic systems interacting in real time with their environment ( for which , of course , much more than just building associative representations will be needed ) .    in view of further progress",
    ", some of the simplifying choices we made , and some limitations must be discussed and put in perspective .",
    "first , it would be tempting to compare the performance of our neuromorphic network with theoretical predictions about the limit of capacity , i.e. the maximum number of networks configurations that can be embedded as retrievable memories .",
    "hebbian dynamic learning with bounded ( two - states in our case ) synapses entail a capacity scaling as the logarithm of the number of synapses @xcite ( as opposed to the linear scaling of the hopfield network @xcite or its equivalent realization with spiking neurons @xcite ) ; this limit can be partially overcome by making learning stochastic ( as in our case ) or lowering the coding level ( see @xcite for a discussion and further developments ) .",
    "however , a quantitative comparison with the theoretical estimates is affected by several factors ( for the chip and largely for simulations as well ) , e.g. the fact that the probability of synaptic changes is not really constant along the learning history ; that mismatch in the synaptic circuits effectively creates - at any stage of learning - a distribution of probabilities of synaptic changes across the network ( notice that we do not compensate for mismatch modulating the connectivity among neurons as suggested in @xcite ) ; that finite - size effects influence synaptic dynamics through the distribution of firing rates they induce @xcite .",
    "empirically , the memory capacity of our small vlsi network is three patterns , beyond which learning becomes unstable ; of course the small size of the network prevented us from checking how the capacity scales with the number of synapses . from the hardware point of view , achieving higher capacity with larger network hits at a scalability issue .",
    "designing new chips that embed more neurons and synapses would provide a limited option for scaling up ( unless radical changes in the implementation technology are considered - e.g. memristors ) .",
    "the other natural approach would be to combine many neural chips ; this would face challenges related to the bandwidth for inter - chip communication , that would need to ensure reliable real - time aer - like spike delivery .",
    "second , we choose orthogonal stimuli ( also with the same coding level ) to be learnt by the network , which clearly facilitates learning by minimizing the interference induced on synaptic changes by different stimuli .",
    "a natural question then arises , as to the implication of including more naturalistic stimuli with arbitrary overlaps ( and possibly coding levels ) . as for overlaps ,",
    "a computationally effective , and biologically motivated , modification of synaptic dynamics was proposed in @xcite ; for the same stochastic , bistable hebbian synaptic model here implemented , a regulatory mechanism was there introduced , preventing further potentiation or depression of a synapse when the post - synaptic neuron was recently highly active or poorly active , respectively .",
    "such modified ` stop - learning ' synapses were implemented and successfully demonstrated in neuromorphic chips @xcite , and would provide a good option to make the development of attractor representations more robust to the spatial structure of the stimuli .",
    "allowing for different coding levels is instead essentially a matter of size of the network , allowing , even for low coding level , for a good averaging of synaptic mismatches across the dendritic tree of each neuron .",
    "putting our work in the context of previously published work , the attempt to embed learning capabilities in hardware devices is of course not new in general , starting from the supervised ` adaptive pattern classification machine ' described by widrow and hoff @xcite in the 60s .",
    "not exhaustively : one of the first steps towards learning microchips was taken in the 80s by alspector and colleagues @xcite , who demonstrated unsupervised learning in a feed - forward network ( for a broad review of pioneering works see @xcite ) .",
    "more recently , @xcite demonstrates a feedforward architecture to classify patterns of mean firing rates imposed by synthetic stimuli .",
    "a complex neuromorphic chain including visual sensing and processing ( convolution filters ) was described described in @xcite .",
    "massive efforts have been devoted to develop various kinds of plastic synaptic circuits ( using standard cmos @xcite or with new memristor devices @xcite ; for a recent review of technologies employed for the purpose see @xcite ) .",
    "still , very few works demonstrated learning in neuromorphic hardware at the network level . as mentioned above , hebbian stop - learning synapses @xcite have been used in pure feed - forward networks @xcite trained in a semi - supervised way to discriminate among syntethic patterns of mean firing rates , while @xcite demonstrates how spike - timing - dependent - plasticity increases synchronicity in a network with local connectivity among neighboring neurons .    to our knowledge , learning synthetic stimuli in a spiking recurrent network with massive feedback has been dealt with only off - chip in @xcite , reporting a digital implementation .    in the domain of recurrent spiking networks implementing associative memories",
    "this work takes a significant step towards autonomous operation in naturalistic conditions by removing - as we did - the artificial separation between a ` learning phase ' , in which pre - computed synapses are ` downloaded ' to the chip , and a ` retrieval phase ' , in which the associative memory is tested with frozen synapses .",
    "finally , concerning the prospective interest of point attractor networks as ` reuseble building blocks ' of neuromorphic systems ( see introduction ) we would like to remark that , in the face of the rich repertoire of dynamic states exhibited by the brain ( and neural networks ) , attempts are under way to bridge such elementary point attractor dynamics and the complexity of neural dynamics over multiple time scales ( see @xcite and references therein ) , and the domain of application of the attractor concept has gradually evolved to include models of working memory , information integration , decision making , multi - stable perception .",
    "this provides , we believe , a prospective rich context for the implementation reported here .",
    "we choose 64 excitatory neurons , and for each one we set up 128 input synapses , all configured as ` aer synapses ' , of which 64 are configured to be non - plastic ( efficacy set to 0.05 ) , the remaining 64 were plastic , with efficacy set to zero ( in this way they do not affect the neuron s firing , but the synapse can still perform transitions between its two binary states ) . in this way , firing @xmath20 of each neuron is driven by input from the 64 non - plastic aer synapses , receiving on their pre - synaptc terminal synthetic spike trains through the pci - aer board ; the plastic aer synapses are pre - synaptically driven by synthetic spike trains with average rate @xmath21 . to explore the ( @xmath22 ) plane , for each @xmath20 ( i.e. for each average rate of external spikes to the non - plastic synapses ) ,",
    "@xmath21 ( the average rate of external spikes to the plastic synapses ) is varied . for each ( @xmath22 ) pair ,",
    "the initial state of the synapse ( i.e. the value of the internal variable @xmath23 ) is set ( high , to measure ltd , low , to measure ltp ) , and its binary state is checked after 1 sec .    10    mead c. _ analog vlsi implementation of neural systems . _ [ mead , c. & ismail , m. ( eds ) ] ( springer , usa , 1989 ) .",
    "cordeschi r. _ the discovery of the artificial : behavior , mind and machines before and beyond cybernetics . _ ( kluwer academic publisher , the netherlands , 2002 ) .",
    "liu s .- c .",
    ", van schaik a. , minch b.a . , and delbruck , t. asynchronous binaural spatial audition sensor with 2x64x4 channel output .",
    "_ ieee trans . on biomedical circuits and systems",
    "_ * 99,*1 - 12 ( 2013 ) .",
    ", delbruck t. neuromorphic sensory systems .",
    "_ current opinions in neurobiology _ * 20,*1 - 8 , ( martin k. and scott k. ed . ) ( 2010 ) .",
    "neftci e. , binas j. , rutishauser u. , chicca e. , indiveri g. and douglas r.j .",
    "synthesizing cognition in neuromorphic electronic systems . _",
    "* 110,*e3468-e3476 ( 2012 ) .",
    "liu s.c . , delbruck t. , indiveri g. , whatley a. , douglas r. _ event - based neuromorphic systems _ ( wiley , 2015 )    chicca e. , stefanini , f. , bartolozzi , c. , indiveri , g. neuromorphic electronic circuits for building autonomous cognitive systems .",
    "_ proc . of the ieee .",
    "_ * 102,*1 - 22 ( 2014 ) .",
    "_ modeling brain function : the world of attractor neural networks _",
    "( cambridge university press , 1989 ) .",
    "wang x - j .",
    "attractor network models _ encyclopedia of neuroscience .",
    "_ squire l.r .",
    "667 - 679 ( oxford academic press 2008 ) .",
    "mongillo g. , curti e. , romani s. , amit d.j .",
    "learning in realistic networks of spiking neurons and spike - driven plastic synapses .",
    ". j. neuroscience _ * 21,*3143 - 3160 ( 2005 ) .",
    "amit d. j. , brunel n. learning internal representations in an attractor neural network with analogue neurons .",
    "_ network : computation in neural systems _ * 6,*359 - 388 ( 1995 ) .",
    "del giudice p. , fusi s. , mattia m. modelling the formation of working memory with networks of integrate - and - fire neurons connected by plastic synapses .",
    "_ j. physiol .",
    "paris _ * 97,*659 - 681 ( 2003 ) .",
    "del giudice p. , mattia m. long and short - term synaptic plasticity and the formation of working memory : a case study .",
    "_ neurocomputing_. * 38,*1175 - 1180 ( 2001 )",
    ".    mattia m. , del giudice p. finite - size dynamics of inhibitory and excitatory interacting spiking neurons .",
    "matter phys .",
    "_ * 70,*052903 ( 2004 ) .",
    "lichtsteiner p.,posch c. , delbruck t. a 128x128 , 120 db 15 micro s latency asynchronous temporal contrast vision sensor . _",
    "ieee journal of solid state circuits _",
    "* 43,*566 - 576 ( 2008 ) .",
    "mahowald m. _ an analog vlsi system for stereoscopic vision _",
    "( springer 1994 ) .",
    "dante v. , del giudice p. whatley a.m. , pci - aer - hardware and software for interfacing to address - event based neuromorphic systems .",
    "_ the neuromorphic engineer _",
    "* 2,*5 - 6 ( 2005 ) .",
    "m. giulioni et al . a vlsi network of spiking neurons with plastic fully configurable ",
    "stop - learning ",
    "_ international conference on electronics , circuits , and systems , icecs_. * * 678 - 681 ; doi:10.1109/icecs.2008.4674944 ( 2008 ) .",
    "giulioni m. , pannunzi m. , badoni d. , dante v. , del giudice p. classification of correlated patterns with a configurable analog vlsi neural network of spiking neurons and self - regulating plastic synapses .",
    "_ neural comput . _ * 21,*3106 - 3129 ( 2009 ) .",
    "fusi s. , annunziato m. , badoni d. , salamon a. , amit d. j. spike - driven synaptic plasticity : theory , simulation , vlsi implementation .",
    "_ neural comput . _ * 12,*2227 - 2258 ( 2000 ) .",
    "giulioni m. et al .",
    "robust working memory in an asynchronously spiking neural network realized in neuromorphic vlsi . _",
    "frontiers in neuroscience _ , * 5 * ; doi:10.3389/fnins.2011.00149 ( 2012 ) .",
    "mascaro m. , amit d.j .",
    "effective neural response function for collective population states .",
    "_ network : computation in neural systems_. * 10,*351 - 373 ( 1999 ) .",
    "fusi s. hebbian spike - driven synaptic plasticity for learning patterns of mean firing rates .",
    "_ biological cybernetics_. * 87,*459 - 470 ( 2002 ) .",
    "hopfield j. j. neural networks and physical systems with emergent collective computational abilities . _ proc",
    "natl acad sci usa_. * 79,*2554 - 2558 ( 1982 ) .",
    "curti e. , mongillo g. , la camera g. , amit d. j. , mean field and capacity in realistic networks of spiking neurons storing sparsely coded random memories .",
    "_ neural comput . _ * 16,*2597 - 2637 ( 2004 ) .",
    "fusi s. , abbott l. limits on the memory storage capacity of bounded synapses .",
    "_ nature neuroscience_. * 10,*485 - 493 ( 2007 ) .",
    "neftci e. , indiveri g. a device mismatch compensation method for vlsi neural networks .",
    "_ biomedical circuits and systems conference ( biocas ) , ieee_. * * 262 - 265 ; doi:10.1109/biocas.2010.5709621 ( 2010 ) .    brader j. m. , senn w. , fusi s. learning real - world stimuli in a neural network with spike - driven synaptic dynamics .",
    "_ neural comput . _ * 19,*2881 - 2912 ( 2007 ) .",
    "mitra s. , fusi s , indiveri g. real - time classification of complex patterns using spike - based learning in neuromorphic vlsi .",
    "_ ieee trans . on biomedical circuits and systems_. * 3,*32 - 42 ( 2009 ) .",
    "widrow b. , hoff m. e. adaptive switching circuits .",
    "_ neurocomputing : foundations of research_. ( the mit press , cambridge , usa , 1988 ) .",
    "alspector j. , gupta b. , allen r. b. performance of a stochastic learning microchip .",
    "_ advances in neural information processing systems_. touretzky d.s .",
    "( ed . ) * * 748 - 760 ( morgan - kaufmann , 1989 ) .",
    "cauwenberghs g. [ neuromorphic learning vlsi systems : a survey ] _ neuromorphic systems engineering _ [ sejnowski , t.j .",
    "( ed . ) ] [ 381 - 408 ] ( springer , usa , 1998 ) .",
    "hafliger p. adaptive wta with an analog vlsi neuromorphic learning chip .",
    "_ neural networks , ieee trans . on_. * 18,*551 - 572 ( 2007 ) .",
    "r. serrano - gotarredona et al .",
    "caviar : a 45k neuron , 5 m synapse , 12 g connects / s aer hardware sensory processing learning actuating system for high - speed visual object recognition and tracking._ieee trans . on neural",
    "networks_. * 20,*1417 - 1438 ( 2009 ) .",
    "hafliger p. kolle riis h. a multi - level static memory cell .",
    "_ ieee international symposium on circuits and systems , ieee_. * 1,*25 - 28 ( 2003 ) .",
    "gordon c. , hasler p. biological learning modeled in an adaptive floating - gate system .",
    "_ ieee international symposium on circuits and systems , ieee_. * 5,*609 - 612 ( 2002 ) .",
    "liu s. c. , mockel , r. temporally learning floating - gate vlsi synapses . _ ieee international symposium on circuits and systems , ieee_. * * 2154 - 2157 ; doi:10.1109/iscas.2008.4541877 ( 2008 ) .    ramakrishnan s. , hasler p. e. , gordon c. floating gate synapses with spike - time - dependent plasticity .",
    "_ ieee trans . on biomedical circuits and systems_.",
    "* 5,*244 - 252 ( 2011 ) .",
    "bofill - i - petit a. , murray a. f. synchrony detection and amplification by silicon neurons with stdp synapses .",
    "_ ieee trans . on neural networks_. * 15,*1296 - 1304 ( 2004 ) .",
    "schemmel j. , grubl a. , meier k. , mueller e. implementing synaptic plasticity in a vlsi spiking neural network model .",
    "_ international joint conference on neural networks , ieee_. * * 1 - 6 ; doi:10.1109/ijcnn.2006.246651 ( 2006 ) .",
    "bamford s. a. , murray a. f. , willshaw , d. j. spike - timing - dependent plasticity with weight dependence evoked from physical constraints .",
    "_ ieee trans . on biomedical circuits and systems_. * 6,*385 - 398 ( 2012 ) .",
    "rachmuth g. , shouval h. z. , bear m. f. , poon c. s. a biophysically - based neuromorphic model of spike rate - and timing - dependent plasticity .",
    "usa _ * 108,*e1266-e1274 ( 2011 ) .",
    "indiveri g. , chicca e. , douglas r. a vlsi array of low - power spiking neurons and bistable synapses with spike - timing dependent plasticity",
    ". _ ieee trans . on neural networks_. * 17,*211 - 221 ( 006 ) .",
    "indiveri g. , stefanini f. , chicca e. spike - based learning with a generalized integrate and fire silicon neuron .",
    "_ ieee international symposium on circuits and systems ieee _ , * * 1951 - 1954 ; doi:10.1109/iscas.2010.5536980 ( 2010 ) .",
    "s. brink et al .",
    "a learning - enabled neuron array is based upon transistor channel models of biological phenomena .",
    "_ ieee trans . on biomedical circuits and systems_. * 7,*71 - 81 ( 2013 ) .",
    "m. suri et al .",
    "physical aspects of low power synapses based on phase change memory devices .",
    "_ journal of applied physics_. * 112,*054904 ( 2012 ) .",
    "mayr c. et al .",
    "waveform driven plasticity in bifeo3 memristive devices : model and implementation .",
    "_ advances in neural information processing systems_. * * 2323 - 2326 ( 2012 ) .",
    "serrano - gotarredona t. , masquelier t. , prodromakis t. , indiveri g. , linares - barranco b. stdp and stdp variations with memristors for spiking neuromorphic learning systems .",
    "_ frontiers in neuroscience_. * 7 , * doi:10.3389/fnins.2013.00002 ( 2013 ) .",
    "gelencser a. , prodromakis t. , toumazou c. , roska t. biomimetic model of the outer plexiform layer by incorporating memristive devices .",
    "_ phys rev e stat nonlin soft matter phys_. * 85,*041918 ( 2012 ) .",
    "kuzum d. , yu s. , wong h. p.",
    "synaptic electronics : materials , devices and applications .",
    "_ nanotechnology_. * 24,*382001 ( 2013 ) .",
    "arthur j. , boahen , k. learning in silicon : timing is everything .",
    "_ advances in neural information processing systems_. * * 75 - 82 ( the mit press , usa , 2006 ) .",
    "seo j. et al .",
    "a 45 nm cmos neuromorphic chip with a scalable architecture for learning in networks of spiking neurons . _ ieee custom integrated circuits conference , cicc _",
    "( ieee , piscataway , nj ) . * * 1 - 4 ; doi:10.1109/cicc.2011.6055293 ( 2001 ) .",
    "braun j. , mattia m. attractors end noise : twin drivers of decisions and multistability .",
    "_ neuroimage _ * 52,*740 - 751 ( 2010 ) .",
    "this work was partially supported by the eu fet project coronet .",
    "we are indebted to tobi delbruck for having made available to us early prototypes of the silicon retina , and for his assistance .",
    "we gratefully thank maurizio mattia for his precious contribution of ideas and suggestions along the way , and stefano fusi for a critical reading of the manuscript .",
    "m.g . and f.c .",
    "wrote the software , performed the experiments , analysed the results and wrote the paper .",
    "v.d . designed the test system and provided the firmware .",
    "p.d.g . supervised the entire work and contributed to data analysis and paper writing .",
    "all authors reviewed the manuscript .",
    "correspondence and requests for materials should be addressed to massimiliano giulioni  ( email : massimiliano.giulioni@iss.infn.it ) ."
  ],
  "abstract_text": [
    "<S> neuromorphic chips embody computational principles operating in the nervous system , into microelectronic devices . in this domain </S>",
    "<S> it is important to identify computational primitives that theory and experiments suggest as generic and reusable cognitive elements . </S>",
    "<S> one such element is provided by attractor dynamics in recurrent networks . </S>",
    "<S> point attractors are equilibrium states of the dynamics ( up to fluctuations ) , determined by the synaptic structure of the network ; a ` basin ' of attraction comprises all initial states leading to a given attractor upon relaxation , hence making attractor dynamics suitable to implement robust associative memory . </S>",
    "<S> the initial network state is dictated by the stimulus , and relaxation to the attractor state implements the retrieval of the corresponding memorized prototypical pattern . in a previous work we demonstrated that a neuromorphic recurrent network of spiking neurons and suitably chosen , fixed synapses supports attractor dynamics . </S>",
    "<S> here we focus on learning : activating on - chip synaptic plasticity and using a theory - driven strategy for choosing network parameters , we show that autonomous learning , following repeated presentation of simple visual stimuli , shapes a synaptic connectivity supporting stimulus - selective attractors . </S>",
    "<S> associative memory develops on chip as the result of the coupled stimulus - driven neural activity and ensuing synaptic dynamics , with no artificial separation between learning and retrieval phases . </S>"
  ]
}