{
  "article_text": [
    "in many real world problems , optimization decisions have to be made with limited information . whether it is a static optimization or dynamic control problem , obtaining detailed and accurate information about the problem or system",
    "can often be a costly and time consuming process . in some cases",
    ", acquiring extensive information on system characteristics may be simply infeasible . in others",
    ", the observed system may be so nonstationary that by the time the information is obtained , it is already outdated due to system s fast - changing nature .",
    "therefore , the only option left to the decision - maker is to develop a strategy for collecting information efficiently and choose a model to estimate the `` missing portions '' of the problem in order to solve it satisfactorily and according to a given objective .    to make the discussion more concrete ,",
    "consider the problem of maximizing a ( lipschitz ) continuous _ nonconvex _ objective function , which is unknown except from its value at only a small number of data points .",
    "the decision maker may have no a priori information about the function and start with zero data points .",
    "furthermore , only a limited number of possibly noisy observations may be available before making a decision on the maximum value and its location .",
    "the function itself , however , remains unknown even after the decision is made .",
    "_ what is the best strategy to address this problem _ ?",
    "the decision making framework presented in this paper captures the posed problem by taking into account the information collection ( observation ) , estimation ( regression ) , and ( multi - objective ) optimization aspects in a holistic and structured manner .",
    "hence , the framework enables the decision maker to solve the problem by expressing preferences for each aspect quantitatively and concurrently .",
    "it explicitly incorporates many concepts that have been implicitly considered by heuristic schemes , and builds upon many results from seemingly disjoint but relevant fields such as information theory , machine learning , and optimization and control theories .",
    "specifically , it combines concepts from these fields by    * explicitly quantifying the information acquired using the entropy measure from information theory , * modeling and estimating the ( nonconvex ) function or ( nonlinear ) system adopting a bayesian approach and using gaussian processes as a state - of - the - art regression method , * using an iterative scheme for observation , learning , and optimization , * capturing all of these aspects under the umbrella of a multi - objective `` meta '' optimization formulation .    despite methods and approaches from machine ( statistical ) learning",
    "are heavily utilized in this framework , the problem at hand is very different from many classical machine learning ones , even in its learning aspect . in most classical application domains of",
    "machine learning such as data mining , computer vision , or image and voice recognition , the difficulty is often in handling significant amount of data in contrast to lack of it .",
    "many methods such as expectation - maximization ( em ) inherently make this assumption , except from `` active learning '' schemes @xcite .",
    "information theory plays plays an important role in evaluating scarce ( and expensive ) data and developing strategies for obtaining it .",
    "interestingly , data scarcity converts at the same time the disadvantages of some methods into advantages , e.g. the scalability problem of gaussian processes .",
    "it is worth noting that the class of problems described here are much more frequently encountered in practice than it may first seem .",
    "for example , the class of black - box methods known as `` kriging '' @xcite have been applied to such problems in geology and mining as well as to hydrology since mid-1960s .",
    "in addition , the solution framework proposed is applicable to a wide variety of fields due to its fundamental nature .",
    "one example is decentralized resource allocation decisions in networked and complex systems , e.g. wired and wireless networks , where parameters change quickly and global information on network characteristics are not available at the local decision - making nodes .",
    "another example is security - related decisions where opponents spend a conscious effort to hide their actions .",
    "a related area is security and information technology risk management in large - scale organizations , where acquiring information on individual subsystems and processes can be very costly .",
    "yet another example application is in biological systems where individual organisms or subsystems operate autonomously ( even if they are part of a larger system ) under limited local information .",
    "a concrete definition of the motivating problem mentioned in the introduction section is helpful for describing the multiple aspects of the limited information decision making framework . without loss of any generality ,",
    "let @xmath0 be a nonempty , convex , and compact ( closed and bounded ) subset of the original problem domain @xmath1 of @xmath2 dimensions .",
    "the original domain @xmath1 does not have to be convex , compact , or even fully known .",
    "however , adopting a `` divide and conquer '' approach , the subset @xmath3 provides a reasonable starting point .",
    "define next the objective function to be maximized @xmath4 which is unknown except from on a finite number of points ( possibly imperfectly ) observed . as a simplifying assumption ,",
    "let @xmath5 be lipschitz continuous on @xmath3 .",
    "one of the main distinguishing characteristics of this problem is the limitations on set of observations @xmath6 due to cost of obtaining information or non - stationarity of the underlying system .",
    "assume for now that the cost of observing the value of the objective function @xmath7 is the same for any @xmath8 .",
    "then , a basic search problem is defined as follows :    [ prob : search1 ] consider a lipschitz - continuous objective function @xmath9 on the @xmath2-dimensional nonempty , convex , and compact set @xmath10 . the function is unknown except from on a finite number of observed data points .",
    "what is the best search strategy @xmath11 that solves @xmath12 for a given @xmath13 ?",
    "the number of observations , @xmath13 , in problem  [ prob : search1 ] may be imposed by the nature of the specific application domain . in many problems , where there is no time constraint , adopting an iterative ( one - by - one ) approach , and",
    "hence choosing @xmath14 is clearly beneficial as it allows for usage of incoming new information at each step .",
    "alternatively , the assumption on the equal observation cost can be relaxed and be formulated as a constraint @xmath15 where @xmath16 is the observation cost function , and the scalar @xmath17 is the total `` exploration budget '' .",
    "it is also possible to define this cost iteratively based on the ( distance from ) previous observation , e.g. @xmath18 . in such cases ,",
    "a location - based iterative search scheme can be considered .    the simplest ( both conceptually and computationally ) strategy to solve problem",
    "[ prob : search1 ] is random search on the domain @xmath3 . as such no attempt is made to `` learn '' the properties of the function @xmath5 .",
    "unless , @xmath5 is `` algorithmically random '' @xcite , which is rarely the case , this strategy wastes the information collected on @xmath5 .",
    "a slightly more complicated and very popular set of strategies combine random search with simple modeling of the function through gradient methods . in this case , the collected information is used to model @xmath5 rudimentarily using derived gradients to `` define slopes '' in a heuristic manner . then , these slopes of @xmath5 are explored step - by - step in the upwards direction to find a local maximum , after which the search algorithm randomly jumps to another location .",
    "it is also possible to randomize the gradient climbing scheme for additional flexibility @xcite .",
    "the framework presented in this paper takes one further step and * explicitly * models the ( entire ) objective function @xmath5 ( on the set @xmath3 ) using the information collected instead of heuristically describing only the slopes .",
    "the function @xmath19 , which models , approximates , and estimates @xmath5 , belongs to a certain class functions such that @xmath20 .",
    "the selection and properties of this class is based on `` a priori '' information available and can be interpreted as the `` world view '' of the decision maker .",
    "these properties can often be expressed using meta - parameters which are then updated based on the observations through a separate optimization process . likewise , a slower time - scale process can be used for model selection if processing capabilities permit a multi - model approach .",
    "this model - based search process , which lies at the center of the framework , is fundamentally a manifestation of the bayesian approach @xcite .",
    "it first imposes explicit and a priori modeling assumptions by choosing @xmath19 from a certain class of functions , @xmath21 , and then infers ( learns , updates ) @xmath19 in a structured manner as more information becomes available through observations .    from a computational point of view",
    ", the decision making framework with limited information lies at one end of the computation vs. observation spectrum , while random search is at the opposite end .",
    "the framework tries to utilize each piece of information to the maximum possible extent almost regardless of the computational cost .",
    "the underlying assumption here is : * observation is very costly whereas computation is rather cheap*. this assumption is not only valid for a wide variety of problems from different fields ranging from networking and security to economics and risk management , but also inspired from biological systems . in many biological organisms , from single cells to human beings , operating close to this end of the computation - observation spectrum is more advantageous than doing random search .",
    "when doing random search on the domain @xmath3 , at each stage i.e. given the previous observations , each remaining candidate data point provides equivalent amount of information .",
    "however , this is not the case when doing model - based search . depending on the model adopted and previous information collected ,",
    "different unexplored points provide different amount of information .",
    "this information can be exactly quantified using the definition of entropy and information from the field of ( shannon ) information theory .",
    "accordingly , the scalar quantity @xmath22 denotes the aggregate information obtained from the set of observations @xmath23 within the model represented by @xmath19 .",
    "a related issue is the reliability and possibly noisy nature of observations , which will be discussed in further detail in the next section .",
    "an extension of problem  [ prob : search1 ] that captures the aspects discussed above is defined next .",
    "[ prob : search2 ]",
    "let @xmath9 be an objective function on the @xmath2-dimensional nonempty , convex , and compact set @xmath10 , which is unknown except from on a finite number of observed data points .",
    "further let @xmath24 be an estimate of the objective function obtained using an a priori model and observed data .",
    "what is the best search strategy @xmath25 that solves the multi - objective problem with the following components ?    * _ objective 1 : _ @xmath26 * _ objective 2 : _ @xmath27 * _ objective 3 : _",
    "@xmath28    here , @xmath29 is a risk or expected loss function quantifying the mismatch between actual and estimated functions on the observation data @xcite .",
    "the scalar quantity @xmath30 is the aggregate information obtained from the set of observations @xmath31 within the model represented by @xmath19 .",
    "the cardinality of @xmath31 , @xmath13 , can be either given , e.g. @xmath14 , or defined as an additional constraint @xmath32 , where @xmath16 is the observation cost function , and the scalar @xmath17 is the total `` exploration budget '' .",
    "it is important to observe here that the three objectives defined in problem  [ prob : search2 ] are ( almost ) independent from and orthogonal to each other despite being closely related . _",
    "objective 1 _ purely aims to maximize the unknown objective function @xmath5 using the best estimate ( model ) @xmath19 . _",
    "objective 2 _ focuses on minimizing the error between the estimate @xmath19 and the real unknown function @xmath5 based on the observations made .",
    "_ objective 3 _ tries to maximize the amount of information provided by each ( costly ) observation or experiment .",
    "it is worth noting that _ objective 3 _ is independently formulated from _ objective 2 _ , in other words , exploration is done independently from estimation . in contrast , ensuring a balance between _ objective 1 _ and _ 2 _ is necessary to ensure that solution is robust .",
    "these objectives and the fundamental aspects of decision making with limited information are visually depicted in figure  [ fig : goals1 ] .",
    ".fundamental trade - offs [ cols=\"<,^ , < \" , ]     [ tbl : tradeoff ]    there are multiple trade - offs that are inherent to this problem as listed in table  [ tbl : tradeoff ] .",
    "the first one , exploration versus exploitation , puts exploration or obtaining more observations against exploitation , i.e. trying to achieve the given objective .",
    "observation versus computation captures the trade - off between building sophisticated models using the available information to the fullest extend and making more observations .",
    "robustness versus optimization puts risk avoidance against optimization with respect to the original objective as in exploitation .",
    "this section presents the methods that are utilized within the framework which addresses the problem defined in the previous one .",
    "first , the regression model and gaussian processes ( gp ) are presented .",
    "subsequently , modeling and measurement of information is discussed based on ( shannon ) information theory .",
    "problem  [ prob : search2 ] presented in the previous section involves inferring or learning the function @xmath5 using the set of observed data points .",
    "this is known as the _ regression _ problem in machine learning and is a supervised learning method since the observed data constitutes at the same time the learning data set .",
    "this learning process involves selection of a `` model '' , where the learned function @xmath19 is , for example , expressed in terms of a set of parameters and specific basis functions , and at the same time minimization of an error measure between the functions @xmath5 and @xmath19 on the learning data set .",
    "gaussian processes ( gp ) provide a nonparametric alternative to this but follow in spirit the same idea .    the main goal of regression involves a trade - off . on the one hand",
    ", it tries to minimize the _ observed _ error between @xmath5 and @xmath19 . on the other , it tries to infer the `` real '' shape of @xmath5 and make good estimations using @xmath19 even at unobserved points .",
    "if the former is overly emphasized , then one ends up with `` over fitting '' , which means @xmath19 follows @xmath5 closely at observed points but has weak predictive value at unobserved ones .",
    "this delicate balance is usually achieved by balancing the prior `` beliefs '' on the nature of the function , captured by the model ( basis functions ) , and fitting the model to the observed data .",
    "this paper focuses on gaussian process @xcite as the chosen regression method within the framework developed without loss of any generality .",
    "there are multiple reasons behind this preference .",
    "firstly , gp provides an elegant mathematical method for easily combining many aspects of the framework .",
    "secondly , being a nonparametric method gp eliminates any discussion on model degree .",
    "thirdly , it is easy to implement and understand as it is based on well - known gaussian probability concepts . fourthly , noise in observations is immediately taken into account if it is modeled as gaussian . finally , one of the main drawbacks of gp namely being computational heavy",
    ", does not really apply to the problem at hand since the amount of data available is already very limited .",
    "it is not possible to present here a comprehensive treatment of gp .",
    "therefore , a very rudimentary overview is provided next within the context of the decision making problem .",
    "consider a set of @xmath33 data points @xmath34 where each @xmath35 is a @xmath36dimensional vector , and the corresponding vector of scalar values is @xmath37 .",
    "assume that the observations are distorted by a zero - mean gaussian noise , @xmath38 with variance @xmath39 .",
    "then , the resulting observations is a vector of gaussian @xmath40 .",
    "a gp is formally defined as a collection of random variables , any finite number of which have a joint gaussian distribution .",
    "it is completely specified by its mean function @xmath41 and covariance function @xmath42 , where @xmath43 \\text { and } c(x,\\tilde x)=e[(\\hat f(x)-m(x))(\\hat f(\\tilde x)-m(\\tilde x ) ) ] ,   \\ ; \\forall x , \\tilde x \\in { \\mathcal}d.\\ ] ]    let us for simplicity choose @xmath44 .",
    "then , the gp is characterized entirely by its covariance function @xmath42 .",
    "since the noise in observation vector @xmath45 is also gaussian , the covariance function can be defined as the sum of a _ kernel function _ @xmath46 and the diagonal noise variance @xmath47 where @xmath48 is the identity matrix .",
    "while it is possible to choose here any ( positive definite ) kernel @xmath49 , one classical choice is @xmath50.\\ ] ] note that gp makes use of the well - known _ kernel trick _ here by representing an infinite dimensional continuous function using a ( finite ) set of continuous basis functions and associated vector of real parameters in accordance with the _ representer theorem _ @xcite .",
    "the ( noisy ) is positive definite ] training set @xmath51 is used to define the corresponding gp , @xmath52 , through the @xmath53 covariance function @xmath54 , where the conditional gaussian distribution of any point outside the training set , @xmath55 , given the training data @xmath56 can be computed as follows .",
    "define the vector @xmath57\\ ] ] and scalar @xmath58 then , the conditional distribution @xmath59 that characterizes the @xmath60 is a gaussian @xmath61 with mean @xmath19 and variance @xmath62 , @xmath63    this is a key result that defines gp regression as the mean function @xmath24 of the gaussian distribution and provides a prediction of the objective function @xmath7 . at the same time , it belongs to the well - defined class @xmath64 , which is the set of all possible sample functions of the gp @xmath65 where @xmath66 is defined in ( [ e : gcov ] ) and @xmath67 through ( [ e : k ] ) , ( [ e : kappa ] ) , and ( [ e : gp1 ] ) , above .",
    "furthermore , the variance function @xmath68 can be used to measure the uncertainty level of the predictions provided by @xmath19 , which will be discussed in the next subsection .      in",
    "the framework presented , each observation provides a data point to the regression problem ( estimating @xmath5 by constructing @xmath19 ) as discussed in the previous subsection .",
    "many works in the learning literature consider the `` training '' data used in regression available ( all at once or sequentially ) and do not discuss the possibility of the decision maker influencing or even optimizing the data collection process . the _ active learning _ problem defined in section  [ sec : problem ] requires , however , exactly addressing the question of `` how to quantify information obtained and optimize the observation process ? '' .",
    "following the approach discussed in @xcite , the framework here provides a precise answer to this question .    making any decision on the next ( set of ) observations in a principled manner",
    "necessitates first _ measuring the information obtained from each observation within the adopted model_. it is important to note that the information measure here is dependent on the chosen model .",
    "for example , the same observation provides a different amount of information to a random search model than a gp one .",
    "shannon information theory readily provides the necessary mathematical framework for measuring the information content of a variable .",
    "let @xmath69 be a probability distribution over the set of possible values of a discrete random variable @xmath70 .",
    "the * entropy * of the random variable is given by @xmath71 , which quantifies the amount of uncertainty .",
    "then , the information obtained from an observation on the variable , i.e. reduction in uncertainty , can be quantified simply by taking the difference of its initial and final entropy , @xmath72 it is important here to avoid the common conceptual pitfall of equating entropy to information itself as it is sometimes done in communication theory literature . within this framework , ( shannon )",
    "_ information is defined as a measure of the decrease of uncertainty after ( each ) observation ( within a given model)_. this can be best explained with the following simple example .",
    "choose a number between @xmath73 and @xmath74 randomly with uniform probability ( prior ) .",
    "what is the best searching strategy for finding this number ? let the random variable @xmath70 represent this number .",
    "in the beginning the entropy of @xmath70 is @xmath75 the information maximization problem is defined as @xmath76 since @xmath77 , the entropy before the action ( obtaining information ) is constant .",
    "the entropy @xmath78 is the one after information is obtained , and hence is directly affected by the specific action chosen .",
    "now , define the action as setting a threshold @xmath79 to check whether the chosen number is less or higher than this threshold @xmath80 . to simplify the analysis ,",
    "consider a continuous version of the problem by defining @xmath69 as the probability of the chosen number being less than the threshold .",
    "thus , in this uniform prior case , the problem simplifies to @xmath81 which has the derivative @xmath82 clearly , the threshold @xmath83 is the global minimum , which roughly corresponds to @xmath84 ( ignoring quantization and boundary effects ) .",
    "thus , bisection from the middle is the optimal search strategy for the uniform prior . in this example",
    ", the number can be found in the worst - case in @xmath85 steps , each providing one bit of information .",
    "nonuniform probabilities ( priors ) can be handled in a similar way .    if this search process ( bisection ) is repeatedly applied without any feedback , then it results in the optimal quantization of the search space both in the uniform case above and for the nonuniform probabilities .",
    "if feedback is available , i.e. one learns after each bisection whether the number is larger or less than the boundary , then this is as shown the best search strategy .",
    "the model adopted in the framework for decision making with limited information builds on the methods presented in the previous section and addresses the problem introduced in section  [ sec : problem ] .",
    "the model consists of three main parts : observation , update of gp for regression , and optimization to determine next action .",
    "these three steps , shown in figure  [ fig : model1 ] are taken iteratively to achieve the objectives in problem  [ prob : search2 ] . as a result of its iterative nature",
    ", this approach can be considered in a sense similar to the well - known expectation - maximization algorithm @xcite .",
    "observations , given that they are a scarce resource in the class of problems considered , play an important role in the model .",
    "uncertainties in the observed quantities can be modeled as additive noise .",
    "likewise , properties ( variance or bias ) of additive noise can be used to model the reliability of ( and bias in ) the data points observed .",
    "gps provide a straightforward mathematical structure for incorporating these aspects to the model under some simplifying assumptions .",
    "the set of observations collected provide the ( supervised ) training data for gp regression in order to estimate the characteristics of the function or system at hand .",
    "this process relies on the gp methods described in subsection  [ sec : gp ] .",
    "thus , at each iteration an up - to - date description of the function or system is obtained based on the latest observations .",
    "specifically , @xmath19 provides an estimate of the original function @xmath5 . assuming an additive gaussian noise model , the noise variance @xmath86 can be used to model uncertainties , e.g. older and noisy data resulting in higher @xmath86 values .",
    "the final and most important part of the model provides a basis for determining the next action after an optimization process that takes into account all three objectives in problem  [ prob : search2 ] .",
    "the information aspect of these objectives is already discussed in subsection  [ sec : obsinfo ] .",
    "an important issue here is the fact that there are infinitely many candidate points in this optimization process , but in practice only a finite collection of them can be evaluated .",
    "when making a decision on the next action through multi - objective optimization , there are ( infinitely ) many candidate points .",
    "a pragmatic solution to the problem of finding solution candidates is to ( adaptively ) sample the problem domain @xmath3 to obtain the set @xmath87 that does not overlap with known points . in low ( one or two ) dimensions ,",
    "this can be easily achieved through grid sampling methods . in higher dimensions , ( quasi )",
    "monte carlo schemes can be utilized . for large problem domains ,",
    "the current domain of interest @xmath3 can be defined around the last or most promising observation in such a way that such a sampling is computationally feasible .",
    "likewise , multi - resolution schemes can also be deployed to increase computational efficiency .",
    "although such a solution may seem restrictive at first glance , it is in spirit not very different from other schemes such as simulated annealing , which are widely used to address nonconvex optimization problems .",
    "however , a major difference between this and other schemes is the fact that the candidate sampling and evaluation are done here `` a priori '' due to experimentation being costly while other methods rely on abundance of information .    a natural question that arises is : whether and under what conditions does such a sampling method give satisfactory results .",
    "the following result from @xcite provides an answer to this question in terms of number of samples required .",
    "[ thm : sampling ] define a multivariate function @xmath7 on the convex , compact set @xmath3 , which admits the maximum @xmath88 .",
    "based on a set of @xmath13 random samples @xmath89 from the entire set @xmath3 , let @xmath90 be an estimate of the maximum @xmath91 .    given an @xmath92 and @xmath93 , the minimum number of random samples @xmath13 which guarantees that @xmath94 \\leq \\varepsilon \\right ) \\geq 1-\\delta,\\ ] ] i.e. the probability that the probability of the real maximum surpassing the estimated one being less than @xmath95 is larger than @xmath96 , is @xmath97 furthermore , this bound is tight if the function @xmath5 is continuous on @xmath3 .",
    "it is interesting and important to note that this bound is independent of the sampling distribution used ( as long as it covers the whole set @xmath3 with nonzero probability ) , the function @xmath5 itself , as well as the properties and dimension of the set @xmath3 .",
    "the information measurement and gp approaches in section  [ sec : methods ] can be directly combined .",
    "let the zero - mean multivariate gaussian ( normal ) probability distribution be denoted as @xmath98^t|c_p(x)|^{-1 } [ x - m]\\right ) , \\ ;   x \\in { \\mathcal}x,\\ ] ] where @xmath99 is the determinant , @xmath100 is the mean ( vector ) as defined in ( [ e : gp1 ] ) , and @xmath101 is the covariance matrix as a function of the newly observed point @xmath8 given by @xmath102 .\\ ] ] here , the vector @xmath103 is defined in ( [ e : k ] ) and @xmath104 in ( [ e : kappa ] ) , respectively .",
    "the matrix @xmath105 is the covariance matrix based on the training data @xmath106 as defined in ( [ e : gcov ] ) .",
    "the entropy of the multivariate gaussian distribution ( [ e : multivargauss ] ) is @xcite @xmath107 where @xmath2 is the dimension .",
    "note that , this is the entropy of the gp estimate at the point @xmath108 based on the available data @xmath106 .",
    "the aggregate entropy of the function on the region @xmath3 is given by @xmath109    the problem of choosing a new data point @xmath110 such that the information obtained from it within the gp regression model is maximized can be formulated in a way similar to the one in the bisection example : @xmath111 \\ , dx = \\arg \\min_{\\tilde x }   \\int_{x \\in { \\mathcal}x } \\dfrac{1}{2 } \\ln |c_q(x,\\tilde x)| dx,\\ ] ] where the integral is computed over all @xmath8 , and the covariance matrix @xmath112 is defined as @xmath113 , \\ ] ] and @xmath114 .",
    "here , @xmath105 is a @xmath53 matrix and @xmath115 is a @xmath116 one , whereas @xmath104 and @xmath117 are scalars and @xmath118 is a @xmath119 vector .",
    "this result is summarized in the following proposition .",
    "[ thm : gpsearch ] as a maximum information data collection strategy for a gaussian process with a covariance matrix @xmath105 , the next observation @xmath110 should be chosen in such a way that @xmath120 where @xmath112 is defined in ( [ e : covxbar ] ) .",
    "given a set of ( candidate ) points @xmath121 sampled from @xmath3 , the result in proposition  [ thm : gpsearch ] can be revisited . the problem in ( [ e : infocollect1 ] ) is then approximated @xcite by @xmath122 using monotonicity property of the natural logarithm and the fact that the determinant of a covariance matrix is non - negative .",
    "thus , the following counterpart of proposition  [ thm : gpsearch ] is obtained :    [ thm : gpsearch2 ] as an approximately maximum information data collection strategy for a gaussian process with a covariance matrix @xmath105 and given a collection of candidate points @xmath121 , the next observation @xmath123 should be chosen in such a way that @xmath124 where @xmath112 is given in ( [ e : covxbar ] ) .",
    "although it is an approximation , finding a solution to the optimization problem in proposition  [ thm : gpsearch2 ] can still be computationally costly .",
    "therefore , a greedy algorithm is proposed as a computationally simpler alternative .",
    "let , @xmath125 be defined as @xmath126 where the matrix @xmath127 is given by ( [ e : covx ] ) @xcite . the first term above",
    ", @xmath128 is fixed and the second one , @xmath129 is the same as the gp variance @xmath68 in ( [ e : gp1 ] ) .",
    "hence , the sample @xmath91 is one of those with the maximum variance in the set @xmath121 , given current data @xmath106 .",
    "it follows from ( [ e : covxbar ] ) and basic matrix theory that if @xmath130 for a given @xmath108 then @xmath131 is minimized . as a simplification ,",
    "ignore the dependencies between @xmath112 matrices for different @xmath132 .",
    "then , choosing the maximum variance @xmath110 as @xmath133 leads to a large ( possibly largest ) reduction in @xmath134 , and hence provides a rough approximate solution to ( [ e : infocollect2 ] ) and to the result in proposition  [ thm : gpsearch ] .",
    "this result is consistent with widely - known heuristics such as `` maximum entropy '' or `` minimum variance '' methods @xcite and a variant has been discussed in @xcite .",
    "[ thm : gpsearch3 ] given a gaussian process with a covariance matrix @xmath105 and a collection of candidate points @xmath121 , an approximate solution to the maximum information data collection problem defined in proposition  [ thm : gpsearch ] is to choose the sample point(s ) @xmath135 in such a way that it has ( they have ) the maximum variance within the set @xmath121 .",
    "let @xmath9 be the unknown lipschitz - continuous function of interest on the @xmath2-dimensional nonempty , convex , and compact set @xmath10 .",
    "the amount of information about this function available to the decision maker is limited to a finite number of possibly noisy observations .",
    "since the observations are costly , the goal of the decision maker is to find the maximum of @xmath5 , estimate @xmath5 as accurately as possible using available observations , and select the most informative data points , at the same time .",
    "this naturally calls for an iterative and myopic optimization procedure since each new observation provides a new data point that concurrently affects the maximization , function estimation ( regression ) , and information quantity .",
    "the first and basic objective is the maximization of the function @xmath7 on @xmath8 . as a simplification",
    ", observations are assumed to be sequential , one at a time .",
    "since @xmath5 is basically unknown , this problem has to be formulated as @xmath136 where @xmath19 is the best estimate obtained through gp regression ( [ e : gp1 ] ) using the current data set @xmath106 .",
    "data uncertainty ( observation errors ) is modeled through additive gaussian noise with variance @xmath86 as a first approximation .",
    "the second objective is to minimize the difference ( estimation error ) between @xmath19 and @xmath5 .",
    "define @xmath137 .",
    "given the set of noisy observations @xmath138 where @xmath139 denotes zero mean gaussian noise , it is possible to use another gp regression ( [ e : gp1 ] ) to estimate this error function , @xmath140 , on the entire set @xmath3 .",
    "thus , the second objective is to ensure that the next observation @xmath135 solves @xmath141 note that , @xmath142 here corresponds to a risk or loss estimate function .",
    "the third objective is to maximize the amount of information obtained with each observation @xmath135 , or @xmath143 given the best estimate of the original function , @xmath19 .",
    "this objective has already been discussed in section  [ sec : obsinfo ] in detail .",
    "the values of the three objectives , @xmath144 , can not be evaluated numerically on the entire set @xmath3 .",
    "therefore , a sampling method is used as described in section  [ sec : model ] to obtain a set of solution candidates @xmath121 , which replaces @xmath3 in the maximization and minimization problems above .",
    "next , specific problem formulations are presented based on such a sampling of solution candidates .",
    "the overall structure of the framework is visualized in figure  [ fig : model2 ] .",
    "the most common approach to multi - objective optimization is the * weighted sum method * @xcite .",
    "the three objectives discussed above can be combined to obtain a single objective using the respective weights @xmath145 $ ] , @xmath146 , @xmath147 . assuming a single data point is chosen from and observed among the candidates @xmath121 at each step , i.e. @xmath148",
    ", a specific weighted sum formulation to address problem  [ prob : search2 ] is obtained .",
    "[ prob : weight1 ] the solution , @xmath149 , to the optimization problem @xmath150 constitutes the best search strategy for this weighted sum formulation of problem  [ prob : search2 ] .    as discussed in subsection  [",
    "sec : obsinfo ] and stated in proposition  [ thm : gpsearch2 ] , the information objective , @xmath151 , in ( [ e : pw1 ] ) can be approximated by substituting it with gp variance @xmath68 in ( [ e : gp1 ] ) to decrease computational load .",
    "thus , an approximation to the solution in proposition  [ prob : weight1 ] is :    [ prob : weight2 ] the solution , @xmath149 , to the optimization problem @xmath152 where @xmath153 is defined in ( [ e : gp1 ] ) , approximates the search strategy in proposition  [ prob : weight1 ] .    the weighting scheme described is only meaningful if the three objectives are of the same order of magnitude .",
    "therefore , the original objective functions , @xmath154 , @xmath155 , have to be transformed or `` normalized '' .",
    "there are many different approaches to perform such a transformation @xcite .",
    "the most common one , which coincidentally is known as normalization , aims to map each objective function to a predefined interval , e.g. @xmath156 $ ] . to do this , estimate first an upper @xmath157 and lower @xmath158 bound on each individual objective @xmath159",
    "then , the @xmath160 normalized objective is @xmath161    the main issue in normalization is to determine the appropriate upper and lower bounds , which is a very problem - dependent one . in the case of proposition  [ prob : weight2 ] , the estimated functions @xmath19 and @xmath162 on the set @xmath121 as well as the existing observations @xmath106 , can be utilized to obtain these values .",
    "the specific bounds for the respective objectives @xmath163 , @xmath164 , @xmath165 , @xmath166 , @xmath167 , and @xmath168 provide a suitable starting estimate and can be combined with a prior domain knowledge if necessary .",
    "thus , a normalized version of the formulation in proposition  [ prob : weight2 ] is obtained .",
    "[ prob : weight3 ] the solution , @xmath149 , to the optimization problem @xmath169 where @xmath170 , provides an approximation to the best search strategy for solving the normalized weighted - sum formulation of problem  [ prob : search2 ] .",
    "the * bounded objective function * method provides a suitable alternative to the weighted sum formulation above in addressing the multi - objective problem defined .",
    "the bounded objective function method minimizes the single most important objective , in this case @xmath171 , while the other two objective functions @xmath172 and @xmath173 are converted to form additional constraints .",
    "such constraints are in a sense similar to qos ones that naturally exist in many real life problems @xcite . as an advantage , in the bounded objective formulation",
    "there is no need for normalization .",
    "the bounded objective counterpart of the result in proposition  [ prob : weight2 ] is as follows .",
    "[ prob : bound1 ] the solution , @xmath149 , to the constrained optimization problem @xmath174 where @xmath175 and @xmath176 are given ( predetermined ) scalar bounds on @xmath142 and @xmath151 , respectively , provides an approximate best search strategy for a bounded - objective formulation of problem  [ prob : search2 ] .",
    "the advantage of the bounded objective function method is that it provides a bound on the information collection and estimation objectives while maximizing the estimated function .",
    "this leads in practice to an initial emphasis on information collection and correct estimation of the objective function . in that sense",
    ", the method is more `` classical '' , i.e. follows the common method of learn first and maximize later .",
    "furthermore , it does not require normalization , i.e. it is easier to deploy .",
    "the method has , however , a significant disadvantage which makes its usage prohibitive . in large - scale or high - dimensional problems ,",
    "the space to explore to satisfy any bound on information is simply immense .",
    "therefore , one does not have the luxury of identifying the function first to maximize it later as it would take too many samples to do this . in such cases , it makes more sense to deploy the weighted sum method , possibly along with a cooling scheme to modify the weights as part of a cooling scheme to balance depth - first vs. breadth - first search .    until now ,",
    "it has been ( implicitly ) assumed that the static optimization problem at hand is stationary .",
    "however , in a variety of problems this is not the case and the function @xmath177 changes with time .",
    "the decision making framework allows for modeling such systems in the following way .",
    "let @xmath178 be the set of noisy or unreliable past observations until time @xmath80 , where @xmath179 is the zero mean gaussian `` noise '' term at time @xmath80 .",
    "now , the deterioration in the past information due to change in @xmath177 can be captured by increasing the variance of the noise term , @xmath180 , with time .",
    "for example , a simple linear dynamic can be defined as @xmath181 where @xmath182 captures the level of stationarity , e.g. a large @xmath183 indicates a rapidly changing system and function @xmath177 .",
    "an algorithmic summary of the solution approaches discussed above for a specific set of choices is provided by algorithm  [ alg : algopt1 ] , which describes both weighted - sum and bounded objective variants .",
    "function domain , @xmath3 , gp meta - parameters , objective weights @xmath184 $ ] or bounds @xmath185 , initial data set @xmath51 .",
    "use gp with a gaussian kernel and specific expected error variances for function @xmath19 and error function @xmath162 estimation .",
    "sample domain @xmath3 to obtain @xmath186 . in some cases , @xmath187 .",
    "estimate @xmath19 and @xmath162 based on observed data @xmath51 on @xmath186 using gps .",
    "compute variance , @xmath68 , of @xmath19 ( [ e : gp1 ] ) on @xmath186 as an estimate of @xmath188 .",
    "next action maximizes a normalized and weighted sum of objectives @xmath189 as stated in proposition [ prob : weight3 ] .",
    "next action is solution to the constrained problem in proposition [ prob : bound1 ] .",
    "update the observed data @xmath51 .",
    "the algorithm [ alg : algopt1 ] is illustrated next with multiple numerical examples .",
    "it is worth reminding that the main issue here is to solve the optimization problems with minimum data using active learning . in all examples ,",
    "a uniform grid is used to sample the solution space rather than resorting to a more sophisticated method since the examples are chosen to be only one or two dimensional for visualization purposes .",
    "the first numerical example aims to visualize the presented framework and algorithm .",
    "hence , the chosen function is only one dimensional , @xmath190 on the interval @xmath191 $ ] . the interval is linearly sampled to obtain a grid with a distance of @xmath192 between points , i.e. @xmath193 . a gaussian kernel with variance @xmath194",
    "is chosen for estimating both @xmath19 and @xmath162 .",
    "the weights are equal to one , @xmath195 $ ] , in the weighted - sum method .",
    "the bounds are @xmath196 for the error bound and @xmath197 for the bound on maximum variance estimate in the bounded objective method .",
    "the initial data consists of a single point , @xmath198 .",
    "figure  [ fig : weighted6 ] shows the results based on the normalized weighted - sum method in proposition  [ prob : weight3 ] after @xmath199 iterations ( @xmath85 samples in total , together with the initial data point ) .",
    "the variance here is @xmath68 of the estimated function @xmath19 using data points @xmath106 . clearly , the estimated peak is not the one of the real function @xmath5 .",
    "next , figure  [ fig : weighted12 ] shows that after @xmath200 iterations ( @xmath201 data points in @xmath106 ) , the function and the location of its peak is estimated correctly .",
    "the sequence of points selected during the iteration process are : @xmath202     data points . ]",
    "data points . ]     and entropy @xmath30 on @xmath121 at each iteration step . ]     data points . ]",
    "the amount of information obtained during the iterative optimization is of particular interest .",
    "figure  [ fig : info1 ] depicts the mean variance @xmath62 and entropy @xmath30 of the estimated function @xmath19 on @xmath121 at each iteration step . in this specific example , the two quantities are very well correlated .",
    "note , however , that this correlation is a function of the relative weights between information collection and other objectives .    finally , figure  [ fig : bounded1 ] depicts the results of the bounded objective method with the given bounds .",
    "the number of iterations is @xmath200 as before , which gives an opportunity of direct comparison with the weighted - sum method .",
    "the sequence of points selected during the iteration process are : @xmath202      the objective function in the second numerical example is the goldstein&price function  @xcite , which is shown in figure  [ fig : gpfunc ] in its inverted form to ensure consistency with the maximization formulation in this paper .",
    "the problem domain consists of the two dimensional rectangular region @xmath203 \\times [ -2 , 2]$ ] , which is linearly sampled to obtain a uniform grid with a @xmath204 interval between sample points . a gaussian kernel with variance @xmath205 and @xmath194",
    "is chosen for estimating @xmath19 and @xmath162 , respectively .",
    "the weighted - sum method is utilized in algorithm  [ alg : algopt1 ] with the weights @xmath206 $ ] .",
    "the search budget is chosen as @xmath207 before stopping the algorithm ( for the search space of approx .",
    "@xmath208 samples in the grid ) .",
    "the real global minimum ( peak ) of the ( inverted ) goldstein&price function is at @xmath209 and the location found by the algorithm using the @xmath207 data points is @xmath210 .",
    "figure  [ fig : gpopt1 ] depicts the estimated function , the data points as well as the optimum found .",
    "although the real optimum value is @xmath211 ( in the inverted version ) while the obtained one is @xmath212 , the result is still very satisfactory considering that the simple sampling scheme used and the goldstein&price function takes values in a range of @xmath73 million , i.e. the error is less than @xmath213 percent of the range .",
    "finally , figure  [ fig : infogp ] depicts the mean variance @xmath62 and entropy @xmath30 of the estimated function @xmath19 on @xmath121 at each iteration step .",
    "data points . ]     and entropy @xmath30 on @xmath121 at each iteration step . ]      the third example uses the same setup as the second one but this time with the ( inverted ) brain function  @xcite shown in figure  [ fig : braninfunc ] .",
    "the rectangular problem domain @xmath214 \\times [ 0 , 15]$ ] is sampled uniformly to obtain a grid of points with a @xmath215 interval .",
    "the real global minimums ( peaks ) of the ( inverted ) branin function are at @xmath216 , @xmath217 , and @xmath218 whereas the locations found by the algorithm are @xmath219 , @xmath220 , and @xmath221 .",
    "the values at these locations found vary between @xmath222 and @xmath223 compared to the real global value of @xmath224 ( of the inverted function ) .",
    "thus , the algorithm again performs satisfactorily .",
    "figure  [ fig : gpopt1 ] shows the computed location of one optimum , the data points , as well as the estimated function based on the data points .",
    "data points . ]",
    "the fourth example is based on the six - hump camel function  @xcite ( see figure  [ fig : camfunc ] ) on the domain @xmath203 \\times [ -2,2]$ ] , which is sampled uniformly with a @xmath204 interval .",
    "all of the parameters are chosen to be the same as before .",
    "figure  [ fig : camopt1 ] shows the computed location of two optimums , the @xmath207 data points , as well as the estimated function based on the data points .",
    "the optimum locations found are @xmath225 and @xmath226 with respective values of @xmath227 and @xmath228 , whereas the real locations are @xmath229 and @xmath230 with the value @xmath231 .",
    "data points . ]",
    "decision making with limited information is related to search theory .",
    "the idea of using information ( theory ) in this context is hardly new as evidenced by the article `` a new look at the relation between information theory and search theory '' from 1979 @xcite .",
    "the subject is further studied in @xcite .",
    "the topic of optimal search is more recently revisited by @xcite , which contains substantial historical notes and studies problems where the search target distribution in itself is unobservable .",
    "the book @xcite provides important and valuable insights into the relationship between information theory , inference , and learning . measuring information content of experiments using shannon information",
    "is explicitly mentioned and a slightly informal version of the bisection example in subsection  [ sec : obsinfo ] is discussed . however , focusing mainly on more traditional coding , communication , and machine learning topics , the book does not discuss the type of decision making problems presented in this paper .",
    "learning plays an important role in the presented framework , especially _ regression _ , which is a classical machine ( or statistical ) learning method .",
    "a very good introduction to the subject can be found in @xcite .",
    "a complementary and detailed discussion on kernel methods is in @xcite .",
    "another relevant topic is bayesian inference @xcite , which is in the foundation of the presented framework . in machine",
    "learning literature , gaussian processes ( gps ) are getting increasingly popular due to their various favorable characteristics .",
    "the book @xcite presents a comprehensive treatment of gps .",
    "additional relevant works on the subject include @xcite , which also discuss gp regression .",
    "convex optimization @xcite is a well - understood topic that is often easy to handle even if available information is limited .",
    "optimizing nonconvex functions , however , is still a research subject @xcite .",
    "it is interesting to note that the method known as _ kriging _ in global optimization is almost the same as gp regression in machine learning .",
    "the field _",
    "stochastic programming _ focuses on optimization under uncertainty but assumes a certain amount of prior knowledge on the problem at hand and models the uncertainty probabilistically @xcite .",
    "the popular heuristic method _ simulated annealing _",
    "@xcite is essentially based on iterative random search .",
    "another popular heuristic scheme particle swarm optimization @xcite is also based on random search but parallel in nature as a distinguishing characteristic rather than iterative .",
    "gaussian processes have been recently applied to the area of optimization and regression @xcite as well as system identification @xcite . while the latter mentions active learning , neither work discusses explicit information quantification or builds a connection with shannon information theory .",
    "the recent articles @xcite , which utilize gp regression for optimization in a setting similar to the one in this paper and for state - space inference and learning , respectively , do not consider information - theoretic aspects of the problem , either . likewise , the article @xcite on stochastic black box optimization , which considers a problem similar to the one here , does not take into account explicit measurement of information .",
    "the area of active learning or experiment design focuses on data scarcity in machine learning and makes use of shannon information theory among other criteria @xcite .",
    "the paper @xcite discusses objective functions which measure the expected informativeness of candidate measurements within a bayesian learning framework .",
    "the subsequent study @xcite investigates active learning for gp regression using variance as a ( heuristic ) confidence measure for test point rejection .",
    "the foundation of the approach adopted in this paper is bayesian inference , where the main idea is to choose an a priori model and update it with actual experimental data observed ( see ( * ? ? ?",
    "2 ) for a beautiful introductory discussion on the subject ) . as long as the a priori model is close to the reality ( of the problem at hand ) , this inference methodology works very efficiently as indicated by the numerical examples in section  [ sec : numeric ] . in many cases this background information , which is sometimes referred to as `` domain knowledge '' ,",
    "is already available .",
    "however , in others one has to explore the model domain and learn model meta - parameters in a time scale naturally longer than the one of actual optimization @xcite .",
    "the gp regression adopted in the presented framework is only one method for function estimation and other , e.g. parametric , methods can easily replace gp for the regression part .",
    "in any case , the regression methodology here is consistent with the principle of `` occam s razor '' , more specifically its interpretation using kolmogorov complexity @xcite . a priori ,",
    "the optimization problems at hand are more probable to be simple rather than complex to describe in accordance with _",
    "universal distribution _",
    "@xcite . hence , given a data set",
    "it is reasonable to start describing it with the simplest explanation .",
    "gp regression already incorporates this line of thinking by relying on a kernel - based approach and making use of the representer theorem ( * ? ? ?",
    "6.2 ) . as a visual example",
    ", we refer figures [ fig : weighted6 ] and [ fig : weighted12 ] for a comparison of function estimates with different sets of available data .",
    "this paper considers a class of problems where data is scarce and obtaining it is costly .",
    "information theory plays an especially important role in devising optimal schemes for obtaining new data points ( active learning ) .",
    "the entropy measure from shannon information theory provides the necessary metric for this purpose , which quantifies the `` exploration '' aspect of the problem . using a multi - objective optimization formulation",
    ", the presented framework allows explicit weighting of _ exploration _ vs. _ exploitation _ aspects .",
    "this trade - off is also very similar to one between the well - known depth - first vs. breadth - first search algorithms in search theory .    the amount of information obtained from each data point is different here only because a specific a priori general model is utilized to explain the observed data ( gp regression ) . because of this the amount of information obtained is specific to the model",
    "otherwise , without this bayesian approach , each data point would give the same information ( inversely proportional to the total number of candidate points ) .",
    "the illustrative examples discussed are low - dimensional , which makes it possible to use grids for sampling . however , in higher dimensions ( i.e. when the problem is much more `` difficult '' ) this `` luxury '' is not affordable and one has to necessarily resort to monte carlo methods . in such cases ,",
    "the trade - off between exploration and exploitation is even more emphasized .",
    "possible methods to address this issue include , `` cooling '' approaches similar to those used in simulated annealing , multi - resolution sampling based on region of interest or using topological properties of gaussian mixtures to intelligently estimate candidate points based on the current state .",
    "the optimization approach presented here can also be interpreted from a biological perspective .",
    "if an analogy between the decision - maker and a biological organism is established , then the a - priori bayesian model ( meta parameters of the gp ) that is refined over a long time scale corresponds to evolution of a species in an environment ( problem domain ) .",
    "each individual organism belogning to the species obtains new information to achieve its objective while preserving resources as much as possible . the existing evolutionary basis ( gp model )",
    "gives them an advantage to find a solution much faster compared to random search . from the perspective of the species",
    ", it also makes sense for some of its members to explore the model ( meta parameter ) domain and further refine it through adaptation . those with better meta parameters achieve then their objectives even more efficiently and obtain an evolutionary edge in natural selection ( assuming competition ) .",
    "the decision making framework presented in this paper addresses the problem of decision making under limited information by taking into account the information collection ( observation ) , estimation ( regression ) , and ( multi - objective ) optimization aspects in a holistic and structured manner .",
    "the methodology is based on gaussian processes and active learning .",
    "various issues such as quantifying information content of new data points using information theory , the relationship between information and gp variance as well as related approximation and multi - objective optimization schemes are discussed .",
    "the framework is demonstrated with multiple numerical examples .",
    "the presented framework should be considered mainly as an initial step .",
    "future research directions are abundant and include further investigation of the exploration - exploitation trade - off , adaptive weighting parameters , and random sampling methods for problems in higher dimensional spaces .",
    "additional research topics are the relationship of the framework with genetic / evolutionary methods , dynamic control problems , and multi - person decision making , i.e. game theory .",
    "this work is supported by deutsche telekom laboratories .",
    "the author wishes to thank lacra pavel , slawomir stanczak , holger boche , and kivanc mihcak for stimulating discussions on the subject .",
    "t.  alpcan , x.  fan , t.  baar , m.  arcak , and j.  t. wen , `` power control for multicell cdma wireless networks : a team optimization approach , '' _ wireless networks _ , vol .",
    "14 , no .  5 , pp . 647657 ,",
    "october 2008 .",
    "[ online ] .",
    "available : papers / alpcan - winet.pdf        p.  boyle , `` gaussian processes for regression and optimisation , '' ph.d .",
    "dissertation , victoria university of wellington , wellington , new zealand , 2007 .",
    "[ online ] .",
    "available : http://researcharchive.vuw.ac.nz/handle/10063/421    f.  h. branin , `` widely convergent method for finding multiple solutions of simultaneous nonlinear equations , '' _ ibm journal of research and development _",
    "504522 , september 1972 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1147/rd.165.0504    l.  c.  w. dixon and g.  p. szego , `` the optimization problem : an introduction , '' in _ towards global optimization ii _ , l.  c.  w. dixon and g.  p. szego , eds.1em plus 0.5em minus 0.4emnew york , ny , usa : north - holland , 1978 .",
    "o.  grodzevich and o.  romanko , `` normalization and other topics in multi - objective optimization , '' proceedings of the fields  mitacs industrial problems workshop , 2006 .",
    "[ online ] .",
    "available : http://www.maths-in-industry.org/miis/233/      e.  t. jaynes , `` entropy and search - theory , '' in _ maximum - entropy and bayesian methods in inverse problems _ , c.  r. smith and j.  w. t. grandy , eds.1em plus 0.5em minus 0.4emspringer , 1985 , p. 443 .",
    "[ online ] .",
    "available : http://bayes.wustl.edu/etj/articles/search.pdf    d.  r. jones , `` a taxonomy of global optimization methods based on response surfaces , '' _ journal of global optimization _",
    "21 , pp . 345383 ,",
    "december 2001 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1023/a:1012771025575        d.  lizotte , t.  wang , m.  bowling , and d.  schuurmans , `` gaussian process regression for optimization , '' in nips 2005 workshop on value of information in inference , learning and decision - making , 2005 .",
    "[ online ] .",
    "available : http://domino.research.ibm.com / comm / research_projects.nsf / pages / nips05w% orkshop.index.html[http://domino.research.ibm.com / comm / research_projects.nsf / pages / nips05w% orkshop.index.html ]    d.  j.  c. mackay , `` introduction to gaussian processes , '' in _ neural networks and machine learning _ , ser .",
    "nato asi series , c.  m. bishop , ed .",
    "1em plus 0.5em minus 0.4emkluwer academic press , 1998 , pp .",
    "133166 .",
    " , `` information - based objective functions for active data selection , '' _ neural computation _ , vol .  4 , no .  4 , pp . 590604 , 1992 .",
    "[ online ] .",
    "available : http://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.4.590      r.  t. marler and j.  s. arora , `` survey of multi - objective optimization methods for engineering , '' _ structural and multidisciplinary optimization _",
    "26 , no .  6 , pp .",
    "369395 , 2004 .",
    "[ online ] .",
    "available : http://www.springerlink.com / openurl.asp?genre = article&id = doi:10.1007/s0% 0158 - 003",
    "- 0368 - 6[http://www.springerlink.com / openurl.asp?genre = article&id = doi:10.1007/s0% 0158 - 003 - 0368 - 6 ]    y.  pan , l.  pavel , and t.  alpcan , `` a system performance approach to osnr optimization in optical networks , '' _ ieee transactions on communications _ , vol .",
    "58 , no .  4 , pp . 11931200 , april 2010 .",
    "[ online ] .",
    "available : papers / tcomm_preprint_v7.pdf      j.  g. pierce , `` a new look at the relation between information theory and search theory , '' office of naval research , arlington , va , usa , tech . rep .",
    ", june 1978 .",
    "[ online ] .",
    "available : http://handle.dtic.mil/100.2/ada063845        n.  v. sahinidis , `` optimization under uncertainty : state - of - the - art and opportunities , '' _ computers & chemical engineering _ , vol .",
    "6 - 7 , pp .",
    "971983 , june 2004 , focapo 2003 special issue .",
    "[ online ] .",
    "available : http://www.sciencedirect.com / science / article / b6tft-49yh97t-1/2/f15875aa% d97740410effc526416289aa[http://www.sciencedirect.com / science / article / b6tft-49yh97t-1/2/f15875aa% d97740410effc526416289aa ]      s.  seo , m.  wallat , t.  graepel , and k.  obermayer , `` gaussian process regression : active data selection and test point rejection , '' in _ proc . of ieee - inns - enns intl .",
    "joint conf . on neural networks",
    "ijcnn 2000 _ , vol .  3 , july 2000 , pp . 241246 .",
    "r.  tempo , e.  w. bai , and f.  dabbene , `` probabilistic robustness analysis : explicit bounds for the minimum number of samples , '' _ systems & control letters _ , vol .  30 , no .  5 , pp . 237242 , 1997 .",
    "[ online ] .",
    "available : http://www.sciencedirect.com / science / article / b6v4x-3sp7dcd-4/2/3dc65510% 7eff50f12b326ea10f58ef0a[http://www.sciencedirect.com / science / article / b6v4x-3sp7dcd-4/2/3dc65510% 7eff50f12b326ea10f58ef0a ]        m.  e. tipping , `` bayesian inference : an introduction to principles and practice in machine learning , '' in _",
    "advanced lectures on machine learning _ , 2003 , pp",
    "[ online ] .",
    "available : http://springerlink.metapress.com / openurl.asp?genre = article{&}issn=0302% -9743{&}volume=3176{&}spage=41[http://springerlink.metapress.com / openurl.asp?genre = article\\{&}issn=0302% -9743\\{&}volume=3176\\{&}spage=41 ]    r.  turner , m.  p. deisenroth , and c.  e. rasmussen , `` state - space inference and learning with gaussian processes , '' in _ proc . of 13th",
    "conf . on artificial intelligence and statistics ( aistats )",
    "_ , chia laguna resort , sardinia , italy , may 2010 ."
  ],
  "abstract_text": [
    "<S> in many real world problems , optimization decisions have to be made with limited information . </S>",
    "<S> the decision maker may have no a priori or posteriori data about the often nonconvex objective function except from on a limited number of points that are obtained over time through costly observations . </S>",
    "<S> this paper presents an optimization framework that takes into account the information collection ( observation ) , estimation ( regression ) , and optimization ( maximization ) aspects in a holistic and structured manner . explicitly quantifying the information acquired at each optimization step using the entropy measure from information theory , the ( nonconvex ) objective function to be optimized ( maximized ) </S>",
    "<S> is modeled and estimated by adopting a bayesian approach and using gaussian processes as a state - of - the - art regression method . </S>",
    "<S> the resulting iterative scheme allows the decision maker to solve the problem by expressing preferences for each aspect quantitatively and concurrently . </S>"
  ]
}