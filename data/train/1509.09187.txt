{
  "article_text": [
    "deep neural networks appear to provide scalable learning architectures for high - dimensional learning , with impressive results on many different type of data and signals @xcite . despite their efficiency",
    ", there is still little understanding on the properties of these architectures .",
    "deep neural networks alternate pointwise linear operators , whose coefficients are optimized with training examples , with pointwise non - linearities . to obtain good classification results ,",
    "strong constraints are imposed on the network architecture on the support of these linear operators @xcite .",
    "these constraints are usually derived from an experimental trial and error processes .",
    "section [ sec:2 ] introduces a simple deep haar scattering architecture , which only computes the sum of pairs of coefficients , and the absolute value of their difference .",
    "the architecture preserves some important properties of deep networks , while reducing the computational complexity and simplifying their mathematical analysis . through this architecture",
    ", we shall address major questions concerning invariance properties , learning complexity , consistency , and the specialization of such architectures .",
    "convolution networks are particular classes of deep networks , which compute translation invariant descriptors of signals defined over uniform grids @xcite .",
    "scattering networks were introduced as convolution networks computed with iterated wavelet transforms , to obtain invariants which are stable to deformations @xcite . with appropriate architecture constraints on haar scattering networks ,",
    "section [ sec:3 ] defines locally displacement invariant representations of signals defined on general graphs . in social , sensor or transportation networks ,",
    "high dimensional data vectors are supported on a graph @xcite . in most cases ,",
    "propagation phenomena require to define translation invariant representations for classification .",
    "we show that an appropriate configuration of an orthogonal haar scattering defines such a translation invariant representation on a graph .",
    "it is computed with a product of haar wavelet transforms on the graph , and is thus closely related to non - orthogonal translation invariant scattering transforms @xcite .",
    "the connectivity of graph data is often unknown . in social or financial networks",
    ", we typically have information on individual agents , without knowing the interactions and hence connectivity between agents .",
    "building invariant representations on such graphs requires to estimate the graph connectivity .",
    "such information can be inferred from unlabeled data , by analyzing the joint variability of signals defined on the unknown graph .",
    "this paper studies unsupervised learning strategies , which optimize deep network configurations , without class label information .",
    "most deep neural networks are fighting the curse of dimensionality by reducing the variance of the input data with contractive non - linearities @xcite .",
    "the danger of such contractions is to nearly collapse together vectors which belong to different classes .",
    "learning must preserve discriminability despite this variance reduction resulting from contractions .",
    "hierarchical unsupervised architectures have been shown to provide efficient learning strategies @xcite .",
    "we show that unsupervised learning can optimize an average discriminability by computing sparse features .",
    "sparse unsupervised learning , which is usually np hard , is reduced to a pair matching problem for haar scattering .",
    "it can thus be computed with a polynomial complexity algorithm . for haar scattering on graphs",
    ", it recovers a hierarchical connectivity of groups of vertices . under appropriate assumptions ,",
    "we prove that pairing problems avoid the curse of dimensionality .",
    "it can recover an exact connectivity with arbitrary high probability if the training size grows almost linearly with the signal dimension , as opposed to exponentially .",
    "haar scattering classification architectures are numerically tested over image databases defined on uniform grids or irregular graphs , whose geometries are either known or estimated by unsupervised learning .",
    "results are compared with state of the art unsupervised and supervised classification algorithms , applied to the same data , with known or unknown geometry .",
    "all computations can be reproduced with a software available at _ www.di.ens.fr / data / scattering / haar_.",
    "we progressively introduce orthogonal haar scattering by specializing a general deep neural network .",
    "we explain the architecture constraints and the resulting contractive properties .",
    "the input network is a positive @xmath0-dimensional signal @xmath1 , which we write @xmath2 .",
    "we denote by @xmath3 the network layer at the depth @xmath4 .",
    "a deep neural network computes the next network layer by applying a linear operator @xmath5 to @xmath6 followed by a non - linear operator .",
    "particular deep network architectures impose that @xmath5 preserves distances , up to a constant normalization factor @xmath7 @xcite : @xmath8 the network is contractive if it applies a pointwise contraction @xmath9 to each value of the output vector @xmath10 .",
    "this means that for any @xmath11 @xmath12 .",
    "rectifications and sigmoids are examples of such contractions .",
    "we use an absolute value @xmath13 because it preserves the amplitude , and it yields a permutation invariance which will be studied . for any vector @xmath14 ,",
    "the pointwise absolute value is written @xmath15 .",
    "the next network layer is thus : @xmath16 this transform is iterated up to a maximum depth @xmath17 to compute the network output @xmath18 .",
    "we shall further impose that each layer @xmath3 has the same dimension as @xmath19 , and hence that @xmath5 is an orthogonal operator in @xmath20 , up to the scaling factor @xmath7 .",
    "geometrically , @xmath21 is thus obtained by rotating @xmath3 with @xmath5 , and by contracting each of its coordinate with the absolute value .",
    "the geometry of this contraction is thus defined by the choice of the operator @xmath5 which adjusts the one - dimensional directions along which the contraction is performed .",
    "an orthogonal haar scattering is implemented with an orthogonal haar filter @xmath5 .",
    "the vector @xmath22 regroups the coefficients of @xmath23 into @xmath24 pairs and computes their sums and differences .",
    "the rotation @xmath5 is thus factorized into @xmath24 rotations by @xmath25 in @xmath26 , and multiplications by @xmath27 .",
    "the transformation of each coordinate pair @xmath28 is : @xmath29 the operator @xmath30 applies an absolute value to each output coordinate , which has no effect on @xmath31 if @xmath32 and @xmath33 , but it removes the sign of their difference : @xmath34 observe that this non - linear operator defines a permutation invariant representation of @xmath35 .",
    "indeed , the output values are not modified by a permutation of @xmath36 and @xmath37 , and the two values of @xmath36 , @xmath37 are recovered without order , by @xmath38 the operator @xmath30 can thus also be interpreted as a calculation of @xmath24 permutation invariant representations of pairs of coefficients .     by pairing the coefficients of the previous layer @xmath3 , and storing the sum of coefficients in each pair and the amplitude of their difference . ]    applying @xmath30 to @xmath3 computes the next layer @xmath39 , obtained by regrouping the coefficients of @xmath40 into @xmath24 pairs of indices written @xmath41 : @xmath42 @xmath43 the pairing @xmath44 specifies which index @xmath45 is paired with @xmath46 , but the ordering index @xmath47 is not important .",
    "it specifies the storing position in @xmath21 of the transformed values . for classification applications",
    ", @xmath44 will be optimized with training examples .",
    "this deep network computation is illustrated in figure [ fig:1 ] .",
    "the network output @xmath18 is calculated with @xmath48 additions , subtractions and absolute values .",
    "each coefficient of @xmath18 is calculated by cascading @xmath49 permutation invariant operators over pairs , and thus defines an invariant over a group of @xmath50 coefficients .",
    "the network depth @xmath49 thus corresponds to an invariance scale @xmath50 .",
    "since the network is computed by iterating orthogonal linear operators , up to a normalization , and a contractive absolute value , the following theorem proves that it defines a contractive transform , which preserves the norm , up to a normalization .",
    "it also proves that an orthogonal haar scattering transform @xmath18 is obtained by applying an orthogonal matrix to @xmath19 , which depends upon @xmath19 and @xmath49 .    [ prop : norm - preserve ] for any @xmath51 , and any @xmath52 @xmath53 moreover @xmath54 where @xmath55 is an orthogonal matrix which depends on @xmath19 and @xmath49 , and @xmath56    since @xmath39 where @xmath5 is an orthogonal operator multiplied by @xmath27 , @xmath57 since @xmath58 , equation ( [ nsdf8 ] ) is verified by induction on @xmath4 .",
    "we can also rewrite @xmath59 where @xmath60 is a diagonal matrix where the diagonal entries are @xmath61 , with a sign which depend on @xmath3 .",
    "since @xmath62 is orthogonal , @xmath63 is also orthogonal so @xmath64 is orthogonal , and depends on @xmath19 and @xmath49 .",
    "it results that @xmath65 .      a single haar scattering transform looses information since it applies orthogonal operators followed by an absolute value which looses the information of the sign .",
    "however , the following theorem proves that @xmath19 can be recovered from @xmath50 distinct orthogonal haar scattering transforms , computed with different pairings @xmath44 at each layer .",
    "[ thm : multi - layer - recovery2 ] there exist @xmath50 different orthogonal haar scattering transforms such that almost all @xmath66 can be reconstructed from the coefficients of these @xmath50 transforms .",
    "this theorem is proved by observing that a haar scattering transform is computed with permutation invariants operators over pairs .",
    "inverting these operators allows to recover values of signal pairs but not their locations .",
    "however , recombining these values on enough overlapping sets allows one to recover their locations and hence the original signal @xmath19 .",
    "this is proved on the following lemma applied to _ interlaced pairings_.",
    "we say that two pairings @xmath67 and @xmath68 are interlaced if there exists no strict subset @xmath69 of @xmath70 such that @xmath71 and @xmath72 are pairing elements within @xmath69 .",
    "the following lemma shows that a single - layer scattering operator is invertible with two interlaced pairings .",
    "[ lemma : interlacing ] if two pairings @xmath71 and @xmath73 of @xmath74 are interlaced then any @xmath75 whose coordinates have more than @xmath76 different values can be recovered from the values of @xmath77 computed with @xmath71 and the values of @xmath77 computed with @xmath73 .",
    "let us consider a triplet @xmath78 where @xmath79 is a pair in @xmath71 and @xmath80 is a pair in @xmath73 . from @xmath77",
    "computed with @xmath71 we get @xmath81 and we saw in ( [ eq : recover ] ) that it determines the values of @xmath82 up to a permutations .",
    "similarly , @xmath83 are determined up to a permutation by @xmath77 computed with @xmath73 .",
    "then unless @xmath84 and @xmath85 the three values @xmath86 are recovered .",
    "the interlacing condition implies that @xmath73 pairs @xmath87 to an index @xmath88 which can not be @xmath89 or @xmath90 .",
    "thus , the four values of @xmath91 are specified unless @xmath92 .",
    "this interlacing argument can be used to extend to @xmath93 the set of all indices @xmath94 for which @xmath95 is specified , unless @xmath19 takes only two values .",
    "suppose that the @xmath50 haar scatterings are associated to the @xmath49 hierarchical pairings @xmath96 where @xmath97 , where for each @xmath4 , @xmath98 and @xmath99 are two interlaced pairings of @xmath0 elements .",
    "the sequence @xmath100 is a binary vector taking @xmath50 different values .",
    "the constraint on the signal @xmath19 is that each of the intermediate scattering coefficients takes more than @xmath76 distinct values , which holds for @xmath75 except for a union of hyperplanes which has zero measure .",
    "thus for almost every @xmath75 , the theorem follows from applying lemma [ lemma : interlacing ] recursively to the @xmath4-th level scattering coefficients for @xmath101 .",
    "lemma [ lemma : interlacing ] proves that only two pairings is sufficient to invert one haar scattering layer .",
    "the argument proving that @xmath50 pairings are sufficient to invert @xmath49 layers is quite brute - force .",
    "it is conjectured that the number of pairings needed to obtain a complete representation for almost all @xmath75 does not need to grow exponentially in @xmath49 but rather linearly .",
    "theorem [ thm : multi - layer - recovery2 ] suggests to define a signal representations by aggregating different haar orthogonal scattering transforms .",
    "we shall see that this bagging strategy is indeed improving classification accuracy .",
    "a free orthogonal haar scattering transform of depth @xmath49 is computed with a pairing at each of the @xmath49 network layers , which may be chosen freely .",
    "we now explain how to optimize these pairings from @xmath102 unlabeled examples @xmath103 . as previously explained , an orthogonal haar scattering is strongly contractive .",
    "each linear haar operator rotates the signal space , and the absolute value suppresses the sign of each difference , and hence projects coefficients over a smaller domain . optimizing the network thus amounts to find the best directions along which to perform the space compression .",
    "contractions reduce the space volume and hence the variance of scattering vectors but it may also collapse together examples which belong to different classes . to maximize the `` average discriminability '' among signal examples , we shall thus maximize the variance of the scattering transform over the training set . following @xcite , we show that it yields a representation whose coefficients are sparsely excited .",
    "the network layers are optimized with a greedy layerwise strategy similar to many deep unsupervised learning algorithms @xcite , which consists in optimizing the network parameters layer per layer , as the depth @xmath4 increases .",
    "let us suppose that haar scattering operators @xmath104 are computed for @xmath105 .",
    "one can thus compute @xmath3 for any @xmath75 .",
    "we now explain how to optimize @xmath5 to maximize the variance of the next layer @xmath21 .",
    "the non - normalized empirical variance of @xmath106 over the training set @xmath107 is @xmath108 the following proposition , adapted from @xcite , proves that the scattering variance decreases as the depth increases , up to a factor @xmath76 .",
    "it gives a condition on @xmath5 to maximize the variance of the next layer .",
    "for any @xmath109 and @xmath75 , @xmath110 .",
    "maximizing @xmath111 given @xmath3 is equivalent to finding @xmath5 which minimizes @xmath112    since @xmath113 and @xmath114 , we have @xmath115 optimizing @xmath111 is thus equivalent to minimizing ( [ l1energy0 ] ) .",
    "moreover , @xmath116 which proves the first claim of the proposition .",
    "this propocsition relies on the energy conservation @xmath117 .",
    "because of the contraction of the absolute value , it proves that the variance of the normalized scattering @xmath118 decreases as @xmath4 increases .",
    "moreover the maximization of @xmath119 amounts to minimize a mixed @xmath120 and @xmath121 norm on @xmath122 , where the sparsity @xmath120 norm is along the realization index @xmath123 where as the @xmath121 norm is along the feature index @xmath47 of the scattering vector .    minimizing the first @xmath120 norm for @xmath47 fixed tends to produce a coefficient indexed by @xmath47 which is sparsely excited across the examples indexed by @xmath123 .",
    "it implies that this feature is discriminative among all examples .",
    "on the contrary , the @xmath121 norm along the index @xmath47 has a tendency to produce @xmath120 sparsity norms which have a uniformly small amplitude .",
    "the resulting `` features '' indexed by @xmath47 are thus uniformly sparse .",
    "because @xmath5 preserves the norm , the total energy of coefficients is conserved : @xmath124 it results that a sparse representation along the index @xmath123 implies that @xmath122 is also sparse along @xmath47 .",
    "the same type of result is thus obtained by replacing the mixed @xmath120 and @xmath121 norm ( [ l1energy0 ] ) by a simpler @xmath120 sparsity norm along both the @xmath123 and @xmath47 variables @xmath125 this sparsity norm is often used by sparse autoencoders for unsupervised learning of deep networks @xcite . numerical results in section [ sec : exp ]",
    "verify that both norms have very close classification performances .",
    "for haar operators @xmath5 , the @xmath120 norm leads to a simpler interpretation of the result .",
    "indeed a haar filtering is defined by a pairing @xmath44 of @xmath0 integers @xmath70 .",
    "optimizing @xmath5 amounts to optimize @xmath44 , and hence minimize @xmath126 but @xmath127 does not depend upon the pairing @xmath44 . minimizing the @xmath120 norm ( [ l1energy ] )",
    "is thus equivalent to minimizing latexmath:[\\[\\label{l1energy2 } \\sum_n \\sum_i     average variation within pairs , and thus tries to regroup pairs having close values .",
    "finding a linear operator @xmath5 which minimizes ( [ l1energy0 ] ) or ( [ l1energy ] ) is a `` dictionary learning '' problem which is in general an np hard problem . for a haar dictionary , we show that it is equivalent to a pair matching problem and can thus be solved with @xmath129 operations . for both optimization norms ,",
    "it amounts to finding a pairing @xmath44 which minimizes an additive cost @xmath130 where @xmath131 for ( [ l1energy ] ) and @xmath132 for ( [ l1energy0 ] ) .",
    "this linear pairing cost is minimized exactly by the blossom algorithm with @xmath129 operations .",
    "greedy method obtains a @xmath133-approximation in @xmath134 time @xcite .",
    "randomized approximation similar to @xcite could also be adapted to achieve a complexity of @xmath135 for very large size problems .",
    "theorem [ thm : multi - layer - recovery2 ] proves that several haar scattering transforms are necessary to obtain a complete signal representation .",
    "we learn @xmath136 haar scattering transforms by dividing the training set @xmath137 in @xmath136 non - overlapping subsets .",
    "a different haar scattering transform is optimized for each training subset .",
    "next section describes a supervised classifier applied to the resulting bag of @xmath136 haar scattering transforms .",
    "strong invariants are computed by the supervised classifier which essentially computes adapted linear combinations of haar scattering coefficients .",
    "bagging @xmath136 orthogonal haar scattering representations defines a set of @xmath138 scattering coefficients .",
    "a supervised dimension reduction is first performed by selecting a subset of scattering coefficients .",
    "it is implemented with an orthogonal least square forward selection algorithm @xcite .",
    "the final supervised classification is implemented with a gaussian kernel svm classifier applied to this reduced set of coefficients .",
    "we select @xmath139 scattering coefficient to discriminate each class @xmath140 from all other classes , and decorrelate these features before applying the svm classifier . discriminating a class @xmath140 from all other classes",
    "amounts to approximating the indicator function @xmath141    let us denote by @xmath142 the dictionary of @xmath143 scattering coefficients to which is added the constant @xmath144 .",
    "an orthogonal least square linearly approximates @xmath145 with a sparse subset of @xmath139 scattering coefficients @xmath146 which are greedily selected one at a time . to avoid correlations between selected features , it includes a gram - schmidt orthogonalization which decorrelates the scattering dictionary relatively to previously selected features .",
    "we denote by @xmath147 the scattering dictionary , which was orthogonalized and hence decorrelated relatively to the first @xmath148 selected scattering features . for @xmath149",
    ", we have @xmath150 . at the @xmath151 iteration ,",
    "we select @xmath152 which yields the minimum linear mean - square error over training samples : @xmath153 because of the orthonormalization step , the linear regression coefficients are @xmath154 and @xmath155 the error ( [ error ] ) is thus minimized by choosing @xmath156 having a maximum correlation : @xmath157 the scattering dictionary is then updated by orthogonalizing each of its element relatively to the selected scattering feature @xmath158 : @xmath159    this orthogonal least square regression greedily selects the @xmath139 decorrelated scattering features @xmath160 for each class @xmath140 . for a total of @xmath161 classes , the union of all these features defines a dictionary of size @xmath162 .",
    "they are linear combinations of the original haar scattering coefficients @xmath163 . in the context of a deep neural network",
    ", this dimension reduction can be interpreted as a last fully connected network layer , which takes in input @xmath143 scattering coefficients and outputs a vector of size @xmath164 .",
    "the parameter @xmath164 optimizes the bias versus variance trade - off .",
    "it may be set a priori or adjusted by cross validation in order to yield a minimum classification error at the output of the gaussian kernel svm classifier .",
    "a gaussian kernel svm classifier is applied to the @xmath164-dimensional orthogonalized scattering feature vectors .",
    "the euclidean norm of this vector is normalized to @xmath165 . in the applications of section [ sec : exp ]",
    ", @xmath164 is set to @xmath166 and hence remains large . since the feature vectors lies on a high - dimensional unit sphere , the standard deviation @xmath167 of the gaussian kernel svm must be of the order of @xmath165 . indeed , a gaussian kernel svm performs its classification by fitting separating hyperplane over different balls of radius of radius @xmath167 .",
    "if @xmath168 then the number balls covering the unit sphere grows like @xmath169 . since @xmath164 is large , @xmath167 must remain in the order of @xmath165 to insure that there are enough training samples to fit a hyperplane in each ball .",
    "signals such as images are sampled on uniform grids .",
    "many classification problems are translation invariant , which motivates the calculation of translation invariant representations .",
    "a translation invariant representation can be computed by averaging signal samples , but it removes too much information .",
    "wavelet scattering operators @xcite are calculated by cascading wavelet transforms and absolute values .",
    "each wavelet transform computes multiscale signal variations on the grid .",
    "it yields a large vector of coefficients , whose spatial averaging defines a rich set of translation invariant coefficients .",
    "data vectors may be defined on non - uniform graphs @xcite , for example in social , financial or transportation networks .",
    "a graph displacement moves data samples on the grid but is not equivalent to a uniform grid translation .",
    "orthogonal haar scattering transforms on graphs are computed from local multiscale signal variations on the graph .",
    "the calculation of displacement invariant features is left to the final supervised classifier , which adapts the averaging to the classification problem .",
    "section [ stransdsec ] introduces this har scattering on a graph as a particular case of orthogonal haar scattering .",
    "section [ haarwave ] proves that an orthogonal haar scattering on graphs can be written as a product of wavelet transforms , as usual wavelet scattering operators .",
    "when the graph connectivity is unknown , unsupervised learning can calculate a haar scattering on the unknown graph and estimate the graph connectivity .",
    "the consistency of such estimations is studied in section [ subsec : learn - connect ] .       by pairing the rows of the previous layer @xmath3 . for each pair of rows",
    ", it stores their sum and the absolute values of their difference , in a twice bigger row . ]",
    "0.55   ( a , b ) : two different examples of hierarchical partitions of a graph into connected sets @xmath170 of size @xmath171 , for @xmath172 ( green ) , @xmath173 ( purple ) and @xmath174 ( red ) .",
    "( c ) hierarchical partitions on a square image grid .",
    ", title=\"fig : \" ]    0.55   ( a , b ) : two different examples of hierarchical partitions of a graph into connected sets @xmath170 of size @xmath171 , for @xmath172 ( green ) , @xmath173 ( purple ) and @xmath174 ( red ) .",
    "( c ) hierarchical partitions on a square image grid .",
    ", title=\"fig : \" ]    0.30   ( a , b ) : two different examples of hierarchical partitions of a graph into connected sets @xmath170 of size @xmath171 , for @xmath172 ( green ) , @xmath173 ( purple ) and @xmath174 ( red ) .",
    "( c ) hierarchical partitions on a square image grid .",
    ", title=\"fig : \" ]    the free orthogonal haar scattering transform of section [ sec:2 ] freely associates any two elements of an internal network layer @xmath3 .",
    "a haar scattering on a graph is constructed by pairing elements according to their position in the graph , which requires to structure the pairing and the network layers .",
    "we denote by @xmath175 the set of @xmath0 vertices of this graph , and assume that @xmath0 is a power of 2 .",
    "the vector @xmath3 of size @xmath0 is structured as a two - dimensional array @xmath176 of size @xmath177 . for each @xmath109",
    ", we shall see that @xmath178 is a `` spatial '' index of a set of @xmath170 of @xmath171 graph vertices .",
    "the @xmath171 parameters @xmath179 are indexing different permutation invariant coefficients computed from the values of @xmath19 in @xmath170 .",
    "the input network layer is @xmath180 .",
    "we compute @xmath21 by pairing the @xmath181 rows of @xmath6 .",
    "the row pairing @xmath182 is pairing each @xmath183 with @xmath184 for @xmath185 .",
    "it imposes a row structure on the free pairing of section [ freesec ] .",
    "applying the absolute haar filter ( [ permasn ] ) to each pair gives @xmath186 and @xmath187 applying these equation for @xmath188 defines a structured haar network illustrated in figure [ fig:1 - 2 ] .",
    "if we remove the absolute value from ( [ eqn22 ] ) then these equations iterate linear haar filters and define an orthogonal walsh transform @xcite .",
    "the absolute value completely modifies the properties of this transform but section [ haarwave ] proves that it can be still be written as a product of orthogonal haar wavelet transforms , alternating with absolute value non - linearities .",
    "the following proposition proves that this structured haar scattering is a transformation on a hierarchical grouping of the graph vertices , derived from the row pairing ( [ pairing ] ) .",
    "let @xmath189 for @xmath190 .",
    "for any @xmath109 and @xmath191 , we define @xmath192 we verify by induction on @xmath4 that it defines a partition @xmath193 , where each @xmath170 is a set of @xmath171 vertices .",
    "[ thm : vjn ] the coefficients @xmath194 are computed by applying a hadamard matrix to the restriction of @xmath19 to @xmath195 .",
    "this hadamard matrix depends on @xmath19 , @xmath49 and @xmath47 .",
    "theorem [ prop : norm - preserve ] proves that @xmath196 is computed by applying am orthogonal transform to @xmath19 . to prove that it is a hadamard matrix ,",
    "it is sufficient to show that its entries are @xmath61 .",
    "we verify by induction on @xmath197 that @xmath198 only depends on restriction of @xmath19 to @xmath170 , by applying ( [ eqn22 ] ) and ( [ eqn12 ] ) together with ( [ regroup ] ) .",
    "we also verify that each @xmath199 for @xmath200 appears exactly once in the calculation , with an addition or a subtraction .",
    "because of the absolute value , the addition or subtraction which are @xmath165 and @xmath201 in the hadamard matrix , which therefore depends upon @xmath19 , @xmath49 and @xmath47    an orthogonal haar scattering on a graph can thus be interpreted as an adaptive hadamard transform over groups of vertices , which outputs positive coefficients .",
    "walsh matrices are particular cases of hadamard matrices .",
    "the induction ( [ regroup ] ) defines sets @xmath170 with connected nodes in the graph if for all @xmath4 and @xmath47 , each pair @xmath202 regroups two sets @xmath203 and @xmath204 which are connected .",
    "it means that at least one element of @xmath203 is connected to one element of @xmath204 .",
    "there are many possible connected dyadic partitions of any given graph .",
    "figure [ fig:2](a , b ) shows two different examples of connected graph partitions .    for images sampled on a square grid ,",
    "a pixel is connected with @xmath205 neighbors .",
    "a structured haar scattering can be computed by pairing neighbor image pixels , alternatively along rows and columns as the depth @xmath4 increases .",
    "when @xmath4 is even , each @xmath170 is then a square group of @xmath171 pixels , as illustrated in figure [ fig:2](c ) .",
    "shifting such a partition defines a new partition .",
    "neighbor pixels can also be grouped in the diagonal direction which amounts to rotate the sets @xmath170 by @xmath25 to define a new dyadic partition .",
    "each of these partitions define a different structured haar scattering .",
    "section [ sec : exp ] applies these structured haar image scattering to image classification .",
    "scattering coefficients have very different properties depending upon the number of absolute values which are used to compute them . a scattering coefficient of order @xmath206 is a coefficient computed by cascading @xmath206 absolute values .",
    "their amplitude have a fast decay as the order @xmath206 increases , and their locations are specified by the following proposition .",
    "[ musdfords ] if @xmath207 then @xmath198 is a coefficient of order @xmath208 . otherwise , @xmath198 is a coefficient of order @xmath209 if there exists @xmath210 such that @xmath211 there are @xmath212 coefficients of order @xmath206 in @xmath3 .",
    "this proposition is proved by induction on @xmath4 .",
    "for @xmath213 all coefficients are of order @xmath208 since @xmath214 .",
    "if @xmath198 is of order @xmath206 then ( [ eqn12 ] ) and ( [ eqn22 ] ) imply that @xmath215 is of order @xmath206 and @xmath216 is of order @xmath217 .",
    "it results that ( [ coeffssd ] ) is valid for @xmath218 if is valid for @xmath4 .",
    "the number of coefficients @xmath198 of order @xmath206 corresponds to the number of choices for @xmath179 and hence for @xmath210 , which is @xmath219 .",
    "this must be multiplied by the number of indices @xmath47 which is @xmath220 .",
    "the amplitude of scattering coefficients typically decreases exponentially when the scattering order @xmath206 increases , because of the contraction produced by the absolute value .",
    "high order scattering coefficients can thus be neglected .",
    "this is illustrated by considering a vector @xmath19 of independent gaussian random variables of variance @xmath165 .",
    "the value of @xmath198 only depends upon the values of @xmath19 in @xmath170 . since @xmath170 does not intersect with @xmath221",
    "if @xmath222 , we derive that @xmath223 and @xmath224 are independent .",
    "they have same mean and same variance because @xmath19 is identically distributed .",
    "scattering coefficients are iteratively computed by adding pairs of such coefficients , or by computing the absolute value of their difference . adding two independent random variables multiplies their variance by @xmath76 . subtracting two independent random variables of same mean and variance",
    "yields a new random variable whose mean is zero and whose variance is multiplied by @xmath76 . taking the absolute value reduces the variance by a factor which depends upon its probability distribution . if this distribution is gaussian then this factor is @xmath225 . if we suppose that this distribution remains approximately gaussian , then applying @xmath206 absolute values reduces the variance by approximately @xmath226",
    "since there are @xmath227 coefficients of order @xmath206 , their total normalized variance @xmath228 is approximated by @xmath229 .",
    "table [ table : gaussianvar ] shows that @xmath230 is indeed of the same order of magnitude as the value @xmath231 computed numerically .",
    "this variance becomes much smaller for @xmath232 .",
    "this observation remains valid for large classes of signals @xmath19 .",
    "scattering coefficients of order @xmath232 usually have a negligible energy and are thus removed in classification applications .    .",
    "[ table : gaussianvar ] @xmath231 is the normalized variance of all order @xmath206 coefficients in @xmath18 , computed for a gaussian white noise @xmath19 with @xmath233 .",
    "it is decays approximately like @xmath234 .",
    "[ cols=\"^,^,^,^,^,^\",options=\"header \" , ]      a data basis of irregularly sampled images on a sphere is provided in @xcite .",
    "it is constructed by projecting the mnist image digits on @xmath235 points randomly sampled on the 3d sphere , and by randomly rotating these images on the sphere .",
    "the random rotation is either uniformly distributed on the sphere or restricted with a smaller variance ( small rotations ) @xcite .",
    "the digit ` 9 ' is removed from the data set because it can not be distinguished from a ` 6 ' after rotation .",
    "examples sphere digits are shown in figure [ fig : spheremnist ] .",
    "this geometry of points on the sphere can be described by a graph which connects points having a sufficiently small distance on the sphere .",
    "the classification algorithms introduced in @xcite take advantage of the known distribution of points on the sphere , with a representation based on the graph laplacian .",
    "table [ table : smnist ] gives the results reported in @xcite , with a fully connected neural network , and with a spectral graph laplacian network .",
    "as opposed to these algorithms , the unsupervised structured haar scattering algorithm does not use this geometric information and learns the graph information by pairing .",
    "computations are performed on a scrambled set of signal values .",
    "haar scattering transforms are calculated up to the maximum scale @xmath236 .",
    "a total of @xmath237 connected dyadic partitions are estimated by unsupervised learning , and the classification is performed from @xmath238 selected coefficients .",
    "although the graph geometry is unknown , the structured haar scattering reduces the error rate both for small and large 3d random rotations .",
    "in this case a free orthogonal haar scattering has a smaller error rate than a structured haar scattering for small rotations , but a larger error for large rotations .",
    "it illustrates the tradeoff between the structural bias and the feature variance in the choice of the algorithms . for small rotation ,",
    "the variability within classes is smaller and a free scattering can take advantage of more degrees of freedom . for large rotations ,",
    "the variance is too large and dominates the problem .",
    "two points of the sphere of radius @xmath165 are considered to be connected if their geodesic distance is smaller than @xmath239 . with this convention , over the 4096 points , each point has on average @xmath205 connected neighbors .",
    "the unsupervised haar learning performs a hierachical pairing of points on the sphere . for small and large rotations , the percentage of connected sets @xmath170 remains above @xmath240 for @xmath241 .",
    "this is computed over @xmath242 of the points points having a nonneglegible energy .",
    "it shows that the multiscale geometry on the sphere is well estimated by hierachical pairings .",
    "this work was supported by the erc grant invariantclass 320959 .",
    "10    f.  anselmi , j.  z. leibo , l.  rosasco , j.  mutch , a.  tacchetti , and t.  poggio .",
    "unsupervised learning of invariant representations in hierarchical architectures . , 2013 .",
    "y.  bengio , a.  courville , and p.  vincent .",
    "representation learning : a review and new perspectives .",
    ", 35(8):17981828 , 2013 .",
    "j.  bruna and s.  mallat .",
    "invariant scattering convolution networks . , 35(8):18721886 , 2013 .",
    "j.  bruna , w.  zaremba , a.  szlam , and y.  lecun .",
    "spectral networks and deep locally connected networks on graphs . , 2014 .",
    "s.  chen , c.  f. cowan , and p.  m. grant .",
    "orthogonal least squares learning algorithm for radial basis function networks .",
    ", 2(2):302309 , 1991 .    c.  coifman , y.  meyer , and m.  wickerhauser",
    ". wavelet analysis and signal processing .",
    "pages 153178 , 1992 .",
    "m.  gavish , b.  nadler , and r.  r. coifman .",
    "multiscale wavelets on trees , graphs and high dimensional data : theory and applications to semi supervised learning .",
    "pages 367374 , 2010 .",
    "i.  j. goodfellow , d.  warde - farley , m.  mirza , a.  courville , and y.  benjio .",
    "maxout networks . , 2013 .",
    "g.  hinton , s.  osindero , and y .- w .",
    "teh . a fast learning algorithm for deep belief nets .",
    ", 18(7):15271554 , 2006 .",
    "g.  e. hinton , n.  srivastava , a.  krizhevsky , i.  sutskever , and r.  salakhutdinov .",
    "improving neural networks by preventing co - adaptation of feature detectors . , 2012 .",
    "y.  jia , c.  huang , and t.  darrell . beyond spatial pyramids : receptive field learning for pooled image features . , pages 33703377 , 2012 .",
    "w. jones , a.  osipov , and v.  rokhlin .",
    "randomized approximate nearest neighbors algorithm .",
    ", 108(38):1567915686 , 2011 .",
    "k.  labusch , e.  barth , and martinetz .",
    "t. simple method for highperformance digit recognition based on sparse coding . , 19(11):19851989 , 2008 .",
    "q.  le , t.  sarlos , and a.  smola .",
    "fastfood - approximating kernel expansions in loglinear time . , 2013 .",
    "y.  lecun , k.  kavukvuoglu , and c.  farabet .",
    "convolutional networks and applications in vision . , 2010 .",
    "lee , s.  xie , p.  gallagher , z.  zhang , and z.  tu . deeply supervised nets . , 2014 .",
    "lin and h .-",
    "stable and efficient representation learning with nonnegativity constraints . ,",
    "2014 .    s.  mallat .",
    "group invariant scattering .",
    ", 65(10):13311398 , 2012 .",
    "s.  mallat and i.  waldspurger .",
    "deep learning by scattering . , 2013 .",
    "b.  maurey . some deviation inequalities .",
    ", 1(2):188197 , 1991 .",
    "j.  ngiam , z.  chen , d.  chia , p.  w. koh , q.  v. le , and a.  y. ng . tiled convolutional neural networks . in _ advances in neural information processing systems _ , pages 12791287 , 2010 .",
    "e.  oyallon and s.  mallat .",
    "deep roto - translation scattering for object classification . , 2014 .",
    "g.  pisier .",
    "probabilistic methods in the geometry of banach spaces . ,",
    "pages 167241 , 1985 .",
    "r.  preis .",
    "linear time 1/2-approximation algorithm for maximum weighted matching in general graphs .",
    "pages 259269 , 1999 .",
    "s.  rifai , p.  vincent , x.  muller , x.  glorot , and y.  bengio .",
    "contractive auto - encoders : explicit invariance during feature extraction . in _ proceedings of the 28th international conference on machine learning ( icml-11 ) _ , pages 833840 , 2011 .",
    "r.  rustamov and l.  guibas .",
    "wavelets on graphs via deep learning .",
    "pages 9981006 , 2013 .",
    "p.  sermanet , d.  eigen , x.  zhang , m.  mathieu , r.  fergus , and y.  lecun .",
    "overfeat : integrated recognition , localization and detection using convolutional networks . , 2013 .",
    "d.  i. shuman , s.  k. narang , p.  frossard , a.  ortega , and p.  vandergheynst .",
    "the emerging field of signal processing on graphs : extending high - dimensional data analysis to networks and other irregular domains .",
    ", 30(3):8398 , 2013 .",
    "d.  yu and l.  deng .",
    "deep convex net : a scalable architecture for speech pattern classification .",
    ", pages 22852288 , 2011 .",
    "we derive from the definition of a scattering transform in equations ( 3,4 ) in the text that @xmath243 where @xmath244 .",
    "define @xmath245 .",
    "observe that @xmath246 thus @xmath247 is calculated from the coefficients @xmath248 of the previous layer with @xmath249 since @xmath250 , the coefficient @xmath251 is calculated from @xmath252 by @xmath253 times additions , and thus @xmath254 combining equations ( [ eqn : fromjmtojm+1 - 1 ] ) and ( [ eqn : jm+1 ] ) gives @xmath255 we go from the depth @xmath256 to the depth @xmath257 by computing @xmath258 together with ( [ eqn : jm+1b ] ) it proves the equation ( [ propsdfnsd ] ) of the proposition .",
    "the summation over @xmath259 comes from the inner product @xmath260 .",
    "this also proves that @xmath261 is the index of a coefficient of order @xmath217 .",
    "the theorem is proved by analyzing the concentration of the objective function around its expected value as the sample number @xmath102 increases .",
    "we firstly introduce the pisier and maurey s version of the gaussian concentration inequality for lipschitz functions .",
    "[ prop : gaussian - concentration - lipschitz ] let @xmath262 be i.i.d @xmath263 random variabls , and @xmath264 a 1-lipschitz function , then there exists @xmath265 so that @xmath266 < \\exp\\{-c_{0}t^{2 } \\}~ \\mbox{and}~   { \\mathbf{pr } } [ f - { \\mathbb{e}}f < - t]<\\exp\\{-c_{0}t^{2 } \\ } , \\quad\\forall t>0.\\ ] ]      to prove the theorem , recall that the pairing problem is computed by minimizing the @xmath120 norm ( [ msdifnsdf2 ] ) which up to a normalization amounts to compute : @xmath269 where @xmath270 is a pairing of @xmath0 elements and we denote by @xmath271 the set of all possible such pairings .",
    "the following lemma proves that @xmath272 is a lipschitz function of independent gaussian random variables , with a lipschitz constant equal to @xmath273 , where @xmath274 is the operator norm of the covariance",
    ". we prove it on the normalized function @xmath275 .",
    "[ lemma : f - lipschitz ] let @xmath276 with @xmath277 i.i.d .",
    "given any pairing @xmath278 , define @xmath279 then @xmath280 is a lipschitz function with constant @xmath281 , which does not depend on @xmath270 .    with slight abuse of notation ,",
    "denote by @xmath282 if two nodes @xmath283 and @xmath284 are paired by @xmath270 , then we have @xmath285 where @xmath286 is a vector of length @xmath0 whose entries are @xmath287",
    ". then @xmath288 and it follows that @xmath289    observe that the eigenvalues of @xmath290 are the discrete fourier transform coefficients of the periodic correlation function @xmath291 @xmath292 observe that @xmath293 for each @xmath123 , that is , @xmath294 is orthogonal to the eigenvector of @xmath295 .",
    "so the lipschitz constant @xmath296 can be slightly improved to be @xmath297 .",
    "let us now prove the claim of theorem [ thm : n > dlogd ] . since the pairing has a probability larger than @xmath298 to be connected if @xmath299",
    "< \\epsilon$ ] , we need to show that under the inequality ( [ theoansdf ] ) the probability @xmath300 $ ] is less than @xmath301 .",
    "let us denote @xmath302 and define @xmath303 then eqn . ( [ theoansdf ] ) can be rewritten as @xmath304    as a result of proposition [ prop : gaussian - concentration - lipschitz ] and lemma [ lemma : f - lipschitz ] , if @xmath305 then @xmath306 , @xmath307<\\exp\\{-c\\delta^{2}\\ } ~\\mbox{and}~ \\ , { \\mathbf{pr } } [ f(\\pi ) -   { \\mathbb{e}}f(\\pi )   < - \\delta]<\\exp\\{-c\\delta^{2}\\ } , \\quad\\forall\\delta>0.\\ ] ] observe that @xmath308 where @xmath309 are the set of pairings which have @xmath206 non - neighbor pairs .",
    "@xmath310 is the set of pairings which only pair connected nodes in the graph , and for the ring graph @xmath311 the two of which interlace .",
    "for any @xmath312 , suppose that there are @xmath313 pairs in @xmath314 so that the distance between the two paired nodes is @xmath315 , @xmath316 , @xmath317 .",
    "recalling the definition of @xmath318 in eq .",
    "( [ alphadef ] ) , we verify that @xmath319 when @xmath320 , and @xmath321 thus when @xmath320 , @xmath322 define @xmath323 and we have that @xmath324    & = & { \\mathbf{pr}}[\\exists \\pi \\in\\bigcup_{m=1}^{d/2}\\pi_{d}^{(m)},\\ ,    f ( \\pi ) < \\min\\{f(\\pi_{0}^{(0)}),f ( \\pi_{1}^{(0)})\\}]\\nonumber \\\\   & \\le & { \\mathbf{pr}}[f(\\pi_{0}^{(0)})>{\\mathbb{e}}f(\\pi_{0}^{(0)})+\\delta_{1}]\\nonumber \\\\   &   &   + { \\mathbf{pr}}[f(\\pi_{0}^{(0)})<{\\mathbb{e}}f(\\pi_{0}^{(0)})+\\delta_{1},\\ ,      \\exists \\pi \\in \\bigcup_{m=1}^{d/2}\\pi_{d}^{(m)},\\ , f ( \\pi ) < f(\\pi_{0}^{(0)})]\\nonumber \\\\   & \\le & { \\mathbf{pr}}[f(\\pi_{0}^{(0 ) } ) > { \\mathbb{e}}f(\\pi_{0}^{(0)})+\\delta_{1 } ] \\nonumber \\\\   &   & + { \\mathbf{pr}}[\\exists \\pi \\in\\bigcup_{m=1}^{d/2}\\pi_{d}^{(m)},\\ , f(\\pi ) < { \\mathbb{e}}f(\\pi ) -\\delta_{m}],\\ ,          ( \\text{by that } ( \\bar{\\alpha}_{2}-\\alpha_{1})m-\\delta_{1}\\ge\\delta_{m})\\nonumber \\\\   & \\le & \\exp\\{-c\\delta_{1}^{2}\\}+\\sum_{m=1}^{d/2}|\\pi_{d}^{(m)}|\\exp\\{-c\\delta_{m}^{2}\\}\\nonumber \\\\   & = & \\exp\\{-c_{\\rho}\\frac{n}{d}\\ }           + \\sum_{m=1}^{d/2}|\\pi_{d}^{(m)}|\\exp\\{-c_{\\rho}\\frac{n}{d}m^{2}\\ } , \\label{eq : prob - fail1}\\end{aligned}\\ ] ] where @xmath325 is as in eq .",
    "( [ crho ] ) .",
    "one can verify the following upper bound for the cardinal number of @xmath309 : @xmath326 with the crude bound @xmath327 , the above inequality inserted in ( [ eq : prob - fail1 ] ) gives @xmath328 \\le\\exp\\{-c_{\\rho}\\frac{n}{d}\\}+\\sum_{m=1}^{d/2}d^{2m}\\exp\\{-c_{\\rho}\\frac{n}{d}m^{2}\\}. \\label{eq : prob - fail2}\\ ] ] if we keep the factor @xmath329 , the upper bound for the summation over @xmath206 can be improved to be @xmath330 where @xmath331 is an absolute constant . by applying this in the final bound in the theorem , the constant in front of @xmath332 is 2 instead of 3 .",
    "the constant of the theorem is not tight , while the @xmath333 is believed to be the tight order as @xmath0 increases .    to proceed , define the function @xmath334 and observe that @xmath335 whenever @xmath336 which holds as long as eq .",
    "( [ theoansdf-2 ] ) is satisfied .",
    "thus we have @xmath337 then the inequality ( [ eq : prob - fail2 ] ) becomes @xmath338   \\le\\left(\\frac{d^{3}}{2}+1\\right)\\exp\\{-c_{\\rho}\\frac{n}{d}\\ } \\le\\exp\\{-c_{\\rho}\\frac{n}{d}+3\\log d\\}.\\ ] ] to have @xmath299 < \\epsilon$ ] , a sufficient condition is therefore @xmath339 which is reduced to eq .",
    "( [ theoansdf-2 ] ) and equivalently eq . ( [ theoansdf ] ) ."
  ],
  "abstract_text": [
    "<S> an orthogonal haar scattering transform is a deep network , computed with a hierarchy of additions , subtractions and absolute values , over pairs of coefficients . </S>",
    "<S> it provides a simple mathematical model for unsupervised deep network learning . </S>",
    "<S> it implements non - linear contractions , which are optimized for classification , with an unsupervised pair matching algorithm , of polynomial complexity . a structured haar scattering over graph data computes permutation invariant representations of groups of connected points in the graph . </S>",
    "<S> if the graph connectivity is unknown , unsupervised haar pair learning can provide a consistent estimation of connected dyadic groups of points . </S>",
    "<S> classification results are given on image data bases , defined on regular grids or graphs , with a connectivity which may be known or unknown . </S>",
    "<S> deep learning , neural network , scattering transform , haar wavelet , classification , images , graphs   + 2000 math subject classification : 68q32 , 68t45 , 68q25 , 68t05 </S>"
  ]
}