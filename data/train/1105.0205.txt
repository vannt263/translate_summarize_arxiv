{
  "article_text": [
    "many situations lead to the comparison of two random processes . in a parametric case ,",
    "the problem of change detection has been widely studied in the time series literature .",
    "a common problem is to test a change in the mean or in the variance of the time series by using a parametric model ( see for instance @xcite or @xcite , and references therein ) . in the gaussian case comparisons of processes are considered through their covariance structures ( see @xcite , @xcite ) .",
    "these distribution assumptions can be relaxed when the study concerns processes observed through panel data .",
    "this situation is frequently encountered in medical follow - up studies when two groups of patients are observed and compared .",
    "each subject in the study gives rise to a random process @xmath0 denoting the measurement of the patient up to time @xmath1 ( such data are referred to as panel data ) .",
    "in this context , @xcite considered the problem of testing the equality of mean functions and proposed new multi - sample tests for panel count data .    in this paper",
    "we consider the general problem of comparison of two processes which may differ by a transformation of their distributions .",
    "our purpose is to test whether this transformation changes over time . for this , two panels are considered : @xmath2 and @xmath3 , not necessarily independent ; that is , we can have i.i.d .",
    "paired observations @xmath4 with dependence between @xmath5 and @xmath6 .",
    "it is assumed that for each @xmath1 , the @xmath7 ( resp .",
    "@xmath8 are i.i.d .",
    "random variables with common distribution function @xmath9 ( resp .",
    "@xmath10 ) and with support @xmath11 ( resp .",
    "@xmath12 ) .",
    "also we assume that for all @xmath13 there exists monotone transformations @xmath14 such that the following equality in distribution holds : @xmath15 . without loss of generality",
    "we consider that the functions @xmath16 are increasing .",
    "note that if @xmath9 is invertible then there exists a trivial transformation @xmath14 given by @xmath17 .",
    "we are interested in testing whenever this transformation is time independent ; that is , for all @xmath1 , the equality @xmath18 occurs .",
    "a simple illustration is the case where @xmath19 and @xmath20 are gaussian processes with mean @xmath21 and @xmath22 and variance @xmath23 and @xmath24 , respectively . in that case",
    "the function @xmath25 is linear .",
    "more generally , observing both processes @xmath26 and @xmath27 with panel data we want to test @xmath28 it is clear that @xmath29 coincides with the equality in distribution : @xmath30 , for all @xmath1 .",
    "following @xcite ( see also @xcite ) , we construct a non parametric test statistic based on the empirical estimator of @xmath31 , denoted by @xmath32 . we show that @xmath32 is proportional to a brownian bridge under @xmath29 .",
    "when @xmath29 is not rejected , it is of interest to estimate @xmath25 and to interpret its estimator @xmath33 .",
    "then this test can be viewed as a first step permitting to legitimate estimation and interpretation of a constant transformation @xmath25 between the distributions of two samples , possibly paired .",
    "the paper is organized as follows : in section 2 we construct the test statistic . in section 3 we perform a simulation study using a bootstrap procedure to evaluate the finite sample property of the test . the power is evaluated against alternatives where there are smooth scale or position time changes in the process distribution .",
    "section 4 contains brief concluding remarks .",
    "a natural nonparametric estimator of @xmath14 is given by @xmath34where @xmath35 denotes the @xmath36th order statistic and @xmath37 is the empirical distribution function of @xmath38 , that is @xmath39 a nonparametric test is considered to test the variation of @xmath31 . for",
    "@xmath40 , @xmath41 , write @xmath42}\\widehat{h}_{t}(x)-\\frac{[n\\tau ] } { n}\\sum_{t=1}^{n}\\widehat{h}_{t}(x)\\right ) ,   \\label{bn}\\end{aligned}\\]]where @xmath43    for a given square integrable function @xmath44 we define the following test statistic    @xmath45    to establish the limiting distribution of the statistic @xmath46 under the null , we need the following assumptions :    * assumption 1",
    ". there exists @xmath47 such that @xmath48 * assumption 2 .",
    "there exist @xmath49 and @xmath50 such that @xmath51 and @xmath52 for all @xmath53 , where @xmath54 and @xmath55 are the density  functions of @xmath26 and @xmath56 * assumption 3 . for all @xmath57",
    ", there exist @xmath58 such that @xmath59 where @xmath60 * assumption 4 .",
    "@xmath61    assumptions 1 and 2 are standard .",
    "assumption 3 states that the second moments converge on average . if assumption 1 is satisfied , assumption 4 is equivalent to @xmath62 or @xmath63 .",
    "[ theo]let assumptions 1 - 4 hold . then under the null @xmath64 we have the following convergence in distribution @xmath65where @xmath66 , and",
    "@xmath67 is a brownian bridge .",
    "the cumulative distribution function of @xmath68 is given by ( see @xcite ) @xmath69    before proving theorem 1 , we state three lemmas .    [ lem1 ] under assumption 1 we have @xmath70 where @xmath71 is given by ( [ sig ] ) .    [ [ proof ] ] proof + + + + +    @xmath72 is an i.i.d sequence with mean @xmath73 and variance @xmath74 hence an immediate application of the central limit theorem yields @xmath75 by the delta - method the last convergence implies that @xmath76 for @xmath770;1[$ ] fixed , denote by @xmath78 the sample @xmath79-quantile ; that is , @xmath80 , where @xmath81 + 1 $ ] . by theorem 3 of @xcite we obtain @xmath82 let @xmath83 denotes the characteristic function of the random variable @xmath84 and let @xmath85 denotes the conditional characteristic function of the random variable @xmath84 conditional on @xmath86 .",
    "we have @xmath87 where @xmath88",
    "@xmath89then we get @xmath90 \\right ) \\\\ & = & { \\mathbb e}\\left ( \\exp ( iu\\widetilde{h}_{2,t}\\ ) \\ { \\mathbb e}\\left [ \\exp ( iu\\widetilde{h}_{1,t}\\ ) \\mid y_{t}\\right ]",
    "\\right ) .\\end{aligned}\\]]moreover @xmath91 & = & \\phi _ { \\widetilde{h}_{1,t\\mid y_{t}}}(u )   \\label{f1 } \\\\ & = & \\phi _ { n_{x}^{1/2}\\left ( \\widehat{f}_{t}^{-1}(\\widehat{g}_{t}(x))-f_{t}^{-1}(\\widehat{g}_{t}(x))\\right ) \\mid y_{t}}\\left ( \\left ( n_{y}/(n_{x}+n_{y})\\right )",
    "^{1/2}u\\right )   \\notag\\end{aligned}\\]]from ( [ co3 ] ) it follows that , @xmath92 , @xmath93as @xmath94 , where@xmath95the convergence ( [ co1 ] ) yields @xmath96 @xmath97 @xmath98 , as @xmath99 which implies , combined with ( [ f1])-([f2 ] ) , assumption 1 and @xmath100 , that @xmath91 & \\underrightarrow{d } & \\exp \\left ( -\\frac{1}{2}(1-a)u^{2}\\sigma _ { t}^{2}(x)\\right ) ,    \\label{co4}\\end{aligned}\\ ] ] as @xmath94 and @xmath101 .",
    "moreover we have @xmath102.\\end{aligned}\\]]since the function @xmath103 is continuous , then the convergence ( [ co2 ] ) and assumption 1 yield @xmath104where @xmath105 is centered gaussian distributed with variance equal to @xmath106 . from ( [ co4 ] ) and ( [ co5 ] ) it follows that , as @xmath107 and @xmath101 , @xmath108   & & \\nonumber \\\\ \\rightarrow ^{\\!\\!\\!\\!\\!d } \\",
    "\\exp ( iua^{1/2}h_{2,t}\\ ) & & \\exp \\left ( -\\frac{1}{2}(1-a)u^{2}\\sigma _ { t}^{2}(x)\\right ) .",
    "\\label{co6 } \\\\ & &   \\notag\\end{aligned}\\]]since @xmath109 $ ] and @xmath110 are bounded almost surely , it follows from ( [ co6 ] ) that @xmath111 \\right ) \\\\ & \\rightarrow & { \\mathbb e}\\left ( \\exp \\left ( iua^{1/2}h_{2,t}\\right ) \\exp \\left ( -\\frac{1}{2}(1-a)u^{2}\\sigma _ { t}^{2}(x)\\right ) \\right ) \\text { , as } n_{x}\\rightarrow \\infty , n_{y}\\rightarrow \\infty \\\\ & = & \\exp \\left ( -\\frac{1}{2}au^{2}\\sigma _ { t}^{2}(x)\\right ) \\exp \\left ( -\\frac{1}{2}(1-a)u^{2}\\sigma _ { t}^{2}(x)\\right ) \\\\ &",
    "= & \\exp \\left ( -\\frac{1}{2}u^{2}\\sigma _ { t}^{2}(x)\\right),\\end{aligned}\\]]therefore the desired conclusion ( [ nor ] ) holds .",
    "+    lemma [ lem1 ] implies that @xmath112 where @xmath113 is given by ( [ sig ] ) , @xmath114 is a standard gaussian white noise and the remainder term @xmath115 is such that @xmath116 let @xmath117 $ ]  be the space of random functions that are right - continuous and have left limits , endowed with the skorohod topology .",
    "the weak convergence of a sequence of random elements @xmath118 in @xmath119 to a random element @xmath84 in @xmath119 will be denoted by @xmath120 let @xmath121}\\sigma _ { 1,t}(x)\\varepsilon _ { t},\\text { \\ \\ \\ } \\tau \\in \\lbrack 0,1 ] .",
    "\\label{xy}\\end{aligned}\\ ] ]    [ lem2 ] under assumptions 1 - 3 we have @xmath122where @xmath123 stands for the standard brownian motion .    [ [ proof-1 ] ] proof + + + + +    assumption 2 implies that @xmath124for some positive constant @xmath125 and @xmath126 and @xmath127 large enough .  hence @xmath113 is a bounded deterministic sequence , therefore the weak convergence ( [ wn ] ) follows from theorem a.1 of @xcite .",
    "+    [ lem3 ] under the null @xmath64 , as @xmath128 , @xmath94 and @xmath101 we have @xmath129    [ [ proof-2 ] ] proof + + + + +    under the null @xmath64 : @xmath130 the equality ( mo1 ) becomes @xmath131 let @xmath132 ,  @xmath133 then by using the same argument as in theorem 1 of @xcite we obtain@xmath134 we have @xmath135 where @xmath136 from ( [ rt ] ) it follows that @xmath137 which implies that @xmath138 by using the cauchy shwartz inequality , we have @xmath139 hence by using ( [ cvy ] ) and ( [ rt2 ] ) we get @xmath140 the desired conclusion ( [ csig ] ) holds by combining ( [ cvy])-([yr ] ) .",
    "+    [ [ proof - of - theorem-1 ] ] proof of theorem 1 + + + + + + + + + + + + + + + + + +    under the null , the process @xmath141 in ( [ bn ] ) can be rewritten as @xmath142}\\sigma _ { 1,t}(x)\\varepsilon _ { t}-\\frac{[n\\tau ] } { n}\\sum_{t=1}^{n}\\sigma _ { 1,t}(x)\\varepsilon _ { t}\\right ) + r_{n}(\\tau , x ) \\\\ & = & \\frac{1}{\\widehat{\\sigma } _ { n}}\\left ( w_{n}(\\tau ) -\\frac{[n\\tau ] } { n}w_{n}(1)\\right ) + r_{n}(\\tau , x),\\end{aligned}\\ ] ] where the remainder term @xmath143 is given by @xmath144}r_{t}-\\frac{[n\\tau ] } { n}\\sum_{t=1}^{n}r_{t}\\right).\\end{aligned}\\ ] ] now observe that @xmath145}r_{t}&=&o_{p}\\left ( [ n\\tau ] \\left ( ( n_{x}+n_{y})/n_{x}n_{y}\\right ) ^{1/2}\\right ) , \\end{aligned}\\]]which together with ( [ csig ] ) implies that@xmath146 hence @xmath147}{n}w_{n}(1)\\right ) + o_{p}(1),\\end{aligned}\\ ] ] which combined with ( [ wn ] ) and ( [ csig ] ) yields @xmath148where @xmath149 is a brownian bridge .",
    "therefore @xmath150let @xmath151 be the space of square integrable functions endowed with the uniform norm @xmath152 for a given square integrable function @xmath44 , the functional @xmath153 : @xmath154 defined by@xmath155 is continuous . to obtain the convergence ( [ snf ] )",
    "it is sufficient to apply ( [ cbn ] ) and the continuous mapping theorem",
    "for simplicity we consider @xmath156 .",
    "data are generated from three models : first , @xmath20 is normally distributed with mean @xmath157 and variance @xmath158 , and @xmath19 is generated independently by the transformation @xmath159 , where @xmath160 is another gaussian process with mean @xmath157 and variance @xmath158 .",
    "second , @xmath20 is an autoregressive process of order 1 ( ar1 ) with correlation coefficient equal to 0.5 , and @xmath19 is generated independently by the transformation @xmath159 , where @xmath160 is another ar1 process .",
    "for the last model random variables are paired : @xmath20 are independent gaussian variables with mean @xmath157 and variance @xmath158 , and @xmath161 , that is , the time transformation is on the random variables .",
    "it is clear that this implies the same transformation for the corresponding distributions .",
    "[ [ alternatives . ] ] alternatives .",
    "+ + + + + + + + + + + + +    the following five alternatives are considered     + * first alternative : a1 * + change in the mean . @xmath162 .",
    "+ * second alternative : a2 * + change in the variance .",
    "@xmath163 .",
    "+ * third alternative : a3 *   + jump . @xmath164 , + where @xmath165 if @xmath166 and 0 otherwise .   + * fourth alternative : a4 * + smooth change in the mean .",
    "+ * fifth alternative : a5 * + smooth change in the mean .",
    "@xmath168     + all alternatives are smooth and are less rough than classical rupture on the mean or on the variance , except a3 which coincides with a jump on the mean .",
    "the first two alternatives a1-a2 tend quickly to the null model under @xmath29 when the length @xmath169 increases .",
    "figure [ figa1 ] illustrates the proximity of @xmath31 to a constant for large times length in the case of alternative a1 . in opposition",
    ", alternatives a4-a5 are very smooth and converge slowly to the null model .",
    "figure [ figa4 ] illustrates this smooth convergence under alternative a4 .    [ [ bootstrap - procedure . ] ] bootstrap procedure .",
    "+ + + + + + + + + + + + + + + + + + + +    to evaluate the power of our testing procedure we first consider a monte carlo statistic . given @xmath170 points @xmath171 in @xmath12 we consider @xmath172 where @xmath173with @xmath174 the convergence of the statistic @xmath175 is not guaranteed since the @xmath176 are dependent . to carry out this problem ,",
    "a bootstrap procedure is proposed .",
    "we construct a naive bootstrap statistic ; that is , the test statistic @xmath175 given in ( [ stat ] ) is compared to the empirical bootstrapped distribution obtained from @xmath177 , with @xmath178 constructed from the bootstraped sample drawn randomly with replacement and satisfying the size equalities @xmath179 and @xmath180 .",
    "we fix @xmath44 as a constant .",
    "note that if @xmath84 and @xmath86 are paired , the bootstrap procedure consists in drawing randomly with replacement @xmath181 pairs @xmath182 from the data .",
    "we fix @xmath183 bootstrap replications .",
    "[ [ powers . ] ] powers .",
    "+ + + + + + +    for each alternative , the test statistic is computed , based on sample sizes @xmath184 for a theoretical level @xmath185 .",
    "the lengths of time s intervals are @xmath186 and @xmath187 ; that is , the function @xmath31 is observed @xmath181 times for each @xmath1 varying in @xmath188 $ ] , or @xmath189 $ ] , or @xmath190 $ ] , with a step equal to one .",
    "the empirical power of the test is defined as the percentage of rejection of the null hypothesis over @xmath191 replications of the test statistic under the alternative .",
    "figure [ fig1 ] presents empirical powers of the bootstrap test for all alternatives , in the case where @xmath19 are independent standard gaussian variables .",
    "solid lines and dotted lines correspond to @xmath192 and @xmath193 respectively .",
    "it can be observed that the power decreases with the length for alternatives a1 and a2 .",
    "it is in accordance with the previous remark : @xmath31 is close to the null hypothesis for relatively large values of @xmath169 .",
    "then passing from a time length equal 20 to a time length equal to 200 corresponds to adding variables with nearly constant transformation in distribution ( see figure [ figa1 ] ) .",
    "alternatives a4-a5 have similar behaviors , with a power increasing with @xmath169 .",
    "it can be explained by the very slow convergence to the null model . here",
    ", passing from a time length equal 20 to a time length equal to 200 corresponds to adding new observations with a time depending transformation ( see figure [ figa4 ] ) .",
    "it is also observed that power associated to alternative a3 increases with @xmath169 .    in figure",
    "[ fig2 ] empirical powers are presented in the case where @xmath20 follows an ar1 process with a correlation coefficient equal to 0.5 . here",
    "powers are slightly better and more stable with respect to the length .",
    "this is due to the correlation inducing more stability of the process @xmath20 and permitting a better estimation of @xmath31 .",
    "figure [ fig3 ] presents results in the case of paired data , with @xmath20 normally distributed .",
    "powers are good , due to the fact that transformations occur not randomly since we have considered @xmath161",
    ". then @xmath31 can be efficiency estimated and its variations are well detected .    ) and a2 ( @xmath194 ) on the left , a3 ( @xmath195 ) , a4 ( @xmath196 ) and a5 ( @xmath197 ) on the right , with @xmath19 distributed as @xmath198 .",
    "solid lines correspond to @xmath192 and dotted lines correspond to @xmath199 .",
    "the lengths of time s intervals are @xmath200    ) and a2 ( @xmath194 ) on the left , a3 ( @xmath195 ) , a4 ( @xmath196 ) and a5 ( @xmath197 ) on the right , with @xmath19 following an ar1 process with correlation 0.1 .",
    "solid lines correspond to @xmath192 and dotted lines correspond to @xmath199 .",
    "the lengths of time s intervals are @xmath200    ) and a2 ( @xmath194 ) on the left , a3 ( @xmath195 ) , a4 ( @xmath196 ) and a5 ( @xmath197 ) on the right , with @xmath19 and @xmath20 paired .",
    "solid lines correspond to @xmath192 and dotted lines correspond to @xmath199 .",
    "the lengths of time s intervals are @xmath200",
    "the proposed method concerns the comparison of two processes when panel data are available .",
    "the test permits to detect a change in the relation between the two process distributions .",
    "therefore it can detect a change in a higher moments ( not only in the mean and/or in the variance as almost tests do in this framework ) .",
    "the asymptotic distribution of the proposed statistic was derived under the null of no change in the relation between the two process distributions .",
    "the monte carlo simulations show that our test performs well in finite sample and has a good power against either abrupt or smooth changes .",
    "it is also valid for paired processes and then it can be used to detect a change in @xmath31 in the relation @xmath161 ( see the paired case in our simulations ) .",
    "the test can also be used as a first step permitting to legitimate estimation and interpretation of a constant transformation @xmath25 between two panel data , as for instance in a medical follow - up study .",
    "bou boutahar , m. ( 2009 ) .",
    "testing for change in the mean of heteroskedastic time series .",
    "dav davidson , j. ( 1994 ) .",
    "_ stochastic limit theory_. oxford : oxford university press .",
    "pan panaretos , v.m . ,",
    "kraus , d. & maddocks , j.h .",
    "second - order comparison of gaussian random functions and the geometry of dna minicircles .",
    "_ journal of the american statistical association _ 490 , 670682 ."
  ],
  "abstract_text": [
    "<S> this paper considers the problem of comparing two processes with panel data . </S>",
    "<S> a nonparametric test is proposed for detecting a monotone change in the link between the two process distributions . </S>",
    "<S> the test statistic is of cusum type , based on the empirical distribution functions . </S>",
    "<S> the asymptotic distribution of the proposed statistic is derived and its finite sample property is examined by bootstrap procedures through monte carlo simulations .    * * keywords**nonparametric estimation panel data process </S>"
  ]
}