{
  "article_text": [
    "the generality of such a representation follows from the arguments justifying the generality of the tensor network factorization of the partition function as , for example , appears in ref .",
    "@xcite . here",
    "we give explicit expressions for the tensors @xmath23}$ ] corresponding to the two systems discussed in the main text : the one - dimensional ( 1d ) open ising chain and the two - dimensional ( 2d ) periodic square ising system , both with @xmath10 .",
    "for the 1d open ising chain the elements of tensors @xmath23}$ ] are @xmath160 z_1}_{i_1 } = & \\exp \\left [ \\beta z_1 ( j   i_1/2 + \\lambda ) \\right ] ,   \\nonumber \\\\ a^{[\\ell ] z_\\ell}_{i_{\\ell-1 } i_\\ell } = & \\exp \\left[\\beta z_\\ell ( j   ( i_{\\ell-1 } + i_\\ell ) /2",
    "+ \\lambda ) \\right ] , \\ ; \\ ; \\ ; 1 < \\ell < n ,   \\nonumber\\\\ a^{[n ] z_n}_{i_{n-1 } } = & \\exp \\left [ \\beta z_n ( j   i_{n-1}/2 + \\lambda ) \\right ] .",
    "\\nonumber\\end{aligned}\\ ] ] the labeling of the indices @xmath161 , for @xmath162 , makes it clear which pairs are contracted .",
    "the indices take values @xmath163 and so the matrices @xmath23 z_\\ell}$ ] for @xmath164 are @xmath165 ( the boundary vectors @xmath166 z_1}$ ] and @xmath167 z_n}$ ] are @xmath168 and @xmath169 , respectively ) .    for the 2d periodic square ising system the tensors @xmath23}$ ]",
    "are identical and their elements are given by @xmath170 z_\\ell}_{i_1 i_2 i_3 i_4 } = & \\exp \\left[\\beta z_\\ell \\left ( j   \\sum_{\\ell'=1}^4 i_{\\ell'}/2 + \\lambda   \\right ) \\right ] .",
    "\\nonumber \\end{aligned}\\ ] ] the indices should be paired to form a square periodic lattice .",
    "the order of the indices at any one site is not important since the tensors are symmetric in the indices @xmath171 .",
    "as before , the indices @xmath172 for @xmath173 take values @xmath163 .      in our calculations",
    "the bias @xmath59 is varied between @xmath94 and @xmath95 according to a smoothed tanh ramp .",
    "the exact time - dependence used is @xmath174 \\right)}{\\tanh \\left ( 1   \\right ) } \\right ] , \\nonumber\\end{aligned}\\ ] ] where @xmath74 is the duration of the quench .      with the system out of equilibrium",
    ", we wish to determine the effect of compressing a distribution @xmath44 ( similarly @xmath69 ) at some time @xmath6 . in other words",
    "we wish to construct a tensor network of small dimension @xmath26 that closely approximates the distribution @xmath175 z_\\ell}_{{\\mathbf{i}}_{\\ell } } . \\nonumber\\ ] ] for a 1d system with open boundaries the tensor network takes the matrix product form @xmath176 z_\\ell}_{{\\mathbf{i}}_{\\ell } } = a^{[1 ] z_1 } a^{[2 ] z_2 } \\cdots a^{[n ] z_n } , \\nonumber\\ ] ] with @xmath177 matrices @xmath23 z_\\ell}$ ] for @xmath164 , and @xmath178 and @xmath179 boundary vectors @xmath166 z_1}$ ] and @xmath167 z_n}$ ] .",
    "it is well - known how to find an accurate matrix product approximation to a distribution @xmath44 using singular value decomposition decimation , as explained in refs .",
    "note that the matrix product obtained may not be optimally accurate for the observable of interest , and thus even more effective compression may be possible than that we have reported .",
    "however , singular value decomposition decimation is closely related to how compression occurs in the operation of the time - evolution block decimation ( tebd ) algorithm , and thus we expect the conclusions regarding compressibility at a single time here to be most relevant to repeated compression within the tebd algorithm .",
    "tebd is a standard method , detailed in refs .",
    "@xcite , for evolving a matrix product representation of some distribution @xmath44 according to an equation of the form @xmath180 the algorithm proceeds by breaking the evolution into timesteps @xmath109 .",
    "evolving the distribution @xmath44 over a timestep takes it away from one expressed in terms of a matrix product of dimension @xmath26 .",
    "thus the distribution is repeatedly , within each timestep , re - compressed to this form using the singular value decomposition .",
    "the method we use to discretize time is a typical one and is explained in detail in ref .",
    "it is accurate up to errors that are second - order in @xmath109 .",
    "the most typical implementations of tebd are compatible with two - site nearest - neighbor hamiltonians only .",
    "since the values of the transition rates appearing in glauber dynamics depend on the energies at three nodes , the hamiltonian is not naturally two - site . to bring it into standard form , rather than @xmath7 single nodes with @xmath10 ,",
    "the system is broken down into @xmath181 supernodes with @xmath182 according to @xmath183 , where @xmath184 for @xmath185 . in the supernode picture",
    "the hamiltonian is naturally formed of two - site nearest - neighbor terms .    finally , the calculation results and scaling of resources quoted in the main text is for fixed ieee 64-bit double precision computing .",
    "though not necessary for the calculations presented here , higher than normal precision may be required for very large systems ( see e.g.  ref .",
    "@xcite ) due to the number of orders of magnitude spanned by the observable and probability distributions .",
    "we have used the tensor renormalization group method to calculate the sum @xmath149 for the case of an @xmath140 2d periodic square ising system , @xmath146 and @xmath147 the set of configurations with @xmath142 .",
    "we are able to use tensor renormalization group methods because of the equilibrium structure @xmath103 taken by the non - equilibrium distribution @xmath69 for this observable .",
    "to see this , we use the above to rewrite @xmath186 , where @xmath187 is the equilibrium gibbs distribution corresponding to the bias @xmath64 at time @xmath6 , and @xmath105 the corresponding partition function .",
    "thus the calculation reduces to calculating partition functions @xmath105 and partial sums @xmath188 over gibbs distributions conditioned upon the value of a single spin @xmath142 .",
    "given the representation of a gibbs distribution @xmath189 by a tensor network , the partition function @xmath190 } _ { { \\mathbf{i}}_{\\ell}}$ ] is merely a contraction of transfer tensors @xmath33 } = \\sum_{z_\\ell } a^{[\\ell ] z_\\ell}$ ] with a square geometry .",
    "the partial sum @xmath191 } _ { { \\mathbf{i}}_{\\ell}}$ ] is similar except with @xmath192 } = a^{[\\ell ] z_1}$ ] .",
    "these contractions can be performed using the well - known tensor renormalization group method  @xcite , the relevant details about which we provide here .",
    "the strategy is first to perform the contractions involving each set of four nodes making up a node of a coarse - grained square lattice .",
    "we are then left with tensor network in a square lattice with the number of nodes reduced by a factor of four , but the dimension of each index potentially increased by a power of @xmath193 .",
    "next , we compress this to obtain smaller index dimension @xmath26 .",
    "the contraction / compression process is then repeated iteratively until a single node remains , and the contraction can be trivially evaluated .",
    "we have used dynamical monte carlo to calculate the sum @xmath194 for the case of an @xmath140 periodic square ising system and @xmath147 the set of configurations with @xmath142 .",
    "we were able to use dynamical monte carlo because , by defining an observable @xmath60 taking values @xmath106 if @xmath195 and @xmath65 otherwise , we have @xmath196 i.e.  the quantity of interest is just the expected value @xmath108 of this observable .",
    "we implement a variable - timestep dynamical monte carlo algorithm ( as e.g.  explained in ref .",
    "@xcite ) to sample @xmath197 paths and estimate @xmath108 by averaging the values taken by the observable over such paths .",
    "because the variance of the observable is necessarily less than unity , the expected error due to insufficient sampling must be less than @xmath198 , which is small for the @xmath199 we used .",
    "the only source of error other than insufficient sampling is an approximation made when estimating the time until the next transition as part of the sampling of a path .",
    "we assumed for simplicity that the transition rates stayed constant between transitions , which ignores their varying due to the change in @xmath64 .",
    "since the average time between transitions is on the order of @xmath200 and @xmath201 is varied by unity over a time @xmath202 in our example , the approximation induced by this is negligible .",
    "e. evenbly and g. vidal .",
    "_ quantum criticality with the multi - scale entanglement renormalization ansatz_. in e. avella and f. mancini ( eds . ) _ strongly correlated systems : numerical methods _ , pp .",
    "99130 ( springer , heidelberg , 2013 ) .",
    "t. nishino , j. phys .",
    "64 , 3598 ( 1995 ) ; t. nishino and k. okunishi , in _ transfer - matrix approach to classical systems _ , edited by i. peschel , x. wang , and k. hallberg , lecture note in physics vol .",
    "* 528 * ( springer , berlin , 1999 ) , pp ."
  ],
  "abstract_text": [
    "<S> estimating the expected value of an observable appearing in a non - equilibrium stochastic process usually involves sampling . if the observable s variance is high , many samples are required . </S>",
    "<S> in contrast , we show that performing the same task without sampling , using tensor network compression , efficiently captures high variances in systems of various geometries and dimensions . </S>",
    "<S> we provide examples for which matching the accuracy of our efficient method would require a sample size scaling exponentially with system size . in particular , the high - variance observable @xmath0 , motivated by jarzynski s equality , with @xmath1 the work done quenching from equilibrium at inverse temperature @xmath2 , is exactly and efficiently captured by tensor networks .    </S>",
    "<S> dynamical stochastic processes are used throughout the natural and social sciences when inaccessible degrees of freedom are well - represented by random variables  @xcite . to calculate expected observable values , </S>",
    "<S> numerical methods are usually required . </S>",
    "<S> out of equilibrium , the typical method is dynamical monte carlo  @xcite . essentially , averaging over randomly sampled paths provides an unbiased estimate for the expected value of an observable . to obtain a fixed expected fractional error , </S>",
    "<S> the number of paths sampled must scale linearly with the variance divided by the square of the expected value . </S>",
    "<S> for a multitude of important observables , such as those appearing in the estimation of free energies  @xcite and likelihoods of rare events  @xcite , this ratio is large : such observables are said to have high variance and sampling methods struggle when applied to them .    here </S>",
    "<S> we present an approach that is very different to sampling . </S>",
    "<S> we simultaneously follow all paths , which is made efficient by using controlled data compression , usually approximate but exact in special cases , based on tensor networks . </S>",
    "<S> while tensor networks have previously been used in conjunction with stochastic processes  @xcite , the question of how their performance relates to variance has remained unanswered . </S>",
    "<S> understanding this is crucial if we are to know whether or not tensor networks , which have had a revolutionary effect in simulating quantum systems  @xcite and have been used to great effect in solving partial differential equations  @xcite , provide a useful and perhaps essential complementary technique to sampling in stochastic processes .    in this letter </S>",
    "<S> we address this question and our answer is very clear : high variance does not limit the accuracy of tensor network compression , and tensor networks can be applied efficiently to tackle problems , even out of equilibrium , for which sampling - based methods struggle . </S>",
    "<S> this opens the door for the use of tensor network methods on a wide - variety of non - equilibrium stochastic systems for which capturing high variance is essential . in particular , we show that a distribution of weighted expectation values of high - variance observable @xmath0 , with @xmath1 the work done quenching from equilibrium at inverse temperature @xmath2 , is represented exactly by a highly compressed tensor network .     </S>",
    "<S> ( color online ) _ tensor network compression_. ( a ) an ising system whose degrees of freedom ( blue circles with arrows ) interact , in this case , with a two - dimensional lattice geometry ( red lines ) . </S>",
    "<S> ( b ) the probability distributions @xmath3 and @xmath4 , and @xmath5 ( see main text ) at any time @xmath6 are compressed by representing them ( approximately or , in special cases , exactly ) by a contraction of tensors ( green circles ) with the same geometry as the interactions . </S>",
    "<S> each black leg corresponds to an index of a tensor , and the joining of two legs represents the contraction of the two corresponding indices . </S>",
    "<S> , width=325 ]    we focus on an ising system , an example of which is shown in fig .  [ </S>",
    "<S> fig : tensornetwork](a ) . </S>",
    "<S> it comprises @xmath7 nodes , labeled by @xmath8 , the configuration @xmath9 of each taking one of @xmath10 discrete values @xmath11 . </S>",
    "<S> the configuration of all @xmath7 nodes is given by the @xmath7-tuple @xmath12 and the probability of being in configuration @xmath13 is @xmath14 . </S>",
    "<S> tensor networks best suit systems for which crucial quantities , like energy , are @xmath15-bodied , with @xmath15 small . </S>",
    "<S> we consider the simplest non - trivial case of an energy comprising single and two - body terms @xmath16 where @xmath17 are @xmath18 edges connecting interacting nodes .    _ </S>",
    "<S> equilibrium._the relationship between compressibility and variance out of equilibrium builds on that in equilibrium . </S>",
    "<S> the equilibrium gibbs distribution at inverse temperature @xmath2 is @xmath19 , normalized to the partition function @xmath20 . </S>",
    "<S> it is always possible to represent @xmath21 ( or any distribution ) by a tensor network of the form shown in fig .  </S>",
    "<S> [ fig : tensornetwork](b ) @xmath22 z_\\ell}_{{\\mathbf{i}}_{\\ell}},\\ ] ] that shares the same geometry as the interactions . here </S>",
    "<S> @xmath23}$ ] is a tensor associated with node @xmath8 . </S>",
    "<S> it has a physical index @xmath9 and @xmath24 auxiliary indices @xmath25 , one for each edge connected to @xmath8 , and each taking one of @xmath26 values , which may in principle be large . </S>",
    "<S> the sum is over the values taken by all auxiliary indices , which is just a sum over an @xmath18-tuple @xmath27 of indices . </S>",
    "<S> the gibbs distribution is important because the tensor network representation is exact for @xmath28 ( see supplemental material @xcite ) . </S>",
    "<S> this implies that @xmath29 numbers may be used to represent @xmath30 others , providing a significant yet exact compression if the degrees @xmath24 are limited , as in lattice systems with local interactions .    as well as compression , </S>",
    "<S> tensor networks offer a means of calculating the partition function @xmath31 , since @xmath32 } _ { { \\mathbf{i}}_{\\ell}}$ ] with transfer tensors @xmath33 } = \\sum_{z_\\ell } a^{[\\ell ] z_\\ell}$ ]  @xcite . for a one - dimensional ( 1d ) chain </S>",
    "<S> the partition function @xmath31 relates to a product of transfer matrices and requires @xmath34 or @xmath35 resources to compute for open or periodic boundaries , respectively . in higher dimensions ( </S>",
    "<S> if the tensor network has a large treewidth  @xcite ) the tensor contractions can not in general be performed both exactly and efficiently , but efficient strategies exist to perform them approximately . levin and nave  @xcite demonstrated that this can be done accurately for two - dimensional ( 2d ) non - critical lattice systems using tensor renormalization group  @xcite .    </S>",
    "<S> contrastingly , estimating the partition function @xmath31 directly by evaluating the sum @xmath36 through random sampling is made difficult by the fact that , in general , the variance of observables requiring estimation grows quickly with system size  @xcite . </S>",
    "<S> the tensor network representations of @xmath37 and @xmath31 show that high variance does not imply difficulty in equilibrium , away from criticality .    </S>",
    "<S> _ non - equilibrium._out of equilibrium , the dynamics of a markovian system  @xcite depends only on its current configuration , and the evolution of the distribution @xmath4 is described by a master equation of the form @xmath38 each non - negative off - diagonal element @xmath39 for @xmath40 is the poisson rate of a transition from @xmath41 to @xmath13 at time @xmath6 , and together these fix the non - positive diagonals @xmath42 such that the normalization of @xmath4 is conserved . </S>",
    "<S> @xmath43 is commonly referred to as the hamiltonian .    to simulate such dynamics using non - equilibrium tensor network methods , we represent @xmath44 at any time by a tensor network , as in eq .  </S>",
    "<S> ( [ eq : tn ] ) , with a small @xmath26 . </S>",
    "<S> doing so assumes that this representation , while not necessarily exact , is accurate . </S>",
    "<S> there is no guarantee of this accurate compressibility on all occasions , but it is expected in many situations . </S>",
    "<S> for example , consider a quench from one hamiltonian @xmath45 to another @xmath46 , where the system begins in the stationary state @xmath47 satisfying @xmath48 . for much later times @xmath49 ( on the timescale @xmath50 , where @xmath51 is some hamiltonian - specific convergence rate ) , the system will converge to another stationary state @xmath52 satisfying @xmath53 . </S>",
    "<S> numerous examples have revealed that stationary states of local stochastic processes are accurately compressible via tensor network representations  @xcite . </S>",
    "<S> thus in such quenches both initial and long - time distributions @xmath54 and @xmath55 are accurately compressible . unlike for quantum systems  </S>",
    "<S> @xcite , compression errors are limited even when a system is driven away from equilibrium . the example on which we focus here is that of a thermalizing ( equilibrating ) hamiltonian @xmath56 for which the gibbs distribution is stationary , @xmath57 , and it is the energy @xmath58 that is quenched by varying the bias @xmath59 appearing in eq .  </S>",
    "<S> ( [ eq : ising ] ) .    </S>",
    "<S> the probability distribution @xmath4 over configurations contains only partial information about the full probability distribution over the possible paths through configuration space taken by the stochastic process . as such , the expected values of only certain observables </S>",
    "<S> may be calculated from @xmath4 . </S>",
    "<S> these include observables whose values @xmath60 depend on the configuration @xmath13 of the system at a single time @xmath6 , thus having expected value @xmath61 . </S>",
    "<S> we call such observables configuration - dependent and use the example of the magnetization @xmath62 . </S>",
    "<S> the values of some other observables depend on the full path taken by the system and their expected values can not be calculated from @xmath4 . </S>",
    "<S> we call such observables path - dependent , and use the example of the work done @xmath63 by varying @xmath64 between times @xmath65 and @xmath6 .    </S>",
    "<S> although not previously considered in the literature , the expected values of some path - dependent observables can indeed be calculated using tensor networks and , as we will show , provide us with a stark example of exact compressibility in the face of high variance out of equilibrium . </S>",
    "<S> the idea is to represent the relevant path - dependent information locally in time , not with @xmath4 , but through the distribution of weighted conditional expected values @xmath66 . here </S>",
    "<S> @xmath67 is the expected value an observable has accumulated by time @xmath6 conditioned upon the system arriving at configuration @xmath13 at that time . </S>",
    "<S> the expected value of interest @xmath68 is then obtained from @xmath69 . </S>",
    "<S> it follows from eq .  </S>",
    "<S> ( [ eq : markovian ] ) that the distribution @xmath70 evolves as @xmath71 with @xmath72 , where @xmath73 is the rate of increase of the natural logarithm of the observable at configuration @xmath13 and time @xmath6 .    </S>",
    "<S> it is desirable to predict , as we have for @xmath44 , the accuracy of compressing @xmath69 at any time during its evolution using tensor networks . </S>",
    "<S> consider the quench in @xmath59 between times @xmath65 and @xmath74 , starting from equilibrium . </S>",
    "<S> the distributions are initially equal @xmath75 , thus the accurate compressibility of the latter implies the same of the former . additionally , @xmath73 is only non - zero for times @xmath76 and the stochastic evolution is ergodic . </S>",
    "<S> thus after a sufficiently long time @xmath77 ( relative again to convergence timescale @xmath50 ) the configurations will have mixed such that @xmath78 is independent of @xmath13 and thus once again @xmath79 is as accurately compressible as @xmath44 .     </S>",
    "<S> ( color online ) _ accurate compression out of equilibrium_. ( a ) an @xmath7-node open ising chain with exchange @xmath80 and bias @xmath81 . </S>",
    "<S> ( b ) the bias @xmath59 is varied , driving the system away from equilibrium . </S>",
    "<S> ( c ) the fractional error @xmath82 between calculating @xmath83 using probability distribution @xmath4 and its compressed tensor network approximation of dimension @xmath26 ( see supplemental material  @xcite ) . </S>",
    "<S> a dashed line marks the end of the quench at time @xmath84 . </S>",
    "<S> ( d ) similarly , the fractional error @xmath85 between calculating @xmath86 by summing @xmath5 and its compressed tensor network approximation . </S>",
    "<S> the parameters used are @xmath87 , @xmath88 , @xmath89 , @xmath90 and @xmath91 . </S>",
    "<S> , width=325 ]    _ numerical examples._we demonstrate these behaviors for a system undergoing thermalizing glauber dynamics  @xcite via local transitions , @xmath92^{-1},\\ ] ] for @xmath40 . here </S>",
    "<S> @xmath93 are symmetric rates equaling a non - zero rate @xmath51 only where @xmath13 and @xmath41 differ by the configuration of a single node . the energy is quenched via the parameter @xmath59 varying from @xmath94 to @xmath95 over time @xmath96 according to a smoothed tanh ramp ( see supplemental material  @xcite ) , as drawn in fig .  </S>",
    "<S> [ fig : introfig](b ) . </S>",
    "<S> we focus on configuration - dependent observable @xmath97 and path - dependent observables @xmath98 and @xmath99 . </S>",
    "<S> all have variance over mean squared growing exponentially with system size @xmath7 . </S>",
    "<S> initially , we consider the ising nodes to be in an open 1d chain , illustrated in fig .  [ </S>",
    "<S> fig : introfig](a ) .    to assess the accuracy of compression , we exactly calculate distributions @xmath4 and @xmath5 for small @xmath91 at time @xmath6 , </S>",
    "<S> then calculate the error in the expected value of observables induced by compressing the distributions as a tensor network . </S>",
    "<S> the errors shown in figs .  [ fig : introfig](c ) and ( d ) for @xmath97 and @xmath98 , respectively , show that , despite large variances , expected values are relatively unaffected by tensor network compression . </S>",
    "<S> the distributions are exactly compressible at @xmath100 and thus no error occurs , as expected . the errors due compression initially increase as @xmath59 varies , then decrease exponentially to small values again on a timescale @xmath101 . </S>",
    "<S> interestingly , errors begin to decrease even at times @xmath76 prior to the end of the quench . for @xmath102 </S>",
    "<S> the compression is near - exact at all times . </S>",
    "<S> we arrive at similar conclusions for other types of variation tried e.g.  linear , variations in @xmath80 , and other observables .    </S>",
    "<S> a striking example is found in the path - dependent observable @xmath99 . </S>",
    "<S> the observable has received particular attention due to its featuring in several non - equilibrium identities in statistical physics , such as that by jarzynski  @xcite . </S>",
    "<S> crucially for our discussion , the _ non - equilibrium _ </S>",
    "<S> distribution @xmath66 for this special case has an _ equilibrium _ </S>",
    "<S> structure @xmath103  @xcite , where we have used shorthands of the form @xmath104 for the gibbs distribution corresponding to @xmath64 and @xmath105 for the corresponding partition function ( where from now on we normalize the gibbs distribution to @xmath106 ) . </S>",
    "<S> it immediately follows from our discussion of systems in equilibrium that @xmath69 , despite containing information about non - equilibrium high - variance observables , has an exact highly - compressed @xmath107 tensor network representation at all times @xmath6 . </S>",
    "<S> this can be used to efficiently and accurately calculate not only @xmath108 but a range of properties of the work distribution during such dynamics . </S>",
    "<S> note that this exact behavior is particular to @xmath99 and does nt even extend to its square @xmath98 .    </S>",
    "<S> we have so far demonstrated accurate single - time compressibility . </S>",
    "<S> we next examine how this extends to a dynamical tensor network simulation , where compression of @xmath44 or @xmath69 occurs not only at a single time but at all times during their evolution . </S>",
    "<S> while one might expect the compression errors at single times to accumulate , we find this is mitigated by the ergodicity of the evolution ( unlike in quantum systems ) . </S>",
    "<S> for example , errors in @xmath4 will not change the distribution to which it converges , and thus the significance of transient errors diminish , rather than accumulate , in time . in </S>",
    "<S> what follows , the specific algorithm we use to perform the evolutions of eqs .  </S>",
    "<S> ( [ eq : markovian ] ) and ( [ eq : evolution ] ) is time - evolution block decimation ( tebd )  @xcite . </S>",
    "<S> the tebd algorithm uses a timestep @xmath109 resulting in an error , beyond that due to compression , of @xmath110 and requires time @xmath111  ( see the supplemental material  @xcite ) .    </S>",
    "<S> we first calculated @xmath112 and @xmath113 , where @xmath114 and @xmath115 are values at the end of the quench @xmath84 . </S>",
    "<S> there are no exact values available to compare against for large systems , but the compressibility expected from figs .  </S>",
    "<S> [ fig : introfig](c ) and ( d ) is confirmed by our tebd results in figs .  </S>",
    "<S> [ fig : jarzynskiverification](a ) and ( b ) , respectively . </S>",
    "<S> the calculated expected values converge approximately exponentially with increasing @xmath26 , reaching acceptably converged values by @xmath116 , despite the variance over mean squared being very large for the @xmath117 considered .    </S>",
    "<S> we next calculated @xmath118 . </S>",
    "<S> since compression is exact for this observable if @xmath119 , the error @xmath120 is purely due to the finite timestep @xmath109 and , as stated above , scales as @xmath121 . </S>",
    "<S> meanwhile the estimated variance over mean squared @xmath122 scales with @xmath123 , as shown in fig .  </S>",
    "<S> [ fig : jarzynskiverification](c ) . </S>",
    "<S> it is then particularly clear with this example that , to achieve the same fractional accuracy ( namely , @xmath121 ) as we efficiently obtain , a nave sampling method would require a sample size and thus time @xmath123 in contrast to the tebd algorithm that requires time @xmath124 . </S>",
    "<S> explicitly , our @xmath117 , @xmath125 , @xmath126 , @xmath127 calculation takes less than an hour and achieves an error @xmath128 . </S>",
    "<S> we do not compare this against the cpu time of any one sampling algorithm , as this choice is likely to be unrepresentative . </S>",
    "<S> instead , we note that , since @xmath129 , our accuracy would require @xmath130 samples to reproduce , and so matching our cpu time would require each path to be sampled in @xmath131 .     </S>",
    "<S> ( color online ) _ compression and variance_. ( a ) the fractional error @xmath82 of the calculated expected value @xmath132 relative to the @xmath133 result . </S>",
    "<S> ( b ) the analogous error @xmath85 for observable @xmath134 . </S>",
    "<S> ( c ) the variance over mean squared @xmath135 of @xmath0 as a function of @xmath136 and @xmath7 , obtained by calculating @xmath137 using @xmath133 and @xmath138 , and using an exact result for @xmath139 . </S>",
    "<S> ( d ) for a periodic @xmath140 system , the expectation value @xmath141 , conditional on one spin taking value @xmath142 at @xmath6 ( full line ) . also , the ratio @xmath143 of this value relative to that for a reversible quench ( dashed line ) . unless stated otherwise the parameters used are @xmath87 , @xmath88 , @xmath89 , @xmath90 , @xmath117 and @xmath144 . </S>",
    "<S> , width=325 ]    as our final example , we demonstrate both the diversity of information stored in the distribution @xmath69 and the application of our method to 2d lattice geometries , specifically a @xmath140 square periodic lattice . we consider @xmath145 , the expected value of @xmath146 given the system s configuration @xmath13 is in subset @xmath147 at time @xmath6 , where @xmath147 is the set configurations in which one node has value @xmath142 . </S>",
    "<S> the observable has a very large variance . </S>",
    "<S> its conditional expected value can be rewritten @xmath148 . </S>",
    "<S> the numerator @xmath149 corresponds to a high - variance observable but can nevertheless be efficiently evaluated using equilibrium techniques for gibbs distributions , due to the equilibrium structure of the non - equilibrium distribution @xmath69 . in this case </S>",
    "<S> we use the tensor renormalization group method  @xcite with intermediary dimension @xmath150 ( see supplemental material  @xcite ) , which suffices as the model parameters are far from criticality . the denominator @xmath151 is simply the low - variance expected value of an observable taking value unity when @xmath152 , otherwise zero . this can be accurately calculated using our tensor network methods or the more common dynamical monte carlo methods  @xcite . </S>",
    "<S> we used the latter , with sample size @xmath153 ( see supplemental material  @xcite ) . the result is plotted in fig .  [ fig : jarzynskiverification](d ) . </S>",
    "<S> also plotted is @xmath143 , the ratio of the expected value @xmath154 to its value were the system in equilibrium at all times . </S>",
    "<S> this emphasizes that , despite exploiting an equilibrium structure , the dynamics being simulated is truly irreversible .    _ </S>",
    "<S> discussion._we have shown that tensor networks provide a way to overcome the challenges faced by sampling methods when estimating expected values of high - variance observables out of equilibrium , even finding an exactly compressible non - equilibrium example . while advanced techniques for variance reduction exist , such as sequential importance sampling  @xcite and branching methods  @xcite </S>",
    "<S> , using these techniques usually requires judicious choices specific to the models to be simulated based on prior intuition about the process . </S>",
    "<S> no such intuition or choices are needed for a tensor network calculation . however , whether tensor networks remain accurate and efficient for geometries beyond 1d and 2d lattices , and other models e.g.  describing frustration or disorder , must still be established .    </S>",
    "<S> finally let us comment on how our findings relate to the wider use of tensor networks . while outstanding efficiency is possible using dynamical tensor network algorithms for finite 1d pure _ quantum _ systems </S>",
    "<S> , there is an ongoing effort from the community to reach larger dimensions @xmath26 and sizes @xmath7 in 2d . </S>",
    "<S> reference @xcite gives a state - of - the - art demonstration in which the ground state of an @xmath155 system with @xmath10 is calculated using @xmath156 . </S>",
    "<S> meanwhile we have seen here that often very small dimensions @xmath26 are required to simulate _ classical _ systems , even during real time dynamics . </S>",
    "<S> further , since only one copy of @xmath14 is needed in classical algorithms , compared to two copies of the wavefunction in quantum algorithms , those that take time e.g. @xmath157 for quantum systems will instead take time e.g. @xmath158 for classical systems </S>",
    "<S> . it may therefore be the case that classical stochastic systems are currently in an even better position than quantum systems to benefit from current high - dimensional tensor network algorithms .    </S>",
    "<S> thj , src , and dj thank the national research foundation and the ministry of education of singapore for support . </S>",
    "<S> tje thanks st anne s college of the university of oxford for financial support . </S>",
    "<S> this research received funding from the erc under the european unions seventh framework programme ( fp7/2007 - 2013)/erc grant agreement no . </S>",
    "<S> 319286 q - mac and from the epsrc through projects ep / k038311/1 and ep / j010529/1 . </S>"
  ]
}