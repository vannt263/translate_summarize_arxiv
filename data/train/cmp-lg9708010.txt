{
  "article_text": [
    "the problem of data sparseness affects all statistical methods for natural language processing .",
    "even large training sets tend to misrepresent low - probability events , since rare events may not appear in the training corpus at all .",
    "we concentrate here on the problem of estimating the probability of _ unseen _ word pairs , that is , pairs that do not occur in the training set .",
    "katz s back - off scheme @xcite , widely used in bigram language modeling , estimates the probability of an unseen bigram by utilizing unigram estimates . this has the undesirable result of assigning unseen bigrams the same probability if they are made up of unigrams of the same frequency .",
    "class - based methods @xcite cluster words into classes of similar words , so that one can base the estimate of a word pair s probability on the averaged cooccurrence probability of the classes to which the two words belong .",
    "however , a word is therefore modeled by the average behavior of many words , which may cause the given word s idiosyncrasies to be ignored . for instance , the word `` red '' might well act like a generic color word in most cases , but it has distinctive cooccurrence patterns with respect to words like `` apple , '' `` banana , '' and so on .",
    "we therefore consider similarity - based estimation schemes that do not require building general word classes . instead ,",
    "estimates for the most similar words to a word @xmath0 are combined ; the evidence provided by word @xmath1 is weighted by a function of its similarity to @xmath0 .",
    "dagan , markus , and markovitch ( 1993 ) propose such a scheme for predicting which unseen cooccurrences are more likely than others .",
    "however , their scheme does not assign probabilities . in what follows ,",
    "we focus on probabilistic similarity - based estimation methods .",
    "we compared several such methods , including that of dagan , pereira , and lee and the _ cooccurrence smoothing method _ of essen and steinbiss , against classical estimation methods , including that of katz , in a decision task involving unseen pairs of direct objects and verbs , where unigram frequency was eliminated from being a factor .",
    "we found that all the similarity - based schemes performed almost 40% better than back - off , which is expected to yield about 50% accuracy in our experimental setting .",
    "furthermore , a scheme based on the total divergence of empirical distributions to their average yielded statistically significant improvement in error rate over cooccurrence smoothing",
    ".    we also investigated the effect of removing extremely low - frequency events from the training set .",
    "we found that , in contrast to back - off smoothing , where such events are often discarded from training with little discernible effect , similarity - based smoothing methods suffer noticeable performance degradation when singletons ( events that occur exactly once ) are omitted .",
    "we wish to model conditional probability distributions arising from the coocurrence of linguistic objects , typically words , in certain configurations . we thus consider pairs @xmath2 for appropriate sets",
    "@xmath3 and @xmath4 , not necessarily disjoint . in what follows , we use subscript @xmath5 for the @xmath6 element of a pair ; thus @xmath7 is the conditional probability ( or rather , some empirical estimate , the true probability being unknown ) that a pair has second element @xmath8 given that its first element is @xmath9 ; and @xmath10 denotes the probability estimate , according to the base language model , that @xmath11 is the first word of a pair given that the second word is @xmath12 .",
    "@xmath13 denotes the base estimate for the unigram probability of word @xmath0 .    a similarity - based language model consists of three parts : a scheme for deciding which word pairs require a similarity - based estimate , a method for combining information from similar words , and , of course , a function measuring the similarity between words .",
    "we give the details of each of these three parts in the following three sections .",
    "we will only be concerned with similarity between words in @xmath14 .",
    "data sparseness makes the _ maximum likelihood estimate ( mle ) _ for word pair probabilities unreliable .",
    "the mle for the probability of a word pair @xmath15 , conditional on the appearance of word @xmath11 , is simply @xmath16 where @xmath17 is the frequency of @xmath15 in the training corpus and @xmath18 is the frequency of @xmath11 .",
    "however , @xmath19 is zero for any unseen word pair , which leads to extremely inaccurate estimates for word pair probabilities .",
    "previous proposals for remedying the above problem @xcite adjust the mle in so that the total probability of seen word pairs is less than one , leaving some probability mass to be redistributed among the unseen pairs . in general , the adjustment involves either _ interpolation _ , in which the mle is used in linear combination with an estimator guaranteed to be nonzero for unseen word pairs , or _ discounting _ , in which a reduced mle is used for seen word pairs , with the probability mass left over from this reduction used to model unseen pairs .",
    "the discounting approach is the one adopted by : @xmath20    where @xmath21 represents the good - turing discounted estimate @xcite for seen word pairs , and @xmath22 denotes the model for probability redistribution among the unseen word pairs .",
    "@xmath23 is a normalization factor .",
    "following dagan , pereira , and lee , we modify katz s formulation by writing @xmath24 instead @xmath25 , enabling us to use similarity - based estimates for unseen word pairs instead of basing the estimate for the pair on unigram frequency @xmath25 .",
    "observe that similarity estimates are used for unseen word pairs only .",
    "we next investigate estimates for @xmath26 derived by averaging information from words that are distributionally similar to @xmath11 .",
    "similarity - based models assume that if word @xmath27 is `` similar '' to word @xmath11 , then @xmath27 can yield information about the probability of unseen word pairs involving @xmath11 .",
    "we use a weighted average of the evidence provided by similar words , where the weight given to a particular word @xmath27 depends on its similarity to @xmath11 .",
    "more precisely , let @xmath28 denote an increasing function of the similarity between @xmath11 and @xmath27 , and let @xmath29 denote the set of words most similar to @xmath11",
    ". then the general form of similarity model we consider is a @xmath30-weighted linear combination of predictions of similar words : @xmath31 where @xmath32 is a normalization factor . according to this formula , @xmath12 is more likely to occur with @xmath11 if it tends to occur with the words that are most similar to @xmath11 .",
    "considerable latitude is allowed in defining the set @xmath29 , as is evidenced by previous work that can be put in the above form .",
    "essen and steinbiss and karov and edelman ( implicitly ) set @xmath33 .",
    "however , it may be desirable to restrict @xmath34 in some fashion , especially if @xmath14 is large .",
    "for instance , use the closest @xmath35 or fewer words @xmath27 such that the dissimilarity between @xmath11 and @xmath27 is less than a threshold value @xmath36 ; @xmath35 and @xmath36 are tuned experimentally .    now , we could directly replace @xmath24 in the back - off equation ( [ genmodel ] ) with @xmath37",
    ". however , other variations are possible , such as interpolating with the unigram probability @xmath25 : @xmath38 where @xmath39 is determined experimentally @xcite .",
    "this represents , in effect , a linear combination of the similarity estimate and the back - off estimate : if @xmath40 , then we have exactly katz s back - off scheme . as we focus in this paper on alternatives for @xmath41",
    ", we will not consider this approach here ; that is , for the rest of this paper , @xmath42 .",
    "we now consider several word similarity functions that can be derived automatically from the statistics of a training corpus , as opposed to functions derived from manually - constructed word classes @xcite .",
    "all the similarity functions we describe below depend just on the base language model @xmath43 , not the discounted model @xmath44 from section [ sec : redistribute ] above .",
    "_ kullback - leibler ( kl ) divergence _ is a standard information - theoretic measure of the dissimilarity between two probability mass functions @xcite",
    ". we can apply it to the conditional distribution @xmath45 induced by @xmath11 on words in @xmath46 : @xmath47    for @xmath48 to be defined it must be the case that",
    "@xmath49 whenever @xmath50 . unfortunately , this will not in general be the case for mles based on samples , so we would need smoothed estimates of @xmath51 that redistribute some probability mass to zero - frequency events .",
    "however , using smoothed estimates for @xmath52 as well requires a sum over all @xmath53 , which is expensive for the large vocabularies under consideration .",
    "given the smoothed denominator distribution , we set @xmath54 where @xmath55 is a free parameter .",
    "a related measure is based on the total kl divergence to the average of the two distributions : @xmath56 where @xmath57 shorthand for the distribution @xmath58 since @xmath59 , @xmath60 .",
    "furthermore , letting @xmath61 , @xmath62 and @xmath63 , it is straightforward to show by grouping terms appropriately that    @xmath64    where @xmath65 . therefore , @xmath66 is bounded , ranging between @xmath67 and @xmath68 , and smoothed estimates are not required because probability ratios are not involved .",
    "in addition , the calculation of @xmath66 requires summing only over those @xmath12 for which @xmath52 and @xmath51 are both non - zero , which , for sparse data , makes the computation quite fast .    as in the kl divergence case , we set @xmath28 to be @xmath69",
    ".      the _ @xmath70 norm _ is defined as @xmath71 by grouping terms as before , we can express @xmath72 in a form depending only on the `` common '' @xmath12 : @xmath73 this last form makes it clear that @xmath74 , with equality if and only if there are no words @xmath12 such that both @xmath75 and @xmath51 are strictly positive .    since we require a weighting scheme that is decreasing in @xmath76 , we set @xmath77 with @xmath55 again free",
    ".      introduced _ confusion probability _ , which estimates the probability that word @xmath27 can be substituted for word @xmath11 :    latexmath:[\\[\\begin{aligned } p_{{\\mbox{\\protect\\scriptsize",
    "c}}}(w_1 ' | w_1 ) & = & w(w_1,w_1 ' ) \\nonumber \\\\ & = & \\sum_{w_2 } \\frac{p(w_1     unlike the measures described above , @xmath11 may not necessarily be the `` closest '' word to itself , that is , there may exist a word @xmath27 such that @xmath79 .",
    "the confusion probability can be computed from empirical estimates provided all unigram estimates are nonzero ( as we assume throughout ) .",
    "in fact , the use of smoothed estimates like those of katz s back - off scheme is problematic , because those estimates typically do not preserve consistency with respect to marginal estimates and bayes s rule .",
    "however , using consistent estimates ( such as the mle ) , we can rewrite @xmath80 as follows : @xmath81    this form reveals another important difference between the confusion probability and the functions @xmath82 , @xmath83 , and @xmath76 described in the previous sections .",
    "those functions rate @xmath27 as similar to @xmath11 if , roughly , @xmath51 is high when @xmath84 is .",
    "@xmath85 , however , is greater for those @xmath27 for which @xmath86 is large when @xmath87 is .",
    "when the ratio @xmath87 is large , we may think of @xmath12 as being exceptional , since if @xmath12 is infrequent , we do not expect @xmath52 to be large .",
    "several features of the measures of similarity listed above are summarized in table [ table : simsum ] .",
    "`` base lm constraints '' are conditions that must be satisfied by the probability estimates of the base language model .",
    "the last column indicates whether the weight @xmath88 associated with each similarity function depends on a parameter that needs to be tuned experimentally .",
    "[ cols=\"<,<,^,^\",options=\"header \" , ]     since the back - off models consistently performed worse than the mle models , we chose to use only the mle models in our subsequent experiments .",
    "therefore , we only ran comparisons between the measures that could utilize unsmoothed data , namely , the @xmath70 norm , @xmath72 ; the total divergence to the average , @xmath89 ; and the confusion probability , @xmath90 .",
    "norm . ] in the full paper , we give detailed examples showing the different neighborhoods induced by the different measures , which we omit here for reasons of space .",
    "figure [ fig : mle1 ] shows the results on the five test sets , using 1 as the base language model .",
    "the parameter @xmath55 was always set to the optimal value for the corresponding training set .",
    ", which is shown for comparison purposes , simply chooses the weights @xmath88 randomly .",
    "@xmath29 was set equal to @xmath14 in all cases .",
    "the similarity - based methods consistently outperform the mle method ( which , recall , always has an error rate of .5 ) and katz s back - off method ( which always had an error rate of about .51 ) by a huge margin ; therefore , we conclude that information from other word pairs is very useful for unseen pairs where unigram frequency is not informative .",
    "the similarity - based methods also do much better than @xmath91 , which indicates that it is not enough to simply combine information from other words arbitrarily : it is quite important to take word similarity into account . in all cases ,",
    "@xmath83 edged out the other methods .",
    "the average improvement in using @xmath83 instead of @xmath80 is .0082 ; this difference is significant to the .1 level ( @xmath92 ) , according to the paired t - test .",
    "the results for the 1 case are depicted in figure [ fig : mleo1 ] .",
    "again , we see the similarity - based methods achieving far lower error rates than the mle , back - off , and @xmath91 methods , and again , @xmath83 always performed the best . however , with singletons omitted the difference between @xmath83 and @xmath80 is even greater , the average difference being @xmath93 , which is significant to the .01 level ( paired t - test ) .",
    "an important observation is that all methods , including , were much more effective if singletons were included in the base language model ; thus , in the case of unseen word pairs , katz s claim that singletons can be safely ignored in the back - off model does not hold for similarity - based models .",
    "similarity - based language models provide an appealing approach for dealing with data sparseness . we have described and compared the performance of four such models against two classical estimation methods , the mle method and katz s back - off scheme , on a pseudo - word disambiguation task .",
    "we observed that the similarity - based methods perform much better on unseen word pairs , with the measure based on the kl divergence to the average , being the best overall .",
    "we also investigated katz s claim that one can discard singletons in the training data , resulting in a more compact language model , without significant loss of performance .",
    "our results indicate that for similarity - based language modeling , singletons are quite important ; their omission leads to significant degradation of performance .",
    "we thank hiyan alshawi , joshua goodman , rebecca hwa , stuart shieber , and yoram singer for many helpful comments and discussions .",
    "part of this work was done while the first and second authors were visiting at&t labs .",
    "this material is based upon work supported in part by the national science foundation under grant no .",
    "the second author also gratefully acknowledges support from a national science foundation graduate fellowship and an at&t grpw / alfp grant .",
    "dagan , ido , fernando pereira , and lillian lee .",
    "similarity - based estimation of word cooccurrence probabilities . in _ proceedings of the 32nd annual meeting of the acl _ , pages 272278 , las cruces , nm .      gale , william , kenneth church , and david yarowsky",
    "work on statistical methods for word sense disambiguation . in _ working notes , aaai fall symposium series , probabilistic approaches to natural language _ ,",
    "pages 5460 .",
    "jelinek , frederick , robert  l. mercer , and salim roukos .",
    "principles of lexical language modeling for speech recognition . in sadaoki furui and m.  mohan sondhi , editors , _ advances in speech signal processing_. mercer dekker ,",
    ", pages 651699 ."
  ],
  "abstract_text": [
    "<S> we compare four similarity - based estimation methods against back - off and maximum - likelihood estimation methods on a pseudo - word sense disambiguation task in which we controlled for both unigram and bigram frequency . </S>",
    "<S> the similarity - based methods perform up to 40% better on this particular task . </S>",
    "<S> we also conclude that events that occur only once in the training set have major impact on similarity - based estimates .    </S>",
    "<S> 11 11 11 11 </S>"
  ]
}