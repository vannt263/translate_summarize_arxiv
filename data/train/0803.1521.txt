{
  "article_text": [
    "we have seen increasing reliance on services provided over the internet .",
    "these services are expected to be continuously available over extended period of time ( typically 24x7 and all year long ) .",
    "unfortunately , the vulnerabilities due to insufficient design and poor implementation are often exploited by adversaries to cause a variety of damages , e.g. , crashing of the applications , leaking of confidential information , modifying or deleting of critical data , or injecting of erroneous information into the application data .",
    "these malicious faults are often modeled as byzantine faults  @xcite , and they are detrimental to any online service providers .",
    "such threats can be coped with using byzantine fault tolerance ( bft ) techniques , as demonstrated by many research results @xcite .",
    "the byzantine fault tolerance algorithms assume that only a small portion of the replicas can be faulty .",
    "when the number of faulty replicas exceeds a threshold , bft may fail .",
    "consequently , castro and liskov @xcite proposed a proactive recovery scheme that periodically reboots replicas and refreshes their state , even before it is known that they have failed .",
    "as long as the number of compromised replicas does not exceed the threshold within a time window that all replicas can be proactively recovered ( such window is referred to as window of vulnerability  @xcite , or vulnerability window ) , the integrity of the bft algorithm holds and the services being protected remain highly reliable over the long term .    however , the reboot - based proactive recovery scheme has a number of issues .",
    "first , it assumes that a simple reboot ( _ i.e.,@xmath0_power cycle the computing node ) can successfully repair a compromised node , which might not be the case , as pointed out in @xcite .",
    "second , even if a compromised node can be repaired by a reboot , it is often a prolonged process ( typically over 30@xmath1 for modern operating systems ) . during the rebooting step",
    ", the bft services might not be available to its clients ( _ e.g.,@xmath0_if the rebooting node happens to be a nonfaulty replica needed for the replicas to reach a byzantine agreement ) .",
    "third , there lacks coordination among replicas to ensure that no more than a small portion of the replicas ( ideally no more than @xmath2 replicas in a system of @xmath3 replicas to tolerate up to @xmath2 faults ) are undergoing proactive recovery at any given time , otherwise , the services may be unavailable for extended period of time .",
    "the static watchdog timeout used in @xcite also contributes to the problem because it can not automatically adapt to various system loads .",
    "the staggered proactive recovery scheme in @xcite is not sufficient to prevent this problem from happening .    in this paper",
    ", we present a novel proactive recovery scheme based on service migration , which addresses all these issues .",
    "our proactive recovery scheme requires the availability of a pool of standby computing nodes in addition to the active nodes where the replicas are deployed .",
    "the basic idea is outlined below .",
    "periodically , the replicas initiate a proactive recovery by selecting a set of active replicas , and a set of target standby nodes for a service migration . at the end of the service migration ,",
    "the source active nodes will be put under a series of preventive sanitizing and repairing steps ( such as rebooting and swapping in a clean hard drive with the original system binaries ) before they are assigned to the pool of standby nodes , and the target nodes are promoted to the group of active nodes .",
    "the unique feature of this design is that the sanitizing and repairing step is carried out _ off the critical path of proactive recovery _ and consequently , it has minimum negative impact on the availability of the services being protected .",
    "this paper makes the following research contributions :    * we propose a novel migration - based proactive recovery scheme for long - running byzantine fault tolerant systems .",
    "the scheme significantly reduces the recovery time , and hence , the vulnerability window , by moving the time - consuming replica sanitizing and repairing step off the critical path . *",
    "our proactive recovery scheme ensures a coordinated periodical recovery , which prevents harmful excessive concurrent proactive recoveries .",
    "* we present a comparison study on the performance of the reboot - based and our migration - based proactive recovery schemes in the presence of faults , both by analysis and by experiments .",
    "we assume a partially asynchronous distributed system in that all message exchanges and processing related to proactive recovery can be completed within a bounded time .",
    "this bound can be initially set by a system administrator and can be dynamically adjusted by the recovery mechanisms . however , the safety property of the byzantine agreement on all proactive recovery related decisions ( such as the selection of source nodes and destination nodes for service migration ) is maintained without any system synchrony requirement .",
    "we assume the availability of a pool of nodes to serve as the standby nodes for service migration , in addition to the @xmath3 active nodes required to tolerate up to @xmath2 byzantine faulty replicas .",
    "the pool size is large enough to repair damaged nodes while enabling frequent service migration for proactive recovery .",
    "furthermore , both active nodes and standby nodes can be subject to malicious attacks ( in addition to other non - malicious faults such as hardware failures ) .",
    "however , we assume that the rate of successful attacks on the standby nodes is much smaller than that on active nodes , _ i.e.,@xmath0_the tolerated successful attack rate on active nodes is determined by the vulnerability window the system can achieve , and the tolerated successful attack rate on standby nodes is determined by the repair time",
    ". the allowed repair time can be much larger than the achievable vulnerability window given a sufficiently large pool of standby nodes .",
    "if the above assumptions are violated , there is no hope to achieve long - term byzantine fault tolerance .",
    "we assume the existence of a trusted configuration manager , as described in @xcite , to manage the pool of standby nodes , and to assist service migration .",
    "example tasks include frequently probing and monitoring the health of each standby node , and repairing any faulty node detected .",
    "we will not discuss the mechanisms used by the manager to carry out such tasks , they are out of the scope of this paper .    other assumptions regarding the system is similar to those in @xcite and they are summarized here .",
    "all communicating entities ( clients , replicas and standby nodes ) use a secure hash function such as sha1 to compute the digest of a message and use the message authentication codes ( macs ) to authenticate messages exchanged , except for key exchange messages , which are protected by digital signatures . for point to point message exchanges ,",
    "a single mac is included in each message , while multicast messages are protected by an authenticator @xcite .",
    "each entity has a pair of private and public keys .",
    "the active and standby nodes each is equipped with a secure coprocessor and sufficiently large read - only memory . in these nodes ,",
    "the private key is stored in the coprocessor and all digital signing and verification is carried out by the coprocessor without revealing the private key .",
    "the read - only memory is used to store the execution code for the server application and the bft framework .",
    "we do not require the presence of a hardware watchdog timer because of the coordination of migration and the existence of a trusted configuration manager .",
    "finally , we assume that an adversary is computational bound so that it can not break the above authentication scheme .",
    "the proactive service migration mechanisms collectively ensure the following objectives :    1 .   to ensure that correct active replicas have a consistent membership view of the available standby nodes .",
    "2 .   to determine when to migrate and how to initiate a migration .",
    "3 .   to determine the set of source and target nodes for migration . 4 .   to transfer a correct copy of the system state to the new replicas . 5 .   to notify the clients the new membership after each proactive recovery .    the first objective is clearly needed because otherwise the replicas can not possibly agree on the set of target nodes for migration .",
    "the second and third objectives are critical to ensure a coordinated periodic proactive recovery .",
    "the fourth objective is obviously necessary for the new replicas to start from a consistent state .",
    "the fifth objective is essential to ensure that the clients know the correct membership of the server replicas so that they do not accept messages from possibly faulty replicas that have been migrated out of active executing duty , and they can send requests to the new replicas .",
    "each standby node is controlled by the trusted configuration manager and is undergoing constant probing and sanitization procedures such as reboot .",
    "if the configuration manager suspects the node to be faulty and can not repair it automatically , a system administrator might be called in to manually fix the problem . each time a standby node completes a sanitization procedure",
    ", it notifies the active replicas with a join - request message in the form of @xmath4join - request@xmath5@xmath6@xmath7 , where @xmath8 is the counter value maintained by the secure coprocessor of the standby node , @xmath9 is the identifier of the standby node , and @xmath10 is the authenticator .",
    "the registration protocol is illustrated in figure  [ joinfig ] .",
    "= 3.0 in    an active replica accepts the join - request if it has not accepted one from the same standby node with the same or greater @xmath8 .",
    "the join - request message , once accepted by the primary , is ordered the same way as a regular message with a sequence number @xmath11 , except that the primary also assigns a timestamp as the join time of the standby node and piggybacks it with the ordering messages .",
    "the total ordering of the join - request is important so that all active nodes have the same membership view of the standby nodes .",
    "the significance of the join time will be elaborated later in this section .",
    "when a replica executes the join - request message , it sends a join - approved message in the form of @xmath4join - approved@xmath12@xmath6@xmath13 to the requesting standby node .",
    "the requesting standby node must collect @xmath14 consistent join - approved messages with the same @xmath8 and @xmath11 from different active replicas .",
    "the standby node then initiates a key exchange with all active replicas for future communication .",
    "a standby node might go through multiple rounds of proactive sanitization before it is selected to run an active replica .",
    "the node sends a new join - request reconfirming its membership after each round of sanitization .",
    "the active replicas subsequently updates the join time of the standby node .",
    "it is also possible that the configuration manager deems a registered standby node as faulty and it requires a lengthy repair , in which case , the configuration manager deregisters the faulty node from active replicas by sending a leave - request .",
    "the leave - request is handled by the active replicas in a similar way as that for join - request . in the unlikely case that the faulty standby node has been selected as the new active node",
    ", the mechanisms react in the following ways : ( 1 ) if the migration is still ongoing when the leave - request arrives , it is aborted and restarted with a different set of target standby nodes , and ( 2 ) if the migration has been completed , an on - demand service migration will be initiated to swap out the faulty node .",
    "the on - demand service migration mechanism is rather similar to the proactive migration mechanism , as will be discussed in section  [ ondemandsec ] .",
    "[ [ when - and - how - to - initiate - a - proactive - service - migration ] ] when and how to initiate a proactive service migration ?",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the proactive service migration is triggered by the software - based migration timer maintained by each replica .",
    "the timer is reset and restarted at the end of each round of migration .",
    "( an on - demand service migration may also be carried out upon the notification from the configuration manager , as mentioned in the previous subsection . )    how to properly initiate a proactive service migration , however , is tricky .",
    "we can not depend on the primary to initiate a proactive recovery because it might be faulty .",
    "therefore , the migration initiation must involve all replicas .    on expiration of the migration timer ,",
    "a replica chooses a set of @xmath2 active replicas , and a set of @xmath2 standby nodes , and multicasts an init - migration request to all other replicas in the form @xmath4init - migration@xmath15@xmath6@xmath13 , where @xmath16 is the current view , @xmath8 is the migration number ( determined by the number of successful migration rounds recorded by replica @xmath17 ) , @xmath18 is the set of identifiers for the @xmath2 active replicas to be migrated , @xmath19 is the set of identifiers for the @xmath2 standby nodes as the targets of the migration , @xmath17 is the identifier for the sending replica , and @xmath20 is the authenticator for the message .    on receiving an init - migration message , a replica @xmath21 accepts the message and stores the message in its data structure provided that the message carries a valid authenticator , it has not accepted an init - migration message from the same replica @xmath17 in view @xmath16 with the same or higher migration number , and the replicas in @xmath18 and @xmath19 are consistent with the sets determined by itself according to the selection algorithm ( to be introduced next ) .",
    "each replica waits until it has collected @xmath22@xmath23@xmath24 init - migration messages from different replicas ( including its own init - migration message ) before it constructs a migration - request message .",
    "the migration - request message has the form @xmath4migration - request@xmath25@xmath6@xmath26 .",
    "the primary , if it is correct , should place the migration - request message at the head of the request queue and order it immediately . the primary orders the migration - request in the same way as that for a normal request coming from a client , except that ( 1 ) it does not batch the migration - request message with normal requests , and ( 2 ) it piggybacks the migration - request and the @xmath22@xmath23@xmath24 init - migration messages ( as proof of validity of the migration request ) with the pre - prepare message .",
    "the reason for ordering the migration - request is to ensure a consistent synchronization point for migration at all replicas .",
    "an illustration of the migration initiation protocol is shown as part of figure  [ migrationfig ] .",
    "each replica starts a view change timer when the migration - request message is constructed ( just like when it receives a normal request ) so that a view change will be initiated if the primary is faulty and does not order the migration - request message . the new primary ,",
    "if it is not faulty , should continue this round of proactive migration .    in this work",
    ", we choose not to initiate a view change when the primary is migrated if the state is smaller than a tunable parameter ( 100 kb is used in our experiment ) . for larger state ( _ i.e.,@xmath0_when",
    "the cost of state transfer is more than that of the view change ) , the primary multicasts a view - change message before it is migrated , similar to @xcite .",
    "= 3.0 in    [ [ migration - set - selection . ] ] migration set selection .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    the selection of the set of active replicas to be migrated is relatively straightforward .",
    "it takes four rounds of migration ( each round for @xmath2 replicas ) to proactively recover all active replicas at least once .",
    "the replicas are recovered according to the reverse order of their identifiers , similar to that used in @xcite .",
    "for example , for the very first round of migration , replicas with identifiers of @xmath27 will be migrated , and this will be followed by replicas with identifiers of @xmath28 in the second round , replicas with identifiers of @xmath29 in the third round , and finally replicas with identifiers of @xmath30 in the fourth round .",
    "( the example assumed @xmath31 .",
    "it is straightforward to derive the selections for the cases when @xmath32 . )",
    "the selection is deterministic and can be easily computed based on the migration number .",
    "note that the migration number constitutes part of middleware state and will be transfered to all recovering replicas .",
    "the selection is independent of the view the replicas are in .",
    "the selection of the set of standby nodes as the target of migration is based on the elapsed time since the standby nodes were last sanitized .",
    "that is why each replica keeps track of the join time of each standby nodes . for each round of migration ,",
    "the @xmath2 standby nodes with the least elapsed time will be chosen .",
    "this is out of consideration that the probability of these nodes to be compromised at the time of migration is the least ( assuming brute - force attacks by adversaries ) .",
    "[ [ migration - synchronization - point - determination . ] ] migration synchronization point determination .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    it is important to ensure all ( correct ) replicas to use the same synchronization point when performing the service migration .",
    "this is achieved by ordering the migration - request message .",
    "the primary starts to order the message by sending a pre - prepare message for the migration - request to all backups , as described previously .    a backup verifies the piggybacked migration - request in a similar fashion as that for the init - migration message , except now the replica must check that it has received all the @xmath22@xmath23@xmath24 init - migration messages that the primary used to construct the migration - request , and the sets in @xmath18 and @xmath19 match those in the init - migration messages .",
    "the backup requests the primary to retransmit any missing init - migration messages .",
    "the backup accepts the pre - prepare message for the migration - request provided that the migration - request is correct and it has not accepted another pre - prepare message for the same sequence number in view @xmath16 . from now on , the replicas executes according to the three - phase bft algorithm  @xcite as usual until they commit the migration - request .",
    "[ [ state - transfer . ] ] state transfer .",
    "+ + + + + + + + + + + + + + +    when it is ready to execute the migration - request , a replica @xmath17 takes a checkpoint of its state ( both the application and the bft middleware state ) , and multicasts a migrate - now message to the @xmath2 standby nodes selected .",
    "the migrate - now message has the form @xmath4migrate - now@xmath33@xmath6@xmath13 , where @xmath34 is the sequence number assigned to the migration - request , @xmath35 is the digest of the checkpoint , and @xmath36 contains @xmath2 tuples . each tuple contains the identifiers of a source - node and target - node pair @xmath4@xmath37@xmath6 .",
    "the standby node @xmath38 , once completes the proactive recovery procedure , assumes the identifier @xmath1 of the active node it replaces .",
    "a replica sends the actual checkpoint ( together with all queued request messages , if it is the primary ) to the target nodes in separate messages .",
    "if a replica belongs to the @xmath2 nodes to be migrated , it performs the following additional actions : ( 1 ) it stops accepting new request messages , and ( 2 ) it reports to the trusted configuration manager as a candidate standby node .",
    "this replica is then handed over to the control of the configuration manager for sanitization .    before a standby node can be promoted to run an active replica , it must collect @xmath22@xmath23@xmath24 consistent migrate - now messages with the same sequence number and the digest of the checkpoint from different active replicas .",
    "once a standby node obtains a stable checkpoint , it applies the checkpoint to its state and starts to accept clients requests and participate the bft algorithm as an active replica .",
    "one can envisage that a fault node might want to continue sending messages to the active replicas and the clients , even if it has been migrated , before it is sanitized by the configuration manager . it is important to inform the clients the new membership so that they can ignore such messages sent by the faulty replica .",
    "the membership information is also important for the clients to accept messages send by new active replicas , and to send messages to these replicas .",
    "this is guaranteed by the new membership notification mechanism .",
    "the new membership notification is performed in a lazy manner to improve the performance unless a new active replica assumes the primary role , in which case , the notification is sent immediately to all known clients ( so that the clients can send their requests to the new primary ) . furthermore , the notification is sent only by the existing active replicas ( _ i.e.,@xmath0_not the new active replicas because the clients do not know them yet ) .",
    "normally , the notification is sent to a client only after the client has sent a request that is ordered after the migration - request message , _ i.e.,@xmath0_the sequence number assigned to the client s request is bigger than that of the migration - request .",
    "the notification message has the form @xmath4new - membership@xmath39@xmath6@xmath13 ( basically the same as the migration - now message without the checkpoint ) , where @xmath16 is the view in which the migration occurred , and @xmath34 is the sequence number assigned to the migration - request , and @xmath36 contains the tuples of the identifiers for the replicas in the previous and the new membership .",
    "note all active replicas should have the information .",
    "when a client collects @xmath40 consistent new - membership messages from different replicas , it updates its state accordingly and starts to accept replies from , and to send requests to , the new replicas .",
    "one demand migration can happen when the configuration manager detects a node to be faulty after it has been promoted to run an active replica .",
    "it can also happen when replicas have collected solid evidence that one or more replicas are faulty , such as a lying primary .",
    "the on - demand migration mechanism is rather similar to that for proactive recovery , with only two differences : ( 1 ) the migration is initiated on - demand , rather than by a migration timeout .",
    "however , replicas still must exchange the init - migration messages before the migration can take place ; ( 2 ) the selection procedure for the source node is omitted because the nodes to be swapped out are already decided , and the same number of target nodes are selected accordingly .",
    "the primary benefit of using the migration - based proactive recovery is a reduced vulnerability window .",
    "the term vulnerability window ( or window of vulnerability ) @xmath41 is introduced in @xcite . here",
    "@xmath42 is the time elapsed between when a replica becomes faulty and when it fully recovers from the fault , and @xmath43 is the key refreshment period . as long as no more than @xmath2 replicas become faulty during the window of @xmath44 , the invariants for byzantine fault tolerance will be preserved .    in the reboot - based proactive recovery scheme ,",
    "the vulnerability window @xmath45 is characterized to be @xmath46 , as shown in the upper half of figure  [ windowfig ] , where @xmath47 is the watchdog timeout , @xmath48 is the recovery time for a nonfaulty replica under normal load conditions .",
    "the dominating factors for recovery time include @xmath49 , the reboot time , and @xmath50 , the time it takes to save , restore and verify the replica state .",
    "the watchdog timeout @xmath47 is set roughly to @xmath51 to enable a staggered proactive recovery of @xmath2 replicas at a time .",
    "= 3.0 in    the composition of the vulnerability window for the migration - based proactive recovery is shown in the lower half of figure  [ windowfig ] .",
    "the time intervals specific to migration - based proactive recovery is labeled by the @xmath52 superscript . because the migration is coordinated in this recovery scheme , no watchdog timer is used and the term @xmath53 is now interpreted as the migration timer , _ i.e.,@xmath0_the time elapsed between two consecutive rounds of migrations of @xmath2 replicas each .",
    "this is very different from the watchdog timeout @xmath47 , which is statically configured prior to the start of each replica .",
    "because the recovery time in the migration - based proactive recovery is much shorter than that in the reboot - based recovery , and the migration is coordinated , it takes much shorter time to fully recovery all active replicas once .",
    "hence , @xmath42 can be much shorter for the migration - based recovery , which leads to a smaller vulnerability window .      under fault - free condition , neither the reboot - based nor the migration - based recovery scheme has much negative impact to the runtime performance unless the state is very large , as shown in the experimental data in  @xcite and in section  [ perfsec ] of this paper .",
    "however , in the presence of faulty nodes , the system availability can be reduced significantly in the reboot - based proactive recovery scheme , while the reduction in availability remains small in our migration - based recovery scheme .",
    "the see the benefit of the migration - based proactive recovery regarding the system availability in the presence of faults , we consider a specific case when the number of faulty nodes is @xmath2 and @xmath54 . while developing a thorough analytical model is certainly desirable , it is out of the scope of this paper .",
    "we assume that there are @xmath54 faulty replica at the beginning of the set of four rounds of migration to eradicate it .",
    "( recall that we assume that at most @xmath2 replicas can be compromised in one vulnerability window , which constitutes four rounds proactive recovery of @xmath2 replicas at a time and @xmath55 , therefore , it is not possible to end up with more than @xmath2 faulty replicas with this assumption . )",
    "we further assume that the proactive recovery rounds after the removal of the faulty replica has no negative impact on the system availability , and so does the case when the faulty replica is recovered in the same round of recovery .",
    "we also ignore the differences between the recovery time of a normal replica and that of a faulty one .    since @xmath54",
    ", the faulty node must be recovered in one of the four rounds of recovery .",
    "assuming that the faulty node is chosen randomly , it is recovered in even probability in either of the four rounds , _",
    "i.e.,@xmath0_@xmath56 , where @xmath57 .",
    "if the faulty replica is recovered in the first round of recovery , there is no reduction of system availability @xmath58 ( _ i.e.,@xmath0_@xmath59 ) .",
    "if the faulty replica is recovered in round @xmath17 , where @xmath60 , the system will not be available while a replica is recovering during each round because there will be insufficient number of correct replicas until the recovery is completed , and hence , the system availability @xmath61 in this case will be determined as @xmath62 therefore , the total system availability is @xmath63    for the reboot - based recovery , @xmath64 , and for the migration - based recovery , @xmath65 .",
    "it is not unreasonable to assume @xmath66 because the network bandwidth is similar to the disk io bandwidth in modern general - purpose systems . as shown in figure  [ avaifig](a )",
    ", the migration - based recovery can achieve much better system availability if the reboot time @xmath49 is large , which is generally the case .",
    "furthermore , as indicated in figure  [ avaifig](b ) , for the range of vulnerability window considered , the system availability is consistently higher for the migration - based proactive recovery than that for the reboot - based proactive recovery .",
    "= 3.0 in    = 6.0 in",
    "the proactive service migration mechanisms have been implemented and incorporated into the bft framework developed by castro , rodriguos and liskov  @xcite . due to the potential large state ,",
    "an optimization has been made , similar to the optimization on the reply messages in the original bft framework , _",
    "i.e.,@xmath0_instead of every replica sends its checkpoint to the target nodes of migration , only one actually sends the full checkpoint .",
    "the target node can verify if the copy of the full checkpoint is correct by comparing the digest of the checkpoint with the digests received from the replicas .",
    "if the checkpoint is not correct , the target node asks for a retransmission from other replicas .",
    "similar to @xcite , the performance measurements are carried out in general - purpose servers without hardware coprocessors .",
    "the related operations are simulated in software .",
    "furthermore , the trusted configuration manager is not developed as this is one of the no goals of this paper .",
    "the motivation of the measurements is to assess the runtime performance of the proactive service migration scheme for practical use .",
    "our testbed consists of a set of dell sc440 servers connected by a 100 mbps local - area network .",
    "each server is equipped with a single pentium dual - core 2.8ghz cpu and 1 gb of ram , and runs the suse linux 10.2 operation system .",
    "the micro - benchmarking example included in the original bft framework is adapted as the test application .",
    "the request and reply message length is fixed at 1 kb , and each client generates requests consecutively in a loop without any think time . each server replica simply echoes the payload in the request back to the client .",
    "four active nodes , four standby nodes , and up to eight client nodes are used in the experiment .",
    "this setup can tolerate a single byzantine faulty replica .",
    "the service migration interval is set to 70@xmath1 , corresponding to the minimum possible vulnerability window for a key exchanged interval of 15s and a maximum recovery time ( for a single replica ) of 10@xmath1 .    to characterize the runtime cost of the service migration scheme , we measure the recovery time for a single replica with and without the presence of clients , and the impact of proactive migration on the system performance perceived by clients . the recovery time is determined by measuring the time elapsed between the following two events : ( 1 ) the primary sending the pre - prepare message for the migration - request , and ( 2 ) the primary receiving a notification from the target standby node indicating that it has collected and applied the latest stable checkpoint .",
    "( the notification message is not part of the recovery protocol .",
    "it is inserted solely for the purpose of performance measurements . )",
    "we refer to this time interval as the service migration latency .",
    "the impact on the system performance is measured at the client by counting the number of calls it has made during one vulnerability window , with and without proactive migration - based recovery .",
    "the measurement results are summarized in figure  [ perfig ] .",
    "figure  [ perfig](a ) shows the service migration latency for various state sizes ( from 100 kb to about 10 mb ) .",
    "it is not surprising to see that the cost of migration is limited by the bandwidth available ( 100mbps ) because in our experiment , the time it takes to take a local checkpoint ( to memory ) and to restore one ( from memory ) is negligible .",
    "this is intentional for two reasons : ( 1 ) the checkpointing and restoration cost is very application dependent , and ( 2 ) such cost is the same regardless of the proactive recovery schemes used .",
    "furthermore , we measure the migration latency as a function of the system load in terms of the number of concurrent clients . the results are shown in figure  [ perfig](b ) .",
    "as can be seen , the migration latency increases more significantly for larger state when the system load is higher . when there are eight concurrent clients , the migration latency for a state size of 5 mb exceeds 10@xmath1 , which is the maximum recovery time we assumed in our availability analysis .",
    "this observation suggests the need for dynamic adjusting of some parameters related to the vulnerability window , in particular , the watchdog timeout used in the reboot - based recovery scheme .",
    "if the watchdog timeout is too short for the system to go through four rounds of proactive recovery ( of @xmath2 replicas at a time ) , there will be more than @xmath2 replicas going through proactive recoveries concurrently , which will decrease the system availability , even without any fault .",
    "our migration - based proactive recovery does not suffer from this problem . due to the use of coordinated recovery , when the system load increases",
    ", the vulnerability window automatically increases .",
    "figure  [ perfig](c ) shows the performance impact of proactive service migration as perceived by a single client . in the experiment",
    ", we choose to use the parameters consistent with those used in the availability analysis ( for migration - based recovery ) , _ i.e.,@xmath0_key exchange period of 15@xmath1 , maximum recovery time of 10@xmath1 , and a vulnerability window of 70@xmath1 .",
    "as can be seen , the impact of proactive migration on the system performance is quite acceptable . for a state smaller than 1 mb",
    ", the throughput is reduced only by 10% or less comparing with the no - proactive - recovery case .",
    "in addition , we have measured the migration performance in the presence of one ( crash ) faulty replica ( the recovering recovery is different from the crashed replica ) .",
    "the system throughput degradation is similar to that in fault - free condition .",
    "note that when there are only three correct replicas , the system throughput is reduced even without proactive migration , as shown in the figure .",
    "ensuring byzantine fault tolerance for long - running systems is an extremely challenging task .",
    "the pioneering work is carried out by castro and liskov . in",
    "@xcite , they proposed a reboot - based proactive recovery scheme as a way to repair compromised nodes periodically . the work is further extended by rodrigues and liskov in  @xcite .",
    "they proposed additional infrastructure support and related mechanisms to handle the cases when a damaged replica can not be repaired by a simple reboot .",
    "our work is inspired by both work .",
    "the novelty and the benefits of our service - migration scheme over the reboot - based proactive recovery scheme have been elaborated in section  [ benefitsec ] .",
    "other related work includes  @xcite . in  @xcite ,",
    "_ extended the bft algorithm to handle replicated clients and introduced another byzantine agreement ( ba ) step to ensure that all replicated clients receive the same set of replies . it was claimed that the mechanisms can also be used to perform online upgrading , which is important for long - running applications and not addressed in our work . however , it is not clear if the ba step on the replies is useful , while incurring significantly higher cost .",
    "if there are more than @xmath2 compromised server replicas , the integrity of the service is already broken , in which case , there is no use for the client replicas to receive the same faulty reply .",
    "finally , the reliance on extra nodes beyond the @xmath3 active nodes in our scheme may somewhat relates to the use of @xmath22 additional witness replicas in the fast byzantine consensus algorithm  @xcite .",
    "however , the extra nodes are needed for completely different purposes . in our scheme , they are required for proactive recovery for long - running byzantine fault tolerant systems . in @xcite , however , they are needed to reach byzantine consensus in fewer message delays .",
    "in this paper , we presented a novel proactive recovery scheme based on service migration for long - running byzantine fault tolerant systems .",
    "we described in detail the challenges and mechanisms needed for our migration - based proactive recovery to work .",
    "the migration - based recovery scheme has a number of unique benefits over previous work , including a smaller vulnerability window by shifting the time - consuming repairing step out of the critical recovery path , higher system availability under faulty conditions , and self - adaptation to different system loads .",
    "we validated these benefits both analytically and experimentally . for future work",
    ", we plan to investigate the design and implementation of the trusted configuration manager , in particular , the incorporation of the code attestation methods @xcite into the fault detection mechanisms , and the application of the migration - based recovery scheme to practical systems such as networked file systems .",
    "j. cowling , d. myers , b. liskov , r. rodrigues , and l. shrira .",
    "hq replication : a hybrid quorum protocol for byzantine fault tolerance . in _ proceedings of the seventh symposium on operating systems design and implementations _ ,",
    "seattle , washington , november 2006 .",
    "t. garfinkel , b. pfaff , j. chow , m. rosenblum , and d. boneh .",
    "terra : a virtual machine - based platform for trusted computing . in _ proceedings of the 19th symposium on operating system principles",
    "_ , october 2003 .",
    "m.  merideth , a.  iyengar , t.  mikalsen , s.  tai , i.  rouvellou , and p.  narasimhan .",
    "thema : byzantine - fault - tolerant middleware for web services applications . in _ proceedings of the ieee symposium on reliable distributed systems _ ,",
    "pages 131142 , 2005 .",
    "s. pallemulle , l. wehrman and k. goldman .",
    "byzantine fault tolerant execution of long - running distributed applications . in _ proceedings of the iasted international conference on parallel and",
    "distributed computing and systems _ , dallas ,",
    "tx , november 2006 .",
    "j.  yin , j .-",
    "martin , a.  venkataramani , l.  alvisi , and m.  dahlin . separating agreement from execution for byzantine fault tolerant services . in _ proceedings of the acm symposium on operating systems principles _ , pages 253267 , bolton landing , ny , usa , 2003 ."
  ],
  "abstract_text": [
    "<S> in this paper , we describe a novel proactive recovery scheme based on service migration for long - running byzantine fault tolerant systems . </S>",
    "<S> proactive recovery is an essential method for ensuring long term reliability of fault tolerant systems that are under continuous threats from malicious adversaries . </S>",
    "<S> the primary benefit of our proactive recovery scheme is a reduced vulnerability window . </S>",
    "<S> this is achieved by removing the time - consuming reboot step from the critical path of proactive recovery . </S>",
    "<S> our migration - based proactive recovery is coordinated among the replicas , therefore , it can automatically adjust to different system loads and avoid the problem of excessive concurrent proactive recoveries that may occur in previous work with fixed watchdog timeouts . </S>",
    "<S> moreover , the fast proactive recovery also significantly improves the system availability in the presence of faults .    </S>",
    "<S> * keywords : * proactive recovery , byzantine fault tolerance , service migration , replication , byzantine agreement </S>"
  ]
}