{
  "article_text": [
    "we consider discrete - time stochastic dynamic systems of the form @xmath2 where @xmath3 is the state , @xmath4 is the measurement at time step @xmath5 , @xmath6 is i.i.d .",
    "gaussian system noise , @xmath7 is i.i.d .",
    "gaussian measurement noise , @xmath8 is the transition / system function and @xmath9 is the measurement function . the graphical model of the considered dynamic system is given in fig .",
    "[ fig : gm ] .",
    "the noise covariance matrices @xmath10 , @xmath11 , the system function @xmath8 , and the measurement function @xmath9 are assumed known . if not stated otherwise , we assume nonlinear functions @xmath8 and @xmath9 . the initial state @xmath12 of the time series is distributed according to a gaussian prior distribution @xmath13 .",
    "the purpose of filtering and smoothing is to find approximations to the posterior distributions @xmath14 , where a subscript @xmath15 abbreviates @xmath16 , with @xmath17 for filtering and @xmath18 for smoothing .    in this paper",
    ", we consider gaussian approximations @xmath19 of the latent state posteriors @xmath14 .",
    "we use the shorthand notation @xmath20 where @xmath21 denotes the mean @xmath22 and @xmath23 denotes the covariance , @xmath24 denotes the time step under consideration , @xmath25 denotes the time step up to which we consider measurements , and @xmath26 denotes either the latent space ( @xmath27 ) or the observed space ( @xmath28 ) .    let us assume a prior @xmath29 and a sequence @xmath30 of noisy measurements of the latent states @xmath31 through the measurement function @xmath9 .",
    "the objective of _ filtering _ is to compute a posterior distribution @xmath32 over the latent state as soon as a new measurement @xmath33 is available .",
    "_ smoothing _ extends filtering and aims to compute the posterior state distribution of the hidden states @xmath34 , @xmath35 , given _ all _ measurements @xmath36 ( see e.g. ,  @xcite ) .",
    "given a prior @xmath37 on the initial state and a dynamic system ( e.g. , eqs .",
    "( [ eq : system equation])([eq : measurement ] ) ) , the objective of _ filtering _ is to infer a posterior distribution @xmath38 of the hidden state @xmath34 , @xmath39 , incorporating the evidence of the measurements @xmath40 .",
    "specific for gaussian filtering is that posterior distributions are approximated by gaussians  @xcite .",
    "approximations are required since generally a gaussian distribution mapped through a nonlinear function does not stay gaussian .",
    "assume a gaussian filter distribution @xmath41 is given ( if not , we employ the prior @xmath42 on the initial state .",
    "using bayes theorem , the filter distribution at time @xmath43 is @xmath44    [ prop : filter distribution ] gaussian filters approximate the filter distribution @xmath38 using a gaussian distribution @xmath45 .",
    "the moments of this approximation are in general computed through @xmath46 since the true moments of the joint distribution @xmath1 can in general not be computed analytically , approximations / estimates are used ( hence the @xmath47-symbols ) .    generally , filtering proceeds by alternating between predicting ( _ time update _ ) and correcting ( _ measurement update _ )  @xcite :    1 .",
    "time update ( predictor ) 1 .",
    "compute the predictive distribution @xmath48 .",
    "measurement update ( corrector ) 1 .",
    "compute the joint distribution @xmath49 of the next latent state and the next measurement .",
    "2 .   measure @xmath33 .",
    "3 .   compute the posterior @xmath50 .    in the following , we detail these steps to prove prop .",
    "[ prop : filter distribution ] .      1 .",
    "compute the predictive distribution @xmath48 .",
    "the predictive distribution of state @xmath51 at time @xmath43 given the evidence of measurements up to time @xmath52 is @xmath53 where @xmath54 is the transition probability .",
    "in gaussian filters , the predictive distribution @xmath48 in eq .   is approximated by a gaussian distribution , whose exact mean and covariance are given by @xmath55\\ ! = \\ ! { \\mathds{e}}_{{{\\boldsymbol{\\mathbf{x}}}}_{t-1},{{\\boldsymbol{\\mathbf{w}}}}_t}[f({{\\boldsymbol{\\mathbf{x}}}}_{t-1})\\!+\\!{{\\boldsymbol{\\mathbf{w}}}}_t|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1 } ] = \\int f({{\\boldsymbol{\\mathbf{x}}}}_{t-1}){{p}}({{\\boldsymbol{\\mathbf{x}}}}_{t-1}|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1}){\\operatorname{d}\\!}{{\\boldsymbol{\\mathbf{x}}}}_{t-1}\\ , ,    \\label{eq : mean state predicted measurement}\\\\    { { \\ensuremath{\\mathbf{\\sigma}}}}_{t|t-1}^x & \\coloneqq{\\mathrm{cov}}_{{{\\boldsymbol{\\mathbf{x}}}}_t}[{{\\boldsymbol{\\mathbf{x}}}}_t|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1 } ] = { \\mathrm{cov}}_{{{\\boldsymbol{\\mathbf{x}}}}_{t-1}}[f({{\\boldsymbol{\\mathbf{x}}}}_{t-1})|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1 } ] + { \\mathrm{cov}}_{{{\\boldsymbol{\\mathbf{w}}}}_t}[{{\\boldsymbol{\\mathbf{w}}}}_t ] \\nonumber \\\\ & = \\underbrace{\\int f({{\\boldsymbol{\\mathbf{x}}}}_{t-1})f({{\\boldsymbol{\\mathbf{x}}}}_{t-1}){^\\top}{{p}}({{\\boldsymbol{\\mathbf{x}}}}_{t-1}|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1}){\\operatorname{d}\\!}{{\\boldsymbol{\\mathbf{x}}}}_{t-1}-{{\\boldsymbol{\\mathbf{\\mu}}}}_{t|t-1}^x\\big({{\\boldsymbol{\\mathbf{\\mu}}}}_{t|t-1}^x){^\\top } } _ { =      { \\mathrm{cov}}_{{{\\boldsymbol{\\mathbf{x}}}}_{t-1}}[f({{\\boldsymbol{\\mathbf{x}}}}_{t-1})|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1 } ] } + \\underbrace{{{\\ensuremath{\\mathbf{q } } } } } _ { = { \\mathrm{cov}}_{{{\\boldsymbol{\\mathbf{w}}}}_t}[{{\\boldsymbol{\\mathbf{w}}}}_t ] }    \\label{eq : cov state predicted measurement}\\end{aligned}\\ ] ] respectively . in eq .",
    ", we exploited that the noise term @xmath56 in eq .",
    "has mean zero and is independent . a gaussian approximation to the time update @xmath48",
    "is then given by @xmath57 .      1 .",
    "compute the joint distribution @xmath58 in gaussian filters , a gaussian approximation to this joint is an intermediate step toward the desired gaussian approximation of the posterior @xmath38 . if the mean and the covariance of the joint in eq .",
    "( [ eq:1st joint ] ) can be computed or estimated , the desired filter distribution corresponds to the conditional @xmath38 and is given in closed form  @xcite . +",
    "our objective is to compute a gaussian approximation @xmath59 to the joint @xmath1 in eq .",
    "( [ eq:1st joint ] ) . since a gaussian approximation @xmath60 to the marginal @xmath48",
    "is known from the time update , it remains to compute the marginal @xmath61 and the cross - covariance @xmath62 $ ] .",
    "* the marginal @xmath63 of the joint in eq .   is @xmath64 where the state @xmath34 is integrated out according to the time update @xmath48 .",
    "the measurement eq .",
    "( [ eq : measurement ] ) , yields @xmath65 . hence , the exact mean of the marginal is @xmath66= { \\mathds{e}}_{{{\\boldsymbol{\\mathbf{x}}}}_t}[g({{\\boldsymbol{\\mathbf{x}}}}_t)|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1 } ] = \\int g({{\\boldsymbol{\\mathbf{x}}}}_t)\\underbrace{{{p}}({{\\boldsymbol{\\mathbf{x}}}}_t|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1})}_{\\text{time update}}{\\operatorname{d}\\!}{{\\boldsymbol{\\mathbf{x}}}}_t \\label{eq : mean predicted measurement}\\end{aligned}\\ ] ] since the noise term @xmath67 in the measurement eq .",
    "( [ eq : measurement ] ) is independent and has zero mean .",
    "similarly , the exact covariance of the marginal @xmath61 is @xmath68 + { \\mathrm{cov}}_{{{\\boldsymbol{\\mathbf{v}}}}_t}[{{\\boldsymbol{\\mathbf{v}}}}_t]\\nonumber \\\\   & = \\underbrace{\\int g({{\\boldsymbol{\\mathbf{x}}}}_t)g({{\\boldsymbol{\\mathbf{x}}}}_t){^\\top}{{p}}({{\\boldsymbol{\\mathbf{x}}}}_t|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1}){\\operatorname{d}\\!}{{\\boldsymbol{\\mathbf{x}}}}_t-{{\\boldsymbol{\\mathbf{\\mu}}}}_{t|t-1}^{z}\\big({{\\boldsymbol{\\mathbf{\\mu}}}}_{t|t-1}^{z}){^\\top}}_{={\\mathrm{cov}}_{{{\\boldsymbol{\\mathbf{x}}}}_t}[g({{\\boldsymbol{\\mathbf{x}}}}_t)|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1 } ] } + \\underbrace{{{\\ensuremath{\\mathbf{r}}}}}_{={\\mathrm{cov}}_{{{\\boldsymbol{\\mathbf{v}}}}_t}[{{\\boldsymbol{\\mathbf{v}}}}_t]}\\ , .",
    "\\label{eq : covariance predicted measurement}\\end{aligned}\\ ] ] hence , a gaussian approximation to the marginal measurement distribution @xmath61 is given by @xmath69 with the mean and covariance given in eqs .",
    "( [ eq : mean predicted measurement ] ) and  ( [ eq : covariance predicted measurement ] ) , respectively . * due to the independence of @xmath67 , the exact cross - covariance terms of the joint in eq .",
    "are @xmath70 = { \\mathds{e}}_{{{\\boldsymbol{\\mathbf{x}}}}_t,{{\\boldsymbol{\\mathbf{z}}}}_t}[{{\\boldsymbol{\\mathbf{x}}}}_t{{\\boldsymbol{\\mathbf{{z}}}}}_t{^\\top}|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1}]-{\\mathds{e}}_{{{\\boldsymbol{\\mathbf{x}}}}_t}[{{\\boldsymbol{\\mathbf{x}}}}_t|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1}]{\\mathds{e}}_{{{\\boldsymbol{\\mathbf{{z}}}}}_t}[{{\\boldsymbol{\\mathbf{{z}}}}}_t|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1}]{^\\top}\\nonumber\\\\    & = \\iint{{\\boldsymbol{\\mathbf{x}}}}_t{{\\boldsymbol{\\mathbf{{z}}}}}_t{^\\top}{{p}}({{\\boldsymbol{\\mathbf{x}}}}_t,{{\\boldsymbol{\\mathbf{{z}}}}}_t|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1}){\\operatorname{d}\\!}{{\\boldsymbol{\\mathbf{{z}}}}}_t{\\operatorname{d}\\!}{{\\boldsymbol{\\mathbf{x}}}}_t-{{\\boldsymbol{\\mathbf{\\mu}}}}_{t|t-1}^x({{\\boldsymbol{\\mathbf{\\mu}}}}_{t|t-1}^{z}){^\\top}\\nonumber\\,.\\end{aligned}\\ ] ] plugging in the measurement eq .",
    ", we obtain @xmath71 2 .",
    "measure @xmath33 .",
    "3 .   compute a gaussian approximation of the posterior @xmath38 .",
    "this boils down to computing a conditional from the gaussian approximation to the joint distribution @xmath1 in eq .  .",
    "the expressions from eqs .",
    "( [ eq : mean state predicted measurement ] ) ,  ,  ( [ eq : mean predicted measurement ] ) ,  ( [ eq : covariance predicted measurement ] ) , and  ( [ eq : cross - covariance x and z ] ) , yield a gaussian approximation @xmath72 of the filter distribution @xmath38 , where @xmath73    generally , the required integrals in eqs .  , , , , and can not be computed analytically .",
    "hence , approximations of the moments are typically used in eqs .   and .",
    "this concludes the proof of prop .",
    "[ prop : filter distribution ] .      in any bayes filter",
    "@xcite , the sufficient components to computing the gaussian filter distribution in eqs .",
    "( [ eq : generic filter mean ] ) and  ( [ eq : generic filter covariance ] ) are the mean and the covariance of the joint distribution @xmath1 .",
    "generally , the required integrals in eqs .  , , , , and can not be computed analytically .",
    "one exception are _ linear _ functions @xmath8 and @xmath9 , where the analytic solutions to the integrals are embodied in the kalman filter  @xcite : using the rules of predicting in linear gaussian systems , the kalman filter equations can be recovered when plugging in the respective means and covariances into eq .",
    "( [ eq : generic filter mean ] ) and  ( [ eq : generic filter covariance ] )  @xcite . in many",
    "_ nonlinear _",
    "dynamic systems , filtering algorithms approximate probability distributions ( see e.g. , the ukf  @xcite and the ckf  @xcite ) or the functions @xmath8 and @xmath9 ( see e.g. , the ekf  @xcite or the gp - bayes filters  @xcite ) . using the means and ( cross-)covariances computed by these algorithms and plugging them into eqs .",
    "( [ eq : generic filter mean])([eq : generic filter covariance ] ) , recovers the corresponding filter update equations for the ekf , the ukf , the ckf , and the gp - bayes filters .",
    "in this section , we present a general probabilistic perspective on gaussian rts smoothers and derive sufficient conditions for gaussian smoothing .    the smoothed state distribution is the posterior distribution of the hidden state given _ all _ measurements @xmath74    [ prop : smoothing ] for gaussian smoothers , the mean and the covariance of a gaussian approximation to the distribution @xmath75 are generally computed as @xmath76{\\mathrm{cov}}[{{\\boldsymbol{\\mathbf{x}}}}_t|{{\\boldsymbol{\\mathbf{{z}}}}}_{1:t-1}]{^{-1}}\\nonumber\\\\    & = { { \\ensuremath{\\mathbf{\\sigma}}}}_{t-1,t|t-1}^x({{\\ensuremath{\\mathbf{\\sigma}}}}_{t|t-1}^x){^{-1}}\\,.\\end{aligned}\\ ] ]    the smoothed state distribution at the terminal time step @xmath77 is equivalent to the filter distribution @xmath78  @xcite .",
    "the distributions @xmath79 , @xmath80 , of the smoothed states can be computed recursively according to @xmath81 by integrating out the smoothed hidden state at time step @xmath43 . in eq .",
    ", we exploited that @xmath82 is conditionally independent of the future measurements @xmath83 given @xmath34 .    to compute the smoothed state distribution in eq .",
    "( [ eq : smoothing recursion ] ) , we need to multiply a distribution in @xmath34 with a distribution in @xmath82 and integrate over @xmath34 .",
    "to do so , we follow the steps :    1 .",
    "compute the conditional @xmath84 .",
    "2 .   formulate @xmath85 as an unnormalized distribution in @xmath34 .",
    "3 .   multiply the new distribution with @xmath75 .",
    "4 .   solve the integral in eq .  .",
    "we now examine these steps in detail .",
    "assume a known ( gaussian ) smoothed state distribution @xmath75 .    1 .",
    "compute a gaussian approximation to the conditional @xmath84 .",
    "we compute the conditional in two steps : first , we compute a gaussian approximation to the joint distribution @xmath86 .",
    "second , we apply the rules of computing conditionals to this joint gaussian .",
    "let us start with a gaussian approximation @xmath87 to the joint @xmath0 and have a closer look at its components : a gaussian approximation of the filter distribution @xmath88 at time step @xmath52 is known and is the first marginal distribution in eq .",
    "( [ eq : p(x_t-1,x_t|y ) ] ) .",
    "the second marginal @xmath60 is the time update and also known from filtering . to fully determine the joint in eq .",
    "( [ eq : p(x_t-1,x_t|y ) ] ) , we require the cross - covariance matrix @xmath89 where we used the means @xmath90 and @xmath91 of the measurement update and the time update , respectively . the zero - mean independent noise in the system eq .",
    "( [ eq : system equation ] ) does not influence the cross - covariance matrix .",
    "the cross - covariance matrix in eq .",
    "( [ eq : cross - covariance smoothing ] ) can be pre - computed during filtering since it does not depend on future measurements .",
    "+ this concludes the first step ( computation of the joint gaussian ) of the computation of the desired conditional . + in the second step ,",
    "we apply the rules of gaussian conditioning to obtain the desired conditional distribution @xmath84 . for a shorthand notation",
    ", we define @xmath92 and obtain a gaussian approximation @xmath93 of the conditional distribution @xmath94 with @xmath95 2 .",
    "formulate @xmath93 as an unnormalized distribution in @xmath34 .",
    "the square - root of the exponent of @xmath93 contains @xmath96 with @xmath97 , which is a linear function of both @xmath82 and @xmath34 .",
    "we now reformulate the conditional gaussian @xmath93 as a gaussian in @xmath98 with mean @xmath99 and the unchanged covariance matrix @xmath100 .",
    "we obtain the conditional @xmath101 and @xmath102 .",
    "note that @xmath93 is an unnormalized gaussian in @xmath34 , see eq .  .",
    "the matrix @xmath103 defined in eq .",
    "( [ eq : j - matrix smoothing ] ) is quadratic , but not necessarily invertible , in which case we take the pseudo - inverse .",
    "however , we will see that this inversion will be unnecessary to obtain the final result .",
    "3 .   multiply the new distribution with @xmath75 . to determine @xmath79 , we multiply the gaussian in eq .",
    "( [ eq : n(rx ) is n(x ) ] ) with the smoothed gaussian state distribution @xmath104 , which yields the gaussian approximation @xmath105 of @xmath106 , for some @xmath107 , @xmath108 , where @xmath109 is the inverse normalization constant of @xmath110 .",
    "4 .   solve the integral in eq .  .",
    "since we integrate over @xmath34 in eq .",
    "( [ eq : smoothing recursion ] ) , we are solely interested in the parts that make eq .",
    "( [ eq : density mult backward ] ) unnormalized , i.e. , the constants @xmath111 and @xmath109 , which are independent of @xmath34 . the constant @xmath109 in eq .",
    "( [ eq : density mult backward ] ) can be rewritten as @xmath112 by reversing the step that inverted the matrix @xmath103 , see eq .",
    "( [ eq : n(rx ) is n(x ) ] ) .",
    "then , @xmath112 is given by @xmath113 since @xmath114 ( plug eq .   into eq .  ) , the desired smoothed state distribution is @xmath115 where the mean and the covariance are given in eq .",
    "( [ eq : mean smoother ] ) and eq .",
    "( [ eq : cov smoother ] ) , respectively .",
    "this result concludes the proof of prop .",
    "[ prop : smoothing ] .",
    "after filtering , to determine a gaussian approximation to the distribution @xmath79 of the smoothed state at time @xmath52 , only a few additional ingredients are required : the matrix @xmath103 in eq .",
    "and gaussian approximations to the smoothed state distribution @xmath116 at time @xmath43 and the predictive distribution @xmath48 . everything but the matrix @xmath103",
    "can be precomputed either during filtering or in a previous step of the smoothing recursion .",
    "note that @xmath103 can also be precomputed during filtering .",
    "hence , for gaussian rts smoothing it is sufficient to determine gaussian approximations to both the joint distribution @xmath117 of the state and the measurement for the filter step and the joint distribution @xmath0 of two consecutive states .",
    "[ sec : implications ] using the results from secs .  [ sec : filtering ] and  [ sec : smoothing ] , we conclude that for filtering and rts smoothing it is sufficient to compute or estimate the means and the covariances of the joint distribution @xmath0 between two consecutive states ( smoothing ) and the joint distribution @xmath1 between a state and the subsequent measurement ( filtering and smoothing ) .",
    "this result has two implications :    1 .",
    "gaussian filters / smoothers can be distinguished by their approximations to these joint distributions .",
    "2 .   if there exists an algorithm to compute or to estimate the means and the covariances of the joint distributions @xmath118 , where @xmath119 , the algorithm can be used for filtering and rts smoothing .    in the following ,",
    "we first consider common filtering and smoothing algorithms and describe how they compute gaussian approximations to the joint distributions @xmath0 and @xmath1 , respectively , which emphasizes the first implication ( sec .",
    "[ sec : example realizations ] ) .",
    "after that , for the second implication of our results , we take an algorithm for estimating means and covariances of joint distributions and turn this algorithm into a filter / smoother ( sec .  [ sec : gibbs filter ] ) .",
    "[ tab : joint ]    tab .",
    "[ tab : joint ] gives an overview of how the kalman filter , the ekf , the ukf , and the ckf represent the means and the ( cross-)covariances of the joint distributions @xmath1 and @xmath0 . in tab .",
    "[ tab : joint ] , we use the shorthand notation @xmath120 . for example , we defined @xmath121 .    in the kalman filter ,",
    "the transition function @xmath8 and the measurement function are linear and represented by the matrices @xmath122 and @xmath123 , respectively .",
    "the ekf linearizes @xmath8 and @xmath9 resulting in the matrices @xmath124 and @xmath125 , respectively .",
    "the ukf computes @xmath126 sigma points @xmath127 and uses their mappings through @xmath8 and @xmath9 to compute the desired moments , where @xmath128 and @xmath129 are the weights used for computing the mean and the covariance , respectively ( see  @xcite , pp .",
    "the ckf computations are nearly equivalent to the ukf s computations with slight modifications : first , the ckf only requires @xmath130 cubature points @xmath127 .",
    "the cubature points are chosen as the intersection of a @xmath131-dimensional unit sphere with the coordinate system .",
    "thus , the sums run from 1 to @xmath130 .",
    "second , the weights @xmath132 are all equal  @xcite .    although none of these algorithms computes the joint distributions @xmath1 and @xmath0 explicitly , they all do so implicitly . using the means and covariances in fig .  [",
    "tab : joint ] in the filtering and smoothing eqs .",
    "( [ eq : proposition filter mean ] ) ,  ( [ eq : proposition filter covariance ] ) ,  ( [ eq : proposition mean smoother ] ) , and  ( [ eq : proposition cov smoother ] ) , the results from the original papers  @xcite are recovered .",
    "to the best of our knowledge , tab .",
    "[ tab : joint ] is the first presentation of the cks .",
    "we now derive a gaussian filter and rts smoother based on gibbs sampling  @xcite .",
    "gibbs sampling is an example of a markov chain monte carlo ( mcmc ) algorithm and often used to infer the parameters of the distribution of a given data set . in the context of filtering and rts smoothing , we use gibbs sampling for inferring the mean and the covariance of the distributions @xmath0 and @xmath1 , respectively , which is sufficient for gaussian filtering and rts smoothing , see sec .",
    "[ sec : implications ] .",
    "* init : * @xmath133 infer moments of @xmath0 infer moments of @xmath1 measure @xmath33 compute @xmath134 compute @xmath135    alg .",
    "[ alg : gibbs - rtss ] details the high - level steps of the gibbs - rtss .",
    "[ fig : gibbs filter ]    at each time step , we use gibbs sampling to infer the moments of the joint distributions @xmath0 and @xmath1 . fig .",
    "[ fig : gibbs filter ] shows the graphical model for inferring the mean @xmath22 and the covariance @xmath136 from the joint data set @xmath127 using gibbs sampling .",
    "the parameters of the conjugate priors on the mean @xmath22 and the covariance @xmath136 are denoted by @xmath137 and @xmath138 , respectively .    to infer the moments of the joint @xmath0",
    ", we first generate i.i.d .",
    "samples from the filter distribution @xmath88 and map them through the transition function @xmath8 . the samples and their mappings serve as samples @xmath127 from the joint distribution @xmath0 . with a conjugate gaussian prior @xmath139 on the joint mean , and a conjugate inverse wishart prior distribution @xmath140 on the joint covariance matrix",
    ", we infer the posterior distributions on @xmath22 and @xmath136 . by sampling from these posterior distributions ,",
    "we obtain unbiased estimates of the desired mean and the covariance of the joint @xmath0 as the sample average ( after a burn in ) .",
    "to infer the mean and the covariance of the joint @xmath1 , we proceed similarly : we generate i.i.d .",
    "samples from the distribution @xmath48 , which are subsequently mapped through the measurement function .",
    "the combined data set of i.i.d . samples and their mappings define the joint data set @xmath127 .",
    "again , we choose a conjugate gaussian prior on the mean vector and a conjugate inverse wishart prior on the covariance matrix of the joint @xmath1 . using gibbs sampling , we sample means and covariances from the posteriors and",
    "obtain unbiased estimates for the mean and the covariance of the joint @xmath1 .",
    "[ alg : gibbs ] outlines the steps for computing the joint distribution @xmath1 .",
    "pass in marginal distribution @xmath141 , burn - in period @xmath142 , number @xmath143 of gibbs iterations , size @xmath144 of data set    init .",
    "conjugate priors on joint mean and covariance @xmath145 and @xmath146    @xmath147_{i=1}^n$ ] generate joint data set    sample @xmath148    sample @xmath149    update @xmath150 posterior parameter ( mean ) of @xmath151    update @xmath152 posterior parameter ( covariance ) of @xmath151    sample @xmath153 sample mean of the joint    update @xmath154 posterior hyper - parameter ( scale matrix ) of @xmath155    update @xmath156 posterior hyper - parameter ( degrees of freedom ) of @xmath155    sample @xmath157 sample covariance of the joint    @xmath158 $ ]    @xmath159 $ ] unbiased estimate of the covariance of the joint distribution    @xmath160    since the chosen priors for the mean and the covariance are conjugate priors , all updates of the posterior hyper - parameters can be computed analytically  @xcite .",
    "the moments of @xmath0 , which are required for smoothing , are computed similarly by exchanging the pass - in distributions and the mapping function .",
    "as a proof of concept , we show that the gibbs - rtss proposed in sec .  [",
    "sec : gibbs filter ] performs well in linear and nonlinear systems . as performance measures ,",
    "we consider the expected root mean square error ( rmse ) and the expected negative log - likelihood ( nll ) per data point in the trajectory . for a single trajectory ,",
    "the nll is given by @xmath161 where @xmath162 for filtering and @xmath163 for smoothing .",
    "while the rmse solely penalizes the distance of the true state and the mean of the filtering / smoothing distribution , the nll measures the coherence of the filtering / smoothing distributions , i.e. , the nll values are high if @xmath164 is an unlikely observation under @xmath165 , @xmath166 . in our experiments , we chose a time horizon @xmath167 .",
    "first , we tested the performance of the gibbs - filter / rtss in the linear system @xmath168 where @xmath169 . in a linear system ,",
    "the ( e)kf is optimal and unbiased  @xcite . the gibbs - filter / rtss perform as well as the ekf / eks as shown in fig .",
    "[ tab : results linear ] , which shows the expected performances ( with the corresponding standard errors ) of the filters / smoothers over 100 independent runs , where @xmath170 .",
    "the gibbs - sampler parameters were set to @xmath171 , alg .",
    "[ alg : gibbs ] .      as a nonlinear example",
    ", we consider the dynamic system @xmath172 with exactly the same setup as in  @xcite : @xmath173 , @xmath174 , and @xmath175 .",
    "this system is challenging for gaussian filters due to its quadratic measurement equation and its highly nonlinear system equation .",
    "we run the gibbs - rtss , the eks , the cks , and the urtss  @xcite for comparison .",
    "we chose the gibbs parameters @xmath171 .",
    "for 100 independent runs starting from @xmath170 , we report the expected rmse and nll performance measures in fig .  [",
    "tab : results nonlinear ] .",
    "both high expected nll - values and the fact that smoothing makes them even higher hint at the incoherencies of the ekf / eks , the ckf / cks , and the ukf / urtss .",
    "the gibbs - rtss was the only considered smoother that consistently improved the results of the filtering step .",
    "therefore , we conclude that the gibbs - filter / rtss is coherent .",
    "[ fig : results ] shows example realizations of filtering and smoothing using the gibbs - filter / rtss , the ekf / eks , the ckf / cks , and the ukf / urtss , respectively .",
    "the gibbs - filter / rtss appropriately inferred the variances of the latent state while the other filters / smoothers did not ( neither of them is moment - preserving ) , which can lead to incoherent filtering / smoothing distributions  @xcite , see also fig .  [",
    "tab : results nonlinear ] .",
    "our gibbs - filter / rtss differs from  @xcite , where gibbs sampling is used to infer the noise in a linear system .",
    "instead , we infer the means and covariances of the full joint distributions @xmath0 and @xmath1 in nonlinear systems from data . neither the gibbs - filter nor the gibbs - rtss require to know the noise matrices @xmath176 , but they can be inferred as a part of the joint distributions if access to the dynamic system is given . unlike the gaussian particle filter  @xcite ,",
    "the proposed gibbs - filter is not a particle filter .",
    "therefore , it does not suffer from degeneracy due to importance sampling .",
    "although the gibbs - filter is computationally more involved than the ekf / ukf / ckf , it can be used as a baseline method to evaluate the accuracy and coherence of more efficient algorithms : when using sufficiently many samples the gibbs - filter can be considered a close approximation to a moment - preserving filter in nonlinear stochastic systems .    the sampling approach to inferring the means and covariances of two joint distributions proposed in this paper can be extended to infer the means and covariances of a single joint , namely , @xmath177 . this would increase the dimensionality of the parameters to be inferred , but it would remove slight inconsistencies that appear in the present approach : ideally , the marginals @xmath48 , i.e. , the time update , which can be obtained from both joints @xmath0 and @xmath1 are identical . due to the finite number of samples , small errors are introduced . in our experiments , they were small , i.e. , the relative difference error was smaller than @xmath178 . using",
    "the joint @xmath177 would avoid this kind of error .",
    "the gibbs - filter / rtss only need to be able to evaluation the system and measurement functions .",
    "no further requirements such as differentiability are needed .",
    "a similar procedure for mcmc - based smoothing is applicable when , instead of gibbs sampling , slice sampling  @xcite or elliptical slice sampling  @xcite is used , potentially combined with gps that model the functions @xmath8 and @xmath9 .",
    "the gibbs - rtss code is publicly available at mloss.org .    in the context of gaussian process dynamic systems , the gp - ekf , the gp - ukf  @xcite , and the gp - adf  @xcite",
    "can directly be extended to smoothers using the results from this paper .",
    "the gp - urtss ( smoothing extension of the gp - ukf ) and the gp - rtss ( smoothing extension of the gp - adf ) are presented in  @xcite .",
    "using a general probabilistic perspective on gaussian filtering and smoothing , we first showed that it is sufficient to determine gaussian approximations to two joint probability distributions to perform gaussian filtering and smoothing .",
    "computational approaches to gaussian filtering and rauch - tung - striebel smoothing can be distinguished by their respective methods used to determining two joint distributions .",
    "second , our results allow for a straightforward derivation and implementation of novel gaussian filtering and smoothing algorithms , e.g. , the cubature kalman smoother .",
    "additionally , we presented a filtering smoothing algorithm based on gibbs sampling as an example .",
    "our experimental results show that the proposed gibbs - filter / gibbs - rtss compares well with state - of - the - art gaussian filters and rts smoothers in terms of robustness and accuracy .",
    "the authors thank s. mohamed , p. orbanz , m. krainin , and d. fox for valuable suggestions and discussions .",
    "mpd has been supported by onr muri grant n00014 - 09 - 1 - 1052 and by intel labs .",
    "ho has been partially supported by the swedish foundation for strategic research in the center moviii and by the swedish research council in the linnaeus center cadics ."
  ],
  "abstract_text": [
    "<S> we present a general probabilistic perspective on gaussian filtering and smoothing . </S>",
    "<S> this allows us to show that common approaches to gaussian filtering / smoothing can be distinguished solely by their methods of computing / approximating the means and covariances of joint probabilities . </S>",
    "<S> this implies that novel filters and smoothers can be derived straightforwardly by providing methods for computing these moments . </S>",
    "<S> based on this insight , we derive the cubature kalman smoother and propose a novel robust filtering and smoothing algorithm based on gibbs sampling .    </S>",
    "<S> inference in latent variable models is about extracting information about a not directly observable quantity , the latent variable , from noisy observations . </S>",
    "<S> both recursive and batch methods are of interest and referred to as _ filtering _ respective _ smoothing_. filtering and smoothing in latent variable time series models , including hidden markov models and dynamic systems , have been playing an important role in signal processing , control , and machine learning for decades  @xcite .    in the context of dynamic systems , filtering is widely used in control and robotics for online bayesian state estimation  @xcite , while smoothing is commonly used in machine learning algorithms for parameter learning  @xcite . for computational efficiency reasons , many filters and smoothers </S>",
    "<S> approximate appearing probability distributions by gaussians . </S>",
    "<S> this is why they are referred to as _ gaussian filters / smoothers_.    in the following , we discuss gaussian filtering and smoothing from a general probabilistic perspective without focusing on particular implementations . </S>",
    "<S> we identify the high - level concepts and the components required for filtering and smoothing , while avoiding getting lost in the implementation and computational details of particular algorithms ( see e.g. , standard derivations of the kalman filter  @xcite ) .    </S>",
    "<S> we show that for gaussian filters / smoothers for ( non)linear systems ( including common algorithms such as the extended kalman filter ( ekf )  @xcite , the cubature kalman filter ( ckf )  @xcite , or the unscented kalman filter ( ukf )  @xcite ) can be distinguished by their means to computing gaussian approximations to the joint probability distributions @xmath0 and @xmath1 . </S>",
    "<S> our results also imply that novel filtering and smoothing algorithms can be derived straightforwardly , given a method to determining the moments of these joint distributions . using this insight , </S>",
    "<S> we present and analyze the cubature kalman smoother ( cks ) and a filter and an rts smoother based on gibbs sampling .    </S>",
    "<S> we start this paper by setting up the problem and the notation , sec .  </S>",
    "<S> [ sec : probform ] . </S>",
    "<S> we thereafter proceed by reviewing gaussian filtering and rts smoothing from a high - level probabilistic perspective to derive sufficient conditions for gaussian filtering and smoothing , respectively ( secs .  </S>",
    "<S> [ sec : filtering ] and  [ sec : smoothing ] ) . </S>",
    "<S> the implications of this result are discussed in sec .  </S>",
    "<S> [ sec : results ] , which lead to the derivation of a novel gaussian filter and rts smoother based on gibbs sampling . </S>",
    "<S> sec .  </S>",
    "<S> [ sec : numerical evaluation ] provides proof - of - concept numerical evaluations for the proposed method for both linear and nonlinear systems . </S>",
    "<S> secs .  </S>",
    "<S> [ sec : discussion][sec : conclusion ] discuss related work and conclude the paper . </S>"
  ]
}