{
  "article_text": [
    "one of the most elementary system configuation models in multi - user information theory is the broadcast channel ( bc ) .",
    "it has been introduced in the early seventies of the twentieth century by cover @xcite , and since then , a vast amount of papers and books , studying different topics of the broadcast problem , have been published .",
    "generally speaking , the bc is a communication model , where a single transmitter wishes to communicate different messages to two or more receivers .",
    "the various messages may be private ( i.e. , aimed to one receiver only ) or common ( i.e. , aimed to two or more receivers ) .",
    "although the characterization of the capacity region of the general bc is still an open problem , some special cases have been solved , most notably , the degraded bc ( dbc ) , first presented in @xcite . the capacity region of the dbc , conjectured by cover , was first proved to be achievable by bergmans @xcite , and the converse was established by bergmans @xcite and gallager @xcite .",
    "another special case , which is somewhat more general than the dbc and which was first introduced and solved by krner and marton @xcite , is the broadcast channel with degraded message sets , also known as the asymmetric broadcast channel ( abc ) .",
    "the direct part of their coding theorem relys on bergmans scheme , which suggested the use of an hierarchical random code : first generate  cloud centers \" , which designate messages intended to both the receiver with the relatively high channel quality , henceforth referred to as the _ strong user _ , and the receiver with the relatively low channel quality , henceforth referred to as the _ weak user_. then , in the second step ,  around \" each cloud center , generate a codeword for each message that is intended to the strong user only . the transmitter sends a codeword pertaining to one of the clouds .",
    "the strong decoder fully decodes both the common message and his private message , whereas the weak decoder decodes the common message only .",
    "other channels in which one receiver is superior to another and channels with nested information were studied by csiszr and krner @xcite and by el gamal @xcite , to name a few .",
    "multi  user information theory is , first and foremost , driven by the quest to characterize capacity regions , i.e. , the region of all sets of rates that allow reliable communication ( a.k.a .",
    "achievable rates ) . a somewhat sharper performance metric concerns the exponential decay rate ( the error exponent ) of the probability of error for each user , as a function of the coding rates within the interior of the capacity region . on top of that ,",
    "an interesting question concerns the trade  off between the error exponent of the strong user and the one of the weak user , or equivalently , the achievable region in the plane of error exponents for a given set of coding rates .",
    "while the capacity regions of the dbc and the abc have been known for many years , only little has been known about their reliability functions .",
    "earlier works on error exponents for the general dbc and abc include those of gallager @xcite and krner and sgarro @xcite , respectively . in both works ,",
    "the coding scheme of @xcite was adopted , but the decoder was sub  optimal .",
    "more recently , kaspi and merhav @xcite have derived some tighter lower bounds to the reliability functions of both users by analyzing random coding error exponents of their optimal decoders . while their derivation was exponentially tight at most of the steps",
    ", there were still some steps in @xcite where exponential tightness might have been compromised .",
    "moreover , kaspi and merhav have analyzed ensembles of i.i.d.codes , which are not as good as ensembles of fixed composition codes ( * ? ? ?",
    "* section 7.3 ) .",
    "these two points give rise to the thought that there is room for improvement upon the results of @xcite , and indeed , such an improvement is one of the contributions of this work .",
    "in fact , the exponential error bounds , derived in this paper , both for the strong user and the weak one , are tight in the sense that they provide the exact random coding exponents for the ensemble of fixed composition codes . moreover , the resulting expressions are much simpler and easier to calculate than those of the best exponential bounds of kaspi and merhav ( see , in particular , the second part of @xcite ) .    interestingly , one of the ingredients that contributes significantly to this simplification in the error exponent expressions , is the derivation of _ universal decoders _ for both users , and this simplification is achieved thanks to a simple sandwich argument , asserting that a lower bound to the error exponent of the universal decoder can not be larger than an upper bound to the error exponent of the optimal decoder , but on the other hand , the latter turns out to be mathematically smaller than or equal to the former , and so , by contrasting the two exponential error bounds , which must therefore be equivalent , the expressions are considerably simplified .",
    "in other words , beyond this simplification of the error exponent bounds , there is an additional bonus , which is in obtaining universal decoders for both users .",
    "these decoders achieve the same random coding error exponents as the corresponding optimal decoders of the two users .",
    "both universal decoders are certain variants of the maximum mutual information ( mmi ) decoder ( * ? ? ?",
    "* theorem 5.2 ) , but they are different from the earlier proposed mmi - like universal decoders for the abc , due to krner and sgarro @xcite . for one thing ,",
    "our universal decoder for the weak user depends explicitly on the entire code , unlike the one in @xcite , which depends on the cloud centers only .    since we rely heavily on the method of types , our exponential error bounds have the flavor of those of csiszr and krner @xcite .",
    "while exponentially tight , their shortcoming is that they are not easy to calculate since they involve minimizations over auxiliary channels , and these might be computationally painful especially for large alphabets . to alleviate this difficulty , we also propose gallager ",
    "style bounds @xcite , which require optimizations over very few ( one or two ) parameters , but the caveat is that exponential tightness might be sacrificed .",
    "moreover , the gallager ",
    "style bounds lend themselves to better intuitive understanding on the behavior of the error exponents for both of the users . specifically , we derive a _ phase diagram _ for the weak user , which fully describes the functional behavior of the bound in different regions of the plane of rates .",
    "we also demonstrate our results numerically for an example of the binary symmetric bc , and compare our results to those in earlier works , showing explicitly the improvement .",
    "the remaining part of the paper is organized as follows . in section 2 , we establish notation conventions , formalize the model and the problem , and finally , review some preliminaries . in section 3 , we summarize the main theoretical results of this paper , and give some numerical results for the binary symmetric bc .",
    "section 4 provides the proofs concerning the strong user in the abc ( the exact random coding error exponent and the universal decoder ) , and section 5 contains a similar treatment for the weak user . in section 6",
    ", we derive lower bounds on the exact random coding error exponents , and in section 7 we study them .",
    "throughout the paper , random variables will be denoted by capital letters , specific values they may take will be denoted by the corresponding lower case letters , and their alphabets will be denoted by calligraphic letters .",
    "random vectors and their realizations will be denoted , respectively , by capital letters and the corresponding lower case letters , both in the bold face font .",
    "their alphabets will be superscripted by their dimensions .",
    "for example , the random vector @xmath0 , ( @xmath1 - positive integer ) may take a specific vector value @xmath2 in @xmath3 , the @xmath1-th order cartesian power of @xmath4 , which is the alphabet of each component of this vector .",
    "sources and channels will be subscripted by the names of the relevant random variables / vectors and their conditionings , whenever applicable , following the standard notation conventions , e.g. , @xmath5 , @xmath6 , and so on . when there is no room for ambiguity , these subscripts will be omitted . for a generic joint distribution @xmath7 , which will often be abbreviated by @xmath8",
    ", information measures will be denoted in the conventional manner , but with a subscript @xmath8 , that is , @xmath9 is the marginal entropy of @xmath10 , @xmath11 is the conditional entropy of @xmath10 given @xmath12 , @xmath13 is the mutual information between @xmath10 and @xmath12 , and so on .",
    "the weighted divergence between two conditional distributions ( channels ) , say , @xmath14 and @xmath15 , with weighting @xmath5 is defined as @xmath16 where logarithms , here and throughout the sequel , are taken to the natural base .",
    "the probability of an event @xmath17 will be denoted by @xmath18 , and the expectation operator with respect to ( w.r.t . )",
    "a probability distribution @xmath19 will be denoted by @xmath20 . for two positive sequences @xmath21 and @xmath22 , the notation @xmath23 will stand for equality in the exponential scale , that is , @xmath24 .",
    "the indicator function of an event @xmath17 will be denoted by @xmath25 .",
    "the notation @xmath26_{+}$ ] will stand for @xmath27 .",
    "the empirical distribution of a sequence @xmath28 , which will be denoted by @xmath29 , is the vector of relative frequencies , @xmath30 , of each symbol @xmath31 in @xmath32 .",
    "the type class of @xmath28 , denoted @xmath33 , is the set of all vectors @xmath34 with @xmath35 .",
    "when we wish to emphasize the dependence of the type class on the empirical distribution @xmath36 , we will denote it by @xmath37 .",
    "information measures associated with empirical distributions will be denoted with hats and will be subscripted by the sequences from which they are induced .",
    "for example , the entropy associated with @xmath29 , which is the empirical entropy of @xmath32 , will be denoted by @xmath38 .",
    "similar conventions will apply to the joint empirical distribution , the joint type class , the conditional empirical distributions and the conditional type classes associated with pairs ( and multiples ) of sequences of length @xmath1 .",
    "accordingly , @xmath39 would be the joint empirical distribution of @xmath40 , @xmath41 or @xmath42 will denote the joint type class of @xmath43 , @xmath44 will stand for the conditional type class of @xmath32 given @xmath45 , @xmath46 will designate the empirical joint entropy of @xmath32 and @xmath45 , @xmath47 will be the empirical conditional entropy , @xmath48 will denote the empirical mutual information , and so on .",
    "when we wish to emphasize the dependence of @xmath44 upon @xmath45 and the relevant empirical conditional distribution , @xmath49 , we denote it by @xmath50 .",
    "similar conventions will apply to triples of sequences , say , @xmath51 , etc .",
    "likewise , when we wish to emphasize the dependence of empirical information measures upon a given empirical distribution given by @xmath8 , we denote them using the subscript @xmath8 , as described above .",
    "we consider a memoryless abc with a finite input alphabet @xmath4 and finite output alphabets @xmath52 and @xmath53 .",
    "let @xmath54 and @xmath55 denote the single  letter input ",
    "output transition probability matrices , associated with the strong user and the weak user , respectively .",
    "when these channels are fed by an input vector @xmath28 , they produce the corresponding output vectors @xmath56 and @xmath57 , according to @xmath58 we are interested in sending one out of @xmath59 messages to the strong user , that observes @xmath45 , and one out of @xmath60 messages to the weak user , that observes @xmath61 .",
    "specifically , consider the following mechanism of random selection of an hierarchical code for the abc .",
    "let @xmath62 be a finite alphabet , let @xmath63 be a given probability distribution on @xmath62 , and let @xmath64 be a given matrix of conditional probabilities of @xmath10 given @xmath65 .",
    "we first select , independently at random , @xmath66 @xmath1-vectors ( `` cloud centers '' ) , @xmath67 , all under the uniform distribution over the type class @xmath68 .",
    "next , for each @xmath69 , we select conditionally independently ( given @xmath70 ) , @xmath71 codewords , @xmath72 , under the uniform distribution across the conditional type class @xmath73 .",
    "we denote the sub - code @xmath74 .",
    "once selected , the entire codebook @xmath75 , together with the collection of all cloud centers , @xmath76 , are revealed to the encoder and to both decoders .",
    "+   + the optimal decoder for the strong user is given by @xmath77 = \\operatorname*{arg\\,max}_{0 \\leq",
    "i \\leq m_{z}-1 , 0 \\leq j \\leq m_{y}-1 } w_{1}({\\mbox{\\boldmath $ y$}}|{\\mbox{\\boldmath $ x$}}_{i , j } ) , \\ ] ] while the optimal decoder for the weak user ( the bin index decoder ) is given by @xmath78 where @xmath79    let @xmath80 and @xmath81 be the channel outputs resulting from the transmission of @xmath82 . define the average error probabilities of decoders ( [ mlstrong ] ) and ( [ mlweak ] ) as @xmath83 \\neq ( i , j ) \\big| \\mathbf{x}_{i , j}~\\mbox{sent } \\big\\ } , \\end{aligned}\\ ] ] and @xmath84 where in both definitions , @xmath85 designates probabilities associated with the randomness of the codebook , as well as that of the channel outputs given its input .",
    "the corresponding random coding error exponents are defined as @xmath86,\\ ] ] and @xmath87,\\ ] ] provided that the limits exist .",
    "our main objective is to obtain single ",
    "letter expressions for @xmath88 and @xmath89 . as for the universal decoders ,",
    "consider first the weak user .",
    "we wish to find a function @xmath90 , that is independent of the ( unknown ) parameters of the channel @xmath91 , such that the following _ universal decoder _ for the weak user @xmath92 achieves an average error probability whose exponent is @xmath93 . by the same token",
    ", we wish to find a universal decoder for the strong user , of the form @xmath94   = \\operatorname*{arg\\,max}_{0 \\leq i \\leq m_{z}-1 , 0 \\leq j \\leq m_{y}-1 } g({\\mbox{\\boldmath $ y$}},{\\mbox{\\boldmath $ u$}}_{i},{\\mbox{\\boldmath $ x$}}_{i , j } ) , \\ ] ] where the function @xmath95 is independent of @xmath96 , yet the decoder @xmath97 $ ] achieves @xmath88 .",
    "let @xmath98 and @xmath99 denote two generic joint probability distributions of the random vectors @xmath100 and @xmath101 , whose @xmath102-marginals are both identical to @xmath103 .",
    "define @xmath104_{+ } - r_{z }   \\big]_{+ } , \\nonumber \\\\ & \\big [ i_{q}(x;y|u ) - r_{y } \\big]_{+ }     \\big\\},\\end{aligned}\\ ] ] and @xmath105_{+ } - r_{z }   \\big]_{+}.\\ ] ] our first main result is the following . + * theorem 1 .",
    "* under the assumptions of section 2 , the limits ( [ strongexponent ] ) and ( [ weakexponent ] ) exist and are given by the following single ",
    "letter expressions : @xmath106 we prove the result concerning the strong user in section 4 and the result concerning the weak user in sections 5 . notice that both error exponents depend on both coding rates , in contrast to the error exponents given in the previous works @xcite and @xcite .",
    "several remarks are now in order .",
    "+ @xmath107 an immediate byproduct of theorem 1 is finding the set of rate pairs @xmath108 for which both @xmath109 and @xmath110 .",
    "it is not difficult to show that this set is given by : @xmath111 evaluated with the distribution @xmath112 .",
    "the convex hull of the closure of the union over all code distributions @xmath113 gives the capacity region .",
    "we may also consider an _",
    "individual attainable region _ for each user , i.e. , the set of rate pairs for which the probability of error vanishes for one of the users , but without taking into account the other user .",
    "later on , individual attainable regions will become relevant when we consider the phase diagrams .",
    "it is not difficult to show that the attainable region for the weak user , to be denoted by @xmath114 , is given by @xmath115 , evaluated with the distribution @xmath116 , while the attainable region for the strong user , to be denoted by @xmath117 , is given by @xmath118 , evaluated with the distribution @xmath119 .",
    "notice that the attainable region of the weak user is not bounded , i.e. , reliable bin index decoding may still be guaranteed for any satellites rate @xmath120 , as long as @xmath121 .",
    "+ @xmath107 the computation of the error exponents involves minimizations over auxiliary channels @xmath122 and @xmath123 . for large input and output alphabets ,",
    "we are motivated to look for alternative expressions for the error exponents , whose optimization does not depend on the alphabet sizes , even at the expense of some loss in the exponential tightness .",
    "we will discuss such an alternative form in the sequel .",
    "+ @xmath107 both error exponents depend on the input distribution .",
    "while in the single - user regime , we may maximize the final expression over the input distribution in order to maximize the error exponent , this is no longer the case for the abc . even in the simplest case of a binary symmetric bc ,",
    "we see that the best code for the strong user is the worst one for the weak user , and vice versa . to see why is that true , let @xmath124 and let @xmath64 be a bsc with a crossover probability @xmath125 . in this case , the hierarchy of the codebook degenerates , i.e. , the codebook has a constant composition , which is best for the strong user . in the other extreme ,",
    "@xmath64 is a bsc with a crossover probability @xmath126 .",
    "the error probability of the strong user is almost one , but the error exponent of the weak user is the largest and independent of @xmath120 .",
    "hence , the choice of the input distribution trades off between the error exponents of the two users .",
    "+ @xmath107 as can be seen from the minimum in eq .",
    "( [ innerfunctionstrong ] ) , there are two different kinds of error events for the strong user",
    ". let @xmath127 denote the minimizer in ( [ strongexponents ] ) .",
    "now , if for some @xmath108 , the inequality @xmath128_{+ } - r_{z }   \\big]_{+ } >",
    "\\big [ i_{q^{*}}(x;y|u ) - r_{y } \\big]_{+}$ ] holds , then the dominant error event for the strong user is caused by competing codewords from the true cloud , otherwise , the dominant error event is caused by competitive clouds .",
    "+ @xmath107 in fact , the cardinality @xmath129 is a free parameter in our problem . as such",
    ", we may let @xmath130 , and it is definitely not obvious that a finite @xmath129 is optimal .",
    "this is because we can not see how to apply the usual cardinality bounding techniques based on the support lemma @xcite .",
    "it must be clear that even if the optimal @xmath129 is finite , it may not be the same as the bound given in the converse theorem of the capacity region of the abc ( @xmath131 ) @xcite .      as mentioned in the introduction , universal mmi decoders for both receivers",
    "were proposed in @xcite , where for the weak user , this decoder was defined by : @xmath132 the error exponent of such a decoder is inferior to the error exponent of the optimal ( ml ) decoder , because for one thing , it makes no use of @xmath133 , but only of the cloud centers .",
    "the universal decoder ( [ ks_weak ] ) achieves the following error exponent @xcite @xmath134_{+ }    \\big\\},\\end{aligned}\\ ] ] and by comparing it numerically to ( [ weakexponents ] ) in the case of the binary symmetric bc ( see subsection 3.4 ) , it is evident that @xmath135 can be strictly higher than @xmath136 , due to the additional term in ( [ innerfunctionweak ] ) .",
    "hence , one may wonder whether a different universal decoder exists , whose error exponent is as large as @xmath137 .",
    "it turns out that the answer to this question is affirmative , and indeed , this universal decoder relies entirely on @xmath138 and @xmath139 . in section 5 ,",
    "we prove the following theorem . + * theorem 2 . *",
    "define the function @xmath140_{+ } \\big\\ } .\\end{aligned}\\ ] ] the universal decoder @xmath141 achieves @xmath142 .",
    "it turns out that there is also another universal decoder ( with the same error exponent ) , whose structure is much more similar to the ml decoder of ( [ mlweak ] ) , in the sense that its metric is based on summation over @xmath143 , except that here , the unknown likelihood function is replaced by the exponentiated empirical mutual information . in the appendix",
    "we prove the following theorem . + * theorem 3 . *",
    "the universal decoder @xmath144 achieves @xmath142 .",
    "we next proceed to the strong user and present a universal decoder .",
    "it turns out that the mmi  like metric of the universal bin index decoder , as given in theorem 2 ( but with @xmath61 replaced by @xmath45 ) , works well also for the strong user . the main difference between them",
    "is rooted in the way they use the metric . while the weak user first maximizes it within each cloud , and",
    "only then finds the cloud with the maximal value , the strong user maximizes it over both indices simultaneously .",
    "more precisely , we claim the following , which is proved in section 4 . + * theorem 4 . *",
    "define the function @xmath145_{+ }    .",
    "\\end{aligned}\\ ] ] the universal decoder @xmath146 = \\operatorname*{arg\\,max}_{0 \\leq i \\leq m_{z}-1 , 0 \\leq j \\leq m_{y}-1 }      g({\\mbox{\\boldmath $ y$ } } , { \\mbox{\\boldmath $ u$}}_{i } , { \\mbox{\\boldmath $ x$}}_{ij } )   \\end{aligned}\\ ] ] achieves @xmath147 .    at this point , it is interesting to compare @xmath148 $ ] to the universal decoder of the strong user in @xcite , @xmath149 = \\operatorname*{arg\\,max}_{0 \\leq i \\leq m_{z}-1 , 0 \\leq j \\leq m_{y}-1 }   \\hat{i}_{{\\mbox{\\boldmath $ u$}}_{i}{\\mbox{\\boldmath $ x$}}_{ij}{\\mbox{\\boldmath $ y$}}}(ux;y),\\ ] ] and whose random coding error exponent is given by @xcite @xmath150 where @xmath151_{+ } , \\big [ i_{q}(x;y|u ) - r_{y } \\big]_{+ }     \\big\\}.\\ ] ] by the identity @xmath152 , it is easy to see that @xmath153 , proving that ( [ ks_strong ] ) has an error exponent as that of ( [ mlstrong ] ) , a fact that was not asserted in @xcite .",
    "as mentioned before , the calculations of ( [ strongexponents ] ) and ( [ weakexponents ] ) involve minimizations over auxiliary channels , which become painful when the input and output alphabets are large .",
    "for this reason , we look for other forms of error exponent formulas , where the number of parameters to be optimized does not grow with the alphabet sizes , but the price of this might be some loss in the tightness of the bounds , i.e. , we obtain _ lower bounds _ on the random coding error exponents . even in the single user case ,",
    "the random coding error exponent involves a minimization over an auxiliary channel , where csiszr and krner ( * ? ? ?",
    "* exercise 10.24 ) show that the exact error exponent is lower bounded by the following expression @xmath154 } \\bigg\\ {   - \\log \\sum_{y } \\bigg [ \\sum_{x } p(x)w^{\\frac{1}{1+\\rho}}(y|x )      \\bigg]^{1+\\rho }   -\\rho r    \\bigg\\},\\ ] ] where the subscript g stands for  gallager \" , who was the first to derive and analyze the error exponent in this form @xcite .",
    "it is important to note that for the optimal code distribution , ( [ singleusergallager ] ) is not only a lower bound , but the exact random coding error exponent @xcite .",
    "it turns out that the exact random coding error exponents of the two users in the abc can be lower bounded by the same methods as in @xcite . in section 6 , we prove the following theorem . + * theorem 5 .",
    "* define the functions @xmath155^{\\frac{1}{1+s } } , \\\\",
    "\\psi(u , z , s ) & = \\sum_{x } p(x|u)[w_{2}(z|x)]^{\\frac{1}{1+s}}. \\end{aligned}\\ ] ] the exact random coding error exponent of the strong user is lower bounded by @xmath156 where @xmath157 } \\bigg\\ {   - \\sum_{u }",
    "p(u ) \\log \\bigg ( \\sum_{y }    \\phi^{1+\\rho}(u ,",
    "y,\\rho )   \\bigg )   - \\rho r_{y }      \\bigg\\ } ,    \\\\",
    "\\label{strongterm2 } e _ { y,2 } ( r_{y } , r_{z } ) & = \\max_{\\mu \\in [ 0,1 ] } \\max_{\\lambda \\in [ 0,\\mu ] } \\bigg\\ {        -   \\log \\bigg [ \\sum_{y }   \\bigg (   \\sum_{u } p(u )   \\phi^{\\frac{1+\\lambda}{1+\\mu}}(u , y,\\lambda )     \\bigg)^{1+\\mu }   \\bigg ]    -   \\lambda r_{y }   -\\mu r_{z }   \\bigg\\ }   .\\end{aligned}\\ ] ] in addition , the random coding error exponent of the weak user is lower bounded by @xmath158 } \\max_{\\lambda \\in [ 0,\\mu ] } \\bigg\\ {        -   \\log \\bigg [ \\sum_{z }   \\bigg (   \\sum_{u }",
    "p(u )   \\psi^{\\frac{1+\\lambda}{1+\\mu}}(u , z,\\lambda )     \\bigg)^{1+\\mu }   \\bigg ]    -   \\lambda r_{y }   -\\mu r_{z }   \\bigg\\ }   .\\end{aligned}\\ ] ] + these lower bounds involve maximizations over one or two parameters only , in contrast to the original error exponents , and so , they are much easier to evaluate . in section 7 , we study them and show how they behave in different regions of the plane of rates .",
    "in contrast to the single user case , both lower bounds of the two users depend on the code distribution , but now we are no longer able to optimize both of them simultaneously , for the reason we mentioned above , in subsection 3.1 .",
    "we next provide some numerical results , comparing our exponents to those of @xcite and @xcite .",
    "let @xmath96 and @xmath91 be two binary symmetric channels ( bscs ) with crossover parameters @xmath159 and @xmath160 , respectively ( @xmath161 ) .",
    "let @xmath62 be binary as well and let @xmath63 be uniformly distributed over @xmath162 .",
    "also , let @xmath64 be a bsc with crossover parameter @xmath163 $ ] .",
    "+ the capacity region of our model is given by : @xmath164 where @xmath165 and @xmath166 is the binary entropy function .      using theorem 5 , we find that for the strong user , @xmath167 where , @xmath168 } \\bigg\\ {   -   \\log \\bigg\\ {   \\bigg [   ( 1-\\beta)(1-p_{y})^{\\frac{1}{1+\\rho } } + \\beta \\cdot p_{y}^{\\frac{1}{1+\\rho } }      \\bigg]^{1+\\rho }   \\nonumber \\\\ & \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\ ;   +   \\bigg [   ( 1-\\beta ) \\cdot p_{y}^{\\frac{1}{1+\\rho } } + \\beta \\cdot ( 1-p_{y})^{\\frac{1}{1+\\rho } }      \\bigg]^{1+\\rho }    \\bigg\\ }    - \\rho r_{y }     \\bigg\\ } , \\\\",
    "& e_{y,2}(r_{y } , r_{z } ) \\nonumber \\\\ & = \\max_{\\mu \\in [ 0,1 ] } \\max_{\\lambda \\in [ 0,\\mu ] } \\bigg\\ {   -   \\ln 2   - ( 1+\\mu ) \\cdot",
    "\\log   \\bigg\\ {   \\frac{1}{2 } \\cdot \\bigg [ ( 1-\\beta)(1-p_{y})^{\\frac{1}{1 + \\lambda } } + \\beta \\cdot p_{y}^{\\frac{1}{1+\\lambda } }     \\bigg]^{\\frac{1+\\lambda}{1+\\mu } } \\nonumber \\\\ & \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\ ; +   \\frac{1}{2 } \\cdot \\bigg [   ( 1-\\beta ) \\cdot p_{y}^{\\frac{1}{1 + \\lambda } } + \\beta \\cdot ( 1-p_{y})^{\\frac{1}{1 + \\lambda } }     \\bigg]^{\\frac{1+\\lambda}{1+\\mu } }   \\bigg\\ }     - \\lambda r_{y }   -\\mu r_{z }     \\bigg\\}.\\end{aligned}\\ ] ] for the weak user , @xmath169 } \\max_{\\lambda \\in [ 0,\\mu ] } \\bigg\\ {   -   \\ln 2   - ( 1+\\mu ) \\cdot",
    "\\log   \\bigg\\ {   \\frac{1}{2 } \\cdot \\bigg [ ( 1-\\beta)(1-p_{z})^{\\frac{1}{1 + \\lambda } } + \\beta \\cdot p_{z}^{\\frac{1}{1+\\lambda } }     \\bigg]^{\\frac{1+\\lambda}{1+\\mu } } \\nonumber \\\\ & \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\ ; +   \\frac{1}{2 } \\cdot \\bigg [   ( 1-\\beta ) \\cdot p_{z}^{\\frac{1}{1 + \\lambda } } + \\beta \\cdot ( 1-p_{z})^{\\frac{1}{1 + \\lambda } }     \\bigg]^{\\frac{1+\\lambda}{1+\\mu } }   \\bigg\\ }     - \\lambda r_{y }   -\\mu r_{z }     \\bigg\\}.\\end{aligned}\\ ] ] we present the lower bounds by plotting families of curves , one for each exponent , as a function of one rate , while the other rate is kept fixed .",
    "let us choose the channel probabilities to be @xmath170 and @xmath171 , and @xmath172 . in fig .  1",
    ", we plot lower bounds to @xmath88 as a function of @xmath120 , as given by ( [ stronglowerbound ] ) , where @xmath173 takes five different values .    .",
    "[ overflow],width=415 ]    as long as @xmath174 , the dominant error event is caused by wrong codewords from the true cloud . in this case , the error exponent is independent of the number of clouds and is given by the dark blue curve . as @xmath173 increases more , we find that above some critical rate , the error exponent begins to depend on the number of clouds , since the dominant error event is due to wrong codewords from competitive clouds .",
    "when the rate of the weak user is high , i.e. , when the exponential number of clouds is higher than the capacity of the channel to the strong user ( @xmath175 ) , reliable communication is no longer possible .",
    "in fig .  2 , we plot lower bounds to @xmath135 as a function of @xmath173 , as given by ( [ weaklowerbound ] ) , where @xmath120 takes five different values .    .",
    "[ overflow],width=415 ]    at @xmath176 , we should obtain the error exponent of a single user . in this case , the numerical value at zero - rate is given by @xmath177 , and @xmath178 vanishes at @xmath179 , which is the capacity of the channel to the weak user . for @xmath180 , @xmath135 becomes independent of @xmath120 , and is given by the red curve . in this case",
    ", we get a lower bound to the error exponent of the equivalent binary symmetric channel from the cloud center @xmath65 to the channel output of the weak user @xmath181 .      as for the exact random coding error exponents , given by theorem 1",
    ", the optimization problems require the minimization over the auxiliary channels @xmath122 and @xmath123 .",
    "let us compare the gallager - style lower bounds to the exact exponents . in fig .  3",
    ", we see two pairs of curves of the exact exponents and their lower bounds , where @xmath182 and @xmath183 takes two different values .",
    "the exact exponents are strictly better than the gallager - style exponents .",
    "similar results are obtained for the weak user as well ( not shown here ) .",
    "it is important to note that in some regions in the @xmath184 plane , the lower bounds are equal to the exact random coding error exponents .    ,",
    "width=415 ]      as far as we know , no other works on universal decoding for the abc exists , besides the one of @xcite .",
    "although the error exponent of the strong user given there is optimal w.r.t .  the ml decoder , it is not the case for the weak user .",
    "the universal decoder of @xcite for the weak user uses only the cloud centers and is independent of @xmath120 , while the new universal decoder of theorem 2 makes use of the entire codebook , which is the main reason for the resulted improvement .",
    "the difference between the error exponents is larger for lower values of @xmath120 .",
    "as before , let @xmath171 and @xmath172 .",
    "4 demonstrates the difference between the error exponents of the two universal decoders in the extreme case of @xmath176 .    ,",
    "width=415 ]    to the best of our knowledge , the most up - to - date work on exponential lower bounds to the reliability functions of the abc is @xcite , where random coding error exponents were derived using two different techniques .",
    "each of those derivations includes at least one step that may not be exponentially tight .",
    "also , in @xcite , the random codebooks are assumed to be drawn i.i.d .. we expect our proposed exact random coding error exponents to improve on @xcite , because of two reasons : first , our analysis is exponentially tight , and second , our ensemble is of the uniform distribution across types .",
    "this kind of random codes are known ( * ? ? ?",
    "* section 7.3 ) to be better than the i.i.d .  ensembles .",
    "our comparison here focuses on the error exponent of the weak user only .",
    "again , let @xmath171 , @xmath172 and @xmath185 .",
    "fig .  5 compares the two error exponents , and shows that the new exponent is better .    , width=415 ]      in the single user case",
    ", it is known that the error exponent behaves differently in different ranges of rates , i.e. , it is affine at low rates and curvy at high rates . by the same token , for the abc",
    ", the plane of rates can be divided into several different regions , where in each one of them , the error exponents behaves differently .",
    "this partition of the plane of rate pairs is of course , more involved than in the single - user case .",
    "we refer to it as a phase diagram , a term borrowed from physics . in order to study the various types of behavior of the lower bound of theorem 5 ,",
    "let us invoke the following alternative and equivalent lower bound for the random coding error exponent of the weak user @xmath186 } \\max_{s \\in [ 0,1 ] } \\bigg\\ {        -   \\log \\bigg [ \\sum_{z }   \\bigg (   \\sum_{u }",
    "p(u )   \\psi^{\\frac{1+s \\mu}{1+\\mu}}(u , z , s\\mu )     \\bigg)^{1+\\mu }   \\bigg ]    -   s \\mu r_{y }   -\\mu r_{z }   \\bigg\\ }   .\\ ] ] since the maximization region is now the unit square , this form is more convenient to analyze than that of ( [ lowerboundweak ] ) .",
    "6 displays a partition of the plane @xmath184 to different regions for the gallager - style lower bound of the weak user , where @xmath187 , and @xmath171 .",
    "although not shown here , the phase diagrams of the exact exponents behave similarly .    ) .",
    "[ overflow],width=529 ]    the study in section 7 provides a characterization of the different regions from the viewpoint of the type of dependence of the error exponent upon the rates and the maximizers @xmath188 and @xmath189 ( see table 1 ) .",
    ".dependence of ( [ altlowerbound ] ) on @xmath120 and @xmath173 in various regions in the plane ( see fig .",
    "6 ) . [ cols=\"^,^,^,^\",options=\"header \" , ]",
    "we have the following : + * lemma 1 . * for every empirical distribution @xmath225 , @xmath226_{+ } - r_{z }   \\big]_{+ } , \\nonumber \\\\ & \\big [ i_{q}(x_{00};y|u_{0 } ) - r_{y } \\big]_{+ }     \\bigg\\}.\\end{aligned}\\ ] ] + * proof . * we start by recalling that the function @xmath227 is defined as @xmath228_{+ }    , \\nonumber \\\\   & \\big [ e_{2 } ( f(q_{u_{0}x_{00}y } ) , q_{y } ) -   r_{z } \\big]_{+ } \\big\\},\\end{aligned}\\ ] ] and we separately upper bound each one of the terms .",
    "we can upper bound them by choosing any specific distribution , instead of minimizing over them .",
    "let us start with the left term : @xmath229_{+ } \\\\ & = \\min _ {   \\ { q_{x|u_{0}y } \\in \\mathcal{s}(q_{u_{0}y } ) : ~     f(q_{u_{0}xy } ) \\geq f(q_{u_{0}x_{00}y } )     \\ } } \\big [ i_{q}(x;y|u_{0 } ) - r_{y } \\big]_{+ }   \\\\ & \\leq   \\big [ i_{q}(x_{00};y|u_{0 } ) - r_{y } \\big]_{+}.\\end{aligned}\\ ] ] for the right term inside the minimum of ( [ minimumofstrong ] ) , we have the following @xmath230_{+ } \\\\ & = \\min _ { \\ { q_{u'|y } \\in \\mathcal{s}(q_{y } )    \\ } }   \\big [   i _ { q}(u';y )   +    e_{1 } ( f(q_{u_{0}x_{00}y } ) , q_{u'y } )   -   r_{z }    \\big]_{+ }    \\\\ & \\leq    \\big [   i _ { q}(u_{0};y )   +    e_{1}(f(q_{u_{0}x_{00}y } ) , q_{u_{0}y } )   -   r_{z } \\big]_{+ } \\\\ & =    \\bigg [   i _ { q}(u_{0};y )   +   \\min _ { \\ { q_{x|u_{0}y } \\in \\mathcal{s}(q_{u_{0}y } ) : ~    f(q_{u_{0}xy } ) \\geq f(q_{u_{0}x_{00}y } )     \\ } } \\big [ i_{q}(x;y|u_{0 } ) - r_{y } \\big]_{+ }    -   r_{z }    \\bigg]_{+ } \\\\",
    "& \\leq    \\big [   i _ { q}(u_{0};y )   +   \\big [ i_{q}(x_{00};y|u_{0 } ) - r_{y } \\big]_{+ }   -   r_{z }    \\big]_{+ } .\\end{aligned}\\ ] ] combining both upper bounds , we see that ( [ lemma4result ] ) holds , thus completing the proof .",
    "let us now select @xmath232_{+}. \\end{aligned}\\ ] ] we show that ( [ optimalfstrong ] ) achieves the maximum value of @xmath233 , as given by lemma 1 , and therefore , this decoder has the same error exponent as the one of the optimal ( ml ) decoder .",
    "as before , we start with the left term inside the minimum of ( [ minimumofstrong ] ) , and get @xmath229_{+ } \\\\ & = \\min _ {   \\ { q_{x|u_{0}y } \\in \\mathcal{s}(q_{u_{0}y } )   \\ } } \\big\\ { \\big [ i_{q}(x;y|u_{0 } ) - r_{y } \\big]_{+ }   : ~     f(q_{u_{0}xy } ) \\geq f(q_{u_{0}x_{00}y } )   \\big\\ } \\\\ & = \\min _ {   \\ { q_{x|u_{0}y } \\in \\mathcal{s}(q_{u_{0}y } )   \\ } } \\big\\ { \\big [ i_{q}(x;y|u_{0 } ) - r_{y } \\big]_{+ }   : \\nonumber     \\\\ &",
    "i_{q}(u_{0};y )   + [ i_{q}(x;y|u_{0 } ) - r_{y}]_{+ } \\geq i_{q}(u_{0};y )   + [ i_{q}(x_{00};y|u_{0 } ) - r_{y}]_{+ }   \\big\\ } \\\\ & = \\min _ {   \\ { q_{x|u_{0}y } \\in \\mathcal{s}(q_{u_{0}y } )   \\ } } \\big\\ { \\big [ i_{q}(x;y|u_{0 } ) - r_{y } \\big]_{+ }   :   \\nonumber     \\\\ &   [ i_{q}(x;y|u_{0 } ) - r_{y}]_{+ } \\geq   [ i_{q}(x_{00};y|u_{0 } ) - r_{y}]_{+ }   \\big\\ } \\\\ \\label{aaa } & = [ i_{q}(x_{00};y|u_{0 } ) - r_{y}]_{+}.\\end{aligned}\\ ] ] for the right term inside the minimum of ( [ minimumofstrong ] ) , @xmath230_{+ } \\\\ & = \\min _ { \\ { q_{ux|y } \\in \\mathcal{s}(q_{y } ) : ~    f(q_{uxy } ) \\geq f(q_{u_{0}x_{00}y } )   \\ } }    \\big [   i _ { q}(u;y )   +    \\big [ i_{q}(x;y|u ) - r_{y } \\big]_{+ }   -   r_{z }    \\big]_{+ }   \\\\ & = \\min _ { \\ { q_{ux|y } \\in \\mathcal{s}(q_{y } )    \\ } } \\bigg\\ {   \\big [   i _ { q}(u;y )   +    \\big [ i_{q}(x;y|u ) - r_{y } \\big]_{+ }   -   r_{z }    \\big]_{+ } : \\nonumber     \\\\ & i_{q}(u;y )   + [ i_{q}(x;y|u ) - r_{y}]_{+ } \\geq i_{q}(u_{0};y )   + [ i_{q}(x_{00};y|u_{0 } ) - r_{y}]_{+ } \\bigg\\ } \\\\ \\label{bbb } & = \\big [   i _",
    "{ q}(u_{0};y )   +    \\big [ i_{q}(x_{00};y|u_{0 } ) - r_{y } \\big]_{+ }   -   r_{z }    \\big]_{+}.\\end{aligned}\\ ] ] finally , compare the minimum between ( [ aaa ] ) and ( [ bbb ] ) to the right hand side of ( [ lemma4result ] ) .",
    "let us first derive the exact random coding error exponent of the following bin index decoder , @xmath234 where @xmath235 and assume that @xmath236 is upper bounded by a real number @xmath237 .",
    "note that ( [ generaldecoder ] ) includes the optimal ml decoder ( [ mlweak ] ) as a special case .",
    "in this section , we prove theorem 5 .      we start by changing the clipping operator to a maximization problem and using convexity properties to change the order of the maximization and the minimization : @xmath390_{+ }     \\bigg\\ } \\\\ & =   \\min _ { v } \\bigg\\ {   d ( v \\| w |   p    )   +   \\max_{\\rho \\in [ 0,1 ] } \\big\\ { \\rho \\cdot   \\big [ i(x;y|u ) - r_{y } \\big ] \\big\\ }     \\bigg\\ } \\\\",
    "\\label{changeorder2 } & =   \\max_{\\rho \\in [ 0,1 ] } \\bigg\\ { - \\rho r_{y }   +   \\min _ { v }   \\big\\ {   d ( v \\| w |   p    )   +    \\rho \\cdot   i(x;y|u )   \\big\\ }     \\bigg\\}.\\end{aligned}\\ ] ] next , @xmath391",
    "\\bigg\\ } \\\\",
    "\\label{beforeminv } & = \\min _ { v , q }   \\bigg\\ {   \\sum_{x , u , y } p(x , u)v(y|x , u )      \\log \\bigg [ \\frac{v^{1+\\rho}(y|x , u)}{w(y|x)q^{\\rho}(y|u ) }   \\bigg ]   \\bigg\\}.\\end{aligned}\\ ] ] first , we minimize over the auxiliary channel @xmath392 . holding the auxiliary channel @xmath8 fixed , and differentiating w.r.t .",
    "@xmath393 , we find that the minimizing distribution is given by @xmath394 substituting it back into ( [ beforeminv ] ) and summing over @xmath395 , we get that @xmath396^{1+\\rho } } { w(y|x)q^{\\rho}(y|u ) }   \\bigg ]   \\bigg\\ } \\nonumber   \\\\   & =   \\min _ { q }   \\bigg\\ { -(1+\\rho ) \\sum_{x , u } p(x , u )      \\log \\bigg [ \\sum_{y } w^{\\frac{1}{1+\\rho}}(y|x)q^{\\frac{\\rho}{1+\\rho}}(y|u )   \\bigg ]   \\bigg\\ } \\\\ & =   \\min _ { q }   \\bigg\\ { -(1+\\rho ) \\sum_{u } p(u )   \\sum_{x } p(x|u )      \\log \\bigg [ \\sum_{y } w^{\\frac{1}{1+\\rho}}(y|x)q^{\\frac{\\rho}{1+\\rho}}(y|u )   \\bigg ]   \\bigg\\ } \\\\",
    "\\label{jensen1 } & \\geq   \\min _ { q }   \\bigg\\ { -(1+\\rho ) \\sum_{u } p(u )        \\log \\bigg [ \\sum_{x } p(x|u ) \\sum_{y } w^{\\frac{1}{1+\\rho}}(y|x)q^{\\frac{\\rho}{1+\\rho}}(y|u )   \\bigg ]   \\bigg\\},\\end{aligned}\\ ] ] where inequality ( [ jensen1 ] ) is due to jensen s inequality .",
    "next , we minimize the lower bound over @xmath8 . differentiating the last expression w.r.t .",
    "@xmath397 , we find that the minimizing distribution is given by @xmath398^{1+\\rho }   } { \\sum_{y ' } \\big [ \\phi(u , y',\\rho ) \\big]^{1+\\rho } } .",
    "\\end{aligned}\\ ] ] substituting ( [ miniq1 ] ) into ( [ jensen1 ] ) , we get @xmath399^{\\frac{\\rho}{1+\\rho } }   \\bigg ] \\nonumber \\\\    & = -(1+\\rho ) \\sum_{u } p(u )       \\log   \\left [ \\sum_{x }",
    "p(x|u )   \\sum_{y } w^{\\frac{1}{1+\\rho}}(y|x ) \\frac {    \\big [ \\phi(u , y,\\rho ) \\big]^{\\rho }   } { \\bigg\\ {   \\sum_{y ' }   \\big [ \\phi(u , y',\\rho ) \\big]^{1+\\rho } \\bigg\\}^{\\frac{\\rho}{1+\\rho } } }   \\right ]    \\\\    & = -(1+\\rho ) \\sum_{u } p(u )        \\log   \\left [     \\frac { \\sum_{y } \\big (   \\phi(u , y,\\rho )   \\cdot   \\big [ \\phi(u , y,\\rho ) \\big]^{\\rho } \\big ) } { \\bigg\\ {   \\sum_{y ' } \\big [ \\phi(u , y',\\rho ) \\big]^{1+\\rho } \\bigg\\}^{\\frac{\\rho}{1+\\rho } }   }   \\right ] \\\\",
    "& = -(1+\\rho ) \\sum_{u } p(u )        \\log    \\left [   \\frac { \\sum_{y }   \\big [ \\phi(u , y,\\rho ) \\big]^{1+\\rho }   } { \\bigg\\ {   \\sum_{y ' } \\big [ \\phi(u , y',\\rho ) \\big]^{1+\\rho } \\bigg\\}^{\\frac{\\rho}{1+\\rho } }   }   \\right ]   \\\\    & = -(1+\\rho ) \\sum_{u } p(u )        \\log   \\left [   \\bigg\\ {   \\sum_{y } \\big [ \\phi(u , y,\\rho ) \\big]^{1+\\rho } \\bigg\\}^{\\frac{1}{1+\\rho } } \\right ]    \\\\   & = -\\sum_{u } p(u )        \\log   \\left\\ {   \\sum_{y } \\big [ \\phi(u , y,\\rho ) \\big]^{1+\\rho }   \\right\\ }   , \\end{aligned}\\ ] ] which completes the proof of eq .",
    "( [ strongterm1 ] ) . @xmath231",
    "as in the single user case , we expect to find a critical rate and a maximal rate . by maximal rate , that will be denoted by @xmath411 , we mean @xmath412 . by critical rate , to be denoted by @xmath413 , we mean the boundary between the range where @xmath410 is affine and the range where it is curvy .",
    "+ let @xmath414 } \\big\\ { e_{0}(\\rho )   - \\rho r_{y }    \\big\\ } , \\end{aligned}\\ ] ] where we have defined @xmath415^{1+\\rho}.\\end{aligned}\\ ] ] setting the partial derivative of the bracketed part of ( [ dm0 ] ) equal to 0 , we get @xmath416 following the same considerations as in ( * ? ? ?",
    "* section 5.6 ) , if some @xmath417 $ ] satisfies ( [ solutionforry ] ) , then it must maximize ( [ dm0 ] ) .",
    "it turns out that a solution to ( [ solutionforry ] ) exists if @xmath418 in this range , it is convenient to use ( [ solutionforry ] ) to relate @xmath410 and @xmath120 parametrically as functions of @xmath419 . for the interval @xmath420",
    ", this gives @xmath421 for @xmath422 , the parametric equations are not valid . in this case",
    ", the maximum occurs at @xmath423 .",
    "thus , @xmath410 is affine with slope @xmath424 : @xmath425 where @xmath426^{2}.\\end{aligned}\\ ] ] now , we can find @xmath411 and @xmath413 , which are given by the right - most side and the left - most side of ( [ ryrange ] ) , respectively . differentiating @xmath427 w.r.t .",
    "@xmath419 and substituting @xmath428 gives @xmath429 where @xmath430 is the conditional mutual information induced by the channel @xmath431 and the code distribution @xmath432 .",
    "next , define @xmath433 and @xmath434 after some algebra , we find that @xmath435 } { \\sum_{y ' }   f^{2}(u , y ' ) } .",
    "\\end{aligned}\\ ] ]        let us now turn to the other extreme , where @xmath436 depends solely on @xmath173 .",
    "this happens if and only if the maximizing @xmath462 $ ] is @xmath479 . in this case ,",
    "@xmath480 } \\bigg\\ {   -   \\log \\sum_{y }   \\bigg\\ {   \\sum_{u } p(u )   \\bigg [ \\sum_{x } p(x|u)w(y|x )    \\bigg]^{\\frac{1}{1+\\mu } }   \\bigg\\}^{1+\\mu }      -\\mu r_{z }     \\bigg\\ } \\\\ & = \\max_{\\mu \\in [ 0,1 ] } \\bigg\\ {   -   \\log \\sum_{y }   \\bigg\\ {   \\sum_{u } p(u )   v^{\\frac{1}{1+\\mu}}(y|u )     \\bigg\\}^{1+\\mu }      -\\mu r_{z }     \\bigg\\},\\end{aligned}\\ ] ] which means that @xmath481 , i.e. , the ordinary random coding error exponent at rate @xmath173 for an i.i.d .",
    "code with distribution @xmath63 , where @xmath392 is defined to be the equivalent channel from @xmath65 to @xmath12 .",
    "the simple explanation for the fact that @xmath436 becomes independent of @xmath120 , for high @xmath120 , is that the satellite codewords behave like pure noise .",
    "next , we find the region where @xmath436 depends solely on @xmath173",
    ". consider the rate pairs @xmath449 for which both ( [ dm1 ] ) and ( [ dm2 ] ) hold with @xmath482 : @xmath483 let @xmath484 denote the curve given by eqs .",
    "( [ s0-eq1])-([s0-eq2 ] ) : @xmath485 in addition , we have the following corner point for @xmath486 : @xmath487 and we use it to define the straight line connecting that corner point to the @xmath120-axis : @xmath488 which is the set of all @xmath449 for which the maximizers are @xmath479 and @xmath489 .",
    "+   + let @xmath490 be defined by @xmath491 .",
    "in fact , the curve @xmath490 is the borderline between the region where @xmath436 depends on @xmath120 ( affine or curvy ) and the region where @xmath436 is independent of @xmath120 .",
    "the set of all @xmath449 that are above the curve @xmath490 defines the region where @xmath436 is independent of @xmath120 .",
    "in addition , let us obtain a simple informational expression for the maximum of @xmath173 .",
    "according to ( [ s0_curve ] ) , we only have to differentiate @xmath492 w.r.t .",
    "@xmath443 and then substitute @xmath475 .",
    "we get @xmath493 where @xmath494 is the mutual information induced by the channel @xmath495 and @xmath496 . in the region @xmath497 the maximizer is @xmath498 , and @xmath436 is affine in @xmath173 and is given by @xmath499 where @xmath500 + the third region is the set of all @xmath449 for which the maximizing @xmath199 is in @xmath501 . in this case , we use ( [ generalrep1])-([generalrep2 ] ) , which hold for every @xmath502 and @xmath503 $ ] such that both ( [ dm1 ] ) and ( [ dm2 ] ) are satisfied .",
    "this region can be devided into two complementary regions . in the first one ,",
    "the maximizer is @xmath498 , and @xmath436 is affine in @xmath173 and curvy in @xmath120 , while in the second one , the maximizer @xmath189 is in @xmath501 , and @xmath436 is curvy in both @xmath173 and @xmath120 .",
    "the borderline between those two regions is given by the curve @xmath504 for @xmath505 the maximizers are @xmath506 , and then @xmath507 .",
    "regarding decoder ( [ generaldecoder])-([generaldecoder2 ] ) , let us select @xmath508 we show that ( [ agoodfunction2 ] ) achieves the maximum of @xmath380 , given by lemma 3 , and therefore , the error exponent of this decoder is as large as that of the optimal decoder .",
    "first , the threshold @xmath509 can be easily simplified as @xmath510 \\\\ & =   r_{y } + \\max_{\\big\\ { q_{x|u_{0}z } \\in \\mathcal{s}(q_{u_{0}z } ) : ~ i_{q}(x;z|u_{0 } )   \\leq r_{y } \\big\\ } } \\big [ i_{q}(u_{0};z )   +   i_{q}(x;z|u_{0 } )    - i_{q}(x;z|u_{0 } ) \\big ]   \\\\ & =   r_{y } +   i_{q}(u_{0};z ) , \\end{aligned}\\ ] ] such that @xmath511 in general , the constraint of the inner minimization problem defining @xmath512 is given by @xmath513_{+ } \\geq s_{1}(q_{u_{0}x_{00}z}),\\end{aligned}\\ ] ] which can now be written as @xmath514_{+ } \\nonumber   \\\\ & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\geq i_{q}(u_{0};z )   +    \\max \\big\\ { r_{y } ,   i_{q}(x_{00};z|u_{0 } )   \\big\\}.\\end{aligned}\\ ] ] substracting @xmath120 from both sides gives @xmath515_{+ } \\nonumber   \\\\",
    "& ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\geq i_{q}(u_{0};z )   +    \\max \\big\\ { 0 ,   i_{q}(x_{00};z|u_{0 } )   -   r_{y } \\big\\ } ,   \\end{aligned}\\ ] ] or , @xmath516_{+ }   \\geq   i_{q}(u_{0};z )   +   \\big [   i_{q}(x_{00};z|u_{0 } )   -   r_{y } \\big]_{+}.   \\end{aligned}\\ ] ] defining @xmath517_{+}$ ] ,",
    "we have @xmath518_{+ } , \\end{aligned}\\ ] ] which is the same as on the right hand side of ( [ lemma2eq ] ) . @xmath231            r. g. gallager ,  capacity and coding for degraded broadcast channels , \" _ probl",
    "inform . _ , vol .",
    "3 , pp . 3-14 , july-september 1974 ; translated in _ probl . inform .",
    "_ , pp . 185-193 , july-september 1974 .",
    "n. shulman , _ communication over an unknown channel via common broadcasting , _ ph.d .",
    "dissertation , department of electrical engineering - systems , tel aviv university , july 2003 ."
  ],
  "abstract_text": [
    "<S> this work contains two main contributions concerning the asymmetric broadcast channel . </S>",
    "<S> the first is an analysis of the exact random coding error exponents for both users , and the second is the derivation of universal decoders for both users . </S>",
    "<S> these universal decoders are certain variants of the maximum mutual information ( mmi ) universal decoder , which achieve the corresponding random coding exponents of optimal decoding . </S>",
    "<S> in addition , we introduce some lower bounds , which involve optimization over very few parameters , unlike the original , exact exponents , which involve minimizations over auxiliary probability distributions . numerical results for the binary symmetric broadcast channel show improvements over previously derived error exponents for the same model </S>",
    "<S> . + * index terms : * error exponent , asymmetric broadcast channel , universal decoding , mmi .    the andrew & erna viterbi faculty of electrical engineering + technion - israel institute of technology + technion city , haifa 3200004 , israel + \\{rans@campus , merhav@ee}.technion.ac.il </S>"
  ]
}