{
  "article_text": [
    "factor analysis ( fa ) is one of the cornerstones of data analysis , the tool of choice for capturing and understanding linear relationships between variables @xcite .",
    "it provides a set of @xmath0 factors , each explaining dependencies between some of the features in a vectorial data sample @xmath1 based on the model @xmath2 where @xmath3 is the value of the @xmath4th unobserved factor , @xmath5 contains its loadings , and @xmath6 is gaussian noise . to correctly capture the relationships , we need to assume a diagonal noise covariance with free variance for each of the variables .",
    "if the noise model was more flexible , having non - diagonal covariance , it would allow describing some of the relationships as noise .",
    "on the other hand , forcing the variances to be equal would imply that heteroscedastic noise would need to be explained as factors , reducing the model to probabilistic pca @xcite .",
    "building on our preliminary conference paper @xcite , we generalize factor analysis to a novel problem formulation of _ group factor analysis _ ( gfa ) , where the task is to explain relationships between groups of variables .",
    "we retain the linear - gaussian family of fa , but modify the model so that each factor now describes dependencies between some of the feature groups instead of individual variables . again",
    "the choice of residual noise is crucial : it needs to be flexible enough to model everything that is not a true relationship between two variable groups , but restricted enough so that all actual relationships will be modeled as individual factors . for fa these requirements",
    "were easily satisfied by assuming independent variance for each dimension . for gfa more",
    "elaborate constructions are needed , but the same basic idea applies .    from another perspective , gfa extends multi - battery factor analysis ( mbfa ) , introduced by mcdonald @xcite and browne @xcite as a generalization of inter - battery factor analysis ( ibfa ) @xcite to more than two variable groups .",
    "mbfa is a factor analysis model for multiple co - occurring data sets , or , equivalently , for a vectorial data sample whose variables have been split into groups .",
    "it includes a set of factors that model the relationships between all variables , as well as separate sets of factors explaining away the noise in each of the variable groups .",
    "these group - specific factor sets are sufficiently flexible for modeling all variation within each group .",
    "however , each of the remaining factors is assumed to describe relationships between _ all _ of the groups , which is not sufficient for providing interpretable factors that reveal the relationships between the data sets as will be explained below .",
    "nevertheless , the mbfa models are useful tools for multi - source data analysis , illustrated by the fact that the problem has been re - discovered in machine learning literature several times ; see section  [ sec : related ] for more details .    to solve the gfa problem",
    ", we need to have also factors that describe relationships between subsets of the groups .",
    "this makes the solutions to the problem both more flexible and more interpretable than mbfa .",
    "for example , a strong factor tying two groups while being independent of the other groups can then be explicitly modeled as such .",
    "the mbfa - based models would , falsely , reveal such a factor as one that is shared by all groups .",
    "alternatively , they would need to , again incorrectly , split them into multiple group - specific ones .    in recent years , the need for the gfa solution has been identified by several authors , under different terminology .",
    "jia et al . learned sparse matrix factorization by convex optimization @xcite , and van deun et al . used group - lasso penalty to constrain the factors of a simultaneous component analysis ( sca ) model @xcite .",
    "various bayesian techniques have also been proposed for learning shared and individual subspaces of multiple data sources @xcite .    in this work we lay the foundation for future development of gfa solutions , by properly defining the problem setup and terminology .",
    "we also present a general solution outline and show that the solutions mentioned above are all instances of the same basic approach ; they all learn structured sparse fa models with varying techniques for obtaining group - wise sparsity for the factor loadings .",
    "we then propose a novel gfa solution that does not make a strong simplifying assumption shared by all the previous approaches .",
    "they all assume that we can independently infer , for each factor - group pair , whether that factor describes variation related to that group , whereas our solution explicitly models also these associations with an additional linear model . in brief ,",
    "our model hence consists of two linear hierarchical levels .",
    "the first models the relationships between the groups , and the latter models the observed data given the output of the higher level .",
    "alternatively , it can be viewed as a direct generalization of @xcite with a more advanced structured sparsity prior making it possible to reduce the degrees of freedom in the model when needed .    before delving into the details on how we solve the gfa problem , we introduce some general application scenarios .",
    "the model is useful for analyzing multi - view setups where we have several data sets with co - occurring samples .",
    "the variables can be grouped according to the data sets : all variables in one set belong to one group etc",
    ". then gfa explains relationships between data sources , and for two data sets it equals the problem of canonical correlation analysis ( cca ; see @xcite for a recent overview from a probabilistic perspective ) .",
    "alternatively , each group could contain a collection of variables chosen to represent a multi - dimensional concept , such as cognitive abilities of a subject , which can not be summarized with a single feature .",
    "then gfa could be used for associating cognitive abilities with other multi - dimensional concepts .",
    "the groups can also represent a meaningful partitioning of larger data sets ; we present two practical examples of this kind of a setup . in one example we split a high - dimensional feature vector over the human genome into subsets according to functional pathways to describe drug responses , and in the other example we split magnetic resonance images of the human brain into local regions to study relationships between brain areas .",
    "the group factor analysis problem is as follows : assume a collection of observations @xmath7 for @xmath8 collected in a data matrix @xmath9 , and a disjoint partition of the @xmath10 variables into @xmath11 groups @xmath12 .",
    "the gfa task is to find a set of @xmath0 factors that describe @xmath13 so that relationships between the groups can be separated from relationships within the groups . for notational simplicity ,",
    "assume that the first @xmath14 variables correspond to the first group @xmath15 , the following @xmath16 variables to @xmath17 , and so on",
    ". then we can write @xmath18 $ ] , where @xmath19 is a subset of the data corresponding to @xmath20 .",
    "we use @xmath21 to denote the @xmath22th sample ( row ) of @xmath19 . throughout this paper",
    "we use the superscript @xmath23 to denote variables related to the @xmath24th group or data set .",
    "a general solution to the gfa problem can be formulated as a joint factor model for the observed data sets .",
    "the model for the @xmath24th group of the @xmath22th sample is @xmath25 where @xmath26 $ ] , @xmath27 , and @xmath28 is noise precision .",
    "equivalently , we can directly write @xmath29 , where @xmath6 is gaussian noise with diagonal covariance but separate variance for each group , by denoting @xmath30 $ ] .    to make the factors interpretable in the gfa - sense , that is , to describe relationships between the groups , we need to make @xmath31 sparse so that it satisfies the following properties ( for a visual illustration see figure [ fig : factorization ] ) :    1 .",
    "some factors are private to each group , so that @xmath32 only for one @xmath24 .",
    "these factors explain away the variation independent of all the other groups , and play the role of residual noise in regular fa .",
    "the rest of the factors describe relationships between some arbitrary subset of the groups ; they are non - zero for those groups and zero for the others .",
    "a trivial solution would explicitly split the factors into separate sets so that there would be one set of factors for each possible subset of the groups ( including the singletons and the set of all groups ) .",
    "this can be done for small @xmath11 ; for example klami and kaski proposed such a model for @xmath33 @xcite and gupta et al . formulated the model for general @xmath11 but ran experiments only with @xmath34 @xcite .",
    "due to the exponential number of subsets , these approaches can not generalize to large @xmath11 .",
    "a better approach is to associate the projection matrix @xmath31 with a structural sparsity prior that encourages solutions that satisfy the necessary properties .",
    "this strategy was first presented for @xmath33 by virtanen et al .",
    "@xcite , and extended for general @xmath11 independently by several authors @xcite . despite technical differences in how the structural sparsity is obtained , all of these approaches can be seen as special instances of our gfa solution principle .",
    "also the non - bayesian models that can be used to solve the gfa problem follow the same principle @xcite .",
    "we propose a novel gfa solution that is another instantiation of the general approach described above .",
    "the technical novelty is in a more advanced structural sparsity prior which takes into account possible dependencies between the groups , instead of assuming the group - factor activations to be _ a priori _ independent as in the earlier solutions .",
    "the model can also be interpreted as a two - level model that uses one level to model association strengths between individual groups and the other level to model the observations given the association strength .",
    "this interpretation clarifies the conceptual novelty , explicating how the new structural sparsity prior has an intuitive interpretation .",
    "the generative model is the one given in coupled with suitable priors . for @xmath35 we use the unit gaussian prior @xmath36 , and for the noise precisions @xmath28 we employ a gamma prior with both shape and rate parameters set to @xmath37 ; the model is fairly insensitive to these hyperparameters . to find a gfa solution these standard choices need to be complemented with structured sparse priors for @xmath31 , described next .",
    "we denote by @xmath38 the inverse strength of association between the @xmath24th group and the @xmath4th factor , and directly interpret it as the precision parameter of the prior distribution for @xmath39 , the projection mapping the @xmath4th factor to the observed variables in the @xmath24th group .",
    "that is , we assume the prior @xmath40 the same prior was used in our preliminary work @xcite , where we drew @xmath38 independently from a flat gamma prior to implement group - wise extension to automatic relevance determination ( ard ) .",
    "here we replace the independent draws with a linear model for @xmath41 to explicitly model the association strengths between group - factor pairs . since the entries correspond to precisions for the second level projections , we model them in the log - space as @xmath42 where @xmath43 and @xmath44 .",
    "the vectors @xmath45 and @xmath46 model the mean profiles . here",
    "@xmath47 is the rank of the linear model , and typically @xmath48 so that we get a low - rank decomposition for the association strengths , obtained by element - wise exponentiation @xmath49 .",
    "finally , we place an element - wise normal prior for the matrices @xmath50 and @xmath51 with zero mean and precision set to a fixed constant @xmath52 ; extensions to further hierarchical priors would also be tractable if needed .",
    "the resulting gfa model is visualized as a plate diagram in figure [ fig : gfaplate ] , highlighting the two levels of the model .",
    "the motivation for modeling the @xmath38 instead of assuming them independent comes from the original modeling task of gfa . the goal is to understand the relationships between the groups , and",
    "hence we should explicitly model them .",
    "the earlier models with independent priors assume that the groups are independent , which is unlikely to hold in practical applications .",
    "our model , in turn , directly represents correlations between the group activation profiles .",
    "an alternative formulation for correlated groups would directly draw @xmath53 from a multivariate distribution , such as multivariate normal @xcite .",
    "however , specifying the correlations for such a model would require @xmath54 parameters , making the approach infeasible for large @xmath11 . since modeling the correlations",
    "is expected to be the most useful for large number of groups , it is clearly beneficial to use the low - rank model that requires only @xmath55 parameters .      as mentioned above , the model can be interpreted in two alternative ways .",
    "the straightforward interpretation is that of a factor analysis model for the @xmath10 observed variables , with a structural sparsity prior for making the projections implement the gfa properties .",
    "this viewpoint illustrates the relationship between the earlier bayesian solutions for gfa @xcite ; they follow the same general approach presented in section  [ sec : general ] , but our sparsity prior is more advanced .",
    "perhaps the more interesting interpretation is to consider as the primary model .",
    "then the entries of @xmath41 are considered as unobserved data describing the groups ; @xmath50 are the factor loadings and @xmath51 provide the latent factors for the groups .",
    "the mapping from @xmath41 to the observations , parameterized by @xmath56 and @xmath31 , is then merely a set of nuisance parameters .",
    "from this perspective , the earlier models presented for the gfa problem are very simple . they do not assume any structure between the groups , but instead draw the association strengths independently .",
    "their results will naturally still reveal such associations , but not as well as the proposed model that models them explicitly .",
    "as we later empirically demonstrate , the rank @xmath47 of the group association level can typically be very low even for very high - dimensional data collections with a large number of groups .",
    "this makes it possible to visually study the associations between the groups , for example via a scatter plot of the columns of @xmath50 for @xmath57 .",
    "we discuss approaches for determining the value of @xmath47 for practical applications in section 5.4 .      for inference , we use mean - field variational approximation .",
    "we approximate the posterior with a factorized distribution @xmath58 where @xmath59 , and find the approximation that minimizes the kullback - leibler divergence from @xmath60 to @xmath61 .",
    "equivalently , this corresponds to estimating the marginal likelihood @xmath62 with maximal lower bound , @xmath63 the mean - field algorithm proceeds by updating each of the terms in turn , always finding the parameters that maximize the expected lower bound @xmath64 , given all the other factorized distributions .",
    "since the model uses conjugate priors for everything except @xmath50 and @xmath51 , the updates for most parameters are straightforward and match those of other fa models , for example @xcite .",
    "the terms @xmath65 and @xmath66 are more complex and hence we derive the updates for that part in detail below . the vb inference is summarized in algorithm [ algorithm ] , and the parameters of the variational distributions can be found in the appendix .",
    "an open - source implementation of the model in r is available as part of the ccagfa package in cran ( http://cran.r-project.org/package=ccagfa ) .",
    "input : initialized @xmath67 , and either @xmath68 or @xmath50 and @xmath51 .    for @xmath65 and @xmath66",
    "we use fixed - form distributions , that is , we choose point distributions @xmath69 and @xmath70 , and optimize the lower bound numerically and @xmath71 have been appended as parts of @xmath50 and @xmath51 , respectively . ] .",
    "the bound as a function of @xmath50 and @xmath51 is given by @xmath72 where @xmath73 denotes the second moment of @xmath74 with respect to @xmath75 , and @xmath76 collects the prior terms affecting the factorization .",
    "since the parameters @xmath50 and @xmath51 are highly coupled , we optimize jointly with second order approximate gradient method ( l - bfgs ) , using the gradients @xmath77 where @xmath78 .",
    "full variational inference over these parameters would also be possible @xcite , but we did not consider it necessary for the practical applications .",
    "an interesting special case is obtained when @xmath79 .",
    "then the factorization is not low - rank , but instead we can find a unique optimal solution for as @xmath80 assuming @xmath81 is negligibly small .",
    "this is identical to the variational update for a model that draws @xmath38 from a gamma prior with the parameters approaching zero ( the uniform distribution limit of gamma ) .",
    "this is the prior used by some of the earlier gfa solutions @xcite , and hence we get the earlier models as special cases of ours .",
    "the inference scales linearly in @xmath11 , @xmath10 and @xmath82 , and has cubic complexity with respect to @xmath0 . in practice",
    ", it is easily applicable for large data sets as long as @xmath0 is reasonable ( at most hundreds ) . however , during inference empty factors may occur and in this case they can be removed from the model to speed up and stabilize computationth factor from the model if @xmath83 . ] .      even though the gfa model is in this work",
    "formulated primarily as a tool for exploring relationships between variable groups , it can readily be used also as a predictive model . in this prediction setting new ( test ) samples are observed for a subset of groups and the task is to predict unobserved groups based on observed data .    for simplicity of presentation , we assume that only the @xmath24th group is unobserved",
    ". then the missing data are represented by the predictive distribution @xmath84 , where @xmath85 denotes partially observed test data consisting of all the other groups .",
    "however , this distribution involves marginalization over both @xmath31 and @xmath56 that is analytically intractable and hence we need to approximate it .",
    "in particular , given @xmath85 , @xmath86 and @xmath87 , we learn the approximate posterior distribution for the latent variables @xmath88 corresponding to @xmath85 and approximate the mean of the predictive distribution as @xmath89 where @xmath90 and @xmath91 . in the experiments we use this mean value for prediction .",
    "the gfa problem and our solution for it are closely related to several matrix factorization and factor analysis techniques . in the following ,",
    "the related work is discussed from two perspectives .",
    "first we cover other techniques for solving the group factor analysis problem or its special cases",
    ". then we proceed to relate the proposed solution to multiple regression , which is a specific use - case for gfa .      for a single group , @xmath92 ,",
    "the model is equivalent to bayesian principal component analysis @xcite ; all of the factors are active for the one group and they describe the variation with linear components",
    ". we can also implement sparse fa with the model , by setting @xmath93 so that each group has only one variable .",
    "the residual noise has independent variance for each variable , and the projections become sparse because of the ard prior .    for two groups , @xmath33 , the model is equivalent to bayesian cca and inter - battery factor analysis @xcite ; some factors model the correlations whereas some describe the residual noise within either group .    most multi - set extensions of cca , however , are not equivalent to our model . for example , archambeau et al .",
    "@xcite and deleus et al .",
    "@xcite extend cca for @xmath94 , but instead of gfa they solve the more limited problem of multiple - battery factor analysis @xcite .",
    "the mbfa models provide one set of factors that describe the relationships between _ all _ groups , and then model the variation specific to each group either with a free covariance matrix or a separate set of factors for that group . besides the multi - set extensions of cca , also the probabilistic interpretation of sparse matrix factorization @xcite , and the jive model for integrated analysis of multiple data types @xcite belong to the family of mbfa models .",
    "these models differ in their priors , parameterization and inference , but are all conceptually equivalent .    in recent years",
    ", a number of authors have independently proposed solutions for the gfa problem .",
    "they all follow the general solution outline presented in section  [ sec : general ] with varying techniques for obtaining the group - wise sparsity .",
    "common to all of them is that they do not explicitly model the relationships between the groups , but instead assume that the choice of whether a factor describes variation in one particular group can be made independently for all factor - group pairs .",
    "this holds for the non - bayesian solutions of multi - view sparse matrix factorizations @xcite and the group lasso penalty variant of sca @xcite , as well as for the earlier bayesian gfa models @xcite .",
    "compared to these , our model explicitly describes the relationships between the groups , which helps especially for setups with a large number of groups .",
    "finally , we get the sparsity priors of @xcite and @xcite as special cases of our model .",
    "the gfa problem can also be related to supervised learning , by considering one of the groups as dependent variables and the others as explanatory variables . for just one group of dependent variables ( that is , @xmath33 in total ) ,",
    "gfa is most closely related to a setting called factor regression @xcite .",
    "it shares the goal of learning a set of latent factors that are useful for predicting one group from the other . for more recent advances of factor regression models ,",
    "see @xcite . by splitting the explanatory variables into multiple groups",
    ", gfa provides a group - wise sparse alternative for these models . assuming the split corresponds to meaningful prior information on the structure of the explanatory variables",
    ", this will usually ( as demonstrated in experiments ) reduce overfitting by allowing the model to ignore group - specific variation in predictions .",
    "other models using group - wise sparsity for regression have also been presented , most notably group lasso @xcite that uses a group norm for regularizing linear regression .",
    "compared to gfa , lasso lacks the advantages of factor regression ; for multiple output cases it predicts each variable independently , instead of learning a latent representation that captures the relationships between the inputs and outputs .",
    "gfa has the further advantage that it learns the predictive models for not only all variables but in fact for all groups at the same time .",
    "given a gfa solution one can make predictions for arbitrary subsets of the groups given another subset , instead of needing to specify in advance the split into explanatory and dependent variables .",
    "in this section we demonstrate the proposed gfa model on artificial data . to illustrate the strengths of the proposed method we compare it with bayesian implementations of the most closely related factor models , always using a variational approximation for inference and ard for complexity control also for the competing methods .",
    "in particular , we compare against the regular factor analysis ( fa ) and its sparse version ( sfa ) to show that one should not completely ignore the group information .",
    "we also compare against mbfa , to demonstrate the importance of modeling also relationships between subsets of the groups , and against the gfa solution of @xcite , obtained as a special case of the proposed model by setting @xmath79 , as an example of a method that makes the group - factor activity decisions independently for each pair .",
    "for mbfa we use two different implementations depending on the setup ; for low - dimensional data we model the group - specific variation with full - rank covariance , whereas for high - dimensional data we use a separate set of factors for each group ; see klami et al .",
    "@xcite for discussion on these two alternatives for the special case of @xmath33 .",
    "we also compare against sca @xcite , a non - bayesian solution for the gfa problem using group lasso penalty , using the same initial @xmath0 as for the bayesian models , with 5-fold cross - validation for the the group lasso regularization parameter . for predictive inference",
    "we compute point estimates for the latent variables of the test samples .      for evaluating the quality of the models we use an indirect measure of predictive accuracy for left - out groups , based on the intuition that if a factor analysis model is able to make accurate predictions it must have learned the correct structure .",
    "in particular , given @xmath11 groups we will always compute the test data predictive error for each of the groups one at a time , using the rest of the groups as explanatory variables .",
    "since we use a regression task for measuring the quality , we will also compare gfa against alternative standard solutions for multiple output regression , in addition to the alternative factor models mentioned in the previous section .",
    "in particular , we will show comparison results with group lasso @xcite and simple regularized multiple output linear regression ( mlr ) model that ignores the group structure . for mlr",
    "the prediction is obtained as @xmath95 , where the weight matrix is given by @xmath96 , and @xmath97 is a regularization parameter . for this model",
    "we learn a separate model for each choice of the dependent variable groups , which results in @xmath11-fold increase in computational cost compared to gfa that learns all tasks simultaneously .",
    "furthermore , we validate for the regularization parameters via 10-fold cross - validation within the training set , which further increases the computational load .",
    "we generated @xmath98 samples from a gfa model with @xmath99 split into @xmath34 equally - sized groups .",
    "the true generative model had @xmath100 factors , including one factor specific for each group as well as for each possible subset between the groups .",
    "figure  [ fig : w ] shows the true model as well as the solutions found by the proposed gfa model ( using @xmath101 ) , mbfa , and both regular and sparse fa .",
    "the proposed model finds the correct structure , whereas mbfa suggests spurious correlations ; each factor describing correlation between just two of the groups is falsely indicating activity also in the third one , according to the mbfa specification .",
    "the regular fa result is simply considerably noisier overall , while sparse fa suffers from a few false factor loadings . for",
    "this simple demonstration sca learns the same result as gfa , after manually optimizing the thresholding of component activity . for all methods we set @xmath0 to a sufficiently large number , allowing ard to prune out unnecessary components , chose the best solution by comparing the lower bounds of @xmath102 runs with random initializations , and for illustrative purposes ordered the components based on their similarity ( cosine ) with the true ones .",
    "the conclusion of this experiment is that the proposed method indeed solves the gfa problem , whereas the mbfa and fa solutions do not provide as intuitive and interpretable factors . for this data gfa with @xmath103",
    "( not shown ) fails to unveil the underlying ( full - rank ) structure . instead",
    ", the loadings lie between those of gfa and fa of figure  [ fig : w ] , which is understandable since fa is closely related to gfa with @xmath104 .",
    "next we studied how well the proposed solution scales up for more groups .",
    "we generated @xmath105 samples from a gfa model with @xmath106 true factors .",
    "we used @xmath107 variables for each group and let the number of groups grow from @xmath108 to @xmath109 .",
    "in other words , the total dimensionality of the data grew from @xmath110 to @xmath111 .",
    "the variable groups were divided into four types of equal size , so that the groups within one type had the same association strengths for all factors .",
    "this implies a low - rank structure for the associations .        for validation we used average leave - one - group - out prediction error , further averaged over 50 independent data sets .",
    "figure  [ fig : m ] ( left ) shows that the proposed model outperforms the other fa methods by a wide margin . for small @xmath11",
    "the difference between the @xmath112 and @xmath79 solutions are negligible , since a small number of factor - group association strengths can just as well be selected independently . for large @xmath11 , however , explicitly modeling the relationships pays off and results in better predictions .",
    "the prediction errors for gfa models with rank @xmath47 between 2 and 10 are very similar , and hence for clarity , only one ( @xmath47=4 ) is shown in figure  [ fig : m ] .",
    "gfa also outperforms sca , an alternative group - sparse factor model , for all cases except @xmath113 , for which the two methods are equal . for cases with only 4 or 8",
    "groups multiple linear regression is the most accurate method , but for all other cases , when the total dimensionality increases , gfa clearly outperforms also these supervised methods .",
    "besides showing better prediction accuracy , the low - rank solution ( here @xmath112 ) captures the underlying group structure better than the naive model of @xmath79 .",
    "figure  [ fig : m ] ( middle ) compares the two by plotting the number of groups active in each factor for the case with @xmath109 .",
    "there are @xmath106 factors that should each be active in @xmath114 groups . with @xmath112",
    "we find almost exactly the correct solutions , whereas the naive model finds @xmath115 factors , many of which it believes to be shared by only @xmath116 groups .",
    "in other words , it has split some real factors into multiple ones , finding a local optimum , due to making all the activity decisions independently . for illustrative purposes ,",
    "the components were considered active if the corresponding @xmath41-values were below 10 .",
    "the cardinality plot is sensitive to the threshold , but the key result stays the same regardless of the threshold : inferring the component activities independently results in a less accurate model .    finally , figure  [ fig : m ] ( right ) illustrates the relationships between the groups as a scatter plot of the latent factors for the @xmath57 solution , revealing clearly the four types of variable groups .",
    "gfa contains a free parameter @xmath47 and this value needs to be specified by the user . acknowledging that choosing the correct model order is in general a difficult problem , we resort to demonstrating two practical approaches that seem to work well for the proposed model .",
    "the first choice is @xmath117-fold cross - validation within the training set , using the predictive performance for left - out groups as the validation criterion .",
    "a computationally more efficient alternative is to use the elbow-principle for the variational lower bounds @xmath64 computed for different choices of @xmath47 .",
    "the bounds improve monotonically and @xmath51 are ignored . ] as a function of @xmath47 , but for typical data sets the growth rate rapidly diminishes after the correct rank , producing an elbow.        we tested both of these principles for 50 independent artificial data sets generated from the gfa model with parameters @xmath118 , @xmath119 , @xmath120 and @xmath121 , for three different data ranks : @xmath122 , representing the kinds of values one would typically expect for real applications . the prediction and lower bound curves as a function of model rank are shown for a representative example in figure  [ fig : lb ] . in the 5-fold cross - validation the correct rank was found correctly with over half of the data sets , and the most common error was overestimating the rank by one .",
    "the computationally lighter elbow-principle allows the analyst to choose roughly the correct rank by simply comparing the lower bounds .",
    "the rank @xmath47 influences also the computation time , as illustrated in figure  [ fig : lb ] .",
    "the computation time increases roughly linearly as a function of @xmath47 , but for ranks smaller than the optimal the algorithm requires more iterations for convergence which slows it down . in this experiment ,",
    "the low - rank model is slower than the special case updating @xmath41 in closed form , but only by a small factor . in the real data experiments reported in sections  [ sec : brain ] and [ sec : bio ]",
    "the low - rank variant was slightly faster to compute for all the tested values of @xmath47 ; the relative time spent on updating @xmath41 becomes small for larger data , and the low - rank models prune out excess factors faster .    in the remaining experiments we do not explicitly select a particular value for @xmath47 , but instead present the results for a range of different values",
    "this is done to illustrate the relative insensitivity of the model for the precise choice ; for both real - world applications a wide range of values outperform the alternative models and also the special case of @xmath79 , implying that picking exactly the right rank is not crucial .",
    "functional mri experiments are commonly used to study the interactions ( or connectivity ) between brain regions of interest ( rois ) @xcite . one way to learn these interactions is based on calculating correlations between individual fmri bold ( blood - oxygen - level dependent ) signals using pca or fa models @xcite .",
    "we applied gfa to brain imaging data to analyze connections between multiple brain rois , using fmri data recorded during natural stimulation with a piece of music @xcite , for @xmath123 time points covering two seconds each . we have @xmath124 subjects , and we computed a separate gfa model for each , providing @xmath124 independent experiments .",
    "we chose @xmath125 rois , some of which are lateralized , using the fsl software package @xcite , based on the harvard - oxford cortical and subcortical structural atlases @xcite and the probabilistic cerebellar atlas @xcite .",
    "further , we divided the voxels in these regions to @xmath126 local uniformly distributed supervoxels by spatial averaging . in the end , each roi was represented on average by eight such supervoxels and each supervoxel contained on average 120 voxels .",
    "these rois and their corresponding dimensionalities are given in supplementary material available at http://research.ics.aalto.fi/mi/papers/gfasupplementary.pdf .",
    "for quantitative comparison we again use leave - one - group - out prediction , predicting the activity of each roi based on the others for unseen test data .",
    "we set aside half of the data as test set and train the models varying the amount of training data for @xmath127 .",
    "the prediction errors are given in figure [ fig : brainpred ] , averaged over the rois and @xmath124 subjects .",
    "the proposed solution outperforms ( wilcoxon signed - rank test , @xmath128 ) all other factor models for a wide range of ranks , from @xmath57 to @xmath129 , and in particular is also clearly better than the special case with @xmath79 @xcite . for these ranks , with @xmath105 training samples , gfa also outperforms all the supervised regression models , whereas for a larger training set , @xmath130 , group lasso provides comparable accuracy .    figure  [ fig : brainpred ] shows also a visualization of @xmath50 for @xmath57 , averaged over all subjects using all observations . since @xmath50 is not identifiable , before averaging we projected each @xmath50 to a common coordinate system .",
    "each dot is one roi and the lines connect each roi to its spatially closest neighbor ( minimum euclidean distance of supervoxels between the corresponding rois ) to reveal that the model has learned interesting structure despite not knowing anything about the anatomy of the brain .",
    "further inspection reveals that the model partitions visual areas , frontal areas , and auditory areas as separate clusters .",
    "note that these results are a demonstration of the model s capability of discovering structure between groups ; for a serious attempt to discover functional or anatomic connectivity further tuning that takes into account the properties of fmri and the anatomy of the brain should be done .",
    "both chemical descriptors and biological responses can be used to analyze the properties of drugs . however",
    ", the shared variation between these two views may reveal more of the drug properties than either one of the views independently @xcite .",
    "we applied gfa to the drug response data of @xcite , consisting of the responses of @xmath131 drugs when applied to three cancer cell lines ( _ hl60 , mcf7 _ and _ pc3 _ ) .",
    "for the analysis , we selected the genes found in cp : biocarta gene sets , where the genes are grouped according to functional pathway information .",
    "we preprocessed the data as khan et al .",
    "@xcite and left duplicate genes only in the largest groups , removing groups with less than two genes left .",
    "as in @xcite , we augmented the data by adding 76 chemical descriptors ( _ volsurf _ ) of the drugs , here as separate variable groups . hence the whole data contain @xmath132 groups : 13 contain chemical descriptors of the drugs , whereas 139 groups describe the response in each of the three cell lines .",
    "the feature groups and their dimensionalities are listed in the supplementary material available at http://research.ics.aalto.fi/mi/papers/gfasupplementary.pdf . the total data dimensionality is @xmath133 .",
    "figure  [ fig : drug ] illustrates gfa results with @xmath134 on this data ( a component amount high enough for gfa with @xmath135 ) . already with @xmath57 the model partitions the groups",
    "almost perfectly into four different clusters ( bottom left subfigure ) , one for each of the three cell lines and one for the chemical descriptors .",
    "that is , the model effectively learns the underlying structure it has no information about .",
    "it also reveals clearer factor - group association structure ( top left subfigure ) compared to the earlier solution with @xmath136 @xcite . with @xmath137",
    "the four different clusters can be perfectly separated .",
    "next we quantitatively validated the performance of the different solutions , using a drug retrieval task @xcite .",
    "using one of the drugs as a query , we ranked the remaining drugs based on their similarity with the query , and used an external database of drug functions for assessing the retrieval results . by comparing the similarity in the latent space of the models",
    ", we can indirectly evaluate the quality of the representations the models have learned .",
    "it has been shown that the chemical volsurf descriptors capture drug functions significantly better than raw gene expression data for this data @xcite , and hence we computed the similarity measures of all the models based on factors that are active in at least one chemical descriptor group .",
    "for this purpose we thresholded the activities by regarding components with @xmath138 as active ; the results were not very sensitive for the exact threshold level .",
    "the retrieval can be quantified by measuring the average precision @xcite , further averaged over all the drugs ( separate retrieval tasks ) , which is a measure that gives most weight to the drugs that are retrieved first .",
    "figure  [ fig : drug ] ( top right ) shows that the proposed solution again outperforms all of the competing methods for all ranks above zero ( wilcoxon signed - rank test , @xmath128 ) , and for @xmath57 to @xmath139 and @xmath140 it significantly ( @xmath141 ) outperforms also the chemical descriptors that are considerably more accurate than any of the competing methods .",
    "the shown retrieval accuracies are based on the ten most similar drugs , but the results are consistent for sets of several sizes .",
    "in addition to the retrieval task , we measured the performance of the models in a leave - one - group - out prediction task with @xmath142 .",
    "the average errors are shown in figure  [ fig : drug ] ( bottom right ) .",
    "gfa with @xmath143 outperforms all the competing models significantly .",
    "joint analysis of multiple data sets is one of the trends in machine learning , and integrated factor analysis of multiple real - valued matrices is one of the prototypical scenarios for that task . in recent years",
    "multiple authors have re - discovered the multiple - battery factor analysis ( mbfa ) task originating from the early works in statistics @xcite , calling it either multi - set cca @xcite , or simply as a model for integrated analysis of multiple data sources @xcite . despite varying technical details ,",
    "all of these models can be seen as fa models with two sets of factors : one set describes dependencies between all of the variable groups , whereas the other set describes , or explains away , variation specific to each group .",
    "the group factor analysis problem formulated in this article , extending the preliminary treatment in @xcite , differs from the mbfa models in one crucial aspect . instead of only modeling relationships between _ all _ of the groups",
    ", we also introduce factors that model relationships between any subset of them .",
    "while some other recent works @xcite have also addressed the same problem , in this paper the gfa setup is for the first time introduced explicitly , putting it into its statistical context .",
    "we described a general solution principle that covers the earlier solutions , identifying the structural sparsity prior as the key element .",
    "we then presented a more advanced sparsity prior that results in a novel gfa solution : instead of choosing the activities of each group - factor pair independently , we explicitly model the relationships between the groups with another linear layer .",
    "our model hence directly provides factor loadings also between the groups themselves , which was exactly the original motivation for the gfa problem .",
    "our preliminary model @xcite is a special case with _ a priori _ independent loadings",
    ".    we showed , using artificial data , how the gfa problem and solution differ from the mbfa - problem and classical fa .",
    "we also demonstrated that , especially for a large number of groups or data sets , it pays off to explicitly model the relationships between the groups .",
    "finally , we applied the model on two real - world exploratory analysis scenarios in life sciences .",
    "we demonstrated that the model is applicable to connectivity analysis of fmri data , as well as for revealing structure shared by structural description of drugs and their response in multiple cell lines .",
    "these demonstrations illustrated the kinds of setups the gfa is applicable for , but should not be considered as detailed analyses of the specific application problems .",
    "besides showing that the proposed model solves the gfa problem considerably better than the alternatives mbfa , fa and sca @xcite , the empirical experiments revealed that there is a qualitative difference between the proposed model having the more advanced structural sparsity prior and the earlier gfa solutions such as @xcite .",
    "even though the earlier models also solve the gfa problem reasonably well , they are outperformed by supervised regression models in predictive tasks .",
    "the proposed solution with a low - rank model for the group association strengths is clearly more accurate in prediction tasks and , at least for small training sets , outperforms also dedicated regression models trained specifically to predict the missing groups .",
    "this is a strong result for a model that does not know in advance which groups correspond to explanatory variables and which to the dependent variables , but that instead learns a single model for all possible choices simultaneously .    the model presented here is limited to scenarios where each training sample is fully observed .",
    "support for missing observations could be added using the fully factorized variational approximation used for pca and collective matrix factorization with missing data @xcite .",
    "a similar approach can also be used for semi - paired setups where some samples are available only for some groups @xcite , by filling in the remaining groups by missing observations .",
    "empirical comparisons on these are left for future work . another possible direction for future work concerns more justified inference for the rank parameter @xmath47 ; even though the experiments here suggest that the method is robust to the choice , the method would be more easily applicable if it was selected automatically .",
    "we thank the academy of finland ( grant numbers 140057 , 266969 , and 251170 ; finnish centre of excellence in computational inference research coin ) , the aivoaalto project of aalto university , and digile ict shok ( d2i programme ) for funding the research .",
    "we would also like to thank suleiman a. khan for his help with the biological application , and enrico glerean for his help with the neuroscience application .",
    "we acknowledge the computational resources provided by aalto science - it project .",
    "10 [ 1]#1 url@samestyle [ 2]#2 [ 2]l@#1=l@#1#2    l.  thurstone , `` multiple factor analysis , '' _ psychological review _",
    "38 , no .  5 , pp .",
    "406427 , 1931 .",
    "c.  m. bishop , `` bayesian pca , '' _ advances in neural information processing systems _ , vol .  11 , pp .",
    "382388 , 1999 .",
    "s.  virtanen , a.  klami , s.  a. khan , and s.  kaski , `` , '' _ _ , pp .  12691277 , 2012 .",
    "r.  mcdonald , `` three common factor models for groups of variables , '' _ psychometrika _ , vol .",
    "37 , no .  1 ,",
    "pp .  173178 , 1970 .",
    "m.  browne , `` factor analysis of multiple batteries by maximum likelihood , '' _ british j. mathematical and statistical psychology _ , vol .",
    "33 , no .  2 , pp .",
    "184199 , 1980 .",
    "l.  r. tucker , `` an inter - battery method of factor analysis , '' _ psychometrika _ , vol .",
    "23 , no .  2 , pp .",
    "111136 , 1958 .",
    "m.  w. browne , `` the maximum - likelihood solution in inter - battery factor analysis , '' _ british j. mathematical and statistical psychology _ , vol .",
    "32 , no .  1 ,",
    "pp .  7586 , 1979 .",
    "y.  jia , m.  salzmann , and t.  darrell , `` factorized latent spaces with structured sparsity , '' _ advances in neural information processing systems _ , vol .",
    "23 , pp .  982990 , 2010 .    k.  v. deun , t.  f. wilderjans , r.  a.  v. berg , a.  antoniadis , and i.  v. mechelen , `` a flexible framework for sparse simultaneous component based data integration , '' _ bmc bioinformatics _",
    "12 , no .  1 ,",
    "pp .  448 , 2011 .    s.  k. gupta , d.  phung , b.  adams , and s.  venkatesh , `` a bayesian framework for learning shared and individual subspaces from multiple data sources , '' _ proc .",
    "advances in knowledge discovery and data mining , 15th pacific - asia conf .",
    ", pakdd _ , pp .  136147 , 2011 .",
    "s.  k. gupta , d.  phung , and s.  venkatesh , `` a bayesian nonparametric joint factor model for learning shared and individual subspaces from multiple data sources , '' _ proc .",
    "12th siam int .",
    "data mining _ , pp .  200211 , 2012 .",
    "a.  damianou , c.  ek , m.  titsias , and n.  lawrence , `` manifold relevance determination , '' _ proc .",
    "machine learning _ , pp .  145152 , 2012 .",
    "a.  klami , s.  virtanen , and s.  kaski , `` bayesian canonical correlation analysis , '' _ j. machine learning research _ , vol .",
    "14 , no .  1",
    ", pp .  899937 , 2013 .",
    "a.  klami and s.  kaski ,",
    "`` probabilistic approach to detecting dependencies between data sets , '' _ neurocomputing _ , vol .",
    "72 , no .  1 ,",
    "pp .  3946 , 2008 .",
    "s.  virtanen , a.  klami , and s.  kaski , `` , '' _ _ , pp .  457464 , 2011 .",
    "j.  aitchison , `` the statistical analysis of compositional data , '' _ j. royal statistical society .",
    "series b ( methodological ) _ , vol .",
    "44 , no .  2 , pp .  139 - 177 , 1982 .    a.  ilin , and t.  raiko , `` practical approaches to principal component analysis in the presence of missing data , '' _ j. machine learning research _ , vol .  11 , pp .  1957 - 2000 , 2010 .    o.  dikmen and c.  fvotte , `` nonnegative dictionary learning in the exponential noise model for adaptive music signal representation , '' _ advances in neural information processing systems _ , vol .  24 , pp .",
    "22672275 , 2011 .    c.  archambeau and f.  bach , `` sparse probabilistic projections , '' _ advances in neural information processing systems _ , vol .  21 , pp .",
    "7380 , 2009 .",
    "f.  deleus and m.  v. hulle , `` functional connectivity analysis of fmri data based on regularized multiset canonical correlation analysis , '' _ j. neuroscience methods _ , vol .  197 , no .  1 ,",
    "pp .  143157 , 2011 .",
    "x.  qu and x.  chen , `` sparse structured probabilistic projections for factorized latent spaces , '' _ proc .",
    "20th acm int .",
    "information and knowledge management _ , pp .  13891394 , 2011 .",
    "e.  lock , k.  hoadley , j.  marron , and a.  nobel , `` joint and individual variation explained ( jive ) for integrated analysis of multiple datatypes , '' _ annals of applied statistics _ , vol .  7 , no .  1 ,",
    "pp .  523542 , 2013 .    p.  ray , l.  zheng , y.  wang , j.  lucas , d.  dunson , and l.  carin , `` bayesian joint analysis of heterogeneous data , '' duke univ . ,",
    "dept . of elec . and comp .",
    "eng . , tech . rep . , 2013 .",
    "m.  west , `` bayesian factor regression models in the `` large p , small n '' paradigm , '' _ bayesian statistics _ ,",
    "vol .  7 , pp .",
    "733742 , 2003 .",
    "n.  chen , j.  zun , and e.  xing , `` predictive subspace learning for multi - view data : a large margin approach , '' _ advances in neural information processing systems _ , pp.361369 , 2010 .",
    "p.  rai , and h.  daum , `` the infinite hierarchical factor regression model , '' _ advances in neural information processing systems _ , vol .  21 , pp .  13211328 , 2009 .",
    "m.  yuan , and y.  li , `` model selection and estimation in regression with grouped variables , '' _",
    "j. royal statistical society : series b ( statistical methodology ) _ , vol .",
    "68 , no .  1 ,",
    "pp .  4967 , 2006 .",
    "p.  breheny , and j.  huang , `` group descent algorithms for nonconvex penalized linear and logistic regression models with grouped predictors , '' _ statistics and computing _ ,",
    "pp.115 , 2013 .",
    "k.  j.  friston , `` functional and effective connectivity in neuroimaging : a synthesis , '' _ human brain mapping _ , vol .  2 , no .",
    "( 1 - 2 ) , pp .  5678 , 1994 .    s.  smith et al .",
    ", `` correspondence of the brain s functional architecture during activation and rest , '' _ proc",
    ". national academy of sciences ( pnas ) _ , vol .",
    "106 , no .",
    "31 , pp .  1304013045 , 2009 .",
    "k.  friston , c.  frith , p.  liddle , and r.  frackowiak , `` functional connectivity : the principal - component analysis of large ( pet ) data sets,''__j .",
    "cerebral blood flow and metabolism _ _ , vol .  13 , no .  1 ,",
    "pp .  514 , 1993 .",
    "v.  alluri , p.  toiviainen , i.  p. jskelinen , e.  glerean , m.  sams , and e.  brattico , `` large - scale brain networks emerge from dynamic processing if musical timbre , key and rhythm , '' _ neuroimage _ , vol .  59 , no .  4 , pp . 36773689 , 2012 .    m.  jenkinson , c.  beckmann , t.  behrens , m.  woolrich , and s.  smith , `` fsl , '' _ neuroimage _ , vol .  62 , no .  2 , pp .  782790 , 2012 .    r.  desikan , f.  segonne , b.  fischl , b.  quinn , b.  dickerson , d.  blacker , r.  buckner , a.  dale , r.  maguire , b.  hyman , m.  albert , and r.  killiany , `` an automated labeling system for subdividing the human cerebral cortex on mri scans into gyral based regions of interest , '' _ neuroimage _ , vol .  31 , no .  3 , pp .",
    "980986 , 2006 .",
    "j.  diedrichsen , j.  balster , e.  cussans , and n.  ramnani , `` a probabilistic mr atlas of the human cerebellum , '' _ neuroimage _ , vol .",
    "46 , no .  1 , pp . 3946 , 2009 .",
    "s.  a. khan , a.  faisal , j.  p. mpindi , j.  a. parkkinen , t.  kalliokoski , a.  poso , o.  p. kallioniemi , k.  wennerberg , and s.  kaski , `` , '' _ _ , vol .",
    "13 , no .",
    "112 , pp .  115 , 2012 .",
    "j.  lamb , e.  crawford , d.  peck , j.  modell , i.  blat , m.  wrobel , j.  lerner , j.  brunet , a.  subramanian , k.  ross _",
    "_ , `` the connectivity map : using gene - expression signatures to connect small molecules , genes , and disease , '' _ science _ , vol .",
    "313 , no .",
    "5795 , pp .  19291935 , 2006 .",
    "r.  baeza - yates , b.  ribeiro - neto , `` modern information retrieval , '' acm press new york , 1999 .",
    "a.  klami , g.  bouchard , and a.  tripathi , `` group - sparse embeddings in collective matrix factorization , '' _ proc .",
    "conf . learning representations _ , 2014 .",
    "x.  chen , s.  chen , h.  xue , and x.  zhou , `` a unified dimensionality reduction framework for semi - paired and semi - supervised multi - view data , '' _ pattern recognition _",
    "45 , no .  5 , pp .",
    "20052018 , 2012 .",
    "the latent variables are updated as @xmath144 , where @xmath145        finally , for the low - rank model , @xmath152 is updated by optimizing the lower bound numerically .",
    "the bound as a function of @xmath50 and @xmath51 is given by @xmath153 the gradients w.r.t .",
    "the cost function are given as @xmath154 where @xmath155 ."
  ],
  "abstract_text": [
    "<S> factor analysis provides linear factors that describe relationships between individual variables of a data set . </S>",
    "<S> we extend this classical formulation into linear factors that describe relationships between groups of variables , where each group represents either a set of related variables or a data set . </S>",
    "<S> the model also naturally extends canonical correlation analysis to more than two sets , in a way that is more flexible than previous extensions . </S>",
    "<S> our solution is formulated as variational inference of a latent variable model with structural sparsity , and it consists of two hierarchical levels : the higher level models the relationships between the groups , whereas the lower models the observed variables given the higher level . </S>",
    "<S> we show that the resulting solution solves the group factor analysis problem accurately , outperforming alternative factor analysis based solutions as well as more straightforward implementations of group factor analysis . </S>",
    "<S> the method is demonstrated on two life science data sets , one on brain activation and the other on systems biology , illustrating its applicability to the analysis of different types of high - dimensional data sources .    </S>",
    "<S> factor analysis , multi - view learning , probabilistic algorithms , structured sparsity </S>"
  ]
}