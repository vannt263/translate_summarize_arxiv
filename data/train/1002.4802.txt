{
  "article_text": [
    "a cornerstone principle of many disciplines is that observations are noisy measurements of hidden variables of interest .",
    "this is particularly prominent in fields such as social sciences , psychology , marketing and medicine .",
    "for instance , data can come in the form of social and economical indicators , answers to questionnaires in a medical exam or marketing survey , and instrument readings such as fmri scans .",
    "such indicators are treated as measures of latent factors such as the latent ability levels of a subject in a psychological study , or the abstract level of democratization of a country . the literature on structural equation models ( sems ) @xcite approaches such problems with directed graphical models , where each node in the graph is a noisy function of its parents .",
    "the goals of the analysis include typical applications of latent variable models , such as projecting points in a latent space ( with confidence regions ) for ranking , clustering and visualization ; density estimation ; missing data imputation ; and causal inference @xcite .",
    "this paper introduces a nonparametric formulation of sems with hidden nodes , where functions connecting latent variables are given a gaussian process prior .",
    "an efficient but flexible sparse formulation is adopted .",
    "to the best of our knowledge , our contribution is the first full gaussian process treatment of sems with latent variables .",
    "we assume that the model graphical structure is given .",
    "structural model selection with latent variables is a complex topic which we will not pursue here : a detailed discussion of model selection is left as future work . @xcite and @xcite discuss relevant issues .",
    "our goal is to be able to generate posterior distributions over parameters and latent variables with scalable sampling procedures with good mixing properties , while being competitive against non - sparse gaussian process models .    in section [ sec : model ]",
    ", we specify the likelihood function for our structural equation models and its implications . in section [ sec : sparse ] , we elaborate on priors , bayesian learning , and a sparse variation of the basic model which is able to handle larger datasets .",
    "section [ sec : mcmc ] describes a markov chain monte carlo ( mcmc ) procedure .",
    "section [ sec : experiments ] evaluates the usefulness of the model and the stability of the sampler in a set of real - world sem applications with comparisons to modern alternatives .",
    "finally , in section [ sec : related ] we discuss related work .    [ cols=\"^,^ \" , ]     we also evaluate how the non - sparse gpsem - lv behaves compared to the sparse alternative .",
    "notice that while * consumer * and * housing * have each approximately 300 training points in each cross - validation fold , * abalone * has over 3000 points . for the non - sparse gpsem ,",
    "we subsampled all of * abalone * training folds down to 300 samples .",
    "results are presented in table [ tab : llik ] .",
    "each dataset was chosen to represent a particular type of problem .",
    "the data in * consumer * is highly linear . in particular , it is important to point out that the gpsem - lv model is able to behave as a standard structural equation model if necessary , while the quadratic polynomial model shows some overfitting .",
    "the * abalone * study is known for having clear functional relationships among variables , as also discussed by @xcite . in this case",
    ", there is a substantial difference between the non - linear models and the linear one , although gplvm seems suboptimal in this scenario where observed variables can be easily clustered into groups . finally , functional relationships among variables in * housing * are not as clear @xcite , with multimodal residuals .",
    "gpsem still shows an advantage , but all sems are suboptimal compared to gplvm .",
    "one explanation is that the dag on which the models rely is not adequate .",
    "structure learning might be necessary to make the most out of nonparametric sems .",
    "although results suggest that the sparse model behaved better that the non - sparse one ( which was true of some cases found by snelson and ghahramani , 2006 , due to heteroscedasticity effects ) , such results should be interpreted with care .",
    "* abalone * had to be subsampled in the non - sparse case .",
    "mixing is harder in the non - sparse model since all datapoints @xmath0 are dependent .",
    "while we believe that with larger sample sizes and denser latent structures the non - sparse model should be the best , large sample sizes are too expensive to process and , in many sem applications , latent variables have very few parents .",
    "it is also important to emphasize that the wallclock sampling time for the non - sparse model was an order of magnitude larger than the sparse case with @xmath1 @xmath2 even considering that 3000 training points were used by the sparse model in the * abalone * experiment , against 300 points by the non - sparse alternative .",
    "non - linear factor analysis has been studied for decades in the psychometrics literaturecook / movabletype / archives/ 2009/01/a_longstanding.html ] .",
    "a review is provided by @xcite .",
    "however , most of the classic work is based on simple parametric models . a modern approach based on gaussian processes",
    "is the gaussian process latent variable model of @xcite . by construction ,",
    "factor analysis can not be used in applications where one is interested in learning functions relating latent variables , such as in causal inference . for embedding ,",
    "factor analysis is easier to use and more robust to model misspecification than sem analysis .",
    "conversely , it does not benefit from well - specified structures and might be harder to interpret .",
    "@xcite discusses the interplay between factor analysis and sem .",
    "practical non - linear structural equation models are discussed by @xcite , but none of such approaches rely on nonparametric methods .",
    "gaussian processes latent structures appear mostly in the context of dynamical systems ( e.g. , @xcite ) .",
    "however , the connection is typically among data points only , not among variables within a data point , where on - line filtering is the target application .",
    "the goal of graphical modeling is to exploit the structure of real - world problems , but the latent structure is often ignored .",
    "we introduced a new nonparametric approach for sems by extending a sparse gaussian process prior as a fully bayesian procedure .",
    "although a standard mcmc algorithm worked reasonably well , it is possible as future work to study ways of improving mixing times",
    ". this can be particularly relevant in extensions to ordinal variables , where the sampling of thresholds will likely make mixing more difficult . since the bottleneck of the procedure is the sampling of the pseudo - inputs",
    ", one might consider a hybrid approach where a subset of the pseudo - inputs is fixed and determined prior to sampling using a cheap heuristic .",
    "new ways of deciding pseudo - input locations based on a given measurement model will be required .",
    "evaluation with larger datasets ( at least a few hundred variables ) remains an open problem .",
    "finally , finding ways of determining the graphical structure is also a promising area of research .",
    "we thank patrick hoyer for several relevant discussions concerning the results of section [ sec : identifiability ] , and irini moustaki for the consumer data .",
    "we use a mcmc sampler to draw all variables of interest from the posterior distribution of a gpsem model .",
    "let @xmath3 denote the number of pseudo - inputs per latent function @xmath4 , @xmath5 be the sample size , @xmath6 the number of latent variables and @xmath7 the common number of gaussian mixture components for each exogenous latent variable .",
    "the sampler is a standard metropolis - hastings procedure with block sampling : random variables are divided into blocks , where we sample each block conditioning on the current values of the remaining blocks .      * the linear coefficients for the structural equation of each observed variable @xmath8 : @xmath9 * the conditional variance for the structural equation of each observed variable @xmath8 : @xmath10 * the @xmath11-th instantiation of each latent variable @xmath12 , @xmath13 ; * the set of latent function values @xmath14 for each particular endogenous latent variable @xmath12 * the conditional variance for the structural equation of each latent variable @xmath12 : @xmath15 * the set of latent mixture component indicators @xmath16 for each particular exogenous latent variable @xmath12 * the set of means @xmath17 for the mixture components of each particular exogenous latent variable @xmath12 * the set of variances @xmath18 for the mixture components of each particular exogenous latent variable @xmath12 * mixture distribution @xmath19",
    "corresponding to the probability over mixture components for exogenous latent variable @xmath12      * all instantiations of a given latent variable @xmath13 , for @xmath20 , are mutually independent conditioned on the functions , pseudo - inputs and pseudo - functions . as such , they can be treated as a single block of size @xmath5 , where all elements are sampled in parallel ; * the @xmath11-th instantiation of each pseudo - input @xmath21 for @xmath22 * all instantiations of latent functions and pseudo - latents functions @xmath23 for any particular @xmath12 are conditionally multivariate gaussian and can be sampled together    we adopt the convention that , for any particular step described in the following procedure , any random variable that is not explicitly mentioned should be considered fixed at the current sampled value . moreover ,",
    "any density function that depends on such implicit variables uses the respective implicit values .",
    "the measurement model can be integrated out in principle , if we adopt a conjugate normal - inverse gamma prior for the linear regression of observed variables @xmath24 on @xmath25 .",
    "however , we opted for a non - conjugate prior in order to evaluate the convergence of the sampler when this marginalization can not be done ( as in alternative models with non - gaussian error terms ) .",
    "given the latent variables , the corresponding conditional distributions for the measurement model parameters boil down to standard bayesian linear regression posteriors . in our metropolis - hastings scheme",
    ", our proposals correspond to such conditionals , as in gibbs sampling ( and therefore have an acceptance probability of 1 ) .",
    "let @xmath26 be the parents of observed variable @xmath8 in the graph and let the @xmath11-th instantiation of the corresponding regression input be @xmath27^\\mathsf{t}$ ] .",
    "let each cofficient @xmath28 have an independent gaussian prior with mean zero and variance @xmath29 .",
    "conditioned on the error variance @xmath10 , the posterior distribution of the vector @xmath30^\\mathsf{t}$ ] is multivariate gaussian with covariance @xmath31 and mean @xmath32 , where @xmath33 is a @xmath34 identity matrix .      for a fixed set of linear coefficients @xmath35 , we now sample the conditional variance @xmath10 .",
    "let this variance have a inverse gamma prior @xmath36 .",
    "its conditional distribution is an inverse gamma @xmath37 , where @xmath38 , @xmath39 , and @xmath40 .      for all @xmath41 and @xmath20",
    ", we propose each new latent variable value @xmath42 individually , and accept or reject it based on a gaussian random walk proposal centered at the current value @xmath13 .",
    "we accept the move with probability @xmath43 where , if @xmath12 is not an exogenous variable in the graph , @xmath44    recall that @xmath4 is a function of the parents @xmath45 of @xmath12 in the graph .",
    "the @xmath11-th instantiation of such parents assume the value @xmath46 .",
    "we use @xmath47 as a shorthand notation for @xmath48 .",
    "morever , let @xmath49 denote the latent children of @xmath12 in the graph .",
    "the symbol @xmath50 refers to the respective function values taken by @xmath51 in data points @xmath52 .",
    "function @xmath53 is the conditional density of @xmath54 given @xmath50 , according to the gaussian process prior .",
    "the evaluation of this factor costs @xmath55 using standard submatrix cholesky updates @xcite . as",
    "such , sampling all latent values for @xmath12 takes @xmath56 .",
    "finally , @xmath57 denotes the observed children of @xmath12 , and function @xmath58 is the corresponding density of observed child @xmath59 evaluated at @xmath60 , given its parents ( which includes @xmath61 ) and ( implicit ) measurement model parameters .",
    "this factor can be dropped if @xmath60 is missing .",
    "if variable @xmath12 is an exogenous variable , then the factor @xmath62 gets substituted by @xmath63 where @xmath64 is the latent mixture indicator for the marginal mixture of gaussians model for @xmath12 , with means @xmath65 and variances @xmath18 .",
    "given all latent variables , latent function values @xmath66 are multivariate gaussian with covariance matrix @xmath67 where @xmath68 is the corresponding kernel matrix and @xmath33 is a @xmath69 identity matrix .",
    "the respective mean is given by @xmath70 , where @xmath71^\\mathsf{t}$ ] .",
    "this operation costs @xmath56 .",
    "we sample from this conditional as in a standard gibbs update .    sampling each latent conditional variance @xmath15",
    "can also be done by sampling from its conditional .",
    "let @xmath15 have an inverse gamma prior @xmath72 .",
    "the conditional distribution for this variance given all other random variables is inverse gamma @xmath73 , where @xmath74 and @xmath75 .",
    "we are left with sampling the mixture model parameters that correspond to the marginal distributions of the exogenous latent variables .",
    "once we condition on the latent variables , this is completely standard .",
    "if each mixture mean parameter @xmath76 is given an independent gaussian prior with mean zero and variance @xmath77 , its conditional given the remaining variables is also gaussian with variance @xmath78 , where @xmath79 is the subset of @xmath80 such that @xmath81 if and only if @xmath82 .",
    "the corresponding mean is given by @xmath83 .",
    "if each mixture variance parameter @xmath84 is given an inverse gamma prior @xmath85 , its conditional is an inverse gamma @xmath86 , where @xmath87 , and @xmath88 .",
    "the conditional probability @xmath89 is proportional to @xmath90 .",
    "finally , given a dirichlet prior distribution @xmath91 for each @xmath19 , its conditional is also dirichlet with parameter vector @xmath92 appendix b : a note on directionality detection ----------------------------------------------",
    "the assumption of linearity of the measurement model is not only a matter of convenience . in sem applications , observed variables",
    "are carefully chosen to represent different aspects of latent concepts of interest and often have a single latent parent .",
    "as such , it is plausible that children of a particular latent variable are different noisy linear transformations of the target latent variable .",
    "this differs from other applications of latent variable gaussian process models such as those introduced by @xcite , where measurements are not designed to explicitly account for target latent variables of interest .",
    "moreover , this linearity condition has important implications on distinguishing among candidate models .",
    "we assumed that the dag @xmath93 is given .",
    "a detailed discussion of model selection is left as future work .",
    "instead , we discuss some theoretical aspects of a very particular but important structural feature that will serve as a building block to more general model selection procedures , in the spirit of @xcite : determining sufficient conditions for the subproblem of detecting edge directionality from the data . given a measurement model for two latent variables @xmath94 and @xmath95 , we need to establish conditions in which we can test whether the only correct latent structure is @xmath96 , @xmath97 , the disconnected structure , or either directionality . the results of @xcite can be extended to the latent variable case by exploiting the conditions of identifiability discussed in section [ sec : identifiability ] as follows .",
    "our sufficient conditions are a weaker set of assumptions than that of @xcite .",
    "we assume that @xmath94 has at least two observable children which are not children of @xmath95 and vice - versa .",
    "call these sets @xmath98 and @xmath99 , respectively .",
    "assume all error terms ( @xmath100 ) are non - gaussian and latent error terms are allowed to be gaussian , as in our original model description are also possible and will be treated in the future . ] .",
    "the variance of all error terms is assumed to be nonzero . as in @xcite",
    ", we also assume @xmath94 and @xmath95 are unconfounded .    to test whether the model where @xmath94 and @xmath95 are independent becomes easy in this case : the independence model entails that ( say ) @xmath101 and @xmath102 are marginally independent .",
    "this can be tested using the nonparametric marginal independence test of @xcite .    for the nontrivial case where latent variables are dependent ,",
    "the results of section [ sec : identifiability ] imply that the measurement model of @xmath103 is identifiable up to the scale and sign of the latent variables , including the marginal distributions of @xmath104 and @xmath105 .",
    "an analogous result applies to @xmath106 .",
    "since the measurement model @xmath107 of @xmath108 is identifiable , assume without loss of generality that the linear coefficients corresponding to @xmath109 and @xmath110 are fixed to 1 , i.e. , @xmath111 and @xmath112 .",
    "also from section [ sec : identifiability ] , it follows that the distribution of @xmath113 can be identified under very general conditions .",
    "the main result of @xcite can then be directly applied .",
    "that is , data generated by a model @xmath114 , with @xmath115 being non - gaussian and independent of @xmath94 , can not be represented by an analogous generative model @xmath116 except in some particular cases that are ruled out as implausible .",
    "the test for comparing @xmath117 against @xmath118 in @xcite can be modified to our context as follows : we can not regress @xmath95 on @xmath94 and estimate the residuals @xmath119 since @xmath94 and @xmath95 are latent .",
    "however , we can do a error - in - variables regression of @xmath102 on @xmath101 using @xmath120 and @xmath121 as instrumental variables @xcite : this means we find a function @xmath122 such that @xmath123 and @xmath124 , for non - gaussian latent variables @xmath125 and @xmath126 .",
    "we then calculate the estimated residuals @xmath127 of this regression , and test whether such residuals are independent of @xmath101 @xcite .",
    "if this is true , then we have no evidence to discard the hypothesis @xmath117 .",
    "the justification for this process is that , if the true model is indeed @xmath128 , then @xmath129 and @xmath130 in the limit of infinite data , since the error - in - variables regression model is identifiable in our case @xcite , with @xmath131 being a consequence of deconvolving @xmath101 and @xmath104 . by this result",
    ", @xmath127 will be independent of @xmath101 .",
    "however , if the opposite holds ( @xmath132 ) then , as in @xcite , the residual is not in general independent of @xmath101 : given @xmath94 ( @xmath133 ) , there is a d - connecting path @xmath134 @xcite , and @xmath127 will be a function of @xmath135 , which is dependent on @xmath101 .",
    "this is analogous to @xcite , but using a different family of regression techniques .",
    "error - in - variables regression is a special case of the gaussian process sem .",
    "the main practical difficulty on using gpsem with the pseudo - inputs approximation in this case is that such pseudo - inputs formulation implies a heteroscedastic regression model @xcite .",
    "one has either to use the gpsem formulation without pseudo - inputs , or a model linear in the parameters but with an explicit , finite , basis dictionary on the input space ."
  ],
  "abstract_text": [
    "<S> in a variety of disciplines such as social sciences , psychology , medicine and economics , the recorded data are considered to be noisy measurements of latent variables connected by some causal structure . </S>",
    "<S> this corresponds to a family of graphical models known as the structural equation model with latent variables . </S>",
    "<S> while linear non - gaussian variants have been well - studied , inference in nonparametric structural equation models is still underdeveloped . </S>",
    "<S> we introduce a sparse gaussian process parameterization that defines a non - linear structure connecting latent variables , unlike common formulations of gaussian process latent variable models . </S>",
    "<S> the sparse parameterization is given a full bayesian treatment without compromising markov chain monte carlo efficiency . </S>",
    "<S> we compare the stability of the sampling procedure and the predictive ability of the model against the current practice . </S>"
  ]
}