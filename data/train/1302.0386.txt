{
  "article_text": [
    "autonomous robots are inherently complex machines that have to cope with a dynamic and often hostile environment . they face an even more demanding context when they operate for a long time without any assistance , whether when exploring remote places  @xcite or , more prosaically , in a house without any robotics expert  @xcite . as famously pointed out by @xcite ,",
    "when designing such complex systems , `` [ we should not ] wonder _ if _ some mishap may happen , but rather ask _",
    "what _ one will do about it when it occurs '' . in autonomous robotics ,",
    "this remark means that robots must be able to pursue their mission in situations that have not been anticipated by their designers .",
    "legged robots clearly illustrate this need to handle the unexpected : to be as versatile as possible , they involve many moving parts , many actuators and many sensors  @xcite ; but they may be damaged in numerous different ways .",
    "these robots would therefore greatly benefit from being able to autonomously find a new behavior if some legs are ripped off , if a leg is broken or if one motor is inadvertently disconnected  ( fig .  [",
    "fig : scenarios ] ) .",
    "+    fault tolerance and resilience are classic topics in robotics and engineering .",
    "the most classic approaches combine intensive testing with redundancy of components @xcite .",
    "these methods undoubtedly proved their usefulness in space , aeronautics and numerous complex systems , but they also are expensive to operate and to design .",
    "more importantly , they require the identification of the faulty subsystems and a procedure to bypass them , whereas both operations are difficult for many kinds of faults  for example mechanical failures .",
    "another classic approach to fault tolerance is to employ robust controllers that can work in spite of damaged sensors or hardware inefficiencies  @xcite .",
    "such controllers usually do not require diagnosing the damage , but this advantage is tempered by the need to integrate the reaction to all faults in a single controller .",
    "last , a robot can embed a few pre - designed behaviors to cope with anticipated potential failures  @xcite .",
    "for instance , if a hexapod robot detects that one of its legs is not reacting as expected , it can drop it and adapt the position of the other legs accordingly  @xcite .",
    "an alternative line of thought is to _ let the robot learn on its own _ the best behavior for the current situation .",
    "if the learning process is open enough , then the robot should be able to discover new compensatory behaviors in situations that have not been foreseen by its designers .",
    "numerous learning systems have been experimented in robotics ( for reviews , see @xcite ) , with different levels of openness and various a priori constraints .",
    "many of them primarily aim at automatically tuning controllers for complex robots  @xcite whereas only a handful of these systems has been explicitly tested in situations in which a robot needs to adapt itself to unexpected situations  @xcite .",
    "finding the behavior that maximizes performance in the current situation is a _ reinforcement learning problem _",
    "@xcite , but classic reinforcement learning algorithms ( e.g. td - learning , sarsa , ... ) are designed for discrete state spaces  @xcite .",
    "they are therefore hard to use when learning continuous behaviors such as locomotion patterns .",
    "policy gradient algorithms  @xcite are reasonably fast learning algorithms that are better suited for robotics ( authors typically report learning time of 20 minutes to a few hours ) , but they are essentially limited to a local search in the parameter space : they lack the openness of the search that is required to cope with truly unforeseen situations . evolutionary algorithms ( eas )  @xcite can optimize reward functions in larger , more open search spaces ( e.g. automatic design of neural networks , design of structures )  @xcite , but this openness is counterbalanced by substantially longer learning time ( according to the literature , 2 to 10 hours for simple robotic behaviors ) .    all policy gradient and evolutionary algorithms spend most of their running time in evaluating the quality of controllers by testing them on the target robot . since , contrary to simulation , reality can not be sped up , their running time can only be improved by finding strategies to evaluate fewer candidate solutions on the robot . in their `` starfish robot '' project , @xcite designed a general approach for resilience that makes an important step in this direction .",
    "the algorithm of bongard et al .",
    "is divided into two stages : ( 1 ) automatically building an internal simulation of the whole robot by observing the consequences of a few elementary actions ( about 15 in the demonstrations of the paper )  this internal simulation of the whole body is called a _ _ self - model _",
    "_  @xcite ; ( 2 ) launching _ in this simulation _ an ea to find a new controller . in effect , this algorithm transfers most of the learning time to a computer simulation , which makes it increasingly faster when computers are improved  @xcite .",
    "bongard s algorithm highlights how mixing a self - model with a learning algorithm can reduce the time required for a robot to adapt to an unforeseen situation .",
    "nevertheless , it has a few important shortcomings .",
    "first , actions and models are undirected : the algorithm can `` waste '' a lot of time to improve parts of the self - model that are irrelevant for the task .",
    "second , it is computationally expensive because it includes a full learning algorithm ( the second stage , in simulation ) and an expensive process to select each action that is tested on the robot .",
    "third , there is often a `` reality gap '' between a behavior learned in simulation and the same behavior on the target robot  @xcite , but nothing is included in bongard s algorithm to prevent such gap to happen : the controller learned in the simulation stage may not work well on the real robot , even if the self - model is accurate .",
    "last , one can challenge the relevance of calling into question the full self - model each time an adaptation is required , for instance if an adaptation is only temporarily useful .    in the present paper we introduce a new resilience algorithm that overcomes these shortcomings while still performing most of the search in a simulation of the robot .",
    "our algorithm works with any parametrized controller and it is especially efficient on modern , multi - core computers . more generally , it is designed for situations in which :    * behaviors optimized on the undamaged robot are not efficient anymore on the damaged robot ( otherwise , adaptation is useless ) and qualitatively new behavior is required ( otherwise , local search algorithms should perform better ) ; * the robot can only rely on internal measurements of its state ( truly autonomous robots do not have access to perfect , external sensing systems ) ; * some damages can not be observed or measured directly ( otherwise more explicit methods may be more efficient ) .",
    "our algorithm is inspired by the `` transferability approach ''  @xcite , whose original purpose is to cross the `` reality gap '' that separates behaviors optimized in simulation to those observed on the target robot .",
    "the main proposition of this approach is to make the optimization algorithm aware of the limits of the simulation . to this end , a few controllers are transferred during the optimization and a regression algorithm ( e.g. a svm or a neural network ) is used to approximate the function that maps behaviors in simulation to the difference of performance between simulation and reality . to use this",
    "approximated _ transferability function _ , the single - objective optimization problem is transformed into a multi - objective optimization in which both performance in simulation and transferability are maximized .",
    "this optimization is typically performed with a stochastic multi - objective optimization algorithm but other optimization algorithms are conceivable .    as this paper will show , the same concepts can be applied to design a fast adaptation algorithm for resilient robotics , leading to a new algorithm that we called `` t - resilience '' ( for transferability - based resilience ) .",
    "if a damaged robot embeds a simulation of itself , then behaviors that rely on damaged parts will not be transferable : they will perform very differently in the self - model and in reality . during the adaptation process",
    ", the robot will thus create an approximated transferability function that classifies behaviors as `` working as expected '' and `` not working as expected '' .",
    "hence the robot will possess an `` intuition '' of the damages but it will not explicitly represent or identify them . by optimizing both the transferability and the performance , the algorithm will look for the most efficient behaviors among those that only use the reliable parts of the robots .",
    "the robot will thus be able to sustain a functioning behavior when damage occurs by learning to avoid behaviors that it is unable to achieve in the real world . besides this damage recovery scenario",
    ", the t - resilience algorithm opens a new class of adaptation algorithms that benefit from moore s law by transferring most of the adaptation time from real experiments to simulations of a self - model .",
    "we evaluate the t - resilience algorithm on an 18-dofs hexapod robot that needs to adapt to leg removal , broken legs and motor failures ; we compare it to stochastic local search  @xcite , policy gradient  @xcite and bongard s algorithm  @xcite .",
    "the behavior on the real robot is assessed on - board thanks to a rgb - d sensor coupled with a state - of - the - art slam algorithm  @xcite .",
    "discovering a new behavior after a damage is a particular case of _ learning _ a new behavior , a question that generates an abundant literature in artificial intelligence since its beginnings  @xcite .",
    "we are here interested in reinforcement learning algorithms because we consider scenarios in which evaluating the performance of a behavior is possible but the optimal behavior is unknown",
    ". however , classic reinforcement learning algorithms are primarily designed for discrete states and discrete actions  @xcite , whereas autonomous robots have to solve many continuous problems ( e.g. motor control ) .",
    "two alternative families of methods are currently prevalent for continuous reinforcement learning in robotics ( table [ table : direct_methods ] ) : policy gradient methods and evolutionary algorithms .",
    "these two approaches both rely on optimization algorithms that directly optimize parameters of a controller by measuring the overall performance of the robot ( fig .",
    "[ fig : approaches_policy ] ) ; learning is thus here regarded as an optimization of these parameters .",
    "policy gradient methods  @xcite use iterative stochastic optimization algorithms to find a local extremum of the reward function .",
    "the search starts with a controller that can be generated at random , designed by the user or inferred from a demonstration .",
    "the algorithm then iteratively modifies the parameters of the controller by estimating gradients in the control space and applying slight changes to the parameters .",
    "typical policy gradient methods iterate the following steps :    * generation of @xmath0 controllers in the neighborhood of the current vector of parameters ( by variating one or multiple parameter values at once ) ; * estimation of the gradient of the reward function in the control space ; * modification of parameter values according to the gradient information .",
    "these steps are iterated until a satisfying controller is found or until the process converges .",
    "policy gradient algorithms essentially differ in the way gradient is estimated .",
    "the most simple way is the finite - difference method , which independently estimates the local gradient of the reward function for each parameter  @xcite : considering a given parameter , if higher ( resp .",
    "lower ) values lead to higher rewards on average on the @xmath0 controllers tested during the current iteration , the value of the parameter is increased ( resp .",
    "decreased ) for the next iteration .",
    "such a simple method for estimating the gradient is especially efficient when parameters are mostly independent . strong dependencies between the parameters often require more sophisticated estimation techniques .",
    "policy gradient algorithms have been successfully applied to locomotion tasks in the case of quadruped  @xcite and biped robots  @xcite but they typically require numerous evaluations on the robot , most of the times more than 1000 trials in a few hours ( table [ table : direct_methods ] ) . to make learning tractable , these examples all use carefully designed controllers with only a few degrees of freedom .",
    "they also typically start with well - chosen initial parameter values , making them efficient algorithms for imitation learning when these values are extracted from a demonstration by a human  @xcite .",
    "recent results on the locomotion of a quadruped robot suggest that using random initial controllers would likely require many additional experiments on the robot  @xcite .",
    "consistent results have been reported on biped locomotion with computer simulations using random initial controllers that make the robot fall  @xcite ( about 10 hours of learning for 11 control parameters ) .",
    "evolutionary algorithms ( eas )  @xcite are another family of iterative stochastic optimization methods that search for the optima of function  @xcite .",
    "they are less prone to local optima than policy gradient algorithms and they can optimize arbitrary structures ( neural networks , fuzzy rules , vector of parameters , ... )  @xcite .",
    "while there exists many variants of eas , the vast majority of them iterate the following steps :    * ( first iteration only ) random initialization of a population of candidate solutions ; * evaluation of the performance of each controller of the population ( by testing the controller on the robot ) ; * ranking of controllers ; * selection and variation around the most efficient controllers to build a new population for the next iteration .",
    "learning experiments with eas are reported to require many hundreds of trials on the robot and to last from two to tens of hours ( table [ table : direct_methods ] ) .",
    "eas have been applied to quadruped robots  @xcite , hexapod robots  @xcite and humanoids  @xcite .",
    "eas have also been used in a few studies dedicated to resilience , in particular on a snake - like robot with a damaged body  @xcite ( about 600 evaluations/10 hours ) and on a quadrupedal robot that breaks one of its leg  @xcite ( about 670 evaluations/2 hours ) .    aside from these two main types of approaches",
    ", several authors proposed to use other black - box optimization algorithms : global methods like nelder - mead descent  @xcite , local methods like powell s method  @xcite or surrogate - based optimization  @xcite .",
    "published results are typically obtained with hundreds of evaluations on the robot , requiring several hours ( table  [ table : direct_methods ] ) .",
    "regardless of the optimization technique , reward functions are , in most studies , evaluated with external tracking devices ( table  [ table : direct_methods ] , last column ) . while this approach is useful when researchers aims at finding the most efficient controllers ( e.g. @xcite ) , learning algorithms that target adaptation and resilience need to be robust to the inaccuracies and constraints of on - board measurements .",
    "instead of directly learning control parameters , @xcite propose to improve the resilience of robots by equipping robots with a _ self - model_. if a disagreement is detected between the self - model and observations , the proposed algorithm first infers the damages by chosing motor actions and measuring their consequences on the behavior of the robot ; the algorithm then relies on the updated model of the robot to learn a new behavior .",
    "this approach has been successfully tested on a starfish - like quadrupedal robot  @xcite . by adapting its self - model",
    ", the robot manages to discover a new walking gait after the loss of one of its legs .    in bongard",
    "s algorithm , the identification of the self - model is based on an active learning loop that is itself divided into an _ action selection loop _ and a _ model selection loop _ ( fig .  [",
    "fig : approaches_bongard ] ) .",
    "the action selection loop aims at selecting the action that will best distinguish the models of a population of candidate models .",
    "the model selection loop looks for the models that best predict the outcomes of the actions as measured on the robot . in the `` starfish '' experiment  @xcite",
    ", the following steps are repeated :    * action selection ( _ exploration _ ) : * * each of the 36 possible actions is tested on each of the @xmath1 candidate models to observe the orientation of robot s body predicted by the model ; * * the action for which models of the population disagree at most is selected ; * * this action is tested on the robot and the corresponding exact orientation of robot s body is recorded by an external camera ; * model selection loop ( _ estimation _ ) : * * a stochastic optimization algorithm ( an ea ) is used to optimize the population of models so that they accurately predict what was measured with the robot , for each tested action ; * * if less than @xmath2 actions have been performed , the action selection loop is started again .",
    "once the @xmath2 actions have been performed , the best model found so far is used to learn a new behavior using an ea :    * controller optimization ( _ exploitation _ ) : * * a stochastic optimization algorithm ( an ea ) is used to optimize a population controllers so that they maximize forward displacement within the simulation of the self - model ; * * the best controller found in the simulation is transferred to the robot , making it the new controller .",
    "the population of models is initialized with the self - model that corresponds to the morphology of the undamaged robot .",
    "since the overall process only requires @xmath2 tests on the robot , its speed essentially depends on the performance of the employed computer .",
    "significant computing times are nonetheless required for the optimization of the population of models .    in the results reported by @xcite , only half of the runs led to correct self - models .",
    "as bongard s .",
    "approach implies identifying a full model of the robot , it would arguably require many more tests to converge in most cases to the right morphology . for comparison , results obtained by the same authors but in a simulated experiment required from 600 to 1500 tests to consistently identify the model  @xcite .",
    "it should also be noted that these authors did not measure the orientation of robot s body with internal sensors , whereas noisy internal measurements could significantly impair the identification of the model .",
    "other authors experimented with self - modeling process similar to the one of bongard et al . , but with a humanoid robot  @xcite .",
    "preliminary results suggest that thousands of evaluations on the robot would be necessary to correctly identify 8 parameters of the global self - model .",
    "alternative methods have been proposed to build self - models for robots and all of them require numerous tests , e.g. on a manipulator arm with about 400 real tests  @xcite or on a hexapod robot with about 240 real tests  @xcite .",
    "overall , experimental costs for building self - models appear expensive in the context of resilience applications in both the number of tests on the real robot and in computing time .",
    "furthermore , controllers obtained by optimizing in a simulation  as does the algorithm proposed by bongard et al .",
    " often do no work as well on the real robot than in simulation  @xcite . in effect",
    ", this classic problem has been observed in the starfish experiments  @xcite . in these experiments",
    ", it probably originates from the fact that the identified self - model can not perfectly model every detail of the real world ( in particular , slippage , friction and very dynamic behaviors ) .      based on this short survey of the literature",
    ", two main thoughts can be drawn :    1 .",
    "policy gradient methods and eas can both be used to discover original behaviors on a damaged robot ; nevertheless , when they do nt start from already good initial controllers , they require a high number of real tests ( at least a few hundred ) , which limits the speed of the resulting resilience process .",
    "methods based on self - modeling are promising because they transfer some of the learning time to a simulation ; however building an accurate global model of the damaged robot requires many real tests ; reality gap problems can also occur between the behavior learned with self - model and the real , damaged robot .",
    "following bongard et al . , we equip our robot with a self - model .",
    "a direct consequence is that detecting the occurrence of a damage is facilitated : if the observed performance is significantly different from what the self - model predicts , then the robot needs to start a recovery process to find a better behavior .",
    "nevertheless , contrary to bongard et al .",
    ", we propose that a damaged robot discovers new original behaviors _ using the initial , hand - designed self - model _ , that is without updating the self - model .",
    "since we do not attempt to diagnose damages , the solved problem is potentially easier than the one solved by bongard et al ; we therefore expect our algorithm to perform faster .",
    "this speed increase can , however , comes at the price of slightly less efficient post - damage behaviors .",
    "the model of the undamaged robot is obviously not accurate because it does not model the damages .",
    "nonetheless , since damages ca nt radically change the overall morphology of the robot , this `` undamaged '' self - model can still be viewed as a reasonably accurate model of the damaged robot .",
    "most of the degrees of freedom are indeed correctly positionned , the mass of components should not change much and the body plan is most probably not radically altered .",
    "imperfect simulators and models are an almost unavoidable issue when robotic controllers are first optimized in simulation then transferred to a real robot .",
    "the most affected field is probably evolutionary robotics because of the emphasis on opening the search space as much as possible : behaviors found within the simulation are often not anticipated by the designer of the simulator , therefore it s not surprising that they are often wrongly simulated .",
    "researchers in evolutionary robotics explored three main ideas to cross this `` reality gap '' : ( 1 ) automatically improving simulators  @xcite , ( 2 ) trying to prevent optimized controllers from relying on the unreliable parts of the simulation ( in particular , by adding noise )  @xcite , and ( 3 ) model the difference between simulation and reality  @xcite .",
    "translated to resilient robotics , the first idea is equivalent to improving or adapting the self - model , with the aforementioned shortcomings ( sections [ sec : intro ] and [ sec : review - self - model ] ) .",
    "the second idea corresponds to encouraging the robustness of controllers so that they can deal with an imperfect simulation .",
    "it could lead to improvements in resilient robotics but it requires that the designer anticipates most of the potential damages .",
    "the third idea is more interesting for resilient robotics because it acknowledges that simulations are never perfect and mixes reality and simulation during the optimization . among the algorithms of this family , the recently - proposed transferability approach  @xcite explicitly searches for high - performing controllers that work similarly in both simulation and reality .",
    "it led to successful solutions for quadruped robot ( 2 parameters to optimize ) and for a khepera - like robot in a t - maze ( weights of a feed - forward neural networks to optimize )  @xcite .",
    "the main assumption of the transferability approach is that some transferable behaviors exist in the search space .",
    "although formulated in the context of the reality gap , this assumption holds well in resilient robotics .",
    "for instance , if a hexapod robot breaks a leg , then gaits that do not critically rely on this leg should lead to similar trajectories in the self - model and on the damaged robot .",
    "such gaits are numerous : those that make the simulated robot lift the broken leg so that it never hits the ground ; those that make the robot walk on its `` knees '' ; those that are robust to leg damages because they are closer to crawling than walking .",
    "similar ideas can be found for most robots and for most mechanical and electrical damages , provided that there are different ways to achieve the mission .",
    "for example , any redundant robotic manipulator with a blocked joint should be able to follow a less efficient but working trajectory that does not use this joint .",
    "a _ transferability function _ is a function that maps , for the whole search space , descriptors of solutions ( e.g. control parameters , or behavior descriptors ) to a _ transferability score _ that represents how well the simulation matches the reality .",
    "this function is usually not accessible but it can be learned with a regression algorithm ( neural networks , support vector machines , etc . ) by recording the behavior of a few controllers in reality and in simulation ."
  ],
  "abstract_text": [
    "<S> * damage recovery is critical for autonomous robots that need to operate for a long time without assistance . </S>",
    "<S> most current methods are complex and costly because they require anticipating each potential damage in order to have a contingency plan ready . as an alternative </S>",
    "<S> , we introduce the t - resilience algorithm , a new algorithm that allows robots to quickly and autonomously discover compensatory behaviors in unanticipated situations . </S>",
    "<S> this algorithm equips the robot with a self - model and discovers new behaviors by learning to avoid those that perform differently in the self - model and in reality . </S>",
    "<S> our algorithm thus does not identify the damaged parts but it implicitly searches for efficient behaviors that do not use them . </S>",
    "<S> we evaluate the t - resilience algorithm on a hexapod robot that needs to adapt to leg removal , broken legs and motor failures ; we compare it to stochastic local search , policy gradient and the self - modeling algorithm proposed by bongard et al . </S>",
    "<S> the behavior of the robot is assessed on - board thanks to a rgb - d sensor and a slam algorithm . using only 25 tests on the robot and an overall running time of 20 minutes </S>",
    "<S> , t - resilience consistently leads to substantially better results than the other approaches . * </S>"
  ]
}