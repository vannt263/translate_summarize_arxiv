{
  "article_text": [
    "the starting point of our analysis is a high - dimensional linear regression model of the form @xmath0 where @xmath1 , @xmath2 , are i.i.d .",
    "random pairs with @xmath3 and @xmath4 .",
    "we will assume without loss of generality that @xmath5 for all @xmath6 .",
    "furthermore , @xmath7 is a vector of parameters in @xmath8 and @xmath9 are centered i.i.d . real random variables independent with @xmath10 with @xmath11 .",
    "the dimension @xmath12 of the vector of parameters is assumed to be typically larger than the sample size @xmath13 .",
    "roughly speaking , model ( [ linear - model ] ) comprises two main situations which have been considered independently in two separate branches of statistical literature .",
    "on one side , there is the situation where @xmath10 represents a ( high - dimensional ) vector of different predictor variables .",
    "another situation arises when the regressors are @xmath12 discretizations ( e.g. , at different observations times ) of a same curve . in this case model ( [ linear - model ] ) represents a discrete version of an underlying continuous _ functional linear model_. in the two setups , very different strategies for estimating @xmath7 have been adopted , and underlying structural assumptions seem to be largely incompatible . in this paper we will study similarities and differences of these methodologies , and we will show that a combination of ideas developed in the two settings leads to new estimation procedures which may be useful in a number of important applications .    the first situation is studied in a large literature on model selection in high - dimensional regression .",
    "the basic structural assumptions can be described as follows :    * there is only a relatively small number of predictor variables with @xmath14 which have a significant influence on the outcome @xmath15 . in other words ,",
    "the set of nonzero coefficients is sparse , @xmath16 . * the correlations between different explanatory variables",
    "@xmath17 and @xmath18 , @xmath19 , are `` sufficiently '' weak .    the most popular procedures to identify and estimate nonzero coefficients @xmath20 are lasso and the dantzig selector .",
    "some important references are @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite .",
    "much work in this domain is based on the assumption that the columns @xmath21 , @xmath6 , of the design matrix are almost orthogonal .",
    "for example , @xcite require that `` every set of columns with cardinality less than @xmath22 approximately behaves like an orthonormal system . ''",
    "more general conditions have been introduced by @xcite or @xcite .",
    "the theoretical framework developed in these papers also allows one to study model selection for regressors with substantial amount of correlation , and it provides a basis for the approach presented in our paper .    in sharp contrast , the setup considered in the literature on functional regression rests upon a very different type of structural assumptions .",
    "we will consider the simplest case that @xmath23 for random functions @xmath24)$ ] observed at an equidistant grid @xmath25 .",
    "structural assumptions on coefficients and correlations between variables can then be subsumed as follows :    * @xmath26 , where @xmath27)$ ] is a continuous slope function , and as @xmath28 , @xmath29 .",
    "* there are very high correlations between explanatory variables @xmath23 and @xmath30 , @xmath31 . as @xmath28 , @xmath32 for any fixed  @xmath33 .",
    "some important applications as well as theoretical results on functional linear regression are , for example , presented in @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite . obviously , in this setup",
    "no variable @xmath23 corresponding to a specific observation at grid point @xmath34 will possess a particulary high influence on @xmath35 , and there will exist a large number of small , but nonzero coefficients @xmath20 of size proportional to @xmath36 .",
    "one may argue that dimensionality reduction and therefore some underlying concept of `` sparseness '' is always necessary when dealing with high - dimensional problems .",
    "however , in functional regression sparseness is usually not assumed with respect to the coefficients @xmath20 , but the model is rewritten using a `` sparse '' expansion of the predictor functions  @xmath37.=-1    the basic idea relies on the so - called karhunen  love decomposition which provides a decomposition of random functions in terms of functional principal components of the covariance operator of @xmath37 . in the discretized case analyzed in this paper this amounts to consider an approximation of @xmath37 by the principal components of the covariance matrix @xmath38 . in practice , often a small number @xmath39 of principal components will suffice to achieve a small @xmath40-error .",
    "an important points is now that even if @xmath41 the eigenvectors corresponding to the leading eigenvalues @xmath42 of @xmath43 can be well estimated by the eigenvectors ( estimated principal components ) @xmath44 of the empirical covariance matrix @xmath45 .",
    "this is due to the fact that if the predictors @xmath17 represent discretized values of a continuous functional variable , then for sufficiently small @xmath39 the eigenvalues @xmath46 will necessarily be of an order larger than @xmath47 and will thus exceed the magnitude of purely random components . from a more general point of view",
    "the underlying theory will be explained in detail in section [ sec4 ] .",
    "based on this insight , the most frequently used approach in functional regression is to approximate @xmath48 in terms of the first @xmath39 estimated principal components @xmath49 , and to rely on the approximate model @xmath50 . here",
    ", @xmath39 serves as smoothing parameter .",
    "the new coefficients @xmath51 are estimated by least squares , and @xmath52 .",
    "resulting rates of convergence are given in @xcite .",
    "the above arguments show that a suitable regression analysis will have to take into account the underlying structure of the explanatory variables @xmath17 .",
    "the basic motivation of this paper now is to combine the points of view of the above branches of literature in order to develop a new approach for model adjustment and variable selection in the practically important situation of strongly correlated regressors .",
    "more precisely , we will concentrate on _ factor models _ by assuming that the @xmath53 can be decomposed in the form @xmath54 where @xmath55 and @xmath56 are two uncorrelated random vectors in @xmath8 .",
    "the random vector @xmath55 is intended to describe high correlations of the @xmath17 while the components @xmath57 , @xmath6 , of @xmath56 are uncorrelated .",
    "this implies that the covariance matrix @xmath43 of @xmath10 adopts the decomposition @xmath58 where @xmath59 , while @xmath60 is a diagonal matrix with diagonal entries @xmath61 , @xmath6 .",
    "note that factor models can be found in any textbook on multivariate analysis and must be seen as one of the major tools in order to analyze samples of high - dimensional vectors . also recall that a standard factor model is additionally based on the assumption that a finite number @xmath39 of factors suffices to approximate @xmath55 precisely .",
    "this means that the matrix @xmath62 only possesses @xmath39 nonzero eigenvalues . in the following",
    "we will more generally assume that a small number of eigenvectors of @xmath62 suffices to approximate @xmath55 with high accuracy .",
    "we want to emphasize that the typical structural assumptions to be found in the literature on high - dimensional regression are special cases of ( [ predictor ] ) . if @xmath63 and thus @xmath64 , we are in the situation of uncorrelated regressors which has been widely studied in the context of model selection . on the other hand , @xmath65 and thus @xmath66 reflect the structural assumption of functional regression .",
    "in this paper we assume that @xmath67 as well as @xmath57 represent nonnegligible parts of the variance of @xmath17 .",
    "we believe that this approach may well describe the situation encountered in many relevant applications .",
    "although standard factor models are usually considered in the case @xmath68 , ( [ predictor ] ) for large values of @xmath12 may be of particular interest in time series or spatial analysis .",
    "indeed , factor models for large @xmath12 with a finite number @xmath39 of nonzero eigenvalues of @xmath62 play an important role in the econometric study of multiple time series and panel data .",
    "some references are @xcite , @xcite , @xcite , @xcite and bai ( @xcite , @xcite ) .",
    "our objective now is to study linear regression ( [ linear - model ] ) with respect to explanatory variables which adopt decomposition ( [ predictor ] ) .",
    "each single variable @xmath17 , @xmath6 , then possesses a _",
    "specific _ variability induced by @xmath57 and may thus explain some part of the outcome @xmath35 .",
    "one will , of course , assume that only few variables have a significant influence on @xmath35 which enforces the use of model selection procedures .",
    "on the other hand , the term @xmath67 represents a _",
    "common _ variability .",
    "corresponding principal components quantify a simultaneous variation of many individual regressors . as a consequence",
    ", such principal components may possess some additional power for predicting @xmath35 which may go beyond the effects of individual variables .",
    "a  rigorous discussion will be given in section [ sec3 ] .",
    "we want to note that the concept of `` latent variables , '' embracing the common influence of a large group of individual variables , plays a prominent role in applied , parametric multivariate analysis .",
    "these arguments motivate the main results of this paper .",
    "we propose to use an `` augmented '' regression model which includes principal components as additional explanatory variables .",
    "established model selection procedures like the dantzig selector or the lasso can then be applied to estimate the nonzero coefficients of the augmented model .",
    "we then derive theoretical results providing bounds for the accuracy of the resulting estimators .",
    "the paper is organized as follows : in section [ sec2 ] we formalize our setup .",
    "we show in section [ sec3 ] that the traditional sparseness assumption is restrictive and that a valid model may have to include principal components .",
    "the augmented model is thus introduced with an estimation procedure .",
    "section [ sec4 ] deals with the problem how accurately true principal components can be estimated from the sample @xmath69 .",
    "finite sample inequalities are given , and we show that it is possible to obtain sensible estimates of those components which explain a considerable percentage of the total variance of all @xmath17 , @xmath6 .",
    "section [ sec5 ] focuses on theoretical properties of the augmented model , while in section [ sec6 ] we present simulation results illustrating the finite sample performance of our estimators .",
    "we study regression of a response variable @xmath35 on a set of i.i.d .",
    "predictors @xmath53 , @xmath2 , which adopt decomposition ( [ predictor ] ) with @xmath70 , @xmath71 , @xmath72 , @xmath73 for all @xmath74 , @xmath75 . throughout the following sections we additionally assume that there exist constants @xmath76 and @xmath77 such that with @xmath78 the following assumption ( a.1 ) is satisfied for all  @xmath12 :    @xmath79 for all @xmath6 .",
    "recall that @xmath80 is the covariance matrix of @xmath10 with @xmath81 , where @xmath82 and @xmath83 is a diagonal matrix with diagonal entries @xmath84 , @xmath85 .",
    "we denote as @xmath86 the empirical covariance matrix based on the sample @xmath10 , @xmath2 .",
    "eigenvalues and eigenvectors of the standardized matrices @xmath87 and @xmath88 will play a central role .",
    "we will use @xmath89 and @xmath90 to denote the eigenvalues of @xmath91 and @xmath88 , respectively , while @xmath92 and @xmath93 denote corresponding orthonormal eigenvectors . note that all eigenvectors of @xmath88 and @xmath43 ( or @xmath91 and @xmath62 ) are identical , while eigenvalues differ by the factor @xmath36 .",
    "standardization is important to establish convergence results for large @xmath12 , since the largest eigenvalues of @xmath94 tend to infinity as @xmath28 .",
    "from a conceptional point of view we will concentrate on the case that @xmath12 is large compared to @xmath13 .",
    "another crucial , _ qualitative _ assumption characterizing our approach is the dimensionality reduction of @xmath95 using a small number @xmath96 of eigenvectors ( principal components ) of @xmath91 such that ( in a good approximation ) @xmath97 .",
    "we also assume that @xmath98 .",
    "_ then all leading principal components of @xmath99 corresponding to the @xmath39 largest eigenvalues explain a considerable percentage of the total variance of @xmath55 and @xmath10 . _    indeed , if @xmath100 , we necessarily have @xmath101 and @xmath102",
    ". then @xmath103 , and the first principal component of @xmath91 explains a considerable proportion @xmath104 of the total variance of @xmath55 .",
    "we want to emphasize that this situation is very different from the setup which is usually considered in the literature on the analysis of high - dimensional covariance matrices ; see , for example , @xcite .",
    "it is then assumed that the variables of interest are only weakly correlated and that the largest eigenvalue @xmath105 of the corresponding scaled covariance matrix @xmath88 is of order @xmath106 .",
    "this means that for large @xmath12 the first principal component only explains a negligible percentage of the total variance of @xmath10 , @xmath107 .",
    "it is well known that in this case no consistent estimates of eigenvalues and principal components can be obtained from an eigen - decomposition of @xmath108 .",
    "however , we will show in section [ sec4 ] that principal components which are able to explain a considerable proportion of total variance can be estimated consistently .",
    "these components will be an intrinsic part of the augmented model presented in section [ sec3 ] .",
    "we will need a further assumption which ensures that all covariances between the different variables are well approximated by their empirical counterparts :    there exists a @xmath109 such that @xmath110    hold simultaneously with probability @xmath111 , where @xmath112 as @xmath113 , @xmath114 .",
    "the following proposition provides a general sufficient condition on random vectors for which ( a.2 ) is satisfied provided that the rate of convergence of @xmath115 to 0 is sufficiently fast .",
    "[ propcov ] consider independent and identically distributed random vectors @xmath116 , @xmath2 , such that for @xmath6 , @xmath117 and @xmath118 for positive constants @xmath119 and @xmath120 with moreover @xmath121 .",
    "then , for any positive constant @xmath122 such that @xmath123 and @xmath124 @xmath125\\\\[-8pt ] & & \\qquad\\ge 1-p^{2-c^2_0/(8(c_1+c_0^{3/2}/3))}+2p^2nc_1e^{-({a}/{\\sqrt { 2}})(c_0n/\\log p)^{1/4}}.\\nonumber\\end{aligned}\\ ] ]    note that as @xmath113 the right - hand side of ( [ propcovresult ] ) converges to 1 provided that @xmath122 is chosen sufficiently large and that @xmath126 for some @xmath127 .",
    "therefore , assumption ( a.2 ) is satisfied if the components of the random variables @xmath17 possess some exponential moments . for the specific case of",
    "centered normally distributed random variables , a more precise bound in ( [ propcovresult ] ) may be obtained using lemma 2.5 in @xcite and large deviations inequalities obtained by @xcite . in this case",
    "it may also be shown that for sufficiently large @xmath122 events ( [ b21])([b24 ] ) hold with probability tending to 1 as @xmath28 without any restriction on the quotient @xmath128 .",
    "of course , the rate @xmath129 in ( [ b21])([b24 ] ) depends on the tails of the distributions : it would be possible to replace this rate with a slower one in case of heavier tails than in proposition [ propcov ] .",
    "our theoretical results could be modified accordingly .",
    "let us now consider the structural model ( [ predictor ] ) more closely .",
    "it implies that the vector @xmath10 of predictors can be decomposed into two uncorrelated random vectors @xmath55 and @xmath56 .",
    "each of these two components separately may possess a significant influence on the response variable @xmath35 . indeed ,",
    "if @xmath55 and @xmath56 were known , a possibly substantial improvement of model ( [ linear - model ] ) would consist in a regression of @xmath35 on the @xmath130 variables @xmath55 _ and _  @xmath56 @xmath131 with different sets of parameters @xmath132 and @xmath20 , @xmath85 , for each contributor .",
    "we here again assume that @xmath133 , @xmath2 , are centered i.i.d .",
    "random variables with @xmath11 which are independent of @xmath67 and @xmath57 .    by definition , @xmath67 and @xmath57 possess substantially different interpretations .",
    "@xmath57 describes the part of @xmath17 which is _ uncorrelated with all other variables_. a nonzero coefficient @xmath134 then means that the variation of @xmath17 has a _ specific _ effect on @xmath35 .",
    "we will of course assume that such nonzero coefficients are _ sparse _ , @xmath135 for some @xmath136 .",
    "the true variables @xmath57 are unknown , but with @xmath137 model ( [ eq-30 ] ) can obviously be rewritten in the form @xmath138    the variables @xmath67 are heavily correlated .",
    "it therefore does not make any sense to assume that for some @xmath139 any particular variable @xmath67 possesses a specific influence on the predictor variable .",
    "however , the term @xmath140 may represent an important , _ common _ effect of all predictor variables .",
    "the vectors @xmath55 can obviously be rewritten in terms of principal components .",
    "let us recall that @xmath89 denote the eigenvalues of the standardized covariance matrix of @xmath55 , @xmath141 and @xmath142 corresponding orthonormal eigenvectors .",
    "we have @xmath143 where @xmath144 . as outlined in the previous sections we now assume that the use of principal components allows for a considerable reduction of dimensionality , and that a small number of leading principal components will suffice to describe the effects of the variable @xmath55 .",
    "this may be seen as an analogue of the sparseness assumption made for the @xmath57 .",
    "more precisely , subsequent analysis will be based on the assumption that the following _ augmented model",
    "_ holds for some suitable @xmath145 : @xmath146 where @xmath147 and @xmath148 .",
    "the use of @xmath149 instead of @xmath150 is motivated by the fact that @xmath151 , @xmath152 . therefore the @xmath149 are standardized variables with @xmath153 . fitting an augmented model requires us to select an appropriate @xmath39 as well as to determine sensible estimates of @xmath154 .",
    "furthermore , model selection procedures like lasso or dantzig have to be applied in order to retrieve the nonzero coefficients @xmath155 , @xmath152 , and @xmath156 , @xmath6 .",
    "these issues will be addressed in subsequent sections .",
    "obviously , the augmented model may be considered as a synthesis of the standard type of models proposed in the literature on functional regression and model selection .",
    "it generalizes the classical multivariate linear regression model ( [ linear - model ] ) .",
    "if a @xmath39-factor model holds exactly , that is , @xmath157 , then the only substantial restriction of ( [ eq-30])([augmented - model ] ) consists in the assumption that @xmath35 depends _ linearly _ on @xmath158 and @xmath159 .",
    "we want to emphasize , however , that our analysis does not require the validity of a @xmath39-factor model .",
    "it is only assumed that there exists `` some '' @xmath159 and @xmath158 satisfying our assumptions which lead to ( [ augmented - model ] ) for a sparse set of coefficients @xmath20 .",
    "let @xmath160 and @xmath161 .",
    "since @xmath162 , @xmath152 , are eigenvectors of @xmath62 we have @xmath163 for all @xmath164 , @xmath165 . by assumption the random vectors @xmath95 and @xmath56 are uncorrelated , and hence @xmath166 for all @xmath167 . furthermore , @xmath168 for all @xmath169 .",
    "if the augmented model ( [ augmented - model ] ) holds , some straightforward computations then show that under ( a.1 ) for any alternative set of coefficients @xmath170 , @xmath171 , @xmath172 ^ 2\\biggr ) \\nonumber\\\\ & & \\qquad\\geq\\sum_{r=1}^k\\bigl(\\alpha_r-\\alpha_r^*+\\sqrt{p\\lambda_r } \\bolds{\\psi}_r(\\bolds{\\beta}-\\bolds{\\beta}^*)\\bigr)^2\\\\ & & \\qquad\\quad{}+d_1\\| \\bolds{\\beta}-\\bolds{\\beta}^*\\|^2_2 .",
    "\\nonumber\\end{aligned}\\ ] ] we can conclude that the coefficients @xmath155 , @xmath152 and @xmath20 , @xmath6 , in ( [ augmented - model ] ) are uniquely determined .    of course , an inherent difficulty of ( [ augmented - model ] ) consists of the fact that it contains the unobserved , `` latent '' variables @xmath147 .",
    "to study this problem , first recall that our setup imposes the decomposition ( [ var - decomp ] ) of the covariance matrix @xmath43 of @xmath10 . if a factor model with @xmath39 factors holds exactly , then the @xmath62 possesses rank @xmath39 .",
    "it then follows from well - established results in multivariate analysis that if @xmath173 the matrices @xmath62 and @xmath174 are uniquely identified .",
    "if @xmath175 , then also @xmath176 are uniquely determined ( up to sign ) from the structure of @xmath177 .    however , for large @xmath12 , identification is possible under even more general conditions .",
    "it is not necessary that a @xmath39-factor model holds exactly .",
    "we only need an additional assumption on the magnitude of the eigenvalues of @xmath91 defining the @xmath39 principal components of @xmath55 to be considered .",
    "the eigenvalues of @xmath91 are such that @xmath178 for some @xmath179 with @xmath180 .",
    "in the following we will qualitatively assume that @xmath96 as well as @xmath181 .",
    "more specific assumptions will be made in the sequel .",
    "note that eigenvectors are only unique up to sign changes . in the following",
    "we will always assume that the right `` versions '' are used .",
    "this will go without saying .",
    "[ thmiden1 ] let @xmath182 and @xmath183 . under assumptions",
    "and we have for all @xmath184 , @xmath6 and all @xmath185 satisfying : @xmath186\\\\[-8pt ] \\delta_{rj}^2&\\leq&\\frac{d_0}{pv(k)},\\nonumber \\\\ \\label{thmiden-3 } \\mathbb{e}([\\xi_{ir}- \\xi_{ir}^*]^2 ) & \\leq&\\frac{d_2}{p\\mu_r}+\\frac{(8\\lambda_1 + 1)d_2 ^ 2}{p^2v(k)^2\\mu_r},\\nonumber\\\\[-8pt]\\\\[-8pt ] \\mathbb{e}\\biggl(\\biggl[\\xi_{ir}- \\frac{\\bolds{\\psi}^t_r\\mathbf{x}_i}{\\sqrt{p\\lambda_r}}\\biggr]^2\\biggr ) & \\leq&\\frac{d_2}{p\\lambda_r},\\nonumber \\\\ \\label{thmiden-4 } \\mathbb{e}\\biggl(\\biggl[\\sum_{r=1}^k\\alpha_r\\xi_{ir } - \\sum_{r=1}^k\\alpha_r\\xi_{ir}^ * \\biggr]^2\\biggr ) & \\leq & k\\sum_{r=1}^k\\alpha_r^2\\biggl(\\frac{d_2}{p\\mu_r}+ \\frac{(8\\lambda_1 + 1)d_2 ^ 2}{p^2v(k)^2\\mu_r}\\biggr).\\end{aligned}\\ ] ]    for small @xmath12 , standard factor analysis uses special algorithms in order to identify  @xmath162 .",
    "the theorem tells us that for large @xmath12 this is unnecessary since then the eigenvectors @xmath187 of @xmath188 provide a good approximation .",
    "the predictor @xmath189 of @xmath149 possesses an error of order @xmath190 .",
    "the error decreases as @xmath12 increases , and @xmath189 thus yields a good approximation of @xmath149 _ if @xmath12 is large_. indeed , if @xmath28 [ for fixed @xmath191 , @xmath192 , @xmath193 and @xmath194 then by ( [ thmiden-3 ] ) we have @xmath195 ^ 2)\\rightarrow0 $ ] .",
    "furthermore , by ( [ thmiden-4 ] ) the error in predicting @xmath196 by @xmath197 converges to zero as @xmath28 .",
    "a crucial prerequisite for a reasonable analysis of the model is sparseness of the coefficients @xmath20 .",
    "note that if @xmath12 is large compared to @xmath13 , then by ( [ thmiden-4 ] ) the error in replacing @xmath149 by @xmath189 is negligible compared to the estimation error induced by the existence of the error terms @xmath133 .",
    "if @xmath96 and @xmath198 , then the true coefficients @xmath199 and @xmath20 provide a _ sparse _ solution of the regression problem .",
    "established theoretical results [ see @xcite ] show that under some regularity conditions ( validity of the `` restricted eigenvalue conditions '' ) model selection procedures _ allow to identify such sparse solutions _ even if there are multiple vectors of coefficients satisfying the normal equations .",
    "the latter is of course always the case if @xmath41 .",
    "indeed , we will show in the following sections that factors can be consistently estimated from the data , and that a suitable application of lasso or the dantzig - selector leads to consistent estimators @xmath200 , @xmath201 satisfying @xmath202 , @xmath203 , as @xmath113 .",
    "when replacing @xmath149 by @xmath189 , there are alternative sets of coefficients leading to the same prediction error as in ( [ thmiden-4 ] ) .",
    "this is due to the fact that @xmath204 .",
    "however , all these alternative solutions are _ nonsparse _ and can not be identified by lasso or other procedures . in particular , it is easily seen that @xmath205\\\\[-8pt ] & & \\eqntext{\\displaystyle \\mbox{with } \\beta^{\\mathit{lr}}_j:=\\beta_j+ \\sum_{r=1}^k\\alpha_r\\frac{\\delta_{rj}}{\\sqrt{p\\mu_r}}.}\\end{aligned}\\ ] ]    by ( [ thmiden-2 ] ) all values @xmath206 are of order @xmath207 . since @xmath208",
    ", this implies that many @xmath206 are nonzero .",
    "therefore , if @xmath209 for some @xmath210 , then @xmath211 contains a large number of small , nonzero coefficients and is not at all sparse .",
    "if @xmath12 is large compared to @xmath13 no known estimation procedure will be able to provide consistent estimates of these coefficients .    summarizing the above discussion we can conclude :",
    "if the variables @xmath17 are heavily correlated and follow an approximate factor model , then one may reasonably expect substantial effects of the common , joint variation of all variables and , consequently , nonzero coefficients @xmath132 and @xmath155 in ( [ eq-30 ] ) and ( [ augmented - model ] ) .",
    "but then a `` bet on sparsity '' is unjustifiable when dealing with the standard regression model ( [ linear - model ] ) .",
    "it follows from ( [ stanmodident ] ) that for large @xmath12 model ( [ linear - model ] ) holds approximately for a nonsparse set of coefficients @xmath212 , since many small , nonzero coefficients are necessary in order to capture the effects of the common joint variation .",
    "the augmented model offers a remedy to this problem by pooling possible effects of the joint variation using a small number of additional variables . together with the familiar assumption of a small number of variables possessing a specific influence , this leads to a sparse model with at most @xmath213 nonzero coefficients which can be recovered from model selection procedures like lasso or the dantzig - selector .    in practice ,",
    "even if ( [ augmented - model ] ) only holds approximately , since a too - small value of @xmath39 has been selected , it may be able to quantify at least some important part of the effects discussed above .",
    "compared to an analysis based on a standard model  ( [ linear - model ] ) , this may lead to a substantial improvement of model fit as well as to more reliable interpretations of significant variables .      for a pre - specified @xmath145",
    "we now define a procedure for estimating the components of the corresponding augmented model ( [ augmented - model ] ) from given data .",
    "this obviously specifies suitable procedures for approximating the unknown values @xmath149 as well as to apply subsequent model selection procedures in order to retrieve nonzero coefficients @xmath155 and @xmath20 , @xmath152 , @xmath6 .",
    "a discussion of the choice of @xmath39 can be found in the next section .",
    "recall from theorem [ thmiden1 ] that for large @xmath12 the eigenvectors @xmath214 of @xmath87 are well approximated by the eigenvectors of the standardized covariance matrix @xmath215 .",
    "this motivates us to use the empirical principal components of @xmath216 in order to determine estimates of @xmath162 and @xmath149 .",
    "theoretical support will be given in the next section .",
    "define @xmath217 as the eigenvalues of the standardized empirical covariance matrix @xmath218 , while @xmath219 are associated orthonormal eigenvectors .",
    "we then estimate @xmath149 by @xmath220    when replacing @xmath149 by @xmath221 in ( [ augmented - model ] ) , a direct application of model selection procedures does not seem to be adequate , since @xmath221 and the predictor variables @xmath17 are heavily correlated .",
    "we therefore rely on a projected model .",
    "consider the projection matrix on the orthogonal space of the space spanned by the eigenvectors corresponding to the @xmath39 largest eigenvalues of @xmath108 @xmath222 then model ( [ augmented - model ] ) can be rewritten for @xmath2 , @xmath223 where @xmath224 , @xmath225 @xmath226 and @xmath227 .",
    "it will be shown in the next section that for large @xmath13 and @xmath12 the additional error term @xmath228 can be assumed to be reasonably small .    in the following",
    "we will use @xmath229 to denote the vectors with entries @xmath230 .",
    "furthermore , consider the @xmath231-dimensional vector of predictors @xmath232 , @xmath233 .",
    "the gram matrix in model ( [ model - projected ] ) is a block matrix defined as @xmath234 where @xmath235 is the identity matrix of size @xmath39 .",
    "note that the normalization of the predictors in ( [ model - projected ] ) implies that the diagonal elements of the gram matrix above are equal to 1 .",
    "arguing now that the vector of parameters @xmath236 in model ( [ model - projected ] ) is @xmath237-sparse , we may use a selection procedure to recover / estimate the nonnull parameters . in the following",
    "we will concentrate on the lasso estimator introduced in @xcite . for a pre - specified parameter @xmath238 , an estimator @xmath239",
    "is then obtained as @xmath240 @xmath241 being the @xmath242-dimensional matrix with rows @xmath243 .",
    "we can alternatively use the dantzig selector introduced in @xcite .    finally , from @xmath244",
    ", we define corresponding estimators for @xmath155 , @xmath152 , and @xmath20 , @xmath6 , in the unprojected model ( [ augmented - model ] ) . @xmath245 and @xmath246",
    "the following theorem shows that principal components which are able to explain a considerable proportion of total variance can be estimated consistently .    for simplicity",
    ", we will concentrate on the case that @xmath13 as well as @xmath247 are large enough such that    @xmath248 and @xmath249 .",
    "[ thmapp1 ] under assumptions and under events ( [ b21])([b24 ] ) we have for all @xmath184 and all @xmath6 , @xmath250\\\\[-8pt ] & \\leq&\\frac{6}{5 } \\frac{d_0+c_0(\\log p / n)^{1/2}}{pv(k)}.\\nonumber\\end{aligned}\\ ] ]    theorem [ thmapp1 ] shows that for sufficiently large @xmath12 ( @xmath251 ) the eigenvalues and eigenvectors of @xmath252 provide reasonable estimates of @xmath253 and @xmath162 for @xmath152 .",
    "quite obviously it is not possible to determine sensible estimates of _ all _ @xmath12 principal components of @xmath88 .",
    "following the proposition it is required that @xmath253 as well as @xmath191 be of order at least @xmath129 .",
    "any smaller component can not be distinguished from pure `` noise '' components . up to the @xmath254-term this corresponds to the results of @xcite who study the problem of the number of principal components that can be consistently estimated in a functional principal component analysis .",
    "the above insights are helpful for selecting an appropriate @xmath39 in a real data application . in tendency",
    ", a suitable factor model will incorporate @xmath39 components which explain a large percentage of the total variance of @xmath10 , while @xmath255 is very small . if for a sample of high - dimensional vectors @xmath10 a principal component analysis leads to the conclusion that the first ( or second , third@xmath256 ) principal components",
    "explains a _ large _ percentage of the total ( empirical ) variance of the observations , then such a component can not be generated by `` noise '' but reflects an underlying structure .",
    "in particular , such a component may play a crucial role in modeling a response variable @xmath35 according to an augmented regression model of the form ( [ augmented - model ] ) .",
    "@xcite develop criteria of selecting the dimension @xmath39 in a high - dimensional factor model .",
    "they rely on an adaptation of the well - known aic and bic procedures in model selection .",
    "one possible approach is as follows : select a maximal possible dimension @xmath257 and estimate @xmath258 by @xmath259 . then determine an estimate @xmath260 by minimizing @xmath261 over @xmath262 .",
    "@xcite show that under some regularity conditions this criterium ( as well as a number of alternative versions ) provides asymptotically consistent estimates of the true factor dimension @xmath39 as @xmath113 . in our context",
    "these regularity conditions are satisfied if ( a.1)(a.4 ) hold for all @xmath13 and @xmath12 , @xmath263 and if there exists some @xmath264 such that @xmath265 , for all @xmath266 .",
    "now recall the modified version ( [ model - projected ] ) of the augmented model used in our estimation procedure .",
    "the following theorem establishes bounds for the projections @xmath267 as well as for the additional error terms @xmath268 .",
    "let @xmath183 denote the population version of @xmath269 .",
    "[ thmapp2 ] assume and .",
    "there then exist constants @xmath270 , @xmath271 , @xmath272 , such that for all @xmath273 satisfying and , all @xmath274 , @xmath31 , @xmath275 hold with probability @xmath276 , while @xmath277\\\\[-8pt ] & \\leq&\\frac{k\\alpha^2_{\\mathrm{sum}}m_3}{v(k)^3}\\biggl(\\frac{\\log p}{n}+ \\frac{v(k)^2}{p}\\biggr)\\nonumber\\end{aligned}\\ ] ] holds with probability at least @xmath278 . here , @xmath279 .",
    "note that if @xmath10 satisfies a @xmath39-dimensional factor model , that is , if the rank of @xmath91 is equal to @xmath39 , then @xmath280 .",
    "the theorem then states that for large @xmath13 and @xmath12 the projected variables @xmath267 , @xmath281 , `` in average '' behave similarly to the specific variables @xmath57 .",
    "variances will be close to @xmath282 .",
    "we come back to model  ( [ augmented - model ] ) .",
    "as shown in section [ sec32 ] , the lasso or the dantzig selector may be used to determine estimators of the parameters of the model .",
    "identification of sparse solutions as well as consistency of estimators require structural assumptions on the explanatory variables .",
    "the weakest assumption on the correlations between different variables seems to be the so - called _ restricted eigenvalue _",
    "condition introduced by @xcite ; see also @xcite .",
    "we first provide a theoretical result which shows that for large @xmath266 the design matrix of the projected model ( [ model - projected ] ) satisfies the restricted eigenvalue conditions given in @xcite with high probability .",
    "we will additionally assume that @xmath266 are large enough such that    @xmath283 ,    where @xmath270 is defined as in theorem [ thmapp2 ] .",
    "let @xmath284 denote an arbitrary subset of indices , @xmath285 with @xmath286 .",
    "for a vector @xmath287 , let @xmath288 be the vector in @xmath289 which has the same coordinates as @xmath290 on @xmath284 and zero coordinates on the complement @xmath291 of @xmath284 .",
    "we define in the same way @xmath292 .",
    "now for @xmath293 and for an integer @xmath294 , @xmath295 , denote by @xmath296 the subset of @xmath297 corresponding to @xmath33 largest in absolute value coordinates of @xmath298 outside of @xmath284 , and define @xmath299 . furthermore , let @xmath300 .",
    "[ propsparse2 ] assume and .",
    "there then exists a constant @xmath301 , such that for all @xmath302 , @xmath293 , satisfying , and @xmath303 @xmath304 & & \\quad:=\\min_{j_0\\subset\\{1,\\ldots , k+p\\}\\dvtx|j_0|\\leq k+s } \\min_{\\bolds{\\delta}\\neq0\\dvtx\\|\\bolds{\\delta}_{j_0^c}\\|_1\\leq c_0\\| \\bolds{\\delta}_{j_0}\\|_1 } \\frac{[\\bolds{\\delta}^t({1}/{n})\\sum_{i=1}^n \\bolds{\\phi } _ i\\bolds{\\phi}_i^{t } \\bolds{\\delta}]^{1/2}}{\\|\\bolds{\\delta } _ { j_{0,k+s}}\\|_2}\\hspace*{-12pt } \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\quad\\geq\\biggl(\\frac{d_1}{d_0+c_0n^{-1/2}\\sqrt{\\log p } } -\\frac{8(k+s)c_0m_4k^2n^{-1/2}\\sqrt{\\log p}}{v(k)(d_1-m_1kv(k)^{1/2}n^{-1/2}\\sqrt{\\log p})}\\biggr)_+^{{1}/{2 } } \\nonumber\\\\[-2pt ] & & \\quad = : \\mathbf{k}_{n , p}(k , s , c_0)\\nonumber\\end{aligned}\\ ] ] holds with probability @xmath276 .",
    "asymptotically , if @xmath13 and @xmath12 are large , then @xmath305 , @xmath303 , provided that @xmath39 , @xmath22 and @xmath306 are sufficiently small compared to @xmath266 . in this case the proposition implies that with high probability the restricted eigenvalue condition @xmath307 of @xcite [ i.e. , @xmath308 is satisfied .",
    "the same holds for the conditions @xmath309 which require @xmath310 , where @xmath311 & & \\qquad:=\\min_{j_0\\subset\\{1,\\ldots , k+p\\}\\dvtx|j_0|\\leq k+s } \\min_{\\bolds{\\delta}\\neq0\\dvtx\\|\\bolds{\\delta}_{j_0^c}\\|_1\\leq c_0\\| \\bolds{\\delta}_{j_0}\\|_1 } \\frac{[\\bolds{\\delta}^t({1/n})\\sum_{i=1}^n \\bolds{\\phi } _ i\\bolds{\\phi}_i^{t } \\bolds{\\delta}]^{1/2}}{\\|\\bolds{\\delta } _",
    "{ j_{0}}\\|_2}\\\\[-3pt ] & & \\qquad\\geq\\kappa(k+s , k+s , c_0).\\end{aligned}\\ ] ]    the following theorem now provides bounds for the @xmath312 estimation error and the @xmath40 prediction loss for the lasso estimator of the coefficients of the augmented model .",
    "it generalizes the results of theorem 7.2 of @xcite obtained under the standard linear regression model . in our analysis",
    "merely the values of @xmath313 for @xmath314 are of interest .",
    "however , only slight adaptations of the proofs are necessary in order to derive generalizations of the bounds provided by @xcite for the dantzig selector ( @xmath315 ) and for the @xmath316 loss , @xmath317 . in the latter case",
    ", @xmath313 has to be replaced by @xmath318 . in the following ,",
    "let @xmath270 and @xmath319 be defined as in theorem  [ thmapp2 ] .",
    "[ thmaug ] assume , and suppose that the error terms @xmath133 in model ( [ augmented - model ] ) are independent @xmath320 random variables with @xmath321 .",
    "now consider the lasso estimator @xmath244 defined by ( [ dantzig2 ] ) with @xmath322 where @xmath323 , @xmath324 is a positive constant and @xmath325 .    if @xmath326 is sufficiently large , then for all @xmath273 , @xmath293 , satisfying as well as @xmath327 , the following inequalities hold with probability at least @xmath328 : @xmath329\\\\[-8pt ] & & { } \\times\\rho\\biggl(1+\\frac{k(d_0+c_0n^{-1/2}\\sqrt{\\log p})^{1/2}}{(d_1-m_1({kn^{-1/2}\\sqrt{\\log p}}/{v(k)^{1/2}}))^{1/2}}\\biggr),\\nonumber\\\\ \\label{thmaug-2 } \\sum_{j=1}^p|\\widehat{\\beta}_j-\\beta_j| & \\le & \\frac{16(k+s)}{\\kappa ^2(d_1-m_1({kn^{-1/2}\\sqrt{\\log p}}/{v(k)^{1/2}}))^{1/2}}\\rho,\\end{aligned}\\ ] ] where @xmath330 .",
    "moreover , @xmath331\\\\[-9pt ] & & \\qquad\\le\\frac{32(k+s ) } { \\kappa^2}\\rho^2+\\frac{2k\\alpha^2_{\\mathrm{sum}}m_3}{v(k)^3}\\biggl(\\frac{\\log p}{n}+ \\frac{v(k)^2}{p}\\biggr)\\nonumber\\end{aligned}\\ ] ] holds with probability at least @xmath332 .",
    "of course , the main message of the theorem is asymptotic in nature . if @xmath266 tend to infinity for fixed values of @xmath39 and @xmath22 , then the @xmath333 estimation error and the @xmath40 prediction error converge at rates @xmath334 and @xmath335 , respectively . for values of @xmath39 and @xmath22 tending to infinity as the sample size tends to infinity ,",
    "the rates are more complicated .",
    "in particular , they depend on how fast @xmath192 converges to zero as @xmath336 .",
    "similar results hold for the estimators based on the dantzig selector .",
    "note that proposition [ propsparse2 ] as well as the results of theorem [ thmaug ] heavily depend on the validity of assumption ( a.1 ) and the corresponding value @xmath337 , where @xmath338 .",
    "it is immediately seen that the smaller the @xmath193 the smaller the value of @xmath318 in ( [ sparse02 ] ) .",
    "this means that _ all _ variables @xmath339 , @xmath6 have to possess a sufficiently large specific variation which is not shared by other variables . for large @xmath12",
    "this may be seen as a restrictive assumption .",
    "in such a situation one may consider a restricted version of model ( [ augmented - model ] ) , where variables with extremely small values of @xmath340 are eliminated .",
    "but for large @xmath266 we can infer from theorem [ thmapp2 ] that a small value of @xmath341 indicates that also @xmath340 is small .",
    "hence , an extension of our method consists of introducing some threshold @xmath342 and discarding all those variables @xmath17 , @xmath139 , with @xmath343 .",
    "a precise analysis is not in the scope of the present paper .    if @xmath344 the augmented model reduces to the standard linear regression model ( [ linear - model ] ) with a sparse set of coefficients , @xmath345 for some @xmath346 .",
    "an application of our estimation procedure is then unnecessary , and coefficients may be estimated by traditional model selection procedures .",
    "bounds on estimation errors can therefore be directly obtained from the results of @xcite , provided that the restricted eigenvalue conditions are satisfied .",
    "but in this situation a slight adaptation of the proof of proposition [ propsparse2 ] allows us to establish a result similar to ( [ sparse02 ] ) for the standardized variables @xmath347 .",
    "define @xmath348 as the @xmath349-matrix with generic elements @xmath350 .",
    "when assuming ( a.1 ) , ( a.2 ) as well as @xmath351 , then for @xmath352 the following inequality holds with probability @xmath276 : @xmath353^{1/2}}{\\|\\bolds{\\delta}_{j_{0,s}}\\|_2 } \\\\ & & \\qquad\\geq\\biggl(\\frac{d_1}{d_0+c_0n^{-1/2}\\sqrt{\\log p}}-\\frac { 8sc_0c_0 n^{-1/2}\\sqrt{\\log p}}{d_1 - 3c_0n^{-1/2}\\sqrt{\\log p}}\\biggr ) _",
    "+ ^{1/2 } , \\nonumber\\end{aligned}\\ ] ] where @xmath303 .",
    "recall , however , from the discussion in section [ sec31 ] that @xmath344 is a restrictive condition in the context of highly correlated regressors .",
    "in this section we study the finite sample performance of the estimators discussed in the proceeding sections .",
    "we consider a factor model with @xmath354 factors .",
    "the first factor is @xmath355 , @xmath6 , while the second factor is given by @xmath356 , @xmath357 , and @xmath358 , @xmath359 . for different values of @xmath360 and @xmath361 observations",
    "@xmath362 with @xmath363 , @xmath6 , are generated according to the model @xmath364 where @xmath365 , @xmath366 , @xmath367 , and @xmath368 are independent variables .",
    "our study is based on @xmath369 nonzero @xmath370-coefficients whose values are @xmath371 , @xmath372 , @xmath373 and @xmath374 , while the error variance is set to @xmath375 .",
    "the parameters of the augmented model with @xmath354 are estimated by using the lasso - based estimation procedure described in section [ sec32 ] .",
    "the behavior of the estimates is compared to the lasso estimates of the coefficients of a standard regression model ( [ linear - model ] ) .",
    "all results reported in this section are obtained by applying the lars - package by hastie and efron implemented in r. all tables are based on 1,000 repetitions of the simulation experiments .",
    "the corresponding r - code can be obtained from the authors upon request .",
    "figure [ figur1 ] and table [ tabla1 ] refer to the situation with @xmath376 , @xmath377 , @xmath378 and @xmath379 .",
    "we then have @xmath363 , @xmath6 , while the first and second factor explain 40% and 20% of the total variance of @xmath17 , respectively .    ;",
    "black  estimates of nonzero @xmath20 ; red  estimates of coefficients with @xmath380 ; blue@xmath381 . ]",
    "@lcccd2.2ccd2.2d2.2@ & & + & & + @xmath382 & & & & & & & & +   + 50 & 50 & 0.3334 & 0.8389 & 4.53 & 0.0498 & 0.1004 & 1.76 & 1.55 + 100 & 100 & 0.2500 & 0.5774 & 6.84 & 0.0328 & 0.0480 & 3.50 & 3.29 + 250 & 250 & 0.1602 & 0.3752 & 12.27 & 0.0167 & 0.0199 & 7.55 & 7.22 + 500 & 500 & 0.1150 & 0.2752 & 18.99 & 0.0096 & 0.0106 & 12.66 & 12.21 + 5,000 & 100 & 0.0378 & 0.0733 & 48.33 & 0.0152 & 0.0154 & 27.48 & 26.74 + 100 & 2,000 & 0.2741 & 0.8664 & 10.58 & 0.0420 & 0.0651 & 5.42 & 5.24 + @xmath382 & @xmath383 & & & & & & & +   + 50 & 50 & 2.2597 & 1.8403 & & 0.0521 & 0.1370 & 0.92 & + 100 & 100 & 2.2898 & 1.9090 & & 0.0415 & 0.0725 & 1.90 & + 250 & 250 & 2.3653 & 1.7661 & & 0.0257 & 0.0345 & 4.12 & + 500 & 500 & 2.4716 & 1.7104 & & 0.0174 & 0.0207 & 6.87 & + 5,000 & 100 & 0.5376 & 1.5492 & & 0.0161 & 0.0168 & 10.14 & + 100 & 2,000 & 3.7571 & 2.2954 & & 0.0523 & 0.1038 & 3.17 & +    figure [ figur1 ] shows estimation results of one typical simulation with @xmath384 .",
    "the left panel contains the parameter estimates for the augmented model .",
    "the paths of estimated coefficients @xmath201 for the 4 significant variables ( black lines ) , the 96 variables with @xmath380 ( red lines ) , as well as of the untransformed estimates @xmath381 ( blue lines ) of @xmath155 , @xmath366 , are plotted as a function of @xmath385 .",
    "the four significant coefficients as well as @xmath386 and @xmath387 can immediately been identified in the figure .",
    "the right panel shows a corresponding plot of estimated coefficients when lasso is directly applied to the standard regression model ( [ linear - model ] ) .",
    "as has to be expected by ( [ stanmodident ] ) the necessity of compensating the effects of @xmath388 by a large number of small , nonzero coefficients generates a general `` noise level '' which makes it difficult to identify the four significant variables in ( [ simul-2 ] ) .",
    "the penalties @xmath385 in the figure as well as in subsequent tables have to be interpreted in terms of the scaling used by the lars - algorithm and have to be multiplied with @xmath389 in order to correspond to the standardization used in the proceeding sections .",
    "the upper part of table [ tabla1 ] provides simulation results with respect to the augmented model for different sample sizes @xmath13 and @xmath12 . in order to access the quality of parameter estimates",
    "we evaluate @xmath390 as well as latexmath:[$\\sum_{j=1}^p    where the minimum of latexmath:[$\\sum_{r=1}^2 |\\widehat\\alpha_r-\\alpha_r|+\\sum_{j=1}^p    of @xmath385 where the minimal sample prediction error @xmath392 ia attained . for the same value @xmath385 we also determine the exact prediction error @xmath393 for a new observation @xmath394 independent of @xmath395 .",
    "the columns of table [ tabla1 ] report the average values of the corresponding quantities over the 1,000 replications .",
    "to get some insight into a practical choice of the penalty , the last column additionally yields the average value of the parameters @xmath385 minimizing the @xmath396-statistics .",
    "@xmath396  is computed by using the r - routine `` summary.lars '' and plugging in the true error variance @xmath397 .",
    "we see that in all situations the average value of @xmath385 minimizing @xmath396 is very close to the average @xmath385 providing the smallest prediction error .",
    "the penalties for optimal parameter estimation are , of course , larger .",
    "it is immediately seen that the quality of estimates considerably increases when going from @xmath398 to @xmath399 .",
    "an interesting result consists of the fact that the prediction error is smaller for @xmath399 than for @xmath400 , @xmath401 .",
    "this may be interpreted as a consequence of ( [ thmiden-4 ] ) .",
    "the lower part of table [ tabla1 ] provides corresponding simulation results with respect to lasso estimates based on the standard regression model .",
    "in addition to the minimal error latexmath:[$\\sum_{r=1}^p    @xmath20 of ( [ simul-2 ] ) we present the minimal @xmath333-distance @xmath403 is the ( nonsparse ) set of parameters minimizing the population prediction error .",
    "sample and exact prediction errors are obtained by straightforward modifications of ( [ simul-3 ] ) and ( [ simul-4 ] ) .",
    "quite obviously , no reasonable parameter estimates are obtained in the cases with @xmath404 . only for @xmath400 , @xmath401",
    ", the table indicates a comparably small error latexmath:[$\\sum_{r=1}^p    somewhat better behavior .",
    "it is , however , always larger than the prediction error of the augmented model .",
    "the relative difference increases with @xmath12 .",
    "it was mentioned in section [ sec4 ] that a suitable criterion to estimate the dimension @xmath39 of an approximate factor model consists in minimizing ( [ baicrit ] ) .",
    "this criterion proved to work well in our simulation study . recall that the true factor dimension is @xmath354 .",
    "for @xmath398 the average value of the estimate @xmath260 determined by ( [ baicrit ] ) is 2.64 . in all other situations reported in table [ tabla1 ] an estimate @xmath406",
    "is obtained in each of the 1,000 replications .",
    "@ld1.2cd2.1ccd2.2ccc@ & & & & & + & & & & & + & & & & & & & & & +   + 0.06 & 0.03 & 1 & -0.5 & 0.3191 & 0.6104 & 10.37 & 0.0670 & 0.2259 & 2.46 + 0.2 & 0.1 & 1 & -0.5 & 0.2529 & 0.5335 & 8.19 & 0.0414 & 0.0727 & 3.92 + 0.4 & 0.2 & 1 & -0.5 & 0.2500 & 0.6498 & 7.86 & 0.0319 & 0.0454 & 4.35 + 0.6 & 0.3 & 1 & -0.5 & 0.2866 & 1.1683 & 8.46 & 0.0273 & 0.0350 & 4.56 + 0.06 & 0.03 & 0 & 0 & 0.0908 & 0.4238 & 7.42 & 0.0257 & 0.0311 & 4.62 + 0.2 & 0.1 & 0 & 0 & 0.1044 & 0.4788 & 7.64 & 0.0257 & 0.0316 & 4.69 + 0.4 & 0.2 & 0 & 0 & 0.1192 & 0.6400 & 7.84 & 0.0250 & 0.0314 & 4.74 + 0.6 & 0.3 & 0 & 0 & 0.1825 & 1.1745 & 8.78 & 0.0221 & 0.0276 & 5.13 + & & & & & & & & & +   + 0.06 & 0.03 & 1 & -0.5 & 5.0599 & 1.9673 & & 0.0777 & 0.3758 & 1.95 + 0.2 & 0.1 & 1 & -0.5 & 3.4465 & 2.3662 & & 0.0583 & 0.1403 & 2.63 + 0.4 & 0.2 & 1 & -0.5 & 2.9215 & 2.0191 & & 0.0425 & 0.0721 & 2.45 + 0.6 & 0.3 & 1 & -0.5 & 3.2014 & 2.2246 & & 0.0277 & 0.0387 & 1.47 + 0.06 & 0.03 & 0 & 0 & 0.4259 & 0.4259 & & 0.0216 & 0.0285 & 4.80 + 0.2 & 0.1 & 0 & 0 & 0.4955 & 0.4955 & & 0.0222 & 0.0295 & 4.24 + 0.4 & 0.2 & 0 & 0 & 0.6580 & 0.6580 & & 0.0228 & 0.0303 & 3.17 + 0.6 & 0.3 & 0 & 0 & 1.1990 & 1.1990 & & 0.0215 & 0.0283 & 1.66 +    finally , table [ tabla2 ] contains simulations results for @xmath407 , @xmath408 , and different values of @xmath409 .",
    "all columns have to be interpreted similar to those of table [ tabla1 ] . for @xmath410 suitable parameter estimates",
    "can obviously only been determined by applying the augmented model",
    ". for @xmath411 model ( [ simul-2 ] ) reduces to a sparse , standard linear regression model .",
    "it is then clearly unnecessary to apply the augmented model .",
    "both methods then lead to roughly equivalent parameter estimates .",
    "we want to emphasize that @xmath412 , @xmath413 constitutes a particularly difficult situation .",
    "then the first and second factor only explain 6% and 3% of the variance of @xmath17 .",
    "consequently , @xmath192 is very small and one will expect a fairly large error in estimating @xmath414 .",
    "somewhat surprisingly the augmented model still provides reasonable parameter estimates , the only problem in this case seems to be a fairly large prediction error .",
    "another difficult situation in an opposite direction is @xmath415 , @xmath416 .",
    "then both factors together explain 90% of the variability of @xmath17 , while @xmath57 only explains the remaining 10% .",
    "consequently , @xmath193 is very small and one may expect problems in the context of the restricted eigenvalue condition .",
    "the table shows that this case yields the smallest prediction error , but the quality of parameter estimates deteriorates .",
    "proof of proposition [ propcov ] define @xmath417 , @xmath2 , @xmath418 . for any @xmath419 and @xmath420 , noting that @xmath421 , we have @xmath422 where @xmath423 is the indicator function .",
    "we have @xmath424 and @xmath425 applying the bernstein inequality for bounded centered random variables [ see @xcite ] we get @xmath426\\\\[-8pt ] & & \\qquad\\le\\exp\\biggl\\ { \\frac{-\\varepsilon^2n}{8(c_1+c\\varepsilon/3 ) } \\biggr\\ } .\\nonumber\\end{aligned}\\ ] ] we have now @xmath427\\\\[-8pt ] & & \\qquad\\le\\sum_{i=1}^np(|q_{ijl}| > c)+p\\bigl(\\mathbb { e}\\bigl(|q_{ijl}|i(|q_{ijl}| > c)\\bigr ) > \\varepsilon/4 \\bigr).\\nonumber\\end{aligned}\\ ] ] using markov s inequality and ( [ exponential - condition ] ) we obtain @xmath428 choose @xmath429 and @xmath430 , where @xmath122 is a positive constant such that @xmath431 and @xmath432 .",
    "note now that @xmath433 while @xmath434 which implies @xmath435 inequalities ( [ term1 ] ) , ( [ term2 ] ) and ( [ term3 ] ) lead finally to @xmath436\\\\[-8pt ] & & \\qquad\\le p^{-c^2_0/(8(c_1+c_0^{3/2}/3))}+2nc_1e^{-({a/2})(n/\\log p)^{1/4}}.\\nonumber\\end{aligned}\\ ] ] the result ( [ propcovresult ] ) is now a consequence of ( [ term4 ] ) since @xmath437    proof of theorem [ thmiden1 ] for any symmetric matrix @xmath438 , we denote by @xmath439 its eigenvalues .",
    "weyl s perturbation theorem [ see , e.g. , @xcite , page 63 ] implies that for any symmetric matrices @xmath440 and @xmath441 and all @xmath442 @xmath443 where @xmath444 is the usual matrix norm defined as @xmath445 since @xmath446 , ( [ knut01 ] ) leads to @xmath447 . by assumption",
    ", @xmath448 is a diagonal matrix with diagonal entries @xmath449 , @xmath6",
    ". therefore @xmath450 and ( [ thmiden-1 ] ) is an immediate consequence .    in order to verify ( [ thmiden-2 ] ) first note that lemma a.1 of @xcite implies that for symmetric matrices @xmath440 and @xmath441 @xmath451\\\\[-8pt ] & & { } + \\frac{6\\|\\mathbf{b}\\|^2}{\\min_{j\\neq r}|\\lambda_j(\\mathbf{a})-\\lambda _ r(\\mathbf{a})|^2},\\nonumber\\end{aligned}\\ ] ] where @xmath452 , @xmath453 are the eigenvectors corresponding to the eigenvalues @xmath454 by assumption ( a.3 ) this implies @xmath455 for all @xmath152 . since @xmath456 ,",
    "the second part of ( [ thmiden-2 ] ) follows from @xmath457 , @xmath6 .    by ( a.3 )",
    "we necessarily have @xmath458 for all @xmath152 .",
    "consequently , @xmath459 .",
    "furthermore , note that @xmath460 .",
    "since @xmath158 and @xmath159 are uncorrelated , ( [ thmiden-1 ] ) and ( [ thmiden-2 ] ) lead to @xmath461 ^ 2)\\\\ & & \\qquad= \\frac{1}{\\mu_r } \\bolds\\delta^t_r \\frac{1}{p}\\bolds{\\psi}\\bolds\\delta_r\\\\ & & \\qquad\\quad{}+\\biggl(\\frac{\\bolds{\\delta}_r-\\bolds{\\psi}_r}{\\sqrt{\\mu_r } } + \\frac{\\sqrt{\\mu_r}-\\sqrt{\\lambda_r}}{\\sqrt{\\mu_r}\\sqrt{\\lambda _ r}}\\bolds{\\psi}_r\\biggr)^t \\frac{1}{p}\\bolds{\\gamma}\\biggl(\\frac{\\bolds{\\delta}_r-\\bolds { \\psi}_r}{\\sqrt{\\mu_r } } + \\frac{\\sqrt{\\mu_r}-\\sqrt{\\lambda_r}}{\\sqrt{\\mu_r}\\sqrt{\\lambda _ r}}\\bolds{\\psi}_r\\biggr)\\\\ & & \\qquad\\leq\\frac{d_2}{p\\mu_r}+2\\frac{\\lambda_1}{\\mu_r}\\|\\bolds{\\delta } _ r-\\bolds{\\psi}_r\\|^2 + 2\\frac{1}{\\mu_r}\\bigl(\\sqrt{\\mu_r}-\\sqrt{\\lambda_r}\\bigr)^2\\\\ & & \\qquad\\leq\\frac{d_2}{p\\mu_r}+\\frac{8\\lambda_1d_2 ^ 2}{\\mu_rp^2v(k)^2}+\\frac { d_2 ^ 2}{2\\mu_rp^2v(k)}.\\end{aligned}\\ ] ] since @xmath462 the second part of ( [ thmiden-3 ] ) follows from similar arguments . finally , using the cauchy ",
    "schwarz inequality ( [ thmiden-4 ] ) is a straightforward consequence of ( [ thmiden-3 ] ) .",
    "proof of theorem [ thmapp1 ] with @xmath463 and @xmath464 , inequality ( [ knut01 ] ) implies that for all @xmath210 @xmath465 but under events ( [ b21])([b24 ] ) we have @xmath466^{1/2 } \\\\ & & \\qquad= \\sup_{\\| \\mathbf{u}\\|_2=1}\\biggl [ \\frac{1}{p^2}\\sum_{j=1}^p \\biggl(\\sum_{l=1}^p\\biggl(\\frac{1}{n}\\sum _ { i=1}^nx_{i , j}x_{i , l}-\\operatorname{cov}(x_{i , j},x_{i , l})\\biggr)u_l\\biggr)^2 \\biggr]^{1/2 } \\\\ & & \\qquad\\leq\\sup_{\\| \\mathbf{u}\\|_2=1}\\biggl [ \\frac{1}{p^2}\\sum_{j=1}^p \\| \\mathbf{u}\\|_2 ^ 2 \\sum_{l=1}^p\\biggl(\\frac{1}{n}\\sum _ { i=1}^nx_{i , j}x_{i , l}-\\operatorname{cov}(x_{i , j},x_{i , l})\\biggr)^2 \\biggr]^{1/2 } \\\\ & & \\qquad\\leq c_0 \\sqrt{\\frac{\\log p}{n}}.\\end{aligned}\\ ] ] on the other hand , @xmath467 , and we can conclude that @xmath468 relation ( [ thmapp1 - 1 ] ) now is an immediate consequence of ( [ knut02 ] ) and ( [ knut03 ] )",
    ".    relations ( [ knut04 ] ) and ( [ knut03 ] ) together with ( a.3 ) , ( a.4 ) and ( [ b21])([b24 ] ) lead to @xmath469 which gives ( [ thmapp1 - 2 ] ) .",
    "it remains to show ( [ thmapp1 - 3 ] ) and ( [ thmapp1 - 4 ] ) .",
    "note that the spectral decompositions of @xmath470 and @xmath45 imply that for all @xmath6 @xmath471 under events ( [ b21])([b24 ] ) we therefore obtain for all @xmath472 @xmath473 but by assumptions ( a.3 ) and ( a.4 ) , relation ( [ thmapp1 - 1 ] ) leads to @xmath474 .",
    "equations ( [ thmapp1 - 3 ] ) and ( [ thmapp1 - 4 ] ) then are immediate consequences of ( [ knut05 ] ) and ( [ knut06 ] )    proof of theorem [ thmapp2 ] choose an arbitrary @xmath475 . note that @xmath476 .",
    "since @xmath339 we obtain the decomposition @xmath477 under events ( [ b21])([b24 ] ) , we have @xmath478 as well as @xmath479 .",
    "furthermore , @xmath480 and @xmath481 for @xmath19 .",
    "therefore , @xmath482 obviously , @xmath483 .",
    "it now follows from theorem [ thmapp1 ] that there exists a constant @xmath484 , which can be chosen independently of all values @xmath302 satisfying assumptions ( a.3 ) and ( a.4 ) , such that @xmath485 since events ( [ b21])([b24 ] ) have probability @xmath276 , assertion ( [ pkapp0 ] ) is an immediate consequence .    in order to show ( [ pkapp1 ] ) first recall that the eigenvectors of @xmath108 possess the well - known `` best basis '' property , that is , @xmath486 for @xmath6 and @xmath152 define @xmath487 by @xmath488 and @xmath489 , @xmath490 .",
    "the above property then implies that for any @xmath491 @xmath492 since the vectors @xmath493 and @xmath494 only differ in the @xmath491th element , one can conclude that for any @xmath6 @xmath495 the spectral decomposition of @xmath45 implies that @xmath496 with @xmath497 it therefore follows from ( [ prthmapp1 ] ) that @xmath498\\\\[-8pt ] & & { } + \\sum_{r=1}^k \\psi_{rj}^2 p\\widehat{\\lambda}_r.\\nonumber\\end{aligned}\\ ] ] we obtain @xmath499 as well as @xmath500 for @xmath19 .",
    "at the same time under events ( [ b21])([b24 ] ) , @xmath501 note that @xmath502 for all @xmath503 . by the cauchy ",
    "schwarz inequality , theorem [ thmapp1 ] and assumption ( a.4 ) , we have @xmath504 as well as @xmath505 . the bounds for @xmath506 and @xmath507 derived in theorem [ thmapp1 ] then imply that under events ( [ b21])([b24 ] ) there exists a constant @xmath508 , which can be chosen independently of all values @xmath302 satisfying assumptions ( a.3 ) and ( a.4 ) , such that @xmath509\\\\[-8pt ] & & \\qquad\\leq\\widetilde{m}_2 \\frac{k}{v(k)^{3/2 } } n^{-1/2}\\sqrt{\\log p}.\\nonumber\\end{aligned}\\ ] ] at the same time , by theorem [ thmapp1 ] it follows that there exist constants @xmath510 such that and @xmath152 , @xmath511\\\\[-8pt ]    \\widetilde{m}_2^{**}}{v(k ) } n^{-1/2}\\sqrt{\\log p}.\\nonumber\\end{aligned}\\ ] ] note that @xmath512 . under events ( [ b21])([b24 ] ) , we can now conclude from ( [ prthmapp2])([prthmapp3 ] ) that there exists a constant @xmath513 , which can be chosen independently of all values @xmath302 satisfying assumptions ( a.3 ) and ( a.4 ) , such that @xmath514\\\\[-8pt ] & & \\qquad\\quad { } + \\widetilde{m}_2^ { * * * } \\frac{k}{v(k ) } n^{-1/2}\\bigl(\\sqrt{\\log p}\\bigr)^2\\nonumber\\\\ & & \\qquad=\\sigma_j^2+\\mathbb{e}((\\mathbf{p}_k\\mathbf{w}_i)_j^2 ) + \\widetilde{m}_2^ { * * * } \\frac{k}{v(k ) } n^{-1/2}\\sqrt{\\log p}. \\nonumber\\end{aligned}\\ ] ] relations ( [ prthmapp11 ] ) and ( [ prthmapp5 ] ) imply that under ( [ b21])([b24 ] ) @xmath515\\\\[-8pt ] & & \\qquad\\leq\\mathbb{e}((\\mathbf{p}_k\\mathbf{w}_i)_j^2)+ m_2^ * \\frac { k}{v(k)^{3/2 } } n^{-1/2}\\sqrt{\\log p}\\nonumber\\end{aligned}\\ ] ] holds with @xmath516 . since events ( [ b21])([b24 ] ) have probability @xmath276 , assertion ( [ pkapp1 ] ) of theorem [ thmapp2 ] now is an immediate consequence of ( [ prthmapp11 ] ) , ( [ prthmapp5 ] ) and ( [ prthmapp12 ] ) .    it remains to show ( [ pkapp2 ] ) .",
    "we have @xmath517 but for all @xmath152 @xmath518\\\\[-8pt ] & & { } + \\frac{2}{n}\\sum_{i=1}^n \\frac{((\\bolds{\\psi}_r-\\widehat{\\bolds{\\psi}}_r)^t\\mathbf { x}_i)^2}{p\\lambda_r},\\nonumber\\end{aligned}\\ ] ] and theorem [ thmapp1 ] and assumptions ( a.1)(a.4 ) imply that under events ( [ b21])([b24 ] ) there exist some constants @xmath519 , which can be chosen independently of all values @xmath302 satisfying assumptions ( a.3 ) and ( a.4 ) , such that @xmath520 and @xmath521 hold for all @xmath152 .",
    "now note that our setup implies that @xmath522 and @xmath523 hold for all @xmath152 .",
    "the chebyshev inequality thus implies that the event @xmath524 holds with probability at least @xmath525",
    ". we can thus infer from ( [ thmapp2-eq3])([lem2 - 31 ] ) that there exists some positive constant @xmath272 , which can be chosen independently of the values @xmath302 satisfying ( a.3)(a.5 ) , such that under events ( [ b21])([b24 ] ) and ( [ b25 ] ) @xmath526 recall that events ( [ b21])([b24 ] ) and ( [ thmapp2-eq1 ] ) simultaneously hold with probability at least @xmath527 , while ( [ b25 ] ) is satisfied with probability at least @xmath525 .",
    "this proves assertion ( [ pkapp2 ] ) with @xmath528 .",
    "proof of proposition [ propsparse2 ] let @xmath529 denote the @xmath530 diagonal matrix with diagonal entries @xmath531 and split the @xmath231-dimensional vector @xmath532 in two vectors @xmath533 and @xmath534 , where @xmath533 is the @xmath39-dimensional vector with the @xmath39 upper components of @xmath535 , and @xmath534 is the @xmath12-dimensional vector with the @xmath12 lower components of @xmath535 . then @xmath536 the matrix @xmath174 is a diagonal matrix with entries @xmath537 , and @xmath538 for all @xmath539 .",
    "together with the bounds for @xmath540 derived in theorem [ thmapp1 ] we can conclude that under ( [ b21])([b24 ] ) there exists a constant @xmath541 , which can be chosen independently of all values @xmath302 satisfying assumptions ( a.3 ) and ( a.4 ) , such that @xmath542 we have @xmath543 and since @xmath544 , this leads under ( [ b21])([b24 ] ) to @xmath545 on the other hand @xmath546 where @xmath547 , respectively , @xmath548 , is the @xmath12-dimensional vector with the last @xmath12 coordinates of @xmath549 , respectively , @xmath550 . the cauchy ",
    "schwarz inequality leads to @xmath551 .",
    "since @xmath552 we have @xmath553 and the same upper bound holds for the terms @xmath554 and @xmath555 so that @xmath556 obviously , @xmath557 for all @xmath539 . using theorem [ thmapp1 ] , one can infer that under ( [ b21])([b24 ] ) , @xmath558 where the constant @xmath559 can be chosen independently of all values @xmath302 satisfying assumptions ( a.3 ) and ( a.4 ) . when combining the above inequalities , the desired result follows from ( a.5 ) and the bound on@xmath560 to be obtained from ( [ pkapp0 ] )    proof of theorem [ thmaug ] the first step of the proof consists of showing that under events ( [ b21])([b24 ] ) the following inequality holds with probability at least @xmath561 @xmath562 where @xmath563 , @xmath323 and @xmath324 is a sufficiently large positive constant .",
    "since @xmath564 and , hence , @xmath221 and @xmath565 are independent of the i.i.d .",
    "error terms @xmath566 , it follows from standard arguments that @xmath567 holds with probability at least @xmath561 .",
    "therefore , in order to prove ( [ thmapp2-eq1 ] ) it only remains to show that under events ( [ b21])([b24 ] ) there exists a positive constant @xmath326 , which can be chosen independently of the values @xmath302 satisfying ( a.3)(a.5 ) , such that @xmath568 we will now prove ( [ thmapp2-eq3 ] ) .",
    "for all @xmath152 we have @xmath569 using the cauchy ",
    "schwarz inequality and the fact that @xmath570 , inequalities ( [ lem2 - 30 ] ) and ( [ lem2 - 31 ] ) imply that under events ( [ b21])([b24 ] ) , one obtains @xmath571\\\\[-8pt ] & & \\qquad\\quad\\hspace*{43.4pt } { } + \\biggl(\\frac{1}{n}\\sum_{i=1}^n\\frac{((\\bolds{\\psi } _ s-\\widehat{\\bolds{\\psi}}_s)^t\\mathbf{x}_i)^2}{p\\lambda_s } \\biggr)^{1/2}+ \\biggl|\\frac{1}{n}\\sum_{i=1}^n\\widehat{\\xi}_{ir}\\frac { \\bolds{\\psi}_s^t\\mathbf{z}_i}{\\sqrt{p\\lambda_s}}\\biggr| \\biggr)\\nonumber\\\\ & & \\qquad\\leq \\alpha_{\\mathrm{sum } } \\biggl(\\frac{\\sqrt{m_3^{*}}}{v(k)}\\sqrt{\\frac{\\log p}{n}}+ \\frac{\\sqrt{m_3^{**}}}{v(k)^{3/2}}\\sqrt{\\frac{\\log p}{n } } + \\sup_s\\biggl|\\frac{1}{n}\\sum_{i=1}^n\\widehat{\\xi}_{ir}\\frac{\\bolds { \\psi}_s^t\\mathbf{z}_i}{\\sqrt{p\\lambda_s}}\\biggr| \\biggr).\\nonumber\\end{aligned}\\ ] ] since also @xmath572 , similar arguments show that under ( [ b21])([b24 ] ) @xmath573\\\\[-8pt ] & & \\hspace*{66.6pt } { } + \\sup_s\\biggl|\\frac{1}{n}\\sum_{i=1}^n\\widetilde{x}_{ij}\\frac{\\bolds { \\psi}_s^t\\mathbf{z}_i}{\\sqrt{p\\lambda_s}}\\biggr| \\biggr)\\nonumber\\end{aligned}\\ ] ] for all @xmath6 .",
    "the cauchy ",
    "schwarz inequality yields @xmath574 , @xmath575 , as well as @xmath576 necessarily , @xmath577 and hence @xmath578 .",
    "it therefore follows from ( [ thmapp1 - 3 ] ) , ( [ thmapp1 - 4 ] ) , ( [ pkapp0 ] ) and ( a.5 ) that under events ( [ b21])([b24 ] ) there are some constants @xmath579 , @xmath580 such that for all @xmath581 and @xmath6 , @xmath582\\\\[-8pt ] & \\le&\\frac{m_5^{***}}{v(k)}\\sqrt{\\frac{\\log p}{n}}\\nonumber\\end{aligned}\\ ] ] and @xmath583\\\\[-8pt ] & \\le&\\frac{\\widetilde{m}_5^{***}}{v(k)^{3/2}}\\sqrt{\\frac{\\log p}{n}}.\\nonumber\\end{aligned}\\ ] ] result ( [ thmapp2-eq3 ] ) is now a direct consequence of ( [ thmapp2-eq4])([lem2 - 5 ] ) .",
    "note that all constants in ( [ thmapp2-eq4])([lem2 - 5 ] ) and thus also the constant @xmath326 can be chosen independently of the values @xmath302 satisfying ( a.3)(a.5 ) .    under event ( [ thmapp2-eq1 ] ) as well as @xmath584",
    ", inequalities ( b.1 ) , ( 4.1 ) , ( b.27 ) and ( b.30 ) of @xcite may be transferred in our context which yields @xmath585 where @xmath284 is the set of nonnull coefficients of @xmath586 .",
    "this implies that @xmath587    events ( [ b21])([b24 ] ) hold with probability @xmath276 , and therefore the probability of event ( [ thmapp2-eq1 ] ) is at least @xmath588 . when combining ( [ thmapp1 - 4 ] ) , ( [ pkapp0 ] ) and ( [ bickel1 ] ) , inequalities ( [ thmaug-1 ] ) and ( [ thmaug-2 ] ) follow from the definitions of @xmath589 and @xmath590 , since under ( [ b21])([b24 ] ) @xmath591 and @xmath592 it remains to prove assertion ( [ thmaug2 - 1 ] ) on the prediction error .",
    "we have @xmath593\\\\[-8pt ] & & \\qquad\\le\\frac{2}{n}\\sum_{i=1}^n\\biggl(\\sum_{r=1}^k\\widehat{\\xi } _ { ir}(\\widehat{\\widetilde{\\alpha}}_r-\\widetilde{\\alpha}_r ) + \\sum_{j=1}^p\\widetilde{x}_{ij}(\\hspace*{1.5pt}\\widehat{\\hspace*{-1.5pt}\\widetilde{\\beta}}_j-\\widetilde { \\beta}_j)\\biggr)^2\\nonumber\\\\ & & \\qquad\\quad{}+ \\frac{2}{n}\\sum_{i=1}^n\\biggl(\\sum_{r=1}^k(\\widehat{\\xi}_{ir}-\\xi _ { ir})\\alpha_r\\biggr)^2.\\nonumber\\end{aligned}\\ ] ] under event ( [ thmapp2-eq1 ] ) as well as @xmath584 , the first part of inequalities ( b.31 ) in the proof of theorem 7.2 of @xcite leads to @xmath594 under events ( [ b21])([b24 ] ) , ( [ thmapp2-eq1 ] ) as well as ( [ b25 ] ) , inequality ( [ thmaug2 - 1 ] ) now follows from ( [ thmaug2-eq1 ] ) , ( [ thmaug2-eq2 ] ) and ( [ pkapp2 ] ) . the assertion then is a consequence of the fact that ( [ b21])([b24 ] ) are satisfied with probability @xmath276 , while ( [ thmapp2-eq1 ] ) and ( [ b25 ] ) hold with probabilities at least @xmath595 and @xmath525 , respectively ."
  ],
  "abstract_text": [
    "<S> the paper considers linear regression problems where the number of predictor variables is possibly larger than the sample size . </S>",
    "<S> the basic motivation of the study is to combine the points of view of model selection and functional regression by using a factor approach : it is assumed that the predictor vector can be decomposed into a sum of two uncorrelated random components reflecting common factors and specific variabilities of the explanatory variables . </S>",
    "<S> it is shown that the traditional assumption of a sparse vector of parameters is restrictive in this context . </S>",
    "<S> common factors may possess a significant influence on the response variable which can not be captured by the specific effects of a small number of individual variables . </S>",
    "<S> we therefore propose to include principal components as additional explanatory variables in an augmented regression model . </S>",
    "<S> we give finite sample inequalities for estimates of these components . </S>",
    "<S> it is then shown that model selection procedures can be used to estimate the parameters of the augmented model , and we derive theoretical properties of the estimators . </S>",
    "<S> finite sample performance is illustrated by a simulation study .    .    . </S>"
  ]
}