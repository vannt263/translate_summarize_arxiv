{
  "article_text": [
    "the success of deep learning is to a large part based on advanced and efficient input representations @xcite .",
    "these representations are sparse and hierarchical .",
    "sparse representations of the input are in general obtained by rectified linear units ( relu ) @xcite and dropout @xcite . the key advantage of sparse representations is that dependencies between coding units are easy to model and to interpret . most importantly ,",
    "distinct concepts are much less likely to interfere in sparse representations . using sparse representations , similarities of samples often break down to co - occurrences of features in these samples . in bioinformatics",
    "sparse codes excelled in biclustering of gene expression data @xcite and in finding dna sharing patterns between humans and neanderthals @xcite .",
    "representations learned by relus are not only sparse but also _ non - negative_. non - negative representations do not code the degree of absence of events or objects in the input . as the vast majority of events",
    "is supposed to be absent , to code for their degree of absence would introduce a high level of random fluctuations .",
    "we also aim for _ non - linear _ input representations to stack models for constructing _ hierarchical representations_. finally , the representations are supposed to have a",
    "_ large number of coding units _ to allow coding of rare and small events in the input .",
    "rare events are only observed in few samples like seldom side effects in drug design , rare genotypes in genetics , or small customer groups in e - commerce .",
    "small events affect only few input components like pathways with few genes in biology , few relevant mutations in oncology , or a pattern of few products in e - commerce . in summary ,",
    "our goal is to construct input representations that ( 1 ) are sparse , ( 2 ) are non - negative , ( 3 ) are non - linear , ( 4 ) use many code units , and ( 5 ) model structures in the input data ( see next paragraph ) .",
    "current unsupervised deep learning approaches like autoencoders or restricted boltzmann machines ( rbms ) do not model specific structures in the data . on the other hand ,",
    "generative models explain structures in the data but their codes can not be enforced to be sparse and non - negative .",
    "the input representation of a generative model is its posterior s mean , median , or mode , which depends on the data .",
    "therefore sparseness and non - negativity can not be guaranteed independent of the data .",
    "for example , generative models with rectified priors , like rectified factor analysis , have zero posterior probability for negative values , therefore their means are positive and not sparse @xcite .",
    "sparse priors do not guarantee sparse posteriors as seen in the experiments with factor analysis with laplacian and jeffrey s prior on the factors ( see tab .",
    "[ tab : compare ] ) . to address the data dependence of the code",
    ", we employ the _ posterior regularization method _ @xcite .",
    "this method separates model characteristics from data dependent characteristics that are enforced by constraints on the model s posterior .",
    "we aim at representations that are feasible for many code units and massive datasets , therefore the computational complexity of generating a code is essential in our approach . for non - gaussian priors",
    ", the computation of the posterior mean of a new input requires either to numerically solve an integral or to iteratively update variational parameters @xcite .",
    "in contrast , for gaussian priors the posterior mean is the product between the input and a matrix that is independent of the input . still the posterior regularization method leads to a quadratic ( in the number of coding units ) constrained optimization problem in each e - step ( see eq .",
    "below ) . to speed up computation ,",
    "we do not solve the quadratic problem but perform a gradient step . to allow for stochastic gradients and fast gpu implementations ,",
    "also the m - step is a gradient step .",
    "these e - step and m - step modifications of the posterior regularization method result in a _ generalized alternating minimization _",
    "( gam ) algorithm @xcite . we will show that the gam algorithm used for rfn learning ( i ) converges and ( ii ) is correct .",
    "correctness means that the rfn codes are non - negative , sparse , have a low reconstruction error , and explain the covariance structure of the data .",
    "this supplement contains additional information complementing the main manuscript and is structured as follows : first , the rectified factor network ( rfn ) learning algorithm with e- and m - step updates , weight decay and dropout regularization is given in section  [ sec : alg ] . in section [ sec : conv ] , we proof that the ( rfn ) learning algorithm is a `` generalized alternating minimization '' ( gam ) algorithm and converges to a solution that maximizes the rfn objective .",
    "the correctness of the rfn algorithm is proofed in section [ sec : correct ] .",
    "section  [ sec : mlfa ] describes the maximum likelihood factor analysis model and the model selection by the em - algorithm .",
    "the rfn objective , which has to be maximized , is described in section  [ sec : objective ] .",
    "next , rfn s gam algorithm via gradient descent both in the m - step and the e - step is reported in the section  [ sec : gam ] .",
    "the following sections  [ sec : gradientm ] and [ sec : gradiente ] describe the gradient - based m- and e - step , respectively . in section  [ sec : prior ] , we describe how the rfns sparseness can be controlled by a gaussian prior . additional information on the selected hyperparameters of the benchmark methods is given in section  [ s_sec : hyperpars ] .",
    "the sections [ s_sec : data1 ] and [ s_sec : data2 ] describe the data generation of the benchmark datasets and report the results for three different experimental settings , namely for extracting 50 ( undercomplete ) , 100 ( complete ) or 150 ( overcomplete ) factors / hidden units .",
    "finally , section  [ s_sec : convnets ] describes experiments , that we have done to assess the performance of rfn _ first layer _ pretraining on _ cifar-10 _ and _ cifar-100 _ for three deep convolutional network architectures : ( i ) the alexnet @xcite , ( ii ) deeply supervised networks ( dsn ) @xcite , and ( iii ) our 5-convolution - network - in - network ( 5c - nin ) .",
    "our goal is to construct representations of the input that ( 1 ) are sparse , ( 2 ) are non - negative , ( 3 ) are non - linear , ( 4 ) use many code units , and ( 5 ) model structures in the input .",
    "structures in the input are identified by a generative model , where the model assumptions determine which input structures to explain by the model .",
    "we want to model the covariance structure of the input , therefore we choose maximum likelihood factor analysis as model .",
    "the constraints on the input representation are enforced by the _ posterior regularization method _ @xcite . _",
    "non - negative constraints _ lead to sparse and non - linear codes , while _ normalization constraints _ scale the signal part of each hidden ( code ) unit . normalizing constraints",
    "avoid that generative models explain away _ rare and small signals _ by noise .",
    "explaining away becomes a serious problem for models with many coding units since their capacities are not utilized .",
    "normalizing ensures that all hidden units are used but at the cost of coding also random and spurious signals .",
    "spurious and true signals must be separated in a subsequent step either by supervised techniques , by evaluating coding units via additional data , or by domain experts .",
    "a generative model with hidden units @xmath0 and data @xmath1 is defined by its prior @xmath2 and its likelihood @xmath3 .",
    "the full model distribution @xmath4 can be expressed by the model s posterior @xmath5 and its evidence ( marginal likelihood ) @xmath6 : @xmath7 .",
    "the representation of input @xmath1 is the posterior s mean , median , or mode .",
    "the posterior regularization method introduces a _ variational distribution _",
    "@xmath8 from a family @xmath9 , which approximates the posterior @xmath5 .",
    "we choose @xmath9 to constrain the posterior means to be non - negative and normalized .",
    "the full model distribution @xmath10 contains all model assumptions and , thereby , defines which structures of the data are modeled .",
    "@xmath11 contains data dependent constraints on the posterior , therefore on the code .    for data @xmath12 ,",
    "the posterior regularization method maximizes the objective @xmath13 @xcite : @xmath14 where @xmath15 is the kullback - leibler distance . maximizing @xmath13",
    "achieves two goals simultaneously : ( 1 ) extracting desired structures and information from the data as imposed by the generative model and ( 2 ) ensuring desired code properties via @xmath16 .",
    "r0.35        the factor analysis model @xmath17 extracts the _ covariance structure _ of the data .",
    "the prior @xmath18 of the hidden units ( factors ) @xmath19 and the noise @xmath20 of visible units ( observations ) @xmath21 are independent .",
    "the model parameters are the weight ( loading ) matrix @xmath22 and the noise covariance matrix @xmath23 .",
    "we assume diagonal @xmath24 to explain correlations between input components by the hidden units and not by correlated noise .",
    "the factor analysis model is depicted in fig .",
    "[ fig : rfa ] . given the mean - centered data @xmath12 , the posterior @xmath25 is gaussian with mean vector @xmath26 and covariance matrix @xmath27 : @xmath28    a _ rectified factor network _",
    "( rfn ) consists of a single or stacked factor analysis model(s ) with constraints on the posterior . to incorporate the posterior constraints into the factor analysis model",
    ", we use the posterior regularization method that maximizes the objective @xmath13 given in eq .",
    "@xcite . like the expectation - maximization ( em )",
    "algorithm , the posterior regularization method alternates between an e - step and an m - step . minimizing the first @xmath15 of eq",
    ".   with respect to @xmath29 leads to a constrained optimization problem . for gaussian distributions , the solution with @xmath26 and@xmath27 from eq .",
    "is @xmath30 with @xmath31 and the quadratic problem : @xmath32 where `` @xmath33 '' is component - wise .",
    "this is a constraint non - convex quadratic optimization problem in the number of hidden units which is too complex to be solved in each em iteration .",
    "therefore , we perform a step of the _ gradient projection algorithm _",
    "@xcite , which performs first a gradient step and then projects the result to the feasible set .",
    "we start by a step of the _ projected newton method _ , then we try the _ gradient projection algorithm _ , thereafter the _ scaled gradient projection algorithm _ with reduced matrix @xcite ( see also @xcite ) . if these methods fail to decrease the objective in eq .",
    ", we use the _ generalized reduced method _ @xcite .",
    "it solves each equality constraint for one variable and inserts it into the objective while ensuring convex constraints .",
    "alternatively , we use rosen s gradient projection method @xcite or its improvement @xcite .",
    "these methods guarantee a decrease of the e - step objective .    since the projection @xmath34 by eq .",
    "is very fast , the projected newton and projected gradient update is very fast , too .",
    "a projected newton step requires @xmath35 steps ( see eq .",
    "and @xmath34 defined in theorem  [ th : rectnorm ] ) , a projected gradient step requires @xmath36 steps , and a scaled gradient projection step requires @xmath37 steps .",
    "the rfn complexity per iteration is @xmath38 ( see alg .",
    "[ alg : rfn ] ) .",
    "in contrast , a quadratic program solver typically requires for the @xmath39 variables ( the means of the hidden units for all samples ) @xmath40 steps to find the minimum @xcite .",
    "we exemplify these values on our benchmark datasets mnist ( @xmath41k , @xmath42 ) and cifar ( @xmath41k , @xmath43 ) .",
    "the speedup with projected newton or projected gradient in contrast to a quadratic solver is @xmath44 , which gives * speedup ratios of @xmath45 for mnist and @xmath46 for cifar .",
    "* these speedup ratios show that efficient e - step updates are essential for rfn learning . furthermore , on our computers , ram restrictions limited quadratic program solvers to problems with @xmath47k .",
    "the m - step decreases the _ expected reconstruction error _ @xmath48 from eq .   with respect to the model parameters @xmath49 and @xmath24 .",
    "definitions of @xmath50 , @xmath51 and @xmath52 are given in alg .",
    "[ alg : rfn ] .",
    "the m - step performs a gradient step in the newton direction , since we want to allow stochastic gradients , fast gpu implementation , and dropout regularization .",
    "the newton step is derived in the supplementary which gives further details , too .",
    "also in the e - step , rfn learning performs a gradient step using projected newton or gradient projection methods .",
    "these projection methods require the euclidean projection @xmath34 of the posterior means @xmath53 onto the _ non - convex _ feasible set : @xmath54 the following theorem  [ th : rectnorm ] gives the euclidean projection @xmath34 as solution to eq .  .",
    "[ th : rectnorm ] if at least one @xmath55 is positive for @xmath56 , then the solution to optimization problem eq .",
    "is @xmath57_j \\ = \\",
    "\\frac{\\hat{\\mu}_{ij}}{\\sqrt{\\frac{1}{n } \\ \\sum_{i=1}^{n } \\hat{\\mu}_{ij}^2 } } \\ \\ , \\",
    "\\quad   \\hat{\\mu}_{ij } \\ = \\",
    "\\left\\ { \\begin{array}{lcl } 0 & \\mathrm{for } & ( \\mu_p)_{ij } \\ \\leq \\ 0 \\\\ ( \\mu_p)_{ij } & \\mathrm{for } &   ( \\mu_p)_{ij } \\",
    "> \\ 0 \\end{array } \\right.\\ .\\end{aligned}\\ ] ] if all @xmath55 are non - positive for @xmath56 , then the optimization problem eq .   has the solution @xmath58 for @xmath59 and @xmath60 otherwise .",
    "see supplementary material .    using the projection @xmath34 defined in eq .  , the e - step updates for the posterior means @xmath61 are : @xmath62 where we set for the projected newton method @xmath63 ( thus @xmath64 ) , and for the projected gradient method @xmath65 . for the scaled gradient projection algorithm with reduced matrix , the @xmath66-active set for @xmath67 consists of all @xmath68 with @xmath69 .",
    "the reduced matrix @xmath70 is the hessian @xmath71 with @xmath66-active columns and rows @xmath68 fixed to unit vectors @xmath72 .",
    "the resulting algorithm is a posterior regularization method with a gradient based e- and m - step , leading to a _ generalized alternating minimization _",
    "( gam ) algorithm @xcite .",
    "the rfn learning algorithm is given in alg .",
    "[ alg : rfn ] .",
    "dropout regularization can be included before e - step2 by randomly setting code units @xmath73 to zero with a predefined dropout rate ( note that convergence results will no longer hold ) .",
    "2    @xmath74 * e - step1 * @xmath75 @xmath76 * constraint posterior * ( 1 ) projected newton , ( 2 ) projected gradient , ( 3 ) scaled gradient projection , ( 4 ) generalized reduced method , ( 5 ) rosen s gradient project . *",
    "e - step2 * @xmath77 @xmath78 * m - step * @xmath79 @xmath80 @xmath81 if stopping criterion is met : stop = true    objective @xmath13 : @xmath82 ; e - step1 : @xmath83 ; projected newton : @xmath35 ; projected gradient : @xmath36 ; scaled gradient projection : @xmath37 ; e - step2 : @xmath84 ; m - step : @xmath85 ; overall complexity with projected newton / gradient for @xmath86 : @xmath38 .",
    "[ [ sec : convergence ] ] convergence of rfn learning .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    theorem  [ th : conv ] states that alg .",
    "[ alg : rfn ] converges to a maximum of @xmath13 .",
    "[ th : conv ] the rectified factor network ( rfn ) learning algorithm given in alg .",
    "[ alg : rfn ] is a `` generalized alternating minimization '' ( gam ) algorithm and converges to a solution that maximizes the objective @xmath13 .",
    "we present a sketch of the proof which is given in detail in the supplement . for convergence , we show that alg .",
    "[ alg : rfn ] is a gam algorithm which convergences according to proposition  5 in @xcite .",
    "[ alg : rfn ] ensures to decrease the m - step objective which is convex in @xmath49 and @xmath87 .",
    "the update with @xmath88 leads to the minimum of the objective .",
    "convexity of the objective guarantees a decrease in the m - step for @xmath89 if not in a minimum .",
    "[ alg : rfn ] ensures to decrease the e - step objective by using gradient projection methods .",
    "all other requirements for gam convergence are also fulfilled .",
    "proposition  5 in @xcite is based on zangwill s generalized convergence theorem , thus updates of the rfn algorithm are viewed as point - to - set mappings @xcite .",
    "therefore the numerical precision , the choice of the methods in the e - step , and gpu implementations are covered by the proof .",
    "[ [ sec : correctnes ] ] correctness of rfn learning .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    the goal of the rfn algorithm is to explain the data and its covariance structure .",
    "the _ expected approximation error _",
    "@xmath90 is defined in line 14 of alg .",
    "[ alg : rfn ] .",
    "theorem  [ th : fixedpointdiagnonal ] states that the rfn algorithm is correct , that is , it explains the data ( low reconstruction error ) and captures the covariance structure as good as possible .",
    "[ th : fixedpointdiagnonal ] the fixed point @xmath49 of alg .",
    "[ alg : rfn ] minimizes @xmath91 given @xmath61 and @xmath92 by ridge regression with @xmath93 where @xmath94 .",
    "the model explains the data covariance matrix by @xmath95 up to an error , which is quadratic in @xmath24 for @xmath96 .",
    "the reconstruction error @xmath97 is quadratic in @xmath24 for @xmath96 .",
    "the fixed point equation for the @xmath49 update is @xmath98 . using the definition of @xmath51 and @xmath52",
    ", we have @xmath99 .",
    "@xmath49 is the ridge regression solution of @xmath100 where @xmath101 is the trace . after multiplying out all @xmath102 in @xmath103",
    ", we obtain : @xmath104 for the fixed point of @xmath24 , the update rule gives : @xmath105 .",
    "thus , @xmath49 minimizes @xmath91 given @xmath61 and @xmath92 . multiplying the woodbury identity for @xmath106 from left and right by @xmath24 gives @xmath107 inserting this into the expression for @xmath108 and taking the trace gives @xmath109 therefore for @xmath96 the error is quadratic in @xmath24 .",
    "@xmath110 follows from fixed point equation @xmath111 . using this and eq .  , eq .",
    "is @xmath112 using the trace norm ( nuclear norm or ky - fan n - norm ) on matrices , eq .",
    "states that the left hand side of eq .",
    "is quadratic in @xmath24 for @xmath96 .",
    "the trace norm of a positive semi - definite matrix is its trace and bounds the frobenius norm @xcite .",
    "thus , for @xmath113 , the covariance is approximated up to a quadratic error in @xmath24 according to eq .  .",
    "the diagonal is exactly modeled .",
    "since the minimization of the expected reconstruction error @xmath91 is based on @xmath61 , the quality of reconstruction depends on the correlation between @xmath61 and @xmath114 .",
    "we ensure maximal information in @xmath61 on @xmath114 by the i - projection ( the minimal kullback - leibler distance ) of the posterior onto the family of rectified and normalized gaussian distributions .",
    "l*3 > p2.8em*3>p2.8em*3 > p2.8em & & & + ( r0pt)2 - 4 ( lr)5 - 7 ( l0pt)8 - 10 & sp & er & co & sp & er & co & sp & er & co + rfn & 75@xmath1150 & 249@xmath1153 & 108@xmath1153  & 81@xmath1151 & 68@xmath1159 & 26@xmath1156   & 85@xmath1151 & 17@xmath1156 & 7@xmath1156 + rfnn & 74@xmath1150 & 295@xmath1154 & 140@xmath1154  & 79@xmath1150 & 185@xmath1155 & 59@xmath1153   & 80@xmath1150 & 142@xmath1154 & 35@xmath1152 + dae & 66@xmath1150 & 251@xmath1153 &   & 69@xmath1150 & 147@xmath1152 &    & 71@xmath1150 & 130@xmath1152 &  + rbm & 15@xmath1151 & 310@xmath1154 &   & 7@xmath1151 & 287@xmath1154 &    & 5@xmath1150 & 286@xmath1154 &  + fasp & 40@xmath1151 & 999@xmath11563 & 999@xmath11599  & 63@xmath1150 & 999@xmath11565 & 999@xmath11599   & 80@xmath1150 & 999@xmath11565 & 999@xmath11599 + falap & 4@xmath1150 & 239@xmath1156 & 341@xmath11519  & 6@xmath1150 & 46@xmath1154 & 985@xmath11545   & 4@xmath1150 & 46@xmath1154 & 976@xmath11553 + [ -0.30ex ] ica & 2@xmath1150 & 174@xmath1152 &   & 3@xmath1151 & 0@xmath1150 &    & 3@xmath1151 & 0@xmath1150 &  + [ -0.30ex ] sfa & 1@xmath1150 & 218@xmath1155 & 94@xmath1153  & 1@xmath1150 & 16@xmath1151 & 114@xmath1155   & 1@xmath1150 & 16@xmath1151 & 285@xmath1157 + fa & 1@xmath1150 & 218@xmath1154 & 90@xmath1153  & 1@xmath1150 & 16@xmath1151 & 83@xmath1154   & 1@xmath1150 & 16@xmath1151 & 263@xmath1156 + pca & 0@xmath1150 & 174@xmath1152 &   & 2@xmath1150 & 0@xmath1150 &  & 2@xmath1150 & 0@xmath1150 &  +    [ [ rfns-vs.other-unsupervised-methods . ]",
    "] rfns vs.  other unsupervised methods .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we assess the performance of rectified factor networks ( rfns ) as unsupervised methods for data representation .",
    "we compare ( 1 ) * rfn * : rectified factor networks , ( 2 ) * rfnn * : rfns without normalization , ( 3 ) * dae * : denoising autoencoders with relus , ( 4 ) * rbm * : restricted boltzmann machines with gaussian visible units , ( 5 ) * fasp * : factor analysis with jeffrey s prior ( @xmath116 ) on the hidden units which is sparser than a laplace prior , ( 6 ) * falap * : factor analysis with laplace prior on the hidden units , ( 7 ) * ica * : independent component analysis by fastica @xcite , ( 8) * sfa * : sparse factor analysis with a laplace prior on the parameters , ( 9 ) * fa * : standard factor analysis , ( 10 ) * pca * : principal component analysis .",
    "the number of components are fixed to 50 , 100 and 150 for each method .",
    "we generated nine different benchmark datasets ( d1 to d9 ) , where each dataset consists of 100 instances .",
    "each instance has 100 samples and 100 features resulting in a 100@xmath117100 matrix . into these matrices , biclusters",
    "are implanted @xcite .",
    "a bicluster is a pattern of particular features which is found in particular samples like a pathway activated in some samples .",
    "an optimal representation will only code the biclusters that are present in a sample .",
    "the datasets have different noise levels and different bicluster sizes .",
    "large biclusters have 2030 samples and 2030 features , while small biclusters 38 samples and 38 features .",
    "the pattern s signal strength in a particular sample was randomly chosen according to the gaussian @xmath118 .",
    "finally , to each matrix , zero - mean gaussian background noise was added with standard deviation 1 , 5 , or 10 .",
    "the datasets are characterized by dx=@xmath119 with background noise @xmath120 , number of large biclusters @xmath121 , and the number of small biclusters @xmath122 : d1=(1,10,10 ) , d2=(5,10,10 ) , d3=(10,10,10 ) , d4=(1,15,5 ) , d5=(5,15,5 ) , d6=(10,15,5 ) , d7=(1,5,15 ) , d8=(5,5,15 ) , d9=(10,5,15 ) .",
    "+ we evaluated the methods according to the ( 1 ) _ sparseness _ of the components , the ( 2 ) input _ reconstruction error _ from the code , and the ( 3 ) _ covariance reconstruction error _ for generative models . for rfns sparseness is the percentage of the components that are exactly 0 , while for others methods it is the percentage of components with an absolute value smaller than 0.01 .",
    "the reconstruction error is the sum of the squared errors across samples .",
    "the covariance reconstruction error is the frobenius norm of the difference between model and data covariance .",
    "see supplement for more details on the data and for information on hyperparameter selection for the different methods . tab .",
    "[ tab : compare ] gives averaged results for models with 50 ( undercomplete ) , 100 ( complete ) and 150 ( overcomplete ) coding units .",
    "results are the mean of 900 instances consisting of 100 instances for each dataset d1 to d9 . in the supplement ,",
    "we separately tabulate the results for d1 to d9 and confirm them with different noise levels .",
    "falap did not yield sparse codes since the variational parameter did not push the absolute representations below the threshold of 0.01 .",
    "the variational approximation to the laplacian is a gaussian distribution @xcite .",
    "_ rfns had the sparsest code , the lowest reconstruction error , and the lowest covariance approximation error of all methods that yielded sparse representations ( sp>10% ) .",
    "_    [ [ rfn - pretraining - for - deep - nets . ] ] rfn pretraining for deep nets .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +     +   +   +    we assess the performance of rectified factor networks ( rfns ) if used for pretraining of deep networks .",
    "stacked rfns are obtained by first training a single layer rfn and then passing on the resulting representation as input for training the next rfn .",
    "the deep network architectures use a rfn pretrained first layer ( rfn-1 ) or stacks of 3 rfns giving a 3-hidden layer network .",
    "the classification performance of deep networks with rfn pretrained layers was compared to ( i ) support vector machines , ( ii ) deep networks pretrained by stacking denoising autoencoders ( sdae ) , ( iii ) stacking regular autoencoders ( sae ) , ( iv ) restricted boltzmann machines ( rbm ) , and ( v ) stacking restricted boltzmann machines ( dbn ) .",
    "+    ll * 5 > p3.7em * 1 > p5.2em dataset & & svm & rbm & dbn & sae & sdae & rfn + mnist & & * 1.40@xmath1150.23 * & * 1.21@xmath1150.21 * & * 1.24@xmath1150.22 * & * 1.40@xmath1150.23 * & * 1.28@xmath1150.22 * & * 1.27@xmath1150.22 * ( 1 ) + basic & & 3.03@xmath1150.15 & 3.94@xmath1150.17 & 3.11@xmath1150.15 & 3.46@xmath1150.16 & * 2.84@xmath1150.15 * & * 2.66@xmath1150.14 * ( 1 ) + bg - rand & & 14.58@xmath1150.31 & 9.80@xmath1150.26 & * 6.73@xmath1150.22 * & 11.28@xmath1150.28 & 10.30@xmath1150.27 & 7.94@xmath1150.24 ( 3 ) + bg - img & & 22.61@xmath1150.37 & * 16.15@xmath1150.32 * & * 16.31@xmath1150.32 * & 23.00@xmath1150.37 & * 16.68@xmath1150.33 * & * 15.66@xmath1150.32",
    "* ( 1 ) + rect & & 2.15@xmath1150.13 & 4.71@xmath1150.19 & 2.60@xmath1150.14 & 2.41@xmath1150.13 & 1.99@xmath1150.12 & * 0.63@xmath1150.06 * ( 1 ) + rect - img & & 24.04@xmath1150.37 & 23.69@xmath1150.37 & 22.50@xmath1150.37 & 24.05@xmath1150.37 & 21.59@xmath1150.36 & * 20.77@xmath1150.36 * ( 1 ) + convex & & 19.13@xmath1150.34 & 19.92@xmath1150.35 & 18.63@xmath1150.34 & 18.41@xmath1150.34 & 19.06@xmath1150.34 & * 16.41@xmath1150.32 * ( 1 ) + norb & & 11.6@xmath1150.40 & 8.31@xmath1150.35 & - & 10.10@xmath1150.38 & 9.50@xmath1150.37 & * 7.00@xmath1150.32 * ( 1 ) + cifar & & 62.7@xmath1150.95 & * 40.39@xmath1150.96 * & 43.38@xmath1150.97 & 43.25@xmath1150.97 & - & * 41.29@xmath1150.95 * ( 1 ) +    the benchmark datasets and results are taken from previous publications @xcite and contain : ( i ) _ mnist _ ( original mnist ) , ( ii ) _ basic _ ( a smaller subset of mnist for training ) , ( iii ) _ bg - rand _ ( mnist with random noise background ) , ( iv ) _ bg - img _ ( mnist with random image background ) , ( v ) _ rect _ ( discrimination between tall and wide rectangles ) , ( vi ) _ rect - img _ ( discrimination between tall and wide rectangular images overlayed on random background images ) , ( vii ) _ convex _ ( discrimination between convex and concave shapes ) , ( viii ) _ cifar-10 _ ( 60k color images in 10 classes ) , and ( ix ) _ norb _ ( 29,160 stereo image pairs of 5 generic categories ) .",
    "for each dataset its size of training , validation and test set is given in the second column of tab .",
    "[ tab : tab_res ] . as preprocessing we only performed median centering .",
    "model selection is based on the validation set performance @xcite .",
    "the rfns hyperparameters are ( i ) the number of units per layer from @xmath123 and ( ii ) the dropout rate from @xmath124 .",
    "the learning rate was fixed to its default value of @xmath125 . for supervised fine - tuning with stochastic gradient descent ,",
    "we selected the learning rate from @xmath126 , the masking noise from @xmath127 , and the number of layers from @xmath128 . fine - tuning was stopped based on the validation set performance , following @xcite .",
    "the test error rates together with the 95% confidence interval ( computed according to @xcite ) for deep network pretraining by rfns and other methods are given in tab .",
    "[ tab : tab_res ] .",
    "[ fig : repfield ] shows learned filters .",
    "the result of the best performing method is given in bold , as well as the result of those methods for which confidence intervals overlap .",
    "rfns were only once significantly worse than the best method but still the second best . in six out of the nine experiments rfns performed best , where in four cases it was significantly the best .",
    "[ [ rfns - in - drug - discovery . ] ] rfns in drug discovery .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    using rfns we analyzed gene expression datasets of two projects in the lead optimization phase of a big pharmaceutical company @xcite .",
    "the first project aimed at finding novel antipsychotics that target pde10a . the second project was an oncology study that focused on compounds inhibiting the fgf receptor .",
    "in both projects , the expression data was summarized by farms @xcite and standardized .",
    "rfns were trained with 500 hidden units , no masking noise , and a learning rate of @xmath125 .",
    "the identified transcriptional modules are shown in fig .",
    "[ fig : qstar ] . panels a and b illustrate that rfns found rare and small events in the input . in panel a only a few drugs",
    "are genotoxic ( rare event ) by downregulating the expression of a small number of tubulin genes ( small event ) .",
    "the genotoxic effect stems from the formation of micronuclei ( panel c and d ) since the mitotic spindle apparatus is impaired .",
    "also in panel b , rfn identified a rare and small event which is a transcriptional module that has a negative feedback to the mapk signaling pathway .",
    "rare events are unexpectedly inactive drugs ( black dots ) , which do not inhibit the fgf receptor .",
    "both findings were not detected by other unsupervised methods , while they were highly relevant and supported decision - making in both projects @xcite .",
    "we have introduced rectified factor networks ( rfns ) for constructing very sparse and non - linear input representations with many coding units in a generative framework .",
    "like factor analysis , rfn learning explains the data variance by its model parameters .",
    "the rfn learning algorithm is a posterior regularization method which enforces non - negative and normalized posterior means .",
    "we have shown that rfn learning is a generalized alternating minimization method which can be proved to converge and to be correct .",
    "rfns had the sparsest code , the lowest reconstruction error , and the lowest covariance approximation error of all methods that yielded sparse representations ( sp@xmath12910% ) .",
    "rfns have shown that they improve performance if used for pretraining of deep networks . in two pharmaceutical drug discovery studies",
    ", rfns detected small and rare gene modules that were so far missed by other unsupervised methods .",
    "these gene modules were highly relevant and supported the decision - making in both studies .",
    "rfns are geared to large datasets , sparse coding , and many representational units , therefore they have high potential as unsupervised deep learning techniques .",
    "[ [ acknowledgment . ] ] acknowledgment .",
    "+ + + + + + + + + + + + + + +    the tesla k40 used for this research was donated by the nvidia corporation .",
    "algorithm  [ s_alg : rfn ] is the rectified factor network ( rfn ) learning algorithm .",
    "the rfn algorithm calls algorithm  [ alg : projestep ] to project the posterior probability @xmath130 onto the family of rectified and normalized variational distributions @xmath131 .",
    "algorithm  [ alg : projestep ] guarantees an improvement of the e - step objective @xmath132 .",
    "projection algorithm  [ alg : projestep ] relies on different projections , where a more complicated projection is tried if a simpler one failed to improve the e - step objective .",
    "if all following newton - based gradient projection methods fail to decrease the e - step objective , then projection algorithm  [ alg : projestep ] falls back to gradient projection methods . first the equality constraints are solved and inserted into the objective .",
    "thereafter , the constraints are convex and gradient projection methods are applied .",
    "this approach is called `` generalized reduced gradient method ''",
    "@xcite , which is our preferred alternative method .",
    "if this method fails , then rosen s gradient projection method @xcite is used .",
    "finally , the method of haug and arora @xcite is used .",
    "first we consider newton - based projection methods , which are used by algorithm  [ alg : projestep ] .",
    "algorithm  [ alg : psimple ] performs a simple projection , which is the projected newton method with learning rate set to one .",
    "this projection is very fast and ideally suited to be performed on gpus for rfns with many coding units .",
    "algorithm  [ alg : psimplerec ] is the fast and simple projection without normalization even simpler than algorithm  [ alg : psimple ] .",
    "algorithm  [ alg : projscale ] generalizes algorithm  [ alg : psimple ] by introducing step sizes @xmath133 and @xmath134 .",
    "the step size @xmath133 scales the gradient step , while @xmath134 scales the difference between to old projection and the new projection . for both @xmath133 and @xmath134 annealing steps ,",
    "that is , learning rate decay is used to find an appropriate update .",
    "if these newton - based update rules do not work , then algorithm  [ alg : projscalered ] is used .",
    "algorithm  [ alg : projscalered ] performs a scaled projection with a reduced hessian matrix @xmath70 instead of the full hessian @xmath71 . for computing @xmath70 an @xmath66-active set",
    "is determined , which consists of all @xmath68 with @xmath135 .",
    "the reduced matrix @xmath70 is the hessian @xmath71 with @xmath66-active columns and rows @xmath68 fixed to unit vector @xmath72 .",
    "the rfn algorithm allows regularization of the parameters @xmath49 and @xmath24 ( off - diagonal elements ) by weight decay .",
    "priors on the parameters can be introduced .",
    "if the priors are convex functions , then convergence of the rfn algorithm is still ensured .",
    "the weight decay algorithm  [ alg : wd ] can optionally be used after the m - step of algorithm  [ s_alg : rfn ] .",
    "coding units can be regularized by dropout .",
    "however dropout is not covered by the convergence proof for the rfn algorithm .",
    "the dropout algorithm  [ alg : dropout ] is applied during the projection between rectifying and normalization .",
    "methods like mini - batches or other stochastic gradient methods are not covered by the convergence proof for the rfn algorithm .",
    "however , in @xcite it is shown how to generalize the gam convergence proof to mini - batches as it is shown for the incremental em algorithm .",
    "dropout and other stochastic gradient methods can be show to converge similar to mini - batches .    for @xmath136",
    ": @xmath137 , number of coding units @xmath138    @xmath139 , @xmath140 , @xmath141 , @xmath142 , @xmath143 , @xmath144 , @xmath145    @xmath146 , @xmath49 element - wise random in @xmath147 $ ] , @xmath148 , stop = false    * e - step1 * @xmath149 @xmath150 * projection * perform projection of @xmath26 onto the feasible set by algorithm  [ alg : projestep ] giving @xmath61 * e - step2 * @xmath77 @xmath78 * m - step * @xmath151 @xmath79 _",
    "update _ @xmath152 _ diagonal @xmath24 update _ @xmath153 _",
    "full @xmath24 update _ @xmath154 _ bound",
    "parameters _ @xmath155 @xmath156 if stopping criterion is met : stop = true    obtain @xmath157 that decrease the e - step objective    @xmath158 , @xmath159 for @xmath136 : @xmath26 , @xmath160 , @xmath161 simple projection @xmath34 ( rectified or rectified & normalized ) , e - step objective : @xmath162 @xmath163 , @xmath164 , @xmath165 , @xmath166 , @xmath66 ( for @xmath66-active set )    * simple projection * perform newton projection by algorithm  [ alg : psimple ] or algorithm  [ alg : psimplerec ] * scaled projection * following loop for : ( 1 ) @xmath134 , ( 2 ) @xmath133 , or ( 3 ) @xmath134 and @xmath133 annealing @xmath167 @xmath168 ( skipped for @xmath133 annealing ) @xmath169 ( skipped for @xmath134 annealing ) perform scaled newton projection by algorithm  [ alg : projscale ] * scaled projection with reduced matrix * determine @xmath66-active set as all @xmath68 with @xmath135 set @xmath70 to @xmath71 with @xmath66-active columns and rows @xmath68 fixed to @xmath72 following loop for : ( 1 ) @xmath134 , ( 2 ) @xmath133 , or ( 3 ) @xmath134 and @xmath133 annealing @xmath167 @xmath168 ( skipped for @xmath133 annealing ) @xmath169 ( skipped for @xmath134 annealing ) perform scaled projection with reduced matrix by algorithm  [ alg : projscalered ] * general gradient projection * use generalized reduced gradient @xcite or use rosen s gradient projection @xcite or use method of haug and arora @xcite    for @xmath136 : project @xmath26 onto feasible set giving @xmath61",
    "@xmath26    @xmath170_j \\right\\ } $ ]    for @xmath136 : project @xmath26 onto feasible set giving @xmath61    for @xmath136 : @xmath26    @xmath171_j \\right\\ } $ ]    @xmath172 @xmath173_{\\hat{j } } \\}\\\\ 0 & \\mathrm{otherwise } & \\end{array } \\right.$ ]    perform a scaled newton step with subsequent projection    for @xmath136 : @xmath26 for @xmath136 : @xmath160 simple projection @xmath34 ( rectified or rectified & normalized ) , @xmath133 ( gradient step size ) , @xmath134 ( projection difference )    @xmath174 @xmath175    perform a scaled projection step with reduced matrix    for @xmath136 : @xmath26 for @xmath136 : @xmath160 simple projection @xmath34 ( rectified or rectified & normalized ) , @xmath133 , @xmath134 , @xmath70 , @xmath71    @xmath176 @xmath177    parameters @xmath49 weight decay factors @xmath178 ( gaussian ) and @xmath179 ( laplacian )    @xmath180    @xmath181 @xmath182    for @xmath136 : @xmath61 dropout probability @xmath183    @xmath184 @xmath185",
    "[ th : rfnconvergence ] the rectified factor network ( rfn ) learning algorithm given in algorithm  [ s_alg : rfn ] is a `` generalized alternating minimization '' ( gam ) algorithm and converges to a solution that maximizes the objective @xmath13 .",
    "the factor analysis em algorithm is given by eq .",
    "and eq .   in section  [ sec : mlfa ] . algorithm  [ s_alg : rfn ] is the factor analysis em algorithm with modified the e - step and the m - step .",
    "the e - step is modified by constraining the variational distribution @xmath29 to non - negative means and by normalizing its means across the samples .",
    "the m - step is modified to a newton direction gradient step .",
    "like em factor analysis , algorithm  [ s_alg : rfn ] aims at maximizing the negative _ free energy _",
    "@xmath13 , which is @xmath186 @xmath15 denotes the kullback - leibler ( kl ) divergence @xcite , which is larger than or equal to zero .",
    "algorithm  [ s_alg : rfn ] decreases @xmath187 ( the e - step objective ) in its e - step under constraints for non - negative means and normalization .",
    "the constraint optimization problem from section  [ sec : fulle ] for the e - step is @xmath188 the m - step of algorithm  [ s_alg : rfn ] aims at decreasing @xmath189 algorithm  [ s_alg : rfn ] performs one gradient descent step in the newton direction to decrease @xmath190 , while em factor analysis minimizes @xmath190 .    from the modification of the e - step and",
    "the m - step follows that algorithm  [ s_alg : rfn ] is a _ generalized alternating minimization ( gam ) _ algorithm according to @xcite .",
    "gam is an em algorithm that increases @xmath13 in the e - step and increases @xmath13 in the m - step ( see also section  [ sec : gam ] ) .",
    "the most important requirements for the convergence of the gam algorithm according to theorem  [ th : gamconvergence ] ( proposition  5 in @xcite ) are the increase of the objective @xmath13 in both the e - step and the m - step .",
    "therefore we first show these two decreases before showing that all requirements of convergence theorem  [ th : gamconvergence ] are met .",
    "* algorithm  [ s_alg : rfn ] ensures to decrease the m - step objective . *",
    "the m - step objective @xmath190 is convex in @xmath49 and @xmath87 according to theorem  [ th : newtonloading ] and theorem  [ th : newtoninvnoise ] .",
    "the update with @xmath191 leads to the minimum of @xmath190 according to theorem  [ th : newtonloading ] and theorem  [ th : newtoninvnoise ] .",
    "the convexity of @xmath190 guarantees that each update with @xmath192 decreases the m - step objective @xmath190 , except the current @xmath49 and @xmath87 are already the minimizers .",
    "* algorithm  [ s_alg : rfn ] ensures to decrease the e - step objective . * the e - step decrease of algorithm  [ s_alg : rfn ] is performed by algorithm  [ alg : projestep ] . according to theorem  [ th : projnewton ] the scaled projection with reduced matrix ensures a decrease of the e - step objective for rectifying constraints ( convex feasible set ) .",
    "according to theorem  [ th : gradproj ] also gradient projection methods ensure a decrease of the e - step objective for rectifying constraints . for rectifying constraints and normalization ,",
    "the feasible set is not convex because of the equality constraints . to optimize such problems",
    ", the generalized reduced gradient method @xcite solves each equality constraint for one variable and inserts it into the objective . for our problem eq .   gives the solution and eq .   the resulting convex constraints .",
    "now scaled projection and gradient projection methods can be applied . for rectifying and normalizing constraints ,",
    "also rosen s @xcite and haug & arora s @xcite gradient projection method ensures a decrease of the e - step objective since they can be applied to non - convex problems .    we show that the requirements as given in section  [ sec : gam ] for gam convergence according to theorem  [ th : gamconvergence ] ( proposition  5 in @xcite ) are fulfilled :    1",
    ".   the learning rules , that is , the e - step and the m - step , are closed maps @xmath193 ensured by continuous and continuous differentiable maps , 2 .",
    "the parameter set is compact @xmath193 ensured by bounding @xmath24 and @xmath49 , 3 .",
    "the family of variational distributions is compact ( often described by the feasible set of parameters of the variational distributions ) @xmath193 ensured by continuous and continuous differentiable functions for the constraints and by the bounds on the variational parameters @xmath194 and @xmath92 determined by bounds on the parameters and the data , 4 .",
    "the support of the density models does not depend on the parameter @xmath193 ensured by gaussian models with full - rank covariance matrix , 5 .",
    "the density models are continuous in the parameters @xmath193 ensured by gaussian models 6 .",
    "the e - step has a unique maximizer @xmath193 ensured by the convex , continuous , and continuous differentiable function that is minimized @xcite together with compact feasible set for the variational parameters , the maximum may be local for non - convex feasible sets stemming from normalization , 7 .",
    "the e - step increases the objective if not at the maximizer @xmath193 ensured as shown above , 8 .",
    "the m - step has a unique maximizer ( this is not required ) @xmath193 ensured by minimizing a convex , continuous and continuous differentiable function in the model parameter and a convex feasible set , the maximum is a global maximum , 9",
    ".   the m - step increases the objective if not at the maximizer @xmath193 ensured as shown above .",
    "since this proposition  5 in @xcite is based on zangwill s generalized convergence theorem , updates of the rfn algorithm are viewed as point - to - set mappings @xcite .",
    "therefore the numerical precision , the choice of the methods in the e - step , and gpu implementations are covered by the proof . that the m - step has a unique maximizer",
    "is not required to proof theorem  [ th : rfnconvergence ] by theorem  [ th : gamconvergence ] .",
    "however we obtain an alternative proof by exchanging the variational distribution @xmath29 and the parameters @xmath195 , that is , exchanging the e - step and the m - step .",
    "a theorem analog to theorem  [ th : gamconvergence ] but with e - step and m - step conditions exchanged can be derived from zangwill s generalized convergence theorem @xcite .",
    "the resulting model from the gam procedure is at a local maximum of the objective given the model family and the family of variational distributions .",
    "_ the solution minimizes the kl - distance between the family of full variational distributions and full model family_. `` full '' means that both the observed and the hidden variables are taken into account , where for the variational distributions the probability of the observations is set to 1 . the _",
    "desired family _ is defined as the set of all probability distributions that assign probability one to the observation . in our case",
    "the family of variational distributions is not the desired family since some distributions are excluded by the constraints .",
    "therefore the solution of the gam optimization does not guarantee stationary points in likelihood @xcite .",
    "this means that we do not maximize the likelihood but minimize @xmath196 according to eq .  , where @xmath197 is a constant independent of @xmath29 and independent of the model parameters .",
    "the rfn algorithm is correct if it has a low reconstruction error and explains the data covariance matrix by its parameters like factor analysis .",
    "we show in theorem  [ s_th : fixedpointdiagnonal ] and theorem  [ th : fixedpointfull ] that the rfn algorithm    1 .",
    "minimizes the reconstruction error given @xmath61 and @xmath92 ( the error is quadratic in @xmath24 ) ; 2 .   explains the covariance matrix by its parameters @xmath49 and @xmath24 plus an estimate of the second moment of the coding units @xmath52 .    since the minimization of the reconstruction error is based on @xmath61 , the quality of reconstruction and covariance explanation depends on the correlation between @xmath61 and @xmath114 .",
    "the larger the correlation between @xmath61 and @xmath114 , the lower the reconstruction error and the better the explanation of the data covariance .",
    "we ensure maximal information in @xmath61 on @xmath114 by the i - projection ( the minimal kullback - leibler distance ) of the posterior onto the family of rectified and normalized gaussian distributions .    the reconstruction error for given mean values",
    "@xmath61 is @xmath198 where @xmath199 the reconstruction error for using the whole variational distribution @xmath200 instead of its means is @xmath24 .",
    "below we will derive eq .",
    ", which is @xmath201 therefore @xmath24 is the reconstruction error for given mean values plus the variance @xmath202 introduced by the hidden variables .",
    "[ s_th : fixedpointdiagnonal ] the fixed point @xmath49 minimizes @xmath91 given @xmath61 and @xmath92 by ridge regression with @xmath203 where we used the error @xmath204 the model explains the data covariance matrix by @xmath205 up to an error , which is quadratic in @xmath24 for @xmath113 .",
    "the reconstruction error @xmath206 is quadratic in @xmath24 for @xmath113 .    the fixed point equation for the @xmath49 update is @xmath207 using the definition of @xmath51 and @xmath52 , the fixed point equation eq .",
    "gives @xmath208 therefore @xmath49 is a _",
    "ridge regression _ estimate , also called _ generalized tikhonov regularization _ estimate , which minimizes @xmath209 where we used the reconstruction error @xmath199    we obtain with this definition of the error @xmath210    therefore from the fixed point equation for @xmath24 with the diagonal update rule follows @xmath211 where `` @xmath212 '' projects a matrix to a diagonal matrix . from this",
    "follows that @xmath213 consequently , the fixed point @xmath49 minimizes @xmath91 given @xmath61 and @xmath92 .",
    "after convergence of the algorithm @xmath214 holds .",
    "the woodbury identity ( matrix inversion lemma ) states @xmath215 from which follows by multiplying the equation from right and left by @xmath24 that @xmath216    inserting this equation eq .   into eq .",
    "gives @xmath217 therefore we have @xmath218 it follows that @xmath219 the inequality uses the fact that for positive definite matrices @xmath220 and @xmath221 inequality @xmath222 holds @xcite .",
    "thus , for @xmath113 the error @xmath223 is quadratic in @xmath24 .    multiplying the fixed point equation eq .   by @xmath52",
    "gives @xmath111 .",
    "therefore we have : @xmath224 inserting eq .   into the first line of eq .",
    "and eq .   for simplifying the last line of eq",
    ".   gives @xmath225 using the trace norm ( nuclear norm or ky - fan n - norm ) on matrices , eq .",
    "states that the left hand side is quadratic in @xmath24 for @xmath96 .",
    "the trace norm of a positive semi - definite matrix is its trace and bounds the frobenius norm @xcite .",
    "furthermore , eq .   states that the left hand side of this equation has zero diagonal entries .",
    "therfore it follows that @xmath226 holds except an error , which is quadratic in @xmath24 for @xmath96 .",
    "the diagonal is exactly modeled according to eq .  .",
    "therefore the model corresponding to the fixed point explains the empirical matrix of second moments @xmath50 by a noise part @xmath24 and a signal part @xmath227 . like factor analysis the data variance",
    "is explained by the model via the parameters @xmath24 ( noise ) and @xmath49 ( signal ) .",
    "[ th : fixedpointfull ] the fixed point @xmath49 minimizes @xmath91 given @xmath61 and @xmath92 by ridge regression with @xmath203 where we used the error @xmath204 the model explains the data covariance matrix by @xmath228 the reconstruction error @xmath229 is quadratic in @xmath24 for @xmath96 .",
    "the first part follows from previous theorem  [ s_th : fixedpointdiagnonal ] . the fixed point equation for the @xmath24 update is @xmath230 using eq .",
    "this leads to @xmath231    from eq",
    ".   follows for the fixed point of @xmath24 with the full update rule : @xmath232 inserting eq .   into eq",
    "gives @xmath233 from which follows @xmath234 thus , the error @xmath235 is quadratic in @xmath24 , for @xmath96 .",
    "we are given the data @xmath236 which is assumed to be centered .",
    "centering can be done by subtracting the mean @xmath194 from the data .",
    "the model is @xmath237 where @xmath238 the model includes the _ observations _ @xmath21 , the _ noise _ @xmath239 , the _ factors _ @xmath19 , the _ factor loading matrix _ @xmath22 , and the _ noise covariance matrix _ @xmath23 .",
    "typically we assume that @xmath24 is a diagonal matrix to explain data covariance by signal and not by noise .",
    "the data variance is explained through a signal part @xmath240 and through a noise part @xmath241 .",
    "the parameters of the model are @xmath49 and @xmath24 . from the model assumption",
    "it follows that if @xmath0 is given , then only the noise @xmath241 is a random variable and we have @xmath242    we want to derive the _",
    "likelihood _ of the data under the model , that is , the likelihood that the model has produced the data .",
    "let @xmath243 denote the expectation of the data including the prior distribution of the factors and the noise distribution .",
    "we obtain for the first two moments and the variance : @xmath244    the observations are gaussian distributed since their distribution is the product of two gaussian densities divided by a normalizing constant . therefore , the marginal distribution for @xmath1 is @xmath245    the @xmath246-likelihood @xmath247 of the data @xmath248 under the model @xmath249 is @xmath250 where @xmath251 denotes the absolute value of the determinant of a matrix .    to maximize",
    "the likelihood is difficult since a closed form for the maximum does not exists .",
    "therefore , typically the expectation maximization ( em ) algorithm is used to maximize the likelihood . for the em algorithm a variational distribution @xmath29 is required which estimates the factors given the observations .",
    "we consider a single data vector @xmath114 .",
    "the posterior is also gaussian with mean @xmath26 and covariance matrix @xmath27 : @xmath252 where we used the fact that @xmath253 and @xmath254 the em algorithm sets @xmath29 to the posterior distribution for data vector @xmath114 : @xmath255 therefore we obtain for standared em @xmath256    the matrix inversion lemma ( woodbury identiy ) can be used to compute @xmath257 and @xmath92 : @xmath258 using this identity , the mean and the covariance matrix can be computed as : @xmath259    the em algorithm maximizes a lower bound @xmath13 on the @xmath246-likelihood : @xmath260 @xmath15 denotes the kullback - leibler ( kl ) divergence @xcite which is larger than zero .",
    "@xmath13 is the em objective which has to be maximized in order to maximize the likelihood . the * e - step * maximizes @xmath13 with respect to the variational distribution @xmath29 , therefore the e - step minimizes @xmath261 . after the standard unconstrained e - step ,",
    "the variational distribution is equal to the posterior , i.e.@xmath262 .",
    "therefore the kl divergence @xmath263 is zero , thus @xmath13 is equal to the log - likelihood @xmath264 ( @xmath265 ) . the * m - step * maximizes @xmath13 with respect to the parameters @xmath195 , therefore the m - step maximizes @xmath266 .",
    "we next consider again all @xmath267 samples @xmath236 .",
    "the _ expected reconstruction error _",
    "@xmath190 for these @xmath267 data samples is @xmath268 and objective to maximize becomes @xmath269    the m - step requires to minimize @xmath190 : @xmath270 where @xmath101 gives the trace of a matrix .",
    "the derivatives with respect to the parameters are set to zero for the optimal parameters : @xmath271 and @xmath272    solving above equations gives : @xmath273 and @xmath274    we obtain the following em updates : @xmath275    the em algorithms can be reformulated as : @xmath276",
    "_ our goal is to find a sparse , non - negative representation of the input which extracts structure from the input .",
    "_ a sparse , non - negative representation is desired to code only events or objects that have caused the input .",
    "we assume that only few events or objects caused the input , therefore , we aim at sparseness . furthermore , we do not want to code the degree of absence of events or objects . as the vast majority of events and objects",
    "is supposed to be absent , to code for their degree of absence would introduce a high level of random fluctuations .",
    "we aim at extracting structures from the input , therefore generative models are use as they explicitly model input structures .",
    "for example factor analysis models the covariance structure of the data .",
    "however a generative model can not enforce sparse , non - negative representation of the input .",
    "the input representation of a generative model is the posterior s mean , median , or mode .",
    "generative models with rectified priors ( zero probability for negative values ) lead to rectified posteriors .",
    "however these posteriors do not have sparse means ( they must be positive ) , that is , they do not yield sparse codes @xcite .",
    "for example , rectified factor analysis , which rectifies gaussian priors and selects models using a variational bayesian learning procedure , does not yield posteriors with sparse means @xcite . a generative model with hidden units @xmath0 and data @xmath1 is defined by its prior @xmath2 and its likelihood @xmath3 .",
    "the posterior @xmath5 supplies the input representation of a model by the posterior s mean , median , or mode .",
    "however , the posterior depends on the data @xmath1 , therefore sparseness and non - negativity of its means can not be guaranteed independent of the data .",
    "problem at coding the input by generative models is the data - dependency of the posterior means .",
    "therefore we use the _ posterior regularization method _",
    "( _ posterior constraint method _ ) @xcite .",
    "the posterior regularization framework separates model characteristics from data dependent characteristics like the likelihood or posterior constraints .",
    "posterior regularization incorporates data - dependent characteristics as constraints on model posteriors given the observed data , which are difficult to encode via model parameters by bayesian priors .",
    "a generative model with prior @xmath2 and likelihood @xmath3 has the full model distribution @xmath4 .",
    "it can be written as @xmath277 , where @xmath5 is the model posterior of the hidden variables and @xmath6 is the evidence , that is , the likelihood of the data to be produced by the model .",
    "the model family and its parametrization determines which structures are extracted from the data .",
    "typically the model parameters enter the likelihood @xmath3 and are adjusted to the observed data . for the posterior regularization method , a family @xmath9 of allowed posterior distributions is introduced .",
    "@xmath9 is defined by the expectations of constraint features . in our case",
    "the posterior means have to be non - negative .",
    "distributions @xmath16 are called _ variational distributions _",
    "( see later for using this term ) .",
    "the full variational distribution is @xmath278 with @xmath8 .",
    "the distribution @xmath279 is the unknown distribution of observations as determined by the world or the data generation process .",
    "this distribution is approximated by samples drawn from the world , namely the training samples .",
    "_ @xmath10 contains all model assumptions like the structures used to model the data , while @xmath280 contains all data dependent characteristics including data dependent constraints on the posterior . _",
    "the goal is to achieve @xmath281 , to obtain ( 1 ) a desired structure that is extracted from the data and ( 2 ) desired code properties .",
    "however in general it is to achieve this identity , therefore we want to minimize the distance between these distributions .",
    "we use the kullback - leibler ( kl ) divergence @xcite @xmath15 to measure the distance between these distributions . therefore our objective is @xmath282 .",
    "minimizing this kl divergence ( 1 ) extracts the desired structure from the data by increasing the likelihood , that is , @xmath283 , and ( 2 ) enforces desired code properties by @xmath284 .",
    "thus , the code derived from @xmath11 has the desired properties and t extracts the desired input data structures .",
    "we now approximate the kl divergence by approximating the expectation over @xmath279 by the empirical mean of samples @xmath236 drawn from @xmath279 : @xmath285 the last term @xmath286 neither depends on @xmath29 nor on the model , therefore we will neglect it . in the following , we often abbreviate @xmath287 by @xmath200 or write @xmath288 , since the hidden variable is based on the observation @xmath114 .",
    "similarly we often write @xmath289 instead of @xmath290 and even more often @xmath25 instead of @xmath291 .",
    "we obtain the objective @xmath13 ( to be maximized ) of the _ posterior constraint method _",
    "@xcite : @xmath292 the first line is the negative objective of the posterior constraint method while the third line is the negative eq .   without the term @xmath286 .",
    "* @xmath13 is the objective in our framework which has to be maximized .",
    "* maximizing @xmath13 ( 1 ) increases the model likelihood @xmath293 , ( 2 ) finds a proper input representation by small @xmath261 .",
    "thus , the data representation ( 1 ) extracts structures from the data as imposed by the generative model while ( 2 ) ensuring desired code properties via @xmath294 .    in the variational framework",
    ", @xmath29 is the variational distribution and @xmath13 is called the negative _ free energy _ @xcite .",
    "this physical term is used since variational methods were introduced for quantum physics by richard feynman @xcite .",
    "the hidden variables can be considered as the fictive causes or explanations of environmental fluctuations @xcite .    if @xmath295 , then @xmath296 and we obtain the classical em algorithm .",
    "the em algorithm maximizes the lower bound @xmath13 on the @xmath246-likelihood as seen at the first line of eq",
    ".   and ensures in its e - step @xmath296 .",
    "instead of the em algorithm we use the _ generalized alternating minimization ( gam ) _",
    "algorithm @xcite to allow for gradient descent both in the m - step and the e - step .",
    "the representation of an input by a generative model is the vector of the mean values of the posterior , that is , the most likely hidden variables that produced the observed data .",
    "we have to modify the e - step to enforce variational distributions which lead to sparse codes via zero values of the components of its mean vector .",
    "sparse codes , that is , many components of the mean vector are zero , are obtained by enforcing non - negative means .",
    "this rectification is analog to rectified linear units for neural networks , which have enabled sparse codes for neural networks .",
    "therefore the variational distributions are restricted to stem from a family with non - negative constraints on the means . to impose constraints on the posterior",
    "is known as the _ posterior constraint method _ @xcite .",
    "the posterior constraint method maximizes the objective both in the e - step and the m - step .",
    "the posterior constraint method is computationally infeasible for our approach , since we assume a large number of hidden units . for models with many hidden units ,",
    "the maximization in the e - step would take too much time .",
    "the posterior constraint method does not support fast implementations on gpus and stochastic gradients , which we want to allow in order to use mini - batches and dropout regularization .",
    "therefore we perform only one gradient descent step both in the e - step and in the m - step . unfortunately , the convergence proofs of the em algorithm are no longer valid .",
    "however we show that our algorithm is a generalized alternating minimization ( gam ) method .",
    "gunawardana and byrne showed that the gam converges @xcite ( see also @xcite ) .    the following gam convergence theorem  [ th : gamconvergence ] is proposition  5 in @xcite and proves the convergence of the gam algorithm to a solution that minimizes @xmath297 .",
    "[ th : gamconvergence ] let the point - to - set map @xmath298 the composition @xmath299 of point - to - set maps @xmath300 and @xmath301 .",
    "suppose that the point - to - set maps @xmath302 and @xmath303 are defined so that    1 .",
    "@xmath302 and @xmath303 are closed on @xmath304 2 .",
    "@xmath305 and @xmath306    suppose also that @xmath302 is such that all @xmath307 have @xmath308 and satisfy @xmath309 with equality only if @xmath310 with @xmath311 being the unique minimizer .",
    "suppose also that the point - to - set map @xmath303 is such that all @xmath312 have @xmath313 and satisfy @xmath314 with equality only if @xmath315 then ,    1 .",
    "the point - to - set map @xmath298 is closed on @xmath304 2 .",
    "@xmath316    and @xmath298 satisfies the gam and eq conditions of the gam convergence theorem , that is , theorem  3 in @xcite",
    ".    see proposition  5 in @xcite .",
    "the point - to - set mappings allow extended e - step and m - steps without unique iterates .",
    "therefore , theorem  [ th : gamconvergence ] holds for different implementations , different hardware , different precisions of the algorithm under consideration .    for a gam method to converge",
    ", we have to ensure that the objective increases in both the e - step and the m - step .",
    "@xmath29 is from a constrained family of variational distributions , while the posterior and the full distribution ( observation and hidden units ) are both derived from a model family .",
    "the model family is a parametrized family . for our models",
    "( i ) the support of the density models does not depend on the parameter and ( ii ) the density models are continuous in their parameters .",
    "gam convergence requires both ( i ) and ( ii ) .",
    "furthermore , both the e - step and the m - step must have unique maximizers and they increase the objective if they are not at a maximum point .    the learning rules , that is , the e - step and the m - step are closed maps as they are continuous functions .",
    "the objective for the e - step is strict convex in all its parameters for the variational distributions , simultaneously @xcite .",
    "it is quadratic for the mean vectors on which constraints are imposed .",
    "the objective for the m - step is convex in both parameters @xmath49 and @xmath87 ( we sometimes estimate @xmath24 instead of @xmath87 ) .",
    "the objective is quadratic in the loading matrix @xmath49 . for rectifying only",
    ", we guarantee unique global maximizers by convex and compact sets for both the family of desired distributions and the set of possible parameters . for this convex optimization problem with one _ global",
    "_ maximum . for rectifying and normalizing , the family of desired distributions is not convex due to equality constraints introduced by the normalization",
    ". however we can guarantee _ local _ unique maximizers .",
    "summary of the requirements for gam convergence theorem  [ th : gamconvergence ] :    1 .",
    "the learning rules , that is , the e - step and the m - step , are closed maps , 2 .",
    "the parameter set is compact , 3 .",
    "the family of variational distributions is compact ( often described by the feasible set of parameters of the variational distributions ) , 4 .",
    "the support of the density models does not depend on the parameter , 5 .",
    "the density models are continuous in the parameters , 6 .",
    "the e - step has a unique maximizer , 7",
    ".   the e - step increases the objective if not at the maximizer , 8 .",
    "the m - step has a unique maximizer ( not required by theorem  [ th : gamconvergence ] ) , 9 .",
    "the m - step increases the objective if not at the maximizer .",
    "the resulting model from the gam procedure is at a local maximum of the objective given the model family and the family of variational distributions . _",
    "the solution minimizes the kl - distance between the family of full variational distributions and full model family_. `` full '' means that both the observed and the hidden variables are taken into account , where for the variational distributions the probability of the observations is set to 1 . the _",
    "desired family _ is defined as the set of all probability distributions that assign probability one to the observation . in our case",
    "the family of variational distributions is not the desired family since some distributions are excluded by the constraints .",
    "therefore the solution of the gam optimization does not guarantee stationary points in likelihood @xcite .",
    "this means that we do not maximize the likelihood but minimize the kl - distance between variational distributions and model .",
    "the gradients in the m - step are : @xmath317 and @xmath318    alternatively , we can estimate @xmath87 which leads to the derivatives : @xmath319    scaling the gradients leads to : @xmath320 and @xmath321 or @xmath322    only the sums @xmath323 and @xmath324 must be computed for both gradients .    @xmath325    is the estimated covariance matrix ( matrix of second moments for zero mean ) .",
    "* the generalized em algorithm update rules are : *    @xmath326      instead of gradient ascent , we now consider a newton update step .",
    "the newton update for finding the roots of @xmath327 is @xmath328 where @xmath329 is a small step size and @xmath70 is the hessian of @xmath330 with respect to @xmath1 evaluated at @xmath331 .",
    "we denote the update direction by @xmath332      [ th : newtonloading ] the m - step objective @xmath190 is quadratic in @xmath49 , thus convex in @xmath49 .",
    "the newton update direction for @xmath49 in the m - step is @xmath333    the m - step objective is the _ expected reconstruction error _ @xmath190 , which is according to eq .",
    "@xmath334 where @xmath101 gives the trace of a matrix .",
    "this is a quadratic function in @xmath49 , as stated in the theorem .",
    "the hessian @xmath335 of @xmath336 with respect to @xmath49 as a vector is : @xmath337 where @xmath338 is the kronecker product of matrices .",
    "@xmath335 is positive definite , thus the problem is convex in @xmath49 .",
    "the inverse of @xmath335 is @xmath339    for the product of the inverse hessian with the gradient we have : @xmath340    if we apply a newton update , then the update direction for @xmath49 in the m - step is @xmath333    this is the exact em update if the step - size @xmath329 is 1 .",
    "since the objective is a quadratic function in @xmath49 , one newton update would lead to the exact solution .",
    "we define the expected approximation error by @xmath341    [ [ bmpsi - as - parameter . ] ] @xmath24 as parameter .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    [ th : newtonnoise ] the newton update direction for @xmath24 as parameter in the m - step is @xmath342 an update with @xmath343 ( @xmath344 ) leads to the minimum of the m - step objective @xmath190 .",
    "the m - step objective is the _ expected reconstruction error _ @xmath190 , which is according to eq .",
    "@xmath334 where @xmath101 gives the trace of a matrix .    since @xmath345 is @xmath346 the minimum of @xmath190 with respect to @xmath24 .",
    "therefore an update with @xmath347 leads to the minimum .",
    "the hessian @xmath348 of @xmath349 with respect to @xmath24 as a vector is : @xmath350 the expected approximation error @xmath90 is a sample estimate for @xmath24 , therefore we have @xmath351",
    ". the hessian may not be positive definite for some values of @xmath90 , like for small values of @xmath90 . in order to guarantee a positive definite hessian , more precisely an approximation to it , for minmization",
    ", we set @xmath352 and obtain @xmath353 we derive an approximate newton update that is very close to the newton update .",
    "the inverse of the approximated @xmath348 is @xmath354    for the product of the inverse hessian with the gradient we have : @xmath355    if we apply a newton update , then the update direction for @xmath24 in the m - step is @xmath342 this is the exact em update if the step - size @xmath329 is 1 .",
    "[ [ bmpsi-1-as - parameter . ] ] @xmath87 as parameter .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    [ th : newtoninvnoise ] the m - step objective @xmath190 is convex in @xmath87 .",
    "the newton update direction for @xmath87 as parameter in the m - step is @xmath356 a first order approximation of this newton direction for @xmath24 in the m - step is @xmath357 an update with @xmath343 ( @xmath344 ) leads to the minimum of the m - step objective @xmath190 .",
    "the m - step objective is the _ expected reconstruction error _ @xmath190 , which is according to eq .",
    "@xmath334 where @xmath101 gives the trace of a matrix .    since @xmath358 is @xmath359 the minimum of @xmath190 with respect to @xmath87 .",
    "therefore an update with @xmath347 leads to the minimum .",
    "the hessian @xmath360 of @xmath349 with respect to @xmath87 as a vector is : @xmath361 since the hessian is positive definite , the e - step objective @xmath190 is convex in @xmath87 , which is the first statement of the theorem .",
    "the inverse of @xmath360 is @xmath362    for the product of the inverse hessian with the gradient we have : @xmath363    if we apply a newton update , then the update direction for @xmath87 in the m - step is @xmath356    we now can approximate the update for @xmath24 by the first terms of the taylor expansion : @xmath364 we obtain for the update of @xmath24 @xmath365 this is the exact em update if the step - size @xmath329 is 1 .",
    "the newton update derived from @xmath87 as parameter is the newton update for @xmath24 .",
    "consequently , the newton direction for both @xmath24 and @xmath87 is in the m - step @xmath342",
    "the representation of data vector @xmath1 by the model is the variational mean vector @xmath366 . in order to obtain sparse codes we want to have non - negative @xmath366 .",
    "we enforce non - negative mean values by constraints and optimize by projected newton methods and by gradient projection methods .",
    "non - negative constraints correspond to rectifying in the neural network field .",
    "therefore we aim to construct sparse codes in analogy to the rectified linear units used for neural networks .",
    "we constrain the variational distributions to the family of normal distributions with non - negative mean components .",
    "consequently we introduce non - negative or * rectifying constraints * : @xmath367 where the inequality `` @xmath33 '' holds component - wise .",
    "however generative models with many coding units face a problem .",
    "they tend to _ explain away small and rare signals by noise_. for many coding units , model selection algorithms prefer models with coding units which do not have variation and , therefore , are removed from the model .",
    "other coding units hardly contribute to explain the observations . the likelihood is larger if small and rare signals are explained by noise , than the likelihood if coding units are use to explain such signals .",
    "coding units without variance are kept on their default values , where they have maximal contribution to the likelihood .",
    "if they are used for coding , they deviate from their maximal values for each sample . in accumulation",
    "these deviations decrease the likelihood more than it is increased by explaining small or rare signals . for our rfn models",
    "the problem can become severe , since we aim at models with up to several tens of thousands of coding units . to avoid the explaining away problem",
    ", we enforce the selected models to use all their coding units on an equal level .",
    "we do that by keeping the variation of each noise - free coding unit across the training set at one .",
    "consequently , we introduce a * normalization constraint * for each coding unit @xmath368 : @xmath369 this constraint means that the noise - free part of each coding unit has variance one across samples .",
    "we will derive methods to increase the objective in the e - step both for only rectifying constraints and for rectifying and normalization constraints .",
    "these methods ensure to reduce the objective in the e - step to guarantee convergence via the gam theory .",
    "the resulting model from the gam procedure is at a local maximum of the objective given the model family and the family of variational distributions . _",
    "the solution minimizes the kl - distance between the family of full variational distributions and full model family_. `` full '' means that both the observed and the hidden variables are taken into account .",
    "the e - step maximizes @xmath13 with respect to the variational distribution @xmath29 , therefore the e - step minimizes the kullback - leibler divergence ( kl - divergence ) @xcite @xmath370 .",
    "the kl - divergence between @xmath29 and @xmath371 is @xmath372    _ rectifying constraints _ introduce non - negative constraints .",
    "the minimization with respect to @xmath200 gives the constraint minimization problem : @xmath373 where @xmath61 is the mean vector of @xmath200 .",
    "_ rectifying and normalizing constraints _ introduce non - negative constraints and equality constraints .",
    "the minimization with respect to @xmath200 gives the constraint minimization problem : @xmath374 where @xmath61 is the mean vector of @xmath200 .",
    "first we consider the families from which the model and from which the variational distributions stem .",
    "the posterior of the model with gaussian prior @xmath2 is gaussian ( see section  [ sec : mlfa ] ) : @xmath375 to be as close as possible to the posterior distribution , we restrict @xmath29 to be from a gaussian family : @xmath376    for gaussians , the kullback - leibler divergence between @xmath29 and @xmath371 is @xmath377 this kullback - leibler divergence is convex in the mean vector @xmath366 and the covariance matrix @xmath378 of @xmath29 , simultaneously @xcite .",
    "we now minimize eq .",
    "with respect to @xmath29 . for the moment we do not care about the constraints introduced by non - negativity and by normalization",
    ".   has a quadratic form in @xmath366 , where @xmath378 does not enter , and terms in @xmath378 , where @xmath366 does not enter .",
    "therefore we can separately minimize for @xmath378 and for @xmath366 .    for the minimization with respect to @xmath378 ,",
    "we require @xmath379 and @xmath380 for optimality the derivative of the objective @xmath381 with respect to @xmath378 must be zero : @xmath382 this gives @xmath383 we often drop the index @xmath384 since for @xmath136 all covariance matrices @xmath378 are equal to @xmath27 .",
    "the mean vector @xmath366 of @xmath29 is the solution of the minimization problem : @xmath385 which is equivalent to @xmath386    the derivative and the hessian of this objective is : @xmath387        rectifying is realized by non - negative constraints .",
    "the mean vector @xmath366 of @xmath29 is the solution of the minimization problem : @xmath388 this is a convex quadratic minimization problem with non - negativity constraints ( convex feasible set ) .",
    "if @xmath389 is the lagrange multiplier for the constraints , then the dual is @xmath390 the karush - kuhn - tucker conditions require for the optimal solution for each component @xmath391 : @xmath392 further the derivative of the lagrangian with respect to @xmath194 gives @xmath393 which can be written as @xmath394    this minimization problem can not be solved directly .",
    "therefore we perform a gradient projection or projected newton step to decrease the objective .      to decrease the objective , we perform a gradient projection or a projected newton step .",
    "we will base our algorithms on _ euclidean least distance projections_. if projected onto convex sets , these projections do not increase distances . the euclidean projection onto the feasible set",
    "is denoted by @xmath34 , that is , the map that takes @xmath395 to its nearest point @xmath194 ( in the @xmath396-norm ) in the feasible set .    for rectifying constraints , the projection @xmath34 ( euclidean least distance projection ) of @xmath395 onto the convex feasible set",
    "is given by the solution of the convex optimization problem : @xmath397 the following theorem  [ th : rectifying ] shows that update eq .",
    "is the projection @xmath34 defined by optimization problem eq .  .",
    "[ th : rectifying ] the solution to optimization problem eq .  , which defines the euclidean least distance projection , is @xmath398_j \\ = \\",
    "\\left\\ {   \\begin{array}{lcl } 0 & \\mathrm{for } & ( \\mu_p)_j \\ \\leq \\ 0 \\\\ ( \\mu_p)_j & \\mathrm{for } &   ( \\mu_p)_j \\ > \\ 0 \\end{array } \\right .",
    "\\end{aligned}\\ ] ]    for the projection we have the minimization problem : @xmath399 the lagrangian @xmath400 with multiplier @xmath401 is @xmath402 the derivative with respect to @xmath194 is @xmath403 the karush - kuhn - tucker ( kkt ) conditions require for the optimal solution that for each constraint @xmath68 : @xmath404    if @xmath405 then eq .   requires @xmath406 because the lagrangian @xmath407 is larger than or equal to zero : @xmath408 . from the kkt conditions eq .",
    "follows that @xmath409 and , therefore , @xmath410 . if @xmath411 then @xmath412 , because the constraints of the primal problem require @xmath413 . from eq .",
    "follows that @xmath414 . from the kkt conditions eq .",
    "follows that @xmath415 and @xmath416 . if @xmath415 , then eq .   and the kkt conditions eq .   lead to @xmath417 .",
    "therefore the solution of problem eq .",
    "is @xmath418    this finishes the proof .",
    "if we also consider normalizing constraints , then we have to minimize all kl - divergences simultaneously .",
    "the normalizing constraints connect the single optimization problems for each sample @xmath114 . for the e - step",
    ", we obtain the minimization problem : @xmath419 the `` @xmath33''-sign is meant component - wise .",
    "the @xmath138 equality constraints lead to non - convex feasible sets .",
    "the solution to this optimization problem are the means vectors @xmath61 of @xmath200 .",
    "[ [ generalized - reduced - gradient . ] ] generalized reduced gradient .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the equality constraints can be solved for one variable which is then inserted into the objective .",
    "the equality constraint gives for each @xmath420 : @xmath421 these equations can be inserted into the objective and , thereby , we remove the variables @xmath422 .",
    "we have to ensure that the @xmath422 exist by @xmath423 these constraints define a convex set feasible set . to solve the each equality constraints for a variable and insert it into the objective",
    "is called _ generalized reduced gradient _",
    "method @xcite . for solving the reduced problem",
    ", we can use methods for constraint optimization were we now ensure a convex feasible set .",
    "these methods solve the original problem eq .  .",
    "we only require an improvement of the objective with a feasible value .",
    "for the reduced problem , we perform one step of a _ gradient projection method_.    [ [ gradient - projection - methods . ] ] gradient projection methods .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    also for the original problem eq .",
    ", _ gradient projection methods _ can be used .",
    "the gradient projection method has been generalized by rosen to _ non - linear constraints _ @xcite and was later improved by @xcite .",
    "the gradient projection algorithm of rosen works for _ non - convex feasible sets_. the idea is to linearize the nonlinear constraints and solve the problem .",
    "subsequently a restoration move brings the solution back to the constraint boundaries .      to decrease the objective ,",
    "we perform a gradient projection , a projected newton step , or a step of the generalized reduced method .",
    "we will base our algorithms on _ euclidean least distance projections_. if projected onto convex sets , these projections do not increase distances . the euclidean projection onto the feasible set",
    "is denoted by @xmath34 , that is , the map that simultaneously takes @xmath53 to the nearest points @xmath424 ( in the @xmath396-norm ) in the feasible set .    for rectifying and normalizing constraints the projection ( euclidean least distance projection ) of @xmath53 onto the * non - convex",
    "* feasible set leads to the optimization problem @xmath425 by using @xmath426 , we see that the objective contains the sum @xmath427 .",
    "the constraints enforce this sum to be constant . therefore inserting the equality constraints into the objective , optimization problem eq .",
    "is equivalent to @xmath428    the following theorem  [ s_th : rectnorm ] shows that updates eq .   and",
    "eq .   form the projection defined by optimization problem eq .  .",
    "[ s_th : rectnorm ] if at least one @xmath55 is positive for @xmath56 , then the solution to optimization problem eq .  , which defines the euclidean least distance projection , is @xmath429_j \\ = \\",
    "\\frac{\\hat{\\mu}_{ij}}{\\sqrt{\\frac{1}{n } \\",
    "\\sum_{i=1}^{n } \\hat{\\mu}_{ij}^2 } } \\ .\\end{aligned}\\ ] ] if all @xmath55 are non - positive for @xmath56 , then the optimization problem eq .",
    "has the solution @xmath430    in the following we show that updates eq .   and eq .   are the projection onto the feasible set . for the projection of @xmath53 onto the feasible set",
    ", we have the minimization problem : @xmath431 the feasible set is non - convex because of the quadratic equality constraint .",
    "the lagrangian with multiplier @xmath401 is @xmath432 the karush - kuhn - tucker ( kkt ) conditions require for the optimal solution : @xmath433    the derivative of @xmath400 with respect to @xmath73 is @xmath434 we multiply this equation by @xmath73 and obtain : @xmath435 the kkt conditions give @xmath436 , therefore this term can be removed from the equation .",
    "next we sum over @xmath67 : @xmath437 using the equality constraint @xmath438 and dividing by 2 and gives : @xmath439 solving for @xmath440 leads to : @xmath441    we insert @xmath440 into eq .",
    "@xmath442 we immediately see , that if @xmath443 then @xmath444 .",
    "therefore we can assume @xmath445 .",
    ".   with @xmath73 and using the kkt conditions gives @xmath446 therefore @xmath447 and @xmath448 have the same sign or @xmath449 .",
    "since @xmath450 , we deduce that @xmath55 and @xmath451 have the same sign or @xmath449 .",
    "since the sum is independent of @xmath67 , all @xmath55 with @xmath452 have the same sign for @xmath453 . solving eq",
    ".   for @xmath73 gives @xmath454    * i. * if all @xmath55 are non - positive for @xmath56 , then the sum @xmath455 is negative .",
    "from the first order derivative of the lagrangian in eq .  , we can compute the second order derivative @xmath456 we inserted the expression of eq .   for @xmath440 .",
    "since all mixed second order derivatives are zero , the ( projected ) hessian of the lagrangian is diagonal with negative entries .",
    "therefore it is strict negative definite .",
    "thus , the second order necessary conditions can not be fulfilled .",
    "the minimum is a border point of the constraints .    for each @xmath68",
    "for which all @xmath55 are non - positive for @xmath56 , optimization problem eq .",
    "defines a plane that has a normal vector in the positive orthant ( hyperoctant ) .",
    "for such a @xmath68 the corresponding equality constraint defines a hypersphere .",
    "minimization means that the plane containing the solution is parallel to the original plane and should be as close to the origin as possible .",
    "if we move the plane parallel from the origin into the positive orthant , then the first intersection with the hypersphere is @xmath457 this is the solution for @xmath73 with @xmath56 to our minimization problem .",
    "* ii . * if one @xmath55 is positive , then from eq .   with this @xmath55",
    "follows that @xmath458 is positive , otherwise eq .",
    "has only negative terms on the left hand side .",
    "in particular , the second order necessary conditions are always fulfilled as eq .",
    "is positive .",
    "for @xmath459 it follows from eq .",
    "that @xmath460 and from the kkt conditions that @xmath443 .",
    "for @xmath461 it follows from eq .   that @xmath452 and from the kkt conditions that @xmath462",
    ". therefore we define : @xmath463 we write the solution as @xmath464 we now use the equality constraint : @xmath465 solving for @xmath466 gives : @xmath467 therefore the solution is @xmath468    this finishes the proof .        the _ projected gradient descent _ or _ gradient projection algorithm _",
    "@xcite performs first a gradient step and then projects the result to the _ feasible set_. the projection onto the feasible set is denoted by @xmath34 , that is , the map that takes @xmath194 into the nearest point ( in the @xmath396-norm ) in the feasible set to @xmath194 .",
    "the feasible set must be convex , however later we will introduce gradient projection methods for non - convex feasible sets .",
    "the gradient projection method is in our case @xmath469    the lipschitz constant for the gradient is @xmath470 , the largest eigenvalue of @xmath71 .",
    "the following statement is theorem 5.4.5 in @xcite .",
    "[ th : gradproj ] the _ sufficient decrease _",
    "condition @xmath471 ( e.g.  with @xmath472 ) holds for all @xmath133 such that @xmath473    see @xcite .",
    "_ theorem  [ th : gradproj ] guarantees that we can increase the objective by gradient projection in the e - step , except the case where we already reached the maximum . _    for a fast upper bound on the maximal eigenvalue we use @xmath474 and @xmath475 where the latter follows from @xmath476 improved methods for finding an appropriate @xmath133 by line search methods have been proposed @xcite .",
    "we use a search with @xmath477 with @xmath478 and @xmath479 or @xmath480 .",
    "a special version of the gradient projection method is the _ generalized reduced method _ @xcite .",
    "this method is able to solve our optimization problem with equality constraints .",
    "the gradient projection method has been generalized by rosen to non - linear constraints @xcite .",
    "the gradient projection algorithm of rosen can also be used for a region which is not convex .",
    "the idea is to linearize the nonlinear constraints and solve the problem .",
    "subsequently a restoration move brings the solution back to the constraint boundaries .",
    "rosen s gradient projection method was improved by @xcite . _",
    "these methods guarantee that we can increase the objective in the e - step for non - convex feasible sets , except the case where we already reached the maximum . _",
    "these algorithms for non - convex feasible sets will only give a local maximum .",
    "also the gam algorithm will only find a local maximum .",
    "both the _ scaled gradient projection algorithm _ and the _ projected newton method _ were proposed in @xcite .",
    "we follow @xcite .",
    "the idea is to use a newton update instead of the a gradient update : @xmath481    @xmath482 can be an arbitrary strict positive definite matrix .",
    "if we set @xmath63 , then we have a newton update of the _ projected newton method _ @xcite .",
    "for @xmath483 we obtain @xmath484 otherwise @xmath485    the search direction for the unconstrained problem can be rotated by @xmath482 to be orthogonal to the direction of decrease in the inactive directions for the constrained problem .    to escape this possible problem ,",
    "an @xmath66-active set is introduced which contains all @xmath68 with @xmath135 . all columns and rows of the hessian having an index in the @xmath66-active set",
    "are fixed to @xmath72 . after sorting the indices of the @xmath66-active set together ,",
    "they form a block which is the sub - identity matrix .",
    "@xmath70 is set to the hessian @xmath27 where the @xmath66-active set columns and rows are replaced by unit vectors .",
    "the following theorem  [ th : projnewton ] is lemma 5.5.1 in @xcite . _",
    "theorem  [ th : projnewton ] states that the objective decreases using the reduced hessian in the projected newton method for convex feasible sets_.    [ th : projnewton ] the _ sufficient decrease _ condition @xmath486 holds for all @xmath133 smaller than a bound depending on @xmath70 and @xmath66 .",
    "see @xcite .    in practical applications ,",
    "a proper @xmath133 is found by line search .",
    "the _ projected newton method _ uses @xmath483 to set @xmath66 @xcite : @xmath487      following @xcite we use the following very general update rule , which includes the gradient projection algorithm , the scaled gradient projection algorithm , and the projected newton method .",
    "we use following update for the e - step : @xmath488 we have to project twice since the equality constraint produces a manifold in the parameter space .",
    "we iterate this update until we see a decrease of the objective in the e - step : @xmath489 for the constraints we have only to optimize the mean vector @xmath194 to ensure @xmath490 even @xmath491 can be sufficient if minimizing @xmath492 ensures @xmath493    we use following schedule :    1 .   * @xmath494 * @xmath483 * @xmath495 + that is @xmath496 2 .   *",
    "@xmath494 * @xmath483 * @xmath497 $ ] + that is @xmath498 3 .   *",
    "@xmath494 * @xmath499 $ ] * @xmath495 + that is @xmath500 4 .   * @xmath494 * @xmath499 $ ] * @xmath501 $ ] + that is @xmath502 5 .   *",
    "@xmath503 * @xmath499 $ ] * @xmath501 $ ] + @xmath504 denotes the reduced matrix ( hessian or a positive definite ) according to the projected newton method or the scaled gradient projection algorithm . for convex feasible sets we can guarantee at this level already an increase of the objective at the e - step .",
    "6 .   * @xmath505 * @xmath499 $ ] * @xmath501 $ ] + this is the gradient projection algorithm . in particular we include the generalized reduced method and rosen s gradient projection method .",
    "at this step we guarantee an increase of the objective at the e - step even for non - convex feasible sets because we also use complex methods for constraint optimization .",
    "step 5 . ensures an improvement if only using rectifying constraints according to the theory of projected newton methods @xcite .",
    "ensures an improvement if using both rectifying constraints and normalizing constraints , because we use known methods for constraint optimization . to set @xmath506 is sufficient to increase the objective at the e - step if @xmath492 decreases the kl divergence .",
    "however we will not always set @xmath506 to avoid accumulation points outside the solution set .",
    "we assume @xmath0 is gaussian with covariance @xmath507 and mean @xmath508 @xmath509 we derive the posterior for this prior .",
    "the likelihood is gaussian since a affine transformation of a gaussian random variable is again a gaussian random variable and the convolution of two gaussians is gaussian , too .",
    "thus , @xmath510 is gaussian if @xmath0 and @xmath241 are both gaussian . for the prior moments we have @xmath511 and for the likelihood of @xmath1 we obtain the moments",
    "@xmath512    we need some algebraic identities to derive the posterior .",
    "the woodbury matrix identity gives @xmath513 multiplying this equation from the left hand side with @xmath514 gives @xmath515 it follows that @xmath516    the posterior @xmath5 is derived from gaussian conditioning because both the likelihood @xmath6 and the prior @xmath2 are gaussian distributed .",
    "the conditional distribution @xmath517 of two random variables @xmath518 and @xmath519 that both follow a gaussian distribution is a gaussian : @xmath520    therefore we need the second moments between @xmath1 and @xmath0 : @xmath521 the covariances between @xmath1 and @xmath0 are @xmath522    thus , the mean of @xmath5 is @xmath523 the covariance matrix of @xmath5 is @xmath524    in particular , the variable @xmath508 may be used to enforce more sparseness by setting its components to negative values . since the covariance matrix @xmath525 is positive semi - definite ,",
    "we ensure that @xmath526 if @xmath527 ( @xmath528 is the vector with all components being one ) , then the largest absolute components of @xmath529 must be negative .",
    "thus , @xmath527 leads to sparser solutions .",
    "the performance of rectified factor networks ( rfns ) as unsupervised methods for data representation was compared with:(1 ) * rfn * : rectified factor networks,(2 ) * rfnn * : rfns without normalization,(3 ) * dae * : denoising autoencoders with rectified linear units,(4 ) * rbm * : restricted boltzmann machines with gaussian visible units and hidden binary units,(5 ) * fasp * : factor analysis with jeffrey s prior ( @xmath116 ) on the hidden units which is sparser than a laplace prior,(6 ) * falap * : factor analysis with laplace prior on the hidden units,(7 ) * ica * : independent component analysis by fastica @xcite,(8 ) * sfa * : sparse factor analysis with a laplace prior on the parameters,(9 ) * fa * : standard factor analysis,(10 ) * pca * : principal component analysis.the number of components are fixed to 50 , 100 , or 150 for each method .",
    "the used hyperparameters are listed in tab .",
    "[ s_table : hyperparams ] .",
    "l*1>p25em * method & * used hyperparameters + rfn & \\{learning rate=0.1 , iterations=1000 } + rfnn & \\{learning rate=0.1 , iterations=1000 } + dae & \\{corruption level=0.2 , learning rate=1e-04 , iterations=1000 } + rbm & \\{learning rate=0.01 , iterations=1000 } + fasp & \\{iterations=500 } + falap & \\{iterations=500 } + sfa & \\{laplace weight decay factor=5e-05 , iterations=500 } + * *",
    "the number of components are fixed to 50 , 100 or 150 .",
    "we generated nine different benchmark data sets ( d1 to d9 ) , where each data set consists of 100 instances for averaging the results .",
    "each instance consists of 100 samples and 100 features resulting in a 100@xmath117100 data matrix . into these data matrices ,",
    "structures are implanted as biclusters @xcite .",
    "a bicluster is a pattern consisting of a particular number of features which is found in a particular number of samples .",
    "the size of the bicluster is given by the number of features that form the pattern and by the number of samples in which the pattern is found .",
    "the data sets had different noise levels and different bicluster sizes .",
    "we considered large and small bicluster sizes , where large biclusters have 2030 samples and 2030 features , while small biclusters have 38 samples and 38 features .",
    "the signal strength ( scaling factor ) of a pattern in a sample was randomly chosen according to the gaussian @xmath118 .",
    "finally , to each data matrix background noise was added , where the noise is distributed according to a zero - mean gaussian with standard deviation 1 , 5 , or 10 .",
    "the data sets are described in tab .  [",
    "tab : data ] .",
    "the remaining components of the spanning outer product vectors were drawn by @xmath530 .",
    "l*9>p2em & d1&d2&d3&d4&d5&d6&d7&d8&d9 + noise&1&5&10&1&5&10&1&5&10 + @xmath121&10&10&10&15&15&15&5&5&5 + @xmath122&10&10&10&5&5&5&15&15&15 +    l*3 > p2.9em*3>p2.9em*3 > p2.9em*3>p2.9em*3 > p2.9em    & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&74@xmath1150&58@xmath1151&5@xmath1150&75@xmath1150&233@xmath1153&66@xmath1151&75@xmath1150&456@xmath1155&253@xmath1156&74@xmath1150&63@xmath1151&6@xmath1151&75@xmath1150&236@xmath1153&68@xmath1152 + rfnn&73@xmath1150&85@xmath1153&13@xmath1152&75@xmath1150&272@xmath1153&85@xmath1152&75@xmath1150&531@xmath1156&321@xmath1157&72@xmath1150&95@xmath1154&17@xmath1152&74@xmath1150&276@xmath1154&89@xmath1153 + dae&65@xmath1150&65@xmath1152&&66@xmath1150&233@xmath1152&&66@xmath1150&456@xmath1154&&65@xmath1151&71@xmath1152&&66@xmath1150&237@xmath1152& + rbm&25@xmath1152&86@xmath1153&&11@xmath1151&287@xmath1153&&10@xmath1151&558@xmath1155&&25@xmath1152&94@xmath1153&&11@xmath1151&292@xmath1153& + fasp&39@xmath1151&232@xmath11531&654@xmath11599&40@xmath1151&999@xmath11541&999@xmath11599&41@xmath1151&999@xmath11599&999@xmath11599&38@xmath1151&318@xmath11533&999@xmath11599&40@xmath1151&999@xmath11548&999@xmath11599 + falap&4@xmath1150&53@xmath1152&144@xmath11536&4@xmath1150&224@xmath1155&185@xmath1155&5@xmath1150&439@xmath1159&692@xmath11516&4@xmath1150&55@xmath1152&180@xmath11539&4@xmath1150&226@xmath1155&192@xmath1156 + ica&2@xmath1150&34@xmath1150&&2@xmath1150&164@xmath1152&&2@xmath1150&324@xmath1154&&2@xmath1150&35@xmath1150&&2@xmath1150&166@xmath1152& + sfa&1@xmath1150&42@xmath1151&11@xmath1152&1@xmath1150&206@xmath1154&56@xmath1152&1@xmath1150&406@xmath1159&215@xmath1157&1@xmath1150&42@xmath1151&13@xmath1152&1@xmath1150&208@xmath1154&58@xmath1152 + fa&1@xmath1150&42@xmath1151&6@xmath1151&1@xmath1150&206@xmath1154&54@xmath1152&1@xmath1150&407@xmath1158&210@xmath1156&1@xmath1150&42@xmath1151&8@xmath1151&1@xmath1150&208@xmath1154&56@xmath1152 + pca&1@xmath1150&34@xmath1150&&0@xmath1150&164@xmath1152&&0@xmath1150&324@xmath1154&&1@xmath1150&35@xmath1150&&0@xmath1150&166@xmath1152& + & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&75@xmath1150&458@xmath1155&256@xmath1156&75@xmath1150&53@xmath1151&4@xmath1151&75@xmath1150&230@xmath1153&64@xmath1151&75@xmath1150&454@xmath1155&251@xmath1155&75@xmath1150&249@xmath1153&108@xmath1153 + rfnn&75@xmath1150&532@xmath1156&323@xmath1157&73@xmath1150&73@xmath1153&10@xmath1152&75@xmath1150&268@xmath1153&82@xmath1152&75@xmath1150&528@xmath1156&317@xmath1157&74@xmath1150&295@xmath1154&140@xmath1154 + dae&66@xmath1150&458@xmath1154&&65@xmath1150&58@xmath1151&&66@xmath1150&230@xmath1152&&66@xmath1150&453@xmath1155&&66@xmath1150&251@xmath1153& + rbm&10@xmath1151&561@xmath1155&&23@xmath1152&76@xmath1152&&11@xmath1151&282@xmath1153&&10@xmath1151&555@xmath1155&&15@xmath1151&310@xmath1154& + fasp&40@xmath1152&999@xmath11599&999@xmath11599&39@xmath1151&152@xmath11526&345@xmath11599&40@xmath1151&999@xmath11531&999@xmath11599&41@xmath1151&999@xmath11599&999@xmath11599&40@xmath1151&999@xmath11563&999@xmath11599 + falap&5@xmath1150&443@xmath1159&701@xmath11515&4@xmath1150&50@xmath1152&110@xmath11537&4@xmath1150&221@xmath1155&177@xmath1154&5@xmath1150&439@xmath11510&686@xmath11515&4@xmath1150&239@xmath1156&341@xmath11519 + ica&2@xmath1150&325@xmath1154&&2@xmath1150&34@xmath1150&&2@xmath1150&163@xmath1152&&2@xmath1150&322@xmath1154&&2@xmath1150&174@xmath1152& + sfa&1@xmath1150&408@xmath1159&217@xmath1157&1@xmath1150&42@xmath1151&8@xmath1152&1@xmath1150&204@xmath1154&54@xmath1152&1@xmath1150&405@xmath1159&213@xmath1157&1@xmath1150&218@xmath1155&94@xmath1153 + fa&1@xmath1150&409@xmath1159&212@xmath1157&1@xmath1150&42@xmath1151&4@xmath1151&1@xmath1150&205@xmath1154&53@xmath1152&1@xmath1150&405@xmath1158&208@xmath1156&1@xmath1150&218@xmath1154&90@xmath1153 + pca&0@xmath1150&325@xmath1154&&1@xmath1150&34@xmath1150&&0@xmath1150&163@xmath1152&&0@xmath1150&322@xmath1154&&0@xmath1150&174@xmath1152& +    l*3 > p2.9em*3>p2.9em*3 > p2.9em*3>p2.9em*3 > p2.9em    & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&79@xmath1151&23@xmath1153&2@xmath1150&82@xmath1151&63@xmath1159&16@xmath1153&82@xmath1151&120@xmath11517&61@xmath11515&78@xmath1151&27@xmath1153&2@xmath1151&82@xmath1151&62@xmath1157&16@xmath1153 + rfnn&77@xmath1150&61@xmath1154&6@xmath1151&80@xmath1150&169@xmath1154&36@xmath1152&80@xmath1150&326@xmath1158&135@xmath1156&76@xmath1151&73@xmath1154&9@xmath1152&79@xmath1150&171@xmath1155&37@xmath1152 + dae&67@xmath1150&48@xmath1152&&70@xmath1150&134@xmath1151&&70@xmath1150&260@xmath1152&&67@xmath1150&54@xmath1152&&70@xmath1150&137@xmath1151& + rbm&14@xmath1151&81@xmath1153&&4@xmath1150&266@xmath1153&&4@xmath1150&514@xmath1156&&15@xmath1151&88@xmath1152&&4@xmath1150&270@xmath1153& + fasp&72@xmath1150&233@xmath11532&499@xmath11599&62@xmath1150&999@xmath11543&999@xmath11599&56@xmath1150&999@xmath11599&999@xmath11599&71@xmath1150&320@xmath11534&878@xmath11599&62@xmath1150&999@xmath11549&999@xmath11599 + falap&6@xmath1150&27@xmath1153&202@xmath11517&6@xmath1150&38@xmath1153&756@xmath11533&6@xmath1150&74@xmath1155&999@xmath11583&6@xmath1150&31@xmath1153&274@xmath11523&6@xmath1150&39@xmath1153&778@xmath11534 + ica&3@xmath1152&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1152&0@xmath1150&&3@xmath1151&0@xmath1150& + sfa&1@xmath1150&6@xmath1150&30@xmath1155&1@xmath1150&14@xmath1150&68@xmath1153&1@xmath1150&28@xmath1151&243@xmath1158&1@xmath1150&8@xmath1150&38@xmath1155&1@xmath1150&15@xmath1150&72@xmath1153 + fa&1@xmath1150&6@xmath1150&18@xmath1153&1@xmath1150&14@xmath1150&50@xmath1152&1@xmath1150&28@xmath1151&182@xmath1157&1@xmath1150&8@xmath1150&24@xmath1154&1@xmath1150&15@xmath1150&52@xmath1152 + pca&4@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150&&1@xmath1150&0@xmath1150&&4@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150& + & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&82@xmath1151&120@xmath11516&60@xmath11513&80@xmath1151&18@xmath1152&1@xmath1150&82@xmath1151&61@xmath1157&15@xmath1153&82@xmath1151&122@xmath11513&60@xmath11511&81@xmath1151&68@xmath1159&26@xmath1156 + rfnn&80@xmath1150&329@xmath1157&137@xmath1156&78@xmath1150&49@xmath1153&4@xmath1151&80@xmath1150&165@xmath1154&34@xmath1151&80@xmath1150&325@xmath1157&134@xmath1156&79@xmath1150&185@xmath1155&59@xmath1153 + dae&70@xmath1150&261@xmath1152&&68@xmath1150&39@xmath1152&&70@xmath1150&132@xmath1151&&70@xmath1150&259@xmath1152&&69@xmath1150&147@xmath1152& + rbm&4@xmath1150&517@xmath1156&&12@xmath1151&71@xmath1152&&4@xmath1150&261@xmath1153&&4@xmath1150&512@xmath1155&&7@xmath1151&287@xmath1154& + fasp&56@xmath1151&999@xmath11599&999@xmath11599&73@xmath1150&149@xmath11528&237@xmath11562&62@xmath1150&999@xmath11534&999@xmath11599&56@xmath1150&999@xmath11599&999@xmath11599&63@xmath1150&999@xmath11565&999@xmath11599 + falap&6@xmath1150&74@xmath1156&999@xmath11591&6@xmath1150&22@xmath1153&134@xmath11514&6@xmath1150&37@xmath1152&733@xmath11528&6@xmath1150&73@xmath1156&999@xmath11584&6@xmath1150&46@xmath1154&985@xmath11545 + ica&3@xmath1151&0@xmath1150&&3@xmath1152&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150& + sfa&1@xmath1150&28@xmath1151&247@xmath1158&1@xmath1150&5@xmath1150&21@xmath1155&1@xmath1150&14@xmath1150&64@xmath1152&1@xmath1150&27@xmath1151&240@xmath1157&1@xmath1150&16@xmath1151&114@xmath1155 + fa&1@xmath1150&28@xmath1151&184@xmath1158&1@xmath1150&5@xmath1150&11@xmath1153&1@xmath1150&14@xmath1150&47@xmath1152&1@xmath1150&27@xmath1151&179@xmath1157&1@xmath1150&16@xmath1151&83@xmath1154 + pca&1@xmath1150&0@xmath1150&&4@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150&&1@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150& +    l*3 > p2.9em*3>p2.9em*3 > p2.9em*3>p2.9em*3 >",
    "p2.9em    & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&83@xmath1151&7@xmath1152&0@xmath1151&86@xmath1150&15@xmath1151&3@xmath1151&86@xmath1152&33@xmath11520&18@xmath11523&83@xmath1151&9@xmath1152&1@xmath1150&86@xmath1151&15@xmath1153&4@xmath1151 + rfnn&79@xmath1150&48@xmath1153&4@xmath1151&81@xmath1150&129@xmath1153&21@xmath1151&81@xmath1150&250@xmath1157&80@xmath1154&78@xmath1150&60@xmath1154&6@xmath1151&81@xmath1150&131@xmath1153&22@xmath1151 + dae&68@xmath1150&44@xmath1152&&72@xmath1150&118@xmath1151&&72@xmath1150&229@xmath1152&&68@xmath1150&50@xmath1152&&72@xmath1150&120@xmath1152& + rbm&10@xmath1151&81@xmath1153&&3@xmath1150&265@xmath1153&&3@xmath1150&514@xmath1156&&10@xmath1151&88@xmath1152&&3@xmath1150&270@xmath1154& + fasp&83@xmath1151&233@xmath11532&340@xmath11571&79@xmath1150&999@xmath11543&999@xmath11599&77@xmath1150&999@xmath11599&999@xmath11599&81@xmath1151&320@xmath11534&574@xmath11599&79@xmath1151&999@xmath11549&999@xmath11599 + falap&4@xmath1150&27@xmath1153&295@xmath11525&4@xmath1150&38@xmath1153&791@xmath11541&3@xmath1150&74@xmath1155&999@xmath11591&4@xmath1150&31@xmath1153&394@xmath11531&4@xmath1150&39@xmath1153&817@xmath11539 + ica&3@xmath1152&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1152&0@xmath1150&&3@xmath1151&0@xmath1150& + sfa&1@xmath1150&6@xmath1150&49@xmath1157&1@xmath1150&14@xmath1150&173@xmath1154&1@xmath1150&28@xmath1151&632@xmath11510&1@xmath1150&8@xmath1150&61@xmath1157&1@xmath1150&15@xmath1150&181@xmath1155 + fa&1@xmath1150&6@xmath1150&40@xmath1155&1@xmath1150&14@xmath1150&160@xmath1154&1@xmath1150&28@xmath1151&590@xmath11510&1@xmath1150&8@xmath1150&51@xmath1156&1@xmath1150&15@xmath1150&168@xmath1154 + pca&4@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150&&1@xmath1150&0@xmath1150&&4@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150& + & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&86@xmath1151&30@xmath11513&15@xmath11516&84@xmath1152&5@xmath1153&0@xmath1151&86@xmath1150&14@xmath1151&3@xmath1151&86@xmath1151&30@xmath1158&15@xmath1159&85@xmath1151&17@xmath1156&7@xmath1156 + rfnn&81@xmath1150&251@xmath1156&81@xmath1153&80@xmath1150&37@xmath1153&2@xmath1150&81@xmath1150&126@xmath1153&20@xmath1151&81@xmath1150&248@xmath1156&79@xmath1153&80@xmath1150&142@xmath1154&35@xmath1152 + dae&72@xmath1150&230@xmath1152&&70@xmath1150&36@xmath1152&&72@xmath1150&116@xmath1151&&72@xmath1150&227@xmath1152&&71@xmath1150&130@xmath1152& + rbm&3@xmath1150&516@xmath1156&&8@xmath1151&71@xmath1152&&3@xmath1150&260@xmath1154&&3@xmath1150&511@xmath1155&&5@xmath1150&286@xmath1154& + fasp&77@xmath1150&999@xmath11599&999@xmath11599&84@xmath1150&149@xmath11528&168@xmath11555&80@xmath1150&999@xmath11534&999@xmath11599&77@xmath1151&999@xmath11599&999@xmath11599&80@xmath1150&999@xmath11565&999@xmath11599 + falap&3@xmath1150&74@xmath1156&999@xmath11597&4@xmath1150&22@xmath1153&198@xmath11517&4@xmath1150&37@xmath1152&768@xmath11540&3@xmath1150&73@xmath1156&999@xmath11593&4@xmath1150&46@xmath1154&976@xmath11553 + ica&3@xmath1151&0@xmath1150&&3@xmath1152&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150& + sfa&1@xmath1150&28@xmath1151&640@xmath11511&1@xmath1150&5@xmath1150&34@xmath1156&1@xmath1150&14@xmath1150&164@xmath1153&1@xmath1150&27@xmath1151&625@xmath1159&1@xmath1150&16@xmath1151&285@xmath1157 + fa&1@xmath1150&28@xmath1151&596@xmath11510&1@xmath1150&5@xmath1150&27@xmath1155&1@xmath1150&14@xmath1150&153@xmath1153&1@xmath1150&27@xmath1151&583@xmath1159&1@xmath1150&16@xmath1151&263@xmath1156 + pca&1@xmath1150&0@xmath1150&&4@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150&&1@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150& +",
    "this data sets was generate as described in section [ s_sec : data1 ] , but instead of drawing the remaining components of the spanning outer product vectors from @xmath530 , they were now drawn from @xmath531 .",
    "l*3 > p2.9em*3>p2.9em*3 > p2.9em*3>p2.9em*3",
    "> p2.9em    & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&72@xmath1151&74@xmath1152&11@xmath1151&75@xmath1150&240@xmath1153&72@xmath1152&75@xmath1150&462@xmath1155&260@xmath1156&72@xmath1151&79@xmath1152&12@xmath1151&75@xmath1150&244@xmath1153&75@xmath1152 + rfnn&68@xmath1151&122@xmath1155&32@xmath1154&74@xmath1150&285@xmath1154&97@xmath1153&74@xmath1150&537@xmath1157&331@xmath1158&65@xmath1151&144@xmath1156&48@xmath1156&74@xmath1150&290@xmath1154&102@xmath1154 + dae&61@xmath1150&82@xmath1152&&66@xmath1150&243@xmath1152&&66@xmath1150&461@xmath1154&&60@xmath1150&88@xmath1152&&66@xmath1150&247@xmath1153& + rbm&22@xmath1151&106@xmath1153&&11@xmath1151&301@xmath1153&&10@xmath1151&566@xmath1156&&22@xmath1151&113@xmath1153&&11@xmath1151&308@xmath1154& + fasp&37@xmath1151&469@xmath11538&999@xmath11599&40@xmath1151&999@xmath11550&999@xmath11599&40@xmath1152&999@xmath11599&999@xmath11599&37@xmath1151&610@xmath11544&999@xmath11599&40@xmath1151&999@xmath11558&999@xmath11599 + falap&4@xmath1150&50@xmath1151&392@xmath11566&4@xmath1150&228@xmath1155&135@xmath11513&5@xmath1150&443@xmath1159&406@xmath11518&4@xmath1150&51@xmath1151&477@xmath11563&4@xmath1150&230@xmath1156&147@xmath11518 + ica&2@xmath1150&35@xmath1150&&2@xmath1150&168@xmath1152&&2@xmath1150&327@xmath1154&&2@xmath1150&35@xmath1150&&2@xmath1150&170@xmath1152& + sfa&1@xmath1150&42@xmath1151&26@xmath1153&1@xmath1150&210@xmath1155&61@xmath1152&1@xmath1150&409@xmath1158&220@xmath1156&1@xmath1150&41@xmath1151&32@xmath1154&1@xmath1150&211@xmath1155&63@xmath1152 + fa&1@xmath1150&42@xmath1151&13@xmath1152&1@xmath1150&210@xmath1154&58@xmath1152&1@xmath1150&409@xmath1158&214@xmath1156&1@xmath1150&41@xmath1151&17@xmath1152&1@xmath1150&212@xmath1155&60@xmath1152 + pca&0@xmath1150&35@xmath1150&&0@xmath1150&168@xmath1152&&0@xmath1150&327@xmath1154&&0@xmath1150&35@xmath1150&&0@xmath1150&170@xmath1152& + & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&75@xmath1150&464@xmath1155&264@xmath1156&73@xmath1150&68@xmath1152&9@xmath1151&75@xmath1150&237@xmath1153&69@xmath1151&75@xmath1150&459@xmath1155&257@xmath1156&74@xmath1150&259@xmath1153&114@xmath1153 + rfnn&74@xmath1150&541@xmath1156&336@xmath1158&71@xmath1151&106@xmath1154&23@xmath1153&74@xmath1150&279@xmath1153&91@xmath1152&75@xmath1150&533@xmath1156&325@xmath1158&72@xmath1151&315@xmath1155&154@xmath1155 + dae&66@xmath1150&465@xmath1154&&62@xmath1150&75@xmath1152&&66@xmath1150&238@xmath1152&&66@xmath1150&458@xmath1154&&64@xmath1150&262@xmath1153& + rbm&10@xmath1151&570@xmath1156&&20@xmath1151&97@xmath1153&&11@xmath1151&294@xmath1153&&10@xmath1151&562@xmath1155&&14@xmath1151&324@xmath1154& + fasp&41@xmath1151&999@xmath11599&999@xmath11599&38@xmath1151&335@xmath11532&999@xmath11599&41@xmath1151&999@xmath11540&999@xmath11599&41@xmath1151&999@xmath11599&999@xmath11599&39@xmath1151&999@xmath11569&999@xmath11599 + falap&5@xmath1150&447@xmath1159&413@xmath11519&4@xmath1150&49@xmath1151&292@xmath11557&4@xmath1150&227@xmath1155&123@xmath11511&5@xmath1150&443@xmath1159&401@xmath11517&4@xmath1150&241@xmath1155&310@xmath11531 + ica&2@xmath1150&329@xmath1154&&2@xmath1150&35@xmath1150&&2@xmath1150&167@xmath1152&&2@xmath1150&325@xmath1154&&2@xmath1150&177@xmath1152& + sfa&1@xmath1150&412@xmath1158&223@xmath1157&1@xmath1150&42@xmath1151&19@xmath1153&1@xmath1150&209@xmath1154&59@xmath1152&1@xmath1150&408@xmath1159&218@xmath1157&1@xmath1150&221@xmath1155&102@xmath1154 + fa&1@xmath1150&412@xmath1158&217@xmath1157&1@xmath1150&42@xmath1151&10@xmath1151&1@xmath1150&209@xmath1154&57@xmath1152&1@xmath1150&409@xmath1159&213@xmath1157&1@xmath1150&221@xmath1155&95@xmath1153 + pca&0@xmath1150&329@xmath1154&&0@xmath1150&35@xmath1150&&0@xmath1150&167@xmath1152&&0@xmath1150&325@xmath1154&&0@xmath1150&177@xmath1152& +    l*3 > p2.9em*3>p2.9em*3 > p2.9em*3>p2.9em*3 > p2.9em    & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&76@xmath1151&34@xmath1153&4@xmath1151&82@xmath1151&67@xmath1158&18@xmath1153&82@xmath1151&124@xmath11516&63@xmath11512&75@xmath1151&38@xmath1153&5@xmath1151&82@xmath1151&69@xmath11510&19@xmath1155 + rfnn&71@xmath1151&110@xmath1157&25@xmath1154&79@xmath1150&180@xmath1155&42@xmath1152&80@xmath1150&331@xmath1158&139@xmath1157&65@xmath1152&143@xmath1159&47@xmath1158&79@xmath1150&185@xmath1155&45@xmath1153 + dae&63@xmath1150&66@xmath1152&&70@xmath1150&142@xmath1152&&70@xmath1150&264@xmath1153&&62@xmath1150&73@xmath1152&&70@xmath1150&146@xmath1152& + rbm&12@xmath1151&100@xmath1153&&5@xmath1150&282@xmath1154&&4@xmath1150&522@xmath1156&&12@xmath1151&106@xmath1153&&5@xmath1151&288@xmath1154& + fasp&71@xmath1150&474@xmath11538&999@xmath11599&62@xmath1150&999@xmath11553&999@xmath11599&56@xmath1151&999@xmath11599&999@xmath11599&70@xmath1150&616@xmath11544&999@xmath11599&62@xmath1150&999@xmath11560&999@xmath11599 + falap&6@xmath1150&21@xmath1152&425@xmath11528&6@xmath1150&40@xmath1152&827@xmath11535&6@xmath1150&75@xmath1156&999@xmath11599&6@xmath1150&23@xmath1152&523@xmath11532&6@xmath1150&42@xmath1153&865@xmath11543 + ica&3@xmath1152&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1152&0@xmath1150&&3@xmath1151&0@xmath1150& + sfa&1@xmath1150&10@xmath1150&71@xmath1157&1@xmath1150&15@xmath1150&84@xmath1154&1@xmath1150&28@xmath1151&254@xmath1158&1@xmath1150&12@xmath1150&87@xmath1158&1@xmath1150&16@xmath1150&92@xmath1155 + fa&1@xmath1150&10@xmath1150&48@xmath1155&1@xmath1150&15@xmath1150&59@xmath1153&1@xmath1150&28@xmath1151&189@xmath1157&1@xmath1150&12@xmath1151&61@xmath1156&1@xmath1150&16@xmath1150&64@xmath1153 + pca&4@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150&&1@xmath1150&0@xmath1150&&3@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150& + & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&82@xmath1151&127@xmath11517&65@xmath11514&77@xmath1151&30@xmath1153&3@xmath1151&82@xmath1151&64@xmath1158&17@xmath1154&82@xmath1151&123@xmath11515&62@xmath11513&80@xmath1151&75@xmath1159&28@xmath1156 + rfnn&80@xmath1150&334@xmath1158&141@xmath1157&74@xmath1151&86@xmath1154&14@xmath1152&79@xmath1150&174@xmath1154&39@xmath1152&80@xmath1150&329@xmath1157&137@xmath1156&76@xmath1151&208@xmath1156&70@xmath1155 + dae&70@xmath1150&266@xmath1152&&64@xmath1150&57@xmath1152&&70@xmath1150&138@xmath1151&&70@xmath1150&262@xmath1152&&68@xmath1150&157@xmath1152& + rbm&4@xmath1150&527@xmath1156&&11@xmath1151&92@xmath1152&&4@xmath1150&274@xmath1154&&4@xmath1150&518@xmath1156&&7@xmath1151&301@xmath1154& + fasp&56@xmath1150&999@xmath11599&999@xmath11599&71@xmath1150&338@xmath11533&999@xmath11599&62@xmath1151&999@xmath11542&999@xmath11599&56@xmath1151&999@xmath11599&999@xmath11599&63@xmath1150&999@xmath11574&999@xmath11599 + falap&6@xmath1150&75@xmath1156&999@xmath11589&6@xmath1150&18@xmath1152&337@xmath11524&6@xmath1150&40@xmath1153&793@xmath11537&6@xmath1150&74@xmath1156&999@xmath11589&6@xmath1150&45@xmath1153&999@xmath11553 + ica&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150& + sfa&1@xmath1150&28@xmath1151&260@xmath1159&1@xmath1150&8@xmath1150&52@xmath1157&1@xmath1150&15@xmath1150&76@xmath1153&1@xmath1150&28@xmath1151&248@xmath1157&1@xmath1150&18@xmath1151&136@xmath1156 + fa&1@xmath1150&28@xmath1151&193@xmath1158&1@xmath1150&8@xmath1150&33@xmath1155&1@xmath1150&15@xmath1150&54@xmath1152&1@xmath1150&28@xmath1151&185@xmath1156&1@xmath1150&18@xmath1151&99@xmath1155 + pca&1@xmath1150&0@xmath1150&&4@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150&&1@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150& +    l*3 > p2.9em*3>p2.9em*3 > p2.9em*3>p2.9em*3 > p2.9em    & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&81@xmath1151&12@xmath1152&1@xmath1151&86@xmath1150&16@xmath1151&4@xmath1151&86@xmath1150&29@xmath1154&15@xmath1155&80@xmath1151&15@xmath1155&2@xmath1152&86@xmath1151&17@xmath1155&5@xmath1153 + rfnn&72@xmath1151&100@xmath1158&19@xmath1154&80@xmath1150&137@xmath1154&24@xmath1151&81@xmath1150&254@xmath1156&83@xmath1154&66@xmath1150&113@xmath1153&52@xmath1155&80@xmath1150&141@xmath1154&26@xmath1152 + dae&64@xmath1150&62@xmath1152&&71@xmath1150&125@xmath1152&&72@xmath1150&232@xmath1152&&63@xmath1150&69@xmath1152&&71@xmath1150&129@xmath1152& + rbm&8@xmath1150&101@xmath1153&&4@xmath1150&282@xmath1154&&3@xmath1150&521@xmath1156&&8@xmath1150&106@xmath1153&&4@xmath1150&289@xmath1154& + fasp&81@xmath1151&474@xmath11538&999@xmath11599&79@xmath1150&999@xmath11553&999@xmath11599&77@xmath1151&999@xmath11599&999@xmath11599&80@xmath1151&616@xmath11544&999@xmath11599&79@xmath1151&999@xmath11560&999@xmath11599 + falap&4@xmath1150&21@xmath1152&607@xmath11534&4@xmath1150&40@xmath1152&879@xmath11540&3@xmath1150&75@xmath1156&999@xmath11596&4@xmath1150&23@xmath1152&749@xmath11542&4@xmath1150&42@xmath1153&926@xmath11545 + ica&3@xmath1152&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1152&0@xmath1150&&3@xmath1151&0@xmath1150& + sfa&1@xmath1150&10@xmath1150&103@xmath1159&1@xmath1150&15@xmath1150&204@xmath1157&1@xmath1150&28@xmath1151&656@xmath11512&1@xmath1150&12@xmath1150&126@xmath11510&1@xmath1150&16@xmath1150&220@xmath1158 + fa&1@xmath1150&10@xmath1150&87@xmath1158&1@xmath1150&15@xmath1150&187@xmath1155&1@xmath1150&28@xmath1151&611@xmath11511&1@xmath1150&12@xmath1151&108@xmath1159&1@xmath1150&16@xmath1150&200@xmath1156 + pca&4@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150&&1@xmath1150&0@xmath1150&&3@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150& + & & & & & + ( r)2 - 4 ( lr)5 - 7 ( lr)8 - 10 ( lr)11 - 13 ( l)14 - 16 & sp & er & co & sp & er & co & sp & er & co & sp & er & co & sp & er & co + rfn&86@xmath1151&29@xmath1157&15@xmath1156&82@xmath1151&10@xmath1153&1@xmath1151&86@xmath1151&17@xmath11510&5@xmath1159&86@xmath1151&31@xmath11519&16@xmath11513&84@xmath1151&20@xmath1156&7@xmath1154 + rfnn&81@xmath1150&255@xmath1156&84@xmath1153&76@xmath1151&74@xmath1155&9@xmath1152&81@xmath1150&133@xmath1153&23@xmath1151&81@xmath1150&250@xmath1157&81@xmath1154&77@xmath1150&162@xmath1155&45@xmath1153 + dae&72@xmath1150&234@xmath1152&&65@xmath1150&53@xmath1152&&72@xmath1150&122@xmath1151&&72@xmath1150&230@xmath1152&&69@xmath1150&140@xmath1152& + rbm&3@xmath1150&525@xmath1156&&8@xmath1150&93@xmath1153&&3@xmath1150&273@xmath1154&&3@xmath1150&517@xmath1156&&5@xmath1150&301@xmath1154& + fasp&77@xmath1151&999@xmath11599&999@xmath11599&81@xmath1151&338@xmath11533&673@xmath11599&79@xmath1150&999@xmath11542&999@xmath11599&77@xmath1151&999@xmath11599&999@xmath11599&79@xmath1151&999@xmath11574&999@xmath11599 + falap&3@xmath1150&75@xmath1156&999@xmath11594&4@xmath1150&18@xmath1152&479@xmath11531&4@xmath1150&40@xmath1153&831@xmath11543&3@xmath1150&74@xmath1156&999@xmath11595&4@xmath1150&45@xmath1153&999@xmath11558 + ica&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150&&3@xmath1151&0@xmath1150& + sfa&1@xmath1150&28@xmath1151&668@xmath11512&1@xmath1150&8@xmath1150&78@xmath1158&1@xmath1150&15@xmath1150&188@xmath1155&1@xmath1150&28@xmath1151&644@xmath1159&1@xmath1150&18@xmath1151&321@xmath1159 + fa&1@xmath1150&28@xmath1151&622@xmath11511&1@xmath1150&8@xmath1150&64@xmath1157&1@xmath1150&15@xmath1150&173@xmath1154&1@xmath1150&28@xmath1151&599@xmath1159&1@xmath1150&18@xmath1151&294@xmath1158 + pca&1@xmath1150&0@xmath1150&&4@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150&&1@xmath1150&0@xmath1150&&2@xmath1150&0@xmath1150& +",
    "we assess the performance of rfn _ first layer _ pretraining on _ cifar-10 _ and _ cifar-100 _ for three deep convolutional network architectures : ( i ) the alexnet @xcite , ( ii ) deeply supervised networks ( dsn ) @xcite , and ( iii ) our 5-convolution - network - in - network ( 5c - nin ) .    both cifar datasets contain 60k 32x32 rgb - color images , which were divided into 50k train and 10k test sets , split between 10 ( cifar10 ) and 100 ( cifar100 ) categories .",
    "both datasets are preprocessed by global contrast normalization and zca whitening @xcite .",
    "additionally , the datasets were augmented by padding the images with four zero pixels at all borders . for data",
    "augmentation , at the beginning of every epoch , images in the training set were distorted by random translation and random flipping in horizontal and vertical directions . for the alexnet , we neither preprocessed nor augmented the datasets",
    ".    inspired by the network in network approach @xcite , we constructed a 5-convolution - network - in - network ( 5c - nin ) architecture with five convolutional layers , each followed by a 2x2 max - pooling layer ( stride 1 ) and a multilayer perceptron ( mlp ) convolutional layer .",
    "relus were used for the convolutional layers and dropout for regularization . for weight initialization , learning rates , and learning policies we used same strategy as in the alexnet @xcite .",
    "the networks were trained using mini - batches of size 100 and 128 for 5c - nin and alexnet , respectively .    for rfn pretraining",
    ", we randomly extracted 5x5 patches from the training data to construct 192 filters for dsn and 5c - nin while 32 for alexnet .",
    "these filters constitute the first convolutional layer of each network which is then trained using default setting . for assessing the improvement by rfns , we repeated training with randomly initialized weights in the first layer .",
    "the results are presented in tab .",
    "[ tab : tab_res2 ] . for comparison ,",
    "the lower panel of the table reports the performance of the currently top performing networks : network in network ( nin , @xcite ) , maxout networks ( mn , @xcite ) and deepcnin @xcite .",
    "_ in all cases pretraining with rfns decreases the test error rate . _    * 1>p3.8em*4>p1.5em*1>p3.5em dataset & & & + ( lr)2 - 3(rl)4 - 5 & org & rfn & org & rfn & augmented + alexnet & 18.21 & 18.04 & 46.18 & 45.80 & + dsn & 7.97 & 7.74 & 34.57 & - & @xmath532 + 5c - nin & 7.81 & 7.63 & 29.96 & 29.75 & @xmath532 + nin & 8.81 & - & 35.68 & - & @xmath532 + mn & 9.38 & - & 38.57 & - & @xmath532 + deepcnin & 6.28 & - & 24.30 & - & @xmath532 +"
  ],
  "abstract_text": [
    "<S> we propose rectified factor networks ( rfns ) to efficiently construct very sparse , non - linear , high - dimensional representations of the input . </S>",
    "<S> rfn models identify rare and small events in the input , have a low interference between code units , have a small reconstruction error , and explain the data covariance structure . </S>",
    "<S> rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non - negative and normalized posterior means . </S>",
    "<S> we proof convergence and correctness of the rfn learning algorithm .    on benchmarks , </S>",
    "<S> rfns are compared to other unsupervised methods like autoencoders , rbms , factor analysis , ica , and pca . </S>",
    "<S> in contrast to previous sparse coding methods , rfns yield sparser codes , capture the data s covariance structure more precisely , and have a significantly smaller reconstruction error . </S>",
    "<S> we test rfns as pretraining technique for deep networks on different vision datasets , where rfns were superior to rbms and autoencoders . on gene expression data from two pharmaceutical drug discovery studies </S>",
    "<S> , rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods . </S>"
  ]
}