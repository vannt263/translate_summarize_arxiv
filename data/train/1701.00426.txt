{
  "article_text": [
    "the use of processors based on multi- and many - core architectures is a common option in high performance computing ( hpc ) .",
    "several variants of these processors exist , differing mainly in the number and architecture of the cores integrated in a single silicon die .",
    "conventional cpus integrate tens of fat cores sharing a large on - chip cache .",
    "fat cores include several levels of caches and complex control structures , able to perform hardware optimization techniques ( branch - speculation , instruction scheduling , register renaming , etc ) .",
    "vector instructions are also supported by these cores , with a moderate level of data parallelism : 2 to 4 vector elements are processed by one vector instruction .",
    "this architecture is reasonably efficient for many type of regular and non - regular applications and delivers a level of performance of the order of hundreds of gigaflops per processor .    on the other side of the spectrum",
    "we have graphics processor units ( gpu ) , available as accelerator boards attached to conventional cpus .",
    "gpus integrate thousands of slim cores able to efficiently support regular streams of computation , and deliver performances of the order of several teraflops .",
    "gpus are extremely aggressive in terms of data - parallelism , implementing vector units with large vector sizes ( 16 and 32 words are presently available options ) .",
    "midway between these two architectures , we have the intel _ many integrated cores _",
    "( mic ) architecture based on several tens of slim cores . in this case , cores are similar to their fat counterparts , but their design has been simplified removing many hardware control structures ( instruction scheduler , register renaming , etc ) and adopting wider vector units , able to process up to 4 or 8 vector elements in parallel .",
    "large scale computing centers today have not reached a common consensus on the `` best '' processor option for hpc systems , also because system choices are driven not only by application performances , but also by cost of ownership and energy aspects which are becoming increasingly critical parameters@xcite .",
    "several computing centers do adopt machines based on gpus , but other ones prefer to stay on more traditional cpus , offering a lower peak performance , but better computing efficiency for a wider range of applications .",
    "in this scenario , the development of applications would greatly benefit from the availability of a unique code version , written in an appropriate programming framework , able to offer portability , in terms of code and performance , across several present and possibly future state - of - the - art processor architectures .",
    "a single code version , portable across several architectures , is of great convenience in particular for scientific applications , where code changes and development iterations are very frequent , so keeping several architecture - specific code versions up - to - date is a tedious and error prone effort@xcite .",
    "directives based programming models are going exactly in this direction , abstracting parallel programming to a descriptive level as opposite to a prescriptive level , where programmers must specify how the code should be mapped onto the target machine .",
    "openmp@xcite and openacc@xcite are among the most common such programming models , already used by a wide scientific community .",
    "both are based on directives : openmp was introduced to manage parallelism on traditional multi - core cpus , while openacc is mainly used to target gpus ( although designed to be architecture agnostic)@xcite .",
    "these two frameworks are in fact converging and extending their scope to cover a large subset of hpc applications and architectures : openmp version 4 has been designed to support also accelerators , while compilers supporting openacc ( such as the pgi@xcite ) are starting to use directives also to target multi - core cpus .    in this work",
    "we describe the implementation of a lattice qcd ( lqcd ) monte carlo code designed to be portable and efficient across several architectures .",
    "lqcd simulations represent a typical and well known hpc grand challenge , with physics results strongly limited by available computational resources@xcite ; over the years , several generations of parallel machines , optimized for lqcd , have been developed@xcite , while the development of lqcd codes running on many core architectures , in particular gpus , has seen large efforts in the last 10 years@xcite .",
    "our goal is to have just one code able to run on several processors without any major code changes , and possibly to have roughly the same level of efficiency , looking for an acceptable trade - off between portability and efficiency@xcite . as a programming model we have selected openacc , as it currently has a wider compiler support , in particular targeting nvidia gpus , which are widely used in hpc clusters and commonly used for scientific computations .",
    "openacc has been successfully used to port and run other scientific codes , such as lattice boltzmann applications@xcite in computational fluid - dynamics , showing a good level of code and performance portability on several architectures .",
    "the migration of our code to openmp4 , if needed , as soon as compiler support becomes more mature , is expected to be a simple additional effort .",
    "we have developed a code with all key features for a state - of - the - art simulations of qcd with dynamical fermions . using this code as a user test case",
    ", we assess : i ) if it is possible to write the code in such a way that the most computationally critical kernels can be executed on accelerators , as in previous cuda implementations@xcite ; ii ) how many of the presently available multi and many - core architectures can be really used ; iii ) how efficient are these codes , and in particular what is the price to pay in terms of performance with respect to a code written and optimized for a specific architecture ( e.g. , using cuda for gpus ) .",
    "we believe that our work is a non trivial step forward in the development of a fully portable production - grade lqcd monte carlo code , using the openacc programming model .",
    "an earlier paper@xcite presented tests of selected portions of an openacc lqcd implementation on fermi and k20 nvidia gpus , comparing performances with an openmp implementation for cpus .",
    "similarly , in a preliminary study@xcite , we compared the performance of selected kernels of a full simulation , written in openacc , with an equivalent cuda implementation , on a k20 nvidia gpu . in this work , we extend the use of openacc in several new directions : i ) we show the portability of a complete implementation across several architectures ; ii ) we show performance figures for the same openacc code on a variety of multi and many - core processors , including the most recent gpus like the k80 and the recently released p100 ; iii ) we compare results with a previous implementation of the same full application written in cuda@xcite .    the remainder of the paper is organized as follows : in section  [ simalg ] we give a brief introduction to lqcd and to the main computational aspects of our application ; in section  [ hpctrend ] we highlight recent developments in hpc hardware and programming tools ; in section  [ implementation ] we describe the openacc implementation of our code ; in section  [ results ] we analyze our results ; finally , section  [ conclusions ] , contains our concluding remarks .",
    "quantum chromodynamics ( qcd ) is the quantum field theory that describes strong interactions in the standard model of particle physics .",
    "it is a non - abelian gauge theory , based on the @xmath0 group ( the `` color '' group ) , describing the interactions of six different species ( `` flavors '' ) of quarks , mediated by @xmath1 vector bosons , the `` gluons '' .    in principle qcd",
    "is not different from the theory that describes other sectors of the standard model ( i.e. the electroweak interaction ) ; however , strong interactions are indeed strong , i.e. the coupling constant of qcd is generically not small .",
    "asymptotic freedom ensures that the coupling constant gets smaller and smaller as the energy scale increases ( a summary of experimental results is available in  9.4 of the particle data group review@xcite ) , but a wealth of interesting phenomena take place for energies well below the perturbative regime ; a systematically improvable computational scheme , that does not rely on the smallness of the coupling constant , is needed to study this phenomenology from first principles .",
    "lattice qcd provides such a scheme .",
    "lqcd uses the feynman path - integral quantization and approximates the infinite dimensional path - integral by a finite dimensional integral : continuous space - time is replaced by a finite lattice of sizes @xmath2 , @xmath3 , @xmath4 , @xmath5 and lattice spacing @xmath6 . in order to maintain gauge invariance , the variables @xmath7 associated with the gauge fields are elements of the @xmath0 group and live on the links of the lattice ; the quark fields @xmath8 live on the lattice sites and transform under the gauge group as @xmath9dimensional complex vectors@xcite .",
    "the fundamental problem of lqcd is the evaluation of expectation values of given functions of the fields , @xmath10 $ ] , that is integrals of the form @xmath11\\det(m[u])e^{-s_g[u]}\\ , \\quad z = \\int \\mathscr{d}u \\det(m[u])e^{-s_g[u]}\\ ; \\end{aligned}\\ ] ] the exponent @xmath12 is the discretization of the action of the gauge fields ( usually written as a sum of traces of products of @xmath7 along closed loops ) and @xmath13 describes the gluon - quark interaction . here , @xmath14 $ ] is a large and sparse structured matrix ( i.e. containing both space - time and color indexes ) which is the discretization of the continuum fermion operator @xmath15 where @xmath16 is the fermion mass , multiplying the identity operator , and @xmath17 is the dirac operator , which is constructed in terms of covariant derivatives . the integral in @xmath18 extends over all the @xmath7 variables on the lattice using the haar measure of @xmath0 .",
    "( [ eq : pathint ] ) refers to a single quark species ( flavor ) ; in the realistic case of multiple flavors , @xmath19 , @xmath20 , @xmath21 , @xmath22 , @xmath23 and ordered by increasing quark mass . in a realistic simulation ,",
    "one usually takes into account the first 3 ( or 4 , at most ) flavors , since the heaviest species give a negligible contribution to the low - energy dynamics of the theory .",
    "] , one has to introduce a separate determinant for each flavor .",
    "this formulation makes contact with a standard problem in statistical mechanics : importance sampling of the distribution @xmath24)e^{-s_g[u]}$ ] .",
    "what is non - standard is the form of this distribution and in particular the presence of the determinant .",
    "the best strategy devised so far to cope with this problem is to introduce the so called pseudofermion fields@xcite @xmath25 and rewrite the integral as follows : @xmath26\\det(m[u ] ) e^{-s_g[u]}\\propto   \\int \\mathscr{d}u \\mathscr{d}\\phi \\",
    ", o[u]\\exp\\left(-s_g[u]-\\phi^{\\dag } m[u]^{-1}\\phi\\right)\\ ; \\end{aligned}\\ ] ] the action is still a non - local function of the field variables , but the computational burden required for the solution of a large sparse linear system is much lower than the one needed for the computation of its determinant .",
    "the explicit form of @xmath27 $ ] and @xmath14 $ ] is not fully determined , as these functions only have the constraint to go over to the correct continuum limit as the lattice spacing goes to zero .",
    "much in the same way as several discretization schemes exist for the numerical solution of a partial differential equation , several discretization schemes of the qcd action exist . in this paper",
    "we consider a specific state - of - the - art discretization , the tree - level symanzik improved action@xcite for the gauge part and the stout - improved@xcite `` staggered '' action for the fermion part .",
    "staggered actions have a residual degeneracy , that has to be removed by taking the @xmath28th root of the determinant .",
    "so , becomes in the staggered case @xmath29\\exp\\big(-s_g[u]-\\phi^{\\dag } m[u]^{-1/4}\\phi\\big)\\ .\\end{aligned}\\ ] ]      the physical system that one would like to simulate by the lattice box has a characteristic physical length @xmath30 , which is of the order of @xmath31 m. in order to reduce systematic effects related to discretization and to the finite box size , one would like that , at the same time , the lattice spacing @xmath6 be much smaller , and the box size @xmath32 much larger than @xmath30 , i.e. @xmath33 . making the reasonable approximation that @xmath34 translates into one order of magnitude means that the number of sites in each direction should be @xmath35 ; the corresponding fermion matrix , considering also internal ( e.g. , color ) indexes , has a dimension slightly exceeding @xmath36 ; note that it is a sparse matrix , since the discretization of the dirac operator @xmath17 connects only neighbor lattice sites .",
    "in finite temperature simulations the size of the lattice is typically smaller , since in that case the temporal direction is shortened and equal to the inverse of the temperature , @xmath37 .",
    "the most computationally demanding task in the typical lqcd algorithm is the solution of a linear system involving the fermion matrix @xmath38 .",
    "the numerical difficulty of this problem is fixed by the condition number of @xmath38 , hence , since the highest eigenvalue is typically @xmath39 , by the smallest eigenvalue of @xmath38 . here",
    "the physical properties of qcd play a significant role : the eigenvalues of the dirac operator are dense around zero , a property related to the so - called _ spontaneous breaking of chiral symmetry _ , so the smallest eigenvalue is set by @xmath40 where @xmath16 is quark mass . since nature provides us with two quark flavors ( @xmath41 and @xmath19 quarks )",
    "whose mass is significantly lower ( by two orders of magnitude ) than other energy scales of the theory , typical values of @xmath40 are typically very small , resulting in a bad condition number ( @xmath42 being a typical value ) . also regarding this aspect ,",
    "the situation becomes better when one is interested in the regime of very high temperatures , since in that case the spontaneous breaking of chiral symmetry disappears , the minimum eigenvalue of @xmath17 is non - zero , and the condition number significantly improves .",
    "in lqcd , the usual local updates adopted in statistical mechanics scale badly with the volume , as the action of is non - local .",
    "this problem is partly solved by the hybrid monte carlo ( hmc ) algorithm@xcite ; in hmc we associate fake conjugate momenta  entering quadratically in the action  to each degree of freedom of the system . for an @xmath0 gauge theory , momenta conjugate to the link variable",
    "are again @xmath43 matrices @xmath44 associated to each link of the lattice , this time living in the group algebra ( hence hermitian and traceless ) .",
    "( [ eq : rooting ] ) is rewritten as @xmath45\\exp\\left(-\\frac{1}{2 } h^2 -s_g[u]-\\phi^{\\dag } m[u]^{-1/4}\\phi\\right)\\ , \\end{aligned}\\ ] ] where the momenta term is a shorthand to indicate the sum of @xmath46 over the whole lattice .",
    "the update then proceeds as follows :",
    "random gaussian initial momenta @xmath47 and pseudofermions @xmath25 are generated ; 2 .",
    "starting from the initial configuration and momenta @xmath48 , a new state @xmath49 is generated by integrating the equations of motion ; 3 .",
    "the new state @xmath49 is accepted with probability @xmath50 , where @xmath51 is the change of the total ( i.e. included the momenta ) action .",
    "step 2 is an unphysical evolution in a fictitious time and , under mild conditions on the numerical integration of the equations of motion , it can be shown to satisfy the detailed balance principle@xcite , so it provides a stochastically exact way to estimate the integral in .",
    "the more time consuming steps of the update are the ones that involve the non - local term in the exponent of .",
    "in particular , the most time consuming single step of the whole algorithm is the solution of a linear system @xmath52\\varphi = b \\ , .\\ ] ] this calculation is needed to compute the forces appearing in the equations of motion and also to evaluate @xmath51 , and one usually resorts to krylov solvers . in the case of staggered fermions , corresponding to , it is customary to use the so - called rational hmc ( rhmc ) algorithm@xcite , in which the algebraic matrix function appearing in is approximated to machine precision by a rational function . in this case one replaces by @xmath53 equations ( @xmath53 is the order of the approximation adopted ) @xmath54+\\sigma_i)\\varphi_i = b\\ , \\quad i\\in\\{1,\\ldots , r\\}\\ , \\ ] ] where the real numbers @xmath55 are the poles of the rational approximations .",
    "these equations can again be solved by using krylov methods : by exploiting the shift - invariance of the krylov subspace it is possible to write efficient algorithms that solve all the equations appearing in ( [ eq : shlineq ] ) at the same time , using at each iteration only one matrix - vector product@xcite .    for most of the discretizations adopted in qcd ( and in particular for the one we use ) , the matrix @xmath14 $ ] can be written in block form @xmath56 matrices @xmath57 and @xmath58 connect only even and odd sites .",
    "it is thus convenient to use an even / odd preconditioning@xcite ; in this case , is replaced by : @xmath59 @xmath60 is defined only on even sites and the matrix is positive definite ( because of ) , so we can use the simplest of the krylov solvers : the conjugate gradient ( or its shifted counterpart ) .    over the years",
    ", many improvements of this basic scheme have been developed ; these are instrumental in reducing the computational cost of actual simulations but",
    "their implementation is straightforward , once the basic steps of the `` naive '' code are ready .",
    "for this reason we will not discuss in the following the details of multi - step integrators@xcite , improved integrators@xcite , multiple pseudofermions@xcite or the use of different rational approximations and stopping residuals in different parts of the hmc@xcite , even if our code uses all these improvements .",
    "our most important data structures are the collection of all gauge variables @xmath7 ( elements of the group of @xmath0 matrices , one for each link of the four - dimensional lattice ) and of the pseudofermion fields @xmath61 ( @xmath9dimensional complex vectors , one for each even site of the lattice when using the even / odd preconditioning ) .",
    "we also need many derived and temporary data structures , such as :    1 .",
    "the configurations corresponding to different stout levels ( @xmath62 , again @xmath0 matrices ) , used in the computation of the force ( typically less than five stout levels are used ) and the momenta configuration ( which are @xmath43 hermitian traceless matrices ) ; 2 .   some auxiliary structures needed to compute the force acting on the gauge variables , like the so called `` staples '' @xmath63 and the @xmath64 and @xmath65 matrices@xcite ; @xmath63 and @xmath64 are generic @xmath66 complex matrices and @xmath65 are @xmath66 hermitian traceless matrices ; 3 .   the solutions @xmath67 of and some auxiliary pseudofermion - like structure needed in the krylov solver .    at the lowest level ,",
    "almost all functions repeatedly multiply two @xmath66 complex matrices ( e.g. , in the update of the gauge part ) , or a @xmath66 complex matrix and a @xmath9dimensional complex vector ( e.g. , in the krylov solver ) or compute dot products and linear combinations of complex @xmath9vectors .",
    "all these operations have low computational intensity , so it is convenient to compress as much as possible all basic structures by exploiting their algebraic properties .",
    "the prototypical example is @xmath7 : one only stores the first two rows of the matrix and recovers the third one on the fly as the complex conjugate of the wedge product of the first two rows@xcite .",
    "this overhead is negligible with respect to the gain induced , at least for gpus , by the reduction of the memory transfer@xcite real numbers , but in this case the reconstruction algorithm presents some instabilities@xcite . ] .    at a higher level",
    "the single most time consuming function is the krylov solver , which may take @xmath68 of the total execution time of a realistic simulation ( depending e.g. on the value of the temperature ) and consists basically of repeated applications iterations are needed to reach convergence , depending on the temperature .",
    "] of the @xmath57 and @xmath58 matrices defined in , together with some linear algebra on the pseudofermion vectors ( basically _ zaxpy_-like functions ) .",
    "an efficient implementation of @xmath58 and @xmath57 multiplies is then of paramount importance , the effectiveness of this operation being often taken as a key figure of merit in the lqcd community .",
    "there is a clear trend in high - performance computing ( hpc ) to adopt multi - core processors and accelerator - based platforms .",
    "typical hpc systems today are clusters of computing nodes interconnected by fast low - latency communication networks , e.g. infiniband .",
    "each node typically has two standard multi - core cpus , each attached to one or more accelerators , either graphic processing unit ( gpu ) or many - core systems .",
    "recent development trends see a common path to performance for cpus and accelerators , based on an increasing number of independent cores and on wider vector processing facilities within each core . in this common landscape , accelerators offer additional computing performance and better energy efficiency by further pushing the granularity of their data paths and using a larger fraction of their transistors for computational data paths , as opposed to control and memory structures . as a consequence ,",
    "even if cpus are more tolerant for intrinsically unstructured and irregular codes , in both class of processors computing efficiency goes through careful exploitation of the parallelism available in the target applications combined with a regular and ( almost ) branch - free scheduling of operations .",
    "this remark supports our attempt to write just one lqcd code which is not only portable , but also efficiency - portable across a large number of state - of - the - art cpus and accelerators . in this paper",
    "we consider intel multi - core cpus , and nvidia and amd gpus , commonly used today by many scientific hpc communities .",
    "[ tab : architecture ] summarizes some key features of the systems we have used@xcite , that we describe very briefly in the following .",
    "intel xeon - e5 architectures are conventional x86 multi - core architectures .",
    "we have used two generations of these processors , differing for the number of cores and for the amount of integrated last - level cache .",
    "performances in both cases rely on the ability of the application to run on all cores and to use 256-bit vector instructions .",
    "nvidia gpus are also multi - core processors .",
    "a gpu hosts several streaming multiprocessors ( sm ) , which in turn include several ( depending on the specific architecture ) compute units called cuda - cores . at each",
    "clock - cycle sms execute multiple warps , i.e. groups of 32 instructions , belonging to different cuda - threads , which are executed in _ single instructions multiple threads _ ( simt ) fashion .",
    "simt is similar to simd execution but more flexible , e.g. different cuda - threads of a simt - group are allowed to take different branches of the code , although at a performance penalty .",
    "each cuda - thread has access to its copy of registers and context switches are almost at zero cost .",
    "this structure has remained stable across several generations with minor improvements .",
    "the nvidia k80 has two gk210 gpus ; each gpu has 13 _ next generation _ streaming multiprocessor , ( smx ) running at a base frequency of @xmath69 mhz that can be increased to @xmath70 mhz under specific condition of work - load and power .",
    "the corresponding aggregate peak performance of the two gk210 units is then @xmath71 and @xmath72 tflops in double precision .",
    "the peak memory bandwidth is @xmath73  gb / s considerably higher compared to that of e5-xeon cpus .",
    "the gp100 gpu , based on the pascal architecture , has recently become available .",
    "it has 56 streaming processors running at base - frequency of @xmath74 that can be increased to @xmath75  ghz , delivering a peak double - precision performance of @xmath76 and @xmath77  tflops .",
    "peak memory bandwidth has been increased to @xmath78  gb / s .",
    "the amd gpus are conceptually similar to nvidia gpu",
    ". the amd firepro w9100 has 44 processing units , each one with 64 compute units ( stream processors ) , running at @xmath79  mhz .",
    "this board delivers a peak double - precision performance of @xmath80  tflops , and has a peak memory - bandwidth of @xmath81  gb / s .",
    "native programming models , commonly used for the systems shown in  [ tab : architecture ] , differ in several aspects .    for xeon - e5 cpus ,",
    "the most common models are openmp and openmpi .",
    "both models support core - parallelism , running one thread or one mpi process per logical core .",
    "moreover openmp is a directive based programming model , and allows to exploit vector - parallelism properly annotating for - loops that can be parallelized@xcite .",
    "on gpus , the native programming model is strongly based on data - parallel models , with one thread typically processing one element of the application data domain .",
    "this helps exploit all available parallelism of the algorithm and hide latencies by switching among threads waiting for data coming from memory and threads ready to run .",
    "the native language is cuda - c for nvidia gpus and opencl for amd systems .",
    "both languages have a very similar programming model but use a slight different terminology ; for instance , on opencl the cuda - thread is called work - item , the cuda - block work - group , and the cuda - kernel is a device program .",
    "a cuda - c or opencl program consists of one or more functions that run either on the host , a standard cpu , or on a gpu .",
    "functions that exhibits no ( or limited ) parallelism run on the host , while those exhibiting a large degree of data parallelism can go onto the gpu .",
    "the program is a modified c ( or c++ , fortran ) program including keyword extensions defining data parallel functions , called _",
    "kernels _ or _",
    "device programs_. kernel functions typically translate into a large number of threads , i.e. a large number of independent operations processing independent data items .",
    "threads are grouped into blocks which in turn form the execution _",
    "grid_. when all threads of a kernel complete their execution , the corresponding grid terminates . since threads run in parallel with host cpu threads , it is possible to overlap in time processing on the host and the accelerator .",
    "new programming approaches are now emerging , mainly based on directives , moving the coding abstraction layer at an higher lever , over the hardware details .",
    "these approaches should make code development easier on heterogeneous computing systems@xcite , simplifying the porting of existing codes on different architectures .",
    "openacc is one such programming models , increasingly used by several scientific communities .",
    "openacc is based on _ pragma _ directives that help the compiler to identify those parts of the code that can be implemented as _",
    "parallel functions _ and offloaded on the accelerator or divided among cpu cores .",
    "the actual construction of the parallel code is left to the compiler making , at least in principle , the same code portable without modifications across different architectures and possibly offering more opportunities for performance portability .",
    "this make openacc more descriptive compared to cuda and opencl which are more prescriptive oriented .    ....",
    "# pragma acc data copyin(x ) , copy(y ) {       # pragma acc kernels present(x ) present(y ) async(1 )      # pragma acc loop vector(256 )      for ( int i = 0 ; i < n ; + + i )       y[i ] = a*x[i ] + y[i ] ;       # pragma wait(1 ) ;    } ....    listing  [ lst : saxpy ] shows an example of the _ saxpy _ operation of the _ basic linear algebra subprogram _ ( blas ) set coded in openacc .",
    "pragma acc kernels _",
    "clause identifies the code fragment running on the accelerator , while _ pragma acc loop ... _ specifies that the iterations of the for - loop can execute in parallel .",
    "the standard defines several directives , allowing a fine tuning of applications . as an example",
    ", the number of threads launched by each device function and their grouping can be tuned by the _ vector _ , _ worker _ and _ gang _ directives , in a similar fashion as setting the number of _ work - items _ and _ work - groups _ in cuda .",
    "data transfers between host and device memories are automatically generated , and occur on entering and exiting the annotated code regions .",
    "several data directives are available to allow the programmer to optimize data transfers , e.g. overlapping transfers and computation .",
    "for example , in listing  [ lst : saxpy ] the clause _",
    "copyin(ptr ) _ copies the array pointed by _ ptr _ from the host memory into the accelerator memory before entering the following code region ; while _",
    "copy(ptr ) _ perform the additional operation of copying it also back to the host memory after leaving the code region .",
    "an asynchronous directive _",
    "async _ is also available , instructing the compiler to generate asynchronous data transfers or device function executions ; a corresponding clause ( i.e. _ # pragma wait(queue ) _ ) allows to wait for completion .",
    "openacc is similar to the openmp ( open multi - processing ) framework widely used to manage parallel codes on multi - core cpus in several ways@xcite ; both frameworks are directive based , but openacc targets accelerators in general , while at this stage openmp targets mainly multi - core cpus ; the latest release of openmp4 standard has introduced directives to manage also accelerators , but currently , compilers support is still limited .",
    "regular c / c++ or fortran code , already developed and tested on traditional cpu architectures , can be annotated with openacc pragma directives ( e.g. _ parallel _ or _ kernels _ clauses ) to instruct the compiler to transform loop iterations into distinct threads , belonging to one or more functions to run on an accelerator .",
    "ultimately , openacc is particularly well suited for developing scientific hpc codes for several reasons :    * it is highly hardware agnostic , allowing to target several architectures , gpus and cpus , allowing to develop and maintain one single code version ; * the programming overhead to offload code regions to accelerators is limited to few _ pragma _ lines , in contrast to cuda and in particular opencl verbosity ; * the code annotated with openacc _ pragmas _ can be still compiled and run as plain c code , ignoring the _ pragma _ directives .",
    "in this section we describe the openacc implementation of our lqcd code .",
    "we first describe the data structures used , then we highlight the most important openacc - related details of our implementation . in writing the openacc version ,",
    "we started from our previous code implementations@xcite : a c++/cuda@xcite developed for nvidia gpus aggressively optimized with cuda - specific features , and a c++ one , developed using openmp and mpi directives , targeting large cpu clusters@xcite .",
    "data structures have a strong impact on performance@xcite and can hardly be changed on an existing implementation : their design is in fact a critical step in the implementation of a new code .",
    "we have analyzed in depth the impact of data - structures for lqcd on different architectures ( i.e. a gpu and a couple of cpus ) , confirming that the _ structure of arrays _ ( soa ) memory data layout is preferred when using gpus , but also when using modern cpus@xcite .",
    "this is due to the fact that the soa format allows vector units to process many sites of the application domain ( the lattice , in our case ) in parallel , favoring architectures with long vector units ( e.g. with wide simd instructions ) .",
    "modern cpus tend indeed to have longer vector units than older ones and we expect this trend to continue in the future .",
    "for this reason , all data structures related to lattice sites in our code follow the soa paradigm .     of each array",
    "c0 , c1 and c2 is a c99 complex value . see sections  [ memalloc_section ] for details.,scaledwidth=80.0% ]     matrices ; this structure contains 3 vectors . to mitigate memory - bandwidth requirements , one can avoid reading and writing the r2 member and recompute it on the fly , exploiting the unitarity constraint.,scaledwidth=60.0% ]    in our implementation , we use the c99 double complex as basic data - type which allows to use built - in complex operators of the c library making coding easier and more readable without loss of performance .    the algorithm is based on even / odd preconditioning , so the pseudo - fermion variables ( implemented as vec3_soa data - types ) live only on the even sites of the lattice .",
    "this comes at the price of requiring that all sides of the lattice must be even ; in the following we call lnh_sizeh half the number of lattice sites . the pseudofermion field has three complex values for each even lattice site , corresponding to the three qcd `` colors '' that we label c0 , c1 , c2 .",
    "a schematic representation of the vec3_soa structure is shown in fig .  [ fig : mem - vec ] and a lexicographical ordering was used for the even lattice sites : @xmath82}{2 } \\qquad       \\mathrm{s.t.}\\ \\sum_{i=0}^3 x_i \\% 2 = 0\\ , \\ ] ] where lnh_n0 , lnh_n1 and lnh_n2 are the lattice sizes ; we allow for full freedom in the mapping of the physical directions @xmath83 , @xmath84 , @xmath85 and @xmath23 onto the logical directions @xmath86 , @xmath87 , @xmath88 and @xmath89 , as this option will be important for future versions of the code able to run on many processors and accelerators .    the data structure used for the generic @xmath66 complex matrices is the su3_soa data - type , matrices , while actual @xmath0 matrices require in principle less memory . ]",
    "used e.g. for the `` staples '' @xmath90 and the @xmath91 matrices needed in the stouting procedure@xcite .",
    "structure su3_soa is a collection of 3 vec3_soa structures ( r0 , r1 , r2 , see fig .",
    "[ fig : mem - su3 ] ) , and data that has to be stored in this structure typically involve a number of matrices equal to the number of links present in the lattice , i.e. 8 lnh_sizeh ; this means that an array of 8 su3_soa elements is required .",
    "gauge configurations , i.e. the set of the gauge links @xmath7 and their stouted counterparts , are stored in memory as an array of 8 su3_soa structures . as previously explained the algorithm is typically bandwidth limited and for @xmath0 matrices",
    "it is convenient to read and write just the first two rows , computing the third one on the fly as @xmath92 .",
    "note that the soa memory layout avoids the prefetching problems discussed in similar cases@xcite .",
    "other data structures are needed to store in memory @xmath66 traceless hermitian matrices or @xmath66 traceless anti - hermitian matrices . in these cases ,",
    "only 8 real parameters per matrix are needed : 3 complex numbers for the upper triangular part and the first two elements of the diagonal , which are real ( imaginary ) numbers for ( anti-)hermitian traceless matrices .",
    "these data structures have been implemented according to the soa scheme as follows : thmat_soa and tamat_soa contain 3 vectors of c99 double complex numbers and 2 vectors of double numbers , in a form that closely resemble the one of vec3_soa .",
    "data movements between device and host are negligible , with significant transfers happening only at the beginning and at the end of each monte carlo update , and managed mainly with the update device and update host openacc directives .      to initially assess the performance level achievable using openacc , we have developed a mini - application benchmark of the dirac operator@xcite . as previously underlined",
    "this is the fundamental building block of the krylov solver , commonly accounting for not less than @xmath93 of the running time , and reaching up to @xmath94 in low temperature simulations .",
    "this compute intensive part of an lqcd simulation is where most of the optimization efforts are usually concentrated@xcite .",
    "the dirac operator code uses three functions : deo , doe ( corresponding respectively to the application of functions @xmath58 and @xmath57 defined in ) and a _",
    "zaxpy_-like function which is negligible in terms of execution time .",
    "a direct comparison indicated that the performance of the openacc versions of the double precision deo and doe functions were comparable with the cuda ones@xcite .",
    "this promising start was a strong indication that also for lqcd the higher portability of the openacc implementation is not associated with a serious loss of performance , and motivated us to proceed to an openacc implementation of the full rhmc code .",
    "as a side benefit , the use of the openacc programming model significantly simplified the implementation of algorithmic improvements .",
    "the implementation of these new features started with the coding and testing of the improvements on a single thread version .",
    "after the algorithm is validated , the acceleration is switched on by annotating the code with # pragma directives . in order to have a more readable code ,",
    "the most complex kernels have been split in several functions .",
    "while small functions can be used in kernels if declared as static inline , for larger ones we had to use the routine seq openacc directive as large functions can not be inlined .",
    "kernels have been parallelized following two different approaches .",
    "those using data belonging to nearest ( and/or next - to - nearest ) neighbors have been parallelized via the # pragma acc loop directive on @xmath95 nested loops , one for each dimension .",
    "this allows to use 3d thread blocks , which should improve data reuse between threads thus reducing bandwidth requirements , which is our major performance concern .",
    "the other kernels , i.e. the ones performing only single - site operations , have been parallelized using a single cycle running on the lattice sites .    after the implementation of a first full working openacc simulation , various optimization iterations took place , in particular for the performance critical steps .",
    "these include the dirac operator in the first place , but also the gauge part of the molecular dynamics steps , since their relative impact on the overall execution time is very large , as shown in  [ tab : functions ] for a few representative examples .    during the full development phase , every time a new openacc feature has been introduced",
    ", extensive checks have been performed to ensure the correctness of the improved code , against possible semantic misunderstanding of openacc clauses or compiler bugs .",
    "this section describes the overall structure of our code , and focuses on the openacc implementation of selected performance - critical parts .",
    "read gauge configuration @xmath96 create momenta @xmath97 generate pseudofermions by heatbath [ pseudoferm_generation_algorithm ] calculation of initial action molecular dynamics [ possibly in single precision ] [ moldyn_step_algorithm ] calculate action variation @xmath51 [ final_action_calculation_algorithm ] montecarlo step accepted with probability @xmath98 take measurements    algorithm  [ big_scheme_of_things_algorithm ] is a top - level description of the full code , showing the main computational tasks . for performances ,",
    "the most critical steps are molecular dynamics ( step  [ moldyn_step_algorithm ] ) followed by the heatbath generation of the pseudofermions ( step  [ pseudoferm_generation_algorithm ] ) , and the calculation of the final action ( step  [ final_action_calculation_algorithm ] ) . steps  [ pseudoferm_generation_algorithm ] and  [ final_action_calculation_algorithm ] consist basically in function calls to the multishift inverter routine , with a high target accuracy .",
    "the outer level of the multistep integrator for molecular dynamic evolution ( step  [ moldyn_step_algorithm ] ) in algorithm  [ big_scheme_of_things_algorithm ] is expanded in algorithm  [ outer_cycle_algorithm ] . as explained in sec.([grand_challenge_section ] ) , in zero temperature simulations or for small quark masses usually the heaviest computational parts are the calculations of the fermion force , while in high temperature simulations the load is shifted inside the gauge cycles , as already shown in  [ tab : functions ] .",
    "the fermion force calculation step is implemented following@xcite ; for this step a large fraction of the execution time is is spent in computation of deo and doe functions implementing the dirac operator .",
    ".... void acc_deo ( _ _ restrict const su3_soa * const u ,                     _ _ restrict vec3_soa * const out ,                     _ _ restrict const vec3_soa * const in ,                     _ _ restrict const double_soa * const backfield ) {    int hd0 , d1 , d2 , d3 ;    # pragma acc kernels present(in )",
    "present(out )                         present(u )   present(backfield ) async(1 )    # pragma acc loop independent gang(gang )    for(d3=0 ; d3<nd3;d3++ ) {      # pragma acc loop independent vector tile(tile0,tile1,tile2 )      for(d2=0 ; d2<nd2 ; d2++ ) {        for(d1=0 ; d1<nd1 ;",
    "d1++ ) {          for(hd0=0 ; hd0 < nd0h ; hd0++ ) {            ...                  }        }      }    } } ....    the deo openacc implementation is shown in listing  [ lst : dirac ] , showing the 4 dimension nested loops and the corresponding pragma directives . in this",
    "listing openacc directives are used : i ) to identify the data structures already present in the accelerator memory , when targeting accelerators ( present ( ) clause ) ; ii ) to make the compiler aware of the data independence of loops iterations ( independent clause ) ; iii ) to request to group iterations in order to execute them in the same ( or close ) compute units ( tile clause ) . in particular",
    ", the tile openacc clause asks the compiler to split or strip - mine each loop in the nest into two loops , an outer tile loop and an inner element loop .",
    "where possible ( e.g. in deo and doe ) , performing computations of adjacent lattice sites in close hardware compute units may increase data reuse ( i.e. matrices shared between sites)@xcite for all the architectures where data caches are present , which means almost every modern processing architecture",
    ". the tile sizes offering the best performance depend , for each kernel , on several features of each specific architecture , e.g. vector units size , register numbers , cache levels and sizes .",
    "we keep a door open for limited architecture - specific optimization , by allowing to specify the tile0 , tile1 , tile2 variables at compile time , telling the compiler how to group together iterations involving adjacent lattice sites .    fermion force calculation evolve momenta for @xmath99 @xcite ( @xmath100 ) fermion force calculation evolve momenta for @xmath101 ( @xmath100 ) fermion force calculation evolve momenta for @xmath102 ( @xmath100 ) fermion force calculation evolve momenta for @xmath101 ( @xmath100 ) fermion force calculation evolve momenta for @xmath103    the actual evolution of the gauge configuration happens inside the inner gauge cycles , where the gauge contribution to the momenta evolution is also calculated . among the tasks performed in the gauge cycles , the computation of staples in the gauge force calculation is the most time consuming .",
    "it consists of calculating 6 products of 3 and 5 @xmath0 matrices representing links on c - shaped paths on the lattice .",
    "the implementation of one of these functions is sketched in listing  [ lst : rect_staples ] : also in this case the parallelization has been done using the tile directive over the 3 innermost nested cycles .",
    "this allows us also in this case to use 3d thread blocks , which should improve data reuse between threads , reducing the bandwidth needs .",
    "we shall also remark that in this case , since second - nearest - neighbor - site addressing is needed , for the sake of simplicity we use indirect addressing .",
    "notice that the function staple_type1 ( as well as similar ones ) has to be declared with # pragma acc routine seq to be used inside a kernel .",
    "....   # pragma acc routine seq   void staple_type1 ( ... ) { ... }     void calc_staples_type1 ( _ _ restrict const su3_soa * const u ,                            _ _ restrict su3_soa * const loc_stap ) {     int d0 , d1 , d2 , d3 , mu , iter ;     # pragma acc kernels present(u ) present(loc_stap )                          present(nnp_openacc ) present(nnm_openacc )     # pragma acc loop independent gang(impstapgang3 )     for(d3=0 ; d3<nd3 ; d3++ ) {       # pragma acc loop independent vector tile(impstaptile0 ,                                                impstaptile1 ,                                                impstaptile2 )       for(d2=0 ; d2<nd2 ; d2++ ) {         for(d1=0 ; d1<nd1 ; d1++ ) {           for(d0=0 ;",
    "d0 < nd0 ; d0++ ) {             # pragma acc loop seq             for(mu=0 ; mu<4 ; mu++ ) {               ...               const int idx_pmu = nnp_openacc[idxh][mu][parity ] ;               ...               staple_type1(&u[dir_nu_1r ] , idx_pmu , ... ) ....    in order to improve performance , we also implemented a single precision version of the code for the molecular dynamics evolution . due to the low arithmetic density of the lqcd algorithms , on gpus at least , all kernels are memory - bound",
    "; this means that , when precision is not an issue , it is preferable to have single precision versions of selected functions and structures , as a plain @xmath104 increase in performance is expected with respect to the double precision implementation .",
    "to compare the performance of our code on different architectures we consider two different benchmarks taking into account the most computational intensive parts of the code .",
    "the first benchmark evaluates the performance of the dirac operator , both single and double precision version , and the latter evaluates the performance of the gauge part of the molecular dynamics step . depending on input configuration parameters either the former or the latter kernels make up most of the execution time of a typical simulation , as shown in  [ tab : functions ] .",
    "we present the execution time per site of the dirac operator for different lattice sizes in  [ tab : dirac - norm ] .",
    "exactly the same code has been run on all platforms without requiring any change ; we have just re - compiled it with different flags instructing the pgi 16.10 compiler to target the corresponding architectures and using the best tile dimensions for each of them .",
    "we tested two different nvidia gpus , the k80 based on the kepler architecture and the recently released p100 board based on the pascal architecture . for the k80 the single precision version takes @xmath105 per lattice site , while the double precision version requires @xmath106 . running on the p100 we measure @xmath107 for single and @xmath108 for double precision , improving approximately by a factor @xmath109 over the k80 .",
    "this results perfectly scales with architecture performance of p100 that has @xmath110 more cores and @xmath111 more memory bandwidth , see  [ tab : architecture ] .    concerning intel cpus ,",
    "we have compared two different processors , the 8-core e5 - 2630v3 cpu based on haswell architecture , and the 18-core e5 - 2697v4 cpu based on broadwell . since computing resource of the cpus are roughly @xmath109 lower than on gpus , see  [ tab : architecture ] , a performance drop is expected .",
    "however , the actual performance drop measured on both cpus is much larger than this expected theoretical figure ; indeed time per site is approximately @xmath112 or larger on the haswell than on one k80 gpu .",
    "the broadwell performs approximately a factor @xmath113 better compared to haswell , at least for some lattice sizes .",
    "we have identified two main reasons for this non - optimal behavior , and both of them point to some still immature features of the pgi compiler when targeting x86 architectures , that  we expect  should be soon resolved :    * * parallelization * - the compiler is only able to split outer - loops across different threads , while inner loops are executed serially or vectorized within each thread .",
    "this explains why on the broadwell cpu running on a lattice @xmath114 we have a performance @xmath113 better than for a @xmath115 lattice , which has the same volume but allows to split the outer loop only on @xmath1 threads . * * vectorization * - as reported by the compilation logs , the compiler fails to vectorize the deo and doe functions computing the dirac operator ( see listing  [ lst : dirac ] ) reporting to be unable to vectorize due to the use of `` mixed data - types '' . to verify",
    "if this is related to how we have coded these functions , we have translated the openacc pragmas into the corresponding openmp ones  without changing the c code  and compiled using the intel compiler ( version 17.0.1 ) . in this case",
    "the compiler succeeds in vectorizing the two functions , running a factor @xmath116 faster compared to the openacc version compiled by pgi compiler .",
    "[ tab : md - norm ] shows the execution time of the gauge part of the molecular dynamics step .",
    "as already remarked this is one of the two most time - consuming steps together with the application of the dirac operator .",
    "as we see the update time per site is quite stable for all lattice sizes we have tried and for all architectures .",
    "going from the nvidia k80 to the p100 the time improves by a factor @xmath117 , while between haswell and broadwell we have roughly a factor @xmath118 / @xmath119 .",
    "we finally mention that we have also been able to compile and run our code on an amd firepro w9100 gpu and on the latest version of the intel xeon phi processor , the knights landing ( knl )",
    ". however , in these cases , results are still preliminary .",
    "in more details , the compiler itself crashes when compiling the code for the amd gpu for some specific lattice sizes ; for the knl , specific compiler support is still missing , but this processor is able to run the code compiled for the haswell architecture , implying however that 512-bit vectorization is not used .",
    "these problems do not allow us to perform a systematic comparison of performance for these architectures .",
    "once again , we believe that this is due to some immaturity of the compiler , and we expect that these issues will be resolved in future versions .    [ tab : cuda - openacc ] addresses the question of the efficiency costs ( if any ) of our architecture - portable code ; the table compares the execution time for a _ full _ monte carlo step ( in double precision ) of the openacc code and a previously developed cuda implementation@xcite , optimized for nvidia gpus .",
    "although the two codes are not exactly in a one to one correspondence , the implementations are similar enough to make such a test quantitatively meaningful .",
    "one immediately sees that the performances of the two implementations are comparable and the use of openacc does not imply a dramatic performance loss , the differences between the execution times of the two versions being of the order of @xmath120 .    the worst case is the one of the @xmath121 lattice , in which openacc is about @xmath122 slower than cuda . since we are comparing an high - level version of the code with one specifically developed for nvidia gpus ,",
    "this would not be a dramatic loss , however in this case the comparison is also not completely fair .",
    "indeed for this high temperature simulation the gauge part of the molecular dynamic step starts to be the computationally heaviest task and , in the cuda implementation , part of it had been explicitly hard coded in single precision .",
    "for the low - temperature test cases the differences between the cuda and the openacc implementation are much smaller and , in fact , in one case the openacc version is the fastest one .",
    "a possible explanation of this is the following : in the cuda version unidimensional blocks are adopted to parallelize the dirac operator , while in the openacc implementation three - dimensional block structures are used , that fit better the larger cache of recent gpus and , especially on larger lattices , improves data reuse .",
    "in this work we have developed a full state - of - the - art production - grade code for lattice qcd simulations with staggered fermions , using the openacc directive - based programming model .",
    "our implementation includes all steps of a complete simulation , and most of them run on accelerators , minimizing the transfer of lattice data to and from the host .",
    "we have used the pgi compiler , which supports the openacc standard and is able to target almost all current architectures relevant for hpc computing , even if with widely different levels of maturity and reliability .",
    "exactly the same code runs successfully on nvidia many - core gpus and intel multi - core cpus , and for both architectures we have measured roughly comparable levels of efficiency . also , the performance of the complete code is roughly the same as that of an equivalent code , specifically optimized for nvidia gpus and written in the cuda language .",
    "our code also runs on amd gpus and on the knl intel phi processor , even if the compilation and run - time environment for these processors is still unable to deliver production - grade codes ; in these cases , we have strong indications that these problems come from a residual immaturity of the compilation chain and we expect that they will be soon resolved .",
    "all in all , our final result is a lqcd monte carlo code portable on a large subset of hpc relevant processor architectures and with consistent performances .",
    "some further comments are in order : i ) using a directive - based programming model , we are able to target different computing platforms presently used for hpc , avoiding to rewrite the code when moving from one platform to another ; ii ) the openacc standard provides a good level of hardware abstraction requiring the programmer to only specify the function to be parallelized and executed on the accelerator ; the compiler is then able to exploit the parallelism according to the target processor , hiding from the programmer most hardware optimizations ; iii ) the openacc code has roughly the same level of performance of that implemented using a native language such as cuda for nvidia gpus , allowing to efficiently exploit the computing resources of the target processor .    in the near future",
    "we plan to carefully assess performances on amd and knl systems , in order to enlarge the platform portfolio of our code .",
    "we also plan to assess whether openmp4 provides the same level of portability as openacc , as soon as compilers supporting this programming standard become available .",
    "this is important to have a unique directive - based programming model which is widely used by several scientific communities and supported by several compilers ( gcc , icc ,  ) .",
    "finally , we are already working on a massively parallel version of our code , able to run concurrently on a large clusters of cpus and accelerators .",
    "we warmly thank francesco sanfilippo ( the developer of the nissa code@xcite ) for his advice and support .",
    "we thank the infn computing center in pisa for providing us with the development framework , and universit degli studi di ferrara and infn - ferrara for the access to the coka gpu cluster .",
    "this work has been developed in the framework of the suma , coka and cosa projects of infn .",
    "fn acknowledges financial support from the infn suma project .",
    "o.  villa , d.  r. johnson , m.  oconnor , e.  bolotin , d.  nellans , j.  luitjens , n.  sakharnykh , p.  wang , p.  micikevicius , a.  scudiero , s.  w. keckler and w.  j. dally , scaling the power wall : a path to exascale , in _ proceedings of the international conference for high performance computing , networking , storage and analysis _ , sc 14 ( ieee press , piscataway , nj , usa , 2014 ) .",
    "doi : 10.1109/sc.2014.73[doi : 10.1109/sc.2014.73 ] .    c.  bonati , e.  calore , s.  coscetti , m.  delia , m.  mesiti , f.  negro , s.  f. schifano and r.  tripiccione , development of scientific software for hpc architectures using openacc : the case of lqcd , in _ the 2015 international workshop on software engineering for high performance computing in science ( se4hpcs ) _ , icse companion proceedings2015 .",
    "doi : 10.1109/se4hpcs.2015.9[doi : 10.1109/se4hpcs.2015.9 ] .",
    "d.  pflger , m.  mehl , j.  valentin , f.  lindner , d.  pfander , s.  wagner , d.  graziotin and y.  wang , the scalability - efficiency / maintainability - portability trade - off in simulation software engineering : examples and a preliminary systematic literature review , in _ proceedings of the fourth international workshop on software engineering for hpc in computational science and engineering _ , se - hpccse 162016 .",
    "doi : 10.1109/se - hpccse.2016.8[doi : 10.1109/se - hpccse.2016.8 ] .",
    "s.  wienke , c.  terboven , j.  beyer and m.  mller , _ lecture notes in computer science ( including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics ) _ * 8632 * , 812 ( 2014 ) .",
    "m.  g. lopez , v.  v. larrea , w.  joubert , o.  hernandez , a.  haidar , s.  tomov and j.  dongarra , towards achieving performance portability using directives for accelerators , in _ proceedings of the third international workshop on accelerator programming using directives _ , waccpd 162016 .",
    "doi : 10.1109/waccpd.2016.9[doi : 10.1109/waccpd.2016.9 ] .",
    "c.  bernard _ et al .",
    "_ , nucl .",
    "phys .  proc .",
    "suppl .   * 106 * , 199 ( 2002 ) .",
    "g.  bilardi , a.  pietracaprina , g.  pucci , f.  schifano and r.  tripiccione , _ lecture notes in computer science _ * 3769 * , 386 ( 2005 ) , doi :    10.1007/11602569_41[doi :    10.1007/11602569_41 ] .",
    "f.  belletti , s.  f. schifano , r.  tripiccione , f.  bodin , p.  boucaud , j.  micheli , o.  pene , n.  cabibbo , s.  de  luca , a.  lonardo , d.  rossetti , p.  vicini , m.  lukyanov , l.  morin , n.  paschedag , h.  simma , v.  morenas , d.  pleiter and f.  rapuano , _ computing in science and engineering _ * 8 * , 50 ( 2006 ) , doi : 10.1109/mcse.2006.4[doi : 10.1109/mcse.2006.4 ] .",
    "p.  a. boyle , d.  chen , n.  h. christ , m.  clark , s.  cohen , z.  dong , a.  gara , b.  joo , c.  jung , l.  levkova , x.  liao , g.  liu , r.  d. mawhinney , s.  ohta , k.  petrov , t.  wettig , a.  yamaguchi and c.  cristian , qcdoc : a 10 teraflops computer for tightly - coupled calculations , in _ supercomputing , 2004 .",
    "proceedings of the acm / ieee sc2004 conference _ , nov 2004 .",
    "doi : 10.1109/sc.2004.46[doi : 10.1109/sc.2004.46 ] .",
    "h.  baier , h.  boettiger , m.  drochner , n.  eicker , u.  fischer , z.  fodor , a.  frommer , c.  gomez , g.  goldrian , s.  heybrock , d.  hierl , m.  hsken , t.  huth , b.  krill , j.  lauritsen , t.  lippert , t.  maurer , b.  mendl , n.  meyer , a.  nobile , i.  ouda , m.  pivanti , d.  pleiter , m.  ries , a.  schfer , h.  schick , f.  schifano , h.  simma , s.  solbrig , t.  streuer , k .- h .",
    "sulanke , r.  tripiccione , j .- s .",
    "vogt , t.  wettig and f.  winter , _ computer science - research and development _ * 25 * , 149 ( 2010 ) , doi : 10.1007/s00450 - 010 - 0122 - 4[doi : 10.1007/s00450 - 010 - 0122 - 4 ] .    j.  doi , peta - scale lattice quantum chromodynamics on a blue gene / q supercomputer , in _ proceedings of the international conference on high performance computing , networking , storage and analysis _ , sc 12 ( ieee computer society press , los alamitos , ca , usa , 2012 ) .",
    "r.  babich , m.  a. clark and b.  joo , parallelizing the quda library for multi - gpu calculations in lattice quantum chromodynamics , in _",
    "sc 10 ( supercomputing 2010 ) new orleans , louisiana , november 13 - 19 , 2010 _ , 2010 .            o.  philipsen , c.  pinke , a.  sciarra and m.  bach , cl@xmath123qcd - lattice qcd based on opencl , in _ proceedings , gpu computing in high - energy physics ( gpuhep2014 ) : pisa , italy , september 10 - 12 , 2014 _ , 2015 .",
    "doi : 10.3204/desy - proc-2014 - 05/30[doi : 10.3204/desy - proc-2014 - 05/30 ] .",
    "s.  blair , c.  albing , a.  grund and a.  jocksch , accelerating an mpi lattice boltzmann code using openacc , in _ proceedings of the second workshop on accelerator programming using directives _ , waccpd 15 ( acm , new york , ny , usa , 2015 ) .",
    "doi : 10.1145/2832105.2832111[doi : 10.1145/2832105.2832111 ] .",
    "j.  kraus , m.  schlottke , a.  adinetz and d.  pleiter , accelerating a c++ cfd code with openacc , in _ accelerator programming using directives ( waccpd ) , 2014 first workshop on _",
    "doi : 10.1109/waccpd.2014.11[doi : 10.1109/waccpd.2014.11 ] .",
    "b.  jo , m.  smelyanskiy , d.  d. kalamkar and k.  vaidyanathan , wilson dslash kernel from lattice qcd optimization , in _ high performance parallelism pearls volume 2 : multicore and many - core programming approaches _ , eds .",
    "j.  reinders and j.  jeffers ( morgan kaufmann , boston , 2015 ) , pp .",
    "doi : 10.1016/b978 - 0 - 12 - 803819 - 2.00023 - 9[doi : 10.1016/b978 - 0 - 12 - 803819 - 2.00023 - 9 ] .",
    "e.  calore , n.  demo , s.  f. schifano and r.  tripiccione , experience on vectorizing lattice boltzmann kernels for multi- and many - core architectures , in _ parallel processing and applied mathematics : 11th international conference , ppam 2015 , krakow , poland , september 6 - 9 , 2015 . revised selected papers ,",
    "part i _ , eds .",
    "r.  wyrzykowski , e.  deelman , j.  dongarra , k.  karczewski , j.  kitowski and k.  wiatrlecture notes in computer science ( springer international publishing , cham , 2016 ) , pp .",
    "doi : 10.1007/978 - 3 - 319 - 32149 - 3_6[doi : 10.1007/978 - 3 - 319 - 32149 - 3_6 ] .",
    "b.  jo , m.  smelyanskiy , d.  d. kalamkar and k.  vaidyanathan , chapter 9 - wilson dslash kernel from lattice qcd optimization , in _ high performance parallelism pearls _ , eds . j.  reinders and j.  jeffers ( morgan kaufmann , 2015 ) , pp .",
    "139  170 .",
    "doi : 10.1016/b978 - 0 - 12 - 803819 - 2.00023 - 9[doi : 10.1016/b978 - 0 - 12 - 803819 - 2.00023 - 9 ] .",
    "b.  jo , d.  d. kalamkar , t.  kurth , k.  vaidyanathan and a.  walden , _ optimizing wilson - dirac operator and linear solvers for intel knl _ , in _ high performance computing : isc high performance 2016 international workshops _ , eds . m.  taufer , b.  mohr and j.  m. kunkel ( springer international publishing , june 2016 ) , pp .",
    "415427 . revised selected papers ."
  ],
  "abstract_text": [
    "<S> the present panorama of hpc architectures is extremely heterogeneous , ranging from traditional multi - core cpu processors , supporting a wide class of applications but delivering moderate computing performance , to many - core gpus , exploiting aggressive data - parallelism and delivering higher performances for streaming computing applications . in this scenario , </S>",
    "<S> code portability ( and performance portability ) become necessary for easy maintainability of applications ; this is very relevant in scientific computing where code changes are very frequent , making it tedious and prone to error to keep different code versions aligned . in this work we present the design and optimization of a state - of - the - art production - level lqcd monte carlo application , using the directive - based openacc programming model . </S>",
    "<S> openacc abstracts parallel programming to a descriptive level , relieving programmers from specifying how codes should be mapped onto the target architecture . </S>",
    "<S> we describe the implementation of a code fully written in openacc , and show that we are able to target several different architectures , including state - of - the - art traditional cpus and gpus , with the same code . </S>",
    "<S> we also measure performance , evaluating the computing efficiency of our openacc code on several architectures , comparing with gpu - specific implementations and showing that a good level of performance - portability can be reached . </S>"
  ]
}