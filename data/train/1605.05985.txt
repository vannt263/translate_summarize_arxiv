{
  "article_text": [
    "the clt is an essential tool for inferring on parameters of interest in a nonparametric framework .",
    "the strength of the clt stems from the fact that , as the sample size increases , the usually unknown sampling distribution of a pivot , a function of the data and an associated parameter , approaches the standard normal distribution .",
    "this , in turn , validates approximating the percentiles of the sampling distribution of the pivot by those of the normal distribution , in both univariate and multivariate cases .",
    "the clt is an approximation method whose validity relies on large enough samples . in other words ,",
    "the larger the sample size is , the more accurate the inference , about the parameter of interest , based on the clt will be .",
    "the accuracy of the clt can be evaluated in a number ways .",
    "measuring the distance between the sampling distribution of the pivot and the standard normal distribution is the common feature of these methods . naturally , the latter distance is a measure of the error of the clt .",
    "the most well known methods of evaluating the clt s error are berry - essen inequalities and edgeworth expansions .",
    "these methods have been extensively studied in the literature and many contributions have been made to the area ( cf .",
    ", for example , barndorff - nielsen and cox @xcite , bentkus _ et al . _",
    "@xcite , bentkus and gtze @xcite , bhattacharya and rao @xcite , dasgupta @xcite , hall @xcite , petrov @xcite , senatov @xcite , shao @xcite and shorack @xcite .    despite their differences , the berry - essen inequality and the edgeworth expansion , when the data have a finite third moment , agree on concluding that , usually , the clt is in error by a term of order @xmath3 , as @xmath4 , where @xmath2 is the sample size . in the literature ,",
    "the latter asymptotic conclusion is referred to as the first order accuracy or efficiency of the clt .",
    "achieving more accurate clt based inferences requires alternative methods of extracting more information , about the parameter of interest , from a given sample that may not be particularly large .    in this paper",
    "we introduce a method to significantly enhance the accuracy of confidence regions for the population mean via creating new pivots for it based on a given set of data .",
    "more precisely , by employing appropriately chosen random weights , we construct new randomized pivots for the mean .",
    "these randomized pivots are more symmetrical than their classical counterpart the student @xmath1-statistic and , consequently , they admit clts with smaller errors for both univariate and multivariate data .",
    "in fact , by choosing the random weights appropriately , we will see that the clts for the introduced randomized pivots , under some conventional conditions , can already be second order accurate ( see sections [ error of convergence ] and [ multivariate pivots ] ) .",
    "the randomization framework in this paper can be viewed not only as an alternative to the inferences based on the classical clt , but also to the bootstrap .",
    "the bootstrap , introduced by efron @xcite , is a method that also tends to increase the accuracy of clt based inferences ( cf . ,",
    "e.g. , hall @xcite and singh @xcite ) .",
    "the bootstrap relies on repeatedly re - sampling from a given data set ( see , for example , efron and tibshirani @xcite ) .",
    "the methodology introduced in this paper , on the other hand , reduces the error of the clt in a customary fashion , in both univariate and multivariate cases , and it does not require re - sampling from the given data ( see remark [ comparison to the bootstrap ] below for a brief comparison between the randomization approach of this paper and the bootstrap ) .    for confidence regions based on clts to capture a parameter of interest , in addition to the accuracy , it is desirable to also address their volume .    in this paper",
    "we also address the volume of the resulting confidence regions based on our randomized pivotal quantities in both univariate and multivariate cases . in the randomization framework of this paper , and in view of the clts for the randomized pivots introduced in it , studying the volume of the resulting randomized confidence regions for the mean is rather straightforward .",
    "this , in turn , enables one to easily trace the effect of the reduction in the error , i.e. , the higher accuracy , on the volume of the resulting confidence regions . as a result",
    ", one will be able to regulate the trade - off between the precision and the volume of the randomized confidence regions ( see section [ length of the confidence intervals for mu ] , section [ multivariate pivots ] and appendix i ) .",
    "the rest of this paper is organized as follows . in section [ main results ]",
    "we introduce the new randomized pivots for the mean of univariate data . in section",
    "[ error of convergence ] we use edgeworth expansions to explain how the randomization techniques introduced in section [ main results ] result in a higher accuracy of the clt . in section [ length of the confidence intervals for mu ] , for univariate data , we investigate the length of the confidence intervals that result from the use of the randomized pivots introduced in section [ main results ] . extensions of the randomization techniques of section [ main results ] to classes of triangular random weights are presented in section [ multinomially weighted pivots ] .",
    "generalization of the results in sections [ main results ] and [ edgeworth exapnsions ] to vector valued data are presented in section [ multivariate pivots ] .",
    "let @xmath5 , @xmath6 , be i.i.d .",
    "random variables with @xmath7 , @xmath8 and @xmath9 .",
    "the student @xmath1-statistic , the classical pivot for @xmath0 , is defined as :    @xmath10    where @xmath11 and @xmath12 are the sample variance and the sample mean , respectively . under the assumption @xmath13 ,",
    "the berry - essen inequality and the edgeworth expansion unanimously assert that , without restricting the class of distributions of the data , @xmath14 converges in distribution to standard normal at the rate @xmath3 , i.e. , the clt for @xmath14 is first order accurate .",
    "we are now to improve upon the accuracy of @xmath14 by using a broad class of random weights .",
    "the improvement will result from replacing the pivot @xmath14 by randomized versions of it that continue to serve as pivots for @xmath0 .",
    "we now define the aforementioned randomized pivots for @xmath0 , as follows :    @xmath15    where @xmath16 s are some random weights and @xmath17 , to which we refer as the window , is a real valued constant . the weights @xmath16 s and the window constant @xmath17 are to be chosen according to either one of the following two scenarios , namely , method i and method ii .",
    "+      to construct the randomized pivot @xmath18 in this scenario , we let the weights @xmath19 be a _ random sample _ with @xmath20 .",
    "moreover , these weights should be _ independent _ from the data @xmath21 .",
    "the window constant @xmath17 , should be chosen in such a way that it satisfies the following two properties : +   + ( i ) @xmath22 , +   + ( ii ) @xmath23 , +   + where @xmath24 is a given number such that @xmath25 can be arbitrary small or zero .",
    "+    the notation @xmath26 is an abbreviation for * * s**kewness * * r**educing * * f**actor ( see ( [ eq 2  ] ) below for a justification for this notation ) .",
    "the weights @xmath16 s in method i can be generated , independently from the data , using some statistical software .",
    "this remark is applicable to all randomized pivots discussed in this paper .",
    "* discussion of method i : when the weights have a skewed distribution * + in terms of the error of the clt , an ideal realization of condition ( ii ) of method i could be when the weights @xmath16 s have a skewed distribution and the window constant @xmath17 is a real root for the cubic equation @xmath27 , i.e. , when @xmath28 . condition ( ii ) of method i is so that it also allows generating the @xmath16 s from skewed distributions and finding a window constant @xmath17 such that @xmath22 and @xmath29 is close enough to zero . hence , when @xmath30 , but @xmath25 is chosen to be small , then @xmath17 does not necessarily have to be a root of the equation @xmath27 .    as it can be inferred from the results in section [ error of convergence ] , the closer the value of the @xmath29 is to zero , the smaller the error of the clt for @xmath31 , as in ( [ eq 2 ] ) , will be . +   + * discussion of method i : when the weights are symmetrical about their mean * + when the @xmath16 s are generated from a distribution that is symmetrical about its mean , in view of method i , a refinement can be achieved by taking the window constant @xmath17 to be close to @xmath32 but not equal to it .",
    "this choice of @xmath17 will result in @xmath29 that are not exactly zero , but can be arbitrarily close to it .",
    ", @xmath33 , where @xmath34 s are i.i.d ( 1 ) with empirical pearson s measure of skewness equal to 1.98 .",
    "panel ( b ) is the frequency histogram of the randomized data @xmath35 , where the weights @xmath36 s are i.i.d .",
    "@xmath37 , @xmath38 and srf@xmath39 , with empirical pearson s measure of skewness equal to @xmath40 .",
    "panel ( c ) is the frequency histogram of the randomized data @xmath41 , where the weights @xmath36 s are i.i.d .",
    "bernoulli(1/3 ) , @xmath42 and @xmath43 , with empirical pearson s measure of skewness equal to @xmath44 . , width=14 ]    method ii that follows can also be used to construct a more accurate randomized pivot @xmath45 , as defined in ( [ eq 2 ] ) , via generating the random weights from some symmetrical distributions .      in this scenario",
    ", we let the weights @xmath46 be a _ random sample _ with a symmetrical ( about its mean ) distribution and @xmath47 .",
    "moreover , we assume that the weights are _ independent _ from the data @xmath21 and we take the window constant @xmath17 to be equal to the mean of the random weights , i.e. , @xmath48 .    taking @xmath49 together with the symmetry of the distribution of the weights , imply that , in the scenario of method ii , we have @xmath50 , where @xmath29 is as defined in ( ii ) of method i.      using the randomized pivot @xmath45 , as in ( [ eq 2 ] ) , and generating its associated random weights @xmath16 s according to either method i or method ii , can result in a significant refinement in inferring about @xmath0 .",
    "the reason for this claim is given in section [ error of convergence ] .    in spite of the higher accuracy of @xmath51 , provided by both method i and method ii , we emphasize that the former is more desirable .",
    "this is so since , in both univariate and multivariate cases , method i yields randomized confidence regions for @xmath0 whose volumes shrink to zero as the sample size @xmath2 increases to infinity ( see ( [ eq 9 ] ) and appendix i ) .",
    "method ii , on the other hand , fails to yield shrinking confidence regions .",
    "in fact , choosing the weights @xmath16 s for the pivot @xmath45 under the scenario of method ii , yields confidence regions for @xmath0 whose volumes , as @xmath52 , approach a limiting distribution rather than vanishing ( see ( [ eq 10 ] ) and table 3 below ) .",
    "[ remark 1 ] the term @xmath53 in the denominator of @xmath31 , as in ( [ eq 2 ] ) , under both methods i and ii , can , equivalently , be replaced by @xmath54 .    in the above description of different weights in methods i and ii , we excluded the case when the weights @xmath16 s have a skewed distribution and @xmath55 .",
    "this case was omitted since , in general , it does not necessarily provide a refinement in the clt for the resulting randomized pivot @xmath45 , nor does it result in confidence regions whose volumes shrink to zero , as the sample size increases .",
    "the main idea behind methods i and ii is to transform the classical pivot @xmath14 , as in ( [ eq 1 ] ) , to @xmath45 , as in ( [ eq 2 ] ) , that has a smaller _ skewness_. to further develop the idea , we first note that @xmath45 is governed by the joint distribution of the data @xmath56 and the weights @xmath16 s . in view of this observation , we let @xmath57 stand for the joint distribution of the data and the wights , and we represent its associated mean by @xmath58 .",
    "recalling now that in both method i and method ii the weights are independent from the data , we conclude that @xmath59 and , consequently , @xmath60 .",
    "+ now observe that @xmath61 in view of the preceding observation , we now obtain the skewness of the random variables @xmath62 , under both methods i and ii , as follows :    @xmath63    the second term of the product on the r.h.s . of ( [ eq 2  ] ) , i.e. , @xmath64 , is the skewness of the original data .",
    "the closer it is to zero the nearer the sampling distribution of @xmath14 , as defined in ( [ eq 1 ] ) , will be to the standard normal .",
    "however , one usually has no control over the skewness of the original data .",
    "the idea in methods i and ii is to incorporate the random weights @xmath16 s and to appropriately choose a window constant @xmath17 in such a way that @xmath65 is arbitrarily small .",
    "this , in view of ( [ eq 2  ] ) , will result in smaller skewness of the random variables @xmath62 ( see also appendix ii for the effect of the skewness reduction methods on vector valued data ) .",
    "the latter property , in turn , under appropriate conditions , can result in the second order accuracy of the clts for @xmath45 , as defined in ( [ eq 2 ] ) , under both methods i and ii . the accuracy of @xmath45 is to be discussed later in this section in the univariate case and , in section [ multivariate pivots ] in the multivariate case .    in view of ( [ eq 2  ] )",
    ", it is now easy to appreciate that when @xmath17 is chosen in such a way that @xmath50 , then the skewness of @xmath62 will be exactly zero .",
    "the latter case can happen under method i when the distribution of the @xmath16 s is skewed and the cubic equation @xmath66 has at least one real root and @xmath17 is taken to be one of these real roots .",
    "the other way to make @xmath29 equal to zero is when the weights @xmath16 s have a symmetrical distribution and @xmath67 , i.e. , method ii .",
    "however , when method ii is used to construct @xmath51 , having an @xmath29 that is exactly zero , as it was already mentioned in section [ main results ] , will come at the expense of having confidence regions for @xmath0 whose volumes do not vanish ( see section [ length of the confidence intervals for mu ] and appendix i ) .",
    "we use edgeworth expansions to illustrate the higher accuracy of the clt for the randomized pivot @xmath45 , as in ( [ eq 2 ] ) , under methods i and ii , as compared to that of the classical clt for the pivot @xmath14 , as in ( [ eq 1 ] ) .",
    "edgeworth expansions are used in our reasoning below since they provide a direct link between the skewness of a pivotal quantity and the error admitted by its clt .    in order to state the edgeworth expansion for the sampling distribution @xmath68 , for all @xmath69 , we first define @xmath70 also , we consider arbitrary positive @xmath71 and @xmath72 , and we let @xmath73 be so that @xmath74 , where @xmath75 stands for the standard normal distribution function .    in view of the above setup , we now write the following approximation .",
    "@xmath76    under the assumption @xmath7 , from baum and katz @xcite , we conclude that , as @xmath52 , @xmath77 by virtue of this result , we conclude that replacing @xmath45 by @xmath78 produces an error that approaches zero at the rate @xmath79 , as @xmath4 .",
    "combining now the preceding conclusion with ( [ eq 2 ] ) and letting @xmath80 , we arrive at @xmath81 the preceding relation implies the asymptotic equivalence of @xmath82 up to an error of order @xmath79 . in view of this equivalence and also recalling that in both methods i and ii the weights have a finite third moment , we write a one - term edgeworth expansion for @xmath83 , @xmath69 , as follows :    @xmath84    where @xmath85 is the density function of the standard normal distribution and @xmath86 .    under the condition @xmath87 ,",
    "the following ( [ eq 5 ] ) and ( [ eq 6 ] ) are the respective counterparts of the approximations ( [ eq 3 ] ) and ( [ eq 4 ] ) for the classical @xmath14 , as in ( [ eq 1 ] ) , and they read as follows :    @xmath88    where @xmath89 and    @xmath90    a comparison between the expansions ( [ eq 6 ] ) and ( [ eq 4 ] ) shows how incorporating the weights @xmath16 s and their associated window @xmath17 , as specified in methods i and ii , results in values of @xmath91 which are closer to the standard normal distribution @xmath92 than those of @xmath93 . more precisely , under methods i and ii , having an @xmath29 , such that @xmath94 is small or negligible , results in smaller or negligible values of the skewness of @xmath18 , as defined in ( [ eq 2 ] ) . the latter reduction of the skewness , when @xmath94 is negligible , by virtue of ( [ eq 3 ] ) and ( [ eq 4 ] ) , yields a one - term edgeworth expansion for the sampling distribution of @xmath95 whose magnitude of error is @xmath79 rather than @xmath3 .",
    "on the other hand , in view of ( [ eq 5 ] ) and ( [ eq 6 ] ) , the rate of convergence of the clt for the classical @xmath14 , as in ( [ eq 1 ] ) , is of order @xmath3 .    in order to further elaborate on the refinement provided by the skewness reduction approach provided by methods",
    "i and ii above , we now assume that the data @xmath56 and the weights @xmath16 both have a finite fourth moment .",
    "in addition to the latter assumption , we also assume that the data @xmath56 satisfy cramr s condition that @xmath96 .",
    "cramr s condition is required for the sampling distributions @xmath97 and @xmath98 to admit two - term edgeworth expansions .",
    "it is noteworthy that typical examples of distributions for which cramr s condition holds true are those with a proper density ( cf .",
    "hall @xcite ) .    once again here , replacing @xmath45 by @xmath78 , as in ( [ eq 2 ] ) and ( [ eq 2 + 1 ] ) , generates the error term @xmath99 , where @xmath72 is an arbitrary small positive constant . in view of our moment assumption at this stage , @xmath100 , from baum and katz @xcite we conclude that , as @xmath4    @xmath101    hence , replacing @xmath51 by @xmath102 generates an error of order @xmath103 . by virtue of the latter conclusion , an argument similar to the one used to derive ( [ eq 3 ] ) , yields    @xmath104    also , a similar argument to ( [ eq 5 ] ) yields    @xmath105    where @xmath106 is as defined in ( [ eq 4 + 1 ] ) .",
    "the approximation result ( [ eq 3 + 1 ] ) implies that @xmath51 and @xmath102 are equivalent up to an error of order @xmath103 and ( [ eq 3 + 2 ] ) yields the same conclusion for @xmath14 and @xmath106 . by virtue of the latter two equivalences , we now write two - term edgeworth expansions for @xmath107 and @xmath108 , @xmath69 , as follows :    @xmath109    where @xmath110 is as in ( [ eq 4 ] ) , @xmath111 and @xmath112 .    as to @xmath106 , it admits the following two - term edgeworth expansion .",
    "@xmath113    in view of ( [ eq 7 ] ) , and also ( [ eq 3 + 1 ] ) , when the data and the weights have four moments and the data satisfy cramr s condition , we conclude that for both methods i and ii , when @xmath94 is small , the clt for @xmath45 becomes more accurate .",
    "in particular , when @xmath94 is negligible then the clt for @xmath45 is second order accurate , i.e. , of order @xmath114 .",
    "in contrast , by virtue of ( [ eq 7 + 1 ] ) , and also ( [ eq 3 + 2 ] ) , one can readily see that , under the same conditions for the data , the clt for @xmath14 is only first order accurate , i.e. , of order @xmath3 .",
    "in view of methods i and ii , we are now to put the refinement provided by the randomized pivots @xmath51 , as in ( [ eq 2 ] ) , to use by constructing more accurate confidence intervals for the population mean @xmath0 , in the case of univariate data . in this section",
    "we also study the length of these confidence intervals .",
    "the use of @xmath51 as a pivot results in asymptotic @xmath115 , @xmath116 , size confidence intervals for @xmath0 of the form :    @xmath117,\\ ] ]    where @xmath118 and @xmath119 is the @xmath120th percentile of the standard normal distribution .",
    "we now examine the length of @xmath121 which is    @xmath122    it is easy to see that , for @xmath121 when it is constructed by the means of method i , since @xmath123 , as @xmath4 , we have    @xmath124    in other words , choosing the weights and their associated window constants in accordance with method i , to create the randomized pivot @xmath45 , as in ( [ eq 2 ] ) , results in confidence intervals for @xmath0 whose lengths approach zero , as the sample size increases .    on the other hand , in the scenario of method",
    "ii we have @xmath48 .",
    "the latter choice of @xmath17 implies that , as @xmath4 , for all @xmath125    @xmath126    the preceding clt for the weights , in view of ( [ eq 9 ] ) , implies that , as @xmath52    @xmath127    where @xmath128 and @xmath129 is a standard normal random variable .    [ page 14 ] in view of ( [ eq 10 ] )",
    ", the length of a confidence interval based on the pivot @xmath45 , when it is constructed in accordance with method ii , converges in distribution to a scaled inverse of a folded standard normal random variable rather than shrinking , while , as it was seen in section [ error of convergence ] , this method results in clts for @xmath45 that , under appropriate conditions , are second order accurate ( cf .",
    "( [ eq 3 + 1 ] ) , ( [ eq 7 ] ) and table 3 ) , recalling that in method ii , @xmath50 .",
    "in this section we present some numerical results to illustrate the refinement provided by the randomized confidence intervals @xmath121 , as in ( [ eq 8 ] ) , when the random weights and their associated window constants are chosen in accordance with methods i and ii .",
    "in addition to examining the accuracy in terms of empirical probabilities of coverage , here , we also address the length of the randomized confidence intervals @xmath121 .    in our numerical studies in tables 1 - 3 below , we generate 1000 randomized confidence intervals as in ( [ eq 8 ] ) , with nominal size of @xmath130 , using the cut - off points @xmath131 therein , and 1000 classical @xmath1-confidence intervals @xmath132 , based on the same generated data with the same nominal size and cut - off points .    in tables 1 - 3 coverage(@xmath121 ) and length(@xmath121 ) stand , respectively , for the empirical probabilities of coverage and the empirical lengths of the generated confidence intervals @xmath121 .",
    "also , coverage(@xmath133 ) and length(@xmath133 ) stand , respectively , for the empirical probabilities of coverage and the empirical lengths of the generated @xmath1-confidence intervals @xmath133 with nominal size 95% .    in the following tables 1 - 2 , under the scenario of method i , we examine the higher accuracy provided by the randomized pivot @xmath45 , as in ( [ eq 2 ] ) , over the classical @xmath14 , as in ( [ eq 1 ] ) .",
    "@xmath134    [ table 1 ]    @xmath135    [ table 2 ]    from tables 1 and 2 , it is evident that the randomized pivots @xmath45 , as in ( [ eq 2 ] ) , when constructed according to method i , can significantly outperform @xmath14 , as in ( [ eq 1 ] ) , in terms of accuracy .    in the following table 3",
    "we examine numerically the performance of @xmath45 when it is constructed based on method ii .",
    "[ table 3 ]    note that in table 3 , as the sample size increases , the lengths of the confidence intervals @xmath136 , as in ( [ eq 8 ] ) with @xmath137 therein , that are constructed based on method ii , fluctuate rather than shrink ( see ( [ eq 10 ] ) ) .",
    "in this section we put the scenario of method i into perspective , and extend it to also include triangular weights .",
    "the idea here is to relate the size of the given sample to the random weights .    in this section ,",
    "we let @xmath138 be a _ triangular _ array of random weights that is _ independent _ from the data @xmath139 .",
    "the random weights @xmath140 here , can _ either _ be an i.i.d .",
    "array of random variables with @xmath141 , _ or _ they can have a @xmath142ultinomial distribution with size @xmath143 , i.e. ,        we are now to introduce method i.1 , as a generalization of method i , that can yield asymptotically , in @xmath2 , srf s whose absolute values are small or negligible .",
    "+   + * _ method i.1 _ * : let @xmath147 be as above . choose a real valued constant @xmath148 in such a way that for given @xmath24 , so that @xmath25 can be arbitrary small or zero , +   + ( i ) @xmath149 and + ( ii ) @xmath150 +   + moreover , as @xmath4 , @xmath148 should also satisfy the following maximal negligibility condition .",
    "the clt in ( [ eq 13 ] ) is a consequence of the well known lindeberg - feller clt in a conditional sense .",
    "we further elaborate on the clt in ( [ eq 13 ] ) by noting that , in light of the dominated convergence theorem , ( [ eq 13 ] ) follows from the following conditional clt : + _ as @xmath158 , for all @xmath159 , ( [ eq 12 + 1 ] ) suffices to have _        it is noteworthy that a typical condition under which ( [ eq 12 + 1 ] ) holds true is when the identically distributed triangular weights @xmath140 s , for each @xmath2 , have a finite @xmath163th moment , where @xmath164 , and @xmath165 , for some positive constant @xmath166 .",
    "the validity of the latter claim can be investigated by an application of markov s inequality for @xmath167 , where @xmath71 is an arbitrary positive number .          on taking @xmath172 , for example , in method i.1 , when the weights are @xmath142ultiniomially distributed as in ( [ eq 14 + 1 ] ) ,",
    "the randomized pivot @xmath173 , as in ( [ eq 11 ] ) , assumes the following specific form :      the window constant @xmath172 , in view of method i.1 , when the weights are @xmath142ultinomial as in ( [ eq 14 + 1 ] ) , was obtained from the following three steps : + step 1 : obtain the general form of @xmath175 in this case as follows :            we note that the maximal negligibility condition ( [ eq 12 + 1 ] ) holds for the @xmath142ultinomial weights as in ( [ eq 14 + 1 ] ) .",
    "the latter is true since , in this case , we have @xmath180 , where @xmath166 is a positive number whose value is not specified here ( cf . the paragraph following remark [ remark 3 ] ) . by this , we conclude that , on taking @xmath172 , all the assumptions in method i.1 hold true for the @xmath142ultinomial weights , as in ( [ eq 14 + 1 ] ) .    in the present context of @xmath142ultinomial weights , the @xmath115 confidence intervals for @xmath0 , based on the pivot @xmath181 , as in ( [ eq 15 ] ) , follow the general form ( [ eq 8 ] ) .",
    "however , the fact that here we have the constrain @xmath182 , enables us to specify ( [ eq 8 ] ) for @xmath0 in this context , as follows :      @xmath142ultinomial random variables , of the form ( [ eq 14 + 1 ] ) , also appear in the area of the weighted bootstrap , also known as the generalized bootstrap ( cf . , for example , arenal - gutirrez and matrn @xcite , barbe and bertail @xcite , csrg _",
    "et al_. @xcite , mason and newton @xcite and references therein ) , where they represent the count of the number of times each observation is selected in a re - sampling with replacement from a given sample",
    ". motivated by this , somewhat remote , relation between the bootstrap and our randomized approach in method i.1 , when the weights are as in ( [ eq 14 + 1 ] ) , we are now to conduct a numerical comparison between the two methods . after some further elaborations on the weighted bootstrap",
    ", we present our numerical results in table 4 below .    to explain the viewpoint of the weighted bootstrap , we first consider a bootstrap sample @xmath184 that is drawn with replacement from the original sample @xmath185 .",
    "observe now that for the bootstrap sample mean @xmath186 we have      where , for each @xmath188 , @xmath170 , @xmath189 is the count of the number of times the index @xmath188 of @xmath34 was selected .",
    "it is easy to observe that the weights @xmath190 are @xmath142ultinomially distributed , as in ( [ eq 14 + 1 ] ) , and they are independent from the data @xmath185 .",
    "to conduct our numerical comparisons , we consider the bootstrap @xmath1-confidence intervals ( cf . efron and tibshirani @xcite ) that are generally known to be efficient of the second order in probability-@xmath191 ( cf .",
    ", for example , hall @xcite , shao and tu @xcite and singh @xcite ) . to construct a bootstrap @xmath1-confidence interval for the population mean @xmath0 , first a large number , say @xmath192 , of independent bootstrap samples of size @xmath2 are drawn from the original data .",
    "let us represent them by @xmath193 , where @xmath194 .",
    "the bootstrap version of @xmath14 , as in ( [ eq 1 ] ) , is computed for each one of these @xmath192 bootstrap sub - samples to have @xmath195 , where      @xmath197 is the bootstrap sample variance and @xmath140 s are as in ( [ eq 14 + 1 ] ) .",
    "these @xmath192 bootstrap @xmath1-statistics are then sorted in ascending order to have @xmath198\\leq \\ldots \\leq t_{n}^{*}[b]$ ] .",
    "when , for example , @xmath199 , a bootstrap @xmath1-confidence interval for @xmath0 with the nominal size @xmath130 is constructed by setting :      for the same nominal size of 95% , we are now to compare the performance of the randomized confidence interval @xmath201 , as in ( [ eq 16 + 1 ] ) , to that of the bootstrap @xmath1-confidence interval @xmath202 , in table 4 below .    in table 4 , we generate 1000 confidence intervals @xmath201 .",
    "to do so , we use 1000 replications of the data sets @xmath203 , and the @xmath142ultinomial weights @xmath204 , as in ( [ eq 14 + 1 ] ) . for each one of the generated data sets , based on @xmath199 bootstrap samples , we also generate 1000 bootstrap @xmath1-confidence intervals @xmath202 , with nominal size of 95% .",
    "similarly to our setups for tables 1 - 3 , in table 4 , we let coverage(@xmath201 ) and length(@xmath201 ) stand for the empirical coverage probabilities and the empirical lengths of the therein generated randomized confidence intervals @xmath201 . also , in table 4 , we let coverage(@xmath202 ) and length(@xmath202 ) stand for the empirical probabilities of coverage and the empirical lengths of the bootstrap confidence intervals @xmath202 .",
    "the relatively close performance , in terms of accuracy , of the bootstrap @xmath1-confidence intervals with @xmath199 bootstrap samples , and the randomized pivot @xmath181 , as in ( [ eq 15 ] ) , in table 4 is interesting .",
    "further refinements to the randomization approach method i.1 that results in randomized pivots that can outperform , in terms of accuracy , method i.1 are presented in method i.2 in subsection [ fixed sample approach ] below .",
    "it is worth noting that the class of @xmath142ultinomial random weights ( [ eq 14 ] ) is far richer than the particular form ( [ eq 14 + 1 ] ) .",
    "our focus on the latter was mainly the result of its application in the area of the weighted bootstrap .",
    "clearly different choices of the size @xmath143 and/or @xmath205 in ( [ eq 14 ] ) yield different randomizing weights .",
    "[ comparison to the bootstrap ] the use of the randomized pivots introduced in this paper to construct confidence intervals for the mean by no means is computationally intensive , while the bootstrap is a computationally demanding method .",
    "also , using the randomization methods discussed in this paper , one does not have to deal with the problem of how large the number of bootstrap replications @xmath192 , should be .",
    "moreover , the error reduction methods introduced in this paper enable one to easily trace down the effect of the randomization on the length of the confidence intervals in the univariate case , and the volume of the randomized confidence rectangles when the data are multivariate ( cf .",
    "( [ eq 8 ] ) , section [ multivariate pivots ] and appendix i ) .",
    "it is also worth noting that the randomization framework allows regulating the error of an inference by choosing a desired value for the srf .",
    "this can be done by choosing the random weights from a virtually unlimited class , as characterized in the above method i , method ii , method i.1 and also method i.2 below .",
    "the approach discussed in method i.1 considers triangular random weighs , to tie the random weights to the sample size , and chooses the window constant @xmath148 therein in such a way that it makes the absolute value of the srf arbitrarily small , in the limit . here",
    ", we also consider the triangular random weights as described at the beginning of this section and introduce a method to increase the accuracy of the clt based inferences about the mean for fixed sample sizes",
    ".    for each fixed sample size @xmath2 , the following method i.2 yields a further sharpening of the asymptotic refinement provided by method i.1 and it reads as follows : +   + * _ method i.2 _ * : let the weights @xmath140 s be as described right above method i.1 . if for a given @xmath24 , so that @xmath25 can be arbitrary small or zero , there exist a real value @xmath206 so that for the weights @xmath140 s , we have + @xmath207 , + @xmath208 and + @xmath209 + then , for each @xmath2 , choose a real valued constant @xmath210 in such a way that it satisfies the following conditions ( iv ) and ( v ) .",
    "+   + @xmath211 , + @xmath212 .",
    "+   + the viewpoint in method i.2 , in principle , requires choosing different @xmath210 for different sample sizes @xmath2 , for a given @xmath24 . also , it is not difficult to see that method i.1 is the asymptotic version of method i.2 .",
    "we note that , for each fixed @xmath2 and given @xmath24 , when @xmath25 is small , method i.2 and its associated pivots @xmath213 , as in ( [ eq 12 ] ) , yield higher accuracy than those that result from the use of method i.1 and its associated pivots @xmath152 , as in ( [ eq 11 ] ) .",
    "this is true since , in method i.2 , the window constants @xmath210 are tailored for each fixed @xmath2 to make @xmath217 .",
    "this is in contrast to the viewpoint of method i.1 in which the therein defined skewness reducing factor @xmath218 assumes the given value @xmath24 in the limit .    despite their differences in the context of finite samples ,",
    "both method i.1 and method i.2 yield randomized pivots , as in ( [ eq 11 ] ) and ( [ eq 12 ] ) , that can outperform their classical counterpart @xmath14 , as in ( [ eq 1 ] ) , in terms of accuracy ( see tables 4 above and also tables 5 and 6 below ) .    under the scenario of method i.2 , the confidence intervals for @xmath0 based on the randomized pivots @xmath213 , also admit the general form ( [ eq 8 ] ) , only with @xmath140 in place of @xmath16 and @xmath210 in place of @xmath17 therein . hence ,",
    "in the following numerical studies we denote them by @xmath219 .    in order to illustrate the refinement provided by method i.2 , we consider random samples of sizes @xmath220 and @xmath221 from the heavily skewed lognormal(0,1 ) .",
    "we also consider @xmath142ultinomially distributed weights as in ( [ eq 14 + 1 ] ) .",
    "choosing the random weights here to be @xmath142ultinomially distributed , as in ( [ eq 14 + 1 ] ) is so that the numerical results in tables 5 and 6 below should be comparable to their counterparts in table 4 above where the data have a lognormal(0,1 ) distribution .    on taking @xmath222 in method i.2 , we saw in subsection [ multinomially weighted pivots ] , that for @xmath172 we have + @xmath223 and + @xmath224 .",
    "+ recall that for the @xmath142ultinomial weights , as in ( [ eq 14 + 1 ] ) , the general form of @xmath175 was already derived in ( [ eq 16 ] ) . in view of the latter result ,",
    "it is easy to check that when @xmath220 , on taking @xmath225 we have @xmath226 . also , for @xmath221 , taking @xmath227 yields @xmath228 .",
    "consider now @xmath229 and @xmath230 , the confidence intervals for @xmath0 of nominal size @xmath130 based on method i.2 and samples of size @xmath220 and @xmath221 , which result , respectively , from setting :      in the following tables 5 and 6 we generate 1000 replications of lognormal(0,1 ) data and @xmath142ultinomial weights , as in ( [ eq 14 + 1 ] ) , for @xmath220 and @xmath221 .",
    "we let coverage(@xmath229 ) and coverage(@xmath230 ) stand for the respective empirical probabilities of coverage of @xmath229 and @xmath230 .",
    "we also let length(@xmath229 ) and length(@xmath230 ) stand for the respective empirical lengths of @xmath229 and @xmath230 .         [ [ appendix - i - asymptotically - exact - size - randomized - confidence - rectangles ] ] appendix i : asymptotically exact size randomized confidence rectangles ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    in the case of multivariate data , the effect of the randomization methods discussed in section [ multivariate pivots ] , on the volume of the resulting randomized ( hyper ) confidence rectangles can be studied by looking at the marginal confidence intervals for each component of the mean vector . to further elaborate on the idea , for simplicity",
    "we restrict our attention to two dimensional data as the idea is the same for data with higher dimensions .",
    "furthermore , here , we borrow the notation used in section [ multivariate pivots ] , and note that we first consider the randomization approach of method i. the effect of the other randomization methods on the volume of the resulting randomized confidence rectangles are to be addressed later on .",
    "consider the i.i.d .",
    "bivariate data @xmath233 , @xmath234 , with mean @xmath235 .",
    "furthermore , for ease of notation , let @xmath236 , where @xmath237 , as defined in ( [ eq 20 ] ) with @xmath238 , is the sample covariance matrix .",
    "the randomized version of the confidence rectangle ( [ classical confidence rectangle ] ) for @xmath246 , in view of method i , and based on the randomized pivot @xmath247 , as defined in ( [ eq 19 ] ) , is of the following form :            hence , similarly to the univariate case , in case of multidimensional data , under the conditions of section [ multivariate pivots ] , in view of method i , as @xmath4 , we have @xmath251 . in other words , method i yields randomized confidence regions for the mean vector , that shrink as the sample size increases .",
    "we remark that , in the multivariate case , methods i.1 and i.2 also yield randomized confidence rectangles of the form ( [ randomized confidence rectangle ] ) , with the notation @xmath16 therein replaced by @xmath140 , that shrink as the sample size increases . a similar argument to the one used to derive ( [ eq 10 ] ) shows that the latter conclusion concerning the shrinkage of the randomized confidence regions , in view of methods i , i.1 and i.2 , does not hold true when the randomized pivot @xmath252 is constructed using method ii .    [ [ appendix - ii - the - effect - of - method - i - on - mardias - measure - of - skewness ] ] appendix ii : the effect of method i on mardia s measure of skewness ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    a number of definitions for the concept of skewness of multivariate data can be found in the literature when the assumption of normality is dropped .",
    "mardia s characteristics of skewness for multivariate data , cf .",
    "mardia @xcite , is , perhaps , the most popular in the literature .",
    "this measures of skewness is valid when the covariance matrix of the distribution is nonsingular . for further discussions and developments on mardia s skewness and kurtosis characteristics , we refer to kollo @xcite and references therein .",
    "the following reasoning shows how small values of @xmath94 , as in method i , result in smaller values for mardia s measure of skewness for the randomized vectors @xmath259 as compared to that of @xmath260 .        where @xmath263 and @xmath264 are i.i.d . with respect to the joint distribution @xmath265 .",
    "the preceding relation shows that employing method i enables one to make mardia s characteristic of skewness arbitrarily small .",
    "sepanski , s. j. ( 1994 ) .",
    "asymptotics for multivariate t - statistic for random vectors in the generalized domain of attraction of the multivariate normal law .",
    "_ journal of multivaraite analysis _ , * 49 * , 41 - 54 .",
    "shao , q. m. ( 2005 ) .",
    "an explicit berry - essen bound for student s @xmath1-statistic via stein s method . in _ stein s method and applications , lect .",
    "notes ser .",
    "singap . _ * 5 * , 143 - 155 .",
    "singapore university press , singapore ."
  ],
  "abstract_text": [
    "<S> in this paper we present randomization methods to enhance the accuracy of the central limit theorem ( clt ) based inferences about the population mean @xmath0 . </S>",
    "<S> we introduce a broad class of randomized versions of the student @xmath1-statistic , the classical pivot for @xmath0 , that continue to possess the pivotal property for @xmath0 and their skewness can be made arbitrarily small , for each fixed sample size @xmath2 . </S>",
    "<S> consequently , these randomized pivots admit clts with smaller errors . </S>",
    "<S> the randomization framework in this paper also provides an explicit relation between the precision of the clts for the randomized pivots and the volume of their associated confidence regions for the mean for both univariate and multivariate data . </S>",
    "<S> this property allows regulating the trade - off between the accuracy and the volume of the randomized confidence regions discussed in this paper . </S>"
  ]
}