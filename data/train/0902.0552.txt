{
  "article_text": [
    "the rapid development and wide application of computer techniques permits to collect and store a huge amount data , where the number of measured variables is usually large .",
    "such high dimensional data occur in many modern scientific fields , such as micro - array data in biology , stock market analysis in finance and wireless communication networks .",
    "traditional estimation or test tools are no more valid , or perform badly for such high - dimensional data , since they typically assume a large sample size @xmath2 with respect to the number of variables @xmath0 . a better approach in this high - dimensional data setting would be based on asymptotic theory which has both @xmath2 and @xmath0 approaching infinity . to illustrate this purpose ,",
    "let us mention the case of hotelling s @xmath3-test .",
    "the failure of @xmath3-test for high - dimensional data has been mentioned as early as by @xcite . as a remedy ,",
    "dempster proposed a so - called non - exact test .",
    "however , the theoretical justification of dempster s test arises much later in @xcite inspired by modern random matrix theory ( rmt ) .",
    "these authors have found necessary correction for the @xmath3-test to compensate effects due to high dimension .    in this paper , we consider two lr tests concerning covariance matrices .",
    "we first give a theoretical explanation for the fail of these tests in high - dimensional data context .",
    "next , with the aid of random matrix theory , we provide necessary corrections to these lr tests to cope with the high dimensional effects .",
    "first , we consider the problem of one - sample covariance hypothesis test .",
    "suppose that @xmath4 follows a @xmath0-dimensional gaussian distribution @xmath5 and we want to test @xmath6 where @xmath7 denotes the @xmath0-dimensional identity matrix .",
    "note that testing @xmath8 with an arbitrary covariance matrix @xmath9 can always be reduced to the above null hypothesis by the transformation @xmath10 .",
    "let @xmath11 be a sample from @xmath4 , where we assume @xmath12 .",
    "the sample covariance matrix is @xmath13 and set @xmath14 the likelihood ratio test statistic is @xmath15 keeping @xmath0 fixed while letting @xmath16 , then the classical theory depicts that @xmath17 converges to the @xmath18 distribution under @xmath19 .",
    "however , as it will be shown , this classical approximation leads to a test size much higher than the nominal test level in the case of high - dimensional data , because @xmath17 approaches infinity for large @xmath0 .",
    "as seen from table 1 in  [ sec : test1 ] , for dimension and sample sizes @xmath20 , the realized size of the test is 22.5% instead of the nominal 5% level .",
    "the result is even worse for the case @xmath21 , with a 100% test size .",
    "based on a recent clt for linear spectral statistics ( lss ) of large - dimensional sample covariance matrices @xcite , we construct a corrected version of @xmath17 in  [ sec : test1 ] .",
    "as shown by the simulation results of  [ sec : simul1 ] , the corrected test performs much better in case of high dimensions .",
    "moreover , it also performs correctly for moderate dimensions like @xmath22 or 20 . for dimension and sample sizes",
    "@xmath23 cited above , the sizes of the corrected test are 5.9% and 5.2% , respectively , both close to the 5% nominal level .",
    "the second test problem we consider is about the equality between two high - dimensional covariance matrices .",
    "let @xmath24 and @xmath25 @xmath26 be observations from two @xmath0-dimensional normal populations @xmath27 , respectively .",
    "we wish to test the null hypothesis @xmath28 the related sample covariance matrices are @xmath29 where @xmath30 , @xmath31 are the respective sample means .",
    "let @xmath32 where @xmath33 and @xmath34 denote @xmath35 the likelihood ratio test statistic is @xmath36 and when @xmath37 , we get @xmath38 under @xmath39 . of cause , in this limit scheme ,",
    "the data dimension @xmath0 is held fixed .",
    "however , employing this @xmath1 limit distribution for dimensions like 30 or 40 , increases dramatically the size of the test .",
    "for instance , simulations in  [ sec : simul2 ] show that , for dimension and sample sizes @xmath40 , the test size equals 21.2% instead of the nominal 5% level .",
    "the result is worse for the case of @xmath41 , leading to a 49.5% test size .",
    "the reason for this fail of classical lr test is the following .",
    "modern rmt indicates that when both dimension and sample size are large , the likelihood ratio statistic @xmath42 drifts to infinity almost surely .",
    "therefore , the classical @xmath1 approximation leads to many false rejections of @xmath19 in case of high - dimensional data .",
    "based on recent clt for linear spectral statistics of @xmath43-matrices from rmt , we propose a correction to this lr test in   [ sec : test2 ] .",
    "although this corrected test is constructed under the asymptotic scheme @xmath44 , @xmath45 , @xmath46 , simulations demonstrate an overall correct behavior including small or moderate dimensions @xmath0 .",
    "for example , for the above cited dimension and sample sizes @xmath47 , the sizes of the corrected test equal 5.6% and 5.2% , respectively , both close to the nominal 5% level .",
    "related works include @xcite , @xcite and @xcite .",
    "these authors propose several procedures in the high - dimensional setting for testing that i ) a covariance matrix is an identity matrix , proportional to an identity matrix ( spherecity ) and is a diagonal matrix or ii ) several covariance matrices are equal .",
    "these procedures have the following common feature : their construction involves some well - chosen distance function between the null and the alternative hypotheses and rely on the first two spectral moments , namely the statistics tr@xmath48 and tr@xmath49 from sample covariance matrices @xmath48",
    ". therefore , the procedures proposed by these authors are different from the likelihood - based procedures we consider here . another important difference concerns the gaussian assumption on the random variables used in all these references . actually , for testing the equality between two covariance matrices , the correction proposed in this paper applies equally for non - gaussian and high - dimensional data leading to a valid pseudo - likelihood test .",
    "the rest of the paper is organized as following .",
    "preliminary and useful rmt results are recalled in  [ sec : useful ] . in  [ sec : test1 ] and  [ sec : test2 ] , we introduce our results for the two tests above .",
    "proofs and technical derivations are postponed to the last section .",
    "we first recall several results from rmt , which will be useful for our corrections to tests . for any @xmath50 square matrix @xmath51 with real eigenvalues @xmath52",
    ", @xmath53 denotes the empirical spectral distribution ( esd ) of @xmath51 , that is , @xmath54 we will consider random matrix @xmath51 whose esd @xmath53 converges ( in a sense to be precised ) to a limiting spectral distribution ( lsd ) @xmath55 . to make statistical inference about a parameter @xmath56 , it is natural to use the estimator @xmath57 which is a so - called linear spectral statistic ( lss ) of the random matrix @xmath51 .",
    "let @xmath58 be a double array of @xmath59 complex variables with mean 0 and variance 1 .",
    "set @xmath60 , the vectors @xmath61 is considered as an @xmath62 sample from some @xmath0-dimensional distribution with mean @xmath63 and covariance matrix @xmath7 . therefore the sample covariance matrix is @xmath64    for @xmath65 , let @xmath66 and @xmath67 .",
    "the marenko - pastur distribution of index @xmath68 , denoted as @xmath69 , is the distribution on @xmath70 $ ] with the following density function @xmath71[x - a(\\theta)]},\\quad   a(\\theta)\\le x\\le b(\\theta).\\ ] ]      assume that @xmath83 , and @xmath84 are @xmath59 random variables , such that @xmath85 moreover , @xmath86 as @xmath87 + then : + _ ( i ) _ real case .",
    "assume @xmath84 are real and @xmath88 then the random vector @xmath89 weakly converges to a @xmath90-dimensional gaussian vector with mean vector , @xmath91 and covariance function @xmath92 + where  @xmath93 is the stieltjes transform of   @xmath94 .",
    "the contours in ( [ 04var ] ) are non overlapping and both contain the support of @xmath95 . + _",
    "( ii ) _ complex case .",
    "assume @xmath84 are complex and @xmath96 , @xmath97 .",
    "then the conclusion of _",
    "( i ) _ also holds , except the mean vector is zero and the covariance function is half of the function given in ( [ 04var]).[t1.1 ]    it is worth noticing that theorem 1.1 in @xcite covers more general sample covariance matrices of form @xmath98 where @xmath99 is a given sequence of positive - definite hermitian matrices . in the `` white '' case @xmath100 as considered here , in a recent preprint @xcite , the authors offer a new extension of the clt where the constraints @xmath101 or 2 , as stated above , are removed .",
    "let @xmath58 and @xmath102 are two independent double arrays of @xmath59 complex variables with mean 0 and variance 1 .",
    "write @xmath103 and @xmath104 .",
    "also , for any positive integers @xmath105 , the vectors @xmath106 and @xmath107 can be thought as independent samples of size @xmath108 and @xmath109 , respectively , from some @xmath0-dimensional distributions .",
    "let @xmath110 and @xmath111 be the associated sample covariance matrices ,   @xmath112 @xmath113 then , the following so - called f - matrix generalizes the classical fisher - statistics for the present @xmath0-dimensional case , @xmath114 where @xmath115 . here",
    "we use the notation @xmath116 .    let @xmath117 under suitable moment conditions , the esd @xmath118 of @xmath119 has a lsd @xmath120 , which has a density",
    "[ see p72 of @xcite ] , given by @xmath121      & \\displaystyle{0 , \\quad\\quad \\quad\\quad \\mbox{otherwise}. }    \\end{array }    \\right.\\label{lsdden}\\ ] ] where @xmath122      recently , @xcite establishes a general clt for lss of large - dimensional f matrix .",
    "the following theorem is a simplified one quoted from it , which will play an important role .",
    "let @xmath132 , and assume : + for each p , @xmath133 and @xmath134 variables are @xmath135 , @xmath136 @xmath137 @xmath138   @xmath139 + then + _",
    "( i ) _ real case .",
    "assume @xmath140 and @xmath141 are real , @xmath142 , then the random vector @xmath143 weakly converges to a k - dimensional gaussian vector with the mean vector @xmath144\\nonumber\\\\      & & \\quad\\frac{1}{4\\pi i}\\oint_{|\\zeta|=1 }      f_j(z(\\zeta))\\left[\\frac{1}{\\zeta-{1\\over r}}+\\frac{1}{\\zeta+{1\\over            r}}-\\frac{2}{\\zeta+{y_2\\over            { hr}}}\\right]d\\zeta \\label{e1}\\\\      & & \\quad + \\frac{\\beta\\cdot y_1(1-y_2)^2}{2\\pi",
    "i \\cdot        h^2}\\oint_{|\\zeta|=1}f_j(z(\\zeta))\\frac{1}{(\\zeta+\\frac{y_2}{hr})^3}d\\zeta\\label{e1betax}\\\\      & & \\quad + \\frac{\\beta\\cdot y_2(1-y_2)}{2 \\pi i \\cdot        h}\\oint_{|\\zeta|=1}f_j(z(\\zeta))\\frac{\\zeta +        \\frac{1}{hr}}{(\\zeta+\\frac{y_2}{hr})^3}d\\zeta , \\label{e1betay }      \\quad\\quad j=1 , \\cdots , k ,    \\end{aligned}\\ ] ] where @xmath145 , \\quad    h = \\sqrt{y_1+y_2-y_1y_2 } , $ ] @xmath146 and the covariance function as @xmath147 @xmath148}\\nonumber\\\\      & &   -\\displaystyle\\frac{1}{2\\pi^2}\\oint_{|\\zeta_2|=1 }      \\oint_{|\\zeta_1|=1}\\frac{f_j(z(r_1\\zeta_1))f_\\ell(z(r_2\\zeta_2))r_1r_2}{(r_2\\zeta_2-r_1\\zeta_1)^2 }      d\\zeta_1d\\zeta_2,\\label{cov1}\\\\      & & -\\frac{\\beta \\cdot ( y_1+y_2)(1-y_2)^2}{4\\pi^2h^2 }      \\oint_{|\\zeta_1|=1}\\frac{f_j\\left ( z(\\zeta_1 )        \\right)}{(\\zeta_1+\\frac{y_2}{hr_1})^2}d\\zeta_1      \\oint_{|\\zeta_2|=1}\\frac{f_\\ell\\left ( z(\\zeta_2 )        \\right)}{(\\zeta_2+\\frac{y_2}{hr_2})^2}d\\zeta_2      \\label{cov1betax}\\\\      & & j , \\ell \\in \\{1,\\cdots , k\\}.\\nonumber    \\end{aligned}\\ ] ]    _ ( ii ) _ complex case .",
    "assume @xmath140 and @xmath141 are complex , @xmath149 then the conclusion of _ ( i ) _ also holds , except the means are @xmath150 $ ] and the covariance function is @xmath151 , $ ] where @xmath152 [ t2.1 ]    we should point out that zheng s clt for @xmath43-matrices covers more general situations then those cited in theorem  [ t2.1 ]",
    ". in particular , the fourth - moments @xmath153 and @xmath154 can be different .",
    "the following lemma will be used in ",
    "[ sec : test2 ] for an application of theorem [ t2.1 ] to obtain the formula ( [ teste ] ) and ( [ testvar ] ) .    for the function @xmath155 ,",
    "let @xmath156 be the unique solution to the equations @xmath157 analogously , let @xmath158 be the constants similar to @xmath159 but for the function @xmath160 then , the mean and covariance functions in ( [ e1 ] ) and ( [ cov1 ] ) equal to @xmath161 [ lem1 ]",
    "to test the hypothesis @xmath162 , let be the sample covariance matrix * s * and likelihood ratio statistic @xmath17 as defined in ( [ s ] ) and ( [ singlvclassict ] ) , respectively .",
    "for @xmath163 the array @xmath164 contains @xmath0-dimensional standard normal variables under @xmath19 .",
    "let @xmath165 and @xmath166    assuming that the conditions of theorem [ t1.1 ] hold , @xmath167 is defined as ( [ l * ] ) and @xmath168 . then , under @xmath19 and when @xmath169 @xmath170 \\rightarrow n \\left ( 0 ,      1\\right ) ,      \\label{singlsta}\\ ] ] where @xmath171 is the marenko - pastur law of index @xmath172 .    because the difference between @xmath173 and @xmath174 is a rank-1 matrix , @xmath173 and @xmath175 have the same lsd .",
    "so , @xmath167 and @xmath176 have the same asymptotic distribution",
    ". we also have @xmath177 so that @xmath178 by theorem [ t1.1 ] , @xmath179 weakly converges to a gaussian vector with the mean @xmath180 and variance @xmath181 for the real case , which are calculated in  [ sec : proofs ] . for the complex case ,",
    "the mean @xmath182 is zero and the variance is half of @xmath183 then , by ( [ singlesd - plsd ] ) we arrive at @xmath184 where @xmath185 can be calculated by the density of lsd of sample covariance matrix in  [ sec : proofs ] .",
    "because @xmath176 and @xmath186 have the same asymptotic distribution and ( [ 1connec ] ) , finally we get @xmath187 \\rightarrow n \\left ( 0 , 1\\right).\\nonumber    \\end{aligned}\\ ] ]      for different values of @xmath23 , we compute the realized sizes of traditional likelihood ratio test ( lrt ) and the corrected likelihood ratio test ( clrt ) proposed previously .",
    "the nominal test level is set to be @xmath188 , and for each @xmath189 , we run 10,000 independent replications with real gaussian variables .",
    "results are given in table  1 and figure  1 below .",
    ".sizes and powers of the traditional lrt and the corrected lrt , based on 10,000 independent applications with real gaussian variables .",
    "powers are estimated under the alternative @xmath190 . [ cols=\"<,^,^,^,^,^ \" , ]      with real gaussian variables .",
    "10 000 independent runs with 5% nominal level and sample size @xmath191.[1],title=\"fig:\",width=340 ] +    as seen from table  1 , the traditional lrt always rejects @xmath19 when @xmath0 is large , like @xmath192 or 300 , while the sizes produced by the corrected lrt perfectly matches the nominal level . for moderate dimensions like @xmath193 ,",
    "the corrected lrt still performs correctly while the traditional lrt has a size much higher than 5% .",
    "let @xmath194 and @xmath195 be observations from two normal populations @xmath27 , respectively .",
    "we examine the test defined in ( [ a0 ] ) and ( [ l1ab ] ) .",
    "the aim is to find a good scaling of the lr statistic @xmath42 , such that the scaled statistic weakly converges to some limiting distribution .",
    "let @xmath196 where @xmath197 denotes the common covariance matrix under @xmath19 .",
    "note that in a strict sense , the vectors @xmath198 and the matrices @xmath199 depend on @xmath0 .",
    "however we do not signify this dependence in notations for ease of statements . due to gaussian assumption",
    ", the arrays @xmath200 and @xmath201 contain i.i.d .",
    "@xmath202 variables , for which we can apply theorem [ t2.1 ] .",
    "let @xmath203 where @xmath204 note that @xmath205 forms a random f - matrix and we have @xmath206    [ t4.1 ] assuming that the conditions of theorem [ t2.1 ] hold under @xmath19 , @xmath207 as defined in ( [ l1ab ] ) and @xmath208 then , under @xmath19 and as @xmath209 , @xmath210 \\rightarrow n \\left ( 0 , 1\\right).\\label{lst}\\ ] ]    as @xmath211 and @xmath212 are rank-1 random matrices , @xmath213 and @xmath214 have the same lsd . also by ( [ s1s2cd ] ) , @xmath215 and",
    "@xmath216 have the same asymptotic distribution . because @xmath217df_n^{v_n}(x ) .",
    "\\end{aligned}\\ ] ] define @xmath218 , by @xmath219 and @xmath220 , also it can be written as @xmath221 from @xmath222 we get @xmath223    by theorem [ t2.1 ] , @xmath224 weakly converges to a gaussian vector with mean @xmath225\\label{teste}\\ ] ] and variance @xmath226 for the real case , which are calculated by lemma [ lem1 ] in  [ sec : proofs ] . for the complex case ,",
    "the mean @xmath227 is zero and the variance is half of @xmath228 . in other words , @xmath229 where @xmath230      \\quad      & + & \\displaystyle{\\frac{(y_{n_1}+y_{n_2}-y_{n_1}y_{n_2})}{y_{n_1}y_{n_2}}\\log{(y_{n_1}+y_{n_2 } ) } }      + \\displaystyle{\\frac{y_{n_1}(1-y_{n_2})}{y_{n_2}(y_{n_1}+y_{n_2})}\\log{(1-y_{n_2})}}\\nonumber\\\\      \\quad\\quad\\quad&+&\\displaystyle{\\frac{y_{n_2}(1-y_{n_1})}{y_{n_1}(y_{n_1}+y_{n_2})}\\log{(1-y_{n_1})}},\\nonumber      \\label{limit }    \\end{aligned}\\ ] ] is derived by use of the density of @xmath129 in  [ sec : proofs ] . because @xmath215and @xmath216 have the same asymptotic distribution and by ( [ 2connec ] ) , we get by letting @xmath231 , @xmath232 \\rightarrow n \\left ( 0 , 1\\right).\\ ] ]      for different values of @xmath233 , we compute the realized sizes of the traditional lrt and the corrected lrt with 10,000 independent replications .",
    "the nominal test level is @xmath234 and we use real gaussian variables .",
    "results are summarized in table 2 and figure 2 .     & & + ( p , @xmath235 , @xmath236 ) & size & difference with 5% & power & size & power + ( 5 , 100 , 100 ) & 0.0770&0.0270&1 & 0.0582&1 + ( 10 , 200 , 200 ) & 0.0680&0.0180&1 & 0.0684&1 + ( 20 , 400 , 400 ) & 0.0593&0.0093&1 & 0.0872&1 + ( 40 , 800 , 800 ) & 0.0526&0.0026&1 & 0.1339&1 + ( 80 , 1600 , 1600 ) & 0.0501 & 0.0001&1&0.2687&1 + ( 160 , 3200 , 3200 ) & 0.0491 & -0.0009&1&0.6488&1 + ( 320 , 6400 , 6400 ) & 0.0447 & -0.0053&0.9671&1&1 +     +     & & + ( p , @xmath235 , @xmath236 ) & size & difference with 5% & power & size & power + ( 5 , 100 , 50 ) & 0.0781&0.0281&0.9925 & 0.0640&0.9849 + ( 10 , 200 , 100 ) & 0.0617&0.0117&0.9847 & 0.0752&0.9904 + ( 20 , 400 , 200 ) & 0.0573&0.0073&0.9775 & 0.1104&0.9938 + ( 40 , 800 , 400 ) & 0.0561&0.0061&0.9765 & 0.2115&0.9975 + ( 80 , 1600 , 800)&0.0521 & 0.0021&0.9702 & 0.4954&0.9998 + ( 160 , 3200 , 1600 ) & 0.0520&0.0020&0.9702&0.9433&1 + ( 320 , 6400 , 3200 ) & 0.0510 & 0.0010&1&0.9939&1 +    .",
    "right : @xmath237.[fig:2],title=\"fig:\",width=264 ] .",
    "right : @xmath237.[fig:2],title=\"fig:\",width=264 ]    as we can see , when the dimension @xmath0 increases , the traditional lrt leads to a dramatically high test size while the corrected lrt remains accurate .",
    "furthermore , for moderate dimensions like @xmath238 or 40 , the sizes of the traditional lrt are much higher than 5% , whereas the ones of corrected lrt are very close . by a closer look at the column showing the difference with 5%",
    ", we note that this difference rapidly decrease as @xmath0 increases for the corrected test .",
    "figure  2 gives a vivid sight of these comparisons between the traditional lrt and the corrected lrt in term of test sizes .",
    "as said in introduction , previous related works as @xcite , @xcite or @xcite all assume gaussian variables .",
    "in contrast , theorem  [ t4.1 ] applies for general distributions having a fourth moment . for these non gaussian data ,",
    "we consider the corrected lrt as generalized pseudo - likelihood ratio test ( or gaussian lrt ) .",
    "moreover , the methods proposed by these authors all rely on an appropriate normalization of the trace of squared difference between two sample covariances following the idea of @xcite .",
    "we believe that their method would strongly depend on the normality assumption ( which was supported by simulation results below ) .",
    "on the other hand , based on general understanding , the lrt contains much higher information from data and its poor performance observed up to now is just caused by its large bias when dimension is large .",
    "thus , from the intuitive understanding , we are confined ourselves to modify the lrt .",
    "let us develop in more details an example .",
    "assume that * x * follows a normalized @xmath239-distribution with 5 degree of freedom , that is @xmath240 * x * and * y * are i.i.d .",
    ", hence @xmath241 @xmath242 and @xmath243 we still employ the result in theorem [ t4.1 ] for the test of equality between two covariance matrices , where @xmath244      & \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad      + \\displaystyle{\\frac{6y_1 ^ 2y_2}{(y_1+y_2)^2}+\\frac{6y_1y_2 ^ 2}{(y_1+y_2)^2}\\big]}\\label{m1f}\\end{aligned}\\ ] ] and @xmath245 instead of @xmath227 and @xmath228 for real case , respectively .",
    "( [ m1f ] ) and ( [ v1f ] ) are calculated in ",
    "[ sec : proofs ] .",
    "the following table  [ tab : t ] summarizes a simulation study where we compare this corrected pseudo - lrt with the test proposed in @xcite .",
    "we use 1,000 independent replications with the above @xmath239-distributed variables .",
    "again , the nominal test level is @xmath234 .",
    "as we can see , the corrected pseudo - lrt performs correctly while schott s test is no more valid here since the variables are not gaussian .     &",
    "clrt size & schott s size + ( 10,100 , 200 ) & 0.067&0.517 + ( 20 , 200 , 400 ) & 0.065&0.603 + ( 40 , 400 , 800 ) & 0.054&0.703 + ( 80 , 800 , 1600)&0.048 & 0.764 + ( 160 , 1600 , 3200 ) & 0.045&0.826 + ( 320 , 3200 , 6400 ) & 0.051&0.854 +",
    "by theorem [ t1.1 ] , for @xmath168 , by using the variable change @xmath246 , we have @xmath247d\\theta\\\\    & = & \\frac{y-\\log(1-y)}{2}-\\frac{1}{4\\pi}\\int_{0}^{2\\pi }    \\left[y-2\\sqrt{y}\\cos\\theta-\\log|1-\\sqrt{y}e^{i        \\theta}|^2\\right]d\\theta\\\\    & = & -\\frac{\\log(1-y)}{2},\\end{aligned}\\ ] ] where @xmath248 is calculated in @xcite .      for @xmath249 , by theorem [ t1.1 ] , we have @xmath250 and @xmath251 it is easy to see that @xmath252 , where * 1 * means constant function equals to 1 . for stieltjes transform of @xmath95 , the following equation is given in @xcite , for @xmath253 , @xmath254 + let @xmath255 . for fixed @xmath256",
    ",  we have on a contour enclosed 1 , @xmath257 and -1 , but not 0 , @xmath258 and @xmath259^{-1 }      \\cdot ( m_{2}+1)^{-2 } \\cdot ( 1-\\frac{m_{1}+1}{m_{2}+1})^{-2 }      dm_{1}}\\\\    & = & \\displaystyle { y\\oint ( \\frac{1}{1+m_{1}}+\\frac{1-y}{y } ) \\cdot      \\sum\\limits^\\infty_{j=0}(1+m_1)^j ( m_2 + 1)^{-2 }      \\sum\\limits^\\infty_{\\ell=1}\\ell(\\frac{m_{1}+1}{m_{2}+1})^{\\ell-1}dm_1}\\\\    & = & \\displaystyle{2\\pi i\\cdot   \\frac{y}{(m_2 + 1)^{2}}}.\\end{aligned}\\ ] ] then we also get @xmath260 . similarly , @xmath261 .",
    "furthermore , @xmath262 and @xmath263^{-1 } dm_2}\\\\    & = & \\displaystyle { \\frac{y}{\\pi i } \\oint ( \\frac{1}{m_2 + 1}- \\frac      { 1}{m_2 - 1/(y-1)})(\\frac{1}{1+m_{2}}+\\displaystyle\\frac{1-y}{y } )      \\sum\\limits^\\infty_{j=0}(1+m_2)^j dm_2 } \\\\    & = & 2y.\\end{aligned}\\ ] ] by a computation in @xcite , we know that @xmath264 .",
    "finally , we obtain @xmath265      since @xmath171 is the marenko - pastur law of index @xmath266 , by using the variable change @xmath267 we have @xmath2684y_n\\sin^2\\theta d\\theta\\\\    & = & \\frac{1}{2\\pi}\\int_0^{2\\pi}\\left[2\\sin^2\\theta-\\frac{2\\sin^2\\theta}{1+y_n-2\\sqrt{y_n}\\cos\\theta }      \\left(\\log |1-\\sqrt{y_n}e^{i\\theta}|^2 - 1\\right)\\right]d \\theta\\\\    & = & 1-\\frac{y_n-1}{y_n}\\log ( 1-y_n),\\end{aligned}\\ ] ] where @xmath269 is calculated in @xcite .",
    "we use the variable change @xmath270 , where @xmath271 .",
    "when @xmath272 satisfy @xmath273 we have @xmath274 similarly , @xmath275 let @xmath276 note that @xmath277 . by theorem [ t2.1 ] , we have @xmath278d\\xi}\\nonumber\\\\    & = & \\displaystyle{\\frac{1}{4\\pi}\\int^{2\\pi}_{0 }      f(z(e^{i\\theta}))\\left[\\frac{1}{e^{i\\theta}-{1\\over r } }        + \\frac{1}{e^{i\\theta}+{1\\over r}}-\\frac{2}{e^{i\\theta}+{y_2\\over { hr}}}\\right]e^{i\\theta}d\\theta}\\nonumber\\\\    & = & \\displaystyle{\\frac{1}{4\\pi}\\int^{2\\pi}_{0 }      f(z(e^{i\\theta}))\\left[\\frac{1}{e^{-i\\theta}-{1\\over r } }        + \\frac{1}{e^{-i\\theta}+{1\\over r}}-\\frac{2}{e^{-i\\theta}+{y_2\\over            { hr}}}\\right]e^{-i\\theta}d\\theta}\\nonumber\\\\    & = & \\displaystyle\\frac{1}{8\\pi}\\int^{2\\pi}_{0 }    f(z(e^{i\\theta}))\\bigg\\{\\left[\\frac{1}{e^{i\\theta}-{1\\over r } }      + \\frac{1}{e^{i\\theta}+{1\\over r}}-\\frac{2}{e^{i\\theta}+{y_2\\over { hr}}}\\right]e^{i\\theta}+\\nonumber\\\\    & & \\left[\\frac{1}{e^{-i\\theta}-{1\\over          r}}+\\frac{1}{e^{-i\\theta}+{1\\over r } }      -\\frac{2}{e^{-i\\theta}+{y_2\\over { hr}}}\\right]e^{-i\\theta}\\bigg\\}d\\theta\\nonumber\\\\    & = & \\frac{1}{8\\pi}\\re\\bigg\\{\\int^{2\\pi}_{0 }    \\widetilde{f}(z(e^{i\\theta}))\\bigg[\\left(\\frac{1}{e^{i\\theta}-{1\\over          r } }      + \\frac{1}{e^{i\\theta}+{1\\over r}}-\\frac{2}{e^{i\\theta}+{y_2\\over { hr}}}\\right)e^{i\\theta}+\\nonumber\\\\      & & \\left(\\frac{r}{r - e^{i\\theta}}+\\frac{r}{r+e^{i\\theta } }      -\\frac{2hr}{y_2e^{i\\theta}+hr}\\right)\\bigg]d\\theta\\bigg\\}\\nonumber\\\\    & = & \\re\\bigg\\{\\frac{1}{8\\pi i}\\oint_{|\\xi|=1 }    \\widetilde{f}(z(\\xi))\\bigg[\\left(\\frac{1}{\\xi-{1\\over          r}}+\\frac{1}{\\xi+{1\\over r}}-\\frac{2}{\\xi+{y_2\\over { hr } } }      \\right)\\nonumber\\\\      & & + \\left(\\frac{r}{r-\\xi}+\\frac{r}{r+\\xi }      -\\frac{2hr}{y_2\\xi+hr}\\right)\\xi^{-1}\\bigg]d\\xi\\bigg\\}\\nonumber\\\\    & = & \\frac{1}{4}\\left(\\widetilde{f}(z(\\frac{1}{r}))+\\widetilde{f}(z(-\\frac{1}{r } ) )    -2\\widetilde{f}(z(-\\frac{y_2}{hr}))\\right)\\nonumber\\\\    & \\rightarrow & ^{r\\downarrow1}\\frac{1}{4}\\left[\\widetilde{f}(z(1))+\\widetilde{f}(z(-1))-2\\widetilde{f}(z(-\\frac{y_2}{h}))\\right]\\nonumber\\\\    & = & \\frac{1}{2}\\log\\frac{(c^2-d^2)h^2}{(ch - y_2d)^2}.\\nonumber\\end{aligned}\\ ] ]    let @xmath279 , where @xmath280 @xmath281 and @xmath282 .",
    "by theorem  [ t2.1 ] , we have @xmath283 when @xmath284 are poles .",
    "we can then choose @xmath285 so that @xmath286 is a not a pole .",
    "then we get @xmath287    & = & \\displaystyle{\\oint_{|\\xi_1|=1}\\frac{\\left(\\log(a+bz(r_1\\xi_1))\\right)'}{r_1\\xi_1-r_2\\xi_2}\\cdot      r_2 d\\xi_1}\\\\[3 mm ]    & = & \\displaystyle{\\oint_{|\\xi_1|=1}\\bigg[\\frac{bhr_1\\xi_1}{(r_1\\xi_1-r_2\\xi_2)(c+dr_1\\xi_1)c}\\cdot        \\frac{1}{\\xi_1+\\frac{d}{cr_1}}}\\\\      & & \\quad-\\frac{bhr_1^{-1}}{(r_1\\xi_1-r_2\\xi_2)(c+dr_1\\xi_1)c}\\cdot      \\frac{1}{(\\xi_1+\\frac{d}{cr_1})\\xi_1}\\cdot r_2\\bigg]d\\xi_1\\\\[5 mm ]    & = & \\displaystyle{2\\pi i}\\left (    \\frac{bhd^{-1}c^{-1}}{\\xi_2}-\\frac{bhd^{-1}r_2}{d+cr_2\\xi_2}\\right).\\end{aligned}\\ ] ]",
    "so , @xmath288 since the function @xmath289 is analytic , when @xmath290 but sufficiently close to 1 , we have @xmath291 for some constant @xmath292 .",
    "thus we have @xmath293      \\left(\\frac{bhd^{-1}c^{-1}}{\\xi_2}-\\frac{bhd^{-1}r_2}{d+cr_2\\xi_2}\\right)d\\xi_2\\right|}\\\\[4 mm ]    \\rightarrow & 0 \\quad \\mbox{as } \\quad r_2 \\downarrow 1 , \\end{array}\\ ] ] where the estimations are done according to@xmath294 or @xmath295 or not .",
    "thus , @xmath296 where @xmath297 because @xmath298 for @xmath158 satisfying @xmath299 and if @xmath300 we have @xmath301 .",
    "therefore , @xmath302d\\theta\\\\    & = & \\frac{1}{2\\pi}\\re\\bigg\\{\\int_{0}^{2\\pi}\\widetilde{g}(z(e^{i\\theta } ) )    \\bigg[\\left(\\frac{bhd^{-1}c^{-1}}{e^{i\\theta}}-\\frac{bhd^{-1}r_2}{d+cr_2e^{i\\theta}}\\right )      e^{i\\theta }      + bhd^{-1}c^{-1}-\\frac{bhd^{-1}r_2}{de^{i\\theta}+cr_2}\\bigg]d\\theta\\bigg\\}\\\\    & = & \\re\\bigg\\{\\frac{1}{2\\pi      i}\\oint_{|\\xi|_2=1}\\widetilde{g}(z(\\xi_2))\\bigg[\\left(\\frac{bhd^{-1}c^{-1}}{\\xi_2}-\\frac{bhd^{-1}r_2}{d+cr_2\\xi_2}\\right )      + \\left(bhd^{-1}c^{-1}-\\frac{bhd^{-1}r_2}{d\\xi_2+cr_2}\\right)\\xi_2^{-1}\\bigg]d\\xi_2\\bigg\\}\\\\    & = & bhd^{-1}c^{-1}\\left[\\widetilde{g}(z(0))-\\widetilde{g}(z(-\\frac{d}{cr_2}))\\right]\\\\    & \\rightarrow & bhd^{-1}c^{-1}\\left[\\widetilde{g}(z(0))-\\widetilde{g}(z(-\\frac{d}{c}))\\right]\\\\    & = & 2bhd^{-1}c^{-1}\\log\\frac{c\\gamma}{c\\gamma - d\\eta}.\\end{aligned}\\ ] ]      because * @xmath303 * and * @xmath304 * are gaussian variables , for real case , @xmath305**@xmath303**@xmath306 then ( [ e1betax ] ) , ( [ e1betay ] ) and ( [ cov1betax ] ) are all 0 .",
    "consider ( [ e1 ] ) and ( [ cov1 ] ) , as @xmath307 , by the computations done in the proof of lemma  [ lem1 ] , we see that termes tending to zero could be neglected in the considered contour integrals . hence we can put @xmath308 and use @xmath309 instead of @xmath310 .",
    "consider the variable change @xmath270 , where @xmath311 , \\quad h = \\sqrt{y_1+y_2-y_1y_2}$ ] .",
    "as @xmath312 we have by lemma  [ lem1 ] , @xmath313}\\\\    & = & \\displaystyle{\\frac{1}{2}\\left[\\log\\left(\\frac{y_1+y_2-y_1y_2}{y_1+y_2}\\right )        -\\frac{y_1}{y_1+y_2}\\log(1-y_2)-\\frac{y_2}{y_1+y_2}\\log(1-y_1)\\right]},\\end{aligned}\\ ] ] and @xmath314      by ( [ f(x ) ] ) and the density of @xmath315 ( the limiting distribution in ( [ lsdden ] ) but with @xmath130 in place of @xmath131 ) , where @xmath316 , @xmath317 and @xmath318 using the substitution @xmath319 we have @xmath320 therefore , @xmath321    & = & { \\int_{a_n}^{b_n}f(x)\\frac{(1-y_{n_2})\\sqrt{(b_n - x)(x - a_n)}}{2\\pi        x(y_{n_1}+y_{n_2}x)}dx}\\\\[6 mm ]    & = & { ( 1-y_{n_2})\\int_{a_n}^{b_n }      \\left[\\log\\left(y_{n_1}+y_{n_2}x\\right )        -\\frac{y_{n_2}}{y_{n_1}+y_{n_2}}\\log        x\\right]\\frac{\\sqrt{(b_n - x)(x - a_n)}}{2\\pi x(y_{n_1}+y_{n_2}x)}dx}\\\\    & & \\quad\\quad-{\\log\\left(y_{n_1}+y_{n_2}\\right)}\\\\[6 mm ]    & = & { \\frac{2(1-y_{n_2})}{\\pi}\\int_{0}^{\\pi}\\left [        \\log\\frac{\\left|h_n - y_{n_2}e^{i\\theta}\\right|^2}{(1-y_{n_2})^2 }        -\\frac{y_{n_2}}{y_{n_1}+y_{n_2}}\\log\\frac{\\left|1-h_ne^{i\\theta}\\right|^2 }        { ( 1-y_{n_2})^2}\\right]}\\\\[6 mm ] & & \\quad\\quad    { \\cdot\\frac{h_n^2\\sin^2\\theta}{\\left|1-h_ne^{i\\theta}\\right|^2        \\left|h_n - y_{n_2}e^{i\\theta}\\right|^2}d\\theta}- {      \\log\\left(y_{n_1}+y_{n_2}\\right)}\\\\[6 mm ]    & = & { \\frac{2(1-y_{n_2})}{\\pi}\\int_{0}^{\\pi } \\left [        \\log\\left|h_n - y_{n_2}e^{i\\theta}\\right|^2        -\\frac{y_{n_2}}{y_{n_1}+y_{n_2}}\\log\\left|1-h_ne^{i\\theta}\\right|^2        \\right ] } \\\\[4 mm ]    & & \\cdot{\\frac{h_n^2\\sin^2\\theta }      { \\left|1-h_ne^{i\\theta}\\right|^2\\left|h_n - y_{n_2}e^{i\\theta}\\right|^2}d\\theta }    -2\\left(1-\\frac{y_{n_2}}{y_{n_1}+y_{n_2}}\\right )    \\log(1-y_{n_2})-\\log{(y_{n_1}+y_{n_2})}\\\\    & = & { \\re\\bigg\\{\\frac{2(1-y_{n_2})}{\\pi}\\int_{0}^{2\\pi }      \\left [ \\log(h_n - y_{n_2}e^{i\\theta } )        -\\frac{y_{n_2}}{y_{n_1}+y_{n_2}}\\log(1-h_ne^{i\\theta})\\right]}\\\\    & &        { \\frac{h_n^2\\sin^2\\theta}{\\left|1-h_ne^{i\\theta}\\right|^2            \\left|h_n - y_{n_2}e^{i\\theta}\\right|^2}d\\theta\\bigg\\}}-\\frac{2y_{n_1}}{y_{n_1}+y_{n_2 } }        \\log(1-y_{n_2})-\\log{(y_{n_1}+y_{n_2})}\\\\[5 mm ]        & = & { \\re\\bigg\\{\\frac{-(1-y_{n_2})}{2\\pi i}\\oint_{|z|=1 }          \\left[\\log(h_n - y_{n_2}z)-\\frac{y_{n_2}}{y_{n_1}+y_{n_2}}\\log(1-h_nz)\\right]}\\\\[5 mm ]        & & \\cdot{\\frac{h_n^2(z - z^{-1})^2}{z\\left|1-h_nz\\right|^2            \\left|h_n - y_{n_2}z\\right|^2}dz\\bigg\\}}-{\\frac{2y_{n_1}}{y_{n_1}+y_{n_2 } }          \\log(1-y_{n_2})-\\log{(y_{n_1}+y_{n_2})}}\\\\[5 mm ]        & = & { \\re\\bigg\\{\\frac{y_{n_2}-1}{y_{n_2}}\\cdot\\frac{1}{2\\pi            i}\\oint_{|z|=1}\\left [            \\log(h_n - y_{n_2}z)-\\frac{y_{n_2}}{y_{n_1}+y_{n_2}}\\log(1-h_nz)\\right]}\\\\[6 mm ]        & & \\cdot{\\frac{(z^2 - 1)^2}{z(z - h_n)(z-\\frac{1}{h_n } )            ( z-\\frac{y_{n_2}}{h_n})(z-\\frac{h_n}{y_{n_2}})}dz}\\bigg\\ }        -{\\frac{2y_{n_1}}{y_{n_1}+y_{n_2 } }          \\log(1-y_{n_2})-\\log{(y_{n_1}+y_{n_2})}}.\\end{aligned}\\ ] ] there are three poles inside the unit circle : 0 , @xmath322 .",
    "their corresponding residues are @xmath323,\\nonumber\\\\    r(\\frac{y_{n_2}}{h_n})&= &    \\frac{(y_{n_2}^2-h_n^2)}{y_{n_2}(y_{n_2}-h_n^2 ) } \\left[\\log      ( h^2_n - y_{n_2}^2)-\\log ( h_n)-\\frac{y_{n_2}}{y_{n_1}+y_{n_2}}\\log      ( 1-y_{n_2 } ) \\right].\\nonumber\\end{aligned}\\ ] ] therefore , @xmath324    & & \\quad    + \\displaystyle{\\frac{(y_{n_1}+y_{n_2}-y_{n_1}y_{n_2})}{y_{n_1}y_{n_2}}\\log{(y_{n_1}+y_{n_2 } ) } }    + \\displaystyle{\\frac{y_{n_1}(1-y_{n_2})}{y_{n_2}(y_{n_1}+y_{n_2})}\\log{(1-y_{n_2})}}\\nonumber\\\\    & & \\quad\\quad\\quad+\\displaystyle{\\frac{y_{n_2}(1-y_{n_1})}{y_{n_1}(y_{n_1}+y_{n_2})}\\log{(1-y_{n_1})}}.\\nonumber\\end{aligned}\\ ] ]      because * x * and * y * are random variables from normalized @xmath239-distribution with 5 degree of freedom , * x * and * y * are @xmath59 , @xmath241 @xmath242 and @xmath243 for real case , @xmath305**@xmath303**@xmath325 ( [ e1 ] ) and ( [ cov1 ] ) items are the same to the gaussian variables .",
    "consider the items ( [ e1betax ] ) , ( [ e1betay ] ) and ( [ cov1betax ] ) .",
    "as the same explanation in proof of ( [ teste ] ) and ( [ testvar ] ) , we use @xmath326 instead .    for ( [ e1betax ] ) , we have @xmath327\\\\    & & \\quad\\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad\\quad    \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad    \\quad\\quad\\quad\\cdot \\frac{1}{(\\xi+\\frac{y_2}{hr})^3 } d\\xi\\\\    & = & \\frac{\\beta\\cdot y_1(1-y_2)^2}{2\\pi i \\cdot h^2 } \\oint_{|\\xi|=1 }    2    \\mathcal{r}\\big\\{\\log(h+y_2\\xi)-\\frac{y_2}{y_1+y_2}\\log(1+h\\xi)\\big\\}\\cdot    \\frac{1}{(\\xi+\\frac{y_2}{hr})^3 } d\\xi\\\\    & = & \\frac{\\beta\\cdot y_1(1-y_2)^2}{2\\pi i \\cdot h^2 }    \\oint_{|\\xi|=1}\\bigg\\{\\log(h+y_2\\xi)+\\log(h+y_2\\overline{\\xi})\\\\    & & \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad    -\\frac{y_2}{y_1+y_2}\\big[\\log(1+h\\xi)+\\log(1+h\\overline{\\xi})\\big]\\bigg\\ }    \\cdot\\frac{1}{(\\xi+\\frac{y_2}{hr})^3 } d\\xi\\\\    & = & \\frac{\\beta\\cdot y_1(1-y_2)^2}{2\\pi i \\cdot h^2 }    \\oint_{|\\xi|=1}\\left[\\log(h+y_2\\xi)-\\frac{y_2}{y_1+y_2}\\log(1+h\\xi)\\right ]    \\\\ & & \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad    \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad    \\left[\\frac{1}{(\\xi+\\frac{y_2}{hr})^3 }      + \\frac{\\left(\\frac{hr}{y_2}\\right)^3\\xi}{(\\frac{hr}{y_2}+\\xi)^3 }      \\right ] d\\xi\\\\    & = & \\frac{\\beta\\cdot y_1(1-y_2)^2}{2\\pi i \\cdot h^2 } \\cdot 2\\pi \\cdot    \\frac{1}{2 }    \\left[\\log(h+y_2\\xi)-\\frac{y_2}{y_1+y_2}\\log(1+h\\xi)\\right]''\\bigg|_{\\xi=-\\frac{y_2}{hr}}\\\\    & = & \\frac{\\beta\\cdot y_1(1-y_2)^2}{2 h^2 }    \\left[-\\frac{y_2 ^ 2}{(h+y_2\\xi)^2}+\\frac{y_2}{y_1+y_2}\\frac{h^2}{(1+h\\xi)^2}\\right]\\bigg|_{\\xi=-\\frac{y_2}{hr}}\\\\    & = & \\frac{\\beta y_1 ^ 2y_2}{2(y_1+y_2)^2}.\\end{aligned}\\ ] ]    for ( [ e1betay ] ) , we have @xmath328\\\\    & & \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\cdot    \\frac{\\xi^2-\\frac{y_2}{h^2r^2}}{(\\xi+\\frac{y_2}{hr})^2}\\left [      \\frac1{\\xi-\\frac{\\sqrt{y_2}}{hr}}+\\frac1{\\xi+\\frac{\\sqrt{y_2}}{hr}}-\\frac2{\\xi+\\frac{y_2}{hr}}\\right]d\\xi\\\\    & = & \\frac{\\beta\\cdot ( 1-y_2)y_2}{2\\pi i \\cdot h } \\oint_{|\\xi|=1 }    \\left[\\log\\frac{|h+y_2\\xi|^2}{(1-y_2)^2}-\\frac{y_2}{y_1+y_2}\\log\\frac{|1+h\\xi|^2}{(1-y_2)^2}-\\log(y_1+y_2)\\right]\\\\    & & \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad    \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\cdot    \\left[\\frac{\\xi+\\frac{1}{hr}}{(\\xi+\\frac{y_2}{hr})^3}\\right ]    d\\xi\\\\    & = & \\frac{\\beta\\cdot ( 1-y_2)y_2}{2\\pi i \\cdot h } \\oint_{|\\xi|=1}2    \\mathcal{r}\\big\\{\\log(h+y_2\\xi)-\\frac{y_2}{y_1+y_2}\\log(1+h\\xi)\\big\\ }    \\cdot \\left[\\frac{\\xi+\\frac{1}{hr}}{(\\xi+\\frac{y_2}{hr})^3}\\right ]    d\\xi\\\\    & = & \\frac{\\beta\\cdot ( 1-y_2)y_2}{2\\pi i \\cdot h}\\oint_{|\\xi|=1}\\big [      \\log(h+y_2\\xi)+\\log(h+y_2\\overline{\\xi})\\\\      & & \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad      -\\frac{y_2}{y_1+y_2}\\left(\\log(1+h\\xi)+\\log(1+h\\overline{\\xi})\\right )      \\big]\\cdot    \\left[\\frac{\\xi+\\frac{1}{hr}}{(\\xi+\\frac{y_2}{hr})^3}\\right]d\\xi\\\\    & = & \\frac{\\beta\\cdot ( 1-y_2)y_2}{2\\pi i \\cdot      h}\\oint_{|\\xi|=1}\\left[\\log(h+y_2\\xi)-\\frac{y_2}{y_1+y_2}\\log(1+h\\xi)\\right]\\\\    & & \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\cdot\\big[\\frac{\\xi+\\frac{1}{hr}}{(\\xi+\\frac{y_2}{hr})^3}+      \\frac{\\frac{h^2r^2}{y_2 ^ 3}(\\xi+hr)}{(\\xi+\\frac{hr}{y_2})^3}\\big]d\\xi\\\\    & = & \\frac{\\beta\\cdot ( 1-y_2)y_2}{2\\pi",
    "i \\cdot h}2\\pi i \\cdot    \\frac1{2}\\left([\\log(h+y_2\\xi)-\\frac{y_2}{y_1+y_2}\\log(1+h\\xi ) ]    \\cdot(\\xi+\\frac{1}{hr})\\right)''\\bigg|_{\\xi=-\\frac{y_2}{hr}}\\\\    & = & \\frac{\\beta y_1y_2 ^ 2}{2(y_1+y_2)^2}.\\end{aligned}\\ ] ] therefore , @xmath329.\\nonumber\\end{aligned}\\ ] ]    for covariance , we have @xmath330    \\cdot \\frac{1}{(\\xi+\\frac{y_2}{hr})^2 } d\\xi\\\\    & = & \\oint_{|\\xi|=1 } 2    \\mathcal{r}\\big\\{\\log(h+y_2\\xi)-\\frac{y_2}{y_1+y_2}\\log(1+h\\xi)\\big\\}\\cdot    \\frac{1}{(\\xi+\\frac{y_2}{hr})^2 } d\\xi\\\\    & = & \\oint_{|\\xi|=1}\\bigg\\{\\log(h+y_2\\xi)+\\log(h+y_2\\overline{\\xi})\\\\    & & \\quad\\quad    -\\frac{y_2}{y_1+y_2}\\big[\\log(1+h\\xi)+\\log(1+h\\overline{\\xi})\\big]\\bigg\\ }    \\cdot\\frac{1}{(\\xi+\\frac{y_2}{hr})^2 } d\\xi\\\\    & = &    \\oint_{|\\xi|=1}\\left[\\log(h+y_2\\xi)-\\frac{y_2}{y_1+y_2}\\log(1+h\\xi)\\right ]    \\left[\\frac{1}{(\\xi+\\frac{y_2}{hr})^2 }      + \\frac{\\left(\\frac{hr}{y_2}\\right)^2}{(\\xi+\\frac{hr}{y_2})^2 }      \\right ] d\\xi\\\\    & = & 2\\pi i \\cdot \\left[\\log(h+y_2\\xi)-\\frac{y_2}{y_1+y_2}\\log(1+h\\xi)\\right]'\\bigg|_{\\xi=-\\frac{y_2}{hr}}\\\\    & = & \\pi",
    "i \\cdot    \\left[\\frac{y_2}{h+y_2\\xi}+\\frac{y_2}{y_1+y_2}\\frac{h}{1+h\\xi}\\right]\\bigg|_{\\xi=-\\frac{y_2}{hr}}\\\\    & = & 0.\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> in this paper , we give an explanation to the failure of two likelihood ratio procedures for testing about covariance matrices from gaussian populations when the dimension is large compared to the sample size . </S>",
    "<S> next , using recent central limit theorems for linear spectral statistics of sample covariance matrices and of random f - matrices , we propose necessary corrections for these lr tests to cope with high - dimensional effects . </S>",
    "<S> the asymptotic distributions of these corrected tests under the null are given . </S>",
    "<S> simulations demonstrate that the corrected lr tests yield a realized size close to nominal level for both moderate @xmath0 ( around 20 ) and high dimension , while the traditional lr tests with @xmath1 approximation fails .    </S>",
    "<S> another contribution from the paper is that for testing the equality between two covariance matrices , the proposed correction applies equally for non - gaussian populations yielding a valid pseudo - likelihood ratio test . </S>"
  ]
}