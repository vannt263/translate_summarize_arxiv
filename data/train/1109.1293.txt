{
  "article_text": [
    "consider a sensor network in which a sensor measures a certain physical quantity @xmath0 over time @xmath1 the aim of the sensor is communicating a symbol - by - symbol processed version @xmath2 of the measured sequence @xmath3 to a receiver .",
    "as an example , each element @xmath4 can be obtained by quantizing or denoising @xmath0 , for @xmath1 to this end , based on the observation of @xmath5 and @xmath6 , the sensor communicates a message @xmath7 of @xmath8 bits to the receiver ( @xmath9 is the message rate in bits per source symbol ) .",
    "the receiver is endowed with sensing capabilities , and hence it can measure the physical quantity @xmath6 as well .",
    "however , as the receiver is located further away from the physical source , such measure may come with some delay , say @xmath10 for some @xmath11 . assuming that at time @xmath12 the decoder must put out an estimate @xmath13 of the @xmath14th source symbol @xmath4 by design constraints",
    ", it follows that the estimate @xmath13 can be made to be a function of the message @xmath7 and of the delayed side information @xmath15 ( see @xcite for an illustration ) . following related literature ( e.g. , @xcite )",
    ", we will refer to @xmath16 as the delay for simplicity .",
    "delay @xmath16 may or may not be known at the sensor .",
    "the situation described above can be illustrated schematically as in fig .",
    "[ fig0 ] for the case in which the delay @xmath16 is known at the encoder . in fig .",
    "[ fig0 ] , the encoder ( enc ) represents the sensor and the decoder ( dec ) the receiver .",
    "the decoder at time @xmath14 ( more precisely , @xmath12 ) has access to delayed _ side information _",
    "@xmath17 with delay @xmath18 fig .",
    "[ fig2 ] accounts for a setting where the side information at the decoder , unbeknownst to the encoder , _ may _ be delayed by @xmath16 or not delayed , where the first case is modelled by decoder 1 and the second by decoder 2 .",
    "note that , in the latter case , the receiver has available the sequence @xmath19 at time @xmath14 . for generality , in the setting in fig .",
    "[ fig2 ] , we further assume that the encoder is allowed to send additional information in the form of a message @xmath20 of @xmath21 bits when the side information is not delayed",
    ". this can be justified in the sensor example mentioned above , as a non - delayed side information may entails that the receiver is closer to the transmitter and is thus able to decode an additional message of rate @xmath22 ( bits / source symbol ) .          to start ,",
    "let us first assume that sequences @xmath5 and @xmath6 are _ memoryless sources _ so that the entries ( @xmath23 ) are arbitrarily correlated for a given index @xmath14 but independent identically distributed ( i.i.d . ) for different @xmath24 to streamline the discussion , the following lemma summarizes the optimal trade - off between rate @xmath9 and distortion @xmath25 , as measured by a distortion metric @xmath26 , for the point - to - point setting of fig .",
    "[ fig0 ] with memoryless sources .",
    "similar conclusions apply for the more general set - up of fig .",
    "[ fig2 ] .",
    "@xcite for memoryless source , and zero delay , i.e. , @xmath27 , the rate - distortion function for the point - to - point system in fig .",
    "[ fig0 ] is given by the conditional rate - distortion function @xmath28\\leq d}}}i(x;z|y).\\ ] ] this result remains unchanged even if the decoder has access to non - causal side information , i.e. , if the reconstruction @xmath13 can be based on the entire sequence @xmath6 , rather than only @xmath29 . instead , for strictly positive delay @xmath30",
    ", the rate - distortion function is the same as if there was no side information , namely @xmath31\\ensuremath{\\leq}d}}}i(x;z)$ ] .",
    "similar conclusions can be easily shown to apply also for the more general model of fig .",
    "[ fig2 ] , as it will be discussed in the paper ( see sec . [",
    "sec : lossy - source - coding ] ) . specifically , if @xmath30 and the sources are memoryless , the rate - distortion function for the system of fig .",
    "2 with @xmath32 reduces to the one obtained by kaspi in @xcite for a model in which decoder 1 has no side information , and , for general @xmath33 , the rate - distortion region coincides with the one obtained in @xcite for a model with no side information at decoder 1 .",
    "we have seen in lemma 1 that , for memoryless sources , no advantages can be accrued by leveraging a ( strictly ) delayed side information , i.e. , with @xmath34 however , this conclusion does not generally hold if the sources have memory . in this context , a number of works have focused on the scenario of fig .",
    "[ fig0 ] where @xmath35 for @xmath36 this entails that the decoder observes sequence @xmath5 itself , but with a delay of @xmath16 symbols .",
    "this setting is typically referred to as _ source coding with feedforward _ , and was introduced in @xcite .",
    "reference @xcite derived the rate - distortion function for this problem ( i.e. , fig .",
    "[ fig0 ] with @xmath35 ) for ergodic and stationary sources in terms of multi - letter mutual informations .",
    "the result was also extended to arbitrary sources using information - spectrum methods .",
    "achievability was obtained via the use of a codebook of codetrees .",
    "the function was explicitly evaluated for some special cases in @xcite ( see also @xcite ) , and @xcite proposed an algorithm for its numerical calculation .    the more general case of fig .",
    "[ fig0 ] with @xmath37 was studied in @xcite assuming stationary and ergodic sources @xmath5 and @xmath6 .",
    "the rate - distortion function was expressed in terms of multi - letter mutual informations .",
    "no specific examples were provided for which the function is explicitly computable .",
    "we finally remark that for more complex networks than the ones studied here , strictly delayed side information may be useful even in the presence of memoryless sources .",
    "this was illustrated in @xcite for a multiple description problem with feedforward .",
    "the goal of this work is to characterize the rate - distortion trade - offs for the setting in fig .",
    "[ fig0 ] and the more general set - up in fig .",
    "[ fig2 ] for a specific class of sources @xmath5 and @xmath6 .",
    "specifically , we assume that @xmath6 is a markov chain , and @xmath5 is such that @xmath4 is obtained by passing @xmath0 through a channel @xmath38 for @xmath39 as illustrated in fig .",
    "[ figmarkov ] .",
    "the process is thus a hidden markov model .",
    "this model complies with the type of sensor network scenarios described above , where @xmath6 is the physical quantity of interest , modelled as a markov chain , and @xmath5 is a symbol - by - symbol processed version of @xmath40 the main contributions and the paper organization are as follows .",
    "after the description of the system model in sec .",
    "[ sec : system - model ] , for the source statistics described above ,    * we derive a single - letter characterization of the minimal rate ( bits / source symbol ) required for asymptotically lossless compression in the point - to - point model of fig .",
    "[ fig0 ] for any delay @xmath11 ( sec .",
    "[ sec : lossless - source - coding ] ) .",
    "achievability is based on a novel scheme that consists of simple multiplexing / demultiplexing operations along with standard entropy coding techniques ; * we derive a single - letter characterization of the minimal rate ( bits / source symbol ) required for lossy compression for the point - to - point model of fig .",
    "[ fig0 ] and , more generally , for the model of fig .",
    "[ fig2 ] in which the side information may be delayed , for delays @xmath27 and @xmath41 ( sec . [ sec : lossy - source - coding ] ) ; * we solve a number of specific examples , namely binary - alphabet sources with hamming distortion and gaussian sources with minimum mean square error distortion , and present related numerical results ( sec .",
    "[ sec : examples ] ) .",
    "we present the system model for the scenario of fig .",
    "[ fig2 ] . as detailed below , the scenarios of fig .",
    "[ fig0 ] is obtained as a special case .",
    "the system is characterized by a delay @xmath11 ; finite alphabets @xmath42 , @xmath43 , @xmath44 , @xmath45 conditional probabilities @xmath46 , with @xmath47 and @xmath48 with @xmath49 and @xmath50 ( i.e. , we have @xmath51 and @xmath52 for all @xmath53 ) ; and distortion metrics @xmath54 : @xmath55 $ ] , such that @xmath56 for all @xmath57 for @xmath58 .",
    "as explained below , the subscript `` 1 '' in @xmath46 indicates that @xmath46 denotes one - step transition probabilities .    the random process @xmath59 , @xmath60 , is a stationary and ergodic markov chain with transition probability @xmath61=w_{1}(a|b).$ ]",
    "we define the probability @xmath62\\triangleq\\pi(a)$ ] and also the @xmath63-step transition probability @xmath64\\triangleq w_{k}(a|b),$ ] which are both independent of @xmath14 by the stationarity of @xmath0 .",
    "these quantities can be calculated using standard markov chain theory from the transition matrix associated with @xmath46 ( see , e.g. , @xcite ) .",
    "we also set , for notational convenience , @xmath65 .",
    "sequence @xmath3 is thus distributed as @xmath66 for any integer @xmath67    the random process @xmath68",
    ", @xmath60 is such that vector @xmath69 , for any integer @xmath70 , is jointly distributed with @xmath6 so that @xmath71 in other words , process @xmath68 , @xmath60 corresponds to a hidden markov model with underlying markov process given by @xmath40    we now define encoder and decoders for the setting of fig .",
    "specifically , an @xmath72 code is defined by : ( _ i _ ) an encoder function @xmath73\\times\\lbrack1,2^{n\\delta r}],\\label{eq : encoder}\\ ] ] which maps sequences @xmath5 and @xmath6 into messages @xmath74 $ ] and @xmath75;$ ] ( _ ii _ ) a sequence of decoding functions for decoder 1 @xmath76\\times\\mathcal{y}^{i - d}\\rightarrow\\mathcal{z}_{1},\\label{eq : decoder}\\ ] ] for @xmath77 $ ] , which , at each time @xmath78 map message @xmath79 or rate @xmath9 [ bits / source symbol ] , and the delayed side information @xmath17 into the estimate @xmath80 ; ( _ iii _ ) a sequence of decoding function for decoder 2 @xmath81\\times[1,2^{n\\delta r}]\\times\\mathcal{y}^{i}\\rightarrow\\mathcal{z}_{2}\\label{eq : decoder2}\\ ] ] for @xmath77 $ ] , which , at each time @xmath78 map messages @xmath79 or rate @xmath82 and @xmath83 of rate or rate @xmath84 and the non - delayed side information @xmath29 into the estimate @xmath85 . in ( [ eq : encoder])-([eq : decoder2 ] ) , for @xmath86 integer with @xmath87 , we have defined @xmath88 $ ] as the interval @xmath89 $ ] with @xmath88=\\phi$ ] if @xmath90 . and @xmath91",
    "are implicitly considered to be rounded up to the nearest larger integer . ] encoding / decoding functions ( [ eq : encoder])-([eq : decoder2 ] ) must satisfy the distortion constraints @xmath92\\leq d_{j},\\text { for } j=1,2.\\label{dist constraints}\\ ] ] note that these constraints are fairly general in that they allow to impose not only requirements on the lossy reconstruction of @xmath4 or @xmath0 ( obtained by setting @xmath93 independent of @xmath94 or @xmath95 respectively ) , but also on some function of both @xmath4 and @xmath0 ( by setting @xmath93 to be dependent on such function of ( @xmath96 ) ) .    given a delay @xmath11 , for a distortion pair ( @xmath97 )",
    ", we say that rate pair ( @xmath98 ) is achievable if , for every @xmath99 and sufficiently large @xmath100 , there exists a @xmath101 code .",
    "we refer to the closure of the set of all achievable rates for a given distortion pair ( @xmath97 ) and delay @xmath16 as the _ rate - distortion region _ @xmath102 .    from the general description above for the setting of fig .",
    "[ fig2 ] , the special case of fig .",
    "[ fig0 ] is produced by neglecting the presence of decoder 2 , or equivalently by choosing @xmath103 . in this case , the rate - distortion region @xmath102 is fully characterized by a function @xmath104 as @xmath105@xmath106 .",
    "function @xmath104 hence characterizes the infimum of rates @xmath9 for which the pair @xmath107 is achievable , and is referred to as the _ rate - distortion function _ for the setting of fig .",
    "[ fig0 ] . for the special case of the model in fig .",
    "[ fig2 ] in which @xmath32 , we define the rate - distortion function @xmath108 in a similar way .    _ notation _ : for @xmath86 integer with @xmath109 , we define @xmath110 ; if instead @xmath109 we set @xmath111 .",
    "we will also write @xmath112 for @xmath113 for simplicity of notation . given a sequence",
    "@xmath114 $ ] and a set @xmath115,$ ] we define sequence @xmath116 as @xmath117 $ ] where @xmath118 .",
    "random variables are denoted with capital letters and corresponding values with lowercase letters .",
    "given random variables , or more generally vectors , @xmath119 and @xmath120 we will use the notation @xmath121 or @xmath122 for @xmath123 $ ] , and @xmath124 or @xmath125 for @xmath126 $ ] , where the latter notations are used when the meaning is clear from the context .",
    "given set @xmath42 , we define @xmath127 as the @xmath100-fold cartesian product of @xmath42 .",
    "we denote any function of @xmath99 that tends to zero as @xmath128 as @xmath129 . when referring to @xmath130typical sequences , we refer to the notion of strong typicality as treated in @xcite .",
    "in this section , we study the point - to - point model in fig .",
    "[ fig0 ] .",
    "we start by characterizing the rate - distortion function @xmath104 for any delay @xmath11 under the hamming distortion metric for @xmath131 .",
    "the hamming distortion metric is defined as @xmath132 , where @xmath133 if @xmath134 is true and @xmath135 otherwise .",
    "this implies that the distortion constraint ( [ dist constraints ] ) for @xmath136 becomes @xmath137=\\frac{1}{n}\\sum\\limits _ { i=1}^{n}\\pr[x_{i}\\neq z_{1i}]=0.\\label{dist constraints-1}\\ ] ] in other words , from the definition of achievability given above , we impose that the sequence @xmath5 be recovered with vanishingly small average symbol error probability as @xmath138 .",
    "we refer to this scenario as asymptotically lossless , or lossless for short .",
    "we have the following characterization of @xmath139 .    [",
    "prop : for - any - delay 1]for any delay @xmath11 , the rate - distortion function for the set - up in fig .",
    "[ fig0 ] under hamming distortion at @xmath131 is given by @xmath140 where the conditional entropy is calculated with respect to the distribution @xmath141 } } \\prod\\limits _ { i=2}^{d+1}w_{1}(y_{i}|y_{i-1})q(x_{i}|y_{i})\\text { for } d\\geq1.\\label{eq : distributions}\\end{aligned}\\ ] ]    the proof of converse of the proposition above is based on an appropriate use of the fano inequality and is reported in appendix [ sub : proof - of - converse ] .",
    "to prove the direct part of the proposition , we propose a simple achievable scheme , which , to the best of the authors knowledge , has not appeared before , in sec . [",
    "sub : proof - of - achievability ] .",
    "expression ( [ lossless ] ) consists of a conditional entropy of @xmath142 random variables , namely @xmath143,@xmath144 , ... , @xmath145 .",
    "these variables are distributed as the corresponding entries in the random vectors @xmath5 and @xmath6 , as per ( [ eq : distributions - a])-([eq : distributions ] ) ( cf .",
    "( [ eq : hidden markov ] ) ) .",
    "we have therefore used the same notation for the involved random variables as in sec . [",
    "sec : system - model ] .",
    "proposition 1 provides a `` single - letter '' characterization of @xmath139 for the setting of fig .",
    "[ fig0 ] , since it only involves a finite number of variables .",
    "this contrasts with the general characterization for stationary ergodic processes of @xmath146 given in @xcite , which is a `` multi - letter '' expression , whose computation can generally only attempted numerically using approaches such as the ones proposed in @xcite .",
    "note that a multi - letter expression is also given in @xcite to characterize @xmath146 for i.i.d .",
    "sources with _ negative _ delays @xmath147 .",
    "finally , it should be emphasized that the simple characterization ( [ lossless ] ) for the scenario of interest here hinges on the assumed statistics of the sources ( @xmath148 ) .    by setting @xmath27 in ( [ lossless ] )",
    "we obtain @xmath149 .",
    "this result generalizes ( * ? ? ?",
    "* ; * ? ? ?",
    "* remark 3 , p. 5227 ) from i.i.d .",
    "sources ( @xmath148 ) to the hidden markov model ( [ eq : hidden markov ] ) considered here . note that , for @xmath41 , we instead obtain @xmath150 as another notable special case , if side information is absent , or equivalently @xmath151 , in accordance to well - known results , we obtain that @xmath152 equals the entropy rate ( see , e.g. , @xcite ) @xmath153 in fact , we have @xmath154 by ( * ? ? ?",
    "* theorem 4.5.1 ) .",
    "[ rem : is - delayed - side]is _ delayed _ side information useful ( when known also at the encoder ) ? that this is generally the case follows from the inequality @xmath155 since @xmath152 is the required rate without side information . this result is proved by the chain of inequalities @xmath156 where the first inequality follows by the data processing inequality and the second by conditioning reduces entropy",
    ". however , inequality ( [ eq : entropy rate in ] ) may not be strict , and thus side information may not be useful .",
    "a first example is the case where @xmath4 is an i.i.d .",
    "process , which is obtained by making @xmath38 independent of @xmath94 . as another example ,",
    "consider the setting of source coding with feedforward @xcite , i.e. , @xmath35 . in this case",
    ", our assumption ( [ eq : hidden markov ] ) entails that @xmath5 is a markov chain , and we have @xmath157 for @xmath158 . therefore ,",
    "delayed feedforward ( with @xmath159 is not useful for the lossless compression of markov chains , as already shown in @xcite .",
    "this conclusion need not hold for lossy compression ( i.e. , for @xmath160 ) @xcite ( see also sec . [",
    "sub : binary - hidden - markov ] ) .    if @xmath148 are general jointly stationary and ergodic processes ( and not necessarily stationary ergodic hidden markov models )",
    ", one can adapt in a straightforward way the proofs of appendix [ sub : proof - of - converse ] and sec .",
    "[ sub : proof - of - achievability ] , and conclude that the rate distortion function can be written as @xmath161 where @xmath162 is the causally conditioned entropy @xmath163 ( see , e.g. , @xcite ) . comparing ( [ eq : directed info ] ) with the rate @xmath164 necessary in the absence of any side information , we conclude that the reduction in the compression rate obtained by leveraging delayed side information at the decoder , when side information is known at the encoder , is given for stationary and ergodic processes by @xmath165 in ( [ eq : directed mut info ] ) , we have used the definition of _ directed _ mutual information @xmath166 ( see , e.g. , @xcite ) .",
    "note that the rate gain ( [ eq : directed mut info ] ) complements the results given in @xcite on the interpretation of the directed mutual information ( see also next remark ) .",
    "[ rem : consider - a - variable - length]consider a _ variable - length _ ( strictly ) lossless source code that operates symbol by symbol such that , for every symbol @xmath167 $ ] , it outputs a string of bits @xmath168 which is a function of @xmath169 and @xmath17 .",
    "encoding is constrained so that the code @xmath170 for each ( @xmath171 ) is prefix - free .",
    "the decoder , based on delayed side information , can then uniquely decode each codeword @xmath170 as soon as it is received .",
    "following the considerations in ( * ? ? ?",
    "* ; * ? ? ?",
    "iv ) , it is easy to verify that rate @xmath139 ( and , more generally , ( [ eq : directed info ] ) ) is also the infimum of the average rate in bits / source symbol required by such code .",
    "moreover , it is possible to construct universal context - based compression strategies by adapting the approach in @xcite .",
    "we refer to sec .",
    "[ sec : examples ] for some examples that further illustrate some implications of proposition 1 .",
    ", for @xmath172 ( symbols corresponding to out - of - range indices are set to zero).,width=345 ]      ( achievability ) here we propose a coding scheme that achieves rate ( [ lossless ] ) .",
    "the basic idea is a non - trivial extension of the approach discussed in ( * ? ? ?",
    "* ; * ? ? ?",
    "* remark 3 , p. 5227 ) and is described as follows . a block diagram is shown in fig .",
    "[ figmux ] for encoder ( fig .",
    "[ figmux]-(a ) ) and decoder ( fig .",
    "[ figmux]-(b ) ) .",
    "we first describe the _ encoder , _ which is illustrated in fig .",
    "[ figmux]-(a ) . to encode sequences",
    "@xmath173 we first partition the interval @xmath174 $ ] into @xmath175 subintervals , which we denote as @xmath176 $ ] , for all @xmath177 and @xmath178 .",
    "every such subinterval @xmath179 is defined as @xmath180\\text { and } y_{i - d}=\\tilde{y},\\text { } x_{i - d+1}^{i-1}=\\tilde{x}^{d-1}\\}.\\label{eq : i(xy)}\\ ] ] in words , the subinterval @xmath179 contains all symbol indices @xmath14 such that the corresponding delayed side information available at the decoder is @xmath181 and the previous @xmath182 samples in @xmath183 are @xmath184 .",
    "we refer to the value of the tuple ( @xmath185 ) as the _ context _ of sample @xmath186 .",
    ", this definition of context is consistent with the conventional one given in @xcite when specialized to markov processes .",
    "see also remark [ rem : consider - a - variable - length ] . ] for the out - of - range indices @xmath187 $ ] , one can assume arbitrary values for @xmath188 and @xmath189 , which are also shared with the decoder once and for all . note that @xmath190 $ ] .",
    "[ figillustration ] illustrates the definitions at hand for @xmath172 .    as a result of the partition described above , the encoder `` demultiplexes '' sequence @xmath183 into @xmath175 sequences @xmath191 , one for each possible context ( @xmath192 .",
    "this demultiplexing operation , which is controlled by the previous values of source and side information , is performed in fig .",
    "[ figmux]-(a ) by the block labelled as `` demux '' , and an example of its operation is shown in fig .",
    "[ figillustration ] . by the ergodicity of process",
    "@xmath4 and @xmath0 , for every @xmath99 and all sufficiently large @xmath100 , the length of any sequence @xmath191 is guaranteed to be less than @xmath193 symbols with probability arbitrarily close to one .",
    "this because the length @xmath194 of the sequence @xmath191 equals the number of occurrences of the context ( @xmath195 ) and by birkhoff s ergodic theorem ( see ( * ? ? ?",
    "16.8 ) ) . in particular",
    ", for any @xmath99 we can find an @xmath100 such that @xmath196\\leq\\frac{\\epsilon}{2|\\mathcal{x}|^{d-1}|\\mathcal{y}|},\\label{eq : error_1}\\ ] ] where we have defined the `` error '' event @xmath197    each sequence @xmath191 is encoded by a separate encoder , labelled as `` enc '' in fig . [",
    "figmux]-(a ) . in case",
    "the cardinality @xmath194 does not exceed @xmath193 ( i.e. , the `` error '' event @xmath198 does not occur ) , the encoder compresses sequence @xmath191 using an entropy encoder , as explained below . if the cardinality condition is instead not satisfied ( i.e. , @xmath198 is realized ) , then an arbitrary bit sequence of length @xmath199 , to be specified below , is selected by the encoder `` enc '' .",
    "the entropy encoder can be implemented in different ways , e.g. , using typicality or huffman coding ( see , e.g. , @xcite ) . here",
    "we consider a typicality - based encoder .",
    "note that the entries @xmath4 of each sequence @xmath200 are i.i.d . with distribution @xmath201 , since conditioning on the context @xmath202 makes the random variables @xmath4 independent . as it is standard practice , the entropy encoder assigns a distinct label to all @xmath203-typical sequences @xmath204 with respect to such distribution , and an arbitrary label to non - typical sequences . from the asymptotic equipartion property ( aep )",
    ", we can choose @xmath100 sufficiently large so that ( see , e.g. , @xcite ) @xmath205\\leq\\frac{\\epsilon}{2|\\mathcal{x}|^{d-1}|\\mathcal{y}|},\\label{eq : error_2}\\ ] ] where we have defined the `` error '' event @xmath206 moreover , by the aep , a rate in bits per source symbol of @xmath207 is sufficient for the entropy encoder to label all @xmath203-typical sequences .    from the discussion above",
    ", it follows that the proposed scheme encodes each sequence @xmath191 with @xmath208 bits . by concatenating the descriptions of all the @xmath175 sequences",
    "@xmath209 , we thus obtain that the overall rate @xmath9 of message @xmath7 for the scheme at hand is @xmath210 .",
    "the concatenation of the labels output by each entropy encoder is represented in fig .",
    "[ figmux]-(a ) by the block `` mux '' .",
    "we emphasize that encoder and decoder agree a priori on the order in which the descriptions of the different subsequences are concatenated .",
    "for instance , with reference to the example in fig .",
    "[ figillustration ] ( with @xmath172 ) , message @xmath7 can contain first the description of the sequence corresponding to @xmath211 , then @xmath212 , etc .",
    "we now describe the _ decoder , _ which is illustrated in fig . [ figmux]-(b ) . by undoing the multiplexing operation just described",
    ", the decoder , from the message @xmath7 , can recover the individual sequences @xmath191 through a simple demultiplexing operation for all contexts @xmath213 .",
    "this operation is represented by block `` demux '' in fig . [ figmux]-(b ) . to be precise , this demultiplexing is possible , unless the encoding `` error '' event @xmath214 takes place .",
    "in fact , occurrence of the `` error '' event @xmath215 implies that some of the sequences @xmath191 was not correctly encoded and hence can not be recovered at the decoder .",
    "the effect of such errors will be accounted for below .",
    "assume now that no error has taken place in the encoding . while the individual sequences @xmath191 can be recovered through the discussed demultiplexing operation",
    ", this does not imply that the decoder is also able to recover the original sequence @xmath183 .",
    "in fact , that decoder does not know a priori the partition @xmath216 : @xmath177 and @xmath217 of the interval @xmath174 $ ] and thus can not reorder the elements of sequences @xmath191 to produce @xmath183 .",
    "recall , moreover , that such re - ordering operation should be done in a causal fashion following the decoding rule ( [ eq : decoder ] ) .",
    "we now argue that the re - ordering mentioned above is in fact possible using a decoding rule that complies with ( [ eq : decoder ] ) via a multiplexing block controlled by the previous estimates of the source samples ( block `` mux '' in fig .",
    "[ figmux]-(b ) ) .",
    "in fact , note that at time @xmath14 , the decoder knows @xmath218 and the previously decoded @xmath219 and can thus identify the subinterval @xmath179 to which the current symbol @xmath4 belongs .",
    "this symbol can be then immediately read as the next yet - to - be - read symbol from the corresponding sequence @xmath191 .",
    "note that for the first @xmath16 symbols , the decoder uses the values for @xmath186 and @xmath220 at the out - of - range indices @xmath14 that were agreed upon with the encoder ( see above ) . in conclusion , we remark that the scheme described above , by choosing @xmath203 small enough and @xmath100 large enough , is able to satisfy the constraint ( [ dist constraints-1 ] ) to any desired accuracy .",
    "we also note that the controlled multiplexing / demultiplexing operation used in the proof is reminiscent of the scheme proposed in @xcite for transmission on fading channels with side information at the transmitter and receiver .",
    "we finally need to study the effect of errors . given the choices made above , we have that the probability of an encoding error is @xmath221 & \\leq\\sum_{\\tilde{x}^{d-1}\\in\\mathcal{x}^{d-1},\\mbox { } \\tilde{y}\\in\\mathcal{y}}\\pr[\\mathcal{e}_{1}(\\tilde{y},\\tilde{x}^{d-1})]+\\pr[\\mathcal{e}_{2}(\\tilde{y},\\tilde{x}^{d-1})]\\leq\\epsilon,\\end{aligned}\\ ] ] where the first inequality follows from the union bound and the second from ( [ eq : error_1 ] ) and ( [ eq : error_2 ] ) .",
    "this implies that the distortion in ( [ dist constraints-1 ] ) is upper bounded by @xmath203 as desired .",
    "in fact , from the definition of encoder and decoder given above , we can conclude that @xmath222=\\pr[\\mathcal{e}]\\leq\\epsilon$ ] , where we recall that @xmath223 is the sequence reconstructed at the decoder .",
    "moreover , the following inequality holds in general @xmath224\\geq\\frac{1}{n}\\sum\\limits _",
    "{ i=1}^{n}\\pr[x_{i}\\neq z_{1i}].\\label{eq : ineqperr}\\ ] ] therefore ,",
    "we have @xmath225\\leq\\epsilon$ ] , which concludes the proof .    an alternative proof of achievability can be given by using the idea of codetrees and extending the notions of typicality introduced in @xcite .",
    "the proof discussed above is based on a conceptually and algorithmically simpler approach , albeit its applicability is limited to lossless compression ( see next subsection ) .    from the inequality ( [ eq : ineqperr ] ) , it follows that the optimality of the scheme above can be proved also under the more stringent block error probability constraint ( see also ( * ? ? ?",
    "* ; * ? ? ?",
    "3.6.4 ) ) .      here",
    ", we obtain a characterization of the rate - distortion function @xmath104 , for @xmath27 and @xmath41 .",
    "the proof follows as a special case of that of proposition [ pro : for - any - delay ] to be discussed in the next section , and is based on similar arguments as for proposition [ prop : for - any - delay 1 ] .",
    "[ cor : for - any - delay]for any delay @xmath11 and distortion @xmath226 , the following rate is achievable for the setting of fig .",
    "[ fig0 ] @xmath227 with mutual informations evaluated with respect to the joint distribution @xmath228 and where minimization is done over all conditional distributions @xmath229 such that @xmath230\\leq d_{1}.\\label{eq : dist const r1d1d2 - 1 - 1}\\ ] ] moreover , rate ( [ eq : rda])-([eq : dist const r1d1d2 - 1 - 1 ] ) is the rate - distortion function , i.e. , , for @xmath27 and @xmath41 .    the optimality of the conditional codebook strategy for lossless compression shown in proposition [ prop : for - any - delay 1 ] hinges on the following fact : conditioned on the context ( _ @xmath231 _ @xmath232 ) , the samples _",
    "@xmath4 _ are independent of the past samples @xmath219 by the hidden markov model assumption .",
    "recall that the fact that the decoder has available the past source samples ( @xmath233@xmath232 ) since its estimates are correct with high probability . due to this independence property , and to the availability of the side information also at the encoder",
    ", the latter need not use multi - letter compression codes and can instead use simple single - letter entropy codes conditioned on the values of ( _ _ @xmath231__@xmath232 ) without loss of optimality . in the lossy case considered in proposition [ cor : for - any - delay ] , instead , even for the point - to - point model , the independence condition discussed above does not hold for delays @xmath16 strictly larger than 1 .",
    "in fact , at each time @xmath14 , the decoder has available the delayed side information @xmath17 only , conditioned on which the source samples @xmath4 are not independent of the past samples @xmath219 .",
    "but , for @xmath41 , the independence condition at hand does apply and thus the optimality of single - letter codes can be proved as done in proposition [ cor : for - any - delay ] .",
    "in this section , we consider the problem of lossy compression for the set - up of fig . [ fig2 ] .",
    "note that the asymptotically lossless case follows from proposition [ prop : for - any - delay 1 ] , since , in order to guarantee lossless reconstruction also at the decoder with delayed side information , rate @xmath9 must satisfy the conditions in proposition [ prop : for - any - delay 1 ] . here , we obtain an achievable rate region @xmath234 for all delays @xmath11 for the model in fig .",
    "[ fig2 ] , and show that such region coincides with the rate - distortion region , i.e. , @xmath235 , for @xmath27 and @xmath41 .    to streamline the discussion ,",
    "we start by consider the special case where @xmath32 and obtain a characterization of the rate - distortion function @xmath108 for @xmath27 and @xmath41 .    [ pro:2]for any delay @xmath11 and distortion pair @xmath236",
    ", the following rate is achievable for the set - up of fig .",
    "[ fig2 ] with @xmath32 @xmath237 with mutual informations evaluated with respect to the joint distribution @xmath238 and where minimization is done over all conditional distributions @xmath239 such that @xmath240\\leq d_{j}\\text { , for } j=1,2.\\label{eq : dist const r1d1d2 - 1}\\ ] ] moreover , rate ( [ eq : r1d1d2a])-([eq : r1d1d2 - 1 ] ) is the rate - distortion function , i.e. , , for @xmath27 and @xmath41 .",
    "[ rem : rate inter-1]rate ( [ eq : r1d1d2a ] ) can be easily interpreted in terms of achievability . to this end",
    ", we remark that variable @xmath241 plays the role of the delayed side information @xmath17 at decoder 1 .",
    "the coding scheme achieving rate ( [ eq : r1d1d2a ] ) operates in two successive phases . in the first phase ,",
    "the encoder encodes the reconstruction sequence @xmath223 for decoder 1 . since decoder 1 has available delayed side information , using a strategy similar to the one discussed in sec .",
    "[ sub : proof - of - achievability ] , this operation requires @xmath242 bits per source sample , as further detailed in sec .",
    "[ sub : proof - of - achievability-2 ] .",
    "note that decoder 2 is able to recover @xmath223 as well , since decoder 2 has available side information @xmath29 , and thus also the delayed side information @xmath17 . in the second phase",
    ", the reconstruction sequence @xmath243 for decoder 2 is encoded .",
    "given the side information available at decoder 2 , this operation requires rate @xmath244 , using again an approach similar to the one discussed in sec .",
    "[ sub : proof - of - achievability ] .",
    "the converse proof is in appendix [ sub : proof - of - converse-2 ] .",
    "[ rem : for - memoryless - sources-1]for memoryless sources @xmath5 and @xmath6 , obtained by setting the transition probability @xmath245 to be independent of @xmath246 , it can be seen that the achievable rate ( [ eq : r1d1d2a])-([eq : r1d1d2 - 1 ] ) is the rate - distortion function for the scenario of fig .",
    "[ fig2 ] with @xmath32 _ for all delays _ @xmath11 .",
    "this observation extends lemma 1 to the more general set - up of fig .",
    "[ fig2 ] with @xmath32 . to see this , note that for @xmath158 , rate ( [ eq : r1d1d2a])-([eq : r1d1d2 - 1 ] ) is given by @xmath247 with mutual informations evaluated with respect to the joint distribution @xmath248 and where minimization is done over all conditional distributions @xmath239 such that the distortion constraints ( [ eq : dist const r1d1d2 - 1 ] ) are satisfied .",
    "rate ( [ eq : rem memoryless-1 ] ) recovers the rate - distortion function derived by @xcite for the case where decoder 1 has _ no _ side information .",
    "therefore , rate ( [ eq : rem memoryless-1 ] ) is achievable even without any state information at decoder 1 .",
    "we then conclude that _ _ delayed side information is not useful for memoryless sources__. _ _ note also that @xcite assumes non - causal availability of the side information at decoder 2 .",
    "the equality of the rate derived in @xcite and the one in proposition [ pro:2 ] thus demonstrates that causal and non - causal side information lead to the same performance in terms of rate - distortion function .",
    "while ( [ eq : r1d1d2a ] ) is easier to interpret in terms of achievability as done in remark [ rem : rate inter-1 ] , the equivalent expression ( [ eq : r1d1d2 - 1 ] ) highlights the rate loss due to the possible delay of the side information .",
    "in fact , the mutual information @xmath249 accounts for the rate that would be needed to convey both @xmath223 and @xmath243 only to decoder 2 , which has non - delayed side information . therefore , the additional term @xmath250 can be interpreted as the extra rate that needs to be expended to enable transmission of @xmath223 also to decoder 1 , which has delayed side information .",
    "we now consider the general model in fig .",
    "[ fig2 ] .",
    "[ pro : for - any - delay]for any delay @xmath11 and any distortion pair ( @xmath97 ) , define @xmath251 as the union of all rate pairs ( @xmath98 ) that satisfy @xmath252 for some joint distribution @xmath253 where minimization is done over all conditional distributions @xmath254 such that @xmath240\\leq d_{j}\\text { , for } j=1,2.\\ ] ] we have that @xmath255 for any @xmath11",
    ". moreover , equation ( [ eq : ach region fig2 ] ) holds with equality , and thus @xmath251 is the rate - distortion region , for @xmath27 and @xmath41 .",
    "let us interpret the rate region @xmath251 in terms of achievability .",
    "first , from remark [ rem : rate inter-1 ] , we observe that ( [ eq : ineq fig1 ] ) is the rate necessary to convey @xmath223 to both decoder 1 and decoder 2 , and an auxiliary codeword @xmath256 only to decoder 2 .",
    "this auxiliary codeword @xmath256 carries information to decoder 2 that is then refined via message @xmath257 in particular , rewriting ( [ eq : ineq fig2 ] ) as @xmath258 , by comparison with ( [ eq : ineq fig1 ] ) , we see that the extra rate @xmath259 is needed to transmit sequence @xmath243 to decoder 2 , thus refining the information available therein due to message @xmath7 . and @xmath20 , which leads to the sum - rate constraint ( [ eq : ineq fig2 ] ) .",
    "]    the considerations in remark [ rem : for - memoryless - sources-1 ] can be also easily extended to the scenario of proposition [ pro : for - any - delay ] with @xmath33 .",
    "( achievability ) we first prove achievability of rate ( [ eq : r1d1d2a ] ) in proposition [ pro:2 ] .",
    "the proof extends the ideas discussed in sec .",
    "[ sub : proof - of - achievability ] , to which we refer for details .",
    "in particular , here we do not detail the calculations of the encoding `` error '' events and distortion levels , as they follow in the same way as in sec .",
    "[ sub : proof - of - achievability ] . to encode sequence ( @xmath260 ) , the encoder partitions the interval @xmath174 $ ] into @xmath261 subintervals , namely @xmath262 for each @xmath178 , so that ( cf .",
    "( [ eq : i(xy ) ] ) ) @xmath263\\text { and } y_{i - d}=\\tilde{y}\\}.\\label{eq : i(xy)-1}\\ ] ] similar to sec .",
    "[ sub : proof - of - achievability ] , a different compression codebook is used for each such interval @xmath262 , and thus for each pair of `` demultiplexed '' subsequences @xmath264 . the compression of each pair of sequences @xmath264 is based on a test channel @xmath265 specifically , the corresponding codewords @xmath223 are generated i.i.d . according to the marginal distribution @xmath266 @xmath267 and compression",
    "is done based on standard joint typicality arguments . by the covering lemma @xcite , compression of sequences @xmath268 into the corresponding reconstruction sequence @xmath269",
    "requires rate @xmath270 bits per source symbol in each interval @xmath262 , and thus an overall rate @xmath271 following the same considerations as in sec .",
    "[ sub : proof - of - achievability ] .",
    "in particular , the encoder multiplexes the compression indices corresponding to the @xmath261 intervals @xmath262 to produce message @xmath7 .",
    "therefore , the latter only carries information about the individual sequences @xmath272 but not about the ordering of each entry within the overall sequence @xmath223 .    based on the sequence @xmath273 produced in the first encoding phase described above , the encoder then performs also a finer partition of the interval @xmath174 $ ] into @xmath274 intervals @xmath275 with @xmath276 , so that @xmath277\\text { and } y_{i - d}=\\tilde{y},\\mbox { } y_{i}=y,\\mbox{\\mbox { } and } z_{i}=z\\}.\\label{eq : i(xy)-1 - 1}\\ ] ] compression of sequence @xmath278 into the corresponding reconstruction @xmath279 is carried out according to test channel @xmath280 as per the discussion above , requiring an overall rate of @xmath281 the compression indices for all sets @xmath282 are concatenated in message @xmath7 following the compression indices obtained from the sets @xmath262 .    upon reception of message @xmath7 , decoder 1 and 2",
    "can both recover the sequences @xmath269 and @xmath279 for all @xmath283 @xmath50 and @xmath284 via simple demultiplexing .",
    "moreover , following the same reasoning as in sec .",
    "[ sub : proof - of - achievability ] , decoder 1 can reconstruct sequence @xmath223 in the correct order in a causal fashion , using a decoder ( [ eq : decoder ] ) , which depends on message and delayed side information , since the value of @xmath80 can be obtained from sequences @xmath269 by knowing the value of @xmath285 similarly , decoder 2 can reorder sequence @xmath243 in a causal fashion using a decoder of the form ( [ eq : decoder2 ] ) .",
    "this concludes the proof of achievability for proposition [ pro:2 ] .",
    "we now turning to the proof of achievability proposition [ pro : for - any - delay ] .",
    "for a fixed distribution ( [ sr : pmf ] ) , we need to prove that the rate region in fig .",
    "[ figregion ] is achievable . to do this",
    ", it is enough , by standard time - sharing arguments , to prove that corner points a and b are achievable .",
    "corner point b corresponds to rate pair @xmath286 and @xmath32 .",
    "but achievability of this region follows immediately from proposition [ pro:2 ] by setting @xmath287 in ( [ eq : r1d1d2a ] ) .",
    "instead , corner point a corresponds to the rate pair @xmath288 this rate pair can be achieved by using a strategy similar to the one discussed above . in this strategy , when encoding the message @xmath20 , which is received only at decoder 2 , the encoder leverages the fact that the latter knows @xmath289 and @xmath80 , by appropriately partitioning the interval @xmath174 $ ] and using different test channels in each subinterval .",
    ".,width=345 ]    in this section , we consider two specific examples relative to the scenario in fig . [ fig0 ] .",
    "the first example consists of binary - alphabet sources , while the second applies the results derived above to ( continuous - alphabet ) gaussian sources .",
    "we focus on a distortion metric of the form @xmath290 that does not depend on @xmath291 in other words , the decoder is interested in reconstructing @xmath5 within some distortion @xmath226 .",
    "we note that , under this assumption , the rate ( [ cor : for - any - delay ] ) equals the simpler expression @xmath292 with mutual informations evaluated with respect to the joint distribution @xmath293 where minimization is done over all distributions @xmath294 such that @xmath295\\leq d_{1}.$ ] note that this simplification is without loss of optimality because the distortion constraint does not depend on the correlation between @xmath296 and @xmath297 therefore , we can impose the markov condition @xmath298 as in ( [ eq : simpler ] ) without changing the distortion , while reducing the mutual information in ( [ eq : rda ] ) .      in the first example , we assume that @xmath0 is a binary markov chain with symmetric transition probabilities @xmath299 .",
    "therefore , we have @xmath300 and @xmath63-step transition probabilities @xmath301 , which can be obtained recursively as @xmath302 and @xmath303 for @xmath304.=\\left[\\begin{array}{cc } 1-\\varepsilon & \\varepsilon\\\\ \\varepsilon & 1-\\varepsilon \\end{array}\\right]^{k}$ ] , well known from markov chain theory ( see , e.g. , @xcite ) . ]",
    "note that this is a logistic map such that @xmath305 for large @xmath63 .",
    "we also set @xmath306 , consistently with the convention adopted in the rest of the paper .",
    "finally , we assume that @xmath307 with `` @xmath308 '' being the modulo-2 sum and @xmath309 being i.i.d .",
    "binary variables , independent of @xmath6 , with @xmath310 , @xmath311 .",
    "we adopt the hamming distortion @xmath312 .",
    "we start by showing in fig .",
    "[ fig4 ] the rate @xmath139 obtained from proposition 1 corresponding to zero distortion ( @xmath131 ) versus the delay @xmath16 for different values of @xmath313 and for @xmath314 .",
    "note that the value of @xmath313 measure the `` memory '' of the process @xmath0 : for @xmath313 small , the process tends to keep its current value , while for @xmath315 , the values of @xmath0 are i.i.d .. for @xmath27 , we have @xmath316 , irrespective of the value of @xmath313 , where we have defined the binary entropy function @xmath317 .",
    "instead , for @xmath16 increasingly large , the rate @xmath139 tends to the entropy rate @xmath164 .",
    "this can be calculated numerically to arbitrary precision following ( * ? ? ?",
    "* sec . 4.5 ) . note that a larger memory , i.e. , a smaller @xmath313 leads to smaller required rate @xmath139 for all values of @xmath16 .",
    "[ fig5 ] shows the rate @xmath139 for @xmath318 versus @xmath319 for different values of @xmath16 . for reference",
    ", we also show the performance with no side information , i.e. , @xmath164 . for @xmath320 ,",
    "the source @xmath5 is i.i.d . and delayed side information is useless in the sense that @xmath321 ( remark [ rem : is - delayed - side ] ) .",
    "moreover , for @xmath322 , we have @xmath35 , so that @xmath4 is a markov chain and the problem becomes one of lossless source coding with feedforward . from remark",
    "[ rem : is - delayed - side ] , we know that delayed side information is useless also in this case , as @xmath323 . for intermediate values of @xmath319 ,",
    "side information is generally useful , unless the delay @xmath16 is too large .",
    "we now turn to the case where the distortion @xmath226 is generally non - zero . to this end , we evaluate the achievable rate ( [ eq : simpler ] ) in appendix [ sec : proof - of-()- ( ) ] obtaining @xmath324 for @xmath325 and @xmath326 otherwise .",
    "in ( [ eq : ex1])-([eq : ex2 ] ) we have defined @xmath327 . recall that rate @xmath328 has been proved to coincide with the rate - distortion function @xmath104 only for @xmath329 ( corollary [ cor : for - any - delay ] ) .    as a final remark ,",
    "we use the result derived above to discuss the advantages of delayed side information . to this end ,",
    "set @xmath322 so that @xmath35 and the problem becomes one of source coding with feedforward .",
    "for @xmath41 , result ( [ eq : ex1])-([eq : ex2 ] ) recovers the calculation in ( * ? ? ?",
    "* example 2 ) ( see also @xcite ) , which states that the rate - distortion function for the markov source @xmath5 at hand with feedforward ( @xmath41 ) is @xmath330 for @xmath331 and @xmath332 otherwise . from @xcite",
    "( see also @xcite ) , it is known that the rate - distortion function of a markov source @xmath5 without feedforward , i.e. , @xmath333 , is equal to ( [ slb ] ) only for @xmath226 smaller than a critical value , but is otherwise larger .",
    "this demonstrates that feedforward , unlike in the lossless setting discussed above , can be useful in the lossy case for distortion levels @xmath226 sufficiently large , as first discussed in @xcite .     for lossless reconstruction for the set - up of fig .",
    "[ fig0 ] with binary sources versus delay @xmath16 ( @xmath314).,width=432 ]     for lossless reconstruction for the set - up of fig .",
    "[ fig0 ] with binary sources versus parameter @xmath319 ( @xmath318).,width=432 ]      we now assume that @xmath6 is a gauss - markov process with zero - mean , power @xmath334=1 $ ] and correlation @xmath335=\\rho$ ] ( so that @xmath336=\\rho^{d}$ ] ) .",
    "moreover , @xmath4 is related to @xmath0 as @xmath337 where samples @xmath309 are i.i.d .",
    "zero - mean gaussian with variance @xmath338 and independent of @xmath6 .",
    "we concentrate on the mean square error  distortion metric @xmath339 . using standard arguments",
    ", we can apply the achievable rate ( [ eq : simpler ] ) to the setting at hand , although the result was derived for discrete alphabet ( see ( * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ch . 3.8 ) ) . by doing so , as shown in appendix [ sec : proof - of- ( ) ] , we get that the following rate is achievable for @xmath11 @xmath340 if @xmath341 and @xmath326 otherwise . as also discussed above , this rate coincides with the rate - distortion function for @xmath27 and @xmath41 .",
    "similar to the discussion in the previous section for a binary hidden markov model , we remark that for @xmath342 , the problem becomes one of lossy source coding with feedforward of a gauss - markov process @xmath5 . in this case",
    ", it is known that the rate - distortion function without feedforward , @xmath333 , equals @xmath343 only for distortions @xmath226 smaller than a critical value @xcite and is otherwise larger . by comparison with ( [ eq : gauss r ] )",
    ", it then follows that feedforward , for sufficiently large distortion levels , can be useful in decreasing the rate - distortion function .",
    "the problem of compressing information sources in the presence of delayed side information finds application in a number of scenarios including sensor networks and prediction / denoising . a general information - theoretic characterization of the trade - off between rate and distortion for this problem can be generally given in terms of multi - letter expressions , as done in @xcite .",
    "such expressions are proved by resorting to complex achievability schemes that operate in increasingly large blocks , and generally require involved numerical evaluations . in this work ,",
    "we have instead focused on a specific class of sources , which evolve according to hidden markov models , and derived single - letter characterizations of the rate - distortion trade - off .",
    "such characterizations are established based on simple achievable scheme that are based on standard `` off - the - shelf '' compression techniques . moreover ,",
    "the analysis has focused not only for the conventional point - to - point setting of @xcite , but also on a more general set - up in which side information may or may not be delayed . the value of the derived characterization is demonstrated by elaborating on two examples , namely binary sources with hamming distortion and gaussian sources with minimum mean square error distortion .",
    "various extensions of the results presented here are possible .",
    "for instance , the optimal strategy for a cascade model with three nodes in which the intermediate node has causal side information @xmath29 and the end decoder has delayed side information @xmath344 can be identified by applying the result in proposition [ pro:2 ] in a manner similar to @xcite .",
    "the authors wish to thank associate editor and reviewers for their thoughtful comments that have helped us improve the quality of the paper .",
    "for @xmath99 , fix a code @xmath345 as defined in sec . [",
    "sec : system - model ] . using the definition of encoder ( [ eq : encoder ] ) , we have the equalities @xmath346 the first term in ( [ eq : app11 ] ) cam be written , using the chain rule for entropy , as @xmath347\\nonumber \\\\   & + \\sum_{i = n - d+1}^{n}h(y_{i}|y^{i-1}x^{n})\\nonumber \\\\   & = a+\\sum_{i = d+1}^{n}\\left[h(y_{i - d}|y^{i - d-1}x^{i-1})+h(x_{i}|y_{i - d}x_{i - d+1}^{i-1})\\right]\\label{eq : app12}\\end{aligned}\\ ] ] where @xmath348 is a finite constant that does not increase with @xmath349 moreover , in the last line we have used the markov chain @xmath350 , which follows from ( [ eq : hidden markov ] )",
    ". the second term in ( [ eq : app11 ] ) can be similarly written as @xmath351\\nonumber \\\\   & \\leq b+\\sum_{i = d+1}^{n}\\left[h(y_{i - d}|y^{i - d-1}x^{i-1})+h(x_{i}|y^{i - d}m)\\right],\\label{eq : app13}\\end{aligned}\\ ] ] where @xmath352 is a finite constant that does not increase with @xmath349 the inequality in ( [ eq : app13 ] ) follows from conditioning reduces entropy .",
    "note also that we have the inequality @xmath353 by conditioning reduces entropy .    by definition ,",
    "a code @xmath345 must satisfy ( cf .",
    "( [ dist constraints-1 ] ) ) @xmath354 where we have defined @xmath355 $ ] .",
    "it follows that @xmath356 the first inequality ( [ eq : app13a ] ) follows from the fact that @xmath80 is a function of @xmath357 by ( [ eq : decoder ] ) and by conditioning reduces entropy ; the second inequality ( [ eq : app14 ] ) follows from fano s inequality and the third from ( [ eq : app13b ] ) .",
    "finally , from ( [ eq : app11]),([eq : app12]),([eq : app13]),([eq : app16 ] ) we obtain @xmath358\\\\   & -b-\\sum_{i = d+1}^{n}\\left[h(y_{i - d}|y^{i - d-1}x^{i-1})+n\\delta(\\epsilon)\\right]\\\\   & = a - b+\\sum_{i = d+1}^{n}h(x_{i}|y_{i - d}x_{i - d+1}^{i-1})+n\\delta(\\epsilon),\\end{aligned}\\ ] ] which concludes the proof .",
    "we prove the converse for proposition [ pro : for - any - delay ] , since proposition [ pro:2 ] follows as a special case .",
    "we focus on @xmath41 , since the proof for @xmath27 can be obtained in a similar fashion . to this end , fix a code @xmath359 as defined in sec .",
    "[ sec : system - model ] .",
    "using the definition of encoder ( [ eq : encoder ] ) and decoder ( [ eq : decoder ] ) we have @xmath360 where we have defined @xmath361 $ ] .",
    "all equalities above follow from standard properties of the entropy and mutual information , while the inequality ( [ eq : app31 ] ) follows by conditioning reduces entropy . following the similar steps ,",
    "we obtain @xmath362 the proof is concluded by introducing a time - sharing variable @xmath363 uniformly distributed in @xmath174 $ ] and defining random variables @xmath364 @xmath365 , @xmath366 , @xmath367 and @xmath368 , and by leveraging the convexity of the mutual informations in ( [ eq : app32 ] ) and ( [ eq : app4 ] ) with respect to the distribution @xmath369 .",
    "here we prove that ( [ eq : ex1])-([eq : ex2 ] ) equals ( [ eq : simpler ] ) for the binary hidden markov model of sec .",
    "[ sub : binary - hidden - markov ] .",
    "first , for @xmath370 , we can simply set @xmath371 to obtain @xmath372 and @xmath373\\leq d_{1}$ ] , which , from ( [ eq : ex1 ] ) and the non - negativity of mutual information , leads to @xmath326 .",
    "similarly , for @xmath374 , we can set @xmath375 to prove that @xmath326 . for the remaining distortion levels @xmath376 , under the constraint that @xmath373\\leq d_{1}$ ] , we have the following inequalities @xmath377 where the third line follows by conditioning decreases entropy and the last line from the fact that @xmath378 is increasing in @xmath379 for @xmath380 .",
    "this lower bound can be achieved in ( [ eq : simpler ] ) by choosing the test channel @xmath294 so that @xmath119 can be written as @xmath381 where @xmath382 is binary with @xmath383 and independent of @xmath296 and @xmath241 , and @xmath296 is also independent of @xmath241 . to obtain @xmath384",
    ", we need to impose that the joint distribution @xmath385 is preserved by the given choice of @xmath294 . to this end , note that the joint distribution @xmath385 is such that we can write @xmath386 , where @xmath387 is binary and independent of @xmath241 , with @xmath388 .",
    "therefore , preservation of @xmath385 is guaranteed if the equality @xmath389=p_{z_{1}}(1)*d_{1}=\\varepsilon^{(d)}*q$ ] holds .",
    "this leads to @xmath390 we remark that @xmath391 , due to the inequality ( [ eq : ex2 ] ) on the distortion @xmath226 .",
    "this concludes the proof .",
    "here we prove that ( [ eq : gauss r ] ) equals ( [ eq : simpler ] ) for the hidden gauss - markov model of sec .",
    "[ sub : gaussian - hidden - markov ] .",
    "this follows by using analogous arguments as done above for the binary hidden markov model .",
    "the only non - trivial adaptation of the proof given above is the choice of the test channel for the case where @xmath392 .",
    "this must be selected so that @xmath119 can be written as @xmath393 where @xmath382 is zero - mean gaussian with @xmath394=d_{1}$ ] and independent of @xmath296 and @xmath241 , and @xmath296 is also zero - mean gaussian and independent of @xmath241 . to obtain @xmath395 $ ] ,",
    "we need to impose that the joint distribution of @xmath119 and @xmath241 is preserved by the given choice of the test channel . to this end , note that the joint distribution of @xmath119 and @xmath241 is such that we can write @xmath396 , where @xmath387 is zero - mean gaussian and independent of @xmath241 and @xmath397 , with @xmath398=1-\\rho^{2d}$ ] .",
    "therefore , preservation of the joint distribution of @xmath119 and @xmath241 is guaranteed if the equality @xmath395+d_{1}=1-\\rho^{2d}+\\sigma_{n}^{2}$ ] holds .",
    "this leads to @xmath399=1-\\rho^{2d}+\\sigma_{n}^{2}-d_{1}.\\ ] ] we remark that @xmath400\\leq1 $ ] , due to the assumed inequality on the distortion @xmath226 .",
    "references r. venkataramanan and s. s. pradhan , source coding with feed - forward : rate - distortion theorems and error exponents for a general source , _ ieee trans .",
    "inform . theory _",
    "2154 - 2179 , jun . 2007 .",
    "r. venkataramanan and s. s. pradhan ,  directed information for communication problems with side - information and feedback / feed - forward , in _ proc .",
    "of the 43rd annual allerton conference _ , monticello , il , 2005 .",
    "r. venkataramanan and s. s. pradhan , `` on computing the feedback capacity of channels and the feed - forward rate - distortion function of sources , '' _ ieee trans .",
    "58 , no . 7 , pp . 18891896 , jul .",
    "2010 .",
    "s. s. pradhan , `` on the role of feedforward in gaussian sources : point - to - point source coding and multiple description source coding , '' _ ieee trans .",
    "inform . theory _",
    "1 , pp . 331 - 349 , jan .",
    "2007 .",
    "h. permuter , y .- h .",
    "kim and t. weissman , `` interpretations of directed information in portfolio theory , data compression , and hypothesis testing , '' _ ieee trans .",
    "inform . theory _",
    "57 , no . 6 , pp . 3248 - 3259 , jun .",
    "2011 .",
    "d. vasudevan , c. tian , and s. diggavi , lossy source coding for a cascade communication system with side - informations , in communication , control , and computing , 2006 44th annual allerton conference on , sept . 2006"
  ],
  "abstract_text": [
    "<S> for memoryless sources , delayed side information at the decoder does not improve the rate - distortion function . </S>",
    "<S> however , this is not the case for sources with memory , as demonstrated by a number of works focusing on the special case of ( delayed ) feedforward . in this paper , a setting is studied in which the encoder is potentially uncertain about the delay with which measurements of the side information , which is available at the encoder , are acquired at the decoder . assuming a hidden markov model for the source sequences , at first , a single - letter characterization is given for the set - up where the side information delay is arbitrary and known at the encoder , and the reconstruction at the destination is required to be asymptotically lossless . </S>",
    "<S> then , with delay equal to zero or one source symbol , a single - letter characterization of the rate - distortion region is given for the case where , unbeknownst to the encoder , the side information may be delayed or not , and additional information can be received by the decoder when the side information is not delayed . finally , examples for binary and gaussian sources are provided .    </S>",
    "<S> rate - distortion function , hidden markov model , markov gaussian process , multiplexing , strictly causal side information , causal conditioning . </S>"
  ]
}