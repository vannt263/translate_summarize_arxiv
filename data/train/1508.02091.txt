{
  "article_text": [
    "describing the content of an image is an easy task for humans , but , until recently , had been difficult or impossible for computers .",
    "recent work in computer vision has addressed this task of automatically generating the caption of an input image with promising results @xcite .",
    "several state - of - the - art approaches couple a pre - trained deep convolutional neural network ( cnn ) for image representation with a recurrent neural network ( rnn ) to generate captions that describe image content .",
    "we consider the possibility that the generation of these captions , however , is not heavily reliant upon the image representation input . for instance , if one was to train a rnn directly on image captions , one could learn a fair amount about the general language of image captions .",
    "sutskever et al .",
    "demonstrate that rnns are capable of producing diverse and surprisingly readable sentences , given a short starting sequence of seed words .",
    "furthermore , non - neural memoization techniques like those proposed by wood et al . and gasthaus et al .",
    "are capable of producing very convincing language models for particular domains .",
    "while it is clear that existing algorithms do discriminate based on image inputs , it is still unclear if the apparently highly specific generated captions are primarily a result of language modeling rather than image modeling . if it could be determined that either image modeling or language modeling is acting as the bottleneck in this multimodal setting",
    ", research efforts could be directed appropriately .    to examine the relative multimodal modeling capacities of existing neural captioning algorithms , we execute a series of experiments where we vary image representation quality produced from a fixed cnn , and examine how the output captions are affected .    for two existing datasets and a new domain",
    "we analyze here , our results suggest that caption quality does not scale well with increased classification accuracy of a fixed cnn .",
    "in fact , as the testing / validation accuracy of a cnn with fixed architecture increases , all seven caption evaluation metrics we consider appear to saturate at surprisingly low classification accuracies .",
    "while this does not prove that better image modeling algorithms could not produce better captions , it appears that many apparently fine - grained aspects of generated natural language are the result of surprisingly coarse grained visual distinctions .    for a fixed vision model ,",
    "our results indicate that there is likely little room for caption improvement via gathering more training images alone .",
    "we further postulate that progress could be made most quickly through the development of language modeling techniques that take better advantage of existing image representations .",
    "in particular , coupling our results with independent but consistent observations made by karpathy and li and vinyals et al . regarding model modifications that lead to overfitting , it s very likely that overfitting language models to image features is still a big problem for many caption generation algorithms .",
    "our analysis highlights what we believe to be an important question for these types of algorithms going forward : if better image representations contain useful , fine - grained information , is it possible to take advantage of that information without overfitting ?",
    "to supplement our analysis of image representations , we consider a new caption generating task : generating recipe titles based on images of food .",
    "the motivation for this new task results from the intuition that image representations might matter more in visually fine - grained domains , where algorithms must be able to discriminate between minute changes in the input images .",
    "we collect a dataset consisting of images of food coupled with recipe titles ( e.g. `` thai chicken curry '' ) from yummly.com for this purpose . when compared to captioning the coarse - grained imagenet domain , the specificity of our food dataset calls for more subtle visual discrimination .    instead of learning a food image representing cnn from scratch to derive representations ,",
    "we apply transfer learning on a dataset of 101k food images . using this approach ,",
    "we significantly surpass current state - of - the - art performance for a classification task on this dataset , despite using a somewhat outdated deep architecture .",
    "we further demonstrate that this transfer learning process does indeed improve food captioning , though we observe a similar `` flattening '' of all linguistic evaluation metrics , after a point .",
    "the model we choose to analyze in detail is the `` neural image captioning '' ( nic ) model detailed by vinyals et al .",
    ", though we believe the experiments we address here are relevant to researchers working on distinct but related models . in a similar fashion to donahue et al . and karpathy and li , nic feeds a pre - classification representation of images produced by an architecture like googlenet @xcite or alexnet @xcite to a lstm recurrent neural network @xcite for language generation .",
    "the rnn weights are usually trained on datasets consisting of pairs of images and several corresponding human - generated annotations , such as flickr8k @xcite , flickr30k @xcite , or microsoft coco @xcite .",
    "the cnn is often pre - trained on a very large set of images such as imagenet @xcite and held fixed while the rnn is trained . for many existing captioning datasets ,",
    "imagenet is a convenient starting point , presumably because images in most modern captioning datasets are of similar objects .",
    "more complicated caption generation models have also demonstrated success on several datasets . to the knowledge of the authors , fang et al .",
    "hold the current best result ( in terms of bleu-4 ) on the mscoco official captioning test set , though vinyals et al .",
    "reportedly outperform fang et al . on 2/5 evaluation metrics detailed on the mscoco captioning leaderboard .",
    "their pipeline involves training a language model directly on captions and a discretized image representation consisting of a likely set of objects in that image .",
    "switching from a finetuned alexnet @xcite to a finetuned vgg - net @xcite improved bleu-4 by 2.4 points , and meteor by 1.4 points . because their image representations were discrete , it s possible that their language models were less prone to overfitting .",
    "it s not immediately obvious that a similar improvement would occur for language models that operate on extracted vector representations of images like nic , however .",
    "in contrast to the previous approaches that provide their rnns with a representation of an image only at the first timestep , mao et al .",
    "propose an extension of a single - layer rnn , dubbed the `` multimodal rnn , '' that feeds a representation of an image to the rnn at _ every _ word generation step .",
    "finally , kiros et al .",
    "propose a model that first uses a cnn and an rnn to embed an image and its corresponding caption in the same semantic space , and then feeds vectors from this space into a `` language generating structure content neural language model '' , an extension of a multiplicative rnn that `` disentangles the structure of a sentence to its content . ''    among models that directly input extracted features to a generating rnn , it is clear that image representations can be mishandled . specifically",
    ", several authors note that passing image representations to the rnn at _ every _ timestep empirically leads to worse performance . while karpathy and li do not offer speculation as to why this is the case , vinyals et al . briefly mention that this operation leads to over - fitting .",
    "these independent observations demonstrate that it is easy to overfit to image features .      to evaluate captions , we use bleu-\\{1,2,3,4 } @xcite meteor @xcite and cider / cider - d @xcite . bleu -",
    "n is a precision measure over n - grams , whereas meteor is a more sophisticated metric that involves the computation of an alignment between candidate and reference captions ; both were originally conceived in the context of machine translation .",
    "cider / cider - d was created to evaluate captions of images and focuses on consensus , particularly in cases where there are multiple reference captions .              to extend the scope of our investigation",
    ", we compile a dataset consisting of images of food coupled with recipe titles from yummly.com . in this dataset ,",
    "the title of a recipe is usually several words long and can be thought of as a `` summary '' of the image , rather than a direct description , as not all image content is described in the caption .",
    "the image associated with `` garlic butter shrimp , '' for instance , contains shrimp , a bowl , a lemon , and a human hand , and the captioning algorithms must learn to pick out which items are important to describe .",
    "furthermore , there is less grammatical structure present in this dataset .",
    "we view this task as distinct from existing captioning tasks for three reasons .",
    "first , the captions within yummly are both short and restricted ; a caption in the yummly setting has an average length of 4.5 words , which is very low compared to flickr or mscoco settings ( both have an average of  10 words per caption ) and the vocabulary is very small ( see figure [ fig : vocabsize ] ) .",
    "second , to address this data fully , models must learn very fine - grained visual distinctions .",
    "compared to the broad imagenet domain , the yummly images generally consist of some food item on a plate , coupled with several words from a small vocabulary . finally , this dataset contains a single caption for each image , thus the learning task is more difficult .",
    "previous work @xcite has emphasized the importance of having multiple captions per image in a caption ranking setting , though its unclear if similar observations extend to a generation setting .    while we are only aware of the work of malmaud et al .",
    "that address food in a multimodal fashion , bossard et al .",
    "compile the food 101 dataset which generalizes and increases the scale of previous food image datasets ( i.e. chen et al .",
    ", yang et al . ) .",
    "their dataset includes 101k images of 101 types of foods and the task they address is classification .      while substantial improvements have been made in terms of classification accuracy on imagenet using increasingly deep architectures , we rely on the canonical neural network described in krizhevsky et al . to generate our representations in most of our experiments",
    "the use of alexnet in particular allows for more direct comparison with previous work ( i.e. bossard et al . ) and faster training time when compared to other deep models .",
    "this is beneficial particularly because our experiments are not specifically designed to produce state - of - the - art results .",
    "we perform 20 random parameter searches to determine decent parameter settings using the neuraltalk library for all captioning experiments , selecting parameter settings resulting in the lowest validation set perplexity , unless specified otherwise .",
    "settings we take as fixed include a minimum vocabulary threshold of 5 , weight optimization using rmsprop @xcite , and a hidden representation size of 256 .",
    "we restrict our consideration to nic because we believe it to be representative of the state - of - the - art in neural captioning .",
    "when we are evaluating models , we generate captions using a beam search of width 20 . for the recipe title prediction evaluation , we include an end - of - caption token to avoid issues relating to predicted zero length captions ; this has the result of artificially inflating evaluation metrics such that numerical cross - dataset comparisons are not valid .      to represent food images properly",
    ", we find it appropriate to learn a model specific to the task of food recognition .",
    "food-101 @xcite consists of only 101k images , which is a relatively low number of images to train a cnn from scratch .",
    "as such , we use a set of imagenet - trained weights as initializations for our training of a cnn on the food-101 classification task .",
    "this process is commonly referred to as transfer learning @xcite .",
    "the intuition behind transfer learning in cnns is that low - level features learned early on in the base network ( which are generally observed to be color blob and gabor features @xcite ) are useful to networks trained on diverse classification tasks .",
    "initializing the weights of the network to weights successful in another classification task should allow training of the new network to converge faster and to a better local optimum than if random initializations were used .",
    "in fact , for the food-101 dataset , we achieve a rank-1 accuracy of 66.80% when using transfer learning , when compared with the 56.40% rank-1 accuracy reported by bossard et al . using the same alexnet architecture ; class - by - class accuracies are given in figure [ fig : food101 ] for comparison with previous work .",
    "our network is learned using only 100k iterations of the caffe library at a reduced learning rate , whereas training from scratch required bossard et al .",
    "450k iterations . for our tuning process , we follow the guidelines and parameter settings specified by the transfer learning example distributed with caffe .",
    "once the network is tuned , we compute 4096 dimensional vector representations for each image in yummly dataset by extracting the network activations in the final fully - connected layer .",
    "after establishing that a cnn could be transfer learned to classify images of dishes at state - of - the - art performance , we were able to shift our focus to caption generation in a food domain .",
    "the food dataset we collect contains roughly 66k recipes , each consisting of a single image - recipe pair .",
    "this data was taken from yummly.com , a website that aggregates and performs analysis of millions of recipes . out of the 66k recipes ,",
    "6k are reserved for testing , 6k are designated as a validation set , and the remaining 54k are used for model training .",
    "this dataset differs from the flickr datasets and mscoco both in terms of vocabulary and in terms of image content .",
    "the vocabulary size per image is smaller than any of the other datasets by a wide margin ( see figure [ fig : vocabsize ] ) .",
    "while it s clear the vision task requires more subtle distinction when compared to imagenet , because the average caption length is shorter , it s ambiguous as to whether or not the yummly language generation task is particularly `` fine - grained . ''",
    "table [ tab : baselines ] presents some baseline results using the algorithms listed .",
    "common-3 predicts a reasonable ordering of the three most common words ( `` with chicken and '' ) for all captions .",
    "nearest neighbor predicts the caption of nearest neighbor in the transfer - learned 4096-dimensional embedding space .",
    "common - tri / bi predict the most common tri / bigram in our dataset ( `` macaroni and cheese''/``ice cream '' ) for all images .    across the board , and particularly for bleu-\\{2,3,4 } scores , the caption generating programs outperform all baselines , which suggests the proposed task is adequately framed .",
    "however , it is worth noting that only roughly 300/6117 ( roughly 5% ) of generated captions are unique .",
    "this is rather low when compared with a representative result for flickr8k , a dataset of similar size , where 200/1000 ( roughly 20% ) of generated captions are unique .",
    "it might be possible to re - frame the yummly generation task as one of classification , however , it s not obvious how one might drive a fixed set of labels . in a later section",
    "we discuss whether or not only having one caption per image or other dataset features is a contributing factor to this result .    .yummly",
    "baseline bleu-\\{1,2,3,4 } scores for several baselines and two high performing language generation algorithms . [",
    "cols=\"<,<,<,<,<\",options=\"header \" , ]",
    "we demonstrate the relationship between cnn classification accuracy and the quality of captions generated by a state of the art neural captioning algorithm .",
    "training increasingly accurate image classifiers does not lead to better captions , after a point .",
    "this early saturation of caption quality is an indication that the performance of neural caption generating algorithms likely can not be increased directly by producing more accurate cnns .",
    "furthermore , many of the apparently highly - specific generated captions output by models like nic are likely due to language models capturing coarse grained information and generating corresponding plausible natural language sequences .",
    "the role of overfitting to image features is difficult to quantify . on one hand",
    ", there is extra information contained in image representations that nic , for instance , does not take advantage of , and even commonly overfits to .",
    "however , it s not clear that this extra , fine - grained information is even worth taking into account .",
    "the success of models that generate language based on discretized image representations ( e.g. @xcite ) demonstrates that algorithms are capable of state - of - the - art performance without consideration of rich , real - valued vector features .",
    "it s likely that these types of models are less prone to overfitting , as well .",
    "we would like to thank jason yosinski for providing his alexnet training snapshots / insights and gregory druck for his help with compiling the data collected from ` yummly.com ` .",
    "we would also like to thank serge belongie , lillian lee , abby lewis , david mimno , xanda schofield , the anonymous reviewers , and the students in the spring 2015 iteration of cs6670 for their helpful discussions and comments ."
  ],
  "abstract_text": [
    "<S> we examine the possibility that recent promising results in automatic caption generation are due primarily to language models . by varying image representation quality produced by a convolutional neural network , we find that a state - of - the - art neural captioning algorithm is able to produce quality captions even when provided with surprisingly poor image representations . </S>",
    "<S> we replicate this result in a new , fine - grained , transfer learned captioning domain , consisting of 66k recipe image / title pairs . </S>",
    "<S> we also provide some experiments regarding the appropriateness of datasets for automatic captioning , and find that having multiple captions per image is beneficial , but not an absolute requirement . </S>"
  ]
}