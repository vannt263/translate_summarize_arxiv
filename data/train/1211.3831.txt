{
  "article_text": [
    "information - geometric optimization ( igo ) @xcite is a unified framework of model based stochastic search algorithms for any optimization problem . as typified by estimation of distribution algorithms ( eda )",
    "@xcite , model based randomized search algorithms build a statistical model @xmath4 on the search space @xmath5 to generate search points .",
    "the parameters @xmath6 of the statistical model are updated over time so that the probability distribution hopefully concentrates around the minimum of the objective function . in most model based algorithms such as edas and ant colony optimization ( aco ) algorithms @xcite , parameter calibration is based on the maximum likelihood principle or other intuitive ways .",
    "igo , unlike them , performs a natural gradient ascent of @xmath6 in the parameter space @xmath7 , having first adaptively transformed the objective function into a function on @xmath7",
    ". this construction offers maximal robustness guarantees with respect to changes in the representation of the problem ( change of parametrization of the search space , of the parameter space , and of the fitness values ) .",
    "importantly , the igo framework recovers several known algorithms ( * ? ? ?",
    "* section 4 ) .",
    "when igo is instantiated using the family of bernoulli distributions on @xmath8 , one obtains the _ population based incremental learning _ ( pbil ) algorithm @xcite . when using the family of gaussian distributions on @xmath9 , igo instantiates as a variant of _ covariance matrix adaptation evolution strategies _",
    "( cma - es ) , the so - called pure rank-@xmath0 cma - es update @xcite . moreover , when using an exponential family with the expectation parameters , the igo instance is equivalent to the cross - entropy method for optimization @xcite .",
    "of course , the igo framework not only provides information - theoretic derivations for existing algorithms but automatically offers new algorithms for possibly complicated optimization problems .",
    "for instance , the igo update rule for the parameters of restricted boltzmann machines has been derived @xcite .",
    "theoretical justification of the igo framework , therefore , is important both to provide a theoretical basis for the recovered algorithms and to make the design principle for future algorithms more reliable .",
    "here we focus on providing a measure of `` progress '' over the course of igo optimization , in terms of quantile values of the objective function .",
    "parameter updates by gradient ascent are somewhat justified in general , at least for infinitesimally small steps , because the gradient points to the direction of steepest ascent of a function .",
    "however , this argument does not apply to the igo algorithm : as the objective function is adaptively transformed in a time - dependent way , the function on which the gradient is computed changes over time , so that its increase does not necessarily mean global improvement .",
    "still , the igo framework comes with a guarantee that an infinitesimally small igo step along the natural gradient leads to monotone improvement of a specified quantity , for any objective function @xmath3 ( * ? ? ?",
    "* proposition  5 ) : a result from @xcite is that the _ @xmath2-quantile value _ of the objective function monotonically improves along the natural gradient .",
    "this result is limited to the exact igo flow , i.e. , an infinite number of sample points is considered and the step size of the gradient ascent is infinitesimal . still this ensures that the randomized algorithm with large sample size stays close to the deterministic trajectory with infinite samples with high probability , provided the step size is sufficiently small .",
    "now the question arises whether actual , non - infinitesimal step sizes still ensure monotone @xmath2-quantile improvement .    in this article , we prove that _ any _ step size not greater than @xmath1 guarantees monotone @xmath2-quantile value improvement in the igo algorithm for an exponential family with a finite step size ( theorem  [ thm : qimp ] ) , thus extending the previous result from infinitesimal steps with continuous time to more realistic algorithmic situations .",
    "for instance , this ensures monotone @xmath2-quantile improvement in pbil ( using uniform weights , see below ) , or in the cross - entropy method for exponential families in expectation parameters .",
    "interestingly , our results show that the admissible step sizes in igo are _ independent of the objective function @xmath3 _ , at least for large population sizes ( this stems from the many invariance properties built into igo ) .",
    "we further extend the result by defining _",
    "blockwise _ updates in igo where different blocks of parameters are adjusted one after another with different step sizes .",
    "our motivation is that in practice the pure rank-@xmath0 update cma - es updates the mean vector and the covariance matrix with different learning rates .",
    "we show that the blockwise update rule recovers the pure rank-@xmath0 cma - es update using different learning rates for the mean vector and the covariance matrix ( proposition  [ prop : igo - cma ] ) .",
    "we prove that _ any _ distinct step sizes less than @xmath1 guarantee monotone @xmath2-quantile improvement , which justifies the parameter setting used for the cma - es in practice ( theorem  [ thm : iter - qimp ] ) .",
    "other examples fitting into this framework are the relative payoff procedure ( rpp , also known as expectation - maximization for reinforcement learning ) @xcite , or situations where fitness - proportional selection is applied using exponential families ( theorem  [ thm : fitness - imp ] ) .",
    "the rpp is considered as an alternative to gradient based methods that allows to use relatively large learning rates .",
    "as it turns out , the rpp can be described as a natural gradient based algorithm with step size @xmath1 , and our result is an extension of the proof of its monotone improvement to generic natural gradient algorithms .",
    "the article is organized as follows . in section  [ sec : igo ] , we explain the igo framework and its implementation in practice",
    ". igo - maximum likelihood ( igo - ml ) , a variant of igo as a maximum likelihood , is presented , followed by the relation between the igo algorithm , igo - ml and the cross - entropy method for optimization , for exponential families of distributions . in section  [ sec : qi ] , we prove monotone @xmath2-quantile improvement in igo - ml .",
    "the result is extended by defining blockwise igo - ml , and @xmath2-quantile improvement in blockwise igo - ml is proved .",
    "we also provide a result with finite but large population sizes . section  [ sec : fit ] is devoted to the natural gradient algorithm with fitness - proportional selection scheme , where monotone improvement of expected fitness is proven .",
    "a short discussion in section  [ sec : disc ] closes the article .",
    "in this article , we consider an objective function @xmath10 to be minimized over any search space @xmath5 .",
    "the search space @xmath5 may be continuous or discrete , finite or infinite .",
    "let @xmath11 be a family of probability distributions on @xmath5 parametrized by @xmath12 and let @xmath13 be the probability density function induced by @xmath14 w.r.t .",
    "an arbitrary reference measure @xmath15 on @xmath5 , namely , @xmath16 . given a family of probability distributions",
    ", igo @xcite evolves the probability distribution @xmath17 at each time @xmath18 so that higher probabilities are assigned to better regions . to do so",
    ", igo transforms the objective function @xmath19 into a new one @xmath20 , defines a function on @xmath7 to be maximized : @xmath21 $ ] , and performs the steepest gradient ascent of @xmath22 on @xmath7 .",
    "hopefully , after some time the distribution @xmath17 concentrates around minima of the objective function .",
    "igo is designed to exhibit as many invariance properties as possible ( * ? ? ?",
    "* section 2 ) .",
    "the first property is invariance under strictly increasing transformations of @xmath3 . for any strictly increasing @xmath23",
    ", igo minimizes @xmath24 as easily as @xmath3 .",
    "this property is realized by a quantile based mapping of @xmath3 to @xmath25 at each time .",
    "the second property is invariance under a change of coordinates in @xmath5 , provided that this coordinate change globally preserves the family of probability distributions @xmath11 .",
    "for example , the igo algorithm for gaussian distributions on @xmath9 is invariant under any affine transformation of the coordinates whereas the igo algorithm for isotropic gaussian distribution is only invariant under any translation and rotation .",
    "invariance under @xmath5-coordinate transformation is one of the key properties for the success of the cma - es .",
    "the last property is invariance under reparametrization of @xmath6 . at least for infinitesimal steps of the gradient ascent",
    ", igo follows the same trajectory on the parameter space whatever the parametrization for @xmath6 is .",
    "this property is obtained by considering the intrinsic ( fisher ) metric on the parameter space @xmath7 and defining the steepest ascent on @xmath7 w.r.t .",
    "this metric , i.e. , by using a natural gradient .",
    "the study of the intrinsic metric on the parameter space of the probability distribution , called a _ statistical manifold _ , is the main topic of _ information geometry _ @xcite .",
    "the most widely used divergence between two points on the space of probability distributions is the kullback  leibler divergence ( kl divergence ) @xmath26 the kl divergence is , by definition , independent of the parametrization @xmath6 .",
    "let @xmath27 . then",
    ", the kl divergence between @xmath14 and @xmath28 expands @xcite as @xmath29 where @xmath30 is the euclidean norm and @xmath31 is the fisher information matrix at @xmath6 defined as @xmath32 the expansion follows from the well - known fact that the fisher information matrix is the hessian of kl divergence . by using the kl divergence",
    ", we have the following property of the steepest ascent direction ( see @xcite , theorem  1 , or @xcite , proposition  1 ) .",
    "[ sta : sad ] let @xmath23 be a smooth function on the parameter space @xmath7 .",
    "let @xmath12 be a nonsingular point where @xmath33 .",
    "then the steepest ascent direction of @xmath23 is given by the so - called natural gradient @xmath34 .",
    "more precisely , @xmath35    since kl divergence does not depend on parametrization , the natural gradient is invariant under reparametrization of @xmath6 .",
    "hence , the natural gradient step  steepest ascent step w.r.t .",
    "the fisher metric  is invariant at least for an infinitesimal step size ( * ? ? ?",
    "* section  2.4 ) .      for completeness , we include here a short description of the igo algorithm .",
    "we refer to @xcite for a more complete presentation .",
    "first , igo transforms the objective function into an adaptive weighted preference by a quantile based approach .",
    "this results in a rank based algorithm , invariant under increasing transformations of the objective function .",
    "define the lower and upper @xmath14-@xmath3-quantiles of @xmath36 as @xmath37 \\\\   { q_\\theta^\\leq}(x ) & { \\mathrel{\\mathop:}=}p_\\theta[y : f(y ) \\leq f(x ) ] \\enspace . \\end{aligned}\\ ] ] the lower quantile value @xmath38 is the probability of sampling strictly better points than @xmath39 under the current distribution @xmath14 , while the upper quantile value @xmath40 is the probability of sampling points better than or equivalent to @xmath39 . given",
    "a weight function ( selection scheme ) @xmath41 \\to { \\mathbb{r}}$ ] that is non - increasing , the weighted preference @xmath42 is defined as @xmath43 this way , the quality of a point is measured by a function of the @xmath14-quantile in which it lies .",
    "a typical choice of the selection scheme @xmath44 is @xmath45 } } / q$ ] , @xmath46 .",
    "we call it the",
    "_ @xmath2-truncation _",
    "selection scheme . using @xmath2-truncation amounts , in the final igo algorithm , to giving the same positive weight to a fraction @xmath2 of the best samples in a population , and weight @xmath47 to the rest , as is often the case in practice .",
    "next , igo turns the original objective function @xmath3 on the search space @xmath5 into a function @xmath22 on the statistical manifold @xmath7 by defining @xmath48 \\enspace .",
    "\\label{eq : jt}\\ ] ] note that @xmath22 depends on the current position @xmath49 .",
    "then , the gradient of @xmath22 is computed as @xmath50 \\\\      & = \\nabla\\!_\\theta \\!\\int { w^f_{\\theta^t}}(x ) \\,p_\\theta(x ) \\ , { { { \\mathrm{d}\\hspace{-0.02em}}{}x}}\\\\      & = \\int { w^f_{\\theta^t}}(x ) \\,p_\\theta(x ) \\,\\nabla\\!_\\theta \\ln p_\\theta(x ) \\,{{{\\mathrm{d}\\hspace{-0.02em}}{}x}}\\\\      & = { \\mathbb{e}}_{p_\\theta } [ { w^f_{\\theta^t}}(x ) \\ , \\nabla\\!_\\theta \\ln p_\\theta(x ) ] \\enspace . \\end{split }   \\label{eq : grad - j}\\ ] ] here we have used the relation @xmath51 .",
    "finally , igo uses natural gradient ascent on the parameter space .",
    "the natural gradient on the statistical manifold @xmath52 equipped with the fisher metric @xmath53 is given by the product of the inverse of the fisher information matrix , @xmath54 , and the vanilla gradient .",
    "that is , the natural gradient of @xmath55 at @xmath6 is written as @xmath56 . according to",
    ", we can rewrite the natural gradient as @xmath57 \\enspace .",
    "\\label{eq : ng}\\ ] ] introducing a finite step size @xmath58 , igo finally updates the parameter as follows @xmath59      when implementing igo in practice , it is necessary to estimate the expectation in .",
    "the approximation is done by the monte carlo method using @xmath60 samples taken from @xmath17 .",
    "let @xmath61 be independent samples from @xmath17 .",
    "first , we need to approximate @xmath62 for each @xmath63 .",
    "define @xmath64 let @xmath65 and set @xmath66 then @xmath67 is a consistent estimator of @xmath62 , in other words , @xmath68 with probability one .",
    "( see the proof of theorem  4 in @xcite . )",
    "if there are no ties in our sample , i.e.  @xmath69 for any @xmath70 , then @xmath71 and simply reads @xmath72 , but is a mathematically neater definition of rank based weights accounting for possible ties . in practice we just design the @xmath60 weight values @xmath73 , instead of the selection scheme @xmath44 .    in the rest of this article , we assume for simplicity that the selection weights @xmath74 are non - negative and sum to @xmath1 .",
    "this is the case , for instance , if the selection scheme @xmath44 is @xmath2-truncation as above .",
    "next , monte carlo sampling is applied to the expectation , using @xmath75 and @xmath76 . replacing the expectation with a sample average @xmath77 and @xmath62 with @xmath78",
    ", we get @xmath79 again , @xmath80 is a consistent estimator of the igo step at @xmath49 , i.e. , of @xmath81 .",
    "see theorem  4 in @xcite .",
    "now the practical igo algorithm implementation can be written in the form of a black - box search algorithm as    1 .",
    "sample @xmath76 , @xmath82 , independently from @xmath17 ; 2 .",
    "evaluate @xmath83 and compute @xmath84 and @xmath85 ; 3 .",
    "evaluate @xmath86 ; 4 .",
    "update the parameter : @xmath87 .    finally , to obtain an explicit form of the parameter update equation , we need to know the explicit form of the natural gradient of the log - likelihood , which depends on a family of probability distributions and its parametrization .",
    "explicit forms of @xmath88 are known for some specific families of probability distributions with specific parametrizations , and the above algorithm sometimes coincides with several known algorithms .",
    "[ ex : pbil ] the family of bernoulli distributions on @xmath89 is defined as @xmath90 .",
    "the natural gradient of the log - likelihood is readily computed as @xmath91 ( section  4.1 in @xcite ) .",
    "the natural gradient update reads @xmath92 this is equivalent to so - called pbil ( population based incremental learning , @xcite).see section  4.1 in @xcite for details .",
    "[ ex : cmaes ] the probability density function of a multivariate gaussian distribution on @xmath93 with mean vector @xmath94 and covariance matrix @xmath95 , is defined as @xmath96 when @xmath97 , the explicit form of @xmath98 is known to be @xmath99 $ ] ( see @xcite ) .",
    "then , the natural gradient update reads @xmath100 this is equivalent to the pure rank-@xmath0 cma - es update @xcite @xmath101 except that @xmath102 in the natural gradient update .      in the sequel , we prove monotone improvement of the objective function for a variant of igo known as igo - maximum likelihood ( igo - ml , introduced in ( * ? ? ?",
    "* section  3 ) ) . the result",
    "is then transferred to igo because the two algorithms exactly coincide in an important class of cases , namely , exponential families using mean value parametrization .",
    "the igo - ml algorithm ( * ? ? ?",
    "* section  3 ) updates the current parameter value @xmath49 by taking a weighted maximum likelihood of the current distribution and the best sampled points .",
    "assume as above that @xmath103 .",
    "then the _ igo - ml update _ is defined as @xmath104 \\\\+\\,{\\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}\\sum_i { \\widehat w_i}\\ln p_{\\theta}(x_i ) \\bigg\\}\\end{gathered}\\ ] ] where we note that the first part is the cross - entropy of @xmath17 and @xmath14 , and thus , taken alone , is maximized for @xmath105 . taking the limit @xmath106 , we also define the _ infinite - population igo - ml update _ as @xmath107 + \\,{\\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}{h_t}(\\theta ) \\big\\}\\ ] ] where we set @xmath108\\ ] ] a `` weighted cross - entropy '' of @xmath6 and @xmath49 .",
    "note that the finite- and infinite - population igo - ml updates only make sense when there is a unique maximizer @xmath6 in and , respectively .",
    "this assumption is always satisfied , for instance , for exponential families of probability distributions , as considered below ( statement  [ stat : unique - ml ] ) .",
    "the igo - ml update is compatible with the igo update , in the sense that for @xmath109 the direction and magnitude of these updates coincide ( * ? ? ?",
    "* section  3 ) .",
    "the igo - ml method is also related to the cross - entropy ( ce ) or maximum - likelihood ( ml ) method for optimization @xcite , which can be written as @xmath110 and its smoothed version which reads @xcite @xmath111 note that igo - ml is parametrization - independent whereas for @xmath112 the smoothed ce / ml method is not .",
    "consequently , in general these updates will differ .",
    "an _ exponential family _ is a set @xmath113 of probability density functions @xmath13 with respect to an arbitrary measure @xmath15 on @xmath5 defined as @xmath114 where @xmath115 is the so - called _ natural _ ( i.e.  canonical ) _",
    "parameter _ ; each @xmath116 , @xmath117 is a map @xmath118 such that @xmath119 are linearly independent ; @xmath120 is the normalization factor .",
    "this linear independence ensures that the manifold of the exponential family is nonsingular .",
    "many probability models , including multivariate gaussian distributions , are expressed as exponential families .",
    "see ( * ? ? ?",
    "* section  2.3 ) for examples .",
    "if we define @xmath121 = \\int t(x ) \\,p_\\theta(x ) \\,{{{\\mathrm{d}\\hspace{-0.02em}}{}x}}\\enspace , \\label{eq : ep}\\ ] ] @xmath122 is the so - called _",
    "expectation parameter_. for example , the expectation parameter for the multivariate gaussian distribution encodes the first moment @xmath123 $ ] and the second moment @xmath124 $ ] .",
    "other examples can be found in ( * ? ? ?",
    "* section  3.5 ) .",
    "we will repeatedly and implicitly make use of the following well - known fact for exponential families .",
    "[ stat : unique - ml ] let @xmath125 be @xmath126 points in @xmath127 and let @xmath128 be non - negative numbers with @xmath129 .",
    "then the value @xmath6 of the parameter such that the associated expectation parameter satisfies @xmath130 , if it belongs to the statistical manifold , is the unique maximizer of the weighted log - likelihood : @xmath131 .",
    "an analogous statement holds if the finite sum is replaced with an integral or a combination of both .",
    "( uniqueness boils down to strict concavity of @xmath132 as a function of @xmath6 .",
    "the restriction placed on @xmath133 to belong to the statistical manifold is necessary : for instance , for gaussian distributions , if the number of points @xmath126 is not greater than the dimension of the ambient space , a degenerate distribution @xmath6 will result . )    the following statement from @xcite shows that the natural gradient of a function in the expectation parametrization is given by the vanilla gradient of the function w.r.t .",
    "the normal parameter , and vice versa .",
    "[ sta : ng - vg ] let @xmath23 be a function on the statistical manifold of an exponential family as above . then the components of the natural gradient w.r.t .",
    "the expectation parameters are given by the vanilla gradient w.r.t .",
    "the natural parameters and vice versa , that is , @xmath134    according to statement  [ sta : ng - vg ] , each component of the natural gradient of the log likelihood @xmath132 under the exponential parametrization @xmath135 is equivalent to each component of the vanilla gradient , i.e. , @xmath136 where the latter equality is well - known , e.g. , ( * ? ? ?",
    "* ( 2.33 ) ) .",
    "the igo update under the expectation parametrization thus reads @xmath137 \\label{eq : igo - exp}\\ ] ] and the natural gradient update with finite sample size reads @xmath138    suppose as above that the selection weights sum to one : @xmath139 = \\int_0 ^ 1 w(q ) { \\mathrm{d}\\hspace{-0.02em}}q = 1 $ ] and thus @xmath103 .",
    "then , igo has a close relation with the ce / ml for optimization . as is stated in theorem  15 in @xcite , for an exponential family the ce / ml method and the igo instance , when expressed with the expectation parametrization ( @xmath135 ) ,",
    "coincide with igo - ml .    for optimization using an exponential family @xmath11 ,",
    "these three algorithms coincide : igo - ml ; the igo expressed in expectation parameters ; the ce / ml expressed in expectation parameters .",
    "that is , for an exponential family with the expectation parametrization , for @xmath140 we have ( writing in turn igo , ce / ml and igo - ml ) @xmath141   \\\\    & \\qquad \\qquad \\qquad \\qquad + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}\\sum_{i=1}^{\\lambda } { \\widehat w_i}\\ln p_\\theta(x_i ) \\biggr\\ }     \\enspace .",
    "\\end{split } \\label{eq : igo - equiv - sample}\\ ] ]    in the limit of infinite sample size @xmath106 this rewrites @xmath142 \\\\    & = ( 1 - { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em } } } ) \\theta^t + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}{\\mathop{\\arg\\,\\max}\\limits}_\\theta { h_t}(\\theta ) \\\\    &",
    "= { \\mathop{\\arg\\,\\max}\\limits}_\\theta \\bigl\\ { ( 1 - { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em } } } ) { \\mathbb{e}}_{p_{\\theta^t } } \\left [ \\ln p_\\theta(x)\\right ] + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}{h_t}(\\theta ) \\bigr\\ }   \\end{split } \\label{eq : igo - equiv}\\ ] ] where we recall that @xmath143 $ ] .",
    "[ rem : malago ] malag  et  al .",
    "@xcite study information - geometric aspects of exponential families for optimization .",
    "one difference from the igo framework is that the optimization problem is defined as the minimization of the expectation of the objective function over @xmath14 , namely @xmath144,\\ ] ] which they call the stochastic relaxation of the original optimization problem .",
    "they study this for an exponential family on a discrete search space with the natural parametrization ( @xmath145 ) and propose the natural gradient descent algorithm .",
    "note that this requires computation of the empirical fisher information matrix to perform natural gradient descent .",
    "however , if the algorithm is modified to use the expectation parameters instead , one can compute the natural gradient descent directly as @xmath146 we study this algorithm in section  [ sec : fit ] .",
    "one possible way to provide theoretical backing for an optimization algorithm is to show monotonic improvement at each step of the algorithm ( although this is by no means necessary : e.g. , for stochastic algorithms , this is not expected to hold at each step ) .",
    "for example , consider the sphere function @xmath147 .",
    "then , it is easy to show that the gradient steps @xmath148 generate a monotonically decreasing sequence @xmath149 provided @xmath150 . for any smooth function ,",
    "infinitesimal gradient steps are guaranteed to improve the objective function values ; but in general the admissible step size strongly depends on the function and has to be adjusted by the user .    when it comes to the counterpart in igo , however , we follow the gradient of the function @xmath22 , which depends on @xmath49 , so that step - by - step improvement in the objective , @xmath151 , does not necessarily mean improvement .",
    "( it might happen that @xmath152 and @xmath151 at the same time . )",
    "a key feature of the igo framework is its invariance under changing the objective function @xmath3 by an increasing transformation ( e.g.  optimizing @xmath153 instead of @xmath3 ) .",
    "thus , any measure of progress that is not compatible with such transformations ( e.g.  the expectation @xmath154 ) is not a good candidate to always improve over the course of igo optimization .    as a measure of improvement ,",
    "arnold  et  al .",
    "@xcite use the notion of _ @xmath2-quantile of @xmath3_. the @xmath2-quantile @xmath155 of @xmath3 under a probability distribution @xmath156 is any number @xmath94 such that @xmath157 \\geq q$ ] and @xmath158 \\geq 1 - q$ ] .",
    "for instance , @xmath155 is the median value of @xmath3 under @xmath156 if @xmath159 . for smooth distributions and continuous @xmath3",
    "there is only one such number @xmath94 , but in general the set of such @xmath94 may be a closed interval , for instance if @xmath3 has `` jumps '' . for the sake of definiteness let us use the largest such value : @xmath160 \\geq q   \\\\ &",
    "\\text{and } p[x : f(x ) \\geq m ] \\geq 1 - q \\big\\ } \\enspace.\\end{aligned}\\ ] ] ( this is because we want to minimize the objective function @xmath3 ; when igo is used for maximization instead , theorem  [ thm : qimp ] has to be written using an infimum in the definition of @xmath155 instead . )",
    "it is proven in @xcite that when using the @xmath2-truncation selection scheme , the @xmath2-quantile value of @xmath3 monotonically decreases along infinitesimal igo steps .    [ proposition  5 in @xcite ] consider the @xmath2-truncation selection scheme @xmath161 } } / q$ ] where @xmath46 is fixed .",
    "then each infinitesimal igo step where @xmath58 is infinitesimal leads to monotonic improvement in the @xmath2-quantile of @xmath3 : @xmath162 .      in practice ,",
    "explicit algorithms do not use continuous time with infinitesimal time steps : the time step @xmath58 may be quite large and its calibration may be an important issue .",
    "it is more interesting and important to see how long steps we can take along the natural gradient , i.e.  how large a @xmath58 we can choose while guaranteeing @xmath2-quantile improvement . when using igo - ml ( and thus when using igo or ce / ml on an exponential family with the expectation parametrization ) , we can obtain such a conclusion ; the size of the steps may even be chosen independently of the objective function .",
    "[ thm : qimp ] let the selection scheme be @xmath161 } } / q$ ] where @xmath163 .",
    "assume that the @xmath164 defining the igo - ml step is uniquely determined .",
    "then for @xmath165 , each infinite - population igo - ml step leads to @xmath2-quantile improvement : @xmath162 .",
    "moreover , equality can hold only if @xmath166 or if @xmath167 > 0 $ ] .    for exponential families written in expectation parameters , on any search space ,",
    "the same holds for the ce / ml method and for the igo algorithm .",
    "note that the first condition for equality means the algorithm has reached a stable point .",
    "the second condition for equality typically happens for discrete search spaces : on such spaces , the @xmath2-quantile evolves in time by discrete jumps even when @xmath49 moves smoothly , so we can not expect strict quantile improvement at each step . on the other hand , with continuous distributions on continuous search spaces ,",
    "the second equality condition can only occur if the objective function has a plateau ( a level set with non - zero measure ) .    if @xmath168 , obviously @xmath169 .",
    "hereunder , we assume @xmath170 .",
    "consider the function @xmath22 defining the expected @xmath17-adjusted fitness of a random point under @xmath14 : @xmath171\\ ] ] and remember that @xmath172 .",
    "the idea is as follows : letting @xmath173 be the set of points with @xmath174 at which the objective function @xmath3 is smallest ( the sublevel set of @xmath3 with @xmath17-mass @xmath2 ) , then with our choice of @xmath44 , @xmath20 is ( up to technicalities ) equal to @xmath175 on @xmath173 and @xmath47 elsewhere , so that @xmath22 represents @xmath175 times the @xmath14-probability of falling into @xmath173 ( hence @xmath172 ) .",
    "thus @xmath176 will mean that the @xmath14-probability of falling into @xmath173 is larger than @xmath2 , so that @xmath14 improves over @xmath17 and the @xmath2-quantile has decreased .",
    "we are going to prove that the igo - ml update satisfies @xmath177 if @xmath178 .",
    "more precisely we prove that @xmath179 this will imply quantile improvement , thanks to the following lemma , the proof of which is postponed .",
    "[ lem : jandq ] let the selection scheme @xmath44 be as above . if @xmath180 , then @xmath162 .",
    "if moreover @xmath181 = 0 $ ] , then @xmath182 .",
    "the lower bound on @xmath183 is obtained as follows . since @xmath184 and @xmath185 for any @xmath39",
    ", @xmath186 can be viewed as a probability density function . since @xmath187 is concave , by jensen s inequality we have @xmath188 thus , if @xmath189 we have @xmath190 .",
    "now , according to , @xmath191 uniquely maximizes the quantity @xmath192 + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}{h_t}(\\theta)$ ] .",
    "therefore , if @xmath193 , we have @xmath194 + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}{h_t}(\\theta^{t+{\\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em } } } } ) \\\\   > ( 1 - { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em } } } ) { \\mathbb{e}}_{p_{\\theta^t } } \\bigl [ \\ln p_{\\theta^t}(x)\\bigr ] + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}{h_t}(\\theta^{t})\\end{gathered}\\ ] ] and rearranging we get @xmath195   -   { \\mathbb{e}}_{p_{\\theta^t } } \\bigl [ \\ln p_{\\theta^{t+{\\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}}}(x)\\bigr ] \\right )   \\\\",
    "= \\frac{1 - { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}}{{\\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em } } } } { d_\\mathrm{kl}(p_{\\theta^t } \\!\\parallel\\ !",
    "p_{\\theta^{t+{\\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em } } } } } ) } \\enspace .",
    "\\label{eq : handkl}\\end{gathered}\\ ] ] the right - hand side of this inequality is non - negative for @xmath165 .",
    "this will prove the theorem once lemma  [ lem : jandq ] is proved , which we now proceed to do .",
    "hereunder , we abbreviate @xmath196 for the @xmath2-quantile value @xmath197 of @xmath3 under @xmath17 .",
    "let us compute the weighted preference @xmath20 .",
    "since the selection scheme @xmath44 satisfies @xmath198 for all @xmath199 $ ] , we have @xmath200 for any @xmath39 .",
    "we claim that @xmath201 implies @xmath202 . indeed , suppose that @xmath39 is such that @xmath201 .",
    "since by definition @xmath94 is the largest value such that @xmath203\\geq 1-q$ ] , we must have @xmath204<1-q$ ] .",
    "hence @xmath205>q$ ] , i.e. , @xmath206 .",
    "now this implies @xmath202 for our choice of selection scheme @xmath44 .",
    "thus @xmath20 is at most @xmath175 and vanishes if @xmath201 . for any probability distribution @xmath14",
    ", this implies that @xmath207\\leq \\frac1q\\ , p_{\\theta}[x : f(x)\\leq m ] \\enspace.\\ ] ] therefore , @xmath208 > q \\\\   \\longrightarrow { } & { q_{p_\\theta}^q}(f ) \\leq { m}\\enspace.\\end{aligned}\\ ] ] if moreover @xmath209 = 0 $ ] , we have @xmath210 = p_\\theta[x : f(x ) < { m}]$ ] hence @xmath211 > q \\\\   \\longleftrightarrow { } & p_\\theta[x : f(x ) \\geq { m } ] < 1 - q",
    "\\\\   \\longrightarrow { } & { q_{p_\\theta}^q}(f ) < { m}\\enspace.\\end{aligned}\\ ] ]    altogether , @xmath180 implies quantile improvement @xmath162 .",
    "moreover , if @xmath212 = 0 $ ] , we have strict quantile improvement @xmath182 .",
    "this completes the proof of theorem  [ thm : qimp ] .",
    "bernoulli distributions constitute an exponential family where the sufficient statistics @xmath213 are @xmath76 .",
    "the parameter @xmath6 used in pbil ( example  [ ex : pbil ] ) is indeed the expectation parameter .",
    "thus , pbil is an instance of igo - ml and can be viewed as a ce / ml method at the same time .",
    "hence , by theorem  [ thm : qimp ] , each infinite - population pbil step leads to @xmath2-quantile improvement if we employ @xmath2-truncation selection , which is not the same as the exponential weights introduced in @xcite .",
    "the proof of the theorem is quantitative : the kullback ",
    "leibler divergence @xmath214 indicates how much progress was made .",
    "more precisely ( assuming for simplicity a continuous situation with no plateaus ) , while the probability under @xmath17 to fall into the best @xmath2 percent of points for @xmath17 is @xmath2 by definition , the probability under @xmath215 to fall into the best @xmath2 percent of points for @xmath17 is at least @xmath216 .",
    "the expectation parameter is not always the most obvious one .",
    "when it comes to multivariate gaussian distributions , the expectation parameter is the mean vector and second moment , ( @xmath94 , @xmath217 ) .",
    "meanwhile , the cma - es and the ce / ml method for continuous optimization parametrize the mean vector and covariance matrix , hence they differ from the igo - ml algorithm . moreover , sometimes different step sizes ( learning rates ) are employed for each parameter , which makes the direction of parameter update different from that of the natural gradient . here",
    ", we justify some of these settings by guaranteeing @xmath2-quantile improvement in an extended framework .",
    "we define an extension of igo - ml , _ blockwise igo - ml _ , that recovers the pure rank-@xmath0 cma - es update with different learning rates for @xmath94 and @xmath95 .",
    "let @xmath218 be any decomposition of the parameter @xmath6 into @xmath126 blocks , and let @xmath219 be a step size for each block . for @xmath220",
    ", define the _",
    "@xmath221-th block partial igo - ml update _ with step size @xmath222 as the map sending a parameter value @xmath6 to @xmath223 where @xmath224    \\\\ + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}_j \\sum_i { \\widehat w_i}\\ln p_{\\theta^*}(x_i ) \\bigg\\ } \\enspace.\\end{gathered}\\ ] ] the _ blockwise igo - ml _ updates the parameter @xmath6 as follows .",
    "given a current parameter value @xmath49 , update the first block of @xmath49 , then the second block , etc .",
    ", in that order ; explicitly , set @xmath225 where we note that the same monte carlo sample @xmath226 from @xmath17 is used throughout the whole range of block updates @xmath227 .",
    "the infinite - population step ( @xmath228 ) reads the same with @xmath229    \\\\ + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}_j \\,{\\mathbb{e}}_{p_{\\theta^t } } \\bigl[{w^f_{\\theta^t}}(x )   \\ln p_{\\theta^*}(x ) \\bigr ] \\bigg\\ } \\enspace .",
    "\\label{eq : ideal - block - igoml}\\end{gathered}\\ ] ]    as before , the finite- and infinite - population blockwise igo - ml updates only make sense if the @xmath164 in or is uniquely determined",
    ".    note that the blockwise igo - ml depends on the decomposition of the parameters into blocks and their update order , while it is independent of the parametrization inside each block .",
    "blockwise igo - ml is not necessarily equivalent to igo - ml even when all @xmath230 are equal to @xmath58 .",
    "[ prop : igo - cma ] the pure rank-@xmath0 cma - es update ( example  [ ex : cmaes ] ) is an instance of blockwise igo - ml for gaussian distributions , with parameter decomposition @xmath231 where @xmath232 , the covariance matrix , and @xmath233 , the mean vector .    given @xmath234 , blockwise igo - ml first updates @xmath95 as follows : @xmath235    \\\\ + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}_c \\sum_i { \\widehat w_i}\\ln p_{(c , m^t)}(x_i ) \\bigg\\ } \\enspace .",
    "\\label{eq : c - igoml}\\end{gathered}\\ ] ] considering @xmath236 as an exponential family of gaussian distributions whose mean vector is fixed to @xmath237 , can be viewed as an ordinary igo - ml step for this restricted model .",
    "then , since ( after shifting the origin of the coordinate system to @xmath237 ) @xmath95 is the expectation parameter of the restricted model , the update is given by   namely @xmath238 next , @xmath94 is updated as @xmath239    \\\\ + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}_m \\sum_i { \\widehat w_i}\\ln p_{(c^ * , m)}(x_i ) \\bigg\\ } \\enspace.\\end{gathered}\\ ] ] to derive @xmath240 , let us differentiate the inside of @xmath164 w.r.t .",
    "@xmath94 and derive the zero point of the derivative .",
    "seeing that @xmath241 , we find the condition @xmath242 which holds if and only if @xmath243 this is equivalent to the pure rank-@xmath0 cma - es update .",
    "quantile improvement as in theorem  [ thm : qimp ] readily extends to this setting as follows .",
    "[ thm : iter - qimp ] let the selection scheme be @xmath161 } } / q$ ] where @xmath163 .",
    "assume that the @xmath164 defining each partial infinite - population igo - ml update is uniquely determined .",
    "then for @xmath244 ( @xmath245 ) , each infinite - population blockwise igo - ml step leads to @xmath2-quantile improvement : @xmath246 .",
    "moreover , equality can hold only if @xmath247 or if @xmath248 > 0 $ ] .    consequently , each infinite - population step of the pure rank-@xmath0 cma - es update guarantees @xmath2-quantile improvement .",
    "indeed , from proposition  [ prop : igo - cma ] this variant of the cma - es is an instance of blockwise igo - ml .",
    "moreover , if each level set of @xmath3 has zero lebesgue measure , which often holds for continuous optimization , we have strict @xmath2-quantile improvement .    if @xmath249 , obviously @xmath250 .",
    "we assume @xmath251 in the following .",
    "set @xmath252 and @xmath253 so that @xmath254 . according to lemma  [ lem : jandq ] , to prove quantile improvement it is enough to show that @xmath255 .",
    "moreover , this implies strict quantile improvement provided @xmath256 = 0 $ ] .    according to , if @xmath257 we have that @xmath258 . to show that @xmath259 we decompose @xmath260 into the sum of partial differences , namely , @xmath261 and we will prove that each term is non - negative . moreover , if @xmath262 for some @xmath263 , we will have @xmath264 for this @xmath221 . since @xmath265 implies @xmath262 for at least one @xmath263 , we will have that @xmath266 , resulting in @xmath258 .",
    "we proceed as in theorem  [ thm : qimp ] .",
    "since @xmath267 is the only maximizer of , we have @xmath268     + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}_j { h_t}(\\theta^{t , j } ) \\\\",
    "\\geq ( 1 - { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}_j ) { \\mathbb{e}}_{p_{\\theta^{t , j-1}}}\\left [ \\ln p_{\\theta^{t , j-1}}(x ) \\right ]     + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}_j { h_t}(\\theta^{t , j-1 } ) \\end{gathered}\\ ] ] with equality holding if and only if @xmath269 . rearranging , we get @xmath270 if @xmath271 , and @xmath272 if @xmath269 .",
    "the right - hand side of the above inequality is non - negative for @xmath165 .",
    "therefore , @xmath273 for all @xmath274 .",
    "moreover , since @xmath251 , for at least one @xmath275 we have @xmath271 and thus @xmath276 for this @xmath221 , implying that @xmath277 .",
    "this completes the proof .",
    "the results above are valid for `` ideal '' updates with infinite sample size . with finite sample size",
    ", the update defines a stochastic sequence ( depending on the random sample @xmath226 ) and so one can not expect monotone @xmath2-quantile improvement at each step . still , we can expect @xmath2-quantile improvement with high probability when the population size is sufficiently large .",
    "we provide an analogue of theorem  [ thm : qimp ] for finite but large population size .",
    "a similar statement holds for blockwise igo - ml .",
    "the proof follows a standard probabilistic approximation argument .",
    "let @xmath278 be the @xmath2-truncation selection scheme : @xmath161 } } / q$ ] where @xmath163 .",
    "let @xmath11 be an exponential family of probability distributions , parametrized by its expectation parameter .",
    "assume that the @xmath164 defining the infinite - population igo - ml step is uniquely defined .",
    "assume that for all @xmath279 , the derivative @xmath280 exists for @xmath14-almost all @xmath281 and has finite second moment : @xmath282<\\infty$ ] .",
    "let @xmath283 .",
    "let @xmath284 be the igo - ml update with sample size @xmath60 , and let @xmath285 be the infinite - population igo - ml update .",
    "assume that @xmath286 .",
    "then , with probability tending to @xmath1 as @xmath106 , the finite - population update @xmath284 results in @xmath2-quantile improvement : @xmath287    consequently , the same holds for the ce / ml method and the igo algorithm when they are applied to an exponential family using the expectation parameters .",
    "note the assumption that the _ ideal _ dynamics has not reached equilibrium yet : @xmath286 .",
    "if @xmath288 , the finite - population dynamics will just randomly wander around this equilibrium value with some noise , resulting in either improvement or deterioration at each step .    also note that the population size @xmath60 needed may depend on the current location @xmath49 in parameter space , as well as the objective function @xmath3 .",
    "for instance , highly oscillating functions @xmath3 likely require higher population sizes for a consistent estimation of the igo - ml update .    for exponential families , the igo and igo - ml updates coincide . under the conditions of the theorem ,",
    "the finite - population igo update is a consistent estimator of the infinite - population igo update ( * ? ? ?",
    "* proposition  18 ) , implying that @xmath289 converges with probability one to @xmath290 . under our regularity assumptions on @xmath14 ,",
    "this implies pointwise convergence of @xmath291 to @xmath292 , which , since @xmath20 is bounded , leads to @xmath293 \\\\",
    "{ \\mathbb{e}}_{p_{{\\theta^{t+{\\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}}}_\\infty } } [ { w^f_{\\theta^t}}(x ) ] = { j({\\theta^{t+{\\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}}}_\\infty\\,|\\,\\theta^t ) } \\quad \\text{as } \\lambda \\to \\infty.\\end{gathered}\\ ] ] now the right - hand side is greater than @xmath1 for @xmath165 unless @xmath294 , as we have shown in the proof of theorem  [ thm : qimp ] .",
    "thus , we have @xmath295 with high probability for sufficiently large @xmath60 .",
    "thus lemma  [ lem : jandq ] entails @xmath2-quantile improvement with high probability .",
    "these results carry over to the use of a composite @xmath24 of a function @xmath23 with the objective function @xmath3 , as a selection weight instead of @xmath25 in the igo framework .",
    "this covers , for instance , fitness - proportional selection ( @xmath296 ) .",
    "we prove that , when considering the natural gradient ascent for an exponential family using the expectation parameter , we can guarantee monotone @xmath297$]-value improvement for updates of step size inversely proportional to @xmath297 $ ] . more precisely ,",
    "[ thm : fitness - imp ] assume @xmath24 is non - negative and not almost everywhere @xmath47 .",
    "consider the update @xmath298 } \\left ( t(x ) -",
    "\\theta^t \\right ) \\right ] \\enspace ,   \\label{eq : fit - igo}\\ ] ] where @xmath135 is the expectation parameter of the exponential family @xmath11 .",
    "then for @xmath165 , we have @xmath299 \\geq { \\mathbb{e}}_{p_{\\theta^t}}[g \\circ f(x ) ] \\enspace .\\ ] ] moreover , equality can occur only if @xmath168 .",
    "gradient based methods with fitness - proportional selection are often employed , especially in reinforcement learning , e.g. _ policy gradient with parameter based exploration _",
    "( pgpe ) @xcite .",
    "one disadvantage of gradient based methods is that the step size has to be calibrated by the user depending on the problem at hand .",
    "alternative methods such as _ expectation - maximization _",
    "@xcite , including the rpp below , are sometimes employed to avoid this issue .",
    "theorem  [ thm : fitness - imp ] , however , ensures that each natural gradient step improves the expected fitness for @xmath165 when an exponential family is used with its expectation parameters .    the relative payoff procedure ( rpp ) @xcite is a reinforcement learning algorithm , also known as expectation - maximization ( em ) algorithm for reinforcement learning @xcite .",
    "the rpp expresses a policy on the action space @xmath89 by a bernoulli distribution @xmath300 parametrized by the expectation parameter .",
    "the objective to be maximized is the expectation @xmath301 $ ] of non - negative reward @xmath302 after taking action @xmath36 .",
    "the rpp updates the parameters to @xmath303}{{\\mathbb{e}}_{p_{\\theta^t}}[r(x ) ] } \\enspace.\\ ] ] remember the sufficient statistics @xmath304 for bernoulli distributions are @xmath305 . thus the rpp is equivalent to with @xmath306 and @xmath307 and can be viewed as a natural gradient ascent with large step .",
    "the rpp is known from @xcite to monotonically improve expected reward , thanks to its expectation - maximization interpretation .",
    "theorem  [ thm : fitness - imp ] can be thought of as an extension of this result , and also shows monotone improvement for the smoothed rpp , where a step size @xmath165 is introduced .",
    "most of the proof of theorem  [ thm : qimp ] carries over .",
    "replacing @xmath25 in with @xmath308 $ ] , still holds and we have @xmath309 } \\left ( t(x ) - \\theta^t \\right ) \\right ] \\\\    & = { \\mathop{\\arg\\,\\max}\\limits}_\\theta \\bigg\\ { ( 1 - { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em } } } ) { \\mathbb{e}}_{p_{\\theta^t } } \\left [ \\ln p_\\theta(x)\\right ] \\\\   & \\quad \\qquad + { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}{\\mathbb{e}}_{p_{\\theta^t } } \\left [ \\frac{g \\circ f(x)}{{\\mathbb{e}}_{p_{\\theta^t}}[g \\circ f(x ) ] } \\ln p_\\theta(x)\\right ] \\bigg\\ } \\end{split } \\label{eq : f - igo - equiv}\\ ] ]    thanks to jensen s inequality , we have the counterpart of as @xmath310 - \\ln { \\mathbb{e}}_{p_{\\theta^t}}[g \\circ f(x ) ] \\\\   \\geq \\frac{{\\mathbb{e}}_{p_{\\theta^t } } \\left [ g \\circ f(x ) \\ln p_\\theta(x)\\right ] }   { { \\mathbb{e}}_{p_{\\theta^t}}[g \\circ f(x ) ] }     -   \\frac{{\\mathbb{e}}_{p_{\\theta^t } } \\left [ g \\circ f(x )",
    "\\ln p_{\\theta^t}(x)\\right]}{{\\mathbb{e}}_{p_{\\theta^t}}[g \\circ f(x ) ] } \\enspace .",
    "\\label{eq : f - jandh}\\end{gathered}\\ ] ]    because of the second equality of , we have the counterpart of as @xmath311 }   { { \\mathbb{e}}_{p_{\\theta^t}}[g \\circ f(x ) ] }     -   \\frac{{\\mathbb{e}}_{p_{\\theta^t } } \\left [ g \\circ f(x )",
    "\\ln p_{\\theta^t}(x)\\right]}{{\\mathbb{e}}_{p_{\\theta^t}}[g \\circ f(x ) ] } \\\\",
    "\\geq   \\frac{1 - { \\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}}{{\\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em } } } } { d_\\mathrm{kl}(p_{\\theta^t } \\!\\parallel\\ ! p_{\\theta^{t+{\\ensuremath{\\delta\\hspace{-.06em}t\\hspace{0.05em}}}}})},\\end{gathered}\\ ] ] and moreover , since the maximizer in is unique , the inequality is strict unless @xmath312 .",
    "hence , since the right - hand side is non - negative , by we have @xmath313 \\geq \\ln { \\mathbb{e}}_{p_{\\theta^t}}[g \\circ f(x)]$ ] with equality only if @xmath314 .",
    "this completes the proof .",
    "as mentioned in remark  [ rem : malago ] , malag  et  al.@xcite propose the natural gradient algorithm for discrete optimization using exponential distributions .",
    "however , as they parametrize the exponential distributions by the natural parameters @xmath145 , theorem  [ thm : fitness - imp ] does not guarantee expected fitness improvement for their algorithms , whereas it does so for the algorithm using the expectation parameters .",
    "these results contribute to bringing theory closer to practice , by waiving the need for infinitesimal step sizes in gradient ascent .",
    "still , they cover only the `` ideal '' situation with infinite population size , as well as finite but very large population sizes ( by a standard probabilistic approximation argument ) .",
    "finite population sizes lead to stochastic behavior and so monotone objective improvement at each step occurs only with high probability .    in practice",
    ", population sizes used can be quite small , @xmath315 , with medium to small step sizes @xcite .",
    "it has been shown in ( * ? ? ? * remark  2 ) that when population size does not tend to infinity , the expectation of the natural gradient estimate is the natural gradient with a _ different _ selection scheme @xmath44 .",
    "so using the truncation weight @xmath316 } } $ ] with a small population size and very small step sizes will result , by the machinery of stochastic approximation @xcite , in simulating an infinite - population igo step with another selection scheme , a situation outside the scope of this article .",
    "our results , on the contrary , suggest using larger populations and larger step sizes instead .",
    "finally , let us stress that objective improvement is not , by itself , a sufficient guarantee that optimization performs well : in situations of premature convergence , the objective still improves at each step .",
    "premature convergence can occur for large values of the learning rate in some instantiations of igo and igo - ml ( see the study in @xcite ) ; our results say nothing about this phenomenon .",
    "y.  akimoto , a.  auger , and n.  hansen .",
    "convergence of the continuous time trajectories of isotropic evolution strategies on monotonic @xmath317-composite functions . in _",
    "parallel problem solving from nature - ppsn xii , 12th international conference _ , number 7491 in lecture notes in computer science , pages 4251 , taormina , italy , september 15 2012 .",
    "springer .",
    "y.  akimoto , y.  nagata , i.  ono , and s.  kobayashi .",
    "bidirectional relation between cma evolution strategies and natural evolution strategies . in r.",
    "schaefer , c.  cotta , j.  kolodziej , and g.  rudolph , editors , _ parallel problem solving from nature - ppsn xi , 11th international conference _ , volume 6238 of _ lecture notes in computer science _ , pages 154163 , krakw , poland , september 1115 2010 .",
    "springer .",
    "l.  malag , m.  matteucci , and g.  pistone . towards the geometry of estimation of distribution algorithms based on the exponential family . in h .-",
    "beyer and w.  b. langdon , editors , _ foga 11 : proceedings of the 11th workshop proceedings on foundations of genetic algorithms _ , pages 230242 .",
    "acm , 2011 ."
  ],
  "abstract_text": [
    "<S> _ information - geometric optimization _ ( igo ) is a unified framework of stochastic algorithms for optimization problems . given a family of probability distributions , igo turns the original optimization problem into a new maximization problem on the parameter space of the probability distributions . </S>",
    "<S> igo updates the parameter of the probability distribution along the natural gradient , taken with respect to the fisher metric on the parameter manifold , aiming at maximizing an adaptive transform of the objective function . </S>",
    "<S> igo recovers several known algorithms as particular instances : for the family of bernoulli distributions igo recovers pbil , for the family of gaussian distributions the pure rank-@xmath0 cma - es update is recovered , and for exponential families in expectation parametrization the cross - entropy / ml method is recovered .    </S>",
    "<S> this article provides a theoretical justification for the igo framework , by proving that any step size not greater than @xmath1 guarantees monotone improvement over the course of optimization , in terms of @xmath2-quantile values of the objective function @xmath3 . </S>",
    "<S> the range of admissible step sizes is independent of @xmath3 and its domain . </S>",
    "<S> we extend the result to cover the case of different step sizes for blocks of the parameters in the igo algorithm . </S>",
    "<S> moreover , we prove that expected fitness improves over time when fitness - proportional selection is applied , in which case the rpp algorithm is recovered .    </S>",
    "<S> = 10000 = 10000    [ optimization ] </S>"
  ]
}