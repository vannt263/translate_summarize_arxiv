{
  "article_text": [
    "the purpose of this work is to describe a unified , and indeed simple , mechanism for non - parametric bayesian analysis of a large class of generative latent feature models which one can describe as generalized notions of indian buffet processes(ibp ) .",
    "this work will not only provide a thorough description of the properties of existing models but also provide a simple template to devise and analyze new models .",
    "that is to say , although one of the goals in this article is to promote a particular calculus , the results are presented in a fashion where it is not necessary for the reader interested primarily in implementation and new constructions to have a command of that calculus .",
    "the ibp , and its generalizations , represent an exciting class of models well suited to handle high dimensional problems now common in this information age .",
    "these points are articulated in for instance @xcite , and for placement in a wider context see  @xcite .",
    "the ibp is based on the usage of conditionally independent bernoulli random variables , one way to think of generalizations is to to use more general random variables .",
    "of note in the current literature are models employing poisson and negative - binomial random variables as described in for instance @xcite .",
    "these are generally coupled with gamma and beta process priors . even in those specialized cases",
    "the properties of these models are not well understood , and the analysis conducted so far requires great care .",
    "additionally , for instance , it is unclear how replacing a gamma process with a generalized gamma process works in the model of @xcite .",
    "indeed a generalized gamma process is not conjugate in the poisson model but it does have extra flexibility over its gamma process counterpart .",
    "this article describes general results for ibp type processes based on variables having any distribution @xmath0 that has mass at zero and otherwise can be discrete or continuous .",
    "examples of models employing continuous @xmath1 are given in  @xcite .",
    "furthermore , the results are presented for general prior models based on completely random measures . that is to say the analysis does not require conjugacy .",
    "the method proposed is via the _ poisson partition calculus _ ( ppc ) devised by the author in  @xcite , and applied further for instance in  @xcite .",
    "the _ partition _",
    "calculus refers to a non - combinatorial approach to systematically derive expressions for combinatorial mechanisms derived from bayesian exchangeable processes whereby @xmath2 are conditionally iid and where @xmath3 is a random probability measure or more generally a random measure , that can be expressed as a functional of a poisson random measure @xmath4 one is interested in the posterior distribution of @xmath5 and the marginal distribution of @xmath6 due to discreteness , the marginal distribution of @xmath6 can be viewed as an analogue of a blackwell - macqueen polya urn scheme , which is the generative sampling scheme corresponding to the marginal joint probability measure induced by setting @xmath7 to be a dirichlet process random probability measure , and modeling @xmath8 to be iid @xmath9 furthermore , these structures describe random partitions of @xmath10 by the number of unique values among @xmath11 , which can be viewed as clusters , and the size of clusters based on the tied values . loosely speaking",
    "these can be described as analogues of chinese restaurant processes(crp ) , most closely associated with the dirichlet and pitman - yor processes which are used as priors / models in bayesian non - parametric statistics and employed extensively in statistical machine learning .",
    "see for instance @xcite .",
    "we shall provide a schematic for this method as it relates to latent feature models , but first describe more details for the ibp and its generalizations .",
    "the basic @xmath12 process , for @xmath13 was ingeniously formulated by griffiths and ghahramani  @xcite whereby a random binary matrix is formed with @xmath14 distinct features or attributes labeling an unbounded number of columns , and m rows , where each row @xmath15 represents the attributes / features / preferences possessed by a single individual by entering @xmath16 in the @xmath17 entry if the feature @xmath18 is possessed and @xmath19 otherwise .",
    "the matrix naturally indicates what features are shared among individuals . the generative process to describe this is cast in terms of individual customers sequentially entering an indian buffet restaurant whereby the @xmath15-th customer chooses one of the @xmath20 already sampled dishes(indicates that customer shares features already exhibited by the previous customers ) according to the most popular dishes already sampled , specifically with probability @xmath21 where @xmath22 denotes the number of customers who have already sampled dish @xmath23 .",
    "otherwise the customer chooses new dishes according to a @xmath24 distribution .",
    "a key insight was made by thibaux and jordan  @xcite , which connected this generative process with a formal bayesian non - parametric framework where @xmath25 are modelled as iid bernoulli processes with base measure @xmath3 that is selected to have a beta process prior distribution whose iid atoms @xmath26 are obtained by specifying a proper probability @xmath27 as its base measure .",
    "the generative process is given by the distribution of @xmath28 in other words the basic ibp@xmath29 is generated from a random matrix with entries @xmath30 where conditioned on the @xmath31 the points of poisson random measure with mean intensity @xmath32 @xmath33 are independent bernoulli @xmath34 variables .",
    "it follows that one can represent each @xmath35 and the beta process @xmath36    the concept of beta processes was developed in the fundamental paper of hjort  @xcite as it relates to bayesian np problems arising in survival analysis , kim  @xcite is also an important reference in this regard . naturally within this context",
    "is the related work of doksum  @xcite .",
    "however it is used in a rather different context in the ibp setting .",
    "other uses of a beta process within a lvy moving process context can be found in james  @xcite which apparently is a precursor to the kernel smoothed beta process in @xcite .",
    "spatial notions of beta and other processes appear in  @xcite .",
    "using the theory of marked poisson point processes we can easily describe the generalization of the ibp that includes the models already mentioned above . simply replace the @xmath37 with more general variables @xmath38 where conditional on @xmath39 @xmath40 are independent random variables(possibly vector valued ) with distribution denoted as @xmath41 where @xmath42 are the points of a poisson random measure with more general lvy density @xmath43 not restricted to @xmath44 $ ] and possibly depending on @xmath45 which reflects dependence on the distribution of the @xmath46 @xmath47 on a space @xmath48 importantly , for each @xmath49 @xmath50 are the points of a poisson random measure with mean intensity @xmath51",
    "furthermore we shall assume that @xmath40 can take on the value zero with positive probability .",
    "so relative to @xmath52 write @xmath53 and hence @xmath54    the general construction is where now @xmath55    notice that one can always represent @xmath56 where @xmath33 are bernoulli variables and @xmath57 is a general random variable that is not necessarily independent of @xmath58 so that @xmath59 in ( [ genpair ] ) can be viewed as a weighted bernoulli process .",
    "the dependent representation makes these more general than models of the form considered in @xcite .",
    "here key to our exposition , it is important to note that @xmath3 can always be represented as @xmath60 where @xmath61 is a poisson random measure with mean intensity @xmath62=\\rho(s|\\omega)dsb_{0}(d\\omega):=\\nu(ds , d\\omega).\\ ] ] as in james@xcite , @xmath63 takes its values in the space of boundedly finite measures , @xmath64    it is further noted that @xmath65 can be defined over any polish space .",
    "this may be relevant in applications where @xmath66 is specified in terms of more general parameters , such as @xcite , say @xmath67 for instance @xmath68 could correspond to a normal distribution with mean and variance parameters @xmath69 such extensions from a technical point of view are easily handled and will not be discussed explicitly here in the univariate case .",
    "however the multivariate extension in section 5 covers this case .",
    "we say that @xmath63 is @xmath70 or more generally @xmath71 and also write @xmath72 when discussing calculations .",
    "@xmath3 is by construction a completely random measure and we shall specify its law by saying @xmath3 is @xmath73 or @xmath74 using this we shall say that @xmath25 are iid @xmath75 if they have the specifications in  ( [ genpair ] ) and call the marginal distribution of @xmath59 @xmath76 or equivalently @xmath77    formally we choose @xmath78 to satisfy @xmath79 however we allow both infinite activity models corresponding to @xmath80 and finite activity / compound models where @xmath81 in the latter case @xmath82 is a proper density .",
    "notice that in  ( [ genpair ] ) the only difference in the setup is the choice of the distribution on @xmath83 obviously different choices of @xmath66 lead to different modeling capabilities and interpretations .",
    "we note further that the ppc is a method that does not rely on conjugacy .",
    "that is to say if it can be applied for one choice of @xmath84 it can be applied for all choices of @xmath85 in the forthcoming section we shall provide a schematic to derive the posterior distribution and related quantities for any @xmath0 and any @xmath85 we will then state formally the posterior distribution for @xmath63 and @xmath3 , marginal distribution of @xmath86 and the distributions of @xmath87 we shall also introduce classes of iid variables @xmath88 which can provide a representation for the marginal distribution of @xmath89 hence all @xmath59 that can be implemented for most choices of @xmath85 this is one of the key elements to describe the appropriate analogues of the indian buffet generative process that can be implemented broadly .",
    "we note that in order for this to apply , @xmath66 must at minimum admit a positive mass at @xmath90 or @xmath78 must be taken to be a finite measure .",
    "overall we shall show how to do the following for any @xmath84 without taking limits , without guesswork and without combinatorial arguments , and lastly without long proofs .",
    "we shall then show details for all choices of @xmath91 bernoulli , poisson , negative - binomial , that have been mentioned above .",
    "* generally apply the poisson calculus .",
    "* describe @xmath92 * hence @xmath93 * describe the marginal structure @xmath94 via integration .",
    "* provide descriptions for the marginal process as @xmath95 that via the introduction of variables @xmath88 leads to tractable sampling schemes for many @xmath78 * describe @xmath96 * these last two points lead to rather explicit descriptions of the marginal process that apply for many @xmath78 and hence lead to generative schemes that generalize the ibp feature selection scheme .    in section 5 we will describe and provide parallel details for a large class of multivariate models .",
    "much of what has been described in the univariate case carries over quite clearly to the multivariate case .",
    "we shall also highlight the use of a beta - dirichlet process described in  @xcite as a natural extension of a beta process in this setting .",
    "one should first note that poisson random measures are the building blocks for most discrete random processes used in the bayesian nonparametrics literature . certainly this is true for any case where one employs completely random measures .      for our purposes here we will need two ingredients of the ppc that can be found as propositions 2.1 and 2.2 in james  @xcite[see also @xcite ] .",
    "these represent direct extensions of updating mechanisms for the dirichlet and gamma processes employed in @xcite and also  @xcite .",
    "note again that @xmath97      first ( * ? ? ?",
    "* proposition 2.1 ) describes a laplace functional exponential change of measure which describes updating @xmath63 through changing the mean measure @xmath98 to @xmath99 @xmath100 where in our setting @xmath101 and @xmath102,$ ] where @xmath103 this action changes @xmath63 from @xmath71 to @xmath104      next using @xmath105 apply the moment measure calculation appearing in ( * ? ? ?",
    "* proposition 2.2 . ) @xmath106\\mathcal{p}(dn|\\nu_{f})=\\mathcal{p}(dn|\\nu_{f},\\mathbf{s,\\omega})\\prod_{\\ell=1}^{k}{\\mbox e}^{-f(s_{\\ell},\\omega_{\\ell})}\\nu(ds_{\\ell},d\\omega_{\\ell})\\ ] ] expressed over @xmath107 uniquely picked points @xmath108 from @xmath4 in other words conditioned on @xmath109 the distribution of these points is determined by the joint measure @xmath110    there are two important things to notice about  ( [ prop2 ] ) .",
    "first is the meaning of @xmath111 which corresponds to the law of the random measure @xmath112 where @xmath113 is also @xmath104 the quantity above is merely a decomposition of @xmath63 in terms of the unique points picked and those remaining .",
    "it says remarkably , and in fact is well known , that @xmath113 maintains the same law as @xmath4 this translates into the following result , for any measurable function @xmath114 @xmath115 furthermore the marginal measure @xmath116=\\mathbb{e}_{\\nu_{f}}[\\prod_{\\ell=1}^{k}n(ds_{\\ell},d\\omega_{\\ell})],\\ ] ] only depends on the @xmath20 unique values and not the multiplicities that might be associated with them . for a proof of this see  james ( *",
    "* proposition 5.2 ) .",
    "the form of the relevant likelihood can be deduced from the discussion above .",
    "however , it is perhaps instructive to first write it similar to the form used in ( * ? ? ?",
    "* appendix a.1 ) @xmath117}^{{\\mathbb{i}}_{\\{a_{i , j}\\neq 0\\ } } } { [ 1-\\mathbb{p}(a\\neq 0|\\tau_{j})]}^{1-{\\mathbb{i}}_{\\{a_{i , j}\\neq 0\\}}}\\ ] ] which , setting @xmath118 can be written as @xmath119}\\prod_{i=1}^{m}\\prod_{j=1}^{\\infty}{\\left[\\frac{g_{a}(da_{i , j}|\\tau_{j } ) } { 1-\\pi_{a}(\\tau_{j})}\\right]}^{{\\mathbb{i}}_{\\{a_{i , j}\\neq 0\\}}}.\\ ] ] it then follows by , arguments similar to ( * ? ? ?",
    "* appendix a.1 ) , and augmenting @xmath120 that one can write the joint distribution of @xmath121 where @xmath122 are the unique jumps , as , @xmath123}^{{\\mathbb{i}}_{\\{a_{i,\\ell}\\neq 0\\}}}.\\ ] ]    where @xmath124.$ ]    now apply the exponential change of measure in ( [ prop1 ] ) to obtain @xmath125}^{{\\mathbb{i}}_{\\{a_{i,\\ell}\\neq 0\\}}}.\\ ] ] noticing that , @xmath126}^{m},\\ ] ] this operation results in the law of @xmath63 changing from @xmath71 to @xmath127 that is @xmath128 is changed to @xmath129}^{m}\\rho(s|\\omega)b_{0}(d\\omega)ds.\\ ] ]    now apply the disintegration in  ( [ prop2 ] ) to obtain the desired final form of the joint distribution @xmath130 or @xmath131}^{m } \\rho(s_{\\ell}|\\omega_{\\ell})b_{0}(d\\omega_{\\ell } ) ) \\prod_{i=1}^{m}h_{i,\\ell}(s_{\\ell})\\ ] ] where @xmath132}^{{\\mathbb{i}}_{\\{a_{i,\\ell}\\neq 0\\}}},\\ ] ] and @xmath133}^{m})\\rho(s|\\omega)dsb_{0}(d\\omega ) .",
    "\\label{psifunctionm}\\ ] ]    note in order to emphasize a notion of partial conjugacy , for any lvy density @xmath78 and @xmath134 we could without loss of generality define a levy density @xmath135}^{\\beta}\\rho(s|\\omega)\\ ] ] which we would assign to the prior measure @xmath3 note of course that @xmath136 hence it follows that for @xmath137 and @xmath138 that @xmath139}^{\\beta+m}\\rho(s|\\omega)\\ ] ] we will use some variant of this for illustration in the special cases of poisson , binomial and negative - binomial models .",
    "it is to be emphasized that once a quantity such as  ( [ mainjoint ] ) has been calculated this contains all the ingredients for a formal posterior analysis .",
    "what remains are arguments via bayes rule and finite - dimensional refinements involving particular choices of @xmath0 @xmath78 etc .      now from ( [ nsplit ] ) and ( [ mainjoint ] ) one can immediately deduce that for fixed @xmath140 that @xmath141 corresponds to the case where @xmath142 is @xmath143 hence @xmath3 can be expressed as @xmath144 where @xmath145 is a @xmath146 what remains to complete the posterior distribution of @xmath147 is to find the distribution of @xmath148 given @xmath149 integrating out @xmath63 in  ( [ mainjoint ] ) leads to the joint distribution of @xmath150 @xmath151}^{m}\\rho(s_{\\ell}|\\omega_{\\ell})b_{0}(d\\omega_{\\ell } ) ) \\prod_{i=1}^{m}h_{i,\\ell}(s_{\\ell}).\\ ] ] note this is now in the usual finite dimensional bayesian setup .",
    "hence @xmath152 are conditionally independent with density , @xmath153}^{m}\\rho(s|\\omega_{\\ell } ) \\prod_{i=1}^{m}h_{i,\\ell}(s)ds\\ ] ] and the marginal of @xmath149 is @xmath154}^{m}\\rho(s|\\omega_{\\ell } ) \\prod_{i=1}^{m}h_{i,\\ell}(s)ds\\right]b_{0}(d\\omega_{\\ell}).\\ ] ]      we now summarize our results . throughout",
    "we use , @xmath155 where @xmath156}^{m}\\rho(s|\\omega).$ ] note that @xmath157 depends on @xmath1 and so will differ over various models . since it should be clear from context , we shall suppress this dependence within the notation .    [ genthm ] suppose that @xmath25 are iid @xmath75 @xmath3 is @xmath158 then    1 .   the posterior distribution of @xmath147 is equivalent to the distribution of @xmath159 where @xmath160 is @xmath161 and the distribution of the conditionally independent @xmath148 is given by  ( [ marginalj ] ) .",
    "the posterior distribution of @xmath93 is equivalent to the distribution of @xmath162 where @xmath145 is @xmath163 3 .   the marginal distribution of @xmath164 is given by  ( [ margz ] ) .",
    "this immediately leads to the next result    [ zproposition ] suppose that @xmath165 are iid @xmath75 , where @xmath3 is @xmath158 then from results in theorem  [ genthm ] , equation ( [ genmu ] ) shows that the distribution of @xmath96 is equivalent to the distribution of @xmath166 where @xmath167 is @xmath168 and each @xmath169 has distribution @xmath170 and the marginal distribution of @xmath171 is specified by ( [ marginalj ] ) .",
    "that is to say @xmath172 has the distribution @xmath173 @xmath174 are the already chosen features .",
    "it should be evident that it easy to make adjustments for the case where the @xmath25 are independent rather than iid .",
    "furthermore semi - parametric models may be treated as in  @xcite , see also  @xcite .      before we continue with a general discussion , we take a look at specifics in the poisson , bernoulli , and negative - binomial cases .",
    "note while we can just employ theorem  [ genthm ] and proposition  [ zproposition ] directly and list out results , we provide some specific developments of those general arguments used in these special cases for illustrative purposes .",
    "note if @xmath175 corresponds to poisson@xmath176 bernoulli@xmath177 , or negative - binomial@xmath178 , denoted as @xmath179 , random variable then respectively the probability mass function with arguments @xmath180 is in the poisson@xmath181 case @xmath182 in the bernoulli@xmath183 case , @xmath184 and in the negative - binomial @xmath178 case @xmath185 hence in the respective cases @xmath186 takes the values @xmath187 where in the latter two cases @xmath188.$ ] furthermore , @xmath189 given in  ( [ hfunction ] ) takes the respective forms @xmath190 now setting @xmath191 it follows that @xmath192 takes the form @xmath193 note to simplify notation we shall do the remaining calculations for homogeneous @xmath85 readers can easily make the adjustments to the case of @xmath194 first some notation and known facts are introduced .",
    "it is important to note that the points and notation described next can be found in gnedin and pitman  @xcite and james  @xcite . which alludes to connection between the poisson , bernoulli , and negative - binomial ibp models and regenerative compositions/ neutral to the right processes .",
    "many explicit calculations and examples for these ibp models can be deduced from those works .",
    "we encourage the reader to take a look at those works for such details .",
    "let @xmath195 be an infinitely divisible random variable where @xmath196 are the ranked jumps of a subordinator determined by the levy density @xmath197 then it follows that @xmath198={\\mbox e}^{-\\phi(\\lambda)},$ ] where @xmath199 from @xmath196 one can construct an infinitely divisible random variable @xmath200 whose points are determined by a levy density @xmath201}(u)=(1-u)^{-1}\\tau_{\\infty}(-\\log(1-u ) ) , u\\in [ 0,1]\\ ] ] and there is also the converse relation @xmath202}(1-{\\mbox e}^{-y } ) , y\\in ( 0,\\infty).\\ ] ] furthermore @xmath203|}(\\lambda)=\\int_{0}^{1}(1-{(1-u)}^{\\lambda})\\tau_{[0,1]}(du)\\ ] ] which is equivalent to @xmath204}(\\lambda)=\\int_{0}^{1}\\lambda{(1-u)}^{\\lambda-1 } \\left[\\int_{u}^{1}\\tau_{[0,1]}(dv)\\right]du.\\ ] ]    note taking @xmath205}(u)=\\theta u^{-1}(1-u)^{\\beta-1}$ ] gives the homogeneous beta process , and it can be read from  gnedin@xcite that for an integer @xmath206 @xmath207}(m)=\\sum_{k=1}^{m}\\frac{\\theta}{\\beta+k-1},\\ ] ] which is the term appearing in @xmath208 in the bernoulli ibp case under a beta process crm .",
    "call @xmath25 in this model , iid @xmath209 processes .",
    "it follows that if @xmath3 is determined by the levy density @xmath210 then it follows for each m that , @xmath211 for any lvy density @xmath85 hence denote the marginal distribution of @xmath212 as @xmath213 furthermore , @xmath214 where @xmath215 . hence setting @xmath216",
    "the joint distribution in  ( [ mainjoint3 ] ) is @xmath217    it should be evident from ( [ poissonjoint ] ) that the marginal distribution in the poisson case is given as @xmath218b_{0}(d\\omega_{\\ell}),\\ ] ] where @xmath219 in addition , the posterior distribution of @xmath92 corresponds to that of the random measures @xmath220 where @xmath160 is @xmath221 and independent of this , the distribution of @xmath222 are conditionally independent with density @xmath223 this implies that @xmath224 is equivalent in distribution to @xmath225 where @xmath145 is @xmath226 note further if we refer to the marginal distribution of @xmath212 as @xmath227 our analysis shows that @xmath87 can be written as @xmath228 where @xmath167 is @xmath229 and @xmath230 are conditionally independent poisson@xmath231 variables .",
    "it is easy to see that although we can not expect @xmath147 and hence @xmath93 to be fully conjugate models , there is a sense of partial conjugacy in the following manner .",
    "suppose that the mean intensity of @xmath63 is @xmath232={\\mbox e}^{-\\beta s}\\rho(s)b_{0}(d\\omega),$ ] then within the posterior decomposition  ( [ poissondecomp ] ) the prm @xmath160 has mean intensity @xmath233={\\mbox e}^{-(\\beta+bm ) s}\\rho(s)b_{0}(d\\omega).\\ ] ]      we now give details for the gamma - poisson model that was investigated in titsias  @xcite and then move beyond that to the case where the gamma process is replaced by a generalized gamma process .",
    "there are no known results for this flexible and natural extension , we should note here that the gamma process model of titsias bears some similarities to lo ( 1982 )  @xcite .",
    "the latter is a reference which is apparently unknown within the general ibp literature .",
    "lo  @xcite shows that if a weighted gamma process is used as a prior for the mean intensity of a poisson process then its posterior distribution is also a weighted gamma process given the observations from @xmath234 poisson processes . in this sense",
    "it can be seen as a precursor to the class of ibp models .",
    "we describe a scalar version of a weighted gamma process , which is just based on a gamma random variable @xmath235 with shape parameter @xmath236 and scale @xmath237 with law denoted as gamma@xmath238 that is to say if @xmath235 is such a variable then @xmath239 is gamma@xmath240 this corresponds to @xmath63 a prm with mean intensity @xmath62=\\tilde{\\rho}_{\\theta,\\beta}=\\theta s^{-1}{\\mbox e}^{-\\beta s}b_{0}(d\\omega),\\ ] ] and hence @xmath3 is a weighted gamma process with parameters @xmath241 note in lo s case one takes @xmath242 to be a function @xmath243 which again presents no extra difficulties here . from the above discussion",
    "it follows that",
    "@xmath244=\\theta s^{-1}{\\mbox e}^{-(\\beta+bm ) s}b_{0}(d\\omega),$ ] and hence @xmath145 is a weighted gamma process with parameters @xmath245 furthermore the distribution of @xmath246 is such that @xmath247 are conditionally independent gamma@xmath248 random variables .",
    "hence the @xmath172 are negative binomial variables as described in titsias .",
    "furthermore the lvy exponent is @xmath249 and hence the joint distribution @xmath208 is given by @xmath250 where @xmath251 note under @xmath252 @xmath253 and the @xmath254 are poisson@xmath255 hence the marginals of @xmath172 are negative - binomial .",
    "now suppose that for @xmath256 @xmath63 is prm with mean intensity @xmath62/(b_{0}(d\\omega)ds)=\\tau_{\\infty}(s):=\\rho_{\\alpha,\\beta}(s):=\\frac{\\alpha s^{-\\alpha-1}{\\mbox e}^{-s\\beta}}{\\gamma(1-\\alpha)}\\ ] ] then @xmath3 is a generalized gamma process with parameters @xmath257 corresponding to a random variable @xmath235 with density @xmath258}f_{\\alpha}(t),$ ] where @xmath259 is the density of a positive stable random variable with index @xmath260 furthermore @xmath261.\\ ] ]    it follows that @xmath145 is a generalized gamma process with parameters @xmath262 and the jumps satisfy @xmath263 are independent gamma @xmath264 random variables .",
    "hence @xmath265 can be expressed via @xmath266 note under @xmath252 @xmath167 is @xmath267 and the @xmath254 are poisson@xmath255 hence the marginals of @xmath172 are again negative - binomial .",
    "note it is interesting to compare the generalized gamma case in this setting to cases of lvy moving average processes  ( * ? ? ?",
    "* , p.23 ) , neutral to the right processes  ( * ? ? ?",
    "* , section 6.1 ) and normalized random measures from an application of  @xcite as it appears in ( * ? ? ?",
    "* example 2.4 , p.341 ) .",
    "when @xmath268 is @xmath269 this corresponds to the original ibp@xmath270 structure where details have been worked out primarily in the case where @xmath3 is some variant of a beta process ,  see @xcite . in those cases results",
    "are deduced from @xcite . here",
    "we provide details for any @xmath78 .",
    "again in order to indicate how updates are made we write for any @xmath137 and any @xmath78 defined on @xmath44,$ ] set @xmath271}^{\\beta}\\rho(s).\\ ] ] the results are stated using this choice , however there is no loss of generality to set @xmath272 assuming that @xmath232=\\hat{\\rho}_{\\beta}(s)b_{0}(d\\omega)ds,$ ] the joint distribution of @xmath273 can be expressed as @xmath274}(\\beta+m)}\\prod_{\\ell=1}^{k}{\\left(\\frac{s_{\\ell}}{1-s_{\\ell}}\\right)}^{c _ { \\ell , m}}\\hat{\\rho}_{\\beta+m}(ds_{\\ell}|\\omega_{\\ell})b_{0}(d\\omega_{\\ell})\\ ] ] where with respect to @xmath78 @xmath275}(\\beta+m)&=&\\phi_{[0,1]}(\\beta+m)-\\phi_{[0,1]}(\\beta)\\\\\\nonumber & = & \\int_{0}^{1}(1-{(1-s)}^{m})\\hat{\\rho}_{\\beta}(s)ds\\end{aligned}\\ ] ]    integrating out @xmath63 leaves a joint distribution of the latent jumps , and the latent feature structure , which we write in a different way as , @xmath276}(\\beta+m)}\\prod_{\\ell=1}^{k}{s_{\\ell}}^{c _ { \\ell , m}}{(1-s_{\\ell})}^{\\beta+m - c_{\\ell , m}}\\rho(ds_{\\ell}|\\omega_{\\ell})b_{0}(d\\omega_{\\ell}),\\ ] ] which can be expressed as @xmath277\\mathbb{p}(z_{1},\\ldots , z_{m})\\ ] ] where for @xmath278 @xmath279}(\\beta+m)}\\prod_{\\ell=1}^{k } \\hat{\\kappa}_{c_{\\ell , m}}(\\hat{\\rho}_{\\beta+m})b_{0}(d\\omega_{\\ell})\\ ] ] and hence showing that the distribution of @xmath171 has density proportional to @xmath280}^{c_{\\ell , m}}\\rho_{\\beta+m}(s).\\ ] ] it follows that @xmath92 can be expressed as @xmath281 where @xmath160 is a prm with @xmath244=\\rho_{\\beta+m}(s)dsb_{0}(d\\omega)$ ] and the distribution of the jumps @xmath148 have been described above .",
    "it follows that @xmath212 under these specifications is marginally @xmath282 which is equivalent in distribution to @xmath283 where @xmath284 is a poisson random variable with mean @xmath285 @xmath87 is equivalent to @xmath286 where @xmath167 is @xmath287 and the @xmath172 are independent bernoulli variables with success probability @xmath288=\\frac{\\int_{0}^{1}{s}^{c _ { \\ell , m}+1}{(1-s)}^{-c_{\\ell , m}}\\hat{\\rho}_{\\beta+m}(s)ds}{\\hat{\\kappa}_{c_{\\ell , m}}(\\hat{\\rho}_{\\beta+m})}.\\ ] ] in particular if @xmath289 as in @xcite , the @xmath148 are independent @xmath290 random variables then the customer @xmath291 chooses an existing dish @xmath292 with probability @xmath288=\\frac{c_{\\ell , m}-\\alpha}{m+\\beta}.\\ ] ] set @xmath293 and @xmath294 to recover the original ibp model of griffiths and ghahramani  @xcite .",
    "note for this case , one can choose @xmath295 , such that @xmath296 which are the specifications for the case where @xmath3 is a stable - beta process discussed in  @xcite .",
    "see section [ bd ] for a multivariate extension of this process we call a stable - beta - dirichlet process .",
    "one can compare the beta process in this setting with that of a smoothed spatial beta process arising in the context of lvy moving averages ( * ? ? ?",
    "* , p. 25 - 26 ) .",
    "as mentioned previously several authors have investigated the case where @xmath268 is @xmath297 and primarily this has been coupled with a crm that is a beta process . with some efforts  @xcite were able to show that the beta process was conjugate in this setting,[see also @xcite ] . again here it is demonstrated that applying the ppc makes analysis of considerably generalized notions of this case rather transparent . as should be expected , the nb case bears some similarities to the bernoulli case , here we start with @xmath63 based on a levy density @xmath298^{\\beta r}\\rho(s)$ ] the joint distribution can be expressed as @xmath299\\mathbb{p}(z_{1},\\ldots , z_{m})\\ ] ] where @xmath300\\mathbb{p}(z_{1},\\ldots , z_{m } ) $ ] is equivalent to @xmath276}((\\beta+m)r ) } \\prod_{\\ell=1}^{k}{s_{\\ell}}^{c _ { \\ell , m}}{(1-s_{\\ell})}^{(m+\\beta)r}\\rho(ds_{\\ell})r_{\\ell , m}b_{0}(d\\omega_{\\ell})\\ ] ] where now @xmath301 it is evident that the distribution of the @xmath171 has density proportional to @xmath302 furthermore setting @xmath303 it follows that @xmath304}((\\beta+m)r ) } \\prod_{\\ell=1}^{k}\\kappa_{c_{\\ell , m}}(\\hat{\\rho}_{(\\beta+m)r ) } ) r_{\\ell , m}b_{0}(d\\omega_{\\ell}).\\ ] ] we quickly identify other relevant quantities .",
    "@xmath160 is a prm with mean satisfying @xmath233/(b_{0}(d\\omega)ds)=\\hat{\\rho}_{(\\beta+m)r}(s).\\ ] ] hence @xmath145 is @xmath305 @xmath87 is such that @xmath306 and the @xmath172 are conditionally @xmath307 variables .",
    "the results above provide a general framework to describe generative processes analogous to the indian buffet sequential latent feature scheme .",
    "what remains is to describe the marginal distributions of @xmath309 and hence @xmath310 which makes up the un - sampled part of @xmath311    since @xmath312 is composed of @xmath313 the theory of marked poisson point processes tells us that the unconditional laplace functional of @xmath314 is given by @xmath315={\\mbox e}^{-\\phi(f)},$ ] where @xmath316 where @xmath317 , $ ] and the distribution of the @xmath318 is determined by the measure @xmath319 this is clear since at @xmath320 @xmath321 is zero .",
    "if we assume that @xmath322 corresponds to an infinite activity process then @xmath323 this means essentially that if @xmath324 does not assign mass to @xmath90 @xmath325 is not a finite measure .",
    "so for instance the @xmath326 after marginalizing out @xmath327 could be the jumps of a gamma process .",
    "for concreteness we pause to give an example of that case .",
    "suppose that @xmath328 is a positive stable random variable of index @xmath256 with log laplace exponent evaluated at @xmath242 equal to @xmath329 then set @xmath330 hence @xmath78 is the levy density of a gamma process with shape @xmath331 and scale @xmath332 then from aldous and pitman  ( * ? ? ?",
    "* eq.(33))there is the identity @xmath333 which shows that in this instance @xmath334 that is to say the marginal distribution of @xmath308 corresponds to a gamma process .",
    "now if @xmath335 as we have assumed , denotes the probability that @xmath336 under @xmath66 then if @xmath337 the @xmath338 are independent with distribution @xmath339 of course if @xmath78 is homogeneous the @xmath318 would be iid @xmath340 in this case set @xmath341 then it follows that the marginal process @xmath308 can be represented as @xmath342 where @xmath343 is a poisson random variable with mean @xmath344 and the @xmath345 are iid @xmath340 however even in this ideal situation .",
    "it will be difficult to sample directly from @xmath346 for arbitrary @xmath85 we now describe the marginal distribution of @xmath308 utilizing an infinite sequence of iid pairs of variables @xmath347    [ hdecomp ] let @xmath348 be @xmath349 where @xmath3 is a @xmath350 with @xmath210 a homogeneous lvy density . then the @xmath308 is said to have an @xmath76 marginal distribution .",
    "suppose that @xmath351 then there exists a sequence of iid pairs @xmath352 which we refer to as an @xmath353 process , such that @xmath354 where @xmath343 is a poisson random variable with mean @xmath344 independent of @xmath347 the pair have the following distributional properties :    1 .   @xmath355 has distribution , not depending on @xmath84 @xmath356 which is equivalent to the conditional distribution of @xmath68 under @xmath52 and @xmath357 has marginal density @xmath358 2 .",
    "the marginal distribution of @xmath345 is @xmath359 and the distribution of @xmath360 is given by @xmath361    the result is straightforward as @xmath362 coupled with the results we discussed previously for marked poisson processes , allows one to manipulate a joint distribution proportional to @xmath363",
    "as mentioned previously one does not need to understand the ppc in order to use the results that have been obtained .",
    "the ingredients for that are listed below .      1 .",
    "specify @xmath175 2 .",
    "identify @xmath118 3 .   for each",
    "@xmath364 @xmath365 implying @xmath366}^{m}\\rho(s|\\omega)b_{0}(d\\omega)ds\\ ] ] and @xmath367}^{m})\\rho(s|\\omega)dsb_{0}(d\\omega)\\ ] ] hence for each @xmath368 define @xmath369}^{m}\\rho(s|\\omega).$ ] 4 .",
    "use this to specify @xmath370 @xmath371 and @xmath372 5 .",
    "identify @xmath373}^{{\\mathbb{i}}_{\\{a_{i,\\ell}\\neq 0\\}}},\\ ] ] and look for simplifications of @xmath374 which certainly happens when @xmath1 is bernoulli , poisson or negative - binomial .",
    "6 .   use this to get the distribution of @xmath148 and marginal of @xmath164 specified by ( [ marginalj ] ) and ( [ margz ] ) .",
    "this gives the distribution of @xmath375 specified in proposition  [ zproposition ] .",
    "note multiplicities ( counts ) seen in the cases of bernoulli , poisson and negative - binomial arise through those choices of @xmath66 and manifest themselves through simplifications of @xmath376      1 .   for each @xmath368 define @xmath369}^{m}\\rho(s).$ ] 2 .   calculate and check , @xmath377 3 .   then if @xmath167 is @xmath378 @xmath379 where @xmath380 and @xmath343 is a poisson@xmath381 random variable , independent of @xmath382 which are taken from a @xmath383 process @xmath384 4 .",
    "@xmath355 has distribution , not depending on @xmath84 @xmath385 and @xmath357 has marginal density @xmath386 5 .   the marginal distribution of @xmath345 is @xmath387 and the distribution of @xmath360 is given by @xmath388    .",
    "we can now use sections [ posterioringredients ] and [ ibpingredients ] to describe the sequential generative process for @xmath389 in the homogeneous case . that is to say how to sample from @xmath390 and",
    "subsequently @xmath28 since under @xmath66 every matrix entry value for a feature can take a wider range of values , one needs to give this a proper interpretation .",
    "this is mostly left to the reader , but here we shall naively say that a customer selects a dish many times , or gives a scoring to that dish . at any rate we use the basic ibp metaphor of people sequentially entering an indian buffet restaurant .",
    "recall again that @xmath391    1 .",
    "customer 1 selects dishes and gives them scores according to a @xmath76 process .",
    "this is done precisely as follows : 1 .",
    "draw a @xmath392 number of variables .",
    "2 .   draw @xmath393 iid from @xmath27 and the distribution for @xmath357 specified by ( [ hmarg ] ) , with @xmath394 3 .   draw @xmath395 following ( [ xcond ] ) , for @xmath396 note if it is straightforward to sample directly from the marginal distribution of @xmath397 then one may bypass sampling @xmath357 2 .",
    "after @xmath398 customers have chosen collectively @xmath399 distinct dishes , and assigned their scores , customer @xmath291 selects each dish @xmath400 and assigns respective scores , according to the distribution of @xmath401 where @xmath402 has conditional distribution @xmath403 in particular the chance that @xmath402 takes the value zero is @xmath404 the distribution of @xmath148 is specified in  ( [ marginalj ] ) .",
    "customer m+1 also chooses and scores new dishes according to a @xmath405 process @xmath406 this follows the same scheme as customer @xmath16 with @xmath369}^{m}\\rho(s)$ ] in place of @xmath78 and details provided in section [ ibpingredients ] .",
    "we next provide more details for @xmath88 in the poisson and negative - binomial cases .      in the poisson ibp setting the random @xmath407 process @xmath408 actually appears in pitman  @xcite . here again for simplicity we shall assume @xmath409 is a homogeneous levy density not depending on @xmath410 the extension is obvious",
    ". in this case @xmath345 takes values @xmath411 with @xmath412 note that @xmath413 and @xmath414 is the relevant total mass .",
    "in addition , @xmath415 is the @xmath23-th cumulant of an infinitely divisible random variable @xmath416 with density @xmath417}f_{t}(s).$ ] now conditionally given all @xmath382 the @xmath357 are conditionally independent with , @xmath418 the unconditional distribution of each @xmath357 is @xmath419 the variable @xmath345 can be obtained and interpreted as follows:@xmath420 is a random variable that is equivalent in distribution to a poisson random variable with intensity @xmath421 conditioned on the event that the poisson variable is at least @xmath422 that is for @xmath423 @xmath424",
    "lastly it is easy to see that @xmath425=b\\int_{0}^{\\infty}s\\rho(s)ds/\\psi(b),$ ] which is possibly infinite .    for sampling @xmath167",
    "replace @xmath210 with @xmath426      we now look at the special cases of the gamma and generalized gamma processes considered earlier .",
    "if for @xmath427 @xmath235 is a @xmath331 stable random variable with density denoted as @xmath428 @xmath429 and @xmath430 then for any @xmath431 @xmath432 this is sometimes referred to as sibuya s distribution , and @xmath433 is a gamma@xmath434 random variable .",
    "furthermore @xmath435=\\infty.$ ] see pitman  @xcite for more details related to this case .",
    "if @xmath235 is a generalized gamma random variable with density @xmath436}f_{\\alpha}(t),$ ] @xmath437 and @xmath438 $ ] then @xmath439 and @xmath433 is gamma @xmath440 @xmath435=b\\alpha\\zeta^{\\alpha-1}/\\psi_{\\alpha,\\zeta}(b).$ ]    in reference to sampling @xmath167 note that @xmath441 and one simply replaces @xmath442 in the above quantities with @xmath443 in the stable case , customer @xmath291 selects a @xmath444)$ ] number of new dishes and draws the corresponding @xmath345 from @xmath445}\\frac{\\alpha \\gamma(j-\\alpha)}{j!\\gamma(1-\\alpha)}.\\ ] ]    suppose that @xmath235 is now a gamma random variable with shape parameter @xmath236 and scale @xmath422 then @xmath446 @xmath447 and @xmath448 and @xmath433 is gamma @xmath449 it is curious that the random variables do not depend on @xmath450 @xmath435=b/[\\log(1+b)].$ ] note in this case the @xmath345 are iid with a discrete logarithmic distribution with parameter @xmath451 and it follows that @xmath452 and @xmath453=\\theta b.$ ] note again that the gamma case is the case considered by titsias .    in order to sample @xmath167 in the gamma case , note that @xmath454 hence @xmath455 and @xmath456 which is a discrete logarithmic distribution with parameter @xmath457 hence it follows that @xmath458 has an @xmath459 distribution .",
    "we now look at the negative binomial setting . for this class of models",
    "the relevant @xmath461 measure is a proper distribution determined by the joint measure , @xmath462 the probability that a negative binomial random variable is greater than @xmath19 is @xmath463 $ ] and we see that the relevant total mass is @xmath464\\rho(p)dp = r\\int_{0}^{1}p{(1-p)}^{r-1}\\rho(p)dp.\\ ] ]    we now say @xmath408 follows a @xmath460 process if @xmath465 has distribution , for @xmath466 @xmath467}\\ ] ] and @xmath357 has marginal density @xmath468\\rho(p)}{\\phi(r)}.\\ ] ] notice that the distribution of @xmath465 corresponds to the distribution of a negative binomial random variable conditioned so it takes values greater than @xmath469 notice also that here , as in the poisson case ( and indeed any @xmath1 ) , this distribution does not depend on @xmath85 obviously the marginal distribution of @xmath345 is @xmath470 and @xmath433 is @xmath471 one can check that @xmath472=r\\int_{0}^{1}p{(1-p)}^{-1}\\rho(p)dp/\\phi(r),$ ] which is certainly not always finite .    for sampling @xmath167",
    "replace @xmath210 with @xmath473      as was mentioned much of the work done so far with the negative binomial case has involved the use of the beta process .",
    "it is quite straightforward to obtain explicit expressions based on our results so such calculations are in fact omitted .",
    "however we have noticed a surprising fact in this case which we find is worthwhile to mention .",
    "consider a simple beta process with intensity @xmath474 which is well defined for any @xmath475 then applying ( [ nbdist ] ) it is easy to see that the marginal distribution of @xmath345 would correspond to the result given in  @xcite .",
    "however what seems not to have been pointed out is the following .",
    "note that if @xmath476 then @xmath477=\\varphi e[x_{1}]=r\\int_{0}^{1}p{(1-p)}^{-1}\\rho(p)dp.\\ ] ] if we use @xmath478 then for @xmath479 @xmath477=\\varphi e[x_{1}]=\\theta r\\int_{0}^{1}{(1-p)}^{\\beta-2}dp.\\ ] ] so it follows that if @xmath480 then @xmath481=\\infty.$ ] from section 4 note this still means that a customer selects a poisson @xmath482 @xmath483 number of dishes , but in these cases gives very high scores to each dish , which is reflected by @xmath484=\\infty.$ ]",
    "the examples of poisson , bernoulli and negative binomial processes we presented in the previous section have all appeared in the recent literature , albeit with respect to more confined choices for @xmath485 there is certainly interest in possible multivariate extensions of such processes .",
    "the purpose of this section is to demonstrate that from a technical viewpoint the @xmath486 is well - equipped to easily handle this .",
    "this importantly would allow one to focus on modeling issues and interpretations that arise in a more intricate multidimensional setting rather than be encumbered by the calculus of random measures .",
    "we present results for a simple multinomial setting then a very general multivariate setting .",
    "the results although stated in a brief form are obtained in a formal manner as the ppc makes such arguments rather transparent .",
    "the joint distributions are very similar to the univariate cases .",
    "we discuss the case of multivariate crm s which have suitable constraints to handle for instance multinomial extensions of the ibp , and provide an example of an ibp - like model .",
    "lets first say a few words about multivariate levy processes with positive jumps . for @xmath487",
    "let @xmath488 denote a sigma - finite function concentrated on @xmath489 then following barndorff - nielsen , pedersen and sato  ( * ? ? ?",
    "* proposition 3.1 ) there exists a multivariate crm @xmath490 with jumps specified by @xmath491 if @xmath492 where @xmath493 in the next section we shall focus on models with the following constraints @xmath494    however in the final section we will not specify this .",
    "naturally we shall assume a common base measure @xmath495 representing again features .",
    "so it follows that one can write @xmath496 furthermore @xmath497 for @xmath498 the multivariate crm @xmath499 is constructed from @xmath63 a poisson random measure on @xmath500 with intensity @xmath501 and can be represented as @xmath502 where @xmath503    for example if @xmath504 then it follows making the transformation @xmath505 yields the marginal levy density for @xmath506 to be @xmath507 see@xcite and section  [ bd ] below for a more general class of models .",
    "now conditional on @xmath508 for each fixed @xmath509 and @xmath15 let @xmath510 denote an independent multinomial @xmath511 vector .",
    "that is for each fixed @xmath17 the joint probability mass function is given by , @xmath512(1-p_{\\cdot , k})^{1-b^{(i)}_{\\cdot , k}}\\ ] ] where at most one of the terms in @xmath510 is one and the others are @xmath469 then we can define a vector valued process @xmath513 where @xmath514 and @xmath515 is an ibp bernoulli process .",
    "we say that @xmath516 are iid @xmath517 and marginally @xmath518 is @xmath519 now based on @xmath398 conditionally iid processes the likelihood can be written as @xmath520(1-p_{\\cdot , k})^{1-b^{(i)}_{\\cdot , k}}= \\left[\\prod_{l=1}^{k}\\prod_{j=1}^{q}{\\left(\\frac{p_{j,\\ell}}{1-p_{\\cdot,\\ell}}\\right)}^{c_{j,\\ell , m}}\\right]{\\mbox e}^{-m\\sum_{j=1}^{\\infty}[-\\log(1-p_{\\cdot , j})]}\\ ] ] where @xmath521 and @xmath522 now let @xmath63 denote a poisson random measure on @xmath500 with intensity @xmath523 now define @xmath524 for @xmath525 despite the added complexity the likelihood is similar in form to the simple bernoulli process case and one can immediately conclude that a joint distribution of @xmath526 is given by , @xmath527\\mathbb{p}(z^{(1)}_{0},\\ldots , z^{(m)}_{0})\\ ] ] where now @xmath528 and hence @xmath529 and the joint distribution of @xmath530 is given by , @xmath531}^{c _ { j,\\ell , m}}\\right]{(1-s_{\\cdot,\\ell})}^{m}\\rho_{q}(d\\mathbf{s}_{q,\\ell}|\\omega_{\\ell})b_{0}(d\\omega_{\\ell}),\\ ] ] where @xmath532^{m})\\rho_{q}(d\\mathbf{s}_{q}|\\omega)b_{0}(d\\omega).$ ] again applying bayes rule , this is enough to conduct a posterior analysis in parallel to the univariate case . set @xmath533{(1-p_{\\cdot})}^{m - c_{\\ell , m}}\\rho_{q}(d\\mathbf{p}_{q}|\\omega_{\\ell})\\ ] ]",
    "where @xmath534 @xmath535    [ multiprop]suppose that @xmath516 are iid @xmath517 as described above in section  [ multi ] .",
    "then ,    1 .   the posterior distribution of @xmath536 is equivalent in distribution to the random measure @xmath537 where @xmath160 is a @xmath538 with mean intensity described in ( [ multinu ] ) and conditionally independent of @xmath539 the @xmath540 are an independent collection of random vectors such that @xmath541 has joint density , @xmath542}^{c _ {",
    "j,\\ell , m}}\\right]{(1-s_{\\cdot,\\ell})}^{m - c_{\\ell , m}}\\rho_{q}(d\\mathbf{s}_{q,\\ell}|\\omega_{\\ell})}{\\kappa_{\\mathbf{c}_{\\ell , m}}(\\rho_{q , m}|\\omega_{\\ell})},\\ ] ] where @xmath534 @xmath535 2 .",
    "it follows that @xmath543 can be represented in terms of @xmath544 which is determined by @xmath160 call it an @xmath545 vector , and @xmath546 vectors @xmath547 where for each fixed @xmath548 @xmath549 has distribution @xmath550 specifically , component - wise @xmath551 can be represented in distribution as @xmath552 where @xmath553 is the j - th component of @xmath554 3 .",
    "marginally for each @xmath548 @xmath555 is @xmath556 where @xmath557 defined for @xmath558 as @xmath559= \\frac{\\int_{\\mathbf{s}_q}s_{k}\\left[\\prod_{j=1}^{q}{[s_{j}]}^{c _ { j,\\ell , m}}\\right]{(1-s_{\\cdot})}^{m - c_{\\ell , m}}\\rho_{q}(d\\mathbf{s}_{q}|\\omega_{\\ell})}{\\kappa_{\\mathbf{c}_{\\ell , m}}(\\rho_{q , m}|\\omega_{\\ell})}.\\ ] ] 4 .",
    "hence @xmath560 is @xmath561 given @xmath562 @xmath555 becomes a simple mutlinomial distribution with joint probability mass function @xmath563 where @xmath564    we will discuss more details for the generative scheme following the next section which describes a general multivariate setting .",
    "let now @xmath565 for @xmath566 a positive integer not necessarily equal to @xmath567 denote a random vector taking values in @xmath568 with conditional distribution @xmath569 where @xmath570 and such that @xmath571 then we can define a vector valued process @xmath572 where @xmath573 and @xmath574 where conditional on @xmath575 for each fixed @xmath576 @xmath577 is independent @xmath578 where @xmath579 are vector valued points of a prm with intensity @xmath580 we say that @xmath516 are iid @xmath581 it follows that , denoting the argument for @xmath582 as @xmath583 the likelihood is given by ,    @xmath584}\\prod_{i=1}^{m}\\prod_{j=1}^{\\infty}{\\left[\\frac{g_{a_{0}}(d\\mathbf{a}^{(i)}_{0,j}|\\mathbf{s}_{q , j } ) } { 1-\\pi_{a_{0}}(\\mathbf{s}_{q , j})}\\right]}^{{\\mathbb{i}}_{\\{\\mathbf{a}^{(i)}_{0,j}\\notin { \\mathbf{0}}\\}}}\\ ] ] despite the extension to the multivariate case , it is evident that the form above looks quite similar to the univariate case and one may conclude that the relevant joint distribution of @xmath585 is given by @xmath586}^{m } \\rho_{q}(d\\mathbf{s}_{q,\\ell}|\\omega)b_{0}(d\\omega_{\\ell } ) \\prod_{i=1}^{m}h_{i,\\ell}(\\mathbf{s}_{q,\\ell})\\ ] ] where now @xmath587 $ ] and hence @xmath588}^{m } \\rho_{q}(d\\mathbf{s}_{q}|\\omega)b_{0}(d\\omega):=\\rho_{q , m}(d\\mathbf{s}_{q}|\\omega)b_{0}(d\\omega),\\ ] ] and similar to the univariate case , @xmath589}^{{\\mathbb{i}}_{\\{\\mathbf{a}^{(i)}_{0,\\ell}\\notin { \\mathbf{0}}\\}}},\\ ] ] and @xmath590}^{m})\\rho_{q}(d\\mathbf{s}_{q}|\\omega)b_{0}(d\\omega ) . \\label{psifunctionm2}\\ ] ]    suppose that @xmath516 are iid @xmath591 where @xmath499 is a multivariate @xmath592 for each @xmath593 and @xmath398 set @xmath594}^{m}\\rho_{q}(\\mathbf{s}_{q}|\\omega).\\ ] ]    1 .",
    "then it follows that the posterior distribution of @xmath595 is equivalent to the distribution of @xmath596 where @xmath160 is a @xmath538 and the vector @xmath597 has joint density , with argument @xmath598 given proportional to @xmath599}^{m } \\rho_{q}(d\\mathbf{s}_{q,\\ell}|\\omega ) \\prod_{i=1}^{m}h_{i,\\ell}(\\mathbf{s}_{q,\\ell}).\\ ] ] 2 .",
    "let @xmath600 denote a multivariate @xmath601 then the posterior distribution of @xmath602 is equivalent to that of a multivariate process whose @xmath23-th component is equivalent in distribution to @xmath603 3 .   the corresponding distribution of @xmath604 can be represented in terms of @xmath605 which is determined by @xmath160 call it an @xmath606 vector , and @xmath546 vectors @xmath607 where for each fixed @xmath548 @xmath608 has distribution @xmath609 in other words component - wise @xmath610 can be represented in distribution as @xmath611 where @xmath612 is the j - th component of @xmath613    it remains to sample @xmath614 we close with a generalization of proposition 2.2 .",
    "[ multisample]let @xmath615 have a @xmath616 distribution , where it is assumed that @xmath491 is homogeneous .",
    "if @xmath617 then for @xmath618 @xmath619 where @xmath620 are iid across @xmath621 and independent of @xmath343 a poisson random variable with mean @xmath622 furthermore setting @xmath623 there are the iid pairs @xmath624 with distributions @xmath625}\\ ] ] and @xmath626    it follows that the distribution of @xmath627 is characterized by the laplace or characteristic functional of @xmath628 for general @xmath629 appealing again to the theory of poisson marked processes one sees the lvy exponent is given by @xmath630 the condition @xmath631 reduces the result to manipulation of the joint measure @xmath632    naturally for each @xmath593 and @xmath398 one uses proposition  [ multisample ] with @xmath633}^{m}\\rho_{q}(\\mathbf{s}_{q})\\ ] ] in order to sample @xmath634      the simplest multivariate model is of course the multinomial case that was outlined in section  [ multi ] .",
    "each non - zero entry in the matrix contains a vector of length @xmath635 where one entry takes the value @xmath16 and the other entries in the vector take the value @xmath469 this could be described in terms of a customer selecting a certain dish but along with that choosing one particular condiment to go along with that dish .",
    "perhaps a mango chutney or simply salt .",
    "this means that 2 or more individuals may have selected the same dish ( have the same basic trait ) but might differ in terms of the accompanying condiment .",
    "since @xmath515 is a bernoulli process it is evident that customers choose basic dishes according to the indian buffet process arising in the univariate case .",
    "in addition , given each new dish chosen , customer @xmath636 selects with it the @xmath23-th of @xmath635 possible condiments with probability @xmath637 customer @xmath291 also will possibly choose a previously selected dish @xmath638 with probability @xmath639 and given this will choose one of q condiments , say j , ( possibly different than what has previously been chosen by others ) with probability @xmath640 that is to say according to a multinomial@xmath641 distribution described in proposition  [ multiprop ] .",
    "the customer will do this for each of the @xmath642 previously selected dishes .",
    "one of the tasks in the mulivariate case is to find convenient priors for @xmath643 in the simple multinomial case we now show how the class of beta - dirichlet priors introduced in kim , james and weissbach @xcite leads to explicit results .",
    "we introduce a slight modification of this model which allows for power - law behavior in the sense of teh and grur@xcite , specify @xmath499 to be a stable - beta - dirichlet process with parameters @xmath644 by setting @xmath645 for @xmath646 @xmath295 , @xmath647 and @xmath648 for @xmath525 when @xmath649 this is the beta - dirichlet process given in  @xcite . making the change of variable @xmath650 it is easy to check that @xmath651 is a stable - beta process in the sense of @xcite with lvy density @xmath652 it follows that for each @xmath548 @xmath653 is such that @xmath654 has a @xmath655 distribution just as the univariate case .",
    "furthermore @xmath656 where @xmath657 is independent of @xmath658 and is a @xmath659 vector .",
    "note that @xmath660 is a stable - beta - dirichlet process with parameters @xmath661    hence customer @xmath291 chooses an existing dish @xmath292 and accompanying condiment @xmath23 with probability @xmath662 for @xmath551 it follows that for @xmath663 @xmath664 where @xmath665 and for each @xmath621 @xmath666 is a simple multinomial with probability mass function @xmath667 thus customer @xmath291 chooses a poisson @xmath344 number of new dishes exactly as in the univariate case and also for each new dish chosen selects a single condiment @xmath23 with probability @xmath668 for @xmath669    note that for each @xmath670 @xmath671 is marginally a beta - dirichlet random vector with parameters @xmath672 this means that @xmath673 is a @xmath674 random variable and the vector @xmath675 for @xmath676 is @xmath677 which leads to ( [ dirm ] ) .",
    "as discussed in  james  @xcite the ppc is a direct extension of a disintegration / fubini calculus employed for gamma and dirichlet processes by albert lo , which lo further credits as techniques developed from conversations with lucien le cam .",
    "the idea to extend this to a general poisson random measure setting was suggested to this author by jim pitman in 2001 .",
    "the ppc seeks out the fundamental prior - posterior disintegration of the bayesian joint distribution , and as demonstrated is designed to exploit the common features of random processes whose behavior is governed by a poisson random measure . in other words",
    "it is a palm calculus tailor - made for data structures arising in non - parametric bayesian settings .",
    "this naturally includes any process based on completely random measures .",
    "this provides a unified approach for posterior analysis and also allows one to pinpoint differences between various processes .",
    "more general descriptions of posterior analysis are given in @xcite .",
    "within our particular context , this analysis paves the way for the treatment of these infinite dimensional processes in a similar fashion to the now well understood dirichlet process in complex applications .",
    "the remaining structure of the ibp and its generalizations mentioned here , as already evidenced by previous works , presents exciting opportunities for it seems an abundance of varied applications .",
    "we note the generalization to the multivariate setting accommodates practically any distribution .",
    "for example , it would be of interest to explore further the usage / interpretation of _ multivariate bernoulli _ distributions  @xcite within the ibp context .",
    "this class of distributions has been recently discussed in @xcite in relation to the the treatment of undirected graphical models with binary nodes as described in  @xcite . for illustration the bivariate bernoulli distribution",
    "is defined in terms of random variables @xmath678 taking values in the space @xmath679 with joint probability mass function , depending on probabilities @xmath680 with @xmath681 @xmath682 this model fits into the framework of section 5 , so it remains to obtain plausible / interesting interpretations and perhaps questions in regards to the choice of @xmath84 for practical implementation . for general ideas in regards to construction of conjugate models",
    "see the work of  @xcite and @xcite .",
    "aldous , d. and pitman , j. ( 2006 ) .",
    "two recursive decompositions of brownian bridge related to the asymptotics of random mappings . in in memoriam paul - andr meyer ( pp . 269 - 303 ) .",
    "springer berlin heidelberg .",
    "pitman , j. ( 2006 ) .",
    "_ combinatorial stochastic processes . _",
    "lectures from the 32nd summer school on probability theory held in saint - flour , july 724 , 2002 . with a foreword by jean picard .",
    "lecture notes in mathematics , 1875 .",
    "springer - verlag , berlin .",
    "williamson , s. , wang , c. , heller , k. , and blei , d. ( 2010 ) . the ibp compound dirichlet process and its application to focused topic modeling.in the 27th international conference on machine learning ( icml 2010 ) .",
    "wood , f , griffiths , t.l . and ghahramani , z ( 2006 ) a non - parametric bayesian method for inferring hidden causes . in : 22nd conference on uncertainty in artificial intelligence ( uai 2006 ) , 13 - 7 - 2006 to 16 - 7 - 2006 , cambridge , ma , us pp . 536 - 543 .."
  ],
  "abstract_text": [
    "<S> the purpose of this work is to describe a unified , and indeed simple , mechanism for non - parametric bayesian analysis , construction and generative sampling of a large class of latent feature models which one can describe as generalized notions of indian buffet processes  ( ibp ) . </S>",
    "<S> this is done via the poisson process calculus as it now relates to latent feature models . </S>",
    "<S> the ibp , first arising in a bayesian machine learning context , was ingeniously devised by griffiths and ghahramani in ( 2005 ) and its generative scheme is cast in terms of customers entering sequentially an indian buffet restaurant and selecting previously sampled dishes as well as new dishes . in this metaphor dishes corresponds to latent features , attributes , preferences shared by individuals . </S>",
    "<S> the ibp , and its generalizations , represent an exciting class of models well suited to handle high dimensional statistical problems now common in this information age . in a survey article griffiths and ghahramani note applications for choice models , modeling protein interactions , independent components analysis and sparse factor analysis , among others . </S>",
    "<S> the ibp is based on the usage of conditionally independent bernoulli random variables , coupled with completely random measures acting as bayesian priors , that are used to create sparse binary matrices . </S>",
    "<S> this bayesian non - parametric view was a key insight due to thibaux and jordan ( 2007 ) . </S>",
    "<S> one way to think of generalizations is to to use more general random variables . of note in the current literature are models employing poisson and negative - binomial random variables . </S>",
    "<S> however , unlike their closely related counterparts , generalized chinese restaurant processes , the ability to analyze ibp models in a systematic and general manner is not yet available . </S>",
    "<S> the limitations are both in terms of knowledge about the effects of different priors and in terms of models based on a wider choice of random variables . </S>",
    "<S> this work will not only provide a thorough description of the properties of existing models but also provide a simple template to devise and analyze new models . </S>",
    "<S> we close by proposing and analyzing general classes of multivariate processes . </S>"
  ]
}