{
  "article_text": [
    "bayesian inferenceis a diverse and robust analysis methodology @xcite based on bayes theorem ,    @xmath0    the prior ,    @xmath1    encodes all knowledge about the parameters @xmath2 before the data @xmath3 have been collected , while the likelihood ,    @xmath4    defines the probabilistic model of how the data are generated .",
    "the evidence ,    @xmath5    ensures proper normalization while allowing for auxiliary applications such as model comparison .",
    "lastly , the posterior ,    @xmath6    is the refinement of the prior @xmath7 given the information inferred from @xmath3 .",
    "all model assumptions are captured by the conditioning hypothesis @xmath8 .",
    "while bayes theorem is simple enough to formulate , in practice the individual components are often sufficiently complex that analytic manipulation is not feasible and one must resort to approximation .",
    "one of the more successful approximation techniques , markov chain monte carlo ( mcmc ) produces samples directly from the posterior distribution that are often sufficient to characterize even high dimensional distributions .",
    "the one manifest limitation of mcmc , however , is the inability to directly calculate the evidence @xmath9 , which , as mackay notes , `` is often the single most important number in the problem '' @xcite .",
    "nested sampling @xcite is an alternative to sampling from the posterior that instead emphasizes the calculation of the evidence .     and",
    "@xmath10 in two dimensions .",
    "@xmath10 parameterizes the contour of constant likelihood @xmath11 while @xmath12 parameterizes translations orthogonal to the contour .",
    "[ fig : alpharepar],width=240 ]",
    "consider the support of the likelihood above a given bound @xmath11 ( fig [ fig : nestedsampling]a , [ fig : nestedsampling]b ) ,    @xmath13    and the associated prior mass across that support ( fig [ fig : nestedsampling]c ) ,    @xmath14    the differential @xmath15 gives the prior mass associated with the likelihood @xmath16 ( fig [ fig : nestedsampling]d ) ,    @xmath17    where @xmath18 is the @xmath19 dimensional boundary of constant likelihood ,    @xmath20    introducing the coordinate @xmath12 perpendicular to the likelihood constraint boundary and the @xmath19 coordinates @xmath10 parallel to the constraint ( fig [ fig : alpharepar ] ) , the integral over @xmath18 simply marginalizes @xmath10 and the differential becomes    @xmath21    returning to the evidence ,    @xmath22    by construction the likelihood is invariant to changes in @xmath10 , @xmath23 , and the integral simplifies to    @xmath24    where @xmath25 is the likelihood bound resulting in the prior mass @xmath26 .",
    "this clever change of variables has reduced the @xmath27 dimensional integration over the parameters @xmath2 to a one dimensional integral over the bounded support of @xmath26 .",
    "although this simplified integral is easier to calculate in theory , it is fundamentally limited by the need to compute @xmath28 .    numerical integration",
    ", however , needs only a set of points @xmath29 and not @xmath30 explicitly . sidestepping @xmath30 ,",
    "consider instead the problem of generating the set @xmath29 directly .",
    "in particular , consider a stochastic approach beginning with @xmath31 samples drawn from @xmath7 .",
    "the sample with the smallest likelihood , @xmath32 , bounds the largest @xmath26 but otherwise nothing can be said of the exact value , @xmath33 , without an explicit , and painful , calculation from the original definition .",
    "the cumulative probability of @xmath33 , however , is simply the probability of @xmath33 exceeding the @xmath26 of each sample ,    @xmath34    where @xmath35 is uniformly distributed ,    @xmath36    simplifying , the cumulative probability of the largest sample reduces to    @xmath37    with the corresponding probability distribution    @xmath38    estimating @xmath33 from the probability distribution @xmath39 immediately yields a pair    @xmath40    a second pair follows by drawing from the constrained prior    @xmath41    or in terms of @xmath26 ,    @xmath42    @xmath31 samples from this constrained prior yield a new minimum @xmath43 with @xmath44 distributed as    @xmath45    making another point estimate gives @xmath46 .",
    "generalizing , the @xmath31 samples at each iteration are drawn from a uniform prior restricted by the previous iteration ,    @xmath47    the distribution of the largest sample , @xmath48 , follows as before ,    @xmath49    note that this implies that the shrinkage at each iteration , @xmath50 , is identically and independently distributed as    @xmath51    moreover , a point estimate for @xmath48 can be written entirely in terms of point estimates for the @xmath52 ,    @xmath53    more appropriate to the large dynamic ranges encountered in many applications , @xmath54 becomes    @xmath55    performing a quick change of variables , the logarithmic shrinkage will be distributed as    @xmath56    with the mean and standard deviation    @xmath57    taking the mean as the point estimate for each @xmath58 finally gives    @xmath59    with the resulting error    @xmath60    parameterizing @xmath48 in terms of the shrinkage proves immediately advantageous  because the @xmath58 are independent , the errors in the point estimates tend to cancel and the estimate for the @xmath48 grow increasingly more accurate with @xmath61 .    at each iteration , then , a pair @xmath29 is given by the point estimate for @xmath48 and the smallest likelihood of the @xmath31 drawn samples .",
    "a proper implementation of nested sampling begins with the initial point @xmath62 . at each iteration ,",
    "@xmath31 samples are drawn from the constrained prior    @xmath63    and the sample with the smallest likelihood provides a `` nested '' sample with @xmath64 and @xmath65 ( figure [ fig : nestedsamples ] ) .",
    "@xmath66 defines a new constrained prior for the following iteration .",
    "note that the remaining samples from the given iteration will already satisfy this new likelihood constraint and qualify as @xmath67 of the samples necessary for the next iteration ",
    "only one new sample will actually need to be generated .",
    "as the algorithm iterates , regions of higher likelihood are reached until the nested samples begin to converge to the maximum likelihood . determining this convergence is tricky , but heuristics have been developed that are quite successful for well behaved likelihoods @xcite .    once the iterations have terminated , the evidence is numerically integrated using the nested samples .",
    "the simplest approach is a first order numerical quadrature :    @xmath68    errors from the numerical integration are dominated by the errors from the use of point estimates and , consequently , higher order quadrature offers little improvement beyond the first order approximation .",
    "the errors inherent in the point estimates can be reduced by instead marginalizing over the shrinkage distributions .",
    "note , however , that in many applications the likelihood will be relatively peaked and most of the prior mass will lie within its tails .",
    "@xmath69 will then be heavily weighted towards exponentially small values of @xmath11 where the likelihood constraint falls below the tails and the prior mass rapidly accumulates .",
    "likewise , the integrand @xmath28 will be heavily weighted towards exponentially small values of @xmath26 and the dominant contributions from the quadrature will come from later iterations , exactly where the point estimates become more precise .",
    "the resulting error in the integration tends to be reasonable , and the added complexity of marginalization offers little improvement",
    ".    the choice of @xmath31 can also be helpful in improving the accuracy of the integration . for larger @xmath31 the shrinkage distribution narrows and the estimates for the @xmath48",
    "become increasingly better .",
    "multiple samples at each iteration also prove valuable when the likelihood is multimodal , as the individual samples allow the modes to be sampled simultaneously @xcite .",
    "lastly , if the @xmath2 yielding the smallest likelihood are stored with each nested sample then posterior expectations can be estimated with the quadrature weights ,    @xmath70    the remaining obstacle to a fully realized algorithm is the matter of sampling from the prior given the likelihood constraint @xmath71 .",
    "sampling from constrained distributions is a notoriously difficult problem , and recent applications of nested sampling have focused on modifying the algorithm in order to make the constrained sampling feasible @xcite .",
    "hamiltonian monte carlo , however , offers samples directly from the constrained prior and provides an immediate implementation of nested sampling .",
    "hamiltonian monte carlo @xcite is an efficient method for generating samples from the @xmath27 dimensional probability distribution                              to a given sample @xmath81 produces a new sample @xmath82 .",
    "note that the properties of hamiltonian dynamics , in particular liouville s theorem and conservation of @xmath8 , guarantee that differential probability masses from @xmath83 are conserved by the mapping . as a result",
    ", this dynamic evolution serves as a transition matrix @xmath84 with the invariant distribution @xmath83 .",
    "moreover , the time reversal symmetry of the equations ensures that the evolution satisfies detailed balance :      because @xmath8 is conserved , however , the transitions are not ergodic and the samples do not span the full support of @xmath83 .",
    "ergodicity is introduced by adding a gibbs sampling step for the @xmath74 . because the @xmath76 and @xmath74 are independent , sampling from the conditional distribution for @xmath74 is particularly easy        in practice the necessary integration of hamilton s equations",
    "can not be performed analytically and one must resort to numerical approximations .",
    "unfortunately , any discrete approximation will lack the symmetry necessary for both liouville s theorem and energy conservation to hold , and the exact invariant distribution will no longer be @xmath83 .",
    "this can be overcome by treating the evolved sample as a metropolis proposal , accepting proposed samples with probability                sampling from @xmath90 is challenging .",
    "the simplest approach is to sample from @xmath91 and discard those not satisfying the constraint . for most nontrivial constraints , however ,",
    "this approach is extremely inefficient as the majority of the computational effort is spent generating samples that will be immediately discarded .",
    "incorporating infinite barriers directly into hamilton s equations is problematic , but physical intuition provides an alternative approach .",
    "particles incident on an infinite barrier bounce , the momenta perpendicular to the barrier perfectly reflecting :        discrete updates proceed as follows .",
    "after each spatial update the constraint is checked and if violated then the normal @xmath94 is computed at the new point and the ensuing momentum update is replaced by reflection ( algo [ algo : bounce ] , fig [ fig : bounce ] ) .",
    "note that the spatial update can not be reversed , nor can an interpolation to the constraint boundary be made , without spoiling the time - reversal symmetry of the evolution .",
    "the normal for many discontinuous constraints , which are particularly useful for sampling distributions with limited support without resorting to computationally expensive exponential reparameterizations , can be determined by the geometry of the problem .        given a seed satisfying the constraint , the resultant markov chain bounces around @xmath90 and avoids the inadmissible regions almost entirely .",
    "computational resources are spent on the generation of relevant samples and the sampling proceeds efficiently no matter the scale of the constraint .",
    "the chmc samples are then exactly the samples from the constrained prior necessary for the generation of the nested samples .",
    "a careful extension of the constraint also allows for the addition of a limited support constraint , making efficient nested sampling with , for example , gamma and beta priors immediately realizable .",
    "initially , the @xmath31 independent samples are generated from @xmath31 markov chains seeded at random across the full support of @xmath7 .",
    "after each iteration of the algorithm , the markov chain generating the nested sample is discarded and a new chain is seeded with one of the remaining chains . note that this new seed is guaranteed to satisfy the likelihood constraint and the resultant chmc will have no problems bouncing around the constrained distribution to produce the new sample needed for the following iteration ."
  ],
  "abstract_text": [
    "<S> nested sampling is a powerful approach to bayesian inference ultimately limited by the computationally demanding task of sampling from a heavily constrained probability distribution . </S>",
    "<S> an effective algorithm in its own right , hamiltonian monte carlo is readily adapted to efficiently sample from any smooth , constrained distribution . utilizing this constrained hamiltonian monte carlo , </S>",
    "<S> i introduce a general implementation of the nested sampling algorithm .    </S>",
    "<S> nested sampling with constrained hamiltonian monte carlo    michael betancourt    _ massachusetts institute of technology , cambridge , ma 02139 _ </S>"
  ]
}