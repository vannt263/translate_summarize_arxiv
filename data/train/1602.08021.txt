{
  "article_text": [
    "a large array of optimization problems arising in signal processing involve functions belonging to @xmath0 , the class of proper lower semicontinuous convex function from @xmath1 to @xmath2-\\infty,+\\infty\\right]}}$ ] , where @xmath1 is a finite - dimensional real hilbert space with norm @xmath3 .",
    "in particular , the following formulation has proven quite flexible and far reaching @xcite .",
    "[ prob:1 ] let @xmath4 , let @xmath50,+\\infty\\right[}}$ ] , and let @xmath6 be a differentiable convex function such that @xmath7 is @xmath8-lipschitz continuous on @xmath1 .",
    "the goal is to @xmath9 under the assumption that the set @xmath10 of minimizers of @xmath11 is nonempty .",
    "a standard method to solve problem  [ prob:1 ] is the forward - backward algorithm @xcite , which constructs a sequence @xmath12 in @xmath1 via the recursion @xmath13 where @xmath140,2\\vartheta\\right[$ ] and @xmath15 is the proximity operator of function @xmath16 , i.e. , @xcite @xmath17 in practice , it may happen that , at each iteration @xmath18 , @xmath19 is not known exactly and is available only through some stochastic approximation @xmath20 , while only a deterministic approximation @xmath21 to @xmath22 is known ; see , e.g. , @xcite . to solve in such uncertain environments , we propose to investigate the following stochastic version of . in this algorithm , at iteration @xmath18 , @xmath23 stands for a stochastic error term modeling inexact implementations of the proximity operator of @xmath24 , @xmath25 is the underlying probability space , and @xmath26 denotes the space of @xmath1-valued random variable @xmath27 such that @xmath28 . our algorithmic model is the following .",
    "[ algo:1 ] let @xmath29 , @xmath30 , and @xmath31 be random variables in @xmath26 , let @xmath32 be a sequence in @xmath20,1\\right]}}$ ] , and let @xmath33 be a sequence in @xmath340,2\\vartheta\\right[$ ] , and let @xmath35 be a sequence of functions in @xmath0",
    ". for every @xmath36 , set @xmath37    the first instances of the stochastic iteration can be traced back to @xcite in the context of the gradient descent method , i.e. , when @xmath38 .",
    "stochastic approximations in the gradient method were then investigated in the russian literature of the late 1960s and early 1970s @xcite .",
    "stochastic gradient methods have also been used extensively in adaptive signal processing , in control , and in machine learning , ( e.g. , in @xcite ) .",
    "more generally , proximal stochastic gradient methods have been applied to various problems ; see for instance @xcite .",
    "the first objective of the present work is to provide a thorough convergence analysis of the stochastic forward - backward algorithm described in algorithm  [ algo:1 ] . in particular",
    ", our results do not require that the proximal parameter sequence @xmath33 be vanishing .",
    "a second goal of our paper is to show that the extension of algorithm  [ algo:1 ] for solving monotone inclusion problems allows us to derive a stochastic version of a recent primal - dual algorithm @xcite ( see also @xcite ) .",
    "note that our algorithm is different from the random block - coordinate approaches developed in @xcite , and that it is more in the spirit of the adaptive method of @xcite .",
    "the organization of the paper is as follows .",
    "section  [ sec : fb ] contains our main result on the convergence of the iterates of algorithm  [ algo:1 ] .",
    "section  [ sec : pd ] presents a stochastic primal - dual approach for solving composite convex optimization problems .",
    "section  [ se : appli ] illustrates the benefits of this algorithm in signal restoration problems with stochastic degradation operators . concluding remarks appear in section  [ se : conclu ] .",
    "throughout , given a sequence @xmath39 of @xmath1-valued random variables , the smallest @xmath40-algebra generated by @xmath41 is denoted by @xmath42 , and we denote by @xmath43 a sequence of sigma - algebras such that @xmath44 furthermore , @xmath45 designates the set of sequences of @xmath46-valued random variables @xmath47 such that , for every @xmath36 , @xmath48 is @xmath49-measurable , and we define @xmath50 and @xmath51 we now state our main convergence result .    [ t:2 ] consider the setting of problem  [ prob:1 ] , let @xmath52 be a sequence in @xmath46 , let @xmath39 be a sequence generated by algorithm  [ algo:1 ] , and let @xmath43 be a sequence of sub - sigma - algebras satisfying .",
    "suppose that the following are satisfied :    1 .",
    "[ a : t2i ] @xmath53 .",
    "[ a : t2ii ] @xmath54 .",
    "[ a : t2iii ] for every @xmath55 , there exists @xmath56 such that @xmath57 and @xmath58 4 .",
    "[ p:23vi ] there exist sequences @xmath59 and @xmath60 in @xmath46 such that @xmath61 , @xmath62 @xmath63 , and @xmath64 5 .",
    "[ a : t2iv ] @xmath65 , @xmath66 , and@xmath67",
    "[ a : t2v ] either @xmath68 or @xmath69 , @xmath70 , and @xmath71 $ ] .",
    "then the following hold for every @xmath72 and for some @xmath10-valued random variable @xmath27 :    1 .",
    "[ t:2i ] @xmath73 2 .",
    "[ t:2ii ] @xmath74 3 .",
    "[ t:2iii ] @xmath39 converges almost surely to @xmath27 .    in the deterministic case",
    ", theorem  [ t:2][t:2iii ] can be found in ( * ? ? ?",
    "* corollary  6.5 ) .",
    "the proof the above stochastic version is based on the theoretical tools of @xcite ( see @xcite for technical details and extensions to infinite - dimensional hilbert spaces ) .    it should be noted that the existing works which are the most closely related to ours do not allow any approximation of the function @xmath22 and make some additional restrictive assumptions .",
    "for example , in ( * ? ? ?",
    "* corollary  8) and @xcite , @xmath33 is a decreasing sequence . in ( * ? ? ?",
    "* corollary  8) , @xcite , and @xcite , no error term is allowed in the numerical evaluations of the proximity operators ( @xmath75 ) . in addition , in the former work , it is assumed that @xmath39 is bounded , whereas the two latter ones assume that the approximation of the gradient of @xmath76 is unbiased , that is @xmath77",
    "the subdifferential @xmath78 of a function @xmath4 is an example of a maximally monotone operator @xcite .",
    "forward - backward splitting has been developed in the more general framework of solving monotone inclusions @xcite .",
    "this powerful framework makes it possible to design efficient primal - dual strategies for optimization problems ; see for instance @xcite and the references therein .",
    "more precisely , we are interested in the following optimization problem ( * ? ? ? * section  4 ) .    [ prob:3 ]",
    "let @xmath4 , let @xmath790,+\\infty\\right[}}$ ] , let @xmath80 be convex and differentiable with a @xmath81-lipschitz - continuous gradient , and let @xmath82 be a strictly positive integer . for every @xmath83 , let @xmath84 be a finite - dimensional hilbert space , let @xmath85 , and let @xmath86 be linear .",
    "let @xmath87 be the direct hilbert sum of @xmath88 , and suppose that there exists @xmath89 such that @xmath90 let @xmath10 be the set of solutions to the problem @xmath91 and let @xmath92 be the set of solutions to the dual problem @xmath93 where @xmath94 denotes the infimal convolution operation and @xmath95 designates a generic point in @xmath96 .",
    "the objective is to find a point in @xmath97 .",
    "we are interested in the case when only stochastic approximations of the gradients of @xmath98 and approximations of the function @xmath22 are available to solve problem  [ prob:3 ]",
    ". the following algorithm , which can be viewed as a stochastic extension of those of @xcite , will be the focus of our investigation .",
    "[ algo:7 ] let @xmath990,+\\infty\\right[}}$ ] , let @xmath35 be a sequence of functions in @xmath0 , let @xmath32 be a sequence in @xmath340,1\\right]$ ] such that @xmath100 , and , for every @xmath83 , let @xmath1010,+\\infty\\right[}}$ ] .",
    "let @xmath29 , @xmath30 , and @xmath102 be random variables in @xmath26 , and let @xmath103 and @xmath104 be random variables in @xmath105 .",
    "iterate @xmath106    one of main benefits of the proposed algorithm is that it allows us to solve jointly the primal problem and the dual one in a fully decomposed fashion , where each function and linear operator is activated individually . in particular",
    ", it does not require any inversion of some linear operator related to the operators @xmath107 arising in the original problem .",
    "the convergence of the algorithm is guaranteed by the following result which follows from ( * ? ? ?",
    "* proposition  5.3 ) .",
    "[ p:3 ] consider the setting of problem  [ prob:3 ] , let @xmath108 be a sequence of sub - sigma - algebras of @xmath109 , and let @xmath39 and @xmath110 be sequences generated by algorithm  [ algo:7 ] .",
    "suppose that the following are satisfied :    1 .",
    "[ a : p30 ] @xmath111 @xmath112 .",
    "[ a : p3i ] @xmath113 and@xmath114 .",
    "[ a : p3ii ] @xmath115 .",
    "[ a : p3iv ] there exists a summable sequence @xmath52 in @xmath46 such that , for every @xmath116 , there exists @xmath117 such that @xmath118 and @xmath119 5 .",
    "[ a : p3v ] there exist sequences @xmath120 and @xmath121 in @xmath46 such that @xmath122 , @xmath123 @xmath63 , and @xmath124 6 .",
    "[ a : p3vi ] @xmath125 .",
    "then , for some @xmath10-valued random variable @xmath27 and some @xmath92-valued random variable @xmath126 , @xmath39 converges almost surely to @xmath27 and @xmath127 converges almost surely to @xmath126 .",
    "we consider the recovery of a signal @xmath128 from the observation model @xmath129 where @xmath130 is a @xmath131-valued random matrix and @xmath132 is a @xmath133-valued random noise vector .",
    "the objective is to recover @xmath134 from @xmath135 , which is assumed to be an identically distributed sequence .",
    "such recovery problems have been addressed in @xcite . in this context",
    ", we propose to solve the primal problem with @xmath136 and @xmath137 while functions @xmath22 and @xmath138 are used to promote prior information on the target solution .",
    "since the statistics of the sequence @xmath135 are not assumed to be known a priori and have to be learnt online , at iteration @xmath139 , we employ the empirical estimate @xmath140 of @xmath141 . the following statement , which can be deduced from ( * ?",
    "* section  5.2 ) , illustrates the applicability of the results of section  [ sec : pd ] .",
    "consider the setting of problem  [ prob:3 ] and algorithm  [ algo:7 ] , where @xmath142 , @xmath143 , and @xmath144 .",
    "let @xmath145 be a strictly increasing sequence in @xmath146 such that @xmath147 with @xmath1480,+\\infty\\right[}}$ ] , and let @xmath149 suppose that the following are satisfied :    1 .",
    "the domain of @xmath22 is bounded .",
    "@xmath150 , is an independent and identically distributed ( i.i.d . )",
    "sequence such that @xmath151 and @xmath152 .",
    "@xmath153 , where @xmath1541-\\delta,1\\right]\\cap [ 0,1]$ ] .    then assumptions  [ a : p30]-[a : p3v ] in proposition  [ p:3 ] hold .    based on this result",
    ", we apply algorithm  [ algo:7 ] to a practical scenario in which a grayscale image of size @xmath155 with pixel values in @xmath156 $ ] is degraded by a stochastic blur .",
    "the stochastic operator corresponds to a uniform i.i.d .",
    "subsampling of a uniform @xmath157 blur , performed in the discrete fourier domain .",
    "more precisely , the value of the frequency response at each frequency bin is kept with probability @xmath158 or it is set to zero .",
    "in addition , the image is corrupted by an additive zero - mean white gaussian noise with standard deviation equal to @xmath159 .",
    "the average signal - to - noise ratio ( snr ) is initially equal to @xmath160  db .",
    "( top ) , restored image ( bottom).,title=\"fig:\",width=264 ] +   ( top ) , restored image ( bottom).,title=\"fig:\",width=264 ] +    in our restoration approach , the function @xmath22 is the indicator function of the set @xmath156^n$ ] , while @xmath161 is a classical isotropic total variation regularizer , where @xmath162 is the concatenation of the horizontal and vertical discrete gradient operators . figs .",
    "[ fig:1][fig:1bis ] displays the original image , the restored image , as well as two realizations of the degraded images .",
    "the snr for the restored image is equal to @xmath163  db .",
    "db ) ( top ) , and degraded image 2 ( snr = @xmath164  db ) ( bottom).,title=\"fig:\",width=264 ] +  db ) ( top ) , and degraded image 2 ( snr = @xmath164  db ) ( bottom).,title=\"fig:\",width=264 ] +    fig .",
    "[ fig:2 ] shows the convergence behavior of the algorithm . in these experiments ,",
    "we have chosen @xmath165     versus the iteration number @xmath18.,width=302 ]",
    "we have proposed two stochastic proximal splitting algorithms for solving nonsmooth convex optimization problems .",
    "these methods require only approximations of the functions used in the formulation of the optimization problem , which is of the utmost importance for solving online signal processing problems .",
    "the almost sure convergence of these algorithms has been established .",
    "the stochastic version of the primal - dual algorithm that we have investigated has been evaluated in an online image restoration problem in which the data are blurred by a stochastic point spread function and corrupted with noise .",
    "f. bach and e. moulines ,  non - asymptotic analysis of stochastic approximation algorithms for machine learning \" , in _ proc",
    "_ , granada , spain , dec .",
    "12 - 17 , 2011 , pp . 451459 .",
    "l combettes , l. condat , j .- c .",
    "pesquet , and b. c. v .",
    " a forward - backward view of some primal - dual optimization methods in image recovery , \" _ proc .",
    "conf . image process .",
    "_ , paris , france , 27 - 30 oct .",
    "2014 , pp . 41414145 .",
    "p. l. combettes and j .- c .",
    "pesquet , ",
    "proximal splitting methods in signal processing , \" in _ fixed - point algorithms for inverse problems in science and engineering , _",
    "( h. h. bauschke et al . ,",
    "eds ) , pp .",
    "springer , new york , 2011 .",
    "p. l. combettes and j .- c .",
    "pesquet ,  primal - dual splitting algorithm for solving inclusions with mixtures of composite , lipschitzian , and parallel - sum type monotone operators , \" _ set - valued var .",
    "20 , pp . 307330 , 2012 .",
    "p. l. combettes and h. j. trussell ,  methods for digital restoration of signals degraded by a stochastic impulse response , \" _ ieee trans .",
    "acoustics , speech , signal process .",
    "37 , pp . 393401 , 1989 .",
    "e.  esser , x.  zhang , and t.  chan ,  a general framework for a class of first order primal - dual algorithms for convex optimization in imaging science , \" _ siam j. imaging sci .",
    "_ , vol .  3 , pp .",
    "10151046 , 2010 .",
    "n. komodakis and j .- c .",
    "pesquet ,  playing with duality : an overview of recent primal - dual approaches for solving large - scale optimization problems , \" _ ieee signal process . mag .",
    "3154 , 2015 .",
    "s. ono , m. yamagishi , and i. yamada ,  a sparse system identification by using adaptively - weighted total variation via a primal - dual splitting approach , \" in _ proc .",
    "acoust . , speech signal process .",
    "_ , vancouver , canada , 26 - 31 may 2013 , pp .",
    "60296033 .",
    "m. pereyra , p. schniter , e. chouzenoux , j .- c .",
    "pesquet , j .- y .",
    "tourneret , a. o. hero , and s. mclaughlin ,  a survey of stochastic simulation and optimization methods in signal processing , \" _ ieee j. selected topics signal process .",
    "10 , pp . 224241 , 2016 .",
    "m. yamagishi , m. yukawa , and i. yamada ,  acceleration of adaptive proximal forward - backward splitting method and its application to sparse system identification , \" in _ proc .",
    "acoust . , speech signal process .",
    "_ , prague , czech republic , may 22 - 27 , 2011 , pp . 42964299 ."
  ],
  "abstract_text": [
    "<S> stochastic approximation techniques have been used in various contexts in data science . </S>",
    "<S> we propose a stochastic version of the forward - backward algorithm for minimizing the sum of two convex functions , one of which is not necessarily smooth . </S>",
    "<S> our framework can handle stochastic approximations of the gradient of the smooth function and allows for stochastic errors in the evaluation of the proximity operator of the nonsmooth function . </S>",
    "<S> the almost sure convergence of the iterates generated by the algorithm to a minimizer is established under relatively mild assumptions . </S>",
    "<S> we also propose a stochastic version of a popular primal - dual proximal splitting algorithm , establish its convergence , and apply it to an online image restoration problem .    </S>",
    "<S> convex optimization , nonsmooth optimization , primal - dual algorithm , stochastic algorithm , parallel algorithm , proximity operator , recovery , image restoration . </S>"
  ]
}