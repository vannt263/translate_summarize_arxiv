{
  "article_text": [
    "we describe our approach for part of speech ( pos ) tagging and noun phrases ( nps ) extraction of mathematical documents as basic tool for key phrase identification and classification of mathematical publications .",
    "nlp methods arising from the field of computer linguistics constitute a statistics- and rule - based machine - learning approach to the processing of speech and language . natural language analysis and",
    "understanding is a central aim of nlp . in western languages , noun phrases ( nps ) are the most significant parts of sentences .",
    "extraction of the nps and finding rules for which of them are relevant are the key aspects of automatic key phrase extraction in documents .",
    "an important part of capturing nps from a text is pos tagging .",
    "it presupposes the availability of information about the tokens in a sentence , especially the linguistic types of the tokens .",
    "almost all state - of - the - art pos taggers rely on dictionaries .",
    "some nlp tools are provided as open source software . in our project , the stanford pos tagger  @xcite is used .",
    "we extended the dictionaries of the pos tagger with large amounts of mathematical text data .",
    "we put a lot of work into these dictionaries as we already mentioned  @xcite at cicm 2013 .",
    "especially , they contain names of mathematicians , acronyms and special terms that only exist in the domain of mathematics .",
    "the stanford tagger uses the penn treebank pos scheme  @xcite , a classification scheme of linguistic types with 45 tags for tokens and punctuation symbols .",
    "this scheme has a relevant drawback for mathematical texts : there is no special tag for mathematical symbols or mathematical formulae ( in the following we subsume both to mathematical formulae ) . we did not change that .",
    "formulae are handled by an auxiliary construct .",
    "this simple and straightforward method allows a slim and easy maintenance of the pos tagging software .    in our approach ,",
    "mathematical formulae ( which are available as tex code ) are transformed to unique but random character sequences .",
    "pos tagging has two main problems : new words and the ambiguity of pos tags ( many tokens of the corpus can belong to more than one word class ) are addressed by determining a suitable pos tag of a token using contextual statistical models and the viterbi algorithm , a dynamic programming technique .",
    "the viterbi algorithm uses information about the surrounding tokens to predict the probable pos tag of the ambiguous or unknown token , e.g. , a formula is mainly tagged as an adjective or a noun .",
    "we illustrate that sequence of substitutions in the following example .",
    "it starts with the original latex sentence and ends with the tagged sequence .",
    "the original sentence :    .... the classical peano theorem states that in finite dimensional spaces the cauchy problem $ x'(t)=f(t , x(t))$ , $ x(t\\sb 0)=x\\sb 0 $ , has a solution provided $ f$ is continuous . ....    the tex formulae are translated into unique , but randomly generated , character sequences :    .... the classical peano theorem states that in finite dimensional spaces the cauchy problem formula - kqnompjyomsqomppsk , formula - kqomolugwpjqolugwk , has a solution provided formula - kyk is continuous . ....",
    "this sentence is fed into the pos tagger .",
    "the stanford tagger assigns an appropriate tag to ervey token .    ....",
    "the_dt classical_jj peano_nnp theorem_nn states_vbz that_in   in_in finite_jj dimensional_jj spaces_nns the_dt cauchy_nnp problem_nn formula - kqnompjyomsqomppsk_nn , formula - kqomolugwpjqolugwk_nn , _ , has_vbz a_dt solution_nn provided_vbn formula - kyk_nn is_vbz continuous_jj ._. ....    the tagged text is transformed back to its tex representation without touching the tags :    .... the_dt",
    "classical_jj peano_nnp theorem_nn states_vbz that_in in_in finite_jj dimensional_jj spaces_nns the_dt cauchy_nnp problem_nn $ x'(t)=f(t , x(t))$_nn , $ x(t\\sb 0)=x\\sb 0$_nn , _ , has_vbz a_dt solution_nn provided_vbn $ k$_nn is_vbz continuous_jj ._. ....    np extraction is done by chunking with regular expressions for special patterns of pos tags .",
    "a very basic form of such a regular expression is :    .... <",
    "dt>?<jj>*<nn > ....    this rule means that an np chunk should be formed whenever the chunker finds an optional determiner ( dt ) followed by any number of adjectives ( jj ) and then a noun ( nn ) .",
    "however , it is easy to find many more complicated examples which this rule will not cover .",
    "the actual set of expressions we used is much more complex .",
    "especially , nps can be combinations of tokens and formulae , e.g. , the cauchy problem @xmath0 @xmath1. after identifying the nps in a document , we get a collection of nps in the first step .",
    "if you would like to experiment with our solution we provide a web - based demo at http://www.zentralblatt-math.org/mathsearch/rs/postagger      a key phrase in our context , information retrieval in the mathematical literature , is a _ phrase that captures the essence of the topic of a document _  @xcite .",
    "mathematical publications , especially journal articles , have a more or less standardized metadata - structure covering important bibliographic data : authors , title , abstract , keywords ( key phrases ) and sometimes also a classification corresponding to the mathematical subject classification ( msc ) . typically",
    ", key phrases are short phrases characterizing    * embedding a publication in its general mathematical context as _ diophantine equations _ or _ optimal control _ * special objects , methods , and results of a publication as _ bipartite complex networks _ , _ k - centroids clustering _",
    "often key phrases are descended form the title , the abstract or review or the fulltext , but this is not mandatory .",
    "key phrases of a document must not be part of the document .",
    "nps are natural candidates for key phrases .",
    "key phrase identification via noun phrases is an usual technique .",
    "@xcite      classification is also an important task within nlp .",
    "the normal approach which uses the full text of a document and favours stemming and term frequency / inverse document frequency ( tf / idf ) to get rid of redundant words , but we chose a different approach .",
    "we use the extracted noun phrases from the texts , and than apply text classification methods .",
    "we tested several machine learning techniques .",
    "the best results were provided by a support vector machine ( svm ) .",
    "the svm we used is john platt s sequential minimal optimization algorithm for support vector classifiers , the kernel is a polynomial kernel , the training data was every item from the database zbmath from the beginning to the end of 2013 . in particular",
    ", we used the sequential minimal optimisation ( smo ) technique from weka .",
    "a few words about the * fig .",
    "1 * block diagram : we start with an article , it is then tokenised into sentences . for every sentence the formulae have to be preprocessed : if there is an acronym in the sentence , it needs to be expanded .",
    "after that , the pos tagger runs and the noun phrases are extracted .",
    "the nps extracted are sent to the classifier .",
    "the candidates for key phrases and classification codes are laid before to human experts .",
    "their evaluations are used to improve our machine - learning techniques .",
    "the pos block in the middle of the diagram is really big in terms of complexity . as said above it ,",
    "if a new release comes from stanford , the new block can easily be integrated into our system .",
    "general remark : there are a couple of emerging machine - learning techniques which have also been used for semantic analysis of documents .",
    "they work with deep belief networks ( deep neural networks ) and the outcomes of these experiments are more than promising .",
    "reviewing journals have a long tradition in mathematics , and nowadays take the form of electronic databases . today",
    "zbmath and mathscinet  @xcite are the most important bibliographic mathematics databases .",
    "they are important tools used by the mathematical community in searching for relevant publications .",
    "these databases provide the most comprehensive bibliographic information about mathematics enhanced by a deep content analysis of the publications . in the gutenberg era , all this information was created manually .",
    "the digital age has changed the situation dramatically .",
    "the digitisation of information allows automation like that we are developing to make the production of databases more efficient and uniformly to improve the quality of the database zbmath .",
    "the math databases have three different layers which are directly geared towards content analysis of a publication :    * bottom layer : reviews or abstracts * second layer : key phrases * top level : classification    every layer has its own characteristics , but these layers interact .",
    "we will show in the following how we have used reviews or abstracts for key phrase extraction and classification .",
    "the relevance of nps for key pheases identification is also valid for our data .",
    "the zentralblatt math , today the database zbmath , has reacted to the increasing number of key phrases in the mathematical literature and has collected them since the 70s .",
    "the field ut _ uncontrolled terms _ was introduced to accentuate single terms or phrases ) of a publication , e.g. , _ marginal function , quasi - differentiable function , directional differentiability , distance function_. this field lists key phrases created by authors and/or reviewers and/or editors of zbmath .",
    "typically , key phrases of authors will be extended by reviewers and editors within the workflow of zbmath .",
    "the key phrases presented in zbmath are searchable ( by the specification @xmath2 in the search field ) and clickable .",
    "the key phrases in zbmath are different in size and quality .",
    "the current number of all key phrases in the database zbmath is greater than ( not disjunct ) 10,200,000 entities .",
    "that means , the average number of key phrase of a publication is not more than 3 , which is not sufficient for a description of the content below the msc level .",
    "the dominant majority of key phrases in zbmath are noun phrases including formulae as @xmath3-algebra . only , a small number of the manually created key phrases are single adjectives as key phrases , e.g. , _",
    "quasiconvex_. no verbs were used as key phrases .",
    "so , we have focused us to identification of noun phrases ( with formulae ) as the most important candidates for key phrases until now .    for automatic creation of key phrases , typically only the titles and reviews or abstracts are available .",
    "this has the advantage that the number of noun phrases which is the list of candicates is small .",
    "moreover , titles and reviews or abstracts are perfectly suited to detecting and extracting key phrases because they are generally understandable and summarise the content of a publication in a highly condensed form . +",
    "but of course , the nlp methods using tokenizing , pos tagging , and chunking , have to be adapted to specific requirements of our data .",
    "* relevance : * the nps extracted are of different values for content analysis .",
    "such phrases as _ in the following paper ( chapter etc . )",
    "_ or _ an important theorem _ are of marginal value for content analysis",
    ". therefore we allocate a weight to each extracted noun phrase .",
    "a noun phrase is given a _ very high score _ if it is    * a named mathematical entity which is defined in a mathematical vocabulary such as wikipedia , planetmath , encyclopaedia of mathematics , etc .",
    "the number of named mathematical entities in these vocabularies is limited , and not more than 50,000 entities",
    ". typically , such phrases are important in assigning the publication to its mathematical context . *",
    "identical with a proposed key phrase of the publication .",
    "most mathematical publications have a ( limited ) number of key phrases created by the author(s ) . *",
    "identical with an existing key phrase in zbmath .",
    "the existing key phrases describe general or special aspects .",
    "( the total number of existing key phrases in the database zbmath is more than 10,200,000 , the number of distinct key phrases is 2,900,000 . )",
    "_ high score _ if it    * contains names of mathematicians : if a noun phrase contains names of mathematicians , it is an indicator that the noun phrase is a name for a special conjecture , theorem , approach or method .",
    "* is a acronym : acronyms are artificial words and have a special spelling .",
    "acronyms are used as abbreviations for longer noun phrases .",
    "acronyms are _ per se _ relevant noun phrases .",
    "we compare the extracted candidates for acronyms with our dictionary and resolve them .",
    "generally , the resolution is not unique and depends from the area ; some acronyms have up to 20 different meanings .",
    "* is or contains specific mathematical formulae : a special mathematical formula in a term , e.g. , @xmath4-control , is a relevant noun . at least all formulae which are not one - character mathematical notations , are important .",
    "_ marginal or negative score _    * if it provides no additional information about the content .",
    "then , the extracted noun phrase is removed from the candidate list .",
    "* incomplete chunking : * sometimes , relevant mathematical key phrases involve a larger number of tokens , e.g. , _ browder  ky fan and ky fan  glicksberg fixed - point theorems_. sometimes , the extracted phrases are incomplete . to solve these problems , the rules for chunking have to be adapted permanently",
    ".      in the following ,",
    "the used methods are listed :    * weighting : the weighting of key phrases is done as described above . *",
    "redundancy : very often , some of the extracted nps are similar . a simple measure for the similarity is the number of different tokens between two phrases .",
    "the method used is the lcs ( longest common subsequence ) algorithm .",
    "the nps are grouped by similarity .",
    "* filtering : groups of similar phrases are replaced by a representative . selecting a representative is done by using the base vocabulary .",
    "existing key phrases and other resources ( e.g. , the labels of the msc classes ) are used to select the most suitable phrase .",
    "* evaluation by experts : the resulting list of possible key phrases is shown to human experts , e.g. , editors or reviewers who can remove , change or add phrases .      * number of key phrases and quality : * in the average , 3  4 key phrases were assigned manually to a publication .",
    "the average number of extracted nps is significantly higher : 10  20 nps . by the methods described above",
    ", the number of candidates is reduced to 7  10 phrases for a publication .    up to now ,",
    "the evaluation of key phrase extraction by human experts has been started only for particular classes because it means additional expense for human involvement . in the first phase , under 40% of the phrases",
    "were accepted by the experts .",
    "the feedback led to a redesign and essential improvements of the methods .",
    "the acceptable proportion of automatically generated key phrases increased to more than 60% by removing irrelevant phrases .",
    "it is planned to integrate the machine - based key phrase extraction in a semiautomatic workflow for zbmath .",
    "of course , the quality of the proposed key phrases is dependent on the title and review ( abstract ) .",
    "* controlled vocabulary : * we applied our tools to the complete zbmath database .",
    "all resulting key phrases and all changes are stored and used for further enrichment and improvement of key phrases .",
    "the set of all positive evaluated key phrases is a first controlled vocabulary of mathematics ; the irrelevant noun phrases define the bad list .",
    "the first version of the prototype of the controlled vocabulary contains 3,500,000 different phrases .",
    "the controlled vocabulary can be structured by topic ( msc classification , see below ) and weighted by frequency .",
    "* key phrases and classification : * the automatically created key phrases were also used for classification as will be described below in detail .",
    "basing classifiers on the extracted key phrases instead of on reviews has significantly improved the quality of automatic classification .",
    "* structuring key phrases : * using our method we get only key phrases which are within a text .",
    "for a further enhancement of the key phrases and the controlled vocabulary , we have to know additional relations between the phrases , e.g. , synonyms , hypernyms , hyponyms , meronyms .",
    "such ontological relations could be used for structuring and improving the extracted key phrases .    * deeper analysis of mathematical formulae : * mathematical symbols and formulae form an important part of mathematical publications but they are more important in the full texts of publications than in reviews . +",
    "an analysis of the symbols and formulae found in zbmath has shown that the reviews , or abstracts , contain over 10,000,000 symbols and formulae .",
    "most of them are simple one - character symbols .",
    "nevertheless , the analysis of symbols and formulae and its combination with text analysis is of great interest , e.g. , the correspondence between a text phrase and a formula seems relevant .",
    "formulae were integrated in pos tagging and noun phrase extraction as described above . a deeper analysis of mathematical symbols and formulae is planned in cooperation with the mathsearch project .",
    "classification is a well - established concept for organising information and knowledge .",
    "although it is a well known method , it is not a trivial task .",
    "the reasons for difficulties are numerous .",
    "two main reasons are the classification scheme and the classifying process .",
    "classes are defined by one or more common properties of the members .",
    "abstracting from individual objects , classification schemes assign the objects to classes .",
    "classification schemes are not given _ a priori _ , they are intellectually designed and depend on the topic , aims , time , interests and views of the developers of the classification scheme .",
    "the msc was designed by the american mathematical society in the 1970s .",
    "the primary goal was to support subject - oriented access to the increasing number of mathematics - relevant publications , e.g. , zbmath lists 35,958 journal articles , books and other publications in mathematics and application areas in 1975 . for a sufficiently fine - grained access to these thousands of documents annually , a hierarchical three - level deep classification scheme with more than 5,000 classes was developed . in particular , the top level of the msc has 63 classes .",
    "typically , the classes of a classification scheme are not pairwise disjoint .",
    "often , an overlapping of classes is part of a concept .",
    "this is also valid for the msc , e.g. , navier - stokes equations are listed in two main topics : 35-xx partial differential equations ( this is the mathematical point of view ) and 76-xx fluid mechanics ( here , the application aspect is dominant ) .",
    "the msc shows not only hierarchical relations but also different kinds of similarity .",
    "moreover , an object can possess the properties of different classes .",
    "typically , a mathematical publication can not be reduced to a unique aspect or property .",
    "publications develop or analyse mathematical models or objects investigated , make quality statements about them , or develop methods or tools to solve problems .",
    "this implies that a publication can be a member of more than one class .",
    "a second reason for the difficulties is the classification process .",
    "this means that the classification codes ( the handles indexing the classes ) assigned an object , e.g. , the msc classification codes given a mathematical publication , are subjective ( there is a certain range for classification codes ) . the classifications can be weighted : zbmath and mathscinet differentiate between primary and secondary msc classifications .    what is the true classification of an object",
    "what is the most important class ? are the classification codes given complete ? generally , there are often no objective answers to these questions .",
    "the uncertainty of the classification scheme and the subjectivity of the human experts work against the objective value of classification . for the reviewing services in mathematics ,",
    "authors , editors and reviewers are involved in the classification process . in more detail ,",
    "the final classification codes in zbmath are the result of the workflow process .",
    "this means , each classification code given by the author(s ) is checked by the reviewer and the editor .",
    "this workflow reduces the impact of subjective decision .    to the best of our knowledge",
    ", there has been no serious analysis of the quality and reliability of assigned classification codes .",
    "checking classification would cover two steps a. ) correctness of a proposed classification code b. ) completeness of proposed classification codes ( often a publication belongs to more than one class , e.g. , look for a control system described by ordinary differential equations and its stability is investigated ) .",
    "but for the development and evaluation of automatic classifiers we are assuming to start with that the classification in zbmath is correct .",
    "one aim of our work is to provide tools for automatic classifying , especially svm methods . in detail",
    ", we took 63 different svm classifiers , one for every top - level of the msc and trained them on key phrases from the corresponding sections of the zbmath database .",
    "quality of classification is usually measured by precision & recall and the f1-score which is their harmonic mean .",
    "precision is the proportion of all publications of a class which are assigned to a specific class by the automatic classifier . for recall",
    "we look at all publications which are not in a class ( in the complement of the class : this means the publication is in some other class ) but assigned to this class by the automatic classifier .",
    "the results of the classification provide a differentiated picture . roughly speaking ;",
    "the precision is sufficient ( for 26 of the 63 top - level classes the precision is higher than 0.75 and only for 4 classes is it smaller than 0.5 ) , the recall is not . in other words publications",
    "which were classified as elements of a particular msc class @xmath5 are mostly correctly classified .",
    "the classifier is precision weighted : for all msc classes the precision is higher than recall .    in the following ,",
    "we discuss the results in more detail .",
    "a central idea in discussing the quality is the overlapping of classes . therefore we have built a matrix , indexed by the top - level classes of the msc , which lists the numbers of publications according to the following definition : let @xmath6 be the number of publications which are classified exclusively with the msc class @xmath5 and @xmath7 the number of publications with the primary classification @xmath5 and secondary classification @xmath8 .",
    "we normalize the elements @xmath7 of the matrix by with @xmath9 where @xmath10 denotes the number of all publications with primary msc code @xmath5    as a first result , it becomes clear that there is a correlation between the overlapping of classes and the results of the automatic classifier .    ; white indicates no overlap ; the darker the cell the more the overlap.,scaledwidth=40.0% ]    * easy classes : * if @xmath11 is near 1 , then both precision and recall are high .",
    "the overlap with other classes is small and the vocabulary differs significantly from the vocabulary of other classes .",
    "giving examples of such classes are those publications which have primary classifications from an application areas .",
    "this seems to be natural , because each application area has its own specific language and terminology .",
    "* difficult classes : * there are have different types of overlap . overlap with other classes",
    "can be focused on some msc classes , e.g. , for the class 31-xx _ potential theory _ and 43 _ harmonic analysis _ , or distributed , e.g. , msc 97-xx _ mathematical education_. in the first case , we propose to further cluster some msc classes which are similar .",
    "in addition , the total number of all publications with the primary classification @xmath5 is relevant : a small number of documents has a negative impact on the classification quality .",
    "it seems that vocabulary and terminology of these classes may not be stable enough .",
    "the difficult classes have less than a few thousand documents .",
    "* use of classification : * a high precision means a high reliability that publications of the class @xmath5 will be also automatically assigned to this class .",
    "this is important for preclassification where precision is more important than recall .",
    "until now we have been deploying the classification tool for preclassification of publications .",
    "we propose to improve recall by a second step of classification analysis .",
    "the key phrases of each publication assigned to the class @xmath5 by the automatic classifier will be analyzed in more detail .",
    "* controlled vocabulary and classification : * classification is  in addition to key phrases  an important piece of metadata in the content analysis of a publication .",
    "each zbmath item can bear more than one classification code .",
    "the database zbmath does not contain a relation between key phrases and classification codes .",
    "it is a @xmath12 relation .",
    "also the hierarchical structure of the msc is a problem too .",
    "to begin we have applied to top classes and assign a msc class to a key phrase if the msc classification ( at the top level ) is unique .",
    "this allows creating an initial vocabulary for each top msc class which has a higher precision of the definition of a msc class than the existing definition .",
    "moreover , the structure of the msc scheme can be analyzed e.g. by the studying the intersection between the controlled vocabularies of different msc classes .",
    "it seems that the machine - based methods we have developed for key phrase extraction and classification are already useful in improving the content analysis of mathematical publications and making the workflow at zbmath more efficient .",
    "we note some positive effects :    * quantity and quality of key phrases is increased by automatic key phrase extraction . *",
    "the integration of formulae into key phrase extraction lays the foundations for including formulae in content analysis .",
    "this could essentially improve content analysis of mathematical documents .",
    "* results of classification can be used to redesign and improve the msc . *",
    "the use of standardized methods guarantees a balanced and standardized quality of content analysis in zbmath .",
    "nguyen , thuy dung , and min - yen kan . _ keyphrase extraction in scientific publications_. asian digital libraries . looking back 10 years and forging new frontiers .",
    "springer berlin heidelberg , 2007 .",
    "317 - 326 ."
  ],
  "abstract_text": [
    "<S> content analysis of scientific publications is a nontrivial task , but a useful and important one for scientific information services . in the gutenberg era it was a domain of human experts ; in the digital age many machine - based methods , e.g. , graph analysis tools and machine - learning techniques , have been developed for it . </S>",
    "<S> natural language processing ( nlp ) is a powerful machine - learning approach to semiautomatic speech and language processing , which is also applicable to mathematics . </S>",
    "<S> the well established methods of nlp have to be adjusted for the special needs of mathematics , in particular for handling mathematical formulae . </S>",
    "<S> we demonstrate a mathematics - aware part of speech tagger and give a short overview about our adaptation of nlp methods for mathematical publications . </S>",
    "<S> we show the use of the tools developed for key phrase extraction and classification in the database zbmath . </S>"
  ]
}