{
  "article_text": [
    "data mining topics such as classification  @xcite , clustering  @xcite , frequent pattern mining  @xcite , frequent sub - structure mining  @xcite , regression  @xcite , data cleaning  @xcite , ranking  @xcite , data warehousing  @xcite , recommender systems  @xcite , bio - informatics  @xcite , outlier detection  @xcite , nearest neighbors  @xcite and social networks  @xcite , have been widely discussed in data management and prediction .",
    "there has been plenty of work on classification as one of the main techniques for supervised learning .",
    "figure  [ fig : classificationexp ] , shows a small example where we have sets of points in a plane , each of which belonging to one category , demonstrated by different shapes . a classifier model ,",
    "is given data vectors in 2 dimensional space ( 2d ) , with labels ( i.e. training ) , and is expected to predict , and assign new objects with missing labels to their correct categories(i.e .",
    "testing ) .",
    "there exist a variety of classification algorithms in machine learning and data mining .",
    "most popular classifiers are known as discriminant classifiers .",
    "discriminant classifiers aim at statistical or probabilistic modeling in order to find an objective function .",
    "then , optimization and numerical methods are used in order to find optimal parameter values .",
    "having found these optimal values , we can find the decision boundaries that divide the space into regions that separate objects from different categories .",
    "it is often the case that data is not linearly separable .",
    "this leads to misclassification errors most of the times . in order to minimize misclassification error , people use methods such as regularization , kernel transformations , feature extraction and feature selection  @xcite . examples of discriminant classifiers include support vector machines and logistic regression  @xcite .    other types of classifiers are decision trees , rule - based  @xcite methods and nearest neighbor methods . most of these methods have practical shortcomings .",
    "discriminant classifiers need to optimize their objective function and this may not be feasible in reasonable time for big data . besides , it is a challenge to find straight forward parallel implementations of these optimization algorithms . in web - based scenarios , data changes very frequently  @xcite .",
    "this requires either algorithms with highly scalable training phase , or models we can sequentially update .",
    "although time always plays a key role and sometimes sequential update may not be optimal  @xcite .",
    "decision trees and rule - based classifiers also suffer from the same shortcoming in practical scenarios . in many cases ,",
    "the theoretical problem defined to solve the classification problem is np - hard .",
    "nearest neighbor methods are efficiently applicable if data is stored in data structures such as kd - trees for nearest neighbor search . despite their efficiency in execution , they lack accuracy even for slightly challenging inputs .",
    "we demonstrate this with experiments in section  [ sec : expn ] , and briefly explain how each classification algorithm works .",
    "in this paper , rather than solving optimization problem , we use * _ computational geometry _ * , in order to build an accurate classifier .",
    "we use 2d convex hulls , using all possible 2 dimensional projections ( i.e. all possible pairs of columns regardless of order ) .",
    "figure  [ fig : convex1 ] , shows an example of the convex hull of a point set @xmath1 . in order to build classifiers ,",
    "we project the input dataset with @xmath2 dimensions to all possible @xmath3 planes . in each plane , having partitioned the training data into different classes , we find the 2d convex hull for each class ( _ select - project - convexhull _ ) .",
    "this results in @xmath4 convex hulls , where @xmath5 is the number of classes . given a new testing instance with @xmath2 feature values , we check for all existing @xmath4 convex hulls , whether they contain the corresponding @xmath6 dimensional projection(@xmath7 ) .",
    "we find the class @xmath8 , that scores highest ( i.e. its boundaries contain the point in more 2d projections ) and assign the class label .",
    "since @xmath2 is typically a small constant in practice , we are not worried about the testing time . besides , using parallelization , testing time is negligible .",
    "we also propose a filtering approach to choose only the most discriminant features in section  [ sec : expn ] , that results in accuracy improvements as well .",
    "we explain our classification algorithm in more detail in section  [ sec : classificationalgo ] , after providing the necessary computational geometry background .",
    "we make the following contributions in this paper :    1 .",
    "we explain the convex hull problem from computational geometry  @xcite .",
    "we provide algorithmic background in terms of the running time , and propose an algorithm with @xmath0 expected running time .",
    "we also prove its correctness .",
    "besides , we calculate the _ `` constant '' _ through probabilistic analysis , and our experiments show our calculated constant is reliable for different sizes of data .",
    "database community has shown tremendous interest in solving problems formulated similar to convex hulls such as designing algorithms for finding skylines  @xcite .",
    "2 .   we propose and explain our classification algorithm , datagrinder(dgr ) , using 2d convex hulls .",
    "we also propose tricks for tuning the classifier by filtering weak features that results in considerable accuracy improvement , in the case of one dataset .",
    "we propose parallel algorithms for implementing datagrinder at different levels including data partitioning as well as parallel convex hull algorithms , using the divide and conquer method .",
    "we propose a method for random data generation and testing classifiers .",
    "our proposed testing methodology controls the hardness of classification using two parameters .",
    "we conduct a comprehensive set of experiments on randomly generated and real datasets .",
    "our experiments show datagrinder is competitive against the most widely known commercial classifiers in accuracy , while being extremely scalable .",
    "convex hull of a point set in 2d , @xmath9 , is a set of points such that every point in @xmath1 can be computed as a positive linear combination ( all the weights are positive ) , of the points in @xmath9 .",
    "for this reason , it is important in applications where we are interested in finding mixtures using some baseline prototype vectors . in a sense ,",
    "convex hull of a point set is a small subset of the points that wraps around a point set , and can represent any point in the point set with a positive linear combination .",
    "we can represent convex hull as a polygon , that contains every other point in @xmath1 , within its boundaries .",
    "this polygon that wraps around all points can be extremely useful for applications in machine learning when we want to define the boundaries of a group of points ( class ) .",
    "it also has the base ingredients to represent any point in that class .",
    "for instance , we can use convex hull along with radial basis or any other sort of function in order to construct a kernel and represent every point in a new feature space .",
    "moreover , in computational geometry , problems such as finding _ half space intersection _ can be reduced to finding convex hull and this highlights the importance of exploring more computationally efficient algorithms . in many real life applications we deal with datasets with thousands or millions of data points , and existing @xmath10 algorithms",
    "fail to find convex hull in a timely manner .",
    "other than problems we can directly model with convex hull , there are many other domains such as web mining where we deal with large graphs .",
    "we can also use properties of these domains such as link structure in order to define entities such as web pages in a multidimensional euclidean space , and use convex hull for modeling  @xcite .",
    "we only focus on the 2d case but our heuristics and ideas are generalizable to higher dimensions .",
    "we use 2d for simplicity because it is more intuitive for problem solving and leave generalized version to our future work .",
    "moreover , in our present application of convex hulls ( i.e. classification ) , we are seeking data boundaries as tight as possible while still maintaining properties of binary feature correlations .",
    "this , kind of resembles entries of a covariance matrix , in multivariate gaussian distributions that can be used for principle component analysis as well .",
    "it is provable that the best possible worse case running time for this problem is @xmath10 , since sorting can be reduced to the convex hull problem  @xcite .",
    "in fact , the most efficient classic convex hull algorithms use sorting .",
    "first , we sort all the points in a dataset according to one coordinate . then , using a left to right scan of the sorted list , we iterate over other points and remove any points that do not belong to the convex hull , in linear time .",
    "in order to do so , they use geometric properties of the points on convex hull and line segments between them .",
    "figure  [ fig : convex1 ] shows a point set along with a polygon that wraps around it . if we extend each line segment in both ends , we obtain a line such that every other point in @xmath1 , is located on one side ( i.e. half space ) .",
    "we devise an algorithm , that despite its @xmath11 worst case running time , achieves @xmath0 expected running time if @xmath1 is distributed uniformly , and dimensions are independent .",
    "we also prove for independent _ normal _ distributions .",
    "previous work in computational geometry also approves the possibility of @xmath0 expected running time , if the algorithm is designed within the given framework  @xcite . here",
    ", we thoroughly describe the algorithm and provide pseudo code as well as average case analysis for computing the constant .",
    "regardless of the data instance , we can always devise strategies to avoid the worst case through smart query optimization , and use of empirical algorithms .",
    "our 2d convex hull algorithm avoids paying the initial @xmath12 sorting time .",
    "instead , in every iteration our new algorithm finds the next minimum of the list in @xmath13 , and using the new point , it uses a heuristic to remove other points from the candidate set , that do not qualify to be on convex hull .",
    "this is what we refer to as _ candidate elimination _ process .",
    "once we process all the candidates and remain with an empty * _ candidate set _ * , we have found the convex hull .",
    "our theoretical analysis as well as our quantitative experimental results , suggest that repeating this process results in @xmath0 expected running time , for finding 2d convex hull . *",
    "this iterative candidate elimination process enables us to find the convex hull of up to @xmath14 points in less than @xmath15 seconds while the existing classic algorithm fails to terminate in in a timely manner ( after 8 hours)*. it is worth highlighting again that although the classic algorithm has a better worst case running time , it fails in practice . in the rest of this section",
    ", we formally define the convex hull problem and discuss naive and classic solutions . in the subsequent subsections ,",
    "we discuss a new algorithm based on candidate elimination , and discuss its expected running time .",
    "eventually , we show with experiments that the improvement achieved using this pruning heuristic is indeed considerable , and indeed it results in linear expected running time .",
    "convex hull of a point set @xmath16 , is best defined intuitively as a polygon that wraps around all the points in @xmath1 .",
    "we can formally define this polygon as follows .",
    "* _ convex hull _ * of a point set , @xmath9 , is the set of * _ all _ * line segments , @xmath17 , between every two pair of points from @xmath1 , such that every other point is located on one side of @xmath17 .",
    "we can also use negative or positive , in order to refer to these two _ `` half spaces''_. in other words , every other point either belongs to the negative half space , or to the positive half space .",
    "naive algorithm for finding @xmath9 is as follows :    * produce every pair of points @xmath18 and @xmath19 : @xmath11 * find the line segment between @xmath18 and @xmath19 in _ constant time_. * check if every other point belongs to either negative or positive half space .",
    "if yes , add the line segment to @xmath9 otherwise discard : @xmath0 .",
    "figure  [ fig : convex2 ] , shows examples of both types of line segments .",
    "overall running time of the naive algorithm is @xmath20 , since it scans @xmath1 once for every pair of points .",
    "this results in a process that takes minimal usage of geometric properties and is extremely inefficient . using geometric properties",
    ", we can aim at designing a more targeted process .",
    "next , we describe @xmath10 algorithm that first sorts all the points by their @xmath21-coordinate .      rather than arbitrarily exploring the search space , first we sort the point set based on one",
    "coordinate ( typically @xmath21 ) .",
    "points in @xmath1 , start from @xmath22 and end at @xmath23 after sorting .    figure  [ fig : upperlowerhulls ] , splits the convex hull into two parts , both from @xmath22 to @xmath23 .",
    "we use upper hull ( @xmath24 ) , in order to refer to the part above the line segment between @xmath22 and @xmath23 ; we use lower hull ( @xmath25 ) , in order to refer to the lower part . _ classic algorithm _ for",
    "finding convex hull invokes @xmath26 and @xmath27 functions , in order to find the convex hull of @xmath1 , each in @xmath0 .",
    "therefore , the total execution time is @xmath10 . finding upper and lower hulls separately are two symmetric procedures with respect to each other .",
    "here , we only present for upper hull .",
    "algorithm  [ algo : findupper ] , computes the upper hull of the sorted point set by scanning from @xmath22 to @xmath23 .",
    "it is intuitive that we visit all the points in @xmath28 in sequence , once we do scanning from left to right , although with the rest of the points in between .",
    "the idea is to : 1 ) perform this scanning ; 2 ) identify and maintain points that belong to @xmath28 , and 3 ) discard all the other points .",
    "we change @xmath29 from @xmath30 to @xmath31 , and start the @xmath32 iteration having computed the correct upper hull of the points @xmath33 .",
    "we add @xmath34 to @xmath28 , because we know it belongs to the upper hull of @xmath35 , with the largest @xmath36coordinate value so far .",
    "we read @xmath28 in reverse order and remove any points that do not belong to the convex hull of @xmath35 , until we stop .",
    "point set @xmath1 , sorted by @xmath36coordinate upper hull of @xmath1 , @xmath28 @xmath37 initialize empty @xmath38 @xmath39 remove @xmath40 from @xmath24 @xmath41 * return * @xmath24    after appending @xmath18 to @xmath24 , we check for the last @xmath42 points in @xmath24 , if they belong to the correct convex hull or not . in order to do so",
    ", @xmath43 returns true if @xmath40 is above the line segment from @xmath44 to @xmath45 .",
    "this means @xmath40 belongs to @xmath46 .",
    "otherwise , it is removed and we repeat this process until @xmath47 returns * true * , and obtain the correct upper hull of @xmath48 to @xmath18 . equation  [ eq : checkup ] , computes a _",
    "sign _ variable . if sign is a non - negative number , @xmath47 returns true .",
    "@xmath49{r } sign = ( p_{\\ell-1}.y - p_{\\ell-2}.y)(p_{\\ell}.x - p_{\\ell-2}.x ) - \\\\ ( p_{\\ell-1}.x - p_{\\ell-2}.x)(p_{\\ell}.y - p_{\\ell-2}.y ) \\end{array}\\ ] ]    figure  [ fig : classicalgoex ] , shows a snapshot during the execution , where two middle points need to be removed after adding @xmath50 .",
    "it also shows the correct upper hull after we exit the while loop .",
    "we exit the while loop when for the first time we find a middle point which passes the convex test ( equation  [ eq : checkup ] ) . when this happens , it is guaranteed that @xmath51 is convex since the last step is convex and also we know the rest is constructed convex starting from @xmath52 .",
    "we exit the while loop also when only @xmath6 points are left , in which case there is no middle point and @xmath53 is always convex .",
    "it is worth noting , it can happen that two points appear in sorted @xmath1 next to each other with the same value for @xmath36coordinate .",
    "in this situation , we order all the points with the same @xmath36value based on their @xmath54coordinate to preserve the correctness of algorithm  [ algo : findupper ] . if two points are exactly the same , one can be removed without hurting the correctness of the convex hull algorithm .",
    "classic convex hull algorithm presented so far needs to perform an initial sorting with cost @xmath10 .",
    "we know we can not do better in the worst case for finding convex hull . despite @xmath10 worst case running time , in many cases we may be able to use heuristics in order to make the problem size smaller and achieve better expected running time . in this section",
    ", we describe a process called _ `` candidate elimination '' _ , that we use , instead of sorting .",
    "we use candidate elimination along with existing _",
    "findupperhull _ procedure , in order to solve the problem .",
    "the idea is to avoid sorting , maintain candidate lists of points for different parts of the convex hull , and find the next _ minimum _ value from a smaller candidate list , rather than paying @xmath10 for sorting in the beginning .",
    "figure  [ fig : candidateelimination1 ] , divides the plane as well as the convex hull of the point set into @xmath55 quarters , using minimum and maximum @xmath21 and @xmath56 values in the point set .",
    "we use @xmath57 , @xmath58 , @xmath59 and @xmath60 , in order to refer to these @xmath55 quarters .",
    "[ lemma : candidate ] all of the points on @xmath57 are on or above the line from @xmath22 to @xmath61 .",
    "we know the upper left hull starts at @xmath22 and ends at @xmath61 .",
    "we also know that the upper left hull is convex .",
    "therefore , none of the points on it can be below the line .",
    "lemma  [ lemma : candidate ] , provides an opportunity for candidate elimination in the beginning .",
    "we can draw a line from @xmath22 to @xmath61 , and remove any points below the line , to obtain a list of @xmath57 candidates . using symmetry ,",
    "we can find a candidate list for @xmath58 by choosing all the points above the line that goes through @xmath61 and @xmath23 .",
    "lower left candidates are those on or below the line from @xmath22 to @xmath62 , and lower right candidates are on or below the line from @xmath62 to @xmath23 . finding minimum and maximum @xmath21 and @xmath56",
    "coordinate values can be done in @xmath0 .",
    "therefore , by paying @xmath0 , we can discard many points and continue with smaller input size and this obviously can considerably improve the performance .",
    "we use _ candidate elimination _ , to refer to this process that makes more targeted use of both @xmath21 and @xmath56 coordinates .",
    "figure  [ fig : candidatearea ] , shows a minimal box that contains all of the points in @xmath1 , using @xmath22 , @xmath23 , @xmath62 and @xmath61 . inside this box",
    ", we separate @xmath55 triangles in @xmath55 corners .",
    "these are the only areas where convex hull candidates can appear .",
    "we use _ * candidate area * _ in order to refer to any area inside the box , where convex hull candidates can appear . in figure",
    "[ fig : candidatearea ] , four triangles form the candidate area .",
    "[ lemma : area1 ] the expected number of candidates after the first candidate elimination is @xmath63 .",
    "we assume points are distributed uniformly in the plane .",
    "we also assume that @xmath21 and @xmath56 coordinates are uniform and independent .",
    "given these assumptions , we define @xmath64 to be a random variable .",
    "we assign @xmath65 , if the @xmath32 point in @xmath1 is in the candidate area .",
    "we know @xmath66 ; therefore , @xmath67 .",
    "there are @xmath31 such points in the dataset , and we can use @xmath68 while @xmath69 is a random variable that takes values in @xmath70 , that indicates the number of candidate points all together after the first candidate elimination .",
    "expected value of @xmath69 is @xmath31 times expected value of @xmath64 , equal to @xmath63 , using linearity of expected value .",
    "the first candidate elimination step reduces the expected number of candidates to half .",
    "although this is a good heuristic , we still need to eliminate more candidates , and find the correct convex hull .",
    "as described earlier , we do this in @xmath55 smaller steps for @xmath57 , @xmath58 , @xmath59 , and @xmath60 , separately . here",
    ", we only describe the process for @xmath57 , and we know the rest is symmetric for the three other quarters of the convex hull .",
    "algorithm  [ algo : findupperleft ] , takes as input the list of upper left candidates after the initial candidate elimination , that are on or above the line from @xmath22 to @xmath61 .",
    "please note , that the list is not sorted by @xmath36coordinate anymore .",
    "the idea is to avoid sorting the candidate list .",
    "instead , we keep finding the next smallest @xmath21 , @xmath71 , and repeat candidate elimination using @xmath71 .",
    "the justification behind replacing sorting with this operation , is the fact that candidate list keeps getting smaller and smaller after performing candidate eliminations in sequence .",
    "this makes the cost of finding the next minimum negligible , even for large @xmath31 .",
    "@xmath72 , list of candidates for upper left hull upper left hull of @xmath1 , @xmath73 @xmath74 initialize empty @xmath75 @xmath76 @xmath77 @xmath78 remove @xmath40 from @xmath79 @xmath41 * return * @xmath79    rather than reading the next point from sorted @xmath1 , in order to find upper left hull , algorithm  [ algo : findupperleft ] , finds @xmath71 in line @xmath42 and removes it from the list of upper left candidates .",
    "we pay @xmath80 cost to find @xmath71 . in line @xmath55",
    ", @xmath81 repeats the same candidate elimination task using @xmath71 . in order to do so ,",
    "we draw a line from @xmath71 to @xmath61 , and remove any candidates below the line . in the rest of algorithm",
    "[ algo : findupperleft ] , we pretend @xmath71 is read from a sorted list and repeat the same process in order to fix @xmath82 that algorithm  [ algo : findupper ] does , already presented in section  [ sec : classicalgo ] .",
    "there are three main steps in each iteration of finding convex hull by _ candidate elimination _ :    * finding @xmath71 , overall @xmath13 * candidate elimination , @xmath13 * fixing upper hull , @xmath5 ( constant )    it is possible in the worst case , that all of the points in @xmath1 belong to the convex hull . in this case ,",
    "candidate elimination results in removing no candidates and repeating a @xmath0 process @xmath31 times , resulting in @xmath11 _ worst case _ running time . for the current classification problem",
    ", worst case scenario rarely happens .",
    "since there are @xmath55 quarters and the expected number of candidates is @xmath63 after the initial candidate elimination , there is an expected number of @xmath83 candidates in each triangle .",
    "it is worth noting , we can use the product of expected values of two random variables as the expected value of their product , because all the random variables are independent  .",
    "this , is a natural assumption , used widely in machine learning  @xcite .",
    "we define @xmath84 to be the elimination ratio , indicating the expected cost of finding @xmath71 , after the initial candidate elimination in each quarter .",
    "we present using @xmath60 , to have more variety in our examples .",
    "subsequently , we can define , @xmath85 , as elimination ratio in iteration @xmath30 and , @xmath86 , as elimination ratio in iteration @xmath6 . the expected size of @xmath87 after iteration @xmath6 is @xmath88 .",
    "expected running time of finding the convex hull of the lower right quarter is @xmath89 .",
    "we know @xmath90 is the initial elimination ratio that reduces the number of lower right candidates to @xmath83 .",
    "therefore , this is the expected size , we start with . in each iteration , we pay the cost @xmath91 .",
    "the number of @xmath87 after iteration @xmath6 , is @xmath92 .",
    "similarly , the number of candidates after the @xmath32 iteration is @xmath93 .",
    "therefore , we pay @xmath94 cost , which is the expected size of @xmath87 . adding up for a maximum of @xmath83 iterations we get @xmath95 , the total expected cost of finding @xmath60 .",
    "although we write the sum for @xmath83 iterations , it is quite likely that in the end the expected cost is @xmath96 or close to @xmath96 .",
    "this is because an exponentially smaller coefficient is multiplied by @xmath83 .",
    "this is because all @xmath97 are smaller than @xmath30 .",
    "[ lemma : candidate2 ] @xmath98 is a decreasing function that approaches @xmath99 .",
    "suppose at some iteration we have found @xmath71 and we perform candidate elimination .",
    "figure  [ fig : candidatearea2 ] , compares candidate area to eliminated area .",
    "candidate area is shown below the line from @xmath71 to @xmath23 .",
    "eliminated area is a triangle with @xmath100 .",
    "total area is @xmath101 .",
    "therefore , @xmath102 .",
    "as we get closer to @xmath23 , @xmath103 gets closer to @xmath104 and the value of @xmath98 decreases to @xmath105 .    using lemma  [ lemma : candidate2 ] ,",
    "we know @xmath106 is a product that decreases with @xmath29 . since @xmath98 is smaller than @xmath30 all the time , and @xmath98 is a decreasing function .",
    "we can _ `` assume '' _ @xmath106 exponentially decreases with @xmath29 and we can bound the expected running time using the sum of a geometric series as follows : @xmath107 . since @xmath108 is a constant between @xmath96 and @xmath30 , we know the sum of the geometric series is constant and so is the expected running time . we know minimum value for @xmath98 is @xmath99 and @xmath109 . using average value of @xmath99 and @xmath30 , we can approximate @xmath110 , resulting in @xmath0 points accessed during the execution of the convex hull algorithm for each corner . finally , we can approximate @xmath111 as the total number of points accessed during the execution for finding the convex hull of @xmath55 quarters .",
    "next , we aim at calculating a * _ constant _ * upper bound for the expected cost , in order to * _ prove _ * the expected cost is linear , when convex hull is found by candidate elimination , instead of using @xmath108 which is only raw approximation !",
    "[ theo : linearproof ] the expected value of @xmath112 is constant @xmath113 and expected running time is bounded by the sum of @xmath112 s geometric series .    in order to choose @xmath71",
    ", we need to draw a point from the uniform distribution specified by the triangle in figure  [ fig : candidatearea2 ] .",
    "the three corners of the triangle have these coordinates:@xmath114 , @xmath115 , + @xmath116 .",
    "we are interested in finding the expected position of @xmath71 on @xmath36axis .",
    "since the distribution is uniform and we are interested in expected @xmath117 , we need to find the point on @xmath36axis , such that if we split the triangle using a vertical line , candidate areas _ inside the triangle _ on both sides of the vertical line are equal .",
    "we assume the perpendicular sides of the triangle have equal expected length .",
    "one is equal to @xmath118 , and the other equal to @xmath119 .",
    "@xmath120 and @xmath121 are two random variables .",
    "@xmath122 depends on the range of values of @xmath21 and @xmath56 coordinates in the point set .",
    "it is usually the case that these coordinates are either in the same range or we can perform normalization and make expected values of @xmath120 and @xmath121 both equal to a value @xmath123 .",
    "thus , without loss of generality we calculate @xmath124 as the triangle area on the left side of @xmath71 .",
    "we also compute @xmath125 , the area inside the triangle on the right side of @xmath71 .",
    "therefore , we need to find the value of @xmath126 in terms of @xmath123 in the following equation : @xmath127 .",
    "we get @xmath128 , by solving the above equation . after drawing a large enough ( constant ) number of points from the distribution",
    ", we can assume the expected value is reached in any instance of the problem . if we rewrite @xmath129 that we computed earlier in the proof of lemma  [ lemma : candidate2 ] , in terms of @xmath123 and @xmath126 , and replace @xmath130 , we get @xmath131 , @xmath132 and @xmath133 .",
    "therefore , we can bound expected running time by @xmath134 . we need to do an initial scanning of the list in the first candidate elimination and read @xmath31 points .",
    "therefore , we compute @xmath135 as an upper bound for the expected number of points read during the execution .",
    "regardless of the exact running time , by proving theorem  [ theo : linearproof ] , we have shown the expected running time of the algorithm is @xmath0 . in the next section , in our experiments",
    "we use counters for the number of points read until we find the convex hull for each experiment . in all of our experiments",
    ", we read almost @xmath111 points during execution .",
    "this emphasises , the importance and reliability of our theoretical analysis for computing the expected running time in this section .    expected running time is linear if @xmath1 follows a _",
    "normal _ distribution .",
    "we have already done the proof for _ uniform _ distribution .",
    "we know normal distribution is more centered around its mean and further from its boundaries .",
    "it is obvious that this results in more probability mass in eliminated areas in all of the proofs regarding the expected running time analysis .",
    "we can say the expected running time when @xmath1 is uniform is an upper bound for the expected running time when @xmath1 is normal .",
    "we performed @xmath136 experiments for different number of points in @xmath1 .",
    "the number of points grows exponentially . in all cases ,",
    "we generate the point set randomly from uniform distribution .",
    "we use @xmath137 for the classic algorithm and @xmath138 for our new algorithm based on candidate elimination .",
    "we use @xmath139 with expected @xmath10 time for sorting in the implementation of classic algorithm which is typically one of the most efficient in practice . in all cases , except for @xmath140 , our running time is almost @xmath111 . in the case of @xmath141 , we perform @xmath142 point reads which is more than @xmath143 .",
    "although the difference is negligible , we relate the additional cost paid to the overhead of finding four quarters of the convex hull separately .",
    "there exists a negligible amount of overhead because @xmath22 , @xmath23 , @xmath62 and @xmath61 belong to candidate sets in more than @xmath30 quarters .",
    "[ cols=\"<,<,<,<,<,<,<\",options=\"header \" , ]     the remarkable results we observe in the above table are : 1 ) linear number of point reads compared to the input size for our new algorithm ; 2 ) finding @xmath144d convex hull of up to @xmath145 points while the classic algorithm fails to do so .",
    "we also notice we pay a lot less cost in order to find the convex hull of @xmath145 points than the classic algorithm pays to find the convex hull of @xmath146 points .",
    "figure  [ fig : classificationexp1 ] , shows the same distribution of data in classes as figure  [ fig : classificationexp ] . it also shows how classes are separated using their 2-dimensional ( 2d ) convex hulls . any new sample with missing label is checked against these three convex hulls .",
    "it is classified in that class if it is inside the corresponding convex hull .",
    "as shown in the figure , these classes overlap in the areas they cover and misclassification is always possible .",
    "it is also the case that if we try to separate these classes using other decision boundaries we face the same problem .",
    "since convex hull tightly wraps around the points from each class , it reduces the chances of misclassification using its tight boundaries .",
    "there can be instances where the point falls outside all convex hulls .",
    "in such cases , we can assign a point to a class using it s proximity in euclidean space . in the rest of this paper",
    ", we only deal with classification problems where there are typically @xmath147 features . in this case , since there are more than 2 dimensions , for each class we produce 2d convex hulls for every permutation of 2 features resulting in @xmath3 convex hulls for @xmath2 features .",
    "we define the notion of _ 2daspect _ as follows :    a two dimensional data aspect + ( _ 2daspect _ ) is a structure containing the following data .    *",
    "* classlabel * ( string ) : the class this 2daspect belongs to . *",
    "* @xmath148 , @xmath149 * ( int ) : indices of a pair of features chosen from the set of all possible pairs . * * upperhull * : ordered _ list _ of points that form the upper hull of data in @xmath148,@xmath149 plane . *",
    "* lowerhull * : ordered _ list _ of points that form the lower hull of data in @xmath148,@xmath149 plane .    in order to check whether a point is covered by a 2daspect , we check if it is below all the lines on the upperhull and above all the lines on the lowerhull .",
    "our convex hull based classifier , is composed of @xmath4 2daspect s , while @xmath5 is the number of classes .",
    "algorithm  [ algo : train ] , provides the pseudo - code for training a datagrinder using 2d convex hulls .",
    "@xmath150 data matrix , @xmath151 corresponding labels of rows in @xmath152 .",
    "all @xmath4 two dimensional aspects @xmath153 empty list @xmath154 @xmath155 upper hull of @xmath1 @xmath156 lower hull of @xmath1 @xmath157 temp = * new * @xmath158 @xmath159 @xmath160 @xmath161 @xmath162 @xmath163 @xmath164 * return * @xmath165    in algorithm  [ algo : train ] , for each class label ( @xmath166 ) , and pair of columns ( features @xmath148 , @xmath149 ) , we select all rows of @xmath152 corresponding to @xmath166 , then project to columns @xmath148 and @xmath149 .",
    "both selection and projection are standard relational algebraic operations and thus we can even implement datagrinder inside a database engine . we find upper and lower hulls of the point set , @xmath1 , in the @xmath167 plane . having found the convex hull , we create a new @xmath157 structure using @xmath166 , @xmath148 , @xmath149 , @xmath24 and @xmath25 .",
    "we repeat the process and construct all @xmath4 2daspects .",
    "testing for a new sample without label is done as follows :    * iterate over all @xmath165 .",
    "* project the input @xmath168 vector to the corresponding @xmath169 for each 2daspect . *",
    "check if the 2daspect contains @xmath170 .",
    "* increment the score for the corresponding class label @xmath166 . *",
    "find the class @xmath8 with the highest score and classify @xmath168 to @xmath8 .",
    "testing is simpler than training and all we need to do is check for all 2daspects , if they contain the new data row @xmath168 , inside their convex hull .",
    "having done this , we keep track of a * _ count _ * for each class , @xmath166 , in how many 2daspects it covers @xmath168 .",
    "we choose the class with the highest score and assign the appropriate class label according to datagrinder .",
    "we can achieve parallelization for both training and testing phases easily by partitioning according to either classes or 2daspects .",
    "this can be done in a straight forward way following a divide and conquer approach .",
    "for instance we can partition data into different classes or partition according to indices of @xmath167 combinations . since this is trivial",
    ", we only describe a simple divide and conquer algorithm for finding the 2d convex hull of a point set @xmath1 , to conclude this section .",
    "it is worth to highlight that in section  [ sec : expectedtime ] , we already showed both theoretically and empirically that our convex hull algorithm reads only @xmath0 expected number of points during its execution .",
    "parallelization of the same algorithm using divide and conquer strategy obviously does not increase the running time .",
    "in fact , there may be no reason for parallelization in many scenarios . in cases where we want to build classifiers on demand for millions of points ,",
    "it is practical to use parallelization .",
    "algorithm  [ algo : parallelconvexhull ] provides the pseudo code .",
    "point set @xmath1 convex hull of @xmath1 , @xmath9 partition @xmath1 into @xmath171 partitions",
    "@xmath172 @xmath173 = empty set of points @xmath174 convex hull of @xmath175 add all the points in @xmath176 to @xmath173 @xmath177 return @xmath9    we use @xmath9 to denote the convex hull of @xmath1 .",
    "as described earlier , it is composed of to halves or four quarters each of which is an ordered set of points by @xmath21 , ( @xmath148 ) , coordinate .",
    "the idea is simple , first we partition @xmath1 , until the size of each @xmath175 is small enough .",
    "typical running times can be estimated according to a simple cost - based analysis , and computing power / trafic available .",
    "we find the convex hull of each @xmath175 , resulting in only a few remaining points on @xmath176 , typically constant .",
    "having done this , we merge all @xmath176 s .",
    "it is guaranteed that we end up with a * _ super set _ * of the points required for the correct answer of @xmath9 .",
    "we find the convex hull of @xmath173 trivially in a final step .",
    "[ theo : convexdivide ] algorithm  [ algo : parallelconvexhull ] , correctly finds the convex hull of @xmath1 , using divide and conquer .",
    "proof is already explained since @xmath178 .",
    "we have already shown how our convex hull algorithm achieves expected @xmath0 point reads . in this section ,",
    "we already report our results regarding accuracy in different cases .",
    "first , we propose a random class generation approach , through which we can control the difficulty of the classification problem instance .",
    "we generate data only using the uniform distribution .",
    "it is known that we can convert other distributions to uniform as well before classification , using _",
    "normalization _  @xcite . here",
    ", our focus is mainly on designing datagrinder and efficient algorithms .",
    "we report our raw results using only the algorithms described and avoid any pre / post processing to leave more room for the future work , and study the _ key _ factors involved in classification accuracy of datagrinder , in its standard and straight forward case .",
    "we will show shortly , how datagrinder ( dgr ) achieves high accuracy even in its simplest form , as described in this paper .",
    "this increases our hopes for designing highly scalable data mining and machine learning algorithms , in the database community .",
    "our data generation aapproach works as follows",
    ". feature values of class @xmath5 ( last class label ) , are generated as : @xmath179 .",
    "this results in producing uniformly distributed random values for all features of class @xmath166 , in the range @xmath180 $ ] . for simplicity , we assume all class labels are integer , and all features are generated from the same distribution .",
    "suppose there are only two classes , figure  [ fig : randomdata ] , shows how the classification gets more complicated with increasing @xmath181 .",
    "there are two class labels @xmath96 and @xmath30 . in the case of @xmath182 ,",
    "the classes are linearly separable from each other .",
    "therefore , any algorithm must be able to achieve @xmath183 classification accuracy , if both training and testing datasets are generated using the same @xmath181 and @xmath5 parameters .",
    "as we increase @xmath181 , the two classes overlap in larger regions and thus the classification gets more complicated .",
    "we have shown our decision boundaries using convex hulls and points on them , for different values of @xmath181 (  [ fig : randomdata ] ) .",
    "it is also commonly known that when there are more classes ( i.e. multi - class classification ) , the classification is more challenging .",
    "this is because we need more decision boundaries , and there is more probability for overlapping areas as well as fewer training samples for each class , compared to the number of samples from other classes . here , we only show examples of 2d convex hulls for binary classification .",
    "as described earlier there are @xmath184 such * _ `` 2daspects''_*. in each case , we generate data ( @xmath152 ) with 5 dimensions @xmath185 .",
    "figure  [ fig : exlambda ] , compares datagrinder classification accuracy , to @xmath42 other well - known methods for @xmath186 , and changing @xmath181 .",
    "for all the other three algorithms , we use standard matlab functions and default parameter setting , since datagrinder is fully non - parametric .",
    "decisintree , is a text - book classifier , that achieves optimization using partitioning , information gain and obtaining a sequence of comparisons that leads to a class label with high accuracy .",
    "nearestneighbor method searches the training dataset for a new testing instance , and assigns class label according to the closest point in the euclidean space .",
    "discriminantclassifier , finds decision boundaries using @xmath187 and @xmath188 regularization  @xcite , in order to avoid overfitting to the training data .",
    "we train and test using @xmath189 samples for training and testing each .",
    "both decision tree and discriminantclassifier may need heavy training time if the dataset is large due to their optimization problems .",
    "typically , at least several _ sequential scans _ of the dataset is the minimum required .",
    "nearestneighbor is the most efficient , if we use space partitioning spacial indices in testing .",
    "however , the results show its accuracy is outperformed by all methods almost in all cases . using our randomly generated data for binary classification",
    ", we find that decision tree and datagrinder achieve the highest accuracy .",
    "we believe this is due to the fact that they both partition the space into regions rather than just using lines or hyperplanes as decision boundaries and our classification scenario is such that the discriminantclassifier fails .",
    "we fix @xmath190 and repeat for multi - class classification while changing @xmath5 . in this case , we notice all classification algorithms fail compared with datagrinder , due to the considerable gap in accuracy . given datagrinder s special scalability features for bigdata , this is a bonus that datagrinder also achieves outstanding accuracy in this experiment compared with commercial classification algorithms in matlab 2012 .",
    "we use two standard datasets also used as examples in machine learning textbooks for the classification problem , iris and wine .",
    "we obtain these datasets from the uci data mining repository  .",
    "both datasets have less than @xmath189 samples and @xmath42 classes .",
    "we use 10-fold cross validation for training and testing , meaning we divide the dataset into @xmath191 partitions , and use the average of @xmath191 experiments . in each experiment",
    ", we use @xmath192 partitions for training and @xmath30 for testing .",
    "all algorithms reach acceptable accuracy on iris dataset @xmath193 , and close to @xmath30 ( figure  [ fig : classificationexpiriswine ] ) . in the case of wine dataset , discriminantclassifier performs slightly superior compared to datagrinder and decisiontree .",
    "nearestneighbor method is significantly outperformed by all the other algorithms .",
    "since discriminantclassifier uses many parameters to achieve this , we also decide to add only @xmath30 hyper - parameter namely * _ filtering ratio ( @xmath194 ) _ * to datagrinder . in order to do this ,",
    "we add an additional variable to each 2daspect , _ classification accuracy_. it refers to the number of training samples that correctly fall inside a 2daspect ( i.e. 2daspect and data labels match ) , over the total number of all samples .",
    "any 2daspects that largely overlap with other classes resulting in classification accuracy less than @xmath195 , are removed from datagrinder .",
    "we vary @xmath195 using a @xmath196 step size from @xmath96 to @xmath30 over the training dataset and record the best testing accuracy .",
    "we also show in figure  [ fig : classificationexpiriswine ] , the best datagrinder accuracy after filtering using a solid bar .",
    "as it is notable , datagrinder accuracy increases after filtering , resulting in less classification errors .",
    "this also adds another dimension to our future research in order to target adding few meaningful parameters or hyper - parameter to the model that increase accuracy . the remarkable fact to highlight about the filtering technique presented",
    "is that we can achieve higher accuracy using tuning techniques and this leaves the door open for future research on datagrinder .",
    "figure  [ fig : ratioexp ] , shows how datagrinder accuracy changes on these datasets with varying @xmath195 . for @xmath197 , i.e. raw datagrinder ,",
    "no 2daspects are filtered . when @xmath198 , all 2daspects are filtered and all samples are assigned to the default class @xmath96 . in both cases",
    "we get the best accuracy around @xmath199 . * _ this is logical , because any features whose classification rate is more than misclassification rate can be useful for discriminating between classes .",
    "when a large enough number of such features are combined , we can achieve high overall accuracy . _ *",
    "in this section , we review the recent works in literature that discuss scalable data mining algorithms and frameworks similar to datagrinder , in motivation and technical contribution .    in  @xcite , authors propose an `` exact indexing '' approach for support vector machines .",
    "they propose indexing strategies in the kernel space , ikernel , that is used for exact top - k query processing when svm is used for ranking . given a svm model , authors use properties of the kernel space such as _ ranking instability _ and _ ordering stability_. they provide an excellent background of support vector machines , and their relevance to databases , top - k query processing and ranking . they only focus on prediction ( i.e. testing ) , and do not aim at designing parallel svm algorithms .",
    "datagrinder , provides a highly scalable algorithmic framework for both training , model updating and testing that achieves high accuracy .",
    "we might be able to focus on future work leveraging convex hulls for constructing kernels as well as ranking .",
    "although this requires leveraging more geometric properties of the data , in order to be able to achieve accuracy as high as support vector machines .",
    "support vector machine is a well researched problem with a complex structure . in contrary",
    ", datagrinder aims at building simpler discrete models with high accuracy and our initial experimental results are promising .",
    "svm also has applications in bioinformatics , where there are thousands of features and we need to improve datagrinder in order to be able to deal with these applications .",
    "biological datasets are typically more complex . regardless of the model structure , they focus on ranking and top - k query processing while we focus on convex hulls and classification .",
    "arraystore  @xcite , is a storage manager for complex array processing .",
    "authors process datasets as big as @xmath200 , using parallel data mining algorithms .",
    "they provide a multi - dimensional array model , suitable for our classification scenario .",
    "they also discuss data access issues .",
    "our _ select_-_project_-_convexhull _ series of operations completely fits within their storage framework .",
    "thus , we do not worry about scalability of datagrinder at all . rather than focus on storage , in this paper",
    "we discuss a _ new discrete classification algorithm _ , that can work on the top of arraystore .",
    "authors already discuss two types of clustering algorithms , but they did not provide any examples on the classic classification problem . we provide a divide and conquer algorithm that makes datagrinder compatible with arraystore .",
    "eracer  @xcite , provides an iterative statistical framework for filling in missing data , as well as data cleaning and fixing corrupted values using conventional statistical methods .",
    "datagrinder can solve their problem in a special case .",
    "extensions of datagrinder can also solve the same exact problem .",
    "we use computational geometry , and theoretical analysis for a @xmath0 expected running time algorithm while maintaining accuracy .",
    "datagrinder can as well fit inside a dbms engine using _",
    "select_-_project_-_convexhull_. we can implement convexhull as an operation using _",
    "table functions_. datagrinder is fully non - parametric , meaning that it is easy to use , and needs no parameter tuning .",
    "datagrinder is completely discrete and we can also count on divide and conquer solutions for intense scalability .",
    "datagrinder is easy to implement , thus suitable for the industry .",
    "datagrinder achieves high accuracy in classification .",
    "we also show with experiments how we can improve accuracy by _",
    "filtering(@xmath195)_. all in all , we find datagrinder a more suitable solution for the database community , due to its strong and fundamental theoretical contributions . spanners  @xcite , is an interesting theoretical contribution , and a formal framework for information extraction .",
    "we believe datagrinder has a similar flavour in its contribution to spanners .",
    "we also aim at designing operations for processing multidimensional data and knowledge discovery .",
    "spanners is focused on information extraction and using regular expressions for text mining using predefined operations .",
    "several other previous works have also tried to achieve the same goal such as  @xcite .",
    "another interesting direction to achieve parallel statistical and data mining algorithms is through sampling  @xcite . in this approach ,",
    "we make _ bigdata _ assumption and use parallelization for processing .",
    "we build many small models and using statistical inference , we combine these models to guarantee reliability and accuracy .",
    "the size of input data and distribution(s ) of data are examples of key parameters we need to take into account .",
    "naturally , we need to focus on how to sample and pay attention to things such as the number of samples , the size of each sample as well as how to effectively combine the models built using different samplings of the data .",
    "this can be done for big data , regardless of the data mining task discussed .",
    "examples of such methods include  @xcite .",
    "rule - based classifiers are other examples of discrete classification algorithms , discussed in the data mining literature  @xcite .",
    "they use frequent patterns and association rules mining in order to find rules with high support and confidence .",
    "they typically achieve reliable accuracy .",
    "they need a rather costly parameter tuning step to construct the best classifier .",
    "they need the exact solution of a np - hard theoretical problem compared to @xmath0 expected running time of datagrinder .",
    "there have been some attempts for parallel frequent pattern mining algorithms which is _ outside the context of this paper_.    convex hull problem has a long history in computational geometry  @xcite .",
    "it is significantly important , because many other important problems in computational geometry can be reduced to this problem .",
    "many efforts have been devoted to improving the _ worst case _ running time and output sensitive algorithms .",
    "we find average case analysis more suitable to the database community , due to its similarity to cost - based query optimization . in  @xcite , there is a proposal for expected @xmath0 algorithms along with theoretical analysis to prove its possibility . in this paper , we provide an _ algorithm with pseudo code _ and calculate a _ exact costant _ , to serve as an upper - bound for the expected running time .",
    "our experimental evaluation backs up all of our arguments , regardless of the running time and programming languages used .",
    "in this paper , we revisited the important problem of finding 2d convex hulls .",
    "we propose an algorithm based on a well - known historical algorithm , with @xmath0 expected running time .",
    "we propose a simpler and shorter proof compared to the previous work , and also calculate a constant that serves as an upper - bound for the expected linear running time .",
    "we conduct experiments to back up all of our arguments .",
    "we perform several experiments and show datagrinder is comparable to the most reliable commercial classification packages in matlab and outperforms many , while maintaining its extreme provable scalability . we show how to achieve several levels of parallelization , while keeping the correctness of our classification algorithm . we intend to focus on more detailed geometrical study of the problem , in order to partition the data more accurately .",
    "specially , remove sparse areas .",
    "we also intend to test for more classification scenarios as well as adding meaningful parameters and hyper - parameters to datagrinder .",
    "we would also like to take datagrinder to the cloud for classification of enormous datasets ."
  ],
  "abstract_text": [
    "<S> it has been a long time , since data mining technologies have made their ways to the field of data management . </S>",
    "<S> classification is one of the most important data mining tasks for label prediction , categorization of objects into groups , advertisement and data management . in this paper , we focus on the standard classification problem which is predicting unknown labels in euclidean space . most efforts in machine learning communities are devoted to methods that use probabilistic algorithms which are heavy on calculus and linear algebra . </S>",
    "<S> most of these techniques have scalability issues for big data , and are hardly parallelizable if they are to maintain their high accuracies in their standard form . </S>",
    "<S> sampling is a new direction for improving scalability , using many small parallel classifiers . in this paper , rather than conventional sampling methods , we focus on a discrete classification algorithm with @xmath0 expected running time . </S>",
    "<S> our approach performs a similar task as sampling methods . </S>",
    "<S> however , we use column - wise sampling of data , rather than the row - wise sampling used in the literature . in either case , </S>",
    "<S> our algorithm is completely deterministic . </S>",
    "<S> our algorithm , proposes a way of combining 2d convex hulls in order to achieve high classification accuracy as well as scalability in the same time . </S>",
    "<S> first , we thoroughly describe and prove our @xmath0 algorithm for finding the convex hull of a point set in 2d . </S>",
    "<S> then , we show with experiments our classifier model built based on this idea is very competitive compared with existing sophisticated classification algorithms included in commercial statistical applications such as matlab . </S>"
  ]
}