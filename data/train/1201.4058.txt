{
  "article_text": [
    "graphical models @xcite are a class of statistical models which combine the rigour of a probabilistic approach with the intuitive representation of relationships given by graphs .",
    "they are composed by a set @xmath15 of _ random variables _ describing the data @xmath4 and a _ graph _",
    "@xmath16 in which each _ vertex _ or _ node _ @xmath17 is associated with one of the random variables in @xmath18 .",
    "nodes and the corresponding variables are usually referred to interchangeably .",
    "the _ edges _",
    "@xmath19 are used to express the dependence relationships among the variables in @xmath18 .",
    "different classes of graphs express these relationships with different semantics , having in common the principle that graphical separation of two vertices implies the conditional independence of the corresponding random variables @xcite .",
    "the two examples most commonly found in literature are _ markov networks _ @xcite , which use undirected graphs ( ugs , see * ? ? ?",
    "* ) , and _",
    "bayesian networks _",
    "@xcite , which use directed acyclic graphs ( dags , see * ? ? ?",
    "* ) . in the context of bayesian networks ,",
    "edges are often called _ arcs _ and denoted with @xmath20 ; we will adopt this notation as well .    the structure of @xmath0 ( that is , the pattern of the nodes and the edges ) determines the probabilistic properties of a graphical model .",
    "the most important , and the most used , is the factorisation of the _ global distribution _",
    "( the joint distribution of @xmath18 ) into a set of lower - dimensional _ local distributions_. in markov networks , local distributions are associated with _ cliques _ ( maximal subsets of nodes in which each element is adjacent to all the others ) ; in bayesian networks , each local distribution is associated with one node conditional on its _ parents _ ( nodes linked by an incoming arc ) . in markov networks",
    "the factorisation is unique ; different graph structures correspond to different probability distributions .",
    "this is not so in bayesian networks , where dags can be grouped into _ equivalence classes _ which are statistically indistinguishable .",
    "each such class is uniquely identified by the underlying ug ( i.e. in which arc directions are disregarded , also known as _ skeleton _ ) and by the set of _ v - structures _ ( i.e. converging connections of the form @xmath21 , @xmath22 , in which @xmath23 and @xmath24 are not connected by an arc ) common to all elements of the class .    as for the global and the local distributions ,",
    "there are many possible choices depending on the nature of the data and the aims of the analysis .",
    "however , literature have focused mostly on two cases : the _ discrete case _",
    "@xcite , in which both the global and the local distributions are multinomial random variables , and the _ continuous case _",
    "@xcite , in which the global distribution is multivariate normal and the local distributions are univariate ( in bayesian networks ) or multivariate ( in markov networks ) normal random variables . in the former , the parameters of interest @xmath1 are the _ conditional probabilities _ associated with each variable , usually represented as conditional probability tables . in the latter ,",
    "the parameters of interest @xmath1 are the _ partial correlation coefficients _ between each variable and its neighbours in @xmath0 . conjugate distributions ( dirichlet and wishart , respectively ) are then used for learning and inference in a bayesian setting .",
    "the choice of an appropriate probability distribution for the set @xmath11 of the possible edges is crucial to make the derivation and the interpretation of the properties of @xmath11 and @xmath25 easier .",
    "we will first note that a graph is uniquely identified by its edge set @xmath26 ( or by its arc set @xmath27 for a dag ) , and that each edge @xmath28 or arc @xmath29 is uniquely identified by the nodes @xmath23 and @xmath30 , @xmath31 it is incident on . therefore ,",
    "if we model @xmath11 with a random variable we have that any edge set @xmath26 ( or arc set @xmath27 ) is just an element of its sample space ; and since there is a one - to - one correspondence between graphs and edge sets , probabilistic properties and inferential results derived for traditional graph - centric approaches can easily be adapted to this new edge - centric approach and vice versa . in addition , if we denote @xmath32 , we can clearly see that @xmath33 . on the other hand , @xmath34 for ugs and even larger for dags @xcite and their equivalence classes @xcite .",
    "we will also note that an edge or an arc has only few possible states :    * an edge can be either present ( @xmath35 ) or missing from an ug ( @xmath36 ) ; * in a dag , an arc can be present in one of its two possible directions ( @xmath37 or @xmath38 ) or missing from the graph ( @xmath39 and @xmath40 ) .",
    "this leads naturally to the choice of a bernoulli random variable for the former , @xmath41 and to the choice of a trinomial random variable for the latter , @xmath42 where @xmath43 is the arc @xmath44 and @xmath45 is the arc @xmath46 .",
    "therefore , a graph structure can be modelled through its edge or arc set as follows :    * ugs , such as markov networks or the skeleton and the moral graph of bayesian networks @xcite , can be modelled by a _ multivariate bernoulli random variable _ ; * directed graphs , such as the dags used in bayesian networks , can be modelled by a _ multivariate trinomial random variable_.    in addition to being the natural choice for the respective classes of graphs , these distributions integrate smoothly with and extend other approaches present in literature .",
    "for example , the probabilities associated with each edge or arc correspond to the _ confidence coefficients _ from @xcite and the _ arc strengths _ from @xcite . in a frequentist setting , they have been estimated using bootstrap resampling @xcite ; in a bayesian setting , markov chain monte carlo ( mcmc ) approaches @xcite have been used instead .",
    "let @xmath47 , @xmath48 be bernoulli random variables with marginal probabilities of success @xmath49 , that is @xmath50 , @xmath51 .",
    "then the distribution of the random vector @xmath52^t$ ] over the joint probability space of @xmath47 is a _ multivariate bernoulli random variable _",
    "@xcite , denoted as @xmath53 .",
    "its probability function is uniquely identified by the parameter collection @xmath54 which represents the _ dependence structure _ among the @xmath55 in terms of simultaneous successes for every non - empty subset @xmath56 of elements of @xmath57 .",
    "other characterisations and fundamental properties of the multivariate bernoulli distribution can be found in @xcite .",
    "@xcite focus on the bivariate models specific to @xmath58 .",
    "additional characterisations and results specific to particular applications can be found in ( * ? ? ?",
    "* variable selection ) , ( * ? ? ? * longitudinal studies ) , ( * ? ? ?",
    "* combinatorial optimisation ) and ( * ? ? ?",
    "* clinical trials ) , among others .    from literature",
    "we know that the expectation and the covariance matrix of @xmath57 are immediate extensions of the corresponding univariate bernoulli ones ; @xmath59^t & & \\text{and } &    & { \\mathsf{cov}}(\\mathbf{b } ) = [ \\sigma_{ij } ] = p_{ij } - p_{i}p_{j}.\\end{aligned}\\ ] ] in particular , the covariance matrix @xmath60 $ ] has some interesting numerical properties . from basic probability theory ,",
    "we know its diagonal elements @xmath61 are bounded in the interval @xmath62 $ ] ; the maximum is attained for @xmath63 , and the minimum for both @xmath64 and @xmath65 . for the cauchy - schwarz theorem then @xmath66 $ ] . as a result ,",
    "we can derive similar bounds for the eigenvalues @xmath67 of @xmath68 , as shown in the following theorem .",
    "[ thm : mvebereigen ] let @xmath69 , and let @xmath68 be its covariance matrix .",
    "let @xmath70 , @xmath51 be the eigenvalues of @xmath68 .",
    "then @xmath71    see appendix [ app : proofs ] .",
    "these bounds define a closed convex set in @xmath72 , described by the family @xmath73\\right\\}\\ ] ] where @xmath74 is the non - standard @xmath75 simplex @xmath76      construction and properties of the multivariate trinomial random variable are similar to the ones illustrated in the previous section for the multivariate bernoulli . for this reason , and because it is a particular case of the multivariate multinomial distribution , the multivariate trinomial distribution is rarely the focus of research efforts in literature .",
    "some of its fundamental properties are covered either in @xcite or in monographs on contingency tables analysis such as @xcite .",
    "let @xmath77 , @xmath48 be trinomial random variables assuming values @xmath78 and denoted as @xmath79 with @xmath80 .",
    "then the distribution of the random vector @xmath81^t$ ] over the joint probability space of @xmath77 is a _ multivariate trinomial random variable _ , denoted as @xmath82 .",
    "the parameter collection @xmath83 which uniquely identifies the distribution is @xmath84 and the reduced parameter collection we will need to study its first and second order moments is @xmath85    from the definition , we can easily derive the expected value and the variance of @xmath86 , @xmath87 ^ 2 \\end{aligned}\\ ] ] and the covariance between two variables @xmath86 and @xmath88 , @xmath89 +                       \\left[\\ , p_{ij(-1,-1 ) } - p_{i(-1)}p_{j(-1 ) }   \\,\\right ] -   \\notag \\\\                   & \\qquad     - \\left[\\ , p_{ij(-1,1 ) } - p_{i(-1)}p_{j(1 ) }   \\,\\right ]                        - \\left[\\ , p_{ij(1,-1 ) } - p_{i(1)}p_{j(-1 ) }   \\,\\right].\\end{aligned}\\ ] ]    again , the diagonal elements of the covariance matrix @xmath68 are bounded .",
    "this can be proved either by solving the constrained maximisation problem @xmath90 or as an application of the following theorem by @xcite .",
    "if a discrete random variable @xmath91 can take values only in the segment @xmath92 $ ] of the real axis , the maximum standard deviation of @xmath91 equals @xmath93 .",
    "the maximum is reached if @xmath91 takes the values @xmath94 and @xmath95 with probabilities @xmath96 each .",
    "see @xcite .    in both cases",
    "we obtain that the maximum variance is achieved for @xmath97 and is equal to @xmath98 , so @xmath99 $ ] and @xmath100 $ ] .",
    "furthermore , we can also prove that the eigenvalues of @xmath68 are bounded using the same arguments as in lemma [ thm : mvebereigen ] .",
    "[ thm : dirlambda ] let @xmath101 , and let @xmath68 be its covariance matrix .",
    "let @xmath70 , @xmath51 be the eigenvalues of @xmath68 .",
    "then @xmath102    see the proof of lemma [ thm : mvebereigen ] in appendix [ app : proofs ] .",
    "these bounds define again a closed convex set in @xmath72 , described by the family @xmath103\\right\\},\\ ] ] where @xmath74 is the non - standard @xmath75 simplex from equation [ eq : simplex ] .    another useful result , which we will use in section [ sec : dagprop ] to link inference on ugs and dags , is introduced below .",
    "[ thm : triber2 ] let @xmath101 ; then @xmath104 and + @xmath105",
    ".    see appendix [ app : proofs ] .",
    "it follows that the variance of each @xmath86 can be decomposed in two parts : @xmath106 the first is a function of the corresponding component @xmath107 of the transformed random vector , while the second depends only on the probabilities associated with @xmath108 and @xmath98 ( which correspond to @xmath45 and @xmath43 in equation [ eqn : tridef ] ) .",
    "the results derived in the previous section provide the foundation for characterising @xmath13 and @xmath14 . to this end",
    ", it is useful to distinguish three cases corresponding to different configurations of the probability mass among the graph structures @xmath109 :    * _ minimum entropy _ :",
    "the probability mass is concentrated on a single graph structure .",
    "this is the best possible configuration for @xmath14 , because only one edge set @xmath26 ( or one arc set @xmath27 ) has a non - zero posterior probability . in other words ,",
    "the data @xmath4 provide enough information to identify a single graph @xmath0 with posterior probability @xmath98 ; * _ intermediate entropy _ : several graph structures have non - zero probabilities .",
    "this is the case for informative priors @xmath13 and for the posteriors @xmath14 resulting from real - world data sets ; * _ maximum entropy _ :",
    "all graph structures in @xmath9 have the same probability .",
    "this is the worst possible configuration for @xmath14 , because it corresponds to the non - informative prior from equation [ eqn : flatprior ] . in other words ,",
    "the data @xmath4 do not provide any information useful in identifying a high - posterior graph @xmath0 .    clearly , _",
    "minimum _ and _ maximum entropy _ are limiting cases for @xmath14 ; the former is non - informative about @xmath110 , while the latter identifies a single graph in @xmath9 .",
    "as we will show in sections [ sec : ugprop ] ( for ugs ) and [ sec : dagprop ] ( for dags ) , they provide useful reference points in determining which edges ( or arcs ) have significant posterior probabilities and in analysing the variability of the graph structure .      in the _ minimum entropy _ case , only one configuration of edges @xmath26 has non - zero probability , which means that @xmath111    the uniform distribution over @xmath9 arising from the _ maximum entropy _ case has been studied extensively in random graph theory @xcite ; its two most relevant properties are that all edges @xmath28 are independent and have @xmath112 . as a result , @xmath113 ; all edges display their maximum possible variability , which along with the fact that they are independent makes this distribution non - informative for @xmath11 as well as @xmath25 .",
    "the _ intermediate entropy _",
    "case displays a middle - ground behaviour between the _ minimum _ and _ maximum entropy _ cases .",
    "the expected value and the covariance matrix of @xmath11 do not have a definite form beyond the bounds derived in section [ sec : mvber ] . when considering posteriors arising from real - world data , we have in practice that most edges in @xmath11 represent conditional dependence relationships that are completely unsupported by the data .",
    "this behaviour has been explained by @xcite with the tendency of `` good '' graphical models to represent the causal relationships underlying the data , which are typically sparse . as a result , we have that @xmath114 and @xmath115 for many @xmath28 , so @xmath68 is almost surely singular unless such edges are excluded from the analysis .",
    "edges that appear with @xmath116 have about the same marginal probability and variance as in the _ maximum entropy _ case , so their marginal behaviour is very close to random noise . on the other hand ,",
    "edges with probabilities near @xmath117 or @xmath98 can be considered to have a good support ( against or in favour , respectively ) . as @xmath118 approaches @xmath117 or @xmath98",
    ", @xmath28 approaches its _",
    "minimum entropy_.    the closeness of a multivariate bernoulli distribution to the _ minimum _ and _ maximum entropy _ cases can be represented in an intuitive way by considering the eigenvalues @xmath119^t$ ] of its covariance matrix @xmath68 .",
    "recall that the @xmath120 can assume values in the convex set @xmath121 defined in equation [ eq : simplex ] , which corresponds to the region of the first orthant delimited by the non - standard simplex @xmath122 . in the _ minimum entropy _",
    "case we have that @xmath123 , so @xmath124 , and in the _ maximum entropy case _",
    "@xmath113 , so @xmath125 ; both points lie on the boundary of @xmath121 , the first in the origin and the second in the middle of @xmath122 .",
    "the distance between @xmath120 and these two points provides an intuitive way of measuring the variability of @xmath11 and , indirectly , the entropy of the corresponding probability distributions @xmath14 and @xmath13 .",
    "it is important to note , however , that different distributions over @xmath9 may have identical first and second order moments when modelled through @xmath11 .",
    "such distributions will have the same @xmath120 and will therefore map to the same point in @xmath121 .",
    "a simple example comprising three different distributions over a set of two edges is illustrated below .",
    "[ ex : base ] consider three multivariate bernoulli distributions @xmath126 , @xmath127 , @xmath128 over two edges ( denoted with @xmath129 and @xmath130 for brevity ) with covariance matrices @xmath131 and eigenvalues @xmath132 their positions in @xmath121 are shown in figure [ fig : base ] .",
    "@xmath126 is the closest to @xmath133 , the point corresponding to the maximum entropy case , while @xmath127 and @xmath128 are farther from @xmath133 than @xmath126 due to the increasing correlation between @xmath134 and @xmath135 ( which are independent in the _ maximum entropy _ case ) .",
    "the correlation coefficients for @xmath126 , @xmath127 and @xmath128 are @xmath136 , @xmath137 , @xmath138 , and they account for the increasing difference between the eigenvalues of each covariance matrix .",
    "in fact , @xmath139 is nearly singular because of the strong linear relationship between @xmath134 and @xmath135 , and it is therefore very close to one of the axes delimiting the first quadrant .",
    ", @xmath140 and @xmath139 from example [ ex : base ] represented as functions of their eigenvalues in the convex set @xmath121 .",
    "the points @xmath141 and @xmath142 correspond to the _ minimum entropy _ and _ maximum entropy _ cases . ]",
    "if we denote with @xmath143 , @xmath144 , @xmath145 , and @xmath146 all possible edge sets and with @xmath147 , @xmath148 , @xmath149 and @xmath150 the associated probabilities , for @xmath126 we have @xmath151 this is indeed close to a uniform distribution .",
    "the probability of both @xmath134 and @xmath135 is @xmath152 and the variance is @xmath153 , which are again similar to the reference values for the _ maximum entropy _ case . on the other hand , for @xmath127 we have @xmath154 these probabilities are markedly different from a uniform distribution ; the probabilities of @xmath134 and @xmath135 are respectively @xmath155 and @xmath156 . considering also the correlation between @xmath134 and @xmath135 , it is intuitively clear why @xmath140 is not as close as @xmath157 to @xmath133 .",
    "this is also true for @xmath128 , which has the same marginal distributions as @xmath127 but with a much stronger correlation .",
    "the behaviour of the multivariate trinomial distribution in the _ minimum _ and _ intermediate entropy _",
    "cases is similar to the one of the multivariate bernoulli in many respects , but presents profound differences in the _ maximum entropy _ case .",
    "the reason for these differences is that the structure of a bayesian network is assumed to be acyclic .",
    "therefore , the state of each arc ( i.e. whether is present in the dag and its direction ) is influenced by the state of all other possible arcs even in the _ maximum entropy _ case , when otherwise they would be independent .",
    "furthermore , the acyclicity constraint can not be written in closed form , making the derivation of exact results on the moments of the distribution of @xmath11 particularly difficult .    to obtain some simple expressions for the expected value and the covariance matrix",
    ", we will first prove a simple theorem on dags , which essentially states that if we reverse the direction of every arc the resulting graph is still a dag .",
    "[ thm : acyclic ] let @xmath158 be a dag , and let @xmath159 another directed graph such that @xmath160 for every @xmath161 .",
    "then @xmath162 is also acyclic .",
    "see appendix [ app : proofs ] .",
    "an immediate consequence of this theorem is that for every dag including the arc @xmath43 there exists another dag including the arc @xmath45 . since all dags have the same probability in the _ maximum entropy case _",
    ", this implies that both directions of every arc have the same probability , @xmath163 then the expected value of each marginal trinomial distribution is equal to @xmath164 and its variance is equal to @xmath165    the joint probabilities associated with each pair of arcs also symmetric in the maximum entropy case , again due to theorem [ thm : acyclic ] .",
    "denote with @xmath166 the event that arc @xmath29 is not present in the dag .",
    "if we consider that both directions of every arc have the same probability and that there is no explicit ordering among the arcs , we have @xmath167 then the expression for the covariance simplifies to @xmath168,\\ ] ] which can be interpreted as the difference in probability between a _ serial connection _",
    "( i.e. @xmath169 , if @xmath170 ) and a _ converging connection _",
    "( i.e. @xmath171 ) if the arcs are incident on a common node @xcite .",
    "this is interesting because v - structures are invariant within equivalence classes , while other patterns of arcs are not @xcite ; indeed , equivalence classes are usually represented as _ partially directed acyclic graphs _ ( pdags )",
    "in which only arcs belonging to v - structures are directed .",
    "all other arcs , with the exclusion of those which could introduce additional v - structures ( known as _ compelled arcs _ ) , are replaced with the corresponding ( undirected ) edges .",
    "therefore , the combination of high values of @xmath172 and @xmath173 is indicative of the belief that the corresponding arcs are directed in the pdag identified by the equivalence class . along with with @xmath174 and @xmath175 , it is also indicative of the stability of the graph structure , both in the arcs and their directions . in an uninformative prior , such as the distribution we are now considering in the _ maximum entropy _ case",
    ", we expect all covariances to be small ; we will show this is the case in theorem [ thm : hoeffding ] . on the other hand , in an informative distribution such as the ones considered in the _ intermediate entropy _",
    "case , we expect covariances to be closer to their upper bounds for arcs that are compelled or part of a converging connection , and closer to zero for arcs whose direction is not determined in the equivalence class",
    ". note that the sign of @xmath176 depends on the way the two possible directions of each arc are associated with @xmath98 and @xmath108 ; a simple way to obtain a consistent parameterisation is to follow the natural ordering of the variables ( i.e. if @xmath177 then the arc incident on these nodes is taken to be @xmath178 , @xmath43 is associated with @xmath98 and @xmath45 with @xmath108 ) .",
    ", @xmath179 , @xmath180 , @xmath181 , and @xmath182 nodes .",
    "the dotted line represents the limiting value in the number of nodes.,scaledwidth=90.0% ]     to @xmath183 nodes .",
    "the dotted line represents the limiting value in the number of nodes.,scaledwidth=90.0% ]    the equalities in equations [ eqn:1storder ] and [ eqn:2ndorder ] drastically reduce the number of free parameters in the _ maximum entropy _ case .",
    "the marginal distribution of each arc now depends only on @xmath184 , whose value can be derived from the following numerical approximation by @xcite .",
    "[ thm : idecozman ] the average number of arcs in a dag with @xmath185 nodes is approximately @xmath186 in the _ maximum entropy _ case .    see @xcite .",
    "[ thm : corollary ] let @xmath158 be a dag with @xmath185 nodes .",
    "then for each possible arc @xmath187 we have that in the maximum entropy case @xmath188    see appendix [ app : proofs ] .    the quality of this approximation is examined in figure [ fig : exactarcprob ] and figure [ fig : mtarcprob ] . in figure",
    "[ fig : exactarcprob ] , the values provided by theorem [ thm : corollary ] for dags with @xmath189 , @xmath179 , @xmath180 , @xmath181 and @xmath182 nodes are compared to the corresponding true values . the latter have been computed by enumerating all possible dags of that size ( i.e. the whole population ) and computing the relative frequency of each possible arc . in figure",
    "[ fig : mtarcprob ] , the values provided by theorem [ thm : corollary ] for dags with @xmath190 to @xmath183 nodes are compared with the corresponding estimated values computed over a set of @xmath191 dags of the same size . the latter have been generated with uniform probability using the algorithm from @xcite as implemented in the bnlearn package @xcite for r @xcite .",
    "we can clearly see that the approximate values are close to the corresponding true ( in figure [ fig : exactarcprob ] ) or estimated ( in figure [ fig : mtarcprob ] ) values for dags with at least @xmath181 nodes .",
    "this is not a significant limitation ; the true values can be easily computed via exhaustive enumeration for dags with @xmath189 , @xmath179 and @xmath180 nodes ( they are reported in appendix [ app : numbers ] , along with other relevant quantities ) .",
    "furthermore , it is evident both from theorem [ thm : corollary ] and from figures [ fig : exactarcprob ] and [ fig : mtarcprob ] that , as the number of nodes diverges , @xmath192 if we take the absolute value of this asymptotic trinomial distribution , the resulting random variable is @xmath193 with @xmath112 , which is the marginal distribution of an edge in an ug in the _ maximum entropy _ case .",
    "the absolute value transformation can be interpreted as ignoring the direction of the arc ; the events @xmath37 and @xmath38 collapse into @xmath35 , while @xmath194 maps to @xmath195 . as a result ,",
    "the marginal distribution of an arc is remarkably similar to the one of the corresponding edge in an undirected graph for sufficiently large dags ; in both cases , the nodes @xmath23 and @xmath30 are linked with probability @xmath96 .",
    "no result similar to theorem [ thm : idecozman ] has been proved for arbitrary pairs of arcs in a directed acyclic graph ; therefore , the structure of the covariance matrix @xmath68 can be derived only in part .",
    "variances can be approximated using the approximate probabilities from theorem [ thm : corollary ] : @xmath196     to @xmath183 nodes .",
    "the dotted lines represent the respective limiting values.,scaledwidth=90.0% ]     to @xmath183 nodes .",
    "the dotted lines represent the respective limiting values.,scaledwidth=90.0% ]    therefore , maximum variance ( of each arc ) and maximum entropy ( of the graph structure ) are distinct , as opposed to what happens in ugs .",
    "however , we can use the decomposition of the variance introduced in equation [ eqn : vardecomp ] to motivate why the _ maximum entropy _",
    "case is still a `` worst case '' outcome for @xmath14 .",
    "as we can see from figure [ fig : vardecomp ] , the contributions of the presence of an arc ( given by the transformation @xmath197 ) and its direction ( given by the @xmath198 term ) to the variance are asymptotically equal .",
    "this is a consequence of the limits in equation [ eqn : limits ] , which imply that an arc ( modulo its direction ) has the same probability to be present in or absent from the dag and that its directions also have the same probability . as a result , we are not able to make any decision about either the presence of the arc or its direction . on the contrary ,",
    "when @xmath174 reaches it maximum at @xmath98 we have that @xmath199 and @xmath200 , so we are sure that the arc will be present in the dag in one of its two possible directions .    as for the covariances , it is possible to obtain tight bounds using _ hoeffding s identity _",
    "@xcite , @xmath201 and the decomposition of the joint distribution of dependent random variables provided by the _ farlie - morgenstern - gumbel _ ( fmg ) family of distributions @xcite , which has the form @xmath202 , &    & |\\varepsilon| \\leqslant 1.\\end{aligned}\\ ] ] in equations [ eqn : hoeffding ] and [ eqn : fmg ] , @xmath203 , @xmath204 and @xmath205 denote the cumulative distribution functions of the joint and marginal distributions of @xmath91 and @xmath206 , respectively .",
    "[ thm : hoeffding ] let @xmath207 be a dag , and let @xmath29 , @xmath31 and @xmath208 , @xmath209 be two possible arcs .",
    "then in the _ maximum entropy _ case we have that @xmath210 ^ 2 \\left[\\frac{1}{4 } + \\frac{1}{4(n - 1)}\\right]^2\\ ] ] and @xmath211 ^ 2 \\left[\\frac{1}{4 } + \\frac{1}{4(n - 1)}\\right].\\ ] ]    see appendix [ app : proofs ] .",
    "the bounds obtained from this theorem appear to be tight in the light of the true values for the covariance and correlation coefficients ( computed again by enumerating all possible dags of size @xmath189 to @xmath182 ) .",
    "figure [ fig : approxcor ] shows the bounds for dags with @xmath181 to @xmath183 nodes ; for dags with @xmath189 , @xmath179 and @xmath180 nodes the approximation of @xmath184 the bounds are based on is loose , and the true values of covariance and correlation are known .",
    "non - null covariances range from @xmath212 ( for dags with @xmath189 nodes ) to @xmath213 ( for dags with @xmath182 nodes ) , while non - null correlation coefficients vary from @xmath214 ( for dags with @xmath189 nodes ) to @xmath215 ( for dags with @xmath182 nodes ) .",
    "both covariance and correlation appear to be strictly increasing in modulus as the number of nodes increases , and converge to the limiting values of the bounds ( @xmath216 and @xmath217 , respectively ) from below .",
    "( solid line ) and @xmath218 ( dashed line ) for dags with @xmath190 to @xmath183 nodes . the dotted line represents their asymptotic value.,scaledwidth=90.0% ]    some other interesting properties are apparent from true values of the covariance coefficients reported in appendix [ app : numbers ] . they are reported below as conjectures because , while they describe a systematic behaviour that emerges from the dags whose sizes we have a complete enumeration for , we were not able to substantiate them with formal proofs .",
    "[ conj : unc ] arcs that are not incident on a common node are uncorrelated .",
    "this is a consequence of the fact that if we consider @xmath178 and @xmath219 with @xmath220 , we have @xmath221 .",
    "therefore @xmath222 .",
    "this property seems to generalise to dags with more than @xmath182 nodes .",
    "figure [ fig : approxp ] shows approximate estimates for @xmath223 and @xmath224 for dags with @xmath190 to @xmath183 nodes , obtained again from @xmath191 dags generated with uniform probability .",
    "the curves for the two probabilities are overlapping and very close to each other for all the considered dag sizes , thus supporting conjecture [ conj : unc ] .",
    "the covariance matrix @xmath68 is sparse .",
    "the proportion of arcs incident on a common node converges to zero as the number of nodes increases ; therefore , if we assume conjecture [ conj : unc ] is true , the proportion of elements of @xmath68 that are equal to @xmath117 has limit @xmath225 furthermore , even arcs that are incident on a common node are not strongly correlated .",
    "[ conj : increasing ] both covariance and correlation between two arcs incident on a common node are monotonically increasing in modulus .",
    "the covariance between two arcs incident on a common node takes values in the interval @xmath226 @xmath227 $ ] in modulus , while the correlation takes values in @xmath228 $ ] in modulus .",
    "these intervals can be further reduced to @xmath229 $ ] and @xmath230 @xmath231 $ ] for dags larger than @xmath182 nodes due to conjecture [ conj : increasing ] .    as far as the other two cases are concerned , in the _ minimum entropy _",
    "case we have that @xmath232 as in the _ minimum entropy _ case of ugs .",
    "the _ intermediate entropy _",
    "case again ranges from being very close to the _ minimum entropy _ case ( when the graph structure displays little variability ) to being very close to the _ maximum entropy _ case ( when the graph structure displays substantial variability ) .",
    "the bounds on the eigenvalues of @xmath68 derived in lemma [ thm : dirlambda ] allow a graphical representation of the variability of the network structure , equivalent to the one illustrated in example [ ex : base ] for ugs .",
    "several functions have been proposed in literature as univariate measures of spread of a multivariate distribution , usually under the assumption of multivariate normality ; for some examples see @xcite and @xcite .",
    "three of them in particular can be used as descriptive statistics for the multivariate bernoulli and trinomial distributions : the _ generalised variance _",
    ", @xmath233 the _ total variance _ , @xmath234 and the squared _ frobenius matrix norm _ of the difference between @xmath68 and a target matrix @xmath235 , @xmath236    both generalised variance and total variance associate high values of the statistic to unstable network structures , and are bounded due to the properties of the multivariate bernoulli and trinomial distributions . for total variance , it is easy to show that either @xmath237 $ ] ( for the multivariate bernoulli ) or @xmath238 $ ] ( for the multivariate trinomial ) , due to the bounds on the variances @xmath61 and on the eigenvalues @xmath70 derived in sections [ sec : mvber ] and [ sec : mvtri ] .",
    "generalised variance is similarly bounded due to hadamard s theorem on the determinant of a non - negative definite matrix @xcite : @xmath239 $ ] for the multivariate bernoulli distribution and @xmath240 $ ] for the multivariate trinomial .",
    "they reach the respective maxima in the _ maximum entropy _ case and are equal to zero only in the _ minimum entropy _ case .",
    "generalised variance is also strictly convex , but it is equal to zero when @xmath68 is rank deficient . for this reason it may be convenient to reduce @xmath68 to a smaller ,",
    "full rank matrix ( say @xmath241 ) and consider @xmath242 instead of @xmath243 ; using a regularised estimator for @xmath68 such as the one presented in @xcite is also a viable option .",
    "the behaviour of the squared frobenius matrix norm , on the other hand , depends on the choice of the target matrix @xmath235 . for @xmath244 ( the covariance matrix arising from the _ minimum entropy _",
    "case for both the multivariate bernoulli and the multivariate trinomial ) , @xmath245 associates high values of the statistic to unstable network structures , like @xmath246 and @xmath243 ; however , @xmath247 does not have a unique maximum and none of its maxima corresponds to the _ maximum entropy _ case , making its interpretation unclear .",
    "a better choice seems to be a multiple of the covariance matrix arising from the _ maximum entropy _ case , say @xmath248 , associating high values of @xmath249 to stable network structures . for the multivariate bernoulli ,",
    "if we let @xmath250 , @xmath251 can be rewritten as @xmath252 it has both a unique global minimum ( because it is a convex function ) , @xmath253 and a unique global maximum , @xmath254 which correspond to the _ maximum _ and _ minimum entropy _ covariance matrices , respectively .",
    "similar results can be derived for the multivariate trinomial distribution , using an approximate estimate for @xmath255 based on the results presented in section [ sec : dagprop ] .",
    "all the descriptive statistics introduced in this section can be normalised as follows : @xmath256 these normalised statistics vary in the @xmath257 $ ] interval and associate high values to graphs whose structures display a high variability .",
    "since they vary on a known and bounded scale , they are easy to interpret as absolute quantities ( i.e. goodness - of - fit statistics ) as well as relative ones ( i.e. proportions of total possible variability ) .    they also have a clear geometric interpretation as distances in @xmath121 , as they can all be rewritten as function of the eigenvalues @xmath67 .",
    "this allows , in turn , to provide an easy interpretation of otherwise complex properties of @xmath13 and @xmath14 and to derive new results .",
    "first of all , the measures introduced in equation [ eqn : normalised ] can be used to select the best learning algorithm @xmath258 in terms of structure stability for a given data set @xmath4 .",
    "different algorithms make use of the information present in the data in different ways , under different sets of assumptions and with varying degrees of robustness .",
    "therefore , in practice different algorithms learn different structures from the same data and , in turn , result in different posterior distributions on @xmath9 .",
    "if we rewrite equation [ eqn : structlearn ] to make this dependence explicit , @xmath259 and denote with @xmath260 the covariance matrix of the distribution of the edges ( or the arcs ) induced by @xmath261 , then we can choose the optimal structure learning algorithm @xmath262 as @xmath263 or , equivalently , using @xmath264 or @xmath265 instead of @xmath266 .",
    "such an algorithm has the desirable property of maximising the information gain from the data , as measured by the distance from the non - informative prior @xmath13 in @xmath121 .",
    "in other words , @xmath262 is the algorithm that uses the data in the most efficient way .",
    "furthermore , an optimal @xmath262 can be identified even for data sets without a `` golden standard '' graph structure to use for comparison ; this is not possible with the approaches commonly used in literature , which rely on variations of hamming distance @xcite and knowledge of such a `` golden standard '' to evaluate learning algorithms ( see , for example * ? ? ?",
    "* ) .    similarly , it is possible to study the influence of different values of a tuning parameter for a given structure learning algorithm ( and again a given data set ) .",
    "such parameters include , for example , restrictions on the degrees of the nodes @xcite and regularisation coefficients @xcite . if we denote these tuning parameters with @xmath267 , we can again choose an optimal @xmath268 as @xmath269    another natural application of the variability measures presented in equation [ eqn : normalised ] is the study of the consistency of structure learning algorithms .",
    "it has been proved in literature that most of structure learning algorithms are increasingly able to identify a single , minimal graph structure as the sample size diverges ( see , for example * ? ? ?",
    "therefore , @xmath14 converges towards the _ minimum entropy _ case and all variability measures converge to zero .",
    "however , convergence speed has never been analysed and compared across different learning algorithms ; any one of @xmath266 , @xmath264 or @xmath265 provides a coherent way to perform such an analysis .",
    "lastly , we may use the variability measures from equation [ eqn : normalised ] as basis to investigate different prior distributions for real - world data modelling and to define new ones .",
    "relatively little attention has been paid in literature to the choice of the prior over @xmath9 , and the uniform _ maximum entropy _",
    "distribution is usually chosen for computational reasons .",
    "its only parameter is the _ imaginary sample size _ , which expresses the weight assigned to the prior distribution as the size of an imaginary sample size supporting it @xcite .",
    "however , choosing a uniform prior also has some drawbacks .",
    "firstly , @xcite and @xcite have shown that both large and small values of the imaginary sample size have unintuitive effects on the sparsity of a bayesian network even for large sample sizes .",
    "for instance , large values of the imaginary sample size may favour the presence of an arc over its absence even when both @xmath13 and @xmath4 imply the variables the arc is incident on are conditionally independent .",
    "secondly , a uniform prior assigns a non - null probability to all possible models .",
    "therefore , it often results in a very flat posterior which is not able discriminate between networks that are well supported by the data and networks that are not @xcite .",
    "following @xcite s suggestion that `` good '' graphical models should be sparse , sparsity - inducing priors such as the ones in @xcite and @xcite should be preferred to the _ maximum entropy _ distribution , as should informative priors @xcite .",
    "for example , the prior proposed in @xcite introduces a prior probability @xmath270 to include ( independently ) each arc in a bayesian network with a given topological ordering , which means @xmath271 and @xmath272 for all @xmath273 in @xmath13 .",
    "thus , @xmath274 , @xmath275 and @xmath276 . the prior proposed in @xcite , on the other hand , controls the number of parents of each node for a given topological ordering .",
    "therefore , it favours low values of @xmath277 in @xmath13 and again @xmath272 for all @xmath273 .",
    "clearly , the amount of sparsity induced by the hyperparameters of these priors determines the variability of both the prior and the posterior , and can be controlled through the variability measures from equation [ eqn : normalised ] .",
    "furthermore , these measures can provide inspiration in devising new priors with the desired form and amount of sparsity .",
    "bayesian inference on the structure of graphical models is challenging in most situations due to the difficulties in defining and analysing prior and posterior distributions over the spaces of undirected or directed acyclic graphs .",
    "the dimension of these spaces grows super - exponentially in the number of variables considered in the model , making even map analyses problematic .    in this paper",
    ", we propose an alternative approach to the analysis of graph structures which focuses on the set of possible edges @xmath11 of a graphical model @xmath278 instead of the possible graph structures themselves .",
    "the latter are uniquely identified by the respective edge sets ; therefore , the proposed approach integrates smoothly with and extends both frequentist and bayesian results present in literature .",
    "furthermore , this change in focus provides additional insights on the behaviour of individual edges ( which are usually the focus of inference ) and reduces the dimension of the sample space from super - exponential to quadratic in the number of variables .    for many inference problems",
    "the parameter space is reduced as well , and makes complex inferential tasks feasible . as an example , we characterise several measures of structural variability for both bayesian and markov networks using the second order moments of @xmath13 and @xmath14 .",
    "these measures have several possible applications and are easy to interpret from both an algebraic and a geometric point of view .",
    "the author would like to thank to adriana brogini ( university of padova ) and david balding ( university college london ) for proofreading this article and providing many useful comments and suggestions .",
    "furthermore , the author would also like to thank giovanni andreatta and luigi salce ( university of padova ) for their assistance in the development of the material .",
    "since @xmath68 is a real , symmetric , non - negative definite matrix , its eigenvalues @xmath70 are non - negative real numbers ; this proves the lower bound in both inequalities .",
    "the upper bound in the first inequality holds because @xmath279 as the sum of the eigenvalues is equal to the trace of @xmath68 .",
    "this in turn implies @xmath280 which completes the proof .",
    "it is easy to show that each @xmath107 , with @xmath281 and @xmath282 .",
    "it follows that the parameter collection @xmath83 of @xmath283 reduces to @xmath284 after the transformation . therefore , @xmath285 is a uniquely identified multivariate bernoulli random variable according to the definition introduced at the beginning of section [ sec : mvber ] .",
    "let s assume by contradiction that @xmath162 is cyclic ; this implies that there are one or more nodes @xmath286 such that @xmath287 for some @xmath288 .",
    "however , this would mean that in @xmath289 we would have @xmath290 which is not possible since @xmath289 is assumed to be acyclic .",
    "each possible arc can appear in the graph in only one direction at a time , so a directed acyclic graph with @xmath185 nodes can have at most @xmath291 arcs .",
    "therefore @xmath292 but in the _ maximum entropy _ case we also have that @xmath293 , so @xmath294 which completes the proof .    in the maximum entropy case ,",
    "all arcs have the same marginal distribution function , @xmath295 \\\\      & \\frac{1}{4 } + \\frac{1}{4(n - 1 ) }   & & \\text{in } ( -1 , 0 ] \\\\      & \\frac{3}{4 } - \\frac{1}{4(n - 1 ) }   & & \\text{in } ( 0 , 1 ] \\\\      & 1                                  & & \\text{in } ( 1 , + \\infty )      \\end{aligned }    \\right.,\\ ] ] so the joint distribution of any pair of arcs @xmath29 and @xmath208 can be written as a member of the farlie - morgenstern - gumbel family of distribution as @xmath296 .",
    "\\end{aligned}\\ ] ]    then if we apply hoeffding s identity from equation [ eqn : hoeffding ] and replace the joint distribution function @xmath297 with the right hand of equation [ eqn : this ] we have that @xmath298   -   f_a(a_{ij})f_a(a_{kl } ) \\right| \\\\      &   = \\sum_{\\{-1 , 0\\}}\\sum_{\\{-1 , 0\\ } } ( 1 - f_a(a_{ij}))(1 - f_a(a_{kl } ) ) .",
    "\\end{aligned}\\ ] ]    we can now compute the bounds for @xmath299 and @xmath300 using only the marginal distribution function @xmath301 from equation [ eqn : distrfun ] and the variance from equation [ eqn : approxvar ] , thus obtaining the expressions in equation [ eqn : boundcov ] and equation [ eqn : boundcor ] .",
    "below are reported the exact values of the parameters of the marginal trinomial distributions and of the first and second order moments of the multivariate trinomial distribution in the maximum entropy case .",
    "all these quantities have been computed by a complete enumeration of the directed acyclic graphs of a given size ( @xmath189 , @xmath179 , @xmath180 , @xmath181 and @xmath182 ) ."
  ],
  "abstract_text": [
    "<S> graphical model learning and inference are often performed using bayesian techniques . in particular , learning is usually performed in two separate steps . </S>",
    "<S> first , the graph structure is learned from the data ; then the parameters of the model are estimated conditional on that graph structure . while the probability distributions involved in this second step have been studied in depth , </S>",
    "<S> the ones used in the first step have not been explored in as much detail .    in this paper , we will study the prior and posterior distributions defined over the space of the graph structures for the purpose of learning the structure of a graphical model . </S>",
    "<S> in particular , we will provide a characterisation of the behaviour of those distributions as a function of the possible edges of the graph . </S>",
    "<S> we will then use the properties resulting from this characterisation to define measures of structural variability for both bayesian and markov networks , and we will point out some of their possible applications .     </S>",
    "<S> marco scutari + genetics institute , university college london , united kingdom + m.scutari@ucl.ac.uk    graphical models @xcite stand out among other classes of statistical models because of their use of graph structures in modelling and performing inference on multivariate , high - dimensional data . </S>",
    "<S> the close relationship between their probabilistic properties and the topology of the underlying graphs represents one of their key features , as it allows an intuitive understanding of otherwise complex models .    in a bayesian </S>",
    "<S> setting , this duality leads naturally to split model estimation ( which is usually called _ learning _ ) in two separate steps @xcite . in the first step , </S>",
    "<S> called _ structure learning _ , the graph structure @xmath0 of the model is estimated from the data . </S>",
    "<S> the presence ( absence ) of a particular edge between two nodes in @xmath0 implies the conditional ( in)dependence of the variables corresponding to such nodes . in the second step , called _ parameter learning _ , the parameters @xmath1 of the distribution assumed for the data are estimated conditional to the graph structure obtained in the first step . </S>",
    "<S> if we denote a graphical model with @xmath2 , so that @xmath3 , then we can write graphical model estimation from a data set @xmath4 as @xmath5 furthermore , following @xcite , we can rewrite structure learning as @xmath6 the prior distribution @xmath7 and the corresponding posterior distribution @xmath8 are defined over the space of the possible graph structures , say @xmath9 . since the dimension of @xmath9 grows super - exponentially with the number of nodes in the graph @xcite , it is common practice to choose @xmath10 as a non - informative prior , and then to search for the graph structure @xmath0 that maximises @xmath8 . </S>",
    "<S> unlike such a _ maximum a posteriori _ ( map ) approach , a full bayesian analysis is computationally unfeasible in most real - world settings @xcite . </S>",
    "<S> therefore , inference on most aspects of @xmath7 and @xmath8 is severely limited by the nature of the graph space .    in this paper </S>",
    "<S> , we approach the analysis of those probability distributions from a different angle . we start from the consideration that , in a graphical model , the presence of particular edges and their layout are the most interesting features of the graph structure . therefore , investigating @xmath7 and @xmath8 through the probability distribution they induce over the set @xmath11 of their possible edges ( identified by the set of unordered pairs of nodes in @xmath0 ) provides a better basis from which to develop bayesian inference on @xmath0 . </S>",
    "<S> this can be achieved by modelling @xmath11 as a multivariate discrete distribution encoding the joint state of the edges . </S>",
    "<S> then , as far as inference on @xmath0 is concerned , we may rewrite equation [ eqn : structlearn ] as @xmath12 as a side effect , this shift in focus reduces the effective dimension of the sample space under consideration from super - exponential ( the dimension of @xmath9 ) to polynomial ( the dimension @xmath11 ) in the number of nodes . </S>",
    "<S> the dimension of the parameter space for many inferential tasks , such as the variability measures studied in this paper , is likewise reduced .    </S>",
    "<S> the content of the paper is organised as follows . </S>",
    "<S> basic definitions and notations are introduced in section [ sec : definitions ] . </S>",
    "<S> the multivariate distributions used to model @xmath11 are described in section [ sec : distributions ] . </S>",
    "<S> some properties of the prior and posterior distributions on the graph space , @xmath13 and @xmath14 , are derived in section [ sec : properties ] . </S>",
    "<S> we will focus mainly on those properties related with the first and second order moments of the distribution of @xmath11 , and we will use them to characterise several measures of structural variability in section [ sec : variability ] . </S>",
    "<S> these measures may be useful for several inferential tasks for both bayesian and markov networks ; some will be sketched in section [ sec : variability ] . </S>",
    "<S> conclusions are summarised in section [ sec : conclusion ] , and proofs for the theorems in sections [ sec : distributions ] to [ sec : variability ] are reported in appendix [ app : proofs ] . </S>",
    "<S> appendix [ app : numbers ] lists the exact values for some quantities of interest for @xmath13 , computed for several graph sizes . </S>"
  ]
}