{
  "article_text": [
    "retrospective estimates of influenza activity ( ili activity level , as reported by the cdc ) were produced using our model , argo , for the time period of 2009 - 03 - 29 to 2015 - 07 - 11 , assuming we had access only to the historical cdc s ili reports up to the previous week of estimation .",
    "we compared argo s estimates with the ground truth : the cdc - reported weighted ili activity level , published typically with one or two weeks delay , by calculating a collection of accuracy metrics described in the materials section .",
    "these metrics include the root mean squared error ( rmse ) , mean absolute error ( mae ) , mean absolute percentage error ( mape ) , correlation with estimation target , and correlation of increment with estimation target . for comparison , we calculated these accuracy metrics for ( a ) gft estimates ( accessed on 2015 - 07 - 11 ) , ( b ) estimates produced using the method of santillana et al .",
    "2014 @xcite , ( c ) estimates produced by combining gft with an ar(3 ) autoregressive model @xcite , ( d ) estimates produced with an ar(3 ) autoregressive model @xcite , and ( e ) a naive method that simply uses the value of the prior week s cdc s ili activity level as the estimate for the current one . for fair comparison , all benchmark models ( b  d )",
    "are dynamically trained with a two - year moving window .",
    "table 1 summarizes these accuracy metrics for all estimation methods for multiple time periods .",
    "the first column shows that argo s estimates outperform all other alternatives , in every accuracy metric for the whole time period .",
    "the other columns of table 1 show the performance of all the methods for the 2009 off - season h1n1 flu outbreak , and each regular flu season since 2010 .",
    "the panels of figure [ fig : all_pred ] display the estimates against the observed cdc - reported ili activity level .",
    "close inspection shows that , in the post-2009 regular flu seasons , argo uniformly outperformed all other alternative estimation methods in terms of root mean squared error , mean absolute error , mean absolute percentage error , and correlation .",
    "argo avoids the notorious over - shooting problem of gft , as seen in figure [ fig : all_pred ] . during the 2009 off - season",
    "h1n1 flu outbreak , argo had the smallest mean absolute percentage error . in terms of root",
    "mean squared error and mean absolute error , argo ( relative rmse = 0.640 , relative mae = 0.584 ) had the second best performance , under - performing slightly only to gft+ar(3 ) model ( relative rmse = 0.580 , relative mae = 0.570 ) . in terms of correlation ,",
    "argo ( r=98.5% ) had similar performance to ( the potentially in - sample data of ) gft ( r=98.9% ) @xcite and gft+ar(3 ) model ( r=98.6% ) , while outperforming all the other alternatives .    to assess the statistical significance of the improved prediction power of argo",
    ", we constructed a 95% confidence interval for the relative efficiency of argo compared to other benchmark methods .",
    "the relative efficiency of method 1 to method 2 is the ratio of the true mean squared error of method 2 to that of method 1 @xcite , which can be estimated by its observed value ( see eq ) ; its confidence interval can be constructed by stationary bootstrap of the error residual time series @xcite .",
    "table [ tab : relative_efficiency ] shows that argo is estimated to be at least twice as efficient as any other alternative and the improvement in accuracy is highly statistically significant .",
    "it is well - known that cdc reports undergo revisions , weeks after their initial publication , that respond to internal consistency checks and lead to more accurate estimates of patients with ili symptoms seeking medical attention .",
    "thus , the available historical cdc information , in a given week , is not necessarily as accurate as it will be .",
    "we tested the effect of using ( potentially inaccurate ) unrevised information by obtaining the historical unrevised and revised reports , and the dates when the reports were revised , from the cdc website for the time period of our study .",
    "we used only the information that would have been available to us , at the time of estimation , and produced a time series of estimates for the whole time period described before .",
    "we compared our estimates to all other methods and found that argo still outperformed them all .",
    "moreover , the values of all five accuracy metrics for argo essentially did not change , suggesting a desirable robustness to revisions in cdc s ili activity reports .",
    "the results are shown in table s1 in the supporting information .",
    "we faced an additional challenge in producing real - time estimates for the latest portion of the 2014 - 2015 flu season . at the time of writing",
    "this article , the only data available to us for the week of march 28 , 2015 and later came from the _ google trends _ website .",
    "the information from _ google trends _ has even lower quality than from _ google correlate _ and changes every week .",
    "these undesired changes affected the quality of our estimates . in order to assess the stability of argo in the presence of these variations in the data",
    ", we obtained the search frequencies of the same query terms from _ google trends _",
    "website on 25 different days during the month of april 2015 , and produced a set of 25 historical estimates using argo .",
    "the results of the accuracy metrics associated to these estimates are shown in table s2 in the supporting information .",
    "this table shows that , despite the observed variation in the _ google trends _ data , argo is threefold more stable than the method of @xcite , and still outperforms on average any other method .",
    ".estimate of relative efficiency of argo compared to other models with 95% confidence interval ( ci ) .",
    "relative efficiency being larger than one suggests increased predictive power of argo compared to the alternative method .",
    "[ cols=\">,>,>\",options=\"header \" , ]",
    "the results presented here demonstrate the superiority of our approach both in terms of accuracy and robustness , when compared to all existing flu tracking models based on google searches .",
    "the value of these results is even higher given the fact that they were produced with low quality input variables .",
    "it is highly likely that our methodology would lead to even more accurate results if we were given access to the input variables that google uses to calculate their estimates .",
    "the combination of seasonal flu information with dynamic reweighting of search information , appears to be a key factor in the enhanced accuracy of argo .",
    "the level of ili activity last week typically has a significant effect on the current level of ili activity , and ili activity half a year ago and/or one year ago could provide further information , as shown in figure s1 of the supporting information , which reflects a strong temporal auto - correlation .",
    "the integration of time series information leads to a smooth and continuous estimation curve and prevents undesired spikes .",
    "however , simply adding gft to an autoregressive model is suboptimal compared to argo , because simply treating gft as an individual variable is incapable to adjust for time series information at the resolution of individual query terms , and many terms included in gft may no longer provide extra information once time series information is incorporated . in fact , once the time series information is included , fewer google search query terms remain significant .",
    "for example , among 100 _ google correlate _ query terms , argo selected 14 terms on average each week , whereas the method of @xcite and gft @xcite selected 38 and 45 terms each week on average , respectively .",
    "the combination of argo s smoothness and sparsity lead to a substantial reduction on the estimation error , as observed in tables 1 and 2 , where argo shows improved performance in all evaluation metrics over the whole time period and is twice as efficient as gft+ar(3 ) .",
    "our methodology allows us to transparently understand how google search information and historical flu information complement one another .",
    "time series models tend to be slow in response to sudden observed changes in cdc s ili activity level .",
    "the ar(3 ) model shows this `` delaying '' effect , despite its seemingly good correlation .",
    "google searches , on the other hand , are better at detecting sudden ili activity changes , but are also very sensitive to public s over - reaction .    to investigate further the responsiveness ( co - movement ) of argo towards the change in ili activity , we calculated the correlation of increment between each estimation model and cdc s ili activity level .",
    "the correlation of increment between two time series @xmath2 and @xmath3 is defined as @xmath4 , which measures how well @xmath2 captures the changes in @xmath3 .",
    "table 1 shows that argo has similar capability in capturing the changes in ili level to that of gft and the method of @xcite , while outperforming the time series model ar(3 ) uniformly .",
    "time series information ( seasonality ) tends to pull argo s estimate towards the historical level .",
    "this was evident at the onset of the off - season h1n1 flu outbreak ( week ending at 05/02/2009 ) , which resulted in argo s under - estimation .",
    "argo self - corrected its performance the following week by shifting a portion of model weights from the time series domain to the google searches domain .",
    "inversely , at the height of 2012 - 13 season , argo , gft and the method of @xcite all missed the peak due to an unprecedented surge of search activity .",
    "argo achieved the fastest self - correction by redistributing the weights not only across google terms but also across time series terms , missing the peak by only 1 week , as opposed to 2 weeks for @xcite and about 4 weeks for gft .",
    "it is important to note that while we have used cdc s ili as our gold standard for influenza activity in the us population , and data from google correlate / trends as our independent variables , our methodology can be immediately adapted to any other suitable ili gold standard and/or set of independent variables .",
    "while argo displays a clear superiority over previous methods , it is not fail - proof .",
    "since it relies on the public s search behavior , any abrupt changes to the inner works of the search engine or any changes in the way health - related search information is displayed to users will affect the accuracy of our methodology @xcite .",
    "we expect that argo will be fast at correcting itself if any such change takes place in the future . as in any predictive method ,",
    "the quality of past performance does not guarantee the quality of future performance . in this article",
    ", we fixed the search query terms after 2010 so as to directly compare our results with gft , which kept the same query terms since 2010 ; future application of argo may update search terms more frequently .",
    "argo can be easily generalized to any temporal and spatial scales for a variety of diseases or social events amenable to be tracked by internet searches or services @xcite .",
    "further improvements in influenza prediction may come from combining multiple predictors constructed from disparate data sources @xcite .",
    "after the submission of this article , google announced that gft would be discontinued and that their raw data would be made accessible to selected scientific teams .",
    "this announcement happened soon after the gft team published a manuscript that proposed a new time - series based method for the ( now discontinued ) gft engine @xcite .",
    "this new development makes our contribution timely and useful in providing a transparent method for disease tracking in the future .",
    "all data used in this article are publicly available .",
    "therefore , irb approval is not needed .       to avoid forward - looking information in our out - of - sample predictions , and to make the search term selection in our approach consistent with the main revision to gft @xcite immediately after the h1n1 pandemic",
    ", we obtained the highest correlated terms to the cdc s ili using google correlate ( www.google.com/trends/correlate ) for two different time periods .",
    "for the first time period ( pre - h1n1 period ) , we inserted only cdc s ili data from jan 2004 to march 28 , 2009 into google correlate , and used the resulting most highly correlated search terms as independent variables for our out - of - sample predictions for the time period april 4 , 2009 - may 22 , 2010 .",
    "for the second time period ( post - h1n1 ) , we inserted only cdc s ili data from jan 2004 to may 22 , 2010 into google correlate to select new search terms as done in @xcite .",
    "these last search terms were used as independent variables for all subsequent predictions presented in this work .",
    "tables s4 and s5 in the supporting information show all query terms identified . for the pre - h1n1 period ( the first time period ) ,",
    "the terms from google correlate include spurious ( or over - fitted ) terms like `` march vacation '' or `` basketball standings '' , as discussed in @xcite .",
    "however , figure s1 in the supporting information shows that these spurious terms were often not selected by argo , i.e. , argo would give them zero weights , demonstrating its robustness . for the post - h1n1 time period ,",
    "the updated query terms from google correlate include mostly flu - related terms ( see table s5 in supporting information ) .",
    "this suggests that spurious terms were `` filtered - out '' by including off - season flu data . for the time period of march 28 , 2015 up to the date of submission of this article ,",
    "we acquired search frequencies for this set of query terms from google trends ( www.google.com/trends . date of access : july 11 , 2015 ) as google correlate only provides data up to march 28 , 2015 at the time of writing this article .",
    "google correlate standardizes the search volume of each query to have mean zero and standard deviation one across time and contains data only from 2004 to mar 2015 . to make google correlate data compatible with google trends data , we linearly transformed the google correlate data to the same scale of 0 to 100 in our analysis .",
    "we used google correlate data up to its last available date , and then switched to google trends data afterwards .",
    "this is indicated in figure [ fig : all_pred ] by different shades of the background .",
    "we used the latest version of google flu trends ( 4th version , revised in oct 2014 ) weekly estimates of ili activity level as one of our comparison methods .",
    "gft is available at www.google.org/flutrends/us/data.txt ( date of access : 2015 - 07 - 11 ) .",
    "we use the weighted version of cdc s ili activity level as the estimation target ( available at gis.cdc.gov/grasp/fluview/fluportaldashboard.html .",
    "date of access : 2015 - 07 - 11 ) .",
    "the weekly revisions of cdc s ili are available at the cdc website for all recorded seasons ( from week 40 of a given year to week 20 of the subsequent year ) .",
    "for example , ili report revision at week 50 of season 2012 - 13 is available at www.cdc.gov/flu/weekly/weeklyarchives2012-2013/data/senallregt50.htm ; ili report revision at week 9 of season 2014 - 15 is available at www.cdc.gov/flu/weekly/weeklyarchives2014-2015/data/senallregt09.html .",
    "our model argo is motivated by a hidden markov model .",
    "the _ logit_-transformed cdc - reported ili activity level @xmath5 is the intrinsic time series of interest .",
    "we impose an autoregressive ( ar ) model with lag @xmath6 on it , which implies that the collection of vectors @xmath7 is a markov chain ( this captures the clinical fact that flu lasts for a period , but not indefinitely ) .",
    "the vector of _",
    "log_-transformed normalized volume of google search queries at time @xmath8 , @xmath9 , depends only on the ili activity at the same time , @xmath10 ( this follows the intuition that flu occurrence causes people to search flu related information online ) . the markovian property on block @xmath11 leads to the ( vector ) hidden markov model structure . @xmath12 our formal mathematical assumptions are : + ( 1 ) @xmath13 + ( 2 ) @xmath14 + ( 3 ) conditional on @xmath10 , @xmath15 is independent of @xmath16 + where @xmath17 , and @xmath18 is the covariance matrix . to make the variables more normal",
    ", we transform the original ili activity level @xmath19 from @xmath20 $ ] to @xmath21 using the logit function , obtaining the @xmath10 , and transform the google search volumes from @xmath22 $ ] to @xmath21 using the log function , obtaining the @xmath15 .",
    "the log function is appropriate because google search frequencies usually have exponential growth rate near peaks and are artificially scaled to @xmath22 $ ] by dividing the running maximum . since _",
    "google trends _ is in integer scale from 0 to 100 , we add a small number @xmath23 before the transformation to avoid taking the log of @xmath24 .",
    "the predictive distribution @xmath25 is normal with mean linear in @xmath26 and @xmath15 and constant variance ( see the supporting information ) .",
    "this observation leads to equation below , which defines the argo model .",
    "let @xmath27 be the _",
    "logit_-transformed cdc s ( weighted ) ili activity level @xmath19 at time @xmath8 , and @xmath28 the _ log_-transfomred google search frequency of term @xmath29 at time @xmath8 .",
    "our argo model is given by @xmath30 where @xmath15 can be thought as the exogenous variables to time series @xmath5 .",
    "we chose @xmath31 ( weeks ) to capture the within - year seasonality in ili activity , and @xmath32 ( google search terms ) following the data availability from _",
    "google correlate_. since we have more independent variables than the number of observations , the usual maximum likelihood estimate ( ordinary least squares ) method will fail .",
    "therefore , we impose regularities for parameter estimation . in general",
    "we have three kinds of penalties , @xmath0 penalty @xcite , @xmath1 penalty @xcite , and a linear combination of @xmath0 and @xmath1 penalties @xcite .",
    "all parameters are dynamically trained every week with a 2-year ( 104 weeks ) rolling window .    in a given week , the goal is to find parameters @xmath33 , @xmath34 , and @xmath35 that minimize @xmath36 where @xmath37 are hyper - parameters .",
    "ideally , we would like to use cross - validation to select all 4 hyper - parameters .",
    "however , since we have only 104 training data points at a given week due to the two - year moving window , the cross - validation result is highly noisy .",
    "thus , we need to pre - specify some of the hyper - parameters . for model simplicity and sparsity , combining with the evidence seen from cross - validation , we set @xmath38 , leading to @xmath0 penalization on both autoregressive and google search terms . with the remaining @xmath39 and @xmath40 , the cross - validation results still have considerable variance . by the same sparsity and simplicity consideration , we further constrained @xmath41 .",
    "therefore , the argo model we finally propose is equation with constraint @xmath42 and @xmath43 .",
    "a detailed discussion of our specification of the hyper - parameters is provided in the supporting information .",
    "the root mean squared error ( rmse ) , mean absolute error ( mae ) , and mean absolute percentage error ( mape ) of estimator @xmath44 to the target ili activity level @xmath45 are defined , respectively , as @xmath46 , @xmath47 , @xmath48 .",
    "the correlation of estimator @xmath44 to the target ili activity level @xmath45 is their sample correlation coefficient .",
    "the correlation of increment between @xmath49 and @xmath19 is defined as + @xmath50 .",
    "the relative efficiency of estimator @xmath51 to estimator @xmath52 is @xmath53 , where @xmath54 $ ] , which can be estimated by @xmath55 the 95% confidence interval can be constructed by time series stationary bootstrap method @xcite , where the replicated time series of the error residual is generated using geometrically distributed random blocks with mean length 52 ( which corresponds to one year ) .",
    "we obtain the basic bootstrap confidence interval for @xmath56 and then recover the original scale by exponentiation .",
    "the non - parametric bootstrap confidence interval takes the autocorrelation and cross - correlation of the errors into account , and is insensitive to the mean block length .",
    "s. c. kou s research is supported in part by nsf grant dms-1510446 .    10    ginsberg j et  al .",
    "( 2009 ) detecting influenza epidemics using search engine query data .",
    "457:10121014 .",
    "polgreen pm , chen y , pennock dm , nelson fd , weinstein ra ( 2008 ) using internet searches for influenza surveillance .",
    "47(11):14431448 .",
    "yuan q et  al .",
    "( 2013 ) monitoring influenza epidemics in china with search query from baidu . 8(5):e64323 .",
    "paul mj , dredze m , broniatowski d ( 2014 ) twitter improves influenza forecasting . .",
    "mciver dj , brownstein js ( 2014 ) wikipedia usage estimates prevalence of influenza - like illness in the united states in near real - time .",
    "10(4):e1003581 .",
    "santillana m , nsoesie eo , mekaru sr , scales d , brownstein js ( 2014 ) using clinicians search query data to monitor influenza epidemics .",
    "59(10):14461450 .",
    "wesolowski a et  al .",
    "( 2014 ) commentary : containing the ebola outbreak  the potential and challenge of mobile network data . .",
    "chan eh , sahai v , conrad c , brownstein js ( 2011 ) using web search query data to monitor dengue epidemics : a new model for neglected tropical disease surveillance . 5(5):e1206 .",
    "preis t , moat hs , stanley he ( 2013 ) quantifying trading behavior in financial markets using google trends .",
    "bollen j , mao h , zeng x ( 2011 ) twitter mood predicts the stock market .",
    "2(1):18 .",
    "wu l , brynjolfsson e ( 2015 ) _ the future of prediction : how google searches foreshadow housing prices and sales_. in _ economic analysis of the digital economy _",
    "avi  goldfarb sg , tucker c. ( university of chicago press ) , pp .",
    "89118 .",
    "helft m ( 2008 ) google uses searches to track flu s spread ( the new york times ) .",
    "butler d ( 2013 ) when google got flu wrong . 494(7436):155 .",
    "cook s , conrad c , fowlkes al , mohebbi mh ( 2011 ) assessing google flu trends performance in the united states during the 2009 influenza virus a ( h1n1 ) pandemic . 6(8):e23610 .",
    "lazer d , kennedy r , king g , vespignani a ( 2014 ) the parable of google flu : traps in big data analysis",
    ". 343(6176):12031205 .",
    "santillana m , zhang dw , althouse bm , ayers jw ( 2014 ) what can digital disease detection learn from ( an external revision to ) google flu trends ? 47(3):341347 .",
    "stefansen c ( 2014 ) google flu trends gets a brand new engine .",
    "googleresearch.blogspot.com/2014/10/google-flu-trends-gets-brand-new-engine.html .",
    "\\(2014 ) influenza ( seasonal ) , fact sheet number 211 . accessed april , 2015 .",
    "shaman j , karspeck a ( 2012 ) forecasting seasonal outbreaks of influenza .",
    "109(50):2042520430 .",
    "lipsitch m , finelli l , heffernan rt , leung gm , redd sc ( 2011 ) improving the evidence base for decision making during a pandemic : the example of 2009 influenza a / h1n1 .",
    "9(2):89115 .",
    "nsoesie eo , brownstein js , ramakrishnan n , marathe mv ( 2014 ) a systematic review of studies on forecasting the dynamics of influenza outbreaks .",
    "8(3):309316 .",
    "chretien jp , george d , shaman j , chitale ra , mckenzie fe ( 2014 ) influenza forecasting in human populations : a scoping review . 9(4):e94130 .",
    "nsoesie e , mararthe m , brownstein j ( 2013 ) forecasting peaks of seasonal influenza epidemics .",
    "soebiyanto rp , adimi f , kiang rk ( 2010 ) modeling and predicting seasonal influenza transmission in warm regions using climatological parameters .",
    "5(3):e9450 .",
    "shaman j , karspeck a , yang w , tamerius j , lipsitch m ( 2013 ) real - time influenza forecasts during the 20122013 season .",
    "yang w , lipsitch m , shaman j ( 2015 ) inference of seasonal and pandemic influenza transmission dynamics .",
    "112(9):27232728 .",
    "paolotti d et  al .",
    "( 2014 ) web - based participatory surveillance of infectious diseases : the influenzanet participatory surveillance experience .",
    "20(1):1721 .",
    "dalton c et  al .",
    "( 2009 ) flutracking : a weekly australian community online survey of influenza - like illness in 2006 , 2007 and 2008 .",
    "33(3):31622 .",
    "smolinski ms et  al .",
    "( 2015 ) flu near you : crowdsourced symptom reporting spanning two influenza seasons .",
    "( 0 ) e1-e7 .",
    "althouse bm , ng yy , cummings da ( 2011 ) prediction of dengue incidence using search query surveillance .",
    "5(8):e1258 .",
    "ocampo aj , chunara r , brownstein js ( 2013 ) using search queries for malaria surveillance , thailand .",
    "12(1):390 .",
    "scarpino sv , dimitrov nb , meyers la ( 2012 ) optimizing provider recruitment for influenza surveillance networks .",
    "8(4):e1002472 .",
    "davidson mw , haim da , radin jm ( 2015 ) using networks to combine `` big data '' and traditional surveillance to improve influenza predictions . 5 .",
    "burkom hs , murphy sp , shmueli g ( 2007 ) automated time series forecasting for biosurveillance .",
    "26(22):42024218 .",
    "everitt bs , skrondal a ( 2002 ) the cambridge dictionary of statistics . .",
    "politis dn , romano jp ( 1994 ) the stationary bootstrap .",
    "89(428):13031313 .",
    "tsukayama h ( 2014 ) google is testing live - video medical advice ( the washington post ) .",
    "accessed on april 20 , 2015 .",
    "gianatasio d ( 2014 ) how this agency cleverly stopped people from googling their medical symptoms : the right ads at the right time ( adweek , online ) .",
    "accessed on april 20 , 2015 .",
    "yang ac , tsai sj , huang ne , peng ck ( 2011 ) association of internet search trends with suicide death in taipei city , taiwan , 20042009 . 132(1):179184",
    ".    cavazos - rehg pa et  al .",
    "( 2014 ) monitoring of non - cigarette tobacco use using google trends . .",
    "tibshirani r ( 1996 ) regression shrinkage and selection via the lasso .",
    "58(1):267288 .",
    "hoerl ae , kennard rw ( 1970 ) ridge regression : biased estimation for nonorthogonal problems .",
    "12(1):5567 .",
    "zou h , hastie t ( 2005 ) regularization and variable selection via the elastic net .",
    "67(2):301320 .",
    "lampos v , miller ac , crossan s , stefansen c ( 2015 ) advances in nowcasting influenza - like illness rates using search query logs . .",
    "5:12760    santillana m , nguyen at , dredze m , paul mj , nsoesie e , brownstein js ( 2015 ) combining search , social media , and traditional data sources to improve influenza surveillance , 11(10):e1004513    * supporting information *",
    "details of our methodology are presented as follows .",
    "first , the predictive distribution in the formulation of the argo model and the corresponding assumptions are described ; second , the statistical strategy to determine the hyper  parameters of the argo model is explained ; third , the results of two sensitivity analysis aimed at testing the robustness of the argo methodology(a ) with respect to subsequent revisions of cdc s ili activity reports , and ( b ) with respect to observed variation of the input variables coming from _ google trends _ data  are presented ; fourth , the exact search query terms identified by google correlate with different data access dates are presented ; fifth , a heatmap showing the coefficients for the time series and google search terms dynamically trained by argo is included .",
    "to improve normality for both the input variables and the dependent variables , the cdc - reported ili activity level was _",
    "logit_transformed , and the linearly normalized volume of google search queries were _ log_transformed . to avoid taking the log of 0 , we add a small number @xmath57 before the log - transformation .",
    "these transformations led to two sets of variables , the intrinsic ( influenza epidemics activity ) time series of interest @xmath58 , and the ( google search ) variable vector @xmath9 at time @xmath8 ( that depends only on @xmath10 ) , respectively .",
    "our formal mathematical assumptions are :    1 .",
    "@xmath13 2 .",
    "@xmath14 3 .",
    "conditional on @xmath10 , @xmath15 is independent of @xmath16    where @xmath59 , and @xmath18 is the covariance matrix . the predictive distribution @xmath60 is given by @xmath61 which is a normal distribution , whose mean is a linear combination of @xmath26 and @xmath15 , and whose variance is a constant .",
    "the optimized parameters of the argo model , @xmath33 , @xmath62 , @xmath63 are obtained by    @xmath64    the training period consists of a two  year ( @xmath65 weeks ) rolling window that immediately precedes the desired date of estimation .",
    "the hyper ",
    "parameters are @xmath37 .",
    "we tested the performance of argo with the following specifications of hyper  parameters :    1 .",
    "restrict @xmath38 and @xmath41 , cross validate on @xmath39 .",
    "this is our proposed argo with the same @xmath0 penalty for google search terms and autoregressive lags.[item : same_l1 ] 2 .",
    "restrict @xmath38 , cross validate on @xmath66 .",
    "this is argo with separate @xmath0 penalties for google search terms and autoregressive lags.[item : sep_l1 ] 3 .",
    "restrict @xmath67 and @xmath68 , cross validate on @xmath69 .",
    "this is argo with the same @xmath1 penalty for google search terms and autoregressive lags.[item : same_l2 ] 4 .",
    "restrict @xmath68 , cross validate on @xmath70 .",
    "this is argo with separate @xmath1 penalties for google search terms and autoregressive lags.[item : sep_l2 ] 5 .",
    "restrict @xmath71 , cross validate on @xmath72 .",
    "this is argo with the same elastic net ( both @xmath0 and @xmath1 ) penalty for google search terms and autoregressive lags.[item : enet ]    table [ tab : hyperpar ] summarizes the in - sample estimation performance for our proposed argo , together with the other specifications of hyper  parameters .",
    "it is apparent from the table that the @xmath0 penalty generally outperforms @xmath1 penalty .",
    "the @xmath0 penalty tends to shrink the coefficients of unnecessary independent variables to be exactly zero , and thus eliminates redundant information ; on the other hand , the @xmath1 penalty can only shrink the coefficients to be close to zero . as a result",
    ", @xmath1 penalized coefficients are not as sparse as their @xmath0 counterparts .",
    "furthermore , from table [ tab : hyperpar ] , we see that argo with separate @xmath0 penalties ( specification [ item : sep_l1 ] ) outperforms argo with separate @xmath1 penalties ( specification [ item : sep_l2 ] ) , in terms of both root mean squared error and mean absolute error .",
    "similarly , argo with the same @xmath0 penalty ( specification [ item : same_l1 ] ) outperforms argo with the same @xmath1 penalty ( specification [ item : same_l2 ] ) , in terms of both root mean squared error and mean absolute error .",
    "the elastic net model , which combines @xmath0 penalty and @xmath1 penalty , does not provide any error reduction . in the cross - validation process of setting @xmath72 for the elastic net model",
    ", 70 weeks out of 116 in - sample weeks showed that the smallest cross - validation mean error when restricting @xmath73 ( i.e. zero @xmath1 penalty ) is within one standard deviation of the global smallest cross - validation mean error , suggesting that restricting @xmath1 penalty term to be zero ( i.e. @xmath73 ) will introduce little bias .",
    "therefore , for the simplicity and sparsity of the model , we drop the @xmath1 penalty terms and use only @xmath0 penalty .",
    "next we want to decide between the remaining two specifications , argo with separate @xmath0 penalties ( specification [ item : sep_l1 ] ) , and argo with the same @xmath0 penalty ( specification [ item : same_l1 ] ) .",
    "one might argue that google search terms and autoregressive lags are different sources of information and thus should have different @xmath0 penalties .",
    "however , empirical evidence in table [ tab : hyperpar ] shows that , again , giving extra flexibility to @xmath66 does not generate improvement compared to fixing @xmath43 . in the cross - validation process of setting @xmath66 for separate @xmath0 penalties ,",
    "99 weeks out of 116 in - sample weeks showed that the smallest cross - validation mean error when restricting @xmath43 ( i.e. same @xmath0 penalty ) is within one standard deviation of the global smallest cross - validation mean error .",
    "this may well be due to the gain from variance reduction when imposing the restriction @xmath43 . based on the same simplicity and sparsity consideration",
    ", we finally decided to restrict @xmath38 and @xmath43 in the setting of hyper ",
    "parameters for argo .",
    "within a flu season , cdc reports are constantly revised to improve their accuracy as new information is incorporated .",
    "thus , cdc s weighted ili figures displayed in previously published reports may change in subsequent weeks . as a consequence , in a given week the available cdc ili information from the most recent weeks may be inaccurate . to test the robustness of argo in the presence of these revisions and",
    "mimic the real - time tracking in our retrospective predictions , we trained argo and all other alternative models based on the following schedule .",
    "suppose @xmath74 is the cdc - reported ili activity level of week @xmath29 accessed at week @xmath75 . since cdc s ili activity report is typically delayed for one week , on week @xmath75 the historical ili activity level data we have is @xmath76 .",
    "due to revisions , ili activity level of week @xmath29 accessed at different weeks @xmath77 may be different but will converge to a finalized value @xmath78 eventually .",
    "hence , to avoid using forward  looking information , in week @xmath75 , we train all models with the ili activity level accessed at that week @xmath76 . in this sense , any future revision beyond week @xmath75 will not be incorporated in the training at week @xmath75 . yet for the accuracy metrics , the estimation target remains the finalized the ili activity level ( @xmath79 ) .",
    "table [ tab : results ] shows the estimation results when using the aforementioned schedule .",
    "note that argo still outperforms all other alternative models .",
    "moreover , the absolute values of all four accuracy metrics for argo trained this way essentially do not change compared to argo trained with finalized ili activity level in the main text , indicating the robustness of argo .    the weekly revisions of cdc s ili activity reports are available at cdc website from week 40 of the year to week 20 of the subsequent year for all seasons studied in this article .",
    "for example , ili activity level revisions at week 50 of season 2012 - 2013 are available at http://www.cdc.gov/flu/weekly/ weeklyarchives2012- 2013/data / senallregt50.htm ; ili activity report revision at week 9 of season 2014 - 2015 is available at http://www.cdc.gov /flu / weekly / weeklyarchives2014 - 2015/data / senallregt09.html ( the webpage has suffix `` htm '' for seasons before 2014 - 2015 and suffix `` html '' for 2014 - 2015 season ) . in this retrospective case study , when the revisions of ili activity level were not available for a particular week during off - season period , the finalized ili activity level was used instead .",
    "_ google trends _ historical data constantly change as a consequence of re - normalizations and algorithm updates .",
    "to study the robustness of argo to _ google trends _ data revisions , we obtained the search frequencies of the search query terms identified by _ google correlate _ on may 22 , 2010 ( see figure 2 in the main text and table [ tab : phrases ] below ) from the _ google trends _ website ( http://www.google.com/trends ) on 25 different days in april 2015 .",
    "we studied the variability of argo s performance when using these 25 different versions of _ google trends _ data as input variables for the common time period of sep 28 , 2014 to mar 29 , 2015 .",
    "we studied the 2014 - 15 flu season only partially ( up to march 2015 ) because this is the longest study period covered by all the obtained versions of _ google trends _ data , at the time ( may 1 , 2015 ) of the first submission of this article .",
    "we want to emphasize that google correlate data were only available up to feb 2014 when accessed in april 2015 .    despite the inevitable variation to the revision of the low - quality data from _ google trends _ ,",
    "argo still achieves considerable stability compared to the method of santillana et al .",
    "@xcite during this time period .",
    "table [ tab : sensi_gt ] suggests that argo is threefold more robust than the method of @xcite .",
    "the incorporation of time series information helps argo achieve the stability . as an extreme example , ar(3 )",
    "model focuses entirely on the time series information and is thus independent of _ google trends _ data revisions .",
    "gft , formulated with the original search variables as inputs , is by construction insensitive to the changes in _ google trends _ data . for this portion of the study",
    ", we included the signal from gft for context only and we treat it as exogenous in our analysis .",
    "based on the results from previous time periods , it is highly likely that if we had access to google s internal raw data ( i.e. , historical search volume for disease - related phrases ) we would have achieved the same stability as well . yet even with these low - quality data , argo outperforms gft uniformly on all versions of data in terms of both root mean squared error and mean absolute error .",
    "tables [ tab : phrases09 ] and [ tab : phrases ] list the search query phrases identified by google correlate as of march 28 , 2009 and of may 22 , 2010 , respectively .",
    "the march 2009 version included spurious terms such as `` college.basketball.standings '' , `` march.vacation '' , `` aloha.ski '' , `` virginia.wrestling '' , etc .",
    "these spurious terms did not appear in the may 2010 version .",
    "figure [ fig : coef ] shows the coefficients for the time series and google search terms dynamically trained by argo via a heatmap .",
    "the level of ili activity last week is seen to have a significant effect on the current level of ili activity , and ili activity half a year ago and/or one year ago could provide further information as the figure shows . among _",
    "google correlate _ query terms , argo selected 14 terms out of 100 on average each week ."
  ],
  "abstract_text": [
    "<S> accurate real - time tracking of influenza outbreaks helps public health officials make timely and meaningful decisions that could save lives . </S>",
    "<S> we propose an influenza tracking model , argo ( autoregression with google search data ) , that uses publicly available online search data . </S>",
    "<S> in addition to having a rigorous statistical foundation , argo outperforms all previously available google - search  based tracking models , including the latest version of google flu trends , even though it uses only low - quality search data as input from publicly available google trends and google correlate websites . </S>",
    "<S> argo not only incorporates the seasonality in influenza epidemics but also captures changes in people s online search behavior over time . </S>",
    "<S> argo is also flexible , self - correcting , robust , and scalable , making it a potentially powerful tool that can be used for real - time tracking of other social events at multiple temporal and spatial resolutions .    </S>",
    "<S> this is the preprint of the paper published at pnas : dx.doi.org/10.1073/pnas.1515373112 . </S>",
    "<S> there are some minor differences between this preprint and the published paper .    </S>",
    "<S> big data sets are constantly generated nowadays as the activities of millions of users are collected from internet - based services . </S>",
    "<S> numerous studies have suggested great potential of these big data sets to detect / manage epidemic outbreaks ( influenza @xcite , ebola @xcite , dengue @xcite ) , predict changes in stock prices @xcite and housing prices @xcite , etc . in 2009 , google flu trends ( gft ) , a digital disease detection system that uses the volume of selected google search terms to estimate current influenza - like illnesses ( ili ) activity , was identified by many as a good example of how big data would transform traditional statistical predictive analysis @xcite . </S>",
    "<S> however , significant discrepancies between gft s flu estimates and those measured by the centers for disease control ( cdc ) in subsequent years led to considerable doubt about the value of digital disease detection systems @xcite . while multiple articles have identified methodological flaws in gft s original algorithm @xcite and </S>",
    "<S> have led to incremental improvements @xcite , a statistical framework that is theoretically sound and capable of accurate estimation is still lacking . here </S>",
    "<S> we present such a framework that culminates in a new method that outperforms all existing methodologies for tracking influenza activity using internet search data .    </S>",
    "<S> influenza outbreaks cause up to 500,000 deaths a year worldwide , and an estimated 3,000 to 50,000 deaths a year in the usa @xcite . </S>",
    "<S> our ability to effectively prepare for and respond to these outbreaks heavily relies on the availability of accurate real - time estimates of their activity . </S>",
    "<S> existing methods to predict the timing , duration and magnitude of flu outbreaks remain limited @xcite . </S>",
    "<S> well - established clinical methods to track flu activity , such as the cdc s ilinet , report the percentage of patients seeking medical attention with ili symptoms ( www.cdc.gov/flu/ ) . </S>",
    "<S> while cdc s % ili is only a proxy of the flu activity in the population , it can help officials allocate resources in preparation for potential surges of patient visits to hospital facilities . </S>",
    "<S> see @xcite for further discussion .    </S>",
    "<S> cdc s ili reports have a delay of one to three weeks due to the time for processing and aggregating clinical information . </S>",
    "<S> this time lag is far from optimal for decision - making purposes . in order to alleviate this information gap , multiple methods combining climate , </S>",
    "<S> demographic and epidemiological data with mathematical models have been proposed for real - time estimation of flu activity @xcite . in recent years </S>",
    "<S> , methods that harness internet - based information have also been proposed , such as google @xcite , yahoo @xcite , and baidu @xcite internet searches , twitter posts @xcite , wikipedia article views @xcite , clinicians queries @xcite , and crowd sourced self - reporting mobile apps such as influenzanet ( europe ) @xcite , flutracking ( australia ) @xcite , and flu near you ( usa ) @xcite . among them , gft has received most attention and has inspired subsequent digital disease detection systems @xcite . </S>",
    "<S> interestingly , google has never made their raw data public , thus , making it impossible to reproduce the exact results of gft .    </S>",
    "<S> we highlight three limitations of the original gft algorithm , previously identified in @xcite . </S>",
    "<S> first , it was shown that a static approach , which does not take advantage of newly available cdc s ili activity reports as the flu season evolves , produced model drift , leading to inaccurate estimates . </S>",
    "<S> second , the idea of aggregating the multiple query terms ( the independent variables in the gft model ) into a single variable did not allow for changes in people s internet search behavior over time ( and thus changes in query terms abilities to track flu ) to be appropriately captured . </S>",
    "<S> third , gft ignored the intrinsic time series properties , such as seasonality of the historical ili activity , thus overlooking potentially crucial information that could help produce accurate real time ili activity estimates .      </S>",
    "<S> the new methodology presented here produces robust and highly accurate ili activity level estimates by addressing the three aforementioned shortcomings of the multiple gft engines . </S>",
    "<S> in addition , we provide a theoretical framework that , for the first time , justifies the prevailing usage of linear models in the digital disease detection literature by incorporating causality arguments through a hidden markov model . </S>",
    "<S> this theoretical framework contains as a special case the model developed in @xcite . </S>",
    "<S> our new model not only achieves the goal of ( a ) dynamically incorporating new information from cdc reports as they become available and ( b ) automatically selecting the most useful google search queries for estimation as in @xcite , but also largely improves estimation by ( c ) including the long - term cyclic information ( seasonality ) from past flu seasons on record as input variables , and ( d ) using a two - year moving window ( which immediately precedes the desired date of estimation ) for the training period to capture the most recent changes in people s search patterns and time series behavior @xcite . </S>",
    "<S> our methodology efficiently builds a prediction model from individual search frequency as well as the past records of ili activity . </S>",
    "<S> it utilizes both sources of information more efficiently than simply combining gft with autoregressive terms as suggested in @xcite , since gft is not optimally aggregated to provide additional information on top of time series information . </S>",
    "<S> furthermore , we provide a quantitative efficiency metric that measures the statistical significance of the improvement of our methodology over other alternatives . for example </S>",
    "<S> , our method is twice as accurate as the method that combines gft with autoregressive terms ( see table [ tab : relative_efficiency ] ) . </S>",
    "<S> finally , even though we use as input only the publicly available , low - quality data from the _ google correlate _ and _ google trends _ websites , our method has significant improvement over the latest version of gft .    </S>",
    "<S> we name our model argo , which stands for autoregression with google search data . statistically speaking </S>",
    "<S> , argo is an autoregressive model with google search queries as exogenous variables ; argo also employs @xmath0 ( and potentially @xmath1 ) regularization in order to achieve automatic selection of the most relevant information . </S>"
  ]
}