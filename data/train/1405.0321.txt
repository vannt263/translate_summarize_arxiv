{
  "article_text": [
    "suppose a random variable @xmath0 has a distribution function @xmath1 with an absolutely continuous density function @xmath2 .",
    "the entropy @xmath3 of the random variable is defined by shannon @xcite to be @xmath4 the non - parametric estimation of @xmath3 has been discussed by many authors , including vasicek @xcite , van es @xcite , ebrahimi _ et al . _",
    "@xcite , correa @xcite and wieczorkowski and grzegorewski @xcite .    among these various entropy estimators ,",
    "vasicek s sample entropy has been most widely used in developing entropy - based goodness - of - fit tests , including ebrahimi _",
    "@xcite , park and park @xcite and alizadeh noughabi @xcite .",
    "vasicek s estimator was based on the fact that the equation ( [ h ] ) can be expressed as @xmath5 the estimate was obtained by replacing the distribution function @xmath6 by the empirical distribution function @xmath7 , and using a difference operator instead of the differential operator .",
    "the derivative of @xmath8 is then estimated by a function of the order statistics .",
    "let @xmath9 be a sample from the distribution @xmath6 .",
    "let @xmath10 denote the order statistics from the sample @xmath11 .",
    "the vasicek s estimator of entropy has a following form @xmath12 where the window size @xmath13 is a positive integer smaller than @xmath14 , @xmath15 if @xmath16 , @xmath17 if @xmath18 .",
    "vasicek proved that his estimator is consistent , i.e. @xmath19 as @xmath20 .",
    "van es @xcite proposed another estimator of entropy and established the consistency and asymptotic normality of this estimator under some conditions .",
    "van es s estimator is given by @xmath21    ebrahimi _ et al .",
    "_ @xcite adjusted the weights of vasicek s estimator , in order to take into account the fact that the differences are truncated around the smallest and the largest data points ( i.e. @xmath22 is replaced by @xmath23 when @xmath24 and @xmath22 is replaced by @xmath25 when @xmath26 ) .",
    "their estimator is given by @xmath27 where @xmath28 they proved that @xmath29 as @xmath20 .",
    "they compared their estimator with vasicek s estimator and by simulation , showed that their estimator has a smaller bias and mean squared error .",
    "correa @xcite proposed a modification of vasicek s estimator . in estimation",
    "the density @xmath30 of @xmath6 in the interval @xmath31 he used a local linear model based on @xmath32 points : @xmath33 .",
    "this yields a following estimator @xmath34 where @xmath35 and @xmath36 he compared his estimator with vasicek s estimator and van es s estimator .",
    "the mse of his estimator is consistently smaller than the mse of vasicek s estimator . also , for some of @xmath13 , his estimator behaves better than the van es s estimator .",
    "wieczorkowski and grzegorzewsky @xcite provided a modification of vasicek estimator and correa estimator .",
    "their estimator is given by @xmath37 where @xmath38 and @xmath39 is the digamma function defined by @xmath40 and for integer arguments @xmath41 where @xmath42 is the euler constant , @xmath43 .",
    "zamanzadeh and arghami @xcite provided two new entropy estimators as follows : @xmath44 where @xmath45 and @xmath46 is @xmath47 and @xmath48 for @xmath24 and @xmath49 , respectively . also , @xmath50 is @xmath51 and @xmath52 for @xmath53 and @xmath54 , respectively .",
    "further , @xmath55 where @xmath56 is chosen to be normal density function and @xmath57 in which @xmath58 and @xmath59    ranked set sampling ( rss ) has been under vast investigations since its introduction .",
    "@xcite comprehensively reviewed works done on rss .",
    "takahasi and wakimoto @xcite and dell and clutter @xcite were the first authors who examined the theory of rss mathematically .",
    "stokes @xcite proposed an rss estimator of the population variance and showed its asymptotic unbiasedness regardless of the presence of errors in the ranking .",
    "@xcite developed an unbiased estimator of the variance of the population based on an rss and demonstrated that this new estimator is more efficient than that of stokes .",
    "stokes and sager @xcite studied the properties of the empirical distribution function based on rss and showed that it is unbiased and has greater precision than that of simple random sampling ( srs ) .",
    "the objective of this article is two fold .",
    "first , we develop the entropy estimators in order to obtain the estimators with less bias and less mean squared error .",
    "they are also more efficient than others .",
    "these estimators are obtained by using the moving average and ranked set sampling methods ; and modifying the van es s @xcite estimator and wieczorkowski and grzegorzewsky s @xcite estimator .",
    "we also consider the scale invariance property of variances and mean squared errors of them .",
    "moreover , the consistency of the first estimator is established .",
    "the second objective of this article is to improve the tests for normality in order to obtain the most powerful tests .",
    "these test statistics are provided based on the proposed entropy estimators .",
    "we also compare the power of these tests with the other tests , using monte carlo computations .",
    "differences in power of the tests are considerable , but the results show that the first test is most powerful against the alternatives with support @xmath60 such as gamma , weibull , log normal , beta and the second is most powerful against the alternatives with support @xmath61 such as t , extreme value , logistic .",
    "the rest of this paper is arranged as follows : in section 2 , we first enhance the moving average and ranked set sampling methods ; and introduce two new entropy estimators .",
    "some properties of them are studied in the same section . also , we compare our estimators with the competitor estimators by a simulation study . in section 3",
    ", we introduce goodness - of - fit tests for normality , based on the proposed entropy estimators and then compare their powers with the powers of other tests of normality .",
    "a real example is also presented and analyzed in section 4 .",
    "in this section , we introduce two entropy estimators and compare them with other estimators .      in statistics , smoothing a data set is to create an approximating function that attempts to capture important patterns in the data , while leaving out noise phenomena .",
    "one of the most common smoothing methods is moving average .",
    "this method is a technique that can be applied to the time series analysis , either to produce smoothed periodogram of data , or to make forecasts @xcite .    a moving average ( ma ) method is the unweighted mean of the previous @xmath52 datum points .",
    "suppose individual observations , @xmath11 are collected .",
    "the moving average of width @xmath62 at time @xmath63 is defined by @xmath64 for periods @xmath65 , we do not have @xmath62 observations to calculate a moving average of width @xmath62 .",
    "now , we develop the construction of the ma method . for this aim , we defined the moving average of width @xmath62 , which is an odd integer , at time @xmath63 as : @xmath66 for @xmath67 and @xmath68 , the moving average at time @xmath63 is defined as the average of all observations that are equal or greater than @xmath69 and equal or smaller than @xmath69 , respectively .",
    "so , we define @xmath70 as follows :    @xmath71    one characteristic of the ma is that if the data have an uneven path , applying the ma will eliminate abrupt variation and cause the smooth path .",
    "rss is known to be a statistical method for data collection that generally leads to more efficient estimators than competitors based on srs .",
    "the concept of rss was used first time by mcintyre @xcite , to estimate the population mean of pasture yields in agricultural experimentation . since",
    "then there has been substantial progress in studying this sampling scheme and its extensions .",
    "when the variable of interest can be more readily ranked than measured , rss provides improved statistical inference .",
    "we now briefly explain the concept of rss for completeness .",
    "* @xmath52 random samples , each of size @xmath52 , are drawn from the population . * the @xmath63th sample ( @xmath72 ) is inspected to identify the unit of ( judgement ) @xmath63th lowest rank .",
    "* finally , the @xmath52 identified units in the ( judgement ) ranked set are measured .    in the next subsection",
    ", we use the ma and rss methods ; and present two new entropy estimators are presented .",
    "suppose @xmath11 are an srs from an unknown absolutely continuous distribution @xmath6 with a probability density function @xmath2 .",
    "the simulation study , which provided with some papers such as wieczorkowski and grzegorzewsky @xcite and alizadeh noughabi @xcite , shows that in most cases @xmath73 , which introduced in ( [ ve ] ) , and @xmath74 , which introduced in ( [ w ] ) , have the least root mean squared errors ( rmses ) and standard deviations ( sds ) in estimating entropy .",
    "now , we use the ma and rss methods to modify @xmath73 and @xmath74 ; and obtain two new entropy estimators .",
    "according to ( [ f ] ) , we know @xmath75 @xmath8 as a function of quantiles in previous equation is the sample path of order statistics , but usually it is not smooth .",
    "so we propose to imply the ma method of proper order , say @xmath62 , to smooth this sample path .    to obtain the new estimators , we select @xmath52 srs and repeat @xmath52 times this work and symbolize the srs in @xmath63th times by @xmath76 . also ,",
    "@xmath77 denote the order statistics from the sample @xmath76 . by using the ma method in each times",
    ", we define the new variables from ( [ y ] ) as @xmath78 so , the new estimators are defined as : @xmath79 where @xmath80 is defined in ( [ c ] ) , @xmath13 is a positive integer smaller than @xmath14 and @xmath81 if @xmath16 , @xmath82 if @xmath18 .",
    "we can easily prove that the scale of the random variable has no effect on the accuracy of @xmath83 and @xmath84 in estimating @xmath3 .",
    "let @xmath85 and @xmath86 denote entropies of the distribution of continuous random variables @xmath87 and @xmath88 , respectively , and @xmath89 , where @xmath90 .",
    "it is easy to see that @xmath91 then the followings hold    * @xmath92 * @xmath93 * @xmath94    where @xmath95 and @xmath96 in which the superscript @xmath97 refer to the corresponding distribution .",
    "@xmath98    _ for the explanation of the ma method , we simulate 30 samples from the standard normal distribution and plot their order statistics in figure @xmath47 with @xmath99 .",
    "the sample path of order statistics is smoothed by ma of order @xmath100 .",
    "new variables are defined from ( [ yy ] ) and the smoothed path of new variables is plotted in figure 1 with @xmath101 .",
    "this plot shows that the new sample path is smoother than the sample path of the original order statistics .",
    "also , with considering ma of order 5 , we define new variables from ( [ yy ] ) and plot them in figure 1 with @xmath102 . even though the smoothing sample path of order statistics by using the ma of order 3 is not as smooth as using ma of order 5 , the plot @xmath101 is very similar to real plot i.e. @xmath99 .",
    "so without loss of generality , we just consider ma of order @xmath103 in ( [ yy ] ) .",
    "@xmath98 _    [ order ]     for all @xmath104 .    * _ proof : _ *    * _ proof : _ *      in this subsection , we report the results of a simulation study which compares the performances of the introduced entropy estimators with the estimators proposed by vasicek @xcite , van es @xcite , ebrahimi _ et al . _",
    "@xcite , correa @xcite , wieczorkowski and grzegorewski @xcite and zamanzadeh and arghami @xcite in terms of their sds and rmses . for selected values of @xmath52 ,",
    "@xmath105 samples of size @xmath52 were generated from exponential , normal and uniform distributions which are the same three distributions considered by ebrahimi _",
    "_ @xcite and correa @xcite .",
    "still an open problem in entropy estimation is the optimal choice of @xmath13 for given @xmath52 .",
    "we choose to use the following heuristic formula @xcite as @xmath106.$ ]    tables 1 - 3 contain the rmse and sd values of the nine estimators at different sample sizes for each of the three considered distributions .",
    "we observe that the proposed estimators perform well compared with other estimators under different distributions . also , the first estimator , @xmath83 has the least rmse among the entropy estimators under exponential and normal distributions . in addition , under uniform distribution",
    "it behaves better than vasicek s estimator , van es s estimator , correa s estimator and ebrahimi s estimator , four of the most important estimators of entropy .",
    "moreover , the second estimator , @xmath84 has the least rmse among the entropy estimators under exponential , normal and uniform distributions . in many cases , we see that the proposed estimators have the lowest sd among all entropy estimators",
    ". moreover , in all cases , rmse and sd decrease with the sample size ; and @xmath107 and @xmath74 have the same sd , because wieczorkowski and grzegorzewski modified the vasicek estimator by adding a bias correction .    [ bia1 ]    ccccccccccc + @xmath52 & @xmath13 & @xmath108 & @xmath109 & @xmath110 & @xmath111 & @xmath112 & @xmath113 & @xmath114 & @xmath115 & @xmath116 + 5 & 2 & 0.930(0.559 ) & 0.596(0.586 ) & 0.743(0.554 ) & 0.651(0.559 ) & 0.561(0.559 ) & 0.575(0.573 ) & 0.576(0.575 ) & 0.428(0.413 ) & 0.425(0.399 ) + 10 & 3 & 0.570(0.360 ) & 0.392(0.373 ) & 0.435(0.361 ) & 0.404(0.360 ) & 0.361(0.360 ) & 0.391(0.383 ) & 0.389(0.383 ) & 0.204(0.202 ) & 0.247(0.218 ) + 15 & 4 & 0.421(0.284 ) & 0.310(0.290 ) & 0.328(0.290 ) & 0.308(0.284 ) & 0.282(0.282 ) & 0.327(0.306 ) & 0.321(0.305 ) & 0.133(0.132 ) & 0.190(0.158 ) + 20 & 4 & 0.356(0.242 ) & 0.274(0.250 ) & 0.272(0.247 ) & 0.263(0.242 ) & 0.242(0.242 ) & 0.300(0.261 ) & 0.286(0.260 ) & 0.104(0.102 ) & 0.150(0.121 ) + 30 & 5 & 0.276(0.198 ) & 0.227(0.201 ) & 0.208(0.197 ) & 0.201(0.187 ) & 0.198(0.198 ) & 0.266(0.209 ) & 0.245(0.208 ) & 0.078(0.069 ) & 0.119(0.088 ) + 50 & 7 & 0.198(0.150 ) & 0.181(0.151 ) & 0.156(0.151 ) & 0.153(0.150 ) & 0.150(0.148 ) & 0.242(0.160 ) & 0.210(0.158 ) & 0.066(0.042 ) & 0.093(0.061 ) +    [ bia2 ]    ccccccccccc + @xmath52 & @xmath13 & @xmath108 & @xmath109 & @xmath110 & @xmath111 & @xmath112 & @xmath113 & @xmath114 & @xmath115 & @xmath116 + 5 & 2 & 0.994(0.425 ) & 0.509(0.452 ) & 0.793(0.418 ) & 0.666(0.425 ) & 0.464(0.413 ) & 0.494(0.407 ) & 0.493(0.407 ) & 0.384(0.370 ) & 0.355(0.344 ) + 10 & 3 & 0.618(0.269 ) & 0.366(0.283 ) & 0.407(0.271 ) & 0.408(0.269 ) & 0.297(0.264 ) & 0.303(0.255 ) & 0.310(0.255 ) & 0.215(0.181 ) & 0.197(0.194 ) + 15 & 4 & 0.474(0.211 ) & 0.318(0.220 ) & 0.348(0.213 ) & 0.294(0.211 ) & 0.233(0.211 ) & 0.222(0.193 ) & 0.232(0.192 ) & 0.182(0.121 ) & 0.147(0.147 ) + 20 & 4 & 0.373(0.179 ) & 0.276(0.185 ) & 0.265(0.182 ) & 0.247(0.179 ) & 0.190(0.178 ) & 0.190(0.170 ) & 0.205(0.169 ) & 0.153(0.095 ) & 0.118(0.117 ) + 30 & 5 & 0.282(0.144 ) & 0.243(0.148 ) & 0.194(0.146 ) & 0.186(0.144 ) & 0.150(0.144 ) & 0.148(0.135 ) & 0.165(0.135 ) & 0.140(0.065 ) & 0.091(0.087 ) + 50 & 7 & 0.199(0.110 ) & 0.212(0.110 ) & 0.134(0.112 ) & 0.128(0.110 ) & 0.110(0.109 ) & 0.108(0.104 ) & 0.127(0.104 ) & 0.137(0.039 ) & 0.070(0.061 ) +    [ bia3 ]    ccccccccccc + @xmath52 & @xmath13 & @xmath108 & @xmath109 & @xmath110 & @xmath111 & @xmath112 & @xmath113 & @xmath114 & @xmath115 & @xmath116 + 5 & 2 & 0.774(0.346 ) & 0.407(0.407 ) & 0.566(0.336 ) & 0.405(0.446 ) & 0.346(0.346 ) & 0.330(0.326 ) & 0.330(0.327 ) & 0.328(0.297 ) & 0.283(0.255 ) + 10 & 3 & 0.455(0.167 ) & 0.216(0.216 ) & 0.295(0.169 ) & 0.235(0.167 ) & 0.166(0.166 ) & 0.179(0.176 ) & 0.180(0.178 ) & 0.165(0.127 ) & 0.134(0.104 ) + 15 & 4 & 0.343(0.110 ) & 0.155(0.155 ) & 0.208(0.112 ) & 0.159(0.110 ) & 0.110(0.110 ) & 0.137(0.123 ) & 0.136(0.127 ) & 0.116(0.082 ) & 0.090(0.063 ) + 20 & 4 & 0.274(0.087 ) & 0.126(0.121 ) & 0.157(0.088 ) & 0.133(0.087 ) & 0.087(0.087 ) & 0.125(0.100 ) & 0.121(0.105 ) & 0.099(0.063 ) & 0.078(0.049 ) + 30 & 5 & 0.210(0.059 ) & 0.086(0.086 ) & 0.110(0.061 ) & 0.096(0.059 ) & 0.059(0.059 ) & 0.112(0.073 ) & 0.104(0.078 ) & 0.074(0.041 ) & 0.056(0.031 ) + 50 & 7 & 0.155(0.037 ) & 0.057(0.057 ) & 0.075(0.038 ) & 0.062(0.037 ) & 0.037(0.037 ) & 0.101(0.051 ) & 0.090(0.055 ) & 0.052(0.024 ) & 0.038(0.017 ) +    [ bia1 ]    .standard deviation and mean squared error of the proposed entropy estimators of the normal distribution for different @xmath52 , @xmath13 and @xmath62 , to obtain the sample size . [ cols=\"^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ fig3 ]    = 2.3 in",
    "in this paper , we first introduced two new entropy estimators of a continuous random variable by modifying the van es s @xcite estimator and wieczorkowski and grzegorzewsky s @xcite estimator in order to obtain two estimators with less bias and less rmse .",
    "we considered some properties of @xmath83 and @xmath84 . also , we compared our estimators with the entropy estimators proposed by vasicek @xcite , van es @xcite , ebrahimi _ et al . _",
    "@xcite , correa @xcite , wieczorkowski and grzegorewski @xcite and zamanzadeh and arghami @xcite .",
    "we observed that @xmath83 behaves better than many of the most important entropy estimators and @xmath84 has generally least bias and rmse among all entropy estimators for different distributions .",
    "* @xmath117 , @xmath118 , @xmath119 , @xmath120 , @xmath121 , which are defined based on entropy estimators , * @xmath122 , @xmath123 which are defined in rss context , * @xmath124 , @xmath125 , @xmath126 , @xmath127 , which are simple non - parametric test statistics .    by comparing the power of these tests , using monte carlo computations",
    ", we found that the test statistics @xmath128 and @xmath129 are most powerful against the alternatives with supports @xmath61 and @xmath60 , respectively .",
    "this work has the potential to be applied in the context of information theory and goodness - of - fit tests .",
    "this paper can elaborate further researches by considering other distributions besides the standard normal distribution , such as pareto , log normal and weibull distributions .",
    "m.d , castellanos .",
    "m.e , morales .",
    "d , and vajda .",
    "i , monte carlo comparison of four normality tests using different entropy estimates , _ communication in statistics : simulation and computation _ , 30 ( 2001 ) , 761 - 785 .",
    "g.s , natarajan .",
    "r , and chaubey .",
    "y.p , a goodness - of - fit test for the inverse gaussian distribution using its independence characterization , _ sankhya : the indian journal of statistics _ , 63 ( 2001 ) , 362 - 374 ."
  ],
  "abstract_text": [
    "<S> we present two new estimators for estimating the entropy of absolutely continuous random variables </S>",
    "<S> . some properties of them are considered , specifically consistency of the first is proved . </S>",
    "<S> the introduced estimators are compared with the existing entropy estimators . also , we propose two new tests for normality based on the introduced entropy estimators and compare their powers with the powers of other tests for normality . </S>",
    "<S> the results show that the proposed estimators and test statistics perform very well in estimating entropy and testing normality . </S>",
    "<S> a real example is presented and analyzed . </S>",
    "<S> +   + _ keywords _ : information theory , entropy estimator , moving average method , ranked set sampling , testing normality . +   + _ mathematics subject classification : _ 62g10 , 62g30 . </S>"
  ]
}