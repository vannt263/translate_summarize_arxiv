{
  "article_text": [
    "the most well known probability distribution of probabilities is the beta distribution . if we have observed @xmath0 ` successes ' , each having a probability @xmath1 , and @xmath2 ` failures ' , each having a probability @xmath3 .",
    "then the corresponding beta distribution if @xmath1 is given as : @xmath4    we will proceed in this paper to derive a whole family of beta - like distributions , which take as their data not only the number of successes and failures , but also values on predictor variables and time to failure or time without failure .",
    "the recurring theme in all this will be that , apart from the ordinary product and sum rules ( _ e.g. _ , bayes theorem and the integrating out of nuisance parameters ) , a change - of - variable or , alternatively , a jacobian transformation allows us to map the uncertainty we have , regarding the unknown parameter(s ) , as captured in the corresponding posterior , unto the probability of interest ; thus , allowing us to construct a probability distribution of the probability of interest .",
    "the beta - like distribution is the distribution that takes into account the epistemological parameter uncertainty , as captured in the posterior distribution of these parameters , of the parameters of a given probability model .",
    "the bayesian model selection , also discussed in this paper , takes into account the epistemological model uncertainty .    in this paper",
    "we will be talking about failure mechanisms which have some underlying probabilistic ` generating ' process . in light of this loose terminolgy , we would like to give , as a caveat , the following quote by jaynes , who is considered by many to be the father of modern bayesianity , @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [ t]he judgment of a competent engineer , based on data of past experience in the field , represents information fully as ` objective ' and reliable as anything we can possibly learn from a random experiment . indeed , most engineers would make a stronger statement ; since a random experiment is , by definition , one in which the outcome - and therefore the conclusion we draw from it - is subject to uncontrollable variations , it follows that the only fully ` objective ' means of judging the reliability of a system is through analysis of stresses , rate of wear , etc . , which avoids random experiments altogether .",
    "in practice , the real function of a reliability test is to check against the possibility of completely unexpected modes of failure ; once a given failure mode is recognized and its mechanism understood , no sane engineer would dream of judging its chances of occurring merely from a random experiment .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in closing , the probability models used in this paper are by no way exhaustive .",
    "for example , we are currently studying the negative binomial probability model . a probability model which is particularly popular among seismologists .",
    "and we have already found that a lot of interesting things can be said about this about the negative binomial generating process . but this will be subject of another paper .",
    "in the logistic regression model , we model the log - odds of some event by way of a regression model , say , @xmath5 where @xmath6 is some predictor value .",
    "identity implies that the probability of success @xmath1 can be written down as as the following function of the unknown parameters @xmath7 and @xmath8 : @xmath9 likewise , the probability of a failure can be written down as : @xmath10      say , we have @xmath0 successes , with corresponding predictor values @xmath11 , and @xmath2 failures , with corresponding predictor values @xmath12 . then , by way of and , the probability of the observed data , or , equivalently , the likelihood of the unknown parameters @xmath7 and @xmath8 , can be written down as : @xmath13    we assign some uniform prior to the unknown parameters @xmath7 and @xmath8 , as is customary in bayesian regression analysis , @xcite : @xmath14 where ` @xmath15 ' is the proportionality sign .    by multiplying the likelihood with the prior , respectively , and , one may obtain , by way of product rule , or , equivalently , bayes theorem , @xcite , the joint distribution of @xmath7 and @xmath8 : @xmath16    now , if we wish to obtain the probability distribution of the unknown parameters @xmath7 and @xmath8 , conditional on the data @xmath17 , or , equivalently , the posterior of @xmath7 and @xmath8 , we must , by way of the product rule , @xcite , divide with the evidence @xmath18 however , if we do this , then the inverse of the evidence , @xmath19 , being a constant not dependent upon the parameters @xmath7 and @xmath8 , gets absorbed in the proportionality sign of ; thus , giving us a posterior : @xmath20 which is proportional to both the likelihood and the joint distribution .",
    "however , we are not that much interested in the probability distribution of @xmath7 and @xmath8 .",
    "since we are aiming for the probability distribution of the probability of a success @xmath1 , given a predictor value @xmath6 , .",
    "would we know the values of @xmath7 and @xmath8 exactly , as we know our predictor value @xmath6 , then we could substitute these values directly into and , thus , get the exact probability @xmath1 .",
    "now , we do not know the values of @xmath7 and @xmath8 exactly .",
    "instead , we have a range of probable values on the @xmath7- and @xmath8-axes , as captured by the posterior .",
    "this corresponds , through a two - to - one mapping , with a range of probable values on the @xmath1-axis .",
    "this two - to - one mapping is , typically , accomplished by way of a jacobian transformation .      by way of",
    ", we have that @xmath21 so , a possible transformation from @xmath22 to @xmath23 is @xmath24 which has a corresponding jacobian of @xmath25    substituting into the posterior , and multiplying it with the jacobian , gives us the transformed posterior : @xmath26 if we numerically integrate out the unwanted parameter @xmath27 out of , we get the posterior of the probability @xmath1 , given the data @xmath17 and some predictor value @xmath6 : @xmath28 which gives us the bayesian logistic regression model we are looking for",
    ".    note that for non - informative data , that is , for predictors which all have the same value , that is , @xmath29 , for @xmath30 and @xmath31 , the terms in the exponentials of all become zero , and , as a consequence , collapses to the ordinary beta distribution : @xmath32 which is in nice correspondence with our intuition .    if the predictors are non - informative , in that they ` flat - line ' , then the only pertinent aspect of the data @xmath17 which remains , is the number of successes and failures , respectively , @xmath0 and @xmath2 , and these are just the sufficient statistics of the beta distribution .",
    "say , we have an exponential failure mechanism , then the probability of a failure at time @xmath33 is @xmath34 consequently , the probability of no failure until time @xmath35 is @xmath36    in most reliability problems we will be interested in determining probability .",
    "that is , in general we will wish to find the probability distribution of @xmath37 where @xmath35 is some desired life - time and @xmath38 is the unknown parameter of the exponential distribution .",
    "say , we have @xmath39 identical units , which we follow in time .",
    "if we observe a sequence of @xmath0 failure times , say , @xmath11 , and @xmath2 units that did not fail , these having , consequently , having times without failure , say , @xmath12 . then , by way of and , the probability of the observed data , or , equivalently , the likelihood of the unknown parameter @xmath38 , can be written down as @xmath40 where we let the constant term @xmath41 be absorbed in the proportionality sign",
    ".    it would not be strange if our our prior information consisted of an initial gues of a life - time of , say , @xmath33 .",
    "this initial guess corresponds with a prior likelihood of @xmath42 combining the prior likelihood with the uninformative jeffreys prior for the inverse failure rate @xmath38 , @xmath43 we get , by way of the product rule and the bayesian proportionality short hand , the informative prior of @xmath38 , based on the initial guess of a life - time of @xmath33 : @xmath44 where we have absorbed both the differential @xmath45 of and the normalizing constant of into the proportionality sign of .",
    "note that the prior may also be obtained through an alternative maximum entropy argument , @xcite .",
    "but we give , instead , the above derivation . because it is analogous to the derivation of the informative prior for a postulated weibull failure mechanism , treated below .    combining the likelihood with the informative prior , respectively , and , by way of the product rule and the bayesian proportionality short hand , we get the posterior for the unknown parameter @xmath38 : @xmath46    the posterior is the probability distribution of the unknown parameter @xmath38 , conditional on the data @xmath17 we have observed and our tentative guess of a life - time of @xmath33 .",
    "however , we are not that much interested in the probability distribution of @xmath38 .",
    "rather , we are aiming for the probability distribution of the probability of the life - time exceeding @xmath35 , .",
    "would we know the value of @xmath38 exactly , then we could substitute this value into and , so , get the exact probability @xmath1 . now , we do not know the value of @xmath38 exactly .",
    "instead , we have a range of probable values on the @xmath38-axis , as captured by the posterior .",
    "this corresponds , through a one - to - one mapping , with a range of probable values on the @xmath1-axis .",
    "this one - to - one mapping is , typically , accomplished by way of a change of variable .      by way of",
    ", we have that @xmath47 so , the corresponding transformation is @xmath48    substituting the change of variable into the posterior , we obtain the transformed posterior distribution of the probability @xmath1 that a given unit will have a life - time exceeding @xmath35 : @xmath49 \\propto \\frac{\\left(-\\log \\theta\\right)^{r}}{\\theta } \\exp\\left[\\frac{\\log \\theta}{\\tau } \\ ; t\\!\\left(d , t\\right)\\right],\\ ] ] where @xmath50 is the total observed time without failure , @xmath0 is the number of observed failures , and @xmath35 is the life - time that has to be exceeded .",
    "if we properly normalize , we get the identity : @xmath51 = \\left[\\frac{t\\!\\left(d , t\\right)}{\\tau}\\right]^{r+1 } \\frac{\\left(-\\log \\theta\\right)^{r}}{r !",
    "\\theta } \\exp\\left[\\frac{\\log \\theta}{\\tau } \\ ;",
    "t\\!\\left(d , t\\right)\\right],\\ ] ]    by way of , the expectation value of the probability that a given unit will a life - time that exceeds @xmath35 , then is @xmath52 d \\theta = \\left[\\frac{t\\!\\left(d , t\\right)}{t\\!\\left(d , t\\right ) + \\tau}\\right]^{r+1}.\\ ] ] this expectation value , which itself is a probability , is the result of example  3 , given in @xcite",
    ". however , there it was not yet recognized that this probability is the mean of an underlying beta - like probability distribution    following jaynes , we subject to various extreme conditions , in order to show the correspondance with the indications of common sense .",
    "if the total , say , unit - hours of the test is small compared to prior expected life - time @xmath33 , that is , if @xmath53 .",
    "then , , @xmath54 and , unless a large number of failures @xmath0 is observed , our state of knowledge about @xmath1 can hardly be changed by the test , and , as a consequence , we have to rely on our prior knowledge only .",
    "if the total , say , unit - hours of the test is large compared to prior expected life - time @xmath33 , that is , if @xmath55 .",
    "then , , @xmath56 and , for all intents and purposes , our final conclusions depend only what we observed in the test , and , as a consequence , these conclusions are almost independent of what we previously thought previously .    in intermediate cases",
    ", our prior knowledge has a weight comparable to that of the test . if @xmath57 , the amount of testing required is appreciably reduced . for",
    "if we were already quite sure that the units are satisfactory , then we require less additional evidence before accepting them .",
    "but if @xmath58 , that is , if we are initially very doubtful about the units , then we demand that the test itself provide compelling evidence in favor of their reliability .",
    "this is a repeat of the previous case , with the difference that we now use a weibull failure mechanism instead of an exponential one .",
    "say , we have a weibull failure mechanism , then the probability of a failure at time @xmath33 is @xmath59 d t.\\ ] ] consequently , the probability of no failure until time @xmath35 is @xmath60 d t = \\exp\\left[\\left(- \\tau \\lambda\\right)^{k}\\right].\\ ] ]    in most reliability problems we will be interested in determining probability .",
    "that is , in general we wish to find the probability distribution of @xmath61,\\ ] ] where @xmath35 is some desired life - time , and @xmath38 and @xmath62 are , respectively , the unknown inverse failure rate and shape parameter of the weibull distribution .",
    "say , we have @xmath39 identical units , which we follow in time .",
    "if we observe a sequence of @xmath0 failure times , say , @xmath11 , and @xmath2 units that did not fail , these having , consequently , having times without failure , say , @xmath12 . then , by way of and , the probability of the observed data , or , equivalently , the likelihood of the unknown parameters @xmath38 and @xmath62 , can be written down as @xmath63 d x_{i } \\prod_{j=1}^{n - r } \\exp\\left[\\left(-\\lambda y_{j}\\right)^{k}\\right ] \\\\      & \\propto \\prod_{i=1}^{r } k \\lambda \\left(\\lambda x_{i}\\right)^{k-1 } \\exp\\left[\\left(-\\lambda x_{i}\\right)^{k}\\right ] \\prod_{j=1}^{n - r } \\exp\\left[\\left(-\\lambda y_{j}\\right)^{k}\\right ] , \\notag\\end{aligned}\\ ] ] where we let the constant term @xmath41 be absorbed in the proportionality sign .    if our our prior information consisted of an initial gues of a life - time of , say , @xmath33 .",
    "this initial guess corresponds with a prior likelihood of @xmath64 d t.\\ ] ] combining the prior likelihood with the uninformative jeffreys prior for the inverse failure rate and shape parameter , respectively , @xmath38 and @xmath62 , @xmath65 we get , by way of the product rule and the bayesian proportionality short hand , the informative prior of @xmath38 and @xmath62 , based on the initial guess of a life - time of @xmath33 : @xmath66,\\ ] ] where we have absorbed both the differential @xmath45 of and the normalizing constant of into the proportionality sign of .    in @xcite ,",
    "an alternative informative prior is derived , where the piece of prior information consists of an initial guess of the time without failure , @xmath67 .",
    "now , would we have no initial guess whatsoever , neither for the time to failure nor for the time without failure , then the proper cause of action would be to assign as an uninformative prior the prior .    combining the likelihood with the informative prior , respectively , and , by way of the product rule and the bayesian proportionality short hand , we get the posterior for the unknown parameters @xmath38 and @xmath62 : @xmath68   \\prod_{i=1}^{r } k \\lambda \\left(\\lambda x_{i}\\right)^{k-1 } \\exp\\left[\\left(-\\lambda x_{i}\\right)^{k}\\right ] \\prod_{j=1}^{n - r } \\exp\\left[\\left(-\\lambda y_{j}\\right)^{k}\\right].\\ ] ]",
    "the posterior is the probability distribution of the unknown parameters @xmath38 and @xmath62 , conditional on the data @xmath17 we have observed and our tentative guess of a life - time of @xmath33 . however , we are not that much interested in the probability distribution of @xmath38 and @xmath62 . rather , we are aiming for the probability distribution of the probability of the life - time exceeding @xmath35 , .",
    "would we know the values of @xmath38 and @xmath62 exactly , then we could substitute this value into and , so , get the exact probability @xmath1 . now , we do not know the values of @xmath38 and @xmath62 exactly .",
    "instead , we have a range of probable values on the @xmath38- and @xmath62-axes , as captured by the posterior .",
    "this corresponds , through a two - to - one mapping , with a range of probable values on the @xmath1-axis .",
    "this two - to - one mapping is , typically , accomplished by way of a jacobian transformation .",
    "by way of , we have that @xmath69.\\ ] ] so , a possible transformation is @xmath70 which has a corresponding jacobian of @xmath71    substituting into the posterior , and multiplying it with the jacobian , gives us the transformed posterior : @xmath72 \\propto \\left(t \\prod_{i=1}^{r } x_{i}\\right)^{\\kappa-1 } \\frac{\\kappa^{r-1}}{\\tau^{\\kappa",
    "\\left(r + 1\\right ) } } \\frac{\\left(-\\log \\theta\\right)^{r}}{\\theta } \\exp\\left[\\frac{\\log\\theta}{\\tau^{\\kappa } } t\\!\\left(d , t , \\kappa\\right)\\right],\\ ] ] where @xmath73 is the shape parameter of the weibull distribution and where @xmath74 is the total power - transformed observed time without failure , @xmath0 is the number of observed failures , and @xmath35 is the life - time that has to be exceeded .    note that if we set the shape parameter to @xmath75 , or , equivalently , we go from the weibull to the more restrictive exponential failure mechanism , then , by way of the proportionality sign , the posterior collapses to , which is at should be .",
    "looking at the statistic , we see that for , say , @xmath76 , one observation having no failure until time @xmath77 is equivalent to 49 observations having no failure until time @xmath78 .    for @xmath75 , where the weibull collapses to the memoryless exponential distribution , one observation having no failure time until time @xmath77 is equivalent to 7 observations having no failure until time @xmath78 .",
    "this reflects the weibull s dependence on the shape parameter @xmath73 . for large values of @xmath73 , extended periods without failure",
    "become less probable .",
    "thus , observing one extended period without failure carries the same weight as observing many more short periods without failure .",
    "if we numerically integrate out the unwanted parameter @xmath73 out of , we get the posterior of the probability @xmath1 , given the data @xmath17 and the initial guess of time without failure , @xmath33 : @xmath79 d \\kappa.\\ ] ]",
    "there are instances were we can not rewrite any of the unknown parameters in the posterior as a function of @xmath1 , as was done , for example , in , , and .",
    "this , then , prohibits us from finding the explicit form of the corresponding beta - like distribution .",
    "however , we may still find the first moments of these intractable distributions .",
    "thus , allowing us to either approximate the corresponding probability distribution or , alternatively , to construct confidence bounds on the estimated probabilities .        here",
    "we define the probability of interest , @xmath1 , to be the probability of @xmath80 events occurring within the time period @xmath35 , by a mechanism which is modeled by an underlying exponential distribution , having an unknown parameter @xmath38 .",
    "the data consists of single events observed at variable , though consecutive , waiting times @xmath81 , and a non - event from the last failure , which is observed at time @xmath82 , onwards , until the end of the time period @xmath35 : @xmath83    an inspection of learns us that the probability of interest has the form of a poisson distribution , which has an expected number of events equal to @xmath84 .",
    "so , we define our probability of interest to be @xmath85      in a total time period of , say , @xmath86 , we observe @xmath39 consecutive times to an even , @xmath87 . assuming an exponential event - generating mechanism , @xmath88 the probability of the observed data , or , equivalently , the likelihood of the unknown parameter @xmath38 , which is the expected of events per time unit , may be written down as @xmath89 \\prod_{i=1}^{n } \\lambda \\exp\\left(-\\lambda x_{i}\\right ) dx_{i } \\\\",
    "& \\propto \\lambda^{n } \\exp\\left(-\\lambda t\\right ) , \\notag\\end{aligned}\\ ] ] where we have absorbed the term @xmath90 into the proportionality sign .",
    "if we have an initial guess that the time to an event is @xmath33 , we may assign the informative prior @xmath91 however , if we do not feel confident enough , to make such a prior guess , we may alternatively , assign an uninformative jeffreys prior @xmath92    multiplying the likelihood with the informative prior , respectively , and , we may obtain the posterior of @xmath38 : @xmath93.\\ ] ] where the pertinent aspects of the data @xmath17 are the number of events , @xmath39 , and the total time of observation @xmath86 .    as an aside , if we have two data sets of the same phenomena under observation , say , @xmath94 and @xmath95 ,",
    "having , respectively , @xmath96 and @xmath97 observed events in the respective periods @xmath98 and @xmath99 , then these data sets , together with the informative prior , would combine to the posterior @xmath100.\\ ] ]    now , would we attempt in to make a change of variable from @xmath38 to @xmath1 , then we find that @xmath38 can not be written unambiguously as a function of @xmath1 for @xmath101 ; where @xmath102 is equivalent to the exponential probability model , .",
    "it follows that we can make no analytical change of variable for the poission model probability , or , equivalently , derive the explicit beta - like distribution for this probability model .",
    "but what we can do , is derive the first moments of this intractable beta - like distribution .",
    "this will allow us to either compute the skewness corrected confidence bounds for this intractable distribution , or , alternatively , construct the maxent distribution which has the same moments as this intractable distribution , respectively , @xcite and @xcite .",
    "in what follows we give a short outline on how to derive fourth - order maxent distributions .",
    "three well - known maxent distributions are the uniform , exponential , and normal distributions .",
    "these distributions correspond , respectively , with zeroth- , first- , and second - order maxent distributions .    for a given probability distribution @xmath103 where @xmath1 is",
    "the parameter of interest and @xmath104 is some set of parameters which make up this probability distribution , the first four cumulants are given as : @xmath105 where @xmath106 is the mean , @xmath107 is the standard deviation , @xmath108 is the skewness , and @xmath73 is the kurtosis of the probability distribution @xmath109 .",
    "the fourth - order maxent distribution incorporates information about the skewness and kurtosis , respectively , @xmath108 and @xmath73 , , as well as the mean and standard deviation , respectively , @xmath106 and @xmath107 .",
    "the algorithm for higher - order maxent distributions is due to rockinger and jondeau , @xcite .    we will now proceed to give the algorithmic steps needed to construct fourth - order maxent approximations of intractable beta - like distributions .",
    "+   + * step 1 . *",
    "+ the fourth - order maxent distribution we seek takes as its input the first four cumulants of the probability distribution we wish to approximate .",
    "for example , if wish to determine the fourth - order maxent distrbution of @xmath1",
    ". then we first compute the first four moments of @xmath1 , , , and : @xmath110^{1 }   \\lambda^{n } \\exp\\left[-\\lambda \\left(t + t\\right)\\right ] d\\lambda ,   \\notag \\\\",
    "m_{2 } & = \\int \\left[\\frac{\\left(\\lambda \\tau\\right)^{m}}{m ! } \\exp\\left(- \\lambda \\tau\\right)\\right]^{2 }   \\lambda^{n } \\exp\\left[-\\lambda \\left(t + t\\right)\\right ] d\\lambda , \\notag \\\\",
    "m_{3 } & = \\int \\left[\\frac{\\left(\\lambda \\tau\\right)^{m}}{m ! } \\exp\\left(-",
    "\\lambda \\tau\\right ) \\right]^{3 }   \\lambda^{n } \\exp\\left[-\\lambda \\left(t + t\\right)\\right ] d\\lambda , \\\\",
    "m_{4 } & = \\int \\left[\\frac{\\left(\\lambda \\tau\\right)^{m}}{m ! } \\exp\\left(-",
    "\\lambda \\tau\\right ) \\right]^{4 } \\lambda^{n } \\exp\\left[-\\lambda \\left(t + t\\right)\\right ] d\\lambda , \\notag \\end{aligned}\\ ] ]    the moments in evaluate to @xmath111    by way of and the identities , @xcite , @xmath112 we may then compute the first four cumulants , needed for the construction of the fourth - order maxent distribution .",
    "+   + * step 2 . * + we now plug the third and fourth cumulant , respectively , @xmath108 and @xmath73 , into the integral function @xmath113 dx,\\ ] ] where ] ] @xmath114    then minimize over the vector @xmath115 , in order to obtain the minimization estimates @xmath116 .",
    "+   + * step 3 .",
    "* + we then make a change of variable from @xmath117 to @xmath1 , where @xmath118 in order to obtain the unscaled maxent distribution on the @xmath1 axis : @xmath119.\\ ] ] the normalizing constant of then may be computed by way of the integral , and , @xmath120    the properly normalized fourth - order maxent approximation of the intractable beta - like distribution , which has its probability model , is then given as , , , , , and : @xmath121,\\ ] ] where @xmath122 .        in the preceeding discussion",
    "we discussed the beta - like distribution of the probability of @xmath80 events occurring within the time period @xmath35 , by a mechanism which is modeled by an underlying exponential distribution , having an unknown parameter @xmath38 .",
    "this probability distribution may be of interest if we have a parallel system of @xmath80 non - redundent fail - safe mechanisms , where each mechanism admits an exponential time to failure model .",
    "now , we can envisage scenarios in which we wish to determine the beta - like distribution of the probability of more than @xmath80 events occurring within the time period @xmath35 , by a mechanism which is modeled by an underlying exponential distribution , having an unknown parameter @xmath38 .",
    "one such scenario may be where we have a system which is subject to successive loads , each load admitting an exponential time to occurrence model .",
    "the system then might be hypothesized to be able up to @xmath80 such loads , before a significant wear and tear occurs .    for this scenario the probability model of interest is @xmath123    now ,",
    "is just a probability , just like , say , is , which admits an uncertainty regarding the actual value of the inverse failure rate @xmath38 , as captured by the posterior .",
    "so , we may proceed to compute the first four moments of the beta - like probability distribution which results from the uncertainty we have regarding the actual value of @xmath38 : @xmath124^{1 }   \\lambda^{n } \\exp\\left[-\\lambda \\left(t + t\\right)\\right ] d\\lambda ,   \\notag \\\\",
    "m_{2 } & = \\int \\left[1 - \\sum_{i=0}^{m } \\frac{\\left(\\lambda \\tau\\right)^{i}}{i ! } \\exp\\left(-",
    "\\lambda \\tau\\right)\\right]^{2 }   \\lambda^{n } \\exp\\left[-\\lambda \\left(t + t\\right)\\right ] d\\lambda , \\notag \\\\",
    "m_{3 } & = \\int \\left[1 - \\sum_{i=0}^{m } \\frac{\\left(\\lambda \\tau\\right)^{i}}{i ! } \\exp\\left(-",
    "\\lambda \\tau\\right)\\right]^{3 }   \\lambda^{n } \\exp\\left[-\\lambda \\left(t + t\\right)\\right ] d\\lambda , \\\\",
    "m_{4 } & = \\int \\left[1 - \\sum_{i=0}^{m } \\frac{\\left(\\lambda \\tau\\right)^{i}}{i ! } \\exp\\left(-",
    "\\lambda \\tau\\right)\\right]^{4 } \\lambda^{n } \\exp\\left[-\\lambda \\left(t + t\\right)\\right ] d\\lambda , \\notag \\end{aligned}\\ ] ] having obtained these moments we may compute the relevant cumulants , by way of , and either proceed to construct the fourth - order maxent approximation of the intractable beta - like distribution of .        in the poisson regression model",
    "the number of events occuring , @xmath80 , in a given time period , @xmath35 , has poisson distribution : @xmath125 where the logarithm of the expected number of events per time unit , @xmath38 , is modeled by way of a regression model : @xmath126 if we take the exponential of , and substitute it into , we get the probability model : @xmath127^{m}}{m ! } \\exp\\left[- \\exp\\left(\\beta_{0 } + \\beta_{1 } z\\right ) \\tau\\right ] \\notag \\\\       & = \\frac{\\tau^{m}}{m ! } \\exp\\left[m \\left(\\beta_{0 } + \\beta_{1 } z\\right ) - \\exp\\left(\\beta_{0 } + \\beta_{1 } z\\right ) \\tau\\right]\\end{aligned}\\ ] ]      the data @xmath17 consists of @xmath39 counts , @xmath128 , with corresponding predictor values @xmath129 .",
    "using , the probability of the data @xmath17 , or , equivalently , the likelihood of the unknown parameters @xmath7 and @xmath8 , may be written down as @xmath130 \\notag\\\\      & \\propto \\prod_{i=1}^{n } \\exp\\left[r_{i } \\left(\\beta_{0 } + \\beta_{1 } x_{i}\\right ) - \\exp\\left(\\beta_{0 } + \\beta_{1 } x_{i}\\right ) \\tau\\right ] \\\\",
    "& \\propto \\exp\\left[\\sum_{i } r_{i } \\left(\\beta_{0 } + \\beta_{1 } x_{i}\\right ) - \\tau \\ : \\sum_{i } \\exp\\left(\\beta_{0 } + \\beta_{1 } x_{i}\\right ) \\right ] \\notag\\end{aligned}\\ ] ]    we assign some uniform prior to the unknown regression parameters @xmath7 and @xmath8 : @xmath131\\ ] ]    by multiplying the likelihood with the prior , one may obtain the unscaled posterior of @xmath7 and @xmath8 : @xmath132.\\ ] ]    since we can make no analytical jacobian transformation from the @xmath7 and @xmath8 to the probability model @xmath1 , we compute the first moments of , by way of the posterior , and proceed to construct either the skewness corrected confidence interval or the maxent approximation of the corresponding beta - like distribution .",
    "we will here construct the beta - like distribution of a poisson - like probability model .",
    "we define the probability of interest , @xmath1 , to be the probability of @xmath80 events occurring within the time period @xmath35 , by a mechanism which is modeled by an underlying weibull distribution , having an unknown parameters @xmath38 and @xmath62 .",
    "the advantage of a weibull over an exponential mechanism , is that the shape parameter @xmath62 of the former represents an extra degree of freedom , as it may take on any value greater than zero ; whereas , in the latter it is dogmatically set to one .",
    "regular poisson distributions have as their event - generating mechanism exponential distributions , . in contrast , a sequence of weibull distributed events leaves us with an analytically intractable integral .",
    "however , making use of an unexpected equivalence relationship , we may work around the encountered integral and , so , proceed to approximate the poisson - like distribution .",
    "we first present the case for the regular poisson distribution , as this will give us a handle on how to generalize from the poisson to the poisson - like distribution .",
    "let @xmath1 be the probability of @xmath80 failures occurring within time period @xmath35 .",
    "the failure mechanism generating these events is the exponential distribution , having an unknown parameter @xmath38 .",
    "the data we will use is failures at variable , though consecutive , times @xmath133 , and a non - failure from the last failure , at time @xmath134 , onwards , until the end of the time period @xmath35 , : @xmath135 where it may be found , by way of induction , that @xmath136    substituting into , we find that @xmath1 is just the traditional poisson probability of @xmath80 events occurring in a time period @xmath35 , : @xmath137 where @xmath84 is the expected number of failures within the time period @xmath35",
    ".    we again define @xmath1 to be the probability of @xmath80 failures occurring within the time period @xmath35 , but now by a failure mechanism which is modelled by an unerlying weibull distribution having parameters @xmath38 and @xmath62 .",
    "the weibull model , having one more parameter , is more flexible than the exponential model .",
    "in fact , the exponential is a special case of the weibull , were we set the shape paramter @xmath62 to one .    the data , again",
    ", consists of failures at variable , though consecutive , times @xmath138 , and a non - failure from the last failure , at time @xmath134 , onwards , until the end of the time period @xmath35 : @xmath139^{k } } dt_{m}\\cdots dt_{2 } dt_{1}. \\notag\\end{aligned}\\ ] ]    integral does not allow for a simple analytical expression like .",
    "so , by way of the curse of dimensionality , as @xmath140 , we are prohibited from evaluating the first cumulants of and , as a consequence , constructing either a confidence bound or a maxent approximative distribution .",
    "however , there is a useful equivalence relation which may be used to find these cumulants after all .",
    "the equivalence relationship is derived first for the regular poisson model . since",
    ", only for this regular case do we have the analytical solution of the target integral with which we can compare the alternative route .",
    "let @xmath141 be the sum of @xmath142 waiting times , which are generated by independent exponential processes : @xmath143 then we may derive the probability density function of the stochastic @xmath141 from the product of @xmath142 exponential distributions , @xmath144 and some appropriate jacobian transformation like , for example , @xmath145    by way of and , while keeping track of the appropriate integration limits of the @xmath39 unwanted parameters , @xmath146 , we may find the probability distribution of @xmath141 : @xmath147    now , as it turns out , the cumulative distribution function of @xmath141 , that is , the sum of @xmath142 exponential waiting times , @xmath148 is equivalent to the probability of observing more than @xmath39 events in the time period @xmath35 , @xmath149 so , the equivalence relationship we will make use of is , and , see also , @xmath150 or , equivalently , @xmath151    in words , if the sum of waiting times for @xmath142 waiting times is smaller than @xmath35 , then it follows that we have observed more than @xmath39 events occurring at time @xmath35 . the equal sign in implies that both states of knowledge have the same truth - value ,",
    "that is , are equivalent .",
    "likewise , if the sum of waiting times for @xmath152 events exceeds @xmath35 , then it follows that we yet have to observe more than @xmath39 events occurring at time @xmath35 . the equal sign in implies that both states of knowledge have the same truth - value ,",
    "that is , are equivalent",
    ".    the product and and sum rules of bayesian probability theory are derived by way of consistency constraints , where consistency is operationalized as follows .",
    "if there are there two different routes that lead us to the self - same proposition , then these routes should result in the same probability assignment .",
    "this then explains the equivalencies and , consistency demands it , @xcite .",
    "the corollary of the equivalence relation is , see , @xmath153 where @xmath154 .",
    "it may be easily checked that the corollary equivalence does indeed hold .",
    "now , we may compute the cumulants of the poisson distribution of @xmath141 either by way of the evalution of the moments of the probability distribution or by way of the evalution of the moments of the stochastic .",
    "the cumulants of a given exponential waiting time is given as : @xmath155",
    "so , the cumulants of @xmath152 exponential waiting times , , are given as : @xmath156    subtituting the cumulants in the maxent approximative distribution .",
    "] , with the integral limits @xmath157 for the integral function , and going through the motions , we obtain for , say , @xmath158 the following maxent distribution :     exponential waiting times , scaledwidth=70.0% ]    we can compare figure  [ fig : plot1 ] with the actual distribution , for @xmath158 :     exponential waiting times , scaledwidth=70.0% ]    by way of the equivalency , the road is now opened to evaluate the first cumulants of .",
    "this will allow us construct the maxent approximation , as discussed in section  [ maxent ] , of the beta - like distribution of a poisson - like process , where the event generating mechanism follows a weibull instead of an exponential distribution .",
    "let @xmath141 be the sum of @xmath142 waiting times , which are generated by independent weibull processes : @xmath159 where the weibull distribution is given as @xmath160.\\ ] ] the cumulants of a given weibull waiting time are given as , : @xmath161 which evaluates to @xmath162^{\\frac{3}{2 } } } , \\\\",
    "\\kappa & = \\frac{k^{4 } \\ :",
    "\\gamma\\!\\left(\\frac{k+4}{k}\\right ) -12 \\ : k^{2 } \\ : \\gamma\\!\\left(\\frac{3}{k}\\right ) \\gamma\\!\\left(\\frac{1}{k}\\right )   + 12 \\ : k \\ : \\gamma\\!\\left(\\frac{2}{k}\\right ) \\gamma\\!\\left(\\frac{1}{k}\\right)^{2 }    - 3 \\ : \\gamma\\!\\left(\\frac{1}{k}\\right)^{4}}{k^{4 } \\left[\\gamma\\!\\left[\\frac{k+2}{k}\\right ) -",
    "\\gamma\\!\\left(\\frac{k+1}{k}\\right)^{2}\\right]^{2}}. \\notag \\end{aligned}\\ ] ] so , the cumulants of @xmath152 weibull waiting times , , are given as , : @xmath163 likewise , the cumulants of @xmath39 weibull waiting times , , are given as , : @xmath164    the cumulants and may be used to construct the maxent approximations of , respectively , the distributions of @xmath152 and @xmath39 weibull waiting times .",
    "these maxent distributions may then substituted into , in order to get an approximation of the intractable poisson - like probability model .    in order to construct the beta - like distribution of the poisson - like probability model , the maxent distributions that take as their inputs the cumulants in and , which are functions of the unknown parameters @xmath62 and @xmath38 , , have to be weighted by the weibull posterior , and summated .",
    "for example , if partition the 6-sigma of @xmath165-parameter space in a @xmath39-by-@xmath39 grid , then we substitute the center coordinates of the @xmath165 squares in the cumulants and , construct and weigh the resulting @xmath166 maxent distributions with probability volumes of the corresponding squares , we then summate these weighed maxent distributions , which will leave us with an approximation of the highly intractable beta - like distribution of the poisson - like probability model .    likewise , if we wish to find the beta - like distribution of the poisson - like equivalence of , then by way of , , and , we may construct , with the above described procedure , of weiging maxent distributions , its approximative distribution .",
    "in bayesian statistic there are four entitities of interest : the prior , the likelihood , the posterior , and the evidence . now , anyone somewhat familiar with bayesian statistics probably knows about the first three of these entitites , since these are needed for bayesian parameter estimation .",
    "however , the fourth entity , the evidence , essential for bayesian model selection , is less well known .",
    "this is unfortunate . because , even if the posterior represents the optimal parameter estimation procedure , if the model employed is inappropriate , then the optimality of the parameter estimation procedure will not make the underlying model less inappropriate .",
    "and we quote skilling :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ i know of no other discipline in which half of the principal equation is so widely ignored , and it should not be ignored here either .",
    "i could ( and often do ) argue that the evidence @xmath167 is even more important than the posterior @xmath168 on the frounds that algebraically it has to be evaluated first , and logically there s no need to proceed to the posterior if the evidence is unacceptably weaker than that from some other candidate .",
    "so it s the posterior that is subordinate to the evidence and definitely _ not _ the other way around .",
    "i myself think of `` bayesian inference '' as the generation of the evidence , with the posterior following if needed .",
    "evidence is primary . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    now , the reason that we have gotten as far as we have , algebraically speaking , without introducing the concept of the evidence , is because we have made use of the fact that the prior time the likelihood is proportional to the posterior : @xmath169 where @xmath170 is proportional to the prior @xmath171 and @xmath172 is proportional to @xmath173 , the probability of the data given the parameter @xmath38 .",
    "note that the normalizing constant @xmath175 is only equal to the evidence @xmath176 if the @xmath177 and @xmath178 are both properly normalized ; where the former is normalized over the unknown parameter @xmath38 and the latter over the data @xmath17 .",
    "note that until now , by way of the use of proportionality sign , we have used the bayesian short hand to present our posteriors . in",
    "what follows , we will compute , for demonstrative purposes , the evidences for the models in which the generating failure mechanisms are exponential and weibull , respectively .",
    "but first we will give a simple outline of the procudure of bayesian model selection .",
    "let @xmath179 be the prior of some parameter @xmath38 , conditional on the prior background information @xmath180 .",
    "let @xmath181 be the probability of the data @xmath17 , conditional on a given parameter @xmath38 and the particular likelihood model @xmath182 which was invoked .",
    "let @xmath183 be the posterior distribution of @xmath38 , conditional on the data @xmath17 , the likelihood model @xmath182 , and the prior background information @xmath180 .",
    "we then have , by way of the product rule , or , equivalently , bayes theorem , that , @xcite : @xmath184 where @xmath185 is the marginalized likelihood of the model @xmath182 and the background information @xmath180 , also known as the evidence of @xmath182 and @xmath180 .",
    "note that the evidence judges judges both the likelihood model , @xmath182 , as well as the prior model , @xmath180 , that went into the construction of the posterior .",
    "now , this could be seen as a weakness of bayesian model selection , since in general all the ingenuity goes into the construction of a sophisticated likelihood model .",
    "so , why bother with some uninformative prior , when we only want to compare different likelihood models ?      firstly , there are instances , for example in image reconstruction @xcite , where all the artfulness goes into the construction of the prior , and there we have that it is the likelihood which is trivial .",
    "so , it is precisely because the bayesian probability theory puts the prior and likelihood models automatically on an equal footing , that bayesian model selection can differentiate between the different prior models of image reconstruction inference problems , without breaking down .",
    "secondly , by judging the prior the evidence automatically guards us against the danger of over - parametrization , that is , choosing such a complex likelihood model , in terms of the number of parameters employed , that we fit the noise in the data as structural part of the data .",
    "say we have @xmath80 different likelihood models , @xmath186 , to choose from and one class of , say , uninformative prior background models , @xmath180",
    ". then we may compute @xmath80 different evidence values @xmath187 , for @xmath188 .",
    "let @xmath189 be the prior probability distribution of the likelihood models @xmath190 , and let @xmath191 be the posterior probability distribution of these models , conditional on the data and the general prior background information @xmath180 .",
    "then we have that @xmath192 for @xmath188 .",
    "note that if we have that @xmath193 , for @xmath194 , then we have that reduces to @xmath195 stated differently , if we assign equal prior probabilities to our different likelihoods models , then the posterior probabilities of these likelihood models reduce to their normalized evidence values .",
    "this , then , is why the likelihood models may be ranked by their respective evidence values , @xcite .      for the exponential model , which we designate @xmath196",
    ", we have that the likelihood model is given as , , @xmath197 \\prod_{i=1}^{r } dx_{i}.\\end{aligned}\\ ] ] as a prior we take the properly normalized uninformative prior : @xmath198 where @xmath199 is the normalizing constant @xmath200 where @xmath201 and @xmath202 define the prior range of possible values of @xmath38 .",
    "cogent prior information regarding @xmath201 is that value of @xmath38 for which , for some given time interval @xmath203 , the expection value @xmath84 would become so small that too few failures would be witnessed in said time period .",
    "cogent prior information regarding @xmath202 is that value of @xmath38 for which , for some given time interval @xmath203 , the expection value @xmath84 would become so large that too many failures would be witnessed in said time period .",
    "multiplying the properly normalized likelihood with the the properly normalized prior , we obtain the the properly normalized bivariate distribution of both the parameter and the data : @xmath204 \\prod_{i = 1}^{r } dx_{i},\\ ] ] which , being properly normalized , will allow us to evaluate the evidence of @xmath196 .    by way of and",
    ", we then evaluate the evidence for the exponential model as @xmath205 d\\lambda \\\\          & \\approx c_{\\lambda }   \\frac{\\left(r-1\\right)!}{\\left(\\sum_{i } x_{i } + \\sum_{j } y_{j}\\right)^{r } } \\prod_{i } dx_{i}. \\end{aligned}\\ ] ]    for the weibull model , which we designate @xmath206 , we have that the likelihood model is given as , , @xmath207 d x_{i } \\prod_{j=1}^{n - r } \\exp\\left[\\left(-\\lambda y_{j}\\right)^{k}\\right ] \\notag \\\\      & = \\lambda^{r k } k^{r } \\exp\\left[- \\lambda^{k } \\left(\\sum_{i=1}^{r }   x_{i}^{k } + \\sum_{j=1}^{n - r }   y_{j}^{k}\\right)\\right ] \\prod_{i=1}^{r } x_{i}^{k-1 } dx_{i}.\\end{aligned}\\ ] ]    as a prior we take the properly normalized uninformative prior : @xmath208 where @xmath199 is as in and @xmath209 is the normalizing constant @xmath210 where @xmath211 and @xmath212 define the prior range of possible values of @xmath62 .    multiplying the properly normalized likelihood with the the properly normalized prior , we obtain the the properly normalized bivariate distribution of both the parameters and the data : @xmath213 \\prod_{i=1}^{r } x_{i}^{k-1 } dx_{i},\\ ] ] which , being properly normalized , will allow us to evaluate the evidence of @xmath206 .    by way of and , we then evaluate the evidence for the weibull model as @xmath214 \\prod_{i } x_{i}^{k-1 } \\ : d\\lambda \\ : dk \\notag \\\\          & \\approx c_{\\lambda } c_{k }   \\ : \\left(r-1\\right ) ! \\ : \\prod_{i } dx_{i }   \\ : \\int_{a_{k}}^{b_{k } } \\frac{k^{r-2 } \\prod_{i } x_{i}^{k-1}}{\\left(\\sum_{i }   x_{i}^{k } + \\sum_{j }   y_{j}^{k}\\right)^{r } }   \\ : dk,\\end{aligned}\\ ] ] where the integral over unknown shape parameter @xmath62 must be evaluated numerically .",
    "say , we do not have any prior preference for either model @xmath196 or @xmath206 . then , letting the data speak for itself , we assign equal prior probabilities to both likelihood models .",
    "we then , by way of , , and , may compute the posterior probability of @xmath196 : @xmath215 \\frac{1}{\\left(\\sum_{i } x_{i } + \\sum_{j } y_{j}\\right)^{r}}}{\\left[c_{\\lambda } \\left(r-1\\right ) !",
    "\\prod_{i } dx_{i}\\right ]   \\frac{1}{\\left(\\sum_{i } x_{i } + \\sum_{j } y_{j}\\right)^{r } } + \\left[c_{\\lambda } \\left(r-1\\right ) !",
    "\\prod_{i } dx_{i } \\right ] c_{k } \\int \\frac{k^{r-2 } \\prod_{i } x_{i}^{k-1}}{\\left(\\sum_{i }   x_{i}^{k } + \\sum_{j }   y_{j}^{k}\\right)^{r } } \\ : dk } \\notag \\\\          & = \\frac{\\left(\\sum_{i } x_{i } + \\sum_{j } y_{j}\\right)^{-r}}{\\left(\\sum_{i } x_{i } + \\sum_{j } y_{j}\\right)^{-r } + c_{k } \\int \\frac{k^{r-2 } \\prod_{i } x_{i}^{k-1}}{\\left(\\sum_{i }   x_{i}^{k } + \\sum_{j }   y_{j}^{k}\\right)^{r } } \\ : dk } , \\end{aligned}\\ ] ] where we have cancelled out all the terms shared by both evidence values and .",
    "furthermore , seeing that , the normalizing constant @xmath199 , , cancels out , we may let be an improper prior and let the constants of integration go to @xmath216 and @xmath217 .",
    "this allows us to replace the ` approximately - equal - to ' signs in and with an equality signs , which propagates through in .    by way of , , and , we may also compute the posterior probability of @xmath206 : @xmath218 c_{k } \\int \\frac{k^{r-2 } \\prod_{i } x_{i}^{k-1}}{\\left(\\sum_{i }   x_{i}^{k } + \\sum_{j }   y_{j}^{k}\\right)^{r } } \\ : dk}{\\left[c_{\\lambda } \\left(r-1\\right ) !",
    "\\prod_{i } dx_{i}\\right ]   \\frac{1}{\\left(\\sum_{i } x_{i } + \\sum_{j } y_{j}\\right)^{r } } + \\left[c_{\\lambda } \\left(r-1\\right ) !",
    "\\prod_{i } dx_{i } \\right ] c_{k } \\int \\frac{k^{r-2 } \\prod_{i } x_{i}^{k-1}}{\\left(\\sum_{i }   x_{i}^{k } + \\sum_{j }   y_{j}^{k}\\right)^{r } } \\ : dk } \\notag \\\\          & = \\frac{c_{k } \\int \\frac{k^{r-2 } \\prod_{i } x_{i}^{k-1}}{\\left(\\sum_{i }   x_{i}^{k } + \\sum_{j }   y_{j}^{k}\\right)^{r } } \\ : dk}{\\left(\\sum_{i } x_{i } + \\sum_{j } y_{j}\\right)^{-r } + c_{k } \\int \\frac{k^{r-2 } \\prod_{i } x_{i}^{k-1}}{\\left(\\sum_{i }   x_{i}^{k } + \\sum_{j }   y_{j}^{k}\\right)^{r } } \\ : dk } , \\end{aligned}\\ ] ] where we have again cancelled out all the terms shared by both evidence values and .      note that for @xmath38 we , eventually , used an improper uninformative prior .",
    "we did so when we let constants of integration in go to @xmath216 and @xmath217 ; thus , giving us a normalizing constant of @xmath219 or , equivalently , @xmath220    the only reason we had the freedom to do so was because the constant @xmath199 cancelled out in and .",
    "thus , removing the inverse infinities , which resulted from the improper , that is , diverging , prior of @xmath38 .",
    "now , had we done the same for constants of integration in , then we would have obtained an inverse infinity @xmath221 , which would have not cancelled out in and ; thus giving us posterior model probabilities : @xmath222 and @xmath223    we see in and how bayesian model selection may punish us for non - parsimoneous priors when the normalizing constants of these priors do not cancel out in the posterior of the competing likelihood models .",
    "to the uninitiated this may seem as a bother .",
    "but we bayesians would not have it any other way . because it is this penalizing mechanism of bayesian model selection , which is just a straight forward of the product and sum rules , which automatically protects us from the dangers of over - fitting .    for example",
    ", these authors had to choose among regression models having four up to a thousand possible regression coefficients to model noisy data . by deriving a parsimoneous prior for the regression coefficients,@xcite , we were able to let the data do the talking .",
    "we found that the bayesian probability theory picked the likelihood model having only sixty - four parameters .",
    "those models having more parameters , though having a better likelihood fit , because of the greater number of parameters , were penelized for their prior probability volume and , as consequence , noise was minimally fitted as part of the structure .",
    "erp  van h.r.n . , linger r.o . , and gelder  van p.h.a.j.m .",
    ": _ constructing cartesian splines_. the open numerical methods journal , 3 , 26 - 30 , ( 2011 ) .",
    "but we recommend to search for the unmutilated arxiv version of this article : arxiv:1409.5955 [ math.na ] , ( 2014 ) .",
    "jaynes e.t . : _ confidence intervals vs bayesian intervals ; reply to kempthorne s comments _ , w.l .",
    "harper and c.a .",
    "hooker , eds . foundations of probability theory , statistical inference , and statistical theories of science , reidel publishing co. , dordrecht , holland , ( 1976 ) ."
  ],
  "abstract_text": [
    "<S> the most well known probability distribution of probabilities is the beta distribution . if we have observed @xmath0 ` successes ' , each having a probability @xmath1 , and @xmath2 ` failures ' , each having a probability @xmath3 . in this paper </S>",
    "<S> we will derive a whole family of beta - like distributions , which take as their data not only the number of successes and failures , but also values on predictor variables and time to failure or time without failure . </S>"
  ]
}