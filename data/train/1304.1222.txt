{
  "article_text": [
    "in this paper we develop the results of  @xcite .",
    "we consider tensor  structured linear systems , which arise naturally from high  dimensional problems , e.g. pdes .",
    "the number of unknowns grows exponentially w.r.t .",
    "the number of dimensions @xmath0 which makes standard algorithms inefficient even for moderate @xmath1 this problem is known as the _ curse of dimensionality _ , and is attacked by different low  parametric approximations , e.g. _ sparse grids _  @xcite and _ tensor product methods _  @xcite .",
    "a particularly simple , elegant and efficient representation of high  dimensional data is a linear tensor network , also called the  _ matrix product states _ ( mps ) and  _ tensor train _ ( tt ) format .",
    "the mps approach was originally proposed in the quantum physics community to represent the quantum states of many ",
    "body systems  @xcite .",
    "this representation was re - discovered as the tt format by oseledets and tyrtyshnikov  @xcite , who were looking for a proper method to generalize a low  rank decomposition of matrices to high  dimensional arrays ( tensors ) .",
    "the mps approach came with the _ alternating least squares _ ( als ) and _ density matrix renormalization group _ ( dmrg )",
    "@xcite algorithms for the ground state problem .",
    "the als considers the minimization of the rayleigh quotient over the vectors with a fixed tensor structure , while dmrg does the same allowing the rank of the solution to change .",
    "experiments from quantum physics point out that the convergence of the dmrg is usually notably fast , while the one of the als can be rather poor .",
    "the general numerical linear algebra context in which the tt format is introduced allows to think more widely about the power of tensor representations .",
    "for instance , we can apply dmrg  like techniques to high  dimensional problems other than just the ground state problem , e.g. interpolation of high - dimensional data  @xcite , solution of linear systems  @xcite , fast linear algebra in tensor formats  @xcite .",
    "we can also consider better alternatives to the dmrg , which follow the same _ alternating linear scheme _ ( als ) framework , but are numerically more efficient .",
    "a tempting goal is to obtain an algorithm which has the dmrg - like convergence and the als - like numerical complexity . in  @xcite we present such an algorithm for a solution of symmetric positive definite ( spd ) linear systems in higher dimensions .",
    "the central idea in  @xcite is to support the alternating steps , i.e. optimization in a fixed tensor manifold , by steps which _ expand _ the basis in accordance with some classical iterative algorithms .",
    "a _ steepest descent _ ( sd ) algorithm is a natural choice for spd problems .",
    "the _ enrichment _ step uses the essential information about the _ global _ residual of the large high  dimensional system on the local optimization step , that helps to escape the spurious local minima introduced by the nonlinear tensor formulation and ensure the global convergence .",
    "the convergence rate of the whole method can then be established adapting a classical theory . in contrast , optimization in the fixed tensor manifolds can be analyzed via the gauss ",
    "seidel theory and only _ local _ convergence estimates are available  @xcite , which hold only in a ( very ) small vicinity of the exact soution .",
    "the global enrichment step used in algorithms `` @xmath2 '' and `` @xmath3 '' in  @xcite modifies all components of the tensor train format simultaneously .",
    "there is nothing particularly wrong with this , but it is interesting to mix the same steps differently to obtain the algorithm which works with only one or two neighboring components at once , similarly to the dmrg technique . in this paper",
    "we develop such a method , namely the _ alternating minimal energy _ ( amen ) algorithm .",
    "we prove the global convergence of amen and estimate the convergence rate w.r.t .",
    "the one of the steepest descent algorithm .",
    "we also propose several methods to compute the required local component of the global residual , using either the svd ",
    "based approximation , or incomplete cholesky decomposition , or low ",
    "rank als approximation .",
    "the rest of the paper is organized as follows . in section [ sec : def ] we introduce necessary definitions and notations . in section [ sec : amen ] we propose the amen algorithm , then we compare it with similar algorithms from  @xcite and prove the convergence theorem . in section [ sec : prac ] we discuss efficient methods to compute the required component of the residual . in section  [ sec : num ] we test the algorithm on a number of high  dimensional problems , including the non - symmetrical fokker ",
    "planck and chemical master equations , for which the efficiency of the method is not fully supported by the theory . in all examples",
    "we observe a convincing fast convergence and high efficiency of the proposed method , as well as the advantages of the amen algorithm over the previously proposed ones .",
    "this paper is based on the notations of  @xcite , which we recall briefly here .",
    "we consider linear systems @xmath4 in @xmath5dimensional space , i.e. assume that a vector @xmath6 has @xmath5 indices @xmath7 and @xmath8 @xmath9 such arrays are referred to as @xmath5tensors @xmath10.$ ] for the purposes of this paper it is convenient to consider a _ vectorization _ of a tensor @xmath11 where @xmath12 denotes a single index combined from @xmath13 or _ little  endian _ convention @xmath14 the big ",
    "endian notation is similar to numbers written in the positional system , while the little  endian notation is used in numerals in the arabic scripts and is consistent with the fortran style of indexing . the definition of the kronecker ( tensor ) product @xmath15 should be also consistent with the chosen endianness .",
    "the orthodox definition in linear algebra assumes the big ",
    "endianness , while the development of the efficient program code usually makes us think in the little  endian way .",
    "the rest of the paper can be read without a particular care of the endianness .",
    "it is enough to remember that @xmath16 means @xmath17 .",
    "this _ index grouping _ is widely used throughout the paper . in the following",
    "we do not distinguish between @xmath18  and  @xmath19    the tensor train ( tt ) representation of @xmath6 is written as the following multilinear map maps tensor train cores to a vectorized representation of a @xmath5tensor , not to the @xmath5tensor itself , cf .",
    "_ quantized _ tensor train ( qtt )  @xcite . in this paper",
    "we do not distinguish between them and keep the notation simple .",
    "] , @xmath20 where @xmath21 are referred to as _ mode _ ( physical ) indices , @xmath22 are the _ rank _ indices , @xmath23 are the tensor train _ cores _ ( tt  cores ) and @xmath24 denotes the tensor train .",
    "we follow the einstein summation convention , which assumes a summation over every pair of repeated indices .",
    "all equations are supposed to hold for all possible values of free ( unpared ) indices .",
    "the @xmath25 mapping is defined also for a subset of tt  cores ( a _ subtrain _ ) and maps it to the _ interface _ matrix of size @xmath26 defined as follows , @xmath27 and similarly for symbols @xmath28 and @xmath29 for @xmath6 given by   we have @xmath30    note that the definition of @xmath25 allows us to write @xmath31 where two last mappings depict the decompositions used in als and dmrg algorithms proposed by s. white et al .",
    "@xcite for the ground state problem in quantum physics .",
    "the original dmrg algorithm is formulated via the minimization of the rayleigh quotient , where the heavily nonlinear high  dimensional optimization is reduced to the sequence of numerically tractable optimizations over the elements of each core .",
    "similarly , we consider the solution of a linear equation @xmath4 through the minimization of the _ energy _",
    "function @xmath32 where @xmath33 is the exact solution , and @xmath34 denotes the @xmath35norm of a vector @xmath36 following the _ alternating linear scheme _ ( als ) ,",
    "the high - dimensional minimization is reduced to the minimization w.r.t .",
    "all cores one - by - one .",
    "each _ local _",
    "minimization is equivalent to the solution of a linear system , which is tractable due to a moderate size .",
    "the high - dimensional linear system can be split into a sequence of one - dimensional systems due to the _ linearity _ of the tensor train format @xmath37 w.r.t .",
    "each tt  core @xmath38 this linearity writes as the following matrix - by - vector product @xmath39 where the vectorized tt  core @xmath40 is a reshape of the three - dimensional array into a vector @xmath41 the elementwise definition of the _ frame matrix _",
    "@xmath42 is the following @xmath43 where @xmath44 is the kronecker symbol , i.e. , @xmath45 if @xmath46 and @xmath47 elsewhere .",
    "similarly we define frame matrices @xmath48 @xmath49 @xmath50 for example , @xmath51 writes @xmath52 the @xmath35orthogonal projector on the subspace @xmath53 is defined as follows @xmath54 it is easy to check that @xmath55 and @xmath56 also , for any @xmath57 such that @xmath58 it holds @xmath59 hence the name @xmath35orthogonal .",
    "one of the main results of the previous paper  @xcite is the algorithm @xmath60 each iteration of this algorithm consists of one ` global ' basis enrichment step which changes all the cores , followed by @xmath5 update steps over all the cores subsequently .",
    "classical optimization algorithms for tensor networks , e.g. als and dmrg , follow the alternating linear framework , i.e. update one or two neighboring cores at a time",
    ". we would like to keep the enrichment as well as other steps within the same idea , and propose another version of the method as alg .",
    "[ alg : amen ] , which we will refer to as the _ alternating minimal energy algorithm _ ( amen ) .",
    "the difference between two algorithms is illustrated by a simple three ",
    "dimensional example in fig .",
    "[ fig : alsamen ] .    to analyse the convergence of the @xmath3 algorithm",
    ", we can see it as a method which implements the ( approximate ) steepest descent step followed by a sequence of optimization steps for the energy function .",
    "the approximate steepest descent step @xmath61 with @xmath62 and optimal @xmath63 gives  ( * ? ? ?",
    "1 ) the convergence rate @xmath64 where @xmath65 denotes the progress of the exact steepest descent step . a fortiori",
    ", the _ global convergence _ of the @xmath3 is proven with the convergence rate not slower than @xmath66 the amen algorithm does not have a global enrichment step and the convergence can not be proven in one line . however , the convergence analysis is possible if alg .",
    "[ alg : amen ] is seen as a recurrent method . though the theoretical estimates do not provide a clear distinction which method is preferable , in numerical experiments in sec .",
    "[ sec : num ] we will observe that the amen technique delivers more accurate solution , while the average convergence rate is almost the same as of the @xmath3 method .      the idea of the convergence analysis is introduced by a two  dimensional example , see fig .",
    "[ fig : amen ] . in two dimensions alg .",
    "[ alg : amen ] can be seen as a sequence of the following operations .    1 .",
    "start from the initial guess @xmath67 2 .",
    "update the first tt  core , minimizing the energy function over the entries of @xmath68 @xmath69 3 .",
    "expand the basis in the first core using the first tt  core of the residual .",
    "@xmath70 4 .",
    "perform the galerkin correction step by minimizing the energy function over the bottom part of second tt  core .",
    "@xmath71 5 .",
    "minimize the energy function over all entries of the second tt ",
    "core @xmath72    the als update steps 2 and 5 reduce the energy function by a factor @xmath73 and @xmath74 respectively , which can be rigorously estimated only locally , i.e. in a very small vicinity of a true solution .",
    "the basis enrichment step 3 does not provide any progress , because it does not change the solution vector , but only its tt  representation .",
    "the galerkin correction step 4 does not technically present in the algorithm .",
    "if we omit it , the update step 5 will deliver the same tt ",
    "core , optimizing the energy function over the positions occupied by both upper and bottom parts of the second core . without actually affecting the result of the computations ,",
    "step 4 is essential to analyse the convergence of the whole method , since the progress of the galerkin correction step can be estimated w.r.t .",
    "the one of the steepest descent .",
    "this idea is formally expressed as the following theorem .",
    "[ thm : amen2 ] in the notations set above for the two - dimensional linear system @xmath4 one iteration of amen alg .",
    "[ alg : amen ] provides the following progress @xmath75 @xmath76 where @xmath77 @xmath78 and @xmath79 is defined by @xmath80    the minimization in step 5 is written as @xmath81 @xmath82 cf .  ,",
    "and the gradient is zero when the galerkin conditions @xmath83 are met .",
    "the solution after one amen iteration writes as follows @xmath84 where @xmath85 is the @xmath35orthogonal projector on @xmath86 cf .  .",
    "since @xmath87 we have @xmath88 similarly , the progress of the galerkin correction step 4 is estimated as follows ( see  @xcite ) , @xmath89 it is not easy to estimate @xmath90 directly . however , since @xmath91 is a part of @xmath92 we have @xmath93 and therefore @xmath94 similarly , since @xmath95 it holds @xmath96 where @xmath97 denotes a progress of the perturbed steepest descent step . the final estimate for @xmath97",
    "is obtained in  ( * ? ? ?",
    "* thm 1 ) with a precise derivation of the asymptotic @xmath98 term .",
    "to finish the proof we note that @xmath99 by construction of step 2 .",
    "[ rem : less1 ] a convergence rate @xmath65 of the steepest descent algorithm is estimated using the kantorovich inequality",
    "as follows @xmath100 where @xmath101 and @xmath102 denote the smallest and largest eigenvalues of @xmath103 for any @xmath65 we can choose such a threshold level @xmath104 that @xmath105 which guarantees the global convergence of the amen algorithm .",
    "system @xmath4 in the tt  format ,",
    "initial guess @xmath106 updated solution @xmath107 update @xmath108 , for local problem   approximate @xmath109 expand the basis @xmath110 recover the tt ",
    "orthogonality of @xmath111 .",
    "@xmath112      in higher dimensions , the amen alg .",
    "[ alg : amen ] can be described using the same scheme .",
    "we start from an initial guess @xmath113 in step 2 we update the first core obtaining @xmath114 in step 3 we approximate the residual @xmath115 and expand the first core by @xmath116 these operations are numerically tractable , see sec .",
    "[ sec : prac ] for details .",
    "minimization problem in step 5 leads to the following linear system @xmath117 which has @xmath118 unknowns and is still too large to be solved directly .",
    "note that @xmath119 is a rank-1 multilevel matrix , hence @xmath120 has the same tt  ranks as @xmath35 , but its tt  representation is shorter by one core .",
    "similarly , @xmath121 represents a smaller right - hand side in the tt format , and the solution @xmath122 is sought in the tt format as well .",
    "therefore , the linear problem in @xmath5 dimensions is reduced to the one in @xmath123 dimensions , i.e. to the minimization over the remaining subtrain , and the same algorithm is applied recurrently .",
    "the convergence rate of amen is defined by a recurrent application of the result of thm .",
    "[ thm : amen2 ] .",
    "we need to consider a sequence of the reduced problems @xmath124 the initial guess is @xmath125 the update step for the core @xmath126 gives @xmath127 further steps of amen return the solution @xmath128 and the true solution is defined by @xmath129 similarly to thm .",
    "[ thm : amen2 ] we have @xmath130 where @xmath131 @xmath132 and @xmath133 denotes the @xmath134orthogonal projector to @xmath135 with these definitions in hand we write the following theorem .",
    "[ thm : amen ] the amen alg .",
    "[ alg : amen ] converges globally with the following convergence rate @xmath136    in two dimensions , the theorem reduces to thm .",
    "[ thm : amen2 ] , which proves the base of recursion .",
    "we suppose that   holds in @xmath123 dimension and prove it recurrently for @xmath5 dimensions .",
    "if @xmath137 is the initial guess , @xmath138 appears after the update of the first core , and @xmath139 is the result returned by one iteration of amen , the error @xmath140 is written as follows @xmath141 where the first term is the error of the first ( ` outer ' ) amen step provided the solution @xmath142 of the reduced problem   is computed exactly , and the second term is the error of other ( ` inner ' ) amen steps , which we find using the assumption of the recurrence . using   we show that these terms are @xmath35orthogonal , @xmath143    the first term writes by thm .",
    "[ thm : amen2 ] as follows , @xmath144    for the second term we need the following norm equivalence , @xmath145 where @xmath146 in the right  hand side we see the error norm of the amen algorithm applied to the linear problem   with @xmath123 cores .",
    "according to our assumption , it writes by   as follows , @xmath147 and for the second term we obtain @xmath148 in the numerator we simplify @xmath149 cf .  , and @xmath150 finally , we write last part of the seconf term as follows , cf .",
    ", @xmath151    substituting both terms into  , we complete the proof by @xmath152 where @xmath153 evaluates the convergence rate of the amen in @xmath123 dimensions by  .    from the recurrence relation   it is clear that if the convergence of the reduced problem @xmath154 then the convergence rate of the full problem @xmath155 by remark  [ rem : less1 ] , we can always choose the approximation threshold @xmath104 to ensure @xmath156 for the amen algorithm in two dimensions .",
    "therefore , we can always choose @xmath104 to provide @xmath157 which guarantees the global convergence of alg .",
    "[ alg : amen ] .    similarly to thm .",
    "[ thm : amen2 ] we can estimate @xmath158 in   as follows , @xmath159 where @xmath160 @xmath161 and @xmath162 in the right  hand side we see the convergence rate of the perturbed steepest descent method applied to the reduced problem with the matrix @xmath163 it is estimated  ( * ? ? ?",
    "* thm 1 ) as follows @xmath164 where @xmath104 denotes the relative accuracy of @xmath165 it can be shown that if all @xmath166 are orthogonal we have @xmath167 @xmath168 where the last term estimates the convergence rate of sd algorithm applied to @xmath169    the requirement for @xmath166 to be orthogonal for @xmath170 is equivalent to the _ tt  orthogonality _ of the tensor train @xmath171 see  @xcite for details . as a counterpart of",
    ", we may say that this requirement prevents the condition numbers of reduced matrices in from increasing , which is essential for numerical stability . by construction , @xmath172 does not provide the tt ",
    "orthogonality of @xmath173 an additional step is required to _ recover _ the orthogonality , i.e. make the tt ",
    "core @xmath23 column  orthogonal .",
    "it is done via a qr decomposition @xmath174 we denote the result after orthogonalization by the same symbol @xmath175 all considerations in theorem [ thm : amen ] remain valid , since all estimates are based on the subspaces , which are unaffected by the qr decomposition . therefore , we imply the tt ",
    "orthogonality silently to simplify the discussion , and the actual operation is fast and does not influence the analysis .    in ( * ? ? ?",
    "3 ) , the convergence rate of the _ greedy descent _",
    "algorithm @xmath2 is given by exactly the same formula as  , but the values @xmath158 are defined differently .",
    "in the amen method , @xmath158 is given by   and relates to the convergence of the reduced problem  .",
    "for the algorithm @xmath2 it is defined as @xmath176 in terms of  . since @xmath177 , for the greedy algorithm it holds @xmath178 for the amen algorithm , we can prove this only for upper bounds @xmath179 as shown in  .",
    "considering the ` width ' of @xmath180 and @xmath181 , we can expect that @xmath182 which is observed in numerical experiments .",
    "it is not clear however whether this heuristic statement holds in general .",
    "in this section we discuss how to compute the approximation on step 3 in amen alg .",
    "[ alg : amen ] efficiently .      in steepest descent schemes proposed in @xcite ,",
    "the low - rank approximation of the residual is computed once per iteration , and a standard svd - based tt - rounding procedure from  @xcite can be used . in amen alg .",
    "[ alg : amen ] we can not approximate each @xmath183 individually by the tt - svd , since it makes the total complexity quadratic in the dimension @xmath5 . to keep the complexity linear in @xmath0 we have to investigate the tensor structure of @xmath184    looking at the tt representation of the reduced system and recalling that @xmath185 has a rank - one structure , we conclude that @xmath186 inherits the blocks @xmath187 from @xmath188 as follows , @xmath189 where @xmath190 .",
    "the similar representation holds for the local matrix @xmath191 .",
    "therefore the local residual @xmath192 writes @xmath193 @xmath194 @xmath195 @xmath196 the tt decomposition of the exact residual @xmath197 has only one block which actually depends on the information gained in the step @xmath198 the others can be precomputed before the iteration . in the approximate residual @xmath199",
    "all blocks depend on the recently computed @xmath200 but only one block @xmath201 is actually required .",
    "this means that if we keep all tt ",
    "cores @xmath202 right  orthogonal , we can compute @xmath203 by the svd compression of @xmath204 only .",
    "therefore , the svd ",
    "based approximation of the residual involves the information from only one core , and the complexity of each enrichment step does not grow with @xmath1 the overall complexity is therefore linear in @xmath0 as required .",
    "the singular value decomposition provides the optimal approximation accuracy for a prescribed rank , but is numerically expensive .",
    "each tt  core @xmath205 has the sizes @xmath206 , where @xmath207 and the qr and svd operations have the complexity @xmath208 which may be inefficient . since a very precise approximation of the residual is not always required , we may avoid expensive qr and svd steps by considering the unfinished cholesky algorithm ( see , e.g.  @xcite ) applied to the gram matrix of the first unfolding of @xmath183 .",
    "a careful implementation allows to reduce the complexity to @xmath209",
    ".    given , its first unfolding reads @xmath210 and the gram matrix @xmath211 computes as follows , @xmath212 where @xmath213 and @xmath214 for @xmath215 .",
    "similarly to the svd  based method , we can precompute @xmath216 before the iteration .",
    "the enrichment vectors are then calculated as the factors @xmath201 in the unfinished cholesky decomposition @xmath217      to reduce the complexity even further , we can approximate @xmath218 using the auxiliary als iteration .",
    "we start from some low - rank initial guess @xmath219 and minimize @xmath220 under the constraint @xmath221 where @xmath222 for a unitary @xmath223 this leads to the extremal condition @xmath224    until the convergence of the fixed - rank als is not proved , this approach is heuristic .",
    "however , as was observed in numerical experiments , it provides the enrichment basis almost of the same quality as the svd - based method , although much faster .",
    "it is enough to conduct two alternating methods simultaneously step by step , which means that only one als update is performed for @xmath225 between the subsequent amen iterations .",
    "the algorithm is organized as follows .",
    "given some low - rank approximation @xmath225 , we assume that @xmath226 is a good approximation basis for @xmath183 as well ( which appears to hold in practice ) . that is , the enrichment is computed as a projection @xmath227 now , we need to update @xmath225 for the forthcoming iterations .",
    "the current solution approximant is @xmath228 , so the tt  core @xmath229 writes @xmath230 note that @xmath229 serves only as an update of the global residual approximation @xmath225 and can not be used as an enrichment directly .",
    "similarly to the previous sections , one may avoid @xmath231 cost of , by performing all calculations involving the same tt blocks ( e.g. @xmath232 ) only once during the amen iteration .",
    "the resulting complexity is therefore that of the fixed - rank als , @xmath233 , where @xmath234 is the tt - rank of @xmath225 . in practice",
    ", it is usually enough to take @xmath235 .",
    "finally , let us note that the minimization @xmath220 is equivalent to the maximization of @xmath236 .",
    "in other words , the amen method solves approximately the following _ minimax _ problem , @xmath237 by performing the subsequent als updates for @xmath225 in the rank-@xmath238 tt format , and @xmath6 in the tt format with a _ varying _ rank @xmath239 .",
    "this allows us to establish a connection between the amen method and the greedy approximations , in particular , the minimax proper generalized decomposition @xcite .",
    "however , the greedy techniques usually perform the optimization over rank-1 separable tensors . as a some improvement one may mention the _ orthogonal _ greedy method , which orthogonalizes the residual to the basis of @xmath240 current canonical factors of the solution ( i.e. selects @xmath240 scalars ) .",
    "the amen approach may be considered as a next milestone in the family of adaptive tensor - structured linear solvers . by updating a larger portion of solution data at a time",
    ", it appears to be more robust and accurate , as was demonstrated in  @xcite .",
    "in these experiments , we compare the matlab versions of amen algorithms proposed above ( svd , chol , als ) with the @xmath3 method from the previous work @xcite , as well as the dmrg method from @xcite .",
    "the amen and dmrg methods were implemented within the framework of the tt - toolbox  2.2 ( routines ` amen_solve2 ` and ` dmrg_solve3 ` , respectively ) , and the computations were done at the linux machine with 2.6 ghz amd opteron cpu , and matlab r2012a .      first , we consider the same symmetric positive definite example as in @xcite .",
    "this is the high  dimensional poisson equation , @xmath241^d , \\qquad \\left .",
    "x\\right|_{\\partial\\omega}=0,\\ ] ] where @xmath242 is the finite difference laplacian discretization on a uniform grid with @xmath243 points in each direction , i.e. , the total size of the system is @xmath244 , and @xmath245 is the vector of all ones .",
    "different greedy - type methods were compared in @xcite , as well as the @xmath3 method .",
    "now we focus on non - greedy techniques , including amen+svd and amen+als , see fig .",
    "[ fig : lap_amen ] .",
    "the tt  rank of the enrichment @xmath225 is @xmath246 , frobenius - norm threshold for the solution @xmath247 , and the problem dimension @xmath248 .",
    "we see that all methods except the dmrg demonstrate comparable performances .",
    "even though in the beginning of the iterations the @xmath3 method seems to be the fastest , it approaches the same cpu times as the amen methods when the rank increases . in this example",
    ", the desired accuracy level is reached by all algorithms .",
    "however , it might be not the case , as we will see in the following .",
    "the second example is the chemical master equation @xcite , applied to the @xmath5-dimensional cascade gene regulatory model @xcite .",
    "this is the huge - sized ode @xmath249 where @xmath250 , @xmath251^{\\otimes d}$ ] , so that @xmath252 , and the operator is formulated as follows , @xmath253 for @xmath254 where @xmath255 is the @xmath256-th identity vector .",
    "the particular model parameters were fixed to the values @xmath257    the chemical master equation serves as an accurate model for gene transcription , protein production and other biological processes .",
    "however , its straightforward solution becomes impossible rapidly with increasing number of species @xmath5 .",
    "existing techniques include the monte - carlo - type methods ( so - called ssa @xcite and its descendants ) , as well as more tensor - related ones : sparse grids @xcite , greedy approximations in the canonical tensor format @xcite and tensor manifold dynamics @xcite .",
    "the first two approaches only relax the curse of dimensionality to some extent ; typical examples involve up to 10 dimensions and may take from 15 minutes to many hours on high - performance machines .",
    "tensor - product low - rank approaches seem to be more promising .",
    "unfortunately , we can not estimate a possible potential of greedy or manifold dynamics methods , whereas up to now our alternating linear solution technique appears to be more efficient . for more intensive study of the cme applications of",
    "the amen and dmrg methods see @xcite and @xcite , respectively .",
    "note that for systems with moderate dimensions and smaller time steps , the dmrg method can be of a good use for such problems , as was demonstrated in @xcite",
    ". however , as we will see , the amen algorithm appears to perform better than dmrg for more complicated problems .",
    "two specific tricks allow to take more benefits from the tensor structuring .",
    "first , we employ the crank - nicolson discretization in time , but instead of the step - by - step propagation , consider the time as a @xmath258-th variable and formulate one global system encapsulating all time steps @xcite , @xmath259 where @xmath260 , @xmath25 is the time step size , and the initial state is @xmath261 , @xmath262 is the first identity vector . in particularly , we choose @xmath263 , @xmath264 , and @xmath265 .",
    "such a time interval is not enough to reach the stationary solution , but the transient process is also of interest . as a result , we end up with a @xmath258-dimensional system of size @xmath266 .",
    "second , we prepare all the initial data and seek the solution not in the @xmath258-dimensional tt - format directly , but in the so - called quantized tt format @xcite : we reshape additionally all tensors to the sizes @xmath267 , and apply the @xmath268-dimensional tt decomposition , but with each mode size reduced to @xmath269 .    however , the matrix is strongly nonsymmetric , which makes difficulties for the dmrg approach .",
    "we fix the truncation tolerance for the solution to @xmath270 , and track the frobenius - norm error of the dmrg solution w.r.t .",
    "the reference one , obtained by the amen+svd method with tolerance @xmath271 , versus the dimension @xmath5 , see fig .",
    "[ fig : casc_dmrg ] .",
    "since the dmrg technique takes into account only local information on the system , its accuracy deteriorates rapidly with the increasing dimension . a stagnation in a local minimum is also reflected by a sharp drop of the cpu time , since the method skips the `` converged '' tt blocks .",
    "this makes the dmrg unreliable for high  dimensional problems , even if the qtt format allows to get rid of large mode sizes .",
    "now , we fix the dimension @xmath272 , and compare both the error and residual accuracies of all methods , as well as the computational times . in all cases ,",
    "the frobenius - norm tolerance was set to @xmath270 , and the enrichment rank to @xmath246 .",
    "first of all , since our methods are proven to converge in the spd case , we shall examine both the initial and symmetrized systems ( fig .",
    "[ fig : casc_amen_symm ] ) .",
    "a well - known way to treat a general problem via a symmetric method is the normal , or symmetrized formulation , @xmath273 .",
    "however , both the condition number and the tt ranks of @xmath274 are the squared ones of @xmath35 , and this approach should be avoided when possible .",
    "three particular techniques are considered : the dmrg method , the amen+svd ( marked as `` amen '' in fig . [ fig : casc_amen_symm ] ) and the @xmath3 one .",
    "the symmetrized versions are denoted by the `` -s '' tag .",
    "in addition , note that the convergence of the methods may be checked locally due to the zero total correction after the enrichment in alg .",
    "[ alg : amen ] : before recomputing the @xmath275-th block , calculate the local residual provided by the previous solution @xmath276 .",
    "if it is below the threshold @xmath277 for all @xmath256 , the method may be considered as converged , and stopped .",
    "occurrences of this fact are marked by red rectangles ( `` stop '' ) .",
    "we observe that the symmetrization allows the dmrg method to converge at least to the accuracy @xmath278 , but increases the cpu time by a factor greater than 100 due to the squaring of the tt ranks and condition number of the matrix .",
    "contrarily , for the amen and @xmath3 methods the symmetrization is completely inefficient and redundant : despite pessimistic theoretical estimates , the nonsymmetric algorithms converge rapidly to an accurate solution approximation .",
    "though the non - symmetrized methods may admit oscillations in the residual , the frobenius - norm error threshold is almost satisfied in both amen and @xmath3 methods .",
    "nevertheless , the amen algorithm appears to be more accurate thanks to the enrichment update in each step .",
    "also , its local stopping criterion is trustful : it fires just after the real error becomes smaller than the tolerance , which is not the case for other methods .    since both amen - type methods in this test",
    "exploit the svd - based residual approximation , they demonstrate almost the same cpu times .",
    "however , using the additional techniques from section [ sec : prac ] we can reduce the complexity while maintaining almost the same accuracy , see fig .",
    "[ fig : casc_var_amen ] .",
    "while the amen+chol method still operates with the exact residual , the amen+als only needs to compute scalar products of the true residual and its low - rank approximation , which makes it more efficient than the amen+svd method , as well as the @xmath3 one .",
    "finally , we test the performance of the two amen realizations with respect to the enrichment rank @xmath238 ( tt - rank of @xmath225 ) , see fig .",
    "[ fig : casc_amr_rho ] .",
    "as expected , the higher @xmath238 is , the more accurate solution can be computed . on the other hand , it is not necessary to pick very large ranks , since the corresponding accuracy improvement does not overcome the significant increase in cpu time .",
    "another example of high - dimensional problems arising in the context of probability distribution modeling , is the fokker - planck equation ( see e.g. @xcite ) . as a particular application , consider the 8-dimensional fokker - planck equation of the polymer micro - model arising in the non - newtonian fluid dynamics @xcite .",
    "the polymer molecules in a solution are subject to the brownian motion , and are often modeled as bead - spring chains ( see fig .",
    "[ fig : beadspring ] ) .",
    "the spring extensions , being the degrees of freedom of the dynamical system , become the coordinates in the fokker ",
    "planck equation .",
    "we consider the case of 4 two - dimensional _ finitely extensible nonlinear elastic _ ( fene ) springs in the shear flow regime according to @xcite , @xmath279 where @xmath280 is the stacked spring extension vectors ( @xmath281 is the displacement of the @xmath282-th spring in the @xmath256-th direction ) , @xmath283 is a spring interaction tensor , @xmath284 is a flow velocity gradient ( shear flow case ) , and @xmath285 is the fene spring force .",
    "note that the singularity in @xmath286 limits the maximal length of a spring to @xmath287 .",
    "moreover , the probability density @xmath288 at the point @xmath289 ( and any with larger modulus ) is zero .",
    "therefore , the domain shrinks to the product of balls @xmath290    a quantity of interest is the average polymeric contribution to the stress tensor , @xmath291 with the normalization assumption @xmath292 .    to recast the problem domain into a hypercube , the polar coordinates are employed , @xmath293 .",
    "the discretization is done via the spectral elements method ( see e.g. @xcite ) .",
    "we will vary the number of spectral elements in each radial direction @xmath294 , but the number of angular elements ( in @xmath295 ) is fixed to @xmath296 . with typical values",
    "@xmath297 , we end up with tensors of size @xmath298 and dense populated matrices , which are intractable in the full format .",
    "since the spectral differentiation matrices are found to be incompressible in the qtt format , the 8-dimensional tt representation is used .",
    "we would like to compute the stationary state of , so we use the simple implicit euler ( inverse power ) method as the time discretization , @xmath299 where @xmath300 is the mass matrix , @xmath35 is the stiffness matrix .",
    "the time integration was performed until @xmath301 , which is enough to approximate the steady state with a satisfactory accuracy , and the ( unnormalized ) initial state was chosen @xmath302 , which corresponds to the zero velocity gradient @xmath303 . since @xmath304 is not a `` time step '' but a parameter of the inverse power method , we will check the performance w.r.t .",
    "@xmath304 as well .",
    "in the previous example we have observed that the amen+svd method is in fact superfluous , since the amen+als method delivers the same accuracy with lower cost . both mode sizes ( up to @xmath305 ) and",
    "tt  ranks ( up to @xmath306 ) in this example are relatively large , so we will consider only the amen+als .",
    "we set the frobenius - norm threshold to @xmath307 , and the enrichment rank @xmath308 .",
    "the initial guess for the amen+als method is taken from the previous euler step .",
    "first , let us track the evolution of the stress tensor components versus euler iterations , see fig .",
    "[ fig : stress ] .",
    "we see that the stress does really stabilize in the chosen time range .",
    "moreover , the last component tends to zero , and can therefore be used as an in - hand measure of the accuracy .",
    "in addition , we compare @xmath309 and @xmath310 with the reference values computed with @xmath311 and @xmath270 , see fig .",
    "[ fig : stress_acc ] . for all @xmath304 except @xmath312 ( which is too large ) , and @xmath313 ,",
    "the accuracy attained is of the order @xmath314 .",
    "note that typical accuracies of greedy or mc methods for many - spring models are of the order @xmath315 @xcite .",
    "finally , the computational times can be seen in fig .",
    "[ fig : fpe_ttimes ] .",
    "as expected , the complexity increases quadratically with the number of spectral elements @xmath294 .",
    "an interesting feature is that the total cpu time decays with increasing @xmath304 .",
    "it points out that the performance of the amen method depends weakly on @xmath304 , and henceforth on the matrix spectrum . on the contrary ,",
    "the quality of the initial guess ( in terms of both ranks and accuracy ) is crucial .",
    "this may motivate attempts to relate the amen methods to newton or krylov iterations in a future research .",
    "in this paper we develop a new version of the fast rank  adaptive solver for tensor  structured symmetric positive definite linear systems in higher dimensions . similarly to the algorithms from  @xcite , the proposed amen method combines the one - dimensional local updates with the steps where the basis is expanded using the information about the global residual of the high  dimensional problem . however , in amen the same steps are ordered in such a way that only one or two neighboring cores are modified at once . both methods from  @xcite and the amen converge globally , and the convergence rate is established w.r.t .",
    "the one of the steepest descent algorithm .",
    "the practical convergence in the numerical experiments is significantly faster than the theoretical estimate .",
    "the amen algorithm appears to be more accurate in practical computations than the previously known methods , especially if local problems are solved roughly .",
    "the asymptotic complexity of the amen is linear in the dimension and mode size , similarly to the algorithms from  @xcite .",
    "the complexity w.r.t .",
    "the rank parameter is sufficiently improved taking into the account that a limiting step is the approximation of the residual , where the high accuracy is not always essential for the convergence of the whole method .",
    "we propose several cheaper alternatives to the svd - based tt - approximation , namely the cholesky decomposition and the inner als algorithm .",
    "the als approach provides a significant speedup , while maintaining almost the same convergence of the algorithm .",
    "finally , we apply the developed amen algorithm to general ( non - spd ) systems , which arise from high  dimensional fokker  planck and chemical master equations .",
    "theoretical convergence analysis can be made similarly to the fom method , which is rather pessimistic and puts very strong requirements on the matrix spectrum . in numerical experiments",
    "we observe a surprisingly fast convergence , even for strongly non  symmetric systems . here",
    "the amen demonstrates a significant advantage over the dmrg technique , which is known to stagnate , especially in high dimensions , see  @xcite and fig .",
    "[ fig : casc_amen_symm ] .",
    "there are many directions of a further research based on the ideas of  @xcite and this paper .",
    "first , the ideas developed in this paper can be generalized to other problems , e.g. finding the ground state of a many - body quantum system or a particular state close to a prescribed energy .",
    "the combination of update and basis enrichment steps looks very promising for a wide class of problems , as soon as the corresponding classical iterative algorithms can be adapted to provide a proper basis expansion in higher dimensions .",
    "a huge work is done in the community of greedy approximation methods , where the cornerstone is a subsequent rank - one update of the solution .",
    "second , there is a certain mismatch between the theoretical convergence estimates , which are at the level of the one  step steepest descent algorithm , and the practical convergence pattern , which looks more like the one of the gmres .",
    "this indicates that there are further possibilities to improve our understanding of the convergence of the amen and similar methods .",
    "our rates can benefit from sharp estimates of the progress of the one - dimensional update steps , which at the moment are available only in a small vicinity of a true solution , which is hard to satisfy in practice , see  @xcite .",
    "the superlinear convergence observed in numerical experiments inspires us to look for possible connections with the theory of krylov  type iterative methods and a family of newton methods .",
    "finally , we look forward to solving more high  dimensional problems , and are sure that they will bring new understanding of the advantages and drawbacks of the proposed method , and new questions and directions for a future research .",
    "40    , _ reduction of the chemical master equation for gene regulatory networks using proper generalized decompositions _ ,",
    "j. numer . meth",
    "engng , 00 ( 2011 ) , pp .",
    "115 .    ,",
    "_ a new family of solvers for some classes of multidimensional partial differential equations encountered in kinetic theory modeling of complex fluids _ , journal of non - newtonian fluid mechanics , 139 ( 2006 ) , pp .  153  176 .    , _ sparse grids _ , acta numerica , 13 ( 2004 ) , pp",
    ".  147269 .    ,",
    "_ simulation of dilute polymer solutions using a fokker - planck equation _ , computers & fluids , 33 ( 2004 ) , pp .",
    "687696 .    , _ tt - gmres : on solution to a linear system in the structured tensor format _ , arxiv preprint 1206.5512 ( to appear in : rus . j. of num .",
    "an . and math . model . ) , 2012 .    , _ tensor - product approach to global time - space - parametric discretization of chemical master equation _ , preprint  68 , mpi mis , 2012 .",
    ", _ fast solution of multi - dimensional parabolic problems in the tensor train / quantized tensor train  format with initial application to the fokker - planck equation _ , siam j. sci .",
    "comput . , 34 ( 2012 ) , p.  a3016a3038 .    , _ solution of linear systems and matrix inversion in the tt - format _",
    ", siam j. sci .",
    "34 ( 2012 ) , pp .",
    "a2718a2739 .    , _ alternating minimal energy methods for linear systems in higher dimensions .",
    "part i : spd systems _ , arxiv preprint 1301.6068 , 2013 .",
    ", _ finitely correlated states on quantum spin chains _ , communications in mathematical physics , 144 ( 1992 ) , pp .  443490 .    , _ a general method for numerically simulating the stochastic time evolution of coupled chemical reactions _ , journal of computational physics , 22 ( 1976 ) , pp",
    ".  403434 .    , _ tensor spaces and numerical tensor calculus _",
    ", springer ",
    "verlag , berlin , 2012 .    ,",
    "_ a solver for the stochastic master equation applied to gene regulatory networks _ , journal of computational and applied mathematics , 205 ( 2007 ) , pp .",
    "708  724 .    , _ the alternating linear scheme for tensor optimization in the tensor train format _ , siam j. sci .",
    "34 ( 2012 ) , pp .",
    "a683a713 .    , _ a dynamical low - rank approach to the chemical master equation _ , bulletin of mathematical biology , 70 ( 2008 ) , pp .  22832302 .    ,",
    "_ direct solution of the chemical master equation using quantized tensor trains _ ,",
    "research report  04 , sam , eth zrich , 2013 .    , _",
    "@xmath316quantics approximation of @xmath317@xmath5 tensors in high - dimensional numerical modeling _ , constr .",
    "appr . , 34 ( 2011 ) ,",
    "257280 .",
    "height 2pt depth -1.6pt width 23pt , _ tensor - structured numerical methods in scientific computing : survey on recent advances _ , chemometr .",
    "intell . lab .",
    "syst . , 110 ( 2012 ) , pp .",
    "119 .    ,",
    "_ matrix product ground states for one - dimensional spin-1 quantum antiferromagnets _ , europhys .",
    "lett . , 24 ( 1993 ) , pp .",
    "293297 .    , _ tensor decompositions and applications _ , siam review , 51 ( 2009 ) , pp",
    ".  455500 .    , _ a fast solver for fokker - planck equation applied to viscoelastic flows calculations : 2d fene model _ , journal of computational physics , 189 ( 2003 ) , pp .",
    "607  625 .    , _ a priori model reduction through proper generalized decomposition for solving time - dependent partial differential equations _ , computer methods in applied mechanics and engineering , 199 ( 2010 ) , pp .",
    "16031626 .",
    ", _ dmrg approach to fast linear algebra in the tt  format _",
    ", comput . meth .",
    "math , 11 ( 2011 ) , pp .",
    "382393 .",
    "height 2pt depth -1.6pt width 23pt , _ tensor - train decomposition _ , siam j. sci .",
    "comput . , 33 ( 2011 ) , pp .",
    "22952317 .    , _ tt - cross approximation for multidimensional arrays _",
    ", linear algebra appl .",
    ", 432 ( 2010 ) , pp .",
    ", _ thermodynamic limit of density matrix renormalization _ ,",
    ", 75 ( 1995 ) , pp .",
    "35373540 .    , _ the fokker - planck equation : methods of solutions and applications , 2nd ed .",
    "_ , springer verlag , berlin , heidelberg , 1989 .    , _ local convergence of alternating schemes for optimization of convex problems in the tt format _",
    ", siam j num .",
    "anal . , ( ( 2013 ) ) . to appear .",
    ", _ iterative methods for sparse linear systems _ , siam , 2003 .    , _ fast revealing of mode ranks of tensor in canonical format _ , numer",
    "theor . meth .",
    "appl . , 2 ( 2009 ) , pp .",
    "439444 .",
    ", _ fast adaptive interpolation of multi - dimensional arrays in tensor train format _",
    ", in proceedings of 7th international workshop on multidimensional systems ( nds ) , ieee , 2011",
    ".    , _ quadrature and interpolation formulas for tensor products of certain class of functions _ , dokl .",
    "nauk sssr , 148 ( 1964 ) , pp .",
    "soviet math .",
    "dokl . 4:240 - 243 , 1963 .    ,",
    "_ spectral methods in matlab _ , siam , philadelphia , 2000 .    ,",
    "_ stochastic processes in physics and chemistry _ , north holland , amsterdam , 1981 .    ,",
    "_ a qmc approach for high dimensional fokker - planck equations modelling polymeric liquids _ , math .",
    ", 68 ( 2005 ) , pp .",
    "4356 .    ,",
    "_ density - matrix algorithms for quantum renormalization groups _ , phys .",
    "b , 48 ( 1993 ) , pp .  1034510356 .",
    "# 1#2 ( )    as was observed in the numerical experiments , the amen method works successfully even being applied directly to non - symmetric systems .",
    "though we can not support this behavior with sharp estimates , one may proceed similarly to section [ sec : amen ] , and establish a formal theory , relating the amen to the full orthogonalization method .",
    "like in the spd case , we begin the analysis from the two - dimensional case . given a linear system @xmath4 and some basis @xmath318 , the projection method is performed as follows , @xmath319 given an initial guess @xmath320 , we assume @xmath321 , and @xmath322 .",
    "then it holds also @xmath323 .",
    "so , performs an oblique projection of the residual .",
    "its analysis is often conducted with the help of the orthogonal projection , @xmath324 i.e. the residual minimization on @xmath318 . the case @xmath325 is known as the minres method .",
    "its convergence was analysed in e.g. @xcite , @xmath326 i.e. @xmath327 is the acute angle between @xmath328 and @xmath329 . the worst convergence rate is estimated as @xmath330 and for a positive definite matrix is guaranteed to be less than 1 .",
    "the same approach may be used for the block case as well , @xmath331 obviously , if @xmath332 , it holds @xmath333 .    unfortunately , for the oblique projection one can not guarantee the monotonous convergence in general . however , assuming a certain well - conditioning of the system , we may relate the old and new residuals by a factor smaller than 1 as well .",
    "first of all , notice that @xmath338 is orthogonal to @xmath318 , @xmath339 then , @xmath340 , where @xmath341 . for the angle we can derive the following chain of inequalities , @xmath342",
    "from which we get @xmath343 . on the other hand , @xmath344 so that @xmath345 .",
    "therefore , the residual estimates as follows , @xmath346    it holds @xmath347 since the minimization over @xmath348 is a restriction w.r.t . the minimization over @xmath329 in the full space .",
    "hence , @xmath349 .",
    "however , @xmath350 might be greater than @xmath351 , and even greater than 1 .",
    "if @xmath318 contains the @xmath352-th krylov subspace , we obtain the so - called fom method .",
    "the progress of the fom can be related to that of the gmres as follows @xcite , @xmath353 where @xmath354 , @xmath355 are the progresses of the @xmath256-step gmres and fom , resp .",
    "note the similar term @xmath356 in lemma [ lem : fom ] .",
    "the condition @xmath335 may reflect the residual approximation , i.e. @xmath357 , but @xmath358 . both svd- and als - based approximations ( see section [ sec : prac ] ) fit to this scheme : the svd approximation reads @xmath359 , where @xmath360 is the singular vectors , and the als approximation reads @xmath361 .",
    "lemma [ lem : fom ] applies immediately to the two - dimensional amen method , by setting @xmath362 . despite the generally pessimistic estimate",
    ", it occurs in practice that @xmath363 is nonsingular , and moreover , @xmath364 is rather small such that @xmath365 and converges rapidly .    a nice property of theorem",
    "[ thm : amen ] is that it itself does not rely on a particular form of @xmath366 .",
    "we only needed that the galerkin conditions @xmath367 make the error @xmath368 strictly smaller than @xmath369 .",
    "[ lem : amr ] suppose in the @xmath256-th step of the multidimensional amen method , the als step provides the residual decrease @xmath370 and the exact computation of the rest cores @xmath371 after the enrichment provides the residual decrease @xmath372 where @xmath373 from lemma [ lem : fom ] with @xmath374 .",
    "then , the total convergence rate of the amen method is bounded by @xmath375        the exact solution for the second block is the oblique projection , hence @xmath381 the last two terms in are similar to that in theorem [ thm : amen ] , @xmath382 but now it is not orthogonal to @xmath383 .",
    "therefore , we can only use the triangle inequality , @xmath384 the first term is the residual after the galerkin solution , which is bounded by @xmath385 . for the second term",
    ", we have the recursion assumption , that is @xmath386 however , the only way to relate @xmath387 and @xmath388 is to use the angle between @xmath389 and @xmath390 , employing , @xmath391 @xmath392 since @xmath393 , and @xmath394 , it holds @xmath395 therefore , for the total residual we have @xmath396 plugging in the als update , the final estimate for now writes as follows , @xmath397 which finishes the recursion .",
    "contrarily to the symmetric positive definite case , where the total progress of the amen method was deteriorating with @xmath5 , but less than 1 in any case , here we may have a situation when the progress bound given by lemma [ lem : amr ] is greater than 1 . up to this moment , the only available estimate is @xmath398 , since we enrich the basis by @xmath203 , i.e. the first krylov vector only . in principle , it is possible to include a larger approximate krylov basis into the enrichment , i.e. @xmath399(k ) } & \\cdots & q^{[m-1](k)}\\end{bmatrix},\\ ] ] where @xmath400 } = \\tau(q^{[p](k ) } , \\ldots , q^{[p](d ) } ) \\approx a_k^p z_k$ ] , @xmath401 .",
    "however , this was not found to be reasonable in practical experiments . in all considered cases ,",
    "the decays @xmath158 and @xmath402 provided by the single enrichment @xmath201 appeared to be sufficiently small to ensure the convergence , fast enough to overcome the work required to prepare several krylov vectors ."
  ],
  "abstract_text": [
    "<S> in this paper we accomplish the development of the fast rank  adaptive solver for tensor  structured symmetric positive definite linear systems in higher dimensions . in  @xcite this problem </S>",
    "<S> is approached by alternating minimization of the energy function , which we combine with steps of the basis expansion in accordance with the steepest descent algorithm . in this paper </S>",
    "<S> we combine the same steps in such a way that the resulted algorithm works with one or two neighboring cores at a time . </S>",
    "<S> the recurrent interpretation of the algorithm allows to prove the global convergence and to estimate the convergence rate . </S>",
    "<S> we also propose several strategies , both rigorous and heuristic , to compute new subspaces for the basis enrichment in a more efficient way . </S>",
    "<S> we test the algorithm on a number of high  dimensional problems , including the non - symmetrical fokker  </S>",
    "<S> planck and chemical master equations , for which the efficiency of the method is not fully supported by the theory . in all examples </S>",
    "<S> we observe a convincing fast convergence and high efficiency of the proposed method .    _ </S>",
    "<S> keywords : _ high  dimensional problems , tensor train format , als , dmrg , steepest descent , convergence rate , superfast algorithms . </S>"
  ]
}