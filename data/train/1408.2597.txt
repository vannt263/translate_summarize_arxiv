{
  "article_text": [
    "in many engineering and machine learning problems , we are facing optimization problems that involve a huge amount of data .",
    "it is often very expensive to use such a huge amount of data for every update of the problem variables , and a more efficient way is to sample a small amount from the collected data for each renewal of the variables .    keeping this in mind , in this paper",
    ", we consider the stochastic program @xmath0 where @xmath1 , are convex constraint sets , the variable @xmath2 is partitioned into disjoint blocks @xmath3 of the dimension @xmath4 , @xmath5 is a random variable , @xmath6 is continuously differentiable , and @xmath7 are regularization functions ( possibly non - differentiable ) such as @xmath8-norm @xmath9 or @xmath10 seminorm @xmath11 for sparse or other structured solutions . throughout the paper ,",
    "we let @xmath12 and for simplicity , we omit the subscript @xmath5 in the expectation operator without causing confusion .    note that by assuming @xmath13 , includes as a special case the following deterministic program @xmath14 where @xmath15 is often very large .",
    "many problems in applications can be written in the form of or such as lasso @xcite , sparse logistic regression @xcite , bilinear logistic regression @xcite , sparse dictionary learning @xcite , low - rank matrix completion problem @xcite , and so on .",
    "we allow @xmath16 and @xmath7 to be nonconvex .",
    "when they are convex , we have sublinear convergence of the proposed method ( see algorithm [ alg : bsg ] ) in terms of objective value . without convexity ,",
    "we establish global convergence in terms of the expected violation of a first - order optimality condition .",
    "in addition , numerical experiments demonstrate that our algorithm can perform very well on both convex and nonconvex problems .      one difficulty to solve is that it may be impossible or very expensive to accurately calculate the expectation to evaluate the objective and its gradient or subgradient .",
    "one approach is the _ stochastic average approximation _ ( saa ) method @xcite , which generates a set of samples and then solves the empirical risk minimization problem by a certain optimization method .",
    "another approach is the _",
    "stochastic gradient _",
    "( sg ) method ( see @xcite and the references therein ) , which assumes that a stochastic gradient @xmath17 of @xmath16 can be obtained by a certain oracle and then iteratively performs the update @xmath18 where @xmath19 , and @xmath20 is a subgradient of @xmath21 at @xmath22 . in , @xmath23 is a realization of @xmath5 at the @xmath24th iteration , and @xmath25 is a stepsize that is typically required to asymptotically reduce to _",
    "zero _ for convergence .",
    "the work @xcite compares saa and sg and demonstrates that the latter is competitive and sometimes significantly outperforms the former for solving a certain class of problems including the stochastic utility problem and stochastic max - flow problem .",
    "the sg method has also been popularly used ( e.g. , @xcite ) to solve deterministic programming in the form of and exhibits advantages over the deterministic gradient method when @xmath15 is large and high solution accuracy is not required . to solve ( deterministic ) problems with separable nonsmooth terms as in , the block coordinate descent ( bcd ) method",
    "( see @xcite and the references therein ) has been widely used . at each iteration , bcd updates only one block of variables and thus can have a much lower per - iteration complexity than methods updating all the variables together .",
    "bcd has been found efficient solving many large - scale problems ( see @xcite for example ) .      in order to take advantages of the structure of and maintain the benefits of bcd",
    ", we generalize sg to a _",
    "block stochastic gradient _",
    "( bsg ) method , which is given in algorithm [ alg : bsg ] .    *",
    "input : * starting point @xmath26 , step sizes @xmath27 , and positive integers @xmath28 .    in the algorithm",
    ", we assume that samples of @xmath5 are randomly generated .",
    "we let @xmath29 be a stochastic approximation of @xmath30 , where @xmath31 is short for @xmath32 . in , @xmath33 is a subgradient of @xmath34 at @xmath35 , and we assume it exists for all @xmath36 and @xmath24 .",
    "we assume that both and are easy to solve .",
    "we perform two different updates . when @xmath37 , we prefer over since proximal gradient iteration is typically faster than proximal subgradient iteration ( see @xcite and the references therein ) ; when @xmath38 , we use , which takes the subgradient of @xmath7 , since minimizing the nonsmooth function @xmath7 subject to constraints is generally difficult .",
    "one can certainly take @xmath39 , @xmath40 , or use larger @xmath41 s .",
    "in general , a larger @xmath41 leads to a lower sample variance and incurs more computation of @xmath29 .",
    "note that at the beginning of each cycle , we allow a reshuffle of the blocks , which can often lead to better overall numerical performance especially for nonconvex problems , as demonstrated in @xcite .",
    "for the convenience of our discussion and easy notation , we assume @xmath42 throughout our analysis , i.e. , all iterations of the algorithm update the blocks in the same ascending order",
    ". however , our analysis still goes through if the order is shuffled at the beginning of each cycle , and some of our numerical experiments use reshuffling .",
    "the bcd and sg methods are special cases of the proposed bsg method . in algorithm",
    "[ alg : bsg ] , if @xmath43 , i.e. , there is only one block of variables , the update in becomes the sg update , and if @xmath44 , and @xmath45 , it becomes the bcd update in @xcite . for solving problem , a special case of , the deterministic bcd method in @xcite",
    "requires the partial gradients of all of the component functions for every block update while bsg uses only one or several of them , and thus the bsg update is much cheaper . on the other hand",
    ", bsg updates the variables in a gauss - seidel - like manner while sg does it in a jacobi - like manner .",
    "hence , bsg often takes fewer iterations than sg , and our numerical experiments demonstrate such an advantage of bsg over sg .",
    "for the reader s convenience , we list the related methods in table [ table : abbr ] .",
    ".list of related methods [ cols=\"<,^\",options=\"header \" , ]      we compared bsg and bcgd on the problem : @xmath46\\big),\\ ] ] where @xmath47 were given training samples with class labels @xmath48 .",
    "the bilinear logistic regression appears to be first used in @xcite for eeg data classification .",
    "it applies the matrix format of the original data , in contrast to the standard linear logistic regression which collapses each feature matrix into a vector .",
    "it has been shown that the bilinear logistic regression outperforms the standard linear logistic regression in many applications such as brain - computer interface @xcite and visual recognition @xcite .    in this test",
    ", we used the eeg dataset ivb from bci competition iii and the dataset concerns motor imagery with uncued classification task .",
    "the 118 channel eeg was recorded from a healthy subject sitting in a comfortable chair with arms resting on armrests .",
    "visual cues ( letter presentation ) were shown for 3.5 seconds , during which the subject performed : left hand , right foot , or tongue .",
    "the data was sampled at 100 hz , and the cues of `` left hand '' and `` right foot '' were marked in the training data .",
    "we chose all the 210 marked data points , and for each data point we randomly subsampled 100 temporal slices independently for 10 times to get , in total , 2,100 samples of size @xmath49 .",
    "we set @xmath50 in for bsg and used the same random starting point for bsg and bcgd .",
    "the left plot of figure [ fig : bci ] depicts their convergence behaviors . from the figure",
    ", we see that bsg significantly outperformed bcgd within 50 epochs .",
    "running bcgd to more epochs , we observed that bcgd could later reach a similar objective as that of bsg .",
    "the right plot of figure [ fig : bci ] shows the prediction accuracy of the solutions of bsg and bcgd , both of which ran to 30 epochs , and that of the result returned by liblinear @xcite , which solved linear logistic regression to its default tolerance .",
    "we ran the three methods 20 times . for each run , we randomly chose 2,000 samples for training and the remaining ones for testing . from the figure , we see that the _ bilinear _ logistic regression problem solved by bsg gave consistently higher prediction accuracies than the _ linear _ logistic regression problem .",
    "the low accuracies given by bcgd were results of its non - convergence in 30 epochs , which can be observed from the left plot of figure [ fig : bci ] . running to more epochs",
    ", bcgd will eventually give similar predictions as bsg .",
    "however , that will take much more time .",
    "we have proposed a bsg ( block stochastic gradient ) method and analyzed its convergence for both convex and nonconvex problems .",
    "the method has a convergence rate similar to that of the sg ( stochastic gradient ) method for convex programming , and its convergence has been established in terms of the expected violation of first - order optimality conditions for the nonconvex case .",
    "numerical results demonstrate its clear advantages over sg and a bsmd ( block stochastic mirror descent ) method on the tested convex problems and over the bcgd ( block coordinate gradient descent ) method one the tested nonconvex problems .",
    "this work was supported in part by nsf grant dms-1317602 and aro muri grant w911nf-09 - 1 - 0383 .",
    "we give the proofs of some lemmas in the paper .      the result in",
    "can be shown by @xmath51}}\\left[{\\mathbb{e}}\\big[\\langle { { \\mathbf{u}}}^k , { \\boldsymbol{\\delta}}_i^k\\rangle|{\\boldsymbol{\\xi}}_{[k-1]}\\big]\\right]\\\\ = & { \\mathbb{e}}_{{\\boldsymbol{\\xi}}_{[k-1]}}\\left[\\big\\langle { \\mathbb{e}}\\big[{{\\mathbf{u}}}^k|{\\boldsymbol{\\xi}}_{[k-1]}\\big ] , { \\mathbb{e}}\\big[{\\boldsymbol{\\delta}}_i^k|{\\boldsymbol{\\xi}}_{[k-1]}\\big]\\big\\rangle\\right]\\\\ \\le & { \\mathbb{e}}_{{\\boldsymbol{\\xi}}_{[k-1]}}\\left[\\big\\|{\\mathbb{e}}[{{\\mathbf{u}}}^k|{\\boldsymbol{\\xi}}_{[k-1]}]\\big\\|\\cdot\\big\\|{\\mathbb{e}}[{\\boldsymbol{\\delta}}_i^k|{\\boldsymbol{\\xi}}_{[k-1]}]\\big\\|\\right]\\\\ \\le & a(\\max_j\\alpha_j^k){\\mathbb{e}}_{{\\boldsymbol{\\xi}}_{[k-1]}}\\left[\\big\\|{\\mathbb{e}}[{{\\mathbf{u}}}^k|{\\boldsymbol{\\xi}}_{[k-1]}]\\big\\|\\right]\\\\ \\le & a(\\max_j \\alpha_j^k){\\mathbb{e}}\\|{{\\mathbf{u}}}^k\\|,\\end{aligned}\\ ] ] where the second equality follows from the conditional independence between @xmath52 and @xmath53 , and the last inequality follows from the jensen s inequality .",
    "now we are ready to prove lemma [ lem : bdh ] . for @xmath65",
    ", we have @xmath66 and thus from lemma [ lem : bdsubdif ] and remark [ rm : exlip ] , it follows that @xmath67 where we have used the cauchy - schwarz inequality in the second inequality . for @xmath68",
    ", we have @xmath69 where we have used the nonexpansiveness of the projection operator in the first inequality .",
    "this completes the proof of lemma [ lem : bdh ] .",
    "* case 1 : * @xmath81 . since @xmath84 , for @xmath85 , there exists a sufficiently large integer @xmath86 such that @xmath87 . therefore , @xmath88 . since @xmath81",
    ", we can choose another sufficiently large integer @xmath89 to have @xmath90 hence , @xmath91 , and the limit of @xmath92 is @xmath93 .    * case 2 : * @xmath82 .",
    "if @xmath94 , or @xmath95 , then the result is obvious .",
    "otherwise , there must exist a sequence @xmath96 such that @xmath97 a_k < \\eta^k , & \\text { if } n_{2m}\\le k < n_{2m+1 } \\end{array}\\right.\\ ] ] note that if @xmath98 or @xmath99 , then it is easy to have @xmath100 as @xmath101 .",
    "in addition , @xmath102 and @xmath103 hence , @xmath104 , and in the same way , one can show @xmath105 therefore , the limit of @xmath92 is @xmath93 .",
    "this completes the proof .                                , _ large - scale matrix factorization with distributed stochastic gradient descent _ , in proceedings of the 17th acm sigkdd international conference on knowledge discovery and data mining , acm , 2011 , pp .",
    "6977 .",
    "height 2pt depth -1.6pt width 23pt , _ optimal stochastic approximation algorithms for strongly convex stochastic composite optimization , ii : shrinking procedures and optimal algorithms _ , siam journal on optimization , 23 ( 2013 ) , pp .",
    "20612089 .",
    ", _ a block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion _",
    ", siam journal on imaging sciences , 6 ( 2013 ) , pp ."
  ],
  "abstract_text": [
    "<S> the stochastic gradient ( sg ) method can quickly solve a problem with a large number of components in the objective , or a stochastic optimization problem , to a moderate accuracy . </S>",
    "<S> the block coordinate descent / update ( bcd ) method , on the other hand , can quickly solve problems with multiple ( blocks of ) variables . </S>",
    "<S> this paper introduces a method that combines the great features of sg and bcd for problems with many components in the objective and with multiple ( blocks of ) variables .    </S>",
    "<S> this paper proposes a block stochastic gradient ( bsg ) method for both convex and nonconvex programs . </S>",
    "<S> bsg generalizes sg by updating all the blocks of variables in the gauss - seidel type ( updating the current block depends on the previously updated block ) , in either a fixed or randomly shuffled order . </S>",
    "<S> although bsg has slightly more work at each iteration , it typically outperforms sg because of bsg s gauss - seidel updates and larger stepsizes , the latter of which are determined by the smaller per - block lipschitz constants .    </S>",
    "<S> the convergence of bsg is established for both convex and nonconvex cases . in the convex case , bsg has the same order of convergence rate as sg . in the nonconvex case , </S>",
    "<S> its convergence is established in terms of the expected violation of a first - order optimality condition . in both cases </S>",
    "<S> our analysis is nontrivial since the typical unbiasedness assumption no longer holds . </S>",
    "<S> bsg is numerically evaluated on the following problems : _ stochastic least squares _ and _ logistic regression _ , which are convex , and _ low - rank tensor recovery _ and _ bilinear logistic regression _ , which are nonconvex . on the convex problems , </S>",
    "<S> bsg performed significantly better than sg . on the nonconvex problems , </S>",
    "<S> bsg significantly outperformed the deterministic bcd method because the latter tends to early stagnate near local minimizers . </S>",
    "<S> overall , bsg inherits the benefits of both stochastic gradient approximation and block - coordinate updates and is especially useful for solving large - scale nonconvex problems . </S>"
  ]
}