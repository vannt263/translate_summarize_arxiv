{
  "article_text": [
    "[ cols=\"^,^ \" , ]",
    "for the statistical estimate of the time shift value @xmath2 a maximum likelihood method is the choice of ref .",
    "its properties are studied in this section using 10@xmath3000 mc generated samples for each of the pd .",
    "the log likelihoods for our pd ( [ ppd ] ) are given by @xmath4,\\ k=1,2\\ ] ] where the times @xmath5 are mc generated with the pd @xmath6 .",
    "it is instructive to replot fig .",
    "[ fig_2ppd ] for logarithms of the probabilities .",
    "this is done in fig .",
    "[ fig_ln2ppd ] with @xmath6 normalized to @xmath7 where ( in accordance with our discretization ) @xmath5 is incremented in 1  ns steps .",
    "these plots exhibit clearly the locations of small and zero probabilities , which data will ( statistically ) avoid when they are created with the underlying pd @xmath6 .",
    "however , when shifting the @xmath5 values by @xmath8 , regions of low probabilities may be hit as discussed in the previous sections .    for our typical sample @xcite , the resulting @xmath9 functions are displayed in fig .",
    "[ fig_lnw ] . for model  1",
    "we see for decreasing @xmath8 , just before @xmath10ns , a sharp drop @xmath11 .",
    "this comes because the smallest time , @xmath12 hits a tiny probability , which we already encountered in the previous section . for model  2",
    "the log - likelihood function @xmath13 is smooth down to @xmath14ns , while for positive @xmath15ns the largest data point @xmath16 hits a region of very small probability .",
    "[ fig_7lnw ] shows a model  2 log - likelihood function @xmath13 for another of our first ten samples and this plot is quite similar to the one for extraction  2 in @xcite . in the samples",
    "one finds a variety of deviations from gaussian shapes , small ones due to the bulk structure of the pd and large ones due to a few events in the tails .",
    "following @xcite a step further , parabolic fits are made to the central region of each sample . to automatize the fitting procedure the central region of a model  @xmath17 sample",
    "was defined to be the @xmath8 range connected with @xmath18 so that @xmath19 holds , where the maximum of the log - likelihood function is @xmath20= \\ln l_k(\\delta t_k)\\ .\\ ] ] such parabolic fits are included in fig .",
    "[ fig_lnw ] and  [ fig_7lnw ] .",
    "let us investigate @xmath18 estimates for random times generated with the pd ( [ pkt ] ) . in the limit of infinite statistics",
    "@xmath21 holds , where the sums ( @xmath22 ) corresponds to our discretization time steps of 1  ns .",
    "they include @xmath23 contributions .",
    "for @xmath24 they are zero due to @xmath25 for @xmath26 , e.g.  already @xmath27ns , we can create mismatched contributions @xmath28 with @xmath29 finite and @xmath30 , so that @xmath31 becomes possible .",
    "hence , in the limit of infinite statistics @xmath32 where @xmath33 are expectation values for samples of @xmath34 data per model .",
    "deviations of @xmath18 from zero reflect statistical fluctuations and bias due to a finite statistics . in the following",
    "we investigate this for @xmath35 .",
    "direct estimates of @xmath18 are obtained by simply evaluating the log - likelihood functions ( [ lnlk ] ) for a sufficiently large region around @xmath24 , which is here taken to be [ @xmath36ns,@xmath37ns ] .",
    "the positions of the maxima of the parabolic fits give also estimates of @xmath18 , which deviate to some extent from the direct estimates .",
    "for instance , for our typical sample of fig .",
    "[ fig_lnw ] we find by direct estimate ( @xmath8 in steps of 1  ns ) @xmath38 while the parabolic fits give @xmath39 where , as in @xcite , the error bars are standard deviations obtained from the parabolic fits under the assumption that the center of the likelihood function is described by a gaussian distribution .",
    "that the @xmath18 values in ( [ 1dmax ] ) and ( [ fdmax ] ) are both positive is an accident .",
    "e.g. , for the sample of fig .",
    "[ fig_7lnw ] one finds by direct estimate @xmath40ns and @xmath41ns from the parabolic fit .",
    "the analysis of our 10@xmath3000 samples allows to improve on the questionable gaussian assumption .",
    "it turns out that the direct estimates of the @xmath18 values from @xmath42 are more robust than the estimates from parabolic fits , which can become unstable due to non - gaussian behavior .",
    "one rare example is shown in fig .  [ fig_161201lnw ] .",
    "in that case a small @xmath8 shift , either to the left or to the right , does immediately shift one of the extrema @xmath43 or @xmath44 into regions of very small likelihood , so that a parabolic fit becomes impossible .    for a finite statistics the extraction of @xmath18 from a given log - likelihood function @xmath45 is a nonlinear procedure , so that we have to anticipate a bias @xcite , which means @xmath46",
    "the exact maximum position , from which the deviation by a shift @xmath8 has to be calculated , is at @xmath47 .",
    "this bias falls off with @xmath48 , so that it tends to get swallowed by the @xmath49 behavior of the statistical noise . for the direct @xmath18",
    "estimates we find in our models @xmath50 which is in each case smaller than our discretization .",
    "the standard deviations were @xmath51 combining both models gives the number of the abstract @xmath52 to obtain estimates of these numbers from parabolic fits one has to eliminate 19 samples for which the fits are erratic . afterwards",
    ", averaging the standard deviations of the fits gives @xmath53 where the error of the error is with respect to estimates on a single sample .",
    "hence , for both models combined @xmath54 the @xmath55ns value from the gaussian fit to our typical sample is well consistent .",
    "bias estimates stay much smaller than the standard deviation , but differ from ( [ maxbias ] ) as the fitting itself imposes a new non - linearity .",
    "we abstain from giving these numbers , because they reflect also details of the fitting procedure like our definition of the central region .",
    "as the distributions of the @xmath18 are non - gaussian , one may wonder whether gaussian confidence limits provide a correct interpretation of the error bar ( [ ebar ] ) .",
    "indeed , the empirical peaked cdf @xcite found for the @xmath2 estimates via the direct method is broader than the gaussian peaked cdf with standard deviation 7.1@xmath3ns .",
    "see fig .",
    "[ fig_epcdf ] , which relies for the empirical peaked cdf on @xmath56 random times samples .",
    "the largest fluctuation in the negative direction is at @xmath57ns which has thus a probability of approximately @xmath58 .",
    "this is safely away from @xmath59ns .",
    "finally in this section , we address correlations between the statistical fluctuations of @xmath60 and those of @xmath18 . using @xmath61 samples ,",
    "we denote @xmath60 for sample @xmath62 by @xmath63 and @xmath18 by @xmath64 by @xmath65 .",
    "they have expectation values @xmath66 the bias correction is so small that it can as well be ignored . defining the correlation coefficients by @xmath67 their calculation from our @xmath68 samples gives @xmath69 this , at the first glance , non - intuitive anti - correlation is for model  1 displayed in fig .",
    "[ fig_scatter ] .",
    "it may be explained as follows .",
    "when @xmath70 fluctuates to negative values , there is an overpopulation of small @xmath8 values in the sample .",
    "therefore a shift in the same direction has a higher probability to produce large negative contributions to the log - likelihood function ( [ lnlk ] ) than a shift in the opposite direction .",
    "we have investigated the problem of identifying a shift in a broad , non - gaussian distribution of data . for two model probability densities ( pd ) shown in fig .",
    "[ fig_2ppd ] methods are developed and illustrated , which allow in samples of 7@xmath3612 mc generated departure times to identify a time shift with a precision of about @xmath0ns while the background noise of statistical fluctuations is with @xmath1ns much larger .    when the boundaries of the distribution are sufficiently sharp , uniform probabilities from binomials of the cdf provide excellent indicators .",
    "subsequently it has been demonstrated that the maximum likelihood method gives a quantitative estimate of the shift @xmath2 and its statistical error , while bootstrap simulations allow one to avoid gaussian assumptions . obviously , the analysis of this paper can be re - done for other desired sample sizes .",
    "* acknowledgements : * i would like to thank peter hoeflich for useful discussions and for extracting fig .",
    "[ fig_2ppd ] from fig .",
    "11 of ref .",
    "this work was in part supported by the doe grant de - fg02 - 97er41022 ."
  ],
  "abstract_text": [
    "<S> statistical methods for the extraction of a small shift in broad data distributions are examined by means of monte carlo simulations . </S>",
    "<S> this work was originally motivated by the cern neutrino beam to gran sasso ( cngs ) experiment for which the opera detector collaboration reported in a broad distribution a time shift by @xmath0ns despite the fact that the fluctuation of the average time is with @xmath1ns much larger . </S>",
    "<S> although the physical result has been withdrawn , statistical methods that make such an identification in a broad distribution possible remain of interest .    </S>",
    "<S> monte carlo methods in statistics , monte carlo , statistics , , neutrino departure time distribution    02.50.-r , 02.50.ng , 14.60.lm , 14.60.st </S>"
  ]
}