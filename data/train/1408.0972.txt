{
  "article_text": [
    "cluster analysis is an important tool used in hundreds of data mining applications like image segmentation , text - mining , genomics , and biological taxonomy .",
    "clustering allows users to find and explore patterns and structure in data without prior knowledge or training information .",
    "hundreds ( if not thousands ) of algorithms exist for this task but no single algorithm is guaranteed to work best on any given class of real - world data .",
    "this inconsistency of performance in cluster analysis is not unique to the clustering algorithms themselves .",
    "in fact , the dimension reduction techniques that are expected to aid these algorithms by revealing the cluster tendencies of data also tend to compete unpredictably , and it is difficult to know beforehand which low - dimensional approximation might provide the best separation between clusters . having many tools and few ways to make an informed decision on which tool to use , high - dimensional cluster analysis is doomed to become an ad hoc science where analysts blindly reach for a tool and hope for the best .",
    "cluster analysis is not the first type of data mining to encounter this problem .",
    "data scientists were quick to develop ensemble techniques to escape the unreliability of individual algorithms for tasks like prediction and classification .",
    "ensemble methods have become an integral part of many areas of data mining , but for cluster analysis such methods have been largely ignored .",
    "an additional problem stems from the fact that the vast majority of these algorithms require the user to specify the number of clusters for the algorithm to create . in an applied setting",
    ", it is unlikely that the user will know this information before hand .",
    "in fact , the number of distinct groups in the data may be the very question that an analyst is attempting to answer .",
    "determining the number of clusters in data has long been considered one of the more difficult aspects of cluster analysis .",
    "this fact boils down to basics : what is a cluster ? how do we define what should and should not count as two separate clusters ?",
    "our approach provides an original answer this question : a group of points should be considered a cluster when a variety of algorithms _ agree _ that they should be considered a cluster . if a majority of algorithms can more or less agree on how to break a dataset into two clusters , but can not agree on how to partition the data into more than two clusters , then we determine the data has two clusters .",
    "this is the essence of the framework suggested herein .",
    "our purpose is to address both problems : determining the number of clusters and determining a final solution from multiple algorithms .",
    "we propose a consensus method in which a number of algorithms form a voting ensemble , proceeding through several rounds of elections until a majority rule is determined .",
    "this allows the user to implement many tools at once , increasing his or her confidence in the final solution .",
    "in recent years , the consensus idea has been promoted by many researchers @xcite . the main challenge to ensemble methods using multiple algorithms",
    "is generally identified to be the wide variety in the results produced by different algorithms due to the different cluster criteria inherent in each algorithm .",
    "thus any direct combination of results from an ensemble will not often generate a meaningful result @xcite .",
    "most often the consensus problem has been formulated as an optimization problem , where the optimal clustering , @xmath1 , minimizes some relative distance metric between @xmath1 and all of the clusterings @xmath2 in the ensemble .",
    "there are many ways to define the distance between two clusterings , for example one could take the minimum number of elements that need to be deleted for the two partitions to become identical @xcite . using @xmath3 to denote some measure of distance between two different clusterings , we d",
    "write @xmath4 this problem is known as the _ median partition problem _ in the literature and dates back to the 1965-74 work of rgnier ( @xcite ) and mirkin ( @xcite ) @xcite",
    ". alternatively , some authors use a relative validity metric like the normalized mutual information @xmath5 in place of a distance function and attempt to maximize the objective function in eq .",
    "[ medianobj ] @xcite .",
    "the median partition problem was shown by krivanek and moravek , and also by wakabayashi , to be np - complete @xcite , but many heuristics have since been proposed to find approximate solutions @xcite .",
    "we believe that these methods are bound to suffer because each clustering in the ensemble is given equal importance .",
    "suppose we had 4 perfect clusterings and 1 terribly inaccurate clustering .",
    "these methods would not take into account the fact that the majority of the algorithms share 100% agreement on a perfect clustering , and instead may shift the optimal clustering away from perfection towards inaccuracy .",
    "thus , we feel that the optimization in eq .",
    "[ medianobj ] leads to a `` middle - of - the - road '' solution or a _ compromise _ between algorithms , rather than a solution of `` agreement '' or consensus . in our method",
    ", the clustering algorithms act as a voting ensemble and continually move through a series of elections until a desired level of consensus is reached .",
    "additionally , we introduce a parameter of intolerance , which allows the user to impose a level of agreement that must be reached between algorithms in order to accept a cluster relationship between objects .",
    "to begin , we introduce some notation . since consensus methods combine multiple solutions from multiple algorithms ( or multiple runs of the same algorithm ) , we start with a * cluster ensemble*. a cluster ensemble , @xmath6 , is a set of @xmath7 clusterings of the @xmath8 data objects @xmath9 . that is , each clustering @xmath10 in the ensemble is a @xmath11-way partition of the data , composed of individual clusters , @xmath12,\\ ] ] where the number of clusters @xmath11 in each clustering may be allowed to vary . in figure",
    "[ ensembleex ] , we illustrate a simple example with @xmath13 clusterings .     clusterings ]    the information from a cluster ensemble is then recorded in a * consensus matrix*.    given a cluster ensemble , @xmath14 of @xmath8 data points @xmath9 , the * consensus matrix * @xmath15 is an @xmath16 matrix such that @xmath17 [ cmatrix ]    one might prefer to think of the consensus matrix as the sum of individual adjacency matrices for each clustering in the ensemble . for a given clustering @xmath2 we could define an adjacency matrix , @xmath18 as @xmath19",
    "then the consensus matrix @xmath20 would be the sum of the adjacency matrices of each clustering in the ensemble : @xmath21 + as an example , the consensus matrix for the ensemble depicted in figure  [ ensembleex ] is given in figure  [ consensusex ] .    ]",
    "the consensus matrix from figure  [ consensusex ] is very interesting because the ensemble that was used to create it had clusterings for various values of @xmath0 .",
    "the most reasonable number of clusters for the colored circles in figure  [ ensembleex ] is @xmath22 .",
    "the 3 clusterings in the ensemble depict @xmath23 clusters . however , the resulting consensus matrix is clearly block - diagonal with @xmath22 diagonal blocks !",
    "this is not an isolated phenomenon , in fact it is something we should expect from our consensus matrices if we labor under the following reasonable assumptions :    * if there are truly @xmath0 distinct clusters in a given dataset , and a clustering algorithm is set to find @xmath24 clusters , then the @xmath0 `` true '' clusters will be broken apart into smaller clusters to make @xmath25 total clusters .",
    "* further , if there is no clear  subcluster \" structure , meaning the original @xmath0 clusters do not further break down into meaningful components , then different algorithms will break the clusters apart in different ways .",
    "this block - diagonal structure is the subject of section  [ perron ] .      as a similarity matrix ,",
    "the consensus matrix offers some benefits overs traditional approaches like the gaussian or cosine similarity matrices .",
    "one problem with these traditional methods is the curse of dimensionality : in high dimensional spaces , distance and similarity metrics tend to lose their meaning .",
    "the range of values for the pairwise distances tightens as the dimensionality of the space grows , and little has been done to address this fact . in figure  [ matrixdist ]",
    "we show the distribution of similarity values for the same 1 million entries in a consensus matrix compared to the cosine similarity matrix .",
    "as you can see , the consensus approach allows a user to witness some very high levels of similarity in high - dimension data , whereas the cosine similarities have a much smaller range .",
    "the dataset , which is more formally introduced in section  [ mcc ] , is the medlars - cranfield - cisi document collection ( @xmath26 documents ) @xcite .",
    "such contrast is typical among high - dimensional datasets .",
    ".49     .49     an additional benefit is that entries in the consensus matrix have depth . by this , we mean that they result from summing entries in adjacency matrices output by individual clustering algorithms , so more information is available about the _ meaning _ of each similarity value .",
    "the cosine of the angle between two data vectors @xmath27 and @xmath28 may tell us something about their correlation , but knowing , for instance that these two objects were clustered together by all algorithms with @xmath29 , by some algorithms with @xmath30 , and never when @xmath31 , provides a depth of insight not previously considered .",
    "while we do not use this information explicitly in our analysis , it may be beneficial in practical research .",
    "the greatest benefit of using a consensus matrix for clustering is that it provides superior information about clustering within the data .",
    "this has been demonstrated time and again in the literature @xcite .",
    "we add to the pile of evidence for this statement with the experiments in this paper .",
    "the consensus approach outlined herein is based on the work in @xcite where the consensus matrix is treated as similarity matrix and used as input to a clustering algorithm to reach a final solution . in @xcite",
    "the authors suggest using many runs of the _ k_-meansalgorithm , initialized randomly , to build the consensus matrix and then using a spectral clustering method , such as normalized cut ( ncut ) @xcite , to determine a final solution . in @xcite ,",
    "the approach is again to build a consensus matrix using many runs of the _ k_-meansalgorithm and then to cluster the consensus matrix with one final run of _ k_-means . in @xcite",
    "a consensus matrix is formed via _",
    "k_-meansand then used as input to the stochastic clustering algorithm ( sca ) .",
    "while all these methods provide better results than individual algorithms , they still rely on a single algorithm to make both the consensus matrix and the final decision on cluster membership .",
    "our method uses a variety of algorithms , rather than just _",
    "k_-means , to create the initial cluster ensemble .",
    "in addition , each algorithm is paired with different dimension reductions because it is often unclear which dimension reduction gives the best configuration of the data ; each lower dimensional representation has the potential to separate different sets of clusters in the data . in this way",
    ", we essentially gather an initial round of _ votes _ for whether or not each pair of objects @xmath32 belong in the same cluster . these votes",
    "are collected in a consensus matrix @xmath20 as defined in definition [ cmatrix ] .",
    "the experiments contained herein use a number of different clustering algorithms .",
    "the details of these popular algorithms are omitted due to space considerations but we refer the reader to the following resources :    [ [ data - clustering - algorithms ] ] data clustering algorithms + + + + + + + + + + + + + + + + + + + + + + + + + +    1 .",
    "spherical @xmath0-means ( solution with lowest objective value from 100 runs , randomly initialized ) @xcite 2 .",
    "pddp : principal direction divisive partitioning @xcite 3 .",
    "pddp-@xmath0-means : @xmath0-means with initial centroids provided by pddp clusters @xcite 4 .",
    "nmfcluster : @xmath0 dimensional nonnegative matrix factorization for clustering @xcite    [ [ graph - clustering - algorithms - requiring - a - similarity - matrix ] ] graph clustering algorithms ( requiring a similarity matrix ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    1 .",
    "pic : power iteration clustering @xcite 2 .",
    "ncut : normalized cuts according to shi and malik @xcite 3 .",
    "njw : normalized cuts according to ng , jordan , and weiss @xcite    each algorithm in our ensemble is assumed to be _",
    "reasonable _ , making good choices on cluster membership most of the time .",
    "it is inevitable that each of the clustering algorithms will make mistakes , particularly on noisy data , but it is assumed that rarely will the majority of algorithms make the _ same _ mistake . to account for this error",
    ", we introduce an _ intolerance parameter _",
    ", @xmath33 , for which entries in the consensus matrix @xmath34 will be set to zero . in other words , @xmath33 is the minimum proportion of algorithms that must agree upon a single cluster relationship @xmath32 in order to keep those `` votes '' in the consensus matrix .",
    "after the initial consensus matrix is formed , we use it as input to each of the clustering algorithms again .",
    "essentially we start a debate between algorithms , asking each of them to use the collective votes of the ensemble to determine a second solution .",
    "again these solutions are collected in a consensus matrix and the process repeats until a simple majority of algorithms agree upon one solution . once the majority of algorithms have chosen a common solution , we say the algorithms have _ reached consensus _ and call that resulting solution the * final consensus solution*. this process is illustrated in the flow chart in figure  [ consensusflowchart ] .          to illustrate the effectiveness of this procedure , we consider a combination of 3 text datasets used frequently in the information retrieval literature . for simplicity",
    ", we assume the number of clusters is known a priori . in section  [ perron ]",
    "this information will be extracted from the data .",
    "the combined medlars - cranfield - cisi ( mcc ) collection consists of nearly 4,000 scientific abstracts from 3 different disciplines .",
    "these 3 disciplines ( medlars = medicine , cranfield = aerodynamics , cisi = information science ) form 3 natural clusters in the data @xcite .",
    "the document data is high - dimensional with @xmath35 features ( words ) . as a result ,",
    "clustering algorithms tend to run slowly on the raw data .",
    "thus , we reduce the dimensions of the data using 3 preferred algorithms :    * nonnegative matrix factorization ( nmf ) by alternating constrained least squares ( acls ) @xcite * singular value decomposition ( svd ) @xcite * principal components analysis ( pca ) @xcite    one should realize that pca is , in fact , a singular value decomposition of data under z - score normalization",
    ". however , in practice , these two decompositions generally provide quite different results , particularly for high - dimensional data .    for each dimension reduction technique",
    ", the number of features is reduced from @xmath36 to @xmath37 creating a total of 9 input data sets . on each input dataset",
    ", 3 different clustering methods were used to cluster the data :    * @xmath0-means * pddp * pddp-@xmath0-means    the accuracy ( proportion of documents classified correctly @xcite ) of each algorithm on each data input are given in table  [ mccresults ] .",
    "input + & _ k_-means&0.8741 & 0.7962&0.8260 + ( @xmath38 ) & pddp & 0.9599 & 0.9049 & 0.9026 + & pddp-_k_-means&0.9599 & 0.9049 & 0.9026 + & _ k_-means & 0.8628&0.8268&0.8286 + ( @xmath39 ) & pddp & 0.9764 & 0.9774 & 0.9481 + & pddp-_k_-means&0.9764 & 0.9774&0.9481 + & _ k_-means & 0.8530&0.8263&0.8281 + ( @xmath40 ) & pddp & 0.9722&0.9802&0.9478 + & pddp-_k_-means&0.6114&0.9802&0.9478 +   +    the accuracy of the results ranges from 61% ( @xmath411,500 misclassified ) to 98% ( 78 misclassified ) .",
    "a reasonable question one might ask is this : why not choose the solution with the lowest _ k_-meansobjective value ? the biggest problem with this boils down to the curse of dimensionality : the distance measures used to compute such metrics lose their meaning in high - dimensional space @xcite .",
    "the only comparison we could make between clusterings would be with the full dimensional data and , surprisingly , the objective function values for the minimum accuracy solution is approximately equal to the maximum accuracy solution !",
    "other internal metrics , like the popular silhouette coefficient @xcite suffer from the same problem .",
    "one must be very careful when attempting to compare high - dimensional clusterings with such metrics .",
    "our suggestion is instead to compile the clusterings from table  [ mccresults ] into a consensus matrix , cluster that consensus matrix with multiple algorithms , and repeat that process until the majority of the algorithms agree upon a solution .",
    "this can be done with or without dimension reduction on the consensus matrix . for simplicity",
    ", we ll proceed without reducing the dimensions of the consensus matrix , but we will include an additional clustering algorithm , nmfcluster , which was not well suited for the analysis on the low - dimensional representations in table  [ mccresults ] .",
    "table  [ mccconsensus ] provides the accuracy of these 4 clustering algorithms on the consensus matrices through iteration .",
    "boxes are drawn around values to indicate a common solution chosen by algorithms .",
    "a final consensus solution is found in the third iteration with 3 of the 4 algorithms agreeing upon a single solution .",
    "the accuracy of this final consensus solution is much greater than the average of all the initial clustering results in table  [ mccresults ] .",
    "such a result is typical across all datasets considered .",
    ".medlars - cranfield - cisi text collection : accuracies for 4 clustering algorithms on consensus matrices through iteration [ cols=\"^,^,^,^\",options=\"header \" , ]      we began our analysis with a collection of documents and algorithms - both for dimension reduction and for clustering .",
    "traditional tools for determining the number of clusters were not successful .",
    "an analyst , having somehow determined the number of clusters and attempting to cluster this data with the given set of tools had a chance of finding a solution with accuracy ranging from 18% to 99% .",
    "if that analyst had chosen the best dimension reduction algorithm ( pca ) for this particular dataset ( a task for which there are no hard and fast guidelines ) , the accuracy of his / her solution may have been between 77% and 99% .",
    "internal cluster validation metrics like the _ k_-meansobjective function would not have been much help in choosing between these solutions , as such measures are difficult to compare on high - dimensional data . however , by using _",
    "all _ of the tools at our disposal in the iterative consensus clustering framework , we found that the clustering algorithms worked out their differences constructively - finally settling down on a solution with the highest level of accuracy achieved by any of the algorithms independently .",
    "herein we have presented a flexible framework for combining results from multiple clustering algorithms and/or multiple data inputs . not only does this framework provide the user with an above average clustering solution , it also contains a practical exploratory procedure for determining the number of clusters .",
    "we have discovered that consensus matrices built using multiple algorithms and multiple values for the number , @xmath0 , of clusters will often allow users to estimate an appropriate number of clusters in data by determining the maximum number of clusters for which algorithms are likely to agree on a common solution .",
    "we have provided several examples to show how this approach succeeds at determining the number of clusters in datasets where other methods fail .",
    "when the initial consensus matrix does not provide this information , it can be refined through the use of an intolerance parameter or iteration to get a clearer picture of how many clusters the algorithms might be able to agree upon .    while the consensus matrix itself is not a new idea ,",
    "the practice of using multiple algorithms and dimension reductions together to create the matrix had not previously been explored , nor had varying the number of clusters for the purposes of approximating @xmath0 .",
    "our approach to building the consensus matrix is novel and improves clustering results from nearly every clustering algorithm on all datasets considered .",
    "this consensus matrix has several advantages over traditional similarity matrices as discussed in section  [ benefits ] .",
    "the icc framework encourages clustering algorithms to agree on a common solution to help escape the unreliability of individual algorithms .",
    "while previous consensus methods have aimed to average cluster solutions in one way or another , ours is the first to emphasize agreement between clustering algorithms . after seeing some of the results of the individual algorithms in our ensemble",
    ", it should be clear that an average solution could be very poor indeed . rather than deciding each clustering",
    "is equally valid , we simply sum the number of times a cluster relationship was made between two points and let the algorithms decide whether this sum is considerable enough to draw those points together , or whether it might be more reasonable to dissolve the connection in favor of others .",
    "this framework iteratively encourages algorithms to agree upon a common solution because the value of the similarity metric reflects the level of algorithmic agreement at each step .",
    "thus , through iteration , cluster relationships upon which the algorithms do not agree are abandoned in favor of relationships with higher levels of agreement .",
    "y. zeng , j. tang , j. garcia - frias , and g.  r. gao . an adaptive meta - clustering approach : combining the information from different clustering algorithms . in _ proceedings of the ieee computer society bioinformatics conference _ , 2002 ."
  ],
  "abstract_text": [
    "<S> a novel framework for consensus clustering is presented which has the ability to determine both the number of clusters and a final solution using multiple algorithms . </S>",
    "<S> a consensus similarity matrix is formed from an ensemble using multiple algorithms and several values for @xmath0 . a variety of dimension reduction techniques and clustering algorithms are considered for analysis . for noisy or high - dimensional data , an iterative technique is presented to refine this consensus matrix in way that encourages algorithms to agree upon a common solution . </S>",
    "<S> we utilize the theory of nearly uncoupled markov chains to determine the number , @xmath0 , of clusters in a dataset by considering a random walk on the graph defined by the consensus matrix . </S>",
    "<S> the eigenvalues of the associated transition probability matrix are used to determine the number of clusters . </S>",
    "<S> this method succeeds at determining the number of clusters in many datasets where previous methods fail . on every considered dataset </S>",
    "<S> , our consensus method provides a final result with accuracy well above the average of the individual algorithms . </S>"
  ]
}