{
  "article_text": [
    "in some settings , it is desirable to have a biologically motivated approach for classifying segments of sequential data , such as spoken words . this paper examines a novel hybrid approach towards such data classification .",
    "the approach uses two components .",
    "the first is the hidden markov model ( hmm )  @xcite and the second is a biologically motivated spiking neural network ( snn )  @xcite that approximates expectation maximization learning ( em ) @xcite .",
    "in addition to the intrinsic interest of exploring statistically based biologically motivated approaches to machine learning , the approach is also attractive because of its possible realization on special purpose hardware for brain simulation @xcite as well as fleshing out the details of a large - scale model of the brain @xcite .",
    "hmms are widely used for sequential data classification tasks , such as speech recognition @xcite .",
    "there have been earlier efforts to build hybrid hmm / neural network models @xcite . in this work ,",
    "the hybrid approach was motivated by the insight that anns perform well for non - temporal classification and approximation while hmms are suitable for modeling the temporal structure of the speech signal .",
    "more recent work has used more powerful networks , such as deep belief networks and deep convolutional networks , for acoustic modeling of the speech signal @xcite .",
    "while these efforts have met with considerable practical success , they are not obviously biologically motivated . in part",
    ", our work differs from the previous work in that we use a biologically motivated snn .    formally , an hmm consists of a set of discrete states , a state transition probability matrix , and a set of emission ( observation ) probabilities associated with each state .",
    "the set of trainable parameters in an hmm can be the initial state probabilities , the transition probabilities , and the emission probabilities .",
    "this paper limits itself to training the emission probabilities using an snn .",
    "this is consistent the approaches of the above - mentioned earlier work .",
    "our work is directly influenced by the important prior work on how an hmm might be implemented in a cortical microcircuit was performed by @xcite .",
    "the cortical microcircuit is a repeated anatomical motif in the neocortex who some have argued is the next functional level of description above the single neuron @xcite . in its most simplified form , the microcolumn can be modeled as a recurrent neural network with lateral inhibition .",
    "kappel et al .",
    "@xcite have recently shown that , with appropriate learning assumptions , a trainable hmm can be realized within this microcircuit . the contribution of the present work is to unwrap this microcircuit into a more discernable hmm .",
    "the motivation for our approach is to recognize the fact that there are many ways to potentially realize an hmm in the brain and we seek a model that may be developed in future work but that does not burn any bridges or make unnecessary commitments .",
    "the motivation for this study is not so much to build the highest performing hmm - based classifier as it is to imagine how : 1 ) an hmm might be realized in the brain , and 2 ) be implemented in brain - like hardware .",
    "since this research combines spike timing - dependent plasticity ( stdp ) learning with hmm classifiers , the next subsections provide background on each topic .",
    "the phenomenon of stdp learning in the brain has been known for at least two decades @xcite .",
    "stdp modifies the connection strengths between neurons at their contact points ( synapses ) .",
    "spikes travel from the presynaptic neuron to the postsynaptic neuron via synapses .",
    "the strength of the synapse , represented by a scalar weight , modulates the likelihood of a presynaptic spike event causing a postsynaptic event .",
    "the weight of a synapse can be modified ( plasticity ) by using learning rules that incorporating information locally available at the synapse ( for example , stdp ) .",
    "generically , an stdp learning rule operates as follows .",
    "if the presynaptic neuron fires briefly before the postsynaptic neuron , then the synaptic weight is strengthened . if the opposite happens , the synaptic weight is weakened .",
    "such phenomena have been experimentally observed in many brain areas @xcite .",
    "a simple intuitive interpretation of this empirically observed constraint is that the synaptic strength is increased when the presynaptic neuron could have played a causal role in the firing of the postsynaptic neuron .",
    "the strength is weakened if causality is violated .",
    "probabilistic interpretations of stdp that could form a theoretical link to machine learning have emerged in the past decade .",
    "most relevant to this paper are the following .",
    "nessler et al .",
    "@xcite developed a version of stdp to compute em within a spiking neural circuit .",
    "building on this , kappel et al .",
    "@xcite built an hmm within a recurrent snn .",
    "the recurrent snn coded for all of the states in the hmm as well as implementing the learning .",
    "this was a significant hypothesis from a brain - simulation because of its very strong claim , that a cortical microcircuit may implement a full - blown hmm .",
    "the hypothesis is also highly , perhaps overly , committed from an engineering perspective .",
    "the present approach seeks to use the right tool for the right job while still linking it to a biomorphic framework .",
    "specifically , we encode states using an hmm but associate a separate copy of the modified version of a trainable nessler - type snn with each state . the purpose of the snn is to learn the emission ( observation ) probabilities for that state . in future work",
    ", one may find other effective ways to fully encode an hmm model as an snn , but it may not necessarily be the approach taken in @xcite .",
    "successive observations of sequential data , such as occurs in speech spectrograms , are highly correlated .",
    "the correlation often drops significantly between observations that are sequentially distant .",
    "an effective way to classify sequential data is to use a markov chain of latent variables ( states ) , otherwise known as an hmm .",
    "the hmm describes the data as a first - order markov chain that assumes the probability of the next state is independent of all of the previous states , given the current state .",
    "[ fig : hmmdiagram ] shows a four - state , left - to - right hmm , whose initial state is . in the figure , arrows entering nodes that are labeled `` '' are state - transition probabilities and arrows entering nodes labeled `` '' represent the causal relationship between a state and an observation .",
    "the hmm allows calculation of the probability of a given observation sequence of feature vectors @xmath0 $ ] , in a structure consisting of states @xmath1 , initial state probabilites @xmath2 , state transition probabilities @xmath3 , and emission probabilities @xmath4 .",
    "the probability of a particular observation sequence is given by @xmath5    where @xmath6 denotes the set of model parameters and @xmath7 denotes the state of the hmm when the observation occurs .",
    "the emission probabilities are given by @xmath8 and these are the parameters learned by the snn .",
    "we will have occasion to use the symbols @xmath7 versus @xmath9 .",
    "the former means the state of the hmm when observation @xmath10 occurs .",
    "the latter simply means state @xmath11 of the hmm . to train and adjust the model parameters in an hmm , the expectation maximization ( em ) approach ( also known as the baum - welch algorithm in the hmm )",
    "is used @xcite . in this paper",
    "we assumed fixed transition probabilities , @xmath3 , for all of the states .",
    "in a bayesian framework , a posterior probability distribution is obtained by multiplying the prior probability with the likelihood of the observation and renormalizing .",
    "recent studies have shown that the prior and likelihood models of observations can be represented by appropriately designed neural networks @xcite .",
    "one such network is a spiking winner - take - all ( wta ) network .",
    "nessler et al .",
    "@xcite showed that a version of the stdp rule embedded in an appropriate snn can perform bayesian computations .",
    "the most general representation of the probability distribution function in the hmm state is a finite mixture of the gaussian distributions ( gmm ) with mean vector , @xmath12 , covariance matrix , @xmath13 , and mixture coefficients , @xmath14 .",
    "each hmm state has its own mixture distribution .",
    "the probability of observation @xmath15 occurring in hmm state @xmath9 is given by    @xmath16    where @xmath17 is the number of hmm states , @xmath18 is the mixing parameter , and @xmath19 is the number of distributions in the mixture .    in our model , the emission distributions for the hmm states approximately implement the gaussian mixture distributions .",
    "there is a separate mixture distribution associated with each state , corresponding to a separate snn .",
    "the snn learns distribution parameters via stdp .",
    "the snn trains the parameters for the emission distributions and each hmm state has a separate snn as shown in fig .",
    "[ fig : statetransdiagram ] . fig .",
    "[ fig : snnarchitecture ] shows the snn architecture in detail .",
    "additionally , nessler et al .",
    "@xcite showed their stdp learning approximates em .",
    "therefore , the proposed snn architecture is able to implement the gmm in each state ( described in section 4.2 ) .",
    "the snn has two layers of stochastic units ( neurons ) that generate poisson spike trains .",
    "the @xmath20 units in the first layer encode input feature vectors to be classified .",
    "the second layer is composed of @xmath21 units that represent classification categories after the network is trained .",
    "the layers are fully feedforward connected from layer @xmath20 to @xmath21 by weights trained according to the stdp rule given in the next subsection .",
    "besides the feedforward connections , the @xmath21 units obey a winner - take - all discipline implemented by a global inhibition signal initiated by any of the @xmath21 units .    the number of @xmath20 units in fig .",
    "[ fig : snnarchitecture ] , @xmath22 , shows the feature vector dimension .",
    "the @xmath21 units specify the output neurons detecting the samples in @xmath19 different clusters .",
    "the number of output neurons , @xmath19 , manipulates the model flexibility in controlling the signal variety in one segment ( analogous to the number of distributions in a gmm ) .",
    "the number of states , @xmath17 , determines the number of segments in a sequential signal .",
    "for example , in spoken word recognition , it can be considered as the number of phoneme bigrams .",
    "the spiking activity of a unit in the @xmath21 layer is governed by an inhomogeneous poisson process .",
    "the rate parameter for this process is controlled by the postsynaptic potential ( psp ) input to the @xmath21 unit .",
    "the psp represents the sum of the synaptic effects coming into the @xmath21 unit .",
    "the instantaneous firing rate of unit @xmath23 is given by @xmath24 and is defined below @xmath25    the @xmath26 itself is the sum of the excitatory inputs into @xmath23 from the @xmath20 layer and a global inhibitory input .",
    "these are denoted respectively as @xmath27 and @xmath28 .",
    "thus , the @xmath26 for unit @xmath23 at time @xmath29 is @xmath30    @xmath31 encodes the composite stimulus input signal to unit @xmath23",
    ". the input signal is provided by the @xmath20 units , which encode the input feature vector .",
    "the quantity @xmath31 is a linear weighted sum of the excitatory postsynaptic potentials ( epsps ) provided by the @xmath20 units as shown below @xmath32    there are @xmath22 ` excitatory ' units in the @xmath20 layer .",
    "@xmath33 denotes the component of the epsp of @xmath23 that originates with unit @xmath34 in the @xmath20 layer .",
    "@xmath35 is the bias weight to unit @xmath23 .",
    "the @xmath36 are the weights from units in the @xmath20 layer to unit @xmath23 in the @xmath21 layer .",
    "the weights and bias are time dependent because their values can change while the network is learning .",
    "@xmath33 is defined by @xmath37\\\\      0&\\mathrm{otherwise}.      \\end{array }      \\right .",
    "\\label{eq : exp_epsp}\\ ] ] the quantity @xmath38 has a value of 1 at time @xmath29 if and only if unit @xmath34 has fired in the previous @xmath39 milliseconds . in the simulations , @xmath40 .        [ [ posterior - probability ] ] posterior probability + + + + + + + + + + + + + + + + + + + + +    the gaussian distribution over the dataset , @xmath41 , is defined as    @xmath42    where @xmath43 and @xmath44 are covariance matrix and mean vector respectively . in the gaussian mixture model with @xmath19 mixtures @xmath45 , @xmath46 , @xmath47 , the probability of a sample , @xmath48 ,",
    "is derived as follows : @xmath49 @xmath50 @xmath51 @xmath52 from eq .",
    "[ eq : p_y ] and eq .",
    "[ eq : gmm ] we have @xmath53 the conditional probability @xmath54 can also be written as @xmath55 which represents the responsibility  @xcite . to simplify the equations , assume that the samples are independent from each other in which @xmath56 .",
    "so , @xmath57 the equation above reaches its maximum when @xmath58 .",
    "now , if we suppose the synaptic weight vector of neuron @xmath23 , @xmath59 ( @xmath60 ) is an @xmath22 dimensional vector which approximates the @xmath61 , the similarity between the sample and mean ( negation of distance measure ) , @xmath62 , can be replaced by similarity measure between @xmath63 and @xmath64 as @xmath65 ( projection of the sample vector on the weight vector reaches the maximum when they are in a same direction ) .",
    "thus , @xmath66 where @xmath67 is a constant .",
    "@xmath68 and @xmath19 denote the mixture coefficients and number of the mixture distributions , respectively .",
    "[ [ stdp - rule - specification ] ] stdp rule specification + + + + + + + + + + + + + + + + + + + + + + +    in the training process of the gmm using em , we have @xmath69 @xmath70 where @xmath71 is the number of training samples . in the proposed snn , @xmath23 is the output neuron that has just fired .",
    "@xmath72 , which is supposed as @xmath61 in the gmm and already represents the previous samples in this cluster , should be updated based on the new samples . instead of calculating the average value of the samples ( eq .  [ eq : em ] ) , new synaptic weight , @xmath73 ( or @xmath74 ) ,",
    "is updated by @xmath75 where @xmath76 has @xmath22 positive and negative numbers corresponding to @xmath77 and @xmath78 respectively .",
    "thus , the new @xmath72 is updated using @xmath55 ( which causes a neuron to fire ) and input presynaptic spikes . for this purpose",
    "we use a modified version of the nessler s ( 2013 ) stdp learning rule .",
    "stdp is an unsupervised learning rule .",
    "following @xcite , weight adjustments occur exactly when some @xmath21 unit @xmath23 emits a spike . when a unit @xmath23 fires , the incoming weights to that unit are subject to learning according to the stdp rule given in eq .",
    "[ eq : nesslerstdp_wts ] .",
    "for each weight , one of two weight - change events occurs , either ltp ( strengthening ) or ltd ( weakening ) .",
    "the weight values are constrainted to be in the range [ -1 1 ] . @xmath79",
    "the first case above describes ltp ( positive ) and the second case describes ltd ( always @xmath80 ) .",
    "another parameter of the gmm is the mixture coefficient @xmath68 which is obtained by eq .",
    "[ eq : em_pi ] .",
    "the bias weight of the proposed snn , @xmath35 , represents average firing of the neuron @xmath23 over data occurrences .",
    "therefore , if the neuron fires , its bias weight increases , otherwise it decreases .",
    "for this purpose we use a modified version of the nessler s ( 2013 ) stdp learning rule analogous to eq .",
    "[ eq : nesslerstdp_wts ] as follows : @xmath81    @xmath82 denotes a weight adjustment that is modulated by another rate parameter @xmath83 .",
    "specifically ,    @xmath84    that is , there is a rate parameter @xmath83 for each @xmath21 unit with @xmath85 as the number of times the unit has fired , starting with 1 .",
    "it satisfies the constraint @xmath86    by considering the mixture coefficient in the snn , @xmath87 , to be defined as follows : @xmath88 @xmath89 the constraints on the mixture coefficients in eq .",
    "[ eq : gmm ] are fulfilled . finally , by combining eq .  [ eq : r_snn ] and eq .",
    "[ eq : pi_snn ] we have @xmath90 @xmath91    [ [ training - procedure ] ] training procedure + + + + + + + + + + + + + + + + + +    since there is a separate snn for each state , the observation function @xmath92 can be trained separately for each state .",
    "the training procedure begins with a set of feature vectors and initial weights .",
    "randomly selected feature vectors from the sample to be recognized are presented to the network for some number of training trials .    to the extent that the feature vectors are similar to previous observations , a subset of output neurons fire and the synaptic weights",
    "are updated according to eq .",
    "[ eq : nesslerstdp_wts ] through eq .",
    "[ eq : wnewformula ] .",
    "a new feature vector , which is different from previous vectors , stimulates a new set of output neurons to fire .",
    "this strategy imposes an unsupervised learning method within the snn to categorize the data in one state .    [ [ extracting - a - probability - value - from - the - snn ] ] extracting a probability value from the snn + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we will let @xmath93 denote the probability that the snn input at simulation step @xmath29 is of the category that the network recognizes .",
    "this value is the maximum of the output units after normalization as described below ( simplified representation of eq .",
    "[ eq : finalgmmsnn ] ) @xmath94    where @xmath95 is a normalizer .",
    "this experiment modeled a pattern classification task that used four spatio - temporal spike sub - patterns to build a larger pattern . each sub - pattern consisted of 80 neurons that simultaneously emitted poisson spike trains .",
    "the duration of all spike trains within a sub - pattern was @xmath96 ms .",
    "this was called a spatio - temporal pattern because the 80 neurons compose the spatial dimension  @xcite .",
    "target patterns were obtained by concatenating the four sub - patterns .",
    "examples of the sub - patterns denoted a , b , c , and d are shown in fig .",
    "[ fig : spiketrainsexp1 ] . different instances of a specific sub - pattern , such as a , will have different spike trains because of the poisson sampling .",
    "each row shows a spike train for a single neuron . for each sub - pattern , twenty of the neurons fire at 340 hz and the remaining sixty neurons fire at 50 hz .",
    "the high frequency spike trains were deemed information - containing and the low - frequency spike trains were considered background noise .    [ [ training - phase ] ] training phase",
    "+ + + + + + + + + + + + + +    only sub - patterns were trained .",
    "the snn training was unsupervised according to stdp explained earlier .",
    "an snn had 80 input units corresponding to each of the 80 spike trains forming the pattern .",
    "the network had eight output units to allow within category diversity .",
    "for each sub - pattern , one snn was trained for ten iterations using stdp .",
    "one iteration meant that the network was allowed to run for @xmath96 ms with stdp enabled and the input neurons maintained poisson firing rates according to their location within the sub - pattern .",
    "synaptic weights were randomly initialized before training .",
    "[ fig : trainedsubpatternweights ] shows average the synaptic weights after training for sub - pattern a. the weights for high - firing - rate spike trains 120 are clearly distinguishable from the weights for low - firing - rate spike trains 2180 .",
    "the results for training the other sub - patterns were analogous .",
    "the plot shows that information can be detected in the presence of noise .",
    "the average synaptic weights , @xmath97 , were calculated by averaging over the eight output units which is shown in eq .",
    "[ eq : avgw ] .    @xmath98    recall that the bias learns to represent the average firing rate",
    ".    patterns to be classified were built from a sequence of the four sub - patterns .",
    "each sub - pattern in a sequence corresponded to one hmm state .",
    "a collection of four hmms , each with four states corresponding to the pattern length , were used to recognize four target patterns abcd , dcba , abdc , and bacd .",
    "table  [ tab : artificialdataresults ] shows the performance results on this data set . in the table ,",
    "_ desired _ means the pattern that was presented and _ recognized _ means the hmm with the highest probability output .",
    "the probabilities along the diagonal ( correctly classified ) are much higher than the other probabilities in each column .",
    "therefore , the proposed model shows initial promising results in categorizing a simple set of the synthesized spatio - temporal patterns .",
    "lrrrr recognized / desired & & & & + & * 0.442 * & 0.112 & 0.227 & 0.219 + & 0.145 & * 0.572 * & 0.140 & 0.143 + & 0.254 & 0.124 & * 0.495 * & 0.126 + & 0.249 & 0.123 & 0.128 & * 0.500 * +    [ [ details - of - the - recognition - mechanism ] ] details of the recognition mechanism + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    during the recognition phase , we used a set of four @xmath17-state hmms ( @xmath99 ) for each of the target patterns .",
    "the four trained ssns are associated with the appropriate hmm state .",
    "the hmm with highest probability for a given input sequence was taken as the best match to the input signal .",
    "for each hmm , the probability of an observation seuquence , @xmath100 , was calculated by expanding eq .",
    "[ eqn : mainhmm ] as follows :    [ eq : hmmrecognitioneqns ] @xmath101    @xmath102 is defined in eq .",
    "[ eq : snn_output_prob ] .",
    "the @xmath103 s and @xmath104 s are all set to @xmath105 ( fixed transitions ) .",
    "all other @xmath106 s are set to zero . in this experiment ,",
    "total pattern duration was 80 ms corresponding to a concatenated sequence of four sub - patterns ( @xmath96 ms ) .",
    "the state probability , @xmath107 , in eq .",
    "[ eq : hmmrecognitioneqns ] is obtained by multiplying the particular state probabilities in @xmath96 sequential time steps ( 1 ms separation ) .",
    "@xmath108 is the duration for the poisson spike trains . from eq .",
    "[ eq : exp_uk ] and eq .",
    "[ eq : snn_output_prob ] , the state probability should have an exponential form as @xmath109 where @xmath110 is the selected weight vector with maximum probability value in eq .",
    "[ eq : snn_output_prob ] .",
    "@xmath111 reports the _ poisson process rate _ @xmath112 which can be interpreted as the feature values of an observation .",
    "therefore , it reversely shows the statistical similarity between two numerical vectors @xmath110 and @xmath63 discussed in section ( 4.2.1 , _ posterior probability _ ) .",
    "the algorithm for training the hybrid hmm / snn model and classifying the sequential patterns is shown in fig .",
    "[ fig : modelalgorithm ] . for this example , lines  2 and 3 were not needed because the sub - patterns were already extracted .",
    "xxxx1xxxxxxxxxxxxxxxxxxxx=1 : hmm - snn(n , k , p , t , signal ) : + 2 : data = feature - extraction(signal , n ) + 3 : sub - patterns = auto - segmentation(data , p ) + 4 : for each sample in sub - patterns : + 5 : spike - trains = extract - poisson - spikes(pattern , t ) // e.g. 80 spike trains + 6 : calculate output neuron status using eq .",
    "[ eq : exp_uk ] + 7 : if ( training - session ) : + 8 : train snns using eqs .",
    "[ eq : nesslerstdp_wts]-[eq : pi_snn ] + 9 : else + 10 : select class with highest pr using eq .",
    "[ eq : hmmrecognitioneqns ] or eq .",
    "[ eq : hmmrecognitioneqnsspeech ] +      this experiment extends the method to speech signal processing .",
    "a speech signal can be characterized as a number of sequential frames with stationary characteristics within the frame .",
    "a speech signal @xmath113 has @xmath71 sequential frames . in humans ,",
    "the signal within the auditory nerve is the result of an ongoing fourier analysis performed by the cochlea of the inner ear .",
    "that is , the frequencies energy and formants carry useful information for the speech recognition problem . in our experiments , we divided speech signals into 20 ms duration frames with 50 percent temporal overlap and converted each frame to the frequency domain ( line  2 of fig .",
    "[ fig : modelalgorithm ] ) .",
    "xxxx1xxxxxxxxxxxxxxxxxxxx=1 : initialize the data into @xmath17 equal - width segments ( sub - patterns ) .",
    "+ 2 : repeat + 3 : sample=1 + 4 : for @xmath11=1 to @xmath114 + 5 : while ( distance(sample and centroid[@xmath11 ] ) @xmath116 distance(sample and centroid[@xmath117 ) + 6 : sample++ + 7 : segment(@xmath11)=sample + 8 : update centroids + 9 : until segment change @xmath116 threshold",
    ". +    [ [ auto - segmentation - preprocessing - step ] ] auto segmentation preprocessing step + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the preprocessing groups the @xmath71 sequential frames into @xmath17 consecutive clusters .",
    "let @xmath0 $ ] denote a sequence of observations that is a member of , say , class @xmath118 .",
    "the goal of the snn is to classify @xmath100 as a member of @xmath118 among the other possible classes .",
    "for instance , @xmath100 can be a speech stream with @xmath71 20 ms duration frames , where each @xmath15 is a frame consisting of @xmath22 features observed at a given 20 ms time step .",
    "we shall call this a feature vector .",
    "the @xmath71 feature vectors ( frames ) should map to the @xmath17 categories corresponding to the hmm states .",
    "the initial problem is to cluster the @xmath71 vectors into the @xmath17 hmm categories . for this purpose ,",
    "a modified @xmath23-means algorithm was used .",
    "the algorithm , given in fig .",
    "[ fig : autosegmentationpreproc ] , compares consecutive ( adjacent ) clusters to group the frames into @xmath17 sequential data segments ( line  3 of fig .",
    "[ fig : modelalgorithm ] ) .",
    "each segment contains approximately similar feature vectors as judged by the clustering algorithm .    to illustrate , fig .",
    "[ fig : zerospectrogram ] shows a spectrogram of the spoken word `` zero . ''",
    "this represents the power spectrum of frequencies in a signal as they vary with time .",
    "the auto segmentation result for this signal is also shown in fig .",
    "[ fig : zerospectrogram ] by the vertical dashed lines .",
    "the signal has been divided into @xmath119 segments containing a varying number of speech frames .",
    "a specific segment consists of similar frames , where the signal is approximately stationary , and corresponds to one state of the hmm .",
    "[ [ converting - a - speech - signal - to - a - spike - train ] ] converting a speech signal to a spike train + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the speech signals were sampled at 8 khz .",
    "frame duration was taken to be 20 ms , which at an 8 khz sampling rate contains 160 sample values .",
    "after converting to the frequency domain , this reduces to 80 sample values .",
    "the magnitudes of the 80 frequency components were converted to rate parameters for 80 poisson spike trains ( line  5 of fig .",
    "[ fig : modelalgorithm ] ) .",
    "the simulation time for the spike trains was @xmath96 ms .",
    "[ [ classifying - spoken - words ] ] classifying spoken words + + + + + + + + + + + + + + + + + + + + + + + +    two experiments were conducted , classifying spoken words into either two or four categories .",
    "data was selected from the aurora dataset @xcite .",
    "the data set contains spoken american english digits taken from male and female speakers sampled at 8 khz .",
    "600 spoken digits belonging five categories `` zero '' , `` one , '' `` four , '' `` eight '' , and `` nine '' were selected . for each word recognized , an hmm with @xmath119 states was used .",
    "a separate snn was associated with each hmm state .",
    "all snns used 80 input units and 8 output units .",
    "the network had @xmath120 adaptive weights .",
    "these dimensions are the same as the network used in the previous experiment .",
    "since the probability distribution functions of the states are independent of each other , the @xmath17 speech segments can be trained in parallel .",
    "the within - class variability for a single state is maintained by the @xmath121 output units which approximate a gaussian mixture model .",
    "the input spike trains are obtained by the poisson process based on the frame s frequency amplitudes ( 80 feature values ) .",
    "the model was trained for 100 iterations .",
    "after training , the synaptic weights reflect the importance of specific frequencies and the final bias weights show the output neurons excitability in each state .",
    "the recognition phase in this experiment is more general than in eq .",
    "[ eq : hmmrecognitioneqns ] such that each segment @xmath122 ( 1 through @xmath17=10 ) , which is determined by a state , contains the number of samples .",
    "thus ,    [ eq : hmmrecognitioneqnsspeech ] @xmath123    where @xmath124 specifies the probability measure of sample @xmath125 of the sub - pattern corresponding to state @xmath11 .",
    "table  [ tab : binaryspeechresults ] shows the binary classification performance using different relative prior probabilities as bias parameters .",
    "[ fig : roc ] illustrates the roc curve of the results shown in table  [ tab : binaryspeechresults ] .",
    "the accuracy rate above 95 percent shows initial success of the model .",
    "table  [ tab : fourclassspeechresults ] shows accuracy rates of the model in recognizing four spoken words .",
    "an average performance above 85 percent accuracy was obtained .",
    "lrrl @xmath126 & fp % & tp % & accuracy % + 0.9500 & 100.00 & 100.00 & 50.26 + 0.9600 & 97.87 & 100.00 & 52.91 + 0.9650 & 94.68 & 100.00 & 51.32 + 0.9830 & 70.21 & 98.95 & 64.55 + 0.9850 & 58.51 & 98.95 & 70.37 + 0.9900 & 31.94 & 98.95 & 83.60 + 0.9965 & 8.51 & 96.84 & 94.18 + 0.9980 & 4.26 & 94.74 & * 95.27 * + 0.9990 & 3.19 & 92.63 & 94.71 + 1.0000 & 3.19 & 90.53 & 93.65 + 1.0030 & 1.06 & 75.79 & 87.30 + 1.0101 & 0.00 & 53.68 & 76.72 + 1.3333 & 0.00 & 0.00 & 49.00 +    [ [ summary - of - parameter - choices ] ] summary of parameter choices + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the number of input units , @xmath127 , was chosen because there were 80 frequency components in the spectrogram at the sampling rate used .",
    "the number of hmm states , @xmath119 , was chosen as the smallest value to qualitatively represent the variations in the acoustic structure of the spectrograms encountered .",
    "the number of output units , @xmath121 , was chosen to be the same as number of distributions considered for the gmm in previous study  @xcite .",
    "previous work was conducted using a traditional support vector data description and an hmm to classify spoken digits using wavelets and frequency - based features @xcite .",
    "accuracy rates above 90 percent were achieved which is better than the results obtained in the present experiments .",
    "however , that model does not have the biomorphic features that exist in the present model . additionally , online learning in the current method makes the model flexible to new data occurrences and is able to be updated efficiently .",
    "furthermore , using the snns which support communication via a series of the impulses instead of real numbers would be useful in vlsi implementation of the human brain functionality in sequential pattern recognition .    lrrl class & accuracy % + `` zero '' & 81.91 + `` four '' & 82.98 + `` eight '' & 96.74 + `` nine '' & 80.65 + * average * & * 85.57 * +",
    "a novel hybrid learning model for sequential data classification was studied .",
    "it consisted of an hidden markov model combined with a spiking neural network that approximated expectation maximization learning .",
    "although there have been other hybrid networks , to our knowledge this is the first using an snn .",
    "the model was studied on synthesized spike - train data and also on spoken word data .",
    "although the studies are preliminary , they demonstrate proof - of - concept in the sense that it provides a useful example of how a statistically based mechanism for signal processing may be integrated with biologically motivated mechanisms , such as neural networks .",
    "our approach derives from the described in @xcite .",
    "the work in @xcite showed how to use stdp learning to approximate expectation maximization .",
    "a complete hmm was encoded in a recurrent spiking neural network in the work of @xcite .",
    "our approach seeks a middle ground where the recurrent neural network is unwrapped into a sequence of hmm states , which each state having an associated nonrecurrent network .",
    "this leaves open the possibility of thinking about other ways to encode hmms in brain circuitry by refining a model that does not make such extreme assumptions .",
    "planned future work includes continuing to study the model s properties , both theoretically and experimentally , improving and extending the model s range of performance , and extending the learning capabilities of the model .",
    "conspicuously , the state transition probabilities of the present model are predetermined and fixed .",
    "our most important goal is to learn the sequential data in an online fashion without the segmentation preprocessing phase .",
    "this would provide a basis to train the state transition probabilities .",
    "nessler , b. , pfeifer , m. , buesing , l. , & maass , w. ( 2013 ) bayesian computation emerges in generic cortical microcircuits through spike - timing - dependent plasticity . _",
    "plos computational biology _ , 9(4 ) , 1 - 30 .",
    "merolla , p. , arthur , j. , alvarez - icaza , r. , cassidy , a. , sawada , j. , et al .",
    "a million spiking - neuron integrated circuit with a scalable communication network and interface . _ science _",
    ", 345 , 668 - 673 .",
    "bourlard , h. , and wellekens c. j. ( 1988 ) links between markov models and multilayer perceptrons . _ in advances in neural information processing systems _ , 502 - 510 , d. s. touretzky , ed .",
    "los altos , ca : morgan kauffman .",
    "niles , l. t. , and silverman , h. f. , ( 1990 ) combining hidden markov models and neural network classifiers .",
    "_ international conference of acoustic , speech , and signal processing _ , 417 - 420 , albuquerque ,",
    "nm , usa .        abdel - hamid , o. , mohamed , a. r. , jiang , h. , and penn , g. ( 2012 , march ) applying convolutional neural networks concepts to hybrid nn - hmm model for speech recognition .",
    "_ in ieee international conference on acoustics , speech and signal processing ( icassp ) , _ 4277 - 4280 , kyoto , japan .",
    "sainath , t. n. , mohamed , a. r. , kingsbury , b. , and ramabhadran , b. ( 2013 ) deep convolutional neural networks for lvcsr .",
    "_ in acoustics , speech and signal processing ( icassp ) , ieee international conference on _ , 8614 - 8618 , vancouver canada .",
    "pearce , d. , hirsch , h. ( 2000 ) .",
    "the aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions .",
    "speech recognition : challenges for the new millenium ( asr-2000 ) _ , 181 - 188 , paris .",
    "tavanaei , a. , ghasemi , a. , tavanaei , m. , sameti , h. , manzuir , m. ( 2012 ) .",
    "support vector data description for spoken word recognition . in _ biosignals : intl .",
    "conf .  on bio - inspired systems and",
    "signal processing _",
    ", 32 - 37 ."
  ],
  "abstract_text": [
    "<S> it is of some interest to understand how statistically based mechanisms for signal processing might be integrated with biologically motivated mechanisms such as neural networks . </S>",
    "<S> this paper explores a novel hybrid approach for classifying segments of sequential data , such as individual spoken works . </S>",
    "<S> the approach combines a hidden markov model ( hmm ) with a spiking neural network ( snn ) . </S>",
    "<S> the hmm , consisting of states and transitions , forms a fixed backbone with nonadaptive transition probabilities . </S>",
    "<S> the snn , however , implements a biologically based bayesian computation that derives from the spike timing - dependent plasticity ( stdp ) learning rule . </S>",
    "<S> the emission ( observation ) probabilities of the hmm are represented in the snn and trained with the stdp rule . </S>",
    "<S> a separate snn , each with the same architecture , is associated with each of the states of the hmm . because of the stdp training , each snn implements an expectation maximization algorithm to learn the emission probabilities for one hmm state .    </S>",
    "<S> the model was studied on synthesized spike - train data and also on spoken word data . </S>",
    "<S> preliminary results suggest its performance compares favorably with other biologically motivated approaches . because of the model s uniqueness and initial promise , it warrants further study . </S>",
    "<S> it provides some new ideas on how the brain might implement the equivalent of an hmm in a neural circuit . </S>",
    "<S> +   + _ keywords : _ sequential data , classification , spiking neural network , stdp , hmm , word recognition </S>"
  ]
}