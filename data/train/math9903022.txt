{
  "article_text": [
    "in adaptive control and recursive parameter estimation one often needs to adjust recursively an estimate @xmath0 of a vector @xmath1 , which comprises @xmath2 constant but unknown parameters , using measurements of a quantity @xmath3 here @xmath4 is a vector of known data , often called the regressor , and @xmath5 is a measurement error signal .",
    "the goal of tuning is to keep both the estimation error @xmath6 and the parameter error @xmath7 as small as possible .",
    "there are several popular methods for dealing with the problem above , for instance least - squares .",
    "maybe the most straightforward involve minimizing the prediction error via gradient - type algorithms of the form : @xmath8 where @xmath9 is a constant , symmetric , positive - definite gain matrix .",
    "let us define @xmath10 and analyze differential equations and , which under the assumption that @xmath11 is identically zero read : @xmath12 the nonnegative function @xmath13 has time derivative @xmath14 hence @xmath15    inspection of the equation above reveals that @xmath16 is limited in time , thus @xmath17 , and also that the error @xmath18 ( norms are taken on the interval @xmath19 where all signals are defined ) .",
    "these are the main properties an algorithm needs in order to be considered a suitable candidate for the role of a tuner in an adaptive control system .",
    "often @xmath20 or something similar is also a desirable property . to obtain the latter , normalized algorithms can be used ; however , the relative merits of normalized versus unnormalized tuners are still somewhat controversial .",
    "another alternative is to use a time - varying @xmath9 , as is done in least - squares tuning .    in  [ sec : acceleration ] we present a tuner that sets the second derivative of @xmath0 , and in  [ sec : covariance ] the effects of a white noise @xmath5 on the performance of the two algorithms are compared .",
    "then we show some simulations and make concluding remarks .",
    "classical tuners are such that the _ velocity _ of adaptation ( the first derivative of the parameters ) is set proportional to the regressor and to the prediction error @xmath21 .",
    "we propose to set the _ acceleration _ of the parameters : @xmath22 notice that the the formula above is implementable ( using @xmath23 integrators ) if measurement error is absent , because the unknown @xmath24 appears only in scalar product with @xmath25 .",
    "choose another function of lyapunovian inspiration : @xmath26 taking derivatives along the trajectories of gives @xmath27 integrating @xmath28 we obtain @xmath29 which leads immediately to the desired properties : @xmath30    the slow variation property @xmath31 follows without the need for normalization , and now we obtain @xmath32 instead of @xmath33 as before",
    ". we might regard @xmath34 as a modified error , which can be used in the stability analysis of a detectable or `` tunable '' adaptive system via an output - injection argument ; see @xcite .",
    "a generalization of is @xmath35 with @xmath36 and @xmath37 constant , symmetric , positive - definite @xmath38 matrices such that @xmath39 and @xmath40 .",
    "the properties of tuner , which can be obtained using the positive - definite function @xmath41 in the same manner as before , are @xmath42",
    "we now consider the effects on the expected value and covariance of @xmath43 of the presence of a measurement error .",
    "the assumptions are that @xmath11 is a white noise with zero average and covariance @xmath44 and that @xmath45 are given , deterministic data . for comparison purposes , first consider what happens when the conventional tuner is applied to in the presence of measurement error @xmath5 : @xmath46 the solution to the equation above can be written in terms of @xmath47 s state transition matrix @xmath48 as follows @xmath49 hence @xmath50 because @xmath51 by assumption . here",
    "the notation @xmath52 , denoting the expectation with respect to the random variable @xmath5 , is used to emphasize that the stochastic properties of @xmath25 are not under consideration .",
    "the conclusion is that @xmath43 will converge to zero in average as fast as @xmath53 does .",
    "the well - known persistency of excitation conditions on @xmath54 are sufficient for the latter to happen .    to study the second moment of the parameter error , write @xmath55",
    "the covariance of @xmath43 can be written as the sum of four terms .",
    "the first is deterministic .",
    "the second term @xmath56 because @xmath11 has zero mean , and the third term is likewise zero .",
    "the fourth term @xmath57 where fubini s theorem and the fact @xmath58 were used .",
    "performing the integration and adding the first and fourth terms results in @xmath59 this equation can be given the following interpretation : for small @xmath60 , when @xmath53 is close to the identity , the covariance of @xmath43 remains close to @xmath61 , the outer product of the error in the initial guess of the parameters with itself .",
    "as @xmath62 , which will happen if @xmath54 is persistently exciting , @xmath63 tends to @xmath64 .",
    "this points to a compromise between higher convergence speeds and lower steady - state parameter error , which require respectively larger and smaller values of the gain @xmath9 .",
    "algorithms that try for the best of both worlds  parameter convergence in the mean - square sense  often utilize time - varying , decreasing gains ; an example is the least - squares algorithm .",
    "we shall now attempt a similar analysis for the acceleration tuner applied to , which results in the differential equation @xmath65 let @xmath66 where @xmath67 , @xmath68 , each @xmath69 is a function of @xmath70 unless otherwise noted , and the dot signifies derivative with respect to the first argument . if @xmath71 , @xmath72    following the same reasoning used for the velocity tuner , one concludes that @xmath73 and that @xmath74 however the properties of the acceleration and velocity tuners are not yet directly comparable because the right - hand side of does not lend itself to immediate integration . to obtain comparable results",
    ", we employ the ungainly but easily verifiable formula ,    @xmath75    ' '' ''    valid for arbitrary scalars @xmath76 and @xmath77 , and make the    [ [ simplifying - assumption ] ] simplifying assumption : + + + + + + + + + + + + + + + + + + + + + + + +    for @xmath78 , and 3 , @xmath79 , where @xmath80 are scalars and @xmath81 is the @xmath82 identity matrix .    premultiplying by @xmath83 $ ] , postmultiplying by @xmath83^\\top$ ] , integrating from 0 to @xmath60 , and using the simplifying assumption gives formula .",
    "@xmath84    ' '' ''    taking @xmath85 in , @xmath86 results positive - semidefinite , therefore @xmath87    the combination of and shows that @xmath88 can be increased without affecting @xmath24 s steady - state covariance . on the other hand , to decrease the covariance we need to increase @xmath89 , which roughly speaking means increasing damping in . since @xmath88 and @xmath89 can be increased without affecting the stability properties shown in ",
    "[ sec : acceleration ] , a better transient @xmath90 steady - state performance compromise might be achievable with the acceleration tuner than with the velocity tuner , at least in the case when @xmath91 , @xmath92 , and @xmath37 are `` scalars . ''",
    "notice that @xmath93 by construction .",
    "[ [ approximate - analysis ] ] approximate analysis : + + + + + + + + + + + + + + + + + + + + + +    the derivation of inequality does not involve any approximations , and therefore provides an upper bound on @xmath94 , valid independently of @xmath54 .",
    "a less conservative estimate of the integral in can be obtained by replacing @xmath95 by its average value @xmath96 in the definition of @xmath86 in .",
    "this approximation seems reasonable because @xmath86 appears inside an integral , but calls for more extensive simulation studies .    to obtain a useful inequality ,",
    "we require @xmath97 ; namely , using the schur complement @xmath98 or , using the simplifying assumption and substituting @xmath95 by its approximation @xmath96 @xmath99 suppose further that @xmath100 .",
    "looking for the least conservative estimate , we pick @xmath101 , the least value of @xmath76 that keeps @xmath97 .",
    "thus @xmath102 with @xmath103 \\bar{m}_1 \\left[\\begin{smallmatrix}{\\phi}^\\top_{11}(t,0 ) \\\\ { \\phi}^\\top_{12}(t,0 )   \\end{smallmatrix}\\right]}{4m_1 ^ 2 m_2m_3r(1+\\mu_2 ) -r}.$ ]    taking @xmath104 we repeat the previous , exact result . for large positive values of @xmath77",
    "the first term of the right - hand side of tends to @xmath105 , which indicates that the steady - state covariance of the parameter error decreases when the signal @xmath25 increases in magnitude , and that it can be made smaller via appropriate choices of the gains @xmath88 and @xmath106 .",
    "the situation for the accelerating tuner is hence much more favorable than for the conventional one .",
    "the simulations in this section compare the behavior of the accelerating tuner with those of the gradient tuner and of a normalized gradient one .",
    "all simulations were done in open - loop , with the regressor a two - dimensional signal , and without measurement noise .",
    "figure  [ fig : step ] shows the values of @xmath107 and @xmath108 respectively when @xmath25 is a two - dimensional step signal . in figure",
    "[ fig : sin ] the regressor is a sinusoid , in figure  [ fig : sia ] an exponentially increasing sinusoid , and in figure  [ fig : prb ] a pseudorandom signal generated using matlab .",
    "no effort was made to optimize the choice of gain matrices ( @xmath91 , @xmath92 , and @xmath37 were all chosen equal to the identity ) , and the effect of measurement noise was not considered .",
    "the performance of the accelerating tuner is comparable , and sometimes superior , to that of the other tuners .",
    "= 2.5 in = 2.5 in    = 2.5 in = 2.5 in    = 2.5 in = 2.5 in    = 2.5 in = 2.5 in",
    "other ideas related to the present one are replacing the integrator in with a positive - real transfer function @xcite , and using high - order tuning ( @xcite ) .",
    "high - order tuning generates as outputs @xmath0 as well as its derivatives up to a given order ( in this sense we might consider the present algorithm a second - order tuner ) , but unlike the accelerating tuner requires derivatives of @xmath25 up to that same order .",
    "we expect that accelerating tuners will find application in adaptive control of nonlinear systems and maybe in dealing with the topological incompatibility known as the `` loss of stabilizability problem '' in the adaptive control literature .",
    "the stochastic analysis in  [ sec : covariance ] indicates that the performance and convergence properties of the accelerating tuner , together with its moderate computational complexity , may indeed make it a desirable tool for adaptive filtering applications .",
    "it seems that a better transient @xmath90 steady - state performance compromise is achievable with the accelerating tuner than with the velocity tuner . to verify this conjecture , a study of convergence properties of the accelerating tuner and their relation with the persistence of excitation conditions",
    "is in order , as well as more extensive simulations in the presence of measurement noise ."
  ],
  "abstract_text": [
    "<S> we propose a tuner , suitable for adaptive control and ( in its discrete - time version ) adaptive filtering applications , that sets the second derivative of the parameter estimates rather than the first derivative as is done in the overwhelming majority of the literature . comparative stability and performance analyses </S>",
    "<S> are presented .    </S>",
    "<S> * key words : * adaptive control ; parameter estimation ; adaptive filtering ; covariance analysis . </S>"
  ]
}