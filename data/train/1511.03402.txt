{
  "article_text": [
    "consider a functional regression model @xmath1 where @xmath2 is the value of unknown function @xmath3 at the @xmath4 observed covariate @xmath5 and @xmath6 is an error term . to fit an unknown function @xmath7",
    "we may consider a process regression model @xmath8 where @xmath9 is a random function and @xmath10 is an error process for @xmath11 .",
    "a gpr model assumes a gaussian process ( gp ) for the random function @xmath12 .",
    "it has been widely used to fit data when the regression function is unknown : for detailed descriptions see rasmussen and william ( 2006 ) , and shi and choi ( 2011 ) and references therein .",
    "gpr has many good features , for example , it can model nonlinear relationship nonparametrically between a response and a set of large dimensional covariates with efficient implementation procedure . in this paper",
    "we introduce an etpr model and investigate advantages in using an extended t - process ( etp ) .",
    "blup procedures in linear mixed model are widely used ( robinson , 1991 ) and extended to poisson - gamma models ( lee and nelder , 1996 ) and tweedie models ( ma and jorgensen , 2007 ) .",
    "efficient blup algorithms have been developed for genetics data ( zhou and stephens , 2012 ) and spatial data ( dutta and mondal , 2015 ) . in this paper",
    ", we show that blup procedures can be extended to gpr models",
    ". however , gpr fits are susceptible to outliers in output space ( @xmath13 ) .",
    "loess ( cleveland and devlin , 1988 ) has been developed for a robust fit against such outliers .",
    "however , it requires fairly large densely sampled data set to produce good models and does not produce a regression function that is easily represented by a mathematical formula . for models with many covariates , it is inevitable to have sparsely sampled regions .",
    "wauthier and jordan ( 2010 ) showed that the gpr model tends to give an overfit of data points in the sparsely sampled regions ( outliers in the input space , @xmath14 ) .",
    "thus , it is important to develop a method which produces good fits for sparsely sampled regions as well as densely sampled regions .",
    "wauthier and jordan ( 2010 ) proposed to use a heavy - tailed process .",
    "however , their copula method does not lead to a close form for prediction of @xmath15 as an alternative to generate a heavy - tailed process , various forms of student @xmath0-process have been developed : see for example yu _ et al_. ( 2007 ) , zhang and yeung ( 2010 ) , archambeau and bach ( 2010 ) and xu _",
    "et al_. ( 2011 ) .",
    "however , shah _ et al_. ( 2014 ) noted that the @xmath0-distribution is not closed under addition to maintain nice properties in gaussian models .    in this paper , we develop a specific etpr model which is closed under addition to retain many favorable properties of gpr models . due to its special structure of construction ,",
    "the resulting etpr model gives computationally efficient algorithm , i.e. a slight modification of the existing blup algorithm provides the robust blup procedure . under the proposed etpr model , marginal and predictive distributions",
    "are in closed forms .",
    "furthermore , it gives a robust blup procedure against outliers in both input and output spaces .",
    "properties of the robust blup procedure are investigated .",
    "the remainder of the paper is as follows .",
    "section 2 presents an etp and its properties .",
    "section 3 proposes an etpr model and discusses the inference and implementation procedures .",
    "robustness properties and information consistency of robust blup predictions are shown in section 3 .",
    "numerical studies and real examples are in section 4 , followed by concluding remarks in section 5 .",
    "all the proofs are in appendix .",
    "as a motivating example , we generated two data sets with sample sizes of @xmath16 and @xmath17 where @xmath18 s are evenly spaced in [ 0 ,  1.5 ] for the 9 ( or 48 ) data points and the remaining point is at 2.0 ( or two points at 1.8 and 2.0 ) .",
    "thus , the remaining point or the two points are sparse ones , meaning they are far away from the other data points in input space . in addition",
    ", we also make the data point 2.0 to be an outlier in output space by adding an extra error from either @xmath19 or @xmath20 , where @xmath21 is the student @xmath0 distribution with two degree of freedom .",
    "prediction curves for simulated data are plotted in figure [ fig1 ] , where circles represent data points , solid and dashed lines stand for prediction and their 95% confidence bounds .",
    "the true function is zero . for a small sample size @xmath16 ,",
    "figure 1(a - f ) shows that loess and gpr predictions are similar and the etpr prediction is the smoothest and shrinks the data point 2.0 the most heavily , i.e. selective shrinkage occurs . for a moderate sample size @xmath17 ,",
    "1(g - l ) shows that loess and etpr predictions are similar .",
    "however , the etpr prediction still shrinks the most at 2.0 . even though unreported , for a large sample size @xmath22 all give similar predictions .",
    "denote observed data set by @xmath23 where @xmath24 and @xmath25 . for random component @xmath26 at a new point @xmath27 ,",
    "the best unbiased predictor is @xmath28 it is called a blup if it is linear in @xmath29 its standard error can be estimated with @xmath30 to have an efficient implementation procedure , it is useful to have explicit forms for the predictive distribution @xmath31 @xmath32 and @xmath33 .    let @xmath34 be a real - valued random function such that @xmath35 .",
    "analogous to double hierarchical generalized linear models ( lee and nelder , 2006 ) , we consider a following hierarchical process , @xmath36 where @xmath37 stands for gp with mean function @xmath38 and covariance function @xmath39 , and @xmath40 stands for an inverse gamma distribution with the density function @xmath41 and @xmath42 is the gamma function .",
    "then , @xmath34 follows an etp @xmath43 implying that for any collection of points @xmath44 , we have @xmath45 where @xmath46 means that @xmath47 has an extended multivariate @xmath0-distribution ( emtd ) with the density function , @xmath48 @xmath49 , @xmath50 and @xmath51 for some mean function @xmath52 and kernel function @xmath53    it follows that at any collection of finite points etp has an analytically representable emtd density being similar to gp having multivariate normal density .",
    "note that @xmath54 is defined when @xmath55 and @xmath56 is defined when @xmath57 .",
    "when @xmath58 , @xmath47 becomes the multivariate @xmath0-distribution of lange _",
    "et al_. ( 1989 ) .",
    "when @xmath59 and @xmath60 , @xmath47 becomes the generalized multivariate @xmath0-distribution of arellano - valle and bolfarine ( 1995 ) .",
    "for @xmath61 it easily obtains that @xmath62 , and @xmath63 thus , we may say that the @xmath64 has a heavier tail than the @xmath65 .",
    "* proposition 1 * _ let @xmath66 _    * _ when @xmath67 as @xmath68 , we have @xmath69 _ * _ let @xmath70 be a p@xmath71 random vector such that @xmath72 . for a linear system @xmath73 with @xmath74 , we have @xmath75 with @xmath76 and @xmath77 _ . *",
    "_  let @xmath27 be a new data point and @xmath78 then , @xmath79 with @xmath80 @xmath81 , @xmath82 for @xmath83 . _    even if the mean and covariance functions of @xmath75 can not be defined when @xmath84 , from proposition 1(iii ) , the mean and covariance functions of the conditional process @xmath85 do always exist if @xmath86 . also from proposition 1(iii ) , the conditional process @xmath87 converges to a gp , as either @xmath88 or @xmath89 goes to @xmath90 .",
    "thus , if the sample size @xmath89 is large enough , the etp behaves like a gp .    for a new point @xmath91",
    ", we have @xmath92 , where @xmath93 and @xmath94 .",
    "note that from lemma 2(iv ) @xmath95 .    under various combinations of @xmath88 and @xmath96 ,",
    "the etp generates various @xmath0-processes proposed in the literature .",
    "for example , @xmath97 is the @xmath0-process of shah _ et al_. ( 2014 ) .",
    "they showed that if covariance function @xmath98 follows an inverse wishart process with parameter @xmath99 and kernel function @xmath100 , and @xmath101 , then @xmath34 has an extended @xmath0-process @xmath102 .",
    "@xmath103 is the student s @xmath0-process of rasmussen and william ( 2006 ) and @xmath104 is that of zhang and yeung ( 2010 ) .",
    "consider the process regression model ( [ assumed ] ) @xmath105 in this section we introduce an etpr model , where @xmath34 and @xmath106 have a joint etp process , @xmath107 kernel function @xmath108 and @xmath109 is an indicator function .",
    "the joint etp above can be constructed hierarchically as @xmath110 and this implies that @xmath111 and @xmath112 to give @xmath113 hence , additivity property of the gp and many other properties hold conditionally and marginally in the etp .",
    "when @xmath114 , the etpr model becomes a gpr model .    for observed data ,",
    "this leads to a functional regression model @xmath115 where @xmath116 and @xmath117 .",
    "now it follows that @xmath118 where @xmath119 .",
    "consider a linear mixed model @xmath120 where is the design matrix for fixed effects @xmath121 , @xmath122 is the design matrix for random effect @xmath123 and @xmath124 is a white noise .",
    "suppose that @xmath125 , @xmath126 , @xmath127 and @xmath128 with @xmath129 and @xmath130 .",
    "then , the linear mixed model becomes the functional regression model with @xmath131 this shows that etpr models extend the conventional normal linear mixed models to a nonlinear functional regression . in contrary to loess , this also shows that the etpr method can produce a regression function , easily represented by a mathematical formula .    in the hierarchical construction of etp ,",
    "there is only one single random effect @xmath132 so that @xmath133 is not estimable , confounded with parameters in covariance matrix .",
    "this means that @xmath88 and @xmath96 are not estimable .",
    "following lee and nelder ( 2006 ) , we set @xmath134 because @xmath135 if @xmath136 thus , the variance does not depend upon @xmath88 as @xmath137 this is also true when @xmath138 by doing this way , the first two moments of gp and etp have the same parametrization . zellner ( 1976 ) also noted that @xmath88 can not be estimated with a single realization of @xmath139 . in multivariate",
    "t - distribution , lange _ et al_. ( 1989 ) proposed to use @xmath140 .",
    "zellner ( 1976 ) suggested that @xmath88 can be chosen according to investigator s knowledge of robustness of regression error distribution .",
    "as @xmath141 , etp tends to gp .",
    "when robustness property is an important issue , a smaller @xmath88 is preferred .",
    "we tried various values for @xmath142 and find that @xmath143 works well . from now on we set @xmath143 to have @xmath144 furthermore , in functional regression models it is conventional to assume @xmath145 thus , without loss of generality we assume @xmath145      so far we have assumed that the covariance kernel @xmath146 is given . to fit the etpr model , we need to choose @xmath146 .",
    "a way is to estimate the covariance kernel nonparametrically ; see e.g. hall _ et al_. ( 2008 ) . however , this method is very difficult to be applied to problems with multivariate covariates .",
    "thus , we choose a covariance kernel from a function family such as a squared exponential kernel and matrn class kernel .",
    "this paper employs a combination of a squared exponential and a non - stationary linear covariance kernel as follows , @xmath147 where @xmath148 are a set of hyper - parameters . in ( [ kerfun ] ) , @xmath149 measure",
    "the length scale of each input covariate , @xmath150 known as scaling parameter which controls the vertical scale of variations of a typical function of the input , and @xmath151 defines the scale of non - stationary linear trends .",
    "the small value of @xmath152 means that the corresponding covariate may have great contribution in the covariance function .",
    "more about kernel function @xmath153 can be seen in rasmussen and william ( 2006 ) and shi and choi ( 2011 ) .",
    "let @xmath154 where @xmath155 is a parameter for @xmath156 and @xmath157 are those for @xmath158 . here",
    "the joint density of @xmath159 is @xmath160 where @xmath161 and @xmath162 are density functions of emtds .",
    "because @xmath163 , the maximum likelihood ( ml ) estimator @xmath164 for @xmath165 can be obtained by solving @xmath166 where @xmath167 , @xmath168 , and @xmath169 the score equations for gpr models above are ml estimating equations in linear mixed models with @xmath170 and @xmath171 thus , a little modification of existing blup procedures gives a parameter estimation for etpr models .",
    "since @xmath172 from lemma 2(iii ) we have @xmath173 with @xmath174 thus , given @xmath175 @xmath176 is linear in @xmath177 i.e. the blup for @xmath178 which is an extension of the blup in linear mixed models to etpr models .",
    "this blup has a form independent of @xmath179 so that it is the blup for gpr models .",
    "however , the conditional variance depends upon @xmath179 except when @xmath180 i.e. @xmath181 under gpr models .",
    "thus , the blups for the etpr and gpr models have a common form , but have different predictors and their variance estimations because of different parameter estimations ( @xmath182 and @xmath183 ) .",
    "furthermore , all quantities necessary to compute @xmath184 and @xmath185  are available during implementing blup procedures .    for a given new data point @xmath91",
    ", we have @xmath186 by lemma 2(iii ) , the predictive distribution @xmath187 is @xmath188 where @xmath189 furthermore , from proposition 1(iii ) , @xmath190 where @xmath191 and @xmath192 from lemma 2(iii ) , we also have @xmath193 with @xmath194 and @xmath195 .",
    "consequently , this conditional predictive process can be used to construct prediction @xmath196@xmath197  of the unobserved response @xmath198 at @xmath199  and its standard error can be formed using the predictive variance , given by @xmath200 , and the proof is in appendix .",
    "the predictive variance for @xmath201 in ( [ spec.cov ] ) differs from that for @xmath202    the prediction of @xmath203 and @xmath204 discussed above is the best unbiased predictions under etpr models , and so is under gpr models .",
    "however , their standard errors ( variance estimators ) differ .",
    "note that @xmath205 where @xmath206 is the blup  for @xmath203 .",
    "thus , the standard error estimate of the blup under the etpr model increases if the model does not fit the responses @xmath207 well while that under the gpr model does not depend upon the model fit .",
    "random - effect models consist with three objects , namely the data @xmath208 , unobservables ( random effects ) and parameters ( fixed unknowns ) @xmath209  for inferences of such models , lee and nelder ( 1996 ) proposed the use of the h - likelihood .",
    "lee and kim ( 2015 ) showed that inferences about unobservables allow both bayesian and frequentist interpretations . in this paper , we see that the etpr model is an extension of random - effect models .",
    "thus , we may view the functional regression model ( [ assumed ] ) either as a bayesian model , where a gp or an etp as a prior , or as a frequentist model where a latent process such as gp and etp is used to fit unknown function @xmath210 in a functional space ( chapter 9 , lee _ et al_. , 2006 ) . with the predictive distribution above",
    ", we may form both bayesian credible and frequentist confidence intervals .",
    "estimation procedures in section 3.1  can be viewed as an empirical bayesian method with a uniform prior on @xmath211 in frequentist ( or bayesian ) approach , ( [ margin ] ) is a marginal likelihood for fixed ( or hyper ) parameters .",
    "let @xmath212 and @xmath213 be the blup for @xmath26 and its variance estimate , respectively , under the etpr model . and let @xmath214 and @xmath215 be those under the gpr model with @xmath181 .",
    "let @xmath216 and @xmath217 be two student t - type statistics for a null hypothesis @xmath218 . under a bounded kernel function , if @xmath219 for some @xmath220 , @xmath221 , while @xmath222 remains bounded .",
    "therefore , @xmath222 for etpr is more robust against outliers in output space compared to that for gpr .",
    "this property still holds for ml estimators .",
    "* proposition 2 * _ if kernel function @xmath223 is bounded , continuous and differentiable on @xmath224 , then the ml estimator @xmath225 from the etpr has bound influence function , while that from the gpr does not .",
    "_      let @xmath226 be the density function to generate the data @xmath207 given @xmath227 under the true model ( [ true ] ) , where @xmath228 is the true underlying function of @xmath34 .",
    "let @xmath229 be a measure of random process @xmath34 on space @xmath230.let @xmath231 be the density function to generate the data @xmath207 given @xmath227 under the assumed etpr model ( [ assum1 ] ) .",
    "thus , the assumed model ( [ assum1 ] ) is not the same as the true underlying model ( [ true ] ) . here",
    "@xmath155 is the common in both models and @xmath232 is the true value of @xmath233 .",
    "let @xmath234 be the estimated density function under the etpr model .",
    "denote @xmath235=\\int ( \\log { p_{1}}-\\log { p_{2}})dp_{1}$ ] by the kullback - leibler distance between two densities @xmath236 and @xmath237 .",
    "then , we have the following proposition .",
    "* proposition 3 * _ under the appropriate conditions in appendix , we have @xmath238)\\longrightarrow 0,{\\mbox{as}}~~n\\rightarrow \\infty , \\ ] ] where the expectation is taken over the distribution of @xmath227 . _    from proposition 3",
    ", the kullback - leibler distance between two density functions for @xmath239 from the true and the assumed models becomes zero , asymptotically .",
    "let @xmath240 and @xmath241 , @xmath242 . in appendix",
    ", we show that @xmath243 where @xmath244 under the true model ( [ true ] ) , similarly to ( [ bspred ] ) , we have @xmath245 seeger _ et al_. ( 2008 ) called @xmath246 and @xmath247 bayesian prediction strategies",
    ". we can show that @xmath248=\\int \\sum_{i=1}^{n}q(y_{i}|\\mbox{{\\boldmath $ { x}$}$_{i}$},\\mbox{{\\boldmath $ { y}$}$_{i-1}$})p_{\\phi _ { 0}}(\\mbox{{\\boldmath $ { y}$}$_{n}$}|f_{0},\\mbox{{\\boldmath $ { x}$}$_{n}$})d\\mbox{{\\boldmath $ { y}$}$_{n}$},\\ ] ] where @xmath249 is a loss function and @xmath250 is called cumulative loss . under the gpr model ,",
    "seeger _ et al_. ( 2008 )  and",
    "wang and shi ( 2014 ) proved proposition 3 , interpreted it as the average of cumulative loss @xmath251 tending to zero asymptotically , and called it the information consistency . in this paper , we show this property for the robust blups . consequently , the frequentist blup procedure is consistent with the bayesian strategy in terms of average risk over an etp prior .",
    "we use simulation studies to evaluate performance of the blup procedures from the etpr model ( [ assum1 ] ) . for gpr and etpr models , we use    * gpr : @xmath252 and @xmath253 ; * etpr : @xmath254 and @xmath255 ;    where kernel function @xmath256 is given in ( [ kerfun ] ) and @xmath257 .",
    "results are based on 500 simulation data .",
    "* selective shrinkage *    when some sparse data points are far away from the dense data points , predictions of the sparse ones from the etpr method are more heavily regularized than those from the loess and the gpr methods . to generate data , from the process model ( [ assumed ] ) we assume @xmath34 follows a gp with mean @xmath258 and the kernel function ( [ kerfun ] ) , and error term follows a normal distribution with mean 0 and variance @xmath155 , denoted by @xmath259 .",
    "we set @xmath260 . in figure",
    "[ fig1 ] , 95% prediction confidence bounds are computed as @xmath261 . at sparse data point , from figure 1 we see that the etpr method has selective shrinkage of wauthier and jordan ( 2010 ) and gives a wider interval .",
    "we compare prediction performance from the loess , gpr and etpr methods in table [ tab1 ] , where @xmath16 and the data point 2.0 is added with an extra error from either @xmath262 or @xmath263 with @xmath264 , 2 , 3 and 4 .",
    "testing data points are evenly spaced from interval ( 0 ,  2.0 ) , denoted by @xmath265 with @xmath266 . prediction performance of the test data points is measured with mean squared error @xmath267 .",
    "table [ tab1 ] shows that robust blups from the etpr model have the smallest mse among the three methods : loess , gpr and etpr .",
    "the improvement is greater with @xmath0 error .",
    ".mean squared errors of prediction results and their standard deviation ( in parentheses ) by the loess , gpr and etpr methods with @xmath268 . [",
    "cols=\"^,^,^,^,^\",options=\"header \" , ]",
    "advantages of a gpr model include that it offers a nonparametric regression model for data with multi - dimensional covariates , the specification of covariance kernel enables to accommodate a wide class of nonlinear regression functions , and it can be applied to analyze many different types of data including functional data . in this paper , we extended the gpr model to the etpr model .",
    "the latter inherits almost all the good features for the gpr , and additionally it provides robust blup procedures in the presence of outliers in both input and output spaces .",
    "numerical studies show that the etpr is overall the best in prediction among the methods considered .",
    "let @xmath269 be an @xmath270 symmetric and positive definite matrix , @xmath271 , @xmath272 and @xmath273 . in this paper",
    ", @xmath274 means that a random vector @xmath275 has the density function , @xmath276 where @xmath42 is the gamma function .",
    "we may construct an emtd via a double hierarchical generalized linear model ( lee and nelder , 2006 ) as follows : * lemma 1 * _ if @xmath277 where @xmath40 stands for an inverse gamma distribution with the density function @xmath41 then , marginally @xmath278 .",
    "_    * proof * : from the construction of @xmath279 , we have @xmath280 which is the density function of emtd.@xmath281    properties of emtd are as follows .",
    "* lemma 2 * _ let @xmath282 _    * _ if @xmath283 as @xmath68 , then @xmath284 _ * _ for any matrix @xmath285 with rank @xmath286 , _ @xmath287 * _ let @xmath288 be partitioned as @xmath289 with lengths @xmath290 and @xmath291 , and @xmath292 and @xmath269 have the corresponding partitions as @xmath293 and @xmath294 . then , _ @xmath295 _ with @xmath296 , @xmath297 , _",
    "@xmath298 @xmath299 _ and @xmath300 .",
    "this gives _",
    "@xmath301 * _ let @xmath133 be a random effect in proposition 1 .",
    "then , @xmath302 with @xmath303 @xmath304 and @xmath305 _    * proof * : the conclusions ( i ) , ( ii ) and @xmath306 in ( iii ) are easily obtained by the definition of emtd and lemma 1 .",
    "now we only prove that @xmath307 .",
    "let @xmath308 and @xmath309 , then @xmath310 .",
    "we have @xmath311 which indicates @xmath312 .    by combining definitions of ig and emtd",
    ", we have @xmath313 which indicates ( iv ) holds in this lemma.@xmath281    * proof of proposition 1 * : proposition 1 can be easily proved by using lemma 2 , so omitted here.@xmath281    * marginal likelihood derivatives : *    we know that @xmath314 . for given @xmath88 , the marginal log - likelihood of @xmath165 is @xmath315 where @xmath316 .",
    "the derivative with respect to @xmath165 is @xmath317 where @xmath167 .",
    "estimates of parameters @xmath165 can be learned by using gradient based methods . and",
    "variances of the estimates can be estimated by computing the second derivatives of @xmath318 on @xmath165 as follows , @xmath319    * variance of prediction value @xmath320 * :    from the hierarchical sampling method described in lemma 1 , we have @xmath321 which suggests that conditional on @xmath133 , @xmath322 and marginal distribution of @xmath323 . for given @xmath133 , it follows from the gpr model that @xmath324 and @xmath325 .",
    "consequently , we have @xmath326 where @xmath327 .",
    "* proof of proposition 2 * : from ( [ score - beta ] ) , the score functions of @xmath328 based on the etpr model is @xmath329 the term @xmath330 in @xmath331 plays an important role in estimating parameter @xmath165 .",
    "for example , when some observations of responses have very large value or tend to infinity ( outliers ) , the score @xmath332 based on the etpr model does not tend to infinity .",
    "let @xmath333 be an estimate of @xmath165 , where @xmath334 is the empirical distribution of @xmath335 and @xmath336 is a functional on some subset of all distributions",
    ". influence function of @xmath336 at @xmath337 ( hampel _ et al_. , 1986 ) is defined as @xmath338 where @xmath339 put mass 1 on point @xmath340 and 0 on others .    for given parameter @xmath341 , following hampel _",
    "et al_. ( 1986 ) estimator @xmath164 of @xmath165 has the influence function @xmath342 note that the matrix @xmath343 is bounded according to @xmath207 , which indicates that the influence function of @xmath164 is bounded under the etpr model .",
    "similarly , we can obtain that the score function under the gpr model is unbound , which leads to unbound influence function of parameter estimate.@xmath281    * proof of the equation ( [ bspred ] ) * :    from sequential bayesian prediction strategy and bayes theorem , we have @xmath344 which shows that the equation ( [ bspred ] ) holds.@xmath281    * lemma 3 * _ suppose @xmath345 are generated from the etpr model ( [ assum1 ] ) with the mean function @xmath346 , and covariance kernel function @xmath256 is bounded and continuous in parameter @xmath347 .",
    "it also assumes that the estimate @xmath225 almost surely converges to @xmath165 as @xmath348 .",
    "then for a positive constant @xmath349 , and any @xmath350 , when @xmath89 is large enough , we have @xmath351 where @xmath352 , @xmath353 , @xmath354 is the @xmath270 identity matrix , and @xmath355 is the reproducing kernel hilbert space norm of @xmath356 associated with kernel function @xmath357 . _    * proof * : from proposition 1 , it follows that there exists a variable @xmath358 with density function @xmath359 , conditional on @xmath133 we have @xmath360 where @xmath361 stands for gaussian process with mean function @xmath38 and covariance function @xmath256 .",
    "then conditional on @xmath133 , the extended t - process regression model ( [ assumed ] ) becomes gaussian process regression model @xmath362 where @xmath363 , @xmath364 , and @xmath365 and error term @xmath366 are independent .",
    "denoted @xmath367 by probability density computation conditional on @xmath133 .",
    "based on the model ( [ rgpr ] ) , let @xmath368 where @xmath369 is the induced measure from gaussian process @xmath370 .",
    "we know that random effect @xmath133 is independent of covariates @xmath371 .",
    "then it easily shows that @xmath372    suppose that for any given @xmath133 , we have @xmath373 which indicates @xmath374 by simple computation , it follows that @xmath375 where @xmath376 is the density function of @xmath377 . from ( [ tpr1 ] ) , ( [ tpr0 ] ) , ( [ loggp ] ) and ( [ gtilde ] )",
    ", we have @xmath378 which shows that lemma 3 holds .",
    "now let us prove the inequality ( [ gpcons ] ) .",
    "since the proof of ( [ gpcons ] ) is similar to those of theorem 1 in seeger _",
    "et al_. ( 2008 ) and lemma 1 in wang and shi ( 2014 ) , here we summarily present the procedure of the proof , details please see in seeger _",
    "et al_. ( 2008 ) and wang and shi ( 2014 ) .",
    "let @xmath379 be the reproducing kernel hilbert space ( rkhs ) associated with covariance function @xmath357 , and @xmath380 , for any @xmath381 .",
    "from the representer theorem ( see lemma 2 in seeger _ et al_. , 2008 ) , it is sufficient to prove ( [ gpcons ] ) for the true underlying function @xmath382 .",
    "then for given @xmath133 , @xmath383 can be written as @xmath384 where @xmath385 and @xmath386 .    by fenchel - legendre duality relationship",
    ", we have @xmath387,\\end{aligned}\\ ] ] where @xmath388 is a measure induced by @xmath389 , and @xmath390 is the posterior distribution of @xmath391 from a gp model with prior @xmath392 and gaussian likelihood term @xmath393 , where @xmath394 and @xmath395 .",
    "then we have @xmath396 .",
    "let @xmath397 , then we have @xmath398=\\frac{1}{2}\\left\\{-\\log|\\hat{\\mbox{\\boldmath $ { k}$}}_{n}^{-1 } \\mbox{{\\boldmath $ { k}$}$_{n}$}|+\\log|\\mbox{\\boldmath $ { b}$}|+tr(\\hat{\\mbox{\\boldmath $ { k}$}}_{n}^{-1 } \\mbox{{\\boldmath $ { k}$}$_{n}$}\\mbox{{\\boldmath $ { b}$}$^{-1}$})\\right .",
    "\\notag \\\\ & \\hskip 2 cm \\left.+r||f_0||^2_k+r\\mbox{\\boldmath $ { \\alpha } $ } \\mbox{{\\boldmath $ { k}$}$_{n}$}(\\hat{\\mbox{\\boldmath $ { k}$}}_{n}^{-1 } \\mbox{{\\boldmath $ { k}$}$_{n}$}-\\mbox{{\\boldmath $ { i}$}$_{n}$})\\mbox{\\boldmath $ { \\alpha}$}-n\\right\\ } ,   \\label{kbdist } \\\\ & e_{q}(-\\log \\tilde{p}(\\mbox{{\\boldmath $ { y}$}$_{n}$}|\\tilde{f}))\\leq -\\log \\tilde{p}(\\mbox{{\\boldmath $ { y}$}$_{n}$}|f_0)+\\frac{1}{2 } \\phi_0^{-1}tr ( \\mbox{{\\boldmath $ { k}$}$_{n}$}\\mbox{{\\boldmath $ { b}$}$^{-1}$ } )   \\notag \\\\ & \\hskip 3.2 cm = -\\log p_{0}(\\mbox{{\\boldmath $ { y}$}$_{n}$}|r,\\mbox{{\\boldmath $ { x}$}$_{n}$})+\\frac{1}{2 } \\phi_0^{-1}tr ( \\mbox{{\\boldmath $ { k}$}$_{n}$}\\mbox{{\\boldmath $ { b}$}$^{-1}$ } ) ,   \\label{eqp}\\end{aligned}\\ ] ] where @xmath399 .",
    "hence , it follows from ( [ fld ] ) , ( [ kbdist ] ) and ( [ eqp ] ) that @xmath400 since the covariance function is bounded and continuous in @xmath401 and @xmath402 , we have @xmath403 as @xmath348 .",
    "hence , there exist positive constants @xmath349 and @xmath404 such that for @xmath89 large enough @xmath405 plugging ( [ eqns ] ) in ( [ gpcons-1 ] ) , we have the inequality ( [ gpcons ] ) .",
    "@xmath281    for proof of proposition 3 , we need condition ( a ) @xmath406 is bounded and @xmath407 .",
    "* proof of proposition 3 * : it easily shows that @xmath408 . under conditions in lemma 3 , and condition ( a ) , it follows from lemma 3 that @xmath238)\\longrightarrow 0,{\\mbox{as}}~~n\\rightarrow \\infty .",
    "\\notag\\ ] ] that proves proposition 3.@xmath281",
    "1 .   archambeau , c. and bach , f. ( 2010 ) , multiple gaussian process models .",
    "_ advances in neural information processing systems_. 2 .",
    "arellano - valle , r. b. and bolfarine , h. ( 1995 ) . on some characterization of the t - distribution .",
    "_ statistics _ & _ probability letters _ , 25 , 79 - 85 .",
    "cleveland , w.s . , devlin , s.j .",
    "( 1988 ) , locally - weighted regression : an approach to regression analysis by local fitting .",
    "_ journal of the american statistical association _ 83 : 596 - 610 .",
    "dutta , s. and mondal , d. ( 2015 ) , an h - likelihood method for spatial mixed linear models based on intrinsic auto - regressions .",
    "_ j. r. statist .",
    "b _ 77 : 699 - 726 . 5 .   hall , p. , mller , h .-",
    "g . , and yao , f. ( 2008 ) , modelling sparse generalized longitudinal observations with latent gaussian processes,_journal of royal statistical society , ser .",
    "b _ , 70 , 703 - 723 .",
    "hampel , f.r . ,",
    "ronchetti , e.m . ,",
    "rousseeuw , p.j . and stahel , w.a .",
    "( 1986 ) , robust statistics : the approach based on influence functions , wiley .",
    "lange , k.l . , little , r. j.a . and taylor j. m.g .",
    "( 1989 ) , robust statistical modelling using the t distribution , _ journal of the american statistical association _ , 84 , 881 - 896 .",
    "lee , y. and kim , g. ( 2015 ) .",
    "h - likelihood predictive intervals for unobservables , _ international statistical review _ , doi : 10.1111/insr.12115",
    "lee , y. and nelder , j.a .",
    "hierarchical generalized linear models .",
    "_ journal of the royal statistical society b _ , 58 , 619 - 678 .",
    "lee , y. and nelder , j.a .",
    "double hierarchical generalized linear models ( with discussion ) . _ journal of the royal statistical society : c ( applied statistics ) _ , 55 , 139 - 185 .",
    "lee , y. , nelder , j.a . and",
    "pawitan , y. ( 2006 ) .",
    "generalized linear models with random effects , unified analysis via h - likelihood .",
    "chapman & hall / crc .",
    "ma , r. and jorgensen , b. ( 2007 ) . nested generalized linear mixed models : an orthodox best linear unbiased predictor approach .",
    "_ journal of the royal statistical society b _ , 69 , 625 - 641 . 13 .",
    "rasmussen , c. e. and williams , c. k. i. ( 2006 ) , gaussian processes for machine learning .",
    "cambridge , massachusetts : the mit press .",
    "robinson , g.k .",
    "( 1991 ) . that blup is a good thing : the estimation of random effects ( with discussion ) .",
    "_ statistical science _ 6 , 15 - 51 . 15 .",
    "seeger m. w. , kakade s. m. and foster d. p. ( 2008 ) , information consistency of nonparametric gaussian process methods , ieee transactions on information theory , 54 , 2376 - 2382 .",
    "shah a. , wilson a.g . and ghahramani z. ( 2014 ) , student - t processes as alternatives to gaussian processes .",
    "proceedings of the 17th international conference on artificial intelligence and statistics ( aistats ) , 877 - 885 . 17 .",
    "shi , j. q. and choi , t. ( 2011 ) , gaussian process regression analysis for functional data , london : chapman and hall / crc .",
    "wang , b. and shi , j.q .",
    "( 2014 ) , generalized gaussian process regression model for non - gaussian functional data .",
    "_ journal of the american statistical association _ , 109 , 1123 - 1133 .",
    "wauthier , f. l. and jordan , m. i. ( 2010 ) .",
    "heavy - tailed process priors for selective shrinkage . in advances in neural information processing systems , 2406 - 2414 . 20 .",
    "xu , p , lee , y. and shi , j. q. ( 2015 ) automatic detection of significant areas for functional data with directional error control . arxiv:1504.08164",
    "xu , z. , yan , f. and qi , y. ( 2011 ) , sparse matrix - variate t process blockmodel . proceedings of the 25th aaai conference on artificial intelligence , 543 - 548 .",
    "yu s. , tresp v. and yu k. ( 2007 ) , robust multi - tast learning with t - process .",
    "proceedings of the 24th international conference on machine learning , 1103 - 1110 . 23 .",
    "zellener , a. ( 1976 ) , bayesian and non - bayesian analysis of the regression model with multivariate student - t error terms , _ journal of the american statistical association _",
    ", 71 , 400 - 405 . 24 .",
    "zhang , y. and yeung , d.y . (",
    "2010 ) , multi - task learning using generalized @xmath0 process .",
    "proceedings of the 13th international conference on artificial intelligence and statistics ( aistats ) , 964 - 971 . 25 .",
    "zhou , x. and stephens , m. ( 2012 ) , genome - wide efficient mixed - model analysis for association studies .",
    "_ nature genetics _ 44 : 821 - 824 ."
  ],
  "abstract_text": [
    "<S> gaussian process regression ( gpr ) model has been widely used to fit data when the regression function is unknown and its nice properties have been well established . in this article </S>",
    "<S> , we introduce an extended t - process regression ( etpr ) model , which gives a robust best linear unbiased predictor ( blup ) . owing to its succinct construction </S>",
    "<S> , it inherits many attractive properties from the gpr model , such as having closed forms of marginal and predictive distributions to give an explicit form for robust blup procedures , and easy to cope with large dimensional covariates with an efficient implementation by slightly modifying existing blup procedures . </S>",
    "<S> properties of the robust blup are studied . </S>",
    "<S> simulation studies and real data applications show that the etpr model gives a robust fit in the presence of outliers in both input and output spaces and has a good performance in prediction , compared with the gpr and locally weighted scatterplot smoothing ( loess ) methods .    </S>",
    "<S> gaussian process regression , selective shrinkage , robustness , extended @xmath0 process regression , functional data </S>"
  ]
}