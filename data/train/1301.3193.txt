{
  "article_text": [
    "models are a standard tool in image processing , computer vision , and many other fields .",
    "exact inference and inference are often intractable , due to the high treewidth of the graph .",
    "much previous work involves approximations of the likelihood .",
    "( section [ sec : loss - functions ] ) . in this paper",
    ", we suggest that parameter learning can instead be done using `` marginalization - based '' loss functions .",
    "these directly quantify the quality of the _ predictions _ of a given marginal inference algorithm .",
    "this has two major advantages .",
    "first , approximation errors in the inference algorithm are taken into account while learning .",
    "second , this is robust to model mis - specification .",
    "the contributions of this paper are , first , the general framework of marginalization - based fitting as implicit differentiation .",
    "second , we show that the parameter gradient can be computed by `` perturbation'' that is , by re - running the approximate algorithm twice with the parameters perturbed slightly based on the current loss .",
    "third , we introduce the strategy of `` truncated fitting '' .",
    "inference algorithms are based on optimization , where one iterates updates until some convergence threshold is reached . in truncated fitting ,",
    "algorithms are derived to fit the marginals produced after a _ fixed number _ of updates , with no assumption of convergence .",
    "we show that this leads to significant speedups .",
    "we also derive a variant of this that can apply to likelihood based learning .",
    "finally , experimental results confirm that marginalization based learning gives better results on difficult problems where inference approximations and model mis - specification are most significant .",
    "markov random fields are probability distributions that may be written as @xmath0 this is defined with reference to a graph , with one node for each random variable . the first product in eq .",
    "[ eq : mrf_def ] is over the set of _ cliques _",
    "@xmath1 in the graph , while the second is over all individual variables .",
    "for example , the graph    = [ draw , shape = circle ] ; ( x1 ) at ( 0 , 0 ) @xmath2 ; ( x2 ) at ( 1 , 0 ) @xmath3 ; ( x3 ) at ( 2 , .4 ) @xmath4 ; ( x4 ) at ( 3 , .4 ) @xmath5 ; ( x5 ) at ( 2,-.4 ) @xmath6 ; ( x6 ) at ( 3,-.4 ) @xmath7 ; ( x1 )  ( x2 ) ( x2 )  ( x3 ) ( x2 )  ( x5 ) ( x3 )  ( x5 ) ( x3 )  ( x4 ) ( x5 )  ( x6 ) ;    corresponds to the distribution    @xmath8    each function @xmath9 or @xmath10 is positive , but otherwise arbitrary .",
    "the factor @xmath11 ensures normalization .",
    "the motivation for these types of models is the hammersleyclifford theorem @xcite , which gives specific conditions under which a distribution can be written as in eq .",
    "[ eq : mrf_def ] .",
    "those conditions are that , first , each random variable is conditionally independent of all others , given its immediate neighbors and , secondly , that each configuration @xmath12 has nonzero probability .",
    "often , domain knowledge about conditional independence can be used to build a reasonable graph , and the factorized representation in an mrf reduces the curse of dimensionality encountered in modeling a high - dimensional distribution .      one is often interested in modeling the conditional probability of @xmath12 , given observations @xmath13 .",
    "for such problems , it is natural to define a conditional random field @xcite",
    "@xmath14    here , @xmath15 indicates that the value for a particular configuration @xmath16 depends on the input @xmath13 . in practice ,",
    "the form of this dependence is application dependent .",
    "suppose we have some distribution @xmath17 , we are given some input @xmath13 , and we need to guess a single output vector @xmath18 .",
    "what is the best guess ?",
    "the answer clearly depends on the meaning of `` best '' .",
    "one framework for answering this question is the idea of a bayes estimator @xcite .",
    "one must specify some utility function @xmath19 , quantifying how `` happy '' one is to have guessed @xmath12 if the true output is @xmath20 .",
    "one then chooses @xmath18 to maximize the expected utility @xmath21    one natural utility function is an indicator function , giving one for the exact value @xmath20 , and zero otherwise .",
    "it is easy to show that for this utility , the optimal estimate is the popular maximum a posteriori ( map ) estimate .",
    "if @xmath22,$ ] then @xmath23    little can be said in general about if this utility function truly reflects user priorities .",
    "however , in high - dimensional applications , there are reasons for skepticism .",
    "first , the actual maximizing probability @xmath24 in a map estimate might be extremely small , so much so that astronomical numbers of examples might be necessary before one could expect to exactly predict the true output .",
    "second , this utility does not distinguish between a prediction that contains only a single error at some component @xmath25 , and one that is entirely wrong .",
    "an alternative utility function , popular for imaging problems , quantifies the hamming distance , or the _ number of components _ of the output vector that are correct . maximizing this results in selecting the most likely value for each component independently .",
    "if @xmath26,$ ] then    @xmath27    this appears to have been originally called maximum posterior marginal ( mpm ) inference @xcite , though it has been reinvented under other names @xcite . from a computational perspective",
    ", the main difficulty is not performing the trivial maximization in eq .",
    "[ eq : mpm - inference ] , but rather computing the marginals @xmath28 .",
    "the marginal - based loss functions introduced in section [ sub : marginal - based - loss - functions ] can be motivated by the idea that at test time , one will use an inference method similar to mpm where one in concerned only with the accuracy of the marginals .",
    "the results of map and mpm inference will be similar if the distribution @xmath17 is heavily `` peaked '' at a single configuration @xmath12 .",
    "roughly , the greater the entropy of @xmath17 , the more there is to be gained in integrating over all possible configurations , as mpm does .",
    "a few papers have experimentally compared map and mpm inference @xcite .",
    "the exponential family is defined by @xmath29    where @xmath30 is a vector of parameters , @xmath31 is a vector of sufficient statistics , and the log - partition function @xmath32    ensures normalization .",
    "different sufficient statistics @xmath31 define different distributions . the exponential family is well understood in statistics .",
    "accordingly , it is useful to note that a markov random field ( eq . [ eq : mrf_def ] ) is a member of the exponential family , with sufficient statistics consisting of indicator functions for each possible configuration of each clique and each variable @xcite , namely ,    @xmath33|\\forall c,{\\bf x}_{c}\\}\\cup\\{i[x_{i}=x_{i}]|\\forall i , x_{i}\\}.\\ ] ]    it is useful to introduce the notation @xmath34 to refer to the component of @xmath30 corresponding to the indicator function @xmath35,$ ] and similarly for @xmath36 . then , the mrf in eq .",
    "[ eq : mrf_def ] would have @xmath37 and @xmath38 .",
    "many operations on graphical models can be more elegantly represented using this exponential family representation .",
    "a standard problem in the exponential family is to compute the mean value of @xmath39 ,    @xmath40 called the `` mean parameters '' .",
    "it is easy to show these are equal to the gradient of the log - partition function .",
    "@xmath41    for an exponential family corresponding to an mrf , computing @xmath42 is equivalent to computing all the marginal probabilities . to see this , note",
    "that , using a similar notation for indexing @xmath42 as for @xmath30 above ,    @xmath43=p({\\bf x}_{c};\\boldsymbol{\\theta}).\\ ] ]    conditional distributions can be represented by thinking of the parameter vector @xmath44 as being a function of the input @xmath13 , where @xmath45 are now the free parameters rather than @xmath30 .",
    "( again , the nature of the dependence of @xmath30 on @xmath13 and @xmath45 will vary by application . )",
    "then , we have that @xmath46 sometimes called a curved conditional exponential family .      the focus of this paper is learning of model parameters from data .",
    "( automatically determining graph _ structure _ remains an active research area , but is not considered here . ) specifically , we take the goal of learning to be to minimize the empirical risk @xmath47 where the summation is over all examples @xmath48 in the dataset , and the loss function @xmath49 quantifies how well the distribution defined by the parameter vector @xmath30 matches the example @xmath48 .",
    "several loss functions are considered in section [ sec : loss - functions ] .",
    "we assume that the empirical risk will be fit by some gradient - based optimization .",
    "hence , the main technical issues in learning are which loss function to use and how to compute the gradient @xmath50 .    in practice , we will usually be interested in fitting conditional distributions . using the notation from eq .",
    "[ eq : conditiona - efam ] , we can write this as    @xmath51    note that if one has recovered @xmath52 @xmath53 is immediate from the vector chain rule as    @xmath54    thus , the main technical problems involved in fitting a conditional distribution are similar to those for a generative distribution : one finds @xmath55 , computes the @xmath56 and @xmath50 on example @xmath48 exactly as in the generative case , and finally recovers @xmath53 from eq .",
    "[ eq : conditional_model_chainrule ] .",
    "so , for simplicity , @xmath13 and @xmath45 will largely be ignored in the theoretical developments below .",
    "this section reviews approximate methods for computing marginals , with notation based on wainwright and jordan @xcite . for readability ,",
    "all proofs in this section are postponed to appendix a.    the relationship between the marginals and the log - partition function in eq .",
    "[ eq : da_dtheta_eq_mu ] is key to defining approximate marginalization procedures . in section [ sub : exact - variational - principle ] , the exact variational principle shows that the ( intractable ) problem of computing the log - partition function can be converted to a ( still intractable ) optimization problem . to derive",
    "a tractable marginalization algorithm one approximates this optimization , yielding some approximate log - partition function @xmath57 .",
    "the approximate marginals are then taken as the _ exact _ gradient of @xmath58 .",
    "we define the reverse mapping @xmath59 to return some parameter vector that yields that marginals @xmath42 .",
    "while this will in general not be unique @xcite , any two vectors that produce the same marginals @xmath42 will also yield the same distribution , and so @xmath60 is unambiguous .",
    "[ exact variational principle]the log - partition function can also be represented as    @xmath61    where @xmath62 is the marginal polytope , and @xmath63    is the entropy .    in treelike graphs ,",
    "this optimization can be solved efficiently . in general graphs , however , it is intractable in two ways .",
    "first , the marginal polytope @xmath64 becomes difficult to characterize .",
    "second , the entropy is intractable to compute .    applying danskin s theorem to eq .",
    "[ eq : a - variational ] yields that    @xmath65    thus , the partition function ( eq . [ eq : a - variational ] ) and marginals ( eq . [ eq : mu - variational ] ) can both be obtained from solving the same optimization problem .",
    "this close relationship between the log - partition function and marginals is heavily used in the derivation of approximate marginalization algorithms . to compute approximate marginals ,",
    "first , derive an approximate version of the optimization in eq .",
    "[ eq : a - variational ] .",
    "next , take the exact gradient of this approximate partition function .",
    "this strategy is used in both of the approximate marginalization procedures considered here : mean field and tree - reweighted belief propagation .",
    "the idea of mean field is to approximate the exact variational principle by replacing @xmath64 with some tractable subset @xmath66 , such that @xmath67 is easy to characterize , and for any vector @xmath68 we can exactly compute the entropy . to create such a set @xmath67 , instead of considering the set of mean vectors obtainable from _ any _ parameter vector ( which characterizes @xmath64 ) , consider a subset of _ tractable _ parameter vectors . the simplest way to achieve this to restrict consideration to parameter vectors @xmath30 with @xmath69 for all factors @xmath1 .",
    "@xmath70    it is not hard to see that this corresponds to the set of _ fully - factorized _ distributions . note also that this is ( in non - treelike graphs ) a non - convex set , since it has the same convex hull as @xmath64 , but is a proper subset .",
    "so , the mean field partition function approximation is based on the optimization    @xmath71    with approximate marginals corresponding to the maximizing vector @xmath42 , i.e.    @xmath72    since this is maximizing the same objective as the exact variational principle , but under a more restricted constraint set , clearly @xmath73    here , since the marginals are coming from a fully - factorized distribution , the exact entropy is available as @xmath74    the strategy we use to perform the maximization in eq .",
    "[ eq : a - meanfield ] is block - coordinate ascent .",
    "namely , we pick a coordinate @xmath75 , then set @xmath76 to maximize the objective , leaving @xmath77 fixed for all @xmath78 .",
    "the next theorem formalizes this .",
    "[ mean field updates]a local maximum of eq .",
    "[ eq : a - meanfield ] can be reached by iterating the updates    @xmath79    where @xmath11 is a normalizing factor ensuring that @xmath80 .      whereas mean field replaced the marginal polytope with a subset , tree - reweighted belief propagation ( trw )",
    "replaces it with a superset , @xmath81 .",
    "this clearly can only increase the value of the approximate log - partition function .",
    "however , a further approximation is needed , as the entropy remains intractable to compute for an arbitrary mean vector @xmath42 .",
    "( it is not even defined for @xmath82 ) thus , trw further approximates the entropy with a tractable upper bound .",
    "taken together , these two approximations yield a tractable upper bound on the log - partition function .",
    "thus , trw is based on the optimization problem    @xmath83    again , the approximate marginals are simply the maximizing vector @xmath42 , i.e. ,    @xmath84    the relaxation of the local polytope used in trw is the _ local polytope _ ,    @xmath85    since any valid marginal vector must obey these constraints , clearly @xmath86",
    ". however , @xmath87 in general also contains unrealizable vectors ( though on trees @xmath88 .",
    "thus , the marginal vector returned by trw may , in general , be inconsistent in the sense that no joint distribution yields those marginals .",
    "the entropy approximation used by trw is @xmath89    where @xmath90 is the univariate entropy corresponding to variable @xmath91 , and @xmath92 is the mutual information corresponding to the variables in the factor @xmath1 .",
    "the motivation for this approximation is that if the constants @xmath93 are selected appropriately , this gives an upper bound on the true entropy .",
    "[ trw entropy bound]let @xmath94 be a distribution over tree structured graphs , and define @xmath95 then , with @xmath96 as defined in eq .",
    "[ eq : trw - entropy ] , @xmath97    thus , trw is maximizing an upper bound on the exact variational principle , under an expanded constraint set .",
    "since both of these changes can only increase the maximum value , we have that @xmath98 .",
    "now , we consider how to actually compute the approximate log - partition function and associated marginals .",
    "consider the message - passing updates    @xmath99    where `` @xmath100 '' is used as an assignment operator to means assigning after normalization .",
    "[ trw updates]let @xmath93 be as in the previous theorem . then , if the updates in eq .",
    "[ eq : trw - msgs ] reach a fixed point , the marginals defined by @xmath101    constitute the global optimum of eq .",
    "[ eq : a - trw ] .",
    "so , if the updates happen to converge , we have the solution .",
    "meltzer et al .",
    "show @xcite that on certain graphs made up of _ monotonic chains _ , an appropriate ordering of messages does assure convergence .",
    "( the proof is essentially that under these circumstances , message passing is equivalent to coordinate ascent in the dual . )",
    "trw simplifies into loopy belief propagation by choosing @xmath102 everywhere , though the bounding property is lost .",
    "for space , only a representative sample of prior work can be cited . a recent review",
    "@xcite is more thorough .",
    "though , technically , a `` loss '' should be minimized , we continue to use this terminology for the likelihood and its approximations , where one wishes to maximize .    for simplicity ,",
    "the discussion below is for the generative setting . using the same loss functions for training a conditional model is simple ( section [ sub : learning ] )",
    ".      the classic loss function would be the likelihood , with @xmath103    this has the gradient @xmath104    one argument for the likelihood is that it is efficient ; given a correct model , as data increases it converges to true parameters at an asymptotically optimal rate @xcite .    some previous work uses tree structured graphs where marginals may be computed exactly @xcite",
    ". of course , in high - treewidth graphs , the likelihood and its gradient will be intractable to compute exactly , due to the presence of the log - partition function @xmath105 and marginals @xmath106 .",
    "this has motivated a variety of approximations .",
    "the first is to approximate the marginals @xmath42 using markov chain monte carlo @xcite .",
    "this can lead to high computational expense ( particularly in the conditional case , where different chains must be run for each input ) .",
    "contrastive divergence @xcite further approximates these samples by running the markov chain for only a few steps , but started at the data points @xcite .",
    "if the markov chain is run long enough , these approaches can give an arbitrarily good approximation .",
    "however , markov chain parameters may need to be adjusted to the particular problem , and these approaches are generally slower than those discussed below .",
    "a seemingly heuristic approach would be to replace the marginals in eq .",
    "[ eq : likelihood - gradient ] with those from an approximate inference method",
    ". this approximation can be quite principled if one thinks instead of approximating the log - partition function in the likelihood itself ( eq . [ eq : likelihood ] ) .",
    "then , the corresponding approximate marginals will emerge as the _ exact _ gradient of this surrogate loss .",
    "this `` surrogate likelihood '' @xcite approximation appears to be the most widely used loss in imaging problems , with marginals approximated by either mean field @xcite , trw @xcite or lbp @xcite .",
    "however , the terminology of `` surrogate likelihood '' is not widespread and in most cases , only the gradient is computed , meaning the optimization can not use line searches .",
    "if one uses a log - partition approximation that provides a bound on the true log - partition function , the surrogate likelihood will then bound the true likelihood .",
    "specifically , mean field based surrogate likelihood is an upper bound on the true likelihood , while trw - based surrogate likelihood is a lower bound .      in many applications , only a subset of variables may be observed .",
    "suppose that we want to model @xmath107 where @xmath108 is observed , but @xmath109 is hidden .",
    "a natural loss function here is the expected maximization ( em ) loss    @xmath110    it is easy to show that this is equivalent to @xmath111 where @xmath112 is the log - partition function with @xmath108 `` clamped '' to the observed values .",
    "if all variables are observed @xmath113 reduces to @xmath114 .",
    "if on substitutes a variational approximation for @xmath113 , a `` variational em '' algorithm ( * ? ? ?",
    "6.2.2 ) can be recovered that alternates between computing approximate marginals and parameter updates . here , because of the close relationship to the surrogate likelihood , we designate `` surrogate em '' for the case where @xmath113 and @xmath105 may both be approximated and the learning is done with a gradient - based method . to obtain a bound on the true em loss , care is required .",
    "for example , lower - bounding @xmath113 using mean field , and upper - bounding @xmath105 using trw means a lower - bound on the true em loss . however , using the same approximation for both @xmath105 and @xmath113 appears to work well in practice @xcite .",
    "a third approximation of the likelihood is to search for a `` saddle - point '' . here",
    ", one approximates the gradient in eq .",
    "[ eq : likelihood - gradient ] by running a ( presumably approximate ) map inference algorithm , and then imagining that the marginals put unit probability at the approximate map solution , and zero elsewhere @xcite .",
    "this is a heuristic method , but it can be expected to work well when the estimated map solution is close to the true map and the conditional distribution @xmath17 is strongly `` peaked ''",
    ".      finally , there are two classes of likelihood approximations that do not require inference .",
    "the first is the classic pseudolikelihood @xcite , where one uses @xmath115    this can be computed efficiently , even in high treewidth graphs , since conditional probabilities are easy to compute .",
    "besag @xcite showed that , under certain conditions , this will converge to the true parameter vector as the amount of data becomes infinite .",
    "the pseudolikelihood has been used in many applications @xcite . instead of the probability of individual variables given all others",
    ", one can take the probability of patches of variables given all others , sometimes called the `` patch '' pseudolikelihood @xcite .",
    "this interpolates to the exact likelihood as the patches become larger , though some type of inference is generally required .",
    "more recently , sutton and mccallum @xcite suggested the piecewise likelihood . the idea is to approximate the log - partition function as a sum of log - partition functions of the different  pieces  of the graph .",
    "there is flexibility in determining which pieces to use . in this paper",
    ", we will use pieces consisting of each clique and each variable , which worked better in practice than some alternatives .",
    "then , one has the surrogate partition function @xmath116    it is not too hard to show that @xmath117 . in practice ,",
    "it is sometimes best to make some heuristic adjustments to the parameters after learning to improve test - time performance @xcite .      given the discussion in section [ sub : the - likelihood ] , one might conclude that the likelihood , while difficult to optimize , is an ideal loss function since , given a well - specified model , it will converge to the true parameters at asymptotically efficient rates . however , this conclusion is complicated by two issues . first , of course , the maximum likelihood solution is computationally intractable , motivating the approximations above .",
    "a second issue is that of _ model mis - specification_. for many types of complex phenomena , we will wish to fit a model that is approximate in nature .",
    "this could be true because the conditional independencies asserted by the graph do not exactly hold , or because the parametrization of factors is too simplistic .",
    "these approximations might be made out of ignorance , due to a lack of knowledge about the domain being studied , or deliberately because the true model might have too many degrees of freedom to be fit with available data .    in the case of an approximate model ,",
    "no `` true '' parameters exist .",
    "the idea of marginal - based loss functions is to instead consider how the model will be used . if one will compute marginals at test - time ",
    "perhaps for mpm inference ( section [ sub : inference - problems ] )  it makes sense to maximize the accuracy of these predictions .",
    "further , if one will use an approximate inference algorithm , it makes sense to optimize the accuracy of the _ approximate _ marginals .",
    "this essentially fits into the paradigm of empirical risk minimization @xcite .",
    "the idea of training a probabilistic model using an alternative loss to the likelihood goes back at least to bahl et al . in the late 1980s",
    "@xcite .",
    "there is reason to think the likelihood is somewhat robust to model mis - specification . in the infinite data limit",
    ", it finds the `` closest '' solution in the sense of kl - divergence since , if @xmath118 is the true distribution , then @xmath119      the univariate logistic loss @xcite is defined by @xmath120 where we use the notation @xmath121 to indicate that the loss is implicitly defined with respect to the marginal predictions of some ( possibly approximate ) algorithm , rather than the true marginals .",
    "this measures the mean accuracy of all univariate marginals , rather than the joint distribution .",
    "this loss can be seen as empirical risk minimization of the kl - divergence between the true marginals and the predicted ones , since @xmath122 if defined on exact marginals , this is a type of composite likelihood @xcite .",
    "perhaps the most natural loss in the conditional setting would be the univariate classification error ,    @xmath123 where @xmath124 is the step function .",
    "this exactly measures the number of components of @xmath12 that would be incorrectly predicted if using mpm inference .",
    "of course , this loss is neither differentiable nor continuous , which makes it impractical to optimize using gradient - based methods .",
    "instead gross et al .",
    "@xcite suggest approximating with a sigmoid function @xmath125 , where @xmath126 controls approximation quality .",
    "there is evidence @xcite that the smoothed classification loss can yield parameters with lower univariate classification error under mpm inference .",
    "however , our experience is that it is also more prone to getting stuck in local minima , making experiments difficult to interpret .",
    "thus , it is not included in the experiments below .",
    "our experience with the univariate quadratic loss @xcite is similar .",
    "any of the above univariate losses can be instead taken based on cliques .",
    "for example , the clique logistic loss is    @xmath127    which may be seen as empirical risk minimization of the mean kl - divergence of the true clique marginals to the predicted ones .",
    "an advantage of this with an exact model is consistency .",
    "simple examples show cases where a model predicts perfect univariate marginals , despite the joint distribution being very inaccurate .",
    "however , if all clique marginals are correct , the joint must be correct , by the standard moment matching conditions for the exponential family @xcite .",
    "marginal - based loss functions can accommodate hidden variables by simply taking the sum in the loss over the _",
    "observed _ variables only .",
    "a similar approach can be used with the pseudolikelihood or piecewise likelihood .",
    ", title=\"fig:\"],title=\"fig:\"],title=\"fig : \" ]                to compare the effects of different loss functions in the presence of model mis - specification , this section contains a simple example where the graphical model takes the following `` chain '' structure :    at ( 3.5 , -.5 ) @xmath128 ; = [ draw , shape = circle ] ; ( x1 ) at ( 0 , 0 ) @xmath2 ; ( x2 ) at ( 1 , 0 ) @xmath3 ; ( x3 ) at ( 2 , 0 ) @xmath4 ; ( x4 ) at ( 3 , 0 ) @xmath5 ; ( x200 ) at ( 4 , 0 ) @xmath129 ; ( y1 ) at ( 0 , -1 ) @xmath130 ; ( y2 ) at ( 1 , -1 ) @xmath131 ; ( y3 ) at ( 2 , -1 ) @xmath132 ; ( y4 ) at ( 3 , -1 ) @xmath133 ; ( y200 ) at ( 4 , -1 ) @xmath134 ; ( x1 )  ( x2 ) ( x2 )  ( x3 ) ( x3 )  ( x4 ) ( x1 )  ( y1 ) ( x2 )  ( y2 ) ( x3 )  ( y3 ) ( x4 )  ( y4 ) ( y1 )  ( y2 ) ( y2 )  ( y3 ) ( y3 )  ( y4 ) ( x200 )  ( y200 ) ;    here , exact inference is possible , so comparison is not complicated by approximate inference .    all variables are binary .",
    "parameters are generated by taking @xmath36 randomly from the interval @xmath135 $ ] for all @xmath91 and @xmath136 .",
    "interaction parameters are taken as @xmath137 when @xmath138 , and @xmath139 when @xmath140 , where @xmath141 is randomly chosen from the interval @xmath135 $ ] for all @xmath142 .",
    "interactions @xmath143 and @xmath144 are chosen in the same way .    to systematically study the effects of differing `` amounts '' of mis - specification , after generating data",
    ", we apply various circular shifts to @xmath12 .",
    "thus , the data no longer corresponds exactly the the structure of the graphical model being fit .",
    "thirty - two different random distributions were created . for each",
    ", various quantities of data were generated by markov chain monte carlo , with shifts introduced after sampling .",
    "the likelihood was fit using the closed - form gradient ( sec .",
    "[ sub : the - likelihood ] ) , while the logistic losses were trained using a gradient obtained via backpropagation ( sec .",
    "[ sec : truncated - fitting ] ) .",
    "[ fig : exactinference - means - smaller ] shows the mean test error ( estimated on 1000 examples ) , while fig . [",
    "fig : exactinference - example ] shows example marginals .",
    "we see that the performance of all methods deteriorates with mis - specification , but the marginal - based loss functions are more resistant to these effects .",
    "another class of methods explicitly optimize the performance of map inference @xcite .",
    "this paper focuses on applications that use marginal inference , and that may need to accommodate hidden variables , and so concentrates on likelihood and marginal - based losses .",
    "we now turn to the issue of how to train high - treewidth graphical models to optimize the performance of a marginal - based loss function , based on some approximate inference algorithm .",
    "now , computing the value of the loss for any of the marginal - based loss functions is not hard .",
    "one can simply run the inference algorithm and plug the resulting marginal into the loss .",
    "however , we also require the gradient @xmath50 .",
    "our first result is that the loss gradient can be obtained by solving a sparse linear system . here , it is useful to introduce notation to distinguish the loss @xmath56 , defined in terms of the parameters @xmath30 from the loss @xmath145 , defined directly in terms of the marginals @xmath42 .",
    "( note that though the notation suggests the application to marginal inference , this is a generic result . )",
    "suppose that @xmath146 define @xmath147 then , letting @xmath148 @xmath149    a proof may be found in appendix b. this theorem states that , essentially , once one has computed the predicted marginals , the gradient of the loss with respect to marginals @xmath150 can be transformed into the gradient of the loss with respect to parameters @xmath50 through the solution of a sparse linear system .",
    "the optimization in eq .",
    "[ eq : implicit_optimization ] takes place under linear constraints , which encompasses the local polytope used in trw message - passing ( eq . [ eq : local - polytope ] ) .",
    "this theorem does not apply to mean field , as @xmath67 is not a linear constraint set when viewed as a function of both clique and univariate marginals .    in any case",
    ", the methods developed below are simpler to use , as they do not require explicitly forming the constraint matrix @xmath151 or solving the linear system .",
    "this section observes that variational methods have a special structure that allows derivatives to be calculated without explicitly forming or inverting a linear system .",
    "we have , by the vector chain rule , that @xmath152    a classic trick in scientific computing is to efficiently compute jacobian - vector products by finite differences .",
    "the basic result is that , for any vector @xmath153 , @xmath154 which is essentially just the definition of the derivative of @xmath42 in the direction of @xmath153 . now",
    ", this does not immediately seem helpful , since eq .",
    "[ eq : loss_chainrule ] requires @xmath155 , not @xmath156 .",
    "however , with variational methods , these are symmetric .",
    "the simplest way to see this is to note that @xmath157    domke @xcite lists conditions for various classes of entropies that guarantee that @xmath158 will be differentiable .",
    "combining the above three equations , the loss gradient is available as the limit    @xmath159    in practice , of course , the gradient is approximated using some finite @xmath160 . the simplest approximation , one - sided differences , simply takes a single value of @xmath160 in eq .",
    "[ eq : implicit_limit ] , rather than a limit .",
    "more accurate results at the cost of more calls to inference , are given using two - sided differences , with @xmath161 which is accurate to order @xmath162 still more accurate results are obtained with `` four - sided '' differences , with @xmath163 which is accurate to order @xmath164 @xcite .",
    "[ alg : calculating - loss - derivatives-2sided ] shows more explicitly how the loss gradient could be calculated , using two - sided differences .",
    "the issue remains of how to calculate the step size @xmath160 .",
    "each of the approximations above becomes exact as @xmath165 .",
    "however , as @xmath160 becomes very small , numerical error eventually dominates . to investigate this issue experimentally , we generated random models on a @xmath166 binary grid , with each parameter @xmath36 randomly chosen from a standard normal , while each interaction parameter @xmath167 was chosen randomly from a normal with a standard deviation of @xmath168 . in each case , a random value @xmath12 was generated , and the `` true '' loss gradient was estimated by standard ( inefficient ) 2-sided finite differences , with inference re - run after each component of @xmath30 is perturbed independently . to this , we compare one , two , and four - sided perturbations . in all cases ,",
    "the step size is , following andrei @xcite , taken to be @xmath169 where @xmath170 is machine epsilon , and @xmath171 is a multiplier that we will vary .",
    "note that the optimal power of @xmath170 will depend on the finite difference scheme ; @xmath172 is optimal for two - sided differences ( * ? ? ?",
    "all calculations take place in double - precision with inference run until marginals changed by a threshold of less than @xmath173 .",
    "[ fig : perurbation - sizes ] shows that using many - sided differences leads to more accuracy , at the cost of needing to run inference more times to estimate a single loss gradient . in the following experiments , we chose two - sided differences with a multiplier of 1 as a reasonable tradeoff between accuracy , simplicity , and computational expense .",
    "welling and teh used sensitivity of approximate beliefs to parameters to approximate joint probabilities of non - neighboring variables @xcite .",
    "1 .   do inference .",
    "@xmath174 2 .   at @xmath175 , calculate the gradient @xmath176 .",
    "3 .   calculate a perturbation size @xmath160 .",
    "4 .   do inference on perturbed parameters .",
    "1 .   @xmath177 2 .",
    ".   recover full derivative as  @xmath179 .    .",
    "top : trw .",
    "bottom : mean field .",
    "two effects are in play here : first , for too small a perturbation , numerical errors dominate . meanwhile , for too large a perturbation , approximation errors dominate .",
    "we see that using 2- or 4-sided differences differences reduce approximation error , leading to better results with larger perturbations.[fig : perurbation - sizes],title=\"fig : \" ] .",
    "top : trw .",
    "bottom : mean field .",
    "two effects are in play here : first , for too small a perturbation , numerical errors dominate . meanwhile , for too large a perturbation , approximation errors dominate .",
    "we see that using 2- or 4-sided differences differences reduce approximation error , leading to better results with larger perturbations.[fig : perurbation - sizes],title=\"fig : \" ] .",
    "top : trw .",
    "bottom : mean field .",
    "two effects are in play here : first , for too small a perturbation , numerical errors dominate . meanwhile , for too large a perturbation , approximation errors dominate .",
    "we see that using 2- or 4-sided differences differences reduce approximation error , leading to better results with larger perturbations.[fig : perurbation - sizes],title=\"fig : \" ]    .",
    "top : trw .",
    "bottom : mean field .",
    "two effects are in play here : first , for too small a perturbation , numerical errors dominate . meanwhile , for too large a perturbation , approximation errors dominate .",
    "we see that using 2- or 4-sided differences differences reduce approximation error , leading to better results with larger perturbations.[fig : perurbation - sizes],title=\"fig : \" ] .",
    "top : trw .",
    "bottom : mean field .",
    "two effects are in play here : first , for too small a perturbation , numerical errors dominate .",
    "meanwhile , for too large a perturbation , approximation errors dominate .",
    "we see that using 2- or 4-sided differences differences reduce approximation error , leading to better results with larger perturbations.[fig : perurbation - sizes],title=\"fig : \" ] .",
    "top : trw .",
    "bottom : mean field .",
    "two effects are in play here : first , for too small a perturbation , numerical errors dominate . meanwhile , for too large a perturbation , approximation errors dominate .",
    "we see that using 2- or 4-sided differences differences reduce approximation error , leading to better results with larger perturbations.[fig : perurbation - sizes],title=\"fig : \" ]",
    "the previous methods for computing loss gradients are derived under the assumption that the inference optimization is solved exactly . in an implementation , of course , some convergence threshold must be used .",
    "different convergence thresholds can be used in the learning stage and at test time . in practice , we have observed that too loose a threshold in the learning stage can lead to a bad estimated risk gradient , and learning terminating with a bad search direction .",
    "meanwhile , a loose threshold can often be used at test time with few consequences . usually , a difference of @xmath180 in estimated marginals has little practical impact , but this can still be enough to prevent learning from succeeding @xcite .",
    "it seems odd that the learning algorithm would spend the majority of computational effort exploring tight convergence levels that are irrelevant to the practical performance of the model . here ,",
    "we _ define _ the learning objective in terms of the approximate marginals obtained after a fixed number of iterations . to understand this",
    ", one may think of the inference process not as an optimization , but rather as a large , nonlinear function .",
    "this clearly leads to a well - defined objective function .",
    "inputting parameters , applying the iterations of either trw or mean field , computing predicted marginals , and finally a loss are all differentiable operations .",
    "thus , the loss gradient is efficiently computable , at least in principle , by reverse - mode automatic differentiation ( autodiff ) , an approach explored by stovanov et al . @xcite . in preliminary work , we experimented with autodiff tools , but found these to be unsatisfactory for our applications for two reasons .",
    "firstly , these tools impose a computational penalty over manually derived gradients .",
    "secondly , autodiff stores all intermediate calculations , leading to large memory requirements .",
    "the methods derived below use less memory , both in terms of constant factors and big - o complexity . nevertheless , some of these problems are issues with current _ implementations _ of reverse - mode autodiff , avoidable in theory",
    ".    both mean field and trw involve steps where we first take a product of a set of terms , and then normalize .",
    "we define a `` backnorm '' operator , which is useful in taking derivatives over such operations , by @xmath181    this will be used in the algorithms here .",
    "more discussion on this point can be found in appendix c.      1 .",
    "initialize @xmath121 uniformly .",
    "2 .   repeat @xmath182 times for all @xmath75 : 1 .",
    "push the marginals @xmath183 onto a stack .",
    "2 .   @xmath184 3 .",
    "compute @xmath56 , @xmath185 and @xmath186 .",
    "4 .   initialize @xmath187 5 .",
    "repeat @xmath188 times for all @xmath75 ( in reverse order ) : 1 .",
    "@xmath189 2 .",
    "@xmath190 3 .",
    "@xmath191@xmath192 4 .",
    "@xmath193 + @xmath194 5 .",
    "pull marginals @xmath183 from the stack .",
    "@xmath195    the first backpropagating inference algorithm , back mean field , is shown as alg .",
    "[ alg : back - mean - field ] .",
    "the idea is as follows : suppose we start with uniform marginals , run @xmath188 iterations of mean field , and then regardless of if mean field has converged or not take predicted marginals and plug them into one of the marginal - based loss functions . since each step in this process is differentiable , this specifies the loss as a differentiable function of model parameters .",
    "we want the exact gradient of this function .",
    "after execution of back mean field , @xmath196    a proof sketch is in appendix c. roughly speaking , the proof takes the form of a mechanical differentiation of each step of the inference process .",
    "note that , as written , back mean field only produces univariate marginals , and so can not cope with loss functions making use of clique marginals .",
    "however , with mean field , the clique marginals , are simply the product of univariate marginals : @xmath197 .",
    "hence , any loss defined on clique marginals can equivalently be defined on univariate marginals .      1 .",
    "initialize @xmath171 uniformly .",
    "repeat @xmath182 times for all pairs @xmath198 , with @xmath199 : 1 .",
    "push the messages @xmath200 onto a stack .",
    "@xmath201 3 .",
    "> p0.3in > p0.01inc @xmath202 & @xmath100 & @xmath203 + + @xmath204 4 .   > p0.3in > p0.01inc @xmath77 & @xmath100 & @xmath205 + + @xmath206 5 .   compute @xmath56 , @xmath207 and @xmath186 . 6 .   for all @xmath208 1 .",
    "> p0.3in > p0.01inc @xmath209 & @xmath210 & @xmath211 + 2 .   > p0.3in > p0.01inc @xmath212 & @xmath213 & @xmath214 + 3 .   > p0.3in",
    "> p0.01inc @xmath215 & @xmath213 & @xmath216 + + @xmath217 4 .   > p0.3in > p0.01inc @xmath218 & @xmath213 & @xmath219 + + @xmath220 7 .   for all @xmath91 , 1 .   > p0.3in > p0.01inc @xmath221 & @xmath210 & @xmath222 + 2 .   > p0.3in > p0.01inc @xmath215 & @xmath213 & @xmath221 + 3 .   > p0.3in > p0.01inc @xmath218 & @xmath213 & @xmath223 + + @xmath224 8 .   repeat @xmath188 times for all pairs @xmath198 ( in reverse order ) 1 .   > p0.3in > p0.01inc @xmath225 & @xmath210 & @xmath226 + 2 .   > p0.3in > p0.01inc @xmath221 & @xmath210 & @xmath227 + 3 .   > p0.3in > p0.01inc @xmath212 & @xmath213 & @xmath228 + 4 .   > p0.3in > p0.01inc @xmath229 & @xmath230 & @xmath231 + + @xmath232 5 .   > p0.3in > p0.01inc @xmath233 & @xmath213 & @xmath234 + +   + @xmath235 6 .",
    "pull messages @xmath200 from the stack .",
    "@xmath236    next , we consider truncated fitting with trw inference . as",
    "above , we will assume that some fixed number @xmath188 of inference iterations have been run , and we want to define and differentiate a loss defined on the current predicted marginals .",
    "[ alg : back - trw ] shows the method .",
    "after execution of back trw , @xmath196    again , a proof sketch is in appendix c.    if one uses pairwise factors only , uniform appearance probabilities of @xmath237 , removes all reference to the stack , and uses a convergence threshold in place of a fixed number of iterations , one obtains essentially eaton and ghahramani s back belief propagation ( * ? ?",
    "? * extended version , fig .",
    ", we refer to the general strategy of using full ( non - truncated ) inference as `` backpropagation '' , either with lbp , trw , or mean field .",
    "applying the truncated fitting strategies to any of the marginal - based loss functions is simple .",
    "applying it to the likelihood or em loss , however , is not so straightforward .",
    "the reason is that these losses ( eqs .",
    "[ eq : likelihood ] and [ eq : em_loss ] ) are defined , not in terms of predicted _ marginals _ , but in terms of _ partition functions_. nevertheless , we wish to compare to these losses in the experiments below . as we found truncation to be critical for speed ,",
    "we instead derive a variant of truncated fitting .",
    "the basic idea is to define a `` truncated partition function '' .",
    "this is done by taking the predicted marginals , obtained after a fixed number of iterations , and plugging them into the entropy approximations used either for mean field ( eq . [ eq : meanfield - entropy ] ) or trw ( eq . [ eq : trw - entropy ] ) .",
    "the approximate entropy @xmath96 is then used in defining a truncated partition function as @xmath238 as we will see below , with too few inference iterations , using this approximation can cause the surrogate likelihood to diverge . to see why , imagine an extreme case where _ zero _ inference iterations are used .",
    "this results in the loss @xmath239 where @xmath240 are the initial marginals .",
    "as long as the mean of @xmath31 over the dataset is not equal to @xmath240 , arbitrary loss can be achieved . with hidden variables ,",
    "@xmath113 is defined similarly , but with the variables @xmath108 `` clamped '' to the observed values .",
    "( those variables will play no role in determining @xmath106 ) .",
    "these experiments consider three different datasets with varying complexity . in all cases",
    ", we try to keep the features used relatively simple .",
    "this means some sacrifice in performance , relative to using sophisticated features tuned more carefully to the different problem domains .",
    "however , given that our goal here is to gauge the relative performance of the different algorithms , we use simple features for the sake of experimental transparency .",
    "we compare marginal - based learning methods to the surrogate likelihood / em , the pseudolikelihood and piecewise likelihood .",
    "these comparisons were chosen because , first , they are the most popular in the literature ( sec . [",
    "sec : loss - functions ] ) .",
    "second , the surrogate likelihood also requires marginal inference , meaning an `` apples to apples '' comparison using the same inference method .",
    "third , these methods can all cope with hidden variables , which appear in our third dataset .    in each experiment , an `` independent '' model , trained",
    "using univariate features only with logistic loss was used to initialize others . the smoothed classification loss , because of more severe issues with local minima , was initialized using surrogate likelihood / em .",
    "all experiments here will be on vision problems , using a pairwise , 4-connected grid . learning uses the l - bfgs optimization algorithm .",
    "the values @xmath241 are linearly parametrized in terms of unary and edge features .",
    "formally , we will fit two matrices , @xmath242 and @xmath243 , which determine all unary and edge features , respectively .",
    "these can be expressed most elegantly by introducing a bit more notation .",
    "let @xmath244 denote the set of parameter values @xmath36 for all values @xmath136 .",
    "if @xmath245 denotes the vector of unary features for variable @xmath91 given input image @xmath13 , then @xmath246    similarly , let @xmath247 denote the set of parameter values @xmath167 for all @xmath248 . if @xmath249 is the vector of edge features for pair @xmath142 , then @xmath250    once @xmath50 has been calculated ( for whichever loss and method ) , we can easily recover the gradients with respect to @xmath242 and @xmath243 by    @xmath251      .binary denoising error rates for different noise levels @xmath252 .",
    "all methods use trw inference with back - propagation based learning with a threshold of @xmath253.[tab : binary - denoising - error ] [ cols=\"^,^,^,^,^,^,^ \" , ]     our final experiments consider the stanford backgrounds dataset .",
    "this consists of 715 images of resolution approximately @xmath254 .",
    "most pixels are labeled as one of eight classes , with some unlabeled .",
    "the unary features @xmath245 we use here are identical to those for the horses dataset . in preliminary experiments , we tried training models with various resolutions .",
    "we found that reducing resolution to 20% of the original after computing features , then upsampling the predicted marginals yielded significantly better results than using the original resolution .",
    "hence , this is done for all the following experiments .",
    "edge features are identical to those for the horses dataset , except only based on the difference of rgb intensities , meaning 22 total edge features @xmath249 .    in a first experiment",
    ", we compare the performance of truncated fitting , perturbation , and back - propagation , using 100 images from this dataset for speed .",
    "we train with varying thresholds for perturbation and back - propagation , while for truncated learning , we use various numbers of iterations .",
    "all models are trained with trw to fit the univariate logistic loss .",
    "if a bad search - direction is encountered , l - bfgs is re - initialized .",
    "results are shown in fig .",
    "[ fig : pert_bprop - trunc ] .",
    "we see that with loose thresholds , perturbation and back - propagation experience learning failure at sub - optimal solutions .",
    "truncated fitting is far more successful ; using more iterations is slower to fit , but leads to better performance at convergence .",
    ", title=\"fig : \" ] , title=\"fig : \" ] , title=\"fig : \" ]    in a second experiment , we train on the entire dataset , with errors estimated using five - fold cross validation . here , an incremental procedure was used , where first a subset of 32 images was trained on , with 1000 learning iterations .",
    "the number of images was repeatedly doubled , with the number of learning iterations halved . in practice this reduced training time substantially .",
    "results are shown in fig .",
    "[ fig : stanford - background - results ] .",
    "these results use a ridge regularization penalty of @xmath255 on all parameters .",
    "( this is relative to the empirical risk , as measured per pixel . ) for em , and marginal based loss functions , we set this as @xmath256 .",
    "we found in preliminary experiments that using a smaller regularization constant caused truncated em to diverge even with 10 iterations .",
    "the pseudolikelihood and piecewise benefit from less regularization , and so we use @xmath257 there .",
    "again the marginal based loss functions outperform others . in particular",
    ", they also perform quite well even with @xmath258 iterations , where truncated em diverges .",
    "training parameters of graphical models in a high treewidth setting involves several challenges . in this paper",
    ", we focus on three : model mis - specification , the necessity of approximate inference , and computational complexity .    the main technical contribution of this paper is several methods for training based on the marginals predicted by a given approximate inference algorithm .",
    "these methods take into account model mis - specification and inference approximations . to combat computational complexity",
    ", we introduce `` truncated '' learning , where the inference algorithm only needs to be run for a fixed number of iterations .",
    "truncation can also be applied , somewhat heuristically , to the surrogate likelihood .    among previous methods",
    ", we experimentally find the surrogate likelihood to outperform the pseudolikelihood or piecewise learning . by more closely reflecting the test criterion of hamming loss , marginal - based loss functions perform still better , particularly on harder problems ( though the surrogate likelihood generally displays smaller train / test gaps . ) additionally marginal - based loss functions are more amenable to truncation , as the surrogate likelihood diverges with too few iterations .",
    "10 [ 1]#1 url@samestyle [ 2]#2 [ 2]l@#1=l@#1#2    j.  besag , `` spatial interaction and the statistical analysis of lattice systems , '' _ journal of the royal statistical society . series b ( methodological ) _ , vol .",
    "36 , no .  2 ,",
    "pp . 192236 , 1974 .",
    "j.  lafferty , a.  mccallum , and f.  pereira , `` conditional random fields : probabilistic models for segmenting and labeling sequence data , '' in _ icml _ , 2001 .    m.  nikolova , `` model distortions in bayesian map reconstruction , '' _ inverse problems and imaging _ , vol .  1 , no .  2 , pp .",
    "399422 , 2007 .",
    "j.  marroquin , s.  mitter , and t.  poggio , `` probabilistic solution of ill - posed problems in computational vision , '' _ journal of the american statistical association _ , vol .",
    "397 , pp . 7689 , 1987 .",
    "s.  s. gross , o.  russakovsky , c.  b. do , and s.  batzoglou , `` training conditional random fields for maximum labelwise accuracy , '' in _ nips _ , 2007 .",
    "s.  kumar , j.  august , and m.  hebert , `` exploiting inference for approximate parameter learning in discriminative fields : an empirical study , '' in _ emmcvpr _ , 2005 .    p.  kohli and p.  torr , `` measuring uncertainty in graph cut solutions , '' _ computer vision and image understanding _",
    "112 , no .  1 ,",
    "pp . 3038 , 2008 .",
    "m.  wainwright and m.  jordan , `` graphical models , exponential families , and variational inference , '' _ found .",
    "trends mach .",
    "_ , vol .  1 , no . 1 - 2 , pp .",
    "1305 , 2008 .",
    "t.  meltzer , a.  globerson , and y.  weiss , `` convergent message passing algorithms - a unifying view , '' in _ uai _ , 2009 .",
    "s.  nowozin and c.  h. lampert , `` structured learning and prediction in computer vision , '' _ foundations and trends in computer graphics and vision _ , vol .  6 , pp .",
    "185365 , 2011 .",
    "h.  cramr , _ mathematical methods of statistics_.1em plus 0.5em minus 0.4emprinceton university press , 1999 .",
    "s.  nowozin , p.  v. gehler , and c.  h. lampert , `` on parameter learning in crf - based approaches to object class image segmentation , '' in _ eccv _ , 2010 .",
    "l.  stewart , x.  he , and r.  s. zemel , `` learning flexible features for conditional random fields , '' _ ieee trans .",
    "pattern anal .",
    "_ , vol .",
    "30 , no .  8 , pp .",
    "14151426 , 2008 .",
    "c.  geyer , `` markov chain monte carlo maximum likelihood , '' in _",
    "symposium on the interface _ , 1991 .",
    "m.  carreira - perpinan and g.  hinton , `` on contrastive divergence learning , '' in _ aistats _ , 2005 .",
    "s.  roth and m.  j. black , `` fields of experts , '' _ international journal of computer vision _ , vol .",
    "82 , no .  2 ,",
    "pp . 205229 , 2009 .",
    "m.  j. wainwright , `` estimating the `` wrong '' graphical model : benefits in the computation - limited setting , '' _ journal of machine learning research _ , vol .  7 , pp . 18291859 , 2006 .",
    "j.  j. weinman , l.  c. tran , and c.  j. pal , `` efficiently learning random fields for stereo vision with sparse message passing , '' in _ eccv _ , 2008 , pp .",
    "617630 .",
    "t.  toyoda and o.  hasegawa , `` random field model for integration of local information and global information , '' _ ieee trans .",
    "pattern anal .",
    "_ , vol .  30 , no .  8 , pp . 14831489 , 2008 .    a.  levin and y.  weiss , `` learning to combine bottom - up and top - down segmentation , '' _ international journal of computer vision _",
    "81 , no .  1 ,",
    "pp . 105118 , 2009 .",
    "s.  kumar , j.  august , and m.  hebert , `` exploiting inference for approximate parameter learning in discriminative fields : an empirical study , '' in _ emmcvpr _ , 2005 .",
    "x.  ren , c.  fowlkes , and j.  malik , `` figure / ground assignment in natural images , '' in _ eccv _ ,",
    "s.  v.  n. vishwanathan , n.  n. schraudolph , m.  w. schmidt , and k.  p. murphy , `` accelerated training of conditional random fields with stochastic gradient methods , '' in _ icml _ , 2006 .",
    "x.  ren , c.  fowlkes , and j.  malik , `` learning probabilistic models for contour completion in natural images , '' _ international journal of computer vision _ ,",
    "77 , no . 1 - 3 ,",
    "pp . 4763 , 2008 .    j.  yuan , j.  li , and b.  zhang , `` scene understanding with discriminative structured prediction , '' in _ cvpr _ , 2008 .",
    "j.  j. verbeek and b.  triggs , `` scene segmentation with crfs learned from partially labeled images , '' in _ nips _",
    ", 2007 .",
    "d.  scharstein and c.  pal , `` learning conditional random fields for stereo , '' in _ cvpr _ , 2007 .",
    "zhong and r.  wang , `` using combination of statistical models and multilevel structural information for detecting urban areas from a single gray - level image , '' _",
    "ieee t. geoscience and remote sensing _ , vol .",
    "45 , no . 5 - 2 ,",
    "pp . 14691482 , 2007 .",
    "j.  besag , `` statistical analysis of non - lattice data , '' _ journal of the royal statistical society .",
    "series d ( the statistician ) _ , vol .",
    "24 , no .  3 , pp .",
    "179195 , 1975 .",
    "x.  he , r.  s. zemel , and m.   .",
    "carreira - perpin , `` multiscale conditional random fields for image labeling , '' in _ cvpr _ , 2004 .    s.  kumar and m.  hebert , `` discriminative random fields , '' _ international journal of computer vision _ ,",
    "68 , no .  2 ,",
    "179201 , 2006 .",
    "s.  c. zhu and x.  liu , `` learning in gibbsian fields : how accurate and how fast can it be ? '' _ ieee transactions on pattern analysis and machine intelligence _",
    ", vol .  24 , pp .",
    "10011006 , 2002 .",
    "c.  sutton and a.  mccallum , `` piecewise training for undirected models , '' in _ uai _ , 2005 .",
    "s.  kim and i .- s .",
    "kweon , `` robust model - based scene interpretation by multilayered context information , '' _ computer vision and image understanding _ , vol .",
    "105 , no .  3 , pp . 167187 , 2007 .",
    "j.  shotton , j.  m. winn , c.  rother , and a.  criminisi , `` textonboost for image understanding : multi - class object recognition and segmentation by jointly modeling texture , layout , and context , '' _ int .",
    "j. of comput .",
    "81 , no .  1 ,",
    "pp . 223 , 2009 .",
    "v.  stoyanov , a.  ropson , and j.  eisner , `` empirical risk minimization of graphical model parameters given approximate inference , decoding , and model structure , '' in _ aistats _ , 2011 .",
    "j.  domke , `` learning convex inference of marginals , '' in _ uai _ , 2008 .",
    "l.  r. bahl , p.  f. bron , p.  v. de  souza , and r.  l. mercer , `` a new algorithm for the estimation of hidden markov model parameters , '' in _ icassp _ , 1988 .",
    "s.  kakade , y.  w. teh , and s.  roweis , `` an alternate objective function for markovian fields , '' in _ icml _ , 2002 .",
    "b.  g. lindsay , `` composite likelihood methods , '' _ contemporary mathematics _ , vol .",
    "221239 , 1988 .",
    "j.  domke , `` learning convex inference of marginals , '' in _ uai _ , 2008 .    c.  desai , d.  ramanan , and c.  c. fowlkes , `` discriminative models for multi - class object layout , '' _ international journal of computer vision _ , vol .",
    "95 , no .  1 ,",
    "pp . 112 , 2011 .",
    "m.  szummer , p.  kohli , and d.  hoiem , `` learning crfs using graph cuts , '' in _ eccv _ , 2008 .",
    "j.  j. mcauley , t.  e. de  campos , g.  csurka , and f.  perronnin , `` hierarchical image - region labeling via structured learning , '' in _ bmvc _ , 2009 .",
    "w.  yang , b.  triggs , d.  dai , and g .- s .",
    "xia , `` scene segmentation via low - dimensional semantic representation and conditional random field , '' hal , tech .",
    "j.  domke , `` implicit differentiation by perturbation , '' in _ nips _ , 2010 .",
    "a.  boresi and k.  chong , _ approximate solution methods in engineering mechanics_.1em plus 0.5em minus 0.4emelsevier science inc . , 1991 .",
    "n.  andrei , `` accelerated conjugate gradient algorithm with finite difference hessian / vector product approximation for unconstrained optimization , '' _ j. comput .",
    "230 , no .  2 ,",
    "pp . 570582 , 2009 .",
    "j.  nocedal and s.  j. wright , _ numerical optimization _ , 2nd  ed.1em plus 0.5em minus",
    "0.4emspringer , 2006 .",
    "m.  welling and y.  w. teh , `` linear response algorithms for approximate inference in graphical models , '' _ neural computation _ , vol .",
    "16 , pp . 197221 , 2004 .",
    "j.  domke , `` parameter learning with truncated message - passing , '' in _ cvpr _ , 2011 .",
    "v.  stoyanov and j.  eisner , `` minimum - risk training of approximate crf - based nlp systems , '' in _ proceedings of naacl - hlt_.    f.  eaton and z.  ghahramani , `` choosing a variable to clamp , '' in _ aistats _ , 2009 .",
    "g.  konidaris , s.  osentoski , and p.  thomas , `` value function approximation in reinforcement leanring using the fourier basis , '' in _ aaai _ , 2011 .",
    "n.  dalal and b.  triggs , `` histograms of oriented gradients for human detection , '' in _ cvpr _ , 2005 .",
    "justin domke obtained a phd degree in computer science from the university of maryland , college park in 2009 . from 2009 to 2012 , he was an assistant professor at rochester institute of technology . since 2012 , he is a member of the machine learning group at nicta .",
    "[ exact variational principle]the log - partition function can also be represented as    @xmath259 where @xmath62 is the marginal polytope , and @xmath63    is the entropy .",
    "[ proof of the exact variational principle]as @xmath158 is convex , we have that @xmath260 where @xmath261 is the conjugate dual .    now , since @xmath262 , if @xmath263 then the infimum for @xmath264 is unbounded above . for @xmath265",
    ", the infimum will be obtained at @xmath59 .",
    "thus @xmath266    now , for @xmath267 we can see by substitution that    @xmath268    and so , finally , @xmath269    which is equivalent to the desired result .",
    "[ mean field updates]a local maximum of eq .",
    "[ eq : mu - trw ] can be reached by iterating the updates    @xmath79    where @xmath11 is a normalizing factor ensuring that @xmath270 .    [",
    "proof of mean field updates]the first thing to note is that for @xmath271 several simplifying results hold , which are easy to verify , namely    @xmath272    now , let @xmath58 denote the approximate partition function that results from solving eq .",
    "[ eq : a - variational ] with the marginal polytope replaced by @xmath67 . by substitution of previous results",
    ", we can see that this reduces to an optimization over univariate marginals only .",
    "@xmath273    now , form a lagrangian , enforcing that @xmath274    @xmath275    setting @xmath276 , solving for @xmath76 , we obtain @xmath277    meaning this is a local minimum .",
    "normalizing by @xmath11 gives the result .",
    "note that only a local maximum results since the mean - field objective is non - concave ( * ? ? ?",
    "two preliminary results are needed to prove the trw entropy bound .",
    "let @xmath278 be the `` projection '' of @xmath42 onto a subgraph @xmath279 , defined by @xmath280    then , for @xmath267 @xmath281    first , note that , by eq .",
    "[ eq : astar_equals_h-1 ] , for @xmath267 @xmath282    now , the entropy of @xmath278 could be defined as an infimum only over the parameters @xmath30 corresponding to the cliques in @xmath279 .",
    "equivalently , however , we can define it as a constrained optimization @xmath283    since the infimum for @xmath284 takes place over a more constrained set , but @xmath42 and @xmath278 are identical on all the components where @xmath30 may be nonzero , we have the result .",
    "our next result is that the approximate entropy considered in eq .",
    "[ eq : trw - entropy ] is exact for tree - structured graphs , when @xmath102 .",
    "for @xmath265 for a marginal polytope @xmath64 corresponding to a tree - structured graph , @xmath285    first , note that for any any tree structured distribution can be factored as @xmath286    ( this is easily shown by induction . ) now , recall our definition of @xmath287 : @xmath63    substituting the tree - factorized version of @xmath288 into the equation yields @xmath289    finally , combining these two lemmas , we can show the main result , that the trw entropy is an upper bound .",
    "[ trw entropy bound]let @xmath94 be a distribution over tree structured graphs , and define @xmath95 then , with @xmath96 as defined in eq .",
    "[ eq : trw - entropy ] , @xmath97    the previous lemma shows that for any specific tree @xmath279 , @xmath281    thus , it follows that @xmath290    [ trw updates]let @xmath93 be as in the previous theorem .",
    "then , if the updates in eq",
    ". [ eq : trw - msgs ] reach a fixed point , the marginals defined by @xmath101    constitute the global optimum of eq .",
    "[ eq : a - trw ] .",
    "the trw optimization is defined by @xmath291    consider the equivalent optimization    @xmath292    which makes the constraints of the local polytope explicit    first , we form a lagrangian , and consider derivatives with respect to @xmath42 , for fixed lagrange multipliers .",
    "@xmath293    setting these derivatives equal to zero , we can solve for the log - marginals in terms of the lagrange multipliers :    @xmath294    now , at a solution , we must have that @xmath295 .",
    "this leads first to the the constraint that    @xmath296    now , define the `` messages '' in terms of the lagrange multipliers as @xmath297    if the appropriate values of the messages were known , then we could solve for the clique - wise marginals as @xmath298 the univiariate marginals are available simply as    @xmath299    we may now derive the actual propagation . at a valid solution , the lagrange multipliers ( and hence the messages ) must be selected so that the constraints are satisfied . in particular , we must have that @xmath300 from the constraint , we can derive constraints on neighboring sets of messages .",
    "@xmath301    now , the left hand side of this equation cancels one term from the product on line [ eq : trw_beforecancel ] , except for the denominator of @xmath302 this leads to the constraint of    @xmath303    this is exactly the equation used as a fixed - point equation in the trw algorithm .",
    "suppose that @xmath304 define @xmath147 then , letting @xmath148 @xmath149    first , recall the implicit differentiation theorem .",
    "if the relationship between @xmath305 and @xmath306 is implicitly determined by @xmath307 , then    @xmath308    in our case , given the lagrangian    @xmath309    our implicit function is determined by the constraints that @xmath310 and @xmath311 .",
    "that is , it must be true that    @xmath312    @xmath313    thus , our implicit function is    @xmath314\\right)=\\left[\\begin{array}{c } \\theta+\\frac{dh}{d\\boldsymbol{\\mu}}+b^{t}\\boldsymbol{\\lambda}\\\\ b\\boldsymbol{\\mu}-{\\bf d } \\end{array}\\right]=\\left[\\begin{array}{c } { \\bf 0}\\\\ { \\bf 0 } \\end{array}\\right]\\ ] ]    taking derivatives , we have that @xmath315^{t}}{d\\boldsymbol{\\theta}}=-\\bigl(\\frac{d{\\bf f}^{t}}{d\\boldsymbol{\\theta}}\\bigr)\\bigl(\\frac{d{\\bf f}^{t}}{d\\left[\\begin{array}{c } \\boldsymbol{\\mu}\\\\ \\boldsymbol{\\lambda } \\end{array}\\right]}\\bigr)^{-1}\\ ] ]    taking the terms on the right hand side in turn , we have    @xmath316^{t}}{d\\boldsymbol{\\theta}}=\\left[\\begin{array}{c } i\\\\ 0 \\end{array}\\right]^{t}\\ ] ]    @xmath317}=\\left[\\begin{array}{cc } \\frac{d^{2}h}{d\\boldsymbol{\\mu}d\\boldsymbol{\\mu}^{t } } & b^{t}\\\\ b & 0 \\end{array}\\right]\\ ] ]    @xmath315^{t}}{d\\boldsymbol{\\theta}}=-\\left[\\begin{array}{c } i\\\\ 0 \\end{array}\\right]^{t}\\left[\\begin{array}{cc } \\frac{d^{2}h}{d\\boldsymbol{\\mu}d\\boldsymbol{\\mu}^{t } } & b^{t}\\\\ b & 0 \\end{array}\\right]^{-1}\\label{eq : du_lambda_dtheta}\\ ] ]    this means that @xmath318 is the upper - left block of the inverse of the matrix on the right hand side of eq .",
    "[ eq : du_lambda_dtheta ] .",
    "it is well known that if @xmath319,\\ ] ] then the upper - left block of @xmath320 is @xmath321    so , we have that @xmath322 where @xmath323 .",
    "the result follows simply from substituting eq .",
    "[ eq : du / dtheta ] into the chain rule @xmath324",
    "several simple lemmas will be useful below . a first one considers the case where we have a `` product of powers '' .",
    "[ proof sketch]the idea is just to mechanically differentiate each step of the algorithm , computing the derivative of the computed loss with respect to each intermediate quantity .",
    "first , note that we can re - write the main mean - field iteration as      now , suppose we have the derivative of the loss with respect to this intermediate vector of marginals @xmath340 .",
    "we wish to `` push back '' this derivative on the values affecting these marginals , namely @xmath341 , @xmath34 ( for all @xmath1 such that @xmath342 ) , and @xmath77 ( for all @xmath78 such that @xmath343 ) . to do this",
    ", we take two steps :                                [ proof sketch]again , the idea is just to mechanically differentiate each step of the algorithm .",
    "since the marginals are derived in terms of the messages , we must first take derivatives with respect to the marginal - producing steps .",
    "first , consider step 3 , where predicted clique marginals are computed . defining @xmath358 we have that @xmath359}{m_{d}(x_{i})}\\sum_{{\\bf x}_{c\\backslash i}}\\overleftarrow{\\nu}\\end{aligned}\\ ] ]                    finally , consider the update to @xmath373 , where @xmath369 .",
    "this will have the previous update , plus the additional term , considering the presence of @xmath374 in the denominator of the main trw update , of @xmath375      > m0.57in > m0.9in > m0.9in > m0.9 in & @xmath377 & @xmath378 & @xmath379 + input & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + surrogate likelihood & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + univariate logistic & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + clique logistic & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + sm .",
    "class @xmath380 & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + sm .",
    "class @xmath381 & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + sm .",
    "class @xmath382 & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + pseudo - likelihood & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + piecewise & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + inde - pendent & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] +    > m0.57in > m0.9in > m0.9in > m0.9 in & @xmath377 & @xmath378 & @xmath379 + input & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + surrogate likelihood & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + univariate logistic & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + clique logistic & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + sm .",
    "class @xmath380 & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + sm .",
    "class @xmath381 & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + sm .",
    "class @xmath382 & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + pseudo - likelihood & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + piecewise & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + inde - pendent & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] +    > m0.57in > m0.9in > m0.9in > m0.9 in & @xmath377 & @xmath378 & @xmath379 + input & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + surrogate likelihood & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + univariate logistic & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + clique logistic & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + sm .",
    "class @xmath380 & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + sm .",
    "class @xmath381 & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + sm .",
    "class @xmath382 & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + pseudo - likelihood & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + piecewise & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] + inde - pendent & .,title=\"fig : \" ] & .,title=\"fig : \" ] & .,title=\"fig : \" ] +"
  ],
  "abstract_text": [
    "<S> likelihood based - learning of graphical models faces challenges of computational - complexity and robustness to model mis - specification . </S>",
    "<S> this paper studies methods that fit parameters directly to maximize a measure of the accuracy of predicted marginals , taking into account both model and inference approximations at training time . </S>",
    "<S> experiments on imaging problems suggest marginalization - based learning performs better than likelihood - based approximations on difficult problems where the model being fit is approximate in nature .    </S>",
    "<S> graphical models , conditional random fields , machine learning , inference , segmentation . </S>"
  ]
}