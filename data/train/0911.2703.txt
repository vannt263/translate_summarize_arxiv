{
  "article_text": [
    "gravitational microlensing has opened a new window on the study of extrasolar planets as it is the only method that is currently able to detect low - mass planets in orbits beyond @xmath2au .",
    "in fact , six of the ten published microlensing planet discoveries have been of planets of less than saturn s mass @xcite and two of these have masses below @xmath3 @xcite .",
    "the range of planetary separations probed by microlensing is particularly relevant to tests of the core accretion model for planet formation , as microlensing is particularly sensitive to planets just beyond the  snow - line \" @xcite where core accretion predicts that the most massive planets should form .",
    "seven of these ten published planetary microlensing events @xcite have been found in high - magnification events , which although rare , have a much higher planet detection probability @xcite .",
    "a corollary of this is that high - magnification events have significant sensitivity to events with signals from multiple planets @xcite and to planets in stellar binary systems .",
    "this point was demonstrated with the discovery of the jupiter - saturn analog system ogle-2006-blg-109lb , c @xcite .",
    "furthermore , this event also demonstrated that microlensing can detect the orbital motion of a planet when the caustic structures are sufficiently large as can occur for a massive planet @xcite or a  resonant \" caustic .",
    "( when a planet is close to the einstein ring , the planetary and central caustics merge to form a  resonant \" caustic . )",
    "this sensitivity to orbital motion and to systems with more than two lens masses makes the modeling of these microlensing events significantly more challenging than other events .",
    "in fact , there is currently a backlog of planetary microlensing events that appear to have three or more lens masses .",
    "there are three such events discovered through the end of the 2008 galactic bulge observing season , but the analysis is complete for only one of these @xcite . in contrast , the analysis is complete for 8 of the 9 planetary microlensing events discovered through the end of 2008 that can be modeled with a single planet and host star .    in this paper",
    ", we present a general method for light curve modeling that is designed to be able to model the most complicated and difficult high - magnification microlensing events .",
    "this method has gradually evolved from the first general method for calculating finite source light curves for binary lens events @xcite .",
    "versions of this modeling method have been used to analyze possible planetary events observed in the 1990 s @xcite , to make the first successful real - time predictions of caustic crossings toward the galactic bulge and magellanic clouds @xcite , and to analyze binary lensing events seen by the macho collaboration @xcite .",
    "more recent versions of this code have been used to model all of the known planetary microlensing events , and this code has played a major role in modeling 7 of the 10 published planetary microlensing signals @xcite .",
    "this method has several unique features . in section  [ sec - int ] , i present a numerical integration scheme with two new features that improve its precision by more than a factor of 100 ( for a fixed number of integration grid points ) .",
    "the first feature is an integration scheme that is specifically designed to handle the singular derivative at the limb of a limb darkened source profile .",
    "this method also features a polar coordinate integration grid with a much larger grid spacing in the angular than in the radial direction to take advantage of the lensing distortion of the images . in section  [ sec - lc_calc ] , i present tests of this method using a number of previously analyzed planetary microlensing events .",
    "these tests confirm the dramatic improvement in precision for high - magnification events .",
    "next , in section  [ sec - markov ] , i present an adaptive @xmath1 minimization recipe based on the @xcite algorithm that is designed to rapidly descend to a minimum of a complicated @xmath1 surface .",
    "this method , along with the integration scheme described in section  [ sec - int ] , was critical for the analysis of the one double - planet microlensing event that has been successfully modeled @xcite .",
    "this event was unusually time consuming to model , due to the important effect of the orbital motion of one of the planets , but the optimizations described in section  [ sec - int ] and [ sec - markov ] made the modeling of this event tractable .",
    "a global fit strategy , designed to find all the competitive @xmath1 minima for a microlensing event is presented in section  [ sec - global ] , and in section  [ sec - global - ex ] , i present examples of this method for a number of single - planet events .",
    "finally , in section  [ sec - conclude ] , i discuss ways in which this method can be improved and reach some conclusions .",
    "gravitational lensing by stars and planets can be approximated to extremely high accuracy by the lens equation for point - masses , @xmath4 where @xmath5 and @xmath6 are the complex positions of the source and image , respectively , and @xmath7 are the complex positions of the lens masses .",
    "this equation uses dimensionless coordinates , normalized to the einstein ring radius of the total lens system mass .",
    "the individual lens masses are represented by @xmath8 , which is the mass fraction of the @xmath9th lens mass , so that @xmath10 .",
    "if we assume a point source , then we can derive a formula for the lensing magnification from the jacobian determinant of the lens equation ( and its complex conjugate ) : @xmath11 where @xmath12 because eq .",
    "[ eq - j ] gives the jacobian determinant of the inverse mapping from the image plane to the source plane , the magnification of each image is given by @xmath13 evaluated at the position of each image . in order to use eq .",
    "[ eq - aj ] to determine the magnification , we must solve the lens equation , [ eq - mult_lens ] . for a lens with two point masses , eq .",
    "[ eq - mult_lens ] can be inverted to yield a fifth order complex polynomial equation @xcite .",
    "the image locations for a given source position are roots of this polynomial , but there are either three or five solutions that correspond to physical image positions .",
    "these polynomial roots can be found with efficient numerical methods ( e.g. @xcite ) , and this provides a very quick calculation of binary microlensing light curves for point sources @xcite . the triple lens version of eq .",
    "[ eq - mult_lens ] can be inverted to yield a tenth order polynomial equation , which corresponds to 4 , 6 , 8 , or 10 physical image solutions @xcite . in general , the lens equation , eq .  [ eq - mult_lens ] , for @xmath14 point masses has a minimum of @xmath15 images and a maximum of @xmath16 images @xcite .",
    "the earliest investigations of the two - point mass lens system light curves have used this point source approximation @xcite . and these point - source solutions are an important aspect of the @xcite method for calculating multiple - lens light curves , which i further develop in this paper . the triple lens solution to eq .",
    "[ eq - mult_lens ] was critical for the modeling of the first double - planet microlensing event @xcite .",
    "however , for systems with extreme mass ratios , such as lens systems with masses similar to the sun , earth , and moon , @xcite standard double precision ( 64-bit ) arithmetic is insufficient to solve the tenth order polynomial .",
    "so , it may be necessary to resort to quadruple precision ( 128-bit ) calculations , which can be up to 100 times slower than double precision in some compiler implementations .",
    "extensions of the point - source approximation have been provided by @xcite and @xcite , who have developed a power series corrections to the point - source approximation .",
    "the @xcite analysis goes to out to the hexadecapole term .",
    "this approximation is valid much closer to the caustics of the multiple lens light curve than the point - source approximation .",
    "this can result in a dramatic improvement in computation time for microlensing events with cusp - approaches , but no caustic crossings , such as ogle-2005-blg-71 @xcite .",
    "events with a source trajectory that runs parallel to a caustic for a long period of time can also see a significant improvement .",
    "but for most events with caustic crossings , the expected improvement in computational time is much more modest - probably not exceeding a factor of two @xcite .    the majority of the computational effort required to compute light curves for planetary microlensing events is devoted to the numerical integrations necessary for finite source calculations .",
    "the most obvious method would be to simply integrate the point - source magnification pattern over the disk of the source star .",
    "however , this method presents severe numerical difficulties due to the singularities in the point - source magnification profile .",
    "the point - source magnification pattern for a source crossing a fold caustic is @xmath17 where @xmath18 and @xmath19 are constants , and @xmath20 is the location of the fold caustic ( which is assumed to be parallel with the @xmath21-axis ) .",
    "so , the point - source magnification is ( formally ) infinite and discontinuous on the line - like caustic curve . in the caustic exterior , there are also pole singularities in the magnification , @xmath22 , in the vicinity of cusps",
    ".    these singularities can be avoided with the ray - shooting method , originally developed by @xcite . for complicated lens systems that consist of more than just a few point masses , solving for the image positions for a given source position can be difficult , or even intractable .",
    "but , if we start with the image positions , we can always use eq .",
    "[ eq - mult_lens ] to determine the source position from the image positions . by covering the image plane with light rays that are shot back towards the source it is possible to find all the images for a given source position .",
    "this is often referred to as the inverse ray - shooting method because without a solution to eq .",
    "[ eq - mult_lens ] , it is only possible to do the lens mapping in the inverse direction : from image to source .    for binary and triple lens systems ,",
    "it is straight forward to solve the lens equation numerically , so it is not necessary to shoot the rays in the inverse direction . instead , the advantage of ray shooting is that the integrands involved in the lens magnification calculations are less singular .",
    "the lens images have a surface brightness equal to the surface brightness of the source , so we can integrate in the image plane and avoid the strong singularities associated with caustic and cusp crossings in the source plane .",
    "this leads to the basic strategy developed by @xcite for the calculation of multiple lens light curves .",
    "the lens equation is solved to locate the images for a point - source located at the source center . if these images are sufficiently far from the critical curves and the source sufficiently far from the caustics , then the point - source approximation is used .",
    "( in most cases , the point source approximation is used if these separations are greater than the 7 source radii times the point source magnification . ) when the point - source approximation can not be used , we build integration grids in the image plane to cover each of the images to be integrated over .",
    "the image grids are increased until their boundaries are completely comprised of points that do not map onto the source .",
    "some care in bookkeeping is required to ensure that images are not double counted as some grids can grow to include more than one image .",
    "there will be times when the source limb has crossed a caustic , while the source center remains on the exterior . to include these partial lensed images we must also build integration grids at the critical curve locations corresponding to the caustic points that overlap the source .",
    "these finite source calculations become quite time consuming for high - magnification events due to the nature of the high - magnification images .",
    "as the lens and source approach perfect alignment , we approach the einstein ring situation , and the images become large circular arcs with a length : thickness ratio that is approximately equal to the total magnification , @xmath23 , which is typically in the range @xmath24 . as i shall discuss below",
    ", it is these long , thin images that are very time consuming to integrate over , and so many of the features of my numerical integration scheme are designed to make these integrals more efficient .    for static lens systems ( where the orbital motion of the lens system is not important ) the image positions at different times in the light curve will often overlap ,",
    "so we will often have to invoke the lens equation , eq .",
    "[ eq - mult_lens ] , many times at the same location .",
    "so , to minimize the number of lens equation calculations for high - magnification events , we store the lens equation solutions on a grid centered on the einstein ring .",
    "( this feature is common to a number of other methods that use a version of ray - shooting @xcite . )",
    "this image centered ray shooting light curve calculation strategy @xcite was the first method that was able to give precise calculations of planetary microlensing light curves including realistic finite source effects .",
    "several potentially promising alternative approaches have been suggested @xcite .",
    "several of these focus on finding solutions with fixed lens mass ratios and relative positions @xcite in order to map out @xmath1 as a function of these parameters .",
    "this is often used for an initial search for solutions , but it is not very efficient when these parameters are not fixed .",
    "furthermore , the number of parameters that must be fixed increases from two to five when a triple lens system is considered , and the brute force method of mapping out the @xmath1 surface seems much less attractive if it must be done in five dimensions .",
    "the methods of @xcite , @xcite , and the  loop - linking \" method of @xcite are somewhat more flexible .",
    "both @xcite and @xcite invoke a stokes theorem approach that is much more efficient for sources that are not limb darkened .",
    "of course , no limb - darkening is an unphysical approximation , but in many cases , the effect of limb darkening on microlensing light curves is relatively small .",
    "thus , it might be sensible to develop a fast code based on the @xcite method to search for approximate solutions without limb darkening .",
    "however , the errors due to the lack of limb darkening could be more serious in events like ogle-2005-blg-390 @xcite and moa-2007-blg-400 @xcite where planetary signal comes from a caustic curve that is much smaller than the source .",
    "the efficiency advantage of the stokes theorem approach is lost when limb darkening is included , but this method may still be competitive . the basic method of @xcite is not tied to specific fixed parameters and does not provoke these limb - darkening concerns , and therefore it seems sensible to continue with this basic approach .      in order to understand how to write a numerical integration scheme for gravitational lensing light curves , let us first consider the simpler question of one - dimensional numerical integration . there has been a lot of work in this field , and there are a number of numerical integration schemes that can give quite precise results for a small number of integration grid points if the integrand is smooth . a good discussion of these methods is given in @xcite , and here i reproduce the relevant points .    for most numerical integration problems ,",
    "the key to an efficient evaluation of the integrals , is to obtain high accuracy with as few evaluations of the integrand function as possible .",
    "this is often accomplished by invoking a higher order integration scheme , which means that the error can be expected to scale as a high power of the integration grid spacing , @xmath25 . of course , high order is no guarantee of high accuracy .",
    "a high order scheme can have a large coefficient in front of the error term that can render it less accurate than a lower order scheme at a given grid spacing .",
    "furthermore , in our case , we are considering two dimensional integrals , so correlations between the numerical errors in different rows of one dimensional integration can have a significant effect on the overall accuracy of the integral .",
    "that is , there might be correlations that tend to make the numerical errors in different rows add coherently , instead of incoherently so that the relative error would fall as the square root of the number of integration rows . as a result",
    ", it is quite difficult to predict the accuracy of a numerical integration scheme with analytic arguments like the ones presented in this section .",
    "so , as is usually the case with numerical calculations , it will be the numerical tests of the method that will show which methods are most precise .",
    "the integrands that we are concerned with are not very smooth , so i restrict the discussion to 2nd order integration schemes . for most numerical integration problems ,",
    "there are two basic building blocks for the 2nd order integration schemes , the trapezoidal rule , @xmath26 and the mid - point rule , @xmath27 these are both formulae for evaluating integrals over a single grid spacing , @xmath25 , using values of the function calculated at integer multiples of the grid spacing .",
    "the function values are @xmath28 .",
    "the error term @xmath29 indicates that the true answer differs from the estimate by an amount that is the product of some numerical coefficient times @xmath30 times the value of the second derivative of the function somewhere in the range of integration .",
    "now , these building block formulae can be strung together to build extended formulae that can be used over finite intervals .",
    "this yields the extended trapezoidal rule , @xmath31 and the extended mid - point rule , @xmath32 usually , the extended mid - point rule ( eq .  [ eq - midp_ext ] ) is presented using integrand values evaluated at half integer grid points , but we have offset this grid by half a grid spacing so that most of the grid points coincide with those of the extended trapezoidal rule ( eq .  [ eq - trap_ext ] ) . when written this way , the extended mid - point rule , the extended trapezoidal rule and the new integration formula presented below will require that the integrand only be evaluated at integer grid points in the interior .",
    "this makes it clear that the only difference between these integration formulae the treatment of the boundary .",
    "one might imagine that we could implement something like eq .",
    "[ eq - trap_ext ] or [ eq - midp_ext ] in two dimensions give an integration scheme with a precision proportional to the inverse square of the total number of grid points .",
    "however , there are two difficulties with this procedure .",
    "first is the fact that our problem is slightly different from the normal numerical integration problem , because we calculate the @xmath33 values before we know where the boundary is .",
    "thus , it is impossible for us to arrange that the boundaries are located at integer or half - integer values of the grid spacing .",
    "if we are interested in an integration scheme that is only first order accurate , then we only need to determine which points on the grid are inside the image boundary without attempting to locate the boundary .",
    "however , for a second order method , we do need to locate the boundary to a precision much greater than the grid spacing .",
    "i solve for the position of the boundary using the brent s method @xcite to find the boundary to a precision of @xmath34 , which @xmath25 is the grid spacing .",
    "this increases the number of lens equation ( eq .  [ eq - mult_lens ] ) calculations that must be done per row by a factor of less than 1.5 .",
    "the second complication is that our integrands are not very smooth .",
    "moving the finite source integration from the source plane to the image plane removes the singularities from the integrand , but for limb darkened sources , there are still singularities in the derivatives of the surface brightness in the image plane , and these will limit our attempt to do these lens magnification integrals efficiently .    the linear limb darkening law of @xcite gives a reasonable approximation to the limb darkening for most stars .",
    "this linear law is given by @xmath35 \\",
    ", \\label{eq - limbd}\\ ] ] where @xmath36 is the central intensity , @xmath37 is the linear limb darkening coefficient , and @xmath38 is the distance from the center toward the limb of the star at @xmath39 .",
    "this limb darkening law has a first derivative that diverges at @xmath39 . also , when the source crosses a caustic , the fraction of the stellar profile that is inside the caustic has two additional images that meet on a critical curve in the image plane .",
    "these images have opposite parity , and the first derivative of the surface brightness profile will have a discontinuity on the critical curve , so the second derivative will diverge .",
    "we should note that the linear limb darkening model is not a perfect match to model atmospheres , with an average difference of @xmath40% @xcite from kurucz s model atmospheres @xcite .",
    "this discrepancy can be reduced by going to more complicated limb darkening models @xcite .",
    "however , the microlensing light curves involve integrals over the limb darkened profiles , and these are generally much more accurate than the limb darkened profiles themselves .",
    "furthermore , there is no guarantee that these models are actually correct , so it is perhaps more sensible to compare with previous well sampled microlensing events that yielded high precision limb darkening measurements .",
    "the first such example is event macho 95-blg-30 @xcite which is the first example of an event detected in progress which exhibited finite source effects .",
    "they employ a  quadratic \" limb darkening model in place of eq .",
    "[ eq - limbd ] , but due to sparse sampling , the @xmath1 improvement with respect to a model without limb darkening was only @xmath41 , so it is likely that the improvement over the linear model is quite small .",
    "the high cadence follow - up observations of the planet collaboration yielded a number of binary lens caustic crossing events with a much stronger limb darkening signal .",
    "for example , planet found that limb darkening improved the fit for the binary microlensing event macho-97-blg-28 @xcite by @xmath42 .",
    "however , the additional improvement with the  square - root \" limb darkening model was only @xmath43 , which they argue is not statistically significant .",
    "one of the most spectacular binary microlensing events ever observed was event eros-2000-blg-5 @xcite , which had a very extended caustic crossing with a duration of @xmath44days followed by a cusp approach to within @xmath45 source radii from the stellar limb four days later .",
    "these features were measured with hundreds of photometric measurements while the source was magnified to a brightness ranging from @xmath46 to @xmath47 using several @xmath2 m class telescopes .",
    "it is difficult to imagine circumstances that would allow a higher s / n measurement of limb darkening effects .",
    "however , the non - linear limb darkening parameters are found to be @xmath48 away from 0 .",
    "so , this event does not yield a significant measurement of limb darkening parameters beyond the linear term .",
    "similar results were also obtained for the well sampled single lens event ogle-2004-blg-254 @xcite .",
    "it is perhaps somewhat more instructive to consider high - magnification events . since these are the events that are the focus of the method presented in this paper .",
    "such events also show only weak evidence that non - linear limb darkening models improve the fits .",
    "@xcite find marginal evidence for terms beyond the linear term of eq .",
    "[ eq - limbd ] for the @xmath49 band in the moa-2002-blg-33 light curve , an event with a very strong caustic crossing at the peak .",
    "but , there was no evidence for a term beyond the linear one in the @xmath50 band data , which dominate the light curve coverage for most events .",
    "similarly , @xcite and @xcite find only a modest improvement @xmath1 for non - linear limb darkening models planetary events with geometries that should maximize limb darkening effects .",
    "more importantly , the more complicated limb darkening models have no significant influence on the non - limb darkening parameters , so there is little reason to consider limb darkening models more complicated than eq .  [ eq - limbd ] unless we are specifically interested in the limb darkening parameters .    for high - magnification events , it is the divergent first derivative at the limb that is , by far , the most serious problem .",
    "the images are highly extended parallel to the einstein ring and compressed by a factor of about two in the radial direction , so they have a very large ratio of boundary to area .",
    "thus , errors at the boundary make a large contribution to the total error in the integral .",
    "the caustic crossing features , on the other hand , are less singular than the limb darkening profile , and they also are also ( usually ) much shorter than the entire extent of the limb in the image plane .    while eq .",
    "[ eq - limbd ] describes the limb darkening in the source plane , the integrals are carried out in the image plane where the source brightness profile is distorted by the gravitational lens .",
    "however , the lowest order behavior near the limb is generically described by @xmath51 where @xmath52 and @xmath53 are constants and @xmath54 is the distance from the limb of the distorted image .",
    "the only case where the @xmath55 behavior is removed by the lens distortion is when the stellar limb just touches the interior of a caustic .",
    "however , this will generally only occur at a single point of contact between the caustic and the limb , so the @xmath55 behavior is generic .",
    "the two building block formulae , eqs .",
    "[ eq - trap ] and [ eq - midp ] , are derived by requiring that they be exact for low order power laws ( as in a power series expansion of @xmath56 ) , and in the second order case , the formulae are exact for @xmath57 and @xmath58 .",
    "this fails for limb darkened profiles , because these can not be expressed as a power series in @xmath59 , where @xmath60 is the location of the limb .",
    "instead , the distorted limb darkening profile can be expressed as a power series in @xmath61 .",
    "we can still demand that the our integration formula is exact for the two leading orders in the power series expansion of the integrand . in eqs .",
    "[ eq - trap ] and [ eq - midp ] , the first term in the power series that does not vanish scales as @xmath30 ( under the assumption that @xmath56 can be expanded in a power series in @xmath54 ) .",
    "however , with half - integer powers of @xmath25 in addition to integer powers , there are more terms in the power series expansion . as a result , the first non - vanishing term in eqs .",
    "[ eq - trap ] and [ eq - midp ] scales as @xmath62 instead of @xmath30 when @xmath63 has a limb darkened form like eq .",
    "[ eq - limbd ] .",
    "in order to cancel this @xmath62 error term , we will demand that our integration formula be exact for @xmath57 and @xmath64 , where @xmath60 is the location of the limb .",
    "this requirements lead us to replace eq .",
    "[ eq - midp ] by @xmath65 \\ , \\label{eq - intlimb}\\ ] ] where @xmath66 and @xmath67 .",
    "the @xmath68 in the numerator of eq .",
    "[ eq - intlimbb ] is somewhat worrisome because @xmath68 can become very small if the limb happens to come very close to a grid point .",
    "conceivably , this could lead to a situation , where the error grows very large , even if it is formally of high order .",
    "therefore , we introduce another parameter , @xmath69 , such that eq .",
    "[ eq - intlimb ] is only invoked for @xmath70 .",
    "when @xmath71 , we invoke a standard  second order \" method that will be converted to 1.5 order by the singular derivative at the limb .",
    "any combination of @xmath72 will satisfy this criteria as long as @xmath73 . experimentation with different @xmath37 and @xmath74 values",
    "indicates that @xmath75 and @xmath76 is a good choice , so it is used below .",
    "we will investigate the effect of this @xmath69 parameter in section  [ sec - lc_calc ] .",
    "we can now write an extended numerical integration rule to take the place of eq .",
    "[ eq - midp_ext ] , @xmath77 where @xmath78 and @xmath79 refer to the the stellar limbs at each limit of the @xmath54 coordinate integral , and the @xmath80 and @xmath81 coefficients are given by @xmath82 where @xmath83 is the heavyside step function and @xmath84 and @xmath85 refer to @xmath68 and @xmath86 for the each of the two stellar limbs ( at @xmath87 , 2 ) on the image being integrated .",
    "if we set @xmath88 , then eq .  [ eq - int_rule ] is accurate to second order , even though eq .",
    "[ eq - intlimb ] has a non - vanishing @xmath89 error term .",
    "the reason for this is because we only invoke eq .",
    "[ eq - int_rule ] at the limbs , and we use eq .",
    "[ eq - midp ] for all the interior points . since the limb darkened profile does have a power series expansion in @xmath54 away from the limb , the error for eq .",
    "[ eq - midp ] does scale as @xmath30 in the interior ( except on a critical curve , where it has a @xmath90 contribution ) .",
    "thus , it is only the @xmath30 error terms that get a @xmath91 contribution from the sum .",
    "so , formally , eq .  [ eq - int_rule ] is second order accurate ( with @xmath88 ) , while eqs .",
    "[ eq - trap_ext ] and [ eq - midp_ext ] are only accurate to the three halves order for a limb darkened source . of course , with @xmath92",
    ", eq .  [ eq - int_rule ] also gains an error term that scales as @xmath62 , but as we shall see , in some cases , even with @xmath92 , eq .  [ eq - int_rule ] can yield second order accurate results . in all cases , eq .",
    "[ eq - int_rule ] with @xmath93 is substantially more accurate than the first order or @xmath94 calculations .",
    "it is possible to derive integration formulae that are more complicated than eqs  [ eq - int_rule ] and [ eq - ab ] that are formally 2nd order accurate without the problem of any of the coefficients growing unreasonably large for any position of the boundary with respect to the grid spacing .",
    "however , experimentation with a number of such integration formulae has not found any such integration scheme that gives results as accurate as the scheme represented by eqs  [ eq - int_rule ] and [ eq - ab ] .      in section  [ sec - int1 ] we developed a one - dimensional numerical integration rule , eq .",
    "[ eq - int_rule ] , which is designed to improve the accuracy of the integration of limb darkened source profiles .",
    "but , of course , we will need to do two dimensional integrals to determine microlensing magnifications .",
    "the integral in the second dimension is not subject to the divergent integrand derivative at the boundary , because this is removed by the integral in the first direction .",
    "( of course , the limb darkening has the same behavior in both directions .",
    "but the integral in the first direction is roughly proportion to the length of the row being integrated , and this generally goes to zero at the boundaries of the integral in the second direction . )",
    "but , we still must deal with the arbitrary location of the image boundary .",
    "i employ the following second order accurate formula for this integration @xmath95 \\ , \\label{eq - inty}\\ ] ] where @xmath96 and @xmath97 refers to the integral over the @xmath54 direction , which has a @xmath21 dependence that is not made explicit in eq .",
    "[ eq - int_rule ] . with eqs .",
    "[ eq - int_rule ] and [ eq - inty ] to handle the numerical integrations , we can now consider the coordinate system to use for the integrations . in this context , it is useful to consider the image geometry for high - magnification microlensing events . consider a typical high - magnification event with a magnification of @xmath98 .",
    "if there are no companion planets or stars , then there will be two lensed images .",
    "the major image will have the shape of a circular arc with a magnification of @xmath99 , and it will be located just outside the einstein ring",
    ". the minor image will be just inside the einstein ring on the opposite side of the lens from the major image , and its magnification will be @xmath100 .",
    "each image will be compressed in the radial direction by about a factor of two , so the images will have the form of long , skinny arcs with a length - to - width ratio of about 200 .",
    "thus , the limb darkening profile will vary 200 times more rapidly in the radial direction than in the angular direction .",
    "this strong distortion of the images suggests that a polar coordinate grid is most appropriate for our problem , and it seems likely that we will require a much larger grid spacing in the angular direction than in the radial direction .",
    "in fact , the 200:1 distortion of the images for our example would seem to suggest that a 200:1 grid spacing ratio might be appropriate .",
    "however , we must also consider the effect of the planetary lenses that are the primary motivation for observing high - magnification microlensing events .",
    "the planetary lenses will distort the single lens images , and if there are caustic crossings , new images will be produced that will not follow the einstein ring as closely as the images that are not significantly influenced by the planet .",
    "so , during the planetary deviations , this image stretching in the angular direction may not be quite as severe as in our example .",
    "however , it is this image stretching in the angular direction that is responsible for the high magnification of these events , so we should expect that the optimal integration grid should include some extension in the angular direction .",
    "following the discussion above , i have arrived at the following two - dimentional integration strategy .",
    "an integration grid is set up in polar coordinates ( @xmath101,@xmath102 ) with a larger grid spacing in the angular than in the radial direction . in section  [ sec - lc_calc ] , we study the effects of varying this axis ratio . the integration is done using using eq .  [ eq - int_rule ] in the radial direction with a fixed value of @xmath69 .",
    "the integrand is given by the value of the limb darkening profile at the integration point , times @xmath101 , to give the proper polar coordinate area element .",
    "however , the integration is done in the image plane , while the limb darkening is known in the source plane .",
    "thus , we must apply the lens equation , eq .",
    "[ eq - mult_lens ] , to determine the appropriate source plane point and the limb darkened surface brightness that corresponds to the integration grid point in the image plane .",
    "the dependence of the light curve calculation precision on the grid size , the angular vs.  radial grid spacing ratio , and on @xmath69 is investigated in section  [ sec - lc_calc ] .",
    "since we do nt know the extend of the images when we start to calculate their magnification , we require a scheme to build an integration grid that covers each image , or at least each image that requires a finite source calculation .",
    "it most efficient to have integration grids that do nt extend far beyond the images because rays are shot from the image plane to the source plane at every grid point .",
    "the method of @xcite was to build a rectangular grid centered on each point source image , and to add rows and columns to each integration grid until the rows and columns at the boundaries of each do not contain any grid points inside an image .",
    "with cartesian coordinates , this method is quite inefficient for high - magnification events , because the grid must cover almost the entire einstein ring disk to integrate over thin images arcs that spread out over much of the einstein ring .",
    "polar coordinates are much more efficient in this case .",
    "cartesian coordinates may be more efficient for low - magnification events .",
    "but the light curves of these events are less time consuming , so computational efficiency is less important .",
    "the complication of using different grid geometries for different events does not seem justified by the very minor improvement in efficiency for low magnification events with a cartesian grid .",
    "a somewhat more efficient method is to build the grid row - by - row , with each row extending just as far as the image does , an approach first implemented by @xcite . the grid is then extended row - by - row until we have a grid that is surrounded by a boundary of grid points that are outside of the image .",
    "our tests have shown that this method is typically about a factor of two faster than building grids that are  rectangular \" in polar coordinates , and it is this method that is used for our timing results presented in section  [ sec - global - ex ] . in both cases , one must check that no images are double - counted .",
    "in addition to building integration grids around at the position of the point source images ( when the point - source approximation can not be used ) , we must also ensure that images associated with caustic crossing are included when the center of the source is outside the caustic",
    ". for static lens systems , this is most efficiently done by simply calculating the caustic curve location and building a grid at the location of any caustic point that is not included in another integration grid . for lens systems with orbital motion , this method can become inefficient because the caustics move and must be recalculated at every time step . another method that can also be used is to calculate the number of images for source points on the boundary of the source .",
    "then , a grid can be built at the location of any new image near the image boundary .",
    "finally , for static lens systems it is possible to speed up the calculations by storing the source positions corresponding to image positions in an annulus centered on the einstein ring .",
    "this avoids the need to recalculate the lens equation , eq .",
    "[ eq - mult_lens ] , for the same image points for the magnification integration at different points on the light curve .",
    "this same optimization method is used more extensively in inverse ray - shooting @xcite and magnification map @xcite methods .      in section  [ sec - lc_calc ] , i will present the results of light curve calculation tests with different values of the grid spacing and different calculation parameters , but first , it will be helpful to consider how much light of light curve precision is needed for practical modeling calculations . in section  [ sec - int1 ] , we discussed the difference between the simple linear limb darkening models and more complicated models that do a better job of reproducing the limb darkening seen in stellar atmosphere calculations as well as a few microlensing events which have good light curve coverage at limb crossings .",
    "however , even these improved models have deviations from the stellar models that are a factor of a few smaller than the deviation from the linear model .",
    "thus , we might still expect light curve errors at the level of @xmath103% at the limb crossings if these improved limb darkening models , are used , compared to errors of perhaps @xmath104% at the limb crossing with the linear model .",
    "these errors of @xmath103 - 0.3% are comparable to the level of systematic photometry errors that we expect in the microlensing light curves .",
    "these systematic errors are expected to affect the entire light curve , instead of just the limb crossings , although they are also likely to have large correlations that may allow some light curve features , such as weak caustic crossings , to be measured with a precision of @xmath105% @xcite .    these arguments might suggest that there is no need to calculate the light curves to a precision better than 0.1% , but in fact , it is usually the case that higher precision is needed .",
    "the reason for this is that the @xcite algorithm that is generally used for modeling multiple lens light curves is able to optimize the numerical errors , so that they tend to minimize @xmath1 .",
    "if the light curves were calculated perfectly , the @xmath1 surface should usually be smooth over small distances in parameter space , and the main difficulty in finding the @xmath1 minima is to follow the steep and twisting valleys in @xmath1 to the local minimum .",
    "however , the numerical errors act to roughen the @xmath1 surface on extremely small scales .",
    "if the numerical calculation errors are similar to the photometric error bars at the light curve peak , then rms variation in @xmath1 would be similar to the number of data points at the peak , which varies between events , but can often be 50 or more .",
    "but with parameters chosen by a @xmath1 minimization scheme , we might expect variations several times larger than this . because of this , @xcite advocate that the numerical precision of the light curve calculations be less than one third of the size of the error bars .",
    "however , numerical errors that are this large can still cause some difficulty . since the modeling code tends to select parameters that allow the numerical error to minimize @xmath1 , the variation in the @xmath1 seen during a modeling run tends to be much larger than the rms value .",
    "therefore , i recommend that random numerical errors be kept at @xmath106 or at least ten times smaller than the smallest photometric error bars to avoid difficulties in locating local @xmath1 minima due to the roughness of the @xmath1 surface .",
    "in order to determine the optimum light curve calculation parameters , i compare light curves seven different sets of model parameters , which are based on models of observed events .",
    "three of these are relatively low magnification events , which are shown in figure  [ fig - lc_lo ] .",
    "these are ogle-2003-blg-235 , the first definitive planetary microlensing event @xcite , ogle-2005-blg-390 with a planet of @xmath107 @xcite , and moa-2007-blg-197 , which has a brown dwarf secondary ( cassan , et al . in preparation ) .",
    "the other four comparison events are high - magnification events , shown in figure  [ fig - lc_hi ] . these include both cusp approach and caustic crossing models for moa-2007-blg-192 @xcite , which includes a planet of @xmath108 , ogle-2005-blg-169 @xcite , and moa-2008-blg-310 @xcite . in both of these figures ,",
    "the red boxes indicate the regions of the light curves used in tests of light curve calculation precision .",
    "it is important only to use regions of the light curve where finite source calculations are done .",
    "otherwise , the comparison of different integration parameters will be diluted by regions where the point source approximation is used ( and the light curves are identical ) .",
    "note that these light curve calculation tests do not always use the published version of the data set for each event , so the resulting parameters will sometimes differ slightly from the published ones . in every case ,",
    "the values published in the discovery or follow - up analysis papers should be considered definitive .",
    "figure  [ fig - compare ] shows a comparison of the rms fractional precision , @xmath109 , as a function of the geometric mean grid spacing , in units of the source star radius , for the 3 low - magnification events in the top two panels and the 4 high - magnification events in the bottom four panels .",
    "( the geometric mean is the square root of the product of the angular and radial grid spacings . )",
    "the short - dashed curves have @xmath110 , so that treatment of the limb - darkening profile is avoided , and the long - dashed black curve is is a first order integration with no attempt to locate the image boundaries on sub - grid - spacing scales .",
    "the blue , green , and red curves have @xmath111 , 0.05 , and 0.15 , respectively .",
    "all the curves use a angular grid spacing 4 times larger than the radial spacing , except for the black - dashed curves , which uses equal grid spacings .",
    "( in discussions of the grid spacing , we refer to the ratio for the grids at the einstein ring radius . for the high - magnification events ,",
    "this is very nearly the exact ratio , since the images are quite close to the einstein ring , but for some low magnification events , like ogle-05 - 390 , the planetary deviation images are well outside the einstein ring ,",
    "so the actual grid spacing ratio is somewhat larger . )",
    "note that for the low magnification events , the blue and green curves are often hidden under the red curve .",
    "these comparisons are done with respect to calculations using @xmath112 and an extremely fine grid , with 800 grid points per source radius in both the radial and angular directions .",
    "i use the rms fractional deviation , @xmath109 , between these very high resolution calculations and the test calculations as the measure of the calculation precision .",
    "the maximum deviation is generally between 2 and @xmath113 , so there appears to be no significant non - gaussian tail in the error distribution .",
    "the curves in figure  [ fig - compare ] mostly have a similar slope , which is close to the @xmath62 slope that was predicted in section  [ sec - int1 ] .",
    "however , for the high - magnification events with @xmath112 , the rms precision scales as @xmath114 .",
    "this may seem slightly surprising because it is only in the @xmath115 limit , where eqs .",
    "[ eq - int_rule ] and [ eq - ab ] achieve second order accuracy .",
    "however , this analysis only applies to a single one - dimensional integral , and a full treatment of the accuracy of the two - dimensional integral must include a number of complications , such as correlations in the error terms for integrations over different rows of the two - dimensional domain of integration .",
    "also , it is always possible for the integration accuracy to scale as a higher power of @xmath25 than expected , because the coefficient of the leading order term could be so small that a sub - leading term will dominate over the interesting range of @xmath25 values .",
    "of course , this is more likely in situations , such as these limb - darkened source integrals , where the error terms are power laws in @xmath116 instead of in @xmath25 .",
    "i expect that this is what has happened for the high - magnification events with @xmath112 , which have a @xmath117 scaling despite the fact that the arguments presented in section  [ sec - int1 ] suggest that the scaling should be @xmath118 .",
    "thus , the arguments presented in section  [ sec - int1 ] and [ sec - int2d ] should be considered to be only qualitative , and the comparison with much higher resolution calculations should be considered to be the definitive measure of the numerical errors .",
    "the long - dashed black curves indicate that the first order calculations seem to do as well as , and often better than the short - dashed black curves , which represent an attempt at a second order correction without the limb darkening terms given in eqs .",
    "[ eq - int_rule ] and [ eq - ab ] .",
    "this might seem somewhat surprising , since the analysis in section  [ sec - int1 ] indicates that the one - dimensional integrals without the limb darkening correction should have errors that scale as @xmath62 , whereas one - dimensional first order integration methods have errors that scale as @xmath25 .",
    "@xcite have also noted a @xmath62 error scaling in calculations with their method , which is also first order . in appendix a.3 of this paper , they note the improvement from the @xmath119 error term of the first order one dimensional integral to the @xmath120 error term observed in the two dimensional integrals , and they attribute this factor of @xmath116 improvement in the fractional error to the @xmath121 poisson decrease expected if the errors in each row are uncorrelated ( where @xmath122 is the number of rows ) .",
    "this assumption of uncorrelated errors is plausible for a first order integration scheme , but it seems unlikely that a higher order scheme would also achieve this @xmath116 improvement when going from one to two dimensions .",
    "so , this might explain why the first order integration scheme has the same @xmath62 behavior as many of the attempted second order integration schemes .",
    "the first order schemes also have the minor advantage that they do nt require additional lens equation ( eq .  [ eq - mult_lens ] ) calculations to locate the boundaries on a scale smaller than the grid spacing , which implies a savings of a factor of up to 1.5 in computation time .",
    "the numerical errors for the ogle-05 - 390 calculation are significantly larger than for other events , but this is easily explained by the details of this event .",
    "it is the only event we consider that has a giant source star .",
    "in fact , the planetary caustic responsible for this planet detection @xcite has a diameter that is 4 - 5 times smaller than the radius of the source , so the caustic crossing regions of the images are sampled more coarsely than the other events for the same grid size to source radius ratio .    for the high - magnification events , there is also a clear improvement from increasing the ratio of the angular to radial grid size from 1 to 4 .",
    "this effect is demonstrated even more clearly by figure  [ fig - grid_ratio ] , which shows the effect of changing the grid size ratio from 1 to 4 to 16 .",
    "the high - magnification event calculations show a clear improvement from the larger ratio of grid sizes , whereas the lower magnification events show the opposite effect ( with the exception of ogle-03 - 235 ) .    in figure",
    "[ fig - improve ] , we show the factor by which @xmath109 is improved compared to the first order calculations with an angular : radial grid size ratio of 1 . in every case , the second order scheme with @xmath112 is the most accurate , although the improvement is modest for the low magnification events . (",
    "note that the blue and green curves for the low magnification events are often hidden under the red curve . )",
    "the low magnification events also benefit from having an angular : radial grid size ratio @xmath123 .",
    "however , there is a dramatic improvement for the high - magnification events .",
    "the improvement ranges from a factor of 10 to a factor of nearly 300 at some grid sizes .",
    "the calculations with @xmath112 and an angular : radial grid size ratio of 16 prove to be the most accurate for the high - magnification events .",
    "these calculations , represented by the cyan curves in figure  [ fig - improve ] , provide a factor of 100 - 300 improvement in precision over the first order calculation case .",
    "if we were to try to reproduce the factor of @xmath124 improvement seen at the mean grid size of 0.1 by simply decreasing the grid size of the @xmath125 , grid ratio @xmath123 calculations , we would have to drop the grid size by a factor of 22 .",
    "but since this is a 2-dimensional calculation , this means an increase in the number of calculations and hence the computing time by nearly a factor of 500 .",
    "so , in summary , it would seem that the new features that we have outlined have the potential to increase the computational efficiency of high - magnification event light curve calculations by a factor of several hundred .",
    "in section  [ sec - int ] , i developed an efficient method for the calculation of high - magnification planetary microlensing light curves , but we also require an efficient method to move through parameter space to find the @xmath1 minimum . due to sharp light curve",
    "features like caustic crossings and cusp approaches , the @xmath1 surface for microlensing event models is not smooth enough to use a method , like levenberg - marquardt , that make use of the assumed smoothness of the @xmath1 surface .",
    "instead , we must use a more robust method , similar to the markov chain monte carlo ( mcmc ) @xcite or the simulated annealing method @xcite , which are both based on the @xcite algorithm .",
    "the metropolis algorithm employs the boltzmann factor from statistical physics to decide whether or not to accept the next proposed step through parameter space .",
    "if the next proposed step reduces @xmath1 , ( i.e.  @xmath126 ) , then it is always accepted , but if @xmath127 , then it is accepted with probability , @xmath128 , where the parameter @xmath129 is referred to as the temperature , in analogy with statistical physics .",
    "ideally , we would like to have a scheme that can find the global @xmath1 minimum automatically without having the specify any particular initial condition .",
    "in fact , the simulated annealing method was developed to find the global minimum in situations where there are many local minima .",
    "the basic idea is to start the method with a high temperature , @xmath129 , in order to explore all of parameter space , and then to gradually decrease @xmath129 and allow the system to relax to the global @xmath1 minimum . while this method has been used to solve a number of difficult problems , it is not clear that it will work for planetary microlensing events .",
    "for some events , it might end up in a broad local minimum that is favored at high @xmath129 that is separated by a @xmath1 barrier from a narrow , but deeper global minimum . also , it is unclear how one would design a schedule for modifying the temperature that would ensure the efficient location of the global @xmath1 for the observed wide variety of planetary microlensing events . therefore , i do not attempt to use the metropolis algorithm to find the global @xmath1 minimum .",
    "a very important aspect of the metropolis algorithm is the choice of the jump function .",
    "the jump function starts from the current set of model parameters and selects a new set of model parameters to be used to calculate @xmath1 , which will yield a value of @xmath128 that will allow us to determine whether to move to this next set of model parameters .    as mentioned above , microlensing light curves are characterized by very sharp features due to caustic crossings and cusp approaches , so we expect that the @xmath1 surfaces we encounter will have steep valleys .",
    "for example , for a caustic crossing event , most directions in parameter space will cause the timing of a caustic crossing to change , which will induce a large change in @xmath1 if the photometric measurements sample the caustic very well .",
    "but there will also be some directions in parameter space that will leave the timing of the caustic crossing fixed .",
    "these will induce much smaller changes in @xmath1 , so these will be the directions of the valleys in @xmath1 space .",
    "an efficient method of locating and exploiting these @xmath1 valleys has been presented by @xcite .",
    "this involves calculating the correlation matrix , @xmath130 of the last @xmath122 ( accepted ) sets of parameters , @xmath131 .",
    "@xmath132 is then diagonalized , and the diagonalized basis vectors can be considered to be a new set of parameters that are uncorrelated over these @xmath122 steps through parameter space .",
    "we then select new parameters at random from the most recently accepted parameter set with a gaussian variance normalized by the elements of the diagonalized covariance matrix .",
    "of course , this scheme can not be used until after @xmath122 steps have been accepted , so jump function scheme is needed for the first @xmath122 steps . for these initial steps ,",
    "i specify initial uncertainty ranges for each parameter , and select new parameters with uniform probability within these ranges .",
    "the use of the metropolis algorithm in the way i have described is often referred to as a markov chain monte carlo ( mcmc ) , with each accepted step considered a link in the chain .",
    "these mcmc runs can be used to estimate the parameter uncertainties , but this requires that the jump function be fixed during the mcmc run .",
    "but , such a strategy is not efficient when searching for a @xmath1 minimum , because the @xmath1 valleys often have many twists and turns , so the minimum is reached much more quickly when the jump function can be modified frequently .",
    "i have found that the following recipe allows the metropolis algorithm to quickly converge to a local @xmath1 minimum for a wide variety of planetary and binary microlensing events .",
    "the initial jump function is used until @xmath133 steps have been taken , where @xmath134 is the number of non - linear fit parameters .",
    "( note that if we were to calculate @xmath132 with @xmath135 , we would have a singular matrix , since we would not have enough points to span the @xmath134-dimensional parameter space . ) then , the parameter correlation matrix , @xmath132 , is calculated and diagonalized .",
    "the new parameters are generated from the most recent step with a gaussian probability distribution following the diagonalized parameter correlation matrix .",
    "of course , these new parameters must be converted back to the original non - diagonal parameters for the light curve calculation and @xmath1 evaluation .",
    "the parameter correlation matrix , @xmath132 , is recalculated and diagonalized whenever the number of saved steps , @xmath122 , increases by 4 , until @xmath122 reaches 100 .",
    "once @xmath136 , the oldest saved parameter set is dropped each time a new one is added , so that the number of saved parameter sets to be used for @xmath132 calculations remains fixed at @xmath136 , but the @xmath132 , is still recalculated and diagonalized every 4th time that a new parameter set is accepted .",
    "this procedure allows the parameter correlation matrix to gradually adjust to twists and turns in the @xmath1 surface , as the modeling code travels toward the local @xmath1 minimum . however , sometimes this gradual modification of the parameter correlation matrix is not sufficient to keep up with the changing @xmath1 surface shape , and so i also have a procedure for modifying @xmath132 more drastically .",
    "if the code attempts 40 consecutive parameter sets without a single one being accepted due to an improvement in @xmath1 or passing the boltzmann probability test , then the oldest @xmath137 of the parameter sets are dropped , and if the @xmath138 condition still holds , then @xmath132 is recalculated , diagonalized and used to select the next set of parameters .",
    "if @xmath139 , then we revert to the initial procedure of selecting new parameters with uniform probability within the initially specified uncertainty ranges . sometimes the reduction in the number of saved parameter sets , @xmath122 , will not be sufficient to allow a new parameter set to be accepted in the next 40 steps , and in these cases , the number of saved parameter sets is again reduced by a factor of @xmath140 .",
    "this procedure can even be invoked four times in a row to drop @xmath122 from 100 down to 14 .",
    "this @xmath1 minimization recipe has been extensively tested and has been shown to be robust and efficient for finding the local @xmath1 minima for a wide variety of microlensing events including all published planetary microlensing events and the four clear planet detections from the 2009 bulge observing season , as well as the orbiting two planet system , ogle-2006-blg-109 @xcite .",
    "some examples are discussed in section  [ sec - global - ex ] .",
    "note that this procedure of modifying the jump function should not be used for a markov chain calculation that might be used to estimate parameter uncertainties .",
    "in addition to a method for calculating planetary microlensing event light curves , we also need methods for efficiently moving through parameter space to find the best fit or fits ( as there are sometimes degeneracies ) . for events with only two detectable lens masses and no orbital motion ,",
    "this is fairly straight forward , and a number of methods have been demonstrated to work . for low magnification planetary events like ogle-2005-blg-390 ( beaulieu et al .",
    "2006 ) , it is possible to determine most of the parameters approximately by inspection of the light curve .",
    "for some high - magnification events , such as ogle-2005-blg-71 ( udalski et al .",
    "2005 ) , it is also possible to determine the parameters approximately by inspection , but it is more prudent to do a systematic search for solutions . perhaps the best documented method is the grid search method of @xcite . in this method , the best fit is found for each point on a three dimensional grid over the mass ratio , @xmath141 , lens separation , @xmath74 , and angle , @xmath102 , between the source trajectory and the lens axis .",
    "for each point on this grid , the remaining parameters , are adjusted using a standard fitting algorithm to find the @xmath1 minimum for the fixed values of @xmath141 , @xmath74 , and @xmath102 .",
    "this method generally works quite well , although it can fail in certain instances , such as the case of moa-2007-blg-192 , where there is a degeneracy involving the source star radius parameter , @xmath142 , which is usually not one of the grid parameters .",
    "however , this problem is a result of the sparse light curve sampling for this particular event , and it seems likely that the magnification map method can be modified to model this event .",
    "a more serious issue with this grid search method is that it is impractical to scale it up to systems with more parameters .",
    "if we add another lens mass , this adds three new parameters ( the mass ratio and 2-d position of the additional mass ) , to the two parameters ( the separation , @xmath74 , and mass ratio , @xmath141 ) that are normally held fixed on the grid .",
    "if all five of these parameters are not held fixed , then the computational advantage of the inverse ray shooting method is lost because the same rays can not be used throughout the calculation .",
    "but it is probably too computationally expensive to have more than three grid parameters . in some cases ,",
    "it is possible to find an approximate solution using a simplified model with fewer parameters , and then to consider perturbations to this simplified model to find the full solution .",
    "this allows the grid search method to be used sequentially in stages , first to find the simplified model , and then to search for the perturbation solutions .",
    "this general strategy has proven to be quite useful for features such as microlensing parallax and lens orbital motion , as these usually produce only small light curve perturbations .",
    "it has also proven to be effective for some triple lens models . however",
    ", this method will not work for all triple lens events .",
    "similarly , orbital motion also threatens to derail the computational advantage of this method , although some strategies to deal with such problems have been suggested @xcite .",
    "thus , if we want a general method to model complicated lens systems , the approach i present here seems more promising .",
    "i have developed the following method , which has been successfully tested on virtually all of the planetary microlensing events observed to date .",
    "first we identify the parameters which are obviously well constrained by the light curve",
    ". typically , this would be the einstein ring crossing time , @xmath143 , the time , @xmath144 , and distance , @xmath145 , of lens - source closest approach , the impact parameter , and the source radius crossing time , @xmath142 . for events with strong caustic crossings",
    ", it may be best to use one or two of the caustic - limb crossing times in place of @xmath144 and/or @xmath143 .",
    "this is similar to the approach advocated by @xcite , but it is simpler in that only the time variables @xmath144 and @xmath143 are modified .",
    "we then set up a coarse grid over the remaining parameters , and evaluate @xmath1 for all the grid points .",
    "the parameters that yield the best few @xmath1 values are then selected as initial conditions for fitting using a modified markov chain monte carlo ( mcmc ) routine .",
    "this procedure is repeated with the next best @xmath1 values from the initial grid search until we find that most of the fits are converging to the same final models .",
    "if the fits converge instead to different models with increasingly worse @xmath1 values , then we repeat this procedure with a denser initial conditions grid .",
    "this procedure still uses a grid for the initial conditions , but the modeling runs allow all the parameters to vary .",
    "most of the computations for this method are done during these full modeling runs instead of the initial condition calculations . as a result",
    ", the computation time does not increase so dramatically with the number of model parameters .",
    "in this section , i present several examples that demonstrate how the initial condition grid search method works .",
    "these examples are intended to show how to find an approximate solution for each event .",
    "these approximate solutions usually do not include all the solutions related by the well known light curve degeneracies , such as the @xmath146 degeneracy for high - magnification events @xcite .",
    "i include one example that has not been used for our previous light curve calculation tests , ogle-2005-blg-71 .",
    "the light curve for this event can be modeled reasonably well without the inclusion of a finite source @xcite .",
    "of course , the source must have a finite size , and the finite source effect is important for the complete analysis , which is able to determine the star and planet masses @xcite .",
    "this section does include the three high - magnification events that have been used in the light curve calculation tests , but of the low - magnification events used for the light curve calculation tests , only ogle-2003-blg-235 is included .",
    "no systematic effort to search for the correct light curve model is needed for ogle-2005-blg-390 , because it is possible to get a very good estimate of the parameters by inspection .",
    "the single lens parameters are well determined by a single lens fit , and the single lens magnification at the time of the planetary deviation determines the separation .",
    "the shape of the deviation indicates a major image perturbation .",
    "this information is sufficient to specify initial parameters that will lead to the correct solution .",
    "the other event of modest magnification that is not discussed in this section is moa-2007-blg-197 .",
    "this event is not included because the primary analysis @xcite is not yet complete .",
    "for all calculations in this section , we use the second order integration scheme given in eqs .",
    "[ eq - int_rule ] and [ eq - ab ] , with @xmath112 .",
    "the angular to radial grid - spacing ratio is adjusted to optimize the calculations based upon the characteristics of each individual event .",
    "this event @xcite is an @xmath147 event with a strong central caustic , cusp approach deviation due to a massive planet with mass ratio of @xmath148 .",
    "the source passes on the side of the primary opposite the location of the planet , and so it approaches the two strong central caustic cusps .",
    "the interval between these cusp approaches is three days , and the real - time detection of the planetary signal plus good weather allowed ogle to obtain good light curve coverage over the entire planetary deviation .",
    "the ogle coverage is good enough to pin down the basic planetary parameters , so we include only the ogle data in our search for the correct planetary model .    based on a single lens fit to the ogle data with the planetary deviation removed , we set @xmath149days , @xmath150 , and @xmath151days ( @xmath152 ) .",
    "this event has no obvious finite source features , so i fix the source radius crossing time , @xmath153 , and search for point - source models .",
    "( finite source effects are @xcite detected in the full analysis of the data , but their inclusion does not significantly modify the other parameters . )",
    "these cusp approach events are among the easiest planetary events to model as the fitting code will converge to the correct solution from a large range of initial condition parameters .",
    "so , i set the initial star - planet separation to @xmath154 , and the planetary mass fraction to either @xmath155 or @xmath156 , and then scan over @xmath157 at a @xmath158 interval .",
    "a range of only @xmath159 is needed for @xmath102 because we know by inspection the approximate source trajectory .",
    "the fitting code is then started at the parameters that yield the best initial @xmath1 value for each initial @xmath160 ( or @xmath141 ) value .",
    "the runs for both initial @xmath160 values converge to essentially the same model with @xmath161 for 305 data points with @xmath162days @xmath163days , @xmath164 , @xmath165 , @xmath166 , @xmath167 ( or @xmath168 ) .",
    "there is , of course , also a solution with @xmath169 , that can easily be found using this solution plus the substitution @xmath170 as an initial condition .",
    "because these runs start far from the final solution , we find that it is most efficient to start at a high metropolis algorithm temperature , @xmath129 .",
    "i then reduce @xmath129 several times during the fit run . for this event ,",
    "i started at @xmath171 , and then dropped it to 5 , 0.5 , and 0.05 to reach the final solution , which was reached after 110,559 @xmath1 calculations from the @xmath172 starting point .",
    "the run starting from @xmath173 required 393,328 @xmath1 calculations to approach this same solution . despite the large number of @xmath1 calculations required ,",
    "the entire solution search is fast because point - source calculations were used .",
    "the total calculation took less than 22 cpu minutes on a single cpu of a 3 ghz quad - core intel xeon processor ( in a macpro computer purchased in 2007 running mac os 10.5 ) .",
    "the search over the initial condition grid took less than a cpu second .",
    "ogle-2003-blg-235 lb was the first definitive exoplanet discovery by microlensing @xcite .",
    "this event reveals a giant planet with a mass ratio of @xmath174 via a caustic crossing binary lens feature in an event with a modest stellar magnification of @xmath175 .",
    "while it is often the case that the basic parameters for these lower magnification events can be found by inspection , in this case , we have a so - called  resonant \" caustic with @xmath176 , so that the planetary caustic is connected to the central caustic . in such cases ,",
    "the caustics are weak , and it can be difficult to locate the caustic crossings if they are not directly observed . in the case of ogle-2003-blg-235 , only the second caustic crossing was observed , so there is some uncertainty in the timing of the first caustic crossing .    in order to find candidate solutions for caustic crossing events , it is most efficient to change variables from @xmath144 and @xmath143 to the times of the first and second caustic crossings , @xmath177 and @xmath178 .",
    "this is somewhat similar to , but simpler than , the scheme of @xcite .",
    "the calculations for this event were done with a angular to radial grid spacing ratio of 4 although figure  [ fig - grid_ratio ] indicates that a ratio of 1 would be more efficient .",
    "the mean grid spacing was 0.08 stellar radii .    for ogle-2003-blg-235 , we fix the following parameters for the initial condition grid calculation : the second caustic crossing time , @xmath179 and @xmath180 .",
    "the remaining 5 parameters are allowed to vary over the following ranges : the planetary mass fraction takes the values @xmath181 .",
    "the separation @xmath74 takes the values 0.80 , 0.84 , 0.88 , 0.92 , 0.96 , 1.04 , 1.08 , 1.12 , 1.16 , 1.20 , 1.24 , and the source trajectory angle ranges over @xmath182 , ... , @xmath159 at @xmath183 intervals .",
    "the initial condition grid also includes three source radius crossing times , @xmath184days , and three first caustic crossing times , @xmath185 .",
    "this gives a total of 12276 initial condition grid points for which i calculate @xmath1 .",
    "next , the best initial condition for each @xmath177 value is selected , and used as an initial condition for @xmath1 minimization .",
    "this @xmath1 minimization is done with the usual time variables of @xmath143 and @xmath144 instead of @xmath177 and @xmath186 .",
    "these minimizations are run with an initial value of @xmath187 , which is dropped to @xmath188 .",
    "two of the final solutions match solutions given in @xcite .",
    "the @xmath189days initial condition leads to the best fit model with @xmath190 ( for 1535 data points and 1524 degrees of freedom ) , @xmath191 , @xmath192 , @xmath193days , and @xmath194days .",
    "the @xmath195days initial condition yields the  early caustic \" model of @xcite , with @xmath196 , @xmath197 , @xmath198 , @xmath199days , and @xmath200days .",
    "the modeling run with the @xmath201days initial condition yields a solution that was not reported in the @xcite discovery paper .",
    "this  late caustic \" crossing model has @xmath202 , @xmath191 , @xmath203 , @xmath204days , and @xmath205days , so it is a somewhat better model than the  early caustic \" crossing model reported in the paper .",
    "however , these parameters are within 1-@xmath109 of the best solution values reported in the discovery paper , so it appears likely this model was included in the error bar calculations .",
    "these @xmath1 minimization runs each included an average of 19,000 @xmath1 calculations and used about 1.4 cpu hours each .",
    "the total number of @xmath1 calculations needed to find these three models was slightly over 70,000 , and these calculations were accomplished in 5.24 cpu hours .",
    "moa-2008-blg-310 @xcite is a high - magnification event in which the angular radius of the source star is larger than the width of the central caustic that it crosses .",
    "as can be seen from figure  [ fig - lc_hi ] , the planetary deviation has a maximum amplitude of only @xmath206% compared with the corresponding single lens model .",
    "since this deviation occurs in a region of the light curve when the magnification due to the stellar lens is changing rapidly , it is difficult to see the planetary deviation in the raw light curve , before it is divided by the single lens model .",
    "the light curve deviation due to the planet in figure  [ fig - lc_hi ] does not resemble the light curve deviations for point sources or sources that are much smaller than the width of the central caustic .",
    "one might worry that this unfamiliar light curve deviation shape could be a sign that modeling such an event would be difficult .",
    "but , in fact , this is not the case .",
    "events such as this or moa-2007-blg-400 @xcite turn out to be relatively easy to model because there are few , if any , local @xmath1 minima besides the global minima for @xmath207 and @xmath208 . as a result ,",
    "a relatively sparse initial grid is all that is required .    in this case , i have selected an initial grid that is somewhat larger than is necessary .",
    "the single lens parameters , plus the source radius crossing time , are fixed to the values from the best fit single lens model : @xmath209days , @xmath210 , @xmath211days , and @xmath212days .",
    "the remaining three binary lens parameters are scanned over the following ranges : @xmath213 ; @xmath214 ; and @xmath182 , ... , @xmath215 at @xmath216 intervals .",
    "thus , the initial condition grid calculations require 1620 @xmath1 evaluations .",
    "the best @xmath1 values from this grid come from the parameter sets ( @xmath217 , @xmath218 , @xmath219 ) and ( @xmath220 , @xmath221 , @xmath222 ) .",
    "@xmath1 minimization runs starting from these initial conditions converge to the same solution , which corresponds to the @xmath207 solution of @xcite .",
    "the parameters of this solution are @xmath223days , @xmath224days , @xmath225 , @xmath226 , @xmath227 , @xmath228 , and @xmath229days .",
    "the wide ( @xmath208 ) solution is easily found from this one .",
    "we note that these parameters differ slightly from those of @xcite due to slight differences in the data sets used and a different treatment of the error bars .",
    "these @xmath1 minimization runs each required approximately 18,000 @xmath1 evaluations , and the combination of the initial condition grid search and @xmath1 minimizations required about 7.5 cpu hours using a angular to radial grid spacing ratio of 16 and a mean grid size of 0.16 source radii .",
    "ogle-2005-blg-169 @xcite is a high - magnification caustic crossing event that is similar , in some ways , to moa-2008-blg-310 .",
    "however , in this case the source is much smaller than the width of the central caustic . like ogle-2003-blg-235",
    ", this event does have one well observed caustic crossing with a caustic crossing time that can be accurately estimated by inspection of the light curve .",
    "but , unlike the case of ogle-2003-blg-235 , the stellar magnification peak at @xmath230 is a much stronger feature of the light curve than the resolved caustic crossing . as a result , it is more sensible to use the standard time parameters , @xmath143 and @xmath144 instead of the caustic crossing times , @xmath177 and @xmath186 .",
    "one difficulty with modeling ogle-2005-blg-169 is the incomplete coverage of the light curve . while the light curve extending from the stellar magnification peak to @xmath231 magnitudes below the peak",
    "is very densely covered with observations from the 2.4 m mdm telescope every 10 seconds , the rising portion of the light curve is only observed about once every two hours . as a result",
    ", there are several points in the light curve where the first caustic crossing could have occurred , and this complicates the search for a solution .",
    "the caustic crossing observed for this event is quite weak , and this implies that the planet - star separation must be very close to the einstein ring .",
    "in such a situation , the shape of the central caustic is a very sensitive function of the planet separation and mass fraction , so i use a denser - than - usual initial condition grid .",
    "the separation , @xmath74 , spans the range from 0.97 to 1.03 at an interval of 0.015 , and the planetary mass fraction ranges from @xmath232 to @xmath233 in logarithmic intervals of @xmath234 . for some source trajectories , these high - magnification",
    ", resonant caustic events do not have the usual @xmath146 symmetry , so it is prudent to search over both @xmath235 and @xmath208 .",
    "the source trajectory angle spans the range @xmath182 , ... , @xmath215 with a @xmath216 interval .",
    "the observed caustic crossing could , in principle , be used to determine the source radius crossing time , @xmath142 , but since the first caustic crossing is unobserved , the angle of the crossing is unknown , we can only be sure that @xmath236days , although a value very much smaller than this would require an unreasonably shallow crossing angle .",
    "i allow @xmath142 to range from @xmath237days to @xmath238days in the initial condition grid .",
    "this parameter set yields an initial condition grid of 28,350 grid points .",
    "the remaining parameters are fixed at their single - lens fit values : @xmath239days , @xmath240days , and @xmath241 .",
    "the initial grid search generates four parameter sets that are used for the subsequent @xmath1 minimizations .",
    "these include two parameter sets with @xmath242 .",
    "these are @xmath243 , @xmath244 , @xmath245 and @xmath246days , as well as @xmath243 , @xmath247 , @xmath248 , and @xmath249days .",
    "@xmath1 minimization from these initial conditions leads to two solutions quite similar to the best fit solution of @xcite .",
    "the best model has @xmath250 , @xmath251 , @xmath252 , @xmath253days , @xmath240days , and @xmath254days with @xmath255 for 605 data points and 588 degrees of freedom .",
    "the other model has a slightly worse @xmath256 with similar parameters except that @xmath257 .",
    "these correspond to the best fit model of @xcite .",
    "the remaining two models have @xmath258 , and they correspond to the secondary local minimal shown in figure 2 of @xcite .",
    "note that @xcite use a different source trajectory angle , @xmath259 that is related to our source trajectory angle by @xmath260 .",
    "the @xmath1 minimization runs for ogle-2005-blg-169 averaged about 14,000 @xmath1 evaluations and each took about 5.4 cpu hours to complete using an angular to radial grid spacing ratio of 16 and a mean grid spacing of 0.16 source star radii . the total number of @xmath1 evaluations needed for ogle-2005-blg-169 modeling is 85,400 , and this required 32.3 cpu hours of computing time .",
    "this event is the most time consuming of our example events because of the high - magnification ( @xmath261 ) and the large number of observations at the peak ( although the mdm data are binned to give a sampling interval of 86.4 seconds . )",
    "this event is probably the most challenging of these example events to model because the sampling of the planetary deviation is quite sparse .",
    "it is a high - magnification event , like ogle-2005-blg-169 , but slightly less than half of the peak region is covered with moa survey observations with a sampling interval of about 50 minutes .",
    "this leads to considerable uncertainty in the planetary models @xcite .",
    "in fact , we can not be sure if the data indicate a cusp approach or caustic approach solution .",
    "additionally , the source radius crossing time , @xmath142 , is not well constrained . as a result , a relatively large initial condition grid must be used .",
    "the initial parameters are : @xmath262 ; @xmath220 , 0.6 , 0.7 , 0.75 , 0.8 , 0.85 , 0.9 , 0.95 , 0.96 , 0.97 , 0.98 , 0.99 , 1.00 , 1.01 , 1.02 , 1.03 , 1.04 ; @xmath263 , 0.03 , 0.047 , 0.064 , 0.081 , 0.099 , 0.116 , 0.133 , @xmath264days ; and @xmath182 , ... , @xmath215 at a @xmath216 interval .",
    "the remaining parameters are fixed to the values from a single lens fit with the planetary signal removed : @xmath265days , @xmath266days , and @xmath267 .",
    "the total number of @xmath1 evaluations in this initial condition grid is 55,080 .",
    "the 12 best results from the grid search were selected for @xmath1 minimization .",
    "7 of these @xmath1 minimization runs converged to a variant of the cusp approach solution listed in table  [ tab - modpar ] ; 4 converged to a variant of the caustic crossing solution , and the remaining run converged to another local minimum with a source trajectory angle , @xmath102 , that differs from the cusp approach and caustic crossing values by almost @xmath268 .",
    "this minimum has a @xmath1 value larger than the best fit value by @xmath269 , so it is not considered to be a viable solution .    to get the final set of solutions for this event",
    ", we must consider both the cusp approach and caustic crossing solutions and their @xmath146 counterparts .",
    "also , this event has a significant microlensing parallax signal that we do not investigate here , and this introduces other degeneracies @xcite .",
    "i have presented a previously unpublished general method for modeling multiple mass microlensing events that has been optimized for high - magnification events .",
    "this method has been developed over a number of years from the first general method for calculating binary lens light curves , the image centered ray shooting method of @xcite .",
    "this method is specifically designed to be computationally efficient for the most demanding high magnification events , i.e. , those with more than two lens masses and/or orbital motion .",
    "there are three aspects to this method : an efficient method for numerical calculation of the integrals that are needed to calculation microlensing magnification , an adaptive version of the metropolis algorithm to quickly find a @xmath1 minimum in parameter space , and a global search strategy that can find all the important local @xmath1 minima even in parameter spaces with many dimensions .",
    "the computational efficiency of the first two elements of this method was critical for finding the solution for the solution of the only triple - lens microlensing system yet to be published @xcite , as well as the study of the lens masses and orbits that are consistent with the light curve data for this event @xcite .",
    "this method has also been successfully tested on all eight published single planet microlensing events @xcite , as well as four planetary events from the 2009 observing season . for two of these 2009 events ,",
    "this method found the correct solution before the planetary signal was completed , using data that spanned less than 25% of the planetary deviation ( although these events did have characteristics that were relatively easy to model ) .",
    "the details of several of these test calculations were presented in section  [ sec - global - ex ] .    in section  [ sec - lc_calc ] , i demonstrated the dramatic improvement in high - magnification light curve calculation precision given by my limb - darkening optimized integration method and polar coordinate grid with a much larger angular than radial grid spacing .",
    "for the high - magnification events tested , these features improve the light curve calculation precision by a factor of @xmath270 . nevertheless , it should still be possible to make significant additional improvements .",
    "one improvement would be to implement the hexadecapole approximation @xcite to do finite source calculations , where the point source approximation does not quite work .",
    "this will reduce the number of lens magnification calculations that require the full finite source integrals . for events without caustic crossings , this can dramatically improve calculation efficiency @xcite , but for most caustic crossing events , the improvement is likely to be only a factor of two or so .    further modifications to the integration grid scheme i present here could provide a more dramatic improvement in computational efficiency . i have used a 16:1 grid spacing ratio for the most efficient high - magnification light curve calculations presented here . however , this is a compromise value .",
    "the integration of the bright images that are generated by the primary lens star could probably benefit from a larger ratio , but the smaller images ( or parts of images ) that are directly perturbed by the planet are done more efficiently with a more modest grid - spacing ratio .",
    "this compromise could be avoided by going to an adaptive grid scheme in which the dimensions of the grid are adjusted to match the distortion of the images over the entire image plane .",
    "of course , it would be quite time consuming directly calculate the lens distortions over the entire lens plane , so this would require a simpler prescription to estimate the lens distortions .",
    "it might be that a direct calculation over a coarse grid would work .",
    "of course , the main goal of this method is to be able to model complicated microlensing events that have yet to be successfully modeled .",
    "so , the best demonstration of the strength of this method would be the successful modeling of a number of these events .",
    "abe , f. , et al .",
    "2003 , , 411 , l493 albrow , m.  d. , et al .  1999 , , 522 , 1011 alcock , c. , et al .",
    "1997 , , 491 , 436 alcock , c. , et al .  1999 , , 518 , 44 alcock , c. , et al .",
    "2000 , , 541 , 270 an , j. , et al .",
    "2002 , , 572 , 521 beaulieu , j .-",
    ", et al .",
    "2006 , , 439 , 437 bennett , d.p . & rhie , s.h .",
    "1996 , , 472 , 660 bennett , d.  p.  & rhie , s.  h. , 2002 , , 574 , 985 bennett , d.  p. , et al .  1996a , , 6361 , 1 bennett , d.  p. , et al .",
    "1996b , nuc .",
    ", 51 , 152 bennett , d.  p. , et al .  1997 , planets beyond the solar system and the next generation of space missions , 119 , 95 ( arxiv : astro - ph/9612208 ) bennett , d.  p. , et  al .",
    "2008 , , 684 , 663 bennett , d.  p. , et  al .",
    "2010 , , 713 , 837 bond , i.a .",
    ", et  al . 2004 , , 606 , l155 cassan , a.  2008 , , 491 , 587 cassan , a.  2006 , , 460 , 277 cassan , a.  2010 , in preparation claret , a. 2000 , a&a , 363 , 1081 dominik , m. 1999 , , 349 , 108 dominik , m.  2007 , , 377 , 1679 dong , s. , et al .  2006 , , , 642 , 842 dong , s. , et al .  2009a , , 695 , 970 dong , s. , et al .  2009b , , 698 , 1826 doran , m. , & mller , c.  m.  2004 , journal of cosmology and astro - particle physics , 9 , 3 gaudi , b.  s. , naber , r.  m. , & sackett , p.  d.  1998 , , 502 ,",
    "l33 gaudi , b.  s. , et al .",
    "2008 , science , 319 , 927 gould , a.  2008 , , 681 , 1593 gould , a. , & gaucherel , c.  1997 , , 477 , 580 gould , a. , et al .  2006 , , 644 , l37 griest , k. , & safizadeh , n.  1998 , , 500 , 37 heyrovsk , d.  2007 , , 656 , 483 ida , s. , & lin , d.n.c .",
    "2004 , , 616 , 567 janczak , j. , et al .",
    "2010 , , 711 , 731 kayser , r. , refsdal , s. , & stabell , r.  1986 , , 166 , 36 kennedy , g.  m. , kenyon , s.  j. , & bromley , b.  c.  2006 , , 650 , l139 khavinson , d. , & neumann , g.  2006 , proc .",
    "soc . , 134 , 1077 kirkpatrick , s. , gelatt , c.  d. , & vecchi , m.  p.  1983",
    ", science , 220 , 671 kurucz , r.  1993a , atlas9 stellar atmosphere programs and 2 km / s grid .",
    "kurucz cd - rom no",
    ".  13 .",
    "cambridge , mass . : smithsonian astrophysical observatory , 1993 . , 13 , kurucz , r.  1993b , limbdarkening for 2 km / s grid ( no .  13 ) : [ + 1.0 ] to [ -1.0 ] .  kurucz cd - rom",
    "cambridge , mass . : smithsonian astrophysical observatory , 1993 . , 16 , kurucz , r.  1993c , limbdarkening for 2 km / s grid ( no .  13 ) : [ + 0.0 ] to [ -5.0 ]",
    ".  kurucz cd - rom no .  17 .",
    "cambridge , mass .",
    ": smithsonian astrophysical observatory , 1993 .",
    ", 17 , kurucz , r.  1994 , solar abundance model atmospheres for 0,1,2,4,8 km / s .",
    "kurucz cd - rom no .  19 .",
    "cambridge , mass . : smithsonian astrophysical observatory , 1994 . , 19 , mao , s.  & paczyski , b.  1991 , , 374 , l37 metropolis , n. , rosenbluth , a.  w. , rosenbluth , m.  n. , teller , a.  h. , & teller , e.  1953 , , 21 , 1087 milne , e.  a.  1921 , , 81 , 361 pejcha , o. , & heyrovsk , d.  2009 , , 690 , 1772 press , w.  h. , teukolsky , s.  a. , vetterling , w.  t. , & flannery , b.  p.  1992",
    ", cambridge : university press ,    p.c.m .",
    "2002 , , 335 , 159 rhie , s.  h.  2002 , arxiv : astro - ph/0202294 rhie , s.  h.  2003 , arxiv : astro - ph/0305166 rhie , s.  h. , & bennett , d.  p.  1996",
    ", nuclear physics b proceedings supplements , vol .",
    "51 , 51 , 86 rhie , s.  h.  et al .",
    "2000 , , 533 , 378 schneider , p. , & weiss , a.  1986 , , 164 , 237 sumi , t. , et al .",
    "2010 , , 710 , 1641 udalski , a.  et al .",
    "2005 , , 628 , l109 verde , l. , et al .",
    "2003 , , 148 , 195 vermaak , p.  2000 , , 319 , 1011 wambsganss , t. r. 1997 , , 284 , 172 witt , h.  j.  1990 , , 236 , 311      moa-07 - 197 & 64.45 & -11.241 & 26.759 & 0.0500 & 1.156 & 2.247 & @xmath271 & 0.0490 + ogle-03 - 235 & 61.52 & -13.863 & -5.063 & 0.1327 & 1.120 & 0.764 & @xmath272 & 0.0593 + ogle-05 - 390 & 11.03 & 8.769 & 11.269 & 0.3589 & 1.610 & 2.756 & @xmath273 & 0.282 + ogle-05 - 169 & 41.72 & -0.301 & 0.299 & 0.00125 & 1.020 & 1.020 & @xmath274 & 0.0184 + moa-08 - 310 & 10.47 & -0.200 & 0.200 & 0.00314 & 1.094 & 1.961 & @xmath275 & 0.0546 + moa-07 - 192a & 74.46 & -1.553 & 1.547 & 0.00360 & 1.120 & 4.262 & @xmath276 & 0.0643 + moa-07 - 192c & 75.05 & -1.562 & 1.538 & 0.00433 & 0.985 & 4.518 & @xmath277 & 0.117 +"
  ],
  "abstract_text": [
    "<S> i present a previously unpublished method for calculating and modeling multiple lens microlensing events that is based on the image centered ray shooting approach of @xcite . </S>",
    "<S> it has been used to model all a wide variety of binary and triple lens systems , but it is designed to efficiently model high - magnification planetary microlensing events , because these high - magnification events are , by far , the most challenging events to model . </S>",
    "<S> it is designed to be efficient enough to handle complicated microlensing events , which include more than two lens masses and lens orbital motion . </S>",
    "<S> this method uses a polar coordinate integration grid with a smaller grid spacing in the radial direction than in the angular direction , and it employs an integration scheme specifically designed to handle limb darkened sources . </S>",
    "<S> i present tests that show that these features achieve second order accuracy for the light curves of a number of high - magnification planetary events . </S>",
    "<S> they improve the precision of the calculations by a factor of @xmath0 compared to first order integration schemes with the same grid spacing in both directions ( for a fixed number of grid points ) . </S>",
    "<S> this method also includes a @xmath1 minimization method , based on the metropolis algorithm , that allows the jump function to vary in a way that allows quick convergence to @xmath1 minima . </S>",
    "<S> finally , i introduce a global parameter space search strategy that allows a blind search of parameter space for light curve models without requiring @xmath1 minimization over a large grid of fixed parameters . instead </S>",
    "<S> , the parameter space is explored on a grid of initial conditions for a set of @xmath1 minimizations using the full parameter space . while this method may be somewhat faster than methods that find the @xmath1 minima over a large grid of parameters , </S>",
    "<S> i argue that the main strength of this method is for events with the signals of multiple planets , where a much higher dimensional parameter space must be explored to find the correct light curve model . </S>"
  ]
}