{
  "article_text": [
    "we study the fundamental learning problem : _ given a random sample from an unknown probability distribution with no , or partial label information , identify a separating hyperplane that avoids splitting any of the distinct groups ( clusters ) present in the sample .",
    "_ we adopt the cluster definition given by hartigan  @xcite , in which a _ high - density cluster _ is defined as a maximally connected component of the level set of the probability density function , @xmath0 , at level  @xmath1 , @xmath2 an important advantage of this approach over other methods is that it is well founded from a statistical perspective , in the sense that a well - defined population quantity is being estimated .",
    "however , since @xmath0 is typically unknown , detecting high - density clusters necessarily involves estimates of this function , and standard approaches to nonparametric density estimation are reliable only in low dimensions . a number of existing _ density clustering _",
    "algorithms approximate the level sets of the empirical density through a union of spheres around points whose estimated density exceeds a user - defined threshold  @xcite .",
    "the choice of this threshold affects both the shape and number of detected clusters , while an appropriate threshold is typically not known in advance .",
    "the performance of these methods deteriorates sharply as dimensionality increases , unless the clusters are assumed to be clearly discernible  @xcite .",
    "an alternative is to consider the more specific problem of allocating observations to clusters , which shifts the focus to local properties of the density , rather than its global approximation .",
    "the central idea underlying such methods is that if a pair of observations belong to the same cluster they must be connected through a path traversing only high - density regions .",
    "graph theory is a natural choice to address this type of problem . @xcite and @xcite have recently proposed algorithms based on this approach .",
    "even these approaches however are limited to problems of low dimensionality by the standards of current applications  @xcite .",
    "an equivalent formulation of the density clustering problem is to assume that clusters are separated through contiguous regions of low probability density ; known as the _ low - density separation _ assumption . in both clustering and semi - supervised classification , identifying the hyperplane with the maximum margin is considered a direct implementation of the low - density separation approach .",
    "motivated by the success of support vector machines ( svms ) in classification , maximum margin clustering ( mmc )  @xcite , seeks the maximum margin hyperplane to perform a binary partition ( bi - partition ) of unlabelled data .",
    "mmc can be equivalently viewed as seeking the binary labelling of the data sample that will maximise the margin of an svm estimated using the assigned labels .    in a plethora of applications data can be collected cheaply and automatically , while labelling observations is a manual task that can be performed for a small proportion of the data only .",
    "semi - supervised classifiers attempt to exploit the abundant unlabelled data to improve the generalisation error over using only the scarce labelled examples .",
    "unlabelled data provide additional information about the marginal density , @xmath0 , but this is beneficial only insofar as it improves the inference of the class conditional density ,  @xmath3 .",
    "semi - supervised classification relies on the assumption that a relationship between @xmath0 and @xmath3 exists .",
    "the most frequently assumed relationship is that high - density clusters are associated with a single class ( cluster assumption ) , or equivalently that class boundaries pass through low - density regions ( low - density separation assumption ) .",
    "the most widely used semi - supervised classifier based on the low - density separation assumption is the semi supervised support vector machine ( s@xmath4vm )  @xcite .",
    "s@xmath4vms implement the low - density separation assumption by partitioning the data according to the maximum margin hyperplane with respect to both labelled and unlabelled data .",
    "encouraging theoretical results for semi - supervised classification have been obtained under the cluster assumption .",
    "if @xmath0 is a mixture of class conditional distributions , @xcite have shown that the generalisation error will be reduced exponentially in the number of labelled examples if the mixture is identifiable .",
    "more recently , @xcite showed that the mixture components can be identified if @xmath0 is a mixture of a finite number of smooth density functions , and the separation between mixture components is large .",
    "@xcite considers the cluster assumption in a nonparametric setting , that is in terms of density level sets , and shows that the generalisation error of a semi - supervised classifier decreases exponentially given a sufficiently large number of unlabelled data .",
    "however , the cluster assumption is difficult to verify with a limited number of labelled examples .",
    "furthermore , the algorithms proposed by  @xcite and  @xcite are difficult to implement efficiently even if the cluster assumption holds .",
    "this renders them impractical for real - world problems  @xcite .",
    "although intuitive , the claim that maximising the margin over ( labelled and ) unlabelled data is equivalent to identifying the hyperplane that goes through regions with the lowest possible probability density has received surprisingly little attention . the work of  @xcite is the only attempt we are aware of to theoretically investigate this claim .",
    "@xcite quantify the notion of a low - density separator by defining the _ density on a hyperplane _ , as the integral of the probability density function along the hyperplane .",
    "they study the existence of universally consistent algorithms to compute the hyperplane with minimum density .",
    "the maximum hard margin classifier is shown to be consistent only in one dimensional problems . in higher dimensions",
    "only a soft - margin algorithm is a consistent estimator of the minimum density hyperplane .",
    "@xcite do not provide an algorithm to compute low density hyperplanes .",
    "this paper introduces a novel approach to clustering and semi - supervised classification which directly identifies low - density hyperplanes in the finite sample setting . in this approach the density on a hyperplane criterion proposed by  @xcite",
    "is directly minimised with respect to a kernel density estimator that employs isotropic gaussian kernels .",
    "the density on a hyperplane provides a uniform upper bound on the value of the empirical density at points that belong to the hyperplane .",
    "this bound is tight and proportional to the density on the hyperplane .",
    "therefore , the smallest upper bound on the value of the empirical density on a hyperplane is achieved by hyperplanes that minimise the density on a hyperplane criterion .",
    "an important feature of the proposed approach is that the density on a hyperplane can be evaluated exactly through a one - dimensional kernel density estimator , constructed from the projections of the data sample onto the vector normal to the hyperplane .",
    "this renders the computation of minimum density hyperplanes tractable even in high dimensional applications .",
    "we establish a connection between the minimum density hyperplane and the maximum margin hyperplane in the finite sample setting .",
    "in particular , as the bandwidth of the kernel density estimator is reduced towards zero , the minimum density hyperplane converges to the maximum margin hyperplane .",
    "an intermediate result establishes that there exists a positive bandwidth such that the partition of the data sample induced by the minimum density hyperplane is identical to that of the maximum margin hyperplane . unlike mmc and",
    " 3vms the estimation of which involves an inherently nonconvex combinatorial optimisation problem , estimating minimum density hyperplanes is a nonconvex but continuous optimisation problem , and so offers considerable computational benefits .",
    "the remaining paper is organized as follows : the formulation of the minimum density hyperplane problem as well as basic properties are presented in section  [ sec : formulation ] .",
    "section  [ sec : maxmarg ] establishes the connection between minimum density hyperplanes and maximum margin hyperplanes .",
    "section  [ sec : methodology ] discusses the estimation of minimum density hyperplanes and the computational complexity of the resulting algorithm .",
    "experimental results are presented in section  [ sec : exper ] , followed by concluding remarks and future research directions in section  [ sec : concl ] .",
    "we study the problem of estimating a hyperplane to partition a finite dataset , @xmath5 , without splitting any of the high - density clusters present .",
    "we assume that @xmath6 is an i.i.d .",
    "sample of a random variable @xmath7 on @xmath8 , with unknown probability density function @xmath9 .",
    "a hyperplane is defined as @xmath10 , where without loss of generality we restrict attention to hyperplanes with unit normal vector , i.e. , those parameterised by @xmath11 , where @xmath12 . following  @xcite we define the _ density on the hyperplane _ @xmath13 as the integral of the probability density function along the hyperplane , @xmath14    we approximate @xmath0 through a kernel density estimator with isotropic gaussian kernels , @xmath15 this class of kernel density estimators has the useful property that the integral in eq .",
    "( [ eq1 ] ) can be evaluated exactly by projecting @xmath6 onto @xmath16 ; constructing a one - dimensional density estimator with gaussian kernels and bandwidth  @xmath17 ; and evaluating the density at  @xmath18 , @xmath19 the univariate kernel estimator @xmath20 approximates the _ projected density on  @xmath16 _ , that is , the density function of the random variable , @xmath21",
    ". henceforth we use @xmath22 to approximate @xmath23 . to simplify terminology we refer to @xmath24 as the _ density on @xmath13 _ , or the _ density integral on @xmath13 _ , rather than the empirical density , or the empirical density integral , respectively . for notational convenience",
    "we also write @xmath25 for @xmath26 , and @xmath27 for @xmath28 , where @xmath6 and @xmath17 are apparent from context .",
    "the following lemma , adapted from  ( * ? ? ?",
    "* lemma 3 ) , shows that @xmath22 provides an upper bound for the maximum value of the empirical density at any point that belongs to the hyperplane .",
    "[ lem : levsetbound ] let @xmath29 , and @xmath30 a kernel density estimator with isotropic gaussian kernels .",
    "then , for any @xmath31 , @xmath32    this lemma shows that a hyperplane , @xmath13 , can not intersect level sets of the empirical density with level higher than @xmath33 .",
    "the proof of the lemma relies on the fact that projection contracts distances , and follows from simple algebra . in eq .",
    "( [ lm : tasoulis ] ) equality holds if and only if there exists @xmath34 and @xmath35 such that all @xmath36 , can be written as @xmath37 .",
    "it is therefore not possible to obtain a uniform upper bound on the value of the empirical density at points that belong to @xmath13 that is lower than @xmath33 using only one - dimensional projections . since the upper bound of lemma  [ lem : levsetbound ] is tight and proportional to @xmath38 , minimising the density on the hyperplane leads to the lowest upper bound on the maximum value of the empirical density along the hyperplane separator .    to obtain hyperplane separators that are meaningful for clustering and semi - supervised classification ,",
    "it is necessary to constrain the set of feasible solutions , because the density on a hyperplane can be made arbitrarily low by considering a hyperplane that intersects only the tail of the density . in other words , for any @xmath16 , @xmath39 can be made arbitrarily low for sufficiently large  @xmath40 . in both problems",
    "the constraints restrict the feasible set to a subset of the hyperplanes that intersect the interior of the convex hull of @xmath6 . in detail , let @xmath41 denote the convex hull of  @xmath6 , and assume @xmath42 . define  @xmath43 to be the set of hyperplanes that intersect @xmath44 , @xmath45 then denote by @xmath46 the set of feasible hyperplanes , where @xmath47 .",
    "we define the _ minimum density hyperplane _ ( mdh ) , @xmath48 to satisfy , @xmath49 in the following subsections we discuss the specific formulations for clustering and semi - supervised classification in turn .",
    "since high - density clusters are formed around the modes of  @xmath0 , the convex hull of these modes would be a natural choice to define the set of feasible hyperplanes .",
    "unfortunately , this convex hull is unknown and difficult to estimate .",
    "we instead propose to constrain the distance of hyperplanes to the origin ,  @xmath18 .",
    "such a constraint is inevitable as for any @xmath50 , @xmath25 can become arbitrarily close to zero for sufficiently large  @xmath40 .",
    "obviously , such hyperplanes are inappropriate for the purposes of bi - partitioning as they assign all the data to the same partition . rather than fixing @xmath18 to a constant ,",
    "we constrain it in the interval , @xmath51 , \\ ] ] where @xmath52 and @xmath53 denote the mean and standard deviation , respectively , of the projections @xmath54 .",
    "the parameter  @xmath55 , controls the width of the interval , and has a probabilistic interpretation from chebyshev s inequality .",
    "smaller values of @xmath56 favour more balanced partitions of the data at the risk of excluding low density hyperplanes that separate clusters more effectively . on the other hand ,",
    "increasing  @xmath56 increases the risk of separating out only a few outlying observations .",
    "we discuss in detail how to set this parameter in the experimental results section .",
    "if @xmath57 , then there exists @xmath58 such that the set of feasible hyperplanes for clustering , @xmath59 , satisfies , @xmath60 where @xmath43 is the set of hyperplanes that intersect @xmath61 , as defined in eq .",
    "( [ eq : domain ] ) .",
    "the minimum density hyperplane for clustering is the solution to the following constrained optimisation problem ,    [ eq : ulconstr1 ] @xmath62    since the objective function and the constraints are continuously differentiable , mdhs can be estimated through constrained optimisation methods like sequential quadratic programming ( sqp ) .",
    "unfortunately the problem of local minima due to the nonconvexity of the objective function seriously hinders the effectiveness of this approach .    to mitigate this we propose a parameterised optimisation formulation , which gives rise to a projection pursuit approach .",
    "projection pursuit methods optimise a measure of `` interestingness '' of a linear projection of a data sample , known as the projection index . for our problem",
    "the natural choice of projection index for  @xmath16 is the minimum value of the projected density within the feasible region , @xmath63 .",
    "this index gives the minimum density integral of feasible hyperplanes with normal vector  @xmath16 . to ensure the differentiability of the projection index we incorporate a penalty term into the objective function .",
    "we define the penalised density integral as , @xmath64 where , @xmath65 , @xmath66 is a constant term that ensures that the penalty function is everywhere continuously differentiable , and @xmath67 .",
    "other penalty functions are possible , but we only consider the above due to its simplicity , and the fact that its parameters offer a direct interpretation : @xmath68 in terms of the derivative of @xmath22 ; and @xmath69 in terms of the desired accuracy of the minimisers of @xmath70 relative to the minimisers of eq .",
    "( [ eq : ulconstr1 ] ) , as discussed in the following proposition .",
    "[ prop : minim ]    for @xmath50 , define , the set of minimisers , @xmath71 for every @xmath72 there exists @xmath73 such that @xmath74 . moreover , there are no minimisers of @xmath70 outside the interval @xmath75 $ ] , @xmath76 = \\emptyset.\\ ] ]    any minimiser in the interior of the feasible region , @xmath77 , also minimises the penalised function , since @xmath78 for all @xmath79 , hence @xmath80 .",
    "next we consider the case when either or both of the boundary points of @xmath81 , @xmath82 and @xmath83 , are contained in @xmath84 .",
    "it suffices to show that , @xmath85 for all @xmath86 , and @xmath87 for all @xmath88 .",
    "we discuss only the case @xmath88 as the treatment of @xmath89 is identical .",
    "assume that @xmath90 ( since in the opposite case the result follows immediately : @xmath91 ) .",
    "from the mean value theorem there exists @xmath92 such that , @xmath93 in the above we used the following facts : @xmath94 , @xmath95 , and @xmath96 .    we define the projection index for the clustering problem as the minimum of the penalised density integral , @xmath97 since the optimisation problem of eq .",
    "( [ eq : prindexul ] ) is one - dimensional it is simple to compute the set of global minimisers @xmath98 . as we discuss in section  [ sec : methodology ] , this is necessary to compute directional derivatives of the projection index , as well as , to determine whether @xmath99 is differentiable .",
    "we call the optimisation of @xmath99 , _ minimum density projection pursuit _ ( mdp@xmath100 ) . for each @xmath16 , considers only the optimal choice of @xmath18 .",
    "this enables it to avoid local minima of the @xmath101 .",
    "most importantly is able to accommodate a discontinuous change in the location of the global minimiser(s ) , @xmath102 , as @xmath16 changes .",
    "neither of the above can be achieved when the optimisation is jointly over @xmath103 as in the original constrained optimisation problem , eq .",
    "( [ eq : ulconstr1 ] ) .",
    "the projection index @xmath99 is continuous , but it is not guaranteed to be everywhere differentiable when @xmath98 is not a singleton .",
    "the resulting optimisation problem is therefore nonsmooth and nonconvex .    to illustrate the effectiveness of to estimate mdhs",
    ", we compare this approach with a direct optimisation of the constrained problem given in eq .",
    "( [ eq : ulconstr1 ] ) using sqp . to enable visualisation we consider the two - dimensional  s1 dataset  @xcite , constructed by sampling from a gaussian mixture distribution with fifteen components , where each component corresponds to a cluster .",
    "figure  [ fig : sqpvsmdp ] depicts mdhs obtained over 100 random initialisations of sqp and mdp@xmath100 .",
    "it is evident that sqp frequently yields hyperplanes that intersect regions with high probability density thus splitting clusters . as sqp always converged in these experiments the poor performance is solely due to convergence to local minima .",
    "in contrast , converges to three different solutions over the 100 experiments , all of which induce high - quality partitions , and none intersects a high - density cluster .      in semi - supervised classification labels",
    "are available for a subset of the data sample .",
    "the resulting classifier needs to predict as accurately as possible the labelled examples , while avoiding intersection with high - density regions of the empirical density .",
    "the mdh formulation can readily accommodate partially labelled data by incorporating the linear constraints associated with the labelled data into the clustering formulation .",
    "without loss of generality assume that the first  @xmath104 examples are labelled by @xmath105 .",
    "the mdh for semi - supervised classification is the solution to the problem ,    [ eq : ssconstr1 ] @xmath106    where @xmath107 , and @xmath53 are computed over the entire data set . if the labelled examples are linearly separable the constraints in eq .",
    "( [ eq : ssconstr1 ] ) define a nonempty feasible set of hyperplanes , @xmath108 eqs .",
    "( [ eq : sslconstr2 ] ) and  ( [ eq : sslconstr3 ] ) act as a _ balancing constraint _ which discourages mdhs that classify the vast majority of unlabelled data to a single class . balancing constraints",
    "are included in the estimation of ",
    "3vms for the same reason  @xcite .    as in the case of clustering , the direct minimisation of eq .",
    "( [ eq : ssconstr1 ] ) frequently leads to locally optimal solutions . to mitigate this",
    "we again propose a projection pursuit formulation .",
    "we define the penalised density integral for semi - supervised classification as , @xmath109 where , @xmath110 is a user - defined constant , which controls the trade - off between reducing the density on the hyperplane , and misclassifying the labelled examples .",
    "the projection index is then defined as the minimum of the penalised density integral , @xmath111",
    "in this section we discuss the connection between mdh and maximum ( hard ) margin hyperplane separators .",
    "the margin of a hyperplane @xmath112 with respect to a data set @xmath6 is defined as the minimum euclidean distance between the hyperplane and its nearest datum , @xmath113 the points whose distance to the hyperplane @xmath13 is equal to the margin of the hyperplane , that is , @xmath114 are called the _ support points _ of @xmath13 .",
    "let @xmath46 denote the set of feasible hyperplanes ; then the _ maximum margin hyperplane _",
    "( mmh ) @xmath115 satisfies , @xmath116    the main result of this section is theorem  [ thm : convergence ] , which states that as the bandwidth parameter , @xmath17 , is reduced to zero mdh converges to the maximum margin hyperplane .",
    "an intermediate result , lemma  [ lm : samedivision ] , shows that there exists a positive bandwidth , @xmath117 such that , for all @xmath118 , the partition of the data set induced by mdh is identical to that of maximum margin hyperplane .    to begin with we discuss some assumptions which allow us to present the associated theoretical results of this section .",
    "as before we assume a fixed and finite data set @xmath119 , and approximate its ( assumed ) underlying probability density function via a kernel density estimator using gaussian kernels with isotropic bandwidth matrix @xmath120 .",
    "we assume that the interior of the convex hull of the data , @xmath121 , is non - empty , and define @xmath43 as the set of hyperplanes that intersect  @xmath122 , as in eq .",
    "( [ eq : domain ] ) .",
    "the set of feasible hyperplanes , @xmath46 , for either clustering or the semi - supervised classification satisfies @xmath47 . by construction",
    "every @xmath123 defines a hyperplane which partitions @xmath6 into two non - empty subsets .",
    "observe that if for each @xmath50 the set @xmath124 is compact , then by the compactness of @xmath125 a maximum margin hyperplane in  @xmath46 exists .",
    "for both the clustering and semi - supervised classification problems this compactness holds by construction .",
    "now , for any @xmath126 , let @xmath127 parameterise a hyperplane which achieves the minimal density integral over all hyperplanes in @xmath46 , for bandwidth matrix @xmath128 .",
    "that is , @xmath129 following the approach of  @xcite we first show that as the bandwidth ,  @xmath17 , is reduced towards zero , the density on a hyperplane is dominated by its nearest point .",
    "this is achieved by establishing that for all sufficiently small values of @xmath17 , a hyperplane with non - zero margin has lower density integral than any other hyperplane with smaller margin .",
    "[ lm : bddmargin ] take @xmath130 with non - zero margin and @xmath131 .",
    "then @xmath132 such that @xmath118 and @xmath133 implies @xmath134 .",
    "using eq .",
    "( [ eq4 ] ) it is easy to see that , @xmath135 therefore , @xmath136    therefore , @xmath137 such that @xmath138 .",
    "an immediate corollary of lemma  [ lm : bddmargin ] is that as  @xmath17 tends to zero the margin of the minimum density hyperplane tends to the maximum margin .",
    "however , this does not necessarily ensure the stronger result that the sequence of minimum density hyperplanes converges to the maximum margin hyperplane . to establish this",
    "we require two technical results , which describe some algebraic properties of the maximum margin hyperplane , and are provided as part of the proof of theorem  [ thm : convergence ] which is given in appendix  [ app : thm : convergence ] .",
    "the next lemma uses the previous result to show that there exists a positive bandwidth , @xmath139 , such that an mdh estimated using @xmath140 induces the same partition of @xmath6 as the mmh .",
    "the result assumes that the maximum margin hyperplane is unique .",
    "notice that if @xmath6 is a sample of realisations of a continuous random variable then this uniqueness holds with probability 1 .",
    "[ lm : samedivision ]    suppose there is a unique hyperplane in @xmath46 with maximum margin , which can be parameterised by @xmath141 .",
    "then @xmath137 s.t .",
    "@xmath142 induces the same partition of @xmath6 as @xmath143 .",
    "let @xmath144 . since @xmath6 is finite @xmath145 s.t .",
    "any hyperplane @xmath146 inducing a partition of @xmath6 different than that induced by @xmath147 , satisfies @xmath148 . by lemma  [ lm : bddmargin ] , @xmath137 s.t . ,",
    "@xmath149 which completes the proof .",
    "the next theorem is the main result of this section , and states that mdh converges to mmh as the bandwidth parameter is reduced to zero .",
    "notice that by the non - unique representation of hyperplanes , the maximum margin hyperplane has two parameterisations in @xmath43 , namely @xmath150 and @xmath151 .",
    "convergence to the maximum margin hyperplane is therefore equivalent to showing that , @xmath152    [ thm : convergence ]    suppose there is a unique hyperplane in @xmath46 with maximum margin , which can be parameterised by @xmath141 .",
    "then , @xmath153    the set  @xmath46 used in theorem  [ thm : convergence ] is generic so it can capture the constraints associated with both clustering and semi - supervised classification , eq .",
    "( [ eq : ulconstr1 ] ) , and eq .",
    "( [ eq : ssconstr1 ] ) respectively . in the case of semi - supervised classification we must also assume that the labelled data are linearly separable .",
    "theorem  [ thm : convergence ] is not directly applicable to the formulations as in this case the function being minimised is not the density on a hyperplane .",
    "the next two subsections establish this result for the formulation of the unsupervised and semi - supervised problem .",
    "we have shown that for the constrained optimisation formulation the minimum density hyperplane converges to the maximum margin hyperplane within the feasible set , @xmath154 .",
    "in addition , proposition  [ prop : minim ] places a bound on the difference between the optima for the two problems , in terms of the parameter @xmath69 . combining these",
    "we can show that the optimal solution to the penalised mdp@xmath100 formulation converges to the maximum margin hyperplane in @xmath59 , provided the parameters within the penalty term suitably depend on the bandwidth parameter , @xmath17 . while the general case can be shown , for ease of exposition",
    "we make the simplifying assumption that the maximum margin hyperplane is strictly feasible , i.e. , if @xmath155 parameterises the maximum margin hyperplane then @xmath156 .",
    "for @xmath157 define @xmath158 to be any global minimiser of @xmath159 , i.e. , @xmath160    [ lem : ulconv ] suppose there is a unique hyperplane in @xmath59 with maximum margin , which can be parameterised by @xmath141 .",
    "suppose further that @xmath161 .",
    "for @xmath126 , let @xmath162 , and @xmath163 .",
    "then , @xmath164    let @xmath165 and as in the proof of lemma [ lm : samedivision ] , let @xmath166 be such that any hyperplane inducing a different partition from @xmath143 has margin at most @xmath167 . since @xmath168 is strictly feasible it must be the unique maximum margin hyperplane in @xmath169 , since the margins of the locally maximum margin hyperplanes for each partition of @xmath6 can increase by at most @xmath170 .",
    "we have used the notation @xmath171 to denote the neighbourhood of @xmath81 given by @xmath172 .",
    "observe now that for @xmath173 we have @xmath174 , by proposition  [ prop : minim ] .",
    "in addition , by theorem  [ thm : convergence ] , we know that the minimisers of @xmath27 over @xmath175 , say @xmath176 , satisfy @xmath177 now , since @xmath143 is strictly feasible @xmath178 s.t . @xmath179 . then for any @xmath180 there exists @xmath181 s.t . for @xmath182 both @xmath183 _ and _ @xmath174 . now for @xmath184 we know that @xmath185 , whereas for @xmath186 and therefore the minimiser of @xmath187 must lie in the neighbourhood @xmath188 , and the result follows .",
    "denote the set of hyperplanes which correctly classify the labelled data by @xmath189 . under the assumption that @xmath190 with non - zero margin",
    ", we can show that , provided the parameter @xmath191 does not shrink too quickly with @xmath17 , the hyperplane that minimises  @xmath192 converges to the maximum margin hyperplane contained in  @xmath193 , where as before we assume that such a maximum margin hyperplane is strictly feasible . to establish this result",
    "it is sufficient to show that there exists @xmath117 such that for all @xmath194 , the optimal hyperplane @xmath195 correctly classifies all the labelled examples .",
    "if this holds , then @xmath196 for all sufficiently small  @xmath17 , and hence lemma  [ lem : ulconv ] can be applied to establish the result .",
    "the proof relies on the fact that the penalty terms associated with the known labels in eq .",
    "( [ eq : penalisedssl ] ) are polynomials in @xmath18 .",
    "provided that @xmath191 is bounded below by a polynomial in @xmath17 , the value of the penalty terms for hyperplanes that do not correctly classify the labelled data dominate the value of the density integral as @xmath17 approaches zero .",
    "therefore the optimal hyperplane must correctly classify the labelled data for small values of @xmath17 .",
    "define @xmath197 and @xmath198 and assume that @xmath199 and that @xmath200 with non - zero margin . for @xmath201 ,",
    "let @xmath162 , @xmath163 and @xmath202 for some @xmath203",
    ". then @xmath204 s.t .",
    "@xmath205 .",
    "consider @xmath206 .",
    "then , @xmath207 where @xmath208 minimises @xmath209 .",
    "therefore , @xmath210 is the unique positive number satisfying , @xmath211 we therefore have , @xmath212 where @xmath213 is a constant which can be chosen independent of @xmath214 . finally , for any @xmath215 with non - zero margin , @xmath204 s.t .",
    "@xmath216 since @xmath213 is independent of @xmath214 , the result follows .",
    "the final set of inequalities holds since the hyperplane @xmath217 is assumed to have non - zero margin , say @xmath218 , and hence @xmath219 , which tends to zero faster than any polynomial in @xmath17 .",
    "in this section we discuss the computation of minimum density hyperplanes .",
    "we first investigate the continuity and differentiability properties required to optimise the projection indices  @xmath220 and  @xmath221 .    since the domain of both projection indices , @xmath220 and  @xmath221 , is the boundary of the unit - sphere in  @xmath8 it is more convenient to express @xmath16 in terms of spherical coordinates , @xmath222 where @xmath223^{d-2 } \\times [ 0,2\\pi]$ ]",
    "is called the _",
    "projection angle_. using spherical coordinates renders the domain , @xmath224 , convex and compact , and reduces dimensionality by one .",
    "as the following discussion applies to both  @xmath220 and  @xmath221 we denote a generic projection index @xmath225 , and the associated set of minimisers , as , @xmath226 where @xmath227 is continuously differentiable , @xmath228 is compact and convex , and the correspondence @xmath229 gives the set of global minimisers of @xmath230 for each @xmath231 .",
    "the definition of  @xmath232 is not critical our formulation .",
    "setting , @xmath233,\\ ] ] where @xmath234 is the variance of the projections along the first principal component , ensures that the set of hyperplanes that satisfy the constraint of eq .",
    "( [ constr1 ] ) will be a subset of @xmath232 for all @xmath16 .",
    "berge s maximum theorem  @xcite , establishes the continuity of @xmath235 and the upper - semicontinuity ( u.s.c . ) of the correspondence @xmath229 .",
    "theorem  3.1 in  @xcite enables us to establish that @xmath235 is locally lipschitz continuous . using theorem 4.13 of  @xcite we",
    "can further show that @xmath235 is directionally differentiable everywhere .",
    "the directional derivative at @xmath231 in the direction @xmath236 is given by , @xmath237 where @xmath238 denotes the derivative with respect to @xmath231 .",
    "it is clear from eq .",
    "( [ optdif ] ) that @xmath235 is differentiable if @xmath239 is the same for all @xmath240 .",
    "if @xmath241 is a singleton then this condition is trivially satisfied and @xmath235 is continuously differentiable at @xmath231 .",
    "it is possible to construct examples in which @xmath241 is not a singleton .",
    "however , with the exception of contrived examples , our experience with real and simulated datasets indicates that when @xmath17 is set through standard bandwidth selection rules @xmath241 is almost always a singleton over the optimisation path .",
    "suppose @xmath241 is a singleton for almost all @xmath242 .",
    "then @xmath235 is continuously differentiable almost everywhere .",
    "the result follows immediately from the fact that if @xmath243 is a singleton , then the derivative @xmath244 , which is continuous .",
    "@xcite has provided early examples of how standard gradient - based methods can fail to converge to a local optimum when used to minimise nonsmooth functions . in the last decade",
    "a new class of nonsmooth optimisation algorithms has been developed based on gradient sampling  @xcite .",
    "gradient sampling methods use generalised gradient descent to find local minima . at each iteration points",
    "are randomly sampled in a radius @xmath245 of the current candidate solution , and the gradient at each point is computed .",
    "the convex hull of these gradients serves as an approximation of the @xmath245-clarke generalised gradient  @xcite .",
    "the minimum element in the convex hull of these gradients is a descent direction .",
    "the gradient sampling algorithm progressively reduces the sampling radius so that the convex hull approximates the clarke generalised gradient .",
    "when the origin is contained in the clarke generalised gradient there is no direction of descent , and hence the current candidate solution is a local minimum .",
    "gradient sampling achieves almost sure global convergence for functions that are locally lipschitz continuous and almost everywhere continuously differentiable .",
    "it is also well documented that it is an effective optimisation method for functions that are only locally lipschitz continuous .",
    "in this subsection we analyse the computational complexity of mdp@xmath100 . at each iteration",
    "the algorithm projects the data sample onto @xmath246 which involves @xmath247 operations . to compute the projection index , @xmath235",
    ", we need to minimise the penalised density integral , @xmath230 .",
    "this can be achieved by first evaluating @xmath230 on a grid of @xmath248 points , to bracket the location of the minimiser , and then applying bisection to compute the minimiser(s ) within the desired accuracy .",
    "the main computational cost of this procedure is due to the first step which involves @xmath248 evaluations of a kernel density estimator with @xmath249 kernels . using the improved fast gauss transform  @xcite",
    "this can be performed in @xmath250 operations , instead of @xmath251 .",
    "bisection requires @xmath252 iterations to locate the minimiser with accuracy  @xmath245 .",
    "if the minimiser of the penalised density integral @xmath253 , is unique the projection index is continuously differentiable at @xmath231 . to obtain the derivative of the projection index it is convenient to define the projection function ,",
    "@xmath254 an application of the chain rule yields , @xmath255 where the derivative of the projections of the data sample with respect to @xmath16 is equal to the data matrix , @xmath256 ; and @xmath257 is the derivative of @xmath16 with respect to the projection angle , which yields a @xmath258 matrix .",
    "the computation of the derivative therefore requires @xmath259 operations .",
    "the original gs algorithm requires @xmath260 gradient evaluations at each iteration which is costly . @xcite",
    "have developed an adaptive gradient sampling algorithm that requires @xmath261 gradient evaluations in each iteration .",
    "more recently , @xcite have strongly advocated that for the minimisation of nonsmooth , nonconvex , locally lipschitz functions , a simple bfgs method using inexact line searches is much more efficient in practice than gradient sampling , although no convergence guarantees have been established for this method .",
    "bfgs requires a single gradient evaluation at each iteration and a matrix vector operation to update the hessian matrix approximation . in our experiments we use this algorithm",
    "in this section we assess the empirical performance of the mdh approach for clustering and semi - supervised classification .",
    "we compare performance with existing state - of - the - art methods for both problems on the following 14 benchmark datasets : banknote authentication ( banknote ) , breast cancer winsconsin original ( br .",
    "cancer ) , forest type mapping ( forest ) , ionosphere , optical recognition of handwritten digits ( optidigits ) , pen - based recognition of hand - written digits ( pendigits ) , seeds , smartphone - based recognition of human activities and postural transitions ( smartphone ) , statlog image segmentation ( image seg . ) , statlog landsat satellite ( satellite ) , synthetic control chart time series ( synth control ) , congressional voting records ( voting ) , wine , and yeast cell cycle analysis ( yeast ) . details of these data sets , in terms of their size @xmath249 , dimensionality @xmath262 and number of clusters @xmath263 , can be seen in table  [ tb : datasets ] .",
    ".details of benchmark data sets [ tb : datasets ] [ cols=\">,>,>,>\",options=\"header \" , ]     \\a .",
    "uci machine learning repository https://archive.ics.uci.edu/ml/datasets.html + b. stanford yeast cell cycle analysis project http://genome-www.stanford.edu/cellcycle/      since mdh yields a bi - partition of a dataset rather than a complete clustering , we propose two measures to assess the quality of a binary partition of a dataset containing an arbitrary number of clusters .",
    "both take values in @xmath264 $ ] with larger values indicating a better partition .",
    "these measures are motivated by the fact that a good binary partition should ( a ) avoid dividing clusters between elements of the partition , and ( b ) be able to discriminate at least one cluster from the rest of the data . to capture this",
    "we modify the cluster labels of the data by assigning each cluster to the element of the binary partition which contains the majority of its members . in the case of a tie the cluster",
    "is assigned to the smaller of the two partitions .",
    "we thus merge the true clusters into two aggregate clusters ,  @xmath265 and @xmath266 .",
    "the first measure we use is the binary v - measure which is simply the v - measure  @xcite computed on @xmath267 with respect to the binary partition , which we denote @xmath268 .",
    "the v - measure is the harmonic mean of homogeneity and completeness . for a dataset containing clusters @xmath269 , partitioned as @xmath270",
    ", homogeneity is defined as the conditional entropy of the cluster distribution within each partition , @xmath271 .",
    "completeness is symmetric to homogeneity and measures the conditional entropy of each partition within each cluster , @xmath272 .",
    "an important characteristic of the v - measure for evaluating binary partitions is that if the distribution of clusters within each partition is equal to the overall cluster distribution in the data set then the v - measure is equal to zero  @xcite .",
    "this means that if an algorithm fails to distinguish the majority of any of the clusters from the remainder of the data , the binary v - measure returns zero performance .",
    "other evaluation metrics for clustering , such as purity and the rand index , can assign a high value to such partitions .    to define the second performance measure we first determine the number of correctly and incorrectly classified samples .",
    "the error of a binary partition , @xmath273 , given in eq .  ( [ eq : error ] ) , is defined as the number of elements of each aggregate cluster which are not in the same partition as the majority of their original clusters .",
    "in contrast , the success of a partition , @xmath274 , eq .",
    "( [ eq : success ] ) , measures the number of samples which are in the same partition as the majority of their original clusters . the success ratio , @xmath275 , eq .",
    "( [ eq : sr ] ) , captures the extent to which the majority of at least one cluster is well - distinguished from the rest of the data .",
    "@xmath276 similarly to the binary v - measure defined above , success ratio takes the value zero if an algorithm fails to distinguish the majority of any cluster from the remainder of the data .",
    "the two most important settings for the performance of the proposed approach are the initial projection direction , and the choice of  @xmath56 , which controls the width of the interval @xmath81 within which the optimal hyperplane falls . despite the ability of the formulation to mitigate the effect of local minima in the projected density @xmath277 ,",
    "the problem remains non - convex and local minima in the projection index can still lead to suboptimal performance .",
    "we have found that this effect is amplified in general when either or both the number of dimensions , and the number of high density clusters in the dataset is large . to better handle the effect of local optima , we use multiple initialisations and select the mdh that maximises the _ relative depth _ criterion , defined in eq .",
    "( [ eq : reldep ] ) . the relative depth of an mdh , @xmath278 , is defined as the smaller of the relative differences in the density on the mdh and its two adjacent modes in the projected density , @xmath279 , @xmath280 where @xmath281 and @xmath282 are the two adjacent modes in @xmath279 . if an mdh does not separate the modes of the projected density , @xmath279 , then its relative depth is set to zero , signalling a failure of to identify a meaningful bi - partition .",
    "the relative depth is appealing because it captures the fact that a high quality separating hyperplane should have a low density integral , and separate well the modes of the projected density  @xmath283 .",
    "note also that the relative depth is equivalent to the inverse of a measure used to define cluster overlap in the context of gaussian mixtures  @xcite .",
    "in all the reported experiments we initialise to the first and second principal component and select the mdh with the largest relative depth . for the data sets listed above it was never the case that both initialisations led to mdhs with zero relative depth .",
    "the choice of  @xmath56 determines the trade - off between a balanced bi - partition and the ability to discover lower density hyperplanes .",
    "the difficulties associated with choosing this parameter are illustrated in figure  [ fig : alpha ] . in each sub - figure the horizontal axis is the candidate projection vector ,  @xmath16 , while the right vertical axis is the direction of maximum variability orthogonal to @xmath16 .",
    "points correspond to projections of the data sample onto this two - dimensional space , while colour indicates cluster membership .",
    "the solid line depicts the projected density on @xmath16 , @xmath101 , while the dotted line depicts the penalised function , @xmath284 .",
    "the scale of both functions is depicted in the left vertical axis .",
    "the solid vertical line indicates the mdh along @xmath16 .",
    "setting @xmath56 to a large value can cause to focus on hyperplanes that have low density because they partition only a small subset of the dataset as shown in figure  [ fig : alpha1 ] .",
    "in contrast smaller values of @xmath56 may cause the algorithm to disregard valid lower density hyperplane separators ( see figure  [ fig : alpha2 ] ) , or for the separating hyperplane to not be a local minimiser of the projected density ( see figure  [ fig : alpha3 ] ) .    rather than selecting a single value for  @xmath56 we recommend solving repeatedly for an increasing sequence of values in the range @xmath285 , where each implementation beyond the first is initialised using the solution to the previous . setting @xmath286 close to zero forces to seek low density hyperplanes that induce a balanced data partition .",
    "this tends to find projections which display strong multimodal structure , yet prevents convergence to hyperplanes that have low density because they partition a few observations , as in the case shown in figure  [ fig : alpha1 ] .",
    "increasing @xmath56 progressively fine - tunes the location of the mdh . to avoid sensitivity to the value of @xmath287 ( set to 0.9 )",
    "the output of the algorithm is the last hyperplane that corresponds to a minimiser of @xmath101 .",
    "figure  [ fig : opti ] illustrates this approach using the optical recognition of handwritten digits dataset from the uci machine learning repository  @xcite .",
    "figure  [ fig : opt1 ] depicts the projected density on the initial projection direction , which in this case is the second principal component .",
    "as shown , the density is unimodal and the clusters are not well separated along this vector . although not shown , if a large value of @xmath56 is used from the outset , will identify a vector for which @xmath101 is unimodal and skewed .",
    "figure  [ fig : opt2 ] shows that after five iterations with @xmath288 has identified a projection vector such that @xmath101 is bimodal . in subsequent iterations",
    "the two modes become more clearly separated , figure  [ fig : opt3 ] , while increasing @xmath56 enables to locate an mdh that corresponds to a minimiser of @xmath101 , as illustrated in figure  [ fig : opt4 ] .",
    "+      we compare the performance of for clustering with the following methods :    1 .",
    "@xmath289-means++  @xcite , a version of @xmath289-means that is guaranteed to be @xmath290competitive to the optimal @xmath289-means clustering .",
    "the adaptive linear discriminant analysis guided @xmath289-means ( lda-@xmath289 m )  @xcite .",
    "lda-@xmath289 m attempts to discover the most discriminative linear subspace for clustering by iteratively using @xmath289-means , to assign labels to observations , and lda to identify the most discriminative subspace .",
    "3 .   the principal direction divisive partitioning ( pddp )  @xcite , and the density - enhanced pddp ( depddp )  @xcite .",
    "both methods project the data onto the first principal component .",
    "pddp splits at the mean of the projections , while depddp splits at the lowest local minimum of the one - dimensional density estimator .",
    "4 .   the iterative support vector regression algorithm for mmc  @xcite using the inner product and gaussian kernel , isvr - l and isvr - g respectively . both",
    "are initialised with the output of 2-means++ .",
    "normalised cut spectral clustering ( scn )  @xcite using the gaussian affinity function , and the automatic bandwidth selection method of  @xcite .",
    "this choice of kernel and bandwidth produced substantially better performance than alternative choices considered . for datasets that are too large for the eigen decomposition of the gram matrix to be feasible we employed the nystrm method  @xcite .",
    "we also considered the density - based clustering algorithm pdfcluster  @xcite , but this algorithm could not be executed on the larger datasets and so its performance is not reported in this paper . with the exception of scn and isvr - g , the methods considered bi - partition the data through a hyperplane in the original feature space . for the 2-means and lda-2 m algorithm",
    "the hyperplane separator bisects the line segment joining the two centroids .",
    "isvr - l directly seeks the maximum margin hyperplane in the original space , while isvr - g seeks the maximum margin hyperplane in the feature space defined by the gaussian kernel .",
    "pddp and depddp use a hyperplane whose normal vector is the first principal component .",
    "pddp uses a fixed split point while depddp uses the hyperplane with minimum density along the fixed projection direction .",
    "table  [ tbl : cluster ] reports the performance of the considered methods with respect to the success ratio ( sr ) and the binary v - measure ( v - m ) on the fourteen datasets .",
    "in addition figures  [ fig : clmetr ] and  [ fig : clreg ] provide summaries of the overall performance on all datasets using boxplots of the raw performance measures as well as the associated _ regret_. the regret of an algorithm on a given dataset is defined as the difference between the best performance attained on this dataset and the performance of this algorithm . by comparing against the best performing clustering algorithm regret accommodates for differences in difficulty between clustering problems , while also making use of the magnitude of performance differences between algorithms .",
    "the distribution of performance with respect to both sr and v - m is negatively skewed for most methods , and as a result the median is higher than the mean ( indicated with a red dot ) .",
    "it is clear from table  [ tbl : cluster ] that no single method is consistently superior to all others , although achieves the highest or tied highest performance on seven datasets ( more than any other method ) .",
    "more importantly is among the best performing methods in almost all cases .",
    "this fact is better captured by the regret distributions in figure  [ fig : clreg ] .",
    "here we see that the average , median , and maximum regret of is substantially lower than any of the competing methods .",
    "in addition achieves the highest mean and median performance with respect to both sr and v - m , while also having much lower variability in performance when compared with most other methods .",
    "pairwise comparisons between and other methods reveal some less obvious facts .",
    "scn achieves higher performance than in more examples ( six ) than any other competing method , however it is much less consistent in its performance , obtaining very poor performance on five of the data sets .",
    "the isvr maximum margin clustering approach is arguably the closest competitor to mdp@xmath100 .",
    "isvr - l and isvr - g achieve the second and third highest average performance with respect to v - m and sr respectively .",
    "the pddp algorithm is the second best performing method on average with respect to sr , but performs poorly with respect to v - m .",
    "the density enhanced variant , depddp , performs on average much worse than mdp@xmath100 .",
    "this approach is similarly motivated by obtaining hyperplanes with low density integral , and its low average performance indicates the usefulness of searching for high quality projections as opposed to always using the first principal component .",
    "finally , neither of the @xmath289-means variants appears to be competitive with in general .      in this section",
    "we evaluate mdhs for semi - supervised classification .",
    "we compare mdhs against three state - of - the - art semi - supervised classification methods : laplacian regularised support vector machines ( lapsvm )  @xcite , simple semi - supervised learning ( sssl )  @xcite , and correlated nystr \" om views ( xnv )  @xcite .",
    "for all methods the inner product kernel was used to render the resulting classifiers linear , and thereby comparable to mdh .",
    "as the mdh is asymptotically equivalent to a linear ",
    "3vm we also considered the continuous formulation for the estimation of a ",
    "3vm proposed by  @xcite .",
    "these results are omitted as this method was not competitive on any of the considered datasets .",
    "the existence of a few labelled examples enables an informed initialisation of mdp@xmath100 .",
    "we consider the first and second principal components as well as the weight vector of a linear svm trained on the labelled examples only , and initialise with the vector that minimises the value of the projection index,@xmath291 .",
    "the penalty parameter @xmath191 is first set to  @xmath292 and with this setting @xmath56 is progressively increased in the same way as for clustering .",
    "after this , @xmath56 is kept at @xmath287 and @xmath191 is increased to  1 and then  10 .",
    "thus the emphasis is initially on finding a low density hyperplane with respect to the marginal density @xmath293 . as the algorithm progresses the emphasis on correctly classifying the labelled examples increases , so as to obtain a hyperplane with low training error within the region of low density already determined .      to assess the effect on performance of the number of labelled examples , @xmath104",
    ", we consider a range of values .",
    "we compare the methods using the subset of datasets used in the previous section in which the size of the smallest class exceeds 100 .",
    "in total eight datasets are used . for each value of @xmath104 , 30 random partitions into labelled and unlabelled data are considered .",
    "as classes are balanced in the datasets considered , performance is measured only in terms of classification error on the unlabelled data . for datasets with more than two classes",
    "all pairwise combinations of classes are considered and aggregate performance is reported .",
    "median ( @xmath294 ) , lapsvm median ( ) , sssl median ( ) , xnv median ( @xmath295 ) , with corresponding interquartile ranges given by shaded regions .    figure  [ ssl : regret1 ] provides plots of the median and interquartile range of the classification error for values of @xmath104 between 5 and 100 for the four datasets with two classes .",
    "overall appears to be most competitive when the number of labelled examples is small .",
    "in addition , is comparable with the best performing method in almost every case .",
    "the only exception is the ionosphere dataset where lapsvm outperforms for all values of @xmath104 .",
    "figure  [ ssl : regret2 ] provides plots of the median and interquartile range of the aggregate classification error on datasets containing more than two classes . as",
    "these datasets are larger we consider up to 300 labelled examples .",
    "note that the interquartile range for xnv is not depicted for the satellite dataset .",
    "the variability of performance of xnv on this dataset was so high that including the interquartile range would obscure all other information in the figure .",
    "exhibits the best performance overall , and obtains the lowest median classification error , or tied lowest , for all datasets and values of @xmath104 .",
    "median ( @xmath294 ) , lapsvm median ( ) , sssl median ( ) , xnv median ( @xmath295 ) , with corresponding interquartile ranges given by shaded regions .",
    "we evaluated the performance of the formulation for finding minimum density hyperplanes for both clustering and semi - supervised classification , on a large collection of benchmark datasets , and in comparison with state - of - the - art methods for both problems .",
    "for clustering , we found that no single method was consistently superior to all others .",
    "this is a result of the vastly differing nature of the datasets in terms of size , dimensionality , number and shape of clusters , etc .",
    "achieved the best performance on more datasets than any of the competing methods , and importantly was competitive with the best performing method in almost every dataset considered .",
    "all other methods performed poorly in at least as many examples .",
    "boxplots of both the raw performance and performance regret , which measures the difference between each method and the best performing method on each dataset , allowed us to summarise the comparative performance of the different methods across datasets .",
    "the mean and median raw performance of is substantially higher than the next best performing method , and the regret is also substantially lower .    in the case of semi - supervised classification it was apparent that is extremely competitive when the number of labelled examples is ( very ) small , but that in some cases its performance does not improve as much as that of the other methods considered , when the labelled examples become more abundant .",
    "our experiments suggest that overall is very competitive with the state - of - the - art for semi - supervised classification problems .",
    "we proposed a new hyperplane classifier for clustering and semi - supervised classification .",
    "the proposed approach is motivated by determining low density linear separators of the high - density clusters within a dataset .",
    "this is achieved by minimising the integral of the empirical density along the hyperplane , which is computed through kernel density estimation . to the best of our knowledge",
    "this is the first direct implementation of the low density separation assumption that underlies high - density clustering and numerous influential semi - supervised classification methods .",
    "we show that the minimum density hyperplane classifier is asymptotically connected with maximum margin support vector classifiers , thereby establishing an important link between the proposed approach , maximum margin clustering , and semi - supervised support vector machines .",
    "the proposed formulation allows us to evaluate the integral of the density on a hyperplane by projecting the data onto the vector normal to the hyperplane , and estimating a univariate kernel density estimator .",
    "this enables us to apply our method effectively and efficiently on datasets of much higher dimensionality than is generally possible for density based clustering methods . to mitigate the problem of convergence to locally optimal solutions we proposed a projection pursuit formulation .",
    "we evaluated the minimum density hyperplane approach on a large collection of benchmark datasets .",
    "the experimental results obtained indicate that the method is competitive with state - of - the - art methods for clustering and semi - supervised classification .",
    "importantly the performance of the proposed approach displays low variability across a variety of datasets , and is robust to differences in data size , dimensionality , and number of clusters . in the context of semi - supervised classification ,",
    "the proposed approach shows especially good performance when the number of labelled data is small .",
    "we wish to thank the reviewers for their valuable comments and suggestions which greatly improved this paper .",
    "nicos pavlidis would like to thank the isaac newton institute for mathematical sciences , cambridge , for support and hospitality during the programme inference for change - point and related processes where work on this paper was undertaken .",
    "david hofmeyr gratefully acknowledges the support of the epsrc funded ep / h023151/1 stor - i centre for doctoral training , as well as the oppenheimer memorial trust .",
    "we thank prof .",
    "david leslie , and dr .",
    "teemu roos for valuable comments and suggestions on this work .",
    "we compute the empirical density on a hyperplane @xmath296 , through the kernel density estimator @xmath297 , as in eq .",
    "( [ eq4 ] ) .",
    "@xmath298 where @xmath299 is a continuously differentiable function of @xmath231 . in our implementation",
    "we set , @xmath300 where @xmath301 is the standard deviation of the projected sample @xmath302 , where @xmath303 .",
    "such a choice is based on recommendations by  @xcite . in particular , for normally distributed data setting @xmath304 minimises the asymptotic mean integrated squared error . if the underlying distribution is multimodal , then @xmath305 is recommended .    to obtain the derivative of @xmath306 with respect to @xmath231",
    "it is convenient to define the function , @xmath307 , @xmath308 then an application of the chain rule on @xmath309 yields , @xmath310 where , @xmath311 , @xmath312 , and @xmath313 .",
    "we consider each partial derivative in turn .",
    "@xmath314 . \\ ] ] if @xmath315 , then @xmath316 is well - defined and continuous .    @xmath317 = \\left [ \\begin{array}{cccc } x_{1,1 } & x_{1,2 } & \\ldots & x_{1,d } \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n,1}&x_{n,2 } & \\ldots & x_{n , d } \\\\ \\frac{\\partial{h } } { \\partial{v_1 } } &   \\frac{\\partial{h } } { \\partial{v_2 } } & \\ldots & \\frac{\\partial{h } } { \\partial{v_n } } \\end{array } \\right ] , \\\\",
    "\\frac{\\partial{h } } { \\partial{v_k } } & = \\frac{\\eta}{s(\\theta)(n-1 ) } \\sum_{i=1}^n \\left ( p_i - \\bar{p } \\right ) \\left ( x_{i , k } - \\bar{x}_{k } \\right ) . \\ ] ]    where @xmath318 .",
    "the condition @xmath319 also ensures that @xmath320 is well - defined and continuous",
    ". finally , @xmath321    we next provide the derivative of the penalised density integral for unsupervised classification , @xmath322 as before we express @xmath323 as a composition of functions @xmath324 .",
    "to obtain the derivative , @xmath325 we only need a formula for the derivative , @xmath326 as @xmath327 and @xmath328 are provided above .",
    "@xmath329 \\right\\ } \\ell(v(\\theta),b ; \\alpha , \\beta)^2 \\\\ \\frac { \\partial{\\ell } } { \\partial{h } } & = - \\frac{2 \\alpha \\beta h}{\\eta^2 } \\exp \\left\\ { -\\beta [ ( b-\\mu(\\theta))^2 - \\alpha \\sigma(\\theta)^2 ] \\right\\ } \\ell(v(\\theta),b ; \\alpha , \\beta)^2 , \\ ] ]    where @xmath330 is the mean of @xmath331 , and in the last equation we make use of the fact that @xmath332 .",
    "before proving theorem  [ thm : convergence ] we require the following two technical lemmata which establish some algebraic properties of the maximum margin hyperplane .",
    "the following lemma shows that any hyperplane orthogonal to the maximum margin hyperplane results in a different partition of the support points of the maximum margin hyperplane .",
    "the proof relies on the fact that if this statement does not hold then a hyperplane with larger margin exists which is a contradiction .",
    "figure  [ fig : mmhorthog ] provides an illustration of why this result holds .",
    "( a ) any hyperplane orthogonal to mmh generates a different partition of the support points of mmh , e.g. , the point highlighted in red in ( b ) is grouped with the lower three by the dotted line but with the upper two by the solid line , the mmh . if an orthogonal hyperplane _ can _ generate the same partition ( c ) , then a larger margin hyperplane than the proposed mmh exists ( d ) .     +   + proposed mmh *  * , orthogonal hyperplane * - - - * , hyperplane with larger + margin , regular points @xmath333 , support points @xmath334 , + differently assigned support point    [ lem : mmhorthog ]    suppose there is a unique hyperplane in @xmath46 with maximum margin , which can be parameterised by @xmath141 .",
    "let @xmath335 , @xmath336 and @xmath337 .",
    "then , @xmath338 , @xmath339 either @xmath340 , or @xmath341 .",
    "suppose the result does not hold , then @xmath342 with @xmath343 and @xmath344 and @xmath345 .",
    "let @xmath346 .",
    "define @xmath347 .",
    "define @xmath348 and @xmath349 . by construction @xmath350 . for any @xmath351 we have ,",
    "@xmath352 similarly one can show that @xmath353 for any @xmath354 , meaning that @xmath355 achieves a larger margin on @xmath356 and @xmath357 than @xmath155 , a contradiction .",
    "the next lemma uses the above result to provide an upper bound on the distance between pairs of support points projected onto any vector , in terms of the angle between that vector and @xmath358 .",
    "[ lm:2mv ]    suppose there is a unique hyperplane in @xmath46 with maximum margin , which can be parameterised by @xmath141 .",
    "define @xmath359 , @xmath360 , and @xmath361 .",
    "there is no vector @xmath362 for which @xmath363 for all pairs @xmath364 .",
    "suppose such a vector exists .",
    "define @xmath365 . by construction @xmath366 . for any pair",
    "@xmath364 we have @xmath367 define @xmath368 . then @xmath369 and @xmath370 , a contradiction .",
    "we are now in a position to provide the main proof of this appendix .",
    "the theorem states that if the maximum margin hyperplane is unique , and can be parameterised by @xmath371 , then @xmath372 where @xmath373 is any collection of minimum density hyperplanes indexed by their bandwidth @xmath201 .",
    "[ [ proof - of - theoremthmconvergence ] ] proof of theorem  [ thm : convergence ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    define @xmath335 , @xmath336 and @xmath374 .",
    "let @xmath375 .",
    "take any @xmath376 and set @xmath377 to satisfy @xmath378 . now",
    ", suppose @xmath379 satisfies , @xmath380 by lemma [ lm:2mv ] we know that @xmath381 s.t .",
    "thus @xmath383 thus @xmath384 .",
    "now , for each @xmath385 and for each @xmath386 .",
    "thus for any such @xmath387 we have , @xmath388 we can now bound the distance between @xmath389 and @xmath390 , @xmath391    we have shown that for any hyperplane @xmath392 that achieves a margin larger than @xmath167 on the support points of the maximum margin hyperplane , @xmath393 , the distance between @xmath389 and @xmath390 is less than @xmath394",
    ". equivalently , any hyperplane @xmath395 such that @xmath396 has a margin less than @xmath167 , as @xmath397 . by symmetry ,",
    "the same holds for any @xmath389 within distance @xmath394 of @xmath398 .    by lemma [ lm : samedivision ]",
    "@xmath399 such that for all @xmath400 , the minimum density hyperplane for @xmath17 , @xmath401 , induces the same partition of @xmath6 as the maximum margin hyperplane , @xmath143 .",
    "by lemma [ lm : bddmargin ] @xmath402 such that for all @xmath403 , @xmath404 .",
    "therefore for @xmath405 , @xmath406 .",
    "since @xmath376 was arbitrarily chosen , this gives the result .",
    "+   + define @xmath408 . we will show that @xmath409 is open and dense in @xmath410 . to begin with",
    "observe that @xmath241 is finite for all @xmath231 since @xmath227 varies continuously in @xmath18 and @xmath6 is finite .",
    "the upper semi - continuity of @xmath241 therefore ensures that @xmath409 is open . to show that @xmath409 is dense , take @xmath411 and @xmath376 s.t .",
    "@xmath412 . notice that we can assume @xmath413 contains at least @xmath414 distinct points since this occurs on a dense subset of @xmath224 , and so if it does not hold we can choose @xmath415 s.t .",
    "the data projected onto @xmath416 contain at least @xmath414 distinct points and continue as below , replacing @xmath231 by @xmath417 and @xmath394 by @xmath418 .",
    "for @xmath419 define @xmath420 .",
    "we can decompose @xmath421 via the chain rule decomposition , @xmath422 , where @xmath423 has @xmath424-th element equal to the partial derivative of the value of @xmath227 with respect to the @xmath424-th projection , @xmath425 , @xmath426 has @xmath424-th row equal to @xmath427 and @xmath428 is given by @xmath429 now , by sylvester s inequality we have , @xmath430 since @xmath411 we know that @xmath431 and @xmath432 and @xmath433 , and therefore rank@xmath434 . therefore rank@xmath435 rank@xmath436 .",
    "in addition , since int@xmath437 we must have rank@xmath438 , and so the equation @xmath439 , where @xmath440 , is uniquely determined .",
    "notice that we lose no generality assuming that the data have zero mean .",
    "consider then that for @xmath419 the partial derivative of @xmath227 with respect to a single projected point , say @xmath441 , satisfies @xmath442 } 2\\frac{l}{\\eta}(\\alpha\\sigma_{\\v } - b)\\frac{\\alpha}{n\\sigma_{\\v}}p\\\\ & + \\mathbb{i}_{[b > \\alpha \\sigma_{\\v } ] } 2\\frac{l}{\\eta}(\\alpha\\sigma_{\\v } + b)\\frac{\\alpha}{n\\sigma_{\\v}}p,\\end{aligned}\\ ] ] where @xmath443}$ ] is the indicator function .",
    "for two distinct @xmath444 we therefore have the difference between their partial derivatives with respect to a single projection being expressible in the form @xmath445 where @xmath446 is a linear function of @xmath441 .",
    "it can be verified that this function has at most 7 roots . since @xmath447 contains at least 8 distinct points we therefore know that @xmath448 .",
    "therefore by above we have @xmath449 .",
    "let @xmath450 , so that for any @xmath419 we have , @xmath451 where @xmath452 is the directional derivative of @xmath453 at @xmath231 and in the direction @xmath236 .",
    "for any @xmath454 we therefore have @xmath455 and by the finiteness of @xmath241 and the continuity of @xmath456 as a function of @xmath18 we therefore know @xmath145 s.t . for any @xmath457",
    "we have @xmath458 moreover we can choose @xmath166 and @xmath459 to be small enough that the following all hold :      we must therefore have that @xmath465 is a singleton , i.e. , @xmath466 .",
    "in addition @xmath464 and since @xmath376 was arbitrary , we must have @xmath409 is dense in @xmath410 .",
    "this proves the result .",
    "@xmath407          d.  arthur and s.  vassilvitskii .",
    "@xmath289-means++ : the advantages of careful seeding . in",
    "_ proceedings of the eighteenth annual acm - siam symposium on discrete algorithms _ , soda 07 , pages 10271035 , 2007 .",
    "s.  ben - david , t.  lu , d.  pl , and m.  sotkov .",
    "learning low - density separators . in d.",
    "van dyk and m.  welling , editors , _ proceedings of the 12th international conference on artificial intelligence and statistics ( aistats ) _ , jmlr workshop and conference proceedings , pages 2532 , 2009 .",
    "v.  castelli and t.  m. cover .",
    "the relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter .",
    "_ ieee transactions on information theory _ , 420 ( 6):0 21022117 , 1996 .    o.  chapelle and a.  zien .",
    "semi - supervised classification by low density separation . in r.",
    "g. cowell and z.  ghahramani , editors , _ proceedings of the international conference on artificial intelligence and statistics ( aistats ) _ , pages 5764 , 2005 .          c. ding and t. li .",
    "adaptive dimension reduction using discriminant analysis and k - means clustering . in _ proceedings of the 24th international conference on machine learning _ , icml 07 , pages 521528 , 2007 .",
    "m.  ji , t.  yang , b.  lin , r.  jin , and j.  han . a simple algorithm for semi - supervised learning with improved generalization error bound . in j. langford and j. pineau , editors , _ proceedings of the 29th international conference on machine learning ( icml-12 ) _ , pages 12231230 , 2012 .",
    "v.  i. morariu , b.  v. srinivasan , v.  c. raykar , r.  duraiswami , and l.  s. davis .",
    "automatic online tuning for fast gaussian summation .",
    "in _ advances in neural information processing systems ( nips ) _ , pages 11131120 , 2008 .",
    "a.  ng , m.  jordan , and y.  weiss .",
    "on spectral clustering : analysis and an algorithm . in t.",
    "dietterich , s.  becker , and z.  ghahramani , editors , _ advances in neural information processing systems 14 _ , volume  14 , pages 849856 , 2002 .",
    "a.  rosenberg and j.  hirschberg .",
    "-measure : a conditional entropy - based external cluster evaluation measure . in _ empirical methods in natural language processing and computational natural language learning _ , volume  7 , pages 410420 , 2007 .",
    "a. singh , r.  d. nowak , and x. zhu .",
    "unlabeled data : now it helps , now it does nt . in d.  koller , d.  schuurmans , y.  bengio , and l.  bottou , editors , _ advances in neural information processing systems 21 _ , pages 15131520 , 2009 .",
    "l.  xu , j.  neufeld , b.  larson , and d.  schuurmans . maximum margin clustering . in l.",
    "k. saul , y.  weiss , and l.  bottou , editors , _ advances in neural information processing systems _ , volume  17 , pages 15371544 , 2004 ."
  ],
  "abstract_text": [
    "<S> associating distinct groups of objects ( clusters ) with contiguous regions of high probability density ( high - density clusters ) , is central to many statistical and machine learning approaches to the classification of unlabelled data . </S>",
    "<S> we propose a novel hyperplane classifier for clustering and semi - supervised classification which is motivated by this objective . </S>",
    "<S> the proposed _ </S>",
    "<S> minimum density hyperplane _ minimises the integral of the empirical probability density function along it , thereby avoiding intersection with high density clusters . we show that the minimum density and the maximum margin hyperplanes are asymptotically equivalent , thus linking this approach to maximum margin clustering and semi - supervised support vector classifiers . </S>",
    "<S> we propose a projection pursuit formulation of the associated optimisation problem which allows us to find minimum density hyperplanes efficiently in practice , and evaluate its performance on a range of benchmark datasets . </S>",
    "<S> the proposed approach is found to be very competitive with state of the art methods for clustering and semi - supervised classification .    </S>",
    "<S> low - density separation , high - density clusters , unsupervised classification , semi - supervised classification , projection pursuit </S>"
  ]
}