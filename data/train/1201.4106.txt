{
  "article_text": [
    "in physics  the invention of the laser , low - loss optical fiber , and the optical amplifier  have driven the exponential growth in worldwide data communications . however , as these technologies mature , system designers have increasingly focused on techniques from communication theory , including forward error correction , to simultaneously increase transmission capacity and decrease transmission costs .",
    "one of the first proposals for fec in an optical system appeared in  @xcite , which demonstrated a shortened @xmath5 hamming code implementation at 565 mbit / s . since then , itu - t recommendations g.975 and g.975.1 have standardized more powerful codes for optical transport networks ( otns ) . more recently , low - density parity - check ( ldpc ) codes  @xcite  which provide the potential for capacity - approaching performance  have been investigated , as aptly summarized in  @xcite .",
    "while implementations exists at 10 gb / s ( for 10gbase - t ethernet networks ) , the blocklengths of such implementations ( @xmath6@xmath7 ) are too short to provide performance close to capacity ; the @xmath8 rs - ldpc code is approximately 3 db from the shannon limit at @xmath2  @xcite , see also  @xcite .",
    "another significant roadblock is that fiber - optic communication systems are typically required to provide bit - error - rates below @xmath2 .",
    "it is well - known that capacity - approaching ldpc codes exhibit error floors  @xcite , and to achieve the targeted error rate would likely require concatenation with an outer code ( e.g. , as in  @xcite ) .",
    "in this work , we focus on product - like codes ( by product - like codes , we mean any generalized ldpc code with algebraic component codes ) , since they possess properties that make them particularly suited to providing error - correction in fiber - optic communication systems .",
    "in particular , for 100 gb / s implementations , we argue that syndrome - based decoding of product - like codes is significantly more efficient than message - passing decoding of ldpc codes .",
    "this paper presents a new class of high - rate binary error - correcting codes  staircase codes  whose construction combines ideas from convolutional and block coding .",
    "indeed , staircase codes can be interpreted as having a ` continuous ' product - like construction . in the context of wireless communications ,",
    "related code constructions include braided block codes  @xcite , braided convolutional codes  @xcite , diamond codes  @xcite and cross parity check convolutional codes  @xcite , each of which is related to the recurrent codes of wyner - ash  @xcite . however , these proposals considered soft decoding of the component codes , which is unsuitable for high - speed fiber - optic communications .",
    "herein , we describe a syndrome - based decoder for staircase codes , that provides excellent performance with an efficient decoder implementation .",
    "in section  [ sec : exist ] , we review the specifications and performance of fec codes defined in itu - t recommendations g.975 and g.975.1 . in section",
    "[ sec : vs ] , we describe the syndrome - based decoder for product - like codes , and argue that it results in a decoder data - flow that is more than two orders of magnitude smaller than the message - passing decoder of an ldpc code .",
    "staircase codes are presented in section  [ sec : sc ] , and a g.709-compatible staircase code is proposed . in section  [ sec : efloor ] , we present an analytical method for determining the error floor of iteratively decoded staircase codes , and show that the proposed staircase code has an error floor at @xmath9 .",
    "finally , in section  [ sec : sim ] , we present fpga - based simulation results , illustrating that the proposed code provides a @xmath1  db ncg at an output error rate of @xmath2 , an improvement of @xmath3  db relative to the best code from the itu - t g.975.1 recommendation , and only @xmath10 db from the shannon limit .",
    "the first error - correction code standardized for optical communications was the @xmath11 reed - solomon code , with symbols in @xmath12 , capable of correcting up to @xmath13 symbol errors in any codeword . for an output - error - rate of @xmath2 ,",
    "the ncg of the rs code is @xmath14 db , which is @xmath15 db from capacity .    in order to provide",
    "improved burst - error - correction , 16 codewords are block - interleaved , providing correction for bursts of as many as 1024 transmitted bits .",
    "a framing row consists of @xmath16 bits , 30592 of which are information bits , and the remaining 2048 bits of which are parity .",
    "the resulting framing structure  a frame consists of four rows  is standardized in itu - t recommendation g.709 , and remains the required framing structure for otns ; as a direct result , the coding rate of any candidate code must be @xmath0 .",
    "as per - channel data rates increased to @xmath17 gb / s , and the capabilities of high - speed electronics improved , the @xmath11 rs code was replaced with stronger error - correcting codes . in itu - t recommendation g.975.1 , several ` next - generation ' coding schemes were proposed ; among the many proposals , the common mechanism for increased coding gain was the use of concatenated coding schemes with iterative hard - decision decoding .",
    "we now describe four of the best proposals , which will motivate our approach in section  [ sec : sc ] .    in appendix i.3 of g.975.1",
    ", a serially concatenated coding scheme is described , with outer @xmath18 binary bch code and inner @xmath19 binary bch code , which are obtained by shortening their respective mother codes .",
    "first , @xmath20 information bits are divided into 8 units , each of which is encoded by the outer code ; we will refer to the resulting unit of @xmath21 bits as a ` block ' .",
    "prior to encoding by the inner code , the contents of consecutive blocks are interleaved ( in a ` continuous ' fashion , similar to convolutional interleavers  @xcite ) .",
    "specifically , each inner codeword in a given block involves ` information ' bits from each of the eight preceding ` outer ' blocks .",
    "note that the interleaving step increases the effective block - length of the overall code , but it necessitates a sliding - window style decoding algorithm , due to the continuous nature of the interleaver .",
    "furthermore , unlike a product code , the parity bits of the inner code are protected by a single component codeword , which reduces their level of protection . for an output - error - rate of @xmath2 ,",
    "the ncg of the i.3 code is @xmath22 db , which is @xmath23 db from capacity .    in appendix i.4 of g.975.1 , a serially concatenated scheme with ( shortened versions of ) an outer @xmath24 rs code and ( shortened versions of ) an inner @xmath25 binary bch code",
    "is proposed .",
    "after encoding @xmath26 bits with the outer code , the coded bits are block interleaved and encoded by the inner bch code , resulting in a block length of @xmath27 bits , i.e. , exactly one g.709 frame . as in the previous case ,",
    "the parity bits of the inner code are singly - protected . for an output - error - rate of @xmath2 , the ncg of the i.4 code is @xmath28 db , which is @xmath29 db from capacity .    in appendix i.5 of g.975.1 , a serially concatenated scheme with an outer @xmath30 rs code and an inner @xmath31 extended - hamming product code is described .",
    "iterative decoding is applied to the inner product code , after which the outer code is decoded ; the purpose of the outer code is to eliminate the error floor of the inner code , since the inner code has small stall patterns ( see section  [ sec : efloor ] ) . for an output - error - rate of @xmath2 , the ncg of the i.5 code is @xmath32 db , which is @xmath33 db from capacity .",
    "finally , in appendix i.9 of g.975.1 , a product - like code with @xmath34 doubly - extended binary bch component codes is proposed .",
    "the overall code is described in terms of a @xmath35 matrix of bits , in which the bits along both the rows of the matrix as well as a particular choice of ` diagonals ' must form valid codewords in the component code . since the diagonals are chosen to include 2 bits in every row , any diagonal codeword has two bits in common with any row codeword ; in contrast , for a product code , any row and column have exactly one bit in common .",
    "note that the i.9 construction achieves a product - like construction ( their choice of diagonals ensures that each bit _ is _ protected by two component codewords ) with essentially half the overall block length of the related product code ( even so , the i.9 code has the longest block length among all g.975.1 proposals ) .",
    "however , the choice of diagonals decreases the size of the smallest stall patterns , introducing an error floor above @xmath36 . for an output - error - rate of @xmath37 ,",
    "the ncg of the i.9 code is @xmath28 db , which is @xmath29 db from capacity .",
    "in this section , we present a high - level view of iterative decoders for ldpc and product codes . due to the differences in their implementations ,",
    "a precise comparison of their implementation complexities is difficult . nevertheless , since the communication complexity of message - passing is a significant challenge in ldpc decoder design , we consider the decoder data - flow , i.e. , the rate of routing / storing messages , as a surrogate for the implementation complexity .",
    "we consider a system that transmits _ information _ at @xmath38 bits / s , using a binary error - correcting code of rate @xmath39for which _ hard _ decisions at @xmath40 bits / s are input to the decoder  and a decoder that operates at a clock frequency @xmath41 hz .",
    "we consider an ldpc decoder that implements sum - product decoding ( or some quantized approximation ) with a parallel - flooding schedule .",
    "we assume @xmath42-bit messages internal to the decoder , an average variable node degree @xmath43 , and @xmath44 decoder iterations ; typically , @xmath42 is 4 or 5 bits , @xmath45 , and @xmath46 .",
    "initially , hard - decisions are input to the decoder at a rate of @xmath40 bits / s and stored in flip - flop registers . at each iteration ,",
    "variable nodes compute and broadcast @xmath42-bit messages over every edge , and similarly for the check nodes , i.e. , @xmath47 bits are broadcast per iteration per variable node .",
    "since bits arrive from the channel at @xmath40 bits / s , the corresponding internal data - flow per iteration is then @xmath48 , and the total data - flow , including initial loading of 1-bit channel messages , is    rcl f_ldpc&=&+ + & & .    for @xmath49 , @xmath50 , @xmath51 , @xmath52 , which corresponds to a data - flow of more than @xmath53 tb / s for @xmath54 gb / s systems .",
    "when the component codes of a product code can be efficiently decoded via syndromes ( e.g. , bch codes ) , there exists an especially efficient decoder for the product code .",
    "briefly , by operating exclusively in the ` syndrome domain'which compresses the received signal  and passing only @xmath55 messages per ( component ) decoding ( for @xmath56-error - correcting component codes ) , the implementation complexity of decoding is significantly reduced .",
    "the following is a step - by - step description of the decoding algorithm :    1 .   from the received data ,",
    "compute and store the syndrome for each row and column codeword .",
    "store a copy of the received data in memory @xmath39 .",
    "2 .   decode those non - zero syndromes corresponding to row codewords . in the event of a successful decoding , set the syndrome to zero , flip the corresponding @xmath56 or fewer positions in memory @xmath39 , and update the @xmath56 or fewer affected column syndromes by a masking operation .",
    "repeat step 2 , reversing the roles of rows and columns .",
    "4 .   if any syndromes are non - zero , and fewer than the maximum number of iterations have been performed , go to step 2 . otherwise ,",
    "output the contents of memory @xmath39 .",
    "we quantify the complexity of decoding a product code by its decoder data - flow . at first glance",
    ", it may seem that this approach ignores the complexity of decoding the ( component ) @xmath56-error - correcting bch codewords .",
    "however , for relatively small @xmath56 , the decoding of a component codeword can be efficiently decomposed into a series of look - up table operations , for which the data - flow interpretation is well - justified . in this section , we will ignore the data - flow contribution of the bch decoding algorithm , but we return to this point in the appendix , where it is shown that the corresponding data - flow is negligible .",
    "we assume that rows are encoded by a @xmath57-error - correcting @xmath58 bch code , and the columns are encoded by a @xmath59-error - correcting @xmath60 bch code , for an overall rate @xmath61 .",
    "we assume each row / column codeword is decoded ( on average , over the course of decoding the overall product code ) @xmath62 times , where typically @xmath62 ranges from 3 to 4 .",
    "the hard - decisions from the channel  at d / r bits / s  are written to a data ram , in addition to being processed by a syndrome computation / storage device .",
    "contrary to the ldpc decoder data - flow , the clock frequency @xmath41 plays a central role , namely in the data - flow of the initial syndrome calculation .",
    "referring to fig .",
    "[ fig : dflowsyn ] , and assuming that the bits in a product code are transmitted row - by - row , the input bus - width ( i.e. , the number of input bits per decoder clock cycle ) is @xmath63 bits . now , assuming these bits correspond to a single row of the product code , each non - zero bit corresponds to some @xmath64-bit mask ( i.e. , the corresponding column of the parity - check matrix of the row code ) , the modulo-2 sum of these is performed by a masking tree , and the @xmath64-bit output is masked with the current contents of the corresponding ( syndrome ) flip - flop register .",
    "that is , each clock cycle causes a @xmath64-bit mask to be added to the contents of the corresponding row in the syndrome bank .",
    "of course , each received bit also impacts a distinct column syndrome , however , the _ same _ @xmath65-bit mask is applied ( when the corresponding received bit is non - zero ) to each of the involved column syndromes ; the corresponding data - flow is then @xmath65 bits per clock cycle .        once the syndromes are computed from the received data , iterative decoding commences . to perform a row decoding ,",
    "an @xmath64-bit syndrome is read from the syndrome bank .",
    "since there are @xmath66 row codewords , and each row is decoded on average @xmath62 times , the corresponding data - flow from the syndrome bank to the row decoder is @xmath67 bits / s . for each row decoding , at most @xmath57 positions",
    "are corrected , each of which is specified by @xmath68 bits .",
    "therefore , the data - flow from the row decoder to the data ram is @xmath69 bits / s .",
    "furthermore , for each corrected bit , a @xmath65-bit mask must be applied to the corresponding column syndrome , which yields a data - flow from the row decoder to the syndrome bank of @xmath70 bits / s .",
    "a similar analysis can be applied to column decodings . in total , the decoder data - flow is    rcl f_p&=&+(r_1+r_2)f_c + & & + ( t_1 _ 2 n_1 + t_1 _ 2 n_2 + r_1+t_1r_2 ) + & & + ( t_2 _ 2 n_1 + t_2 _ 2 n_2 + r_2+t_2r_1 ) .        in this work",
    ", we will focus on codes for which @xmath71 , @xmath72 , @xmath73 , and the decoder is assumed to operate at @xmath74  mhz . for @xmath75",
    ", we then have a data - flow of approximately 293 gb / s . note",
    "that this is more than two orders of magnitude smaller than the corresponding data - flow for ldpc decoding .",
    "intuitively , the advantage arises from two facts .",
    "first , when @xmath76 and @xmath77 , syndromes provide a compressed representation of the received signal .",
    "second , the algebraic component codes admit an economical message - passing scheme , in the sense that message updates are only required for the small fraction of bits that are corrected by a particular ( component code ) decoding .",
    "the staircase code construction combines ideas from recursive convolutional coding and block coding .",
    "staircase codes are completely characterized by the relationship between successive matrices of symbols .",
    "specifically , consider the ( infinite ) sequence @xmath78 of @xmath79-by-@xmath79 matrices @xmath80 , @xmath81 .",
    "herein , we restrict our attention to @xmath80 with elements in @xmath82 , but an analogous construction applies in the non - binary case .",
    "block @xmath83 is initialized to a reference state known to the encoder - decoder pair , e.g. , block @xmath83 could be initialized to the all - zeros state , i.e. , an @xmath79-by-@xmath79 array of zero symbols .",
    "furthermore , we select a conventional fec code ( e.g. , hamming , bch , reed - solomon , etc . ) in systematic form to serve as the _ component _ code ; this code , which we henceforth refer to as @xmath84 , is selected to have block length @xmath85 symbols , @xmath86 of which are parity symbols .",
    "encoding proceeds recursively on the @xmath80 . for each @xmath87 , @xmath88 information symbols ( from the streaming source ) are arranged into the @xmath89 leftmost columns of @xmath80 ; we denote this sub - matrix by @xmath90 .",
    "then , the entries of the rightmost @xmath86 columns ( this sub - matrix is denoted by @xmath91 ) are specified as follows :    1 .   form the @xmath92 matrix , @xmath93 $ ] , where @xmath94 is the matrix - transpose of @xmath95 .",
    "the entries of @xmath91 are then computed such that each of the rows of the matrix @xmath96 $ ] is a valid codeword of @xmath84 .",
    "that is , the elements in the @xmath97th row of @xmath91 are exactly the @xmath86 parity symbols that result from encoding the @xmath98 ` information ' symbols in the @xmath97th row of @xmath99 .    generally , the relationship between successive blocks in a staircase code satisfies the following relation : for any @xmath100 , each of the rows of the matrix @xmath101 $ ] is a valid codeword in @xmath84 .",
    "an equivalent description  from which the term ` staircase codes ' originates  is suggested by fig .",
    "[ fig : sc ] , in which ( the concatenation of the symbols in ) every row ( and every column ) in the ` staircase ' is a valid codeword of @xmath84 ; this representation suggests their connection to product codes .",
    "however , staircase codes are naturally unterminated ( i.e. , their block length is indeterminate ) , and thus admit a range of decoding strategies with varying latencies .",
    "most importantly , we will see that they outperform product codes .",
    "the rate of a staircase code is @xmath102 since encoding produces @xmath86 parity symbols for each set of @xmath89 ` new ' information symbols .",
    "however , note that the related product code has rate    rcl r_p&= & ( ) ^2 + & = & 1-+ ,    which is greater than the rate of the staircase code . however , for sufficiently high rates , the difference is small , and staircase codes outperform product codes of the same rate .    from the context of transmitter latency  which includes encoding latency and frame - mapping latency  staircase codes",
    "have the advantage ( relative to product codes ) that the _ effective _ rate ( i.e. , the ratio of ` new ' information symbols , @xmath89 , to the total number of ` new ' symbols , @xmath79 ) of a component codeword is exactly the rate of the _ overall _ code .",
    "therefore , the encoder produces parity at a ` regular ' rate , which enables the design of a frame - mapper that minimizes the transmitter latency .",
    "we note that staircase codes can be interpreted as generalized ldpc codes with a _",
    "systematic _ encoder and an indeterminate block - length , which admits decoding algorithms with a range of latencies .    using arguments analogous to those used for product codes , a @xmath56-error - correcting component code @xmath84 with minimum distance",
    "@xmath103 has a hamming distance between any two staircase codewords that is at least @xmath104 .",
    "staircase codes are naturally unterminated ( i.e. , their block length is indeterminate ) , and thus admit a range of decoding strategies with varying latencies .",
    "that is , decoding can be accomplished in a sliding - window fashion , in which the decoder operates on the received bits corresponding to @xmath105 consecutively received blocks @xmath106 . for a fixed @xmath87 , the decoder iteratively decodes as follows : first , those component codewords that ` terminate ' in block @xmath107 ( i.e. , whose parity bits are in @xmath107 ) are decoded ; since every symbol is involved in two component codewords , the corresponding syndrome updates are performed , as in section  [ sec : proddec ] .",
    "next , those codewords that terminate in block @xmath108 are decoded .",
    "this process continues until those codewords that terminate in block @xmath80 are decoded .",
    "now , since decoding those codewords terminating in some block @xmath109 affects those codewords that terminate in block @xmath110 , it is beneficial to return to @xmath107 and to repeat the process .",
    "this iterative process continues until some maximum number of iterations is performed , at which time the decoder outputs its estimate for the contents of @xmath80 , accepts in a new block @xmath111 , and the entire process repeats ( i.e. , the decoding window slides one block to the ` right ' ) .      staircase codes have a simple graphical representation , which provides a multi - edge - type  @xcite interpretation of their construction .",
    "the term ` multi - edge - type ' was originally applied to describe a refined class of irregular ldpc codes , in which variable nodes ( and check nodes ) are classified by their degrees with respect to a set of edge types .",
    "intuitively , the introduction of multiple edge types allows degree - one variable nodes , punctured variable nodes , and other beneficial features that are not admitted by the conventional irregular ensemble . in turn , better performance for finite blocklengths and fixed decoding complexities is possible .    in fig .",
    "[ fig : fg ] , we present the factor graph representation of a decoder that operates on a window of @xmath112 blocks ; the graph for general @xmath105 follows in an obvious way .",
    "dotted variable nodes indicate symbols whose value was decoded in the previous stage of decoding .",
    "the key observation is that when these symbols are correctly decoded  which is essentially always the case , since the output ber is required to be less than @xmath2the component codewords in which they are involved are effectively shortened by @xmath79 symbols .",
    "therefore , the most reliable messages are passed over those edges connecting variable nodes to the shortened ( component ) codewords , as indicated in fig .",
    "[ fig : fg ] . on the other hand ,",
    "the rightmost collection of variable nodes are ( with respect to the current decoding window ) only involved in a single component codeword , and thus the edges to which they are connected carry the least reliable messages .",
    "due to the nature of iterative decoding , the intermediate edges carry messages whose reliability lies between these two extremes .     is a standard block interleaver , i.e. , it represents the transpose operation on an @xmath79-by-@xmath79 matrix . ]",
    "the itu - t recommendation g.709 defines the framing structure and error - correcting coding rate for otns . for our purposes",
    ", it suffices to know that an optical frame consists of @xmath27 bits , @xmath26 of which are information bits , and the remaining @xmath113 are parity bits , which corresponds to error - correcting codes of rate @xmath0 . since @xmath114",
    ", we will consider a component code with @xmath115 and @xmath116 .",
    "specifically , the binary @xmath117 bch code with generator polynomial @xmath118 is adapted to provide an additional 2-bit error - detecting mechanism , resulting in the generator polynomial    rcl g(x)&=&(x^10+x^3 + 1)(x^10+x^3+x^2+x+1 ) + & & ( x^10+x^8+x^3+x^2 + 1)(x^2 + 1 ) .    in order to provide a simple mapping to the g.709 frame",
    ", we first note that @xmath119 .",
    "this leads us to define a slight generalization of staircase codes , in which the blocks @xmath80 consist of @xmath120 rows of @xmath121 bits .",
    "the encoding rule is modified as follows :    1 .",
    "form the @xmath122 matrix , @xmath123 $ ] , where @xmath124 is obtained by appending two all - zero rows to the top of the matrix - transpose of @xmath95 .",
    "the entries of @xmath91 are then computed such that each of the rows of the matrix @xmath96 $ ] is a valid codeword of @xmath84 .",
    "that is , the elements in the @xmath97th row of @xmath91 are exactly the @xmath125 parity symbols that result from encoding the @xmath126 ` information ' symbols in the @xmath97th row of @xmath99 .    here , @xmath84 is the code obtained by shortening the code generated by @xmath127 by one bit , since our overall codeword length is @xmath128 .",
    "for iteratively decoded codes , an error floor ( in the output bit - error - rate ) can often be attributed to error patterns that ` confuse ' the decoder , even though such error patterns could easily be corrected by a maximum - likelihood decoder . in the context of ldpc codes",
    ", these error patterns are often referred to as trapping sets  @xcite . in the case of product - like codes with an iterative hard - decision decoding algorithm",
    ", we will refer to them as _ stall patterns _ , due to the fact that the decoder gets locked in a state in which no updates are performed , i.e. , the decoder stalls , as in fig .",
    "[ fig : st ] .",
    "a stall pattern is a set @xmath129 of codeword positions , for which every row and column involving positions in @xmath129 has at least @xmath130 positions in @xmath129 .",
    "we note that this definition includes stall patterns that _ are _ correctable , since an _ incorrect _ decoding may fortuitously cause one or more bits in @xmath129 to be corrected , which could then lead to all bits in @xmath129 eventually being corrected . in this section ,",
    "we obtain an estimate for the error floor by over - bounding the probabilities of these events , and pessimistically assuming that every stall pattern is uncorrectable ( i.e. , if any stall pattern appears during the course of decoding , it will appear in the final output ) .",
    "the methods presented for the error floor analysis apply to a general staircase code , but for simplicity of the presentation , we will focus on a staircase code with @xmath115 and doubly - extended triple - error - correcting component codes .          due to the streaming nature of staircase codes , it is necessary to account for stall patterns that span ( possibly multiple ) consecutive blocks . in order to determine the bit - error - rate due to stall patterns",
    ", we consider a fixed block @xmath80 , and the set of stall patterns that include positions in @xmath80 .",
    "specifically , we ` assign ' to @xmath80 those stall patterns that include symbols in @xmath80 ( and possibly additional positions in @xmath131 ) but no symbols in @xmath95 .",
    "let @xmath132 represent the set of stall patterns assigned to @xmath80 . by",
    "the union bound , we then have @xmath133\\cdot \\frac{|s|}{510 ^ 2}.\\ ] ] therefore , bounding the error floor amounts to enumerating the set @xmath132 , and evaluating the probabilities of its elements being in error .",
    "a minimal stall pattern has the property that there are only @xmath130 rows with positions in @xmath129 , and only @xmath130 columns with positions in @xmath129 .",
    "the minimal stall patterns of a staircase code can be counted in a straightforward manner ; the multiplicity of minimal stall patterns that are assigned to @xmath80 is @xmath134 and we refer to the set of minimal stall patterns by @xmath135 .",
    "the probability that the positions in some minimal stall pattern @xmath129 are _ received _ in error is @xmath136 .",
    "next , we consider the case in which not all positions in some minimal stall pattern @xmath129 are received in error , but that due to incorrect decoding(s ) , all positions in @xmath129 are  at some point during decoding  simultaneously in error .",
    "for some fixed @xmath129 and @xmath137 , @xmath138 , there are @xmath139 ways in which @xmath140 positions in @xmath129 can be received in error . for the moment , let s assume that erroneous bit flips occur independently with some probability @xmath141 , and that @xmath141 does not depend on @xmath137 . then we can _ overbound _ the probability that a _",
    "minimal stall @xmath129 occurs by @xmath142    in order to provide evidence in favor of these assumptions , table  [ tab : stall ] presents empirical estimates , for @xmath143 , @xmath144 and @xmath145 , of the probability that a minimal stall pattern @xmath129 occurs during iterative decoding , given that @xmath140 positions in @xmath129 are ( intentionally ) received in error .",
    "note that even if a minimal stall is received , there exists a non - zero probability that it will be corrected as a result of erroneous decodings ; we will ignore this effect in our estimation , i.e. , we make the worst - case assumption that any minimal stall persists . furthermore , from the results for @xmath144 and @xmath145 , it appears that our stated assumptions regarding @xmath141 hold true , and @xmath146 . for @xmath147 , we did not have access to sufficient computational resources for estimating the corresponding probabilities .",
    "nevertheless , based on the evidence presented in table  [ tab : stall ] , the error floor contribution due to minimal stall patterns is estimated as @xmath148 where @xmath149 when @xmath150 .",
    ".estimated probability of a minimal stall @xmath129 , given that @xmath140 positions are received in error [ cols=\"<,^\",options=\"header \" , ]     note that the dominant contribution to the error floor is due to minimal stall patterns ( i.e. , @xmath151 ) , and that the overall estimate for the error floor of the code is @xmath152 .",
    "finally , we note that by a similar ( but more cumbersome ) analysis , the error floor of the g.709-compliant staircase code is estimated to occur at @xmath9 .",
    "in fig .  [ fig : ber ] , simulation results  generated in hardware on an fpga implementation  are provided for the g.709-compatible staircase code , for @xmath153 .",
    "we also present the bit - error - rate curves for the g.975 rs code , as well as the g.975.1 codes described in section  [ sec : exist ] . for an output error rate @xmath2 , the staircase code provides",
    "approximately @xmath1 db net coding gain , which is within @xmath10 db of the shannon limit , and an improvement of @xmath3 db relative to the best g.975.1 code .",
    "staircase code on a binary symmetric channel with crossover probability @xmath154 , compared with various g.975.1 codes .",
    "the upper scale plots the equivalent binary - input gaussian channel @xmath155 ( in db ) , where @xmath156 . ]",
    "we proposed staircase codes , a class of product - like fec codes that provide reliable communication for streaming sources .",
    "their construction admits low - latency encoding and variable - latency decoding , and a decoding algorithm with an efficient hardware implementation . for @xmath0 ,",
    "a g.709-compatible staircase code was presented , and performance within @xmath10  db of the shannon limit at @xmath2 was provided via an fpga - based simulation .",
    "this section briefly describes known techniques for efficiently decoding triple - error - correcting binary bch codes , and discusses the data - flow associated with a lookup - table - based decoder architecture .    for a syndrome @xmath157 , @xmath158 ,",
    "we first compute @xmath159 and @xmath160 .",
    "a triple - error correcting decoder distinguishes the cases @xmath161 where @xmath62 is the number of positions to invert in order to obtain a valid codeword .    in order to determine the corresponding positions ,",
    "a reciprocal error - locator polynomial @xmath162 is defined , the roots of which identify the positions . from  @xcite , we have : @xmath163 where @xmath164 when @xmath165 , note that all of the coefficients of @xmath162 are nonzero .",
    "it remains to determine the roots of @xmath162 . for @xmath166",
    ", it is trivial to determine the error location . for @xmath167 or @xmath168 , lookup - based methods for solving the corresponding quadratic and cubic equations",
    "are described in @xcite . in the remainder of this section",
    ", we briefly describe these methods , and discuss their data - flow .    for a quadratic equation @xmath169 with @xmath170 , substitute @xmath171 to obtain @xmath172 if @xmath173 then @xmath174",
    ". thus the problem of finding roots of @xmath175 reduces to the problem of finding roots of the suppressed quadratic @xmath176 , which can be solved by lookup using a table with @xmath177 entries , each of which is a pair of elements in @xmath178 .",
    "therefore , when @xmath167 , decoding requires @xmath85 bits to be read from a lookup - table memory .",
    "note that @xmath182 is a linearized polynomial with respect to @xmath82 and hence the set of zeros of @xmath182 is a vector space over @xmath82 .",
    "in particular , the roots of @xmath183 , if distinct , are of the form @xmath184 .",
    "thus only @xmath64 and @xmath65 need to be stored in the lookup table .",
    "two cases arise , depending on the value of @xmath185 . if @xmath186 , so that @xmath187 , then @xmath188 , and the roots can be found by finding the cube roots of @xmath189 , which requires lookup using a table with @xmath177 entries , each of which is a pair of elements in @xmath178 . if @xmath190 , so that @xmath191 , substitute @xmath192 to obtain @xmath193 where @xmath194    the roots of the suppressed cubic @xmath195 can be found by lookup using a table with @xmath177 entries , each of which is a pair of elements in @xmath178 .",
    "therefore , in either case , decoding requires @xmath85 bits to be read from a lookup - table memory .",
    "finally , for @xmath196 , the data - flow contribution of the lookup - table - based decoding architecture is @xmath197 . for @xmath198 , @xmath199 , @xmath75 , @xmath0 and @xmath200 gb / s ,",
    "the corresponding data - flow is @xmath201 gb / s , which is small relative to the data - flow that arises due to those effects considered in section  [ sec : proddec ] .",
    "z.  zhang , v.  anantharam , m.  j. wainwright , and b.  nikolic , `` an efficient 10gbase - t ethernet ldpc decoder design with low error floors , '' _ ieee j. solid - state circuits _ , vol .  45 , no .  4 , pp",
    ". 843855 , apr ."
  ],
  "abstract_text": [
    "<S> staircase codes , a new class of forward - error - correction ( fec ) codes suitable for high - speed optical communications , are introduced . </S>",
    "<S> an itu - t g.709-compatible staircase code with rate @xmath0 is proposed , and fpga - based simulation results are presented , exhibiting a net coding gain ( ncg ) of @xmath1  db at an output error rate of @xmath2 , an improvement of @xmath3  db relative to the best code from the itu - t g.975.1 recommendation . an error floor analysis technique is presented , and the proposed code is shown to have an error floor at @xmath4 .    </S>",
    "<S> staircase codes , fiber - optic communications , forward error correction , product codes , low - density parity - check codes . </S>"
  ]
}