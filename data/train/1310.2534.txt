{
  "article_text": [
    "let @xmath0 be a sequence of random samples obtained from an unknown probability distribution @xmath1 .",
    "the corresponding random measure from @xmath2 samples is the monte carlo estimator of @xmath1 , @xmath3 where @xmath4 is the support of @xmath1 .",
    "the random measure is a maximum likelihood estimator of @xmath1 and is consistent : for all @xmath1-measurable sets @xmath5 , @xmath6    sometimes estimating the entire distribution @xmath1 is of intrinsic inferential interest .",
    "in other cases , this may be desirable if there are no limits on the functionals of @xmath1 which might be of future interest .",
    "alternatively , the random sampling might be an intermediary update of a sequential monte carlo sampler no for which it is desirable that the samples represent the current target distribution well at each step @xcite .",
    "pointwise monte carlo errors are inadequate for capturing the overall rate of convergence of the realised empirical measure @xmath7 to @xmath1 .",
    "this consideration is particularly relevant if @xmath1 is an infinite mixture of distributions of unbounded dimension : in this case it becomes necessary to specify a degenerate , fixed dimension function of interest before monte carlo error can be assessed @xcite .",
    "this necessity is potentially undesirable , since the assessment of convergence will vary depending on which function is selected and that choice might be somewhat arbitrary .",
    "the work presented here considers sampling multiple target distributions in parallel .",
    "this scenario is frequently encountered in real - time data processing , where streams of data pertaining to different statistical processes are collected and analysed in fixed time - window updates .",
    "decisions on how much sampling effort to allocate to each target will be made sequentially , based on the apparent relative complexity of the targets , as higher - dimensional , more complex targets intuitively need more samples to be well represented .",
    "the complexities of the targets will not be known _ a priori _ , but can be estimated from the samples which have been obtained so far . as a consequence , the size of the sample @xmath8 drawn from any particular target distribution will be a realisation of a random variable , @xmath9 , determined during sampling by a random stopping rule governed by the history of samples drawn from that target and those obtained from the other targets .    to extend the applicability of monte carlo error to entire probability measures , the following question is considered : if a new sample of random size @xmath9 were drawn from @xmath1 , how different to @xmath7 might the new empirical measure be ? if repeatedly drawing samples in this way led to relatively similar empirical measures , this suggests that the target is relatively well represented by @xmath9 samples ; whereas if the resulting empirical measures were very different , then there would be a stronger desire to obtain a ( stochastically ) larger number of samples . to formally address this question , a new _ monte carlo divergence error _",
    "is proposed to measure the expected distance between an empirical measure and its target .",
    "correctly balancing sample sizes is a non - trivial problem . apparently sensible , but _",
    "ad hoc _ , allocation strategies can lead to extremely poor performance , much worse than simply assigning the same number of samples to each target . here , a sample - based estimate of the proposed monte carlo divergence error of an empirical measure is derived ; these errors are combined across samplers through a loss function , leading to a fully - principled , sequential sample allocation strategy .",
    "section [ sec : mc_convergence ] formally defines and justifies monte carlo divergence as an error criterion .",
    "section [ sec : rival_samplers ] examines two different loss functions for combining sampler errors into a single performance score .",
    "section [ sec : other_methods ] introduces some alternative sample size selection strategies ; some are derived by adapting related ideas in the existing literature , and some are _ ad hoc_. the collection of strategies are compared on univariate and variable dimension target distributions in section [ sec : examples ] before a brief discussion in section [ sec : discussion ] .",
    "in this section the rate of convergence of the empirical distribution to the target will be assessed by information theoretic criteria . in information theory , it is common practice to discretise distributions of any continuous random variables ( see * ? ? ? * ) . without this discretisation ( or some alternative smoothing )",
    "the intersection of any two separately generated sets of samples would be empty , and distribution - free comparisons of their empirical measures would be rendered meaningless : for example , the kullback - leibler divergence between two independent realisations of will be always be infinite .",
    "when a target distribution relates to that of a continuous random variable , a common discretisation of both the empirical measure and notionally the target will be performed . for the rest of the article ,",
    "both @xmath10 and @xmath1 should be regarded as suitably discretised approximations to the true distributions when the underlying variables are continuous .",
    "when there are multiple distributions , the same discretisation will be used for all distributions . for univariate problems a large but finite grid with fixed spacing will be used to partition @xmath4 into bins ; for mixture problems with unbounded dimension , the same strategy will be used for each component of each dimension , implying an infinite number of bins . later in section [ sec : bin_width ] , consideration will be given to how the number of bins for each dimension should be chosen .",
    "for a discrete target probability distribution @xmath1 , let @xmath11 be the estimator for a prospective sample of size @xmath8 to be drawn from @xmath1 , and let @xmath12 be the same estimator when the sample size is a random stopping time .    for @xmath13 , the monte carlo divergence error of the estimator @xmath11 will be defined as @xmath14 where @xmath15 is shannon s entropy function ; recall that if @xmath16 is a probability mass function , @xmath17 note that @xmath18 is the maximum likelihood estimator of @xmath19 .",
    "the monte carlo divergence error @xmath20 has a direct interpretation : it is _ the expected kullback - leibler divergence of the empirical distribution of a sample of size @xmath8 from the target @xmath1 _ , and therefore provides a natural measure of the adequacy of @xmath11 for estimating @xmath1 .",
    "the monte carlo divergence error of the estimator @xmath12 when @xmath8 is a random stopping time is defined as the expectation of @xmath20 with respect to the stopping rule , or equivalently @xmath21 where the expectation in is now with respect to both @xmath1 and the stopping rule .",
    "this more general definition of monte carlo divergence error should be interpreted as _ the expected kullback - leibler divergence of the empirical distribution of a sample of random size from the target @xmath1_.    to provide a sampling based justification for this definition of monte carlo divergence error , for @xmath22 consider the empirical distribution estimates @xmath23 which would be obtained from @xmath24 independent repetitions of sampling from @xmath1 , where the sample size of each run is a random stopping time from the same rule .",
    "the jensen - shannon divergence @xcite of @xmath23 , @xmath25 measures the variability in these distribution estimates by calculating their average kullback - leibler divergence from the closest dominating measure , which is their average .",
    "the jensen - shannon divergence is a popular quantification of the difference between distributions , and its square root has the properties of a metric on distributions @xcite .",
    "just as monte carlo variance is the limit of the sample variance of @xmath24 sample means as @xmath26 , the monte carlo divergence error defined in is easily seen to be the limit of as @xmath26 : by the strong law of large numbers , @xmath27 and @xmath28 , $ ] the expected entropy of a monte carlo distribution estimate from one of the runs .",
    "it follows that is a biased but consistent estimate of .",
    "finally , it should be noted that there is a second interpretation of the monte carlo divergence error : @xmath29 is also the _ negative bias of the maximum likelihood estimator of the entropy of @xmath1 _ given a random sample . in the next section",
    "it will be shown that this alternative interpretation is very useful , since it leads to a mechanism for estimating @xmath29 from a single sample .      while @xmath18 is the maximum likelihood estimator of @xmath19 , it is known to be a negatively biased since @xmath15 is a concave function @xcite .",
    "various approximate bias corrections for @xmath18 have been proposed in the information theory literature , and these correction terms can serve here as approximately unbiased estimates of @xmath30 . furthermore , note that any unbiased estimate of @xmath20 is also an unbiased estimate of @xmath29 , the error under the random stopping rule .    given a sample of size @xmath8 , the popular miller - madow method estimates the negative bias of the maximum likelihood estimate @xmath31 to be @xmath32 , where @xmath33 is the number of nonempty bins in @xmath10 .",
    "this estimate proves to be too crude for the purpose here of estimating @xmath30 , since any two distributions with the same number of represented bins would be estimated to have the same divergence , regardless of how uniform the corresponding bin probabilities might be .",
    "an improvement on the miller - madow estimate was provided by @xcite , @xmath34 where @xmath35 is the number of samples in the @xmath36th nonempty bin , such that @xmath37 , and @xmath38 where @xmath39 is the digamma function . in this work , will provide an approximately unbiased estimate of @xmath30 , the expected kullback - leibler divergence of the empirical distribution from the target , based on a single run of the sampler .      calculation of during sampling can be updated at each iteration very quickly , using the following equations .",
    "let @xmath40 be the bin in which the @xmath2th observation falls .",
    "then @xmath41 where the forward difference operator @xmath42 .",
    "besides estimating the current monte carlo divergence error of a distribution estimate after @xmath8 samples from @xmath1 , it will also be of interest to estimate the expected reduction in error that would be achieved from obtaining one more sample , @xmath43 to estimate this quantity it is now necessary to assume that samples are drawn approximately independently ( perhaps via thinned mcmc ) , and that the probability of the new sample falling into the @xmath36th bin is approximated by the empirical , maximum likelihood estimate @xmath44 .",
    "then the expected reduction in error from a further sample can be estimated as @xmath45 again considering this calculation iteratively , if the @xmath2th observation falls into bin @xmath40 then @xmath46 where the second forward difference @xmath47 .",
    "it should be noted that further refinements ( additive terms ) to the bias estimate of are provided by @xcite , such as @xmath48 however , these additional terms , which arise as part of a second order approximation of an integral , are unstable , oscillating between positive and negative values . in this context , without careful treatment such terms can incorrectly suggest that the expected error might very slightly increase by taking further samples , which in practice is not true but would make obtaining further samples seem undesirable .",
    "furthermore , due to their oscillating sign , these terms do not affect the overall drift of the function , which will be the quantity of longer term interest when deciding whether more sampling effort should be afforded .",
    "consider @xmath49 probability distributions @xmath50 .",
    "suppose random samples are to be drawn from a sampler for each @xmath51 , and that the empirical distributions of the samples will eventually serve as approximations of the corresponding target distributions .",
    "given a fixed computational resource , which might simply correspond to a final total number of random samples @xmath8 , the decision problem to be addressed is how best to divide those @xmath8 samples between the @xmath49 samplers .",
    "that is , how sample sizes @xmath52 for the @xmath49 targets should be chosen subject to the constraint @xmath53 .",
    "the samplers can be viewed as _",
    "rivals _ to one another for the same fixed computational resource .    without this or a similar constraint , the problem would be ill - posed : for all @xmath54 , @xmath55 should be chosen to be as large as possible , since monte carlo errors are monotonically decreasing with sample size .",
    "a constraint is required for any sample size choice to be practically meaningful .",
    "in contrast , results which establish a minimum sample size for which errors should fall within a ( typically arbitrary ) desired level of precision are theoretically interesting , but are perhaps best viewed in the reverse direction in this context ; given the inevitable usage of the maximum allowable computation time , understanding the level of error this limit implies .",
    "the default choice is for equal samples sizes , @xmath56 , but such an approach disregards any differences in the _ complexities _ of the target distributions , which in general could be arbitrarily different . the aim of this work is to adaptively determine how much sampling effort should be afforded to each sampler .",
    "the preceding section established a general method for assessing the error of each sampler .",
    "the choice of how to balance sample sizes between the samplers will be made according to a loss function for combining those errors .",
    "suppose that the decision to assign @xmath57 of the total @xmath8 samples to the sampler of @xmath51 implies a monte carlo error level @xmath58 for that target .",
    "the specific definition of this monte carlo error can be left open ; for example , this might be the usual monte carlo error of a point estimate ; or if interest lies in summarising the whole distribution , the monte carlo divergence error criterion .",
    "two natural alternative loss functions for combining the individual errors @xmath58 into an overall performance error are considered here .",
    "one possibility is that utility could be derived from controlling the maximum error of the @xmath49 samplers , suggesting an ( expected ) loss function @xmath59 this form of loss function could be applicable in financial trading , for example , where exposure to the worst loss could be unlimited .",
    "alternatively it might be important to control the average error across the samplers , suggesting a different loss function of the form @xmath60 this form could be applicable in portfolio trading , where exposure to loss is spread across the composite stocks . to illustrate the difference between these two loss functions",
    ", consider estimating the means of two distributions with known variances @xmath61 , @xmath62 , with error measured by the monte carlo variance of the sample means ; the optimal ratio of sample sizes , @xmath63 , would be given by the ratio of the variances @xmath64 under @xmath65 , and by the ratio of the standard deviations @xmath66 under @xmath67 .",
    "therefore care should be exercised in specifying the required form of loss function .",
    "other choices , or indeed linear combinations of these two losses , could be examined .",
    "away from the stylised example of the previous section , in general it is more likely that little will be known _ a priori _ about the target distributions being sampled . instead , the aim will be to dynamically decide , during sampling , which samplers should be afforded more sampling effort , conditional on the information learned so far about the targets . a sequential decision approach is taken .",
    "having taken @xmath68 samples , with @xmath55 of these allocated to the @xmath54th sampler , the decision problem is to choose from which sampler to draw the @xmath69th sample , such that the chosen loss function of the estimated monte carlo errors of the samplers @xmath70 is minimised .    in this sequential",
    "setting , the operational difference between the loss functions @xmath65 and @xmath67 becomes clearer . if the aim is to minimise @xmath65 , then the optimal decision for allocating one more sample is to allocate it to the sampler with the highest estimated error , @xmath71 since error is a decreasing function of sample size .",
    "alternatively , if minimising @xmath67 then the new sample should be allocated to the sampler for which the estimated _ decrease _ in error is highest , @xmath72 since this will minimise the overall expected sum .    these sequential decision rules are myopic , looking only one step ahead .",
    "there are three reasons why this is preferred ; first , considering optimal sequences of future allocations leads to a combinatorial explosion unsuitable for a method intended for optimising the use of a fixed computational resource ; second , the final number of samples may even be unknown ; third , the estimated error or expected change in error under the monte carlo divergence criterion , can be updated very efficiently via and : after one more sample , only one bin count @xmath73 for one sampler @xmath54 will have changed .",
    "any sequential sampling allocation scheme which depends on the outcomes of the random draws will imply a random stopping rule for the number of samples eventually allocated to each sampler .",
    "this adds an extra complication , since some stopping rules will introduce bias into monte carlo estimates @xcite . here",
    "this bias arises if the first samples taken from a target distribution have a particularly low estimated monte carlo error , as this will cause the other rival samplers to share all of the remaining samples ; without corrective action , this phenomenon causes monte carlo estimators to be biased towards estimates of this character .",
    "when the monte carlo error is the divergence measure , low error estimates correspond to low entropy empirical measures , which can spuriously arise if the first random samples happen to fall into the same bin .",
    "therefore , to eradicate this bias , a minimum number of samples @xmath74 is recommended for each target distribution , to prevent degenerate sample sizes .",
    "to examine stability , these minima can be chosen in increasing steps until the resulting samples sizes converge .",
    "for the examples in section [ sec : examples ] , due to the relatively fine grid used for binning samples it was enough to set @xmath75 to obtain convergence .",
    "the full algorithm for sequential sampling from rival target distributions to minimise estimated loss is now presented . for @xmath49 samplers of target distributions @xmath50 , let @xmath76 be the minimum number of samples that should be drawn from @xmath51 .",
    "let @xmath77 be the chosen loss function for combining monte carlo errors across the samplers .",
    "the algorithm proceeds as follows :    1 .",
    "initialise for @xmath78 : 1 .",
    "draw @xmath74 samples from @xmath51 and calculate @xmath79 , the binned empirical estimate of @xmath51 assuming @xmath33 bins in each dimension ; let @xmath80 be the number of non - empty bins in @xmath79 , and @xmath81 be the corresponding bin counts ; set @xmath82 .",
    "2 .   if @xmath83 : calculate the divergence estimate for the @xmath54th sampler , @xmath84 , using ; + else if @xmath85 : calculate the estimated increment in divergence for the @xmath54th sampler , @xmath86 , using . 2 .   iterate until the available computational resource is exhausted : 1 .   if @xmath83 : set @xmath87 ; + else if @xmath85 : set @xmath88 .",
    "2 .   sample one new observation from @xmath89 .",
    "set @xmath90 .",
    "let @xmath36 be the bin into which the new observation falls .",
    "set @xmath91 . if bin @xmath36 was previously empty , set @xmath92 .",
    "3 .   update @xmath93 or @xmath94 using or respectively .",
    "the algorithm of section [ sec : alg ] requires a method of discretising samples from continuous distributions .",
    "( for simplicity , a fixed bin width can be assumed for each dimension of a multivariate distribution . )",
    "the following observations offer some insight for what makes a good bin width in this context . in the limit of the bin width going to zero",
    ", the binned empirical distribution after @xmath8 independent draws would have @xmath8 non - empty bins each containing one observation .",
    "although the identity of those bins would vary across samplers , these empirical distributions would be indistinguishable in terms of both entropy and ; so each sampler would be allocated the same sample size . in the opposite limit of the bin width becoming arbitrarily large , all samples of the same dimension would fall into the same bin . for fixed dimension problems",
    ", this would mean all sample sizes would be equal , and otherwise in transdimensional problems the strategies would simplify to working with marginal distributions of the dimension , which reduces the potential diversity of sample sizes .",
    "so a good bin width would lie well within these two extremes , ideally maximising the resulting differences in sample sizes .",
    "that is , a good bin width should distinguish well the varying complexity of the different targets .",
    "further to these observations , the next section suggests a novel maximum likelihood approach for determining an optimal number of bins , which could be deployed adaptively or using the initial @xmath74 samples drawn from each target .",
    "consider a regular histogram of @xmath33 equal width bins on the interval @xmath95 $ ] , and let @xmath16 be the bin probabilities .",
    "the bayesian formulation of this histogram @xcite treats the probabilities @xmath96 as unknown , and a conjugate dirichlet prior distribution based on a lebesgue base measure with confidence level @xmath97 suggests @xmath98 . for @xmath2 samples , the marginal likelihood of observing bin counts @xmath99 under this model",
    "is @xmath100\\prod_{i=1}^k\\gamma\\{\\alpha(b - a)/k+n_i\\}.\\label{eq : density_ml}\\ ] ] using standard optimisation techniques , identifying the pair @xmath101 that jointly maximise suggests that @xmath102 serves as a good number of bins for a regular histogram of the observed data .",
    "to calibrate the performance of the proposed method , some variations of the strategy for selecting sample sizes are considered .",
    "this section considers some alternative measures of the monte carlo error of a sampler , to be used in place of the divergence estimates @xmath93 or @xmath94 in the algorithm of section [ sec : alg ] .      in the context of particle filters",
    ", @xcite proposed a method for choosing the number of samples @xmath2 required from a single sampler to guarantee that , under a chi square approximation , with a desired probability @xmath103 the kullback - leibler divergence between the binned empirical and true distributions does not exceed a certain threshold @xmath104 .",
    "this was achieved by noting an identity between @xmath105 times this divergence and the likelihood ratio statistic for testing the true distribution against the empirical distribution , assuming the true distribution had the same number of bins , @xmath33 , as the observed empirical distribution .",
    "since the likelihood ratio statistic should approximately follow a chi - squared distribution with @xmath106 degrees of freedom , this suggested a sample size of @xmath107 where @xmath108 is the @xmath109 quantile of that distribution .",
    "adapting this idea to the algorithm of section [ sec : alg ] simply requires a rearrangement of to give the approximate error as a function of sample size , @xmath110 this error estimate can be substituted directly into the algorithm in place of the monte carlo divergence error estimate @xmath93 to provide an alternative scheme for choosing sample sizes when using loss function @xmath65 .",
    "the same ( arbitrary ) value of @xmath111 must be used for each rival sampler , and here this was specified as @xmath112 although the results are robust to different choices .    by the central limit theorem",
    ", the chi - squared distribution quantiles grow near - linearly with the degrees of freedom parameter for @xmath113 @xcite , so it should be noted that , which depends only on the number of bins , has much similarity , and almost equivalence , with the miller - madow estimate of entropy error cited in section [ sec : mc_divergence_estimation ] . by the reasoning given in section [ sec : mc_divergence_estimation ] , use of this error function should show some similarity in performance with the proposed method , but be less robust to distinguishing differences in distributions beyond the number of non - empty bins .",
    "recall from section [ sec : sequential ] that the sequential allocation strategy for minimising the loss function @xmath67 requires an estimate of the expected reduction in error which would be achieved from obtaining another observation from a sampler .",
    "since this error criterion depends entirely upon the number of non - empty bins @xmath33 , in this case an estimate is required for the probability of the new observation falling into a new bin . a simple empirical estimate of the probability of falling into a new bin is provided by the proportion of samples after the first one that have fallen into new bins , given by @xmath114 .",
    "note that this estimate will naturally carry positive bias , since discovery of new bins should decrease over time , and so a sliding window of this quantity might be more appropriate in some contexts .      as a convergence diagnostic for transdimensional samplers , @xcite proposed running replicate sampling chains for the same target distribution , and comparing the variability across the chains of the empirical distributions of a distance - based function of interest .",
    "the method requires that the target @xmath1 be a probability distribution for a point process , and maps multidimensional sampled tuples of _ events _ from @xmath1 to a fixed - dimension space . specifically , a set of _ reference points _",
    "@xmath115 are chosen , and for any sampled tuple of events the distance from each reference point to the closest event in the tuple is calculated .",
    "thus @xmath1 is summarised by a @xmath116-dimensional distribution , where @xmath116 is the number of reference points in @xmath115 .",
    "one example considered in @xcite is a bayesian continuous - time changepoint analysis of a changing regression model with an unknown number of changepoint locations .",
    "a variation of this example is analysed in section [ sec : results_multivariate ] in this article , where instead the analysis will be for the canonical problem of detecting changes in the piecewise constant intensity function @xmath117 $ ] of an inhomogeneous poisson process ( see * ? ? ?",
    "* and the subsequent literature ) . for poisson process data ,",
    "there are two natural functions of interest which could be evaluated at each reference point .",
    "the first is the distance to the nearest changepoint , the second is the intensity level .",
    "both will be considered in section [ sec : results_multivariate ] .",
    "note that in @xcite , reference points are selected from random components from an initial sample from the single target distribution . here , since there are multiple target distributions , a grid of one hundred uniformly spaced points across the domain @xmath118 $ ] is used .",
    "the convergence diagnostic of @xcite did not formally provide a method for calibrating error or selecting sample size . here , to compare the performance of the proposed sample size algorithm of section [ sec : alg ] , the sum across the reference points of the monte carlo variances of either of these functions of interest",
    "is used as the error criterion in the algorithm .      to demonstrate the value of the sophisticated sample size selection strategies given above , two simple strategies which have similar motivation but are otherwise _ ad hoc _",
    "are included in the numerical comparisons of section [ sec : examples ] .",
    "these strategies are now briefly explained .",
    "the _ extent _ of a distribution is the exponential of its entropy , and was introduced as a measure of spread by @xcite .",
    "a simple strategy might be to choose sample size proportional to the estimated squared extent of @xmath1 , @xmath119 .",
    "note that the gaussian distribution @xmath120 , has an extent which is directly proportional to the standard deviation @xmath121 , and so in the univariate gaussian example which will be considered in section [ sec : results_univariate ] , this sample allocation strategy will be approximately equivalent to the optimal strategy when minimising the maximum monte carlo error of the sample means ( _ cf .",
    "_ section [ sec : loss ] ) .",
    "@xcite present a class of convergence tests for monitoring the stationarity of the output of a sampler from a single run which operate by splitting the current sample @xmath122 in two and quantifying the difference between the empirical distributions of the first half of the sample @xmath123 , and the second half of the sample @xmath124 . for univariate samplers the kolmogorov - smirnov test , for example ,",
    "is used to obtain a p - value as a measure of evidence that the second half of the sample is different from the first , and hence neither half is adequately representative of the target .",
    "the test statistics which are used condition on the sample size , and so the sole purpose of these procedures is to investigate how well the sampler is mixing and exploring the target distribution .    to adapt these ideas to the current context",
    ", any mixing issues can first be discounted by splitting the sample for each target in half by allocating the samples into two groups alternately , so that the distribution of , say , @xmath125 can be compared with the distribution of @xmath126 .",
    "this method of splitting up the sample is also computationally much simpler in a streaming context , as incrementing the sample size @xmath2 does not change the required groupings of the existing samples .",
    "let @xmath127 and @xmath128 be the respective empirical distributions of these two subsamples .",
    "a crude variation on using the monte carlo divergence error criteria of is to estimate the error of the sampler by the jensen - shannon divergence of @xmath127 and @xmath128 , @xmath129 if sufficiently many samples have been taken for @xmath130 to be a good representation of the target distribution , then both halves of the sample should also provide reasonable approximations of the target and therefore have low divergence between one another .    as in section",
    "[ sec : efficient_calculation ] , calculation of during sampling can be updated at each iteration very quickly .",
    "let @xmath40 be the bin in which the @xmath2th observation falls .",
    "then , for example , updating the first term of simply requires @xmath131",
    "the methodology from this article is demonstrated on three different data problems .",
    "the first two examples assume only two or three data processes respectively , to allow a detailed examination of how the allocation strategies differ .",
    "then finally a larger scale example with 400 data processes is considered , derived from the ieee vast 2008 challenge concerning communication network anomaly detection .",
    "two straightforward , synthetic examples are now considered .",
    "the first is a univariate problem of fixed dimension with two gaussian target distributions , and the second is a transdimensional problem of unbounded dimension , concerning the changepoints in the piecewise constant intensity functions of three inhomogeneous poisson processes . in both examples",
    ", it is assumed that _ a priori _ nothing is known about the target distributions and that computational limitations determine that only a fixed total number of samples can be obtained from them overall , which will correspond to an average of @xmath132 samples per target distribution .",
    "both loss functions from section [ sec : loss ] are considered , measuring either the maximum error or average error across the target samplers . for each loss function , the following sample size allocation strategies are considered :    1 .",
    "`` fixed ''  the default strategy , @xmath132 samples are obtained from each sampler .",
    "dynamically , aiming to minimise the expected loss , with sampling error estimated using the following methods : 1 .",
    "`` grassberger ''  monte carlo divergence error estimation from section [ sec : mc_divergence_estimation ] ; 2 .",
    "`` fox ''  the @xmath133 goodness of fit statistic of section [ sec : fox ] ; 3 .",
    "`` sisson '' ( only for the transdimensional example )  the monte carlo variances of one of the two candidate fixed dimension functions from section [ sec : foi ] evaluated at 100 equally spaced reference points ( denoted `` sisson - i '' , for the intensity function , `` sisson - n '' for the distance to nearest changepoint function ) ; 4 .",
    "`` extent '' and `` jsd ''  two _ ad hoc _ criteria from section [ sec : ad_hoc_strategies ] .",
    "each sample size allocation strategy is evaluated over a large number of replications @xmath24 , where @xmath134 or @xmath135 respectively in the two examples .",
    "good performance of a sample allocation strategy is measured by the chosen loss function when applied to the realised monte carlo divergence error @xmath29 for each sampler .",
    "good estimates of the true values of @xmath29 are obtained by calculating the jensen - shannon divergence of the monte carlo empirical distributions obtained from the @xmath24 runs ( _ cf .",
    "_ section [ sec : mc_divergence ] ) .",
    "note that in all simulations , the same random number generating seeds are used for all strategies , so that all strategies are making decisions based on exactly the same samples .      in the first example",
    ", a total of 100,000 samples are drawn from two gaussian distributions , where one gaussian has twice the standard deviation of the other : @xmath136 , @xmath137 note that if these two distributions were considered on different scales they would be equivalent ; but when losses in estimating the distributions are measured on the same scale , then they are not equivalent . for discretising the distributions ,",
    "the following bins were used : @xmath138 , @xmath139 , @xmath140 , @xmath141 , @xmath142 , @xmath143 , @xmath144 .",
    "this corresponds to an interior range of plus or minus five times the largest of the standard deviations of the two targets , divided into 100 evenly spaced bins , along with two extra bins for the extreme tails .",
    "results are robust to allowing wider ranges or more bins , but are omitted from presentation . for further validation ,",
    "a simple experiment was conducted using the method from section [ sec : bayesian_bin_width ] on @xmath145 $ ] : 100,000 samples were simulated from each of @xmath146 and @xmath147 , leading to estimates @xmath148 and @xmath149 respectively , suggesting 100 as a good number of bins for fitting these densities .    the varying sample sizes obtained from each target from the 1 million simulations using each of the sample allocation strategies listed above and the loss function @xmath65 are shown in fig .",
    "[ fig : univariate_sample_sizes ] . tables [ tab : univariate_losses_max ] and [ tab : univariate_losses_ave ] show the mean sample sizes and the implied monte carlo divergence error for each target distribution using each of the sample allocation strategies listed above ; the two tables correspond to the two choices of loss function for combining errors .",
    "+   +    table [ tab : univariate_losses_max ] gives the results under the loss function which calculates the maximal error across samplers .",
    "optimal performance would imply approximately equal monte carlo divergence errors for the two targets , and the proposed strategy based on grassberger s entropy bias estimate is by far the closest to achieving this objective .",
    "interestingly , note that under this best strategy , the average sample sizes are almost exactly in the ratio 1:2 , the same ratio as the true standard deviations of the target distributions .",
    "recall from section [ sec : loss ] , that such a ratio is optimal in another sense , minimising the _ average _ monte carlo errors of the sample means .",
    "this contrast highlights the importance of carefully specifying the desired error criterion as well as the correct loss function .",
    "one of the two _ ad hoc _ strategies based on calculating the jensen - shannon divergence between the two halves of the sample is only slightly outperformed by the @xmath133 goodness of fit method ; however , note in figure [ fig : univariate_sample_sizes ] the much higher variance of the sample sizes with the jsd method , which is indicative of an unreliable strategy .",
    "the other _ ad hoc _ method which takes sample sizes proportional to the extent of the empirical distributions is seen to overcompensate for the higher variance of the second gaussian , and performs worse than even the default equal sample size strategy . for that strategy ,",
    "note the sample sizes are almost exactly in the ratio 1:4 , the same ratio as the true variances of the target distributions .",
    "recall from section [ sec : extent ] that such a strategy is approximately equivalent to minimising the _ maximum _ monte carlo errors of the sample means , which was noted in section [ sec : loss ] to imply such an allocation ratio .",
    "table [ tab : univariate_losses_ave ] gives the results under the loss function @xmath67 which calculates the average error across samplers .",
    "the monte carlo divergence strategy based on grassberger s entropy bias estimate performs best , although the the @xmath133 goodness of fit method also performs very well here .",
    "the contrasting sample sizes between the loss functions @xmath65 and @xmath67 for all dynamic allocation strategies are noteworthy , as remarked in section [ sec : loss ] .      for a more complex example",
    ", simulated data were generated from three inhomogeneous poisson processes on @xmath118 $ ] with different piecewise constant intensity functions . in each case , prior beliefs for the intensity functions were specified by a homogeneous poisson process prior distribution on the number and locations of the changepoints and independent , conjugate gamma priors on the intensity levels .",
    "the three rival target distributions for inference are the bayesian marginal posterior distributions on the number and locations of the changepoints for each of the three processes .",
    "each of the three simulated poisson processes had two changepoints , located at @xmath150 and @xmath151 .",
    "the intensity levels of the three processes were respectively : @xmath152 , @xmath153 , @xmath154 , so the processes differed only through magnitudes of intensity changes . to make the target distributions closer and",
    "therefore make the inferential problem harder , in each case the prior expectation for the number of changepoints was set to 1 . for illustration of the differences in complexity of the resulting posterior distributions for the changepoints of the three processes , large sample estimates of the true , discretised posterior distributions are shown in fig .",
    "[ fig : targets ] , based upon one trillion reversible jump markov chain monte carlo samples @xcite .",
    "note that the different target distributions place different levels of mass on the number of changepoints , and therefore on the dimension of the problem .",
    "in all cases there is insufficient information to strongly detect both changepoints , and so much of the mass of the posterior distributions is localised at a single changepoint at @xmath155 , the midpoint of the two true changepoints . additionally , fig .",
    "[ fig : fois ] shows the posterior variance of two functions of interest identified in section [ sec : foi ] for @xmath156 $ ] : the distance to the nearest changepoint , and the intensity level .    to determine sample sizes ,",
    "reversible jump markov chain monte carlo simulation was used to sample from the marginal posterior distributions of the changepoints , and the chains were thinned with only one in every fifty iterations retained to give approximately independent samples . to discretise the distributions , the interval @xmath118 $ ] was divided into 50 equally sized bins ; while for a single dimension this would be fewer bins than were used in the previous section , here the bins are applied to each dimension of a mixture model of unbounded dimension , meaning that actually a very large number of bins are visited ; computational storage issues can begin to arise when using an even larger number of bins , simply through storing the frequency counts of the samples .",
    "[ fig:3target_sample_sizes ] shows the distributions of sample sizes obtained from a selection of the strategies over @xmath135 repetitions , and tables [ tab : transdim_losses_max ] and [ tab : transdim_losses_ave ] show results from the different strategies examined for these more complex transdimensional samplers .",
    "performance is similar to the previous section , with the grassberger entropy bias correction method performing best .",
    "+   +    for this transdimensional sampling example , it also makes sense to consider the fixed dimension function of interest methods of @xcite , using the mean intensity function of the poisson process or the distance to nearest changepoint , each evaluated at 100 equally spaced grid points on @xmath118 $ ] .",
    "the monte carlo variances used in these strategies estimate the variances displayed in the plots of fig .",
    "[ fig : fois ] at the reference points , divided by the current sample size .",
    "the performance of these fixed dimensional strategies is particularly poor under the loss function @xmath65 .",
    "importantly , it should also be noted that the sample sizes and performance vary considerably depending upon which of the two arbitrary functions of the reference points are used .",
    "this final example now illustrates how the method performs in the presence of a much larger number of target distributions , in the context of network security .",
    "the ieee vast 2008 challenge data are synthetic but realistically generated records of mobile phone calls for a small community of 400 individuals over a ten day period .",
    "the data can be obtained from www.cs.umd.edu/hcil/vastchallenge08 .",
    "the aim of the original challenge was to find anomalous behaviour within this social network , which might be indicative of malicious coordinated activity .",
    "one approach to this problem is to monitor the call patterns of individual users and detect any changes from their normal behaviour , with the idea that a smaller subset of anomalous individuals will then be investigated for community structure . in particular , this approach has been shown to be effective with these data when monitoring the event process of incoming call times for each individual @xcite . after correcting for diurnal effects on normal behaviour , this approach can be reduced to a changepoint analysis of the intensities of 400 poisson processes of the same character as section [ sec : results_multivariate ] . for the focus of this article ,",
    "it is of interest to see how such an approach could be made more feasible in real time by allocating computational resource between these 400 processes more efficiently .",
    "[ fig : vast ] shows the contrasting performance between an equal computational allocation of one million markov chain monte carlo samples to each process against the variable sample size approach using grassberger s entropy bias estimate , for the same total computational resource of 400 millions samples and using the loss function @xmath65 .",
    "the left hand plot shows the distribution of sample sizes for the individual processes over @xmath157 repetitions , using 5,000 initial samples and an average allocation of one million samples for each posterior target ; the dashed line represents the fixed sample size strategy .",
    "the sample sizes vary enormously across individuals .",
    "however , for each individual the variability between runs is much lower , showing that the method is robust in performance .",
    "the right hand plot shows the resulting monte carlo divergence errors of the estimated distributions from the targets .",
    "ideal performance under @xmath65 would have each of these errors approximately equal , and the variable sample size method gets much closer to this ideal .",
    "the circled case in the right hand plot indicates the process which has the highest error when using a fixed sample size , and this corresponds to the same individual process that always gets the highest sample size allocation under the adaptive sample size strategy in the left hand plot .",
    "this individual has a very changeable calling pattern , suggesting several possible changepoints : no calls in the first five days , then two calls one hour apart , then another two days break , and then four calls each day for the remainder of the period .",
    "it was remarked in the review paper of @xcite on transdimensional samplers that `` a more rigorous default assessment of sampler convergence '' than the existing technology is required , and this has remained an open problem .",
    "this article is a first step towards establishing such a default method from a decision theoretic perspective , proposing a framework and methodology which are rigorously motivated and fully general in their applicability to all distributional settings .",
    "note that when the samplers induce autocorrelation , which is commonplace with metropolis - hastings ( mh ) markov chain monte carlo simulation , then the decision rule for @xmath67 becomes more complicated since independence was assumed in the derivation of .",
    "if one or more of the samplers has very high serial autocorrelation , then drawing additional samples from those targets will become less attractive under @xmath67 , as with high probability very little extra information will be obtained from the next draw .",
    "it is still possible to proceed in this setting by adapting to admit autocorrelation ; for example , the rejection rate of the markov chain could be used to approximate the probability of observing the same bin as the last sample , and otherwise draws could be assumed to be more realistically drawn from the target . however , for reasons of brevity this is not pursued further in this work , and of course the efficacy would depend entirely on the specifics of the mh / other sampler .",
    "importantly , this issue should not be seen as a decisive limitation of the proposed methodology when using @xmath67 , since although thinning was used in the markov chain monte carlo examples of sections [ sec : results_multivariate ] and [ sec : vast_data ] to obtain the next sample for use in calculating the convergence criteria , this would not prevent the full sample from being retained and utilised without thinning for the actual inference problem .",
    "the amount of thinning could be varied between samplers if appropriate , and this could be counterbalanced by weighting the errors in accordingly .",
    "another related problem which could be considered is that of importance sampling . if samples can not be obtained directly from the target @xmath1 but instead from some importance distribution with the same support , then it would be useful to understand how these error estimates and sample size strategies can be extended to the case where the empirical distribution of the samples has associated weights . in addressing the revised question of how large an importance sample should be",
    ", there should be an interesting trade - off between the inherent complexity of the target distributions , which has been the subject of this article , and how well the importance distributions match those targets ."
  ],
  "abstract_text": [
    "<S> it is often necessary to make sampling - based statistical inference about many probability distributions in parallel . given a finite computational resource , this article addresses how to optimally divide sampling effort between the samplers of the different distributions . formally approaching this decision problem </S>",
    "<S> requires both the specification of an error criterion to assess how well each group of samples represent their underlying distribution , and a loss function to combine the errors into an overall performance score . for the first part , a new monte carlo divergence error criterion based on jensen - shannon divergence is proposed . using results from information theory , </S>",
    "<S> approximations are derived for estimating this criterion for each target based on a single run , enabling adaptive sample size choices to be made during sampling .    </S>",
    "<S> sample sizes ; jensen - shannon divergence ; transdimensional markov chains </S>"
  ]
}