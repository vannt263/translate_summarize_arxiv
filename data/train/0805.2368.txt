{
  "article_text": [
    "we address the problem of comparing samples from two probability distributions , by proposing statistical tests of the hypothesis that these distributions are different ( this is called the two - sample or homogeneity problem ) .",
    "such tests have application in a variety of areas . in bioinformatics , it is of interest to compare microarray data from identical tissue types as measured by different laboratories , to detect whether the data may be analysed jointly , or whether differences in experimental procedure have caused systematic differences in the data distributions . equally of interest are comparisons between microarray data from different tissue types , either to determine whether two subtypes of cancer may be treated as statistically indistinguishable from a diagnosis perspective , or to detect differences in healthy and cancerous tissue . in database",
    "attribute matching , it is desirable to merge databases containing multiple fields , where it is not known in advance which fields correspond : the fields are matched by maximising the similarity in the distributions of their entries .",
    "we test whether distributions @xmath0 and @xmath1 are different on the basis of samples drawn from each of them , by finding a well behaved ( e.g.smooth ) function which is large on the points drawn from @xmath0 , and small ( as negative as possible ) on the points from @xmath1 .",
    "we use as our test statistic the difference between the mean function values on the two samples ; when this is large , the samples are likely from different distributions .",
    "we call this statistic the maximum mean discrepancy ( mmd )",
    ".    clearly the quality of the mmd as a statistic depends on the class @xmath2 of smooth functions that define it . on one hand",
    ", @xmath2 must be `` rich enough '' so that the population mmd vanishes if and only if @xmath3 .",
    "on the other hand , for the test to be consistent , @xmath2 needs to be `` restrictive '' enough for the empirical estimate of mmd to converge quickly to its expectation as the sample size increases .",
    "we shall use the unit balls in universal reproducing kernel hilbert spaces @xcite as our function classes , since these will be shown to satisfy both of the foregoing properties ( we also review classical metrics on distributions , namely the kolmogorov - smirnov and earth - mover s distances , which are based on different function classes ) . on a more practical note , the mmd has a reasonable computational cost , when compared with other two - sample tests : given @xmath4 points sampled from @xmath0 and @xmath5 from @xmath1 , the cost is @xmath6 time .",
    "we also propose a less statistically efficient algorithm with a computational cost of @xmath7 , which can yield superior performance at a given computational cost by looking at a larger volume of data .",
    "we define three non - parametric statistical tests based on the mmd .",
    "the first two , which use distribution - independent uniform convergence bounds , provide finite sample guarantees of test performance , at the expense of being conservative in detecting differences between @xmath0 and @xmath1 .",
    "the third test is based on the asymptotic distribution of the mmd , and is in practice more sensitive to differences in distribution at small sample sizes .",
    "the present work synthesizes and expands on results of @xcite , @xcite , and @xcite who in turn build on the earlier work of @xcite .",
    "note that the latter addresses only the third kind of test , and that the approach of @xcite employs a more accurate approximation to the asymptotic distribution of the test statistic .",
    "we begin our presentation in section [ sec : basicstuffandreview ] with a formal definition of the mmd , and a proof that the population mmd is zero if and only if @xmath8 when @xmath2 is the unit ball of a universal rkhs .",
    "we also review alternative function classes for which the mmd defines a metric on probability distributions . in section [ sec : prevwork ] , we give an overview of hypothesis testing as it applies to the two - sample problem , and review other approaches to this problem .",
    "we present our first two hypothesis tests in section [ sec : firstbound ] , based on two different bounds on the deviation between the population and empirical @xmath9 .",
    "we take a different approach in section [ sec : asymptotictest ] , where we use the asymptotic distribution of the empirical @xmath9 estimate as the basis for a third test .",
    "when large volumes of data are available , the cost of computing the mmd ( quadratic in the sample size ) may be excessive : we therefore propose in section [ sec : lineartimestatistic ] a modified version of the mmd statistic that has a linear cost in the number of samples , and an associated asymptotic test . in section",
    "[ sec : relatedmethods ] , we provide an overview of methods related to the mmd in the statistics and machine learning literature .",
    "finally , in section [ sec : experiments ] , we demonstrate the performance of mmd - based two - sample tests on problems from neuroscience , bioinformatics , and attribute matching using the hungarian marriage method .",
    "our approach performs well on high dimensional data with low sample size ; in addition , we are able to successfully distinguish distributions on graph data , for which ours is the first proposed test .",
    "in this section , we present the maximum mean discrepancy ( mmd ) , and describe conditions under which it is a metric on the space of probability distributions .",
    "the mmd is defined in terms of particular function spaces that witness the difference in distributions : we therefore begin in section [ sec : mmdintro ] by introducing the mmd for some arbitrary function space . in section [ sec : mmdinrkhs ] , we compute both the population mmd and two empirical estimates when the associated function space is a reproducing kernel hilbert space , and we derive the rkhs function that witnesses the mmd for a given pair of distributions in section [ sec : mmdwitness ] . finally , we describe the mmd for more general function classes in section [ sec : mmdotherfuncclasses ] .      our goal is to formulate a statistical test that answers the following question :    [ prob : problem ] let @xmath0 and @xmath1 be borel probability measures defined on a domain @xmath10 .",
    "given observations @xmath11 and @xmath12 , drawn independently and identically distributed ( i.i.d . ) from @xmath0 and @xmath1 , respectively , can we decide whether @xmath13 ?",
    "to start with , we wish to determine a criterion that , in the population setting , takes on a unique and distinctive value only when @xmath8 .",
    "it will be defined based on lemma 9.3.2 of @xcite .",
    "[ lem : dudley ] let @xmath14 be a metric space , and let @xmath15 be two borel probability measures defined on @xmath10",
    ". then @xmath8 if and only if @xmath16 for all @xmath17 , where @xmath18 is the space of bounded continuous functions on @xmath10 .",
    "although @xmath18 in principle allows us to identify @xmath8 uniquely , it is not practical to work with such a rich function class in the finite sample setting .",
    "we thus define a more general class of statistic , for as yet unspecified function classes @xmath2 , to measure the disparity between @xmath0 and @xmath1 @xcite .",
    "[ def : mmd ] let @xmath2 be a class of functions @xmath19 and let @xmath20 be defined as above .",
    "we define the maximum mean discrepancy ( mmd ) as @xmath21 }   : = \\sup_{f \\in { \\mathcal{f } } }     \\left({\\mathbf{e}}_{x \\sim p}[f(x ) ] -        { \\mathbf{e}}_{y \\sim q}[f(y ) ] \\right).\\ ] ] @xcite calls this an integral probability metric .",
    "a biased empirical estimate of the mmd is @xmath22 }   : = \\sup_{f \\in { \\mathcal{f } } }   \\left (       \\frac{1}{m } \\sum_{i=1}^m f(x_i ) -        \\frac{1}{n } \\sum_{i=1}^n f(y_i )   \\right).\\ ] ]    the empirical mmd defined above has an upward bias ( we will define an unbiased statistic in the following section ) .",
    "we must now identify a function class that is rich enough to uniquely identify whether @xmath8 , yet restrictive enough to provide useful finite sample estimates ( the latter property will be established in subsequent sections ) .",
    "if @xmath2 is the unit ball in a reproducing kernel hilbert space @xmath23 , the empirical mmd can be computed very efficiently",
    ". this will be the main approach we pursue in the present study .",
    "other possible function classes @xmath2 are discussed at the end of this section .",
    "we will refer to @xmath23 as universal whenever @xmath23 , defined on a compact metric space @xmath10 and with associated kernel @xmath24 , is dense in @xmath18 with respect to the @xmath25 norm .",
    "it is shown in @xcite that gaussian and laplace kernels are universal .",
    "we have the following result :    [ th : stronger ] let @xmath2 be a unit ball in a universal rkhs @xmath23 , defined on the compact metric space @xmath10 , with associated kernel @xmath26",
    ". then @xmath27 } = 0 $ ] if and only if @xmath28 .",
    "it is clear that @xmath29}$ ] is zero if @xmath8 .",
    "we prove the converse by showing that @xmath30}=d$ ] for some @xmath31 implies @xmath29 } > 0 $ ] : this is equivalent to @xmath29}=0 $ ] implying @xmath30}=0 $ ] ( where this last result implies @xmath8 by lemma [ lem : dudley ] , noting that compactness of the metric space @xmath10 implies its separability ) .",
    "let @xmath23 be the universal rkhs of which @xmath2 is the unit ball .",
    "if @xmath30}=d$ ] , then there exists some @xmath32 for which @xmath33}-{\\mathbf{e}}_{q}{\\left[\\tilde{f}\\right]}\\ge d/2 $ ] .",
    "we know that @xmath23 is dense in @xmath18 with respect to the @xmath34 norm : this means that for @xmath35 , we can find some @xmath36 satisfying @xmath37 .",
    "thus , we obtain @xmath38 } - { \\mathbf{e}}_p{\\left[\\tilde{f}\\right]}\\right| } < \\epsilon$ ] and consequently @xmath39}-{\\mathbf{e}}_{q}{\\left[f^{*}\\right]}\\right| }   >      { \\left|{\\mathbf{e}}_{p}{\\left[\\tilde{f}\\right]}-{\\mathbf{e}}_{q}{\\left[\\tilde{f}\\right]}\\right| } - 2\\epsilon > \\textstyle \\frac{d}{2 } - 2 \\frac{d}{8 } = \\frac{d}{4 } > 0.\\ ] ] finally , using @xmath40 , we have @xmath41}-{\\mathbf{e}}_{q}{\\left[f^{*}\\right]}\\right]}/{{\\left\\|f^{*}\\right\\|}_{{\\mathcal{h } } } }     \\ge d / ( 4 { \\left\\|f^{*}\\right\\|}_{{\\mathcal{h}}})>0,\\end{aligned}\\ ] ] and hence @xmath29}>0 $ ] .",
    "we now review some properties of @xmath23 that will allow us to express the mmd in a more easily computable form @xcite .",
    "since @xmath23 is an rkhs , the operator of evaluation @xmath42 mapping @xmath43 to @xmath44 is continuous .",
    "thus , by the riesz representation theorem , there is a feature mapping @xmath45 from @xmath10 to @xmath46 such that @xmath47 .",
    "moreover , @xmath48 , where @xmath49 is a positive definite kernel function .",
    "the following lemma is due to @xcite .",
    "[ le : simplemmd ] denote the expectation of @xmath45 by @xmath50}$ ] ( assuming its existence ) . , which is rearranged as @xmath51<\\infty$ ] , where @xmath52 and @xmath53 are independent random variables drawn according to @xmath0 .",
    "in other words , @xmath54 is a trace class operator with respect to the measure @xmath0 .",
    "] then @xmath55 & = \\sup_{{\\left\\|f\\right\\|}_{\\mathcal{h}}\\leq 1 }     { \\left\\langle \\mu[p ] - \\mu[q],f \\right\\rangle }     = { \\left\\| \\mu[p ] - \\mu[q ] \\right\\|}_{{\\mathcal{h}}}.\\end{aligned}\\ ] ]    @xmath56     & = & \\left[\\sup_{{\\left\\|f\\right\\|}_{\\mathcal{h}}\\leq 1 }   \\left (   { \\mathbf{e}}_p{\\left[f(x)\\right ] } - { \\mathbf{e}}_q{\\left[f(y)\\right ] } \\right ) \\right]^2\\\\    & = & \\left[\\sup_{{\\left\\|f\\right\\|}_{\\mathcal{h}}\\leq 1 }   \\left (   { \\mathbf{e}}_p{\\left[{\\left\\langle \\phi(x),f \\right\\rangle}_{{\\mathcal{h}}}\\right ] } - { \\mathbf{e}}_q{\\left[{\\left\\langle \\phi(y),f \\right\\rangle}_{{\\mathcal{h}}}\\right ] } \\right ) \\right]^2\\\\    & = & \\left[\\sup_{{\\left\\|f\\right\\|}_{\\mathcal{h}}\\leq 1 }     { \\left\\langle \\mu_p - \\mu_q , f \\right\\rangle}_{{\\mathcal{h } } } \\right]^2    = { \\left\\| \\mu_p - \\mu_q \\right\\|}^2_{{\\mathcal{h } } } \\end{aligned}\\ ] ]    given we are in an rkhs , the norm @xmath57 may easily be computed in terms of kernel functions .",
    "this leads to a first empirical estimate of the mmd , which is unbiased .",
    "[ lem : rkhs - mmd ] given @xmath52 and @xmath53 independent random variables with distribution @xmath0 , and @xmath58 and @xmath59 independent random variables with distribution @xmath1 , the population @xmath60 is @xmath61 }   = { \\mathbf{e}}_{x , x ' \\sim p } { \\left[k(x , x')\\right ] }         -2{\\mathbf{e}}_{x \\sim p , y \\sim q } { \\left[k(x , y)\\right ] }        + { \\mathbf{e}}_{y , y ' \\sim q } { \\left[k(y , y')\\right]}.\\ ] ] let @xmath62 be @xmath4 i.i.d .",
    "random variables , where @xmath63 ( i.e. we assume @xmath64 ) .",
    "an _ unbiased _ empirical estimate of @xmath60 is @xmath65 }    =   \\frac{1}{(m)(m-1)}\\sum_{i\\neq j}^{m }   h({z}_i,{z}_{j}),\\ ] ] which is a one - sample u - statistic with @xmath66 ( we define @xmath67 to be symmetric in its arguments due to requirements that will arise in section [ sec : asymptotictest ] ) .",
    "starting from the expression for @xmath68 $ ] in lemma [ le : simplemmd ] , @xmath56      & = & { \\left\\| \\mu_p - \\mu_q \\right\\|}^2_{{\\mathcal{h } } } \\\\    & = & { \\left\\langle \\mu_p,\\mu_p \\right\\rangle}_{\\mathcal{h}}+ { \\left\\langle \\mu_q,\\mu_q \\right\\rangle}_{\\mathcal{h}}- 2{\\left\\langle \\mu_p,\\mu_q \\right\\rangle}_{\\mathcal{h}}\\\\    & = & { \\mathbf{e}}_p { \\left\\langle \\phi(x),\\phi(x ' ) \\right\\rangle}_{\\mathcal{h}}+ { \\mathbf{e}}_q { \\left\\langle \\phi(y),\\phi(y ' ) \\right\\rangle}_{\\mathcal{h}}- 2{\\mathbf{e}}_{p , q}{\\left\\langle \\phi(x),\\phi(y ) \\right\\rangle}_{\\mathcal{h}},\\end{aligned}\\ ] ] the proof is completed by applying @xmath69 ; the empirical estimate follows straightforwardly .",
    "the empirical statistic is an unbiased estimate of @xmath60 , although it does not have minimum variance , since we are ignoring the cross - terms @xmath70 of which there are only @xmath71 .",
    "the minimum variance estimate is almost identical , though ( * ? ? ?",
    "* section 5.1.4 ) .",
    "the biased statistic in ( [ eq : mmd - e ] ) may also be easily computed following the above reasoning . substituting the empirical estimates",
    "@xmath72 : = \\frac{1}{m } \\sum_{i=1}^m \\phi(x_i)$ ] and @xmath73:=\\frac{1}{n } \\sum_{i=1}^n \\phi(y_i)$ ] of the feature space means based on respective samples @xmath74 and @xmath75 , we obtain @xmath76 }   = { \\left[\\frac{1}{m^2 } \\sum_{i , j=1}^m { k(x_i , x_j ) }         -\\frac{2}{m n } \\sum_{i , j=1}^{m , n } k(x_i , y_j )     +   \\frac{1}{n^2 } \\sum_{i , j=1}^n { k(y_i , y_j)}\\right]}^{\\frac{1}{2}}.\\ ] ]    intuitively we expect the empirical test statistic @xmath77 $ ] , whether biased or unbiased , to be small if @xmath3 , and large if the distributions are far apart .",
    "it costs @xmath78 time to compute both statistics .",
    "finally , we note that @xcite recently proposed a modification of the kernel mmd statistic in lemma [ le : simplemmd ] , by scaling the feature space mean distance using the inverse within - sample covariance operator , thus employing the kernel fisher discriminant as a statistic for testing homogeneity .",
    "this statistic is shown to be related to the @xmath79 divergence .       that witnesses the mmd has been scaled for plotting purposes , and was computed empirically on the basis of @xmath80 samples , using a gaussian kernel with @xmath81.[fig : mmddemo1d],scaledwidth=50.0% ]    it is also instructive to consider the witness @xmath82 which is chosen by mmd to exhibit the maximum discrepancy between the two distributions .",
    "the population @xmath82 and its empirical estimate @xmath83 are respectively @xmath84 - \\mu[q ] \\right\\rangle }     & =     & { \\mathbf{e}}_{x ' \\sim p } { \\left[k(x , x')\\right ] } -    { \\mathbf{e}}_{x ' \\sim q } { \\left[k(x , x')\\right ] }   \\\\",
    "\\hat{f}(x ) & \\propto   & { \\left\\langle \\phi(x),\\mu[x ] - \\mu[y ] \\right\\rangle } & =   & \\frac{1}{m } \\sum_{i=1}^m k(x_i , x ) -     \\frac{1}{n } \\sum_{i=1}^n k(y_i , x ) .",
    "\\end{array}\\ ] ] this follows from the fact that the unit vector @xmath85 maximizing @xmath86 in a hilbert space is @xmath87 .",
    "we illustrate the behavior of mmd in figure [ fig : mmddemo1d ] using a one - dimensional example . the data @xmath74 and @xmath75 were generated from distributions @xmath0 and @xmath1 with equal means and variances , with @xmath0 gaussian and @xmath1 laplacian .",
    "we chose @xmath2 to be the unit ball in an rkhs using the gaussian kernel .",
    "we observe that the function @xmath82 that witnesses the mmd  in other words , the function maximizing the mean discrepancy in ( [ eq : mmd - a ] )  is smooth , positive where the laplace density exceeds the gaussian density ( at the center and tails ) , and negative where the gaussian density is larger .",
    "moreover , the magnitude of @xmath82 is a direct reflection of the amount by which one density exceeds the other , insofar as the smoothness constraint permits it .",
    "the definition of the maximum mean discrepancy is by no means limited to rkhs .",
    "in fact , any function class @xmath2 that comes with uniform convergence guarantees and is sufficiently powerful will enjoy the above properties .",
    "let @xmath2 be a subset of some vector space .",
    "the star @xmath88 $ ] of a set @xmath2 is @xmath89 : = { \\left\\{\\alpha x | x \\in { \\mathcal{f}}\\text { and } \\alpha \\in [ 0 , \\infty)\\right\\ } }    \\end{aligned}\\ ] ]    [ th : stronger - star ] denote by @xmath2 the subset of some vector space of functions from @xmath10 to @xmath46 for which @xmath88 \\cap c({\\mathcal{x}})$ ] is dense in @xmath18 with respect to the @xmath90 norm",
    ". then @xmath27 } = 0 $ ] if and only if @xmath3 .",
    "moreover , under the above conditions @xmath91 $ ] is a metric on the space of probability distributions .",
    "whenever the star of @xmath2 is _ not _ dense , @xmath9 is a pseudo - metric space .",
    "satisfies the following four properties : symmetry , triangle inequality , @xmath92 , and @xmath93 .",
    "a pseudo - metric only satisfies the first three properties . ]    the first part of the proof is almost identical to that of theorem  [ th : stronger ] and is therefore omitted . to see the second part",
    ", we only need to prove the triangle inequality .",
    "we have @xmath94 } \\\\   &",
    "\\geq \\sup_{f \\in { \\mathcal{f } } }       { \\left|e_p f - e_r f\\right|}.    \\end{aligned}\\ ] ] the first part of the theorem establishes that @xmath95 $ ] is a metric , since only for @xmath3 do we have @xmath91    = 0 $ ] .",
    "note that any uniform convergence statements in terms of @xmath2 allow us immediately to characterize an estimator of @xmath96 explicitly .",
    "the following result shows how ( we will refine this reasoning for the rkhs case in section  [ sec : firstbound ] ) .",
    "[ th : general ] let @xmath97 be a confidence level and assume that for some @xmath98 the following holds for samples @xmath99 drawn from @xmath0 : @xmath100 - \\frac{1}{m } \\sum_{i=1}^m          f(x_i)\\right| } > \\epsilon(\\delta , m , { \\mathcal{f}})\\right\\ } } \\leq \\delta .",
    "\\end{aligned}\\ ] ] in this case we have that @xmath101 - { \\mathrm{mmd}}_b[{\\mathcal{f}},x , y]\\right| } > 2        \\epsilon(\\delta/2,m,{\\mathcal{f}})\\right\\ } } \\leq \\delta .",
    "\\end{aligned}\\ ] ]    the proof works simply by using convexity and suprema as follows : @xmath102 - { \\mathrm{mmd}}_b[{\\mathcal{f}},x , y]\\right| } \\\\      = & { \\left|\\sup_{f \\in { \\mathcal{f } } } { \\left|{\\mathbf{e}}_p[f ] - { \\mathbf{e}}_q[f]\\right| } -         \\sup_{f \\in { \\mathcal{f } } } { \\left|\\frac{1}{m } \\sum_{i=1}^m f(x_i ) -           \\frac{1}{n } \\sum_{i=1}^n f(y_i)\\right|}\\right| } \\\\",
    "\\leq & \\sup_{f \\in { \\mathcal{f } } } { \\left|{\\mathbf{e}}_p[f ] - { \\mathbf{e}}_q[f ] -         \\frac{1}{m } \\sum_{i=1}^m f(x_i ) +         \\frac{1}{n } \\sum_{i=1}^n f(y_i)\\right| } \\\\",
    "\\leq &       \\sup_{f \\in { \\mathcal{f } } } { \\left|{\\mathbf{e}}_p[f ] - \\frac{1}{m } \\sum_{i=1}^m f(x_i)\\right| } +       \\sup_{f \\in { \\mathcal{f } } } { \\left|{\\mathbf{e}}_q[f ] - \\frac{1}{n } \\sum_{i=1}^n f(y_i)\\right|}.     \\end{aligned}\\ ] ] bounding each of the two terms via a uniform convergence bound proves the claim .",
    "this shows that @xmath103 $ ] can be used to estimate @xmath104 $ ] and that the quantity is asymptotically unbiased .",
    "any classifier which maps a set of observations @xmath105 with @xmath106 on some domain @xmath10 and labels @xmath107 , for which uniform convergence bounds exist on the convergence of the empirical loss to the expected loss , can be used to obtain a similarity measure on distributions  simply assign @xmath108 if @xmath109 and @xmath110 for @xmath111 and find a classifier which is able to separate the two sets . in this case",
    "maximization of @xmath112 - { \\mathbf{e}}_q[f]$ ] is achieved by ensuring that as many @xmath113 as possible correspond to @xmath114 , whereas for as many @xmath115 as possible we have @xmath116 .",
    "consequently neural networks , decision trees , boosted classifiers and other objects for which uniform convergence bounds can be obtained can be used for the purpose of distribution comparison .",
    "for instance ,",
    "* section 4 ) use the error of a hyperplane classifier to approximate the @xmath117-distance between distributions of @xcite .",
    "other function spaces @xmath2 inspired by the statistics literature can also be considered in defining the mmd .",
    "indeed , lemma [ lem : dudley ] defines an mmd with @xmath2 the space of bounded continuous real - valued functions , which is a banach space with the supremum norm @xcite .",
    "we now describe two further metrics on the space of probability distributions , the kolmogorov - smirnov and earth mover s distances , and their associated function classes .",
    "the kolmogorov - smirnov",
    "( k - s ) test is probably one of the most famous two - sample tests in statistics .",
    "it works for random variables @xmath118 ( or any other set for which we can establish a total order ) .",
    "denote by @xmath119 the cumulative distribution function of @xmath0 and let @xmath120 be its empirical counterpart , that is @xmath121 it is clear that @xmath122 captures the properties of @xmath0 .",
    "the kolmogorov metric is simply the @xmath25 distance @xmath123 for two sets of observations @xmath74 and @xmath75 .",
    "@xcite showed that for @xmath3 the limiting distribution of the empirical cumulative distribution functions satisfies @xmath124}^{\\frac{1}{2 } }      { \\left\\|f_x - f_y\\right\\|}_\\infty > x    \\right\\ } } = 2 \\sum_{j=1}^\\infty ( -1)^{j-1 } e^{-2j^2 x^2 } \\text { for } x \\geq 0.\\end{aligned}\\ ] ] this allows for an efficient characterization of the distribution under the null hypothesis @xmath125 .",
    "efficient numerical approximations to ( [ eq : ks - asympt ] ) can be found in numerical analysis handbooks @xcite .",
    "the distribution under the alternative , @xmath126 , however , is unknown .",
    "the kolmogorov metric is , in fact , a special instance of @xmath91 $ ] for a certain banach space ( * ? ? ? * theorem 5.2 )    [ th : ks ] let @xmath2 be the class of functions @xmath127 of bounded variation defined on @xmath128 $ ] is of bounded variation @xmath129 if the total variation is bounded by @xmath129 , i.e. the supremum over all sums @xmath130 where @xmath131 @xcite . ] 1 . then @xmath91 = { \\left\\|f_p - f_q\\right\\|}_\\infty$ ] .",
    "it is well known that @xmath2 is given by the absolute convex hull of indicator functions @xmath132}(\\cdot)$ ] and @xmath133 .",
    "moreover , note that @xmath134 is convex in @xmath82 .",
    "hence the solution of the optimization problem is achieved by checking all the vertices of the convex set @xmath2 .",
    "that is , we may expand @xmath135 $ ] as follows : @xmath136 } - { \\mathbf{e}}_q \\chi_{(-\\infty , x]}\\right| } ,         \\sup_{x \\in { \\mathbb{r } } }         { \\left|{\\mathbf{e}}_p \\chi_{[x,\\infty ) } - { \\mathbf{e}}_q \\chi_{[x,\\infty)}\\right| }       \\right ] } \\nonumber \\\\",
    "\\nonumber      & = \\sup_{x \\in { \\mathbb{r } } } { \\left|f_p(x ) - f_q(x)\\right| } = { \\left\\|f_p - f_q\\right\\|}_\\infty .",
    "\\end{aligned}\\ ] ] this completes to proof .",
    "another class of distance measures on distributions that may be written as an mmd are the earth - mover distances .",
    "we assume @xmath137 is a separable metric space , and define @xmath138 to be the space of probability measures on @xmath139 for which @xmath140 for all @xmath141 and @xmath142 ( these are the probability measures for which @xmath143 when @xmath144 ) .",
    "we then have the following definition @xcite .",
    "let @xmath141 and @xmath145 .",
    "the monge - wasserstein distance is defined as@xmath146 where @xmath147 is the set of joint distributions on @xmath148 with marginals @xmath0 and @xmath1 .",
    "we may interpret this as the cost ( as represented by the metric @xmath149 ) of transferring mass distributed according to @xmath0 to a distribution in accordance with @xmath1 , where @xmath150 is the movement schedule . in general , a large variety of costs of moving mass from @xmath52 to @xmath58 can be used , such as psychooptical similarity measures in image retrieval @xcite .",
    "the following theorem holds ( * ? ? ?",
    "* theorem 11.8.2 ) .",
    "[ thm : kantorovichrubinstein ] let @xmath141 and @xmath145 , where @xmath10 is separable . then a metric on @xmath151 is defined as@xmath152 where@xmath153 is the lipschitz seminorm only for @xmath154 @xcite . ] for real valued @xmath82 on @xmath10 .",
    "a simple example of this theorem is as follows ( * ? ? ?",
    "* exercise 1 , p. 425 ) .",
    "let @xmath144 with associated @xmath155 .",
    "then given @xmath82 such that @xmath156 , we use integration by parts to obtain@xmath157 where the maximum is attained for the function @xmath158 with derivative @xmath159 ( and for which @xmath160 ) .",
    "we recover the @xmath161 distance between distribution functions,@xmath162    one may further generalize theorem [ thm : kantorovichrubinstein ] to the set of all laws @xmath163 on arbitrary metric spaces @xmath139 ( * ? ? ? * proposition 11.3.2 ) .",
    "let @xmath0 and @xmath1 be laws on a metric space @xmath139 .",
    "then@xmath164 is a metric on @xmath163 , where @xmath82 belongs to the space of bounded lipschitz functions with norm@xmath165    we now define a general mean operator @xmath166 in the same fashion as introduced in lemma  [ le : simplemmd ] for rkhs , in the context of banach spaces .",
    "denote by @xmath167 a banach space of functions on @xmath10 and let @xmath168 be its dual . in this case",
    "we denote by @xmath169 the evaluation operator @xmath170 whenever it exists .",
    "the definition is implicit via fischer - riesz .",
    "moreover , for a distribution @xmath0 on @xmath10 we denote by @xmath166 the linear map given by @xmath171}\\end{aligned}\\ ] ] whenever it exists .",
    "note that existence of @xmath166 is far from trivial  for some functions ( or some distributions ) expectations of certain random variable may not exist , hence @xmath172 is undefined in this case .",
    "a sufficient condition for the existence of @xmath166 is that the norm of the evaluation operators is bounded , i.e.  @xmath173 for some @xmath174 .    if @xmath2 is the unit ball in a banach space @xmath167 we have the following result which allows us to find a more concise expression for @xmath175 $ ] and @xmath176 $ ] .",
    "[ th : norms ] assume that @xmath167 is a banach space where the operator @xmath177 is well defined .",
    "then the following holds : @xmath178 = { \\left\\|\\mu_p - \\mu_q\\right\\|}_{{\\mathcal{b}}^ * }      \\text { and }      { \\mathrm{mmd}}_b[{\\mathcal{f } } , x , y ] = { \\left\\|\\mu[x ] - \\mu[y]\\right\\|}_{{\\mathcal{b}}^*}.     \\end{aligned}\\ ] ] the set of all expectations , that is , the marginal polytope @xmath179 \\text { where } p \\in       { \\mathcal{p}}({\\mathcal{x}})\\right\\}}$ ] is convex .",
    "moreover assume that @xmath180 is dense in @xmath18 .",
    "in this case the map @xmath150 is injective .",
    "the first part is a result of a simple duality argument .",
    "we have @xmath181 = \\sup_{f \\in { \\mathcal{f } } } { \\left|{\\mathbf{e}}_p f - { \\mathbf{e}}_q f\\right| } =       \\sup_{{\\left\\|f\\right\\| } \\leq 1 } { \\left|{\\left\\langle \\mu_p - \\mu_q , f \\right\\rangle}\\right| } =       { \\left\\|\\mu_p - \\mu_q\\right\\|}_{{\\mathcal{b}}^ * }    \\end{aligned}\\ ] ] by the very definition of dual norms . the claim for the empirical counterpart follows from the same reasoning . to see the second claim , note that the space of probability distributions is convex .",
    "moreover , @xmath150 is a linear operator , hence its image of a convex set is convex , too . finally , to see the third claim we appeal to theorem  [ th : stronger - star ] .",
    "since the star of the unit ball in @xmath167 is the entire banach space itself , the conditions of theorem  [ th : stronger - star ] hold .",
    "using our first claim this proves injectivity .    as a result",
    "@xmath135 $ ] defines a _ metric _ on the space of probability distributions , induced by the banach space @xmath167 . as we shall see , it is often easier to compute distances in this metric , as it will not require density estimation as an intermediate step .",
    "we now present three background results .",
    "first , we introduce the terminology used in statistical hypothesis testing .",
    "second , we demonstrate via an example that even for tests which have asymptotically no error , one can not guarantee performance at any fixed sample size without making assumptions about the distributions .",
    "finally , we briefly review some earlier approaches to the two - sample problem .",
    "having described a metric on probability distributions ( the mmd ) based on distances between their hilbert space embeddings , and empirical estimates ( biased and unbiased ) of this metric , we now address the problem of determining whether the empirical mmd shows a _ statistically significant _ difference between distributions . to this end",
    ", we briefly describe the framework of statistical hypothesis testing as it applies in the present context , following ( * ? ? ?",
    "* chapter 8) . given i.i.d .",
    "samples @xmath182 of size @xmath4 and @xmath183 of size @xmath5 , the statistical test , @xmath184 is used to distinguish between the null hypothesis @xmath185 and the alternative hypothesis @xmath186 .",
    "this is achieved by comparing the test statistic @xmath187 $ ] with a particular threshold : if the threshold is exceeded , then the test rejects the null hypothesis ( bearing in mind that a zero population mmd indicates @xmath8 ) .",
    "the acceptance region of the test is thus defined as the set of real numbers below the threshold .",
    "since the test is based on finite samples , it is possible that an incorrect answer will be returned : we define the type i error as the probability of rejecting @xmath8 based on the observed sample , despite the null hypothesis having generated the data .",
    "conversely , the type ii error is the probability of accepting @xmath8 despite the underlying distributions being different .",
    "the _ level _",
    "@xmath188 of a test is an upper bound on the type i error : this is a design parameter of the test , and is used to set the threshold to which we compare the test statistic ( finding the test threshold for a given @xmath188 is the topic of sections [ sec : firstbound ] and [ sec : asymptotictest ] ) .",
    "a consistent test achieves a level @xmath188 , and a type ii error of zero , in the large sample limit .",
    "we will see that the tests proposed in this paper are consistent",
    ".      even if a test is consistent , it is not possible to distinguish distributions with high probability at a given , fixed sample size ( i.e. , to provide guarantees on the type ii error ) , without prior assumptions as to the nature of the difference between @xmath0 and @xmath1 .",
    "this is true _",
    "regardless _ of the two - sample test used .",
    "there are several ways to illustrate this , which each give different insight into the kinds of differences that might be undetectable for a given number of samples .",
    "the following example is one such illustration .",
    "assume that we have a distribution @xmath0 from which we draw @xmath4 iid observations .",
    "moreover , we construct a distribution @xmath1 by drawing @xmath189 iid observations from p and subsequently defining a discrete distribution over these @xmath189 instances with probability @xmath190 each .",
    "it is easy to check that if we now draw @xmath4 observations from @xmath1 , there is at least a @xmath191 probability that we thereby will have effectively obtained an @xmath4 sample from @xmath0 .",
    "hence no test will be able to distinguish samples from @xmath0 and @xmath1 in this case .",
    "we could make the probability of detection arbitrarily small by increasing the size of the sample from which we construct @xmath1 .",
    "we next give a brief overview of some earlier approaches to the two sample problem for multivariate data .",
    "since our later experimental comparison is with respect to certain of these methods , we give abbreviated algorithm names in italics where appropriate : these should be used as a key to the tables in section [ sec : experiments ] .",
    "a generalisation of the wald - wolfowitz runs test to the multivariate domain was proposed and analysed by @xcite _ ( fr wolf ) _ , and involves counting the number of edges in the minimum spanning tree over the aggregated data that connect points in @xmath74 to points in @xmath75 .",
    "the resulting test relies on the asymptotic normality of the test statistic , and this quantity is not distribution - free under the null hypothesis for finite samples ( it depends on @xmath0 and @xmath1 ) .",
    "the computational cost of this method using kruskal s algorithm is @xmath192 , although more modern methods improve on the @xmath193 term .",
    "see @xcite for details .",
    "@xcite claim that calculating the matrix of distances , which costs @xmath78 , dominates their computing time ; we return to this point in our experiments ( section [ sec : experiments ] ) .",
    "two possible generalisations of the kolmogorov - smirnov test to the multivariate case were studied in @xcite .",
    "the approach of friedman and rafsky _",
    "( fr smirnov ) _ in this case again requires a minimal spanning tree , and has a similar cost to their multivariate runs test .",
    "a more recent multivariate test was introduced by @xcite .",
    "this entails computing the minimum distance non - bipartite matching over the aggregate data , and using the number of pairs containing a sample from both @xmath74 and @xmath75 as a test statistic .",
    "the resulting statistic is distribution - free under the null hypothesis at finite sample sizes , in which respect it is superior to the friedman - rafsky test ; on the other hand , it costs @xmath194 to compute . another distribution - free test _ ( hall ) _ was proposed by @xcite : for each point from @xmath0 , it requires computing the closest points in the aggregated data , and counting how many of these are from @xmath1 ( the procedure is repeated for each point from @xmath1 with respect to points from @xmath0 ) . as we shall see in our experimental comparisons ,",
    "the test statistic is costly to compute ; @xcite consider only tens of points in their experiments .",
    "yet another approach is to use some distance ( e.g. @xmath195 or @xmath196 ) between parzen window estimates of the densities as a test statistic @xcite , based on the asymptotic distribution of this distance given @xmath8 .",
    "when the @xmath196 norm is used , the test statistic is related to those we present here , although it is arrived at from a different perspective .",
    "briefly , the test of @xcite is obtained in a more restricted setting where the rkhs kernel is an inner product between parzen windows . since we are not doing density estimation , however , we need not decrease the kernel width as the sample grows .",
    "in fact , decreasing the kernel width reduces the convergence rate of the associated two - sample test , compared with the @xmath197 rate for fixed kernels .",
    "we provide more detail in section [ sec : mmdparzen ] .",
    "the @xmath195 approach of @xcite _ ( biau ) _ requires the space to be partitioned into a grid of bins , which becomes difficult or impossible for high dimensional problems .",
    "hence we use this test only for low - dimensional problems in our experiments .",
    "in this section , we introduce two statistical tests of independence which have exact performance guarantees at finite sample sizes , based on uniform convergence bounds .",
    "the first , in section [ sec : boundbiasedstat ] , uses the @xcite bound on the biased mmd statistic , and the second , in section [ sec : boundunbiasedstat ] , uses a @xcite bound for the unbiased statistic .",
    "we establish two properties of the mmd , from which we derive a hypothesis test .",
    "first , we show that regardless of whether or not @xmath8 , the empirical mmd converges in probability at rate @xmath198 to its population value .",
    "this shows the consistency of statistical tests based on the mmd .",
    "second , we give probabilistic bounds for large deviations of the empirical mmd in the case @xmath8 .",
    "these bounds lead directly to a threshold for our first hypothesis test .",
    "we begin our discussion of the convergence of @xmath103 $ ] to @xmath135 $ ] .",
    "[ th : mmd - diff ] let @xmath199 be defined as in problem  [ prob : problem ] , and assume @xmath200",
    ". then @xmath201 - { \\mathrm{mmd}}[{\\mathcal{f } } , p , q]\\right| } >   2\\left ( ( k / m)^{\\frac{1}{2 } } + ( k / n)^{\\frac{1}{2 } } \\right ) + \\epsilon\\right\\ } }      \\leq 2\\exp{\\left(\\textstyle \\frac{-\\epsilon^2 m n}{2 k ( m+n)}\\right)}.     \\end{aligned}\\ ] ]    see appendix [ sec : pqdiffproof ] for proof .",
    "our next goal is to refine this result in a way that allows us to define a test threshold under the null hypothesis @xmath8 . under this circumstance ,",
    "the constants in the exponent are slightly improved .",
    "[ th : mmd - same - a ] under the conditions of theorem  [ th : mmd - diff ] where additionally @xmath3 and @xmath202 , @xmath203 \\leq    \\underset{b_1({\\mathcal{f}},p ) } { \\underbrace {   m^{-{\\frac{1}{2 } } } \\sqrt{2 { \\mathbf{e}}_p{\\left[k(x , x ) - k(x , x')\\right ] } } } }   + \\epsilon \\leq   \\underset{b_2({\\mathcal{f}},p ) } { \\underbrace { ( 2k / m)^{1/2 } } }   + \\epsilon ,    \\end{aligned}\\ ] ] both with probability at least @xmath204 ( see appendix [ sec : largedevunderh0 ] for the proof ) .    in this theorem , we illustrate two possible bounds @xmath205 and @xmath206 on the bias in the empirical estimate ( [ eq : mmd - e - rkhs ] ) .",
    "the first inequality is interesting inasmuch as it provides a link between the bias bound @xmath205 and kernel size ( for instance , if we were to use a gaussian kernel with large @xmath207 , then @xmath208 and @xmath209 would likely be close , and the bias small ) . in the context of testing , however , we would need to provide an additional bound to show convergence of an empirical estimate of @xmath205 to its population equivalent .",
    "thus , in the following test for @xmath8 based on theorem [ th : mmd - same - a ] , we use @xmath206 to bound the bias .",
    "[ lem : firsttest ] a hypothesis test of level @xmath188 for the null hypothesis @xmath8 , that is , for @xmath135 = 0 $ ] , has the acceptance region @xmath210",
    "<   \\sqrt{2k / m } \\left ( 1 + \\sqrt{2\\log \\alpha^{-1 } }   \\right ) .",
    "$ ]    we emphasise that theorem [ th : mmd - diff ] guarantees the consistency of the test , and that the type ii error probability decreases to zero at rate @xmath211 , assuming @xmath64 . to put this convergence rate in perspective , consider a test of whether two normal distributions have equal means , given they have unknown but equal variance ( * ? ? ?",
    "* exercise 8.41 ) . in this case",
    ", the test statistic has a student-@xmath212 distribution with @xmath213 degrees of freedom , and its error probability converges at the same rate as our test .",
    "it is worth noting that bounds may be obtained for the deviation between expectations @xmath214 $ ] and the empirical means @xmath72 $ ] in a completely analogous fashion .",
    "the proof requires symmetrization by means of a _ ghost sample _ , i.e.  a second set of observations drawn from the same distribution . while not the key focus of the present paper , such bounds can be used in the design of inference principles based on moment matching @xcite .      while the previous bounds are of interest since the proof strategy can be used for general function classes with well behaved rademacher averages , a much easier approach may be used directly on the unbiased statistic @xmath215 in lemma [ eq : mmd - e - rkhs - unbiased ] .",
    "we base our test on the following theorem , which is a straightforward application of the large deviation bound on u - statistics of @xcite .",
    "[ thm : hoeffdingquadraticmmd ] assume @xmath216 , from which it follows @xmath217",
    ". then @xmath218 where @xmath219 ( the same bound applies for deviations of @xmath220 and below ) .    a consistent statistical test for @xmath8 using @xmath215 is then obtained .",
    "[ cor : hoeffdingthresh ] a hypothesis test of level @xmath188 for the null hypothesis @xmath8 has the acceptance region @xmath221 .",
    "we now compare the thresholds of the two tests .",
    "we note first that the threshold for the biased statistic applies to an estimate of @xmath9 , whereas that for the unbiased statistic is for an estimate of @xmath60 . squaring the former threshold to make the two quantities comparable ,",
    "the squared threshold in corollary [ lem : firsttest ] decreases as @xmath222 , whereas the threshold in corollary [ cor : hoeffdingthresh ] decreases as @xmath223 .",
    "thus for sufficiently large , this is @xmath224 . ]",
    "@xmath4 , the mcdiarmid - based threshold will be lower ( and the associated test statistic is in any case biased upwards ) , and its type ii error will be better for a given type i bound .",
    "this is confirmed in our section [ sec : experiments ] experiments .",
    "note , however , that the rate of convergence of the squared , biased mmd estimate to its population value remains at @xmath225 ( bearing in mind we take the square of a biased estimate , where the bias term decays as @xmath225 ) .",
    "finally , we note that the bounds we obtained here are rather conservative for a number of reasons : first , they do not take the actual distributions into account .",
    "in fact , they are finite sample size , distribution free bounds that hold even in the worst case scenario . the bounds could be tightened using localization , moments of the distribution , etc .",
    "any such improvements could be plugged straight into theorem  [ th : general ] for a tighter bound .",
    "see e.g.  @xcite for a detailed discussion of recent uniform convergence bounding methods .",
    "second , in computing _ bounds _ rather than trying to characterize the distribution of @xmath226 explicitly , we force our test to be conservative by design . in the following we aim for an exact characterization of the asymptotic distribution of @xmath227 instead of",
    "while this will not satisfy the uniform convergence requirements , it leads to superior tests in practice .",
    "we now propose a third test , which is based on the asymptotic distribution of the unbiased estimate of @xmath60 in lemma [ lem : rkhs - mmd ] .",
    "[ theorem : hoeffdingnormal ] we assume @xmath228 . under @xmath229 , @xmath230 converges in distribution ( see e.g. * ? ? ?",
    "* section 7.2 ) to a gaussian according to @xmath231 } \\right ) \\overset{d}{\\rightarrow } \\mathcal{n}\\left(0 , \\sigma^2_u \\right ) , \\:\\ ] ] where @xmath232   - \\left [ { \\mathbf{e}}_{z , z ' } ( h(z , z ' ) ) \\right]^2 \\right)$ ] , uniformly at rate @xmath225 ( * ? ? ?",
    "* theorem b , p. 193 ) . under @xmath125 ,",
    "the u - statistic is degenerate , meaning @xmath233 . in this case",
    ", @xmath230 converges in distribution according to @xmath234,\\ ] ] where @xmath235 i.i.d .",
    ", @xmath236 are the solutions to the eigenvalue equation @xmath237 and @xmath238 is the centred rkhs kernel .    the asymptotic distribution of the test statistic under @xmath229 is given by ( * ?",
    "* section 5.5.1 ) , and the distribution under @xmath125 follows ( * ? ? ?",
    "* section 5.5.2 ) and ( * ? ? ? * appendix ) ; see appendix [ sec : distribh0 ] for details .",
    "we illustrate the mmd density under both the null and alternative hypotheses by approximating it empirically for both @xmath8 and @xmath239 .",
    "results are plotted in figure [ fg : distributionofmmd ] .",
    ", with @xmath0 and @xmath1 both gaussians with unit standard deviation , using 50 samples from each .",
    "* right : * empirical distribution of the mmd under @xmath229 , with @xmath0 a laplace distribution with unit standard deviation , and @xmath1 a laplace distribution with standard deviation @xmath240 , using 100 samples from each . in both cases ,",
    "the histograms were obtained by computing 2000 independent instances of the mmd.,title=\"fig : \" ] , with @xmath0 and @xmath1 both gaussians with unit standard deviation , using 50 samples from each . *",
    "right : * empirical distribution of the mmd under @xmath229 , with @xmath0 a laplace distribution with unit standard deviation , and @xmath1 a laplace distribution with standard deviation @xmath240 , using 100 samples from each . in both cases ,",
    "the histograms were obtained by computing 2000 independent instances of the mmd.,title=\"fig : \" ]    our goal is to determine whether the empirical test statistic @xmath230 is so large as to be outside the @xmath241 quantile of the null distribution in ( [ eq : mmd_under_h0 ] ) ( consistency of the resulting test is guaranteed by the form of the distribution under @xmath229 ) .",
    "one way to estimate this quantile is using the bootstrap on the aggregated data , following @xcite .",
    "alternatively , we may approximate the null distribution by fitting pearson curves to its first four moments ( * ? ? ?",
    "* section 18.8 ) .",
    "taking advantage of the degeneracy of the u - statistic , we obtain ( see appendix [ sec : momentsh0 ] ) @xmath242 ^ 2\\right ) &   = \\frac{2}{m(m-1)}{\\mathbf{e}}_{z , z'}\\left[h^{2}(z , z ' ) \\right ]   \\text { and } \\\\",
    "\\label{moment3 } { \\mathbf{e}}\\left(\\left[{\\mathrm{mmd}}_u^2\\right]^3\\right ) &   = \\frac{8(m-2)}{m^{2}(m-1)^{2}}{\\mathbf{e}}_{z , z'}\\left[h(z , z'){\\mathbf{e}}_{z''}\\left(h(z , z'')h(z',z'')\\right)\\right ] + o(m^{-4 } ) .",
    "\\end{aligned}\\ ] ] the fourth moment @xmath243 ^ 4\\right)$ ] is not computed , since it is both very small , @xmath244 , and expensive to calculate , @xmath245 .",
    "instead , we replace the kurtosis^4\\right)}{\\left[{\\mathbf{e}}\\left(\\left[{\\mathrm{mmd}}_u^2\\right]^2\\right)\\right]^2 } -3 $ ] . ] with a lower bound due to @xcite , @xmath246 .",
    "note that @xmath247 may be negative , since it is an unbiased estimator of @xmath248)^2 $ ] .",
    "however , the only terms missing to ensure nonnegativity are the terms @xmath249 , which were removed to remove spurious correlations between observations .",
    "consequently we have the bound @xmath250",
    "while the above tests are already more efficient than the @xmath251 and @xmath252 tests described earlier , it is still desirable to obtain @xmath253 tests which do not sacrifice too much statistical power . moreover , we would like to obtain tests which have @xmath254 storage requirements for computing the test statistic in order to apply it to data streams .",
    "we now describe how to achieve this by computing the test statistic based on a subsampling of the terms in the sum . the empirical estimate in this case",
    "is obtained by drawing pairs from @xmath74 and @xmath75 respectively _ without _ replacement .",
    "recall @xmath219 .",
    "the estimator @xmath255 : = \\frac{1}{m_2 } \\sum_{i=1}^{m_2 }     h((x_{2i-1 } , y_{2i-1 } ) , ( x_{2i } , y_{2i}))\\ ] ] can be computed in linear time .",
    "moreover , it is an unbiased estimate of @xmath256 $ ] .    while it is expected ( as we will see explicitly later ) that @xmath257 has higher variance than @xmath230 , it is computationally much more appealing . in particular ,",
    "the statistic can be used in stream computations with need for only @xmath254 memory , whereas @xmath230 requires @xmath253 storage and @xmath258 time to compute the kernel @xmath259 on all interacting pairs .",
    "since @xmath257 is just the average over a set of random variables , hoeffding s bound and the central limit theorem readily allow us to provide both uniform convergence and asymptotic statements for it with little effort .",
    "the first follows directly from ( * ? ? ?",
    "* theorem 2 ) .",
    "[ cor : asy - linear ] assume @xmath260 .",
    "then @xmath261 where @xmath219 ( the same bound applies for deviations of @xmath220 and below ) .",
    "note that the bound of theorem  [ thm : hoeffdingquadraticmmd ] is identical to that of theorem  [ cor : asy - linear ] , which shows the former is rather loose .",
    "next we invoke the central limit theorem .",
    "[ cor : linearstat ] assume @xmath262 .",
    "then @xmath257 converges in distribution to a gaussian according to @xmath263 } \\right )    \\overset{d}{\\rightarrow }    \\mathcal{n}\\left(0 , \\sigma^2_l \\right),\\ ] ] where @xmath264 ^ 2\\right]}$ ] , uniformly at rate @xmath225 .",
    "the factor of @xmath265 arises since we are averaging over only @xmath266 observations .",
    "note the difference in the variance between theorem  [ theorem : hoeffdingnormal ] and corollary  [ cor : linearstat ] , namely in the former case we are interested in the average conditional variance @xmath267 $ ] , whereas in the latter case we compute the full variance @xmath268 $ ] .",
    "we end by noting another potential approach to reducing the computational cost of the mmd , by computing a low rank approximation to the gram matrix @xcite .",
    "an incremental computation of the mmd based on such a low rank approximation would require @xmath269 storage and @xmath270 computation ( where @xmath271 is the rank of the approximate gram matrix which is used to factorize _ both _ matrices ) rather than @xmath253 storage and @xmath258 operations .",
    "that said , it remains to be determined what effect this approximation would have on the distribution of the test statistic under @xmath125 , and hence on the test threshold .",
    "there exists an alternative method which allows us to approximate @xmath60 while enjoying the lower variance of the asymptotic analysis of section  [ sec : asymptotictest ] .",
    "this method can be used in particular verify @xmath13 . in other words , it is able to give us good sufficient conditions for @xmath229 , while we lose the necessary and sufficient conditions required to test effectively for @xmath125 by restricting ourselves to a finite dimensional subspace .    as suggested by @xcite",
    "one may use a low - rank approximation of the kernel @xmath209 via @xmath272}\\end{aligned}\\ ] ] here @xmath273 is the kernel matrix associated with the collection of points @xmath274 , which are chosen to provide a good approximation of the space spanned by the data .",
    "popular choices are to pick the @xmath275 randomly from @xmath74 and @xmath75 , as suggested by @xcite , to use a randomized optimization procedure , as suggested by @xcite , or to use positive diagonal pivoting @xcite .",
    "such methods have been used , e.g.  by @xcite to obtain fast and accurate linear ica algorithms .",
    "clearly , by projecting @xmath52 to a finite dimensional space via @xmath276 , the expectation operator @xmath177 is no longer injective for general @xmath10 .",
    "however , whenever for this reduced - dimensionality variant we still have @xmath277 , our test will be able to detect it . note that computing @xmath276 costs @xmath278 operations , since we need to perform a matrix - vector multiplication with the matrix @xmath279 .",
    "next observe that @xmath230 can be rewritten as @xmath280 an incremental computation of this quantity requires @xmath281 storage and @xmath282 computation rather than @xmath253 storage and @xmath258 operations .",
    "this improvement can be significant , in particular when it comes to online computations . also note that in this case the operations can be readily parallelized , since only the averages rather than the actual values @xmath276 need to be transmitted to a central receiver for a final assessment",
    "our main point is to propose a new kernel statistic to test whether two distributions are the same .",
    "however , it is reassuring to observe links to other measures of similarity between distributions .      in this section",
    ", we demonstrate the connection between our test statistic and the parzen window - based statistic of @xcite .",
    "we show that a two - sample test based on parzen windows converges more slowly than an rkhs - based test , also following @xcite . before proceeding",
    ", we motivate this discussion with a short overview of the parzen window estimate and its properties @xcite .",
    "we assume a distribution @xmath0 on @xmath283 , which has an associated density function also written @xmath0 to minimise notation .",
    "the parzen window estimate of this density from an i.i.d .",
    "sample @xmath74 of size @xmath4 is @xmath284 we may rescale @xmath285 according to @xmath286 .",
    "consistency of the parzen window estimate requires @xmath287 we now show that the @xmath196 distance between parzen windows density estimates @xcite is a special case of the biased mmd in equation ( [ eq : mmd - e - rkhs ] ) .",
    "denote by @xmath288 the @xmath289 distance .",
    "for @xmath290 the distance @xmath291 is known as the levy distance @xcite , and for @xmath292 we encounter distance measures derived from the renyi entropy @xcite .",
    "assume that @xmath293 and @xmath294 are given as kernel density estimates with kernel @xmath295 , that is , @xmath296 and @xmath297 is defined by analogy .",
    "in this case @xmath298}^2 dz & \\\\    & =     \\frac 1{m^2 } \\sum_{i , j=1}^m k(x_i- x_j ) +     \\frac 1{n^2 } \\sum_{i , j=1}^n k(y_i- y_j )      - \\frac 2{mn } \\sum_{i , j=1}^{m , n } k(x_i-",
    "y_j ) , &    \\label{eq : parzenmeasure } \\end{aligned}\\ ] ] where @xmath299 . by its definition @xmath300 is a mercer kernel @xcite , as it can be viewed as inner product between @xmath301 and @xmath302 on the domain @xmath10 .",
    "a disadvantage of the parzen window interpretation is that when the parzen window estimates are consistent ( which requires the kernel size to decrease with increasing sample size ) , the resulting two - sample test converges more slowly than using fixed kernels .",
    "according to @xcite , the type ii error of the two - sample test converges as @xmath303 .",
    "thus , given the schedule for the parzen window size decrease in ( [ eq : parzendecrease ] ) , the convergence rate will lie in the open interval @xmath304 : the upper limit is approached as @xmath305 decreases more slowly , and the lower limit corresponds to @xmath305 decreasing near the upper bound of @xmath306 . in other words , by avoiding density estimation , we obtain a better convergence rate ( namely @xmath223 ) than using a parzen window estimate with _ any _ permissible bandwidth decrease schedule .",
    "in addition , the parzen window interpretation can not explain the excellent performance of mmd based tests in experimental settings where the dimensionality greatly exceeds the sample size ( for instance the gaussian toy example in figure [ fig : gaussians]b , for which performance actually improves when the dimensionality increases ; and the microarray datasets in table [ tab : multivariate ] ) .",
    "finally , our tests are able to employ universal kernels that can not be written as inner products between parzen windows , normalized or otherwise : several examples are given by ( * ? ? ?",
    "* section 3 ) and ( * ? ? ?",
    "* section 3 ) .",
    "we may further generalize to kernels on structured objects such as strings and graphs @xcite : see also our experiments in section [ sec : experiments ] .",
    "@xcite propose kernels to deal with sets of observations .",
    "these are then used in the context of multi - instance classification ( mic ) .",
    "the problem mic attempts to solve is to find estimators which are able to infer from the fact that some elements in the set satisfy a certain property , then the set of observations has this property , too .",
    "for instance , a dish of mushrooms is poisonous if it contains poisonous mushrooms . likewise",
    "a keyring will open a door if it contains a suitable key .",
    "one is only given the ensemble , however , rather than information about which instance of the set satisfies the property .",
    "the solution proposed by @xcite is to map the ensembles @xmath307 , where @xmath308 is the ensemble index and @xmath309 the number of elements in the @xmath308th ensemble , jointly into feature space via @xmath310 and use the latter as the basis for a kernel method .",
    "this simple approach affords rather good performance . with the benefit of hindsight ,",
    "it is now understandable why the kernel @xmath311 produces useful results : it is simply the kernel between the empirical means in feature space @xmath312 ( * ? ? ?",
    "4 ) . @xcite later extended this setting by smoothing the empirical densities before computing inner products .",
    "note , however , that property testing for distributions is probably not optimal when using the mean @xmath214 $ ] ( or @xmath72 $ ] respectively ) : we are only interested in determining whether _ some _ instances in the domain have the desired property , rather than making a statement regarding the distribution of those instances .",
    "taking this into account leads to an improved algorithm @xcite .",
    "we next demonstrate the application of mmd in determining whether two random variables @xmath52 and @xmath58 are independent .",
    "in other words , assume that pairs of random variables @xmath313 are jointly drawn from some distribution @xmath314 .",
    "we wish to determine whether this distribution factorizes , i.e. whether @xmath315 is the same as @xmath0 .",
    "one application of such an independence measure is in independent component analysis @xcite , where the goal is to find a linear mapping of the observations @xmath316 to obtain mutually independent outputs .",
    "kernel methods were employed to solve this problem by @xcite . in the following",
    "we re - derive one of the above kernel independence measures using mean operators instead .",
    "we begin by defining @xmath317 & : = { \\mathbf{e}}_{x , y } { \\left[v((x , y ) , \\cdot)\\right ] } \\\\",
    "\\text{and }    \\mu[\\pr_x \\times \\pr_y ] & : = { \\mathbf{e}}_{x } { \\mathbf{e}}_{y } { \\left[v((x , y ) , \\cdot)\\right]}.\\end{aligned}\\ ] ] here we assumed that @xmath318 is an rkhs over @xmath319 with kernel @xmath320 .",
    "if @xmath52 and @xmath58 _ are _ dependent , the equality @xmath321 = \\mu[\\pr_x \\times \\pr_y]$ ] will not hold .",
    "hence we may use @xmath322 - \\mu[\\pr_x    \\times \\pr_y]\\right\\|}$ ] as a measure of dependence .",
    "now assume that @xmath323 , i.e.  that the rkhs @xmath318 is a direct product @xmath324 of the rkhss on @xmath10 and @xmath325 . in this case",
    "it is easy to see that @xmath326 } - { \\mathbf{e}}_x      { \\left[k(x,\\cdot)\\right ] } { \\mathbf{e}}_y { \\left[l(y,\\cdot)\\right]}\\right\\|}^2 \\\\    & = & { \\mathbf{e}}_{xy } { \\mathbf{e}}_{x'y ' } { \\left[k(x , x ' ) l(y , y')\\right ] } -               2 { \\mathbf{e}}_x { \\mathbf{e}}_y { \\mathbf{e}}_{x'y ' } { \\left[k(x , x ' ) l(y , y')\\right ] }   \\\\    & & + { \\mathbf{e}}_x { \\mathbf{e}}_y { \\mathbf{e}}_{x ' } { \\mathbf{e}}_{y ' } { \\left[k(x , x ' ) l(y , y')\\right]}\\end{aligned}\\ ] ] the latter , however , is exactly what @xcite show to be the hilbert - schmidt norm of the cross - covariance operator between rkhss : this is zero if and only if @xmath52 and @xmath58 are independent , for universal kernels .",
    "we have the following theorem :    denote by @xmath327 the covariance operator between random variables @xmath52 and @xmath58 , drawn jointly from @xmath328 , where the functions on @xmath10 and @xmath325 are the reproducing kernel hilbert spaces @xmath2 and @xmath329 respectively .",
    "then the hilbert - schmidt norm @xmath330 equals @xmath331 .",
    "empirical estimates of this quantity are as follows :    denote by @xmath332 and @xmath333 the kernel matrices on @xmath74 and @xmath75 respectively , and by @xmath334 the projection matrix onto the subspace orthogonal to the vector with all entries set to @xmath335 .",
    "then @xmath336 is an estimate of @xmath337 with bias @xmath338 . with high probability",
    "the deviation from @xmath337 is @xmath211 .",
    "@xcite provide explicit constants . in certain circumstances , including in the case of rkhss with gaussian kernels",
    ", the empirical @xmath337 may also be interpreted in terms of a smoothed difference between the joint empirical characteristic function ( ecf ) and the product of the marginal ecfs @xcite .",
    "this interpretation does not hold in all cases , however , e.g. for kernels on strings , graphs , and other structured spaces .",
    "an illustration of the witness function @xmath339 from definition [ def : mmd ] is provided in figure [ fig : rotationmapping ] .",
    "this is a smooth function which has large magnitude where the joint density is most different from the product of the marginals .     and @xmath58 is shown in black , and the associated function @xmath82 that witnesses the mmd is plotted as a contour .",
    "the latter was computed empirically on the basis of @xmath340 samples , using a gaussian kernel with @xmath341.[fig : rotationmapping],scaledwidth=70.0% ]    we remark that a hypothesis test based on the above kernel statistic is more complicated than for the two - sample problem , since the product of the marginal distributions is in effect simulated by permuting the variables of the original sample .",
    "further details are provided by @xcite .",
    "@xcite define a distance between distributions as follows : let @xmath23 be a set of functions on @xmath10 and @xmath342 be a probability distribution over @xmath2 .",
    "then the distance between two distributions @xmath0 and @xmath1 is given by @xmath343 - { \\mathbf{e}}_{x \\sim        q}[f(x)]\\right|}.\\end{aligned}\\ ] ] that is , we compute the average distance between @xmath0 and @xmath1 with respect to a distribution of test functions .",
    "let @xmath23 be a reproducing kernel hilbert space , @xmath344 , and assume @xmath345 with finite @xmath346 $ ] .",
    "then @xmath347 - \\mu[q]\\right\\|}_{{\\mathcal{h}}}$ ] for some constant @xmath129 which depends only on @xmath23 and @xmath342 .    by definition @xmath348 = { \\left\\langle \\mu[p],f \\right\\rangle}_{{\\mathcal{h}}}$ ] . using linearity of the inner product , equation  ( [ eq : shado ] )",
    "equals @xmath349 - \\mu[q],f \\right\\rangle}_{{\\mathcal{h}}}\\right| } \\mathrm{d}r(f)\\\\     = & { \\left\\|\\mu[p ] - \\mu[q]\\right\\|}_{{\\mathcal{h } } } \\int { \\left|{\\left\\langle \\frac{\\mu[p ] - \\mu[q]}{{\\left\\|\\mu[p ] - \\mu[q]\\right\\|}_{{\\mathcal{h}}}},f \\right\\rangle}_{{\\mathcal{h}}}\\right| } \\mathrm{d}r(f ) , \\end{aligned}\\ ] ] where the integral is independent of @xmath15 . to see this",
    ", note that for any @xmath15 , @xmath350 - \\mu[q]}{{\\left\\|\\mu[p ] - \\mu[q]\\right\\|}_{{\\mathcal{h}}}}$ ] is a unit vector which can turned into , say , the first canonical basis vector by a rotation which leaves the integral invariant , bearing in mind that @xmath342 is rotation invariant .",
    "the above result covers a large number of interesting function classes . to go beyond hilbert spaces ,",
    "let @xmath351 be the transformation from @xmath52 into @xmath352 for all @xmath353 and @xmath354 $ ] be the @xmath355 norm .",
    "then ( [ eq : shado ] ) can also be written as @xmath356 - \\mu[q]\\right\\|}_{{\\mathcal{b}}}$ ] , where @xmath150 is the mean map into banach spaces .",
    "the main drawback is the nontrivial computation for constraint sampling  @xcite and the additional uniform convergence reasoning required . in hilbert spaces",
    "no such operations are needed .      the earth - mover , or monge - kantorovich distance between distributions @xmath0 and @xmath1 denotes the work required to transform one distribution into another .",
    "more formally , it is given by @xmath357 here the minimizer of ( [ eq : monge ] ) represents the movement schedule @xmath150 of transforming @xmath0 into @xmath1 or vice versa . in general , a large variety of costs of moving mass from @xmath52 to @xmath58 can be used , such as psychooptical similarity measures in image retrieval @xcite .",
    "they replace the role of @xmath358 in ( [ eq : monge ] ) . in general , this can only be solved by executing a linear program . in the special case of the cost @xmath359",
    ", however , one may show @xcite that this distance can be rewritten as @xmath360 that is , as the @xmath195 distance between the cumulative distribution functions associated with @xmath0 and @xmath1 .",
    "we now show that for a suitably chosen function class @xmath2 , this distance is a special case of @xmath91 $ ] .",
    "[ th : dualwasser ] let @xmath361 .",
    "denote by @xmath2 the class of differentiable functions on @xmath46 for which both limits @xmath362 and @xmath363 exist and for which @xmath364 for all @xmath365 . in this case",
    "@xmath366 = { \\left\\|f_p - f_q\\right\\|}_b \\text { where }     \\textstyle \\frac{1}{a } + \\frac{1}{b } = 1\\ ] ] whenever @xmath367 is finite .    for @xmath368",
    "this follows from proposition  [ th : ks ] .",
    "hence , assume that @xmath369 .",
    "we exploit integration by parts to obtain for @xmath365 @xmath370 dx &       = f(x ) [ f_p(x ) - f_q(x ) ] \\bigr|_{-\\infty}^\\infty -       \\int f'(x ) [ f_p(x ) - f_q(x ) ] dx \\\\      & = \\int -f'(x ) [ f_p(x)- f_q(x ) ] dx    \\end{aligned}\\ ] ] since @xmath371 = 0 $ ] and since @xmath372 exists .",
    "next note that @xmath373 \\leq { \\left\\|f_p - f_q\\right\\|}_{b}.    \\end{aligned}\\ ] ] this follows from the fact that for all @xmath365 we have @xmath374 , from rewriting the integral as above and from the definition of the dual norm .",
    "the final step is to show that @xmath375 is dense in the unit ball with respect to the @xmath376 norm , since this implies equality in ( [ eq : upbound - la ] ) .    to see this recall @xcite that for @xmath377 we have @xmath378 ) \\subseteq l_1([-c , c])$ ] for all @xmath361 .",
    "moreover , any such function @xmath379)$ ] has a well - defined stem function @xmath380 . in other words , @xmath381 .",
    "finally , since @xmath367 exists , for any @xmath382 there exists a @xmath383 such that the @xmath384 distance of the restrictions of @xmath122 and @xmath385 onto @xmath386 $ ] which we denote by @xmath387 and @xmath388 respectively , is @xmath389-close to @xmath367 .",
    "moreover , by the density of @xmath390)$ ] in @xmath391)$ ] there exists some @xmath392)$ ] with @xmath393 such that @xmath394 \\geq { \\left\\|f_p^c - f_q^c\\right\\| } - \\epsilon/2 $ ] .",
    "this means that for any @xmath395 , the set of test functions @xmath2 contains some @xmath396 which is @xmath395 close to the upper bound on @xmath91 $ ] .",
    "taking @xmath397 completes the proof .",
    "in fact , theorem  [ th : dualwasser ] is somewhat more general than what is needed to deal with the @xmath195 wasserstein distance , since it shows that for any @xmath398 distance on the cumulative distribution function a suitable space of witness functions @xmath2 can be found .",
    "an application related to the two sample problem is that of outlier detection : this is the question of whether a novel point is generated from the same distribution as a particular i.i.d . sample . in a way , this is a special case of a two sample test , where the second sample contains only one observation .",
    "several methods essentially rely on the distance between a novel point to the sample mean in feature space to detect outliers .",
    "for instance , @xcite use a related method to deal with nonstationary time series .",
    "likewise @xcite discuss how to detect novel observations by using the following reasoning : the probability of being an outlier is bounded both as a function of the spread of the points in feature space and the uncertainty in the empirical feature space mean ( as bounded using symmetrisation and mcdiarmid s tail bound ) .    instead of using the sample mean and variance ,",
    "@xcite estimate the center and radius of a minimal enclosing sphere for the data , the advantage being that such bounds can potentially lead to more reliable tests for _ single _ observations .",
    "@xcite show that the minimal enclosing sphere problem is equivalent to novelty detection by means of finding a hyperplane separating the data from the origin , at least in the case of radial basis function kernels .",
    "we conducted distribution comparisons using our mmd - based tests on datasets from three real - world domains : database applications , bioinformatics , and neurobiology .",
    "we investigated both uniform convergence approaches ( @xmath399 with the corollary [ lem : firsttest ] threshold , and @xmath247 h with the corollary [ cor : hoeffdingthresh ] threshold ) ; the asymptotic approaches with bootstrap ( @xmath247 b ) and moment matching to pearson curves ( @xmath247 m ) , both described in section [ sec : asymptotictest ] ; and the asymptotic approach using the linear time statistic ( @xmath400 ) from section [ sec : lineartimestatistic ] .",
    "we also compared against several alternatives from the literature ( where applicable ) : the multivariate t - test , the friedman - rafsky kolmogorov - smirnov generalisation _ ( smir ) _ , the friedman - rafsky wald - wolfowitz generalisation _ ( wolf ) _ , the biau - gyrfi test _ ( biau ) _ , and the hall - tajvidi test _ ( hall)_. see section [ sec : othertestsreview ] for details regarding these tests . note that we do not apply the biau - gyrfi test to high - dimensional problems ( since the required space partitioning is no longer possible ) , and that mmd is the only method applicable to structured data such as graphs .",
    "an important issue in the practical application of the mmd - based tests is the selection of the kernel parameters .",
    "we illustrate this with a gaussian rbf kernel , where we must choose the kernel width @xmath207 ( we use this kernel for univariate and multivariate data , but not for graphs ) .",
    "the empirical mmd is zero both for kernel size @xmath401 ( where the aggregate gram matrix over @xmath74 and @xmath75 is a unit matrix ) , and also approaches zero as @xmath402 ( where the aggregate gram matrix becomes uniformly constant ) .",
    "we set @xmath207 to be the median distance between points in the aggregate sample , as a compromise between these two extremes : this remains a heuristic , similar to those described in @xcite , and the optimum choice of kernel size is an ongoing area of research .      in our first experiment , we investigated the scaling performance of the various tests as a function of the dimensionality @xmath271 of the space @xmath403 , when both @xmath0 and @xmath1 were gaussian .",
    "we considered values of @xmath271 up to 2500 : the performance of the mmd - based tests can not therefore be explained in the context of density estimation ( as in section [ sec : mmdparzen ] ) , since the associated density estimates are necessarily meaningless here .",
    "the levels for all tests were set at @xmath404 , @xmath405 samples were used , and results were averaged over @xmath406 repetitions . in the first case , the distributions had different means and unit variance .",
    "the percentage of times the null hypothesis was correctly rejected over a set of euclidean distances between the distribution means ( 20 values logarithmically spaced from 0.05 to 50 ) , was computed as a function of the dimensionality of the normal distributions . in case of the t - test , a ridge was added to the covariance estimate , to avoid singularity ( the ratio of largest to smallest eigenvalue was ensured to be at most 2 ) . in the second case , samples were drawn from distributions @xmath407 and @xmath408 with different variance .",
    "the percentage of null rejections was averaged over 20 @xmath207 values logarithmically spaced from @xmath409 to @xmath410 .",
    "the t - test was not compared in this case , since its output would have been irrelevant .",
    "results are plotted in figure [ fig : gaussians ] .    in the case of gaussians with differing means ,",
    "we observe the t - test performs best in low dimensions , however its performance is severely weakened when the number of samples exceeds the number of dimensions .",
    "the performance of @xmath411 m is comparable to the t - test for low sample sizes , and outperforms all other methods for larger sample sizes .",
    "the worst performance is obtained for @xmath412 h , though @xmath413 also does relatively poorly : this is unsurprising given that these tests derive from distribution - free large deviation bounds , whereas the sample size is relatively small .",
    "remarkably , @xmath414 performs quite well compared with classical tests in high dimensions .    in the case of gaussians of differing variance ,",
    "the _ hall _ test performs best , followed closely by @xmath411 . _",
    "fr wolf _ and ( to a much greater extent ) _ fr smirnov _ both have difficulties in high dimensions , failing completely once the dimensionality becomes too great .",
    "the linear test @xmath414 again performs surprisingly well , almost matching the @xmath411 performance in the highest dimensionality .",
    "both @xmath412h and @xmath413 perform poorly , the former failing completely : this is one of several illustrations we will encounter of the much greater tightness of the corollary [ lem : firsttest ] threshold over that in corollary [ cor : hoeffdingthresh ] .    .",
    "* a * gaussians have same variance and different means . *",
    "b * gaussians have same mean and different variances . ]      in our next application of mmd , we performed distribution testing for data integration : the objective being to aggregate two datasets into a single sample , with the understanding that both original samples were generated from the same distribution . clearly , it is important to check this last condition before proceeding , or an analysis could detect patterns in the new dataset that are caused by combining the two different source distributions , and not by real - world phenomena .",
    "we chose several real - world settings to perform this task : we compared microarray data from normal and tumor tissues ( health status ) , microarray data from different subtypes of cancer ( subtype ) , and local field potential ( lfp ) electrode recordings from the macaque primary visual cortex ( v1 ) with and without spike events ( neural data i and ii , as described in more detail by * ? ? ?",
    "* ) . in all cases ,",
    "the two data sets have different statistical properties , but the detection of these differences is made difficult by the high data dimensionality ( indeed , for the microarray data , density estimation is impossible given the sample size and data dimensionality , and no successful test can rely on accurate density estimates as an intermediate step ) .",
    "we applied our tests to these datasets in the following fashion .",
    "given two datasets a and b , we either chose one sample from a and the other from b _ ( attributes = different ) _ ; or both samples from either a or b _ ( attributes = same)_. we then repeated this process up to 1200 times .",
    "results are reported in table  [ tab : multivariate ] .",
    "our asymptotic tests perform better than all competitors besides _ wolf _ : in the latter case , we have greater type ii error for one neural dataset , lower type ii error on the health status data ( which has very high dimension and low sample size ) , and identical ( error - free ) performance on the remaining examples .",
    "we note that the type i error of the bootstrap test on the subtype dataset is far from its design value of @xmath415 , indicating that the pearson curves provide a better threshold estimate for these low sample sizes . for the remaining datasets ,",
    "the type i errors of the pearson and bootstrap approximations are close .",
    "thus , for larger datasets , the bootstrap is to be preferred , since it costs @xmath258 , compared with a cost of @xmath252 for pearson ( due to the cost of computing ( [ moment3 ] ) ) . finally , the uniform convergence - based tests are too conservative , with @xmath416 finding differences in distribution only for the data with largest sample size , and @xmath417 h never finding differences .",
    ".distribution testing for data integration on multivariate data .",
    "numbers indicate the percentage of repetitions for which the null hypothesis ( p = q ) was accepted , given @xmath404 . sample size ( dimension ; repetitions of experiment ) : neural i 4000 ( 63 ; 100 ) ; neural ii 1000 ( 100 ; 1200 ) ; health status 25 ( 12,600 ; 1000 ) ; subtype 25 ( 2,118 ; 1000 ) . [",
    "cols=\"<,<,>,>,>,>,>,>,>,>,>,>\",options=\"header \" , ]",
    "we have established three simple multivariate tests for comparing two distributions @xmath0 and @xmath1 , based on samples of size @xmath4 and @xmath5 from these respective distributions .",
    "our test statistic is the maximum mean discrepancy ( mmd ) , defined as the maximum deviation in the expectation of a function evaluated on each of the random variables , taken over a sufficiently rich function class : in our case , a universal reproducing kernel hilbert space ( rkhs ) .",
    "equivalently , the statistic can be written as the norm of the difference between distribution feature means in the rkhs .",
    "we do not require density estimates as an intermediate step .",
    "two of our tests provide type i error bounds that are exact and distribution - free for finite sample sizes .",
    "we also give a third test based on quantiles of the asymptotic distribution of the associated test statistic .",
    "all three tests can be computed in @xmath78 time , however when sufficient data are available , a linear time statistic can be used , which employs more data to get better results at smaller computational cost . in addition , a number of metrics on distributions ( kolmogorov - smirnov , earth mover s , @xmath196 distance between parzen window density estimates ) , as well as certain kernel similarity measures on distributions , are included within our framework .    while our result establishes that statistical tests based on the mmd are consistent for universal kernels on compact domains , we draw attention to the recent introduction of _ characteristic kernels _ by @xcite , these being kernels for which the mean map is injective .",
    "fukumizu et al .",
    "establish that gaussian and laplace kernels are characteristic on @xmath418 , and thus the mmd is a consistent test for this domain .",
    "@xcite further explore the properties of characteristic kernels , providing a simple condition to determine whether convolution kernels are characteristic , and describing characteristic kernels which are not universal on compact domains .",
    "we also note ( following section [ sec : kernelsbetweenmeasures ] ) that the mmd for rkhss is associated with a particular kernel between probability distributions .",
    "@xcite describe several further such kernels , which induce corresponding distances between feature space distribution mappings : these may in turn lead to new and powerful two - sample tests .",
    "two recent studies have shown that additional divergence measures between distributions can be obtained empirically through optimization in a reproducing kernel hilbert space .",
    "@xcite build on the work of @xcite , considering a homogeneity statistic arising from the kernel fisher discriminant , rather than the difference of rkhs means ; and @xcite obtain a kl divergence estimate by approximating the ratio of densities ( or its log ) with a function in an rkhs . by design ,",
    "both these kernel - based statistics prioritise different features of @xmath0 and @xmath1 when measuring the divergence between them , and the resulting effects on distinguishability of distributions are therefore of interest .",
    "finally , we have seen in section [ sec : basicstuffandreview ] that several classical metrics on probability distributions can be written as maximum mean discrepancies with function classes that are not hilbert spaces , but rather banach , metric , or semi - metric spaces .",
    "it would be of particular interest to establish under what conditions one could write these discrepancies in terms of norms of differences of mean elements . in particular , @xcite consider banach spaces endowed with a semi - inner product , for which a general riesz representation exists for elements in the dual .",
    "we need the following theorem , due to @xcite .",
    "let @xmath419 be a function such that for all @xmath420 , there exist @xmath421 for which@xmath422 then for all probability measures @xmath0 and every @xmath423,@xmath424    we also define the rademacher average of the function class @xmath2 with respect to the @xmath4-sample @xmath74 .",
    "[ def : rademacher ] let @xmath2 be the unit ball in a universal rkhs on the compact domain @xmath10 , with kernel bounded according to @xmath200 .",
    "let @xmath74 be an i.i.d .",
    "sample of size @xmath4 drawn according to a probability measure @xmath0 on @xmath10 , and let @xmath425 be i.i.d and take values in @xmath426 with equal probability .",
    "we define the rademacher average @xmath427 where the upper bound is due to ( * ? ? ?",
    "* lemma 22 ) .",
    "similarly , we define @xmath428      we want to show that the absolute difference between @xmath429 and @xmath430 is close to its expected value , independent of the distributions @xmath0 and @xmath1 . to this end , we prove three intermediate results , which we then combine .",
    "the first result we need is an upper bound on the absolute difference between @xmath429 and @xmath430 .",
    "we have @xmath431 second , we provide an upper bound on the difference between @xmath432 and its expectation . changing either of @xmath433 or @xmath434 in @xmath432 results in changes in magnitude of at most @xmath435 or @xmath436 , respectively .",
    "we can then apply mcdiarmid s theorem , given a denominator in the exponent of @xmath437 to obtain @xmath438>\\epsilon\\right)\\le\\exp\\left(-\\frac{\\epsilon^{2}mn}{2k(m+n)}\\right).\\ ] ] for our final result , we exploit symmetrisation , following e.g. @xcite , to upper bound the expectation of @xmath432 . denoting by @xmath439 an i.i.d sample of size @xmath4 drawn independently of @xmath74 ( and likewise for @xmath440 ) , we have @xmath441\\nonumber\\\\   & = & { \\mathbf{e}}_{x , y}\\sup_{f\\in{\\mathcal{f}}}\\left|{\\mathbf{e}}_{p}(f)-\\frac{1}{m}\\sum_{i=1}^{m}f(x_{i})-{\\mathbf{e}}_{q}(f)+\\frac{1}{n}\\sum_{i=1}^{n}f(y_{j})\\right|\\nonumber\\\\   & = & { \\mathbf{e}}_{x , y}\\sup_{f\\in{\\mathcal{f}}}\\left|{\\mathbf{e}}_{x'}\\left(\\frac{1}{m}\\sum_{i=1}^{m}f(x_{i}')\\right)-\\frac{1}{m}\\sum_{i=1}^{m}f(x_{i})-{\\mathbf{e}}_{y'}\\left(\\frac{1}{n}\\sum_{i=1}^{n}f(y_{j}')\\right)+\\frac{1}{n}\\sum_{i=1}^{n}f(y_{j})\\right|\\nonumber\\\\   & \\underset{(a)}{\\le } & { \\mathbf{e}}_{x , y , x',y'}\\sup_{f\\in{\\mathcal{f}}}\\left|\\frac{1}{m}\\sum_{i=1}^{m}f(x_{i}')-\\frac{1}{m}\\sum_{i=1}^{m}f(x_{i})-\\frac{1}{n}\\sum_{i=1}^{n}f(y_{j}')+\\frac{1}{n}\\sum_{i=1}^{n}f(y_{j})\\right|\\nonumber\\\\   & = & { \\mathbf{e}}_{x , y , x',y',\\sigma,\\sigma'}\\sup_{f\\in{\\mathcal{f}}}\\left|\\frac{1}{m}\\sum_{i=1}^{m}\\sigma_{i}\\left(f(x_{i}')-f(x_{i})\\right)+\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{i}'\\left(f(y_{j}')-f(y_{j})\\right)\\right|\\nonumber\\\\   & \\underset{(b)}{\\le } & { \\mathbf{e}}_{x , x'\\sigma}\\sup_{f\\in{\\mathcal{f}}}\\left|\\frac{1}{m}\\sum_{i=1}^{m}\\sigma_{i}\\left(f(x_{i}')-f(x_{i})\\right)\\right|+{\\mathbf{e}}_{y , y'\\sigma}\\sup_{f\\in{\\mathcal{f}}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{i}\\left(f(y_{j}')-f(y_{j})\\right)\\right|\\nonumber\\\\   & \\underset{(c)}{\\le } & 2\\left[r_{m}({\\mathcal{f}},p)+r_{n}({\\mathcal{f}},q)\\right].\\nonumber\\\\   & \\underset{(d)}{\\le } & 2\\left[(k / m)^{1/2 } + ( k / n)^{1/2 } \\right ] , \\label{eq : step3}\\end{aligned}\\ ] ] where ( a ) uses jensen s inequality , ( b ) uses the triangle inequality , ( c ) substitutes definition [ def : rademacher ] ( the rademacher average ) , and ( d ) bounds the rademacher averages , also via definition [ def : rademacher ] .    having established our preliminary results , we proceed to the proof of theorem  [ th : mmd - diff ] .    combining equations ( [ eq : step2 ] ) and ( [ eq : step3 ] ) , gives @xmath442 > \\epsilon\\right)\\le\\exp\\left(-\\frac{\\epsilon^{2}mn}{2k(m+n)}\\right).\\ ] ] substituting equation ( [ eq : firststep ] ) yields the result .      in this section ,",
    "we derive the theorem [ th : mmd - same - a ] result , namely the large deviation bound on the mmd when @xmath8 and @xmath64 .",
    "note also that we consider only positive deviations of @xmath430 from @xmath429 , since negative deviations are irrelevant to our hypothesis test .",
    "the proof follows the same three steps as in the previous section .",
    "the first step in ( [ eq : firststep ] ) becomes@xmath443 the mcdiarmid bound on the difference between ( [ lalalal ] ) and its expectation is now a function of @xmath444 observations in ( [ lalalal ] ) , and has a denominator in the exponent of @xmath445 .",
    "we use a different strategy in obtaining an upper bound on the expected ( [ lalalal ] ) , however : this is now    @xmath446 \\nonumber\\\\   & = & \\frac{1}{m}{\\mathbf{e}}_{x , x'}\\left\\vert \\sum_{i=1}^{m}\\left(\\phi(x_{i})-\\phi(x_{i}')\\right)\\right\\vert \\nonumber \\\\   & = & \\frac{1}{m}{\\mathbf{e}}_{x , x'}\\left[\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\left(k(x_{i},x_{j})+k(x_{i}',x_{j}')-k(x_{i},x'_{j})-k(x_{i}',x_{j})\\right)\\right]^{\\frac{1}{2}}\\nonumber \\\\   & \\le & \\frac{1}{m}\\left[2m{\\mathbf{e}}_{x}k(x , x)+2m(m-1){\\mathbf{e}}_{x , x'}k(x , x')-2m^{2}{\\mathbf{e}}_{x , x'}k(x , x')\\right]^{\\frac{1}{2}}\\nonumber\\\\   & = & \\left[\\frac{2}{m}{\\mathbf{e}}_{x , x'}\\left(k(x , x)-k(x , x')\\right)\\right]^{\\frac{1}{2}}\\label{eq : appendbias1}\\\\   & \\le & \\left(2k / m\\right)^{1/2}.\\label{eq : appendbias2}\\end{aligned}\\ ] ]    we remark that both ( [ eq : appendbias1 ] ) and ( [ eq : appendbias2 ] ) bound the amount by which our biased estimate of the population mmd exceeds zero under @xmath447 . combining the three results , we find that under @xmath447,@xmath448^{\\frac{1}{2}}>\\epsilon\\right ) & < & \\exp\\left(\\frac{-\\epsilon^{2}m}{4k}\\right)\\quad\\mathrm{and}\\\\ p_{x}\\left({\\mathrm{mmd}}_b({\\mathcal{f}},x , x')-\\left(2k / m\\right)^{1/2}>\\epsilon\\right ) & < & \\exp\\left(\\frac{-\\epsilon^{2}m}{4k}\\right).\\end{aligned}\\ ] ]",
    "we derive results needed in the asymptotic test of section [ sec : asymptotictest ] .",
    "appendix [ sec : distribh0 ] describes the distribution of the empirical mmd under @xmath125 ( both distributions identical ) .",
    "appendix [ sec : momentsh0 ] contains derivations of the second and third moments of the empirical mmd , also under @xmath125 .",
    "we describe the distribution of the unbiased estimator @xmath449 $ ] under the null hypothesis . in this circumstance",
    ", we denote it by @xmath450 $ ] , to emphasise that the second sample @xmath439 is drawn independently from the same distribution as @xmath74 .",
    "we thus obtain the u - statistic@xmath451 & = & \\frac{1}{m(m-1)}\\sum_{i\\neq j}k(x_{i},x_{j})+k(x'_{i},x'_{j})-k(x_{i},x'_{j})-k(x_{j},x'_{i})\\label{eq : teststat1}\\\\   & = & \\frac{1}{m(m-1)}\\sum_{i\\neq j}h(z_{i},z_{j}),\\label{eq : teststat2}\\end{aligned}\\ ] ] where @xmath452 . under the null hypothesis ,",
    "this u - statistic is degenerate , meaning@xmath453 the following theorem from ( * ? ? ?",
    "* section 5.5.2 ) then applies .",
    "assume @xmath450 $ ] is as defined in ( [ eq : teststat2 ] ) , with @xmath233 , and furthermore assume @xmath454 .",
    "then @xmath450 $ ] converges in distribution according to@xmath455\\overset{d}{\\rightarrow}\\sum_{l=1}^{\\infty}\\gamma_{l}\\left(\\chi_{1l}^{2}-1\\right),\\ ] ] where @xmath456 are independent chi squared random variables of degree one , and @xmath457 are the solutions to the eigenvalue equation@xmath458    while this result is adequate for our purposes ( since we do not explicitly use the quantities @xmath457 in our subsequent reasoning ) , it does not make clear the dependence of the null distribution on the kernel choice . for this reason , we provide an alternative expression based on the reasoning of ( * ? ? ? *",
    "appendix ) , bearing in mind the following changes :    * we do not need to deal with the bias terms @xmath459 seen by ( * ? ? ? *",
    "appendix ) that vanish for large sample sizes , since our statistic is unbiased ( although these bias terms drop faster than the variance ) ; * we require greater generality , since we deal with distributions on compact metric spaces , and not densities on @xmath283 ; correspondingly , our kernels are not necessarily inner products in @xmath460 between probability density functions ( although this is a special case ) .",
    "our first step is to express the kernel @xmath461 of the u - statistic in terms of an rkhs kernel @xmath462 between feature space mappings from which the mean has been subtracted , @xmath463,\\phi(x_{j})-\\mu[p]\\right\\rangle \\\\   & = & k(x_{i},x_{j})-{\\mathbf{e}}_{x}k(x_{i},x)-{\\mathbf{e}}_{x}k(x , x_{j})+{\\mathbf{e}}_{x , x'}k(x , x').\\end{aligned}\\ ] ] the centering terms cancel ( the distance between the two points is unaffected by an identical global shift in both the points ) , meaning@xmath464 next , we write the kernel @xmath462 in terms of eigenfunctions @xmath465 with respect to the probability measure @xmath466,@xmath467 where@xmath468 and@xmath469    since @xmath470 then when @xmath471 we have that@xmath472 and hence @xmath473 we now use these results to transform the expression in ( [ eq : teststat1 ] ) .",
    "first , @xmath474 where @xmath475 are i.i.d . , and the final relation denotes convergence in distribution , using ( [ eq : uncorrelated ] ) and ( [ eq : zeromean ] ) , following ( * ? ? ?",
    "* section 5.5.2 ) .",
    "likewise@xmath476 where @xmath477 , and@xmath478 combining these results , we get @xmath479.\\end{aligned}\\ ] ] note that @xmath480 , being the difference of two independent gaussian variables , has a normal distribution with mean zero and variance @xmath265 .",
    "this is therefore a quadratic form in a gaussian random variable minus an offset @xmath481 .",
    "[ [ moments - of - the - empirical - mmd - under - mathcalh_0secmomentsh0 ] ] moments of the empirical mmd under @xmath125[sec : momentsh0 ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~      * variance/2nd moment : * this was derived by @xcite , and is also described by ( * ? ? ?",
    "* lemma a p. 183 ) . applying these results , @xmath483 ^ 2\\right ) \\\\   & =    \\left(\\frac{2}{n(n-1)}\\right)^{2}\\left[\\frac{n(n-1)}{2}(n-2)(2){\\mathbf{e}}_{z}\\left[({\\mathbf{e}}_{z'}h(z , z'))^{2}\\right]+\\frac{n(n-1)}{2}{\\mathbf{e}}_{z , z'}\\left[h^{2}(z , z')\\right]\\right ] \\\\   & =   \\frac{2(n-2)}{n(n-1)}{\\mathbf{e}}_{z}\\left[({\\mathbf{e}}_{z'}h(z , z'))^{2}\\right]+\\frac{2}{n(n-1)}{\\mathbf{e}}_{z , z'}\\left[h^{2}(z , z')\\right]\\\\   & =   \\frac{2}{n(n-1)}{\\mathbf{e}}_{z , z'}\\left[h^{2}(z , z')\\right],\\end{aligned}\\ ] ] where the first term in the penultimate line is zero due to ( [ eq : degeneratecondition ] ) .",
    "note that variance and 2nd moment are the same under the zero mean assumption .    *",
    "3rd moment : * we consider the terms that appear in the expansion of @xmath243 ^ 3\\right)$ ] .",
    "these are all of the form@xmath484 where we shorten @xmath485 , and we know @xmath486 and @xmath487 are always independent . most of the terms vanish due to ( [ eq : zeromean2 ] ) and ( [ eq : degeneratecondition ] ) .",
    "the first terms that remain take the form @xmath488 and there are@xmath489 of them , which gives us the expression @xmath490\\nonumber \\\\   & =    \\frac{8(n-2)}{n^{2}(n-1)^{2}}{\\mathbf{e}}_{z , z'}\\left[h(z , z'){\\mathbf{e}}_{z''}\\left(h(z , z'')h(z',z'')\\right)\\right].\\label{eq : pre3rdmoment}\\end{aligned}\\ ] ] note the scaling @xmath491 .",
    "the remaining non - zero terms , for which @xmath492 and @xmath493 , take the form @xmath494,\\ ] ] and there are @xmath495 of them , which gives @xmath496 .",
    "\\label{eq : negligible3rdmoment}\\ ] ] however @xmath497 so this term is negligible compared with ( [ eq : pre3rdmoment ] ) .",
    "thus , a reasonable approximation to the third moment is @xmath498 ^ 3\\right )   \\approx    \\frac{8(n-2)}{n^{2}(n-1)^{2}}{\\mathbf{e}}_{z , z'}\\left[h(z , z'){\\mathbf{e}}_{z''}\\left(h(z , z'')h(z',z'')\\right)\\right].\\ ] ]      we thank philipp berens , olivier bousquet , john langford , omri guttman , matthias hein , novi quadrianto , le song , and vishy vishwanathan for constructive discussions ; patrick warnat ( dkfz , heidelberg ) , for providing the microarray datasets ; and nikos logothetis , for providing the neural datasets .",
    "national ict australia is funded through the australian government s _ backing australia s ability _ initiative , in part through the australian research council .",
    "this work was supported in part by the ist programme of the european community , under the pascal network of excellence , ist-2002 - 506778 , and by the austrian science fund ( fwf ) , project # s9102-n04 .",
    "y.  altun and a.j .",
    "smola . unifying divergence minimization and statistical inference via convex duality . in h.u .",
    "simon and g.  lugosi , editors , _ proc .  annual conf .",
    "computational learning theory _ , lncs , pages 139153 .",
    "springer , 2006 .",
    "n.  anderson , p.  hall , and d.  titterington .",
    "two - sample test statistics for measuring discrepancies between two multivariate probability density functions using kernel - based density estimates .",
    "_ journal of multivariate analysis _",
    ", 50:0 4154 , 1994 .",
    "s.  andrews , i.  tsochantaridis , and t.  hofmann .",
    "support vector machines for multiple - instance learning . in s.",
    "becker , s.  thrun , and k.  obermayer , editors , _ advances in neural information processing systems 15_. mit press , 2003 .                    k.  m. borgwardt , a.  gretton , m.  j. rasch , h .- p .",
    "kriegel , b.  schlkopf , and a.  j. smola .",
    "integrating structured biological data by kernel maximum mean discrepancy . _",
    "bioinformatics ( ismb ) _ , 220 ( 14):0 e49e57 , 2006 .",
    "m.  dudk and r.  e. schapire .",
    "maximum entropy distribution estimation with generalized regularization . in gbor lugosi and hans  u. simon , editors , _ proc .  annual conf .",
    "computational learning theory_. springer verlag , june 2006 .",
    "a.  gretton , o.  bousquet , a.j .",
    "smola , and b.  schlkopf .",
    "measuring statistical dependence with hilbert - schmidt norms . in s.",
    "jain , h.  u. simon , and e.  tomita , editors , _ proceedings algorithmic learning theory _ , pages 6377 , berlin , germany , 2005 .",
    "springer - verlag .",
    "a.  gretton , k.  borgwardt , m.  rasch , b.  schlkopf , and a.  smola .",
    "a kernel method for the two - sample - problem . in _ advances in neural information processing systems 19 _ , pages 513520 , cambridge ,",
    "ma , 2007 . mit press .",
    "a.  gretton , k.  borgwardt , m.  rasch , b.  schlkopf , and a.  smola .",
    "a kernel approach to comparing distributions .",
    "_ proceedings of the 22nd conference on artificial intelligence ( aaai-07 ) _ , pages 16371641 , 2007 .",
    "t.  jebara and i.  kondor .",
    "bhattacharyya and expected likelihood kernels . in b.",
    "schlkopf and m.  warmuth , editors , _ proceedings of the sixteenth annual conference on computational learning theory _ , number 2777 in lecture notes in computer science , pages 5771 , heidelberg , germany , 2003 .",
    "springer - verlag .",
    "a.  j. smola and b.  schlkopf .",
    "sparse greedy matrix approximation for machine learning . in p.",
    "langley , editor , _ proc .",
    "machine learning _ , pages 911918 , san francisco , 2000 .",
    "morgan kaufmann publishers .",
    "smola , a.  gretton , l.  song , and b.  schlkopf . a hilbert space embedding for distributions . in e.",
    "takimoto , editor , _ algorithmic learning theory _ , lecture notes on computer science .",
    "springer , 2007 .",
    "christoper k.  i. williams and matthias seeger .",
    "using the nystrom method to speed up kernel machines .",
    "in t.  k. leen , t.  g. dietterich , and v.  tresp , editors , _ advances in neural information processing systems 13 _ , pages 682688 , cambridge , ma , 2001 . mit press ."
  ],
  "abstract_text": [
    "<S> we propose a framework for analyzing and comparing distributions , allowing us to design statistical tests to determine if two samples are drawn from different distributions . </S>",
    "<S> our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel hilbert space ( rkhs ) . </S>",
    "<S> we present two tests based on large deviation bounds for the test statistic , while a third is based on the asymptotic distribution of this statistic . </S>",
    "<S> the test statistic can be computed in quadratic time , although efficient linear time approximations are available . </S>",
    "<S> several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general ( eg .  a banach space ) . </S>",
    "<S> we apply our two - sample tests to a variety of problems , including attribute matching for databases using the hungarian marriage method , where they perform strongly . </S>",
    "<S> excellent performance is also obtained when comparing distributions over graphs , for which these are the first such tests .    </S>",
    "<S> kernel methods , two sample test , uniform convergence bounds , schema matching , asymptotic analysis , hypothesis testing . </S>"
  ]
}