{
  "article_text": [
    "centrality metrics , such as closeness or betweenness , quantify how central a node is in a network .",
    "they have been successfully used to carry analysis for various purposes such as structural analysis of knowledge networks  @xcite , power grid contingency analysis  @xcite , quantifying importance in social networks  @xcite , analysis of covert networks  @xcite , decision / action networks  @xcite , and even for finding the best store locations in cities  @xcite .",
    "several works which have been conducted to rapidly compute these metrics exist in the literature .",
    "the algorithm with the best asymptotic complexity to compute centrality metrics  @xcite is believed to be asymptotically optimal  @xcite .",
    "research have focused on either approximation algorithms for computing centrality metrics  @xcite or on high performance computing techniques  @xcite .",
    "today , it is common to find large networks , and we are always in a quest for better techniques which help us while performing centrality - based analysis on them .    when the network topology is modified , ensuring the correctness of the centralities is a challenging task .",
    "this problem has been studied for dynamic and streaming networks  @xcite . even for some applications involving a static network such as the contingency analysis of power grids and robustness evaluation of networks , to be prepared and take proactive measures , we need to know how the centrality values change when the network topology is modified by an adversary and outer effects such as natural disasters .",
    "a similar problem arises in network management for which not only knowing but also setting the centrality values in a controlled manner via topology modifications is of concern to speed - up or contain the entity dissemination .",
    "the problem is hard : there are @xmath0 candidate edges to delete and @xmath1 candidate edges to insert where @xmath2 and @xmath0 are the number of nodes and edges in the network , respectively . here , the main motivation can be calibrating the importance / load of some or all of the vertices as desired , matching their loads to their capacities , boosting the content spread , or making the network immune to adversarial attacks .",
    "similar problems , such as finding the most cost - effective way which reduces the entity dissemination ability of a network  @xcite or finding a small set of edges whose deletion maximizes the shortest - path length  @xcite , have been investigated in the literature .",
    "the problem recently regained a lot of attention : a generic study which uses edge insertions and deletions is done by tong  et  al .",
    "they use the changes on the leading eigenvalue to control / speed - up the dissemination process .",
    "other recent works investigate edge insertions to minimize the average shortest path distance  @xcite or to boost the content spread  @xcite . from the centrality point of view , there exist studies which focus on maximizing the centrality of a node set  @xcite or a single node  @xcite by edge insertions . in generic centrality - based network management problem ,",
    "the desired centralities of all the nodes need to be obtained or approximated with a small set of topology modifications . as figure",
    "[ fig : gseq ] shows , the effect of a local topology modification is usually global .",
    "furthermore , existing algorithms for incremental centrality computation are not efficient enough to be used in practice .",
    "thus , novel incremental algorithms are essential to quickly evaluate the effects of topology modifications on centrality values .    , @xmath3 , and @xmath4 , respectively ) insertions / deletions , and values of closeness centrality.,title=\"fig:\",scaledwidth=38.0% ] , @xmath3 , and @xmath4 , respectively ) insertions / deletions , and values of closeness centrality.,title=\"fig:\",scaledwidth=47.0% ]",
    "our contributions can be summarized as follows :    1 .   to attack the variants of the centrality - based network management problem , we propose incremental algorithms which efficiently update the closeness centralities upon edge insertions and deletions .",
    "2 .   the proposed algorithms can serve as a fundamental building block for other shortest - path - based network analyses such as the temporal analysis on the past network data , maintaining centrality on streaming networks , or minimizing / maximizing the average shortest - path distance via edge insertions and deletions .",
    "3 .   compared with the existing algorithms ,",
    "our algorithms have a low - memory footprint making them practical and applicable to very large graphs . for random edge insertions / deletions to the wikipedia users communication graph",
    ", we reduced the centrality ( re)computation time from 2 days to 16 minutes . and",
    "for the real - life temporal dblp coauthorship network , we reduced the time from 1.3 days to 4.2 minutes .",
    "the proposed techniques can easily be adapted to algorithms for approximating centralities . as a result",
    ", one can employ a more accurate and faster sampling and obtain better approximations .",
    "the rest of the paper is organized as follows : section  [ sec : bac ] introduces the notation and formally defines the closeness centrality metric .",
    "section  [ sec : manage ] defines network management problems we are interested .",
    "our algorithms explained in detail in section  [ sec : main ] .",
    "existing approaches are described in section  [ sec : rel ] and the experimental analysis is given in section  [ sec : exp ] .",
    "section  [ sec : con ] concludes the paper .",
    "let @xmath5 be a network modeled as a simple graph with @xmath6 vertices and @xmath7 edges where each node is represented by a vertex in @xmath8 , and a node - node interaction is represented by an edge in @xmath9 .",
    "let @xmath10 be the set of vertices which are connected to @xmath11 in @xmath12 .",
    "a graph @xmath13 is a _ subgraph _ of @xmath12 if @xmath14 and @xmath15 .",
    "a _ path _ is a sequence of vertices such that there exists an edge between consecutive vertices .",
    "a path between two vertices @xmath16 and @xmath17 is denoted by @xmath18  ( we sometimes use @xmath19 to denote a specific path @xmath20 with endpoints @xmath16 and @xmath17 ) .",
    "two vertices @xmath21 are _ connected _ if there is a path from @xmath22 to @xmath11 .",
    "if all vertex pairs are connected we say that @xmath12 is _",
    "connected_. if @xmath12 is not connected , then it is _ disconnected _ and each maximal connected subgraph of @xmath12 is a _ connected component _ , or a component , of @xmath12 .",
    "we use @xmath23 to denote the length of the shortest path between two vertices @xmath24 in a graph @xmath12 . if @xmath25 then @xmath26 . and if @xmath22 and @xmath11 are disconnected , then @xmath27 .    given a graph @xmath5 , a vertex @xmath28",
    "is called an _ articulation vertex _ if the graph @xmath29 ( obtained by removing @xmath11 ) has more connected components than @xmath12 .",
    "similarly , an edge @xmath30 is called a _ bridge _ if @xmath31 ( obtained by removing @xmath32 from @xmath9 ) has more connected components than @xmath12 .",
    "@xmath12 is _ biconnected _ if it is connected and it does not contain an articulation vertex .",
    "a maximal biconnected subgraph of @xmath12 is a _",
    "biconnected component_.      given a graph @xmath12 , the _ farness _ of a vertex @xmath22 is defined as @xmath33 = \\sum_{\\stackrel{v \\in v}{{{d}}_g(u , v ) \\neq",
    "\\infty } } { { d}}_g(u , v).\\ ] ] and the closeness centrality of @xmath22 is defined as @xmath34 = \\frac{1}{{{\\tt far}\\xspace}[u]}. \\label{eq : first}\\ ] ] if @xmath22 can not reach any vertex in the graph @xmath35 = 0 $ ] .    for a sparse unweighted graph @xmath36 with @xmath37 and @xmath38 ,",
    "the complexity of cccomputation is @xmath39 . for each vertex @xmath16 ,",
    "algorithm  [ alg : cc ] executes a single - source shortest paths  ( sssp ) algorithm .",
    "it initiates a breadth - first search  ( bfs ) from @xmath16 , computes the distances to the other vertices , compute @xmath40 $ ] , the sum of the distances which are different than @xmath41 . and , as the last step",
    ", it computes @xmath42 $ ] .",
    "since a bfs takes @xmath43 time , and @xmath2 sssps are required in total , the complexity follows .",
    "the following problem can be considered as a generalized version of the problems investigated in  @xcite .    _",
    "( centrality - based network management ) _ let @xmath5 be a graph . given a centrality metric @xmath44 , a target centrality vector @xmath45 , and an upper bound @xmath46 on the number of inserted / deleted edges , construct a graph @xmath47 , s.t . , @xmath48 and @xmath49 is minimized .",
    "[ def : gencen ]    in this work , we are interested in the closeness metric which is based on shortest paths . hence , implicitly , we are also interested in the following problem partly investigated in  @xcite .    _",
    "( shortest - path - based network management ) _ let @xmath5 be a graph . given an upper bound @xmath46 on the number of inserted / deleted edges , construct a graph @xmath47 where @xmath48 and the ( average ) shortest - path in @xmath50 is minimized / maximized .",
    "[ def : genshor ]    these problems and their variants have several applications such as slowing down pathogen outbreaks , increasing the efficiency of the advertisements , and analyzing the robustness of a network .",
    "consider an airline company with flights to thousands of airports and aim to add some new routes to increase the load of some underutilized airports .",
    "when a new route is inserted , in order to evaluate its overall impact , all the airport centralities need to be re - computed which is a quite expensive task . hence",
    ", we need to have efficient incremental algorithms to tackle this problem .",
    "such algorithms can be used as a fundamental building block to centrality- and shortest - path - based network management problems ( and their variants ) as well as temporal centrality / shortest - path analyses and dynamic network analyses . in this work",
    ", we investigate this subproblem .    _",
    "( incremental closeness centrality ) _ given a graph @xmath5 , its centrality vector @xmath51 , and an edge @xmath52 , find the centrality vector @xmath53 of the graph @xmath54  ( or @xmath55 ) .",
    "many interesting real - life networks are scale free .",
    "the diameters of these networks grow proportional to the logarithm of the number of nodes .",
    "that is , even with hundreds of millions of vertices , the diameter is small , and when the graph is modified with minor updates , it tends to stay small . combining this with their power - law degree distribution",
    ", we obtain the spike - shaped shortest - distance distribution as shown in figure  [ fig : levels ] .",
    "we use two main approaches : _ work filtering _ and _ sssp hybridization _ to exploit these observations and reduce the centrality computation time .      for efficient maintenance of closeness centrality in case of an edge insertion / deletion , we propose a _ work filter _ which reduces the number of sssps in algorithm  [ alg : cc ] and the cost of each sssp .",
    "work filtering uses three techniques : filtering with _ level differences _ , with _ biconnected component decomposition _ , and with _",
    "identical vertices_.      the motivation of level - based filtering is detecting the unnecessary updates and filtering them .",
    "let @xmath56 be the current graph and @xmath52 be an edge to be inserted to @xmath12 .",
    "let @xmath57 be the updated graph .",
    "the centrality definition in   implies that for a vertex @xmath58 , if @xmath59 for all @xmath60 then @xmath42 = { { \\tt cc}\\xspace}'[s]$ ] . the following theorem is used to detect such vertices and filter their sssps .",
    "let @xmath5 be a graph and @xmath22 and @xmath11 be two vertices in @xmath8 s.t .",
    "@xmath61 . let @xmath62",
    ". then @xmath42 = { { \\tt cc}\\xspace}'[s]$ ] if and only if @xmath63.[thm : add ]    if @xmath16 is disconnected from @xmath22 and @xmath11 , @xmath52 s insertion will not change the closeness centrality of @xmath16 .",
    "hence , @xmath42 = { { \\tt cc}\\xspace}'[s]$ ] .",
    "if @xmath16 is only connected to one of @xmath22 and @xmath11 in @xmath12 the difference @xmath64 is @xmath41 , and the closeness centrality score of @xmath16 needs to be updated by using the new , larger connected component containing @xmath16 .    when @xmath16 is connected to both @xmath22 and @xmath11 in @xmath12 , we investigate the edge insertion in three cases as shown in figure  [ fig : leveldiffs ] :    case 1 .",
    "@xmath65 : assume that the path @xmath66@xmath67 is a shortest @xmath18 path in @xmath50 containing @xmath52 . since @xmath65 there exist another path @xmath68 in @xmath50 with one less edge .",
    "hence , @xmath52 can not be in a shortest path : @xmath69 .",
    "@xmath70 : let @xmath71 and assume that @xmath72@xmath73 is a shortest path in @xmath50 containing @xmath52 . since @xmath74 , there exist another path @xmath75 in @xmath50 with the same number of edges .",
    "hence , @xmath76 .",
    "@xmath77 : let @xmath71 .",
    "the path @xmath78@xmath11 in @xmath50 is shorter than the shortest @xmath79 path in @xmath12 since @xmath80 .",
    "hence , an update on @xmath42 $ ] is necessary .",
    "is inserted to the graph @xmath12 , for each vertex @xmath16 , one of them is true : ( a ) @xmath65 , ( b ) @xmath81 , and ( c ) @xmath77.,scaledwidth=45.0% ]    although theorem  [ thm : add ] yields to a filter only in case of edge insertions , the following corollary which is used for edge deletion easily follows .",
    "let @xmath56 be a graph and @xmath22 and @xmath11 be two vertices in @xmath8 s.t .",
    "@xmath82 . let @xmath55",
    ". then @xmath42 = { { \\tt cc}\\xspace}'[s]$ ] if and only if @xmath83 .    with this corollary",
    ", the work filter can be implemented for both edge insertions and deletions .",
    "the pseudocode of the update algorithm in case of an edge insertion is given in algorithm  [ alg : filtered ] .",
    "when an edge @xmath52 is inserted / deleted , to employ the filter , we first compute the distances from @xmath22 and @xmath11 to all other vertices . and , it filters the vertices satisfying the statement of theorem  [ thm : add ] .",
    "@xmath84 @xmath85 \\gets $ ] sssp(@xmath12 , @xmath22 ) @xmath86 \\gets $ ] sssp(@xmath12 , @xmath11 )    in theory , filtering by levels can reduce the update time significantly .",
    "however , in practice , its effectiveness depends on the underlying structure of @xmath12 .",
    "many real - life networks have been repeatedly shown to possess unique characteristics such as a small diameter and a power - law degree distribution  @xcite . and the spread of information is extremely fast  @xcite .",
    "the proposed filter exploits one of these characteristics for efficient closeness centrality updates : the distribution of shortest - path lengths .",
    "its efficiency is based on the phenomenon shown in figure  [ fig : levels ] for a set of graphs used in our experiments : the probability distribution function for a shortest - path length being equal to @xmath87 is unimodular and spike - shaped for many social networks and also some others .",
    "this is the outcome of the short diameter and power - law degree distribution .",
    "on the other hand , for some spatial networks such as road networks , there are no sharp peaks and the shortest - path distances are distributed in a more uniform way .",
    "the work filter we propose here prefer the former .     for four social and web networks . ]",
    "our work filter can be enhanced by employing and maintaining a biconnected component decomposition  ( bcd ) of @xmath5 .",
    "a bcd is a partitioning @xmath88 of the edge set @xmath9 where @xmath89 indicates the component of each edge @xmath30 .",
    "a toy graph and its bcds before and after edge insertions are given in figure  [ fig : bcd ] .",
    "when @xmath52 is inserted to @xmath5 and @xmath90 is obtained , we check if @xmath91 is empty or not . if the intersection is not empty , there will be only one element in it , @xmath92 , which is the i d of the biconnected component of @xmath50 containing @xmath52  ( otherwise @xmath88 is not a valid bcd ) . in this case , @xmath93 is set to @xmath89 for all @xmath94 and @xmath95 is set to @xmath92 . if there is no biconnected component containing both @xmath22 and @xmath11  ( see figure  [ fig : pig2 ] ) , i.e. , if the intersection above is empty , we construct @xmath96 from scratch and set @xmath97 .",
    "@xmath88 can be computed in linear , @xmath98 time  @xcite .",
    "hence , the cost of bcd maintenance is negligible compared to the cost of updating closeness centrality .",
    "let @xmath99 be the biconnected component of @xmath50 containing @xmath52 where @xmath100 let @xmath101 be the set of articulation vertices in @xmath102 .",
    "given @xmath96 , it is easy to detect the articulation vertices since @xmath22 is an articulation vertex if and only if it is part of at least two components in the bcd : @xmath103 .",
    "we will execute sssps only for the vertices in @xmath102 and use the new values to fix the centralities for the rest of the graph .",
    "the contributions of the vertices in @xmath104 are integrated to the sssps by using a representative function @xmath105 which maps each vertex @xmath28 either to a representative in @xmath102 or to @xmath106 ( if @xmath11 and the vertices in @xmath107 are in different connected components of @xmath50 ) .    for each vertex @xmath108 , we set @xmath109 = u$ ] .",
    "for the other vertices , let @xmath110 .",
    "if a vertex @xmath111 and an articulation vertex @xmath112 are connected in @xmath113 , i.e. , @xmath114 , we say that @xmath11 is represented by @xmath22 in @xmath102 and set @xmath115 = u$ ] .",
    "otherwise , @xmath115 $ ] is set to @xmath116 .",
    "the following theorem states that @xmath117 is well defined : each vertex is represented by at most one vertex .",
    "[ thm : rep ] for each @xmath11 in @xmath104 , there is at most one articulation vertex @xmath118 such that @xmath114 .",
    "the proof directly follows from the definition of bcd and is omitted .    since all the  ( shortest ) paths from a vertex @xmath119 to a vertex in @xmath107",
    "are passing through @xmath115 $ ] , the following is a corollary of the theorem .",
    "[ cor : path ] for each vertex @xmath111 with @xmath115 \\neq { \\tt null}$ ] , @xmath120 ) = { { d}}_{g'}(v , { { rep}}[v]),$ ] which is different than @xmath41 .",
    "furthermore , for a vertex @xmath121 which is also represented in @xmath102 but not in the connected component of @xmath122 containing @xmath11 , @xmath123 is equal to @xmath124 ) + { { d}}_{g'}({{rep}}[v],{{rep}}[w ] ) + { { d}}_{g'}({{rep}}[w],w).\\ ] ] if @xmath125 the last term on the right is @xmath126 , since @xmath127 = w$ ] .    to correctly update the new centrality values , we compute two extra values for each vertex @xmath108 , @xmath128 & = \\left|\\{v \\in v : { { rep}}[v ] = u\\}\\right|,\\label{eq : card}\\\\ { { \\tt rf}}[u ] & = \\sum_{\\stackrel{v \\in v}{{{rep}}[v ] = u}}{{d}}_{g'}(u , v).\\label{eq : sumcard}\\end{aligned}\\ ] ] that is , @xmath129 $ ] is the number of vertices in @xmath8 which are represented by @xmath22  ( including @xmath22 ) .",
    "and @xmath130 $ ] is the farness of @xmath22 to these vertices in @xmath50 .",
    "the modified update algorithm is given in algorithm  [ alg : combined ] .",
    "@xmath131 where @xmath132 @xmath133 @xmath134    @xmath135 @xmath136 @xmath137 @xmath138    set @xmath115 $ ] , @xmath139 @xmath129 \\gets \\left|\\{v",
    "\\in v , { { rep}}[v ] = u\\}\\right|$ ] , @xmath140 @xmath130 \\gets \\sum_{v \\in v , { { rep}}[v ] = u}{{d}}_{g'}(u , v)$ ] , @xmath140    @xmath85 \\gets $ ] sssp(@xmath141 , @xmath22 ) , @xmath86 \\gets $ ] sssp(@xmath141 , @xmath11 )    [ lem : bcd1 ] for each vertex @xmath142 , algorithm  [ alg : combined ] computes the correct @xmath143 $ ] value .",
    "we will prove that @xmath144 $ ] is correct for all @xmath142 .",
    "let @xmath145 be the vertex whose closeness centrality update is started at line  [ line : source ] . at line  [ line : far ] of algorithm  [ alg : combined ] , the update on @xmath144 $ ] is @xmath146 + { { \\tt rf}}[w]$ ] which can be rewritten as @xmath147 =   w}}{{d}}_{g'}(v , w ) + { { d}}_{g'}(w , u),\\ ] ] by using   and  . according to corollary  [ cor : path ] , this is equal to @xmath147 = w}}{{d}}_{g'}(v , u).\\ ] ] due to the definition of @xmath117 , only the vertices which are connected to @xmath11 will have an effect on @xmath144 $ ] . and due to theorem  [ thm : rep ] , each vertex can contribute to at most one update . hence @xmath148",
    "= w}}{{d}}_{g'}(v , u ) = \\sum_{\\stackrel{u \\in v}{{{d}}_{g'}(v , u ) \\neq \\infty}}{{d}}_{g'}(v , u),\\ ] ] which is the @xmath144 $ ] in @xmath50 as desired .",
    "[ lem : bcd2 ] for each vertex @xmath111 , algorithm  [ alg : combined ] computes the correct @xmath143 $ ] value .",
    "we will prove that @xmath144 $ ] is correct for all @xmath149 after the fix phase .",
    "let @xmath150 $ ] .",
    "if @xmath22 is null then @xmath11 s farness and hence closeness value will remain the same .",
    "assume that @xmath22 is not null .",
    "let @xmath151 be a vertex with @xmath127 \\neq { \\tt null}$ ] .",
    "if @xmath151 and @xmath11 are in the same connected component of @xmath113 then @xmath152 and @xmath153 . hence , the change on @xmath154 $ ] and @xmath155 $ ] due to @xmath151 are both @xmath126 . on the other hand , if @xmath151 is in a different connected component of @xmath113 according to corollary  [ cor : path ] , @xmath156 ) + { { d}}_{g'}({{rep}}[w],w),\\ ] ] where the sum of the second and the third terms is equal to @xmath157 .",
    "since the first term does not change by the insertion of @xmath52 , the change on @xmath157 is equal to the change on @xmath123 .",
    "that is when aggregated , the change on @xmath154 $ ] is equal to the change on @xmath155 $ ] .",
    "lemma  [ lem : bcd1 ] implies that @xmath155 $ ] is correct .",
    "hence , @xmath144 $ ] , computed at line  [ line : farup ] , must also be correct .",
    "for each vertex @xmath28 , algorithm  [ alg : combined ] computes the correct @xmath143 $ ] value .",
    "follows from lemma  [ lem : bcd1 ] and  [ lem : bcd2 ] .",
    "the complexity of the update algorithm is @xmath39 . and the overhead of filter preparation  ( line  [ line : filter ] through  [ line : update ] )",
    "is @xmath43 since it only contains a constant number of graph traversals . in case of an edge deletion ,",
    "it is enough to get @xmath102 as the biconnected component which was containing the deleted edge .",
    "the rest of the procedure can be adapted in a straightforward manner .",
    "our preliminary analyses on various networks show that some of the graphs contain a significant amount of _ identical _ vertices which have the same / a similar neighborhood structure .",
    "this can be exploited to reduce the number of sssps further .",
    "we investigate two types of identical vertices .    in a graph @xmath12 , two vertices @xmath22 and @xmath11",
    "are _ type - i - identical if and only if @xmath158 .",
    "_    in a graph @xmath12 , two vertices @xmath22 and @xmath11 are _ type - ii - identical if and only if @xmath159 .",
    "_    both types form an equivalance class relation since they are reflexive , symmetric , and transitive . furthermore , all the non - trivial classes they form ( i.e. , the ones containing more than one vertex ) are disjoint .",
    "let @xmath21 be two identical vertices .",
    "one can see that for any vertex @xmath160 , @xmath161 .",
    "then the following is true .",
    "let @xmath162 be a vertex - class containing type - i or type - ii identical vertices .",
    "then the closeness centrality values of all the vertices in @xmath163 are equal .    to construct these equivalance classes for the initial graph ,",
    "we first use a hash function to map each vertex neighborhood to an integer : @xmath164 = \\sum_{v \\in \\gamma_g(u ) } v$ ] .",
    "we then sort the vertices with respect to their hash values and construct the type - i vertex - classes by eliminating false positives due to collisions on the hash function .",
    "a similar process is applied to detect type - ii vertex classes .",
    "the complexity of this initial construction is @xmath165 assuming the number of collisions is small and hence , false - positive detection cost is negligible .    maintaining the equivalance classes in case of edge insertions and deletions",
    "is easy : for example , when @xmath52 is added to @xmath12 , we first subtract @xmath22 and @xmath11 from their classes and insert them to new ones  ( or leave them as singleton if none of the vertices are now identical with them ) .",
    "the cost of this maintenance is @xmath166 .    while updating closeness centralities of the vertices in @xmath8 , we execute an sssp at line  [ line : source ] of algorithm  [ alg : combined ] for at most one vertex from each class . for the rest of the vertices , we use the same closeness centrality value .",
    "the improvement is straightforward and the modifications are minor . for brevity , we do not give the pseudocode .      the spike - shaped distribution given in figure  [ fig : levels ]",
    "can also be exploited for sssp hybridization .",
    "consider the execution of algorithm  [ alg : cc ] : while executing an sssp with source @xmath16 , for each vertex pair @xmath167 , @xmath22 is processed before @xmath11 if and only if @xmath168 .",
    "that is , algorithm  [ alg : cc ] consecutively uses the vertices with distance @xmath169 to find the vertices with distance @xmath170 .",
    "hence , it visits the vertices in a _ top - down _ manner .",
    "sssp can also be performed in a a _ bottom - up _ manner .",
    "that is to say , after all distance  ( level ) @xmath169 vertices are found , the vertices whose levels are unknown can be processed to see if they have a neighbor at level @xmath169 .",
    "figure  [ fig : bfstimes ] gives the execution times of bottom - up and top - down sssp variants for processing each level .",
    "the trend for top - down resembles the shortest distance distribution in small - world networks .",
    "this is expected since in each level @xmath171 , the vertices that are @xmath171 step far away from @xmath16 are processed . on the other hand , for the bottom - up variant , the execution time is decreasing since the number of unprocessed nodes is decreasing .",
    "following the idea of beamer  et  al .",
    "@xcite , we hybridize the sssps throughout the centrality update phase in algorithm  [ alg : combined ] .",
    "we simply compare the number of edges need to be processed for each variant and choose the cheaper one . for the case presented in figure",
    "[ fig : bfstimes ] , the hybrid algorithm is @xmath172 times faster than the top - down variant .     and @xmath173 times faster than the top - down and bottom - up versions respectively . , scaledwidth=40.0% ]",
    "to the best of our knowledge , there are only two works that deal with maintaining centrality in dynamic networks . yet , both are interested in betweenness centrality .",
    "lee  et  al . proposed the * qube * framework which updates betweenness centrality in case of edge insertion and deletion within the network  @xcite .",
    "* qube * relies on the biconnected component decomposition of the graphs . upon an edge insertion or deletion , assuming that the decomposition does not change , only the centrality values within the updated biconnected component are recomputed from scratch .",
    "if the edge insertion / deletion affects the decomposition the modified graph is decomposed into its biconnected components and the centrality values in the affected part are recomputed .",
    "the distribution of the vertices to the biconnected components is an important criteria for the performance of * qube*. if a large component exists , which is the case for many real - life networks , one should not expect a significant reduction on update time .",
    "unfortunately , the performance of * qube * is only reported on small graphs  ( less than 100k edges ) with very low edge density .",
    "in other words , it only performs significantly well on small graphs with a tree - like structure having many small biconnected components .    green  et  al .",
    "proposed a technique to update centrality scores rather than recomputing them from scratch upon edge insertions  ( can be extended to edge deletions )  @xcite .",
    "the idea is storing the whole data structure used by the previous betweenness centrality update kernel .",
    "this storage is indeed useful for two main reasons : it avoids a significant amount of recomputation since some of the centrality values will stay the same . and second",
    ", it enables a partial traversal of the graph even when an update is necessary .",
    "however , as the authors state , @xmath174 values must be kept on the disk . for the wikipedia user communication and dblp coauthorship networks , which contain thousands of vertices and millions of edges , the technique by green  et  al .",
    "requires terabytes of memory .",
    "the largest graph used in  @xcite has approximately @xmath175 vertices and @xmath176 edges ; the quadratic storage cost prevents their storage - based techniques to scale any higher .",
    "on the other hand , the memory footprint of our algorithms are linear and hence they are much more practical .",
    "we implemented our algorithms in c. the code is compiled with gcc v4.6.2 and optimization flags -o2 -dndebug .",
    "the graphs are kept in memory in the compressed row storage  ( crs ) format .",
    "the experiments are run on a computer with two intel xeon e@xmath177 cpu clocked at @xmath178ghz and equipped with @xmath179 gb of main memory .",
    "all the experiments are run sequentially .    for the experiments , we used @xmath180 networks from the ufl sparse matrix collection and we also extracted the coauthor network from current set of dblp papers .",
    "properties of the graphs are summarized in table  [ tab : graph_prop ] .",
    "we symmetrized the directed graphs .",
    "the graphs are listed by increasing number of edges and a distinction is made between small graphs ( with less than 500k edges ) and the large graphs ( with more than 500k ) edges .      to assess the effectiveness of our algorithms",
    ", we need to know that when each edge is inserted to / deleted from the graph .",
    "our datasets from ufl sparse matrix collection do not have this information . to conduct our experiments on these datasets",
    ", we delete 1,000 edges from a graph chosen randomly in the following way : a vertex @xmath181 is selected randomly  ( uniformly ) , and a vertex @xmath182 is selected randomly  ( uniformly ) .",
    "since we do not want to change the connectivity in the graph  ( having disconnected components can make our algorithms much faster and it will not be fair to cc ) , we discard @xmath52 if it is a bridge .",
    "if this is not the case we delete it from @xmath12 and continue .",
    "we construct the initial graph by deleting these 1,000 edges .",
    "each edge is then inserted one by one , and our algorithms are used to recompute the closeness centrality after each insertion . beside these random insertion experiments",
    ", we also evaluated our algorithms on a real temporal dataset of the dblp coauthor graph . in this graph",
    ", there is an edge between two authors if they published a paper .",
    "publication dates are used as timestamps of edges .",
    "we first constructed the graph for the papers published before january 1 , 2013 .",
    "then , we inserted the coauthorship edges of the papers since then .",
    "although our experiments perform edge insertion , edge deletion is a very similar process which should give comparable results .",
    "in addition to cc , we configure our algorithms in four different ways : cc - bonly uses biconnected component decomposition ( bcd ) , cc - bluses bcd and filtering with levels , cc - bliuses all three work filtering techniques including identical vertices . and cc - blihuses all the techniques described in this paper including sssp hybridization",
    ".    table  [ tab : results ] presents the results of the experiments.the second column , cc , shows the time to run the full brandes algorithm for computing closeness centrality on the original version of the graph .",
    "columns @xmath183@xmath184 of the table present absolute runtimes  ( in seconds ) of the centrality computation algorithms .",
    "the next four columns , @xmath185@xmath180 , give the speedups achieved by each configuration .",
    "for instance , on the average , updating the closeness values by using cc - bon _ pgpgiantcompo _ is @xmath186 times faster than running cc .",
    "finally the last column gives the overhead of our algorithms per edge insertion , i.e. , the time necessary to detect the vertices to be updated , and maintain bcd and identical - vertex classes .",
    "geometric means of these times and speedups are also given to provide comparison across instances .",
    "the times to compute closeness centrality using ccon the small graphs range between @xmath187 to @xmath188 seconds . on large graphs ,",
    "the times range from @xmath189 minutes to @xmath190 hours .",
    "clearly , ccis not suitable for real - time network analysis and management based on shortest paths and closeness centrality .",
    "when all the techniques are used  ( cc - blih ) , the time necessary to update the closeness centrality values of the small graphs drops below @xmath183 seconds per edge insertion .",
    "the improvements range from a factor of @xmath191  ( _ cond - mat-2005 _ ) to @xmath192  ( _ pgpgiantcompo _ ) , with an average improvement of @xmath193 across small instances . on large graphs ,",
    "the update time per insertion drops below @xmath194 minutes for all graphs .",
    "the improvements range from a factor of @xmath195  ( _ loc - gowalla _ ) to @xmath196  ( _ dblp - coauthor _ ) , with an average of @xmath197 . for all graphs ,",
    "the time spent filtering the work is below one second which indicates that the majority of the time is spent for sssps .",
    "note that this part is pleasingly parallel since each sssp is independent from each other .",
    "the overall improvement obtained by the proposed algorithms is very significant .",
    "the speedup obtained by using bcds  ( cc - b ) are @xmath198 and @xmath199 on the average for small and large graphs , respectively .",
    "the graphs _ pgpgiantcompo _ , and _ wiki - talk _ benefits the most from bcds  ( with speedups @xmath186 and @xmath200 , respectively ) . clearly using the biconnected component decomposition improves the update performance .",
    "however , filtering by level differences is the most efficient technique : cc - blbrings major improvements over cc - b . for all social networks ,",
    "cc - blincreased the performance when compared with cc - b , the speedups range from @xmath201  ( _ web - notredame _ ) to @xmath202  ( _ dblp - coauthor _ ) .",
    "overall , cc - blbrings a @xmath203 improvement on small graphs and a @xmath204 improvement on large graphs over cc .",
    "for each added edge @xmath52 , let @xmath205 be the random variable equal to @xmath206 . by using 1,000 @xmath52 edges , we computed the probabilities of the three cases we investigated before and give them in fig .",
    "[ fig : leveldiffs01 ] . for each graph in the figure , the sum of first two columns gives the ratio of the vertices not updated by cc - bl .",
    "for the networks in the figure , not even @xmath207 of the vertices require an update  ( @xmath208 ) .",
    "this explains the speedup achieved by filtering using level differences .",
    "therefore , level filtering is more useful for the graphs having characteristics similar to small - world networks .     into three cases we investigated when",
    "an edge @xmath52 is added .",
    ", scaledwidth=44.0% ]    filtering with identical vertices is not as useful as the other two techniques in the work filter .",
    "overall , there is a @xmath209 times improvement with cc - blion both small and large graphs compared to cc - bl .",
    "for some graphs , such as _ web - notredame _ and _ web - google _ , improvements are much higher  ( @xmath210 and @xmath211 , respectively ) .",
    "finally , the hybrid implementation of sssp also proved to be useful .",
    "cc - blihis faster than cc - bliby a factor of @xmath212 on small graphs and by a factor of @xmath213 on large graphs .",
    "although it seems to improve the performance for all graphs , in some few cases , the performance is not improved significantly .",
    "this can be attributed to incorrect decisions on sssp variant to be used .",
    "indeed , we did not benchmark the architecture to discover the proper parameter . cc - blihperforms the best on social network graphs with an improvement ratio of @xmath214  ( _ soc - sign - epinions _ ) , @xmath215  ( _ loc - gowalla _ ) , and @xmath216  ( _ wiki - talk _ ) .",
    "all the previous results present the average update time for 1,000 successively added edges .",
    "hence , they do not say anything about the variance . figure  [ fig : updatedist ] shows the runtimes of cc - band cc - blihper edge insertion for _ web - notredame _ in a sorted order .",
    "the runtime distribution of cc - bclearly has multiple modes . either the runtime is lower than @xmath217 milliseconds or it is around @xmath218 seconds .",
    "we see here the benefit of bcd . according to the runtime distribution , about @xmath219 of _ web - notredame",
    "_ s vertices are inside small biconnected components .",
    "hence , the time per edge insertion drops from 2,845 seconds to 700 .",
    "indeed , the largest component only contains @xmath220 of the vertices and @xmath221 of the edges of the original graph .",
    "the decrease in the size of the components accounts for the gain of performance",
    ".     added edges of _ web - notredame_.,scaledwidth=44.0% ]    the impact of level filtering can also be seen on figure  [ fig : updatedist ] .",
    "@xmath222 of the edges in the main biconnected component do not change the closeness values of many vertices and the updates that are induced by their addition take less than @xmath187 second .",
    "the remaining edges trigger more expensive updates upon insertion . within these @xmath210 expensive edge insertions , identical vertices and sssp hybridization provide a significant improvement  ( not shown in the figure ) .    [",
    "[ better - speedups - on - real - temporal - data ] ] better speedups on real temporal data + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the best speedups are obtained on the dblp coauthor network , which uses real temporal data .",
    "using cc - b , we reach @xmath223 speedup w.r.t .",
    "cc , which is bigger than the average speedup on all networks .",
    "main reason for this behavior is that @xmath224 of the inserted edges are actually the new vertices joining to the network , i.e. , authors with their first publication , and cc - bhandles these edges quite fast .",
    "applying cc - blgives a @xmath225 speedup over cc - b , which is drastically higher than on all other graphs .",
    "indeed , only @xmath226 of the vertices require to run a sssp algorithm when an edge is inserted on the dblp network .",
    "for the synthetic cases , this number is @xmath227 .",
    "cc - bliprovides similar speedups with random insertions and cc - blihdoes not provide speedups because of the structure of the graph .",
    "overall , speedups obtained with real temporal data reaches @xmath228 , i.e. , @xmath229 times greater than the average speedup on all graphs .",
    "our algorithms appears to perform much better on real applications than on synthetic ones .",
    "all the techniques presented in this paper allow to update closeness centrality faster than the non - incremental algorithm presented in  @xcite by a factor of @xmath193 on small graphs and @xmath197 on large ones .",
    "small - world networks such as social networks benefit very well from the proposed techniques .",
    "they tend to have a biconnected component structure that allow to gain some improvement using cc - b",
    ". however , they usually have a large biconnected component and still , most of the gain is derived from exploiting their spike - shaped distance distribution which brings at least a factor of @xmath230 .",
    "identical vertices typically brings a small amount of improvement but helps to increase the performance during expensive updates . using all the techniques",
    ", we achieved to reduce the closeness centrality update time from @xmath231 days to @xmath194 minutes for the graph with the most vertices in our dataset  ( _ wiki - talk _ ) . and",
    "for the temporal dblp coauthorship graph , which has the most edges , we reduced the centrality update time from 1.3 days to 4.2 minutes .",
    "in this paper we propose the first algorithms to achieve fast updates of exact centrality values on incremental network modification at such a large scale .",
    "our techniques exploit the biconnected component decomposition of these networks , their spike - shaped shortest - distance distributions , and the existence of nodes with identical neighborhood . in large networks with more than @xmath232 edges , our techniques proved to bring a @xmath233 times speedup in average . with a speedup of 458",
    ", the proposed techniques may even allow dblp to reflect the impact on centrality of the papers published in quasi real - time .",
    "our algorithms will serve as a fundamental building block for the centrality - based network management problem , closeness centrality computations on dynamic / streaming networks , and their temporal analysis .",
    "the techniques presented in this paper can directly be extended in two ways .",
    "first , using a statistical sampling to compute an approximation of closeness centrality only requires a minor adaptation on the sssp kernel to compute the contribution of the source vertex to other vertices instead of its own centrality .",
    "second , the techniques presented here also apply to betweenness centrality with minor adaptations .    as a future work",
    ", we plan to investigate local search techniques for the centrality - based network management problem using our incremental centrality computation algorithms .",
    "this work was supported in parts by the doe grant de - fc02 - 06er2775 and by the nsf grants cns-0643969 , oci-0904809 , and oci-0904802 .",
    "k.  madduri , d.  ediger , k.  jiang , d.  a. bader , and d.  g. chavarra - miranda . a faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets . in _ proc .",
    "of ipdps _ , 2009 ."
  ],
  "abstract_text": [
    "<S> analyzing networks requires complex algorithms to extract meaningful information . </S>",
    "<S> centrality metrics have shown to be correlated with the importance and loads of the nodes in network traffic . here , we are interested in the problem of centrality - based network management . </S>",
    "<S> the problem has many applications such as verifying the robustness of the networks and controlling or improving the entity dissemination . it can be defined as finding a small set of topological network modifications which yield a desired closeness centrality configuration . as a fundamental building block to tackle that problem , </S>",
    "<S> we propose incremental algorithms which efficiently update the closeness centrality values upon changes in network topology , i.e. , edge insertions and deletions . </S>",
    "<S> our algorithms are proven to be efficient on many real - life networks , especially on small - world networks , which have a small diameter and a spike - shaped shortest distance distribution . </S>",
    "<S> in addition to closeness centrality , they can also be a great arsenal for the shortest - path - based management and analysis of the networks . </S>",
    "<S> we experimentally validate the efficiency of our algorithms on large networks and show that they update the closeness centrality values of the temporal dblp - coauthorship network of 1.2 million users 460 times faster than it would take to compute them from scratch . to the best of our knowledge , </S>",
    "<S> this is the first work which can yield practical large - scale network management based on closeness centrality values .    </S>",
    "<S> [ graph algorithms ] </S>"
  ]
}