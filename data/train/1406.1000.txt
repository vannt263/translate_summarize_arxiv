{
  "article_text": [
    "we consider the estimation under squared error loss of a vector @xmath7 observed with gaussian additive error : @xmath8 , @xmath9 , where @xmath10 are   @xmath11 . it is natural in our applications to consider the index @xmath12 as denoting time , and regard @xmath7 as a realization of a stochastic process .",
    "we analyze , accordingly , two main setups . in the first ,",
    "the estimation is done retrospectively , after all of @xmath13 are observed .",
    "the second case is of sequential estimation , where @xmath0 should be estimated at time @xmath12 , after observing @xmath14 .",
    "let @xmath15 be the data set based on which @xmath0 is estimated , excluding the @xmath12th observation itself .",
    "that is , @xmath16 in the sequential case , and @xmath17 when the estimation is retrospective .",
    "we could consider a more general situation in which the observations are @xmath18 , @xmath9 , where the @xmath19s are observed covariates and @xmath20 however , to simplify the presentation , we discuss only the situation without observed covariates : @xmath21    when @xmath22 are a realization of a gaussian process , the optimal estimator for @xmath0 based on the data set @xmath23 is indeed linear , and is given by the kalman filter ( kf ) .",
    "however , in more general state space models , and certainly when the model is misspecified , the kalman filter , or any other linear scheme , are not optimal .",
    "yet , they may be taken as a reasonable starting point for the construction of a better estimator .",
    "we consider in this paper an empirical bayes improvement of a given linear filter which is nonparametric and does not depend on structural assumptions .",
    "the linear estimator @xmath24 in can be be considered as a weighted average of two components , @xmath5 , and an estimator @xmath25 based on all the observations available at time @xmath12 excluding the @xmath12th observations itself : @xmath26 in the gaussian case , @xmath25 and @xmath24 are typically the sufficient statistics for @xmath0 given the data in @xmath15 and @xmath23 respectively . in the sequential gaussian case",
    "the estimator @xmath27 is called the optimal one step ahead predictor of @xmath0 while @xmath28 is the kf estimator of @xmath0 , @xmath29 . for background about the kf , state - space models , and general facts",
    "about time series see , e.g. , brockwell and davis ( 1991 ) .",
    "we will hardly use that theory in the following development , since we aim for results that are true regardless on whether various common state - space assumptions hold . in the sequel ,",
    "when we want to emphasize that @xmath27 and @xmath28 are the standard kf estimators we will write @xmath30 and @xmath31 , but the following derivation is for a general pair @xmath27 and @xmath28 .",
    "our goal in this paper is to use @xmath25 as a basis for the construction of an estimator which improves upon @xmath24 .",
    "in fact , we try to find the best estimator of the form : @xmath32 thus , we use a simple coordinate - wise function , as was introduced by robbins ( 1951 ) in the context of compound decision :    [ def:1 ] a function @xmath33 is called simple coordinate - wise function , if it has the representation @xmath34 for some @xmath35 .",
    "our improvement , denoted @xmath36 , is a simple coordinate - wise function of @xmath37 .",
    "in the theory of compound decision and empirical bayes , the search for an optimal simple - coordinate - wise decision function is central .",
    "we elaborate in the next section .",
    "the improved estimator @xmath38 is denoted @xmath39 , and in vector notations we write in short @xmath40      the ideas of empirical bayes ( eb ) and compound decision ( cd ) procedures were developed by robbins ( 1951 , 1955 , 1964 ) , see the review papers of copas ( 1969 ) and zhang ( 2003 ) , and the paper of greenshtein and ritov ( 2008 ) for results relating compound decision , simple coordinate - wise decision functions and permutational invariant decision functions .",
    "the classical eb / cd theory is restricted to independent exchangeable observations and to permutation invariant procedures , and in particular it excludes the utilization of explanatory variables .",
    "fay and herriot ( 1979 ) suggested a way to extend the ideas of parametric eb ( i.e , linear decision functions corresponding to gaussian measurement and prior ) to handle covariates .",
    "recently , there is an effort to extend the eb ideas , so they may be incorporated in the presence of covariates also in the context of non - parametric eb , see , jiang and zhang ( 2010 ) , cohen et al .",
    "( 2013 ) , and koenker and mizera ( 2013 ) .",
    "our paper may be viewed as a continuation of this effort .",
    "the above papers extended the discussion to the situation where the observations , due to the covariates , are not exchangeable .",
    "however , the estimated parameters themselves , @xmath7 , are permutation invariant .",
    "thus , in all these problems , centering each response by a linear transformation of the covariates transforms the problem into a classical eb problem . in our setup of a time series ,",
    "the estimated variables are not permutation invariant , and the explanatory variables of @xmath5 are the available observations @xmath41 , so there is an obvious strong dependence between the response variables and the covariates and the response is degenerate conditional on the covariates .",
    "furthermore , in all the above mentioned papers the extension of eb ideas to handle covariates is done in a retrospective setup , where all the observations are given in advance . under the time series structure that we study , it is natural to consider real time sequential estimation of the @xmath4 s . in section [ sec : seq ]",
    "we consider the sequential case , where at stage @xmath12 the decision function should be approximated based on the currently available data .",
    "our analysis would be based on an extension of samuel ( 1965 ) .",
    "the retrospective case is simpler and will be treated first in section [ sec : nonseq ] .",
    "a small simulation study is presented in section [ sec : sim ] , and a real data example is discussed in section [ sec : real ] .",
    "most eb / cd solutions involve simple coordinate - wise functions . by the nature of the problem ,",
    "these functions are estimated from the data , which is used symmetrically .",
    "@xmath42 where @xmath43 are the ordered statistics    unfortunately , any permutation invariant function can be written in this way .",
    "suppose for simplicity that @xmath44 are real .",
    "let @xmath45 be a permutation invariant function .",
    "let @xmath46 be the indicator function .",
    "it is possible to write @xmath47 as in , with @xmath48 or a smooth version of this function .",
    "actually , any function that is estimated from the data and is used only on that data can be written as a simple coordinate - wise function .    intuitively , the set of simple coordinate - wise functions is a strict subset of the set of permutation invariant functions .",
    "we therefore consider a function @xmath49 as simple coordinate - wise function if it approximates a function @xmath50 that is simple coordinate - wise function by definition [ def:1 ] .",
    "this later function may be random ( i.e. , a stochastic process ) , with non - degenerate asymptotic distribution .      the performance of our estimators will be measured by their mean squared error loss , in vector notation : @xmath51 , @xmath52 , and @xmath53 .",
    "let @xmath54 be the smallest @xmath55-field under which @xmath56 , @xmath57 are measurable",
    ". the dependency of different objects on @xmath1 will be suppressed , when there will be no danger of confusion .",
    "[ ass1 ] for every @xmath9 , the estimator @xmath25 is @xmath58 measurable .",
    "it is lipschitz in @xmath56 with a constant @xmath59 , where @xmath60 .",
    "that is : for @xmath57 let @xmath61 be @xmath25 , but computed with @xmath56 replaced by @xmath62 .",
    "then , @xmath63 .",
    "this condition is natural when @xmath64 is kf for a stationary gaussian process , where typically @xmath65 decreases exponentially with @xmath66 .",
    "the main need for generalizing the kf is to include filters which are based on estimated parameters .",
    "the kalman filter for an ergodic process also satisfies the following condition .",
    "it has no real importance for our results , except giving a standard benchmark .",
    "[ ass2 ] suppose that there is a @xmath67 , @xmath68 : @xmath69 as @xmath70 , @xmath71 .",
    "* remark : * our major example is the ergodic normal state - space model .",
    "if the assumed model is correct , and @xmath25 and @xmath24 are the optimal estimators , then @xmath25 is a sufficient statistics for @xmath0 given @xmath15 .",
    "the estimators satisfy with @xmath72 , where @xmath73 is the asymptotic variance of @xmath0 given @xmath27 . in the iterative kalman filter method for computing @xmath28 , with some abuse of notation ,",
    "the values @xmath74 are computed , with @xmath75 the variance of @xmath0 given @xmath27 , and we have @xmath76 .    by considering the functions @xmath77 and @xmath78 in , it is easy to see that @xmath79 has asymptotically mean squared error not larger than @xmath80 and @xmath81 , respectively .",
    "in fact , we argue that unless the process is asymptotically gaussian , there is a strict improvement .",
    "the derivation of the kalman filter is based on an assumed stochastic model for the sequence @xmath7 .",
    "very few properties of the the process are relevant , and it is irrelevant to our discussion whether the model is true or not .",
    "however , we do need some tightness .",
    "we expect that typically @xmath82 is not larger than @xmath83 , and @xmath25 is sensible at least as @xmath84 .",
    "since @xmath85 , the next condition is natural :    [ ass3 ] it holds :",
    "denote , @xmath86 clearly , @xmath87 . since @xmath88 is independent both of @xmath7 and of @xmath89 , @xmath90 ,",
    "it is independent of @xmath91 .",
    "thus , the conditional distribution of @xmath92 given @xmath91 is @xmath93 .",
    "however , this is not a regular eb problem .",
    "it is not so even for the regular kf .",
    ". then @xmath95 .",
    "it is true that @xmath96 , but the vectors @xmath97 and @xmath98 are not independent .",
    "hence @xmath99 .",
    "yet , we rely only on the marginal distributions of @xmath100 , @xmath9 .    to elaborate , therefore ,",
    "the joint density of @xmath101 and @xmath102 is proportional to @xmath103 where @xmath104 is the joint density of the vector @xmath105 .",
    "clearly , unless @xmath104 is multivariate normal , the conditional density of @xmath106 given @xmath107 is not multivariate standard normal .",
    "suppose @xmath108 , we observe @xmath109 , and use @xmath110 , @xmath111 .",
    "then suppose further that @xmath0 is finitely supported .",
    "it follows from the above calculations that the distribution of the vector @xmath106 given the vector @xmath107 is finitely supported as well .",
    "the estimator in vector notation is @xmath112 , where @xmath113 . as discussed in the introduction , simple coordinate - wise functions like @xmath114",
    "are central in eb and cd models .",
    "however , our decision function @xmath79 is not a simple coordinate - wise function of the observations .",
    "it is a hybrid of non - coordinate - wise function @xmath80 and a simple coordinate - wise one , @xmath114 .",
    "the @xmath80 component accounts for the non - coordinate - wise information from the covariates , while @xmath114 aims to improve it in a coordinate - wise way after the information from all other observations was accounted for by @xmath115 .    by assumption [ ass1 ] ,",
    "the dependency between the @xmath92s conditioned on @xmath107 is only local , and hence , if we consider a permutation invariant procedure , which treats neighboring observations and far away ones the same , the dependency disappears asymptotically , and we may consider only the marginal normality of the @xmath107 .",
    "the basic ideas of eb are helpful and we get the representation of @xmath116 as given below .",
    "let @xmath117 where @xmath118 is the standard normal density .",
    "note that this is not a kernel estimator  the kernel is with fixed bandwidth and @xmath119 are unobserved .",
    "let @xmath120 be uniformly distributed over @xmath121 .",
    "denote by @xmath122 the distribution of the random pairs @xmath123 , where @xmath124 are standard normal independent of the other random variables mentioned so far and the randomness is induced by the random index @xmath120 and the @xmath125s .",
    "one marginal distribution of @xmath122 is the empirical distribution of @xmath119 , while the density of the other is given by @xmath126 .",
    "we denote the marginals by @xmath127 and @xmath128 . finally , note that @xmath129 given @xmath130 has the distributing of @xmath131 , i.e. , the conditional distribution of @xmath122 .",
    "it is well known that asymptotically , the bayes procedure for estimating @xmath91 given @xmath92 is approximated by the bayes procedure with @xmath132 as prior , and it is determined by @xmath126 .",
    "the optimal simple coordinate - wise function @xmath133 depends only on marginal joint distribution of @xmath134 . in fact , it depends only on @xmath126 . as in brown ( 1971 ) we have : @xmath135 where @xmath136 is the derivative of @xmath126 .",
    "the dependency on @xmath1 is suppressed in the notations .",
    "note that @xmath137 is a random function , and in fact , if @xmath7 is not an ergodic process , it may not have an asymptotic deterministic limit .",
    "yet , it would be the object we estimate in below .",
    "it is of a special interest to characterize when @xmath133 is asymptotically linear , in which case the improved estimator @xmath138 is asymptotically a linear combination of @xmath27 and @xmath5 . only in such a case",
    "the difference between the loss of the improved estimator @xmath139 and that of the estimator @xmath81 may be asymptotically of @xmath140 .",
    "it follows from that , the optimal decision @xmath141 is approximately @xmath142 , if and only if , @xmath143 is approximately proportional to @xmath144 .",
    "this happens only if @xmath126 converges to a gaussian distribution . since @xmath126 is a convolution of a gaussian kernel with the the prior",
    ", this can happen only if the prior is asymptotically gaussian . in our setup where @xmath127 plays the role of a prior , in order to have asymptotically linear improved estimator",
    "we need that @xmath132 converges weakly to a normal distribution @xmath145 .",
    "the above is formally stated in the following proposition [ prop : lin ] .",
    "[ prop : lin ] under assumptions 1 - 3 , @xmath146 implies that the sequence @xmath147 converges weakly to the zero measure .",
    "given the observations @xmath148 , let @xmath149 be the empirical distribution of @xmath150 .",
    "we will show that as @xmath151 the ` distance ' between @xmath149 and @xmath152 gets smaller , so that @xmath126 and and its derivative may be replaced in by appropriate kernel estimates based on @xmath153 , and yield a good enough estimator @xmath154 of @xmath137 . in the sequel we will occasionally drop the superscript @xmath1 .",
    "note , that we do not assume that @xmath122 approaches some limit @xmath155 as @xmath151 , although this is the situation if we assume that @xmath156 is an ergodic stationary process , however our assumptions on that process are milder .",
    "we now state the above formally .",
    "consider the two kernel estimators @xmath157 and @xmath158 , where @xmath159 , @xmath160 .",
    "for simplicity , we use the same bandwidth to estimate both the density and its derivative .",
    "we define the following estimator @xmath161 for @xmath162 :    @xmath163    brown and greenstein ( 2009 ) used the normal kernel , we prefer to use the logistic kernel @xmath164 ( it is the derivative of the logistic cdf , @xmath165 , and hence its integral is 1 ) .",
    "we suggest this kernel since it ensures that @xmath166 , see the appendix .",
    "however , we do adopt the recommendation of brown and greenshtein ( 2009 ) for a very slowly converging sequence @xmath167 .",
    "denote @xmath168 .",
    "[ th:1 ] under assumptions [ ass1 ] and [ ass3 ] :    * @xmath169 * under assumptions [ ass1][ass3 ] , if @xmath170 and @xmath127 converges weakly to a distribution different from @xmath171 , then there is @xmath172 such that for large enough @xmath1 : @xmath173    * the proof is given in the appendix * the same arguments used to prove part i ) may be used to prove a modification of proposition [ prop : lin ] , in which @xmath174 is replaced by @xmath175 , when assuming in addition that @xmath176 for any @xmath177 .",
    "part i ) of the above theorem assures us that asymptotically the improved estimator does as good as @xmath81 ; part ii ) implies that in general the improved estimator does asymptotically strictly better .",
    "obviously the asymptotic improvement is not always strict since the kalman filter is optimal under a gaussian state - space model .",
    "we consider now the case @xmath178 , @xmath179 .",
    "the definition of the different estimators is the same as in the previous section with the necessary adaption to the different information set .",
    "our aim is to find a sequential estimator , denoted @xmath180 , that satisfies @xmath181 . by a sequential estimator @xmath182 we mean that @xmath183 , @xmath3",
    ". a natural approach , which indeed works , is to let @xmath184 , where @xmath185 is defined as in , but with @xmath186 restricted to the available data @xmath187 , @xmath3 .",
    "let @xmath188 .",
    "we define : @xmath189 our main result in this section :    [ th:2 ] theorem [ th:1 ] holds with @xmath190 and @xmath191 replacing @xmath192 and @xmath193 , respectively .    in order to prove theorem [ th:2 ] we adapt lemma 1 of samuel ( 1965 ) .",
    "samuel s result is stated for a compound decision problem , i.e. , the parameters are fixed , and the observations are independent . the result compares the performance of the optimal estimators in the sequential and retrospective procedures .",
    "it is not clear a priori whether retrospective estimation is easier or more difficult than the sequential .",
    "on the one hand , the retrospective procedure is using more information when dealing with the @xmath12th parameter . on the other hand , the sequential estimator can adapt better to non - stationarity in the parameter sequence .",
    "samuel proved that the latter is more important .",
    "there is no paradox here , since the retrospective procedure is optimal only under the assumption of permutation invariance , and under permutation invariance , the weak inequality in lemma [ lem : samuel ] below is , in fact , equality .",
    "our approach is to rephrase and generalize samuel s lemma .",
    "let @xmath124 be @xmath11 random variables independent of @xmath194 , @xmath9 .",
    "let @xmath195 be the loss for estimating @xmath91 by @xmath196 .",
    "for every @xmath179 let @xmath197 be the decision function that satisfies : @xmath198 where @xmath199 is the expectation over @xmath200 .",
    "that is , @xmath201 is the functional that minimizes the sum of risks for estimating the components @xmath202 , but it is applied only for estimating @xmath91 .",
    "the quantity @xmath203 is the analog of @xmath204 in samuel s formulation . in analogy to samuel ( 1965 ) we define @xmath205 , the empirical bayes risk of the non - sequential problem .",
    "[ lem : samuel ] @xmath206    the proof of the lemma is formally similar to the proof of lemma 1 of samuel ( 1965 ) .    from theorem [ th:1 ] , @xmath207 .",
    "the last fact coupled with lemma [ lem : samuel ] implies part i ) of theorem [ th:2 ] .",
    "part ii ) is shown similarly to part ii ) of theorem [ th:1 ] .",
    "we present now simulation results for the following state - space model .",
    "@xmath208    where @xmath209 , @xmath3 , are independent of each other and of @xmath210 , @xmath3 .",
    "the variables @xmath211 , @xmath3 are independent , @xmath212 where @xmath213 are independent , while @xmath214 are bernoulli with mean 0.1 , independent of each other and of @xmath215 , @xmath3 .",
    "we study the twelve cases that are determined by @xmath216 and @xmath217 . in each case",
    "we investigate both the sequential and the retrospective setups .",
    "if @xmath218 , were i.i.d normal , the data would follow a gaussian state - space , and the corresponding kalman filter estimator would be optimal . since the @xmath211 s are not normal , the corresponding ar(1 ) kalman filter is not optimal ( except in the degenerate case , @xmath219 ) , though it is optimal among linear filters .",
    "this is reflected in our simulation results where for the cases @xmath220 our `` improved '' method @xmath221 performs slightly worse than @xmath222 .",
    "it improves in all the rest .",
    "the above is stated and proved formally in the following proposition .",
    "it could also be shown indirectly by applying part ii ) of theorem [ th:1 ] .",
    "[ prop : strict ] consider the state - space model , as defined by . if @xmath211 are not normally distributed then @xmath223 for a constant @xmath224 and large enough @xmath1",
    ".    given the estimators @xmath225 and @xmath226 , @xmath3 , let @xmath227 .",
    "then @xmath228 .",
    "the distribution @xmath229 of @xmath91 may be normal only if @xmath211 is normal , since @xmath211 is independent of @xmath230 and @xmath27 .",
    "the distributions @xmath229 converge to a distribution @xmath145 as @xmath12 and @xmath231 approach infinity . as before ,",
    "@xmath145 is normal only if @xmath211 are normal .",
    "now , asymptotically optimal estimator for @xmath0 under squared loss and given the observation @xmath5 , is @xmath232 , where @xmath233 is the bayes estimator under a prior @xmath145 on @xmath91 and an observation @xmath234 .",
    "this bayes estimator is linear and coincide with the kf estimator @xmath31 , only if @xmath145 is normal .    analogous discussion and situation are valid also in the sequential case . in our simulations",
    "the parameters @xmath235 and @xmath236 are treated as known . alternatively , maximum likelihood estimation assuming ( wrongly ) normal inovations yields results similar to those reported in table [ tab1 ] .",
    "the simulation results in table [ tab1 ] are for the case @xmath237 .",
    "each entry is based on 100 simulations . in each realization we recorded @xmath238 and @xmath239 , and each entry",
    "is based on the corresponding average . in order to speed the asymptotics we allowed a ` warm up ' of 100 observations prior to the @xmath237 in the sequential case",
    ", we also allowed a ` warm up ' of 50 in both sides of the @xmath237 observations in the retrospective case .",
    "@xmath240 & 0 & 1 & 2 & 3 & 4 & 5 & 0 & 1 & 2 & 3 & 4 & 5 + & & & & & & + @xmath241@xmath242 & 0 & 71 & 156 & 226 & 290 & 333 & 0 & 49 & 147 & 235 & 301 & 350 + @xmath243@xmath244 & 23 & 66 & 125 & 148 & 160 & 177 & 24 & 91 & 166 & 215 & 253 & 271 + & & & & & & + @xmath241 @xmath242 & 0 & 47 & 145 & 234 & 309 & 355 & 0 & 83 & 187 & 264 & 325 & 372 + @xmath243 @xmath244 & 39 & 81 & 129 & 147 & 159 & 158 & 34 & 112 & 184 & 216 & 239 & 253 +    it may be seen that when the best linear filter is optimal or nearly optimal ( when @xmath219 or approximately so ) , our improved method is slightly worse than the kalman filter estimator , however as @xmath240 increases , the advantage of the improved method may become significant .",
    "it seems that in the case @xmath245 the future observations are not very helpful , and in our simulations there are cases where the simulated risk of @xmath81 in the sequential case is even smaller than the corresponding simulated risk of the retrospective case .",
    "this could be an artifact of the simulations , but also a result of noisy estimation of the coefficients @xmath65 of the mildly informative future observations .",
    "r|ddd @xmath246 & @xmath247 & @xmath248 _  _ & @xmath249 + @xmath250 & 27.1 & 19.4 & 20.4 + improved method - @xmath251   & 18.7 & 18.5 & 17.4 + naive method - @xmath252 & 26.4 & 26.4 & 26.4    in this section we demonstrate the performance of our method on real data taken from the fx ( foreign exchange ) market in israel .",
    "the data consists of the daily number of swaps ( purchase of of one currency for another with a given value date , done simultaneously with the selling back of the same amount with a different value date . this way there is no foreign exchange risk ) in the otc ( over - the - counter ) shekel / dollar market .",
    "we consider only the buys of magnitude 5 to 20 million dollars .",
    "the time period is january 2nd , 2009 to december 31st , 2013 , a total of @xmath253 business days .",
    "the number of buys in each day is 24 on the average , with the range of 271 . in our analysis",
    "we used the first 100 observations as a ` warm up ' , similarly to the way it was done in our simulations section .",
    "we denote by @xmath254 , @xmath29 , the number of buys on day @xmath12 and assume that @xmath255 we transform the data by @xmath256 as in brown et al .",
    "( 2010 ) and brown et al .",
    "( 2013 ) in order to get an ( approximately ) normal variable with variance @xmath257    the assumed model in this section is the following state space system of equations :    where @xmath258 and @xmath209 are independent of each other and of the @xmath259 process .",
    "we consider the following three special cases of @xmath260 : @xmath261 , @xmath262 , and @xmath263    under each model there are induced kalman filter estimators @xmath264 , and @xmath265 that correspond to one step prediction and to the update .",
    "similarly , the improved estimator @xmath266 is defined .",
    "we denote the sequential and retrospective estimators similarly with no danger of confusion .    after estimating @xmath0",
    ", we transform the result back to get the estimator @xmath267 for @xmath268 , @xmath269 , @xmath3 , @xmath270 where , @xmath271 is the estimator of @xmath272 by method @xmath273 we evaluate the performances of both estimation methods by the following non - standard cross - validation method as described in brown et al .",
    "it is briefly explained in the following .",
    "let @xmath274 @xmath275 and let @xmath276 be independent given @xmath277 , where @xmath278 are binomial variables .",
    "it is known that @xmath279 and @xmath280 and they are independent given @xmath281 we will use the ` main ' sub - sample @xmath276 for the construction of both estimators ( kalman filter and improvement ) while the ` auxiliary ' sub - sample @xmath282 is used for validation .",
    "consider the following loss function , @xmath283the term @xmath284 and will be ignored .",
    "we estimate @xmath285 by the method of moments : @xmath286    we repeat the cross - validation process 500 times and average the computed values of @xmath287 . when @xmath288 is close to 1 , the average obtained is a plausible approximation of the average squared risk in estimating @xmath268 , @xmath289 . by the above method we approximated also the average risk of the naive estimator @xmath290 , @xmath289 .",
    "the approximations for the retrospective and sequential cases , are displayed in tables [ taba1 ] and [ taba3 ] .",
    "the estimated arima coefficient for the various models are given in table [ taba2 ] .",
    "l|ddd @xmath246 & @xmath247 & @xmath248 & @xmath249 _",
    "_  _ _ + @xmath291 & 12.38 & 14.218 & 0.01 + @xmath292 & -0.28 & -0.341 & -0.6 + @xmath293 & & -0.124 & + @xmath294 & 3.4 & 3.4 & 4.7 +    l|ddd @xmath246 & @xmath247 & @xmath248 & @xmath249 + @xmath250 & 19.2 & 19.2 & 21.2 + improved method - @xmath251   & 19.0 & 19.2 & 22.6 + naive method - @xmath252 & 26.4 & 26.4 & 26.4    from table [ taba1 ] we can observe that in the retrospective case the improved method does uniformly better than the naive estimator and the kalman filter .",
    "in fact , in all except a small deterioration under the arima(1,1,0 ) with sequential filtering , the performance of the improved method is quite uniform , showing its robustness against model miss - specification .",
    "somewhat surprising is that the kalman filter under ar(1 ) with retrospective estimation does not do better than the naive filter , but do so considerably in the sequential case .",
    "the reason is that the ar(1 ) model does not fit the data .",
    "when it is enforced on the data , the kalman filter gives too much weight to the surrounding data , and too little to the `` model free '' naive estimator .",
    "this result show the robustness of our estimator .",
    "in fact , we did a small simulation , where the process was ar(2 ) , with the parameters as estimated for the data .",
    "when an ar(1 ) was fitted to the data , the retrospective kalman filter was strictly inferior to the sequential one .",
    "in the sequential case , table [ taba3 ] , the improved method does better than the naive method , but contrary to the non - sequential case , it improves upon the kalman filter only in the ar(1 ) and ar(2 ) models , while in the arima(1,1,0 ) model the kalman filter does slightly better .",
    "note that by , obviously , @xmath295 .",
    "thus , in order to obtain @xmath296 it is enough to show that @xmath297      by assumption [ ass1 ] , if we replace @xmath25 by a similar ( unobserved ) function @xmath299 , where @xmath56 , @xmath57 , @xmath300 is replaced by @xmath301 , then @xmath302 , for any @xmath303 .",
    "define @xmath304 and @xmath305 as in , but where @xmath25 is replaced by @xmath299 , @xmath9 . since @xmath306 , @xmath307 is ignorable for our approximations . in the following all variables",
    "are replaced by their @xmath308 version , but we drop the @xmath308 for simplicity .",
    "let then since @xmath89 and @xmath314 are independent , this difference has mean 0 .",
    "moreover , since @xmath315 and @xmath316 are independent for @xmath317 , the above expression has variance of order @xmath318 .",
    "the difference @xmath319 is of order @xmath320 .    a similar expansion works for @xmath321 , except that the variance now is of order @xmath322 .",
    "regular large deviation argument shows that when @xmath323 then @xmath324 with exponential small probability . by and it follows that the the approximation @xmath325 holds not only in the mean but also in the mean square , since the exception probability are smaller than @xmath320 .",
    "brown , l.d . ,",
    "cai , t. , zhang , r. , zhao , l. , zhou , h. ( 2010 ) .",
    "the root - unroot algorithm for density estimation as implemented via wavelet block thresholding .",
    "_ probability and related fields _ , * 146 * , 401 - 433 .",
    "greenshtein , e. and ritov , y. ( 2008 ) .",
    "asymptotic efficiency of simple decisions for the compound decision problem .",
    "the 3rd lehmann symposium .",
    "ims lecture notes monograph series , j.rojo , editor .",
    "266 - 275 .",
    "jiang , w. and zhang , c .- h .",
    "empirical bayes in - season prediction of baseball batting average .",
    "_ borrowing strength : theory powering application - a festschrift for l.d .",
    "brown _ j.o .",
    "berger , t.t .",
    "cai , i.m .",
    "johnstone , eds .",
    "ims collections * 6 * , 263 - 273 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating the means @xmath0 of @xmath1 random variables @xmath2 , @xmath3 . assuming some structure on the @xmath4 process , e.g. , a state space model </S>",
    "<S> , one may use a summary statistics for the contribution of the rest of the observations to the estimation of @xmath0 . </S>",
    "<S> the most important example for this is the kalman filter . </S>",
    "<S> we introduce a non - linear improvement of the standard weighted average of the given summary statistics and @xmath5 itself , using empirical bayes methods . </S>",
    "<S> the improvement is obtained under mild assumptions . </S>",
    "<S> it is strict when the process that governs the states @xmath6 is not a linear gaussian state - space model . </S>",
    "<S> we consider both the sequential and the retrospective estimation problems . </S>"
  ]
}