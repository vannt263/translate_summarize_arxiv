{
  "article_text": [
    "we are concerned with the following semi- and nonparametric regression model    @xmath0    where @xmath1 is the observed response from @xmath2-th individual @xmath3 at time @xmath4 for @xmath5 , @xmath6 is the corresponding explanatory variable , @xmath7 and @xmath8 are individual - specific location and scale parameters and @xmath9 is a baseline intensity function . here ,",
    "@xmath10 , @xmath11 , and @xmath12 and @xmath6 are independent .",
    "of interest here is the simultaneous estimation of @xmath7 , @xmath8 and @xmath9 .",
    "we shall assume throughout the paper that @xmath12 @xmath13 are independent and identically distributed ( i.i.d . ) with an unknown distribution function , though most results only require that the errors be independent with zero mean .",
    "model ( [ model ] ) is motivated by analyzing the data generated from mass spectrometer ( ms ) , which is a powerful tool for the separation and large - scale detection of proteins present in a complex biological mixture .",
    "figure 1 is an illustration of ms spectra , which can reveal proteomic patterns or features that might be related to specific characteristic of biological samples .",
    "they can also be used for prognosis and for monitoring disease progression , evaluating treatment or suggesting intervention .",
    "two popular mass spectrometers are seldi - tof ( surface enhanced laser desorption / ionization time - of - fight ) and maldi - tof ( matrix assisted laser desorption and ionization time - of - flight ) . the abundance of the protein fragments from a biological sample ( such as serum ) and their time of flight through a tunnel under certain electrical pressure can be measured by this procedure .",
    "the @xmath14-axis of a spectrum is the intensity ( relative abundance ) of protein / peptide , and the @xmath15-axis is the mass - to - charge ratio ( m / z value ) which can be calculated using time , length of flight , and the voltage applied .",
    "it is known that the seldi intensity measures have errors up to 50% and that the @xmath16 may shift its value by up to 0.1%0.2% @xcite . generally speaking ,",
    "many pre - processing steps need to be done before the ms data can be analyzed . some of the most important steps are noise filtering , baseline correction , alignment , normalization , etc .",
    "see , e.g. , @xcite .",
    "we refer readers to @xcite for an extensive review about the recent advances in mass - spectrometry data analysis . here , we assume all the pre - processing steps have already been taken .        in model ( [ model ] )",
    ", @xmath9 represents the common shape for all individuals while @xmath7 and @xmath8 represents the location and scale parameters for the @xmath2-th individual , respectively .",
    "because @xmath9 is unspecified , model may be viewed as a semiparameteric model .",
    "however , it differs from the usual semi - parametric models in that for model ( [ model ] ) , both the parametric and nonparametric components are of primary interest , while in a typical semiparametric setting , the nonparametric component is often viewed as a nuisance parameter .",
    "model contains many commonly encountered regression models as special cases .",
    "if all the parametric coefficients @xmath7 and @xmath8 are known , model ( [ model ] ) reduces to the classical nonparametric regression . on the other hand ,",
    "if the function @xmath9 is known , then it reduces to the classical linear regression model with each subject having its own regression line . for the present case of @xmath7 , @xmath8 and function @xmath9 being unknown ,",
    "the parameters are identifiable only up to a common location - scale change .",
    "thus we assume , without loss of generality , that @xmath17 and @xmath18 .",
    "it is also clear that for @xmath7 , @xmath8 and @xmath9 to be consistently estimable , we need to require that both @xmath19 and @xmath20 go to @xmath21 .",
    "there is an extensive literature on semiparametric and nonparametric regression . for semiparametric regression , @xcite derived semiparametric information bound while @xcite developed a general approach to constructing @xmath22-consistent estimation for the parametric component .",
    "we refer to @xcite and @xcite for detailed discussions on the subject . for nonparametric regression , kernel and local polynomial smoothing methods",
    "are commonly used @xcite . in particular",
    ", local polynomial smoothing has many attractive properties including the automatic boundary correction .",
    "we refer to @xcite and @xcite for comprehensive treatment of the subject .    the existing methods for dealing with nonparametric and semiparametric problems are not directly applicable to model .",
    "this is due to the mixing of the finite dimensional parameters and the nonparametric component . a natural way to handle such",
    "a situation is to de - link the two aspects of the estimation through a two - step approach . in this paper",
    ", we propose an efficient iterative procedure , alternating between estimation of the parametric component and the nonparametric component .",
    "we show that the proposed approach leads to consistent estimators for both the finite - dimensional parameter and the nonparametric function .",
    "we also establish asymptotic normality for parametric estimator and convergence rate for the nonparametric estimation that is then used for optimal bandwidth selection .",
    "in this section , we develop a multi - step approach to estimating both the finite - dimensional parameters @xmath7 and @xmath8 and the nonparametric baseline intensity @xmath9 .",
    "our approach is an iterative procedure which alternates between estimation of @xmath7 and @xmath8 and that of @xmath9 . we show that under reasonable conditions , the estimation for the parametric component is consistent and asymptotically normal when the bandwidth selection are done appropriately .",
    "the estimation of the nonparametric component can also attain the optimal rate of convergence .      recall that if @xmath7 and @xmath8 were known , the problem would reduce to the standard nonparametric regression setting ;",
    "on the other hand , if @xmath9 were known , it would reduce to the simple linear regression for each @xmath2 . for the nonparametric regression",
    ", we can apply the local linear regression with the weights @xmath23 for suitably chosen kernel function @xmath24 and bandwidth @xmath25 .",
    "for the simple linear regression , the least squares estimation may be applied .",
    "not all parameters in model are identifiable as @xmath7 , @xmath8 and @xmath9 are confounded .",
    "to ensure identifiability , we shall set @xmath17 and @xmath18 .",
    "thus , for @xmath26 , becomes a standard nonparametric regression problem , from which an initial estimator of @xmath9 can be derived .",
    "replacing @xmath9 in by the initial estimator , we can apply the least squares method to get estimators of @xmath7 , @xmath8 for @xmath27 , which , together with @xmath17 and @xmath18 and local polynomial smoothing , can then be used to get an updated estimator of @xmath9 .",
    "this iterative estimation procedure is described as follows .    * set @xmath17 and @xmath18 , so that @xmath28 , @xmath29",
    ". apply local linear regression to @xmath30 , to get initial estimator of @xmath9 @xmath31 where @xmath32 and @xmath33 * with @xmath9 being replaced by @xmath34 as the true function , @xmath7 , @xmath8 , @xmath35 can be estimated by the least squares method , i.e. @xmath36y_{it}}{\\sum_{t=1}^t[\\tilde m(x_{it})-\\bar{\\tilde m}(x_{i\\cdot})]^2 } ,    \\end{aligned}\\ ] ] @xmath37 where @xmath38 * with the estimates @xmath39 and @xmath40 , we can update the estimation of @xmath9 viewing @xmath39 and @xmath40 as true values . specifically , we apply the local linear regression with the same kernel function @xmath41 to get an updated estimator of @xmath9 , @xmath42 where @xmath43 , @xmath44      \\end{aligned}\\ ] ] and @xmath45 note that the bandwidth for this step , @xmath46 , may be chosen differently from @xmath25 in order to achieve better convergence rate .",
    "the optimal choices for @xmath25 and @xmath46 will become clear in the next subsection where large sample properties are studied . * repeat steps ( b ) and ( c ) until both the parametric and the nonparametric estimators converge .",
    "our limited numerical experiences indicate that the final estimator is not sensitive to the initial estimate .",
    "however , as a safe guard , we may start the algorithm with different initial estimates by choosing different individuals as the baseline intensity . in step ( c ) , the @xmath40 is in the denominator , which , when close to @xmath47 , may cause instability .",
    "thus , in practice , we can add a small constant to the denominator to make it stable , though we have not encountered this problem .",
    "the iterative process often converges very quickly .",
    "in addition , our asymptotic analysis in the next subsection shows that no iteration is needed to reach the optimal convergence rate for the estimate of both parametric and nonparametric components when the bandwidths of each step are properly chosen .",
    "therefore , we may stop after step ( c ) to save computation time for large problems .      in this section ,",
    "we study the large sample properties of the estimates for @xmath9 , @xmath7 and @xmath8 . by large sample , we mean that both @xmath19 and @xmath20 are large .",
    "however , the size of @xmath19 and that of @xmath20 can be different . indeed , for ms data , @xmath20 is typically much larger than @xmath19",
    ". the optimal bandwidth selection in the nonparametric estimation will be determined by asymptotic expansions to achieve optimal rate of convergence .",
    "we will also investigate whether or not the accuracy of @xmath39 and @xmath40 may affect the rate of convergence for the estimation of @xmath9 .",
    "the following conditions will be needed to establish the asymptotic theory .    *",
    "the baseline intensity @xmath9 is continuous and has a bounded second order derivative .",
    "* there exist constants @xmath48 and @xmath49 , such that the marginal density @xmath50 of @xmath6 satisfies @xmath51 , and @xmath52 for any @xmath15 and @xmath14 in the support of @xmath50 . *",
    "the conditional variance @xmath53 is bounded and continuous in @xmath15 , where @xmath54 and @xmath55 . *",
    "the kernel @xmath41 is a symmetric probability density function with bounded support .",
    "hence @xmath41 has the properties : @xmath56 and bounded . without loss of generality",
    ", we could further assume the support of @xmath41 lies in the interval @xmath57 $ ] .",
    "condition c1 is a standard condition for nonparametric estimation .",
    "condition c2 requires that the density of @xmath6 is bounded away from 0 , which may be a strong assumption in general but reasonable for mass spectrometry data as @xmath6 are approximately uniformly distributed on the support .",
    "in addition , the density is assumed to satisfy a lipschitz condition .",
    "condition c3 allows for heteroscedasticity while restricting the variances to be bounded .",
    "condition c4 is a standard condition for kernel function used in the local linear regression .",
    "the moments of @xmath24 and @xmath58 are denoted respectively by @xmath59 and @xmath60 for @xmath61 .",
    "[ lm1 ] suppose that conditions c1-c4 are satisfied .",
    "then for @xmath34 defined by , we have , as @xmath62 and @xmath63 , @xmath64 where @xmath65 .",
    "lemma [ lm1 ] allows us to derive the asymptotic bias , variance and mean squared error for the estimator @xmath34 .",
    "this is summarized in the following corollary .",
    "[ co1 ] let @xmath66 denote all the observed covariates @xmath67",
    ". under conditions c1-c4 , the bias , variance and mean squared error of @xmath68 conditional on @xmath66 have the following expressions .",
    "@xmath69^{-1}\\sigma_1 ^ 2(x)\\nu_0+o\\big(\\frac{1}{th}\\big),\\\\ & & \\e[\\{\\tilde m(x)-m(x)\\}^2\\big|\\mathbf x]=\\frac{1}{4}(m''(x)\\mu_2)^2h^4+\\frac{1}{th}[f(x)]^{-1}\\sigma_1 ^ 2(x)\\nu_0+o\\big(h^4+\\frac{1}{th}\\big).\\end{aligned}\\ ] ]    it is clear from the above expansions that in order to minimize the mean squared error of @xmath68 , the bandwidth @xmath25 should be chosen to be of order @xmath70 .",
    "however , we will show later that this is not necessarily optimal for our final estimator @xmath71 . for estimation of scale parameters",
    "@xmath8 , we can apply lemma [ lm1 ] together with the taylor expansion to derive asymptotic bias and variance .",
    "in particular , we have the following theorem .",
    "[ th1 ] suppose that conditions c1-c4 are satisfied and that @xmath62 is chosen so that @xmath63",
    ". then the following expansions hold for @xmath72 .    @xmath73    where @xmath74 @xmath75    [ rm2 ] the asymptotic bias and variance of parameter estimator @xmath39 can be similarly derived .",
    "in fact , they can be inferred from the bias and variance of @xmath40 through its linear relationship with @xmath40 , thus having the same order as those of @xmath40 in and .",
    "[ rm3 ] the bias of @xmath40 is of the order @xmath76 and the variance is of the order @xmath77 . to obtain the @xmath78-consistency for @xmath40 ,",
    "i.e. @xmath79 , the order of bias should be @xmath80 .",
    "this is achieved by choosing @xmath25 to be between @xmath81 and @xmath82 .    from the asymptotic expansion for the mean and variance of the initial functional estimator @xmath34 and parameter estimator @xmath40 , we can obtain the asymptotic expansions for the bias and variance of the subsequent estimator of the baseline intensity , @xmath71 .",
    "[ th2 ] suppose that conditions c1-c4 are satisfied .",
    "suppose also that @xmath25 for @xmath34 and @xmath46 for @xmath71 are chosen so that @xmath62 , @xmath83 , @xmath63 , and @xmath84",
    ". then the following expansions hold : @xmath85 @xmath86\\right)^2\\sigma_1 ^ 2(x_{1t})\\\\&+\\frac{\\nu_0\\sum_{i=2}^n\\beta_i^2f^{-1}(x)\\sigma_i^2(x_{it})}{th^*(\\sum_{i=1}^n\\beta_i^2)^2}+o\\big(\\frac{1}{t}+\\frac{1}{nth^*}\\big).\\\\\\end{aligned}\\ ] ] where @xmath87 are the same as those in theorem [ th1 ] , and @xmath88 , @xmath89 .    in the ideal case when the location - scale parameters are known , the bias and variance of the local linear estimator of baseline intensity @xmath9 should be of the order @xmath90 and @xmath91 . and",
    "the optimal bandwidth in this ideal case should be of order @xmath92 . therefore the bias and variance of the nonparametric estimator are @xmath93 and @xmath94 , respectively .",
    "in addition , the mean squared error is of order @xmath94 .",
    "interestingly , by choosing the bandwidths @xmath25 and @xmath46 separately , we can achieve this optimal rate of convergence for the baseline intensity estimator @xmath71 through the proposed multi - step estimation procedure when the orders of @xmath19 and @xmath20 satisfy certain requirement .",
    "notice that the parametric components will have the optimal @xmath78 convergence rate simultaneously .",
    "the conclusions are summarized in the following theorem .",
    "[ th3 ] suppose that conditions c1-c4 are satisfied .",
    "the optimal parametric convergence rate of location - scale estimators can be attained by setting @xmath25 to be of order @xmath95 ; the optimal nonparametric convergence rate of the baseline intensity estimator @xmath71 can be attained by setting @xmath46 to be of order @xmath92 and @xmath25 of order @xmath95 , when @xmath96 , @xmath97 , and @xmath98 .",
    "[ rm6 ] it is clear from theorem [ th3 ] that if the requirement @xmath98 is not satisfied , then the nonparametric estimator @xmath71 will not achieve the optimal rate of convergence at any choice of the bandwidths .",
    "however , the choice of @xmath25 and @xmath46 is optimal even if @xmath99 does not hold .",
    "[ th4 ] suppose that conditions c1-c4 are satisfied .",
    "in addition , assume @xmath100<\\infty$ ] and @xmath101>0 $ ] for all @xmath102 and @xmath29 .",
    "if we restrict the order of @xmath25 to lie between @xmath103 and @xmath104 , @xmath40 is asymptotic normal : @xmath105 where @xmath106    here , if we assume @xmath107 to be a constant for each subject @xmath2 , then its value can be consistently estimated by the plug - in estimator @xmath108 where @xmath109 and @xmath110    from , the asymptotic variance of @xmath40 is of order @xmath111 , provided that the order of the bandwidth @xmath25 is properly chosen . since the asymptotic expansion for @xmath40 does",
    "not involves the choice of @xmath46 , the specific choice of different @xmath46 will not affect the order of the asymptotic variance of @xmath40 .      in section [ sec::large : sample : properties ] ,",
    "we studied how the choice of bandwidths @xmath25 and @xmath46 may affect the asymptotic properties of the estimators .",
    "however , in practice , we need a data - driven approach to choosing the bandwidths .",
    "our suggestion on this is to use a @xmath24-fold cross - validation bandwidth selection rule .",
    "first , we divide the @xmath19 individuals into @xmath24 groups @xmath112 randomly . here , @xmath113 is the @xmath114-th test set , and the @xmath114-th training set is @xmath115 .",
    "we estimate the baseline curve @xmath9 using the observations in the training set @xmath116 and denote the estimator as @xmath117 , where @xmath25 and @xmath46 are the bandwidths of the two nonparametric regression steps for @xmath34 and @xmath71 , respectively .",
    "recall that at the beginning of the multi - step estimation procedure , we fix the first observation as the baseline to solve the identifiability issue . in the case of cross - validation , for each split , the baseline will corresponds to the first observation inside @xmath116 , which is different for different @xmath114 .",
    "we circumvent the problem of comparing different baseline estimates by using them to predict the test data in @xmath118 , i.e. , after obtaining the estimator of baseline curve from @xmath116 .",
    "we then regress it on the data in @xmath119 , and compute the mean squared prediction error ( mspe ) . @xmath120 ^ 2,\\ ] ] where @xmath121 and @xmath122 are the estimated regression coefficients .",
    "we repeat the calculation for @xmath123 , and the optimal pair @xmath124 is the one which minimizes the average mspe , i.e. @xmath125 the effectiveness of the cross - validation will be evaluated in sections [ sec::realdata ] and [ sec::simulation ] .",
    "we now apply the proposed multi - step method to a seldi - tof mass spectrometry data set from a study of 33 liver cancer patients conducted at changzheng hospital in shanghai . for each patient",
    ", we extract the @xmath16 values in the region 2000 - 10000 da , which is believed to contain all the useful information .",
    "figure [ fig::real - estimates ] contains the curves of 10 randomly picked patients .",
    "there are some noticeable features in the data .",
    "all curves appear to be continuous .",
    "they peak simultaneously around certain locations ; at each location , curves have the same shape but with different heights .",
    "all those features are captured well by our model .        since the observed values of @xmath16 for each person may fluctuate , we need to perform registration to make the analysis easier . here",
    ", we use the observations from the first individual and set his / her @xmath16 values as the reference",
    ". then we use the linear interpolation method to compute the intensities of all the other individuals at the reference @xmath16 locations .",
    "after that we get the preprocessed data which has the same @xmath16 values for each observation .",
    "we use the cross - validation method described in section [ sec::bandwidth.selection ] to select the optimal bandwidths with @xmath126 , i.e. , leave - one - out cross validation .",
    "we compute the mspe at the grid of @xmath127 and @xmath128 .",
    "table [ tb::realdata : mspe ] contains a portion of the result with @xmath129 and @xmath130 .",
    ".mse of the leave - one - out prediction of real data[tb::realdata : mspe ] [ cols=\"^,>,>,>,>,>,>\",options=\"header \" , ]",
    "this paper proposes a semi- and nonparametric model suitable for analyzing the mass spectra data .",
    "the model is flexible and intuitive , capturing the main feature in the ms data .",
    "both the parametric and nonparametric components have natural interpretation .",
    "a multi - step iterative algorithm is proposed for estimating both the individual location and scale regression coefficients and the nonparametric function .",
    "the algorithm combines local linear fitting and the least squares method , both of which are easy to implement and computationally efficient . both simulation studies and real data analysis demonstrate that the proposed multi - step procedure works well .",
    "the local linear fitting for the nonparametric function estimation maybe replaced with other nonparametric estimation techniques .",
    "because the location and scale parameters are subject specific , the empirical bayes method @xcite may be used .",
    "in addition , nonparametric bayes may also be applicable with the nonparametric function being modeling as a realization of gaussian process .",
    "the proposed model and the associated iterative estimation method do not account for the random error in the measurement of @xmath131 .",
    "it is desirable to incorporate the measurement error into the model @xcite .",
    "many studies involving ms data are aimed at classifying patients of different disease types .",
    "the information of peaks are usually applied as the basis of the classifier .",
    "the proposed method provides a natural way of finding the peaks for different group of patients by use the multi - step estimation procedure on each group and find out the corresponding nonparametric baseline function . from the estimated baseline function ,",
    "the information of peaks can be easily extracted , which can then be used for classification .",
    "the research was supported in part by national institutes of health grant r37gm047845 .",
    "the authors would like to thank liang zhu and cheng wu at shanghai changzheng hospital for providing the data .",
    "the appendix contains proofs of lemma 1 , corollary 1 and theorems 1 - 4 .",
    "we begin with some notation , which will be used to streamline some of the proofs .",
    "because all asymptotic expansions are derived with @xmath6 s being fixed , we will , for notational simplicity , use @xmath132 to denote the conditional expectation and @xmath133 to denote the conditional variance given @xmath6 s throughout the appendix . for @xmath134 and @xmath29 , let        it follows from and the definition of @xmath136 that @xmath137 from condition c1 , we have @xmath138 where @xmath139 is uniform in @xmath4 . thus @xmath140}{\\sum_{t=1}^t\\omega_{1t}(x)}\\nonumber\\\\ & \\quad + \\frac{\\sum_{t=1}^t\\omega_{1t}(x)\\sigma_1(x_{1t})\\epsilon_{1t}}{\\sum_{t=1}^t\\omega_{1t}(x)}\\nonumber\\\\ & = m(x)+\\frac{\\sum_{t=1}^t\\omega_{1t}(x)[\\frac{1}{2}m''(x)(x_{1t}-x)^2+o((x_{1t}-x)^2)]}{\\sum_{t=1}^t\\omega_{1t}(x)}+u_1(x)\\nonumber\\\\ & = m(x)+\\frac{(s_{t,2}^2-s_{t,1}s_{t,3})m''(x)}{2(s_{t,0}s_{t,2}-s_{t,1}^2)}+o\\big(\\frac{s_{t,2}^2-s_{t,1}s_{t,3}}{s_{t,0}s_{t,2}-s_{t,1}^2}\\big)+u_1(x),\\label{eq::tildem}\\end{aligned}\\ ] ] where the last equality follows from the definition of @xmath141 .    a standard asymptotic expansion for the local linear smoothing ( fan and gijbels , 1996 , eq 3.13 ) results in @xmath142 note that with @xmath143 and 1 in , we have , @xmath144 and @xmath145 since @xmath146 and @xmath147 , combined with , @xmath148 this completes the proof of lemma [ lm1 ] .",
    "being a weighted average of mean - zero random variables , @xmath149 has zero mean .",
    "thus , from lemma 1 , we have @xmath150=\\frac{1}{2}m''(x)\\mu_2h^2+o(h^2).\\ ] ] for the variance term , from the definition of @xmath34 , we have @xmath151^{-1}\\sigma_1 ^ 2(x)\\nu_0+o\\big(\\frac{1}{th}\\big),\\end{aligned}\\ ] ] where the third equation follows from , and the last equation follows from condition c3 and . combining the above asymptotic expansions for the bias and variance terms leads to the desired expansion for the mean squared error .",
    "first of all , define @xmath152 to simplify the presentation . by definition",
    ", we have the following expansion for @xmath40 when @xmath72 .",
    "@xmath153 from lemma 1 and the proof of corollary [ co1 ] , we have @xmath154    plugging ( [ eq - lm1 ] ) into @xmath155 , we have @xmath156     } { \\sum_{t=1}^t \\tilde w_{it } ^2}\\notag\\\\ & \\quad-\\frac{\\sum_{t=1}^t\\{[w_{it}+o(h^2)]u_1(x_{it})+o(h^2)[u_1(x_{it})-\\bar u_1(x_{i\\cdot})]\\}}{\\sum_{t=1}^t \\tilde w_{it } ^2}\\notag\\\\ & = -\\frac{\\sum_{t=1}^t(u_1(x_{it})-\\bar u_1(x_{i\\cdot}))u_1(x_{it})}{\\sum_{t=1}^tw_{it}^2}-h^2p_i-\\frac{\\sum_{t=1}^tw_{it}u_1(x_{it})}{\\sum_{t=1}^tw_{it}^2}(1+o_p(1))\\notag\\\\ & \\quad+o(h^2+\\frac{1}{th}),\\label{eq::d_i - expansion}\\end{aligned}\\ ] ] where the last asymptotic expansion follows from .",
    "similarly for @xmath157 , we have      we observe that for any @xmath72 , @xmath159 is a linear combination of @xmath160 .",
    "therefore , @xmath159 is independent of @xmath161 . by using the tower property",
    ", we have @xmath162 . therefore , @xmath163 is the only part that contributes to the bias of @xmath40 . in view of these and corollary [ co1 ] , we have the following expansions for the bias and variance terms @xmath164 and @xmath165            we expand @xmath168 in the neighborhood of point @xmath169 using taylor s expansion , @xmath170",
    "since the kernel function @xmath171 vanishes out of the neighborhood of @xmath169 with diameter @xmath25 , we can obtain the following @xmath172\\frac{k_h(x_{it}-x_{1s})(s_{t,2}-(x_{it}-x_{1s})s_{t,1})}{s_{t,0}s_{t,2}-s_{t,1}^2}\\\\ = & m(x_{1s})+o_p(h^2)\\sum_{t=1}^t\\frac{k_h(x_{it}-x_{1s})(s_{t,2}-(x_{it}-x_{1s})s_{t,1})}{s_{t,0}s_{t,2}-s_{t,1}^2}\\\\ = & m(x_{1s})+o_p(h^2),\\end{aligned}\\ ] ] where the functions @xmath173 are evaluated at the point @xmath6 .",
    "combined with @xmath174 , we can have the expansion @xmath175                from the proof of theorem [ th1 ]",
    ", we have @xmath180 then , from the least square expression , we have the asymptotic expansion for @xmath181 as follows . @xmath182\\nonumber\\\\ & = \\alpha_i+\\beta_ih^2r_i+\\bar m(x_{i\\cdot})\\beta_i\\frac{\\sum_{t=1}^t(u_1(x_{it})-\\bar u_1(x_{i\\cdot}))u_1(x_{it})}{\\sum_{t=1}^tw_{it}^2}+o\\big(h^2+\\frac{1}{th}\\big)\\nonumber\\\\ & \\quad+\\sum_{t=1}^tv_{it}\\epsilon_{it}(1+o_p(1))-\\beta_i\\sum_{t=1}^tv_{it}u_1(x_{it})(1+o_p(1)).\\label{eq::hat_alpha_expansion}\\end{aligned}\\ ] ]    now , we plug the above asymptotic expansions and into the right hand side of .",
    "the first part of could be expanded as follows @xmath183 } { \\sum_{i=1}^n\\sum_{t=1}^t\\hat\\beta_i^2k_{h^*}(x_{it}-x)\\left[\\sum_{i=1}^n\\hat\\beta_i^2s_{t,2}^{*(i)}-(x_{it}-x)\\sum_{i=1}^n\\hat\\beta_i^2s_{t,1}^{*(i)}\\right]}\\nonumber\\\\ = & \\frac{\\sum_{i=1}^n(\\alpha_i-\\hat\\alpha_i)\\hat\\beta_i\\sum_{t=1}^tk_{h^*}(x_{it}-x ) \\left[\\sum_{i=1}^n\\hat\\beta_i^2s_{t,2}^{*(i)}-(x_{it}-x)\\sum_{i=1}^n\\hat\\beta_i^2s_{t,1}^{*(i)}\\right ] } { \\sum_{i=1}^n\\hat\\beta_i^2\\sum_{t=1}^tk_{h^*}(x_{it}-x)\\left[\\sum_{i=1}^n\\hat\\beta_i^2s_{t,2}^{*(i)}-(x_{it}-x)\\sum_{i=1}^n\\hat\\beta_i^2s_{t,1}^{*(i)}\\right]}.\\label{eq::hat_m_bias part 1 } \\ ] ]    the numerator of has expansion @xmath184\\nonumber\\\\ = & t^2h^{*2}\\sum_{i=1}^n(\\alpha_i-\\hat\\alpha_i)\\hat\\beta_i\\bigg[f(x)\\{1+o_p(1)\\ } \\sum_{i=1}^n\\hat\\beta_i^2f(x)\\mu_2\\{1+o_p(1)\\}\\nonumber\\\\ & -\\{h^*f'(x)\\mu_2+o_p(h^{*2}+\\frac{1}{\\sqrt{th^*}})\\}\\sum_{i=1}^n\\hat\\beta_i^2\\{h^*f'(x)\\mu_2+o_p(h^{*2}+\\frac{1}{\\sqrt{th^*}})\\}\\bigg]\\nonumber\\\\ = & t^2h^{*2}\\sum_{i=1}^n(\\alpha_i-\\hat\\alpha_i)\\hat\\beta_i\\bigg[f(x)\\sum_{i=1}^n\\beta_i^2f(x)\\mu_2\\{1+o_p(1)\\}\\bigg],\\label{eq : hat_m_bias_p1_nume } \\ ] ] where the last equation following from @xmath185 .",
    "similarly , the denominator of has the following expansion @xmath186\\nonumber\\\\ = & t^2h^{*2}\\sum_{i=1}^n\\hat\\beta_i^2\\bigg[f(x)\\{1+o_p(1)\\ } \\sum_{i=1}^n\\hat\\beta_i^2f(x)\\mu_2\\{1+o_p(1)\\}\\nonumber\\\\ & -\\{h^*f'(x)\\mu_2+o_p(h^{*2}+\\frac{1}{\\sqrt{th^*}})\\}\\sum_{i=1}^n\\hat\\beta_i^2\\{h^*f'(x_i)\\mu_2+o_p(h^{*2}+\\frac{1}{\\sqrt{th^*}})\\}\\bigg]\\nonumber\\\\ = & t^2h^{*2}\\sum_{i=1}^n\\hat\\beta_i^2\\bigg[f(x)\\sum_{i=1}^n\\beta_i^2f(x)\\mu_2\\{1+o_p(1)\\}\\bigg].\\label{eq : hat_m_bias_p1_deno}\\end{aligned}\\ ] ]    then combining the expansions and , we have the following expansion for the first part of .",
    "@xmath187}{t^2h^{*2}\\sum_{i=1}^n\\hat\\beta_i^2\\bigg[f(x)\\sum_{i=1}^n\\beta_i^2f(x)\\mu_2\\{1+o_p(1)\\}\\bigg]}\\\\ = & \\frac{\\sum_{i=2}^n(\\alpha_i-\\hat\\alpha_i)\\beta_i}{\\sum_{i=1}^n\\beta_i^2}(1+o_p(1)).\\end{aligned}\\ ] ] for other parts of , we can apply the same techniques for expansion . as a result ,",
    "the following expansion of @xmath188 holds .",
    "@xmath189}{\\sum_{i=1}^n\\beta_i^2}\\\\ & \\qquad+\\frac{\\sum_{i=1}^n\\beta_i\\sum_{t=1}^tk_{h^*}(x_{it}-x)\\epsilon_{it}(1+o_p(1))}{\\sum_{i=1}^n\\beta_i^2}\\\\ & \\qquad+m(x)\\left\\{\\frac{\\sum_{i=2}^n\\beta_i^2h^2p_i}{\\sum_{i=1}^n\\beta_i^2}+\\frac{\\sum_{i=2}^n\\beta_i^2\\sum_{t=1}^t(u_1(x_{it})-\\bar u_1(x_{i\\cdot}))^2/\\sum_{t=1}^tw_{it}^2}{\\sum_{i=1}^n\\beta_i^2f(x)}\\right\\}\\\\ & \\qquad+m(x)\\left\\{\\frac{\\sum_{i=2}^n\\beta_i^2\\sum_{t=1}^tw_{it}u_1(x_{it})(1+o_p(1))/\\sum_{t=1}^tw_{it}^2}{\\sum_{i=1}^n\\beta_i^2}\\right\\}\\\\ & \\qquad+\\frac{m''(x)}{2}\\mu_2h^{*2}+o\\big(h^2+\\frac{1}{th}+h^{*2}\\big).\\end{aligned}\\ ] ]    then it is straightforward to derive the bias of @xmath190 as follows @xmath191   m(x)\\\\ & \\qquad+\\frac{m''(x)}{2}\\mu_2h^{*2}+o\\big(h^2+\\frac{1}{th}+h^{*2}\\big).\\end{aligned}\\ ] ] for the variance of @xmath190 , we notice that the error terms @xmath192 are independent , which implies the independence of @xmath193 and @xmath159 .",
    "therefore , we have the following asymptotic expansion for the variance . @xmath194 where the expansions follow similar techniques as and . now , by the definition of @xmath195 , we have @xmath196\\bigm)^2\\sigma_1 ^ 2(x_{1s})+\\frac{\\nu_0\\sum_{i=2}^n\\beta_i^2f^{-1}(x)\\sigma_i^2(x_{it})}{th^*(\\sum_{i=1}^n\\beta_i^2)^2}\\\\ & \\qquad+o\\bigm(\\frac{1}{t}+\\frac{1}{nth^*}\\bigm).\\end{aligned}\\ ] ]      from the results of theorem 2 , it is straightforward to show that the order of the mean squared error of @xmath190 is @xmath197 . to minimize the mean",
    "squared error , we can taken @xmath198 and @xmath199 .",
    "under such choices of @xmath25 and @xmath46 , the order of the mean squared error is @xmath200 .",
    "now , following the definition of @xmath203 and applying the same expansion of @xmath204 as in the proof of theorem 1 , @xmath205 ^ 2\\right)\\\\ = & \\text{e}\\biggm(\\bigg[\\sum_{t=1}^t\\bigm[\\frac{\\sum_{s=1}^tk_h(x_{it}-x_{1s})\\sigma_1(x_{1s})\\epsilon_{1s}}{tf(x_{it})}\\bigm]^2\\bigg]^2\\biggm)(1+o(1))\\\\ \\leq & \\frac{1}{t^4}\\text{e}\\biggm(\\sum_{s , u=1}^t\\big(\\sum_{t=1}^t\\frac{k_h^2(x_{it}-x_{1s})}{f^2(x_{it})}\\text{i}_{\\{|x_{1s}-x_{1u}|<2h\\}}\\big)^2\\sigma^2_1(x_{1s})\\sigma^2_1(x_{1u } ) \\biggm)(1+o(1)),\\end{aligned}\\ ] ] where the last inequality follows from exchanging the summation order and the property of the kernel function @xmath41 .",
    "observe that @xmath50 is bounded from below by condition c2 , the following inequality sequence is obtained .",
    "@xmath205 ^ 2\\right ) \\\\ \\leq & \\frac{1}{t^4\\delta^4}\\text{e}\\biggm(\\sum_{s , u=1}^t\\big ( \\frac{t\\nu_0f(x_{1s})}{h}\\big)^2\\text{i}_{\\{|x_{1s}-x_{1u}|<2h\\}}\\sigma^2_1(x_{1s})\\sigma^2_1(x_{1u})\\biggm)(1+o(1))\\\\ \\leq&\\frac{o(1)}{t^2h^2}\\sum_{s , u=1}^t\\text{i}_{\\{|x_{1s}-x_{1u}|<2h\\}},\\end{aligned}\\ ] ] where the last term has the order of @xmath206 by noticing @xmath207 we can also derive the order of the variance for the other two terms , @xmath208 due to the relationship of @xmath25 and @xmath20 , the third term is negligible when calculating the asymptotic variance .",
    "then , the expansion for the bias of @xmath40 can be rewritten as follows @xmath209 where the right hand side is an independent sum of random variables with their variances being of the same order , @xmath111 . as a result",
    ", the central limit theorem can be applied directly for @xmath40 .",
    "@xmath210\\rightarrow^d n(0,\\sigma_i^{*2}),\\ ] ] where the asymptotic variance @xmath211 is finite with the following expression.@xmath212.\\\\\\end{aligned}\\ ] ] notice that if the order of @xmath25 is between @xmath103 and @xmath104 , then @xmath40 is asymptotic unbiased since @xmath213 .    from theorems",
    "[ th1 ] and [ th2 ] we have @xmath214 are consistent estimators of @xmath215 , respectively .",
    "thus , @xmath216 is also consistent for the variance under the assumption that @xmath217 is a constant function for each subject @xmath2 .",
    "baggerly , k.a . ,",
    "morris , j.s . , wang , j. , gold , d. , xiao , l .- c .",
    "and coombes , k.r .",
    "( 2003 ) a comprehensive approach to the analysis of maldi - tof proteomics spectra from serum samples . _",
    "proteomics _ , * 3 * , 1667 - 1672 .",
    "raymond j. carroll , david ruppert , leonard a. stefanski , and ciprian m. crainiceanu .",
    "_ measurement error in nonlinear models : a modern perspective , second edition . _ chapman and hall / crc , 2 edition , june 2006 .",
    "roy , p. , truntzer , c. , maucort - boulch , d , jouve , t. , and molinari , n. ( 2011 ) protein mass spectra data analysis for clinical biomarker discovery : a global review .",
    "_ briefings in bioinformatics _ , * 12 * , 176 - 186          yasui , y. , pepe , m. , thompson , m. , adam , b .-",
    "wright , g. , qu , y. , potter , j. , winget , m. , thornquist , m. , and feng , z. ( 2003 ) a data - analytic strategy for protein biomarker discovery : profiling of high - dimensional proteomic data for cancer detection . _ biostatistics _ * 4(3 ) * , 449463 ."
  ],
  "abstract_text": [
    "<S> motivated by modeling and analysis of mass - spectrometry data , a semi- and nonparametric model is proposed that consists of a linear parametric component for individual location and scale and a nonparametric regression function for the common shape . </S>",
    "<S> a multi - step approach is developed that simultaneously estimates the parametric components and the nonparametric function . under certain regularity conditions </S>",
    "<S> , it is shown that the resulting estimators is consistent and asymptotic normal for the parametric part and achieve the optimal rate of convergence for the nonparametric part when the bandwidth is suitably chosen . </S>",
    "<S> simulation results are presented to demonstrate the effectiveness and finite - sample performance of the method . </S>",
    "<S> the method is also applied to a seldi - tof mass spectrometry data set from a study of liver cancer patients .    </S>",
    "<S> key words : local linear regression ; bandwidth selection ; nonparametric estimation . </S>"
  ]
}