{
  "article_text": [
    "shannon entropy is the most crucial foundation of information theory . in this traditional information theory and its applications , relative entropy ( or kullback - leibler distance ) is the basic information divergence .",
    "shannon information theory is useful for us to develop communications , data science and other subjects about information [ @xcite ] .",
    "due to its success , there are a lot of literature which attempts to advance these concepts .",
    "unfortunately , nearly none of them have been widely adopted except the the rnyi entropy [ @xcite][@xcite ] .",
    "the rnyi entropy has a wide range of applications .",
    "for example , according to [ @xcite ] , it is useful to adopt rnyi entropy in the traditional signal processing . besides that",
    ", the rnyi entropy was also used in independent component analysis ( ica ) with the fact that the information measures based on rnyi entropy could provide the distance measures among a cluster of probability densities [ @xcite ] . moreover",
    ", some studies suggested that it could help us to do blind source separation ( bss ) [ @xcite][@xcite ] .",
    "the chernoff information was developed to measure the information based on rnyi entropy [ @xcite ] .",
    "in fact , the chernoff information is the exponent when minimizing the overall probability of error , so it is very useful in hypothesis testing [ @xcite ] .",
    "+ in the last 68 years of information theory development , a large amount of concepts have been provided to describe the the channel information . among them , the most popular one is channel capacity . as a matter of fact",
    ", the channel capacity is defined as maximization mutual information .",
    "the mutual information is a measure of the amount of information and it is the reduction in uncertainty of one random variable when given the other variable [ @xcite ] . with respect to ( w.r.t . ) channel capacity , the mutual information is easier to use and expand , without loss of the ability to measure the channel `` information '' . as a result , in order to be promoted to other non - shannon entropy , the mutual information between channel input and output is adopted in this paper .",
    "+ unlike traditional research method of information theory , we attempt to revisit the traditional problem in big data viewpoint . obviously , the era of big data is coming .",
    "tha large amount of data can be easily gotten and the data processing is more and more important for us .",
    "two data rules are believed .",
    "for one thing , the data does not lie .",
    "for other thing , we only need to guarantee that our conclusion is correct with very large probability rather than one in the era of big data [ @xcite ] . in this paper , we choose the alternating conditional expectation ( ace ) algorithm as the tool to deal with data .",
    "+ the ace algorithm was proposed by breiman and friedman ( 1985 ) [ @xcite ] for estimating the transformations of dependent variable and a set of independent variables in multiple regression that estimate maximal correlation among the these variables [ @xcite ] .",
    "it can help us analyze multivariate function for it enabling multiple variable separation .",
    "its effectiveness and correctness were provided in [ @xcite ] . with the ace algorithm ,",
    "it is easy to separate the effect of different channel parameters so as to find the nature connection between channel information and the channel parameters .",
    "furthermore , one can investigate the relation between shannon and chernoff as well .",
    "+ the main contribution of this paper can be summarized as follows .",
    "we decompose the channel mutual information of shannon and chernoff w.r.t channel parameters by ace algorithm .",
    "the simulated results show us that shannon shakes hands with chernoff in big data viewpoint .",
    "based on these results , we put forward a conjecture that there is nature of channel information and no matter shannon mutual information , chernoff mutual information or other information measures , which are different measures of the same information quantity .",
    "this conclusion can help us to construct new information measures or judge a new channel information measure is reasonable or not .",
    "+        the mutual information is a measure of the amount of information that one random variable contains about another variable . in fact , it describes the reduction in the uncertainty of one random variable if another one is known [ @xcite ] .",
    "according to [ @xcite ] , the shannon mutual information is defined as : @xmath0 where @xmath1 are two random variables with marginal probability mass functions @xmath2 and @xmath3 and joint probability mass function @xmath4 .",
    "it is noted that @xmath5 is the relative entropy ( or kullback - leibler distance ) between the joint distribution @xmath4 and the product distribution @xmath6 .",
    "the relative entropy is given by [ @xcite ] : @xmath7      the chernoff information is derived from the problem of classic hypothesis testing .",
    "chernoff information is the resulting error exponent when minimizing the overall probability of error [ @xcite ] . as defined in [ @xcite ] , it is given by @xmath8 where @xmath9 is real parameter with @xmath10 .    with the aid of chernoff information to describe the relationship between the two variables , a new mutual information is defined as @xmath11 in this paper , we call it chernoff mutual information relative to shannon mutual information .",
    "+ in fact , both shannon mutual information and chernoff mutual information are to depict the inner relationship between two random variables .",
    "the difference is that they adopt different measures of the distance between two distributions .",
    "the relative entropy is adopted in shannon mutual information and the rnyi divergence is used in chernoff mutual information [ @xcite ] .",
    "the rest of this paper is organized as follows .",
    "section ii gives a brief introduction of the several special channel models . in this section ,",
    "we lists binary symmetric channel ( bsc ) and multiple symmetric channel ( msc ) .",
    "furthermore , this section also give a brief introduction of the ace algorithm . in section iii , some simulated examples are given .",
    "next , section iv gives the analysis of the simulated examples .",
    "finally , we present the conclusion in section v.",
    "the mutual information between the input and output of a channel can be used to describe the transmittability of a channel .",
    "for example , the maximum mutual information is defined as the channel capacity for a discrete memoryless channel .",
    "obviously , the mutual information is the measure of channel information .",
    "+      consider the bsc , which is shown in fig.[fig : channelbsc ] .",
    "it is a binary channel where the probability of the input symbols is @xmath12 .",
    "the transmission error in it is @xmath13 .",
    "it is argued in [ @xcite ] that it reflects common characteristics of the general channel with errors though it is the simplest model .",
    "+     +    the shannon mutual information is given by :    @xmath14    it can be seen as @xmath15 . while the chernoff mutual information is given by : @xmath16 unfortunately , there is no explicit solution for eq.([bsc chernoff information ] ) . as a result",
    ", it is arduous to analyze the chernoff mutual information . +",
    "it is observed that the shannon mutual information seems to be completely different from the chernoff mutual information .",
    "the msc is shown in fig.[fig : channelmbsc ] .",
    "the number of input symbol is @xmath17 .",
    "the probability of the input symbol is @xmath18 with @xmath19 ( @xmath20).the transmission error of each one is @xmath21 .",
    "the bsc is the special case of msc for @xmath22 .",
    "+    similarly , the shannon mutual information is given by : @xmath23 the chernoff mutual information is given by : @xmath24      much of research in regression analysis has examined the optimal transformation between one or more predictors and the response . unlike traditional multiple regression algorithm which requires the priori information of the functional forms , the ace algorithm of breiman and friedman ( 1985 ) in [ @xcite ] does not require that and it is non - parametric transformation .",
    "that is to say , it is a fully automated algorithm to estimate the optimal transformation between predictors and response .",
    "furthermore , it can be also used to estimate maximal correlation among random variables .",
    "the implementation of the ace algorithm can consult [ @xcite][@xcite ] .",
    "assume random variables @xmath25 are predictors and @xmath26 is response . supposing @xmath27 are arbitrary zero mean functions of the corresponding variables , the residual error is @xmath28}^2 } } \\right\\ } } \\over { e\\left [ { { \\theta ^2}(y ) } \\right]}}\\ ] ]    the algorithm is summarized in alg.[alg : ace ] as in [ @xcite ] .",
    "set @xmath29 and @xmath30 ; + iterate until @xmath31fails to decrease ; + iterate until @xmath31fails to decrease ; + for @xmath32 to p do : + @xmath33;\\ ] ] replace @xmath34 with @xmath35 ; + end for loop ; + end inner iteration loop ; + @xmath36 } \\over { \\left\\| { e\\left [ { \\sum\\limits_{i = 1}^p { { \\phi _ i}({x_i})\\left| y \\right . } } \\right ] } \\right\\|}};\\ ] ] replace @xmath37 with @xmath38 ; + end outer iteration loop ; + @xmath39 are the solution @xmath40 ; +    in this paper , we use the ace algorithm help us separate multivariate function .",
    "let @xmath41 be the channel parameters and @xmath26 be measured value of channel information .",
    "the functional relation between them is : @xmath42 it is supposed that @xmath43 are known and independent .",
    "moreover , the functional relation @xmath44 is also known since it is defined by human to describe the channel information . therefore , it is easy to get @xmath26 and they form a data set @xmath45 .",
    "this data set meets the precondition of the ace algorithm , so ace algorithm can be used to analyze them . as a result",
    ", one can get @xmath46 where @xmath47 is residual error . in this case",
    ", it is easy to find out the separate influence of each correspondent channel parameter .",
    "in this section , various numerical simulation results will be presented to analyze the two channel information measures .",
    "we focus on conducting the monte carlo simulation by computer to compare the shannon and chernoff mutual information in different channels by ace algorithm .",
    "the procedure is given by the following simulation procedure .",
    "generate channel parameters , such as @xmath48 and @xmath49 ; calculate the channel information measures , such as shannon mutual information ; decompose these channel information measures by ace algorithm ; repeat above process till the required times .",
    "@xmath50 observations geneated from the eq.([bsc mutual information ] ) and eq.([bsc chernoff information ] ) where @xmath48 is the probability of the input symbol and @xmath49 is the transmission error . @xmath48 and @xmath49 are independently drawn from a uniform distribution @xmath51 . in this case , our channel information is a multivariate case with two input parameters .    the ace algorithm is applied to this simulated data set and the results is shown in fig.[fig : bsc c and i ] .",
    "the correlation between @xmath52 and @xmath53 is extremely close to @xmath54 .",
    "furthermore , the error of the ace decomposition @xmath47 is invariably near to zero .",
    "clearly , both of them have shown that the ace decomposition results are excellent .",
    "it is noted that function curve of @xmath55 and @xmath56 w.r.t .",
    "shannon mutual information is almost coincided with that w.r.t chernoff mutual information after the ace decomposition . as a result , in the range of the errors permitted , the @xmath55 and @xmath56 appears to be coincided with each other .",
    "@xmath55 is monotone decreasing function .",
    "the greater @xmath48 , the faster the decline in @xmath55 .",
    "furthermore , @xmath56 is symmetric around the line @xmath57 for the fact that the channel is symmetrical about @xmath49 .",
    "the results of shannon mutual information are smaller than the chernoff mutual information , but whole variant trend is identical .",
    "@xmath52 increases rapidly when @xmath58 is close to zero and the slope of its curve is becoming smaller as @xmath58 increases .",
    "+      @xmath59 observations generated from the eq.([msc mutual information ] ) and eq.([msc chernoff information ] ) where @xmath60 is the probability of the input symbol and @xmath49 is the transmission error .",
    "they are generated randomly and independently , which is bound in @xmath61 . in this case",
    ", our channel information is a multivariate case with four input parameters .",
    "fig.[fig : msc c and i ] shows similar characteristics that in fig.[fig : bsc c and i ] . the value of the ace decomposition error @xmath47 is still very small . on the other hand ,",
    "the correlation between @xmath52 and @xmath62 is close to @xmath54 .",
    "these two points verify the validity of the ace algorithm again .",
    "it is obviously that the function curves of @xmath52,@xmath63,@xmath64,@xmath65 and @xmath66 are almost the same for these two channel information measures .",
    "it is worth noting that the minimum of @xmath66 occurs when @xmath67 .",
    "the values of @xmath66 is big when @xmath49 is very close to @xmath68 or @xmath54 .",
    "moreover , @xmath63,@xmath64 and @xmath65 are very similar .",
    "their function curve is flat and close to zero when the independent variable ( the probability of the input symbol ) is less than @xmath69 . when the independent variable approaches to @xmath54 , the values of them rapid decrease .",
    "in fact , it is reasonable because these input variables are equivalent for the channel .    as illustrated in fig.[fig : msc c and i ] , the result of shannon mutual information is smaller than the chernoff mutual information , but whole varying trend is identical .",
    "@xmath52 increases rapidly when @xmath58 is also close to zero and the slope of its curve is becoming smaller as @xmath58 increases .     +",
    "when more narrowly examined , there are more interesting conclusions . in this section , we analyze these ace simulated results from the viewpoint of big data .",
    "there are several nature behind it .",
    "first of all , the data does not lie .",
    "naturally , the ace results can provide much true and useful information for us .",
    "furthermore , we always turn to numerical analysis when it is difficult to do theoretical analysis .",
    "the complicated equations of shannon or chernoff mutual information are arduous to do intuitive theory analysis , especially when they are multivariable , so it is quite appropriate to do numerical analysis by ace algorithm . in fact ,",
    "the probably approximately correct ( pac ) model is enough for us in most of time [ @xcite ] . in this model ,",
    "one only need to construct algorithms which guarantee that it is correct with high probability ( not necessarily one ) .",
    "this model is the foundation of support vector machine ( svm ) [ @xcite ] .",
    "the ace results is @xmath70 where @xmath26 is the channel information and @xmath71 is all the set of all the input channel parameters .",
    "the equation ( @xmath72 ) holds because the residual error @xmath47 can be ignored . from the figures above",
    ", it is noted that @xmath73 is almost the same for both shannon and chernoff mutual information .",
    "for these two measures , the only difference is @xmath74 .",
    "the function @xmath75 is different and @xmath76 is the same . from the fig.[fig : bsc c and i ] to fig.[fig : msc c and i ] , it is also noted that the function @xmath74 is monotonic function . as mentioned above , the curves of @xmath74 have the same trend and the values of @xmath37 for shannon mutual information is invariably smaller than those for chernoff information .",
    "therefore , the shannon s @xmath77 is bigger than chernoff s .",
    "actually , the facts extremely agree with the this corollary , which can well explain the phenomena in fig.[fig : bsc1 c and i ] .    in this sense",
    ", shannon shakes hands with chernoff . over the last decades ,",
    "shannon information and chernoff information are always considered to be two kinds of channel information measures and the relation between them is that kullback - leibler divergence is the special case of rnyi divergence when @xmath78 .",
    "the channel information measures deduced from them look very different even for the channel as simple as bsc .",
    "however , with the aid of the ace algorithm , we find the inner relations between them .",
    "in fact , they are different metrics of the same physical quantity . for a channel , in a certain time , if all of its parameters are given , all of its state and properties will be determined .",
    "thus the information about the same channel is always consistent .",
    "for example , the kilometer and centimeter are both measures of length , but they are used in different situations for convenience . generally speaking , we do not adopt the centimeter to measure the distance between two cities .",
    "in contrast , we would like adopt it to measure the length of the steel rule .",
    "their true values are identical when measuring the same things , even their numerical values are different .",
    "similarly , the shannon and chernoff information are two metrics of the same quantity of information .",
    "+      obviously , the ability to convey information of a channel is decided by input parameters .",
    "that is to say , the parameters represent the channel . as a result , we hold opinion that these channel parameters decide the nature of channel . it is noted that @xmath79 in eq.([res_fun ] ) is the same whatever we adopt the shannon channel information measures or chernoff channel information measures . in this paper , we take @xmath79 as the nature of channel information and we get @xmath80 where the @xmath81 is the nature of channel information .",
    "it is only decided by the channel parameters and it contains the core of the channel information .",
    "the other physical quantity about channel information is just the function of @xmath81 , just as @xmath82 where @xmath83 is a measurement of channel information .",
    "as far as shannon mutual information and chernoff mutual information are concerned , their only difference is the function @xmath75 .",
    "they can be seen as a compound function .",
    "the inner is @xmath81 which is the function w.r.t .",
    "channel parameters and the outer is the function w.r.t .",
    "@xmath81 . for different channel information measures ,",
    "the inner is always the same but the outer is different .",
    "there is a conjecture that if a novel information measure is put forward to measure the channel information , it will be decomposed to a function of @xmath75 by ace algorithm from a very large extent .",
    "this rule can be used to judge the rationality of the new information measure .",
    "this conjecture also provides a way for us to propose a new information measure for the fact that we can construct that based on designing the function @xmath75 .",
    "in fact , even if this guess is not right with probability one , the new one which conforms to the law is correct with large probability and we can easily find the relation between that and previous information measures , such as shannon and chernoff information .",
    "in this paper , the different channel information measures in several channels are revisited in big data viewpoint by ace algorithm .",
    "fortunately , the decomposition results of independent variable is the same with a slightly difference in the function of channel information values . that is to say , shannon shakes hands with chernoff with the fact that they are just two metrics of the same quantity of channel information . for every channel",
    ", there is a nature of channel information , which is only decided by the channel parameters no matter what information measures are adopted .",
    "in fact , the big data viewpoint such as ace algorithm provides a new viewpoint for us to reexamine the information theory and find out that shannon and chernoff shake hands with each other , which have been ignored for decades .",
    "furthermore , this method can help us to construct new information measures with keeping the nature of the channel information unchanged .",
    "obviously , we put forward a criterion to judge whether a new channel information measure be appropriate .",
    "this work was supported by the china major state basic research development program ( 973 program ) no.2012cb316100(2 ) and national natural science foundation of china ( nsfc ) no.61171064 ."
  ],
  "abstract_text": [
    "<S> shannon entropy is the most crucial foundation of information theory , which has been proven to be effective in many fields such as communications . </S>",
    "<S> rnyi entropy and chernoff information are other two popular measures of information with wide applications . </S>",
    "<S> the mutual information is effective to measure the channel information for the fact that it reflects the relation between output variables and input variables . in this paper , we reexamine these channel information measures in big data viewpoint by means of ace algorithm . </S>",
    "<S> the simulated results show us that decomposition results of shannon and chernoff mutual information with respect to channel parameters are almost the same . in this sense </S>",
    "<S> , shannon shakes hands with chernoff since they are different measures of the same information quantity . </S>",
    "<S> we also propose a conjecture that there is nature of channel information which is only decided by the channel parameters .    </S>",
    "<S> shannon information;chernoff information ; rnyi divergence ; ace ; big data </S>"
  ]
}