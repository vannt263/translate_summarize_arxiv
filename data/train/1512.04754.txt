{
  "article_text": [
    "the problem of estimating an unknown signal from noisy linear observations is fundamental in signal processing .",
    "the estimation task is often formulated as the linear inverse problem @xmath0 where the objective is to recover the unknown signal @xmath1 from the noisy measurements @xmath2 .",
    "the matrix @xmath3 models the response of the acquisition device and the vector @xmath4 represents the measurement noise , which is often assumed to be independent and identically distributed ( i.i.d . ) gaussian .    a standard approach for solving ill - posed linear inverse problems is the regularized least - squares estimator @xmath5 where @xmath6 is a regularizer that promotes solutions with desirable properties and @xmath7 is a parameter that controls the strength of regularization . in particular ,",
    "sparsity - promoting regularization , such as @xmath8-norm penalty @xmath9 , has proved to be successful in a wide range of applications where signals are naturally sparse .",
    "regularization with the @xmath8-norm is an essential component of compressive sensing theory  @xcite , which establishes conditions for accurate estimation of the signal from @xmath10 measurements .",
    "the minimization   with sparsity promoting penalty is a non - trivial optimization task .",
    "the challenging aspects are the non - smooth nature of the regularization term and the massive quantity of data that typically needs to be processed .",
    "proximal gradient methods  @xcite such as iterative shrinkage / thresholding algorithm ( ista )  @xcite or alternativng direction method of multipliers ( admm )  @xcite are standard approaches to circumvent the non - smoothness of the regularizer while simplifying the optimization problem into a sequence of computationally efficient operations .    for the problem",
    ", ista can be written as    [ eq : ista ] @xmath11    where @xmath12 is the identity matrix and @xmath13 is a step - size that can be set to @xmath14 with @xmath15 to ensure convergence  @xcite .",
    "iteration   combines the gradient descent step   with a proximal operator   that reduces to a pointwise nonlinearity    @xmath16    for convex and separable regularizers such as the @xmath8-norm penalty .",
    "is initialized with a signal @xmath17 which results in the estimate @xmath18 after @xmath19 iterations .",
    "the algorithm proposed here allows to efficiently refine @xmath20 by comparing @xmath18 against the true signal @xmath21 from a set of training examples . ]    in this letter , we consider the problem of learning an optimal nonlinearity @xmath22 for ista given a set of @xmath23 training examples @xmath24}$ ] . specifically , as illustrated in fig .",
    "[ fig : deepnn ] , we interpret iteration   as a simple deep neural network ( dnn )  @xcite with @xmath19 layers and develop an efficient algorithm that allows to determine optimal @xmath22 directly from data .",
    "simulations on sparse statistical signals show that data adaptive ista substantially improves over the @xmath8-regularized reconstruction by approaching the performance of the minimum mean squared error ( mmse ) estimator .",
    "starting from the early works  @xcite , iterative thresholding algorithms have received significant attention in the context of sparse signal estimation . accelerated variants of ista",
    "were proposed by , among others , bioucas - dias and figueiredo  @xcite , and beck and teboulle  @xcite .",
    "the method has also inspired approximate message passing ( amp ) algorithm by donoho _",
    "et al . _",
    "@xcite , as well as its bayesian extensions  @xcite .",
    "in particular , it was shown that , in the compressive sensing setting , one can obtain an optimal estimation quality by adapting the thresholding function of amp to the statistics of the signal  @xcite .",
    "the primary difference of the work here is that the optimal thresholding functions are learned directly from independent realizations of the data , rather than being explicitly designed to the assumed statistics .",
    "the other difference is that ista , unlike amp , contains no inherent assumptions on the randomness of the measurement matrix @xmath25  @xcite .",
    "more recently , several authors have considered relating iterative algorithms to dnns .",
    "for example , in the context of sparse coding , gregor and lecun  @xcite proposed to accelerate ista by learning the matrix @xmath25 from data .",
    "the idea was further refined by sprechmann _",
    "_  @xcite by considering an unsupervised learning approach and incorporating a structural sparsity model for the signal . in the context of the image deconvolution problem , schmidt and",
    "roth  @xcite proposed a scheme to jointly learn iteration dependent dictionaries and thresholds for admm .",
    "similarly , chen _ et al . _",
    "@xcite proposed to parametrize nonlinear diffusion models , which are related to the gradient descent method , and learned the parameters given a set of training images .",
    "this letter extends those works by specifically learning separable thresholding functions for ista . unlike the matrices @xmath25 ,",
    "thresholding functions relate directly to the underlying statistical distributions of i.i.d .",
    "signals @xmath21 .",
    "furthermore , by optimizing for the same nonlinearity across iterations , we obtain the mse optimal ista for a specific statistical distribution of data , which , in turn , allows us to evaluate the best possible reconstruction achievable by ista .",
    "by defining a matrix @xmath26 , vector @xmath27 , as well as nonlinearity @xmath28 , we can re - write ista as follows    [ eq : dista ] @xmath29    where @xmath30 $ ] .",
    "our objective is now to design an efficient algorithm for adapting the function @xmath20 , given a set of @xmath23 training examples @xmath31}$ ] , as well as by assuming a fixed number iterations @xmath19 . in order to devise a computational approach for tuning @xmath20 , we adopt the following parametric representation for the nonlinearities @xmath32 where @xmath33}$ ] , are the coefficients of the representation and @xmath34 is a basis function , to be discussed shortly , positioned on the grid @xmath35 \\subseteq \\delta \\z$ ]",
    ". we can reformulate the learning process in terms of coefficients @xmath36 as follows @xmath37 where @xmath38 is is used to incorporate some prior constraints on the coefficients and @xmath39 is a cost functional that guides the learning .",
    "the cost functional that interests us in this letter is the mse defined as @xmath40 where @xmath18 is the solution of ista at iteration @xmath19 , which depends on both the coefficients @xmath36 and the given data vector @xmath41 .",
    "given a large number of independent and identically distributed realizations of the signals @xmath42 , the empirical mse is expected to approach the true mse of ista for nonlinearities of type  .",
    "thus , by solving the minimization problem   with the cost  , we are seeking the mmse variant of ista for a given statistical distribution of the signal @xmath21 and measurements @xmath41 .      for notational simplicity ,",
    "we now consider the scenario of a single training example and thus drop the indices @xmath43 from the subsequent derivations .",
    "the generalization of the final formula to an arbitrary number of training samples @xmath23 is straightforward .",
    "we would like to minimize the following cost @xmath44 where we dropped the explicit dependence of @xmath18 on @xmath41 for notational convenience .",
    "the optimization of the coefficients is performed via the projected gradient iterations @xmath45 where @xmath46 denotes the iteration number of the training process , @xmath47 is the step - size , which is also called the learning rate , and @xmath48 is an orthogonal projection operator onto the convex set @xmath49 .    * input : * measurements @xmath41 , signal @xmath21 , current value of coefficients @xmath36 , and number of ista iterations @xmath19 . + * output : * the gradient @xmath50 . + * algorithm : *    1 .",
    "run @xmath19 iterations of ista in eq .   by storing intermediate variables",
    "@xmath51}$ ] and the final estimate @xmath18 .",
    "initialize : _ set @xmath52 , @xmath53 , and @xmath54 .",
    "compute : _",
    "+ @xmath55^\\t\\rbf^t\\\\ \\rbf^{t-1 } & \\leftarrow \\sbf^\\t\\diag(\\varphi^\\prime(\\zbf^{t}))\\rbf^t\\end{aligned}\\ ] ] 4 .",
    "if @xmath56 , return @xmath57 , otherwise , set @xmath58 and proceed to step 3 ) .",
    "we now devise an efficient error backpropagation algorithm for computing the derivatives of @xmath39 with respect to coefficients @xmath36 .",
    "first , note that we can write the iteration   with the nonlinearity   as follows @xmath59 for all @xmath30 $ ] .",
    "the gradient can be obtained by evaluating @xmath60^\\t(\\xbf^t(\\cbf)-\\xbf),\\ ] ] where we define    [ eq : derivativeista ] @xmath61 \\\\ & =   \\begin{bmatrix } \\frac{\\partial x_1^t}{\\partial c_{-k } } & \\dots & \\frac{\\partial x_1^t}{\\partial c_{k}}\\\\[0.3em ] \\vdots & \\ddots & \\vdots \\\\[0.3em ] \\frac{\\partial x_n^t}{\\partial c_{-k } } & \\dots & \\frac{\\partial x_n^t}{\\partial c_{k } } \\end{bmatrix}.\\end{aligned}\\ ] ]    by differentiating   and simplifying the resulting expression , we obtain @xmath62,\\ ] ] where we defined a matrix @xmath63 .",
    "then , for any vector @xmath64 , we obtain @xmath65 r_m & = \\sum_{m = 1}^n \\psi_{mk}^t r_m \\\\ & + \\sum_{n = 1}^n \\left[\\frac{\\partial x_n^{t-1}}{\\partial c_k}\\right ] \\sum_{m = 1}^n s_{mn } r_m \\varphi^\\prime(z_m^t ) , \\nonumber\\end{aligned}\\ ] ] which translates to the following vector equation @xmath66^\\t \\rbf = [ \\psibf^t]^\\t \\rbf + \\left[\\frac{\\partial \\xbf^{t-1}}{\\partial \\cbf}\\right]^\\t \\sbf^\\t \\diag(\\varphi^\\prime(\\zbf^t))\\rbf,\\ ] ] where the operator @xmath67 creates a matrix and places the vector @xmath68 into its main diagonal .",
    "note that since the initial estimate @xmath17 does not depend on @xmath36 , we have that @xmath69 by applying the equation   recursively starting from @xmath52 and by using  , we obtain @xmath70^\\t \\rbf^t   & = \\underbrace{[\\psibf^t]^\\t \\rbf^t}_{\\defn \\gbf^{t-1 } } + \\left[\\frac{\\partial \\xbf^{t-1}}{\\partial \\cbf}\\right]^\\t \\underbrace{\\sbf^\\t \\diag(\\varphi^\\prime(\\zbf^t))\\rbf^t}_{\\defn \\rbf^{t-1 } } \\\\ & = \\gbf^{t-1 } + \\left[\\frac{\\partial \\xbf^{t-1}}{\\partial \\cbf}\\right]^\\t \\rbf^{t-1 } \\\\ & = \\underbrace{\\gbf^{t-1 } + [ \\psibf^{t-1}]^\\t\\rbf^{t-1}}_{\\defn \\gbf^{t-2 } } \\\\ & \\quad+ \\left[\\frac{\\partial \\xbf^{t-2}}{\\partial \\cbf}\\right]^\\t \\underbrace{\\sbf^\\t \\diag(\\varphi^\\prime(\\zbf^{t-1}))\\rbf^{t-1}}_{\\defn \\rbf^{t-2 } } \\\\ & = \\gbf^{t-2 } + \\left[\\frac{\\partial \\xbf^{t-2}}{\\partial \\cbf}\\right]^\\t \\rbf^{t-2 } = \\dots \\\\ & = \\gbf^0 + \\left[\\frac{\\partial \\xbf^0}{\\partial \\cbf}\\right]^\\t \\rbf^0 = \\gbf^0.\\end{aligned}\\ ] ] this suggests the error backpropagation algorithm summarized in algorithm  [ algo : backprop ] that allows one to obtain  .",
    "* input : * set of @xmath23 training examples @xmath71}$ ] , learning rate @xmath47 , and constraint set @xmath38 . + * output : * nonlinearity @xmath20 specified by learned coefficients @xmath72 .",
    "+ * algorithm : *    1 .   _",
    "initialize : _ set @xmath73 and select @xmath74 .",
    "2 .   select a small subset @xmath75 and @xmath76 , uniformly at random , from the set of @xmath23 training examples .",
    "3 .   using the selected training examples , update @xmath20 as follows @xmath77 4 .",
    "return @xmath78 if a stopping criterion is met , otherwise set @xmath79 and proceed to step 2 ) .",
    "the remarkable feature of algorithm  [ algo : backprop ] is that it allows one to efficiently evaluate the gradient of ista with respect to the nonlinearity @xmath20 .",
    "its computational complexity is equivalent to running a single instance of ista , which is a first - order method , known to be scalable to very large scale inverse problems .",
    "finally , equipped with algorithm  [ algo : backprop ] , nonlinearity @xmath20 can easily be optimized by using an online learning approach  @xcite summarized in algorithm  [ algo : sgd ] .      in our implementation",
    ", we represent the nonlinearity @xmath20 in terms of its expansion with polynomial b - splines ( for more details , see an extensive review of b - spline interpolation by unser  @xcite ) .",
    "accordingly , our basis function corresponds to @xmath80 , where @xmath81 refers to a b - spline of degree @xmath82 . within the family of polynomial splines ,",
    "cubic b - splines @xmath83 tend to be the most popular in applications  perhaps due to their minimum curvature property  @xcite .",
    "b - splines are very easy to manipulate .",
    "for instance , their derivatives are computed through the following formula @xmath84 which simply reduces the degree by one . by applying this formula to the expansion of @xmath20",
    ", we can easily obtain a closed form expression for @xmath85 in terms of quadratic b - splines @xmath86",
    "when recovering @xmath87 benoulli - gaussian signal @xmath21 from measurements @xmath41 under i.i.d .",
    "@xmath25 . note the proximity of mmse - ista to the support - aware genie . ]    . on the left side , snr of training",
    "is plotted for each training iteration . on the right side ,",
    "the final learned shrinkage ( solid ) is compared to the standard soft - thresholding under optimal @xmath88 . ]    to verify our learning scheme , we report results of ista with learned mse optimal nonlinearities ( denoted _ mmse - ista _ ) on the compressive sensing recovery problem . in particular , we consider the estimation of a sparse bernoulli - gaussian signal @xmath21 with an i.i.d .",
    "prior @xmath89 , where @xmath90 $ ] is the sparsity ratio , @xmath91 is the gaussian probability distribution function of mean @xmath92 and variance @xmath93 , and @xmath94 is the dirac delta distribution . in our experiments , we fix the parameters to @xmath87 and @xmath95 , and we numerically compare the signal - to - noise ratio ( snr ) defined as @xmath96 , for the estimation of @xmath21 from linear measurements of form  , where @xmath97 has variance set to achieve snr of @xmath98 db , and where the measurement matrix @xmath25 is drawn with i.i.d . @xmath99 entries .",
    "we compare results of mmse - ista against three alternative methods . as the first reference method",
    ", we consider standard least absolute shrinkage and selection operator ( _ lasso _ )  @xcite estimator , which corresponds to solving   with @xmath8-norm regularizer .",
    "in addition to lasso , we consider mmse variant of the generalized amp ( _ gamp _ ) algorithm  @xcite , which is known to be nearly optimal for recovery of sparse signals from random measurements .",
    "finally , we consider a support - aware mmse estimator ( _ genie _ ) , which provides an upper bound on the reconstruction performance of any algorithm .",
    "the regularization parameter @xmath88 of lasso was optimized for the best snr performance .",
    "similarly , the parameters of gamp were set to their statistically optimal values .",
    "the implementation of lasso is based on fista  @xcite . both fista and gamp were run for a maximum of @xmath100 iterations or until convergence that was measured using the relative change in the solution in two successive iterations @xmath101 .",
    "the number of layers of mmse - ista was set to @xmath102 .",
    "learning was performed by using online learning in algorithm  [ algo : sgd ] that was run for @xmath100 iterations with the learning rate of @xmath103 .",
    "the nonlinearity @xmath20 was defined with @xmath104 basis functions that were spread uniformly over the dynamic range of the signal and was initialized to correspond to the soft - thresholding function with optimal @xmath88 .",
    "figure  [ fig : snrs ] reports the snr performance of all algorithms under test after averaging the results of @xmath100 monte carlo trials .",
    "the results show that the quality of estimated signal can be considerably boosted by using nonlinearities @xmath20 that are adapted to the data .",
    "in particular , the snr performance of mmse - ista is significantly better than that of lasso and is about 1 db away from the snr obtained by gamp at higher values of @xmath105 .",
    "figure  [ fig : learning ] illustrates the per - iteration evolution of snr evaluated on the training sample during the learning process ( left ) , as well as the final shape of the learned nonlinearity ( right ) . as can be appreciated from these plots , the learning procedure deviates the shape of nonlinearity @xmath20 from the soft - thresholding function , which leads to a significant increase in snr of the solution .",
    "the scheme developed in this letters is useful for optimizing the nonlinearities of ista given a set of independent realizations of data samples . by using this scheme , we were able to benchmark the best possible reconstruction achievable by ista for i.i.d .",
    "sparse signals .",
    "specifically , in the context of compressive sensing , we showed that by optimizing the nonlinearities the performance of ista improves by several dbs and approaches that of the optimal estimator .",
    "future investigations of ista under optimal nonlinearities may lead to an improved understanding of the relationship between statistical estimators and iterative reconstruction algorithms .",
    "e.  j. cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "inf . theory _ ,",
    "52 , no .  2 ,",
    "pp . 489509 , february 2006 .",
    "i.  daubechies , m.  defrise , and c.  d. mol , `` an iterative thresholding algorithm for linear inverse problems with a sparsity constraint , '' _ commun .",
    "pure appl .",
    "_ , vol .",
    "57 , no .  11 , pp . 14131457 , november 2004 .",
    "s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein , `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ foundations and trends in machine learning _",
    ", vol .  3 , no .  1 ,",
    "1122 , 2011 .          j.  m. bioucas - dias and m.  a.  t. figueiredo , `` a new twist : two - step iterative shrinkage / thresholding algorithms for image restoration , '' _ ieee trans .",
    "image process .",
    "_ , vol .  16 , no .  12 ,",
    "29923004 , december 2007 .",
    "s.  rangan , `` generalized approximate message passing for estimation with random linear mixing , '' in _ proc .",
    "information theory _",
    ", st . petersburg , russia , july 31-august 5 , 2011 , pp .",
    "21682172 .",
    "s.  rangan , a.  k. fletcher , p.  schniter , and u.  s. kamilov , `` inference for generalized linear models via alternating directions and bethe free energy minimization , '' in _ proc .",
    "information theory _ , hong kong , june 14 - 19 , 2015 , pp . 16401644 .",
    "y.  chen , w.  yu , and t.  pock , `` on learning optimized reaction diffuction processes for effective image restoration , '' in _ proc .",
    "ieee conf .",
    "computer vision and pattern recognition ( cvpr ) _ , boston , ma , usa , june 8 - 10 , 2015 , pp ."
  ],
  "abstract_text": [
    "<S> iterative shrinkage / thresholding algorithm ( ista ) is a well - studied method for finding sparse solutions to ill - posed inverse problems . in this letter </S>",
    "<S> , we present a data - driven scheme for learning optimal thresholding functions for ista . </S>",
    "<S> the proposed scheme is obtained by relating iterations of ista to layers of a simple deep neural network ( dnn ) and developing a corresponding error backpropagation algorithm that allows to fine - tune the thresholding functions . </S>",
    "<S> simulations on sparse statistical signals illustrate potential gains in estimation quality due to the proposed data adaptive ista . </S>"
  ]
}