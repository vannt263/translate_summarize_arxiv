{
  "article_text": [
    "many astronomical objects , such as the solar system , star clusters , galaxies , and clusters of galaxies and the large scale structure of the universe , are expressed as the system of many `` particles '' interacting through the newtonian gravity .",
    "the numerical simulation of such systems , usually called `` gravitational @xmath0-body simulations '' , proved itself to be one of the most useful tools in computational astrophysics .",
    "it has been used to study systems of all scales , from the formation of the moon @xcite to the entire visible universe @xcite .",
    "many of these simulations are computationally demanding , because of three reasons .",
    "the first reason is that the calculation cost per timestep , at least with naive implementation , scales as @xmath2 , where @xmath0 is the number of particles in the system .",
    "the second reason is that in many cases the evolution timescale of the system is many orders of magnitude longer than the orbital timescale of typical particles in the system .",
    "an example is the long term simulation of the solar system .",
    "we need to follow the orbits of planets and other objects for @xmath3 years , and we need around 100 timesteps per year or more to achieve reasonable accuracy . in the case of the simulation of the formation of planets , we need to follow the evolution of planetesimals for several million orbits .",
    "this requirement of large number of orbits limits the number of particles we can handle to around 1,000 or less @xcite , even on special - purpose computers such as grape-4 @xcite or grape-6 @xcite .",
    "the same is true for the simulation of star clusters . here",
    ", the evolution timescale depends linearly on the number of particles , and the number of timesteps per orbits increases as @xmath4 , since the distance which one particle can move in a single timestep must be a small fraction of the distance to its nearest neighbor . in total , the calculation cost increases as @xmath1 .",
    "the third reason is that different particles can have very different orbital timescales . since the gravitational force is an attracting force",
    ", two particles can become arbitrary close to each other .",
    "thus , the timestep need to be adaptive . moreover , it is not enough to just change the timestep .",
    "if we use the variable , but single , timestep to integrate all particles , when we increase @xmath0 the average step - size for star cluster simulation shrinks as @xmath5 or somewhat faster .",
    "thus , the calculation cost becomes @xmath6 instead of @xmath1 .",
    "we can solve the third problem by allowing individual particles to have their own time and timestep @xcite .",
    "however , with this scheme the number of particles we can integrate in parallel becomes much smaller than @xmath0 , and it becomes more difficult to use large scale parallel computers even when @xmath0 is not very small .",
    "dorband et al .",
    "@xcite reported the performance of parallel implementation of this individual timestep ( or block timestep ) algorithm , combined with simple @xmath2 direct force calculation , on a cray t3e .",
    "they have fine - tuned the communication scheme so that there will be maximal overlap between the communication and calculation .",
    "even so , if the initial distribution of particles is highly anisotropic , they found that performance did not increase for more than 64 processors , for @xmath7 .",
    "more recent works on pc clusters @xcite generally gave much worse scaling , because the communication performance of pc clusters , when normalized by the calculation speed , is orders of magnitude worse than that of cray t3e .",
    "the use of special - purpose grape processors does improve the performance of single - node calculation quite significantly and made it possible to perform long simulations with relatively small number of particles such as 64k or less .",
    "however , in this case the overall performance was limited by the speed of the single host computer , since the parallelization over multiple grape nodes involves much bigger relative overhead compared to that of pc clusters , and it turned out to be difficult to achieve any gain unless the number of particles is quite large @xcite .    in this paper ,",
    "we report the result of the first calculation which used nearly 1,000 processors for the simulation of star clusters with just 64k stars and achieved more than 50% of the theoretical peak speed of the machine .",
    "average speed measured on a 400 cpu cray xd1 ( 2.2ghz dual - core opteron ) was 2.03 tflops , while the nominal theoretical peak speed is 3.52 tflops .",
    "previous works , even when the parallel efficiency was good , achieved less than 15% of the theoretical peak speed of the machine .",
    "thus , our implementation is remarkable in two ways : it is highly scalable for small number of particles , and it can achieve quite high absolute performance .    in the rest of this paper , we describe how we achieved the good scalability and high absolute peak performance at the same time . in section 2",
    "we outline the individual ( block ) timestep algorithm . in section 3",
    "we describe the two - dimensional parallelization scheme we used .",
    "it was originally proposed by makino@xcite .",
    "we describe the modifications we made to achieve high performance . in section 4",
    "we briefly describe the highly optimized force calculation loop .",
    "the details are given in nitadori et al.@xcite . in section 5",
    "we describe the result of a long calculation of a two - component star cluster with 64k particles .",
    "section 6 sums up .",
    "the basic idea of the individual timestep algorithm is to assign each particle its own time and timestep .",
    "thus , particle @xmath8 has its current time @xmath9 and timestep @xmath10 .",
    "the calculation proceeds in the following way :    1 .",
    "select a particle with minimum @xmath11 2 .   integrate that particle to its new time , @xmath12 and determine its new timestep .",
    "3 .   go back to step 1 .    in order to integrate particle @xmath8 ,",
    "it is necessary to calculate the gravitational forces from all other particles at @xmath13 .",
    "for all particles other than particle @xmath8 , it is guaranteed that @xmath14 . to calculate the force on particle @xmath8 at time @xmath15 ,",
    "we perform `` prediction '' of positions of other particles at time @xmath15 . in order to be able to do this prediction ,",
    "we use linear multistep method with variable stepsize as the integration scheme .",
    "a practical problem with this individual timestep algorithm is that only one particle is integrated at each time .",
    "thus , there is little room for parallelization .",
    "mcmillan @xcite introduced the so - called blockstep algorithm , in which timesteps are quantized to integer powers of two , in such a way that particles with the same timestep also has the same time and can be integrated in parallel .",
    "this means that the timestep should be chosen so that the current time is an exact integer multiple of the timestep .",
    "this simple criterion gives us the maximum possible parallelism @xcite .    until early 1990s ,",
    "the 4th - order variable - stepsize adams - bashforth - moulton linear multistep scheme adopted to second - order differential equation had been used as the integration scheme .",
    "currently , 4th - order hermite scheme @xcite is used .",
    "it has several advantages over the original abm scheme .",
    "existing parallel implementation of the block timestep algorithm @xcite are all based on what we call one - dimensional parallelization , which divide the calculation of gravitational interaction along one loop index .",
    "the basic force calculation loop , for the block timestep algorithm , would look like :    .... for(ii=0 ; ii < nblock ; ii++ ) {      i = list_of_particles_in_current_block[ii ] ;      clear the force on particle i ;      for(j=0 ; j < n ; j++ ) {           if(j!=i ) {                calculate and accumulate the force from j to i ;           }       } } ....    here , list_of_particles_in_current_block is the list of particles to be integrated at the current block timestep .",
    "existing parallel implementations are one - dimensional , in the sense that the parallelization is done either to the inner loop or the outer loop , but not both",
    ".    one might think the one - dimensional parallelization is sufficient , if the number of particles , @xmath0 , is significantly larger than the number of processors , @xmath16 .",
    "however , that is not the case for the blockstep algorithm , since the number of particles in the current block , @xmath17 , is much smaller than @xmath0 . in a 64k - particle run which will be described in section 5 , on average @xmath17 is around 400 .",
    "thus , the parallelization over the outer loop ( what is sometimes called @xmath8-parallelization ) is inadequate for more than a few hundred processors .",
    "parallelization of the inner loop ( @xmath18-parallelization ) has its own problem .",
    "it requires summation over all processors , which can be very slow on distributed memory machine with relatively slow network .    moreover , both @xmath8- and @xmath18-parallelization on a distributed - memory machine requires @xmath19 communication per blockstep per node , independent of the number of nodes .",
    "the calculation per node is @xmath20 .",
    "thus , maximum number of nodes with which we can achieve reasonable parallel efficiency is @xmath21",
    ". if the communication is relatively slow , the coefficient in front of @xmath0 can be very small , and that is the reason why the scalability was rather bad in previous works .",
    "we can improve the scalability by applying @xmath8- and @xmath18-parallelization at the same time . in the following ,",
    "we summarize the description by makino @xcite .",
    "the `` regular - bases '' hyper - systolic algorithm@xcite is essentially the same .",
    "one can argue that the basic idea of these schemes is to use up to @xmath2 processors for @xmath0-body problem .",
    "hillis and barnes@xcite described such algorithm .",
    "consider the case in which @xmath16 processors are organized as an @xmath22 two - dimensional array ( @xmath23 ) .",
    "@xmath0 particles are divided into @xmath24 subsets , and processor @xmath25 has the @xmath8-th and @xmath18-th subsets .",
    "the calculation proceeds as follows :    1 .",
    "each of processor @xmath25 calculates the forces from subset @xmath18 to subset @xmath8 .",
    "2 .   take summation over the processors in the same row .",
    "the result is stored in processors at diagonal location , @xmath26 .",
    "they now have the total forces on their subset @xmath8 .",
    "3 .   processors @xmath26 broadcast the summed forces to the column direction .",
    "all processors update the particles in their subset @xmath8 , and diagonal processors broadcast the updated particles to the row direction , so that each processor receives updated @xmath18-th subsets .    in practice , with the individual timestep algorithm",
    "we calculate the forces only on the particles in the current block ( which we call _ active particles _ ) , and all communications in the above procedure applies only to these active particles .",
    "thus , in this algorithm , each processor send / receive @xmath27 data , instead of @xmath19 data as in the case of one - dimensional scheme .",
    "the basic two - dimensional parallelization , applied to the block timestep , has two problems .",
    "one is that the load - balancing can be very bad if the number of active particles is small . at each timestep ,",
    "processor @xmath25 calculates the force from @xmath18-th subset to active particles in @xmath8-th subset .",
    "thus , fluctuations in the number of active particles directly affect the load balance .",
    "this effect can be serious if number of nodes in one dimension @xmath24 is large , since the node with maximum number of particles in the block would determine the calculation time .",
    "another is that we can use only a square array of processors .    here , we present an improved parallelization scheme , which solves both of the above two problems .",
    "the basic idea is still the same : to apply both the @xmath8- and @xmath18-parallelization .",
    "however , the communication pattern and the data decomposition are completely different from those in the basic scheme described in the previous section .",
    "consider the case in which @xmath16 processors are organized as an @xmath28 two - dimensional array .",
    "communications occurs either row - wise or column - wise . in the actual implementation with mpi , for these two patterns",
    ", we construct mpi communicators , to keep the program simple and to take advantage of the mpi library functions for aggregate communications .",
    "@xmath0 particles are divided into @xmath29 subsets , and all processors @xmath30 have the @xmath18-th subsets .",
    "the calculation proceeds as follows    1 .",
    "each processor construct the list of active particles from its subset . here",
    ", the lists of processors @xmath30 are identical for all values of @xmath31 .",
    "each processor horizontally broadcasts the length of its active list and the minimum timestep in its subset .",
    "we use mpi_allgather for this purpose .",
    "if other processor has a smaller timestep , the active list is `` inactivated '' ( none of particles in its subsets are integrated ) .",
    "3 .   each processor divides its active list to @xmath32 subsets , and processor @xmath25 selects @xmath8-th subset as its active list .",
    "each processor horizontally broadcasts its active list , so that all processors in the one row will share their lists of active particles .",
    "we use mpi_allgatherv for this purpose .",
    "processor @xmath25 calculates the force from its subset @xmath18 to the particles in the combined list of active particles .",
    "we take the summation of partial forces over the processors of each row , and store the results to the processors of the original location .",
    "we use mpi_reduce_scatter for this purpose . at this stage",
    ", each processor has the total force on its active particles .",
    "each processor broadcast the forces on its active particles to all other processors in the same column .",
    "we use mpi_allgatherv for this purpose .",
    "now all processors @xmath30 has the forces on the active particles of subset @xmath18 .",
    "each processor integrates the orbits of its active particles . here ,",
    "all processors @xmath30 do redundant operations .    .",
    "circles denote positions , half - toned and filled square denote partial and full forces , respectively .",
    "( a ) 7 , 3 and 9 active particles are found in columns 1 , 2 and 3 .",
    "( b ) each processor in one column decides which particles to handle .",
    "( c ) horizontal broadcast of the positions .",
    "( d ) force calculation ( e ) summation of the partial forces . completed forces",
    "are send back to the original locations .",
    "( f ) vertical broadcast of the forces . ]",
    "figure [ fig:2d - alg ] illustrates how our new 2d algorithm works . in this example , even if the number of active particles varies widely , the amount of work on each processor is quite well balanced .",
    "in gravitational @xmath0-body simulations , if parallel efficiency is reasonable , by definition almost all computing time is spent to calculate the gravitational force from one particle to another . with the hermite integration scheme ,",
    "what we need to calculate is the acceleration , its time derivative , and the potential , given by    @xmath33 \\\\",
    "\\phi_i & = & -\\sum_j { m_j \\over ( r_{ij}^2 + \\varepsilon^2)^{1/2}}\\end{aligned}\\ ] ]    where @xmath34 and @xmath35 are the gravitational acceleration and the potential of particle @xmath8 , the jerk @xmath36 is the time derivative of the acceleration used for the hermite integration @xcite , and @xmath37 , @xmath38 , and @xmath39 are the position , velocity and mass of particle @xmath8 , @xmath40 and @xmath41 .",
    "the calculation of @xmath42 and @xmath43 requires nine multiplications , 10 addition / subtraction operations , one division and one square root calculation .",
    "warren et al .",
    "@xcite used 38 as the total number of floating point operations for this pairwise force calculation , and that number has been used by number of researchers .",
    "so we follow that convention .",
    "in addition , it requires 11 multiplications and 11 addition / subtraction operations to calculate @xmath44 .",
    "thus , the total number of floating - point operations per inner force loop for the hermite scheme is 60@xcite .",
    "the full details of the optimization we applied is described in @xcite .",
    "the basic idea is to take full advantage of new functionalities of sse and sse2 instructions , without sacrificing the accuracy .",
    "the sse2 instruction set offers the register - based instructions which is easier to optimize than the stack - based x87 instruction set .",
    "it also gives simd operations for two double - precision words .",
    "the sse instruction set gives simd operation on four single - precision words .",
    "in addition , it gives fast approximation for inverse square root , which is extremely useful for gravitational force calculation .",
    "our optimized force calculation function uses double - precision sse2 instructions for the first subtraction of positions and final accumulation of the calculated force , but use sse single - precision instructions for other operations . also , fast approximate inverse square root and one newton - raphson iteration",
    "is used to calculate the inverse square root .    on amd k8 ( athlon 64 or opteron ) processors ,",
    "our force loop calculates four gravitational interactions in 120 cpu cycles .",
    "in other words , it effectively performs 240 floating - point operations in 120 cycles . the peak performance we can achieve for the force calculation on a single core with 2.2 ghz clock speed is thus 4.4 gflops .",
    "this number happens to be exactly the same as the theoretical peak performance of k8 processors for double - precision operations .",
    "we used an cray xd1 with 400 2.2ghz dual - core opteron processors .",
    "therefore , the peak speed we can achieve for the force calculation on single chip is 8.8 gflops and that for the entire machine is 3.52 tflops .",
    "in this section , we report the performance of a simulation of the thermal evolution of a star cluster using the parallel individual timestep code we developed .     +    -body unit time . ]",
    "the problem we study here is the evolution of a two - component star cluster .",
    "there have been some works on this kind of system using orbit - averaged fokker - plank models@xcite , but very few works using @xmath0-body simulation .",
    "we constructed an initial model in the dynamical equilibrium .",
    "it is a plummer model in the heggie unit @xcite .",
    "we assign 10% of the total mass to `` heavy '' particles , which are 5 times more massive than `` light '' particles .",
    "thus , initial model contains 64111 light particles and 1425 massive particles .",
    "we used a softened gravitational potential with softening parameter @xmath45 .",
    "[ fig : evolve ] illustrates the evolution of the mass segregation .",
    "the heavy particles lose energy and sink toward the center of the system . fig .",
    "[ fig : core ] shows the time evolution of the size and mass density of the core calculated using the method described in @xcite .",
    "we can see that the core shrinks and the evolution becomes faster as the core becomes smaller .",
    "we are currently making detailed comparison between @xmath0-body result and fp calculation result .",
    "the total calculation time was 26.4 hours and the total number of timesteps was @xmath46 , the total number of floating - point operations was @xmath47 and the average performance was 2.03 tflops .",
    "table [ tab : prof ] gives the breakdown of the calculation time for the period of @xmath48 ( simulation time ) .",
    "average time per blockstep is around 700 @xmath49s .",
    "as we described in the previous section , four complex mpi operations are performed in single blockstep .",
    "the fact that the communication took fairly small time is due to the very good performance of mpi library and communication hardware of cray xd1 .",
    "even with this high - performance mpi library of cray xd1 , without our 2d algorithm , the communication time would increase by at least a factor of 20 , and performance would drop to less than 500 gflops .    .",
    "breakdown of the calculation time . [ cols= \" < , < \" , ]      fig .",
    "[ fig : scale ] shows the performance of our parallel @xmath0-body code on a cray xd1 with up to 800 ( 400 dual - core ) processors , for @xmath0=16k and 64k .",
    "multiple symbols for the same values of @xmath0 and @xmath16 are results for different processor geometries . in the case of @xmath0=64k ,",
    "parallel efficiency is better than 80% for up to 512 processors .",
    "even for 16k particles , efficiency is better than 70% for up to 256 processors , and we achieved the performance of almost one tflops for 16k particles with 512 processors .    to put our performance result into perspective , we summarize some of the previous works .",
    "dorband et al.@xcite obtained around 6 gflops for @xmath50 on a 128-processor cray t3e-600 .",
    "this is around 8% of the theoretical peak of the machine .",
    "the best number so far reported on grape-6@xcite for @xmath50 is around 130 gflops , which is 13% of the theoretical peak .",
    "we achieved 60% of the theoretical peak of a 256-processor machine for the same number of particles .",
    "= 16k ( triangle ) and @xmath0=64k ( circle ) .",
    "the dotted line indicates the ideal linear speedup . ]",
    "we developed a new highly scalable parallel algorithm for direct @xmath0-body simulation with individual ( block ) timestep integration method .",
    "we implemented this new algorithm on a cray xd1 and achieved quite good performance scaling for up to 800 processors ( the largest existing configuration of cray xd1 ) , for small number of particles such as 16k or 64k .",
    "parallel efficiency better than 80% is obtained for 512 processors , and sustained speed of 2.03 tflops is achieved for a long calculation on 800 processors .",
    "for the first time , we demonstrated that a modern mpp with @xmath51 processors can be used for astrophysical @xmath0-body simulations with less than @xmath52 particles .",
    "this capability to achieve high efficiency for small @xmath0 is quite important , since the total calculation cost scales as @xmath1 and @xmath53 is the practical limit of the number of particles we can handle with the effective speed of several tflops .",
    "we thank atsushi kawai and toshiyuki fukushige of k&f computing research for their stimulating discussions .",
    "we are grateful to hideki matsumoto of sony and staffs of cray japan for valuable technical supports on the use of the xd1 system .",
    "thanks shigeo mitsunari at u10 networks for discussions on speeding - up of the calculations with assembly - language programming ."
  ],
  "abstract_text": [
    "<S> in this paper , we describe the performance of an @xmath0-body simulation of star cluster with 64k stars on a cray xd1 system with 400 dual - core opteron processors . </S>",
    "<S> a number of astrophysical @xmath0-body simulations were reported in scxy conferences . </S>",
    "<S> all previous entries for gordon - bell prizes used at least 700k particles . </S>",
    "<S> the reason for this preference of large numbers of particles is the parallel efficiency . </S>",
    "<S> it is very difficult to achieve high performance on large parallel machines , if the number of particles is small . </S>",
    "<S> however , for many scientifically important problems the calculation cost scales as @xmath1 , and it is very important to use large machines for relatively small number of particles . </S>",
    "<S> we achieved 2.03 tflops , or 57.7% of the theoretical peak performance , using a direct @xmath2 calculation with the individual timestep algorithm , on 64k particles . </S>",
    "<S> the best efficiency previously reported on similar calculation with 64k or smaller number of particles is 12% ( 9 gflops ) on cray t3e-600 with 128 processors . our implementation is based on highly scalable two - dimensional parallelization scheme , and low - latency communication network of cray xd1 turned out to be essential to achieve this level of performance . </S>"
  ]
}