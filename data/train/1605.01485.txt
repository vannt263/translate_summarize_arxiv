{
  "article_text": [
    "data with a matrix - valued response for each experimental unit are commonly encountered in contemporary statistical applications .",
    "for example , a longitudinal multivariate response can be treated integrally as a matrix - valued variable by designating rows and columns to be time and variates .",
    "temporal and spatial data , multivariate growth curve data , image data and data from cross - over designs also generate matrix - valued responses .",
    "for example , in a twin cross - over bioassay of insulin by the rabbit blood sugar method ( v@xmath3lund , 1980 ) , each rabbit received two different treatments on two days .",
    "blood sugar was measured at hourly intervals for six hours each day . in this case , the response for each rabbit is a @xmath4 matrix , with rows and columns indicating treatments and time respectively . the eeg data ( li et al .",
    "2010 ) is another example that contains temporal and spatial matrix - formed variates from 77 alcoholic subjects and 45 non - alcoholic subjects .",
    "the electrical records of each subject form a matrix of dimensions @xmath5 that can be treated as a matrix - valued response variable when we investigate the association between the brain signals and alcoholism .    in these examples ,",
    "the components of the matrix - variates are dependent among rows and columns .",
    "this dependence distinguishes longitudinal data in a matrix - valued response from traditional longitudinal modeling of vector - valued responses in which independent units are each measured over time .",
    "vectorizing a matrix - valued response , or modeling the row or column vectors separately , typically loses dependency information and fails to capture the data structure . tackling matrix - variates",
    "directly can circumvent this issue .",
    "research into this topic has gained considerable interest in recent years .",
    "( 2010 ) proposed a class of sufficient dimension reduction ( sdr ) methods , called dimension folding sdr , for data with matrix - valued predictors .",
    "pfeiffer et al . ( 2012 ) extended sliced inverse regression ( sir ) to longitudinal predictors .",
    "ding and cook ( 2014 ) developed model - based dimension folding methods for matrix - valued predictors .",
    "xue and yin ( 2014 ) introduced dimension folding sdr for conditional mean functions . on",
    "another track , hung and wang ( 2013 ) , zhou et al .",
    "( 2013 ) , and zhou and li ( 2014 ) extended generalized linear models ( glm ) to matrix- and tensor - valued predictors for analyzing image data .",
    "all these methods , however , address data with matrix or tensor - valued predictors .",
    "methods for dealing directly with matrix - valued responses are relatively limited in the literature .",
    "viroli ( 2012 ) proposed special matrix - variate regressions assuming independent rows for error terms or covariates . yet in many applications such assumptions are violated as dependency can exist among both rows and columns of a matrix - valued response .",
    "we recently became aware that li and zhang ( 2015 ) studied tensor response regression where a linear relationship between a tensor - valued response and a predictor vector is considered .",
    "their method is more suited for tensor responses and vector - valued predictors .",
    "however , our motivation , rationale , model formulation , and estimation algorithms are different from those of li and zhang ( 2015 ) .",
    "firstly , our work was motivated by contemporary matrix - variate data and was geared to develop demanding yet limited matrix - variate analysis tools for such data .",
    "thus , it is tailored for matrix - valued responses and can handle matrix - valued predictors simultaneously . secondly",
    ", our method can fully utilize intrinsic matrix - variate data structure and allow a matrix linear model to reduce number of parameters in matrix - variate regressions .",
    "moreover , our method of estimation is different than that used by li and zhang ( 2015 ) .",
    "as discussed in section  [ sec:7.2 ] this can have important consequences in applications .    in this article",
    ", we propose matrix - variate regressions in a general framework , where the response @xmath0 on each experimental unit is a random matrix and the predictor @xmath1 can be either a scalar , a vector , or a matrix , treated as non - stochastic in terms of the conditional distribution @xmath2 .",
    "neither the rows nor the columns of the matrix - valued variables are required to be independent .",
    "thus , the proposed methods can capture intrinsic dependent data structures .",
    "moreover , they can reduce the number of parameters and improve efficiency in estimation compared to conventional methods . as matrix - variate responses may have sizable dimensions ,",
    "extraneous error variation may often occur beyond that anticipated by the model . to allow for such situations",
    ", we further propose envelope methods for efficient estimation in matrix - variate regressions . by applying the idea of enveloping",
    ", one can achieve dimension reduction in the analysis , by extracting only the useful information and eliminating redundant information from estimation .",
    "this can lead to substantial efficiency gains in estimation .",
    "the remainder of this article is organized as follows . in section [ sec:2 ]",
    "we propose a new class of matrix - variate regressions and connect them with conventional regression models .",
    "section [ sec:3.1 ] reviews the idea of enveloping and illustrates it with a real example .",
    "section [ sec:3 ] is devoted to the development of envelope methods for matrix - variate regressions .",
    "section [ sec:5 ] studies theoretical properties of matrix regression models and envelopes . sections [ sec:7 ] and [ sec:8 ] are devoted to illustrations with simulations and real data analyses . technical details and proofs are included in a supplement .",
    "generally , a two - way measurement layout can be treated integrally as a matrix - valued variable , denoted as @xmath6 . in application , modeling the relationship between @xmath0 and certain covariates is often of interest , where the covariates can be matrix - valued @xmath7 , vector - valued @xmath8 , or univariate , depending on the specific data . in the insulin assay data , for example",
    ", the covariates are formed as a @xmath9 matrix with elements indicating different treatment and dose levels .",
    "the goal is to investigate how the treatment and dose levels influence blood sugar concentration , a matrix - valued response .    as an obvious first step",
    ", we might use the @xmath10 operator that stacks the columns of a matrix to transform the problem into a standard vector - variate regression : @xmath11 where @xmath12 , @xmath13 , and the error vector @xmath14 has mean 0 and covariance matrix @xmath15 . in view of the potential for many parameters ,",
    "this model may be hard to estimate well and difficult to interpret , unless perhaps some type of sparse structure is imposed . for problems in which a sparsity assumption is untenable",
    ", it seems to be a rather blunt instrument that neglects the matrix structure and may be grossly over - parameterized .",
    "we propose instead a relatively parsimoniously parameterized model that reflects the underlying structure of the matrix - variate response and predictor .    in preparation",
    ", the following operators for matrix - valued variables are predefined . for @xmath6 ,",
    "the expectation of @xmath0 is @xmath16 , the covariance matrix of @xmath0 is assumed to be of the form @xmath17 = { \\mathrm{cov}}_r({\\textbf{y}})\\otimes { \\mathrm{cov}}_c({\\textbf{y}})$ ] , where ` @xmath18 ' stands for the kronecker product , and @xmath19/\\mathrm{tr}[{\\mathrm{cov}}_c({\\textbf{y}})]$ ] and @xmath20/\\mathrm{tr}[{\\mathrm{cov}}_r({\\textbf{y}})]$ ] are defined as the row and column covariance matrices of @xmath0 ( de waal 1985 ) . here ` tr ' means the trace operation .",
    "the two covariance matrices @xmath21 and @xmath22 are uniquely defined up to a proportionality constant . for @xmath23",
    ", we correspondingly define the covariances between @xmath24 and @xmath25 in terms of the row covariance and column covariance , designated @xmath26 $ ] and @xmath27 $ ] .    given a matrix - variate predictor @xmath28",
    ", we define the matrix regression of @xmath6 on @xmath1 as the bilinear model @xmath29 where @xmath30 is the overall mean , and @xmath31 and @xmath32 are the row and column coefficient matrices . here",
    "@xmath33 and @xmath34 are uniquely defined only up to a proportionality constant . for identifiability",
    ", the column coefficient matrix @xmath34 is defined to have frobenius norm 1 and positive element in its first row and column .",
    "the distribution of the matrix - valued random error @xmath35 is assumed be independent of @xmath1 , and have zero mean and covariance matrix @xmath36 = { \\mathrm{\\boldsymbol\\sigma}}_2\\otimes { \\mathrm{\\boldsymbol\\sigma}}_{1}$ ] , where @xmath37 and @xmath38 are the column and row covariance matrices of @xmath35 .",
    "similarly , we require @xmath39 to have unit frobenius norm and positive first diagonal element in order to uniquely identify the two covariance matrices @xmath40 and @xmath39 .",
    "if no constraints are required on @xmath41 and @xmath39 , the kronecker products @xmath42 and @xmath43 are still identifiable but the individual row and column parameter matrices are not .",
    "the kronecker covariance structure supposes a relational characteristic of the matrix - variate @xmath0 , as the covariances of the column vectors of @xmath35 are all proportional to @xmath40 and the covariances of the row vectors of @xmath35 are all proportional to @xmath44 .",
    "such relationships are usually desirable for matrix - valued variables , especially for multivariate repeated measures and multivariate longitudinal data because of the intrinsic relationships among elements . for instance , the eeg data contains measurements of each subject from different time ( row ) and different scalp locations ( column ) .",
    "it seems reasonable to model the data with similar variations among measurements over rows and measurements over columns .",
    "another advantage of formulating the kronecker covariance structure for such data is that the number of parameters in @xmath36 $ ] can be dramatically reduced when the matrix dimension is high .",
    "when the kronecker structure does not hold , a general covariance matrix @xmath36={\\mathrm{\\boldsymbol\\sigma}}$ ] can be applied .",
    "hypothesis tests for the kronecker covariance structure can be found in shitan and brockwell ( 1995 ) , lu and zimmerman ( 2005 ) , and roy and khattree ( 2005 ) .",
    "let @xmath45 denote the @xmath46 indicator vector with a @xmath47 in the @xmath48-th element and zeros elsewhere , and imagine for the moment that @xmath34 is known .",
    "then the multivariate regression implied by ( [ eq : reg1 ] ) for the @xmath49-th column of @xmath0 , @xmath50 , can be seen as standard multivariate regression with response vector @xmath51 , coefficient matrix @xmath33 and predictor vector @xmath52 .",
    "the matrix model ( [ eq : reg1 ] ) can be interpreted in the same way , except the predictors for each column of @xmath0 are estimated as linear combinations of the columns of @xmath1 determined by the rows of @xmath34 .",
    "the matrix regression ( [ eq : reg1 ] ) is a new model formulation that as far as we know has not been discussed in the literature .",
    "it incorporates simple regression , multiple regression and multivariate multiple regression as special cases under different settings of @xmath1 and @xmath0 .",
    "for instance , when the response is univariate and the predictor is vector - valued , ( [ eq : reg1 ] ) reduces to a multiple regression .",
    "when both the response and the predictor are vector - valued , ( [ eq : reg1 ] ) coincides with the usual multivariate regression model .    by capturing the row and column relationships , model ( [ eq : reg1 ] ) reduces the number of parameters by @xmath53 in comparison to ( [ eq : reg0 ] ) , where @xmath54 is the parameter reduction from the coefficients and the rest is the parameter reduction from the covariance matrices",
    ". the total number of the reduced parameters could be very large when the matrix dimensions of @xmath1 and @xmath0 are relatively high .",
    "model ( [ eq : reg1 ] ) can be seen as a new regression tool for matrix - variate analyses in chemometrics .",
    "a variant on partial least square ( pls ) regression is the primary method used to study matrix - variate problems in chemometrics ( smilde et al . , 2004 ) , which regresses both response and predictor on a common latent term with a bilinear coefficient structure .",
    "the relationship between the matrix - variate regression ( [ eq : reg1 ] ) and matrix pls is similar to the relationship between regression and pls in the conventional univariate and multivariate settings .",
    "* concomitant columns*. when the column dimension of @xmath1 is the same as the corresponding dimension of @xmath0 , and in particular when the columns of @xmath1 and the columns of @xmath0 represent repeated measures or similar characteristics , it might be appropriate to build ( [ eq : reg1 ] ) with @xmath55 , an @xmath56 identity matrix , since each column of @xmath0 is usually associated with the corresponding column of @xmath1 . in this case , the matrix regression model can be simplified to : @xmath57 so the regression of each column of @xmath0 on the corresponding column of @xmath1 has the same coefficient matrix @xmath33 . in application , one can apply a likelihood ratio test or other model selection methods to select between ( [ eq : reg1 ] ) and ( [ eq : reg2 ] ) .",
    "similar comments apply to the row dimensions of @xmath1 and @xmath0",
    ".    * univariate predictor .",
    "* when the predictor @xmath58 is univariate , a matrix regression might allow a different linear regression for each element of @xmath0 on @xmath59 : @xmath60 where @xmath61 . in this case , ( [ eq : reg1 ] ) still has a multiplicative coefficient structure but now the column dimensions of @xmath62 and @xmath41 need not be restricted to match the dimension of @xmath59 : @xmath63 where @xmath64 and @xmath65 , for some @xmath66 .",
    "it is easy to see that ( [ eq : reg3 ] ) is a reduced rank ( izenman , 1975 ; reinsel and velu , 1998 ) form of ( [ eq : reg4 ] ) .",
    "again , model selection tools can be applied to select the proper model between ( [ eq : reg4 ] ) and ( [ eq : reg3 ] )",
    ".    * multivariate pod model . * the matrix - variate regression ( [ eq : reg1 ] ) can be seen as an extension of the partial one - dimensional regression model ( pod ) studied by cook and weisberg ( 2004 ) .",
    "the pod was proposed to study data with one response variable , one grouping variable or factor , and several covariates .",
    "a typical pod model represents the response @xmath67 at factor level @xmath68 as @xmath69 , where @xmath70 and @xmath71 represent the @xmath68-th level factor effect , and @xmath72 represents the one - dimensional covariate effect ( @xmath73 ) .",
    "if each subject contains measurements over all @xmath74 categories , then a multivariate response @xmath75 will be observed for each subject . in this case , the pod model can be extended as @xmath76 where @xmath77 and @xmath78 are @xmath74 dimensional vectors .",
    "the mean function of this model has the exactly the same coefficient structure as shown in ( [ eq : reg1 ] ) except the response and the predictor are vector - valued .",
    "when @xmath79 is matrix - valued , for example , @xmath80 response variables measured at each factor level , ( [ eq : pod1 ] ) can be extended to the exact matrix - variate regression setting .      in this section ,",
    "we focus on the estimation of ( [ eq : reg1 ] ) .",
    "the estimation of the special formulations can be similarly derived .    without a specific parametric distribution on the random error",
    ", one can estimate the coefficient parameters in ( [ eq : reg1 ] ) by using a loss function , like a squared loss function , and estimate the covariance matrices using moments .",
    "however , for statistical inference , as in conventional linear regressions , we assume that the random error @xmath35 follows a matrix normal distribution @xmath81 .",
    "background on the matrix normal distribution can be found in dawid ( 1981 ) and de waal ( 1985 ) .",
    "we next describe maximum likelihood estimation of the parameters in ( [ eq : reg1 ] ) assuming the matrix normal .",
    "assume that @xmath82 , @xmath83 , are sampled from the conditional distribution @xmath84 , and that they are independent .",
    "without loss of generality , suppose @xmath85 .",
    "then the mle of @xmath86 is @xmath87 . in matrix - variate analysis , explicit mles of @xmath62 ,",
    "@xmath41 , @xmath40 and @xmath39 are generally unobtainable .",
    "we propose a two - step iterative algorithm to construct the estimators . we first hold @xmath41 and @xmath39 fixed and estimate the remaining parameters .",
    "we then use an iterative algorithm to obtain the full set of the mles .",
    "let @xmath88 and @xmath89 . according to the log - likelihood function ( in the supplement ) , given @xmath41 and @xmath39 fixed , the estimators @xmath62 and @xmath40 are @xmath90 similarly , let @xmath91 and @xmath92 .",
    "given @xmath62 and @xmath40 fixed , @xmath41 and @xmath39 can be estimated by @xmath93    let @xmath94 , @xmath95 , @xmath96 and @xmath97 be the mles of @xmath62 , @xmath41 , @xmath40 and @xmath39 respectively . therefore , by initializing @xmath98 with the required frobenius normalization and initializing @xmath99 , one can obtain @xmath94 , @xmath95 , @xmath96 and @xmath97 by iterating between ( [ eq : mrmle1 ] ) and ( [ eq : mrmle2 ] ) with updated values until the log - likelihood function of ( [ eq : reg1 ] ) meets a convergence criterion .",
    "then normalize @xmath95 as @xmath100 and rescale @xmath94 as @xmath101 , where @xmath102 with @xmath103 being the first element of @xmath95 and ` @xmath104 ' representing the frobenius norm ; and normalize @xmath97 as @xmath105 and rescale @xmath96 as @xmath106 , where @xmath107 and @xmath108 is the first diagonal element of @xmath97 .",
    "if unique identification is not required for the individual parameter matrices @xmath109s and @xmath110s , the last step of normalization and rescaling in the algorithm is not necessary . in this case , we obtain mles for @xmath42 and @xmath111 but not the individual @xmath109s and @xmath110s .",
    "in addition , the aforementioned estimation procedure provides the same results as estimating the likelihood directly under the normalization constraints on @xmath41 and @xmath39 , because the corresponding mles with and without constraints are proportional .",
    "the justification is similar to the matrix normal estimation shown in dutilleul ( 1999 ) and srivastava et al .",
    "( 2008 ) .",
    "the existence of the estimators @xmath94 , @xmath95 , @xmath96 and @xmath97 depends on the existence of the inverse matrices @xmath112 and @xmath113 , which requires at least @xmath114 ( dutilleul , 1999 ) , and on the existence of @xmath115 and @xmath116 , which requires @xmath117 and @xmath118 respectively , when @xmath94 and @xmath95 are full rank matrices . in general , these conditions are fairly mild and require only the row dimensions of the response and the predictor less than their column dimensions multiplied by sample size and vice versa . in section [ sec:5 ] , we further investigate the asymptotic properties of the mles in ( [ eq : reg1 ] ) and show their asymptotic efficiency relative to estimators from conventional vector regression .      to guage",
    "how well the matrix regression ( [ eq : reg1 ] ) fits the observed data , one can test the goodness of fit of the model compared to an alternative .",
    "for instance , to compare ( [ eq : reg1 ] ) with the vector - variate regression of @xmath119 on @xmath120 given in ( [ eq : reg0 ] ) assuming that @xmath121 , one can test the hypothesis : @xmath122 versus @xmath123 is not true .",
    "since ( [ eq : reg1 ] ) is nested within ( [ eq : reg0 ] ) , the likelihood ratio test can be applied .",
    "let @xmath124 and @xmath125 be the maximized likelihoods ( [ eq : reg1 ] ) and ( [ eq : reg0 ] ) . then the test statistic @xmath126 follows a chi - squared distribution with degree of freedom @xmath127 , which is equal to the number of reduced parameters from ( [ eq : reg0 ] ) to ( [ eq : reg1 ] ) .",
    "goodness of fit can also be evaluated by other model selection methods , such as aic , bic or cross validation .",
    "the goodness of fit of ( [ eq : reg1 ] ) compared to other models , such as to the special formulations in section [ sec:2.2 ] , can be similarly derived .",
    "since the envelope methodology introduced by cook et al .",
    "( 2010 ) is still relatively new , we provide a brief review in this section , before turning to envelopes for matrix - variate regressions in section  [ sec:3 ] . to facilitate the discussion ,",
    "we introduce the following notations that will be used in the rest of the article . for an @xmath128 matrix @xmath129",
    ", @xmath130 is the subspace of @xmath131 spanned by the columns of @xmath129 , @xmath132 is the projection onto @xmath130 , and @xmath133 is the orthogonal projection , where @xmath134 is the moore - penrose inverse .",
    "let @xmath135 .",
    "the projections @xmath136 and @xmath137 can be equivalently denoted as @xmath138 and @xmath139 .",
    "a subspace @xmath140 is said to be a reducing subspace of @xmath141 if @xmath142 decomposes @xmath143 as @xmath144 .",
    "if @xmath142 is a reducing subspace of @xmath143 , we say that @xmath142 reduces @xmath143 .",
    "envelope methodology was proposed originally to improve efficiency in the vector - variate linear model @xmath145 where the response vector @xmath146 , the predictor vector @xmath147 , @xmath148 , @xmath149 , and the random error @xmath150 is independent of @xmath1 .",
    "the motivation for enveloping in this context arose from asking if there are linear combinations of @xmath0 whose distribution is invariant to changes in the non - stochastic predictor @xmath1 .",
    "we refer to such linear combinations as @xmath1-invariants .",
    "to gain an operational version of this notion , suppose that the subspace @xmath151 satisfies the two conditions ( i ) the marginal distribution of @xmath152 does not depend on @xmath1 , and ( ii ) given the predictor @xmath1 , @xmath153 and @xmath152 are independent .",
    "then a change in @xmath1 can affect the distribution of @xmath0 only via @xmath153 ( cook et al .",
    "informally , we think of @xmath153 as the part of @xmath0 that is material to the regression , while @xmath152 is @xmath1-invariant and thus immaterial .",
    "let @xmath154 .",
    "cook et al .",
    "( 2010 ) showed that the statistical conditions ( i ) and ( ii ) are equivalent to the algebraic conditions : ( a ) @xmath155 , and ( b ) @xmath156 .",
    "therefore , @xmath142 is a reducing subspace of @xmath157 that contains @xmath158 .",
    "the intersection of all reducing subspaces of @xmath157 that contain @xmath158 is called the @xmath157-envelope of @xmath158 , denoted as @xmath159 , or @xmath160 when used as a subscript .",
    "the envelope @xmath159 serves to distinguish @xmath161 and the maximal @xmath1-invariant @xmath162 in the estimation of @xmath163 and can result in substantial increases in efficiency , sometimes equivalent to taking thousands of additional observations .",
    "reparameterizing ( [ eq : m1 ] ) in terms of a semi - ortogonal basis matrix @xmath164 for @xmath159 , we have its envelope version @xmath165 with @xmath166 .",
    "the standard errors of elements of the maximum likelihood estimator of @xmath167 are often substantially smaller than the corresponding standard errors based on ( [ eq : m1 ] ) .    the overarching premise of the envelope @xmath159  that there may be linear combinations of @xmath0 whose distribution is invariant to changes in @xmath1  can be seen as inducing a type of generalized sparsity in both @xmath163 and @xmath157 . since @xmath168 , @xmath169 and thus linear combinations of @xmath163 are 0 rather than individual elements .",
    "envelopes go a step further by simultaneously requiring a generalized form of sparsity in @xmath166 .",
    "we employ data on concho water snakes ( johnson and wichern , 2007 ) to illustrate the idea of enveloping intuitively .",
    "the data contain measurements on the gender , age , tail length ( mm ) , and snout to vent length ( mm ) for 37 female concho water snakes and 29 male concho water snakes .",
    "we want to investigate the gender effect on the tail length and snout to vent length for young snakes ( age @xmath170 years ) .",
    "the data can be modeled using ( [ eq : m1 ] ) , where @xmath171 is a bivariate response vector with @xmath172 and @xmath173 representing the tail length and snout to vent length , respectively , and @xmath58 is an indicator variable with zero indicating a male snake and with one indicating a female snake .",
    "thus , the intercept @xmath174 is the mean of the female population , and the coefficient vector @xmath175 is mean difference between the female population and the male population , which implies the gender effect .",
    "figure 1(a ) shows the data , where the circles represent the male snakes and the cross points represent the female snakes .    under model ( [ eq : m1 ] ) , inference on the gender difference in tail lengths proceeds by first projecting the data onto the horizontal axis of figure 1(a ) and then using univariate inference methods with the projected data .",
    "the two dashed curves represent the marginal distributions of the male tail length and the female tail length .",
    "these curves overlap substantially , so it may take a large sample size to detect the gender effect .",
    "the estimated envelope and its orthogonal complement are represented by the solid and the dashed lines in figure 1(a ) .",
    "we can see that the two samples differ notably in the direction of the envelope , while differing little if at all in the direction of its orthogonal complement .",
    "accordingly , envelope estimation proceeds by first projecting the data onto the envelope to remove @xmath162 , the @xmath59-invariant part of @xmath0 , and then projecting onto the horizontal axis to focus on the gender difference in tail length .",
    "this process is depicted in figure 1(b ) , which shows the approximated marginal distributions of the male s and the female s tail lengths after enveloping , represented by the solid curves .",
    "the variances of two marginal distributions after enveloping are considerably reduced compared to the original ones ( the dashed curves ) , which reflects the efficiency gain .",
    "the efficiency gains can be massive when the immaterial variation @xmath176 is substantially larger than the material variation @xmath177 .",
    "although the matrix - variate regression ( [ eq : reg1 ] ) is parsimoniously parameterized relative to the naive model ( [ eq : reg0 ] ) , there still may be linear combinations of the rows or columns of @xmath178 whose distribution is invariant to changes in the predictors . to allow for the possibility of @xmath1-invariant linear combinations , we extend the vector - variate model described in section  [ sec:3.1 ] to matrix - variate regression .",
    "the rationale for doing so is generally the same as that described for vector - variate regressions : we hope to achieve efficiency in estimation and prediction better than that for model ( [ eq : reg1 ] ) .",
    "we first establish the envelope structure for the matrix - variate regression model ( [ eq : reg1 ] ) .",
    "envelope models for the special cases are given in the supplement .",
    "to allow for the possibility that there are @xmath1-invariants in both the rows and columns of @xmath0 , we suppose that there exist subspaces @xmath179 and @xmath180 so that @xmath181 where @xmath182 and @xmath183 were defined in section  [ sec:2.1 ] .",
    "these conditions are similar to conditions ( i ) and ( ii ) introduced in section [ sec:3.1 ] for defining @xmath1-invariants of @xmath0 in vector - variate regressions .",
    "let @xmath184 and @xmath185",
    ". conditions ( [ eq : mel]a ) and ( [ eq : mer]a ) mean that the marginal distributions of @xmath186 and @xmath187 do not depend on @xmath1 , which is equivalent to requiring that @xmath188 and @xmath189",
    ". conditions ( [ eq : mel]b ) and ( [ eq : mer]b ) indicate that @xmath186 does not respond to changes in @xmath1 through a linear association with @xmath190 and that @xmath191 does not respond to changes in @xmath1 through a linear association with @xmath192 .",
    "further , condition ( [ eq : mel]b ) holds if and only if @xmath193 , which is equivalent to requiring that @xmath194 reduce @xmath195 , so @xmath196 .",
    "similarly , @xmath197 must reduce @xmath198 , so @xmath199 .",
    "the intersection of all reducing subspaces @xmath194 of @xmath40 that contain @xmath200 is the @xmath40-envelope of @xmath200 , denoted as @xmath201 or @xmath202 when used as a subscript .",
    "similarly , the intersection of all reducing subspaces @xmath197 of @xmath39 that contain @xmath203 is the @xmath39-envelope of @xmath203 , denoted as @xmath204 or @xmath205 when used as a subscript .",
    "these subspaces @xmath201 and @xmath204 always exist , are uniquely defined and serve as the fundamental constructs that allow row and column reduction of @xmath0 . in concert , they imply that @xmath0 responds to changes in @xmath1 only via @xmath206 and they hold the promise of much better estimation of @xmath62 and @xmath41 .",
    "a theoretical justification regarding the variance reduction of enveloping is given by proposition [ prop4 ] in section [ sec:5 ] . because the column spaces @xmath207 and @xmath208 , and the orthogonal decompositions on @xmath40 and @xmath39 , are all invariant under a multiplicative constant ,",
    "the column and row envelopes @xmath209 and @xmath210 are always uniquely defined , whether normalization is required or not .",
    "we next use @xmath201 and @xmath204 to reparameterize ( [ eq : reg1 ] ) and to establish the envelope model .",
    "let @xmath211 and @xmath212 be semi - orthogonal bases of @xmath201 and @xmath204 , respectively , where @xmath213 and @xmath214 are the dimensions of the corresponding row and column envelopes and are temporarily assumed to be known .",
    "the determination of @xmath213 and @xmath214 will be discussed in section [ sec:6 ] . by definition , we know that @xmath215 and @xmath216 , so there exist two coordinate matrices @xmath217 and @xmath218 such that @xmath219 and @xmath220 .",
    "let @xmath221 and @xmath222 be orthogonal matrices",
    ". then the matrix regression model ( [ eq : reg1 ] ) can be re - parameterized as the following envelope model : @xmath223 where @xmath224 , @xmath225 , and @xmath226 and @xmath227 are unknown , @xmath228 .",
    "as @xmath229 and @xmath230 are overparameterized , the matrices @xmath231 and @xmath232 themselves are not identifiable but their column spaces @xmath233 and @xmath234 are identifiable .",
    "the two parameter spaces are grassmannians of dimension @xmath213 and @xmath214 in @xmath131 and @xmath235 with the numbers of unknown real parameters @xmath236 and @xmath237 respectively .",
    "therefore , the total number of real parameters in ( [ eq : envreg1 ] ) is @xmath238 , which is equal to the sum of the numbers of parameters @xmath239 in @xmath86 , @xmath240 in @xmath241 , @xmath242 in @xmath243 , @xmath236 in @xmath231 , @xmath237 in @xmath232 , @xmath244 in @xmath245 , @xmath246 in @xmath247 , and @xmath248 in @xmath249 and @xmath250 , whereas ( [ eq : reg1 ] ) has @xmath251 parameters . here",
    "the normalization constraints on @xmath41 and @xmath39 eliminate two free parameters in each model .",
    "we see that the envelope model further reduce @xmath252 parameters from ( [ eq : reg1 ] ) .",
    "efficiency gain of envelope model ( [ eq : envreg1 ] ) over matrix model ( [ eq : reg1 ] ) can arise in two distinct ways .",
    "the first is through parameter reduction , particularly when the number of real parameters in ( [ eq : reg1 ] ) is large relative to that in ( [ eq : envreg1 ] ) .",
    "but the largest efficiency gains are often realized when the variation in the @xmath1-invariant part of @xmath0 is large relative to the material variation @xmath0 .",
    "letting @xmath253 denote the spectral norm of a matrix , this happens when @xmath254 or @xmath255 . for vector - variate regression",
    ", this is reflected in figure [ ch3_fig1 ] , since the variation along @xmath256 is notably larger than that along @xmath159 .      in this section ,",
    "we describe the mles for the unknown parameters in ( [ eq : envreg1 ] ) . as @xmath85 ,",
    "the mle of @xmath86 is still @xmath87 but the remaining parameter estimates can not be expressed in closed form .",
    "consequently , we propose a two - step iteration algorithm to obtain the mles .",
    "let @xmath257 , @xmath258 , @xmath259 , @xmath260 , @xmath261 and @xmath262 be the envelope mles of the corresponding parameters .",
    "since closed - form expressions for these estimators are unavailable , we first hold @xmath34 and @xmath198 fixed and give the estimators of the remaining parameters .",
    "we later describe an iterative algorithm for computing the full set of mles . assuming then that @xmath34 and @xmath198 are given , let @xmath263 , and let @xmath264 and @xmath265 be defined in ( [ eq : mrmle1 ] ) .",
    "then @xmath231 can be estimated as @xmath266 where the objective function @xmath267 and @xmath268 is taken over all semi - orthogonal matrices @xmath269 .",
    "the estimators of @xmath33 and @xmath195 are then @xmath270 and @xmath271 , where @xmath272 is orthogognal .    similarly , given @xmath62 and @xmath40 , let @xmath273 , and let @xmath274 and @xmath275 be defined in ( [ eq : mrmle2 ] ) .",
    "then @xmath276 where @xmath277 and @xmath278 is taken over all semi - orthogonal matrices @xmath279",
    ". then the intermediate estimators of @xmath34 and @xmath198 are @xmath280 and @xmath281 , where @xmath282 is orthogognal .",
    "the full mles can now be obtained by alternating between @xmath283 and @xmath284 , and then rescaling them :    1 .",
    "initialize @xmath285 and @xmath286 as the mles from ( [ eq : reg1 ] ) .",
    "let @xmath287 and let @xmath288 .",
    "2 .   given @xmath260 and @xmath262 , estimate @xmath231 by ( [ eq : envlme1 ] ) , @xmath289 and @xmath290 .",
    "3 .   given @xmath259 and @xmath261 , estimate @xmath232 by ( [ eq : envlme2 ] ) , @xmath291 and @xmath292 .",
    "4 .   iterate 2 - 3 until the log - likelihood function of ( [ eq : envreg1 ] ) converges .",
    "then normalize @xmath260 as @xmath293 and rescale @xmath259 as @xmath294 , where @xmath295 and @xmath296 is the first element of @xmath260 , and normalize @xmath262 as @xmath297 and rescale @xmath261 as @xmath298 , where @xmath299 and @xmath300 is the first diagonal element of @xmath262 .",
    "similar to the discussion in section [ sec:2.3 ] , the proposed estimation procedure provides the same results as estimating the parameters directly under constrained @xmath41 and @xmath39 .",
    "if one is interested only in estimating the entire kronecker products @xmath42 and @xmath111 , but not individual @xmath109s and @xmath110s , the last rescaling step in the algorithm is unnecessary .",
    "moreover , the optimization of @xmath301 and @xmath302 are attained by a non - grassman algorithm ( cook et al .",
    "2015 ) that is more efficient and faster than existing grassmannian optimization approaches .",
    "in particular , cook et al .",
    "( 2015 ) demonstrated its superiority over the 1d algorithm used by li and zhang ( 2015 ) .",
    "the resulting envelope estimators @xmath303 and @xmath304 in steps 2 and 3 can be viewed as the projections of their corresponding estimators , @xmath264 and @xmath305 , from ( [ eq : reg1 ] ) onto the estimated row and column envelopes @xmath306 and @xmath307 .",
    "in addition , the envelope mles @xmath261 and @xmath262 are both partitioned into the estimated @xmath1-variant and @xmath1 invariant parts of @xmath0 .",
    "similar to the intuitive arguments in figure  [ ch3_fig1 ] , with the envelope projections , the formulations here can lead to more efficient estimation of the parameters .",
    "so far , we have assumed that the dimensions @xmath213 and @xmath214 of the row and column envelopes are known . extending the methods described by cook et al .",
    "( 2010 ) , these dimensions , which are essentially model selection parameters , can be determined by using an information criterion , say aic or bic , or using other model selection methods such as cross validation and likelihood ratio tests .",
    "for the information criteria , the envelope dimensions @xmath213 and @xmath214 can be selected by minimizing the objective function @xmath308 , where @xmath309 is the maximized log - likelihood function of an envelope matrix regression model , @xmath310 is equal to log@xmath311 for bic and is equal to 2 for aic , and @xmath312 is the total number of parameters in the target model",
    ".    when the matrix regression model ( [ eq : reg1 ] ) holds , these procedures for selecting @xmath313 and @xmath314 are in effect methods for determining the appropriateness of a proper envelope model . if it is inferred that @xmath315 and @xmath316 , then the data do not support a proper envelope model . on the other hand , the inference that @xmath315 and @xmath316 may itself be useful depending on the application , because it indicates the absence of @xmath1-invariant linear combinations of the row and columns of @xmath0 .",
    "in this section , we investigate the asymptotic properties of the mles from ( [ eq : reg1 ] ) and ( [ eq : envreg1 ] ) .",
    "we show that the mles from ( [ eq : reg1 ] ) are asymptotically more efficient than the mles obtained from ( [ eq : reg0 ] ) and that the envelope estimators from ( [ eq : envreg1 ] ) can gain efficiency over the mles from ( [ eq : reg1 ] ) .",
    "we neglect @xmath86 in the theoretical development , since its mle is @xmath87 in all the three models , which is asymptotically independent of the mles of all other parameters in the models .",
    "to show the relative efficiency of ( [ eq : reg1 ] ) to ( [ eq : reg0 ] ) , we need to consider only the joint asymptotic distribution of the mles of @xmath317 and @xmath318 , whereas the individual parameter matrices @xmath109s and @xmath110s are not of interest . in this case , the matrix regression ( [ eq : reg1 ] ) can be considered as an over - parameterized structural model of ( [ eq : reg0 ] )",
    ". hence we can apply proposition 4.1 in shapiro ( 1986 ) to derive the target property . for notation convenience ,",
    "let @xmath319 and @xmath320 , where ` vech ' denotes the half - vectorization operator . then under ( [ eq : reg1 ] )",
    ", we can rewrite @xmath164 as @xmath321 , where @xmath322 and @xmath323 . define @xmath324 to be the standard mle of @xmath164 from ( [ eq : reg0 ] ) and define @xmath325 to be the mle of @xmath164 from the matrix regression ( [ eq : reg1 ] ) . for a general statistic @xmath326 , denote the asymptotic variance of @xmath327 as @xmath328 .",
    "let @xmath329 be the fisher information function of @xmath164 and let @xmath330 be the gradient matrix .",
    "proposition [ prop1 ] establishes the asymptotic distribution of @xmath325 and its asymptotic efficiency relative to @xmath324 .    under ( [ eq : reg1 ] ) with a normal error",
    ", @xmath331 converges in distribution to a normal random vector with mean zero and covariance matrix @xmath332 .",
    "moreover , @xmath333 .",
    "[ prop1 ]    consequently , when the matrix regression structure holds , omitting row and column information in the coefficient and covariance formulation can result in loss of efficiency in parameter estimation .",
    "we next establish asymptotic properties for the mle from ( [ eq : envreg1 ] ) and show that enveloping can further improve efficiency in estimating ( [ eq : reg1 ] ) .",
    "let @xmath334 be the collection of the envelope parameters . under ( [ eq : envreg1 ] )",
    ", @xmath335 is reformulated as @xmath336 , where @xmath337 , @xmath338 , @xmath339 , and @xmath340 .",
    "thus , @xmath341 . correspondingly , the envelope mle of @xmath164 is @xmath342 .",
    "let @xmath343 be a gradient matrix .    under ( [ eq : envreg1 ] ) with a normal error",
    ", @xmath344 converges in distribution to a normal random vector with mean zero and covariance matrix @xmath345 .",
    "moreover , @xmath346 .",
    "[ prop2 ]    so far we have derived the asymptotic distributions for the estimators of @xmath164 under different model settings and compared the relative efficiency among different estimators .",
    "we next investigate the asymptotic properties of the individual row and column parameter estimators of @xmath335 in ( [ eq : reg1 ] ) and ( [ eq : envreg1 ] ) , given that these parameters are uniquely defined by normalization as discussed in the model settings .",
    "let @xmath347 denote the mle of @xmath335 from ( [ eq : reg1 ] ) , and let @xmath348 denote the envelope mle of @xmath335 from ( [ eq : envreg1 ] )",
    ".    under ( [ eq : reg1 ] ) with a normal error and uniquely defined row and column parameter matrices , @xmath349 converges in distribution to a normal random vector with mean zero and covariance matrix @xmath350 , where @xmath351 and @xmath352 are matrix - valued functions of @xmath335 given in the supplement .",
    "[ prop3 ]    under ( [ eq : envreg1 ] ) with a normal error and uniquely defined row and column parameter matrices , @xmath353 converges in distribution to a normal random vector with mean zero and covariance matrix @xmath354 .",
    "moreover , @xmath355 .",
    "[ prop4 ]    propositions [ prop3 ] and [ prop4 ] demonstrate the asymptotic normality for both enveloped and non - enveloped estimators of @xmath335 .",
    "proposition [ prop4 ] further indicates that as long as there exists @xmath1-invariants in ( [ eq : reg1 ] ) , one can gain potential efficiency in estimation by enveloping the matrix regression model .",
    "since the row and column coefficient matrices , @xmath356 and @xmath357 , are often of interest in regression analysis , we further derived asymptotic variances for @xmath358 and @xmath359 , which are given in the supplement .",
    "when the random error in ( [ eq : reg1 ] ) is not normal , the estimator of @xmath335 under a type of generalized least square estimation ( as shown in the supplement ) is the same as the mle of ( [ eq : reg1 ] ) under normality .",
    "we thus use the same notation @xmath347 . under mild conditions ,",
    "the estimator @xmath347 is still asymptotically normal , except having a more complex asymptotic covariance matrix .",
    "the terms @xmath360 and @xmath361 described in proposition [ prop3 ] will be used in the following propositions .    under ( [ eq : reg1 ] ) with uniquely defined row and column parameter",
    "matrices , suppose that the random error @xmath35 follows a general matrix - valued distribution with zero mean and column and row covariance matrices @xmath40 and @xmath39 . if @xmath352 is invertible at the true value @xmath335 and @xmath362 is @xmath363 at some @xmath364 between the estimator @xmath347 and the true @xmath335 , where @xmath365/\\partial { \\boldsymbol\\theta}_1^t\\|_{{\\mathrm{f}}}$ ] , then @xmath366 converges in distribution to @xmath367 , where @xmath368 is a matrix of second - order limiting estimating functions given in the supplement .",
    "[ prop5 ]    for the envelope model ( [ eq : envreg1 ] ) with a non - normal error @xmath35 , a good way to achieve parameter estimation is to use the same objective functions ( [ eq : envlme1 ] ) and ( [ eq : envlme2 ] ) to estimate the row and column envelopes , and then estimate the remaining parameters as described in section [ sec:3.3 ] .",
    "the next proposition shows that without normality on the error , the envelope estimators @xmath348 obtained from this procedure still retain desired asymptotic properties .    under ( [ eq : envreg1 ] ) and the conditions in proposition [ prop5 ]",
    ", @xmath353 converges in distribution to @xmath369 , where @xmath370 moreover , the envelope estimators are asymptotically more efficient than the estimators from ( [ eq : reg1 ] ) if @xmath371 is a reducing subspace of @xmath372 .",
    "in this section , we evaluate the performance of matrix - variate regression ( [ eq : reg1 ] ) and envelope matrix - variate regression ( [ eq : envreg1 ] ) numerically , and compare the two models along with the vector regression model ( [ eq : reg0 ] ) .",
    "we first generated data based on ( [ eq : envreg1 ] ) with @xmath373 , @xmath374 , @xmath375 , @xmath376 , @xmath377 , and @xmath378 . here @xmath379 and @xmath380 .",
    "the semi - orthogonal matrices @xmath231 and @xmath232 were generated by orthogonalizing matrices of independent uniform ( 0,1 ) random variables .",
    "the elements of @xmath86 , @xmath241 , @xmath243 and the predictors were selected as independent standard random normal variables .",
    "we then fitted three models : ( [ eq : reg0 ] ) , ( [ eq : reg1 ] ) , and ( [ eq : envreg1 ] ) to the data and evaluated their estimation accuracy of the coefficient parameters , according to the criteria : @xmath381 for the first two matrix regression models , and @xmath382 for the vector regression ( [ eq : reg0 ] ) .",
    "a corresponding evaluation of the covariance estimators showed results similar to the coefficient and is thus omitted .",
    "we used six different sample sizes , @xmath383 . within each sample size",
    ", 200 replicates were simulated .",
    "the average estimation errors were computed over the 200 random samples for each sample size , under each model .",
    "the envelope dimensions were set to the true dimensions @xmath374 . with this setup models ( [ eq : reg0 ] ) , ( [ eq : reg1 ] ) and ( [ eq : envreg1 ] ) have 975 , 105 and 75 real parameters to be estimated .",
    "figure [ fig:2 ] shows the comparison results from the three models .",
    "in particular , the left panel of figure [ fig:2 ] shows a comparison of the three models , and the right panel highlights the improvement of enveloping . over all selected sample sizes ,",
    "the two matrix regression models provided much smaller estimation errors than did the vector regression .",
    "in addition , by effectively removing @xmath1-invariants from estimation , the envelope model further improved estimation accuracy in comparison to the matrix regression model without enveloping .",
    "figure [ fig:3 ] shows the efficiency gains by assessing the asymptotic , actual , and bootstrap standard errors of the first elements in @xmath384 ( or @xmath385 ) from the three models .",
    "similar results were obtained for other elements .",
    "the asymptotic standard errors were estimated according to the results presented in propositions [ prop1 ] and [ prop2 ] .",
    "the actual standard errors were computed using the sample standard errors of @xmath384 ( or @xmath385 ) over 200 samples for each selected sample size .",
    "the bootstrap standard errors were obtained by bootstrapping one sample for each sample size .",
    "again , the left panel in figure [ fig:3 ] provides a comparison among the three models and the right panel highlights the comparison between the two matrix regression models with and without enveloping . clearly , the asymptotic , actual and bootstrap standard errors are very close , indicating that the asymptotic covariance matrices proposed in section [ sec:5 ] are accurate .",
    "in addition , over all selected sample sizes , the matrix regression without enveloping was asymptotically more efficient in parameter estimation than the vector regression , and imposing additional envelope structure further improved the efficiency gains .",
    "the importance of enveloping can be seen clearly in the right panel of figure [ fig:3 ] .",
    "indeed , the ratios of the asymptotic standard errors between the estimators from ( [ eq : reg0 ] ) and ( [ eq : reg1 ] ) range from 23.7 to 54.9 , while the ratios of the asymptotic standard errors between the estimators from ( [ eq : reg0 ] ) and ( [ eq : envreg1 ] ) range from 309.0 and 523.9 .",
    "therefore , effectively utilizing matrix - variate information in model fitting , and at the same time , removing @xmath1-invariants from estimation , one can gain substantial efficiency in parameter estimation .",
    "we next assess the performance of the two matrix regression models ( [ eq : reg1 ] ) and ( [ eq : envreg1 ] ) in terms of the individual row and column parameter estimations , particularly the estimation of @xmath62 and @xmath41 .",
    "the data were generated as in the previous example but to make the row and column parameter matrices identifiable , we normalized @xmath41 and @xmath39 such that each of them has frobenius norm 1 and positive element in its first row and column .",
    "we then evaluated the estimation accuracy of @xmath62 and @xmath41 according to the criterion @xmath386 , @xmath387 for both models .",
    "the results are shown in figure [ fig:4 ] wherein the envelope model notably outperformed the matrix regression ( [ eq : reg1 ] ) for both @xmath62 and @xmath41 at all sample sizes .",
    "figure [ fig:5 ] demonstrates the substantial efficiency gains of the envelope model by comparing the standard errors of the first elements in @xmath259 and @xmath260 obtained from ( [ eq : envreg1 ] ) and ( [ eq : reg1 ] ) .",
    "the comparison results similarly hold for other elements . again , asymptotic , actual and bootstrap standard errors are reported for each model .",
    "the asymptotic standard errors were estimated based on the results from propositions [ prop3 ] and [ prop4 ] .",
    "the actual and bootstrap standard errors were computed as in the previous example . from figure [ fig:5 ] , we see that the asymptotic standard errors of the estimators of @xmath62 and @xmath41 are very close to their corresponding actual and bootstrap standard errors for both models .",
    "this again supports the accuracy of the asymptotic covariance matrices given in section [ sec:5 ] .",
    "in addition , the envelope model shows asymptotic efficiency in comparison to ( [ eq : reg1 ] ) over all selected sample sizes .",
    "the ratios of the standard errors between the estimators from ( [ eq : reg1 ] ) and ( [ eq : envreg1 ] ) over all elements in @xmath62 range from 2.7 to 11.8 .",
    "the ratios of the standard errors between the corresponding estimators of @xmath41 range from 2.5 to 27.2 . enveloping improved both accuracy and efficiency in parameter estimation .",
    "we applied the proposed matrix - variate regression models to a cross - over assay of insulin based on rabbit blood sugar concentration ( v@xmath3lund , 1980 ) .",
    "the design partitioned the animals into four groups of nine rabbits each , with a different treatment assigned to each group .",
    "hence @xmath388 .",
    "let @xmath389 and @xmath390 denote the low and high dose levels , 0.75 units and 1.5 units of the standard treatment , and let @xmath391 and @xmath392 denote the same two dose levels of the test treatment .",
    "the treatment assignment is shown in table [ ch4_tb1 ] .",
    "after injection of the insulin dose each day , the blood sugar concentration of each rabbit was measured at 0 , 1 , 2 , 3 , 4 , and 5 hours .",
    "we consider the percentage decreases of the blood sugar concentrations at 1 , 2 , 3 , 4 , and 5 hours relative to the initial concentration at 0 hours .",
    "the measurements on each rabbit then form a matrix @xmath395 , where the two rows represent the percentage decreases under two different treatments assigned on day 1 and day 2 , and the columns indicate the hourly percentage decreases each day .",
    "we modeled the insulin assay data with treatment effects and dose levels as covariates to explore how these factors affect the percentage decreases of the blood sugar concentrations .",
    "we defined the covariates of each rabbit as a matrix @xmath396 , where the two rows of @xmath1 represent standard and the test treatments , and the two columns of @xmath1 represent day 1 and day 2 .",
    "for example , for the rabbits in group 1 , the covariate matrix @xmath397 , a diagonal matrix with elements 0.75 and 1.5 .",
    "that is because the rabbits in group 1 received the standard treatment at the low dose level 0.75 in day 1 , and received the test treatment at the high dose level 1.5 in day 2 .",
    "similarly , for the rabbits in groups 2 , 3 , and 4 , the covariate matrices can be formed as @xmath398 , @xmath399 and @xmath400 .",
    "we then fitted the relationship between the response @xmath401 and the predictor @xmath396 based on ( [ eq : reg1 ] ) , ( [ eq : reg2 ] ) and ( [ eq : reg0 ] ) as : @xmath402 @xmath403 @xmath404 where @xmath405 is the intercept matrix , @xmath406 , @xmath407 , @xmath408 , and @xmath409 are the corresponding coefficient matrices in the three models , and @xmath35 is the random error that is assumed to be matrix normal in the matrix - variate regression models ( [ eq : rabbitreg1 ] ) and ( [ eq : rabbitreg3 ] ) , and @xmath410 is assumed to be multivariate normal under the multivariate regression ( [ eq : rabbitreg2 ] ) .",
    "since the blood sugar concentration levels observed in the first day can not be affected by the treatment received in the second day , we set the elements in the upper - right corner of @xmath41 in ( [ eq : rabbitreg1 ] ) and of @xmath411 in ( [ eq : rabbitreg2 ] ) to be zero , that is , @xmath412 where @xmath413 are scalars , and @xmath414 .",
    "in this setting , the percentage decreases of the blood sugar concentrations in day 2 can depend on the treatments received in both day 1 and day 2 , whereas the percentage decreases in day 1 can only depend on the treatment received in day 1 .    since ( [ eq : rabbitreg2 ] ) , ( [ eq : rabbitreg1 ] ) and ( [ eq : rabbitreg3 ] ) are nested models , we applied likelihood ratio tests to select the best model among the three and concluded that ( [ eq : rabbitreg3 ] ) was sufficient .",
    "we then performed envelope fitting for ( [ eq : rabbitreg3 ] ) to achieve efficiency gains for the parameter estimation . using bic ,",
    "the envelope dimension was selected to be @xmath415 .",
    "the estimates ( est ) and the asymptotic standard errors ( se ) for each element in @xmath416 are summarized in table [ tab3 ] , with subscripts ` e ' and ` s ' indicating the results from the envelope fit and the fit from ( [ eq : rabbitreg3 ] ) without enveloping .",
    "c|ccc|ccc|c + & & & ratio + & @xmath417 & @xmath418 & @xmath419 & @xmath420 & @xmath421 & @xmath422 & @xmath423 + & 7.74&1.78&4.35&6.34&2.21&2.87&1.24 + & 13.07&2.48&5.28&14.42&2.76&5.22&1.12 + & 18.98&3.22&5.9&21.12&3.36&6.28&1.04 + & 17.41&3.11&5.6&16.67&3.35&4.98&1.08 + & 9.63&2.17&4.43&6.03&2.65&2.28&1.22 + & 7.33&1.73&4.23&6.33&2.21&2.86&1.28 + & 12.37&2.44&5.06&12.45&2.76&4.51&1.13 + & 17.96&3.21&5.6&20.1&3.36&5.98&1.05 + & 16.47&3.09&5.34&16.42&3.35&4.9&1.09 + & 9.11&2.12&4.3&5.68&2.65&2.15&1.25 +      in comparison to the fit from ( [ eq : rabbitreg3 ] ) , the envelope model shows smaller standard errors for all the elements in @xmath416 .",
    "all the parameters in @xmath424 are significant based on the two models .",
    "yet the magnitudes of the ratios @xmath425 from the envelope fitting are mostly higher than those from the fitting without enveloping .",
    "we further evaluated the sum of squared prediction error ( sspe ) by partition the data into training sets of seven subjects within each group and testing sets of two subjects within each group .",
    "the results based on the prediction error @xmath426 are summarized in figure [ ch4_fig7 ] , which shows that the envelope regression dominates for all choices of the envelope dimension @xmath213 .",
    "the smallest prediction error occurs at the optimal dimension @xmath415 . when @xmath427 , the envelope model coincides with the non - enveloped model and so the prediction errors of the two models are identical .",
    "in addition , we also modeled the data with group effects and compared estimates and standard errors with and without enveloping .",
    "the model and the results are given in the supplement . in this case , enveloping provides substantial efficient gains in estimation and can successfully identify the effects of different treatments , while the model without enveloping fails to detect such effects .",
    "the eeg data was briefly introduced in the introduction section .",
    "it contains two groups of subjects : 77 subjects from the alcoholic group and 45 subjects from the control group .",
    "each subject has measurements of electrical scalp activity , which form a @xmath5 matrix . to explore the influence of the alcoholism on the brain activity ,",
    "let @xmath428 if a subject is alcoholic and @xmath429 if a subject is nonalcoholic .",
    "we applied ( [ eq : reg4 ] ) to model the data as @xmath430 where @xmath431 is the matrix - valued brain measurements , @xmath432 is the mean brain activity of the nonalcoholic population ( control group ) , @xmath433 is the difference between the mean brain activity of the alcoholic subjects , denoted as @xmath434 , and the mean brain activity of the nonalcoholic subjects , @xmath435 .",
    "therefore , the coefficient matrix , @xmath436 , compares the alcoholic subjects and the nonalcoholic subjects based on their brain signals .",
    "the random error @xmath35 is assumed to follow @xmath437 . a saturated model such as ( [ eq : reg0 ] )",
    "is not estimable for the eeg data as the dimension of @xmath119 is much higher than the sample size , and the the covariance matrix of @xmath119 given @xmath59 is not likely to be sparse .",
    "matrix - variate regression provides one way to handle such data .    in ( [ eq : eeg ] )",
    ", it is easy to see that the mle of @xmath163 without enveloping is @xmath438 , the difference between the sample means of alcoholic brain activities and nonalcoholic brain activities . to achieve estimation efficiency",
    ", we considered fitting an envelope model on ( [ eq : eeg ] ) as @xmath439 where @xmath231 and @xmath232 are the bases of the @xmath40-envelope of @xmath440 and the @xmath39-envelope of @xmath441 respectively , @xmath442 is the the coordinate matrix , and the rest of the parameters are similarly defined as in ( [ eq : envreg1 ] ) .",
    "the bic chose the envelope dimensions to be @xmath443 and @xmath444 .",
    "similar to the results observed in the bioassay example , the envelope model can be more effective at identifying the differences between the alcoholic and nonalcoholic brain activities because it shows smaller standard errors for the estimators of @xmath163 . with @xmath443 and @xmath444",
    ", we calculated the relative ratios of the standard errors of the estimators from ( [ eq : eeg ] ) to those from ( [ eq : eegenv ] ) , and 90% of them fall in the range of 1.2 and 5.5 .",
    "moreover , since brain regions associated with alcoholism could of interest to researchers , we further investigated the effect of alcoholism on different brain locations ( channels ) by averaging the two estimators of @xmath163 over time ( 256 rows ) .",
    "we then obtained location - based estimators from ( [ eq : eeg ] ) and ( [ eq : eegenv ] ) , each of dimension 64 .",
    "bootstrap standard errors were computed for each element of the estimators and corresponding t - tests were applied . because this is a multiple hypothesis testing problem , we employed the approach proposed by benjamini and yekutieli ( 2001 ) to control the false discovery rate ( fdr ) .",
    "the fdr adjusted p - values ( in @xmath445 scale ) for the estimators with and without enveloping are reported in the left and right panels of figure [ fig:7 ] .",
    "the dashed lines in both panels represent the significance levels @xmath446 and @xmath447 .",
    "some plot characteristics are due to the adjustment process . for instance",
    ", about 25 p - values were adjusted to 1 , which corresponds to @xmath448 in figure  [ fig:7 ] . at the level @xmath446",
    ", the envelope model suggests that 35 out of 64 brain regions are associated with alcoholism , while the model without enveloping was unable to identify relevant regions with only a few borderline significance .",
    "li and zhang ( 2015 ) also analyzed these data , but computationally their method of estimation had to first reduce the number of time points from 256 to 64 by averaging adjacent groups of four .",
    "no such reduction was necessary with our estimation method .",
    "this is an important distinction . by averaging adjacent groups of four time points ,",
    "a tuning parameter was essentially introduced into their estimation method .",
    "different result would surely be obtained by varying the averaging group size .",
    "for instance , using bic their method estimated envelope dimensions of @xmath449 and @xmath450 , while using the full data we estimated @xmath443 and @xmath444 . by averaging adjacent groups , they could lose dimensions .",
    "zhao , q. , caiafa , c. f. , mandic , d. p. , chao , z. c. , nagasaka , y. , fujii , n. , zhang , l. , and cichocki , a. ( 2013 ) .",
    "higher order partial least squares ( hopls ) : a generalized multilinear regression method .",
    "_ ieee trans . pattern anal .",
    "machine intell . _ * 35 * , 16601673 ."
  ],
  "abstract_text": [
    "<S> modern technology often generates data with complex data structures in which both response and explanatory variables are matrix - valued . existing methods in the literature are able to tackle matrix - valued predictors but are rather limited for matrix - valued responses . in this article , we study matrix - variate regressions for such data , where the response @xmath0 on each experimental unit is a random matrix and the predictor @xmath1 can be either a scalar , a vector , or a matrix , treated as non - stochastic in terms of the conditional distribution @xmath2 . </S>",
    "<S> we propose models for matrix - variate regressions and then develop envelope extensions of these models . under the envelope framework , redundant variation can be eliminated in estimation and the number of parameters can be notably reduced when the matrix - variate dimension is large , possibly resulting in significant gains in efficiency .    </S>",
    "<S> # 1    * key words : * matrix - valued response ; matrix - variate regression ; reducing subspace ; sufficient dimension reduction . </S>"
  ]
}