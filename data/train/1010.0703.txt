{
  "article_text": [
    "regularization is a powerful technique in statistics , machine learning , and data analysis for learning from or extracting useful information from noisy data  @xcite .",
    "it involves ( explicitly or implicitly ) making assumptions about the data in order to obtain a `` smoother '' or `` nicer '' solution to a problem of interest .",
    "the technique originated in integral equation theory , where it was of interest to give meaningful solutions to ill - posed problems for which a solution did not exist  @xcite .",
    "more recently , it has achieved widespread use in statistical data analysis , where it is of interest to achieve solutions that generalize well to unseen data  @xcite . for instance , much of the work in kernel - based and manifold - based machine learning is based on regularization in reproducing kernel hilbert spaces  @xcite .    typically , regularization is implemented via a two step process : first , add some sort of norm constraint to an objective function of interest ; and then , exactly optimize the modified objective function .",
    "for instance , one typically considers a loss function @xmath0 that specifies an empirical penalty depending on both the data and a parameter vector @xmath1 ; and a regularization function @xmath2 that encodes prior assumptions about the data and that provides capacity control on the vector  @xmath1 .",
    "then , one must solve an optimization problem of the  form : @xmath3 a general feature of regularization implemented in this manner is that , although one obtains solutions that are `` better '' ( in some statistical sense ) than the solution to the original problem , one must often solve a modified optimization problem that is `` worse '' ( in the sense of being more computationally expensive ) than than the original optimization problem .-",
    "regularized @xmath4-regression problem .",
    "more generally , however , even assuming that @xmath2 is convex , one obtains a linear program or convex program that must solved . ] clearly , this algorithmic - statistical tradeoff is problematic if one is interested in large - scale applications .    on the other hand , it is well - known amongst practitioners that certain heuristics that can be used to speed up computations can sometimes have the side - effect of performing smoothing or regularization implicitly .",
    "for example , `` early stopping '' is often used when a learning model such as a neural network is trained by an iterative gradient descent algorithm ; and `` binning '' is often used to aggregate the data into bins , upon which computations are performed . as we will discuss below ,",
    "we have also observed a similar phenomenon in the empirical analysis of very large social and information networks  @xcite . in these applications ,",
    "the size - scale of the networks renders prohibitive anything but very fast nearly - linear - time algorithms , but the sparsity and noise properties of the networks are sufficiently complex that there is a need to understand the statistical properties _ implicit _ in these fast algorithms in order to draw meaningful domain - specific conclusions from their output .    motivated by these observations",
    ", we are interested in understanding in greater detail the manner in which algorithms that have superior algorithmic and computational properties either do or do not also have superior statistical properties . in particular , we would like to know :    * to what extent can one formalize the idea that performing an approximate computation can _ implicitly _ lead to more regular solutions ?    rather than addressing this question in full generality , in this paper",
    "we will address it in the context of computing the first nontrivial eigenvector of the graph laplacian .",
    "( of course , even this special case is of interest since a large body of work in machine learning , data analysis , computer vision , and scientific computation makes use of this vector . )",
    "our main result is a characterization of this implicit regularization in the context of three random - walk - based procedures for computing an approximation to this eigenvector . in particular :    * we consider three random - walk - based procedures ",
    "one based on the heat kernel of the graph , one based on computing the the pagerank vector associated with the graph , and one based on a truncated lazy random walk  for computing an approximation to the smallest nontrivial eigenvector of a graph laplacian , and we show that these approximation procedures may be viewed as implicitly solving a regularized optimization problem exactly .    interestingly , in order to achieve this identification , we need to relax the standard spectral optimization problem to a semidefinite program .",
    "thus , the variables that enter into the loss function and the regularization term are not unit vectors , as they are more typically in formulations such as problem  ( [ eqn : reg - gen ] ) , but instead they are distributions over unit vectors .",
    "this was somewhat unexpected , and the empirical implications of this remain to be explored .    before proceeding ,",
    "let us pause to gain an intuition of our results in a relatively simple setting .",
    "to do so , consider the so - called power iteration method , which takes as input an @xmath5 symmetric matrix @xmath6 and returns as output a number @xmath7 ( the eigenvalue ) and a vector @xmath8 ( the eigenvector ) such that @xmath9 .",
    "the power iteration method starts with an initial random vector , call it @xmath10 , and it iteratively computes @xmath11 .",
    "under weak assumptions , the method converges to @xmath12 , the dominant eigenvector of @xmath6 .",
    "the reason is clear : if we expand @xmath13 in the basis provided by the eigenfunctions @xmath14 of @xmath6 , then @xmath15 .",
    "if we truncate this method after some very small number , say @xmath16 , iterations , then the output vector is clearly a suboptimal approximation of the dominant eigen - direction of the particular matrix @xmath6 ; but due to the admixing of information from the other eigenvectors , it may be a better or more robust approximation to the best `` ground truth eigen - direction '' in the ensemble from which @xmath6 was drawn .",
    "it is this intuition in the context of computing eigenvectors of the graph laplacian that our main results formalize .",
    "for a connected , weighted , undirected graph @xmath17 , let @xmath6 be its adjacency matrix and @xmath18 its diagonal degree matrix , _",
    "i.e. _ , @xmath19 , where @xmath20 is the weight of edge @xmath21 .",
    "let @xmath22 be the natural random walk transition matrix associated with @xmath23 , in which case @xmath24 is the usual lazy random walk transition matrix .",
    "( thus , we will be _ post_-multiplying by _ column _ vectors . ) finally , let @xmath25 be the normalized laplacian of  @xmath23 .",
    "we start by considering the standard spectral optimization problem . @xmath26 in the remainder of the paper , we will assume that this last constraint always holds , effectively limiting ourselves to be in the subspace @xmath27 , by which we mean @xmath28 .",
    "( omitting explicit reference to this orthogonality constraint and assuming that we are always working in the subspace @xmath27 makes the statements and the proofs easier to follow and does not impact the correctness of the arguments . to check this , notice that the proofs can be carried out in the language of linear operators without any reference to a particular matrix representation in  @xmath29 . )",
    "next , we provide a description of three related random - walk - based matrices that arise naturally when considering a graph @xmath23 .",
    "* * heat kernel . * the heat kernel of a connected , undirected graph @xmath23 can be defined as : @xmath30 where @xmath31 is a time parameter .",
    "alternatively , it can be written as @xmath32 , where @xmath33 is the @xmath34-th eigenvalue of @xmath35 and @xmath36 denotes the projection into the eigenspace associated with @xmath33 .",
    "the heat kernel is an operator that satisfies the heat equation @xmath37 and thus that describes the diffusive spreading of heat on the graph . * * pagerank . *",
    "the pagerank vector @xmath38 associated with a connected , undirected graph @xmath23 is defined to be the unique solution to @xmath39 where @xmath40 is the so - called teleportation constant ; @xmath41 is a preference vector , often taken to be ( up to normalization ) the all - ones vector ; and @xmath42 is the natural random walk matrix associated with @xmath23  @xcite . to be the unique solution to @xmath43 , where @xmath44 is the @xmath45-lazy random walk matrix associated with  @xmath23 .",
    "these two vectors are related as @xmath46  @xcite . ]",
    "if we fix @xmath47 and @xmath48 , then it is known that @xmath49 , and thus that @xmath50 , where @xmath51 this provides an expression for the pagerank vector @xmath38 as a @xmath47-dependent linear transformation matrix @xmath52 multiplied by the preference vector @xmath48  @xcite .",
    "that is , eqn .",
    "( [ eqn : page - rank ] ) simply states that pagerank can be presented as a linear operator @xmath52 acting on the seed @xmath48 . * * truncated lazy random walk . *",
    "since @xmath22 is the natural random walk transition matrix associated with a connected , undirected graph @xmath23 , it follows that @xmath53 represents one step of the @xmath54-lazy random walk transition matrix , in which at each step there is a holding probability @xmath55 $ ] . just as @xmath42 is similar to @xmath56 , which permits the computation of its real eigenvalues and full suite of eigenvectors that can be related to those of @xmath42",
    ", @xmath57 is similar to @xmath58 .",
    "thus , iterating the random walk @xmath57 is similar to applying the power method to @xmath59 , except that the renormalization at each step need not be performed since the top eigenvalue is unity .",
    "each of these three matrices has been used to compute vectors that in applications are then used in place of the smallest nontrivial eigenvector of a graph laplacian .",
    "this is typically achieved by starting with an initial random vector and then applying the heat kernel matrix , or the pagerank operator , or truncating a lazy random walk .    finally , we recall that the solution spectralcan also be characterized as the solution to a semidefinite program ( sdp ) . to see this , consider the following sdp : @xmath60 where @xmath61 stands for the trace , or matrix inner product , operation , _",
    "i.e. _ , @xmath62 for matrices @xmath6 and @xmath63 .",
    "( recall that , both here and below , @xmath64 is the identity on the subspace perpindicular to the all - ones vector . )",
    "sdpis a relaxation of the spectral program spectralfrom an optimization over unit vectors to an optimization over distributions over unit vectors , represented by the density matrix @xmath65 .    to see the relationship between the solution @xmath1 of spectraland the solution @xmath65 of sdp , recall that a density matrix @xmath65 is a matrix of second moments of a distribution over unit vectors . in this case , @xmath66 is the expected value of @xmath67 , when @xmath1 is drawn from a distribution defined by @xmath65 .",
    "if @xmath65 is rank-@xmath68 , as is the case for the solution to @xmath69 , then the distribution is completely concentrated on a vector @xmath8 , and the sdp and vector solutions are the same , in the sense that @xmath70 .",
    "more generally , as we will encounter below , the solution to an sdp may not be rank-@xmath68 . in that case ,",
    "a simple way to construct a vector @xmath1 from a distribution defined by @xmath65 is to start with an @xmath71-vector @xmath72 with entries drawn i.i.d . from the normal distribution @xmath73 , and",
    "consider @xmath74 .",
    "note that this procedure effectively samples from a gaussian distribution with second moment @xmath65 .",
    "here , we will apply regularization technique to the sdp formulation provided by sdp , and we will show how natural regularization functions yield distributions over vectors which correspond to the diffusion - based or random - walk - based matrices . in order to regularize sdp , we want to modify it such that the distribution is not degenerate on the second eigenvector , but instead spreads the probability on a larger set of unit vectors around @xmath8 .",
    "the regularized version of sdpwe will consider will be of the form : @xmath75 where @xmath76 is a trade - off or regularization parameter determining the relative importance of the regularization term @xmath77 , and where @xmath78 is a real strictly - convex infinitely - differentiable rotationally - invariant function over the positive semidefinite cone .",
    "( think of @xmath78 as a strictly convex function of the eigenvalues of @xmath65 . ) for example , @xmath78 could be the negative of the von neumann entropy of @xmath65 ; this would penalize distributions that are too concentrated on a small measure of vectors .",
    "we will consider other possibilities for @xmath78 below .",
    "note that due to @xmath78 , the solution @xmath65 of @xmath79 will in general not be rank-@xmath68 .",
    "our main results on implicit regularization via approximate computation will be based on the following structural theorem that provides sufficient conditions for a matrix to be a solution of a regularized sdp of a certain form .",
    "note that the lagrangian parameter @xmath7 and its relationship with the regularization parameter @xmath80 will play a key role in relating this structural theorem to the three random - walk - based proceudres described previously .",
    "[ thm : main ] let @xmath23 be a connected , weighted , undirected graph , with normalized laplacian @xmath35 . then",
    ", the following conditions are sufficient for @xmath81 to be an optimal solution to @xmath79 .",
    "1 .   @xmath82 , for some @xmath83 , 2 .",
    "@xmath84 , 3 .",
    "@xmath85    for a general function @xmath78 , we can write the lagrangian @xmath86 for @xmath79 as follows : @xmath87 where @xmath88 the dual objective function is @xmath89 as @xmath78 is strictly convex , differentiable and rotationally invariant , the gradient of @xmath78 over the positive semidefinite cone is invertible and the righthand side is minimized when @xmath90 where @xmath91 is chosen such that the second condition in the statement of the theorem is satisfied .",
    "hence , @xmath92 by weak duality , this implies that @xmath81 is an optimal solution to @xmath93    two clarifying remarks regarding this theorem are in order .",
    "first , the fact that such a @xmath94 exists is an assumption of the theorem .",
    "thus , in fact , the theorem is just a statement of the kkt conditions and strong duality holds for our sdp formulations . for simplicity and to keep the exposition self - contained , we decided to present the proof of optimality , which is extremely easy in the case of an sdp with only linear constraints .",
    "second , we can plug the dual solution @xmath95 into the dual objective and show that , under the assumptions of the theorem , we obtain a value equal to the primal value of @xmath96 .",
    "this certifies that @xmath96 is optimal .",
    "thus , we do not need to assume @xmath97 ; we just choose to plug in this particular dual solution .      in this section , we will consider the three diffusion - based or random - walked - based heuristics described in section  [ sxn : background ] , and we will show that each may be viewed as solving @xmath79 for an appropriate value of @xmath78 and @xmath80 .    [ [ generalized - entropy - and - the - heat - kernel . ] ] generalized entropy and the heat kernel .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    consider first the generalized entropy function : @xmath98 for which : @xmath99 hence , the solution to @xmath100 has the form : @xmath101 for appropriately - chosen values of @xmath7 and @xmath80 .",
    "thus , we can establish the following lemma .",
    "let @xmath102 be an optimal solution to @xmath103 , when @xmath104 is the generalized entropy function , given by equation  ( [ eqn : gen - ent ] )",
    ". then @xmath105 }   , \\ ] ] which corresponds to a `` scaled '' version of the heat kernel matrix with time parameter @xmath106 .    from equation  ( [ eqn : sol - gen - ent ] )",
    ", it follows that @xmath107 , and thus by setting @xmath108 , we obtain the expression for @xmath102 given in the lemma .",
    "thus , @xmath109 and @xmath110 , and by theorem  [ thm : main ] the lemma follows .",
    "conversely , given a graph @xmath23 and time parameter @xmath111 , the heat kernel of equation  ( [ eqn : heat - kernel ] ) can be characterized as the solution to the regularized @xmath100 , with the regularization parameter @xmath112 ( and for the value of the lagrangian parameter @xmath7 as specified in the proof ) .",
    "[ [ log - determinant - and - pagerank . ] ] log - determinant and pagerank .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    next , consider the log - determinant function : @xmath113 for which : @xmath114 hence , the solution to @xmath115 has the form : @xmath116 for appropriately - chosen values of @xmath7 and @xmath80 .",
    "thus , we can establish the following lemma .",
    "let @xmath117 be an optimal solution to @xmath103 , when @xmath104 is the log - determinant function , given by equation  ( [ eqn : log - det ] )",
    ". then @xmath118}\\ ] ] which corresponds to a `` scaled - and - streached '' version of the pagerank matrix @xmath119 of equation  ( [ eqn : page - rank ] ) with teleportation parameter @xmath47 depending on @xmath80 .",
    "recall that @xmath25 . since @xmath120 ,",
    "by standard manipulations it follows that @xmath121 thus , @xmath122 if @xmath123 , and @xmath124 if @xmath125 .",
    "if we set @xmath126 ( which varies from @xmath68 to @xmath127 , as @xmath7 varies from @xmath128 to @xmath127 ) , then it can be shown that @xmath129 by requiring that @xmath130 $ ] , it follows that @xmath131\\ ] ] and thus that @xmath132 $ ] .",
    "since @xmath133 , the lemma follows .",
    "conversely , given a graph @xmath23 and teleportation parameter @xmath47 , the pagerank of equation  ( [ eqn : page - rank ] ) can be characterized as the solution to the regularized @xmath115 , with the regularization parameter @xmath80 as specified in the proof .",
    "[ [ standard - p - norm - and - truncated - lazy - random - walks . ] ] standard @xmath134-norm and truncated lazy random walks .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    finally , consider the standard @xmath134-norm function , for @xmath135 : @xmath136 for which : @xmath137 hence , the solution to @xmath138 has the form @xmath139 where @xmath140 is such that @xmath141 , for appropriately - chosen values of @xmath7 and @xmath80 .",
    "thus , we can establish the following lemma .",
    "let @xmath142 be an optimal solution to @xmath103 , when @xmath104 is the standard @xmath134-norm function , for @xmath135 , given by equation  ( [ eqn : pnorm ] ) .",
    "then @xmath143 } \\ ] ] which corresponds to a `` scaled - and - streached '' version of @xmath144 steps of the truncated lazy random walk matrix @xmath57 of equation  ( [ eqn : lazy - walk ] ) with laziness parameter @xmath54 depending on @xmath80 .",
    "recall that @xmath25 .",
    "since @xmath145 , by standard manipulations it follows that @xmath146 thus , @xmath147 if @xmath148 , and @xmath149 if @xmath150 .",
    "if we set @xmath151 ( which varies from @xmath127 to @xmath68 , as @xmath7 varies from @xmath68 to @xmath152 ) , then it can be shown that @xmath153 by requiring that @xmath154 $ ] , it follows that @xmath155 \\right\\}^{1-p}\\ ] ] and thus that @xmath156 \\right\\}^{1-p}$ ] .",
    "since @xmath157 , the lemma follows .",
    "conversely , given a graph @xmath23 , a laziness parameter @xmath54 , and a number of steps @xmath158 , the truncated lazy random walk of equation  ( [ eqn : lazy - walk ] ) can be characterized as the solution to the regularized @xmath138 , with the regularization parameter @xmath80 as specified in the proof .",
    "there is a large body of empirical and theoretical work with a broadly similar flavor to ours . here",
    ", we provide just a few citations that most informed our approach .    * in machine learning ,",
    "belkin , niyogi , and sindhwan describe a geometrically - motivated framework within which semi - supervised learning algorithms can be constructed  @xcite ; saul and roweis ( and many others , but less explicitly ) observe that adding a regularization term to improve numerical properties also `` acts to penalize large weights that exploit correlations beyond some level of precision in the data sampling process ''  @xcite ; rosasco , de  vito , and verri describe how a large class of regularization methods designed for solving ill - posed inverse problems gives rise to novel learning algorithms  @xcite ; zhang and yu show that in boosting , early stopping ( as opposed to waiting for full convergence ) leads to regularization and hence better prediction  @xcite ; shi and yu describe statistical aspects of binning in gaussian kernel regularization  @xcite ; and bishop observes that training with noise can be equivalent to tikhonov regularization  @xcite . * in numerical linear algebra , oleary , stewart , and vandergraft have described issues that arise in estimating the largest eigenvalue of a positive definite matrix with the power method  @xcite ; and parlett , simon , and stringer have described convergence issues that arise when estimating the largest eigenvalue with an iterative method  @xcite .",
    "* in the theory of algorithms , spielman and teng describe how to perform local graph partitioning using truncated random walks  @xcite ; andersen , chung , and lang describe an improved local graph partitioning algorithm pagerank vectors  @xcite ; and chung describes how to perform similar operations by using the heat kernel and viewing it as the so - called pagerank of a graph  @xcite . * in internet data analysis ,",
    "andersen and lang use these methods to try to find communities in large networks  @xcite ; leskovec , lang , and mahoney use these and other methods to show that there do not exist good large communities in large social and information networks  @xcite ; and lu _ et al .",
    "_ empirically evaluate implicit regularization constraints for improved online review quality prediction  @xcite .",
    "we should note that one can interpret our main results from one of two alternate perspectives . from the perspective of worst - case analysis",
    ", we provide a simple characterization of several related methods for approximating the smallest nontrivial eigenvector of a graph laplacian as solving a related optimization problem . by adopting this view",
    ", it should perhaps be less surprising that these methods have cheeger - like inequalities , with related algorithmic consequences , associated with them  @xcite . from a statistical perspective",
    ", one could imagine one method or another being more or less appropriate as a method to compute robust approximations to the smallest nontrivial eigenvector of a graph laplacian , depending on assumptions being made about the data . by adopting this view",
    ", it should perhaps be less surprising that these methods have performed well at identifying structure in sparse and noisy networks  @xcite .",
    "the particular results that motivated us to ask this question had to do with recent empirical work on characterizing the clustering and community structure in very large social and information networks  @xcite . as a part of that line of work , leskovec , lang , and mahoney ( llm )  @xcite were interested in understanding the artifactual properties induced in output clusters as a function of different approximation algorithms for a given objective function ( that formalized the community concept ) .",
    "llm observed a severe tradeoff between the objective function value and the `` niceness '' of the clusters returned by different approximation algorithms .",
    "this phenomenon is analogous to the bias - variance tradeoff that is commonly - observed in statistics and machine learning , except that llm did not perform any explicit regularization ",
    "instead , they observed this phenomenon as a function of different approximation algorithms to compute approximate solutions to the intractable graph partitioning problem .",
    "although we have focused in this paper simply on the problem of computing an eigenvector , one is typically interested in computing eigenvectors in order to perform some downstream data analysis or machine learning task .",
    "for instance , one might be interested in characterizing the clustering properties of the data .",
    "alternatively , the goal might be to perform classification or regression or ranking .",
    "it would , of course , be of interest to understand how the concept of _ implicit regularization via approximate computation _ extends to the output of algorithms for these problems .",
    "more generally , though , it would be of interest to understand how this concept of implicit regularization via approximate computation extends to intractable graph optimization problems ( that are not obviously formulatable as vector space problems ) that are more popular in computer science .",
    "that is : what is the ( perhaps implicitly regularized ) optimization problem that an approximation algorithm for an intractable optimization problem is implicitly optimizing ?",
    "such graph problems arise in many applications , but the the formulation and solution of these graph problems tends to be quite different than that of matrix problems that are more popular in machine learning and statistics .",
    "recent empirical and theoretical evidence , however , clearly suggests that regularization will be fruitful in this more general setting .",
    "r.  andersen , f.r.k .",
    "chung , and k.  lang .",
    "local graph partitioning using pagerank vectors . in _",
    "focs 06 : proceedings of the 47th annual ieee symposium on foundations of computer science _ ,",
    "pages 475486 , 2006 .",
    "j.  leskovec , k.j .",
    "lang , a.  dasgupta , and m.w .",
    "statistical properties of community structure in large social and information networks . in _ www 08 : proceedings of the 17th international conference on world wide web _ , pages 695704 , 2008 .",
    "j.  leskovec , k.j .",
    "lang , and m.w .",
    "empirical comparison of algorithms for network community detection . in _ www 10 : proceedings of the 19th international conference on world wide web _ ,",
    "pages 631640 , 2010 .",
    "y.  lu , p.  tsaparas , a.  ntoulas , and l.  polanyi . exploiting social context for review quality prediction . in _",
    "www 10 : proceedings of the 19th international conference on world wide web _ ,",
    "pages 691700 , 2010 .",
    "spielman and s .- h .",
    "nearly - linear time algorithms for graph partitioning , graph sparsification , and solving linear systems . in _",
    "stoc 04 : proceedings of the 36th annual acm symposium on theory of computing _ , pages 8190 , 2004 ."
  ],
  "abstract_text": [
    "<S> regularization is a powerful technique for extracting useful information from noisy data . </S>",
    "<S> typically , it is implemented by adding some sort of norm constraint to an objective function and then exactly optimizing the modified objective function . </S>",
    "<S> this procedure often leads to optimization problems that are computationally more expensive than the original problem , a fact that is clearly problematic if one is interested in large - scale applications . on the other hand , </S>",
    "<S> a large body of empirical work has demonstrated that heuristics , and in some cases approximation algorithms , developed to speed up computations sometimes have the side - effect of performing regularization implicitly . </S>",
    "<S> thus , we consider the question : what is the regularized optimization objective that an approximation algorithm is exactly optimizing ?    we address this question in the context of computing approximations to the smallest nontrivial eigenvector of a graph laplacian ; and we consider three random - walk - based procedures : one based on the heat kernel of the graph , one based on computing the the pagerank vector associated with the graph , and one based on a truncated lazy random walk . in each case </S>",
    "<S> , we provide a precise characterization of the manner in which the approximation method can be viewed as implicitly computing the exact solution to a regularized problem . </S>",
    "<S> interestingly , the regularization is not on the usual vector form of the optimization problem , but instead it is on a related semidefinite program . </S>"
  ]
}