{
  "article_text": [
    "we consider a generalized eigenvalue problem ( eigenproblem ) for a linear pencil @xmath0 with symmetric ( hermitian in the complex case ) matrices @xmath1 and @xmath2 with positive definite @xmath1 .",
    "the eigenvalues @xmath3 are enumerated in decreasing order @xmath4 and the @xmath5 denote the corresponding eigenvectors .",
    "the largest value of the rayleigh quotient @xmath6 where @xmath7 denotes the standard scalar product , is the largest eigenvalue @xmath8 .",
    "it can be approximated iteratively by maximizing the rayleigh quotient in the direction of its gradient , which is proportional to @xmath9 .",
    "preconditioning is used to accelerate the convergence ; see , e.g. ,  @xcite and the references therein . here",
    "we consider the simplest preconditioned eigenvalue solver ( eigensolver)the gradient iterative method with an explicit formula for the step size , cf .",
    "@xcite , one step of which is described by @xmath10 the symmetric ( hermitian in the complex case ) positive definite matrix @xmath11 in ( [ e.preeig ] ) is called the _",
    "preconditioner_. since @xmath1 and @xmath11 are both positive definite , we assume that @xmath12 the following result is proved in @xcite for symmetric matrices in the real space .    [ t.1 ] if @xmath13 then @xmath14 and @xmath15 the convergence factor @xmath16 can not be improved with the chosen terms and assumptions .",
    "compared to other known non - asymptotic convergence rate bounds for similar preconditioned eigensolvers , e.g. ,   @xcite , the advantages of are in its sharpness and elegance .",
    "method is the easiest preconditioned eigensolver , but still remains the only known sharp bound in these terms for any of preconditioned eigensolvers .",
    "while bound is short and simple , its proof in @xcite is quite the opposite .",
    "it covers only the real case and is not self - contained  in addition it requires most of the material from @xcite .",
    "here we extend the bound to hermitian matrices and give a new much shorter and self - contained proof of theorem  [ t.1 ] , which is a great qualitative improvement compared to that of @xcite .",
    "the new proof is not yet as elementary as we would like it to be ; however , it is easy enough to hope that a similar approach might be applicable in future work on preconditioned eigensolvers .",
    "our new proof is based on novel techniques combined with some old ideas of @xcite .",
    "we demonstrate that , for a given initial eigenvector approximation @xmath17 , the next iterative approximation @xmath18 described by belongs to a cone if we apply any preconditioner satisfying . we analyze a corresponding continuation gradient method involving the gradient flow of the rayleigh quotient and",
    "show that the smallest gradient norm ( evidently leading to the slowest convergence ) of the continuation method is reached when the initial vector belongs to a subspace spanned by two specific eigenvectors , namely @xmath5 and @xmath19 .",
    "this is done by showing that temple s inequality , which provides a lower bound for the norm of the gradient @xmath20 , is sharp only in @xmath21 .",
    "next , we extend by integration the result for the continuation gradient method to our actual fixed step gradient method to conclude that the point on the cone , which corresponds to the poorest convergence and thus gives the guaranteed convergence rate bound , belongs to the same two - dimensional invariant subspace @xmath21 .",
    "this reduces the convergence analysis to a two - dimensional case for shifted inverse iterations , where the sharp convergence rate bound is established .",
    "we start with several simplifications :    [ t.simp ] we can assume that @xmath22 , @xmath23 , @xmath24 is diagonal , eigenvalues are simple , @xmath25 , and @xmath26 in theorem [ t.1 ] without loss of generality .    first , we observe that method and bound are evidently both invariant with respect to a real shift @xmath27 if we replace the matrix @xmath2 with @xmath28 , so without loss of generality we need only consider the case @xmath29 which makes @xmath30 second , by changing the basis from coordinate vectors to the eigenvectors of @xmath31 we can make @xmath2 diagonal and @xmath23 .",
    "third , having @xmath14 if @xmath32 or @xmath33 , or both , bound becomes trivial .",
    "the assumption @xmath22 is a bit more delicate .",
    "the vector @xmath18 depends continuously on the preconditioner @xmath11 , so we can assume that @xmath22 and extend the final bound to the case @xmath34 by continuity",
    ".    finally , we again use continuity to explain why we can assume that all eigenvalues ( in fact , we only need @xmath3 and @xmath35 ) are simple and make @xmath36 and thus @xmath24 without changing anything .",
    "let us list all @xmath2-dependent terms , in addition to all participating eigenvalues , in method ( [ e.ep ] ) : @xmath37 and @xmath18 ; and in bound ( [ e.muest ] ) : @xmath37 and @xmath38 .",
    "all these terms depend on @xmath2 continuously if @xmath2 is slightly perturbed into @xmath39 with some @xmath40 , so we increase arbitrarily small the diagonal entries of the matrix @xmath2 to make all eigenvalues of @xmath39 simple and @xmath36 .",
    "if we prove bound for the matrix @xmath39 with simple positive eigenvalues , and show that the bound is sharp as @xmath41 with @xmath40 , we take the limit @xmath40 and by continuity extend the result to the limit matrix @xmath42 with @xmath29 and possibly multiple eigenvalues .",
    "it is convenient to rewrite ",
    "equivalently by theorem [ t.simp ] as follows denotes the euclidean vector norm , i.e. ,   @xmath43 for a real or complex column - vector @xmath17 , as well as the corresponding induced matrix norm . ] @xmath44 and if @xmath45 and @xmath26 then @xmath14 and @xmath46 now we establish the validity and sharpness of bound assuming and .",
    "[ t.cone ] let us define$ ] between vectors by @xmath47 .",
    "] @xmath48 then @xmath49 and @xmath50 .",
    "let @xmath51 be defined as the vector constrained by @xmath52 and with the smallest value @xmath53",
    ". then @xmath54 .",
    "orthogonality @xmath55 by the pythagorean theorem implies @xmath56 , so @xmath57 , since @xmath58 as @xmath24 , and @xmath59 where @xmath60 as @xmath24 .",
    "a ball with the radius @xmath61 by ( [ e.ass ] ) centered at @xmath62 contains @xmath63 by , so @xmath64 .",
    "the statement @xmath65 follows directly from the definition of @xmath66 .",
    "now , @xmath67 as @xmath24 , so @xmath68 and @xmath69    we denote by @xmath70 the circular cone around @xmath62 with the opening angle @xmath71 . theorem [",
    "t.cone ] replaces @xmath18 with the minimizer @xmath66 of the rayleigh quotient on the cone @xmath72 in the rest of the paper , except at the end of the proof of theorem [ t.2d ] , where we show that bounding below the value @xmath53 instead of @xmath38 still gives the sharp estimate .    later on , in the proof of theorem [ t.wcurve ]",
    ", we use an argument that holds easily only in the real space , so we need the following last simplification .",
    "[ t.real ] without loss of generality we can consider only the real case .",
    "the key observation is that for our positive diagonal matrix @xmath2 the rayleigh quotient depends evidently only on the absolute values of the vector components , i.e. , @xmath73 , where the absolute value operation is applied component - wise .",
    "moreover , @xmath74 and @xmath75 , so @xmath76 .",
    "the cone @xmath72 lives in the complex space , but we also need its substitute in the real space .",
    "let us introduce the notation @xmath77 for the _ real _ circular cone with the opening angle @xmath78 centered at the _ real _ vector @xmath79 next we show that in the real space we have the inclusion @xmath80 .",
    "for any complex nonzero vectors @xmath17 and @xmath81 , we have @xmath82 by the triangle inequality , thus @xmath83 .",
    "if @xmath84 then @xmath85 , i.e. ,  indeed , @xmath86 , which means that @xmath80 as required .    therefore , changing the given vector @xmath17 to take its absolute value @xmath87 and replacing the complex cone @xmath72 with the real cone @xmath77 lead to the relations @xmath88 , but does not affect the starting rayleigh quotient @xmath89 this proves the theorem with the exception of the issue of whether the sharpness in the real case implies the sharpness in the complex case ; see the end of the proof of theorem [ t.2d ] .",
    "[ t.wcurve ] we have @xmath90 and @xmath91 such that @xmath92 .",
    "the inclusion @xmath93 implies @xmath94    . _ _ ]    assuming that @xmath66 is strictly inside the cone @xmath72 implies that @xmath66 is a point of a local minimum of the rayleigh quotient .",
    "the rayleigh quotient has only one local ( and global ) minimum , @xmath95 , but the possibility @xmath96 is eliminated by theorem [ t.cone ] , so we obtain a contradiction , thus @xmath90 .",
    "the necessary condition for a local minimum of a smooth real - valued function on a smooth surface in a real vector space is that the gradient of the function is orthogonal to the surface at the point of the minimum and directed inwards . in our case",
    ", @xmath72 is a circular cone with the axis @xmath62 and the gradient @xmath97 is positively proportional to @xmath98 ; see figure [ f1 ] .",
    "we first scale the vector @xmath66 such that @xmath99 so that the vector @xmath100 is an inward normal vector for @xmath101 at the point @xmath66 .",
    "this inward normal vector must be positively proportional to the gradient , @xmath102 with @xmath103 , which gives @xmath104 , where @xmath105 . here",
    "@xmath106 as otherwise @xmath66 would be an eigenvector , but @xmath107 by theorem [ t.cone ] , where by assumptions @xmath108 , while @xmath26 by theorem [ t.simp ] , which gives a contradiction .",
    "as the scaling of the minimizer is irrelevant , we denote @xmath109 here by @xmath66 with a slight local notation abuse .",
    "finally , since @xmath92 , inclusion @xmath93 gives either the required inclusion @xmath110 or @xmath111 with @xmath112 for some @xmath113 and @xmath114 we now show that the latter leads to a contradiction .",
    "we have just proved that @xmath115 , thus @xmath116 .",
    "let @xmath117 , where we notice that @xmath118 and @xmath119 since @xmath17 is not an eigenvector",
    ". then we obtain @xmath120 where @xmath121 , therefore @xmath122 .",
    "since all eigenvalues are simple , @xmath123 we observe that @xmath124 , i.e. ,   in the mapping of @xmath17 to @xmath66 the coefficient in front of @xmath5 changes by a smaller absolute value compared to the change in the coefficient in front of @xmath19 . thus , @xmath125 using the monotonicity of the rayleigh quotient in the absolute values of the coefficients of the eigenvector expansion of its argument , which contradicts @xmath126 proved in theorem [ t.cone ] .    theorem [ t.wcurve ] characterizes the minimizer @xmath66 of the rayleigh quotient on the cone @xmath72 for a fixed @xmath17 .",
    "the next goal is to vary @xmath17 , preserving its rayleigh quotient @xmath37 , and to determine conditions on @xmath17 leading to the smallest @xmath53 in such a setting .",
    "intuition suggests ( and we give the exact formulation and the proof later in theorem  [ t.2dred ] ) that the poorest convergence of a gradient method corresponds to the smallest norm of the gradient , so in the next theorem we analyze the behavior of the gradient @xmath127 of the rayleigh quotient and the cone opening angle @xmath128    [ t.2dgrad ] let @xmath129 be fixed and the level set of the rayleigh quotient be denoted by @xmath130 .",
    "both @xmath131 and @xmath132 with @xmath133 attain their minima on @xmath134 in @xmath135    by definition of the gradient , @xmath136 for @xmath134 .",
    "the temple inequality @xmath137 is equivalent to the operator inequality @xmath138 , which evidently holds .",
    "the equality here is attained only for @xmath139    finally , we turn our attention to the angles . for @xmath140",
    "the pythagorean theorem @xmath141 shows that @xmath142 is minimized together with @xmath143 .",
    "but for a fixed @xmath144 the function @xmath145 is strictly increasing in @xmath146 which proves the proposition for @xmath147    now we are ready to show that the same subspace @xmath21 gives the smallest change in the rayleigh quotient @xmath148 . the proof is based on analyzing the negative normalized gradient flow of the rayleigh quotient",
    ".    [ t.2dred ] under the assumptions of theorems [ t.wcurve ] and [ t.2dgrad ] we denote @xmath149the set of minimizers of the rayleigh quotient . then @xmath150 ( see figure [ f2 ] ) .        the initial value problem for a gradient flow of the rayleigh quotient , @xmath151 has the vector - valued solution @xmath152 which preserves the norm of the initial vector @xmath66 since @xmath153 as @xmath154 . without loss of generality",
    "we assume @xmath155 the rayleigh quotient function @xmath156 is decreasing since @xmath157 as @xmath158 , the function @xmath156 is strictly decreasing at least until it reaches @xmath159 as there are no eigenvalues in the interval @xmath160\\subset(\\mu_{i+1},\\mu_i),$ ] but only eigenvectors can be special points of ode .",
    "the condition @xmath161 thus uniquely determines @xmath162 for a given initial value @xmath66 .",
    "the absolute value of the decrease of the rayleigh quotient along the path @xmath163 is @xmath164 our continuation method using the _ normalized _ gradient flow is nonstandard , but its advantage is that it gives the following simple expression for the length of @xmath165 , @xmath166    since the initial value @xmath66 is determined by @xmath17 , we compare a generic @xmath17 with the special choice @xmath167 , using the superscript @xmath168 to denote all quantities corresponding to the choice @xmath169 . by theorem  [ t.wcurve ] @xmath170 implies @xmath171 , so we have @xmath172 @xmath173 as @xmath21 is an invariant subspace for the gradient of the rayleigh quotient . at the end points , @xmath174 by their definition .",
    "our goal is to bound the initial value @xmath175 by @xmath176 , so we compare the lengths of the corresponding paths @xmath177 and @xmath165 and the norms of the gradients along these paths .",
    "we start with the lengths .",
    "we obtain @xmath178 by theorem  [ t.2dgrad ] . here",
    "the angle @xmath132 is the smallest angle between any two vectors on the cones boundaries @xmath101 and @xmath179 .",
    "thus , @xmath180 as our one vector @xmath181 by theorem  [ t.wcurve ] , while the other vector @xmath182 can not be inside the cone @xmath72 since @xmath183 by theorem  [ t.cone ] . as @xmath184 is a unit vector , @xmath185 as the angle is the length of the arc  the shortest curve from @xmath186 to @xmath182 on the unit ball .    for our special @xmath168-choice , inequalities from the previous paragraph turn into equalities , as @xmath187 is in the intersection of the unit ball and the subspace @xmath21 ,",
    "so the path @xmath177 is the arc between @xmath188 to @xmath189 itself . combining everything together , @xmath190    by theorem [ t.2dgrad ] on the norms of the gradient , @xmath191 for each pair of independent variables @xmath192 and @xmath193 such that @xmath194 using theorem [ t : iif ]",
    ", we conclude that @xmath195 as @xmath196 , i.e. ,  the subspace @xmath21 gives the smallest value @xmath197    by theorem  [ t.2dred ] the poorest convergence is attained with @xmath93 and with the corresponding minimizer @xmath110 described in theorem [ t.wcurve ] , so finally our analysis is now reduced to the two - dimensional space @xmath21 .",
    "[ t.2d ] bound holds and is sharp for @xmath93 .    assuming @xmath198 and @xmath199 , we derive @xmath200 and similarly for @xmath110 where @xmath92 .    since @xmath24",
    ", we have @xmath201 . assuming @xmath202",
    ", this identity implies @xmath203 which contradicts our assumption that @xmath17 is not an eigenvector . for @xmath204 and @xmath115 by theorem  [ t.wcurve ] , the inverse @xmath205 exists .",
    "next we prove that @xmath206 and that it is a strictly decreasing function of @xmath207 indeed , using @xmath208 and our cosine - based definition of the angles , we have @xmath209 where @xmath210 we substitute @xmath211 , which gives @xmath212 using , multiplication by @xmath213 leads to a simple quadratic equation , @xmath214 for @xmath215 as @xmath216 , @xmath217 , and @xmath218 , the discriminant is positive and the two solutions for @xmath219 , corresponding to the minimum and maximum of the rayleigh quotient on @xmath72 , have different signs . the proof of theorem [ t.wcurve ]",
    "analyzes the direction of the gradient of the rayleigh quotient to conclude that @xmath103 and @xmath220 correspond to the minimum . repeating the same arguments with @xmath221",
    "shows that @xmath222 corresponds to the maximum . but",
    "@xmath223 since @xmath24 , hence the negative @xmath219 corresponds to the maximum and thus the positive @xmath219 corresponds to the minimum .",
    "we observe that the coefficients @xmath216 and @xmath217 are evidently increasing functions of @xmath224 , while @xmath218 does not depend on @xmath225 .",
    "thus @xmath206 is strictly decreasing in @xmath225 , and taking @xmath226 gives the smallest @xmath227 since @xmath92 where now @xmath206 , condition @xmath228 implies @xmath229 and @xmath230 implies @xmath231 so we introduce the convergence factor as @xmath232 where we use and again @xmath92 .",
    "we notice that @xmath233 is a strictly decreasing function of @xmath206 and thus takes its largest value for @xmath234 giving @xmath235 i.e. ,   bound that we are seeking .",
    "the convergence factor @xmath236 can not be improved without introducing extra terms or assumptions .",
    "but @xmath236 deals with @xmath237 , not with the actual iterate @xmath18 .",
    "we now show that for @xmath129 there exist a vector @xmath93 and a preconditioner @xmath11 satisfying such that @xmath238 and @xmath239 in both real and complex cases . in the complex case , let us choose @xmath17 such that @xmath240 and @xmath241 according to , then the real vector @xmath242 is a minimizer of the rayleigh quotient on @xmath72 , since @xmath243 and @xmath244 .    finally , for a real @xmath17 with @xmath240 and a real properly scaled @xmath84",
    "there is a real matrix @xmath11 satisfying such that @xmath245 , which leads to with @xmath246 indeed , for the chosen @xmath17 we scale @xmath84 such that @xmath247 so @xmath248 . as vectors @xmath249 and @xmath250 are real and have the same length there exists a _",
    "real _ householder reflection @xmath251 such that @xmath252 .",
    "setting @xmath253 we obtain the required identity .",
    "any householder reflection is symmetric and has only two distinct eigenvalues @xmath254 , so we conclude that @xmath11 is real symmetric ( and thus hermitian in the complex case ) and satisfies .",
    "the integration of inverse functions theorem follows .    [",
    "t : iif ] let @xmath255\\to{\\bf r}$ ] for @xmath217 be strictly monotone increasing smooth functions and suppose that for @xmath256 $ ] we have @xmath257 .",
    "if for all @xmath258 $ ] with @xmath259 the derivatives satisfy @xmath260 then for any @xmath261 $ ] we have @xmath262    for any @xmath261 $ ] we have ( using @xmath257 ) @xmath263 if @xmath264 , then for the derivatives of the inverse functions it holds that @xmath265 since @xmath266 and @xmath267 are strictly monotone increasing functions the integrands are positive functions and @xmath268 as well as @xmath269 . comparing the lower limits of the integrals gives the statement of the theorem .",
    "we present a new geometric approach to the convergence analysis of a preconditioned fixed - step gradient eigensolver which reduces the derivation of the convergence rate bound to a two - dimensional case .",
    "the main novelty is in the use of a continuation method for the gradient flow of the rayleigh quotient to locate the two - dimensional subspace corresponding to the smallest change in the rayleigh quotient and thus to the slowest convergence of the gradient eigensolver .",
    "an elegant and important result such as theorem [ t.1 ] should ideally have a textbook - level proof .",
    "we have been trying , unsuccessfully , to find such a proof for several years , so its existence remains an open problem .",
    "we thank m. zhou of university of rostock , germany for proofreading .",
    "m. argentati of university of colorado denver , e. ovtchinnikov of university of westminster , and anonymous referees have made numerous great suggestions to improve the paper and for future work .",
    "d e.  g. dyakonov , _ optimization in solving elliptic problems _ , crc press , 1996 .",
    "kny1986 a.  v. knyazev , _ computation of eigenvalues and eigenvectors for mesh problems : algorithms and error estimates _ ,",
    "( in russian ) , dept",
    "moscow , 1986 .",
    "k99 a.  v. knyazev , _ preconditioned eigensolvers : practical algorithms _ , in z.  bai , j.  demmel , j.  dongarra , a.  ruhe , and h.  van  der vorst , editors , templates for the solution of algebraic eigenvalue problems : a practical guide , pp .",
    "siam , philadelphia , 2000 .",
    "k00 a.  v. knyazev , _ toward the optimal preconditioned eigensolver : locally optimal block preconditioned conjugate gradient method _ , siam j. sci .",
    "comput . , 23 ( 2001 ) , pp",
    ". 517541 .",
    "knn2003 a.  v. knyazev and k.  neymeyr , _ a geometric theory for preconditioned inverse iteration .",
    "iii : a short and sharp convergence estimate for generalized eigenvalue problems _ , linear algebra appl .",
    ", 358 ( 2003 ) , pp . 95114 .",
    "ney2001a k.  neymeyr , _ a geometric theory for preconditioned inverse iteration .",
    "i : extrema of the rayleigh quotient _ , linear algebra appl .",
    ", 322 ( 2001 ) , pp .",
    "ney2001b k.  neymeyr , _ a geometric theory for preconditioned inverse iteration .",
    "ii : convergence estimates _ , linear algebra appl .",
    ", 322 ( 2001 ) , pp . 87104 ."
  ],
  "abstract_text": [
    "<S> preconditioned eigenvalue solvers ( eigensolvers ) are gaining popularity , but their convergence theory remains sparse and complex . </S>",
    "<S> we consider the simplest preconditioned eigensolver  the gradient iterative method with a fixed step size  for symmetric generalized eigenvalue problems , where we use the gradient of the rayleigh quotient as an optimization direction . </S>",
    "<S> a sharp convergence rate bound for this method has been obtained in 20012003 . </S>",
    "<S> it still remains the only known such bound for any of the methods in this class . while the bound is short and simple , its proof is not . </S>",
    "<S> we extend the bound to hermitian matrices in the complex space and present a new self - contained and significantly shorter proof using novel geometric ideas .    </S>",
    "<S> iterative method ; continuation method ; preconditioning ; preconditioner ; eigenvalue ; eigenvector ; rayleigh quotient ; gradient iteration ; convergence theory ; spectral equivalence    49m37 65f15 65k10 </S>",
    "<S> 65n25    ( place for digital object identifier , to get an idea of the final spacing . ) </S>"
  ]
}