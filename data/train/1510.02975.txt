{
  "article_text": [
    "of high - quality and real - time image processing algorithms is a key topic in today s context , where humans demand more capabilities and control over their rapidly increasing number of advanced imaging devices such as cameras , smart phones , tablets , etc .",
    "many efforts are being made toward satisfying those needs by exploiting dedicated hardware , such as graphics processing units ( gpus )  @xcite .",
    "some areas where gpus are emerging to improve human computer interaction ( hci ) include cybernetics ( for example , in facial  @xcite or object  @xcite recognition , classification using support vector machines  @xcite and genetic algorithms for clustering  @xcite ) , and image processing ( e.g. , unsupervised image segmentation  @xcite , optical coherence tomography systems  @xcite , efficient surface reconstruction from noisy data  @xcite , remote sensing  @xcite , real - time background subtraction  @xcite , etc . ) . in such hardware - oriented designed algorithms , the computational efficiency of processing tasks is significantly improved by parallelizing the operations .",
    "high level tasks such as ( a ) object / person - tracking or ( b ) video - based gestural interfaces often require a foreground segmentation method as a base building block.,title=\"fig : \" ] +   ( a )    high level tasks such as ( a ) object / person - tracking or ( b ) video - based gestural interfaces often require a foreground segmentation method as a base building block.,title=\"fig : \" ] +   ( b )    automated video analysis applications such as person tracking or video - based gestural interfaces ( see  [ fig : applications ] ) rely on lower - level building blocks like foreground segmentation  @xcite , where the design of efficient computational methods for dedicated hardware has a significant impact .",
    "multimodal nonparametric segmentation strategies have drawn a lot of attention  @xcite since they are able to provide high - quality results even in complex scenarios ( dynamic background , illumination changes , etc . )",
    "however , their main drawback is their extremely high computational cost ( requiring the evaluation of billions of multidimensional gaussian kernels per second ) , which makes them difficult to integrate in the latest generation of image processing applications  @xcite . to overcome this important drawback and achieve real - time performance  @xcite , the use of parallel hardware such as gpus helps but may not be enough by itself , depending on the required resolution , hence the need for algorithms capable of evaluating non - linear ( e.g. , gaussian ) functions at high speed within a required error tolerance .",
    "gpu vendors are aware of this recurrent computing problem and provide hardware implementations of common transcendental functions in special function units ( sfus )  @xcite ; indeed , we can find in the literature examples of successful use of these hardware facilities  @xcite .",
    "however , their ease of use comes at the price of non - customizable reduced numerical precision  @xcite .",
    "we propose a novel , fast , and practical method to evaluate any continuous mathematical function within a known interval .",
    "our contributions include , based on error equalization , a nearly optimal design of two types of piecewise linear approximations ( linear interpolant and orthogonal projection ) in the @xmath0 norm under the constraint of a large budget of evaluation subintervals  @xmath1 . moreover , we provide asymptotically tight bounds for the approximation errors of both piecewise linear representations , improving upon existing ones .",
    "specifically , in addition to the @xmath2 convergence rate typical of these approximations , we quantify their interpolation constants : @xmath3 for the linear interpolant and a further @xmath4 improvement factor in case of the orthogonal projection .",
    "the obtained error relations are parameterized by the number of segments used to represent the complex ( nonlinear ) function , hence our approach allows the user to estimate the errors given @xmath1 or , conversely , estimate the required @xmath1 to achieve a target approximation error .",
    "we also propose an efficient implementation of the technique in a modern gpu by exploiting the fixed - function interpolation routines present in its texture units to accelerate the computation while leaving the rest of the gpu ( and , of course , the cpu ) free to perform other tasks ; this technique is even faster than using sfus  @xcite .",
    "although the initial motivation of the developed method was to improve the efficiency of nonparametric foreground segmentation strategies , it must be noted that it can be also used in many other scientific fields such as computer vision  @xcite or audio signal processing  @xcite , where the evaluation of continuous mathematical functions constitutes a significant computational burden .",
    "section  [ sec : piecewise - linear - approximations ] summarizes the basic facts about piecewise linear approximation of real - valued univariate functions and reviews related work in this topic ; section  [ sec : finding - a - non - uniform ] derives a suboptimal partition of the domain of the approximating function in order to minimize the distance to the original function ( proofs of results are given in the appendixes ) ; section  [ sec : computational - analysis - and ] analyzes the algorithmic complexity of the proposed approximation strategies and section  [ sec : implementation - in - a - gpu ] gives details about their implementation on modern gpus .",
    "section  [ sec : results ] presents the experimental results of the proposed approximation on several functions ( gaussian , lorentzian and bessel s ) , both in terms of quality and computational times , and its use is demonstrated in an image processing application .",
    "finally , section  [ sec : conclusions ] concludes the paper .",
    "in many applications , rigorous evaluation of complex mathematical functions is not practical because it takes too much computational power .",
    "consequently , this evaluation is carried out approximating them by simpler functions such as ( piecewise-)polynomial ones .",
    "piecewise linearization has been used as an attractive simplified representation of various complex nonlinear systems  @xcite .",
    "the resulting models fit into well established tools for linear systems and reduce the complexity of finding the inverse of nonlinear functions  @xcite .",
    "they can also be used to obtain approximate solutions in complex nonlinear systems , for example , in mixed - integer linear programming ( milp ) models  @xcite .",
    "some efforts have been devoted as well to the search for canonical representations in one and multiple dimensions  @xcite with different goals such as black box system identification , approximation or model reduction .",
    "previous attempts to address the problem considered here ( optimal function piecewise linearization ) include  @xcite , the latter two in the context of nonlinear dynamical systems . in  @xcite an iterative multi - stage procedure based on dynamical programming",
    "is given to provide a solution to the problem on sequences of progressively finer 2-d grids . in  @xcite the piecewise linear approximation",
    "is obtained by using the evolutionary computation approach such as genetic algorithm and evolution strategies ; the resulting model is obtained by minimization of a sampled version of the mean squared error and it may not be continuous . in  @xcite the problem",
    "is addressed using a hybrid approach based on curve fitting , clustering and genetic algorithms . here",
    "we address the problem from a less heuristic point of view , using differential calculus to derive a more principled approach  @xcite .",
    "our method solely relies on standard numerical integration techniques , which takes few seconds to compute , as opposed to recursive partitioning techniques such as  @xcite , which take significantly longer .      in this section",
    "we summarize the basic theory behind piecewise linear functions  @xcite and two such approximations to real - valued functions : interpolation and projection ( sections  [ sub : linear - interpolation ] and  [ sub : the - l2-projection ] , respectively ) , pictured in  [ fig : pipvsproj ] along with their absolute approximation error with respect to @xmath5 .",
    "top : fifth - degree polynomial @xmath6 and two continuous piecewise linear  ( cpwl ) approximations , the orthogonal projection @xmath7 and the linear interpolant @xmath8 ; bottom : corresponding absolute approximation errors ( magnified by a @xmath9 factor).,width=317 ]    hat functions constitute a basis of @xmath10 . since all the basis functions are zero outside @xmath11 $ ] , the basis functions associated with boundary nodes are only _ half hats_.,width=317 ]    in essence , a piecewise function over an interval @xmath12 $ ] is a partition of @xmath13 into a set of @xmath1 subintervals @xmath14 , where @xmath15 , and a set of @xmath1 functions @xmath16 , one for each subinterval  @xmath17 . in particular we are interested in continuous piecewise linear  ( cpwl ) functions , which means that all the @xmath16 are linear and @xmath18",
    ". cpwl functions of a given partition @xmath19 are elements of a vector space @xmath10 : the addition of such functions and/or multiplication by a scalar yields another cpwl function defined over the same subintervals . a useful basis for the vector space @xmath10 is formed by the set of _ hat functions _ or _ nodal basis functions _",
    "@xmath20 , pictured in  [ fig : hat - functions - constitute ] and defined in general by the formula @xmath21\\\\ ( x - x_{i+1})/(x_{i}-x_{i+1 } ) , & x\\in\\left[x_{i},x_{i+1}\\right]\\\\ 0 , & x\\notin\\left[x_{i-1},x_{i+1}\\right ] .",
    "\\end{cases}\\ ] ] the basis functions @xmath22 and @xmath23 associated to the boundary nodes @xmath24 and @xmath25 are only _ half hats_.    these basis functions are convenient since they can represent any function @xmath26 in @xmath10 by just requiring the values of @xmath26 at its nodal points , @xmath27 , in the form @xmath28      the piecewise linear interpolant @xmath29 of a continuous function @xmath5 over the interval @xmath13 can be defined in terms of the basis just introduced : @xmath30 while this cpwl approximation is trivial to construct , and may be suitable for some uses , it is by no means the best possible one .",
    "crucially , @xmath31 for any function @xmath5 that is convex in @xmath13 . depending on the application in which this approximation is used , this property could skew the results .",
    "however , as we will see in section  [ sec : finding - a - non - uniform ] , the linear interpolant is useful to analyze other possible approximations .",
    "it is also at the heart of the trapezoidal rule for numerical integration .",
    "let the usual inner product between two square - integrable ( @xmath0 ) functions in the interval @xmath13 be given by @xmath32 then , the vector space @xmath10 can be endowed with the above inner product , yielding an inner product space . as usual , let @xmath33 be the norm induced by the inner product , and let @xmath34 be the distance between two functions @xmath35 .",
    "the orthogonal projection of the function @xmath5 onto @xmath10 is the function @xmath36 such that @xmath37 since @xmath36 , it can be expressed in the nodal basis @xmath38 by @xmath39 where the coefficients @xmath40 solve the linear system of equations @xmath41 .",
    "the @xmath42 matrix @xmath43 has entries @xmath44 , and vector @xmath45 has entries given by @xmath46 .",
    "the gramian matrix @xmath47 is tridiagonal and strictly diagonally dominant .",
    "therefore , the system has exactly one solution @xmath48 , which can be obtained efficiently using the thomas algorithm ( a particular case of gaussian elimination )  @xcite .",
    "@xmath7 is the element in @xmath10 that is closest to @xmath5 in the sense given by the aforementioned @xmath0 distance @xmath49 , as we recall next . for any @xmath50 , @xmath51 where we used property   that the vector @xmath52 . next , applying the cauchy - schwarz inequality , @xmath53 and so @xmath54 , i.e. , @xmath55 with equality if @xmath56 .",
    "this makes @xmath7 most suitable as an approximation of @xmath5 under the @xmath0 norm .",
    "as just established , for a given vector space of cpwl functions @xmath10 , @xmath7 is the function in @xmath10 whose @xmath0 distance to @xmath5 is minimal .",
    "however , the approximation error @xmath57 does not take the same value for every possible @xmath10 ; therefore , we would like to find the optimal partition @xmath58 ( or a reasonable approximation thereof ) corresponding to the space @xmath59 in which the approximation error is minimum , @xmath60    this is a difficult optimization problem : in order to properly specify @xmath7 in and measure its distance to @xmath5 , we need to solve the linear system of equations @xmath41 , whose coefficients depend on the partition itself , which makes the problem symbolically intractable .",
    "numerical algorithms could be used but it is still a challenging problem .",
    "let us examine the analogous problem for the interpolant function @xmath61 defined in section  [ sub : linear - interpolation ] .",
    "again , we would like to find the optimal partition @xmath58 corresponding to the space @xmath59 in which the approximation error is minimum , @xmath62 albeit not as difficult as the previous problem , because @xmath8 is more straightforward to define , this is still a challenging non - linear optimization problem .",
    "fortunately , as it is shown next , a good approximation can be easily found under some asymptotic analysis .    in the rest of this section",
    "we investigate in detail the error incurred when approximating a function @xmath5 by the interpolant @xmath8 and the orthogonal projection @xmath7 defined in section  [ sec : piecewise - linear - approximations ] ( see  [ fig : pipvsproj ] ) .",
    "then we derive an approximation to the optimal partition ( section  [ sub : approximation - to - the - optimal - partition ] ) that serves equally well for @xmath8 and @xmath7 because , as we will show ( section  [ sub : error - in - a - single - interval ] ) , their approximation errors are roughly proportional under the assumption of a sufficiently large number of intervals .",
    "first , let us consider the error incurred when approximating a function @xmath5 , twice continuously differentiable , by its linear interpolant in an interval (  [ fig : errorzone ] ) .",
    "[ thm : lininterpolanterrbound ] the @xmath0 error between a given function @xmath63 and its linear interpolant @xmath64 with @xmath65 , in the interval @xmath66 $ ] , of length @xmath67 , is bounded : @xmath68 where @xmath69 .    see appendix  [ app : proofinterpolant ] .",
    "formula   has the following intuitive explanation : the error measures the deviation of @xmath5 from being straight ( linear ) , and this is directly related to the convexity / concavity of the function , thus the presence of the term @xmath70 to bound the amount of bending .",
    "the other interesting part of   is the @xmath71 dependence with respect to the interval size or the local density of knots .",
    "other works in the literature use the weaker bound that does not require carrying out integration  @xcite : @xmath72 , for some constant @xmath73 .",
    "function @xmath5 in the subinterval @xmath66 $ ] and two linear approximations . on the left , the linear interpolant @xmath8 given by  ; on the right , a general linear segment @xmath74 given by",
    "@xmath75 is a signed distance with respect to @xmath76.,width=317 ]    let us now characterize the error of the orthogonal projection @xmath7 .",
    "we do so in two steps : first we compute the minimum error of a line segment in the interval @xmath77 , and then we use an asymptotic analysis to approximate the error of the orthogonal projection .    stemming from  @xmath78 , we can write any ( linear ) segment in @xmath17 as @xmath79 where @xmath80 and @xmath81 are extra degrees of freedom ( pictured in  [ fig : errorzone ] ) with respect the interpolant @xmath8 that allow the line segment to better approximate the function @xmath5 in @xmath17 . by computing the optimal values of @xmath80 and @xmath82 we obtain the following result",
    ".    [ thm : minerrlinesegment ] the minimum squared @xmath0 error between a given function @xmath63 and a line segment   in an interval @xmath66 $ ] , of length @xmath67 , adopts the expression @xmath83 for some @xmath84 in @xmath85 .",
    "see appendix  [ app : prooflinesegment ] .",
    "[ thm : minerrsmalllinesegment ] if @xmath17 is sufficiently small so that @xmath86 is approximately constant within it , say @xmath87 , then @xmath88    substitute @xmath89 for @xmath90 in  .",
    "the segments in @xmath7 may not strictly satisfy   because @xmath7 must also be continuous across segments .",
    "however , as the size of the intervals @xmath91 become smaller ( due to a finer partition  @xmath19 of the interval  @xmath13 , i.e. , the number of segments @xmath1 in @xmath19 increases ) we may approximate @xmath7 in @xmath77 by the best ( independently - optimized ) segment @xmath74 and , consequently , use corollary  [ thm : minerrsmalllinesegment ] to yield the following result .",
    "[ thm : minerrsmallortproj ] the squared @xmath0 error between a given function @xmath63 and its orthogonal projection @xmath7 in a _ small _ interval @xmath66 $ ] , of length @xmath67 , is @xmath92 where @xmath87 is the approximately constant value that @xmath86 takes within the interval @xmath17 .    see appendix  [ app : convergence ] .    in the same asymptotic situation",
    ", the linear interpolant  , ( see  ) gives a bigger squared error by a factor of six , @xmath93      now we give a procedure to compute a suboptimal partition of the target interval @xmath13 and then derive error estimates for both @xmath94 and @xmath95 on such a partition and the uniform one .",
    "let us consider a partition @xmath19 of @xmath11 $ ] with subintervals @xmath66,\\ , i=1,\\ldots , n$ ] .",
    "a suboptimal partition for a given @xmath1 is one in which every subinterval has approximately equal contribution to the total approximation error  @xcite , which implies that regions of @xmath5 with higher convexity are approximated using more segments than regions with lower convexity .",
    "let us assume @xmath1 is large enough so that @xmath86 is approximately constant in each subinterval and therefore the bound   is tight .",
    "consequently , @xmath96 for some constant @xmath73 , and the lengths of the subintervals ( local knot spacing  @xcite ) should be chosen @xmath97 , i.e. , smaller intervals as @xmath70 increases .",
    "hence , the local knot distribution or density is @xmath98 so that more knots of the partition are placed in the regions with larger magnitude of the second derivative .",
    "then , the proposed approximation @xmath99 to the optimal partition is as follows : @xmath100 , @xmath101 , and take knots @xmath102 given by @xmath103 where the monotonically increasing function @xmath104\\to[0,1]$ ] is @xmath105 in this procedure , pictured in  [ fig : optimization ] , the range of @xmath106 is divided into @xmath1 contiguous sub - ranges of equal length , and the values of @xmath107 are given by the abscissas corresponding to the endpoints of the sub - ranges .      with this partition , we can further estimate approximation error bounds in the entire interval based on those of the subintervals .",
    "[ thm : errorsuboptpartpip]the approximation error of the linear interpolant @xmath108 in the partition @xmath99 given by   in @xmath11 $ ]  is @xmath109    the total squared error for any partition @xmath19 is the sum of squared errors over all subintervals @xmath77 , and by  , @xmath110 which , under the condition of error equalization   for the proposed partition @xmath99 , becomes @xmath111 to compute @xmath112 , let us sum @xmath113 over all intervals @xmath17 and approximate the result using the riemann integral : @xmath114 whose right hand side is independent of @xmath1 . substituting   in   gives the desired approximate bound   for the error in the interval @xmath11 $ ] .",
    "graphical summary of the proposed knot placement technique .",
    "top : local knot density   obtained from input function @xmath5 ( example in  [ fig : pipvsproj ] ) ; middle : cumulative knot distribution function @xmath115 given by   and knots given by the abscissas corresponding to a uniform partition of the range of @xmath115 , as expressed by  ; bottom : approximation of @xmath63 by cpwl interpolant @xmath116 with @xmath117 ( 32  knots ) . in this suboptimal partition",
    ", knots are distributed according to the amount of local convexity / concavity of @xmath5 given by the @xmath118 in the middle plot .",
    "hence , fewer knots are placed around the zeros of the @xmath118 , which correspond to the less steep regions of @xmath115 .",
    ", width=317 ]    since the approximation error of @xmath7 in each interval is roughly proportional to that of @xmath8 , as shown in   and  , the partition   is also a very good approximation to the optimal partition for @xmath7 as the number of subintervals @xmath1 increases .",
    "this is briefly stated next .",
    "[ thm : errorsuboptpartortproj]the approximation error of the orthogonal projection @xmath119 in the partition @xmath99 given by   in @xmath11 $ ]  is @xmath120    both cpwl approximations ( @xmath8 and @xmath7 ) converge to the true function @xmath5 at a rate of at least @xmath2 ( and  ) .",
    "we use a similar procedure to derive an estimate error bound for the uniform partition @xmath121 that can be compared to that of the optimized one .",
    "[ thm : erroruniformpart]the approximation error of the linear interpolant @xmath122 in the uniform partition @xmath121 of the interval @xmath11 $ ]  is @xmath123    for @xmath121 , we may substitute @xmath124 in   and approximate the result using the riemann integral , @xmath125    thus , we can estimate how much we can expect to benefit from optimizing a partition by simply dividing   by  .",
    "since   also shows a @xmath2 convergence , the profit will not depend on @xmath1 ( assuming @xmath1 is large enough ) .",
    "1 determine the subinterval @xmath77 that contains @xmath126 : @xmath127    2 find the fractional distance @xmath128 to the left endpoint of @xmath77 : @xmath129    3 return the interpolated value @xmath130    the implementation of the approximations previously discussed is straightforward ( algorithm  [ alg : given - as - input ] ) and we only need to distinguish two different cases depending on whether the partition @xmath19 of @xmath13 has all subintervals of equal width or not .",
    "although their values are different , @xmath61 and @xmath131 are qualitatively the same from the computational viewpoint .",
    "let us discuss the general case that @xmath19 is non - uniform .",
    "line  1 in algorithm  [ alg : given - as - input ] implies searching in the ordered array @xmath132 for the right index @xmath133 . since the other steps in the algorithm do not depend on the size of the input vectors , line  1",
    "is the dominant step and makes its run - time complexity @xmath134 . in the particular case that @xmath19 is uniform",
    ", no search is needed to determine the index : @xmath135 and the fractional part @xmath136 .",
    "thus , the run - time complexity of the uniform case is @xmath137 .",
    "there is also no need to store the full array @xmath132 but only its endpoints @xmath24 and @xmath25 , roughly halving the memory needed for a non - uniform partition of the same number of intervals .",
    "consequently , approximations based on a uniform partition are expected to perform better in usual cpus or gpus , computationally - wise , than those based on non - uniform partitions . however , optimizing the partition might lead , depending on the specific objective function , to a reduction in the memory requirements for a desired approximation error .",
    "if memory constraints are relevant or hardware - aided search routines become available , optimized partitions could become practical .",
    "the proposed algorithm is simple to implement either in a cpu or in a gpu . however , there are some implementation details that help further optimize the performance in a gpu .",
    "modern gpus implement a single instruction , multiple data ( simd ) execution model in which multiple threads execute exactly the same instructions ( each ) over different data .",
    "if two threads scheduled together in the same execution group or _ warp _ follow different branches in a conditional statement , both branches are not executed in parallel but sequentially , thereby halving throughput . in the case of @xmath19",
    "being non - uniform , the ( binary ) search performed to find the index @xmath133 could lead to divergent paths for different threads evaluating @xmath138 for different values of @xmath139 , thus reducing throughput .",
    "since the purpose of the proposed algorithm is to save computation at run - time , it is reasonable to assume that at least @xmath1 , and possibly @xmath132 and @xmath140 too , have been determined at compile time .",
    "then , we can particularize the search code in compile time , unrolling the loop needed to implement the binary search and , most importantly , replacing conditional statements with a fixed number of invocations of the ternary operator , which is implemented in the gpu with a single instruction and no code divergence  @xcite .    in general , accesses to memory in a gpu need to be arranged so that neighbouring threads access neighbouring addresses in memory .",
    "this allows the host interface of the gpu to coalesce operations from several threads into fewer transactions bigger in size . in the described algorithm , if each thread is evaluating @xmath138 for different values of @xmath139 , it is not guaranteed that the memory access pattern complies with this restriction . although some devices provide caching and isolate the programmer from this issue , not all do .",
    "however , there is a case in a regular computer graphics pipeline in which a similar problem arises .",
    "texture data need to be accessed many times in a short period and it is generally impossible to ensure that neighbouring pixels in screen need to read neighbouring texels .",
    "consequently , most gpus do cache texture data and even have dedicated cache layers for it .",
    "we therefore store the arrays of samples in texture memory rather than in global memory to benefit from texture caches .",
    "the use of the texture units to access our samples provides another important benefit . in the usual computer graphics pipeline ,",
    "gpus need to constantly filter texture data . in order to efficiently perform that task ,",
    "texture units are equipped with hardware fixed - function interpolation routines that include linear interpolation  @xcite .",
    "therefore , we can use them to speed up line  3 in algorithm  [ alg : given - as - input ] ; results in the next section confirm that the texture filtering unit is indeed faster than performing interpolation `` manually '' .",
    "in this section we apply the proposed linearization algorithm to several functions of interest in cybernetics , computer vision and other scientific areas .",
    "the section starts with the analysis of the gaussian function and demonstrates the proposed technique on an image processing application .",
    "the section then analyzes two other sample nonlinear functions of interest : the lorentzian function and the bessel function of the first kind .",
    "the gaussian function @xmath141 is widely used in many applications in every field of science , but it is of particular interest to us in the context of foreground segmentation in video sequences using spatio - temporal nonparametric background models  @xcite . to estimate these models , the gaussian function needs to be evaluated approximately 1300 times per pixel and input image , or around 2 billion times per second at modest video quality ( cif , @xmath142 pixels at 15 fps ) . therefore it is crucial to lower the computing time of the gaussian function . in the performed experiments it was enough to approximate the function in the interval @xmath143 $ ] to achieve the required quality .",
    "@xmath0 distance for different approximations to the gaussian function @xmath141 over the interval @xmath143$].,width=317 ]    [ fig : l2-distance ] shows @xmath0 distances between the gaussian function and the approximations described in previous sections .",
    "it reports actual distances as well as the approximate and tight upper bounds to the distances , i.e. , results  [ thm : errorsuboptpartpip ] to  [ thm : erroruniformpart ] .",
    "it can be observed that the @xmath0 distances between @xmath8 and @xmath5 using the uniform and optimized partitions agree well with results  [ thm : errorsuboptpartpip ] to  [ thm : erroruniformpart ] .",
    "all curves in  [ fig : l2-distance ] have similar slope to that of  , i.e. , their convergence rate is @xmath2 , and the ratio between the distances corresponding to @xmath7 and @xmath8 is approximately the value @xmath144 that stems from result  [ thm : minerrsmallortproj ] and  .    [ cols=\"<,<,^,^,^,^,^ \" , ]     sample detections using nonparametric modeling with cpwl - approximated gaussian kernels . subfigures",
    "( a ) and ( b ) show the original scene and results of the segmentation , respectively , for the tracking application ; subfigures ( c ) and ( d ) show the original scene and results of the segmentation , respectively , for the video - based interface application.,title=\"fig : \" ] + ( a )    sample detections using nonparametric modeling with cpwl - approximated gaussian kernels .",
    "subfigures ( a ) and ( b ) show the original scene and results of the segmentation , respectively , for the tracking application ; subfigures ( c ) and ( d ) show the original scene and results of the segmentation , respectively , for the video - based interface application.,title=\"fig : \" ] + ( c )    sample detections using nonparametric modeling with cpwl - approximated gaussian kernels",
    ". subfigures ( a ) and ( b ) show the original scene and results of the segmentation , respectively , for the tracking application ; subfigures ( c ) and ( d ) show the original scene and results of the segmentation , respectively , for the video - based interface application.,title=\"fig : \" ] + ( b )    sample detections using nonparametric modeling with cpwl - approximated gaussian kernels .",
    "subfigures ( a ) and ( b ) show the original scene and results of the segmentation , respectively , for the tracking application ; subfigures ( c ) and ( d ) show the original scene and results of the segmentation , respectively , for the video - based interface application.,title=\"fig : \" ] + ( d )      the performance of the approximation technique has also been tested on other functions . for example , the lorentzian function with peak at @xmath145 and width @xmath146 is @xmath147 as a probability distribution , is known as the cauchy distribution .",
    "it is an important function in physics since it solves the differential equation describing forced resonance . in such case",
    ", @xmath145 is the resonance frequency and @xmath146 depends on the damping of the oscillator ( and is inversely proportional to the @xmath148 factor , a measure of the sharpness of the resonance ) .",
    "@xmath0 distance for different cpwl approximations to the standard cauchy distribution @xmath149 in the interval @xmath150$].,width=317 ]    [ fig : graphicalerrorscauchy ] reports the @xmath0 distances between @xmath151   and the cpwl approximations described in previous sections , in the interval @xmath152 $ ] .",
    "the measured errors agree well with the predicted approximate error bounds , showing the expected @xmath2 trend .",
    "we have measured the mean per - evaluation execution times of the exact function both in the cpu ( 576  ps ) and in the gpu , both using regular and reduced - precision accelerated division ( 19.5  ps and 9.2  ps , respectively ) .",
    "the evaluation times of our proposed strategy coincide with those of the gaussian function ( table  [ tab : mean - per - evaluation - execution ] ) because the processing time of the cpwl approximation does not depend on the function values .    as expected from the lower complexity of this function ( compared to that of sec .",
    "[ sec : gaussian ] ) , which only involves elementary arithmetic operations , the advantage of our proposal vanishes in the cpu because the operations needed to manually perform interpolation , together with the two values that need to be fetched from memory , are actually more than what the direct evaluation requires . however , note that in the gpu our proposal still remains competitive due to these operations being carried out by the dedicated circuitry of the texture unit .",
    "approximating the bessel function of the first kind @xmath153 is more challenging than approximating the gaussian or lorentzian ( bell - shaped ) functions because it combines both nearly flat and oscillatory parts ( see  [ fig : besselj0 ] ) .",
    "[ fig : graphicalerrorsbesselj0 ] presents the @xmath0 approximation errors for @xmath153 using cpwl functions in the interval @xmath152 $ ] . for @xmath154",
    "the measured errors agree well with the predicted approximate error bounds , whereas for @xmath155 the measured errors slightly differ from the predicted ones ( specifically in the optimized partition ) because in these cases the scarce number of linear segments does not properly represent the oscillations .    a sample optimized partition and the corresponding cpwl interpolant @xmath156 is also represented in  [ fig : besselj0 ] .",
    "the knots of the partition are distributed according to the local knot density ( see  [ fig : besselj0 ] , top ) , accumulating in the regions of high oscillations ( excluding the places around the zeros of the @xmath118 ) .",
    "this function is more complex to evaluate in general because it does not have a closed form .",
    "however , our approximation algorithm works equally well on it .",
    "we have measured the mean per - evaluation execution times in the cpu ( 39  ns using the posix extensions of the gnu  c library and 130  ns  only double - precision provided  using the gnu  scientific library ) and in the gpu ( 78  ps using the cuda  @xcite standard library ) .",
    "we have also measured the execution time of the first term in the asymptotic expansion  ( * ? ? ?",
    "* eq .  9.57a ) , valid for @xmath157 , @xmath158 .",
    "$ ] in the gpu the evaluation takes 42  ps using reduced - precision accelerated functions from the sfu for the cosine and multiplicative inverse of the square root . despite this approximation having a non - customizable error ( decreasing as @xmath139 increases ) and using the fastest available implementation ( sfu ) of the operations involved , our strategy still outperforms it by a sizable margin .",
    "this clearly illustrates that the more complex the function to be evaluated is , the greater the advantage of our proposal .    * * suboptimal partition for the bessel function @xmath159 in the interval @xmath152 $ ] .",
    "top : local knot density ( @xmath118 ) corresponding to @xmath63 ; bottom : cpwl interpolant @xmath156 with @xmath160 ( @xmath161 knots ) overlaid on function @xmath63.,width=317 ]    @xmath0 distance for different cpwl approximations to the bessel function of the first kind @xmath159 in the interval @xmath152$].,width=317 ]",
    "we have developed a fast method to numerically evaluate any continuous mathematical function in a given interval using simpler continuous piecewise linear ( cpwl ) functions and the built - in routines present in the texture units of modern gpus .",
    "our technique allows real - time implementation of demanding computer vision and cybernetics applications that use such mathematical functions .    for this purpose",
    ", we analyzed the cpwl approximations given by the linear interpolant and the @xmath0 orthogonal projection of a function .",
    "we carried out a detailed error analysis in the @xmath0 distance to seek a nearly optimal design of both approximations . in the practical asymptotic case of a large number of subintervals @xmath1",
    ", we used error equalization to achieve a suboptimal design ( partition @xmath58 ) and derived a tight bound on the approximation error for the linear interpolant , showing a @xmath2 convergence rate that was confirmed by experimental results .",
    "the @xmath0 orthogonal projection can only but improve upon the results of the linear interpolant , resulting in a gain factor of @xmath4 .",
    "we discussed the computational complexity and the implementation in a gpu of the numerical evaluation of both cpwl approximations .",
    "our experimental results show that our technique can outperform both the quality and the computational cost of previous similar approaches . in particular , the fastest strategy consists of using the texture units in the gpu to evaluate either of the cpwl approximations defined over a uniform partition .",
    "this is normally faster than performing exact function evaluations , even when using the reduced - precision accelerated implementation of the sfus in a gpu , or evaluating the proposed linear approximations without the assistance of the texture units . in practice , the number of subintervals @xmath1 to be considered for representing the nonlinear function can be decided on its own or based on other considerations such as a target approximation error , speed or memory constraints .",
    "although mathematically sound , the strategies based on a suboptimal ( non - uniform ) partition are not practical to implement in current cpu / gpu architectures due to the high cost incurred to find the subinterval that contains the given point of evaluation .",
    "nevertheless , this opens future research paths to explore practical implementations of such approaches using specialized hardware .",
    "recall one of the theorems on interpolation errors  @xcite . let @xmath5 be a function in @xmath162 $ ] , and",
    "let @xmath163 be a polynomial of degree @xmath164 or less that interpolates the function @xmath5 at @xmath165 distinct points @xmath166 $ ] .",
    "then , for each @xmath167 $ ] there exists a point @xmath168 $ ] for which @xmath169    in the interval @xmath17 , the linear interpolant @xmath8 is given by  . since @xmath8 interpolates the function @xmath5 at the endpoints of @xmath17 , we can apply theorem   ( with @xmath170 ) ; hence , the approximation error solely depends on @xmath86 and @xmath139 , but not on @xmath5 or  @xmath171 : @xmath172    let us integrate the square of   over the interval @xmath17 , @xmath173    next , to simplify the previous integral , let us use the first mean value theorem for integration , which states that if @xmath174\\rightarrow\\r$ ] is a continuous function and @xmath175 is an integrable function that does not change sign on the interval @xmath176 , then there exists a number @xmath177 such that @xmath178    since @xmath179 and @xmath180 for all @xmath181 , let us apply   to compute @xmath182 for some @xmath183 .",
    "finally , if @xmath69 , it is straightforward to derive the @xmath0 error bound   from the square root of  .",
    "the approximation error corresponding to the line segment   is @xmath184 where , by analogy with the form  @xmath78 of @xmath185 we defined @xmath186 .",
    "the proof proceeds by computing the optimal values of @xmath80 and @xmath82 that minimize the squared @xmath0 error over the interval @xmath17 : @xmath187 the first term is given in  .",
    "the second term is @xmath188 and the third term is , applying   and the change of variables @xmath189 to evaluate the resulting integrals , @xmath190 for some @xmath191 and @xmath192 in @xmath85 .    substituting previous results in  ,",
    "@xmath193 we may now find the line segment that minimizes the distance to @xmath5 by taking partial derivatives with respect to @xmath80 and @xmath81 , setting them to zero and solving the corresponding system of equations .",
    "indeed , the previous error is quadratic in @xmath80 and @xmath81 , and attains its minimum at @xmath194 the resulting minimum squared distance is .",
    "by the triangle inequality , the jump discontinuity at @xmath195 between two adjacent independently - optimized segments is @xmath196 where @xmath197 and @xmath198 are the offsets with respect to @xmath199 of the optimized segments   at the left and right of @xmath195 , respectively ; @xmath200 and @xmath201 lie in the interval  @xmath202 .",
    "since we are dealing with functions twice continuously differentiable in a closed interval , the absolute value terms in   are bounded , according to the extreme value theorem ; therefore , if @xmath203 and @xmath204 decrease ( finer partition @xmath19 ) , the discontinuity jumps at the knots of the partition also decrease . in the limit , as @xmath205 , @xmath206 , i.e.",
    ", continuity is satisfied .",
    "therefore the union of the independently - optimized segments @xmath207 , which is the unique piecewise linear function satisfying both continuity ( @xmath208 ) and minimization of the @xmath0 error  .",
    "consequently , if @xmath1 is large we may approximate @xmath209 ; moreover , if @xmath17 is sufficiently small so that @xmath86 is approximately constant within it , @xmath87 , then we use corollary  [ thm : minerrsmalllinesegment ] to get  .",
    "v.  borges , m.  batista , and c.  barcelos , `` a soft unsupervised two - phase image segmentation model based on global probability density functions , '' in _ ieee int .",
    "systems , man and cybernetics ( smc ) _ , oct 2011 , pp . 16871692 .",
    "k.  kapinchev , f.  barnes , a.  bradu , and a.  podoleanu , `` approaches to general purpose gpu acceleration of digital signal processing in optical coherence tomography systems , '' in _ ieee int .",
    "systems , man and cybernetics ( smc ) _ , oct 2013 , pp .",
    "25762580 .",
    "j.  kim , e.  park , x.  cui , h.  kim , and w.  gruver , `` a fast feature extraction in object recognition using parallel processing on cpu and gpu , '' in _ ieee int .",
    "systems , man and cybernetics ( smc ) _ , oct 2009 , pp . 38423847 .",
    "s.  balla - arabe , x.  gao , and b.  wang , `` gpu accelerated edge - region based level set evolution constrained by 2d gray - scale histogram , '' _ ieee trans . image process .",
    "_ , vol .  22 , no .  7 , pp .",
    "26882698 , 2013 .",
    "s.  balla - arabe , x.  gao , b.  wang , f.  yang , and v.  brost , `` multi - kernel implicit curve evolution for selected texture region segmentation in vhr satellite images , '' _ ieee trans .",
    "remote sens .",
    "_ , vol .",
    "52 , no .  8 , pp .",
    "51835192 , aug 2014 .",
    "l.  sigal , m.  isard , h.  haussecker , and m.  black , `` loose - limbed people : estimating 3d human pose and motion using non - parametric belief propagation , '' _ int .",
    "j. comput . vision _ ,",
    "98 , no .  1 ,",
    "pp . 1548 , 2012 .",
    "c.  cuevas , r.  mohedano , and n.  garca , `` adaptable bayesian classifier for spatiotemporal nonparametric moving object detection strategies . ''",
    "_ optics letters _ , vol .",
    "37 , no .",
    "15 , pp . 31593161 , aug .",
    "2012 .",
    "d.  berjn , c.  cuevas , f.  morn , and n.  garca , `` gpu - based implementation of an optimized nonparametric background modeling for real - time moving object detection , '' _ ieee trans .",
    "_ , vol .",
    "59 , no .  2 , pp .",
    "361369 , may 2013 .",
    "m.  sylwestrzak , d.  szlag , m.  szkulmowski , i.  gorczyska , d.  bukowska , m.  wojtkowski , and p.  targowski , `` real time 3d structural and doppler oct imaging on graphics processing units , '' in _ proc .",
    "85710y.1em plus 0.5em minus 0.4emint . soc .",
    "optics and photonics , mar",
    ". 2013 .",
    "m.  sehili , d.  istrate , b.  dorizzi , and j.  boudy , `` daily sound recognition using a combination of gmm and svm for home automation , '' in _ proc .",
    "20th european signal process .",
    "_ , bucharest , 2012 , pp .",
    "16731677 .",
    "r.  tanjad and s.  wongsa , `` model structure selection strategy for wiener model identification with piecewise linearisation , '' in _ int .",
    ", telecommun . and",
    "( ecti - con ) _ , 2011 , pp . 553556",
    ".              s.  ghosh , a.  ray , d.  yadav , and b.  m. karan , `` a genetic algorithm based clustering approach for piecewise linearization of nonlinear functions , '' in _ int .",
    "devices and communications _ , 2011 , pp",
    "g.  gallego , d.  berjn , and n.  garca , `` optimal polygonal @xmath210 linearization and fast interpolation of nonlinear systems , '' _ ieee trans .",
    "circuits syst .",
    "i _ , vol .",
    "61 , no .",
    "11 , pp . 32253234 , nov 2014 .",
    "v.  k. pallipuram , n.  raut , x.  ren , m.  c. smith , and s.  naik , `` a multi - node gpgpu implementation of non - linear anisotropic diffusion filter , '' in _ symp .",
    "application accelerators in high performance computing _ , jul .",
    "2012 , pp . 1118 .          c.  cuevas , n.  garca , and l.  salgado , `` a new strategy based on adaptive mixture of gaussians for real - time moving objects segmentation , '' in _ proc",
    "6811.1em plus 0.5em minus 0.4emint . soc .",
    "optics and photonics , 2008 ."
  ],
  "abstract_text": [
    "<S> many computer vision and human - computer interaction applications developed in recent years need evaluating complex and continuous mathematical functions as an essential step toward proper operation . </S>",
    "<S> however , rigorous evaluation of this kind of functions often implies a very high computational cost , unacceptable in real - time applications . to alleviate this problem , </S>",
    "<S> functions are commonly approximated by simpler piecewise - polynomial representations . </S>",
    "<S> following this idea , we propose a novel , efficient , and practical technique to evaluate complex and continuous functions using a nearly optimal design of two types of piecewise linear approximations in the case of a large budget of evaluation subintervals . to this end </S>",
    "<S> , we develop a thorough error analysis that yields asymptotically tight bounds to accurately quantify the approximation performance of both representations . </S>",
    "<S> it provides an improvement upon previous error estimates and allows the user to control the trade - off between the approximation error and the number of evaluation subintervals . to guarantee real - time operation , the method is suitable for , but not limited to , an efficient implementation in modern graphics processing units ( gpus ) , where it outperforms previous alternative approaches by exploiting the fixed - function interpolation routines present in their texture units . the proposed technique is a perfect match for any application requiring the evaluation of continuous functions ; we have measured in detail its quality and efficiency on several functions , and , in particular , the gaussian function because it is extensively used in many areas of computer vision and cybernetics , and it is expensive to evaluate .    </S>",
    "<S> v # 1#2#1,#2 # 1y_#1 # 1y_#1    computer vision , image processing , numerical approximation and analysis , parallel processing , piecewise linearization , gaussian , lorentzian , bessel . </S>"
  ]
}