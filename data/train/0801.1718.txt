{
  "article_text": [
    "shannon s rate - distortion function @xmath0 for a stationary zero - mean gaussian source @xmath1 with memory and under the mse fidelity criterion can be written in a parametric form ( the reverse water - filling solution )  @xcite    [ eq : shannonsrdf ] @xmath2 where @xmath3 denotes the _ power spectral density _ ( psd ) of @xmath1 and the distortion psd @xmath4 is given by @xmath5    the water level @xmath6 is chosen such that the distortion constraint   is satisfied .",
    "it is well known that in order to achieve shannon s rdf in the quadratic gaussian case , the distortion must be independent of the output .",
    "this clearly implies that the distortion must be _ correlated _ to the source .",
    "interestingly , many well known source coding schemes actually lead , by construction , to source - uncorrelated distortions .",
    "in particular , this is the case when the source coder satisfies the following two conditions : a ) the linear processing stages ( if any ) achieve _ perfect reconstruction _ ( pr ) in the absence of quantization ; b ) the quantization error is uncorrelated to the source .",
    "the first condition is typically satisfied by pr filterbanks  @xcite , transform coders  @xcite and feedback quantizers  @xcite .",
    "the second condition is met when subtractive ( and often when non - subtractive ) dither quantizers are employed  @xcite .",
    "thus , any pr scheme using , for example , subtractively dithered quantization , leads to source - uncorrelated distortions .",
    "an important fundamental question , which was raised by the authors in a recent paper  @xcite , is : `` what is the impact on shannon s rate - distortion function , when we further impose the constraint that the end - to - end distortion must be uncorrelated to the input ? ''    in  @xcite",
    ", we formalized the notion of @xmath7 , which is the quadratic rate - distortion function subject to the constraint that the distortion is uncorrelated to the input . for a gaussian source @xmath8 , we defined @xmath9 as  @xcite @xmath10=\\boldsymbol{0 } , \\\\",
    "\\frac{1}{n } tr(\\boldsymbol{k}_{y - x } ) \\leq d ,    \\frac{1}{n}|\\boldsymbol{k}_{y - x}|^{\\frac{1}n } > 0 } } \\tfrac{1}{n } i(x ; y),\\ ] ] where the notation @xmath11 denotes the covariance matrix of @xmath1 and @xmath12 refers to the determinant . for zero mean gaussian stationary sources",
    ", we showed in  @xcite that the above minimum ( in the limit when @xmath13 ) satisfies the following equations :    [ eq : rperp_equations ] @xmath14 @xmath15    is the psd of the optimal distortion , which needs to be gaussian . notice that here the parameter @xmath16 ( akin to @xmath6 in  ) does not represent a `` water level '' . indeed , unless @xmath1 is white , the psd of the optimal distortion for @xmath9 is not white , _ for all @xmath17_. and shannon s @xmath0 are discussed in  @xcite . ]    in the present paper we prove achievability of @xmath7 by constructing coding schemes based on dithered lattice quantization , which , in the limit as the quantizer dimension approaches infinity , are able to achieve @xmath7 for any positive @xmath18 .",
    "we also show that @xmath9 can be realized causally , i.e. , that for all gaussian sources and for all positive distortions one can build forward test channels that realize @xmath9 without using non - causal filters .",
    "this is contrary to the case of shannon s rate distortion function @xmath0 , where at least one of the filters of the forward test channel that realizes @xmath0 needs to be non - causal  @xcite . to further illustrate the causality of @xmath9 , we present a causal transform coding architecture that realizes it .",
    "we also show that the use of feedback noise - shaping allows one to achieve @xmath9 with memoryless entropy coding .",
    "this parallels a recent result by zamir , kochman and erez for @xmath0  @xcite .",
    "we conclude the paper by showing that , in all the discussed architectures , the rate - loss ( with respect to @xmath9 ) when using a finite - dimensional quantizer can be upper bounded by the space - filling loss of the quantizer .",
    "thus , for any gaussian source with memory , by using noise - shaping and scalar dithered quantization , the _ scalar _ entropy ( conditioned to the dither ) of the quantized output exceeds @xmath9 by at most 0.254 bit / dimension .",
    "a randomized lattice quantizer is a lattice quantizer with subtractive dither @xmath19 , followed by entropy encoding .",
    "the dither @xmath20 is uniformly distributed over a voronoi cell @xmath21 of the lattice quantizer.due to the dither , the quantization error is truly independent of the input .",
    "furthermore , it was shown in  @xcite that the coding rate of the quantizer , i.e.@xmath22 can be written as the mutual information between the input and the output of an additive noise channel @xmath23 , where @xmath24 denotes the channel s additive noise and is distributed as @xmath25 .",
    "more precisely , @xmath26 and the quadratic distortion per dimension is given by @xmath27 .",
    "it has furthermore been shown that when @xmath19 is white there exists a sequence of lattice quantizers @xmath28 where the quantization error ( and therefore also the dither ) tends to be approximately gaussian distributed ( in the divergence sense ) for large @xmath29 .",
    "specifically , let @xmath24 have a probability distribution ( pdf ) @xmath30 , and let @xmath31 be gaussian distributed with the same mean and covariance as @xmath24 .",
    "then @xmath32 with a convergence rate of @xmath33 if the sequence @xmath28 is chosen appropriately  @xcite .    in the next section",
    "we will be interested in the case where the dither is not necessarily white . by shaping the voronoi cells of a lattice quantizer",
    "whose dither @xmath19 is white , we also shape @xmath19 , obtaining a colored dither @xmath34 . this situation was considered in detail in  @xcite from where we obtain the following lemma ( which was proven in  @xcite but not put into a lemma ) .",
    "[ lem : shapedlattice ] let @xmath35 be white , i.e.  @xmath36 is uniformly distributed over the voronoi cell @xmath21 of the lattice quantizer @xmath37 and @xmath38 .",
    "furthermore , let @xmath39 , where @xmath40 denotes the shaped voronoi cell @xmath41 and @xmath42 is some invertible linear transformation . denote the covariance of @xmath24 by @xmath43 .",
    "similarly , let @xmath44 having covariance matrix @xmath45 and let @xmath46 where @xmath47 .",
    "then there exists a sequence of shaped lattice quantizers such that @xmath48    the divergence is invariant to invertible transformations since @xmath49 .",
    "thus , @xmath50 for any @xmath29 .",
    "the simplest forward channel that realizes @xmath9 is shown in fig .  [",
    "fig : forwartdtc ] . according to  ,",
    "all that is needed for the mutual information per dimension between @xmath1 and @xmath51 to equal @xmath9 is that @xmath52 be gaussian with psd equal to the right hand side ( rhs ) of  .    in view of the asymptotic properties of randomized lattice quantizers discussed in section  [ sec : background ] ,",
    "the achievability of  @xmath9 can be shown by replacing the test channel of fig.[fig : forwartdtc ] by an adequately _ shaped _",
    "@xmath29-dimensional randomized lattice quantizer @xmath53 and then letting @xmath54 .",
    "in order to establish this result , the following lemma is needed .",
    "[ lem : excessrate ] _ let @xmath1 , @xmath55 , @xmath52 and @xmath56 be mutually independent random vectors .",
    "let @xmath55 and @xmath56 be arbitrarily distributed , and let @xmath1 and @xmath52 be gaussian having the same mean and covariance as @xmath55 and @xmath56 , respectively",
    ". then @xmath57 _",
    "@xmath58    where @xmath59 stems from the well known result @xmath60 , see , e.g. ,  @xcite .",
    "we can now prove the achievability of @xmath7 .",
    "+    [ thm : achievable ] _ for a source @xmath1 being an infinite length gaussian random vector with zero mean , @xmath7 is achievable . _",
    "let @xmath61 be the sub - vector containing the first @xmath29 elements of @xmath1 .",
    "for a fixed distortion @xmath62 , the average mutual information per dimension @xmath63 is minimized when @xmath64 and @xmath65 are jointly gaussian and @xmath66 see  @xcite .",
    "let the @xmath29-dimensional shaped randomized lattice quantizer @xmath67 be such that the dither is distributed as @xmath68 , with @xmath69 .",
    "it follows that the coding rate of the quantizer is given by @xmath70 .",
    "the rate loss due to using @xmath37 to quantize @xmath64 is given by @xmath71 \\nonumber\\\\ & \\overset{(a)}{\\leq } \\tfrac{1}{n}d(f_{{{e'}^{(n)}}}(e)\\|f_{{{e'_g}^{(n)}}}(e)),\\label{eq : middle}\\end{aligned}\\ ] ] where @xmath72 is the pdf of the gaussian random vector @xmath73 , independent of @xmath74 and @xmath64 , and having the same first and second order statistics as @xmath74 .",
    "in  , inequality  @xmath59 follows directly from lemma  [ lem : excessrate ] , since the use of subtractive dither yields the error @xmath74 independent of @xmath64 .    to complete the proof , we invoke lemma  [ lem : shapedlattice ] , which guarantees that the rhs of   vanishes as @xmath54 .    1 .   for zero mean stationary gaussian random sources , @xmath9 is achieved by taking @xmath1 in theorem  [ thm : achievable ] to be the complete input process . for this case , as shown in  @xcite , the fourier transform of the autocorrelation function of @xmath75 tends to the rhs of  .",
    "2 .   for vector processes ,",
    "the achievability of @xmath9 follows by building @xmath1 in theorem  [ thm : achievable ] from the concatenation of infinitely many consecutive vectors .",
    "3 .   note that if one has an infinite number of parallel scalar random processes , @xmath9 can be achieved _ causally _ by forming @xmath1 in theorem  [ thm : achievable ] from the @xmath76-th sample of each of the processes and using entropy coding after @xmath77 .",
    "the fact that @xmath9 can be realized causally is further illustrated in the following section .",
    "we will next show that for a gaussian random vector @xmath8 with positive definite covariance matrix @xmath11 , @xmath9 can be realized by _ causal _ transform coding  @xcite . a typical transform coding architecture",
    "is shown in fig .",
    "[ fig : causal_tcnf ] . in this figure",
    ", @xmath78 is an @xmath79 matrix , and @xmath80 is a gaussian vector , independent of @xmath1 , with covariance matrix @xmath81 .",
    "the system clearly satisfies the perfect reconstruction condition @xmath82 .",
    "the reconstruction error is the gaussian random vector @xmath83 , and the mse is @xmath84 , where @xmath85 .    by restricting @xmath78 to be lower triangular , the transform coder in fig .",
    "[ fig : causal_tcnf ] becomes causal , in the sense that @xmath86 , the @xmath76-th elements of @xmath87 and @xmath88 can be determined using just the first @xmath76 elements of @xmath1 and the @xmath76-th element of @xmath80 .    to have @xmath89 , it is necessary and sufficient that @xmath90 where the covariance matrix of the optimal distortion is  @xcite@xmath91    since @xmath92 is lower triangular ,   is the cholesky decomposition of @xmath93 , which always exists .",
    ", there exists a unique @xmath78 having only positive elements on its main diagonal that satisfies  , see  @xcite . ]",
    "thus , @xmath9 can be realized by causal transform coding .    in practice ,",
    "transform coders are implemented by replacing the ( vector ) awgn channel @xmath94 by a quantizer ( or several quantizers ) followed by entropy coding .",
    "the latter process is simplified if the quantized outputs are independent . when using quantizers with subtractive dither , this can be shown to be equivalent to having @xmath95 in the transform coder when using the awgn channel .",
    "notice that , since @xmath78 in   is invertible , the mutual information per dimension @xmath96 is also equal to @xmath9 . by the chain rule of mutual information",
    "we have @xmath97 with equality iff the elements of @xmath88 are mutually independent . if @xmath88 is gaussian , this is equivalent to @xmath98 being diagonal .",
    "clearly , this can not be obtained with the architecture shown in fig .",
    "[ fig : causal_tcnf ] using causal matrices ( while at the same time satisfying  ) .",
    "however , it can be achieved by using error feedback , as we show next .",
    "consider the scheme shown in fig .",
    "[ fig : causal_tc ] , where @xmath99 is lower triangular and @xmath100 is strictly lower triangular .",
    "again , a sufficient and necessary condition to have @xmath101 is that @xmath102 , see  , i.e. , @xmath103^{t } = { \\boldsymbol{k}}_{z^{\\star}}\\nonumber\\\\ \\iff ( { \\boldsymbol{i}}-{\\boldsymbol{f}})({\\boldsymbol{i}}-{\\boldsymbol{f}})^{t } = { \\boldsymbol{a}}{\\boldsymbol{k}}_{z^{\\star } } { \\boldsymbol{a}}^{t}/{\\sigma^{2}}_{w}. \\label{eq : i_f_i_f}\\end{aligned}\\ ] ] on the other hand , equality in   is achieved only if @xmath104 for some diagonal matrix @xmath105 with positive elements .",
    "if we substitute the cholesky factorization @xmath106 into  , we obtain @xmath107 , and thus @xmath108 substituting the above into   we obtain @xmath109({\\boldsymbol{i}}-{\\boldsymbol{f}})^{t}\\label{eq : the_one}\\end{aligned}\\ ] ] thus , there exist and @xmath106 , there exists a _ unique _",
    "matrix @xmath110 having zeros on its main diagonal that satisfies  , see  @xcite . ]",
    "@xmath111 and @xmath110 satisfying   and  .",
    "substitution of   into   yields @xmath112 , and @xmath113 . from   and the fact that @xmath114 it follows that @xmath115 , and therefore for gaussian vector sources derived in  @xcite . ] @xmath116 thus achieving equality in  .",
    "we have seen that the use of error feedback allows one to make the average scalar mutual information between the input and output of each awgn channel in the transform domain equal to @xmath9 . in the following section",
    "we show how this result can be extended to stationary gaussian processes .",
    "in this section we show that , for any colored stationary gaussian stationary source and for any positive distortion , @xmath9 can be realized by noise shaping , and that @xmath9 is achievable using _ memory - less _ entropy coding .      the fact that @xmath9 can be realized by the additive colored gaussian noise test channel of fig .",
    "[ fig : forwartdtc ] suggests that @xmath9 could also be achieved by an _ additive white gaussian noise _ ( awgn ) channel embedded in a noise - shaping feedback loop , see fig .",
    "[ fig : block_diag_nsdpcm ] . in this figure",
    ", @xmath117 is a gaussian stationary process with psd @xmath118 .",
    "the filters @xmath119 and @xmath120 are lti .",
    "the awgn channel is situated between @xmath121 and @xmath88 , where white gaussian noise @xmath122 , independent of @xmath117 , is added .",
    "the reconstructed signal @xmath51 is obtained by passing @xmath88 through the filter @xmath123 , yielding the reconstruction error @xmath124 .",
    "the following theorem states that , for this scheme , the _ scalar _ mutual information across the awgn channel can actually equal @xmath125 .",
    "[ thm : realizable_fq ] _ consider the scheme in fig .",
    "[ fig : block_diag_nsdpcm ] .",
    "let @xmath117 , @xmath122 be independent stationary gaussian random processes .",
    "suppose that the differential entropy rate of @xmath117 is bounded , and that @xmath122 is white .",
    "then , for every @xmath17 , there exist causal and stable filters @xmath119 , @xmath123 and @xmath120 such that @xmath126 _    consider all possible choices of the filters @xmath119 and @xmath120 such that the obtained sequence @xmath127 is white , i.e. , such that @xmath128}$ ] . from fig .",
    "[ fig : block_diag_nsdpcm ] , this is achieved iff the filters @xmath119 and @xmath120 satisfy @xmath129 on the other hand , since @xmath122 is gaussian , a necessary and sufficient condition in order to achieve @xmath9 is that @xmath130}.\\label{eq : szstar2}\\end{aligned}\\ ] ] this holds iff @xmath131 . substituting the latter and   into  , and after some algebra , we obtain    [ eq : opt_filters ] @xmath132^{2 } , \\label{eq : uno_min_f_opt }   \\\\ { \\left| a{({\\textrm{e}^{j{\\omega}}})}\\right|}^{2 }   & = 2{\\sigma^{2}}_{\\hat{u } } \\frac{{\\hksqrt{s_{x}{({\\textrm{e}^{j{\\omega}}})}\\!+\\ !",
    "\\alpha } } -\\ ! { \\hksqrt{s_{x}{({\\textrm{e}^{j{\\omega}}})}}}}{\\alpha { \\hksqrt{s_{x}{({\\textrm{e}^{j{\\omega}}})}}}}\\label{eq : aopt}.\\end{aligned}\\ ] ]    notice that the functions on the right hand sides of   are bounded and positive for all @xmath133}$ ] , and that a bounded differential entropy rate of @xmath117 implies that @xmath134 . from the paley - wiener criterion",
    "@xcite ( see also , e.g. ,  @xcite ) , this implies that @xmath135 , @xmath119 and @xmath123 can be chosen to be stable and causal .",
    "furthermore , recall that for any fixed @xmath17 , the corresponding value of @xmath16 is unique ( see  @xcite ) , and thus fixed .",
    "since the variance @xmath136 is also fixed , it follows that each frequency response magnitude @xmath137 that satisfies   can be associated to a unique value of @xmath138 .",
    "since @xmath120 is strictly causal and stable , the minimum value of the variance @xmath138 is achieved when @xmath139 i.e. , if @xmath140 has no zeros outside the unit circle ( equivalently , if @xmath140 is minimum phase ) , see , e.g. ,  @xcite .",
    "if we choose in   a filter @xmath120 that satisfies  , and then we take the logarithm and integrate both sides of  , we obtain @xmath141 } d{\\omega}\\\\ & = \\frac{1 } { 2\\pi}\\ ! { \\int\\limits_{-\\pi}^{\\pi } } { \\!\\log\\ ! \\left [ \\frac { { \\hksqrt{s_{x}{({\\textrm{e}^{j{\\omega}}})}\\!+\\ ! \\alpha } } + { \\hksqrt{s_{x}{({\\textrm{e}^{j{\\omega } } } ) } } } } { { \\hksqrt{\\alpha } } } \\right ] } d{\\omega}= r^{\\perp}(d).\\end{aligned}\\ ] ] where   has been used .",
    "we then have that @xmath142 where @xmath59 follows from the gaussianity of @xmath143 and @xmath144 , and @xmath145 from the fact that @xmath143 is independent of @xmath146 ( since @xmath147 is strictly causal ) .",
    "this completes the proof .",
    "alternatively , @xmath148 in  @xmath59 , equality is achieved iff the right hand side of   equals  , i.e. , if @xmath52 has the optimal psd .",
    "equality  @xmath145 holds because @xmath149 , which follows from  .",
    "the fact that @xmath127 is stationary has been used in  @xmath150 , wherein equality is achieved iff @xmath151 is minimum phase , i.e. , if   holds .",
    "equality in  @xmath152 holds if an only if the elements of @xmath127 are independent , which , from the gaussianity of @xmath127 , is equivalent to  .",
    "finally , @xmath153 stems from the fact that @xmath143 is independent of @xmath146 .",
    "notice that the key to the proof of theorem  [ thm : realizable_fq ] relies on knowing a priori the psd of the end to end distortion required to realize @xmath9 .",
    "indeed , one could also use this fact to realize @xmath9 by embedding the awgn in a dpcm feedback loop , and then following a reasoning similar to that in  @xcite .      in order to achieve @xmath9 by using a quantizer instead of an awgn channel",
    ", one would require the quantization errors to be gaussian .",
    "this can not be achieved with scalar quantizers .",
    "however , as we have seen in  [ sec : background ] , dithered lattice quantizers are able to yield quantization errors approximately gaussian as the lattice dimension tends to infinity",
    ". the sequential ( causal ) nature of the feedback architecture does not immediately allow for the possibility of using vector quantizers .",
    "however , if several sources are to be processed simultaneously , we can overcome this difficulty by using an idea suggested in  @xcite where the sources are processed in parallel by separate feedback quantizers .",
    "the feedback quantizers are operating independently of each other except that their scalar quantizers are replaced by a single vector quantizer .",
    "if the number of parallel sources is large , then the vector quantizer guarantees that the marginal distributions of the individual components of the quantized vectors becomes approximately gaussian distributed .",
    "thus , due to the dithering within the vector quantizer , each feedback quantizer observes a sequence of i.i.d .",
    "gaussian quantization noises .",
    "furthermore , the effective coding rate ( per source ) is that of a high dimensional entropy constrained dithered quantizer ( per dimension ) .",
    "the fact that the scalar mutual information between @xmath146 and @xmath144 equals the mutual information rate between @xmath154 and @xmath127 in each of the parallel coders implies that @xmath9 can be achieved by using a memoryless entropy coder .",
    "the results presented in sections  [ sec : realiz_tc ] and  [ sec : noise_shap ] suggest that if a test channel embedding an awgn channel realizes @xmath9 , then a source coder obtained by replacing the awgn channel by a dithered , finite dimensional lattice quantizer , would exhibit a rate close to @xmath9 .    the next theorem",
    ", whose proof follows the line of the results given in  @xcite , provides an upper bound on the rate - loss incurred in this case .    _",
    "consider a source coder with a finite dimensional subtractively dithered lattice quantizer @xmath77 .",
    "if when replacing the quantizer by an awgn channel the scalar mutual information across the channel equals @xmath9 , then the scalar entropy of the quantized output exceeds @xmath9 by at most @xmath155 bit / dimension .",
    "_    let @xmath80 be the noise of the awgn channel , and @xmath121 and @xmath88 denote the channel input and output signals . from the conditions of the theorem",
    ", we have that @xmath156 if we now replace the awgn by a dithered quantizer with subtractive dither @xmath19 , such that the quantization noise @xmath157 is obtained with the same first and second order statistics as @xmath80 , then the end to end mse remains the same .",
    "the corresponding signals in the quantized case , namely @xmath158 and @xmath159 , will also have the same second order statistics as their gaussian counterparts @xmath121 and @xmath88 .",
    "thus , by using lemma  [ lem : excessrate ] we obtain @xmath160 finally , from  ( * ? ? ?",
    "* theorem  1 ) , we have that @xmath161 . substitution of   into this last equation yields the result .",
    "we have proved the achievability of @xmath9 by using lattice quantization with subtractive dither .",
    "we have shown that @xmath9 can be realized causally , and that the use of feedback allows one to achieve @xmath9 by using memoryless entropy coding .",
    "we also showed that the scalar entropy of the quantized output when using optimal finite - dimensional dithered lattice quantization exceeds @xmath9 by at most @xmath155 bits / dimension .",
    "m.  s. derpich , j.  stergaard , and g.  c. goodwin , `` the quadratic gaussian rate - distortion function for source uncorrelated distortions , '' in _ proc .  of the data compression conference ,",
    "dcc _ , 2008 , to appear ( available from http://arxiv.org ) ."
  ],
  "abstract_text": [
    "<S> we prove achievability of the recently characterized quadratic gaussian rate - distortion function ( rdf ) subject to the constraint that the distortion is uncorrelated to the source . </S>",
    "<S> this result is based on shaped dithered lattice quantization in the limit as the lattice dimension tends to infinity and holds for all positive distortions . </S>",
    "<S> it turns out that this uncorrelated distortion rdf can be realized causally . </S>",
    "<S> this feature , which stands in contrast to shannon s rdf , is illustrated by causal transform coding . </S>",
    "<S> moreover , we prove that by using feedback noise shaping the uncorrelated distortion rdf can be achieved causally and with memoryless entropy coding . whilst achievability relies upon infinite dimensional quantizers , </S>",
    "<S> we prove that the rate loss incurred in the finite dimensional case can be upper - bounded by the space filling loss of the quantizer and , thus , is at most 0.254 bit / dimension . </S>"
  ]
}