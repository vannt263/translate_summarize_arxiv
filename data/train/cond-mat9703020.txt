{
  "article_text": [
    "one of the important features of feed - forward neural networks is their ability of learning a rule from examples @xcite . the student network can adopt its synaptic weights following a set of examples given from the teacher network so that it can make predictions on the output for an input which has been not shown before .",
    "learning of unlearnable rules by a perceptron is a particularly interesting issue because the student usually does not know the structure of the teacher in the real world . for machine learning ,",
    "it is important to improve the learning scheme and minimize the prediction error even if it is impossible to exactly reproduce the input - output relation of the teacher .",
    "only a few papers have appeared concerning learning of unlearnable rules where the teacher and the student have different structures @xcite . in this paper",
    "we study the generalization ability of a simple perceptron using the on - line algorithm from a teacher perceptron with a non - monotonic transfer function of reversed - wedge type that have been investigated as an associative memory @xcite and a perceptron @xcite .",
    "if a simple monotonic perceptron learns a rule from examples presented by a non - monotonic perceptron , the generalization error remains non - vanishing even if an infinite number of examples are presented by the teacher .",
    "we study the limiting value and asymptotic behaviour of the generalization error in such unlearnable cases .",
    "this paper is composed of nine sections . in the next section ,",
    "the problem is formulated and the general properties of generalization error are investigated .",
    "perceptron and hebbian learning algorithms in the on - line scheme are investigated in section 3 . for each learning scheme",
    ", we calculate the asymptotic behaviour of learning curve . in section 4",
    "we investigate the effects of output noise on learning processes .",
    "in section 5 we introduce the optimal learning rate and calculate the optimal generalization error . the optimal learning rate obtained in section 5",
    "contains an unknown parameter for the student in some contradiction to the idea of learning because the learning process depends upon the unknown teacher parameter .",
    "therefore , in section 6 we introduce a learning rate independent of the unknown parameter and optimize the rate to achieve a faster convergence of generalization error . in section 7 , we allow the student to ask queries under the hebbian learning algorithm . it is shown that learning is accelerated considerably if the learning rate is optimized . in section 8",
    "we optimize the learning dynamics by a weight - decay term to avoid an over - training problem in hebbian learning observed in section 3 .",
    "the last section contains summary and discussions .",
    "our problem is defined as follows .",
    "the teacher signal is provided by a single - layer perceptron with an @xmath0-dimensional weight vector @xmath1 and a non - monotonic ( reversed - wedge ) transfer function @xmath2 \\label{ta}\\ ] ] where @xmath3 , @xmath4 is the input vector normalized to unity , @xmath5 is the width of the reversed wedge , and @xmath6 denotes the sign function .",
    "the student is a simple perceptron with the weight vector @xmath7 whose output is @xmath8 where @xmath9 .",
    "the components of @xmath4 are drawn independently from a uniform distribution on the @xmath0-dimensional unit sphere .",
    "the student can learn the rule of the teacher perfectly if and only if @xmath10 .",
    "it is convenient to introduce the following two order parameters .",
    "one is the overlap between @xmath1 and @xmath7 @xmath11 and the other is the norm of the student weight vector @xmath12 in the limit @xmath13 the random variables @xmath14 and @xmath15 obey the normal distribution @xmath16 .",
    "\\label{pq}\\ ] ] the generalization error @xmath17 , or the student probability of producing a wrong answer , can be obtained by integrating the above distribution over the region satisfying @xmath18 in the two - dimensional @xmath14-@xmath15 space .",
    "after simple calculations we find @xmath19 where @xmath20 and @xmath21 . in figure 1",
    "we plot @xmath22 for several values of the parameter @xmath5 . from this figure",
    ", we see that for @xmath10 ( the learnable limit ) , @xmath17 goes to zero when @xmath23 approaches @xmath24 .",
    "in contrast , for @xmath25 , @xmath17 goes to zero when @xmath23 reaches @xmath26 . if @xmath5 is finite , the generalization error shows highly non - trivial behaviour .",
    "the critical value @xmath27 of the order parameter is defined as the point where @xmath28 is locally minimum .",
    "explicitly , @xmath29 which exists for @xmath30",
    ". we plot in figure 2 the value of the global minimum of @xmath28 , the smallest possible generalization error irrespective of learning algorithms . in figure 3",
    ", we show the value of @xmath23 which gives the global minimum .",
    "we notice that for @xmath31 , @xmath32 is also the global minimum , and for @xmath33 , the global minimum is @xmath34 . clearly the optimal generalization error is obtained by training the student weight vector @xmath7 so that @xmath23 goes to 1 ( or @xmath35 ) .",
    "this critical value @xmath36 is given by the condition @xmath37 .",
    "on the other hand , for @xmath38 , the optimal generalization can not be achieved even if the student succeeds in finding @xmath1 completely . in this curious case ,",
    "the optimal generalization is obtained by training the student so that the student finds his weight vector which satisfies @xmath39 instead of @xmath40 . at @xmath41",
    "the generalization error has the maximum value as seen in figure 2 .",
    "we now investigate the learning dynamics with specific learning rules .",
    "we first investigate the perceptron learning @xmath42 where @xmath43 is the step function and @xmath44 stands for the discrete time step of dynamics or the number of presented examples . the standard procedure ( see e.g. @xcite ) yields the rate of changes of @xmath45 and @xmath23 in the limit @xmath13 as @xmath46 \\label{dl } \\\\",
    "\\frac{\\d r}{\\d\\alpha}=\\frac{1}{l^2 } \\left[-\\frac{r}{2}e(r)+\\left ( f(r ) r - g(r)\\right ) l\\right ] \\label{dq}\\end{aligned}\\ ] ] where @xmath47 , @xmath48 and @xmath49 .",
    "the brackets @xmath50 stand for the averaging with respect to the distribution @xmath51 , the integration being carried out over the region where the student and the teacher give different outputs @xmath52 .",
    "hence the definition of @xmath28 coincides with that of the generalization error , @xmath53 , as used in the previous section .",
    "the other quantities @xmath54 and @xmath55 are evaluated in a straightforward manner as @xmath56 @xmath57 where @xmath58 .",
    "we have numerically solved equations and .",
    "the resulting flows of @xmath23 and @xmath45 are shown in figure 4 for @xmath10 under several initial conditions .",
    "this figure indicates that @xmath23 reaches @xmath24 ( perfect generalization state ) in the limit of @xmath59 and @xmath60 for any initial condition . for finite @xmath61 , however , behaviour of the flow strongly depends on the initial condition .",
    "if we take a large @xmath45 as the initial value , the perfect generalization state @xmath62 is achieved after @xmath45 decreases at intermediate steps .",
    "if we choose initial @xmath23 close to 1 and small @xmath45 , the perfect generalization is achieved after a decrease of @xmath23 is observed .",
    "similar phenomena have been reported in the @xmath63 parity machine @xcite .",
    "next we display the flows of @xmath23 and @xmath45 for unlearnable cases , for example , @xmath64 in figure 5 .",
    "there exists a stable and @xmath5-dependent fixed point @xmath65 .",
    "the generalization of the student halts at this fixed point even if the flow of @xmath23 and @xmath45 starts from @xmath40 and large @xmath45 .      when the rule is learnable ( @xmath10 ) , it is straightforward to check the asymptotic behaviour @xmath66 , @xmath67 , from equations and .",
    "when @xmath5 is finite , the fixed point value of @xmath23 is obtained from equations - as @xmath68 . substituting this @xmath69 into @xmath28 , we get the minimum value of the generalization error @xmath70 for perceptron learning . in figures 2 and 3 ,",
    "we show @xmath69 and @xmath71 as functions of @xmath5 .",
    "figure 2 indicates that the learning for @xmath72 , which is obtained from the condition @xmath73 , is equivalent to a random guess , @xmath74 .",
    "linearization of the right - hand side of equations and around the fixed point yields the behaviour of the generalization error near the fixed point .",
    "explicit expressions simplify when @xmath5 is large : it turns out that the generalization error decays toward the minimum value @xmath75 exponentially as @xmath76 .      in the hebbian rule the dynamics of the student weight vector",
    "is @xmath77 this recursion relation of the @xmath0-dimensional vector @xmath7 is reduced to the evolution equations of the order parameters as @xmath78    \\label{dlh}\\\\ \\frac{\\d r}{\\d\\alpha}=\\frac{1}{l^2}\\left [ -\\frac{r}{2}+\\frac{2}{\\sqrt{2\\pi}}(1 - 2{\\delta } ) ( 1-r^{2})l\\right ] .",
    "\\label{dqh}\\end{aligned}\\ ] ]      in figure 6 , we plot the flows in the @xmath23-@xmath45 plane and the generalization error for @xmath10 , @xmath79 and @xmath80 .",
    "we started the dynamics with the initial condition @xmath81 .",
    "this figure shows that @xmath23 reaches 1 for large @xmath5 and @xmath23 approaches @xmath26 for small @xmath5 . in order to find this bifurcation point near @xmath82 ,",
    "we approximate equation around @xmath83 as @xmath84 if @xmath85 , the derivative @xmath86 is positive , and consequently @xmath23 increases and eventually reaches @xmath24 in the limit @xmath87 . if @xmath88 , @xmath23 reaches @xmath26 as @xmath87 .",
    "figure 7 shows how the generalization error behaves according to @xmath5 . for @xmath89",
    ", @xmath17 has a minimum at some intermediate @xmath61 .",
    "when the generalization error @xmath17 passes through this value , @xmath17 begins to increase toward the limiting value @xmath90 . therefore ,",
    "if the student learns excessively , he can not achieve the lowest generalization error located at the global minimum of @xmath53 ( over - training ) @xcite . from figure 1",
    "we see that @xmath23 must pass through a local minimum of @xmath28 at @xmath39 in order to go to the state @xmath91 .",
    "if the parameter @xmath5 satisfies @xmath92 , this local minimum is also the global minimum .",
    "therefore , if @xmath88 , although the generalization error decreases until @xmath23 reaches @xmath27 , it begins to increase as soon as @xmath23 passes through the minimum point @xmath39 and finally reaches a larger value at @xmath91 .",
    "when the parameter @xmath5 lies in the range @xmath93 , the global minimum is located at @xmath40 .",
    "however , since @xmath23 goes to @xmath26 for @xmath94 ( see equation ) , the generalization error increases monotonically from @xmath95 ( random guess ) to @xmath96 for the parameter range @xmath97 .",
    "we can regard this as a special case of over - training .",
    "we conclude that over - training appears for all @xmath88 .      with the same technique as in the previous section ,",
    "we obtain the asymptotic form of the generalization error when @xmath10 in the limit @xmath87 as @xmath98 which is a well - known result @xcite . for finite @xmath5 satisfying @xmath99 , simple manipulations as before show that the stable fixed point is at @xmath40 and the differential equations and yield the asymptotic form of the generalization error as @xmath100 the limiting value @xmath101 is the best possible value obtained in section 2 . on the other hand , for @xmath88 , @xmath102 the rate of approach to the asymptotic value , @xmath103 , in equations and agrees with the corresponding behaviour in the gibbs learning of unlearnable rules @xcite .",
    "we now consider the situation where the output of the teacher is inverted randomly with a rate @xmath104 for each example .",
    "we show that the parameter @xmath5 plays essentially the same role as output noise in the teacher signal .      according to references @xcite , the effect of output noise",
    "is taken into account in the differential equations ( [ dl ] ) and ( [ dq ] ) by replacing @xmath28 , @xmath54 and @xmath55 with @xmath105 , @xmath106 and @xmath107 as follows @xmath108 where @xmath109 , @xmath110 and @xmath111 correspond to @xmath112 , @xmath113 and @xmath114 , the only difference being that the integration is over the region satisfying @xmath115 .",
    "we study the asymptotic behaviour of the learning curve in the limit of small noise level @xmath116 . for the learnable case @xmath10 , equations and with taken into account have the fixed point at @xmath117 , @xmath118 for @xmath119 .",
    "linearization around this fixed point leads to the asymptotic behaviour @xmath120 \\\\",
    "1-r   { \\sim }   ( 1-{r}_{0})\\left [ 1+{\\cal o}({\\rm e}^{-8{\\lambda}^{3/2}{\\alpha}})\\right ] .",
    "\\label{le0 } } \\ ] ] therefore , the generalization error @xmath17 converges to a finite value @xmath121 exponentially , @xmath122 .",
    "according to biehl @xcite , it is useful to distinguish two performance measures of on - line learning , the generalization error @xmath17 and the prediction error @xmath123 .",
    "the generalization error @xmath17 is the probability of disagreement between the student and the genuine rule of the teacher as we have discussed . on the other hand ,",
    "the prediction error @xmath123 is the probability for disagreement between the student and the noisy teacher output for an arbitrary input . in the present case ,",
    "the prediction error @xmath123 and generalization error @xmath17 satisfy the relation @xmath124 for the unlearnable case of large but finite @xmath5 under small noise level , the fixed point value of @xmath23 is found to be @xmath125 .",
    "the expression of the fixed point @xmath126 is too complicated and is omitted here .",
    "linearization near this fixed point shows that the generalization error converges to @xmath127 exponentially as @xmath128 for large @xmath5 and small @xmath129 , where @xmath130 the prediction error is given by @xmath131 .",
    "the differential equations of the order parameters for noisy hebbian learning are @xmath132    \\label{dlda32}\\\\ \\frac{\\d r}{\\d\\alpha } = \\frac{1}{l^2}\\left [ -\\frac{r}{2}+\\frac{2}{\\sqrt{2\\pi } } ( 1 - 2\\delta)(1 - 2\\lambda)(1-r^{2})l\\right ] .",
    "\\label{dqda32}\\end{aligned}\\ ] ] we plot the generalization error for @xmath80 in figure 8 by solving these differential equations numerically .",
    "we saw in the previous section that the over - training appears in the absence of noise if @xmath133 , which is also the case when there is small noise ( e.g. @xmath134 ) . for larger",
    "@xmath135 ( e.g. @xmath136 ) , however , there appears no minimum in @xmath17 as @xmath137 increases .",
    "this implies in terms of figure 1 that @xmath23 becomes stuck at an intermediate @xmath23 before it reaches @xmath27 .",
    "the asymptotic form for the noisy case can be derived simply by replacing @xmath138 in the asymptotic form of the noiseless case with @xmath139 .",
    "thus @xmath140 and @xmath135 have the same effect on the asymptotic generalization ability .",
    "a similar effect is reported for the non - monotonic hopfield model @xcite which works as an associative memory .",
    "if we embed patterns by the hebb rule in the network , the capacity of the network drastically deteriorates for small @xmath5 .",
    "we have so far investigated the learning processes with a fixed learning rate . in this section",
    "we consider optimization of the learning rate to improve the learning performance .",
    "it turns out that the perceptron learning with optimized learning rate achieves the best possible generalization error in the range @xmath141 .",
    "we first introduce the learning rate @xmath142 in our dynamics . as an example",
    ", the learning dynamics for the perceptron algorithm is written as @xmath143 this optimization procedure is different from the technique of kinouchi and caticha @xcite .",
    "they investigated the on - line dynamics with a general weight function @xmath144 as @xmath145 and chose @xmath146 so that it maximizes the increase of @xmath23 per learning step .",
    "in contrast , our optimization procedure adjusts the parameter @xmath142 keeping the learning algorithm unchanged .",
    "the trajectories in the @xmath23-@xmath45 plane can be derived explicitly for the optimal learning rate @xmath147 .",
    "the differential equations with the learning rate @xmath142 are @xmath148l}{l^{2 } } { \\equiv}l(g(\\alpha ) ) .",
    "\\label{dqda41}\\end{aligned}\\ ] ] now we choose the parameter @xmath149 to maximize @xmath150 with the aim to accelerate the increase of @xmath23 @xmath151 l}{re(r ) } .",
    "\\label{ga}\\ ] ] substituting this @xmath149 into equations ( [ dlda41 ] ) and ( [ dqda41 ] ) and taking their ratio , we find @xmath152\\,r}{\\left[f(r)\\,r+g(r)\\right]l } .",
    "\\label{dqda43}\\ ] ] using equations ( [ fq ] ) and ( [ gq ] ) we obtain the trajectory in the @xmath23-@xmath45 plane as @xmath153 where @xmath154 and @xmath155 is a constant .    in figures 9 and 10 ,",
    "we plot the above trajectory for @xmath64 and @xmath95 , respectively , by adjusting @xmath155 to reproduce the initial conditions @xmath156 and @xmath157 .",
    "these figures indicate that the student goes to the state of @xmath40 after infinite learning steps @xmath158 for any initial condition .",
    "the final value of @xmath45 depends on @xmath5 .",
    "if @xmath5 is small ( e.g. , 0.5 ) , @xmath45 increases indefinitely as @xmath159 . on the other hand , for larger @xmath5 , @xmath45 is seen to decrease as @xmath137 goes to @xmath160 .",
    "we investigate this @xmath5-dependence of @xmath45 in more detail in the next subsection .",
    "we plot the corresponding generalization error in figures 11 and 12 .",
    "we see that for @xmath64 , the generalization ability is improved significantly .",
    "however , for @xmath80 , the generalization ability becomes worse than that for @xmath161 ( the unoptimized case ) .",
    "we note that the above optimal learning rate @xmath147 contains the parameter @xmath5 unknown to the student .",
    "thus this choice of @xmath142 is not perfectly consistent with the principles of supervised learning .",
    "we will propose an improvement on this point in section 6 using a parameter - free learning rate .",
    "for the moment , we may take the result of the present section as a theoretical estimate of the best possible optimization result .",
    "let us first investigate the learnable case .",
    "the asymptotic forms of @xmath23 , @xmath45 , @xmath17 and @xmath149 as @xmath162 are obtained from the same analysis as in the previous section as @xmath163 , @xmath164 and @xmath165 where @xmath155 is a constant depending on the initial condition .",
    "the decay rate to vanishing generalization error is improved from @xmath166 for the unoptimized case @xcite to @xmath167 .",
    "this @xmath167-law is the same as in the off - line ( or batch ) learning @xcite .",
    "we also see that @xmath45 approaches @xmath155 as @xmath23 reaches @xmath24 .",
    "we next investigate the unlearnable case @xmath168 .",
    "the asymptotic forms are @xmath169 @xmath170 and the optimal learning rate @xmath171 is @xmath172 from the asymptotic form of @xmath45 , we find that @xmath45 diverges with @xmath137 for @xmath133 and goes to zero for @xmath99 as observed in the previous subsection .",
    "it is interesting that , for @xmath5 exactly equal to @xmath173 , @xmath171 vanishes and the present type of optimization does not make sense .    for @xmath174 , the generalization error converges to the optimal value @xmath101 as @xmath175 .",
    "this is the same exponent as that of the hebbian learning as we saw in the previous section . for @xmath38 , in order to get the optimal overlap @xmath39 , we must stop the on - line dynamics before the system reaches the state @xmath91 .",
    "accordingly , the method discussed in this section is not useful for the purpose of improvement of generalization ability for @xmath38 .",
    "the hebbian learning with learning rate @xmath176 is @xmath177 using the same technique as in the previous subsection , we find the optimal learning rate for the hebbian learning @xmath178 as @xmath179 the @xmath23-@xmath45 trajectory is @xmath180 where @xmath155 is a constant determined by the initial condition .",
    "it is very interesting that this trajectory is independent of @xmath5 .",
    "the asymptotic forms of various quantities for @xmath99 of the hebbian learning are @xmath181 and @xmath182 accordingly , for @xmath99 , the asymptotic form of the generalization error is the same as for @xmath161 . however , in the parameter region @xmath88 , the generalization ability deteriorates by introducing the optimal learning rate if we select an initial condition satisfying @xmath183 . to see this",
    ", we note that @xmath184 is approximated around @xmath82 as @xmath185 with using @xmath186 .",
    "therefore if we start the learning dynamics from @xmath183 , the overlap @xmath23 goes to @xmath24 and the generalization error approaches @xmath101 which is not acceptable at all because it exceeds 0.5 . on the other hand , for @xmath88 and @xmath187",
    ", the generalization error approaches @xmath188 ( less than 0.5 but not optimal ) as @xmath189 thus an over - training appears .",
    "we must notice that the prefactor of the generalization error changes from @xmath190 in equation to @xmath191 in equation by introducing the optimal learning rate .",
    "therefore the optimization by using the learning rate @xmath142 is not very useful for the hebbian learning .",
    "as we mentioned in section 5 , the generalization error obtained there is the theoretical ( not practical ) lower bound because the optimal learning rate @xmath171 contains a parameter @xmath5 unknown to the student . in this section",
    "we propose a method to avoid this difficulty for the perceptron learning algorithm . for the learnable case",
    "we choose the learning rate @xmath149 as @xmath192 which is nothing but the asymptotic form ( [ ga41 ] ) of the previous optimized learning rate . substituting this into equation with , we find @xmath163 when @xmath23 is close to unity and correspondingly @xmath193 which agrees with the result of barkai @xcite .",
    "for the unlearnable case , we assume @xmath194 as before and find the general solution for @xmath195 as @xmath196 where @xmath197 .",
    "the first term dominates asymptotically if @xmath198 . in this case",
    ", we have @xmath199 the second term on the right - hand side is minimized by choosing @xmath200 which satisfies @xmath198 as required .",
    "equation makes sense for @xmath201 if @xmath202 is chosen as above .",
    "when @xmath203 , the asymptotic form of the generalization error is @xmath204 this formula is valid for @xmath205 or @xmath88 .",
    "similar crossover between two types of asymptotic forms was reported in the problem of one - dimensional decision boundary @xcite .",
    "we have so far assumed that the student is trained using examples drawn from a uniform distribution on the @xmath0-dimensional sphere @xmath206 .",
    "it is known for the learnable case @xcite that selecting training examples out of a limited set sometimes improves the performance of learning .",
    "we therefore investigate in the present section how the method of kinzel and rujn @xcite works for an unlearnable rule .",
    "the learning dynamics we choose here is nothing but the hebbian algorithm . in section 3 ,",
    "the student was trained by inputs @xmath4 uniform on @xmath206 . in the present section",
    "we follow reference @xcite and use selected inputs which lie on the borderline , @xmath207 or @xmath208 , at every dynamical step . the idea behind",
    "this choice is that the student is not confident for inputs just on the decision boundary and thus teacher signals for such examples should be more useful than generic inputs .",
    "we use the following conditional distribution , instead of @xmath209 in equation , in order to get the differential equations @xmath210 using this distribution , we obtain the next differential equations @xmath211 @xmath212 . \\label{hq4}\\ ] ] in figure 13 , we plotted the generalization error for @xmath213 by numerical integration of the above differential equations .",
    "we see that the generalization ability of student is improved and the problem of over - training is avoided . in order to investigate the asymptotic form of the generalization error",
    ", we solve the differential equations in the limit of @xmath214 .",
    "equation can be solved easily as @xmath215 . for the learnable case @xmath216 , using @xmath217 and @xmath218 ,",
    "we obtain @xmath219 and the generalization error as @xmath220 the numerical prefactor has been reduced by a half from equation . for finite @xmath5 ,",
    "equation has fixed points at @xmath221 and @xmath222 the latter fixed point exists only for @xmath223 .",
    "thus , if @xmath99 , @xmath224 eventually approaches 1 , and the exponential term in equation can be neglected .",
    "this implies that the asymptotic analysis for the learnable case applies without modification .",
    "the resulting asymptotic form of the generalization error is @xmath225 if @xmath88 , the system is attracted to the fixed point @xmath226 according to the expansion of the right - hand side of equation around @xmath82 , @xmath227 which is negative if @xmath88 .",
    "it is remarkable that @xmath226 coincides with @xmath27 which gives the global minimum of @xmath28 for @xmath92 .",
    "therefore , for @xmath38 , the present hebbian learning with queries achieves the best possible generalization error . in the range @xmath93 , @xmath228 is not the global minimum of @xmath28 but is only a local minimum . however , as seen in figure 13 , over - training has disappeared in this region by introducing queries .    the asymptotic behaviour for @xmath88 is found to be @xmath229 \\nonumber\\\\",
    "\\times{\\exp}\\left [ -\\frac{8{\\log}2}{\\sqrt{\\pi}a } \\sqrt{2{\\log}2-a^{2 } } \\sqrt{\\alpha } \\right ] \\label{hq13}\\end{aligned}\\ ] ] where @xmath230 is the incomplete gamma function and the asymptotic value @xmath231 is optimal for @xmath232 .",
    "we next introduce the parameter @xmath149 into the hebbian learning with queries and optimize @xmath149 so that @xmath23 goes to @xmath24 as quickly as possible . as discussed in section 5 , this strategy works only for @xmath33 since @xmath40 is not the optimal value if @xmath38 . using the same technique as section 5 , we find the optimal learning rate as @xmath233 for the learnable case , the solution for @xmath23 is @xmath234 where @xmath155 is a constant .",
    "the generalization error decays to zero as @xmath235 where @xmath155 is determined by the initial condition .",
    "this exponential decrease for the learnable case is in agreement with reference @xcite where the optimization of the type of equation was used together with queries .",
    "the asymptotic forms of the order parameter @xmath45 and optimal learning rate @xmath171 are @xmath236 @xmath237 where @xmath238 is determined by the initial condition .",
    "next we investigate the case of finite @xmath5 . using the same asymptotic analysis as in the learnable case",
    ", we obtain the asymptotic form of generalization error @xmath17 as @xmath239 the limiting value @xmath101 is the theoretical lower bound for @xmath174 .",
    "we therefore have found a method of optimization to achieve the best possible generalization error with a very fast , exponential , asymptotic approach for @xmath33 .",
    "the present method of optimization does not work appropriately for @xmath38 because @xmath40 , to which the present method is designed to force the system , is not the best value of @xmath23 in this range of @xmath5 .",
    "it is worth investigating whether the exponent of decay changes or not by using a parameter - free optimal learning rate as in section 7 .",
    "if @xmath99 , there exists only one fixed point @xmath40 .",
    "therefore , the @xmath5-dependent term @xmath240 in equation does not affect the asymptotic analysis .",
    "we may therefore conclude that the asymptotic form of generalization error does not change by optimal learning rate without the unknown parameter @xmath5 .",
    "we showed in section 3 that the over - training appears for the unlearnable case @xmath88 by the hebbian learning .",
    "if @xmath88 , the flow of @xmath23 goes to @xmath26 for any initial condition passing through the local minimum of @xmath28 at @xmath39 . consequently , the generalization ability of the student decreases as he learns excessively . in order to avoid this difficulty",
    ", we must stop the dynamics on the way to the state @xmath91 . for this purpose",
    ", we may use the on - line dynamics with a weight - decay term or a forgetting term @xcite .",
    "the on - line dynamics by the hebbian rule is modified with the weight - decay term as @xmath241 the fixed point of the above dynamics is @xmath242 in order to get the optimal value , we choose @xmath69 so that it agrees with @xmath27 which gives the global minimum of @xmath28 for @xmath88 . from this condition",
    ", we obtain the optimal @xmath243 as @xmath244 using this @xmath245 , we solve the differential equations numerically and plot the result in figure 14 for @xmath89 .",
    "we see that the over - training disappears and the generalization error converges to the optimal value .",
    "we next investigate how fast this convergence is achieved .",
    "for this purpose , we linearize the differential equations around the fixed point to obtain @xmath246\\right\\}. \\label{e7 } \\ ] ] we warn here that @xmath245 in equation depends on @xmath5 which is unknown to the student .",
    "therefore , the result obtained in this section gives the theoretical upper bound of the generalization ability .",
    "we have analyzed the problem of on - line learning by the perceptron and hebbian algorithms . for the unlearnable case",
    ", the generalization error decays exponentially to a finite value @xmath247 with @xmath68 in the case of the perceptron learning . for the hebbian learning",
    ", the generalization error decays to @xmath101 , the best possible value , for @xmath99 and to @xmath188 for @xmath88 , both proportionally to @xmath248 . in this latter parameter region @xmath88",
    ", we observed the phenomenon of over - training .",
    "we also investigated the learning under output noise .",
    "for the learnable case of the perceptron algorithm , the order parameters @xmath23 and @xmath45 are attracted toward a fixed point @xmath65 asymptotically with an exponential law . as a result",
    ", the generalization error decays to a finite value exponentially . on the other hand ,",
    "for the unlearnable case of the perceptron learning , the generalization error decays exponentially to a finite value @xmath249 . for the hebbian learning",
    ", the generalization error decays to @xmath101 in proportion to @xmath103 for @xmath99 and to @xmath188 with also proportionally to @xmath103 for @xmath88 .",
    "we introduced the learning rate @xmath142 in on - line dynamics and optimized it to maximize @xmath250 . by this treatment",
    "we obtained a closed form trajectory of @xmath23 and @xmath45 .",
    "the generalization ability of the student has been shown to increase for @xmath174 in the case of the perceptron learning algorithm .",
    "for the unlearnable case , the generalization error decays to the best possible value @xmath101 in proportion to @xmath103 . for the hebbian learning ,",
    "the asymptotic generalization ability did not change by this optimization procedure .",
    "unfortunately , in the parameter range @xmath38 , we found it impossible to obtain an optimal performance for the perceptron learning within our procedure of optimization . to overcome this difficulty",
    ", we investigated the on - line dynamics with a weight - decay term for the hebbian learning . using this method",
    ", we could eliminate the over - training , and the generalization error converges to the optimal value exponentially .",
    "we also introduced a new learning rate independent of the unknown parameter @xmath5 .",
    "we assumed @xmath194 and optimized @xmath202 so that the generalization error decays to the minimum value as quickly as possible . as a result ,",
    "for the unlearnable case of @xmath99 the prefactor was somewhat improved although the exponent of decay did not change .",
    "the hebbian learning with queries was also investigated . if the student is trained by the hebbian algorithm using inputs on the decision boundary , his generalization ability",
    "is improved except in the range @xmath251 .",
    "this is a highly non - trivial result because this choice of query works well for the unlearnable case where student does not know the structure of the teacher .",
    "we next introduced the optimal learning rate in the on - line hebbian learning with queries and obtained very fast convergence of generalization error . for @xmath252 ,",
    "the generalization error converges to its optimal value exponentially .",
    "we have observed exponential decays to limiting values in various situations of unlearnable rules . this fast convergence may originate in the large size of the asymptotic space ; if the liming value of @xmath23 is unity , only a single point in the @xmath7-space , @xmath35 , is the correct destination of learning dynamics , a very difficult task .",
    "if , on the other hand , @xmath23 approaches @xmath253 , there are a continuous number of allowed student vectors , and to find one of these should be a relatively easy process , leading to exponential convergence .",
    "the authors gratefully acknowledge useful discussions with professor shun - ichi amari .",
    "hertz j a , krogh a and palmer r g 1991 _ introduction to the theory of neural computation _ , ( redwood city : addison - wesley ) watkin t h l , rau a and biehl m 1993 499 opper m and kinzel m 1995 in _ physics of neural networks _",
    "iii , eds . domany e , van hemmen j l and schulten k ( berlin : springer ) kim j w and sompolinsky h 1996 * 76 * 3021 saad d and solla s a 1995 e * 52 * 4225 watkin t l h and rau a 1992 a * 45 * 4111 morita m , yoshizawa s and nakano k 1990 _ trans . ieice _ *",
    "j73-d - ii * 242 nishimori h and opris i 1993 _ neural networks _ * 6 * 1061 inoue j 1996 4815 boffetta g , monasson r and zecchina r 1993 l507 monasson r and okane d 1994 _ europhys . lett . _",
    "* 27 * 85 kabashima y 1994 1917 biehl m and schwarze h 1992 _ europhys .",
    "lett . _ * 20 * 733 vallet f 1989 _ europhys .",
    "* 9 * 315 barkai n , seung h s and sompolinsky h 1995 _ proc.of advances in neural information processing system _ ( _ nips _ ) * 7 * 303 biehl m , riegler p and stechert m 1995 e * 52 * 4624 kinouchi o and caticha n 1992 6243 opper m , kinzel w , kleinz j and nehl r 1990 l581 kabashima y and shinomoto s 1995 _ neural comp . _ * 7 * 158 kinzel w and rujn p 1990 _ europhys ."
  ],
  "abstract_text": [
    "<S> we study the generalization ability of a simple perceptron which learns unlearnable rules . </S>",
    "<S> the rules are presented by a teacher perceptron with a non - monotonic transfer function . </S>",
    "<S> the student is trained in the on - line mode . </S>",
    "<S> the asymptotic behaviour of the generalization error is estimated under various conditions . </S>",
    "<S> several learning strategies are proposed and improved to obtain the theoretical lower bound of the generalization error .    </S>",
    "<S> [ on - line learning of non - monotonic rules ]    addtoresetequationsection </S>"
  ]
}