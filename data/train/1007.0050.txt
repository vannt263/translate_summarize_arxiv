{
  "article_text": [
    "infrastructure as a service ( iaas ) cloud computing is emerging as a new and efficient way to provide computing to the research community .",
    "clouds are considered to be a solution to some of the problems encountered with early adaptations of grid computing where the site retains control over the resources and the user must adapt their application to the local operating system , software and policies .",
    "this often leads to difficulties especially when a single resource provider must meet the demands of multiple projects or when projects can not conform to the configuration of the resource provider .",
    "iaas clouds offer a solution to these challenges by delivering computing resources using virtualization technologies .",
    "users lease the resources from the provider and install their application software within a virtual environment .",
    "this frees the providers from having to adapt their systems to specific application requirements and removes the software constraints on the user applications . in most cases ,",
    "it is easy for a user or a small project to create their virtual machine ( vm ) images and run them on iaas clouds .",
    "however , the complexity rapidly increases for projects with large user communities and significant computing requirements . in this paper",
    "we describe a system that simplifies the use of iaas clouds for high throughput computing ( htc ) workloads .",
    "the growing interest in clouds can be attributed in part to the ease in encapsulating complex research applications in virtual machines ( vms ) , often with little or no performance degradation @xcite .",
    "studies have shown , for example , that particle physics application code run equally well in a vm or on the native system @xcite .",
    "today , open source virtualization software such as xen @xcite and kvm @xcite are incorporated into many linux operating system distributions , resulting in the use of vms for a wide variety of applications .",
    "often , special purpose servers , particularly those requiring high availability or redundancy , are built inside a vm making them independent of the underlying hardware and allowing them to be easily moved or replicated .",
    "it is also not uncommon to find an old operating system lacking the drivers for new hardware , a problem which may be resolved by running the old software in a virtual environment .    the deployment and management of many vms in an iaas cloud is labour intensive",
    ". this can be simplified by attaching the vms to a job scheduler and utilizing the vms in a batch environment .",
    "the nimbus project @xcite has developed the _ one - click cluster _ solution .",
    "this provides a batch system on multiple clouds using one type of vm @xcite .",
    "we further simplify the management of vms in an iaas cloud and provide new functionality with the introduction of .",
    "provides a means of managing user - customized vms on any number of science and commercial clouds .    in the following sections we present the architecture of our system and",
    "highlight the role of to manage vms on iaas clouds in a batch htc environment .",
    "we present early results on the operation , and highlight the successes and issues of the system .",
    "we summarize the paper with a discussion of the future developments .",
    "the architecture of the htc environment running user - customized vms on distributed iaas clouds is shown in fig .",
    "[ fig : overall_arch ] .",
    "a user creates their vm and stores it in the _ vm image repository_. they write a job script that includes information about their vm and submits it to the .",
    "the reads the queues of the , requests that one of the available cloud resources boot the user vm , the vm advertises itself to the which then dispatches the user job to that vm .    in this section we describe the following components : _ vm image repository _ , the cloud resources and the .",
    "the is discussed in more detail in the following section .",
    "* vm image repository * + the user builds their vm by first retrieving a base image from a _ vm image repository_. the base images are simple linux vm images or images that include project or application based code .",
    "once the user has modified their image , they will store it back in the _ vm image repository_. the repository may reside at a single site or be distributed , however , it must be accessible to the cloud resources .",
    "* cloud resources * + the system currently supports amazon ec2 and iaas clouds using nimbus @xcite .",
    "support for iaas clouds using opennebula @xcite and eucalyptus @xcite is under development .    * job scheduler * + the job scheduler used in the system is the condor htc job scheduler @xcite .",
    "condor was chosen because it was designed to utilize heterogeneous idle workstations which makes it ideal to use as a job scheduler for a dynamic vm environment .",
    "condor has a central manager which matches user jobs to resources based on job and resource attributes .",
    "the condor _ startd daemon _ must be installed and started when a vm image is booted .",
    "the vm then advertises its existence to the condor central manager .",
    "users submit jobs by issuing the ` condor_submit ` command .",
    "the user must add a number of additional parameters specifying the location and properties of the vm .",
    "the description of the parameters is found in appendix i.",
    "the is an object oriented python - based package designed to manage vms for jobs based on the available cloud resources and job requirements .",
    "users submit jobs to the after they have been authenticated using x.509 proxy certificates @xcite .",
    "the certificates are also used to authenticate starting , shutting down , or polling vms with nimbus clusters .",
    "authentication with ec2 is done by using a standard shared access key and secret key .    in the following subsections ,",
    "we describe the s object classes and how they are used to manage the vms for the jobs .",
    "finally we discuss the job scheduling and load balancing considerations",
    ".      keeps track of jobs and resources with a set of cloud and job management classes ( see tables  [ table : cloudclasses ] and [ table : jobclasses ] ) .",
    "the cloud management classes include the _ resourcepool _ , _ cluster _ and _ vm _ classes .",
    "the _ resourcepool _ is a list of cloud resources that is read on initialization , but can be updated at run - time .",
    "the _ cluster _",
    "class contains static information describing the properties of each cloud and a dynamic list of _ vm _ objects running on that cloud .",
    "the _ vm _ class contains information describing the properties and state of a vm .",
    "the job management classes include the _ jobpool _ and _ job _ classes .",
    "the _ jobpool _ class contains a list of job objects that are derived from the jobs submitted to condor .",
    "the _ job _ class contains the properties of the user job .",
    "when is started it reads the general and cloud configuration files .",
    "it starts the following threads that are run on a periodic basis .    1 .",
    "the _ jobpoller _ thread maintains the state and metadata of the jobs that are queued and running on the .",
    "it effectively maps the queue into the _ jobpool_. 2 .",
    "the _ scheduler _ thread starts and stops vms based on the information in the _ jobpool _ , satisfying the resource demands of the workload .",
    "the design goal of is to leave prioritization and scheduling decisions in the domain of the .",
    "however , the order in which the _ scheduler _ thread provides resources can impact the scheduling algorithms of the .",
    "job scheduling and load balancing are discussed in more detail in the following section .",
    "+ the _ scheduler _ thread also monitors the vms and , if necessary , updates the state of vms in the _ jobpool_. it will shut down vms that are in an error state ; if there are jobs that still require this vm , then the _ scheduler _ will start a new instance of the vm to replace the one it has shut down .",
    "3 .   the _ cleanup _ thread stops vms that are no longer required . it can correct the state of the job in the _",
    "jobpool_. if a vm is shut down due to an error , then the _ cleanup _ thread changes the state of the job in the _ jobpool _ from `` scheduled '' to `` new '' so that a new vm can be created for that job .",
    "when is shut down , it can either shut down all the vms that is has started , or it can persist its state . in the latter case ,",
    "the vms continue to run the jobs .",
    "reloads the state when it is restarted and resumes managing the jobs and resources .",
    "the is designed to manage job prioritization and scheduling @xcite . as mentioned , can impact condor s job scheduling .",
    "for example , consider two queued jobs , the first submitted job requires a vm type of vm - a , and a second submitted job requires a vm type of vm - b . if cloud scheduler starts a vm - b first ( as the resources to boot a vm - b are available ) , then jobs requiring vm - b will run before jobs requiring vm - a .    can be configured to take into account user fairness , resource utilization , and user priorities .",
    "currently , will start as many vms that will fill the resources .",
    "the vms are evenly distributed among all users with jobs in the queue .",
    "as other users submit jobs , re - balances the vm allocations by shutting down over - allocated vms and starting under - allocated vms .",
    "for example , a single user will get the full allocation of s vms , but once a second user submits jobs , half of the first user s vms will be shut down to free resources for the second user .    can re - balance the vm distribution by shutting down vms gracefully or by killing them outright . when configured for graceful shutdown ,",
    "switches the condor - state of pending jobs requiring the over - allocated vms to a `` held '' state thus preventing them from being dispatched to currently running vms .",
    "the next vm of the over - allocated type to finish its job can be safely shutdown without affecting running jobs .",
    "when is configured to kill vms outright , the vms are shutdown immediately without waiting for the job to finish .",
    "if a vm is killed while a job is running , then the re - queues the job for execution .",
    "object & description + clusterlist & the list of _ cluster _ objects +   + object & description + name & the name of cluster + host & the hostname of cluster + cloud_type & the type of iaas software ( nimbus , eucalyptus , etc ) + memory & the ram available for a vm + cpu_archs & the cpu architectures available + networks & the network types available ( private or public or both ) + vm_slots & the maximum number of vms allowed on the cluster + cpu_cores & the maximum number of cpus allowed for a single vm + storage & the scratch space available + vms & the _ vm list _ +   + object & description + name & the name assigned to the vm + i d & the cluster - specific identifier + vmtype & the type of the vm + vmstate & the state of the vm ( starting , running or error ) + hostname & the hostname of the vm + clusteraddr & the address of the iaas head node that controls this vm + network & the type of networking of the vm ( private or public ) + cpuarch & the cpu architecture of the vm ( i386 or x86_64 ) + image & the vm image used to boot the vm + memory & the vm ram + cpucores & the number of cores in the vm + storage & the size of the scratch space used by the vm + errorcount & the number of times the vm has given an error response + lastpoll & the date / time of the latest update + last_state_change & the date / time of the last vm state change +     object & description + newlist & list of new job objects + scheduledlist & list of scheduled job objects +   + object & description + globaljobid & the condor job i d + user & the user that submitted the job + priority & the priority given in the job submission file ( default = 1 ) + vmtype & the vmtype + vmnetwork & the network required ( private / public ) + vmcpuarch & the cpu architecture(x86 or x86_64 ) + vmname & the name of the image the job is to run on + vmloc & the location ( url ) of the image + vmami & the amazon ami of the image + vmmem & the amount of memory in mb + vmcpucores & the number of cpu cores + vmstorage & the amount of storage space +",
    "the system is currently being used for particle physics and astronomy applications . at the moment",
    "we operate two independent systems for each research community . in addition",
    ", we are commissioning a cluster at the university of victoria that will be shared by a number of groups .",
    "the system is being used to generate simulated particle physics events for the babar experiment based at the stanford linear accelerator center ( slac ) .",
    "a number of faculty at the university of victoria are members of the collaboration and one of the responsibilities of the group is to use canadian computing resources for the generation of the simulated data . up to now",
    "we have used standalone facilities and also a grid of facilities @xcite .",
    "the simulation application contains c++ and fortran code .",
    "the current size of the vm is approximately 16 gbytes and requires about 1 gb of ram .",
    "the simulation requires access to calibration databases that vary in size up to 2 tb .",
    "the application accesses the databases on a modest but regular basis .",
    "the databases can be remote if the network connectivity is good . in canada",
    ", we are able to use the canarie network which connects research and educational institutions with a multi - gigabit network @xcite .",
    "our link to amazon ec2 is through the commodity network and for the time being , we have a copy of the databases on amazon storage to overcome the network limitation .",
    "typically the simulation requires 6 hours and produces an output file of 100 gb .",
    "all output files are copied back to the university of victoria where they are merged and sent to slac",
    ".    the simulation production is operated by a single expert user .",
    "there are no other users of the cloud system .",
    "the user prepares the job scripts and submits them to the .",
    "the manages 80 vms provided by three clouds located at the university of victoria , the national research council ( nrc ) of canada in ottawa and the amazon ec2 cloud in the eastern us .",
    "we limit the number of ec2 vms due to the cost , however , we are exploring the use of the ec2 on a variable basis dependent on the price .",
    "the system is performing very well completing more than 2000 seven - hour jobs in approximately one week .",
    "when jobs are submitted to the queue , the successfully boots as many vms as are required .",
    "once booted , the vms successfully identify themselves to the job scheduler and begin to run jobs in the queue . as expected ,",
    "the three cloud sites become a single distributed cloud , and simulation production proceeds identically as on a traditional cluster .",
    "issues that arose during the run were mainly centered around database access . to run efficiently , the jobs require fairly fast , low latency access to the databases .",
    "when the databases are hosted at the uvic site or the nrc site , the database access is over the canarie network described above , and the jobs are limited by the speed of the cpu rather than the i / o . however , when the jobs run on amazon ec2 , the network bandwidth between ec2 and the other two cloud sites is not sufficient for the jobs to run efficiently , and the jobs take approximately double the time to run to completion .",
    "this was solved by hosting a copy of the databases on amazon simple storage system ( s3 ) and accessing that copy from jobs running on ec2 .",
    "another issue involved the quality of the data being produced on ec2 .",
    "the babar collaboration places strict checks on simulation production ; any remote site that produces data for the collaboration must pass a series of tests that compare data produced at the remote site to a set of reference data produced at slac .",
    "when running on standard ec2 instances with older amd cpus , the data that was produced was different enough from the reference data to be unacceptable to the collaboration . running the jobs on amazon s",
    "`` high cpu '' instances with newer intel chips solved this problem .",
    "the system is also a central component of canfar @xcite , an astronomy project led by researchers at the university of victoria and the national research council of canada herzberg institute of astrophysics .",
    "the goal is to provide researchers the ability to create custom environments for analyzing data from survey projects .",
    "currently the system is available to early adopters , using two clusters : one at the westgrid facility located at the university of victoria ( 25 machines , 200 cores ) and one at the herzberg institute of astrophysics ( 6 machines , 32 cores ) .",
    "the system has been tested successfully with over 9,000 jobs utilizing more than 33,000 core hours .",
    "work is currently underway to streamline the process of creating and sharing virtual machines with researchers , to allow them to easily make vm images that suit their needs and ready for deployment on cloud resources .",
    "we have presented a new method for running large scale complex research applications on iaas computing clouds .",
    "the development of simplifies the management of virtual machine resources in a distributed cloud environment @xcite by hiding the complexity of vm management . we have demonstrated that the system works for both astronomy and particle physics applications using multiple cloud resources .",
    "we have shown that the system is robust and fault tolerant .",
    "we described our plans to further develop and address other issues for running htc workloads in a cloud environment .",
    ".... regular condor attributes universe                 = vanilla executable               = script.sh arguments                = one two three log                      = script.log output                   = script.out error                    = script.error should_transfer_files    = yes when_to_transfer_output = on_exit # # cloud scheduler attributes requirements =   + vmtype                  = \" vm - name \" + vmloc                   = \" http://repository.tld/your.vm.img.gz \" + vmami                   = \" ami - dfasfds \" + vmcpuarch               = \" x86 \" + vmcpucores              = \" 1 \" + vmnetwork               = \" private \" + vmmem                   = \" 512 \" + vmstorage               = \" 20 \" queue ....        a. agarwal , a. charbonneau , r. desmarais , r. enge , i. gable , d. grundy , d. penfold - brown , r. seuster , r.j .",
    "sobie , and d.c .",
    "vanderster . deploying hep applications using xen and",
    "globus virtual workspaces .",
    "119 , 062002 ( 2008 ) .",
    "doi : 10.1088/1742 - 6596/119/6/062002    p. barham , b. dragovic , k. fraser , s. hand , t. harris , a. ho , r. neugebauer , i. pratt and a. warfield .",
    "xen and the art of virtualization .",
    "sosp03 : proceedings of the nineteenth acm symposium on operating systems principles .",
    "d. nurmi , r. wolski , c. grzegorczyk , g. obertelli , s. soman , l. youseff , and d. zagorodnov .",
    "the eucalyptus open - source cloud - computing system .",
    "ccgrid 09 : proceedings of the 2009 9th ieee / acm international symposium on cluster computing and the grid .",
    "doi:10.1109/ccgrid.2009.93        a. agarwal , m. ahmed , a. berman , b.l .",
    "caron , a. charbonneau , d. deatrich , r. desmarais , a. dimopoulos , i. gable , l.s .",
    "groer , r. haria , r. impey , l. klektau , c. lindsay , g. mateescu , q. matthews , a. norton , w. podaima , d. quesnel , r. simmonds , r.j .",
    "sobie , b.st .",
    "arnaud , c. usher , d.c .",
    "vanderster , m. vetterli , r. walker , m. yuen .",
    "gridx1 : a canadian computational grid .",
    "future generation computer systems * 23 * ( 2007 ) 680 .",
    "doi:10.1016/j.future.2006.12.006    a. agarwal , p. armstrong , r. desmarais , i. gable , s. popov , s. ramage , s. schaffer , c. sobie , r.j .",
    "sobie , t. sulivan , d.c .",
    "vanderster , g. mateescu , w. podaima , a. charbonneau , r. impey , m. viswanathan , d. quesnel , babar mc production on the canadian grid using a web services approach . j. phys .",
    "( 2007 ) 072002 .",
    "doi : 10.1088/1742 - 6596/119/7/072002"
  ],
  "abstract_text": [
    "<S> the availability of infrastructure - as - a - service ( iaas ) computing clouds gives researchers access to a large set of new resources for running complex scientific applications . </S>",
    "<S> however , exploiting cloud resources for large numbers of jobs requires significant effort and expertise . in order to make it simple and transparent for researchers to deploy their applications , </S>",
    "<S> we have developed a virtual machine resource manager ( ) for distributed compute clouds . </S>",
    "<S> boots and manages the user - customized virtual machines in response to a user s job submission . </S>",
    "<S> we describe the motivation and design of the and present results on its use on both science and commercial clouds .    * cloud scheduler : a resource manager for distributed compute clouds *    p. armstrong@xmath0 , a. agarwal@xmath0 , a. bishop@xmath0 , a. charbonneau@xmath1 , r. desmarais@xmath0 , k. fransham@xmath0 , n. hill@xmath2 , i. gable@xmath0 , s. gaudet@xmath2 , s. goliath@xmath2 , r. impey@xmath1 , c. leavett - brown@xmath0 , j. ouellete@xmath2 , m. paterson@xmath0 , c. pritchet@xmath0 , d. penfold - brown@xmath0 , w. podaima@xmath1 , d. schade@xmath2 , r.j . </S>",
    "<S> sobie@xmath3    @xmath0 department of physics and astronomy , university of victoria , victoria , canada v8w 3p6 + @xmath1 national research council canada , 100 sussex drive , ottawa , canada + @xmath2 national research council herzberg institute of astrophysics , 5071 west saanich road , victoria , bc , canada , v9e 3e7 + @xmath4 institute of particle physics of canada . </S>"
  ]
}