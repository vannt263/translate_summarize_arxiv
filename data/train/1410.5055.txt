{
  "article_text": [
    "compressed sensing is a recently emerged technique for signal sampling and data acquisition which enables to recover sparse signals from much fewer linear measurements @xcite . in this paper",
    ", we study the problem of sparse signal recovery when prior information on the signal s partial support is available . in practice ,",
    "prior information about the support region of the sparse signal may come from the support estimate obtained during a previous time instant .",
    "this is particularly the case for time - varying sparse signals whose support changes slowly over time . for example , in the real - time dynamic magnetic resonance imaging ( mri ) reconstruction , it was shown that the support of a medical image sequence undergoes small variations with support changes ( number of additions and removals ) less than @xmath3 of the support size @xcite .",
    "also , in the source localization problem , the locations of sources may vary slowly over time .",
    "thus the previous estimate of locations can be used as prior knowledge to enhance the accuracy of the current estimate .",
    "the problem of sparse signal recovery with partial support information was studied in several independent and parallel works @xcite . in @xcite ,",
    "the partly known support is utilized to obtain a least - squares residual and compressed sensing is then performed on the least - squares residual instead of the original observation . when the partly known support is accurate , it is expected that the sparse signal associated with the residual has much fewer large nonzero components . hence the least - squares residual - based compressed sensing can improve the recovery performance . later in @xcite , a weighted @xmath4-minimization ( modified basis pursuit )",
    "method was proposed , where the partially known support is excluded from the @xmath4-minimization ( equivalent to setting the corresponding weights to zero ) .",
    "sufficient conditions for exact reconstruction were derived , and it was shown that when a fairly accurate estimate of the true support is available , the exact reconstruction conditions are much weaker than the sufficient conditions for compressed sensing without utilizing the prior information @xcite .",
    "this work was later extended to the noisy case @xcite , where a regularized modified basis pursuit denoising method was introduced and a computable bound on the reconstruction error was obtained .",
    "the weighted @xmath4-minimization approach was also studied in @xcite by assuming a probabilistic support model which assigns a probability of being zero or nonzero to each entry of the sparse signal . the choice of the weights and the associated exact recovery conditions were investigated . in @xcite , a modified iterative reweighted method which incorporates the prior support information was proposed .",
    "similar to @xcite , those weights corresponding to the _ a priori _ known support are assigned a very small value ( close to zero ) , whereas other weights are recursively updated like conventional iterative reweighted methods .",
    "the problem of support knowledge - aided sparse signal recovery was also studied in a time - varying compressed sensing framework , e.g. @xcite , where the temporal support correlation was modeled by a markov chain @xcite or a pattern - coupled structure @xcite .",
    "nevertheless , both works need to specify the inherent support correlation structure between two consecutive time steps , whereas our work here deals with a more general scenario involving no particular correlation structure between the prior support and the current support .",
    "it has been observed @xcite that the sparse recovery performance can be significantly improved through exploiting the prior support knowledge .",
    "nevertheless , this improvement can only be achieved when the prior knowledge is fairly accurate .",
    "existing methods , e.g. @xcite , may suffer from severe recovery performance degradation or even recovery failure in the presence of inaccurate prior knowledge . in practice",
    ", however , signal support estimation inevitably incurs errors , and in some cases , due to the support variation across time , the prior knowledge may contain a considerable amount of errors . in this paper , we first introduce a modified two - layer gaussian - inverse gamma prior model , where an individual parameter @xmath0 , instead of a common parameter @xmath5 used in the conventional framework , is employed to characterize each sparsity - controlling hyperparameter @xmath1 . through assigning different values to the parameters @xmath2 , our new prior model has the flexibility to place non - sparsity - encouraging priors to those coefficients that are believed in the support set . nevertheless , the above modified two - layer hierarchical model does not have a mechanism to learn the true support from the partly erroneous knowledge . to address this issue",
    ", we propose an improved hierarchical prior model which constitutes a three - layer hierarchical form , where a new layer is proposed in addition to the above modified two - layer hierarchical model .",
    "the new layer places a prior on the parameters @xmath2 .",
    "such an approach is capable of distinguishing the true support from erroneous support through learning the values of @xmath2 . by resorting to the variational inference methodology",
    ", we develop a new sparse bayesian learning method which has the ability to learn the true support from the erroneous information .",
    "the rest of the paper is organized as follows . in section [ sec : model ] , we introduce a modified two - layer gaussian - inverse gamma hierarchical prior model and an improved three - layer hierarchical prior model which enables to learn the true support from partly erroneous knowledge .",
    "variational bayesian methods are developed in section [ sec : inference ] .",
    "simulation results are provided in section [ sec : simulation ] , followed by concluding remarks in section [ sec : conclusion ] .",
    "we consider the problem of recovering a sparse signal @xmath6 from noise - corrupted measurements @xmath7 where @xmath8 ( @xmath9 ) is the measurement matrix , and @xmath10 is the additive multivariate gaussian noise with zero mean and covariance matrix @xmath11 .",
    "suppose we have partial but partly erroneous knowledge of the support of the sparse signal @xmath12 .",
    "the prior knowledge @xmath13 can be divided into two parts : @xmath14 , where @xmath15 denotes the subset containing correct information and @xmath16 denotes the error subset .",
    "if we let @xmath17 denote the true support of @xmath12 and @xmath18 denote the complement of the set @xmath17 , i.e. @xmath19 , then we have @xmath20 , and @xmath21 .",
    "note that the only prior information we have is @xmath13 .",
    "the partition of @xmath15 and @xmath16 is unknown to us .",
    "the prior support information can certainly be utilized to improve signal recovery . in the following ,",
    "based on the conventional sparse bayesian learning ( sbl ) framework , we first introduce a modified sbl hierarchical prior model which has the flexibility to place non - sparsity - encouraging priors to those coefficients that are believed in the support set .",
    "furthermore , we propose an improved three - layer hierarchical prior model which enables to learn the true support from the erroneous information and thus exploits the prior support information in a more constructive manner . to facilitate discussions and comparisons , we first provide a brief overview of the conventional sbl hierarchical model .      sparse bayesian learning was originally developed by tipping in @xcite to address regression and classification problems .",
    "later on in @xcite , sparse bayesian learning was adapted to solve the sparse recovery problem and obtained superior performance for sparse signal recovery in a series of experiments . in the conventional sparse bayesian learning framework ,",
    "a two - layer hierarchical prior model was proposed to promote the sparsity of the solution . in the first layer , @xmath12 is assigned a gaussian prior distribution @xmath22 where @xmath23 , and @xmath24 , the inverse variance ( precision ) of the gaussian distribution , are non - negative sparsity - controlling hyperparameters .",
    "the second layer specifies gamma distributions as hyperpriors over the hyperparameters @xmath25 , i.e. @xmath26 where @xmath27 is the gamma function , the parameters @xmath28 and @xmath5 used to characterize the gamma distribution are chosen to be very small values , e.g. @xmath29 , in order to provide non - informative / uniform ( over a logarithmic scale ) hyperpriors over @xmath25 .",
    "as discussed in @xcite , a broad hyperprior allows the posterior mean of @xmath1 to become arbitrarily large . as a consequence",
    ", the associated coefficient @xmath30 will be driven to zero , thus yielding a sparse solution .",
    "this mechanism is also referred to as the `` automatic relevance determination '' mechanism which tends to switch off most of the coefficients that are deemed to be irrelevant , and only keep very few relevant coefficients to explain the data .",
    "when the value of the parameter @xmath5 is relatively large , e.g. @xmath31 , it can be readily observed from ( [ hm-2 ] ) that the hyperpriors are no longer uniform and now they encourage small values of @xmath25 . in this case , an arbitrarily large value of the posterior mean of @xmath1 is prohibited . as a result",
    ", the two - layer hierarchical model no longer results in a sparsity - encouraging prior and therefore does not necessarily lead to a sparse solution .",
    "this fact inspires us to develop a new way to incorporate the prior support information into the sparse bayesian learning framework .",
    "specifically , instead of using a common parameter @xmath5 for all sparsity - controlling hyperparameters @xmath25 , we hereby employ an individual parameter @xmath0 for each hyperparameter @xmath1 , i.e. @xmath32 such a formulation allows us to assign different priors to different coefficients .",
    "if the partial knowledge of the signal s support , @xmath13 , is available , then the associated parameters of @xmath2 can be set to a relatively large value , say @xmath33 , in order to place a non - sparsity - encouraging prior on the corresponding coefficients , whereas the rest parameters of @xmath2 are still assigned a small value , say @xmath29 , to encourage sparse coefficients , that is , @xmath34 where @xmath35 denotes the complement of @xmath13 , i.e. @xmath36 . the above modified hierarchical model seamlessly integrates the prior support information into the sparse bayesian learning framework .",
    "nevertheless , the modified two - layer hierarchical model which assigns fixed values to @xmath2 still lacks the flexibility to learn and adapt to the true situation .",
    "when the prior information contains a considerable portion of errors , this approach may suffer from significant performance loss and even recovery failure .",
    "to address the above issue , we partition the parameters @xmath2 into two subsets : @xmath37 , and @xmath38 . for @xmath39 , the parameters are still considered to be deterministic and assigned a very small value , e.g. @xmath29 ; while for @xmath37 , instead of assigning a fixed large value , we model them as random parameters and place hyperpriors over these parameters . since @xmath37 are positive values , suitable priors over @xmath37 are also gamma distributions . in summary , we have @xmath40 where @xmath41 denotes the dirac delta function , @xmath42 and @xmath43 are parameters characterizing the gamma distribution and their choice will be discussed in section [ sec : inference ] .",
    "we see that the proposed model constitutes a three - layer hierarchical form which allows to learn the parameters @xmath44 in an automatic manner from the data .",
    "the proposed algorithm based on this prior model therefore has the ability to identify the correct support from erroneous information .",
    "we now proceed to perform variational bayesian inference for the proposed hierarchical models . throughout this paper ,",
    "the noise variance @xmath45 is assumed unknown , and needs to be estimated along with other parameters . for notational convenience , define @xmath46 following the conventional sparse bayesian learning framework @xcite , we place a gamma hyperprior over @xmath47 : @xmath48 where the parameters @xmath49 and @xmath50 are set to small values , e.g. @xmath51 . before proceeding ,",
    "we firstly provide a brief review of the variational bayesian methodology .      in a probabilistic model ,",
    "let @xmath52 and @xmath53 denote the observed data and the hidden variables , respectively .",
    "it is straightforward to show that the marginal probability of the observed data can be decomposed into two terms @xmath54 where @xmath55 and @xmath56 where @xmath57 is any probability density function , @xmath58 is the kullback - leibler divergence between @xmath59 and @xmath57 . since @xmath60",
    ", it follows that @xmath61 is a rigorous lower bound on @xmath62 .",
    "moreover , notice that the left hand side of ( [ variational - decomposition ] ) is independent of @xmath57 .",
    "therefore maximizing @xmath61 is equivalent to minimizing @xmath58 , and thus the posterior distribution @xmath59 can be approximated by @xmath57 through maximizing @xmath61 .",
    "the significance of the above transformation is that it circumvents the difficulty of computing the posterior probability @xmath59 directly ( which is usually computationally intractable ) . for a suitable choice for the distribution @xmath57",
    ", the quantity @xmath61 may be more amiable to compute .",
    "specifically , we could assume some specific parameterized functional form for @xmath43 and then maximize @xmath61 with respect to the parameters of the distribution .",
    "a particular form of @xmath57 that has been widely used with great success is the factorized form over the component variables @xmath63 in @xmath53 , i.e. @xmath64 we therefore can compute the posterior distribution approximation by finding @xmath57 of the form ( [ factorization ] ) that maximizes the lower bound @xmath61 .",
    "the maximization can be conducted in an alternating fashion for each latent variable , which leads to @xcite @xmath65 where @xmath66 denotes an expectation with respect to the distributions @xmath67 for all @xmath68 .",
    "let @xmath69 denote all hidden variables .",
    "we assume posterior independence among the variables @xmath12 , @xmath70 , and @xmath47 , i.e. @xmath71 with this mean field approximation , the posterior distribution of each hidden variable can be computed by maximizing @xmath61 while keeping other variables fixed using their most recent distributions , which gives @xmath72 where @xmath73 denotes the expectation with respect to ( w.r.t . ) the distribution @xmath74 . in summary , the posterior distribution approximations are computed in an alternating fashion for each hidden variable , with other variables fixed .",
    "details of this bayesian inference scheme are provided below .    * _ 1 ) .",
    "update of @xmath75 _ * : keeping only the terms that depend on @xmath12 , the variational optimization of @xmath76 yields @xmath77 where @xmath78 denotes the expectation w.r.t .",
    "@xmath79 , and @xmath80 , in which @xmath81 represents the expectation w.r.t .",
    "it can be readily verified that @xmath75 follows a gaussian distribution with its mean @xmath83 and covariance matrix @xmath84 given respectively as @xmath85    * _ 2 ) .",
    "update of @xmath86 _ * : similarly , the approximate posterior @xmath86 can be obtained by computing @xmath87 where @xmath88 denotes the expectation w.r.t . @xmath75 .",
    "thus @xmath70 has a form of a product of gamma distributions @xmath89 in which the parameters @xmath90 and @xmath91 are respectively given as @xmath92    * _ 3 ) .",
    "update of @xmath93 _ * : the approximate posterior distribution @xmath93 can be computed as @xmath94 we can easily see that @xmath95 follows a gamma distribution @xmath96 with the parameters @xmath97 and @xmath98 given respectively by @xmath99 where @xmath100    in summary , the variational bayesian inference involves updates of the approximate posterior distributions for hidden variables @xmath12 , @xmath70 , and @xmath47 . some of the expectations and moments used during the update are summarized as @xmath101 where @xmath102 denotes the @xmath103th element of @xmath83 , and @xmath104 denotes the @xmath103th diagonal element of @xmath84 . for clarity ,",
    "we summarize our algorithm as follows .    *",
    "support aided - sbl with no support learning *    [ cols= \" < , < \" , ]     the above proposed algorithm has the ability to adaptively learn the values of @xmath105 , and thus has the potential to distinguish the correct support from erroneous information . to gain insight into the algorithm",
    ", we examine the update rules for @xmath106 and @xmath107 , i.e. the posterior means of @xmath1 and @xmath0 .",
    "the update rules are given by @xmath108 and @xmath109 respectively .",
    "we see that the two update rules are related to each other , with @xmath107 used in updating @xmath106 and @xmath106 used in obtaning a new @xmath107 .",
    "a closer examination reveals that @xmath106 and @xmath107 are inversely proportional to each other in their respective update rules . specifically , a smaller @xmath107 results in a larger @xmath106 ( c.f .",
    "( [ alpha - posterior - mean - update ] ) ) , which in turn leads to a smaller @xmath107 ( c.f .",
    "( [ bi - posterior - mean - update ] ) ) . this negative feedback mechanism could keep decreasing @xmath107 until it becomes negligible , and eventually lead to an arbitrarily large @xmath106 .",
    "nevertheless , the interaction between @xmath107 and @xmath106 could go the other way around , i.e. a larger @xmath107 results in a smaller @xmath106 , which in turn leads to a larger @xmath107 . in this case",
    ", @xmath106 will eventually converge to a finite value .",
    "the behavior of @xmath110 plays a key role in determining the course of the evolution , and in turn has an impact on the dynamic behavior of @xmath88 itself . with a proper choice of @xmath42 and @xmath43 ,",
    "if @xmath88 becomes sufficiently small during the iterative process , then the process could evolve towards the @xmath111 ( that is , @xmath112 ) direction , otherwise the process will converge to a finite @xmath106 , in which case a non - sparsity - encouraging prior is imposed on the coefficient @xmath30 .",
    "this , clearly , is a sensible strategy to learn the parameters @xmath2 .",
    "we now discuss the choice of the parameters @xmath42 and @xmath43 .",
    "to enable an efficient interaction between @xmath113 and @xmath106 , we hope @xmath114 plays a critical role in determining the value of @xmath107 . to this goal",
    ", the values of @xmath42 and @xmath43 should be set sufficiently small .",
    "our experiments suggest that @xmath115 is a suitable choice which enables effective support learning .",
    "we now carry out experiments to illustrate the performance of our proposed algorithms and compare with other existing methods .",
    "the proposed algorithm in section [ sec : inference - nsl ] is referred to as the support knowledge - aided sparse bayesian learning with no support learning ( sa - sbl - nsl ) , and the one in section [ sec : inference - sl ] as the support knowledge - aided sparse bayesian learning with support learning ( sa - sbl - sl ) , respectively .",
    "the performance of the proposed algorithms will be examined using both synthetic and real data . throughout our experiments ,",
    "the parameters @xmath42 and @xmath43 for our proposed algorithm are set equal to @xmath115 .",
    ".,width=302 ]              suppose a @xmath116-sparse signal is randomly generated with the support set of the sparse signal randomly chosen according to a uniform distribution .",
    "the signals on the support set are independent and identically distributed ( i.i.d . )",
    "gaussian random variables with zero mean and unit variance .",
    "the measurement matrix @xmath8 is randomly generated with each entry independently drawn from gaussian distribution with zero mean and unit variance .",
    "the prior support information @xmath13 consists of two subsets : @xmath14 , where @xmath20 denotes the subset containing the correct information , and @xmath21 is a subset comprised of false information . in our simulations ,",
    "only the prior knowledge @xmath13 is available , the exact partition of @xmath13 into @xmath15 and @xmath16 is unknown .",
    "we compare our proposed algorithms with the conventional sparse bayesian learning ( sbl ) , the basis pursuit ( bp ) method , and the modified basis pursuit ( mbp ) method @xcite which incorporates the partial support information by assigning different @xmath4-minimization weights to different coefficients .",
    "we first consider the noiseless case .",
    "[ fig1 ] plots the success rates of respective algorithms vs. the ratio @xmath117 , where we set @xmath118 , @xmath119 , @xmath120 and @xmath121 , @xmath122 and @xmath123 denote the cardinality ( size ) of the set @xmath15 and @xmath16 , respectively .",
    "the success rate is computed as the ratio of the number of successful trials to the total number of independent runs .",
    "a trial is considered successful if the normalized recovery error , i.e. @xmath124 , is no greater than @xmath125 , where @xmath126 denotes the estimate of the true signal @xmath12 .",
    "results are averaged over 1000 independent runs , with the measurement matrix and the sparse signal randomly generated for each run .",
    "it can be seen that our proposed sa - sbl - sl method presents a substantial performance advantage over the sa - sbl - nsl and the sbl methods .",
    "the performance gain is primarily due to the fact that the sa - sbl - sl method is able to learn the true support from the partly erroneous knowledge and thus make more effective use of the prior support information .",
    "we also observe that when a considerable number of errors are present in the prior knowledge , the methods sa - sbl - nsl and mbp present no advantage over their respective counterparts sbl and bp . to examine the behavior of the sa - sbl - sl method",
    "more thoroughly , we fix the number of elements in the set @xmath15 and increase the number of elements in the error set @xmath16 . fig .",
    "[ fig2 ] depicts the success rates vs. the number of elements in the error set @xmath16 , where we set @xmath127 , @xmath118 , @xmath120 and @xmath123 varies from @xmath128 to @xmath129 . as can be seen from fig .",
    "[ fig2 ] , when a fairly accurate knowledge is available , i.e. the number of errors is negligible or small , the sa - sbl - nsl achieves the best performance .",
    "this is an expected result since little learning is required at this point .",
    "nevertheless , as the number of elements , @xmath123 , increases , the sa - sbl - nsl suffers from substantial performance degradation .",
    "as compared with the sa - sbl - nsl , the sa - sbl - sl method provides stable recovery performance through learning the values of @xmath2 , and outperforms all other algorithms when prior knowledge contains a considerable number of errors .",
    "we , however , notice that the proposed sa - sbl - sl method is surpassed by the conventional sbl method when inaccurate information becomes dominant ( e.g. @xmath130 ) , in which case even learning brings limited benefits and simply ignoring the error - corrupted prior knowledge seems the best strategy .",
    "we now consider the noisy case where the measurements are contaminated by additive noise .",
    "the observation noise is assumed multivariate gaussian with zero mean and covariance matrix @xmath11 .",
    "the normalized mean - squared errors ( nmses ) of respective algorithms as a function of signal - to - noise ratio ( snr ) are plotted in fig .",
    "[ fig3 ] , where we set @xmath127 , @xmath119 , @xmath118 , @xmath120 , and @xmath131 .",
    "the nmse is calculated by averaging normalized squared errors over @xmath132 independent runs .",
    "the snr is defined as @xmath133 .",
    "the mbp - dn is a noisy version of the mbp method @xcite .",
    "we observe that the conventional sbl and bp - dn methods outperform their respective counterparts : sa - sbl - nsl and mbp - dn .",
    "this , again , demonstrates that sa - sbl - nsl and mbp - dn methods are sensitive to prior knowledge inaccuracies . on the other hand , the proposed sa - sbl - sl method which takes advantage of the support learning presents superiority over both the conventional sbl as well as the sa - sbl - nsl method .         for the noiseless case.,width=302 ]    , @xmath134.,width=302 ]      we consider the problem of intensity - based source localization in sensor networks . the sensing field",
    "is partitioned into two - dimensional @xmath135-point virtual grid ( fig .",
    "[ fig6 ] ) which is used to represent possible locations of the @xmath136 sources .",
    "we have @xmath137 randomly distributed sensors .",
    "the measurement collected at sensors can be written as @xmath138 where @xmath139^t$ ] is a sparse vector whose entry @xmath140 denotes the intensity associated with the @xmath103th grid point , @xmath141^t$ ] , @xmath142 $ ] , @xmath143 denotes the distance between the grid point @xmath103 and the sensor @xmath144 , and @xmath145 is the energy - decay factor .    in practice",
    ", sources may keep moving but the current locations estimate could still be useful for the future localization .",
    "for example , we can expect that some sources may move to grid points close to their previous locations given that the interval between two time instants is sufficiently small . in our simulations , we assume that @xmath146 sources move slowly such that their next locations are partially predictable based on their current locations . specifically , for these @xmath147 sources , each source either stays at its current position or moves to two of its immediate neighboring grid points at the next time point ( see fig .",
    "[ fig6 ] ) . for simplicity ,",
    "suppose @xmath148 is the set of locations associated with the @xmath147 sources at time instant @xmath149 , then the set of possible locations of these @xmath147 sources at time instant @xmath150 is given by @xmath151 .",
    "the set @xmath13 can serve as prior support knowledge for source localization at time instant @xmath150 .",
    "nevertheless , the knowledge @xmath13 is partly erroneous since all possible locations of these @xmath147 sources in the next move are included . for the rest @xmath152 sources , their locations",
    "are randomly chosen from the rest @xmath153 grid points . to test the effectiveness of respective algorithms in utilizing the prior knowledge ,",
    "we assume the locations of these @xmath147 sources at time instant @xmath149 are perfectly known to us and examine the recovery performance at time instant @xmath150 . fig .",
    "[ fig4 ] depicts the success rates vs. the ratio @xmath117 for the noiseless case .",
    "results are averaged over 1000 independent runs , with sensors and locations of sources at time instant @xmath149 randomly generated for each run . from fig .",
    "[ fig4 ] , we see that the sa - sbl - nsl method yields performance much worse than the conventional sbl method .",
    "this is not surprising since the prior knowledge contains a substantial amount of erroneous information .",
    "it is also observed that the proposed sa - sbl - sl method which has the ability to learn the true support from erroneous information achieves a significant performance improvement over the sa - sbl - nsl method .",
    "we now consider a noisy case where the measurements are corrupted by additive gaussian noise . when noise is present , exact recovery of sparse signals is impossible .",
    "nevertheless , accurate localization can still be achieved since reliable recovery of the support of sparse signals in the presence of noise is possible . in our simulations ,",
    "locations of sources are estimated as the grid points associated with the largest @xmath116 nonzero coefficients of the estimated signal .",
    "[ fig5 ] plots the localization success rates as a function of the ratio @xmath117 , where the signal - to - noise ratio ( snr ) is set to @xmath154 .",
    "the localization success rate is calculated as the the ratio of the number of successful trials to the total number of independent runs .",
    "a trial is considered successful if all @xmath116 sources locations are estimated correctly .",
    "[ fig5 ] , again , demonstrates the superiority of the proposed sa - sbl - sl method over the sa - sbl - nsl and the conventional sbl methods .",
    ".,width=302 ]     larynx sequence .",
    "normalized mean squared errors vs. @xmath149.,width=302 ]      in this subsection , we carry out experiments on mri images and sequences .",
    "images have sparse ( or approximately sparse ) structures in discrete wavelet transform ( dwt ) basis . by representing an image as a one - dimensional vector ,",
    "the two - dimensional dwt ( a two - level daubechies-4 wavelet is used ) of an image can be expressed as a product of an orthonormal matrix @xmath155 and the image vector @xmath12 . the sensing matrix @xmath156 is therefore equal to @xmath157 , where @xmath158 denotes the measurement acquisition matrix and its entries are i.i.d .",
    "normal random variables .",
    "we test all algorithms on sparsified images .",
    "similar to @xcite , the image is sparsified by computing its 2d - dwt , retaining the coefficients from the @xmath159-energy support while setting others to zero and taking the inverse dwt .",
    "we first evaluate the reconstruction performance of respective algorithms for a sparsified image .",
    "the image is a @xmath160 larynx image obtained by resizing the @xmath161 larynx image , i.e. @xmath162 . for this image , its support size is @xmath163 . in our experiments ,",
    "we fix the size of the set @xmath16 to be @xmath164 , and gradually increase the size of the set @xmath15 .",
    "[ fig7 ] depicts the nmses of respective algorithms vs. the size of @xmath15 , where @xmath165 and @xmath122 varies from @xmath166 to @xmath167 .",
    "results are averaged over 500 independent runs , with the acquisition matrix @xmath158 , the sets @xmath16 and @xmath15 randomly generated for each run . from fig .",
    "[ fig7 ] , we see that when the prior support knowledge is dominated by the error set @xmath16 , the sbl and bp methods outperform their respective `` support - aided '' counterparts sa - sbl - nsl and mbp . nevertheless , the sa - sbl - nsl and the mbp methods surpass and eventually achieve a significant performance improvement over the sbl and bp methods as the prior knowledge becomes more and more accurate .",
    "it can also be observed the sa - sbl - sl method presents uniform superiority over sa - sbl - nsl and mbp for different values of @xmath122 , and the performance gap is wider in the small @xmath122 s region since learning in this region apparently brings more significant benefits as compared with learning in the large @xmath122 s region .",
    "we also conduct a comparison for the sparsified @xmath160 larynx sequence in fig .",
    "since the support of the sequence undergoes a small variation , the prior support knowledge can be obtained as the estimate of the previous time instant . at the very beginning ,",
    "i.e. @xmath168 , the prior knowledge set @xmath13 is empty .",
    "conventional sbl and bp methods are then used to obtain an initial support estimate for their respective support - aided methods .",
    "we set @xmath169 in order to ensure a fairly accurate initial estimate is obtained . for @xmath170 ,",
    "only @xmath171 measurements are collected for the signal reconstruction . from fig .",
    "[ fig8 ] , we see that the all support - aided methods including sa - sbl - sl , sa - sbl - nsl and mbp are able to achieve exact reconstruction with only as few as @xmath172 measurements , whereas the conventional sbl and bp fail to recover the signal with these few measurements .",
    "also , since the prior support knowledge is fairly accurate , support learning does not bring any additional benefits , and thus both the sa - sbl - sl and sa - sbl - nsl methods attain similar recovery performance .",
    "we studied the problem of sparse signal recovery given that part of the signal s support is known _ a priori_. the prior knowledge , however , may not be accurate and could contain erroneous information .",
    "this knowledge inaccuracy may result considerable performance degradation or even recovery failure . to address this issue , we first introduced a modified two - layer gaussian - inverse gamma hierarchical prior model .",
    "the modified two - layer model employs an individual parameter @xmath0 for each sparsity - controlling hyperparameter @xmath1 , and therefore has the ability to place non - sparsity - encouraging priors to those coefficients that are believed in the support set .",
    "based on this two - layer mode , we then proposed an improved three - layer hierarchical prior model , with a prior placed on the parameters @xmath2 in the third layer .",
    "such a model enables to automatically learn the true support from partly erroneous information through learning the values of @xmath2 .",
    "bayesian algorithms are developed by resorting to the mean field variational bayes .",
    "simulation results show that substantial performance improvement can be achieved through support learning since it allows us to make more effective use of the partly erroneous information .",
    "m.  a. khajehnejad , w.  xu , a.  s. avestimehr , and b.  hassibi , `` weighted @xmath4 minimization for sparse recovery with prior information , '' in _ proc .",
    "information theory _ ,",
    "seoul , korea , june 28july 3 2009 .",
    "c.  j. miosso , r.  von borries , m.  argaez , l.  velazquez , c.  quintero , and c.  m. potes , `` compressive sensing reconstruction with prior information by iteratively reweighted least - squares , '' _ ieee trans . signal processing _",
    ", no .  6 , pp .",
    "24242431 , june 2009 .",
    "w.  lu and n.  vaswani , `` regularized modified bpdn for noisy sparse reconstruction with partial erroneous support and signal value knowledge , '' _ ieee trans .",
    "signal processing _",
    ", no .  1 ,",
    "pp . 182196 , january 2012 .",
    "j.  fang , y.  shen , and h.  li , `` pattern coupled sparse bayesian learning for recovery of time - varying sparse signals , '' in _",
    "19th international conference on digital signal processing _ , hong kong , august 2023 2014 .",
    "z.  zhang and b.  d. rao , `` sparse signal recovery with temporally correlated source vectors using sparse bayesian learning , '' _ ieee journal of selected topics in signal processing _",
    ", vol .  5 , no .  5 , pp . 912926 , sept ."
  ],
  "abstract_text": [
    "<S> it has been shown both experimentally and theoretically that sparse signal recovery can be significantly improved given that part of the signal s support is known _ a priori_. in practice , </S>",
    "<S> however , such prior knowledge is usually inaccurate and contains errors . </S>",
    "<S> using such knowledge may result in severe performance degradation or even recovery failure . in this paper </S>",
    "<S> , we study the problem of sparse signal recovery when partial but partly erroneous prior knowledge of the signal s support is available . based on the conventional sparse bayesian learning framework </S>",
    "<S> , we propose a modified two - layer gaussian - inverse gamma hierarchical prior model and , moreover , an improved three - layer hierarchical prior model . </S>",
    "<S> the modified two - layer model employs an individual parameter @xmath0 for each sparsity - controlling hyperparameter @xmath1 , and has the ability to place non - sparsity - encouraging priors to those coefficients that are believed in the support set . </S>",
    "<S> the three - layer hierarchical model is built on the modified two - layer prior model , with a prior placed on the parameters @xmath2 in the third layer . </S>",
    "<S> such a model enables to automatically learn the true support from partly erroneous information through learning the values of the parameters @xmath2 . </S>",
    "<S> variational bayesian algorithms are developed based on the proposed hierarchical prior models . </S>",
    "<S> numerical results are provided to illustrate the performance of the proposed algorithms .    </S>",
    "<S> compressed sensing , sparse bayesian learning , prior support knowledge . </S>"
  ]
}