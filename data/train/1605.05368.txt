{
  "article_text": [
    "hierarchical feature extraction using deep neural networks has been very successful in accomplishing various tasks such as object recognition  @xcite , speech recognition  @xcite , deep vision enhancement  @xcite , multi - modal sensor fusion  @xcite , prognostics  @xcite , engineering design  @xcite , policy reward learning  @xcite , and relating dna variants to diseases  @xcite .",
    "this paper proposes an application of deep learning by fusing multiple architectures in solving design engineering problems , which potentially accelerates the development of various fields such as manufacturing , chemical engineering , and biology .    for sequence prediction , recurrent neural networks ( rnn )  @xcite",
    "are often the go - to architecture for exploiting sequential information where the output is dependent on previous computation .",
    "however , the dependencies of the computation lie in the latent domain which may not be suitable for certain applications involving the prediction of a step - wise transformation sequence that is dependent on the previous computation only in the visible domain  ( fig .",
    "[ fig : rnnvsaction ] ) . in the paper",
    ", we propose a deep learning architecture which simultaneously predicts the intermediate shape between two images and learns a sequence of causal actions contributing to desired shape transformation in the visible domain .",
    "this topological transformation framework has multiple implications .",
    "the architecture can be easily implemented in applications such as learning to transform the belief space for robotic path planning  @xcite , sequential decision making in games  @xcite , learning the material processing pathways to obtain desired microstructures starting from an initial microstructure  @xcite , and learning a sequence of manufacturing steps in additive manufacturing  @xcite , with _",
    "fast - design _ being the main advantage .",
    "the contributions of the paper are outlined next :    1 .",
    "a formulation of learning causal shape transformations to predict a sequence of transformation actions is presented , in the setting where only an initial shape and a desired target shape are provided .",
    "an integrated hierachical feature extraction approach using stacked autoencoders ( sae )  @xcite with convolutional neural networks ( cnn )  @xcite is proposed to capture transformation features to generate the associated sequence resulting in the transformation .",
    "the proposed approach is tested and validated via numerical simulations on an engineering design problem ( i.e. flow sculpting in microfluidic devices ) , with results showing competitive prediction accuracy over previously explored methods .",
    "in this section , we describe the problem setup and provide a brief background on the previous approach . to illustrate how the previous and current architectures can be implemented ,",
    "we will focus our attention on _ microfluidic flow sculpting _ as the application .",
    "inertial fluid flow sculpting via micropillar sequences is a recently developed method of fluid flow control with a wealth of applications in the microfluidics community  @xcite .",
    "this technique utilizes individual deformations imposed on fluid flowing past a single obstacle ( pillar ) in confined flow to create an overall net deformation from a rationally designed sequence of such pillars .",
    "if the pillars are spaced far enough apart ( space @xmath0 , for a pillar diameter @xmath1 ) , the individual deformations become independent , and can thus be pre - computed as building blocks for a highly efficient forward model for the prediction of sculpted flow given an input pillar sequence  @xcite ( fig .",
    "[ fig : fig_problemformulation ] ) . since its debut ,",
    "flow sculpting via micropillar sequence design has been used in fundamental applications of novel particle fabrication  @xcite , and solution transfer  @xcite .",
    "however , practical applications require solving the _ inverse problem _ , that is , to generate a sequence of pillars given a user - defined flow shape . without intelligent computer algorithms ,",
    "such tasks require time - consuming trial and error design iterations .",
    "the automated determination of pillar sequences that yields a custom shape is an impactful advance .",
    "although researchers have tried to frame this inverse problem as an unconstrained optimization problem  @xcite , they are invariable time - consuming .",
    "while many methods are used to solve the forward problem  @xcite , only a limited amount of effort has been done in solving the inverse problem  @xcite . for practical and time - efficient applications ,",
    "deep learning methods are explored to map user - defined flow shapes and the corresponding pillars of sequence .",
    "additionally , this application serves as a very impactful advance towards learning topological transformations .",
    "this framework can be utilized for concrete biomedical applications that require the design of microfluidic devices .",
    "possible applications include : ( a ) designing a device to move fluid surrounding cells ( e.g. lymphoid leukemia cells ) against a far wall of the microfluidic channel where it can be collected at high purity while the cells are maintained at the channel center .",
    "high purity allows the reuse of valuable reagents for staining cells during diagnosis , ( b ) wrapping a fluid around the microchannel surface to characterize binding p24 ( an hiv viral capsid protein ) to anti - p24 antibody immobilized on the microchannel surface .",
    "flow sculpting can enhance reaction of low abundance proteins that can improve diagnostic limits of detection for various diseases .",
    "hence , this study may promise new application areas for the machine learning community related to thermo - fluid sciences and design engineering .      to approach the inverse problem ,",
    "class indices are assigned to pillars with different specifications .",
    "for instance , a pillar located at position @xmath2 with a diameter of @xmath3 is an index of 1 , whereas another pillar at position @xmath4 and diameter @xmath3 is assigned an index of @xmath5 .",
    "diameter and position values are represented as ratios with respect to the channel size and locations , so they are dimensionless quantities to help enable scalability of the fluid channel .",
    "index assignment is performed over a finite combination of pillar positions and diameters that has been obtained by discretizing the design space . in the study , there are 32 possible indices ( or classes ) that describe the diameter and position of a single pillar .",
    "simultaneous multi - class classification is a method used in  @xcite to solve a similar problem . instead of solving a single classification problem",
    ", the model solves a sub - problem for each pillar using the parameters learned by the cnn .",
    "this formulation requires a slight modification in the loss function : for pillar sequence with length @xmath6 , the loss function to be minimized for a data set @xmath7 is the negative log - likelihood defined as : @xmath8_j\\ ] ] where @xmath7 denotes the training set , @xmath9 is the model parameters with @xmath10 as the weights and @xmath11 for the biases , @xmath12 is predicted pillar index whereas @xmath13 is the provided flow shape image .",
    "the total loss is computed by summing the individual losses for each pillar .",
    "the cnn - smc framework in  @xcite is not without drawbacks .",
    "the pillars are predicted in a joint manner , where each of the pillars in the sequence are inferred simultaneously . while it may produce satisfactory sequences which regenerates the desired shape , it is unclear how the successive pillars interact with one another . in this section ,",
    "we propose a deep learning architecture which predicts the intermediate shape between two images and learns a sequence of causal actions contributing to desired shape transformation .",
    "the architecture can be easily implemented in applications such as :    1 .   learning to transform the belief space for robotic path planning .",
    "a robot may be chasing a mobile target while maintaining a posterior ( that is transformed into a visual representation analogous to the flow shape ) corresponding to the location of the target .",
    "if we would like to specifically maximize posterior in a certain region , the corresponding problem would be : ",
    "what is the best course of action that should be taken by the robot in succession ? \" .",
    "2 .   learning the material processing pathways to obtain the desired microstructures . in materials processing , processing the materials by altering the properties in succession will alter the morphology ( microstructures ) of the material .",
    "the equivalent inverse problem would be :  if we want the material to achieve a specific morphology , what are the processing steps that should be taken ? \" .",
    "3 .   learning a sequence of manufacturing steps in additive manufacturing , with _ fast - design _ being the main advantage .",
    "while cnn - smc is capable of predicting a large number of different sequences in a total time of just seconds , a drawback is that the sequence length is constrained because a new model needs to be trained to generate a sequence with different lengths .",
    "furthermore , the sequence which deforms into the target flow shape is predicted in a joint manner and do not provide sufficient insight on the interplay between pillars causing the deformation .",
    "rnn - like architectures are , although scalable , deemed unsuitable because the elements in the output vector in our problem are generally independent of each other , unlike words in a sentence . in this application",
    ", a pillar can be _ independently _ added to the flow channel only after the flow shape resulted from the previous pillar stabilizes , i.e. no longer changes . this way , we guarantee that there are no interactions among pillars ( hence the independence )",
    ". however , all these pillars contribute to the change in the _ overall _ flow shape ( thus they are causal to the final shape ) . in the context of rnn outputs , e.g. for the case of next - word prediction , oftentimes the output vectors are not independent - one word will have a high probability to be the next word of a certain word . in our context , the choice of the next pillar will not depend on the choice of the previous pillar .",
    "rather , we look at the _ now _ by making use of the given generated shape so far regardless of the history .",
    "a related concept is the spatial transformer network  @xcite where the localization network outputs geometrical transformation parameters ; however we desire an exact class attributed to an arbitrary transformation function . in this work ,",
    "we predict the pillar sequence one pillar at a time without disregarding causality , such that the produced sequences deform the flow into one that resembles more closely to the desired shape .",
    "in this section , we present two core architectures and the integrated framework which fuses these two architectures .",
    "to predict the sequence of pillars that results in the desired flow shape , this chapter introduces the notion of _ transformation learning_. fig .",
    "[ fig : fig_apn ] shows the learning approach by supplying juxtaposed flow shapes , one before deformation , and one after , into a cnn that extracts relevant features and predicts the class of the pillar causing the deformation . in the training data generation procedure ,",
    "the input is comprised of three parts : ( a ) the pre - deformed flow shape , which is produced from a randomly generated sequence of varying length with values up to the number of classes of a single pillar ; ( b ) a padding to prevent the convolutional kernels to pick up interfering features from the juxtaposed images ; and ( c ) the post - deformed shape produced from adding a random pillar index to the previous sequence .",
    "this newly - added pillar index will become the label to train the cnn .",
    "essentially , the action prediction network predicts the index of the pillar ( which describes its position and diameter in the flow channel ) given a pre- and post - deformed shape .",
    "when actually inferencing , the right portion ( part c ) of the input image is replaced by the final target shape . the input image is fed into the cnn to obtain a pillar index , which is added to an initially empty pillar sequence . because the forward model ( i.e. going from sequence to flow shape ) is not at all computationally expensive and takes merely milliseconds ,",
    "this pillar sequence is used to regenerate the _ current shape _ which replaces the left portion ( part a ) of the input , while the right portion ( part c , the target ) remains the unchanged .",
    "the input enters the cnn again to obtain a second pillar index , and is subsequently added to the previously obtained pillar sequence . at this point , we have a sequence of length 2 , where the sequence will then again used to regenerate a new current shape .",
    "the process is repeated until the _ current shape _ matches the _ target shape _ , or until a user - defined stopping criterion is met .",
    "an alternative apn is to feed the pre- and post - deformed shapes into the cnn separately as _ isolated channels _ before merging them together in the fully connected layer ( fig .",
    "[ fig : fig_apnc ] ) .",
    "this architecture will be referred to as apn - c ( where the ` c ' denotes _ channels _ and signifies that the pre- and post - deformed shapes are treated as different channels ) . in other words ,",
    "each channel has different sets of filters and the relationship between the two sets will fuse together at the fully connected layer .",
    "however , this method only works well for simpler target shapes . for more complex flow shapes",
    "( e.g. shapes with many sharp angles , jagged edges , swirls and curls ; see fig .  [",
    "fig : fig_results3 ] sample 20a for example ) , the transformation path may be highly nonlinear the _ current shape _ may never converge to the final desired shape .",
    "furthermore , the training data covers a vanishingly small fraction of the design space with coverage shrinking exponentially as the sequence length increases ( i.e. , an @xmath6 sequence will result in @xmath14 different combinations ) , so it is necessary to learn the transformations in a meaningful way . to help alleviate this issue",
    ", we will introduce the intermediate transformation network ( itn ) in the next subsection .",
    "* apn parameters : * the input image into the apn is comprised of two @xmath15 px flow shape image with a @xmath16 px padding in between , resulting in a final dimension of @xmath17 px .",
    "for apn - c , the two inputs are treated as separate channels without introducing padding .",
    "the model contains two convolutional layers with 40 and 100 kernels respectively ( sizes of @xmath18 and @xmath19 px ) , each followed by a @xmath20 maxpooling layer .",
    "the fully connected layer has 500 hidden units .",
    "training is done with 250,240 training examples and 60,000 validation examples in minibatches of 50 with a learning rate of 0.01 .",
    "the training procedure employs the early stopping algorithm where training stops when validation error ceases to decrease .",
    "the itn attempts to construct a flow shape that bridges between the purely undeformed flow shape ( i.e. shape generated with an empty sequence ) and the final desired flow shape in the nonlinear transformation path ( see fig .  [",
    "fig : fig_itn](a ) ) .",
    "we used a deep autoencoder to extract hierarchical features from the desired final shape , and outputs an approximated _ bridging shape_. to generate the training data for this network , the input image is generated with a random pillar sequence with a varying length .",
    "the corresponding bridging shape is generated by truncating the same pillar sequence by half , thus the shape lies in the middle of the transformation pathway ( fig .",
    "[ fig : fig_itn](b ) ) .",
    "formally , if a sequence has length @xmath21 , it is truncated to @xmath22 if @xmath21 is odd , and to @xmath23 if @xmath21 is even .",
    "this pair of images is used to train a deep autoencoder , where the mean squared error ( mse ) between the model outputs and the desired bridging shape is backpropagated to finetune the weights and biases of the model .    during inference",
    ", the edges in the outputs of the itn may appear blur because the bridging shape is only an approximation ( fig .",
    "[ fig : fig_itn](c ) ) . to obtain binary images , we thresholded the pixel values .",
    "the itn may be extended to obtain several _ waypoints _ instead of only the midpoint , where the number of waypoints is dictated by the complexity of the target flow shape ( see section  [ sec : complex ] for details ) .",
    "doing so will allow a smoother transformation pathway , but will require some changes in the training data generation procedure .",
    "we call this the _ recursive itn _ ( ritn ) and leave this as a future work . some may comment that itn is somewhat similar to rnn .",
    "we note that for this problem ( along with the example applications discussed in section  [ sec : aseq ] ) requires transforming the hidden states into the physical domain using a complex forward function before making the next prediction . unlike rnns , itns do not solely use the latent output directly as the input in the next time step .",
    "due to the forward function , we need to constantly transform the output back into the physical domain and re - evaluate at every step .",
    "one may also argue that itn is somewhat similar to rnn in terms of _ memory _ , but by the same argument , the difference in domain is the key . if only the latent domain is used for every step , the final sequence output may not necessarily reflect the true transformation in the physical domain .    * itn parameters : * the autoencoder has 3 layers of 500 hidden units each and accepts flow shape images of @xmath15 px .",
    "training was done on 500,000 training examples and 20,000 validation examples in minibatch of size 1,000 with a learning rate of 0.01 .",
    "training is also done using the early - stopping algorithm .      with the roles of the apn and itn clearly described",
    ", we can now integrate both networks to form an integrated pipeline .",
    "a schematic is shown in fig .",
    "[ fig : fig_integrated ] . at the very beginning",
    ", the itn automatically guesses a candidate for the bridging flow shape , which is then considered as the _ temporary _ target shape , and is concatenated with the undeformed shape placed to its left with a padding .",
    "the concatenated input is supplied into the apn to predict the first pillar causing the deformation , and then added to the sequence which was initially empty , hence resulting in a sequence of length 1 .",
    "the current shape is regenerated with the updated 1-pillar sequence and replaces the left portion of the input image for the next apn iteration to obtain the second pillar index .",
    "the process is repeated as ` stage a ' ( with the bridging shape as a temporary target ) until the current shape is sufficiently similar to the bridging shape , or until an iteration limit is reached .",
    "then , the right portion of the input image ( which was the bridging shape ) is replaced with the final desired shape as the target .",
    "this process is continued as ` stage b ' which undergoes the same process as ` stage a ' , except the target shape is now the final desired shape .",
    "after each iteration , the predicted pillar index is added to the sequence until the reconstructed flow shape matches the desired shape or a stopping criterion is achieved .",
    "the resulting sequence will vary in length for different desired shapes .",
    "in this section we first present the evaluation metrics used in the study , then show the results comparing cnn - smc against our method using apn and the apn+itn hybrid architecture .      the perimetric complexity",
    ", @xmath24 of binary digital images can be described by the following expression proposed by attneave et al .",
    "@xcite : @xmath25 where @xmath26 is the perimeter of the shape , and @xmath27 is the area of the shape .",
    "this measure can be utilized to determine the number of intermediate shapes that should be predicted by the itn .",
    "the higher the shape complexity , the more necessary are the use of bridging shapes .",
    "[ fig : fig_complex ] shows the complexity for various flow shapes .    to quantify the effectiveness of cnn - smc versus our proposed approach",
    ", we evaluated the _ pixel match rate _",
    ", pmr on 20 target flow shapes and computed appropriate statistics .",
    "the pmr , defined in  @xcite , is computed as follows : @xmath28 where @xmath29 is the target image vector , @xmath30 is the predicted image vector , and @xmath31 denotes the cardinality of the vector ( i.e. , the total number of pixels in the image ) . since shape transformation is an essential aspect in this problem formulation , the _ structural similarity index _ ( ssim )  @xcite is used as a supplementary metric to compare how structurally similar are the regenerated flow shape images ( from predicted sequence ) to the target flow shape .",
    "ssim explores the change in image structure and incorporates pixel inter - dependencies .",
    "ssim is expressed as : @xmath32 where @xmath33 is the average of window @xmath13 , @xmath34 is the average of window @xmath12 , @xmath35 is the variance of @xmath13 , @xmath36 is the variance of @xmath12 , @xmath37 is the covariance of @xmath13 and @xmath12 , @xmath38 and @xmath39 are two variables to stabilize the division with weak denominator with @xmath40 and @xmath41 by default , and @xmath42 is the dynamic range of pixel values .      in all of our tests ,",
    "the target flow shape is generated from a 10-pillar sequence which is sufficiently complex .",
    "[ fig : fig_results ] shows four example target shapes with the performance using only apn ( without bridging ) and apn+itn ( with bridging ) . in most cases ,",
    "the apn - only formulation produces pillar sequences resulting in flow shapes that do not resemble the target shape as close as using apn+itn .",
    "this can be clearly seen for cases c and d in fig .",
    "[ fig : fig_results ] . on the other hand , by using the bridging shape as a temporary target , the prediction performance saw great improvements .",
    "in addition , we see that most shapes are able to converge to both the bridging shape as well as the target in the apn+itn formulation . in realistic applications ,",
    "the sequence may be stored in memory and post - processed to remove the redundant pillars , thus producing a shorter sequence which may increase financial savings during the manufacturing process .          [",
    "cols=\"<,^,^,^,^,^\",options=\"header \" , ]     [ tab : pmr_ssim_images ]    20 sample - wise comparison on the performance of cnn+smc , and our methods apn and apn+itn are shown in fig .",
    "[ fig : fig_results2 ] and tabulated in table  [ tab : pmr_ssim_images ] .",
    "the reconstructed flow shapes from the predicted sequences are shown in fig .",
    "[ fig : fig_results3 ] . in both fig .",
    "[ fig : fig_results2](a ) and fig .",
    "[ fig : fig_results2](b ) , the pmr and ssim for apn , apn - c , and apn+itn are clearly higher than the cnn+smc approach .",
    "we observe that for some target flow shapes , the predicted sequence for cnn+smc may result in an entirely dissimilar shape ( e.g. sample 2 , 19 , 20 ) . in some cases ( e.g. samples 14 , 15 , 16 ) apn fared better than the hybrid apn+itn model .",
    "however , apn+itn is consistently more superior in terms of both pmr and ssim than the apn - only architecture ( i.e. apn and apn - c ) .",
    "this shows that having a bridging shape generally helps in producing sequences that generate complex flow shapes",
    ". the inferior performance of cnn+smc is due the model being constrained to always generate a sequence with 10 pillars , thus the resultant flow shapes often become overcomplicated .    additionally , treating the input as separate channels results in lower performance while all other model parameters are kept equal .",
    "this suggests that having a single filter that learns features from both input images simultaneously is more effective in feature extraction compared to learning the features in isolation before merging at the fully connected layer .",
    "a clear advantage of using both apn and itn together is that the model does not need to be retrained for variable sequence lengths , unlike in the cnn - smc model where the number of pillars in the output sequence is constrained .",
    "this method is highly scalable , and has an enormous room for extension into sculpting more highly complex flow shapes .",
    "this paper proposes a deep learning based approach to solve complex design exploration problems , specifically design of microfluidic channels for flow sculpting .",
    "a deep architecture is proposed to learn a sequence of actions that carries out the desired transformation over the input which has great implications on the innovation of manufacturing processes , material sciences , biomedical applications , decision planning , and many more .",
    "the creative integration of dl based tools can tackle the inverse fluid problem and achieve the required design accuracy while expediting the design process .",
    "current efforts are primarily focusing on optimizing the tool - chain ( e.g. ritn ) as well as tailoring for specific application areas such as manufacturing and biology .",
    "s.  ashforth - frost , v.  n. fontama , k.  jambunathan , and s.  l. hartle .",
    "the role of neural networks in fluid mechanics and heat transfer . in _ instrumentation and measurement technology conference , 1995 .",
    "imtc/95 . proceedings . integrating intelligent instrumentation and control .",
    ", page  6 .",
    "ieee , 1995 .",
    "g.  e. hinton , l.  deng , d.  yu , g.  e. dahl , a .-",
    "mohamed , n.  jaitly , a.  senior , v.  vanhoucke , p.  nguyen , t.  n. sainath , and b.  kingsbury .",
    "deep neural networks for acoustic modeling in speech recognition . , 29(6 ) , 2012 .",
    "a.  krizhevsky , i.  sutskever , and g.  e. hinton .",
    "imagenet classification with deep convolutional neural networks . in f.  pereira",
    ", c.  burges , l.  bottou , and k.  weinberger , editors , _ advances in neural information processing systems 25 _ , pages 10971105 .",
    "curran associates , inc . , 2012 .",
    "k.  g. lore , d.  stoecklein , m.  davies , b.  ganapathysubramanian , and s.  sarkar .",
    "hierarchical feature extraction for efficient design of microfluidic flow patterns . in _ proceedings of the 1st international workshop on `` feature extraction : modern questions and challenges '' , nips _ , pages 213225 , 2015 .",
    "s.  sarkar , k.  g. lore , and s.  sarkar .",
    "early detection of combustion instability by neural - symbolic analysis on hi - speed video . in _ workshop on cognitive computation : integrating neural and symbolic approaches ( coco@ nips 2015 ) , montreal , canada _ , 2015 .",
    "d.  silver , a.  huang , c.  j. maddison , a.  guez , l.  sifre , g.  van den  driessche , j.  schrittwieser , i.  antonoglou , v.  panneershelvam , m.  lanctot , et  al . mastering the game of go with deep neural networks and tree search .",
    ", 529(7587):484489 , 2016 .",
    "p.  vincent , h.  larochelle , y.  bengio , and p .- a .",
    "manzagol . extracting and composing robust features with denoising autoencoders . in _ proceedings of the 25th international conference on machine learning _ , pages 10961103 .",
    "acm , 2008 ."
  ],
  "abstract_text": [
    "<S> deep learning became the method of choice in recent year for solving a wide variety of predictive analytics tasks . for sequence prediction , recurrent neural networks ( rnn ) </S>",
    "<S> are often the go - to architecture for exploiting sequential information where the output is dependent on previous computation . </S>",
    "<S> however , the dependencies of the computation lie in the latent domain which may not be suitable for certain applications involving the prediction of a step - wise transformation sequence that is dependent on the previous computation only in the visible domain . </S>",
    "<S> we propose that a hybrid architecture of convolution neural networks ( cnn ) and stacked autoencoders ( sae ) is sufficient to learn a sequence of actions that nonlinearly transforms an input shape or distribution into a target shape or distribution with the same support . </S>",
    "<S> while such a framework can be useful in a variety of problems such as robotic path planning , sequential decision - making in games , and identifying material processing pathways to achieve desired microstructures , the application of the framework is exemplified by the control of fluid deformations in a microfluidic channel by deliberately placing a sequence of pillars . </S>",
    "<S> learning of a multistep topological transform has significant implications for rapid advances in material science and biomedical applications . </S>"
  ]
}