{
  "article_text": [
    "consider a data set of @xmath1 observations ( or units ) jointly categorized according to the random variables @xmath2 and @xmath3 , in @xmath4 and @xmath5 , respectively . the observed counts are @xmath6 , with @xmath7 , and the observed relative frequencies are @xmath8 , with @xmath9 .",
    "the data @xmath10 are considered as a sample from a larger population , characterized by the actual chances @xmath11 , which are the population counterparts of @xmath12 .",
    "both @xmath12 and @xmath13 belong to the @xmath14-dimensional unit simplex .",
    "we consider the statistical problem of analyzing the association between @xmath2 and @xmath3 , given only the data @xmath10 .",
    "this problem is often addressed by measuring indices of independence , such as the statistical coefficient @xmath15 @xcite . in this paper",
    "we focus on the index @xmath16 called _ mutual information _ ( also called _",
    "cross entropy _ or _ information gain _ ) @xcite .",
    "this index has gained a growing popularity , especially in the artificial intelligence community .",
    "it is used , for instance , in learning _ bayesian networks _",
    "@xcite , to connect stochastically dependent nodes ; it is used to infer classification trees @xcite .",
    "it is also used to select _ features _ for classification problems @xcite , i.e.  to select a subset of variables by which to predict the _ class _ variable .",
    "this is done in the context of a _ filter approach _ that discards irrelevant features on the basis of low values of mutual information with the class @xcite .",
    "mutual information is widely used in descriptive rather than inductive way .",
    "the qualifiers ` descriptive ' and ` inductive ' are used for models bearing on @xmath12 and @xmath13 , respectively .",
    "accordingly , @xmath12 are called _ relative frequencies _ , and @xmath13 are called _ chances_. at descriptive level , variables @xmath2 and @xmath3 are found to be either independent or dependent , according to the fact that the _ empirical _ mutual information @xmath17 is zero or is a positive number . at inductive level , @xmath2 and @xmath3",
    "are assessed to be either independent or dependent only with some probability , because @xmath18 can only be known with some ( second order ) probability .",
    "the problem with the descriptive approach is that it neglects the variability of the mutual information index with the sample , and this is a potential source of fragility of the induced models . in order to achieve robustness",
    ", one must move from the descriptive to the inductive side .",
    "this involves regarding the mutual information @xmath16 as a random variable , with a certain distribution .",
    "the distribution allows one to make reliable , probabilistic statements about @xmath16 .    in order to derive the expression for the distribution of @xmath16",
    ", we work in the framework of bayesian statistics .",
    "in particular , we use a second order prior distribution @xmath19 which takes into account our uncertainty about the chances @xmath13 . from the prior @xmath19 and",
    "the likelihood we obtain the posterior @xmath20 , of which the posterior distribution @xmath21 of the mutual information is a formal consequence .",
    "although the problem is formally solved , the task is not accomplished yet .",
    "in fact , closed - form expressions for the distribution of mutual information are unlikely to be available , and we are left with the concrete problem of using the distribution of mutual information in practice .",
    "we address this problem by providing fast analytical approximations to the distribution which have guaranteed levels of accuracy .",
    "we start by computing the mean and variance of @xmath22 .",
    "this is motivated by the central limit theorem that ensures that @xmath22 can be well approximated by a gaussian distribution for large @xmath1 .",
    "section [ secapprox ] establishes a general relationship , used throughout the paper , to relate the mean and variance to the covariance structure of @xmath23 . by focusing on the specific covariance structure obtained when",
    "the prior over the chances is dirichlet , we are then lead to @xmath24 approximations for the mean and the variance of @xmath21 . generalizing the former approach , in section [ secgeneral ]",
    "we report @xmath0 approximations for the variance , skewness and kurtosis of @xmath22 .",
    "we also provide an exact expression for the mean in section [ secexact ] , and improved tail approximations for extreme quantiles .    by an example , section [ secnum ] shows that the approximated distributions , obtained by fitting some common distributions to the expressions above , compare well to the `` exact '' one obtained by monte carlo sampling also for small sample sizes .",
    "section [ secnum ] also discusses the accuracy of the approximations and their computational complexity , which is of the same order of magnitude needed to compute the empirical mutual information .",
    "this is an important result for the real application of the distribution of mutual information .    in the same spirit of making the results useful for real applications , and considered that missing data are a pervasive problem of statistical practice , we generalize the framework to the case of incomplete samples in section [ secmd ] .",
    "we derive @xmath25 expressions for the mean and the variance of @xmath22 , under the common assumption that data are _ missing at random _",
    "these expressions are in closed form when observations from one variable , either @xmath2 or @xmath3 , are always present , and their complexity is the same of the complete - data case . when observations from both @xmath2 and @xmath3 can be missing , there are no closed - form expressions in general but we show that the popular expectation - maximization ( em ) algorithm @xcite can be used to compute @xmath25 expressions .",
    "this is possible as em converges to the global optimum for the problem under consideration , as we show in section [ secmd ] .",
    "we stress that the above results are a significant and novel step to the direction of robustness . to our knowledge",
    ", there are only two other works in literature that are close to the work presented here .",
    "kleiter has provided approximations to the mean and the variance of mutual information by heuristic arguments @xcite , but unfortunately , the approximations are shown to be crude in general ( see section [ secmi ] ) .",
    "wolpert and wolf computed the exact mean of mutual information ( * ? ? ?",
    "* th.10 ) and reported the exact variance as an infinite sum ; but the latter does not allow a straightforward systematic approximation to be obtained .",
    "in section [ sec : discussion ] we move from the theoretical to the applied side , discussing the potential implications of the distribution of mutual information for real applications . for illustrative purposes , in the following section [ secfs ] ,",
    "we apply the distribution of mutual information to feature selection .",
    "we define two new filters based on the distribution of mutual information that generalize the traditional filter based on empirical mutual information @xcite .",
    "several experiments on real data sets show that one of the new filters is more effective than the traditional one in the case of sequential learning tasks .",
    "this is the case for complete data described in section [ ds ] , as well as incomplete data in section [ eawis ] .",
    "concluding remarks are reported in section [ c ] .",
    "consider discrete random variables @xmath26 and @xmath27 and an i.i.d .  random process with outcome @xmath28 having joint chance @xmath29 .",
    "the mutual information is defined by [ mi ] i(v ) = _ i=1^r_j=1^s _ ij = _ ij_ij_ij - _ i_i_i - _ j_j_j , where @xmath30 denotes the natural logarithm and @xmath31 and @xmath32 are marginal chances .",
    "often the descriptive index @xmath33 is used in the place of the actual mutual information .",
    "unfortunately , the empirical index @xmath17 carries no information about its accuracy .",
    "especially @xmath34 can have to origins ; a true dependency of the random variables @xmath2 and @xmath3 or just a fluctuation due to the finite sample size .",
    "in the bayesian approach to this problem one assumes a prior ( second order ) probability density @xmath19 for the unknown chances @xmath29 on the probability simplex . from this one",
    "can determine the posterior distribution @xmath35 ( the @xmath36 are multinomially distributed ) .",
    "this allows to determine the posterior probability density of the mutual information : denotes the mutual information for the specific chances @xmath13 , whereas @xmath16 in the context above is just some non - negative real number .",
    "@xmath16 will also denote the mutual information _ random variable _ in the expectation @xmath37 $ ] and variance @xmath38 $ ] .",
    "expectations are _ always _ w.r.t .  to the posterior distribution @xmath23 .",
    "] [ midistr ] p(i|v ) = ( i(v)-i)p(v|v)d^rsv .",
    "the @xmath39 distribution restricts the integral to @xmath13 for which @xmath40 . since @xmath41 with sharp upper bound @xmath42 , the domain of @xmath22 is @xmath43 $ ] , hence integrals over @xmath16 may be restricted to such interval of the real line .    for large sample size , @xmath23 gets strongly peaked around @xmath44 and @xmath22 gets strongly peaked around the empirical index @xmath45 .",
    "the mean @xmath37 = \\int_0^\\infty i\\cdot p(i|\\v\\n)\\,di = \\int i(\\v\\t)p(\\v\\t|\\v\\n)d^{rs}\\v\\t$ ] and the variance @xmath38=e[(i - e[i])^2]=e[i^2]-e[i]^2 $ ] are of central interest .      in the following",
    "we ( approximately ) relate the mean and variance of @xmath16 to the covariance structure of @xmath23 .",
    "let @xmath46 , with @xmath47 $ ] .",
    "since @xmath23 is strongly peaked around @xmath48 , for large @xmath49 we may expand @xmath18 around @xmath50 in the integrals for the mean and the variance . with @xmath51",
    "$ ] and using @xmath52 we get the following expansion of expression ( [ mi ] ) : [ miexp ] i(v ) = i ( ) + _ ij(_ij_i_j)_ij + _ ij_ij^22_ij - _ i_i^22_i - _ j_j^22_j + o(^3 ) , where @xmath53 is bounded by the absolute value of ( and @xmath54 is equal to ) some homogenous cubic polynomial in the @xmath55 variables @xmath56 .",
    "taking the expectation , the linear term @xmath57=0 $ ] drops out .",
    "the quadratic terms",
    "@xmath58 = \\cov_{(ij)(kl)}[\\v\\t]$ ] are the covariance of @xmath13 under @xmath23 and they are proportional to @xmath59 .",
    "equation ( [ mom3 ] ) in section [ secgeneral ] shows that @xmath60=o(\\npp^{-2})$ ] , whence [ exnlo ] e[i ] = i ( ) + 12 _ ijkl(_ik_jl_ij - _ ik_i - _ jl_j)_(ij)(kl)[v ] + o(^-2 ) .",
    "the kronecker delta @xmath61 is @xmath62 for @xmath63 and @xmath64 otherwise .",
    "the variance of @xmath16 in leading order in @xmath59 is & = & e[(i - e[i])^2 ] e = + [ varlo ] & = & _ ijkl _ ( ij)(kl)[v ] , where @xmath65 denotes equality up to terms of order @xmath66 .",
    "so the leading order term for the variance of mutual information @xmath18 , and the leading and second leading order term for the mean can be expressed in terms of the covariance of @xmath13 under the posterior distribution @xmath23 .",
    "noninformative priors @xmath19 are commonly used if no explicit prior information is available on @xmath13 .",
    "most noninformative priors lead to a dirichlet posterior distribution @xmath67 with interpretation is now the sum of real _ and _ virtual counts , while it formerly denoted the real counts only . in case of haldane s prior ( @xmath68 ) , this change is ineffective . ]",
    "@xmath69 , where the @xmath70 are the number of outcomes @xmath71 , and @xmath72 comprises prior information explicit prior knowledge may also be specified by using virtual units , i.e.  by @xmath73 , leading again to a dirichlet posterior .",
    "the dirichlet distribution is defined as follows : p(v|v ) & = & 1(v)_ij_ij^_ij-1(_-1 ) + [ norm ] n(v ) & = & _ ij_ij^_ij-1(_-1 ) d^rsv= , where @xmath74 is the gamma function . mean and covariance of @xmath23 are [ ecov ] _ ij : = e[_ij]= _ ij = _",
    "ij , _ ( ij)(kl)[v ] = 1 + 1(_ij_ik_jl- _ ij_kl ) .      inserting ( [ ecov ] ) into ( [ exnlo ] ) and ( [ varlo ] ) we get , after some algebra , the mean and variance of the mutual information @xmath18 up to terms of order @xmath66 : [ exnlodi][jdef ] e[i ] & & j + , j : = _ ij_ij = i ( ) , + [ varlodi][kdef ] & & 1 + 1(k - j^2 ) , k : = _ ij_ij()^2 . @xmath75 and",
    "@xmath76 ( and @xmath77 , @xmath78 , @xmath79 , @xmath80 defined later ) depend on @xmath81 only , i.e.  are @xmath82 in @xmath83 . strictly speaking in ( [ exnlodi ] )",
    "we should make the expansion @xmath84 , i.e.  drop the @xmath85 , but the exact expression ( [ ecov ] ) for the covariance suggests to keep it .",
    "we compared both versions with the `` exact '' values ( from monte - carlo simulations ) for various parameters @xmath13 . in many cases the expansion in @xmath86 was more accurate , so we suggest to use this variant .",
    "the first term for the mean is just the descriptive index @xmath17 .",
    "the second term is a correction , small when @xmath49 is much larger than @xmath87 .",
    "kleiter @xcite determined the correction by monte carlo studies as @xmath88 .",
    "this is only correct if @xmath89 or @xmath90 is 2 .",
    "the expression @xmath91/n$ ] he determined for the variance has a completely different structure than ours .",
    "note that the mean is lower bounded by @xmath92 , which is strictly positive for large , but finite sample sizes , even if @xmath2 and @xmath3 are statistically independent and independence is perfectly represented in the data ( @xmath93 ) . on the other hand , in this case , the standard deviation @xmath94}\\sim { 1\\over\\npp}\\sim e[i]$ ] correctly indicates that the mean is still consistent with zero ( where @xmath95 means that @xmath96 and @xmath97 have the same accuracy , i.e. @xmath98 and @xmath99 ) .",
    "our approximations for the mean ( [ exnlodi ] ) and variance ( [ varlodi ] ) are good if @xmath100 is small . for dependent random variables",
    ", the central limit theorem ensures that @xmath22 converges to a gaussian distribution with mean @xmath37 $ ] and variance @xmath38 $ ] .",
    "since @xmath16 is non - negative it is more appropriate to approximate @xmath101 as a gamma ( @xmath102 scaled @xmath103 ) or a beta distribution with mean @xmath37 $ ] and variance @xmath38 $ ] , which are of course also asymptotically correct .",
    "a systematic expansion of all moments of @xmath22 to arbitrary order in @xmath59 is possible , but gets soon quite cumbersome . for the mean we give an exact expression in section [ secfurther ] , so we concentrate here on the variance , skewness and kurtosis of @xmath22 .",
    "the @xmath104 and @xmath105 central moments of @xmath13 under the dirichlet distribution are [ mom3 ] e[_a_b_c ] = [ 2_a_b_c - _ a_b_bc - _ b_c_ca - _ c_a_ab + _ a_ab_bc ] e[_a_b_c_d ] & = & 1 ^ 2 [ 3_a_b_c_d - _ c_d_a_ab - _ b_d_a_ac - _ b_c_a_ad + & & - _ a_d_b_bc - _ a_c_b_bd - _ a_b_c_cd",
    "+ & & + _ a_c_ab_cd + _",
    "a_b_ac_bd + _ a_b_ad_bc ] + o(^-3)with @xmath106 , @xmath107 being double indices , @xmath108 @xmath109 . expanding @xmath110 in @xmath111",
    "$ ] leads to expressions containing @xmath112 $ ] , which can be computed by a case analysis of all combinations of equal / unequal indices @xmath113 using ( [ norm ] ) .",
    "many terms cancel out leading to the above expressions .",
    "they allow us to compute the order @xmath66 term of the variance of @xmath18 .",
    "again , inspection of ( [ mom3 ] ) suggests to expand in @xmath114^{-1}$ ] , rather than in @xmath66 .",
    "the leading and second leading order terms of the variance are given below , [ var2ndo ] & = & k - j^2 + 1 + m+(r - 1)(s - 1)(- j)-q ( + 1)(+2 ) + o(^-3 ) + [ mdef ] m & : = & _ ij ( 1_ij-1_i-1 _ j+1 ) _",
    "ij , + [ qdef ] q & : = & 1-_ij_ij^2_i_j . @xmath75 and @xmath76 are defined in ( [ jdef ] ) and ( [ kdef ] ) . note that the first term @xmath115 also contains second order terms when expanded in @xmath59 .",
    "the leading order terms for the @xmath104 and @xmath105 central moments of @xmath22 are e[(i - e[i])^3 ] = 2 ^ 2[2j^3 - 3kj + l ] + 3 ^ 2[k + j^2 - p ] + o(^-3 ) , l : = _ ij_ij()^3 , p : = _ i(j_i)^2_i + _ j(j_j)^2_j , j_ij : = _ ij , e[(i - e[i])^4 ] = 3 ^ 2[k - j^2]^2 + o(^-3 ) , from which the skewness and kurtosis can be obtained by dividing by @xmath38^{3/2}$ ] and @xmath38 ^ 2 $ ] , respectively .",
    "one can see that the skewness is of order @xmath116 and the kurtosis is @xmath117 .",
    "significant deviation of the skewness from @xmath64 or the kurtosis from @xmath118 would indicate a non - gaussian @xmath16 .",
    "these expressions can be used to get an improved approximation for @xmath22 by making , for instance , an ansatz p(i|v)(1+b i+c i^2 ) p_0(i|,^2 ) and fitting the parameters @xmath119 , @xmath120 , @xmath121 , and @xmath122 to the mean , variance , skewness , and kurtosis expressions above .",
    "@xmath123 is any distribution with gaussian limit .",
    "> from this , quantiles @xmath124 , needed later ( and in @xcite ) , can be computed . a systematic expansion of arbitrarily high moments to arbitrarily high order in @xmath59 leads , in principle , to arbitrarily accurate estimates ( assuming convergence of the expansion ) .",
    "it is possible to get an exact expression for the mean mutual information @xmath37 $ ] under the dirichlet distribution . by noting that @xmath125 , ( @xmath126 )",
    ", one can replace the logarithms in the last expression of ( [ mi ] ) by powers . from ( [ norm ] )",
    "we see that @xmath127={\\gamma(\\n_{ij}+\\beta)\\gamma(\\npp)\\over \\gamma(\\n_{ij})\\gamma(\\npp+\\beta)}$ ] .",
    "taking the derivative and setting @xmath128 we get e[_ij_ij ] = dde[(_ij)^]_=1 = _ ij[(_ij+1)-(+1 ) ] .",
    "the @xmath129 function has the following properties ( see @xcite for details ) : ( z)=d(z)dz=(z)(z ) , ( z+1)=z + 12z - 112z^2 + o(1z^4 ) , [ psi2 ] ( n)=-+_k=1^n-11k , ( n+)=-+22 + 2_k=1^n12k-1 . the value of the euler constant @xmath130 is irrelevant here , since it cancels out . since the marginal distributions of @xmath131 and @xmath132 are also dirichlet ( with parameters @xmath133 and @xmath134 ) , we get similarly e[_i_i ] & = & 1_i_i[(_i+1)-(+1 ) ] , + e[_j_j ] & = & 1_j_j[(_j+1)-(+1 ) ] . inserting this into ( [ mi ] ) and rearranging terms",
    "we get the exact expression [ miexex ] e[i ] = 1_ij_ij [ ( _ ij+1)-(_i+1)- ( _ j+1)+(+1 ) ] .",
    "( this expression has independently been derived in @xcite in a different way . ) for large sample sizes , @xmath135 and ( [ miexex ] ) approaches the descriptive index @xmath17 as it should . inserting the expansion @xmath136 into ( [ miexex ] ) we also get the correction term @xmath137 of ( [ exnlodi ] ) .    the presented method ( with some refinements )",
    "may also be used to determine an exact expression for the variance of @xmath18 .",
    "all but one term can be expressed in terms of gamma functions .",
    "the final result after differentiating w.r.t .",
    "@xmath138 and @xmath139 can be represented in terms of @xmath129 and its derivative @xmath140 .",
    "the mixed term @xmath141 $ ] is more complicated and involves confluent hypergeometric functions , which limits its practical use @xcite .      for extreme quantiles @xmath142 or @xmath143",
    ", the accuracy of the derived approximations in the last sections can be poor and it is better to use tail approximations . in the following we briefly sketch how the scaling behavior of @xmath22 can be determined .",
    "we observe that @xmath18 is small iff @xmath144 describes near independent random variables @xmath2 and @xmath3 .",
    "this suggests the reparametrization @xmath145 in the integral ( [ midistr ] ) . in order to make this representation unique and consistent with @xmath146",
    ", we have to restrict the @xmath147 degrees of freedom ( @xmath148 ) to @xmath149 degrees of freedom by imposing @xmath150 constraints , for instance @xmath151 and @xmath152 ( @xmath153 occurs twice ) . only small @xmath154",
    "can lead to small @xmath18 .",
    "hence , for small @xmath16 we may expand @xmath18 in @xmath154 in expression ( [ midistr ] ) . inserting @xmath155 into ( [ miexp ] ) , we get @xmath156 with @xmath157 $ ] ( cf .",
    "( [ exnlo ] ) ) and @xmath158 and @xmath154 interpreted as @xmath14-dimensional matrix and vector .",
    "@xmath159 describes an @xmath14-dimensional ellipsoid of linear extension @xmath160 . due to the @xmath161 constraints on @xmath154 ,",
    "the @xmath154-integration is actually only over , say , @xmath162 and @xmath163 describes the surface of a @xmath164-dimensional ellipsoid only . approximating @xmath23 by @xmath165 in ( [ midistr ] ) , where @xmath166 we get p(i|v ) = b(v)i^|d2 - 1 + o(i^|d2 - 1 ) b(v)=s _ ( ) p(|v ) d^r+s-2 where @xmath167 is the ellipsoid s surface ( @xmath168 ) .",
    "note that @xmath169 still contains a jakobian from the non - linear coordinate transformation .",
    "so the small @xmath16 asymptotics is @xmath170 ( for any prior ) , but a closed form expression for the coefficient @xmath171 has yet to be derived .",
    "similarly we may derive the scaling behavior of @xmath22 for @xmath172 .",
    "@xmath18 can be written as @xmath173 , where @xmath174 is the entropy . without loss of generality",
    "we may assume @xmath175 .",
    "@xmath176 with equality iff @xmath177 for all @xmath178 .",
    "@xmath179 with equality iff @xmath2 is a deterministic function of @xmath3 .",
    "together , @xmath180 iff @xmath181 , where @xmath182 is any onto map and the @xmath183 respect the constraints @xmath184 .",
    "this suggests the reparametrization @xmath185 in the integral ( [ midistr ] ) for each choice of @xmath186 and suitable constraints on @xmath187 and @xmath188 .",
    "in order to approximate the distribution of mutual information in practice , one needs consider implementation issues and the computational complexity of the overall method .",
    "this is what we set out to do in the following .",
    "regarding computational complexity , there are short and fast implementations of @xmath129 .",
    "the code of the gamma function in @xcite , for instance , can be modified to compute the @xmath129 function . for integer and half - integer values one may create a lookup table from ( [ psi2 ] ) .",
    "the needed quantities @xmath75 , @xmath76 , @xmath77 , @xmath78 , and @xmath80 ( depending on @xmath83 ) involve a double sum , @xmath79 only a single sum , and the @xmath189 quantities @xmath190 and @xmath191 also only a single sum .",
    "hence , the computation time for the ( central ) moments is of the same order @xmath192 as for @xmath17 .    with respect to the quality of the approximation ,",
    "let us briefly consider the case of the variance .",
    "the expression for the exact variance has been taylor - expanded in @xmath193 , so the relative error @xmath194_{approx}-\\mbox{\\scriptsize var}[i]_{exact}}{\\mbox{\\scriptsize var}[i]_{exact}}}$ ] of the approximation is of the order @xmath195 , _ if _ @xmath2 and @xmath3 are dependent . in the opposite case ,",
    "the @xmath25 term in the sum drops itself down to order @xmath196 resulting in a reduced relative accuracy @xmath197 of the approximated variance .",
    "these results were confirmed by numerical experiments that we realized by monte carlo simulation to obtain `` exact '' values of the variance for representative choices of @xmath29 , @xmath90 , @xmath89 , and @xmath1 .",
    "the approximation for the variance , together with those for the skewness and kurtosis , and the exact expression for the mean , allow a good description of the distribution @xmath22 to be obtained for not too small sample bin sizes @xmath36 .",
    "we want to conclude with some notes on _ useful _ accuracy .",
    "the hypothetical prior sample sizes @xmath198 can all be argued to be non - informative @xcite . since the central moments are expansions in @xmath59 , the second leading order term can be freely adjusted by adjusting @xmath199 $ ] .",
    "so one may argue that anything beyond the leading order term is free to will , and the leading order terms may be regarded as accurate as we can specify our prior knowledge . on the other hand ,",
    "exact expressions have the advantage of being safe against cancellations .",
    "for instance , the leading orders of @xmath37 $ ] and @xmath200 $ ] do not suffice to compute the leading order term of @xmath38 $ ] .",
    "let us now consider approximating the overall distribution of mutual information based on the mean and the variance .",
    "fitting a normal distribution is an obvious possible choice , as the central limit theorem ensures that @xmath21 converges to a gaussian distribution with mean @xmath37 $ ] and variance @xmath38 $ ] .",
    "since @xmath16 is non - negative , it is also worth considering the approximation of @xmath101 by a gamma ( i.e. , a scaled @xmath201 ) .",
    "another natural candidate is the beta distribution , which is defined for variables in the @xmath202 $ ] real interval .",
    "@xmath16 can be made such a variable by a simple normalization .",
    "of course the gamma and the beta are asymptotically correct , too .",
    "_ distribution of mutual information for two binary random variables ( the labelling of the horizontal axis is the percentage of i_@xmath203 . )",
    "there are three groups of curves , for different choices of counts @xmath204 .",
    "the upper group is related to the vector @xmath205 , the intermediate one to the vector @xmath206 , and the lower group to @xmath207 .",
    "each group shows the `` exact '' distribution and three approximating curves , based on the gaussian , gamma and beta distributions._,scaledwidth=100.0% ]    we report a graphical comparison of the different approximations by focusing on the special case of binary random variables , and on three possible vectors of counts .",
    "figure [ fig1 ] compares the `` exact '' distribution of mutual information , computed via monte carlo simulation , with the approximating curves .",
    "these curves have been fitted using the exact mean and the approximated variance of the preceding section .",
    "the figure clearly shows that all the approximations are rather good , with a slight preference for the beta approximation .",
    "the curves tend to do worse for smaller sample sizes , as expected .",
    "higher moments may be used to improve the accuracy ( section [ secgeneral ] ) , or this can be improved using our considerations about tail approximations in section [ seclasia ] .",
    "in the following we generalize the setup to include the case of missing data , which often occurs in practice .",
    "we extend the counts @xmath36 to include @xmath208 , which counts the number of instances in which only @xmath3 is observed ( i.e. , the number of @xmath209 instances ) , and the counts @xmath210 for the number of @xmath211 instances , where only @xmath2 is observed .",
    "we make the common assumption that the missing data mechanism is ignorable ( _ missing at random _ and _ distinct _ ) @xcite . the probability distribution of @xmath3 given that @xmath2 is missing coincides with the marginal @xmath132 , and vice versa , as a consequence of this assumption .",
    "the sample size @xmath1 is now @xmath212 , where @xmath213 is the number of complete units .",
    "let @xmath214 denote as before the vector of counts , now including the counts @xmath210 and @xmath208 , for all @xmath178 and @xmath215 .",
    "the probability of a specific data set @xmath216 , given @xmath13 , hence , is @xmath217 .",
    "assuming a uniform prior @xmath218 , bayes rule leads to the posterior ( which is also the likelihood in case of uniform prior ) p(v| ) = 1( ) _ ij_ij^n_ij _",
    "i_i^n_i ? _ j_j^n_?j(_-1 ) where the normalization @xmath219 is chosen such that @xmath220 . with missing data",
    "there is , in general , no closed form expression for @xmath219 any more ( cf .  ( [ norm ] ) ) .    in the following ,",
    "we restrict ourselves to a discussion of leading order ( in @xmath221 ) expressions . in leading order",
    ", any dirichlet prior with @xmath222 leads to the same results , hence we can simply assume a uniform prior",
    ". in leading order , the mean @xmath223 $ ] coincides with the mode of @xmath224 , i.e. the maximum likelihood estimate of @xmath13 .",
    "the log - likelihood function @xmath225 is l(v| ) = _",
    "ijn_ij_ij + _ i n_i?_i + _ j n_?j_j - (  ) - ( _ -1 ) , where we have introduced the lagrange multiplier @xmath226 to take into account the restriction @xmath146 .",
    "the maximum is at @xmath227 .",
    "multiplying this by @xmath29 and summing over @xmath178 and @xmath215 we obtain @xmath228 . the maximum likelihood estimate @xmath12 is , hence , given by [ em ] _ ij=1n(n_ij+n_i?_ij_i + n_?j_ij_j ) .",
    "this is a non - linear equation in @xmath229 , which , in general , has no closed form solution .",
    "nevertheless eq .",
    "( [ em ] ) can be used to approximate @xmath229 .",
    "( [ em ] ) coincides with the popular expectation - maximization ( em ) algorithm @xcite if one inserts a first estimate @xmath230 into the r.h.s .  of ( [ em ] ) and then uses the resulting l.h.s .",
    "@xmath231 as a new estimate , etc .",
    "this iteration ( quickly ) converges to the maximum likelihood solution ( if missing instances are not too frequent ) . using this we can compute the leading order term for the mean of the mutual information ( and of any other function of @xmath29 ) : @xmath37=i(\\hat{\\v\\t})+o(n^{-1})$ ]",
    ". the leading order term for the covariance can be obtained from the second derivative of @xmath77 .",
    "the @xmath232 hessian matrix @xmath233 of @xmath234 and the second derivative in the direction of the @xmath14-dimensional column vector @xmath235 are _(ij)(kl)[v ] : = -l_ij_kl = n_ij_ij^2_ik_jl + n_i?_i^2_ik + n_?j_j^2_jl , v^tv = _",
    "ijklv_ij _(ij)(kl)v_kl =",
    "_ ijn_ij_ij^2v_ij^2 + _ in_i?_i^2v_i^2 + _ jn_?j_j^2v_j^2 0 .",
    "this shows that @xmath234 is a convex function of @xmath13 , hence @xmath224 has a single ( possibly degenerate ) global maximum .",
    "@xmath77 is strictly convex if @xmath236 for all @xmath237 , since @xmath238 @xmath239 in this case .",
    "( note that positivity of @xmath210 for all @xmath178 is not sufficient , since @xmath240 for @xmath241 is possible .",
    "actually @xmath242 . )",
    "this implies a unique global maximum , which is attained in the interior of the probability simplex .",
    "since em is known to converge to a local maximum , this shows that in fact _ em always converges to the global maximum_.      with _(ij)(kl ) : = _(ij)(kl ) [ ] = n , [ kernela ] _",
    "ij:=n_ij^2n_ij , _",
    "i?:=n_i^2n_i ? , _ ?",
    "j:=n_j^2n_?j and @xmath243 , we can represent the posterior to leading order as an @xmath244-dimensional gaussian : [ postgauss ] p(v| ) ~e^-12v^tv ( _ ) .",
    "the easiest way to compute the covariance ( and other quantities ) is to also represent the @xmath245-function as a narrow gaussian of width @xmath246 .",
    "inserting @xmath247 into ( [ postgauss ] ) , where @xmath248 for all @xmath237 ( hence @xmath249 ) , leads to a full @xmath14-dimensional gaussian with kernel @xmath250 , @xmath251 . the covariance of a gaussian with kernel @xmath252 is @xmath253 . using the sherman - morrison formula",
    "@xmath254 @xcite and @xmath255 we get [ covmis ] _ ( ij)(kl)[v ] : = e[_ij_kl ] [ ^-1]_(ij)(kl ) = _ ( ij)(kl ) , where @xmath65 denotes equality up to terms of order @xmath196 .",
    "singular matrices @xmath256 are easily avoided by choosing a prior such that @xmath236 for all @xmath178 and @xmath215 .",
    "@xmath256 may be inverted exactly or iteratively , the latter by a trivial inversion of the diagonal part @xmath257 and by treating @xmath258 as a perturbation .      in the case",
    "only one variable is missing , say @xmath259 , closed form expressions can be obtained .",
    "if we sum ( [ em ] ) over @xmath215 we get @xmath260 . inserting @xmath261 into the r.h.s .  of ( [ em ] ) and solving w.r.t.@xmath81 , we get the explicit expression [ pimfo ] _",
    "ij = n_i+n_i?n .",
    "furthermore , it can easily be verified ( by multiplication ) that @xmath262 $ ] has inverse @xmath263_{(ij)(kl ) } =    { 1\\over n}[\\rho_{ij}\\delta_{ik}\\delta_{jl } -    { \\rho_{ij}\\rho_{kl}\\over\\rho_{i\\p}\\!+\\!\\rho_{i?}}\\delta_{ik } ] $ ] . with the abbreviations q_i?:= _",
    "i?_i?+_i q:=_i_iq_i ?",
    "we get @xmath264_{ij } = \\sum_{kl}[\\v a^{-1}]_{(ij)(kl ) } = { 1\\over n}\\rho_{ij}\\tilde q_{i?}$ ] and @xmath265 . inserting everything into ( [ covmis ] ) we get _",
    "( ij)(kl)[v ] . inserting this expression for the covariance into ( [ varlo ] ) , using @xmath266=\\hat{\\v\\t}+o(n^{-1})$ ] ,",
    "we finally get the leading order term in @xmath267 for the variance of mutual information : [ varmfo ] [ k - j^2/q - p ] , k : = _ ij_ij ( ) ^2 , p : = _ ij_i^2q_i?_i ? , j : = _",
    "i j_iq_i ? , j_i:=_j_ij .",
    "a closed form expression for @xmath268 also exists .",
    "symmetric expressions for the case when only @xmath2 is missing can be obtained .",
    "note that for the complete data case @xmath269 , we have @xmath270 , @xmath271 , @xmath272 , @xmath273 , @xmath274 , and @xmath275 , consistent with ( [ varlodi ] ) .",
    "there is at least one reason for minutely having inserted all expressions into each other and introducing quite a number definitions . in the presented form",
    "all expressions involve at most a double sum .",
    "hence , the overall time for computing the mean and variance when only one variable is missing is @xmath276 .      in the general case when both variables are missing ,",
    "each em iteration ( [ em ] ) for @xmath81 needs @xmath276 operations . the naive inversion of @xmath256 needs time @xmath277 , and using it to compute var[@xmath16 ] time @xmath279 .",
    "since the contribution from unlabelled-@xmath2 instances can be interpreted as a rank @xmath89 modification of @xmath256 in the case of when @xmath2 is not missing , one can use woodbury s formula @xmath280^{-1 } =    \\v b^{-1}-\\v b^{-1}\\v u [ \\v d^{-1}+    \\v v^t\\v b^{-1}\\v u]^{-1}\\v v^t\\v b^{-1 } $ ] @xcite with @xmath281 , @xmath282 , and @xmath283 , to reduce the inversion of the @xmath232 matrix @xmath256 to the inversion of a single @xmath89-dimensional matrix .",
    "the result can be written in the form [ aigen ] [ ^-1]_(ij)(kl ) = 1n , f_ijl : = _ ij_jl - _ ij_kl_i?+_i+ , g_mn : = _ ? n_mn+f_mn .",
    "the result for the covariance ( [ covmis ] ) can be inserted into ( [ varlo ] ) to obtain the leading order term for the variance : [ vargen ] ^t^-1 - ( ^t^-1v e)^2/(^t^-1 ) _ij:=. inserting ( [ aigen ] ) into ( [ vargen ] ) and rearranging terms appropriately , we can compute var[@xmath16 ] in time @xmath276 plus the time @xmath284 to compute the @xmath285 matrix @xmath286 and time @xmath287 to invert it , plus the time @xmath288 for determining @xmath81 , where @xmath289 is the number of iterations of em . of course , one can and should always choose @xmath290 .",
    "note that these expressions converge to the exact values when @xmath1 goes to infinity , irrespectively of the amount of missing data .",
    "the results in the preceding sections provide fast and reliable methods to approximate the distribution of mutual information from either complete or incomplete data .",
    "the derived tools have been obtained in the theoretically sound framework of bayesian statistics , which we regard as their basic justification .",
    "as these methods are available for the first time , it is natural to wonder what their possible uses can be on the application side or , stated differently , what can be gained in practice moving from descriptive to inductive methods .",
    "we believe that the impact on real applications can be significant , according to three main scenarios : _ robust inference methods _ , _ inferring models that perform well _ , and _ fast learning from massive data sets_. in the following we use classification as a thread to illustrate the above scenarios .",
    "_ classification _ is one of the most important techniques for knowledge discovery in databases @xcite .",
    "a classifier is an algorithm that allocates new objects to one out of a finite set of previously defined groups ( or _ classes _ ) on the basis of observations on several characteristics of the objects , called _ attributes _ or _",
    "features_. classifiers are typically learned from data , making explicit the knowledge that is hidden in databases , and using this knowledge to make predictions about new data .",
    "an obvious observation is that descriptive methods can not compete , by definition , with inductive ones when robustness is concerned .",
    "hence , the results presented in this paper lead naturally to a spin - off for reliable methods of inference .    let us focus on classification problems , for the sake of explanation . applying robust methods to classification means to produce classifications that are correct with a given probability . it is easy to imagine sensible ( e.g. , nuclear , medical ) applications where reliability of classification is a critical issue . to achieve reliability ,",
    "a necessary step consists in associating a posterior probability ( i.e. , a guarantee level ) to classification models inferred from data , such as classification trees or bayesian nets .",
    "let us consider the case of bayesian networks .",
    "these are graphical models that represent structures of ( in)dependence by directed acyclic graphs , where nodes in the graph are regarded as random variables @xcite .",
    "two nodes are connected by an arc when there is direct stochastic dependence between them .",
    "inferring bayesian nets from data is often done by connecting nodes with significant value of descriptive mutual information .",
    "little work has been done on robustly inferring bayesian nets , probably because of the difficulty to deal with the distribution of mutual information , with the notable exception of kleiter s work @xcite .",
    "joining kleiter s work with ours might lead to inference of bayesian network structures that are correct with a given probability .",
    "some work has already been done to this direction @xcite .",
    "feature selection might also benefit from robust methods .",
    "_ feature selection _ is the problem of reducing the number of feature variables to deal with in classification problems .",
    "features can reliably be discarded only when they are irrelevant to the class with high probability .",
    "this needs knowledge of the distribution of mutual information . in section [ secfs ]",
    "we propose a filter based on the distribution of mutual information to address this problem .",
    "it is well - known that model complexity must be in proper balance with available data in order to achieve good classification accuracy .",
    "in fact , unjustified complexity of inferred models leads classifiers almost inevitably to _ overfitting _ , i.e.  to memorize the available sample rather than extracting regularities from it that are needed to make useful predictions on new data @xcite .",
    "overfitting could be avoided by using the distribution of mutual information . with bayesian nets , for example ,",
    "this could be achieved by drawing arcs between nodes only if these are supported by data with high probability .",
    "this is a way to impose a bias towards simple structures .",
    "it has to be verified whether or not this approach can systematically lead to better accuracy .",
    "model complexity can also be reduced by discarding features .",
    "this can be achieved by including a feature only when its mutual information with the class is significant with high probability .",
    "this approach is taken in section [ secfs ] , where we show that it can effectively lead to better prediction accuracy of the resulting models .",
    "another very promising application of the distribution of mutual information is related to _ massive data sets_. these are huge samples , which are becoming more and more available in real applications , and which constitute a serious challenge for machine learning and statistical applications . with massive data sets it is impractical to scan all the data , so classifiers must be reliably inferred by accessing only a small subset of the units .",
    "recent work has highlighted @xcite how inductive methods allow this to be realized .",
    "the intuition is the following : the inference phase stops reading data when the inferred model , say a bayesian net , has reached a given posterior probability . by choosing such probability sufficiently high , one can be arbitrarily confident that the inferred model will not change much by reading the neglected data , making the remaining units superfluous .",
    "feature selection is a basic step in the process of building classifiers @xcite .",
    "in fact , even if theoretically more features should provide one with better _ prediction accuracy _",
    "( i.e. , the relative number of correct predictions ) , in real cases it has been observed many times that this is not the case @xcite and that it is important to discard irrelevant , or weakly relevant features .    the purpose of this section is to illustrate how the distribution of mutual information can be applied in this framework , according to some of the ideas in section [ sec : well ] .",
    "our goal is inferring simple models that avoid overfitting and have an equivalent or better accuracy with respect to models that consider all the original features .",
    "two major approaches to feature selection are commonly used in machine learning @xcite : _ filter _ and _ wrapper _ models .",
    "the filter approach is a preprocessing step of the classification task .",
    "the wrapper model is computationally heavier , as it implements a search in the feature space using the prediction accuracy as reward measure . in the following",
    "we focus our attention on the filter approach : we define two new filters and report experimental analysis about them , both with complete and incomplete data .",
    "we consider the well - known filter ( f ) that computes the empirical mutual information between features and the class , and discards low - valued features @xcite .",
    "this is an easy and effective approach that has gained popularity with time .",
    "cheng reports that it is particularly well suited to jointly work with bayesian network classifiers , an approach by which he won the _ 2001 international knowledge discovery competition _ @xcite .",
    "the ` weka ' data mining package implements it as a standard system tool ( see @xcite ) .",
    "a problem with this filter is the variability of the empirical mutual information with the sample .",
    "this may cause wrong judgments of relevance , when those features are selected for which the mutual information exceeds a fixed threshold @xmath291 . in order for the selection to be robust , we must have some guarantee about the actual value of mutual information .",
    "we define two new filters .",
    "the _ backward filter _ ( bf ) _ discards _ an attribute if @xmath292 where @xmath16 denotes the mutual information between the feature and the class , @xmath291 is an arbitrary ( low ) positive threshold and @xmath293 is an arbitrary ( high ) probability .",
    "the _ forward filter _ ( ff ) _ includes _ an attribute if @xmath294 , with the same notations .",
    "bf is a conservative filter , along the lines discussed about robustness in section [ sec : robust ] , because it will only discard features after observing substantial evidence supporting their irrelevance .",
    "ff instead will tend to use fewer features ( aiming at producing classifiers that perform better ) , i.e. only those for which there is substantial evidence about them being useful in predicting the class .",
    "the next sections present experimental comparisons of the new filters and the original filter f.      for the following experiments we use the _ naive bayes classifier _ @xcite .",
    "this is a good classification model  despite its simplifying assumptions @xcite , which often competes successfully with much more complex classifiers from the machine learning field , such as c4.5 @xcite .",
    "the experiments focus on the incremental use of the naive bayes classifier , a natural learning process when the data are available sequentially : the data set is read instance by instance ; each time , the chosen filter selects a subset of attributes that the naive bayes uses to classify the new instance ; the naive bayes then updates its knowledge by taking into consideration the new instance and its actual class .",
    "the incremental approach allows us to better highlight the different behaviors of the empirical filter ( f ) and those based on the distribution of mutual information ( bf and ff ) .",
    "in fact , for increasing sizes of the learning set the filters converge to the same behavior .",
    "for each filter , we are interested in experimentally evaluating two quantities : for each instance of the data set , the average number of correct predictions ( namely , the prediction accuracy ) of the naive bayes classifier up to such instance ; and the average number of attributes used . by these quantities we can compare the filters and judge their effectiveness .",
    "the implementation details for the following experiments include : using the beta approximation ( section [ ba ] ) to the distribution of mutual information , with the exact mean ( [ miexex ] ) and the @xmath295-approximation of the variance , given in ( [ var2ndo ] ) ; using the uniform prior for the naive bayes classifier and all the filters ; and setting the level @xmath293 for the posterior probability to @xmath296 .",
    "as far as @xmath297 is concerned , we can not set it to zero because the probability that two variables are independent ( @xmath298 ) is zero according to the inferential bayesian approach .",
    "we can interpret the parameter @xmath297 as a degree of dependency strength below which attributes are deemed irrelevant .",
    "we set @xmath297 to @xmath299 , in the attempt of only discarding attributes with negligible impact on predictions .",
    "as we will see , such a low threshold can nevertheless bring to discard many attributes .",
    "table [ tab1 ] lists ten data sets used in the experiments for complete data .",
    "these are real data sets on a number of different domains .",
    "for example , shuttle - small reports data on diagnosing failures of the space shuttle ; lymphography and hypothyroid are medical data sets ; spam is a body of e - mails that can be spam or non - spam ; etc .",
    "= 1ex    .[tab1]_complete data sets used in the experiments , together with their number of features , of instances and the relative frequency of the mode .",
    "all but the spam data sets are available from the uci repository of machine learning data sets @xcite .",
    "the spam data set is described in @xcite and available from androutsopoulos s web page . _ [ cols=\"<,>,>,>\",options=\"header \" , ]     the results in table [ tab4 ] show that the filters behave very similarly to the case of complete data .",
    "the filter ff still selects the smallest number of features , and this number usually increases with f and even more with bf .",
    "the selection can be very pronounced , as with the hypothyroidloss data set .",
    "this is also the only data set for which the prediction accuracies of f and ff are significantly different , in favor of ff .",
    "this is better highlighted by figure [ fig5 ] .",
    "_ prediction accuracies of the naive bayes with filters f and ff on the hypothyroidloss data set .",
    "( bf is not reported because there is no significant difference with the f curve . )",
    "the differences between f and ff are significant in the range of observations 71374 .",
    "the maximum difference is achieved at observation 71 , where the accuracies are 0.986 ( ff )  vs. 0.930 ( f)._,scaledwidth=90.0% ]      the most prominent evidence from the experiments is the better performance of ff versus the traditional filter f. in this note we look at ff from another perspective to exemplify and explain its behavior .",
    "ff includes an attribute if @xmath294 , according to its definition .",
    "let us assume that ff is realized by means of the gaussian rather than the beta approximation ( as in the experiments above ) , and let us choose @xmath300 .",
    "the condition @xmath294 becomes @xmath301 - 2\\cdot \\sqrt{\\var[i]}$ ] , or , in an approximate way , @xmath302}$ ] , given that @xmath17 is the first - order approximation of @xmath37 $ ] ( cf . ) .",
    "we can regard @xmath303}$ ] as a new threshold @xmath304 . under this interpretation",
    ", we see that ff is approximately equal to using the filter f with the bigger threshold @xmath304 .",
    "this interpretation makes it also clearer why ff can be better suited than f for sequential learning tasks . in sequential learning ,",
    "@xmath38 $ ] decreases as new units are read ; this makes @xmath304 a self - adapting threshold that adjusts the level of caution ( in including features ) as more units are read . in the limit",
    ", @xmath304 is equal to @xmath291 .",
    "this characteristic of self - adaptation , which is absent in f , seems to be decisive to the success of ff .",
    "this paper has provided fast and reliable analytical approximations for the variance , skewness and kurtosis of the posterior distribution of mutual information , with guaranteed accuracy from @xmath25 to @xmath0 , as well as the exact expression of the mean .",
    "these results allow the posterior distribution of mutual information to be approximated both from complete and incomplete data . as an example",
    ", this paper has shown that good approximations can be obtained by fitting common curves with the mentioned mean and variance . to our knowledge , this is the first work that addresses the analytical approximation of the distribution of mutual information .",
    "analytical approximations are important because their implementation is shown to lead to computations of the same order of complexity as needed for the empirical mutual information .",
    "this makes the inductive approach a serious competitor of the descriptive use of mutual information for many applications .",
    "in fact , many applications are based on descriptive mutual information .",
    "we have discussed how many of these could benefit from moving to the inductive side , and in particular we have shown how this can be done for feature selection . in this context , we have proposed the new filter ff , which is shown to be more effective for sequential learning tasks than the traditional filter based on empirical mutual information .",
    "i.  androutsopoulos , j.  koutsias , k.  v. chandrinos , g.  paliouras , and d.  spyropoulos .",
    "an evaluation of naive bayesian anti - spam filtering . in g.",
    "potamias , v.  moustakis , and m.  van someren , editors , _ proceedings of the workshop on machine learning in the new information age _ ,",
    "pages 917 , 2000 .",
    "11th european conference on machine learning .",
    "u.  m. fayyad and k.  b. irani .",
    "multi - interval discretization of continuous - valued attributes for classification learning . in _ proceedings of the thirteenth international joint conference on artificial intelligence _ , pages 10221027 , san francisco , ca , 1993 .",
    "morgan kaufmann .",
    "m.  hutter .",
    "distribution of mutual information . in t.",
    "g. dietterich , s.  becker , and z.  ghahramani , editors , _ advances in neural information processing systems 14 _ , pages 399406 , cambridge , ma , 2002 . mit press .",
    "m.  hutter and m.  zaffalon .",
    "bayesian treatment of incomplete discrete data applied to mutual information and feature selection . in r.",
    "kruse a.  gnter and b.  neumann , editors , _ proceedings of the twenty - sixth german conference on artificial intelligence ( ki-2003 ) _ , volume 2821 of _ lecture notes in computer science _ ,",
    "pages 396406 , heidelberg , 2003 .",
    "springer .",
    "g.  h. john , r.  kohavi , and k.  pfleger .",
    "irrelevant features and the subset selection problem . in w.",
    "w. cohen and h.  hirsh , editors , _ proceedings of the eleventh international conference on machine learning _ , pages 121129 , new york , 1994 .",
    "morgan kaufmann .",
    "d.  pelleg and a.  moore . using tarjan s red rule for fast dependency tree construction . in s.",
    "becker , s.  thrun , and k.  obermayer , editors , _ advances in neural information processing systems 15 _ , pages 825832 , cambridge , ma , 2003 . mit press .",
    "m.  zaffalon and m.  hutter .",
    "robust feature selection by mutual information distributions . in a.",
    "darwiche and n.  friedman , editors , _ proceedings of the 18th international conference on uncertainty in artificial intelligence ( uai-2002 ) _ , pages 577584 , san francisco , ca . , 2002 .",
    "morgan kaufmann ."
  ],
  "abstract_text": [
    "<S> mutual information is widely used , in a descriptive way , to measure the stochastic dependence of categorical random variables . in order to address questions such as the reliability of the descriptive value </S>",
    "<S> , one must consider sample - to - population inferential approaches . </S>",
    "<S> this paper deals with the posterior distribution of mutual information , as obtained in a bayesian framework by a second - order dirichlet prior distribution . </S>",
    "<S> the exact analytical expression for the mean , and analytical approximations for the variance , skewness and kurtosis are derived . </S>",
    "<S> these approximations have a guaranteed accuracy level of the order @xmath0 , where @xmath1 is the sample size . leading order approximations for the mean and the variance </S>",
    "<S> are derived in the case of incomplete samples . </S>",
    "<S> the derived analytical expressions allow the distribution of mutual information to be approximated reliably and quickly . </S>",
    "<S> in fact , the derived expressions can be computed with the same order of complexity needed for descriptive mutual information . </S>",
    "<S> this makes the distribution of mutual information become a concrete alternative to descriptive mutual information in many applications which would benefit from moving to the inductive side </S>",
    "<S> . some of these prospective applications are discussed , and one of them , namely _ feature selection _ , is shown to perform significantly better when inductive mutual information is used .    mutual information , cross entropy , dirichlet distribution , second order distribution , expectation and variance of mutual information , feature selection , filters , naive bayes classifier , bayesian statistics . </S>"
  ]
}