{
  "article_text": [
    "in the context of the predictive maintenance of the french railway switches ( or points ) which enable trains to be guided from one track to another at a railway junction , we have been brought to parameterize switch operations signals representing the electrical power consumed during a point operation ( see figure [ signal_intro ] ) .",
    "the final objective is to exploit these parameters for the identification of incipient faults .",
    "the method we propose to characterize signals is based on a regression model incorporating a discrete hidden process allowing abrupt or smooth switchings between various regression models .",
    "this approach has a connection with the switching regression model introduced by quandt and ramsey @xcite and is very linked to the mixture of experts ( me ) model introduced by jordan and jacobs @xcite by the using of a time - dependant logistic transition function .",
    "the me model , as discussed in @xcite , uses a conditional mixture modeling where the model parameters are estimated by the expectation maximization ( em ) algorithm @xcite@xcite .",
    "other alternative approaches are based on hidden markov models in a context of regression @xcite .",
    "a dedicated em algorithm including a multi - class iterative reweighted least - squares ( irls ) algorithm @xcite is proposed to estimate the model parameters .",
    "section 2 introduces the proposed model and section 3 describes the parameters estimation via the em algorithm .",
    "the fourth section is devoted to the experimental study using simulated data and real data .",
    "we represent a signal by the random sequence @xmath0 of @xmath1 real observations , where @xmath2 is observed at time @xmath3 .",
    "this sample is assumed to be generated by the following regression model with a discrete hidden logistic process @xmath4 , where @xmath5 : @xmath6 in this model , @xmath7 is the @xmath8-dimensional coefficients vector of a @xmath9 degree polynomial , @xmath10 is the time dependant @xmath8-dimensional covariate vector associated to the parameter @xmath7 and the @xmath11 are independent random variables distributed according to a gaussian distribution with zero mean and variance @xmath12 .      this section defines the probability distribution of the process @xmath4 that allows the switching from one regression model to another .",
    "the proposed hidden logistic process supposes that the variables @xmath13 , given the vector @xmath14 , are generated independently according to the multinomial distribution @xmath15 , where @xmath16 is the logistic transformation of a linear function of the time - dependant covariate @xmath17 , @xmath18 is the @xmath19-dimensional coefficients vector associated to the covariate @xmath20 and @xmath21 .",
    "thus , given the vector @xmath14 , the distribution of @xmath22 can be written as : @xmath23 where @xmath24 if @xmath25 i.e when @xmath2 is generated by the @xmath26 regression model , and @xmath27 otherwise .",
    "the pertinence of the logistic transformation in terms of flexibility of transition can be illustrated through simple examples with @xmath28 components . as it can be shown in figure [ logistic_function_k=2_p=012 ] ( left )",
    ", the dimension @xmath29 of @xmath30 controls the number of changes in the temporal variation of @xmath31 .",
    "more particularly , if the goal is to segment the signals into convex homogenous parts , the dimension @xmath29 of @xmath30 must be set to @xmath32 .",
    "the quality of transitions and the change time point are controlled by the components values of the vector @xmath30 ( see figures [ logistic_function_k=2_p=012 ] ( middle ) and ( right ) ) .    [",
    "cols=\"^,^,^ \" , ]",
    "from the model given by equation ( [ eq.regression model ] ) , it can be proved that the random variable @xmath33 is distributed according to the normal mixture density @xmath34 where @xmath35 is the parameter vector to be estimated .",
    "the parameter @xmath36 is estimated by the maximum likelihood method .",
    "as in the classic regression models we assume that , given @xmath14 , the @xmath37 are independent .",
    "this also involves the independence of @xmath2 @xmath38 .",
    "the log - likelihood of @xmath36 is then written as : @xmath39 since the direct maximization of this likelihood is not straightforward , we use the expectation maximization ( em ) algorithm @xcite@xcite to perform the maximization .",
    "the proposed em algorithm starts from an initial parameter @xmath40 and alternates the two following steps until convergence :    [ [ e - step - expectation ] ] * e step ( expectation ) : * + + + + + + + + + + + + + + + + + + + + + + +    this step consists of computing the expectation of the complete log - likelihood @xmath41 , given the observations and the current value @xmath42 of the parameter @xmath36 ( @xmath43 being the current iteration ) : @xmath44\\nonumber\\\\ & = & \\sum_{i=1}^{n}\\sum_{k=1}^k t^{(m)}_{ik}\\log ( \\pi_{ik}({\\mathbf{w}})\\mathcal{n}(x_{i};{\\boldsymbol{\\beta}}^t_k{\\boldsymbol{r}}_{i},\\sigma^2_k ) ) \\enspace,\\end{aligned}\\ ] ] where @xmath45 is the posterior probability that @xmath2 originates from the @xmath26 regression model .",
    "as shown in the expression of @xmath46 , this step simply requires the computation of @xmath47 .",
    "[ [ m - step - maximization ] ] * m step ( maximization ) : * + + + + + + + + + + + + + + + + + + + + + + + +    in this step , the value of the parameter @xmath36 is updated by computing the parameter @xmath48 maximizing the expectation @xmath46 with respect to @xmath36 . the maximization of @xmath46 can be performed by separately maximizing @xmath49    the maximization of @xmath50 with respect to @xmath51 is a multinomial logistic regression problem weighted by the @xmath47 .",
    "we use a multi - class iterative reweighted least squares ( irls ) algorithm @xcite@xcite@xcite to solve it . maximizing @xmath52 with respect to @xmath53 consists of analytically solving a weighted least - squares problem .",
    "in addition to providing a signal parametrization , the proposed approach can be used to denoise and segment signals .",
    "the denoised signal can be approximated by the expectation @xmath54 where @xmath55 is the parameters vector obtained at the convergence of the algorithm . on the other hand , a signal segmentation can also be deduced by computing the estimated label @xmath56 of @xmath2 : @xmath57 .",
    "this section is devoted to the evaluation of the proposed algorithm using simulated and real data sets .",
    "two evaluation criteria are used in the simulations : the misclassification rate between the simulated partition and the estimated partition and the euclidian distance between the denoised simulated signal and the estimated denoised signal normalized by the sample size @xmath1 .",
    "the proposed approach is compared to the piecewise regression approach @xcite .",
    "each signal is generated according to the regression model with a hidden logistic process defined by eq ( [ eq.regression model ] ) .",
    "the number of states of the hidden variable is fixed to @xmath58 and the order of regression is set to @xmath59 .",
    "the order of the logistic regression is fixed to @xmath60 what guarantees a segmentation into convex intervals .",
    "we consider that all signals are observed over @xmath61 seconds . for each size",
    "@xmath1 we generate 20 samples .",
    "the values of assessment criteria are averaged over the 20 samples .",
    "figure [ fig.error rates ] ( left ) shows the misclassification rate obtained by the two approaches in relation to the sample size @xmath1 .",
    "it can be observed that the proposed approach is more stable for a few number of observations .",
    "figure [ fig.error rates ] ( right ) shows the results obtained by the two approaches in terms of signal denoising .",
    "it can be observed that the proposed approach provides a more accurate denoising of the signal compared to the piecewise regression approach . for the proposed model",
    ", the optimal values of @xmath62 has also been estimated by computing the bayesian information criterion ( bic ) @xcite for @xmath63 varying from @xmath64 to @xmath65 and @xmath9 varying from @xmath27 to @xmath66 .",
    "the simulated model , corresponding to @xmath58 and @xmath59 , has been chosen with the maximum percentage of @xmath67 .",
    "this section presents the results obtained by the proposed model for signals of switch points operations .",
    "one situation corresponding to a signal with a critical defect is presented .",
    "the number of the regressive components is chosen in accordance with the number of the electromechanical phases of a switch points operation ( @xmath68 ) .",
    "the value of @xmath29 has been set to @xmath32 , what guarantees a segmentation into convex intervals , and the degree of the polynomial regression has been set to @xmath69 which is adapted to the different regimes in the signals .",
    "figure [ resultat_signal_aig_2 ] ( left ) shows the original signal and the denoised signal ( given by equation ( [ eq .",
    "signal expectation ] ) ) .",
    "figure [ resultat_signal_aig_2 ] ( middle ) shows the variation of the proportions @xmath70 over time .",
    "it can be observed that these probabilities are very closed to @xmath32 when the @xmath26 regressive model seems to be the most faithful to the original signal .",
    "the five regressive components involved in the signal are shown in figure [ resultat_signal_aig_2 ] ( right ) .",
    "in this paper a new approach for signals parametrization , in the context of the railway switch mechanism monitoring , has been proposed .",
    "this approach is based on a regression model incorporating a discrete hidden logistic process .",
    "the logistic probability function , used for the hidden variables , allows for smooth or abrupt switchings between polynomial regressive components over time .",
    "in addition to signals parametrization , an accurate denoising and segmentation of signals can be derived from the proposed model .",
    "b. krishnapuram , l. carin , m.a.t .",
    "figueiredo and a.j .",
    "hartemink , sparse multinomial logistic regression : fast algorithms and generalization bounds , _ ieee transactions on pattern analysis and machine intelligence , _",
    "27(6 ) : 957 - 968 , june 2005 .",
    "s. r. waterhouse , _ classification and regression using mixtures of experts , _ phd thesis , department of engineering , cambridge university , 1997 .",
    "v. e. mcgee and w. t. carleton , piecewise regression , _ journal of the american statistical association _ , 65 , 1109 - 1124 , 1970 ."
  ],
  "abstract_text": [
    "<S> a new approach for signal parametrization , which consists of a specific regression model incorporating a discrete hidden logistic process , is proposed . </S>",
    "<S> the model parameters are estimated by the maximum likelihood method performed by a dedicated expectation maximization ( em ) algorithm . </S>",
    "<S> the parameters of the hidden logistic process , in the inner loop of the em algorithm , are estimated using a multi - class iterative reweighted least - squares ( irls ) algorithm . </S>",
    "<S> an experimental study using simulated and real data reveals good performances of the proposed approach . </S>"
  ]
}