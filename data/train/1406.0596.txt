{
  "article_text": [
    "`` big data '' often refers to datasets that are large in different ways : there can be many observations , many variables or both , and the size can be measured against some historical standard or against available computational resources ( e.g. , the data might be too large to fit into memory ) .",
    "data can also come from different sources , have inhomogeneities and might have to be processed in a streaming fashion . here , we want to take a look at one specific aspect of `` big data , '' the effect of inhomogeneities in the data in regression modeling .",
    "specifically the question whether one is able to extract ( in a computationally feasible way ) a model that works for data that come from different time - regimes or that , more generally , have different underlying distributions .    from a perhaps slightly naive statistical point of view , a situation where we face computational challenges due to a large number of homogeneous observations in a database",
    "is not problematic .",
    "we can simply discard most of the observations and retain sufficiently many observations , chosen at random , to guarantee good predictive accuracy .",
    "the exact number of observations we have to retain will be a function of the desired predictive accuracy , the number of variables and the noise level .",
    "keeping tens of thousands of observations will be sufficient for most practical purposes .",
    "most estimators can easily deal with datasets of this size .",
    "however , many large - scale datasets do not fit neatly into the standard framework of a single underlying model observed with independent and identically distributed errors .",
    "there are likely to be outliers in the data , and the truth might better be approximated with a mixture of models than a single one and underlying distributions of the variables might shift over time [ @xcite ] .",
    "there has been a lot of work on various aspects of these issues .",
    "while we can not provide an even approximately complete overview , some of the major themes can be found in work on robust estimation [ @xcite , @xcite ( @xcite ) ] , time - varying coefficients models [ @xcite ] , mixture models [ @xcite ] and change - point estimation [ @xcite ] . in a high - dimensional regression dataset",
    ", @xcite showed evidence for the presence of multiple components that can be exploited for variable selection .",
    "mixed- and random - effects models [ @xcite ] are related but do not have an observation - specific random effect .",
    "varying - coefficient models seem particularly attractive to capture shifts in underlying distributions if the data are recorded chronologically , and the approach has been extended to cope with more general estimation problems , including estimation of time - varying graphs [ @xcite ] .",
    "mixture models , on the other hand , do not assume such a structure and try to infer the hidden states of the mixture class membership by using , for example , the em - algorithm [ @xcite ] or related approaches .    ) for the least squares estimator and the maximin effect estimator , respectively , where the aspect ratios are chosen such that the same effect strength will lead to the same slope in all four panels .",
    "there are more than half a million training observations for a model with just 60 free parameters , and yet the least squares estimator overfits , which leads to a degradation in performance on the test data .",
    "the performance of the maximin estimator is more consistent over the training data , which translates into a better performance on the test data . ]    in some applications , trying to infer the full time - varying coefficients in a model or inferring the hidden states in a mixture model can be computationally challenging , and success is not always guaranteed from a statistical point of view .",
    "moreover , we might not be interested in the hidden states or the exact time - evolution of the coefficients but rather in a simple model that can work reliably for all states or times .",
    "our examples will mostly fit into a change - point model , where the underlying distribution can shift abruptly .",
    "an example is given in figure  [ fig6e ] , which is based on price data of twelve financial future instruments ( including foreign exchange , equity and commodity futures ) on time - resolution of minutes over the course of ten years , after 2000 .",
    "we use the past 5 minutes of log - returns of all instruments ( i.e. , 60 predictor variables ) to forecast with a linear model the log - return of the euro  dollar exchange rate over the next minute ( which is the response variable ) .",
    "two - thirds of the data are used for training a least squares estimator , and the cumulative cross products of this model for the training and test data is shown in the first and second panel of figure  [ fig6e ] , respectively , where the cumulative gain up to time @xmath0 is for response values @xmath1 , @xmath2 and predictions @xmath3 , @xmath2 ( both are assumed to mean - centered and predictions are normalized to have a second moment of 1 ) given by @xmath4 the training data show that the model works very well at the beginning of the training period but then tails off , and performance on the test data is much worse than on the training period , even though there are more than half a million observations to fit a model with 60 parameters . in contrast , the third and fourth panel show the cumulative cross products of the `` maximin effects '' estimator that we propose .",
    "the least squares estimator is maximizing the explained variance on the pooled dataset , leading ( as in this example ) to periods or groups of observations where the fit is very good , and others where barely any variance is explained by the fitted values .",
    "in contrast , the `` maximin effects '' estimator maximizes the explained variance for the worst group of observations , which have been divided in this example rather arbitrarily into 3 equally large blocks of consecutive observations ; see section  [ sectionestimator ] for more precise definitions regarding a group of observations .",
    "the estimator in the example is computed without a regularization penalty .",
    "the predictive accuracy is much more constant over time , and performance on the test data is in line with performance on the training data , as the estimator has not been as much influenced by the period at the beginning of the training set as the least squares estimator .",
    "we will set notation and introduce the maximin effects estimator in section  [ sectionestimator ] , while showing some properties for known and unknown group structure in section  [ sectionproperties ] , discussing computational properties in section  [ sectioncomputational ] and concluding with an example in section  [ sectionnumerical ] .",
    "we will first try to give a suitable and intuitive definition of maximin effects in mixture models or varying - coefficient models , while introducing the maximin effects estimator thereafter .",
    "while we focus exclusively on regression here for ease of exposition , the same approach can be used , for example , for classification and graph estimation .",
    "we will work with a mixture model , where for @xmath5 observations of a real - valued response @xmath1 and a @xmath6 predictor variable @xmath7 for @xmath2 , @xmath8 for some unknown distribution @xmath9 , either discrete or continuous .",
    "we also use the standard notation with the @xmath10 response vector @xmath11 , the @xmath12 design matrix @xmath13 and the @xmath10 error vector @xmath14 .",
    "the predictor variables @xmath15 are random and independent , and the noise @xmath16 fulfils @xmath17 .",
    "furthermore , the coefficients @xmath18 are independent from the @xmath15 , @xmath2 .",
    "independent noise is an example , but some dependencies between noise contributions are also possible in this framework , for example , if the observations have a time - ordering .",
    "the inhomogeneity of the data is thus solely caused by the variation of the regression coefficients among the sample points with indices @xmath2 .",
    "we do not necessarily assume that the @xmath18 , @xmath2 are independent .",
    "they can be organized in known or unknown groups .",
    "the following examples indicate the scope of the model : if @xmath9 has point masses at a finite set of points , we are in the setting of classical finite mixture models , where @xmath19 can take one of a finite number of values . in another scenario , realizations @xmath18 are positively correlated over time if the observations are ordered in some chronological order , creating a smoothly varying effect over time . in the latter example , the model behaves more like a varying - coefficient model [ @xcite ] .",
    "a shift in the distribution of the predictor variables could conceivably be handled in a very similar manner . as a final example , the realizations @xmath18 are most often the same , but a small fraction takes other values which can be viewed as outliers or contaminations .",
    "we always assume that the random @xmath15 are identically distributed from a distribution with population gram matrix @xmath20 . for a fixed regression coefficient @xmath21",
    ", we can define two different optimality criteria : @xmath22 is the variance of the residuals in absence of additional errors on the observations , while @xmath23 is the explained variance of predictions with @xmath24 : @xmath25 alternative expressions for  ( [ eqr ] ) and ( [ eqv ] ) under the condition @xmath17 are @xmath26    if we want to find a single @xmath27-dimensional regression coefficient that works optimally on average over @xmath28 , the optimal choice are the pooled coefficients @xmath29 where the expectation is with respect to @xmath28 .",
    "note that in this case it is inconsequential for the pooled estimator , whether we minimize the residuals or maximize the explained variance .",
    "if we are looking for effects that guarantee a good performance throughout all possible parameter values , in analogy to decision theoretic consideration [ @xcite ] , two possible definitions of effects are @xmath30\\label{eqbetastar } \\\\[-8pt]\\nonumber & = & \\mathop{\\operatorname{argmin}}_\\beta\\max_{b \\in f } ( - v_{\\beta;b } ) = \\mathop{\\operatorname{argmax}}_\\beta\\min_{b \\in f } ( v_{\\beta;b } ) , \\nonumber\\end{aligned}\\ ] ] where @xmath31 .",
    "alternatively , @xmath32 could be the smallest region such that @xmath33 for some @xmath34 $ ] , guaranteeing success for a large fraction of randomly chosen coefficient values .",
    "two comments are in order regarding the definition of maximin effects .",
    "first , the effects are optimizing for the worst - case scenario for @xmath35 . to be more precise ,",
    "if we view future samples of @xmath19 to be allowed to be chosen by an adversarial player , the _ maximin effects _ are then of a minimax regret form as they optimize the objective function ( explained variance ) under the most adversarial scenario .",
    "minimax regret strategies have also been explored in game theoretical aspects of decision theory and machine learning , for example , in @xcite and @xcite , and bandit - type problems and on - line decision problems , [ e.g. , @xcite ] .",
    "we do not allow in our framework any choice about which distribution we sample from , contrary to bandit - type problems .",
    "related to our setting is a paper by @xcite , who propose a linear minimax regret estimator which can be computed with convex optimization .",
    "their estimator is optimizing a mean - squared error loss subject to various source uncertainties in the data .",
    "our set - up is conceptually perhaps closest to the minimax framework in robust statistics [ @xcite ] .",
    "however , we consider much more general situations than contaminated samples with a fraction of outliers : as discussed in section  [ secrobust ] , the latter fits into our framework as well .",
    "second , we have shown two different objectives ( minimizing residual variance @xmath22 and maximizing explained variance @xmath23 ) that yield two different minimax - regret estimators ( @xmath36 and @xmath37 ) . using the first choice of minimizing residual variance has the main drawback that it is nonrobust when sampling regression coefficients : adding a small point mass to @xmath9 can change the effects @xmath36 drastically .",
    "the same is true for the pooled effects ( [ eqbetapool ] ) .",
    "explained variance @xmath23 is expressed as residual variance if measured against the baseline of residual variance @xmath38 under a constant 0 prediction ( we assume a mean - centered response throughout ) .",
    "this baseline is often appealing in practice as we would like to avoid doing worse than a constant prediction .",
    "moreover , assume we choose instead a baseline of residual variance @xmath39 for any vector @xmath40 in the convex hull of the support of @xmath32 ( such as @xmath41 or @xmath36 ) .",
    "whichever vector @xmath42 we choose in this scenario , we can not avoid doing worse than the baseline for some values of @xmath43 with the consequence that the problem will become trivial , as the optimal value under the most adversarial scenario can then always be achieved by a vanishing coefficient vector ( thus keeping the baseline solution ) .",
    "theorem  [ theoremmaximin ] will provide a justification for this statement : once we shift the problem by the nonzero baseline @xmath40 , the vector @xmath40 will sit at the origin , and it will be a part of the convex hull of the equally shifted support of @xmath32 , thus leading to a vanishing maximin - effect .     in ( [ eqbetapool ] ) and the maximin effects @xmath44 in ( [ eqbetastar ] ) .",
    "first panel : the green dots indicate the values the random coefficient takes on with a maximin first component and a variable second component .",
    "the blue circle indicates the location of the pooled effects @xmath45 , while the red dot marks the location of the maximin effects @xmath44 .",
    "second and third panel : the cumulative cross product  ( [ eqcum ] ) for the pooled and maximin estimator , respectively .",
    "while the pooled estimator achieves a better overall fit , it does so at the cost of a highly variable performance . ]    a simple two - dimensional example of `` maximin effects '' is shown in figure  [ figsim ] .",
    "the coefficients are chosen as @xmath46 , where @xmath47 varies uniformly in @xmath48 $ ] .",
    "the two random predictor variables are chosen independently with a standard normal distribution .",
    "the pooled estimator ( [ eqbetapool ] ) is marked with a blue circle in the first panel of figure  [ figsim ] , and the corresponding cumulative cross product in  ( [ eqcum ] ) is shown in the second panel if observations are ordered such that @xmath47 decreases monotonically from @xmath49 to @xmath504 .",
    "the pooled effects ( [ eqbetapool ] ) maximizes the overall explained variance but suffers as @xmath47 takes on negative values .",
    "predictions in this range are even negatively correlated with the responses , as can be seen by the negative slope of the cumulative cross product in the figure toward the right half of the observations .",
    "the effect is perhaps more drastic than in the real - data example in figure  [ fig6e ] but of a similar nature .",
    "the maximin effects @xmath44 in contrast take a nonzero value only for the first variables , where the effect is constant .",
    "this yields a constant explained variance throughout the whole parameter range , as shown in the third panel of figure  [ figsim ] .",
    "if we are in a classical regression model with a fixed regression coefficient vector , then @xmath9 has just a point mass at some @xmath51 , and ( [ eqbetapool ] ) , ( [ eqbetastarr ] ) and ( [ eqbetastar ] ) will coincide .",
    "the vector @xmath44 is maximizing the explained variance under the most adverse realization of the random regression coefficient .",
    "the value 0 has a special status since we define the regret with respect to the 0 regression vector .",
    "effects that can take opposite signs are set to 0 when using the maximin explained variance in @xmath44 ( similar to the value 0 getting special status when losing the rotational invariance in coordinate space when replacing a ridge penalty by a lasso - type penalty ) .",
    "the maximin effects have an interesting characterization .    [ theoremmaximin ]",
    "assume that the predictor variables are chosen randomly from a design with full - rank population gram matrix @xmath20 .",
    "let @xmath52 be the convex hull of the support @xmath32 of @xmath9 .",
    "the maximin - effect ( [ eqbetastar ] ) is then given by @xmath53 in particular , if @xmath54 , then @xmath55 .",
    "( green dots or area ) , its convex hull ( black lines ) , the pooled effects ( blue circle ) and the maximin effects ( red cross ) .",
    "the maximin effects are the closest point to the origin in the convex hull of the support of @xmath9 in the distance measure of theorem  [ theoremmaximin ] . in the example on the first panel , the origin",
    "is contained in the convex hull , and the maximin effects thus vanish . in the second example the maximin effects rest on the convex hull of the support , but are not equal to zero . in both examples ,",
    "the maximin effects are not part of the support itself .",
    "the third example shows a continuous support of  @xmath9 , while the last example has unbounded support of  @xmath9 . in both of these examples ,",
    "the maximin effects are identical to a corner point of the support , but generalizations to the edge of the support are easily possible as well . in the last example , the pooled effects are thus infinite whereas the maximin effects have a robustness property by staying at the closest point to the origin . ]    a proof is given in the .",
    "the maximin effects parameter is thus the one that is closest to the origin in the convex hull spanned by the support of @xmath9 . in a classical regression setting with fixed regression coefficients @xmath56 ,",
    "@xmath9 just has a point - mass at a @xmath56 , and the maximin effect will , by theorem  [ theoremmaximin ] , be identical to @xmath56 . figures  [ fig6e ] and [ figlasso ] show examples of datasets with an interesting nonzero solution .",
    "if the origin is included in the convex hull of all coefficients , the best lower bound that can be guaranteed is 0 , and the maximin effects are consequently vanishing identically in this scenario . if the maximin effects vanish , a standard regression analysis will typically be misleading since the inner product between any estimated vector and true effects in @xmath9 can take an arbitrary sign , depending on which effect in @xmath9 is currently active .",
    "a vanishing maximin effect is thus a warning sign that standard regression analysis might not be appropriate .",
    "four examples of maximin effects are shown in figure  [ figgeome ] , comparing the pooled and the maximin effects . for @xmath57 , as defined in ( [ eqbetastarr ] ) , there is no equivalent characterization as in theorem  [ theoremmaximin ] , as the value 0 has no special status .",
    "it is noteworthy that the maximin effects are robust in the following sense : if we add new points to the support of @xmath9 , we will always maintain or lower the distance to 0 as in theorem  [ theoremmaximin ] .",
    "in the most extreme case , adding contaminations to the support of @xmath9 will either leave the maximin effects unchanged or shrink the maximin effects toward 0 .",
    "this is a direct consequence of theorem  [ theoremmaximin ] .",
    "we can also characterize the maximin effects in yet another way , using theorem  [ theoremmaximin ] .",
    "define the predictions and residuals as @xmath58 the maximin effects are then , using theorem  [ theoremmaximin ] , characterized as the effects that maximize the norm of the predictions , subject to the constraint that the inner product between the predictions and the residuals is nonnegative for all possible @xmath43 , @xmath59 if @xmath19 just takes a deterministic fixed value @xmath56 , we recover of course @xmath60 .",
    "the constraint in the optimization above requires that the predictions are never negatively correlated with the residuals if @xmath61 can vary in @xmath32 .",
    "the maximin predictions are in this sense the maximal predictions that are still `` conservative '' in the sense that they can potentially `` under - explain '' a signal but can never be negatively correlated with the residuals .    in summary , if we want to maximize the explained variance if an adversary is allowed to pick a regression vector @xmath43 or if test data are not expected to come from the same distribution as the training data with respect to the random coefficients , then estimating the maximin coefficients ( [ eqbetastar ] ) seems a useful choice .",
    "we introduce the maximin effects estimator first for data where we know a group - structure of the observations in the sense that within each group of observations the regression coefficient has a fixed ( but unknown ) value , which varies between groups .    to be more precise , suppose there are @xmath62 groups of observations @xmath63 , and each group has @xmath64 samples .",
    "the indices belonging to a group are denoted by @xmath65 for all groups @xmath63 .",
    "let @xmath66 denote the @xmath67-dimensional submatrix of @xmath13 that corresponds to choosing the rows in @xmath68 and likewise for @xmath69 and @xmath70 .",
    "if we are in situation where we know that the random coefficient is fixed at @xmath71 within a group , then @xmath72 let @xmath73 .",
    "the empirical counterpart to ( [ eqv ] ) is the explained variance in group @xmath74 , @xmath75    a natural estimator for a sparse , consistent signal @xmath44 is then a penalized version of the empirical minimizer . for @xmath76",
    "$ ] , @xmath77 for @xmath78 , the loss function @xmath79 is identical to quadratic loss up to a constant . for @xmath80 , however , the loss function will be different from quadratic loss . if @xmath81 , one can use the unpenalized version ( @xmath82 ) . in the general case ,",
    "the two most interesting choices for the penalty are @xmath83 , making the estimation lasso - like [ @xcite ] , and @xmath84 for a ridge - type estimation [ @xcite ] .",
    "an equivalent version is given by the constrained optimization , @xmath85 and we will mostly use the constrained version for the theoretical results . in practice",
    "the two versions can be used interchangeably , and the penalty parameter can be chosen by cross - validation , using hold - out samples for each unknown group and choosing the penalty parameter that maximizes the minimally explained variance on the hold - out samples from all groups .",
    "the objective function in  ( [ eqestlambda ] ) or ( [ eqest ] ) is convex in its argument and can thus relatively easily be optimized ; we will return to this issue later in section  [ sectioncomputational ] .      in some applications",
    "there are no a priori known groups on which the realized value of the regression coefficients shows little or no variation . however , if the observations have , for example , a time ordering , and the effects are changing smoothly over time , we would suspect that taking blocks of consecutive observations would result in little variability of the realized coefficients within groups .    for some datasets ,",
    "the groups are entirely unknown ; see @xcite for an example .",
    "we propose in these cases to take @xmath62 groups of  @xmath86 observations , where @xmath86 is approximately of size @xmath87 ( modulo rounding to the next integer ) if we sample without replacement . alternatively , we can sample @xmath62 groups with @xmath86 observations each with replacement such that typically @xmath88 .    for both cases mentioned above , once we have constructed the @xmath62 groups , we use the same estimator as in ( [ eqestlambda ] ) or ( [ eqest ] ) .",
    "we discuss in sections  [ subsectionrecoverypareto][subsectionpareto ] the validity of the procedure based on such estimated groups .",
    "the statistical properties of the lasso - type maximin effects estimator ( [ eqest ] ) with the @xmath89-norm constraint will be examined first for the case of known groups in the observations and later be extended to the considerably more involved case ( both from a theoretical and practical perspective ) of unknown groups , either capturing smooth varying effects ( over time ) or more generally without such a ( time ) structure .      here",
    "we show a result for the lasso - type maximin effect estimator  ( [ eqest ] ) for known groups of observations .    specifically , there are @xmath62 groups , and for simplicity , we assume that each group has @xmath90 observations ( without this assumption , we need to replace in the results below @xmath87 by @xmath91 ) . in each group , the explanatory variables are chosen randomly with gram matrix @xmath20 , yielding design matrices @xmath92 , @xmath63 . in each group , @xmath93 for coefficients @xmath94 that are fixed in each group but can vary between groups .",
    "the maximin estimator is then the set of coefficients that work optimally across all groups in the sense of  ( [ eqbetastar ] ) .    with estimator ( [ eqest ] )",
    ", we are now trying to maximize the explained variance in all groups .",
    "[ theorembasic ] let @xmath95 be the maximal difference between the empirical gram matrix @xmath96 and population gram matrix @xmath20 , @xmath97 . if @xmath98 , then @xmath99 where @xmath100 is the optimal value that can be attained , @xmath101    a proof is given in the .",
    "for @xmath102 ( if the population and empirical versions of the gram matrices are identical , as happens under fixed design ; see a more detailed comment below ) , the estimator thus reaches the optimal value less a term @xmath103 which is a similar result to that of standard lasso estimation [ see , e.g. , @xcite ] , except that the first term @xmath104 will increase when the number @xmath62 of groups grows larger , which is the price we have to pay for estimating the maximin effects ( [ eqbetastar ] ) instead of the pooled effect ( [ eqbetapool ] ) . on the other hand ,",
    "the error is just a function of the noise @xmath14 and not influenced by the variability of @xmath71 across groups , whereas standard lasso - type estimation of the pooled effect ( [ eqbetapool ] ) would suffer if the variability of @xmath71 is high across groups .",
    "we note that one can also derive a similar bound if the gram matrix of the predictors is allowed to depend on the group . in particular , we can have a fixed design in each group . in this case",
    "a corresponding result holds true with @xmath102 .",
    "if the design is random as in theorem  [ theorembasic ] , we have an additional term in the bound that is proportional to @xmath95 times the squared @xmath89-norm of @xmath44 and @xmath71 , @xmath63 . a more careful analysis for the special case of gaussian random design [ @xcite ] could render the bound again linear in @xmath105 , with more general design treated in @xcite .",
    "the two terms that are relevant for the rate are thus @xmath95 and @xmath106 . to give a simple bound for @xmath95",
    "if all predictor variables are drawn from the same population with gram matrix @xmath20 , we can , for example , get the following :    [ lemmadeviation ] if the predictor variables are chosen i.i.d . from a distribution with gram matrix @xmath20 and @xmath107 for @xmath2",
    ", then for any @xmath108 , with probability at least @xmath109 , @xmath110    the proof follows directly from hoeffding s inequality , combined with a union bound over both the  @xmath111 entries in each empirical gram matrix and the number @xmath62 of groups . of course ,",
    "a similar bound could be derived for a gaussian or sub - gaussian distribution of the explanatory variables .",
    "if we additionally make a distributional assumption for the independent noise to control the term @xmath112 , we get a simple bound for the estimator in ( [ eqest ] ) .",
    "[ corfixed ] assume that the predictor variables are chosen i.i.d . from a distribution with gram matrix @xmath20 and @xmath113 .",
    "if the errors @xmath114 , @xmath2 have a i.i.d .",
    "gaussian distribution @xmath115 , then if @xmath116 , with probability at least @xmath109 , @xmath117.\\ ] ]    the error features the same @xmath118 rate as lasso estimation on a single block of homogeneous data with @xmath87 observations of a fixed signal .",
    "the success hinges obviously on the sparsity of the maximin solution .",
    "the bound becomes less tight when @xmath105 grows .",
    "observe that @xmath105 is constrained from below by the sparsity of the regression coefficients .",
    "the problem thus becomes easier for sparse regression coefficients as one would expect from standard sparse regression [ @xcite ] .    in summary ,",
    "the maximin effects estimator ( [ eqest ] ) is able to estimate the maximin effects in a dataset with known groups .",
    "note that an alternative would involve computing the lasso - type estimator on each group and then constructing the estimator that yields the best minimally explained variance across all groups .",
    "in presence of a large number @xmath62 of groups , the statistical properties of such a naive alternative procedure are unclear .",
    "the more difficult scenario is a mixture model , where there is no a priori known group structure for the observations , and each observation has its own realized value of the random coefficients .",
    "we assume that each coefficient @xmath119 , where @xmath32 is compact .    as mentioned previously , for the case of unknown groups , one solution is to apply estimator  ( [ eqest ] ) to chosen groups , which can be chosen at random in the absence of any group information or in some way that reflects prior knowledge , for example , in the case coefficients varying over time",
    ".      we will need to make one main assumption for recovery of the maximin coefficients for unknown groups , which will be discussed in a few examples below .",
    "first , we define an essential subset of regression coefficients .",
    "[ aes ] a set @xmath120 is called an essential subset of @xmath31 if the maximin effects for @xmath121 , where the support of @xmath122 is @xmath123 , are identical to the maximin effects as for the original problem with @xmath28 .",
    "an essential subset is at most of cardinality @xmath124 ( if @xmath125 , at least one @xmath126 could be removed without changing the point that is closest to the origin in the convex hull of these points ) .",
    "two examples serve as simple illustrations : if @xmath127 , then the smallest essential subset is just @xmath44 itself .",
    "if @xmath32 is discrete , then an essential subset is always the support of @xmath32 itself .",
    "we now give the so - called pareto condition which will be shown to be sufficient for recovery .",
    "for known groups , we do not need the condition , as it always fulfilled , as in section  [ sectionpareto ] .",
    "the condition is meant for cases where the groups are sampled randomly according to some mechanism , which we discuss with a few examples and settings in section  [ sectionpareto ] .",
    "[ a1 ] let @xmath128 be the index sets for chosen groups @xmath63 , and let @xmath18 , @xmath2 be the regression coefficients at observation @xmath129 .",
    "the assumption is that , for @xmath130 , with probability @xmath131 with respect to the random coefficients @xmath18 , @xmath2 and a potentially random sampling of the groups , there exists an essential subset @xmath123 of @xmath32 such that for each @xmath132 there exists a group @xmath133 for which @xmath134 for all @xmath135 .",
    "we call this the pareto condition since it implies that the maximin vector is optimal in the sense that making the performance better in one group will make the performance worse in another group .",
    "the condition requires some of the groups to be `` pure '' in the sense that all observations in the group correspond to the same realization of the regression vector .",
    "we emphasize that the pareto condition is formulated as the probability of a certain event : we find this construction simpler than requiring a random event condition .",
    "the pareto condition is fulfilled for a few examples which will be discussed further in section  [ subsectionpareto ] , but the condition is not true in general . without appropriate structure ( of the type shown in the examples ) cases exist where the condition is violated .      using the pareto condition",
    ", we get the following theorem for randomly sampled groups in the estimator ( [ eqest ] ) :    [ theoremrandom ] assume the pareto condition is fulfilled , with corresponding probability @xmath131 for some @xmath136 . if @xmath15 , @xmath2 are i.i.d . from a distribution with gram matrix @xmath20 and @xmath137 and @xmath98 , then with probability at least @xmath138 , @xmath139    a proof is given in the .",
    "if the smallest eigenvalue of the population covariance matrix @xmath20 is bounded from below by some @xmath140 , then it follows further that with the same probability as above , @xmath141 .    if the error distribution is gaussian , we get the following :    [ corrandom ] if the assumptions of theorem  [ theoremrandom ] are fulfilled , and additionally the errors @xmath114 , @xmath2 have a i.i.d .",
    "gaussian distribution @xmath115 , then , with probability at least @xmath138 , the constant @xmath142 in theorem [ theoremrandom ] can be chosen as @xmath143    this result is a generalization of corollary  [ corfixed ] .",
    "if we choose @xmath144 , we obtain the results of corollary  [ corfixed ] .",
    "( note , however , that we need the pareto condition for corollary  [ corrandom ] but not corollary  [ corfixed ] . )",
    "however , the results also show that we can choose @xmath86 much larger than @xmath87 by allowing an observation to appear in multiple groups , thus lowering the statistical error .",
    "another point of view is that we keep the sample size in each group fixed but increase the number of groups @xmath62 , thus increasing the chance that the pareto condition will be fulfilled .",
    "we can thus infer the optimal maximin coefficients by randomly sampling groups and applying the maximin estimator ( [ eqest ] ) to these groups .",
    "the success hinges on the sparsity of the coefficients within the support of the distribution of the random coefficients .",
    "we describe in section  [ sectionnumerical ] a cross - validation method for choosing the number of groups .",
    "theorem  [ theoremrandom ] rests on the pareto condition .",
    "it is evident that an arbitrary random sampling scheme can not lead to success in the sense of theorem  [ theoremrandom ] .",
    "the number @xmath62 of groups , for example , plays a crucial part .",
    "setting @xmath78 leads just to pooled estimation , which yields in general a consistent estimator for the pooled coefficients and can thus not consistently estimate the maximin coefficients if they differ from the pooled coefficients .",
    "[ [ fixed - groups - with - fixed - design ] ] fixed groups with fixed design + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the simplest example where the pareto condition is fulfilled is the case of known groups , where @xmath19 takes a single value @xmath71 within each group @xmath63 . by definition of the maximin coefficients ,",
    "the pareto condition is then fulfilled , and we are back to the setting of section  [ sectionknown ] .",
    "assume the observations have a time - ordering , and we have a change - point model .",
    "consider the case where the support of @xmath9 is finite of cardinality @xmath145 , that is , @xmath146 .",
    "assume that @xmath19 for the first sample , namely @xmath147 , is chosen uniformly at random among the @xmath145 different possibilities .",
    "thereafter , for @xmath148 and some @xmath149 @xmath150 we build @xmath62 groups of consecutive observations .",
    "below , we will further show under which conditions on @xmath62 and @xmath145 the pareto condition is fulfilled with high probability .",
    "the pareto condition is fulfilled if we have for each possible value @xmath151 a @xmath133 such that @xmath152 for all observations @xmath153 in the @xmath74th set .",
    "suppose we fix @xmath62 and condition on @xmath154 for some @xmath155 and some @xmath156 .",
    "let @xmath157 be the conditional length of the segment of observations @xmath158 where @xmath159 .",
    "then @xmath160 if indeed , @xmath154 and @xmath161 , then one block of observations of length @xmath87 is guaranteed to have exclusively realizations of @xmath19 equal to @xmath126 .",
    "the probability that there is at least one @xmath162 for which @xmath154 in @xmath163 is greater than @xmath164 using a union bound over all @xmath145 distinct values the coefficients can take , the pareto condition is fulfilled with corresponding probability at least @xmath131 for @xmath130 if @xmath165 the second condition states that the number of distinct classes @xmath145 can not be too large .",
    "more specifically , @xmath166 is approximately the average number of contiguous blocks of observations that have a realization @xmath126 of the random coefficient .",
    "the condition above implies that this average value needs to be larger than 1 for the scheme to work ( as otherwise a value of the coefficients might not be sampled at all ) .",
    "the first condition is a requirement on the number of groups @xmath62 one has to pick .",
    "it yields an effective sample size @xmath87 of order @xmath167 , which is the order of the length of observations where the regression coefficient stays constant .",
    "[ [ secrobust ] ] contaminated samples and robustness + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    assume that the regression coefficients come from a mixture distribution @xmath168 where @xmath169 follows a distribution @xmath170 such that @xmath171 note that the latter condition implies that @xmath172 .",
    "a fraction @xmath173 of samples are contaminated in the sense that they have a different realized value of @xmath19 .",
    "we build @xmath62 groups of observations by random sampling .",
    "each group consists of @xmath86 samples drawn at random without replacement from all @xmath5 observations , and each group is sampled independently .",
    "we will argue under which circumstances the pareto condition is fulfilled with high probability .",
    "the pareto condition is trivially fulfilled if we have a single group of observations where all realizations are identical to @xmath174 .",
    "suppose we divide the samples into @xmath62 groups .",
    "each group contains @xmath86 observations , drawn at random without replacement from all @xmath5 observations , independently for each group ( and thus , the same re - sampled data point can occur in several groups ) .",
    "if for @xmath130@xmath175 then we guarantee the pareto condition will be fulfilled with corresponding probability at least @xmath131 with respect to the random sampling of the coefficients and random sampling of the groups .",
    "there is also a robustness inherent in the procedure . if sampling  ( [ eqcontam ] ) holds without condition ( [ eqexcon ] ) , then the samples @xmath169 can come from an arbitrary distribution . if condition ( [ eqcondg ] ) is fulfilled , then we have again with probability at least @xmath131 that there is a group where @xmath19 is equal to @xmath176 for all observations in the group",
    ". we can then use theorem  [ theoremmaximin ] to show robustness properties of the estimate , as already discussed in the paragraphs after theorem  [ theoremmaximin ] . adding contaminated samples",
    "can only shrink the maximin effects parameter and the corresponding estimator toward the origin .",
    "the maximin effects estimator thus has robustness properties against outliers as long as at least one group does not contain outliers .",
    "some more examples are possible to derive , including for continuous distributions of @xmath19 , but are beyond the scope of this manuscript .",
    "the basic intuition is that the convex hull of the effective coefficients in each groups needs to approximate the convex hull of the support of the random coefficients @xmath19 in order for the pareto condition to be fulfilled .",
    "the outliers above are referred to as _ b - outliers _ in linear mixed models [ @xcite ] .",
    "an interesting question is whether the method is also robust to outliers in the noise , the so - called _ e - outliers_. if we use a robust version of the explained variance @xmath23 in the maximim estimator definition , then the breakdown point of the maximin estimator will at least be identical to the breakdown point of the robust estimator for the explained variance .",
    "the reason is that the explained variance would have to be corrupted arbitrarily much in every of the @xmath62 groups , requiring in each group @xmath63 at least @xmath177 corrupted samples if @xmath178 is the breakdown point of the robust explained variance estimator .",
    "hence at least @xmath179 samples would have to be corrupted for the modified maximin estimator to take on arbitrarily large values , and the breakdown point of a robust explained - variance estimator would thus be inherited by the maximin estimator .    before presenting some numerical results , we first discuss now the computational aspects of the procedure .",
    "the objective function of estimator  ( [ eqestlambda ] ) is convex , and the penalty is separable .",
    "estimator  ( [ eqestlambda ] ) or the equivalent constrained formulation ( [ eqest ] ) could thus be computed using coordinate - wise updates , with a similar strategy as in the `` glmnet '' approach [ @xcite ] to fitting lasso- and ridge - penalized regression models .",
    "if @xmath27 and @xmath5 are large , this becomes computationally burdensome .",
    "we show two different possibilities .",
    "the estimation can be reduced to a series of weighted standard lasso or ridge estimation .",
    "the minimum in  ( [ eqest ] ) can be approximated for positive terms by a sum @xmath180 this leads to a weighted estimation problem , where the weights are constant in each group , and weights are larger in groups where the explained variance is still small . for a fixed value of @xmath181 ,",
    "the solution of ( [ eqestlambdaapprox ] ) is ( setting @xmath83 for lasso - type estimation and @xmath84 for ridge ) @xmath182 where the weights @xmath183 , @xmath2 are proportional to @xmath184 where @xmath185 is the group that observation @xmath153 belongs to .",
    "the strategy is now to alternate between updating the weights in ( [ weights ] ) , starting with uniform weights , and computing the solution to ( [ reweighted ] ) for fixed weights .",
    "the solution in the first example in figure  [ fig6e ] has been computed in this way , as a series of reweighted least squares estimators with @xmath186 .",
    "a few rounds of the iteration are typically sufficient , and the computational burden is thus similar to standard least squares or lasso - type estimation .      computing estimator ( [ eqestlambda ] ) by coordinate - wise updates or by iteratively reweighted penalized estimation requires , however , that either the design matrix @xmath13 or the gram matrices in all groups are kept in memory .",
    "another option is to look at the limit of @xmath187 as @xmath188 , where @xmath189 is the supremum of the values for which the estimator does not vanish identically . in this limit , @xmath190 where @xmath191 is the solution to @xmath192 the quadratic term disappears in  ( [ eqmax ] ) as it will shrink like @xmath193 if @xmath194 , whereas the remaining two terms ( penalty and objective ) in the estimator scale linearly with  @xmath105 , and the constant @xmath105 thus drops out of the equation or can be replaced with an arbitrary constant ( modulo scaling of the solution ) as @xmath195 .",
    "the constant 1 is chosen arbitrarily , and choosing a different constant would just rescale the solution .",
    "the estimator @xmath191 in  ( [ eqmax ] ) can be computed with linear programming , and most importantly , the data matrix @xmath196 enters only through its inner product with the response vector @xmath197 , achieving a great reduction in problem size .",
    "estimator ( [ eqmax ] ) still has to be re - scaled for optimal least squares prediction , but this is just a univariate optimization .",
    "our only tuning parameter in this case is the number of groups @xmath62 to choose ( unless they are known ) , and we can optimize @xmath62 by using cross - validation ; see the section with numerical examples for details on how the cross - validation is implemented .",
    "the solution @xmath191 in ( [ eqmax ] ) selects in general several variables , not just a single one as might be expected from the analogous situation for the standard lasso . for ridge estimation with @xmath84 , estimator  ( [ eqmax ] ) would correspond to marginal regression if we had only a single group , and this behavior",
    "has recently been examined in @xcite .",
    "however , the variability of the inner products in  ( [ eqmax ] ) across groups leads to sometimes appreciably different solutions and has a similar effect as the quadratic penalty that is written down explicitly in ( [ eqestlambda ] ) .",
    "we will use this latter estimation technique in ( [ eqmax ] ) for the following high - dimensional example in section  [ sectionnumerical ] , which will also demonstrate the computational advantages of this type of estimation .",
    "the maximal penalty solution is suitable if either a fast initial estimator is desirable or if the data are very noisy . in the latter case",
    "the large penalty will be justified not only from a computational but also from a statistical point of view .",
    "the performance of the maximal penalty estimator should be seen as a lower bound for what is achievable with a more expensive estimation with a fine - tuned value of the penalty parameter .    while tight bounds on the worst - case and typical computational complexity are difficult to establish , the memory requirements are more immediate . if fitting a pooled estimation , the required memory is of order @xmath198 , as either the whole matrix @xmath13 or the gram matrix has to be held in memory .",
    "for standard maximin estimation or estimation of mixture models with @xmath62 groups , this is increased to @xmath199 since the gram matrix has to be stored separately for all @xmath62 groups . for the maximal penalty estimator",
    "( [ eqmax ] ) or its ridge counter - part , the memory complexity decreases to @xmath200 , as one only has to store the @xmath62 @xmath27-dimensional cross - products between the predictors and the responses in each group .",
    "the memory complexity of the maximal - penalty estimator is thus substantially reduced in the typical scenario where @xmath201 , while the memory complexity is just marginally increased for the general case with arbitrary @xmath202 .",
    "the example in figure  [ fig6e ] illustrated that overfitting has to be a concern even if we have millions of observations at our disposal to fit quite low - dimensional models with less than one hundred parameters due to the shifts in the underlying distributions .",
    "next , we look at an example with millions of variables and thousands of observations .",
    "@xcite collected a dataset of so - called 10-_k _ reports from thousands of publicly traded u.s .",
    "companies in the years 19962000 . for each report ,",
    "unigrams and bigrams of word frequencies have been computed and used as predictors for the stock return volatility in the twelve - month period after the release of the report , which is here measured against the baseline of the volatility before the filing of the report .",
    "we use 3000 examples as a training set and the remaining just over 16,000 examples for testing .",
    "we compute both a cross - validated lasso and ridge estimator with the `` glmnet '' package [ @xcite ] and the estimator ( [ eqmax ] ) , once for a fixed number of 3 groups and once for a cross - validated number of groups , which is explained in the next paragraph .",
    "below , we will further explain the procedure for cross - validation .",
    "the histograms of@xmath203 are shown for the various methods , where both @xmath11 and @xmath204 are standardized to have mean 0 and variance 1 .",
    "the groups in @xmath205 are chosen randomly as 500 observations out of the training or test data .",
    "form ( [ eqexplvar ] ) avoids the choice of the scaling for the estimators of form ( [ eqmax ] ) as the measure is invariant under rescaling of the predictions .",
    "the results for lasso estimation ( @xmath83 ) are shown in figure  [ figlasso ] and for ridge estimation ( @xmath84 ) in figure  [ figridge ] when selecting a varying number of predictor variables @xmath206 , @xmath207 , @xmath208 , @xmath209 , 4,272,227@xmath210 as a consecutive block in the order given by the dataset in @xcite .",
    "both the lasso and ridge estimators of the maximin effects are calculated under the maximal penalty ( [ eqmax ] ) , which has computational advantages and avoids having to chose a tuning parameter for the penalty .    as can be seen from the figures ,",
    "the variability of the explained variance is indeed higher for pooled estimation , compared with the maximin effects estimators , especially for a lasso - type penalty .",
    "the difference in performance between training and test datasets is also larger for the pooled estimation as it is more prone to overfitting than the maximin effects estimators .",
    "[ figlasso ]          as the dataset is not a priori grouped and the optimal group size is unknown , one needs to decide on the optimal number of groups @xmath62 .",
    "one possibility indicated above is with cross - validation . to this end , we split the @xmath211 training data 100 times into two half - samples of @xmath212 observations , sampled uniformly at random . for each split , we then divide both half - samples of observations into smaller blocks of consecutive observations .",
    "( we would sample at random if the data did not have a time - ordering . ) the first half - sample of @xmath212 observations is split into @xmath62 blocks with a sample size @xmath213 in each block .",
    "the second half - sample is split into @xmath74 blocks , where @xmath214 is chosen as large as possible while still leaving at least a few hundred observations in each block . for each split into two half - samples ,",
    "we compute the maximin estimator on the blocks formed by the first half - sample and compute the explained variance in each of the @xmath74 blocks of the second half - sample .",
    "( the result turns out to be rather insensitive to the precise choice of @xmath74 ; note that we want to choose @xmath74 as high as possible but have to keep a minimal number of observations in each of the `` test '' groups in order not to be overwhelmed by noise in the estimation of the explained variances . )",
    "the worst - case explained variance over these @xmath74 groups is then averaged for each value of @xmath62 across the 100 random splits into two half - samples .",
    "we choose @xmath62 to optimize this averaged worst - case explained variance .",
    "all groups are chosen here as consecutive blocks of equal size from the two half - samples , respectively , since the reports are ordered chronologically and it seems likely that there are shifts in the underlying distributions over time .",
    "if no such time - ordering applies , we would sample the groups at random within each half - sample for cross - validation .",
    "estimation of the standard estimator ( [ eqestlambda ] ) is in general slower than the pooled estimation over all data , at least as long as ( [ eqestlambda ] ) is computed by iteratively reweighted pooled estimation . on the other hand , when going for the maximal penalty estimate as in  ( [ eqmax ] ) , the solution can be computed using quadratic or linear programming for ridge and lasso penalties , respectively , and the design matrix enters only through the inner products on the right - hand side of ( [ eqmax ] ) . figure  [ figtimef ] shows the necessary computational time as a function of the dimensionality @xmath27 of the data and the number @xmath5 of samples .",
    "the advantage of the maximin effects estimator with a cross - validated choice of the number of groups is visible across the entire range of the dimensionality .",
    "the relative speed advantage of the maximin estimation is more than a factor 10 for ridge estimation .",
    "choosing just a fixed number of groups can get the relative advantage to three orders of magnitude for ridge estimation , which can be useful in its own right or as an initial check as to whether there is any signal in the data at all .",
    "the computational complexity is roughly similar as a function of @xmath27 for the methods whereas the maximin effects have a better scaling as a function of @xmath5 , as expected since the dimension @xmath5 drops out of the memory requirements for estimation and is replaced by the much smaller number @xmath62 of groups .",
    "observations for lasso - type estimation ( left ) and ridge estimation ( second from left ) : the cross - validated pooled estimate ( blue ) , and the two estimators of maximin effects with a fixed number of groups @xmath215 ( red ) and a cross - validated choice of the number of groups ( orange ) .",
    "analogous plots for the timings in seconds as a function of the number of samples for @xmath216 variables in the two right panels .",
    "estimation of maximin effects was often orders of magnitudes faster than the pooled estimation . ]",
    "one characteristic of large - sale datasets is the mix of a large number of observations from different sources or different regimes . due to the inhomogeneity of the data , estimating regressions or classifications or graphs over",
    "the pool of all available data is likely to estimate effects that might be strong for one part of the data but very weak or even of opposite sign for another part . here",
    ", we proposed to estimate effects which are present for all possible groupings of the data ( even if they might be masked by noise if we make the groups unreasonably small ) .",
    "the improvement in predictive accuracy can be seen empirically .",
    "we have introduced the notion of maximin effects and proposed an estimator for these effects , using either a lasso or ridge - type penalty .",
    "if we have known groups of observations with a different parameter setting in each group , the estimator is guaranteed to do as well in estimating the maximin effects as standard lasso estimation would in estimating the average effect in a single group of these observations . for datasets with unknown groups ,",
    "we have proposed to sample groups at random from the available observations .",
    "this has a similar flavor to `` stability selection '' [ @xcite ] and the `` bolasso '' [ @xcite ] or [ @xcite ] , where models are fitted repeatedly over random ( bootstrap ) samples of the data .",
    "in contrast to these approaches , though , the estimator is trying to infer the `` maximin effects '' if the underlying regression coefficients change randomly , which is a novel concept .",
    "we have presented theoretical guarantees for the statistical accuracy , an efficient computational algorithm which is feasible for large - scale problems , as well as empirical results on real data demonstrating improved performance for prediction .",
    "we expect that the notion of maximin effects is useful beyond regression and classification for ` big data' applications , both from a statistical as well as computational point of view , potentially helping to avoid detecting too many spurious effects that are not replicable .",
    "let @xmath218 be the cholesky decomposition of @xmath20 .",
    "since we assumed @xmath20 to be full - rank , @xmath219 is invertible , and we define @xmath220 then @xmath221 define @xmath222 to be the choice of weights in the simplex that is minimizing the @xmath223-norm of the corresponding vector in the convex hull @xmath224 , @xmath225 the proof is complete if we can show that @xmath226 since this implies the result , using ( [ eqhe0 ] ) , the invertibility of @xmath219 and @xmath227    by definition of @xmath222 in ( [ eqgs ] ) , it holds true that for every @xmath228 , @xmath229 since @xmath222 and @xmath230 are from a convex set ( namely @xmath224 ) , and @xmath222 minimizes the @xmath223-norm over this convex set . since equality holds for @xmath231 , the derivative of the left - hand side with respect to @xmath232 at @xmath231 has to be positive , which is equivalent to @xmath233 hence @xmath234 choosing @xmath235 yields thus a value of the objective function of at least @xmath236 in  ( [ eq1lts ] ) .    on the other hand , choosing @xmath237 in ( [ eq1lts ] ) , for all @xmath238 , @xmath239 with equality only if @xmath240 .",
    "the value of the objective function in ( [ eq1lts ] ) can hence not exceed @xmath241 .",
    "choosing @xmath235 in ( [ eq1lts ] ) yields thus the optimal value of the objective function and is indeed a solution to ( [ eq1lts ] ) , which completes the proof .",
    "we write @xmath242 instead of @xmath243 to simplify notation , and use the constrained version of the estimator .",
    "note that the theorem is for known groups , and thus @xmath244 for some @xmath245 .",
    "let @xmath246 for all @xmath133 . using the basic inequality , for any fixed vector @xmath247 in the feasible region ,",
    "that is , for all @xmath247 with @xmath248 , @xmath249 using the definition of @xmath250 , @xmath251\\label{eqbasic2 } \\\\[-8pt]\\nonumber & & \\quad\\qquad { }   -2d{\\vert}\\xi{\\vert}_1 \\max _ g{\\vert}b_g{\\vert}_1 - d{\\vert}\\xi{\\vert}_1 ^ 2 -2 { \\vert}\\xi{\\vert}_1 { \\vert}\\delta_g{\\vert}_\\infty.\\nonumber\\end{aligned}\\ ] ] hence , using @xmath252 , and using that by definition of @xmath191 , @xmath253 ( and hence , when using theorem  [ theoremmaximin ] that @xmath44 is in the convex hull of @xmath9 , also @xmath254 ) , it follows that @xmath255 where @xmath256 which completes the proof .",
    "starting as in the proof of theorem  [ theorembasic ] , let @xmath257 be the index set of the @xmath74th group .",
    "the explained variance in group @xmath74 when using a regression vector @xmath258 can then be written as @xmath259 analogous to ( [ eqbasic ] ) , we have the basic inequality for all @xmath247 with @xmath260 , @xmath261\\\\[-8pt]\\nonumber & & \\qquad \\ge\\min _ g \\frac{1 } m \\sum_{i\\in i_g}\\bigl ( 2(x_i \\xi ) ( x_i b_i ) - ( x_i\\xi)^2 + 2(x_i \\xi ) \\varepsilon_i \\bigr).\\end{aligned}\\ ] ] as the pareto condition is fulfilled ( with corresponding probability @xmath131 ) , there exists a subset @xmath262 such that @xmath263 for all @xmath264 , and an essential subset is formed by @xmath265 . restricting the minimum on the left - hand side of ( [ eqbasic2a ] ) over all groups in @xmath266",
    ", we have for all @xmath247 with @xmath260 , @xmath267 where @xmath268 .",
    "hence , using @xmath248 , @xmath269 analogous to ( [ eqbasic2aa ] ) , the first term on the left - hand side is bounded with probability at least @xmath109 by @xmath270 since @xmath123 is by assumption an essential subset , we have that the first term on the left - hand side is bounded with probability at least @xmath109 by @xmath271 thus , for all @xmath247 with @xmath260 , @xmath272 since @xmath44 is in the feasible region , we can use it for @xmath247 to get @xmath273\\\\[-8pt]\\nonumber & & \\qquad \\ge \\min_g \\frac{1 } m \\sum_{i\\in i_g}\\bigl ( 2(x_i b_{\\mathrm{maximin } } ) ( x_i b_i ) - ( x_i b_{\\mathrm{maximin}})^2 \\bigr).\\end{aligned}\\ ] ]    now , by definition of @xmath44 , when letting @xmath52 be the convex hull of @xmath32 ( where @xmath32 is again the support of @xmath9 ) , @xmath274 where we have used in the first equality linearity with respect to the argument @xmath43 and in the second the definition of @xmath44 and the fact that @xmath44 is in the convex hull of the support @xmath32 of @xmath9 .    now bounding the fluctuations on the right - hand side of ( [ eqaddh1 ] )",
    ", we use that @xmath275 . using hoeffding s inequality and a union bound over all groups , for any @xmath276 , if @xmath98 and @xmath277 , @xmath278 plugging this into ( [ eqaddh1 ] ) , it holds with probability @xmath279 for all @xmath247 with @xmath260 that @xmath280 which shows the first part of the claim in ( [ eqtheorand ] ) if we use a union bound to exclude the event which does not correspond to the pareto condition , and hence this excluded event has corresponding probability at most @xmath281 .",
    "the value of @xmath95 can be bounded with the help of ( [ eqdeviation ] ) with probability @xmath109 to yield @xmath282 and the latter bound will then hold true with probability at least @xmath138 .",
    "the second part in ( [ eqtheorand ] ) follows as ( [ eqbasic3c ] ) implies @xmath283 the claim follows by @xmath284 and rearranging terms ."
  ],
  "abstract_text": [
    "<S> large - scale data are often characterized by some degree of inhomogeneity as data are either recorded in different time regimes or taken from multiple sources . </S>",
    "<S> we look at regression models and the effect of randomly changing coefficients , where the change is either smoothly in time or some other dimension or even without any such structure . </S>",
    "<S> fitting varying - coefficient models or mixture models can be appropriate solutions but are computationally very demanding and often return more information than necessary . </S>",
    "<S> if we just ask for a model estimator that shows good predictive properties for all regimes of the data , then we are aiming for a simple linear model that is reliable for all possible subsets of the data . </S>",
    "<S> we propose the concept of `` maximin effects '' and a suitable estimator and look at its prediction accuracy from a theoretical point of view in a mixture model with known or unknown group structure . under certain circumstances </S>",
    "<S> the estimator can be computed orders of magnitudes faster than standard penalized regression estimators , making computations on large - scale data feasible . </S>",
    "<S> empirical examples complement the novel methodology and theory .    </S>",
    "<S> ./style / arxiv - general.cfg </S>"
  ]
}