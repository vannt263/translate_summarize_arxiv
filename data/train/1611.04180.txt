{
  "article_text": [
    "this paper considers the budgeted information gathering problem . our aim is to maximally",
    "explore a world with a robot that has a budget on the total amount of movement due to battery constraints .",
    "this problem fundamentally recurs in mobile robot applications such as autonomous mapping of environments using ground and aerial robots @xcite , monitoring of water bodies @xcite and inspecting models for 3d reconstruction @xcite .    the nature of `` interesting '' objects in an environment and their spatial distribution influence the optimal trajectory a robot might take to explore the environment . as a result , it is important that a robot learns about the type of environment it is exploring as it acquires more information and adapts it s exploration trajectories accordingly . this adaptation must be done online , and we provide such an algorithm in this paper .    to illustrate our point ,",
    "consider two extreme examples of environments for a particular mapping problem , shown in fig .",
    "[ fig : marquee ] . consider a robot equipped with a sensor ( rgbd camera ) that needs to generate a map of an unknown environment .",
    "it is given a prior distribution about the geometry of the world , but has no other information .",
    "this geometry could include very diverse settings .",
    "first it can include a world where there is only one ladder , but the form of the ladder must be explored ( fig .",
    "[ fig : marquee : a ] ) , which is a very dense setting .",
    "second , it could include a sparse setting with spatially distributed objects , such as a construction site ( fig .",
    "[ fig : marquee : b ] ) .",
    "the important task for the robot is to now try to infer which type of environment it is in based on the history of measurements , and thus plan an efficient trajectory . at every time step ,",
    "the robot visits a sensing location and receives a sensor measurement ( e.g. depth image ) that has some amount of information utility ( e.g. surface coverage of objects with point cloud ) .",
    "as opposed to naive lawnmower - coverage patterns , it will be more efficient if the robot could use a policy that maps the history of locations visited and measurements received to decide which location to visit next such that it maximizes the amount of information gathered in the finite amount of battery time available .",
    "the ability of such a learnt policy to gather information efficiently depends on the prior distribution of worlds in which the robot has been shown how to navigate optimally .",
    "[ fig : marquee : a ] shows an efficient learnt policy for inspecting a ladder , which executes a helical motion around parts of the ladder already observed to efficiently uncover new parts without searching naively .",
    "this is efficient because given the prior distribution the robot learns that information is likely to be geometrically concentrated in a particular volume given it s initial observations of parts of the ladder .",
    "similarly fig .",
    "[ fig : marquee : b ] shows an effective policy for exploring construction sites by executing large sweeping motions . here",
    "again the robot learns from prior experience that wide , sweeping motions are efficient since it has learnt that information is likely to be dispersed in such scenarios .",
    "thus our requirements for an efficient information - gathering policy can be distilled to two points :    1 .",
    "_ reasoning about posterior distribution over world maps : _ the robot should use the history of movements and measurements to infer a posterior distribution of worlds . this can be used infer locations that are likely to contain information and efficiently plan a trajectory .",
    "however the space of world maps is very large , and it is intractable to compute this posterior online .",
    "_ reasoning about non - myopic value of information : _ even if the robot is able to compute the posterior and hence the value of information at a location , it has to be cognizant of the travel cost to get to that location .",
    "it needs to exhibit non - myopic behavior to achieve a trade - off that maximizes the overall information gathered .",
    "performing this computationally expensive planning at every step is prohibitively expensive .    even though its is natural to think of this problem setting as a pomdp , we frame this problem as a novel data - driven imitation learning problem @xcite .",
    "we propose an algorithm explore(exploration by learning to imitate an oracle ) that trains a policy on a dataset of worlds by imitating a _",
    "clairvoyant oracle_. during the training process , the oracle has full information about the world map ( and is hence clairvoyant ) and plans movements to maximize information . the policy is then trained to imitate these movements as best as it can using partial information from the current history of movements and measurements . as a result of our novel formulation , we are able to sidestep a number of challenging issues in pomdps like explicitly computing posterior distribution over worlds and planning in belief space .",
    "our contributions are as follows    1 .",
    "we map the budgeted information gathering problem to a pomdp and present an approach to solve it using imitation learning .",
    "2 .   we present an approach to train a policy on the non - stationary distribution of event traces induced by the policy itself .",
    "we show that this implicitly results in the policy operating on the posterior distribution of world maps .",
    "we show that by imitating an oracle that has access to the world map and thus can plan optimal routes , the policy is able to learn non - myopic behavior . since the oracle is executed only during train time , the computational burden does not affect online performance .",
    "the remainder of this paper is organized as follows .",
    "section [ sec : prob ] presents the formal problem , while section [ sec : rel_work ] contains relevant work .",
    "the algorithm is presented in section [ sec : apprach ] and section [ sec : res ] presents experimental results .",
    "finally we conclude in section [ sec : conc ] with discussions and thoughts on future work .",
    "let @xmath0 be a set of nodes corresponding to all sensing locations .",
    "the robot starts at node @xmath1 .",
    "let @xmath2 be a sequence of nodes ( a path ) such that @xmath3 .",
    "let @xmath4 be the set of all such paths .",
    "let @xmath5 be the world map .",
    "let @xmath6 be a measurement received by the robot .",
    "let @xmath7 be a measurement function .",
    "when the robot is at node @xmath8 in a world map @xmath9 , the measurement @xmath10 received by the robot is @xmath11 .",
    "let @xmath12 be a utility function .",
    "for a path @xmath13 and a world map @xmath9 , @xmath14 assigns a utility to executing the path on the world . note that @xmath15 is a set function . given a node @xmath16 , a set of nodes @xmath17 and world @xmath9 , the discrete derivative of the utility function @xmath18 is @xmath19",
    "let @xmath20 be a travel cost function . for a path @xmath13 and a world map @xmath9 ,",
    "@xmath21 assigns a travel cost to executing the path on the world .",
    "we first define the problem setting when the world map is fully known .",
    "[ prob : fully_obs : cons ] given a world map @xmath9 , a travel cost budget @xmath22 and a time horizon @xmath23 , find a path @xmath13 that maximizes utility subject to travel cost and cardinality constraints .",
    "@xmath24    now , consider the setting where the world map @xmath9 is unknown .",
    "given a prior distribution @xmath25 , it can be inferred only via the measurements @xmath26 received as the robot visits nodes @xmath27 .",
    "hence , instead of solving for a fixed path , we compute a policy that maps history of measurements received and nodes visited to decide which node to visit .",
    "[ prob : partially_obs : cons ] given a distribution of world maps , @xmath25 , a travel cost budget @xmath22 and a time horizon @xmath23 , find a policy that at time @xmath28 , maps the history of nodes visited @xmath29 and measurements received @xmath30 to compute node @xmath31 to visit at time @xmath28 , such that the expected utility is maximized subject to travel cost and cardinality constraints .",
    "the markov decision process ( mdp ) is a tuple @xmath32 defined upto a fixed finite horizon @xmath23 .",
    "it is defined over an augmented state space comprising of the ego - motion state space @xmath33 ( which we will refer to as simply the state space ) and the space of world maps @xmath34 .",
    "let @xmath35 be the state of the robot at time @xmath28 .",
    "it is defined as the set of nodes visited by the robot upto time @xmath28 , @xmath36 .",
    "this implies the dimension of the state space is exponentially large in the space of nodes , @xmath37 .",
    "the initial state @xmath38 is the start node .",
    "let @xmath39 be the action executed by the robot at time @xmath28 .",
    "it is defined as the node visited at time @xmath40 , @xmath41 .",
    "the set of all actions is defined as @xmath42 .",
    "given a world map @xmath9 , when the robot is at state @xmath43 the utility of executing an action @xmath44 is @xmath45 .",
    "let @xmath46 be the set of feasible actions that the robot can execute when in state @xmath43 in a world map @xmath9 .",
    "this is defined as follows @xmath47 let @xmath48 be the state transition function . in our setting , this is the deterministic function @xmath49 .",
    "let @xmath50 $ ] be the one step reward function .",
    "it is defined as the normalized marginal gain of the utility function , @xmath51 .",
    "let @xmath52 be a policy that maps state @xmath43 and world map @xmath9 to a feasible action @xmath53 .",
    "the value of executing a policy @xmath54 for @xmath28 time steps on a world @xmath9 starting at state @xmath43 .",
    "@xmath55}\\ ] ] where @xmath56 is the distribution of states at time @xmath57 starting from @xmath43 and following policy @xmath54 .",
    "the state action value @xmath58 is the value of executing action @xmath44 in state @xmath43 in world @xmath9 and then following the policy @xmath54 for @xmath59 timesteps    @xmath60}\\ ] ]    the value of a policy @xmath54 for @xmath23 steps on a distribution of worlds @xmath25 and starting states @xmath61    @xmath62}\\ ] ]    the optimal mdp policy is @xmath63 .",
    "the partially observable markov decision process ( pomdp ) is a tuple @xmath64 .",
    "the first component of the augmented state space , the ego motion state space @xmath33 , is fully observable .",
    "the second component , the space of world maps @xmath34 , is partially observable through observations received .",
    "let @xmath65 be the observation at time step @xmath28 .",
    "this is defined as the measurement received by the robot @xmath66 .",
    "let @xmath67 be the probability of receiving an observation @xmath68 given the robot is at state @xmath43 and executes action @xmath44 . in our setting",
    ", this is the deterministic function @xmath69 .",
    "let the belief at time @xmath28 , @xmath70 , be the history of state , action , observation tuple received so far , i.e. @xmath71 .",
    "note that this differs from the conventional use of the word belief which would usually imply a distribution .",
    "however , we use belief here to refer to the history of state , action , observations conditioned on which one can infer the posterior distribution of world maps @xmath72 .",
    "let the belief transition function be @xmath73 .",
    "let @xmath74 be a policy that maps state @xmath43 and belief @xmath75 to a feasible action @xmath53 .",
    "the value of executing a policy @xmath76 for @xmath28 time steps starting at state @xmath43 and belief @xmath75 is @xmath77}\\ ] ] where @xmath78 is the distribution of beliefs at time @xmath57 starting from @xmath75 and following policy @xmath76 .",
    "@xmath79 is the posterior distribution on worlds given the belief @xmath80 .",
    "similarly the action value function @xmath81 is defined as @xmath82 } + \\\\                          & { \\mathbb{e}_{{\\psi } ' \\sim p({\\psi } ' \\mid { \\psi } , { a } ) , { s } ' \\sim p({s } ' \\mid { s } , { a})}\\left[{\\tilde{v}^{{\\tilde{\\pi}}}_{t-1}}({s } ' , { \\psi}')\\right ] } \\end{aligned}\\ ] ]    the optimal pomdp policy can be expressed as @xmath83}\\ ] ] where @xmath84 is a uniform distribution over the discrete interval @xmath85 , @xmath86 is the distribution of states following policy @xmath76 for @xmath28 steps , @xmath87 is the distribution of belief following policy @xmath76 for @xmath28 steps . the value of a policy @xmath88 for @xmath23 steps on a distribution of worlds @xmath25 , starting states @xmath61 and starting belief @xmath89 @xmath90}\\ ] ] where the posterior world distribution @xmath91 uses @xmath25 as prior",
    "problem  [ prob : fully_obs : cons ] is a submodular function optimization ( due to nature of @xmath18 ) subject to routing constraints ( due to @xmath92 ) . in absence of this constraint",
    ", there is a large body of work on near optimality of greedy strategies by krause et al.@xcite - however naive greedy approaches can perform arbitrarily poorly .",
    "chekuri and pal @xcite propose a quasi - polynomial time recursive greedy approach to solving this problem .",
    "singh et al.@xcite show how to scale up the approach to multiple robots . however , these methods are slow in practice .",
    "iyer and bilmes @xcite solve a related problem of submodular maximization subject to submodular knapsack constraints using an iterative greedy approach .",
    "this inspires zhang and vorobeychik @xcite to propose an elegant generalization of the cost benefit algorithm ( gcb ) which we use as an oracle in this paper .",
    "yu et al.@xcite frame the problem as a correlated orienteering problem and propose a mixed integer based approach - however only correlations between neighboring nodes are considered .",
    "hollinger and sukhatme @xcite use sampling based approaches which require a lot of evaluations of the utility function in practice .    problem  [ prob : partially_obs : cons ] in the absence of the travel cost constraint can be efficiently solved using the framework of adaptive submodularity developed by golovin et al.@xcite as shown by javdani et al.@xcite and chen et al.@xcite .",
    "hollinger et al.@xcite propose a heuristic based approach to select a subset of informative nodes and perform minimum cost tours .",
    "singh et al.@xcite replan every step using a non - adaptive information path planning algorithm .",
    "such methods suffer when the adaptivity gap is large @xcite .",
    "inspired by adaptive tsp approaches by gupta et al.@xcite , lim et al.@xcite propose recursive coverage algorithms to learn policy trees .",
    "however such methods can not scale well to large state and observation spaces .",
    "heng et al.@xcite make a modular approximation of the objective function .",
    "isler et al.@xcite survey a broad number of myopic information gain based heuristics that work well in practice but have no formal guarantees .",
    "online pomdp planning also has a large body of work ( @xcite .",
    "although there exists fast solvers such as pomcp ( silver and veness @xcite ) and despot ( somani et al.@xcite ) , the space of world maps is too large for online planning .",
    "[ fig : alg : overview ] shows an overview of our approach .",
    "the central idea is as follows - we train a policy to imitate an algorithm that has access to the world map at train time .",
    "the policy @xmath93 maps features extracted from state @xmath43 and belief @xmath75 to an action @xmath44 .",
    "the algorithm that is being imitated has access to the corresponding world map @xmath9 .",
    "we now formally define imitation learning as applied to our setting .",
    "let @xmath88 be a policy defined on a pair of state and belief @xmath94 .",
    "let _ roll - in _ be the process of executing a policy @xmath76 from the start upto a certain time horizon .",
    "similarly _ roll - out _ is the process of executing a policy from the current state and belief till the end .",
    "let @xmath95 be the distribution of states induced by roll - in with policy @xmath76 .",
    "let @xmath96 be the distribution of belief induced by roll - in with policy @xmath76 .",
    "let @xmath97 be the loss of a policy @xmath76 when executed on state @xmath43 and belief @xmath76 .",
    "this loss function implicitly captures how well policy @xmath76 imitates a reference policy ( such an an oracle algorithm ) .",
    "our goal is to find a policy @xmath98 which minimizes the observed loss under its own induced distribution of state and beliefs .",
    "@xmath99}\\ ] ]    this is a non - i.i.d supervised learning problem .",
    "ross and bagnell @xcite show how such problems can be reduced to no - regret online learning using dataset aggregation ( dagger ) . the loss function they consider @xmath100 is a mis - classification loss with respect to what the expert demonstrated . ross and bagnell @xcite",
    "extend the approach to the reinforcement learning setting where @xmath100 is the cost - to - go of an oracle reference policy by aggregating values to imitate ( aggrevate ) .",
    "when ( [ eq : pomdp : opt_policy ] ) is compared to the imitation learning framework in ( [ eq : imitation_learning ] ) , we see that in addition to the induced state belief distributions , the loss function analogue @xmath97 is @xmath101 .",
    "this implies rolling out with policy @xmath76 . for poor policies @xmath76",
    ", the action value estimate @xmath102 would be very different from optimal values @xmath103 .    in our approach",
    ", we alleviate this problem by defining a surrogate value functions to imitate - the cumulative reward gathered by a clairvoyant oracle .    given a distribution of world map @xmath25 , a clairvoyant oracle @xmath104 is a policy that maps state @xmath43 and world map @xmath9 to a feasible action @xmath53 such that it approximates the optimal mdp policy , @xmath105 .",
    "the term _ clairvoyant _ is used because the oracle has full access to the world map @xmath9 at train time .",
    "the oracle can be used to compute state action value as follows    @xmath106}\\ ] ]    our approach is to imitate the oracle during training .",
    "this implies that we train a policy @xmath98 by solving the following optimization problem    @xmath107}\\ ] ]      initialize @xmath108 , @xmath109 to any policy in @xmath110 [ alg : qvalagg : init ] initialize sub dataset @xmath111 [ alg : qvalagg : initsub ] let roll in policy be @xmath112 [ alg : qvalagg : mixpol ] collect @xmath113 data points as follows : sample world map @xmath9 from dataset @xmath25 [ alg : qvalagg : sampleworld ] sample uniformly @xmath114 [ alg : qvalagg : sampletime ] assign initial state @xmath38 [ alg : qvalagg : initialstate ] execute @xmath115 up to time @xmath59 to reach @xmath116 [ alg : qvalagg : rollin ] execute any action @xmath117 [ alg : qvalagg : takeaction ] execute oracle @xmath118 from @xmath40 to @xmath23 on @xmath9 [ alg : qvalagg : execoracle ] collect value to go @xmath119 [ alg : qvalagg : collectval ] @xmath120 [ alg : qvalagg : aggrsubdata ] aggregate datasets : @xmath121 [ alg : qvalagg : aggrdata ] train cost - sensitive classifier @xmath122 on @xmath123 + [ alg : qvalagg : updatelearner ]   ( _ alternately : use any online learner @xmath122 on @xmath124 _ ) * return * best @xmath125 on validation    alg .",
    "[ alg : qvalagg ] describes the explorealgorithm .",
    "the algorithm iteratively trains a sequence of learnt policies @xmath126 by aggregating data for an online cost - sensitive classification problem .",
    "@xmath109 is initialized as a random policy ( line  [ alg : qvalagg : init ] ) . at iteration @xmath57 ,",
    "the policy that is used to roll - in is a mixture policy of learnt policy @xmath125 and the oracle policy @xmath118 ( line  [ alg : qvalagg : mixpol ] ) using mixture parameter @xmath127 . a set of @xmath128 cost - sensitive classification datapoints are captured as follows : a world @xmath9 is sampled ( line  [ alg : qvalagg : sampleworld ] ) . the @xmath115 is used to roll - in upto a random time from an initial state to reach @xmath116 ( lines  [ alg : qvalagg : sampletime][alg : qvalagg : rollin ] ) .",
    "an exploratory action is selected ( line  [ alg : qvalagg : takeaction ] ) .",
    "the clairvoyant oracle is given full access to @xmath9 and asked to roll - out and provide an action value @xmath129 ( lines  [ alg : qvalagg : execoracle][alg : qvalagg : collectval ] ) .",
    "@xmath130 is added to a dataset @xmath124 of cost - sensitive classification problem and the process is repeated ( line  [ alg : qvalagg : aggrsubdata ] ) .",
    "@xmath124 is appended to the original dataset @xmath123 and used to train an updated learner @xmath122 ( lines  [ alg : qvalagg : aggrdata][alg : qvalagg : updatelearner ] ) .",
    "the algorithm returns the best learner from the sequence based on performance on a held out validation set ( or alternatively returns a mixture policy of all learnt policies ) .",
    "one can also try variants where all actions @xmath131 are executed , or an online learner is used to update @xmath98 instead of dataset aggregation @xcite .",
    "following the analysis style of aggrevate@xcite , we first introduce a _ hallucinating oracle_.    given a prior distribution of world map @xmath25 , a hallucinating oracle @xmath132 computes the instantaneous posterior distribution over world maps and takes the action with the highest expected value .",
    "@xmath133}\\ ] ]    while the hallucinating oracle is not the optimal pomdp policy ( [ eq : pomdp : opt_policy ] ) , it is an effective policy for information gathering as alluded to in @xcite and we now show that we effectively imitate it .",
    "the policy optimization rule in ( [ eq : imitateclairvoyantoracle ] ) is equivalent to @xmath134}\\ ] ] by using the fact that @xmath135 } =   { \\mathbb{e } _",
    "{ \\substack{{\\phi}\\sim p({\\phi}),\\\\ { \\psi}\\sim p({\\psi}\\mid { \\phi } , { \\tilde{\\pi } } , t)}}\\left[{q^{{\\tilde{\\pi}_{\\mathrm{or}}}}_{t}}({s } , { \\phi } , { a})\\right ] } $ ]    consequently our learnt policy has the following guarantee    n iterations of explore , collecting @xmath113 regression examples per iteration guarantees that with probability at least @xmath136 @xmath137 where @xmath138 is the empirical average online learning regret on the training regression examples collected over the iterations and @xmath139 is the empirical regression regret of the best regressor in the policy class .",
    "for both proofs , refer to @xcite .",
    "our implementation is open source and available for matlab and c++ goo.gl/hxnqws .",
    "the utility function @xmath18 is selected to be a fractional coverage function ( similar to @xcite ) which is defined as follows .",
    "the world map @xmath9 is represented as a voxel grid representing the surface of a 3d model .",
    "the sensor measurement @xmath140 at node @xmath8 is obtained by raycasting on this 3d model .",
    "a voxel of the model is said to be ` covered ' by a measurement received at a node if a point lies in that voxel .",
    "the coverage of a path @xmath13 is the fraction of covered voxels by the union of measurements received when visiting each node of the path .",
    "the travel cost function @xmath92 is chosen to be the euclidean distance .",
    "the values of total time step @xmath23 and travel budget @xmath22 varies with problem instances and are specified along with the results .",
    "we use the generalized cost benefit algorithm ( gcb ) @xcite as the oracle algorithm owing to its small run times and acceptable solution qualities .",
    "the tuple @xmath141 is mapped to a vector of features @xmath142 . the feature vector @xmath143 is a vector of information gain metrics as described in @xcite .",
    "@xmath144 encode the relative rotation and translation required to visit a node .",
    "we use random forest @xcite to regress to @xmath145 values from features @xmath146 .",
    "the learning details are specified in table .",
    "[ tab : learning_details ] .",
    "lcccc & * train * & * test * & * explore * & * feature * + & * dataset @xmath113 * & * dataset * & * iterations @xmath147 * & * dimension @xmath148 * + 2d & @xmath149 & @xmath149 & @xmath149 & @xmath150 + 3d & @xmath149 & @xmath151 & @xmath151 & @xmath150 +    [ tab : learning_details ]      for baseline policies , we compare to the class of information gain heuristics discussed in @xcite .",
    "the heuristics are remarkably effective , however , their performance depends on the distribution of objections in a world map . as exploreuses these heuristic values as part of its feature vector , it will implicitly learn a data driven trade - off between them .",
    "we create a set of 2d exploration problems to gain a better understanding of the behavior of the exploreand baseline heuristics .",
    "a dataset comprises of 2d binary world maps , uniformly distributed nodes and a simulated laser .",
    "the training size is @xmath149 , @xmath152 , @xmath153 .",
    "[ fig : results : matlab : a ] shows a dataset created by applying random affine transformations to a pair of parallel lines .",
    "this dataset is representative of information being concentrated in a particular fashion .",
    "[ fig : results : matlab : c ] shows a comparison of explorewith baseline heuristics .",
    "the heuristic rear side voxelperforms the best , while exploreis able to match the heuristic . fig .",
    "[ fig : results : matlab : e ] shows progress of explorealong with two relevant heuristics - rear side voxeland average entropy .",
    "rear side voxeltakes small steps focusing on exploiting viewpoints along the already observed area .",
    "average entropyaggressively visits unexplored area which is mainly free space .",
    "exploreinitially explores the world but on seeing parts of the lines reverts to exploiting the area around it .",
    "[ fig : results : matlab : b ] shows a dataset created by randomly distributing rectangular blocks around the periphery of the map . this dataset is representative of information being distributed around .",
    "[ fig : results : matlab : c ] shows that the heuristic average entropyperforms the best , while exploreis able to match the heuristic .",
    "rear side voxelsaturates early on and performs worse .",
    "[ fig : results : matlab : e ] shows that rear side voxelgets stuck exploiting an island of information .",
    "average entropytakes broader sweeps of the area thus gaining more information about the world .",
    "explorealso shows a similar behavior of exploring the world map .",
    "thus we see that on changing the datasets the performance of the heuristics reverse while our data driven approach is able to adapt seamlessly .",
    "[ fig : results : extra ] shows results from other 2d datasets such as random disks and block worlds , where exploreis able to outperform all heuristics .",
    "we create a set of 3d exploration problems to test the algorithm on more realistic scenarios .",
    "the datasets comprises of 3d worlds created in gazebo and simulated kinect .      to show the practical usage of our pipeline",
    ", we show a scenario where a policy is trained on synthetic data and tested on a real dataset .",
    "[ fig : results : cpp : a ] shows some sample worlds created in gazebo to represent an office desk environment on which exploreis trained .",
    "[ fig : results : cpp : b ] shows a dataset of an office desk collected by tum computer vision group @xcite .",
    "the dataset is parsed to create a pair of pose and registered point cloud which can then be used to evaluate different algorithms .",
    "[ fig : results : cpp : c ] shows that exploreoutperforms all heuristics .",
    "[ fig : results : cpp : f ] shows exploregetting good coverage of the desk while the best heuristic occlusion awaremisses out on the rear side of the desk .      fig .",
    "[ fig : results : extra ] shows more datasets where training and testing is done on synthetic worlds .",
    "we presented a novel data - driven imitation learning framework to solve budgeted information gathering problems .",
    "our approach , explore , trains a policy to imitate a clairvoyant oracle that has full information about the world and can compute non - myopic plans to maximize information .",
    "the effectiveness of explorecan be attributed to two main reasons : firstly , as the distribution of worlds varies , the clairvoyant oracle is able to adapt and consequently exploreadapts as well .",
    "secondly , as the oracle computes non - myopic solutions , imitating it allows exploreto also learn non - myopic behaviors .",
    "the authors thank sankalp arora for insightful discussions and open source code for exploration in matlab .",
    "benjamin charrow , gregory kahn , sachin patil , sikang liu , ken goldberg , pieter abbeel , nathan michael , and vijay kumar .",
    "information - theoretic planning with trajectory optimization for dense 3d mapping . in _",
    "rss _ , 2015 .",
    "anupam gupta , viswanath nagarajan , and r  ravi .",
    "approximation algorithms for optimal decision trees and adaptive tsp problems . in _",
    "international colloquium on automata , languages , and programming _ , 2010 ."
  ],
  "abstract_text": [
    "<S> the budgeted information gathering problem - where a robot with a fixed fuel budget is required to maximize the amount of information gathered from the world - appears in practice across a wide range of applications in autonomous exploration and inspection with mobile robots . </S>",
    "<S> although there is an extensive amount of prior work investigating effective approximations of the problem , these methods do not address the fact that their performance is heavily dependent on distribution of objects in the world . in this paper , we attempt to address this issue by proposing a novel data - driven imitation learning framework .    </S>",
    "<S> we present an efficient algorithm , explore , that trains a policy on the target distribution to imitate a </S>",
    "<S> _ clairvoyant oracle _ - an oracle that has full information about the world and computes non - myopic solutions to maximize information gathered . </S>",
    "<S> we validate the approach on a spectrum of results on a number of 2d and 3d exploration problems that demonstrates the ability of exploreto adapt to different object distributions . </S>",
    "<S> additionally , our analysis provides theoretical insight into the behavior of explore . </S>",
    "<S> our approach paves the way forward for efficiently applying data - driven methods to the domain of information gathering .     ] </S>"
  ]
}