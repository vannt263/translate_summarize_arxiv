{
  "article_text": [
    "the kohonen algorithm for self organizing feature maps is defined as follows : every stimulus @xmath0 of an euclidian input space @xmath1 is mapped to the neuron with the position @xmath2 in the neural layer @xmath3 which has the weight vector @xmath4 with the minimal distance in input space , corresponding to the highest neural activity , which is called the ` center of excitation ' or ` winner ' ( fig .",
    "+    = .9 mm    ( 175.00,75.00 ) ( 90.00,70.00)(1,0)70.00 ( 90.00,60.00)(1,0)70.00 ( 90.00,50.00)(1,0)70.00 ( 90.00,40.00)(1,0)70.00 ( 90.00,30.00)(1,0)70.00 ( 90.00,20.00)(1,0)70.00 ( 90.00,70.00)(0,-1)50.00 ( 100.00,70.00)(0,-1)50.00 ( 110.00,70.00)(0,-1)50.00 ( 120.00,70.00)(0,-1)50.00 ( 130.00,70.00)(0,-1)50.00 ( 140.00,70.00)(0,-1)50.00 ( 150.00,70.00)(0,-1)50.00 ( 160.00,70.00)(0,-1)50.00 ( 120.00,50.00 ) ( 120.00,50.00 ) ( 120.00,50.00 ) ( 120.00,60.00 ) ( 130.00,50.00 ) ( 120.00,40.00 ) ( 110.00,50.00 ) ( 110.00,60.00 ) ( 130.00,60.00 ) ( 130.00,40.00 ) ( 110.00,40.00 ) ( 90.00,70.00 ) ( 90.00,60.00 ) ( 90.00,50.00 ) ( 90.00,40.00 ) ( 90.00,30.00 ) ( 90.00,20.00 ) ( 100.00,70.00 ) ( 100.00,60.00 ) ( 100.00,50.00 ) ( 100.00,40.00 ) ( 100.00,30.00 ) ( 100.00,20.00 ) ( 110.00,70.00 ) ( 110.00,30.00 ) ( 110.00,20.00 ) ( 120.00,70.00 ) ( 120.00,30.00 ) ( 120.00,20.00 ) ( 130.00,70.00 ) ( 130.00,30.00 ) ( 130.00,20.00 ) ( 140.00,70.00 ) ( 140.00,60.00 ) ( 140.00,50.00 ) ( 140.00,40.00 ) ( 140.00,30.00 ) ( 140.00,20.00 ) ( 150.00,70.00 ) ( 150.00,60.00 ) ( 150.00,50.00 ) ( 150.00,40.00 ) ( 150.00,30.00 ) ( 150.00,20.00 ) ( 160.00,70.00 ) ( 160.00,60.00 ) ( 160.00,50.00 ) ( 160.00,40.00 ) ( 160.00,30.00 ) ( 160.00,20.00 ) ( 120.00,50.00)(-4,-1)79.00 ( 41.00,30.00)(-1,1)6.00 ( 41.00,30.00 ) ( 123.00,47.00)(0,0)[lt]@xmath2 ( 58.00,73.00)(0,0)[cc ] ( 21.00,52.00)(0,0)[cc]@xmath0 ( 43.00,28.00)(0,0)[lt]@xmath4 ( 37.00,38.00 ) ( 0,0)[lb]@xmath5 ( 59.00,18.00)(0,0)[cc ] ( 136.00,8.00)(0,0)[cc ] ( 24.95,46.00)(1,1)3.00 ( 27.95,49.00)(6,5)6.07 ( 34.02,54.02)(5,4)5.02 ( 39.04,57.99)(4,3)3.97 ( 43.01,60.92)(5,3)5.02 ( 48.03,63.85)(2,1)3.97 ( 52.01,65.73)(5,2)6.07 ( 58.08,68.24)(4,1)5.86 ( 63.93,69.71)(5,1)6.07 ( 70.00,70.96)(1,0)6.07 ( 76.07,70.96)(6,-1)5.86 ( 81.92,69.92)(3,-1)6.07 ( 87.99,67.82)(5,-2)5.02 ( 93.01,65.73)(2,-1)5.02 ( 98.03,63.22)(2,-1)6.07 ( 104.10,60.29)(5,-3)3.97 ( 108.08,57.99)(5,-3)5.02 ( 113.10,55.06)(2,-1)1.88 ( 24.95,46.00 ) ( 24.95,46.00 ) ( 4.98,56.95)(9.07,59.08 ) ( 9.07,59.08)(14.94,61.04 ) ( 14.94,61.04)(19.03,61.92 ) ( 19.03,61.92)(25.96,62.99 ) ( 25.96,62.99)(35.92,64.06 ) ( 35.92,64.06)(51.92,64.06 ) ( 51.92,64.06)(57.97,62.99 ) ( 57.97,62.99)(62.95,61.92 ) ( 62.95,61.92)(70.06,61.04 ) ( 70.06,61.04)(73.09,61.04 ) ( 73.09,61.04)(77.00,61.92 ) ( 77.00,61.92)(78.95,62.99 ) ( 78.95,62.99)(77.00,59.97 ) ( 77.00,59.97)(75.04,56.06 ) ( 75.04,56.06)(73.09,51.08 ) ( 73.09,51.08)(72.02,45.92 ) ( 72.02,45.92)(70.95,37.03 ) ( 70.95,37.03)(70.95,30.98 ) ( 70.95,30.98)(72.02,21.91 ) ( 72.02,21.91)(73.09,18.00 ) ( 73.09,18.00)(75.04,11.96 ) ( 75.04,11.96)(70.95,11.07 ) ( 70.95,11.07)(57.97,10.00 ) ( 57.97,10.00)(51.04,10.00 ) ( 51.04,10.00)(43.92,11.07 ) ( 43.92,11.07)(33.96,13.02 ) ( 33.96,13.02)(25.96,13.91 ) ( 25.96,13.91)(19.92,13.91 ) ( 19.92,13.91)(12.98,13.02 ) ( 12.98,13.02)(11.91,16.94 ) ( 11.91,16.94)(11.03,21.91 ) ( 11.03,21.91)(11.03,27.07 ) ( 11.03,27.07)(12.98,35.07 ) ( 12.98,35.07)(12.98,40.05 ) ( 12.98,40.05)(11.91,46.99 ) ( 11.91,46.99)(11.03,50.01 ) ( 11.03,50.01)(9.96,51.97 ) ( 9.96,51.97)(8.00,54.99 ) ( 8.00,54.99)(6.94,56.06 ) ( 6.94,56.06)(4.98,56.95 )     +   _ * fig . 1 : * a self - organizing mapping @xmath6 from an input space v to a neural layer r.",
    "every stimulus @xmath0 gets assigned to a center of excitation @xmath2 .",
    "weight vectors @xmath4 change according to a learning rule that defines each map algorithm . _   +   +   +    2    in the kohonen model the learning rule for each synaptic weight vector @xmath7 is given by @xmath8 with @xmath9 as a gaussian function of euclidian distance @xmath10 in the neural layer .",
    "the function @xmath9 describes the topology in the neural layer .",
    "the parameter @xmath11 determines the speed of learning and can be adjusted during the learning process .",
    "topology preservation is enforced by the common update of all weight vectors whose neuron @xmath12 is adjacent to the center of excitation @xmath2 .",
    "it is illustrative to consider a lowdimensional example how the self - organizing map adapts to the structure of the stimuli data , which is defined only by the input probability density .",
    "now the neural layer is chosen to be only one - dimensional , and the first and last neuron are connected by periodic boundary conditions .    if the probability density is given by a finite sum of delta - peaks , and - provided a siutable parameter decay - the neural weights of the self - organizing map will converge to these stimuli , as shown in fig .  2 and it will approximately find the shortest route visiting all stimuli @xcite .",
    "+   _ * fig . 2 : * solving the travelling salesman problem with the kohonen map .",
    "_    if we call the stimuli ` cities ' , we recognize this as the famous travelling salesman problem , which is believed to be a np - complete problem - no algorithm can be found that computes the optimal solution within a computation time that scales polynomial with the number of cities : instead computation time scales exponentially .",
    "other algorithms have been given that also give near - optimal solutions : the thermodynamic - motivated simulated annealing of kirkpatrick @xcite , the neural approach by hopfield and tank @xcite , and the elastic net of durbin and willshaw @xcite .",
    "the latter two have been found to be limiting cases of a unified approach @xcite .    if we now replace the input probability density by a continuous one that may now be constant within the country , and zero outside , as shown as background in fig .",
    "3 the weight vectors will try to cover the countryside as good as possible , being in conflict between preserving topology as far as possible , and minimizing the reconstruction error .",
    "the necessary dimension reduction takes place by a snake - like folding of the weights to locally step up and down in the excess dimensions . for the input coming from the retina ,",
    "this dimension reduction task is heavier ( from 5 to 2 dimensions ) @xcite .",
    "+   _ * fig . 3 : * for continuous input spaces of higher dimension ,",
    "the self - organizing map approximates by maeandring structures . _",
    "depending on the ( now assumed to be continuos ) input probability density @xmath13 of the stimuli , an adaptive map algorithm can spend more neurons to represent areas of higher probability density , according to a higher resolution .",
    "the magnification factor is defined as the density of neurons @xmath12 ( i.  e. the density of synaptic weight vectors @xmath14 ) per unit volume of input space , and therefore is given by the inverse jacobian of the mapping from input space to neuron layer : @xmath15 .",
    "( in the following we consider the onedimensional case of noninverting mappings , where @xmath16 is positive . )",
    "the magnification factor is a property of the networks response to a given probability density of stimuli @xmath13 . to evaluate @xmath17 in higher dimensions , one in general has to compute the equilibrium state of the whole network and needs therefore the complete global knowledge on @xmath13 .    for one - dimensional mappings ( and possibly for special geometric cases in higher dimensions ) the magnification factor can follow an universal magnification law ,",
    "that is , @xmath18 is a function only of the local probability density @xmath19 and independent of both the location @xmath12 in the neural layer and the location @xmath20 in input space .",
    "an optimal map from the view of information theory would reproduce the input probability exactly ( @xmath21 with @xmath22 ) , according to a power law with exponent  1 .",
    "this is equivalent to the condition that all neurons in the layer are firing with same probability .",
    "an exponent , on the other hand , corresponds to a uniform distribution of weight vectors , which means there is no adaptation to the stimuli at all .",
    "so the magnification exponent is a direct indicator , how far a self organizing map algorithm is away from the optimum predicted by information theory .",
    "as the brain is assumed to be optimized by evolution for information procession , one would postulate that maximal mutual information is a sound principle governing the setup of neural structures .",
    "such an algorithm of maximal mutual information has been defined by linsker @xcite using the gradient descend in mutual information .",
    "it requires computationally costly integrations , and has no local or other learning rule that allows for biological motivation",
    ".    however , both biological network structures and technical applications are ( due to realization constraints ) not necessarily capable of reaching this optimum , being especially for the brain under discussion @xcite .",
    "even if one had quantitative experimental measurements of the magnification behaviour , the question from what self - organizing dynamics the neural structure emerged remains .",
    "so overall it is desirable to formulate other learning rules that minimize mutual information in a simpler way .",
    "to start with the simplest algorithm : for the classical kohonen algorithm the magnification law ( for onedimensional mappings ) is given by a power law @xmath23 with exponent @xmath24 @xcite .",
    "although for a discrete neural layer and especially for neighborhood kernels with different shape and range there are corrections to the magnification law @xcite , we consider the limit of a continuous neural layer , and restrict our analysis to the onedimensional case .",
    "we now consider an energy function that was at first proposed in @xcite for the classical kohonen algorithm , and is given by the mean squared reconstruction error of the resulting map . for continuous input spaces , or when the borders of the voronoi tesselation shift across a localized stimulus , there have to be corrections .",
    "now one can , if an energy function is desired , turn the argumentation around : if the self - organizing map has no energy function , and if the sqared reconstraction error is an approximate one , start from this energy formula and try to derive what learning rule will result .",
    "kohonen has , utilizing some approximations , shown in @xcite for the one- or two - dimensional case that a gradient descent in the mean squared reconstruction error results in a slightly different learning rule only for the winning neuron , due to that also the borders of the voronoi tesselation are shifting if one evaluates the gradient with respect to a weight vector .    as the additional learning term implies an additional elastic relaxation for the winning neuron , it is straightforward to call it ` winner relaxing ' ( wr ) kohonen algorithm .",
    "as the relaxing term acts only in one direction , the winner is relaxed to its neighbours , but the neighbours stay unattracted , it can not strictly be interpreted as an elastic force or physical interaction .",
    "it is straightforward to generalize the winner relaxing algorithm by introducing the free parameter @xmath25 to the generalized winner relaxing kohonen map @xcite",
    "@xmath26 where @xmath2 is the center of excitation for incoming stimulus @xmath0 , and @xmath27 is a gaussian function of distance in the neural layer with characteristic length @xmath28 .",
    "the original algorithm proposed by kohonen @xcite is obtained for @xmath29 , whereas the classical self organizing map algorithm is obtained for @xmath30 .",
    "( note that only for @xmath29 the algorithm is associated with the potential function ! )",
    "the necessary condition for the final state of the algorithm is that for all neurons the expectation value of the learning step vanishes .",
    "this gives a chapman - kolmogorov - equation for the stochastic learning process of serial presentation .",
    "this can be used also to derive the magnification law of the the winner - relaxing kohonen algorithm @xcite : insertion of the update rule into the stationarity condition and integration yields for @xmath31 the differential equation @xmath32 for @xmath33 @xmath34 @xmath35 and making the ansatz @xmath36 of an universal local magnification law ( that may be expected for the one - dimensional case ) we obtain the differential equation @xmath37 with its solution ( provided that @xmath38 ) @xcite : @xmath39    for the winner - relaxing kohonen algorithm @xmath40 the magnification factor follows an exact power law with magnification exponent @xmath41 , which is smaller than @xmath42 for the classical self organizing feature map .",
    "although the winner - relaxing kohonen algorithm is ` somewhat faster ' @xcite in the initial ordering process , the resulting invariant mapping is slightly less optimal in terms of information theory .    from this result",
    "one would try to invert the relaxing effect by choice of negative values for @xmath25 .",
    "the choice of @xmath43 would lead to the magnification exponent one , if the algorithm is stable for this parameter choice .",
    "this is tested by our numerical experiment described below .",
    "we used the following numerical setup : the network should map the unit interval to a onedimensional neural chain of 100 neurons .",
    "the learning rate was @xmath44 .",
    "the stimulus probability density was chosen exponentially as @xmath45 with @xmath46 .",
    "after an adaptation process of @xmath47 ( elastic net ) further @xmath48 of learning steps were used to calculate average slope and its fluctuation of @xmath49 as a function of @xmath50 ( the first and last @xmath48 of neurons were excluded to eliminate boundary effects ) .",
    "the results are shown for several parameters in fig .",
    "4 .     + _ * fig . 4 : * numerical verification of the wrk / wek magnification law .",
    "the upper line is the thoretical prediction , followed by @xmath51 where the error bars show that the theoretical prediction is met within the precision .",
    "the lower lines are for @xmath28 = 2.0 , 1.0 , 0.5 , and 0.1 , showing that a neighborhood interaction of system size destroys adaptation . _    for the winner relaxing and enforcing algorithm family we did simulations for different @xmath25 and neighborhood length @xmath28 . for small @xmath28 , the neighborhood interaction becomes too weak .",
    "if the gaussian neighborhood extends over some neurons @xmath52 , @xmath53 , the exponent follows the predicted dependence of @xmath25 given by @xmath54 . for @xmath55",
    "we found the system to become instable , this is the case where the additional update term of the winner is larger than the sum over all other update terms in the whole network .",
    "the algorithm remains stable on both stability borders @xmath56 and @xmath43 , and the escape time diverges approaching these boundaries from outside .",
    "a detailed discussion of the numerical study will be found in @xcite . as the relaxing effect of @xmath57",
    "is inverted for @xmath58 , fluctuations are larger than in the kohonen case .    apart from the fact that the exponent can be varied by _ a  priori _",
    "parameter choice between @xmath59 and @xmath60 , the simulations show that our winner enforcing algorithm is in fact able to establish information - theoretically optimal self - organizing maps .",
    "one might suspect that the inversion of a smoothing term might lead to larger fluctuations that could enlarge the time needed for convergence . here",
    "the ordering behaviour is analyzed for a standard setup of 100 neurons with weights and stimuli uniform in the unit interval , with a high learning rate @xmath61 corresponding to parameters used in the initial ordering phase ( at that high learning rate , out of the shown interval the ordering time increased by magnitudes of order ) .",
    "5 shows the averaged number of learning steps per neuron that were needed until a monotonously increasing or decreasing list of weights was reached .",
    "in contrast to the initial exspectation , the minimal ordering time is found neither for @xmath30 ( self - organizing map ) nor for the energy - function associated winner - relaxing ( @xmath62 ) case .",
    "here we have the astonishing result that quicker ordering is not in complete contradiction to near - infomax mapping and can both be realized with the winner - enhancing kohonen algorithm .",
    "+ _ * fig . 5 : * ordering behavior : number of learning steps per neuron as a function of parameter @xmath25 . _",
    "after our first study@xcite , herrmann et.al .",
    "@xcite introduced annother modification of the learning process , which was also applied to the neural gas ( which is equivalent to the kohonen map without neighbour interaction ) @xcite .",
    "this approach uses the central idea to make the learning rate @xmath11 locally dependent on the input probability density by a power law with an exponent that is related to the desired magnification exponent , and also an exponent 1 can be obtained . as the input probability density should not be available to the neuronal map that self - organizes from the stimuli drawn from that distribution , it is estimated from the actual local reconstruction mismatch ( being an estimate for the size of the voronoi cell ) and from the time elapsed since the last time being the winner .",
    "due to this estimating character , the learning rate has to be bounded in practical use . from the computational point of view , one has to keep track of the time difference between the firing of two neurons , which introduces some memory term that needs extra storage , and the local learning rate has to be computed , which seems to be more costly than the winner enhancing kohonen .",
    "other modifications consider the _ selection _ of the winner to be probabilistic , leading to much more elegant statistical approaches to potential functions ( see graepel et al .",
    "@xcite and heskes @xcite ) .",
    "the robustness of the winner - relaxing principle has been demonstrated by transferring it to the neural gas architecture , which now allows by this simple approach to obtain a magnification exponent 1 in arbitrary dimension @xcite .",
    "feature maps are self - organizing structures in the brain and in computational neuroscience that can efficiently represent high - dimensional and complex input spaces .",
    "retina , skin and other perceptual receptor areas are represented in a topology - preserving manner , i.  e. if adjacent neighbours are firing , the active receptor cells also are adjacent .",
    "the detailed structure of these neural maps , including all synaptic connections , can not be coded genetically , so it appears necessary to develop models that set up their structure by a self - organizing progress .",
    "the self - organizing map has become the most prominent model and been applied to many technical problems .",
    "several other models , the linsker algorithm , the elastic net algorithm and the winner relaxing kohonen algorithm have also been considered as models for feature maps and used in technical applications .",
    "most of them follow from an extremal principle , given by information theory , physical motivations , or reconstruction error .",
    "but what extremal principles govern the feature maps in the brain ?    to answer this question finally would reqire more quantitative data about the magnification behaviour in experiments , which then would give a basis to judge how close nature comes to the optimum given by information theory .",
    "magnification - adjustable models as the winner - relaxing and winner - enhancing self - organizing map can become a valuable tool for comparison with experiments and further refinement of the theoretical understanding of the brain ."
  ],
  "abstract_text": [
    "<S> the magnification behaviour of a generalized family of self - organizing feature maps , the winner relaxing and winner enhancing kohonen algorithms is analyzed by the magnification law in the one - dimensional case , which can be obtained analytically . </S>",
    "<S> the winner - enhancing case allows to acheive a magnification exponent of one and therefore provides optimal mapping in the sense of information theory . </S>",
    "<S> a numerical verification of the magnification law is included , and the ordering behaviour is analyzed . </S>",
    "<S> compared to the original self - organizing map and some other approaches , the generalized winner enforcing algorithm requires minimal extra computations per learning step and is conveniently easy to implement . </S>",
    "<S> + * keywords : * self - organizing maps , kohonen algorithm , mutual information , magnification exponent    2    the self - organizing map defined by kohonen in 1982 @xcite startet a class of highly successful neural network models , since it has shown both relevance for modeling of biological networks and engineering of artificial neurocomputing including data analysis and self - organized dimension reduction of complex structured and high - dimensional input spaces .    </S>",
    "<S> the qualitative biological inspiration comes from cortical receptor fields as the receptor fields of the retina , and the receptor field of the skin . for both </S>",
    "<S> , neighboured sensory input always will be represented by cortical activity that is also of neighboured location in the neural tissue . such topology - preserving _ </S>",
    "<S> neural maps _ are known quite a long , and apart from error - tolerant computation , two striking properties are known : first , there is high plasticity , e.g. when a finger is cut off , the neurons now lacking sensory information begin to specialize themselves for the adjacent fingers . </S>",
    "<S> second , as the complete structure ( of all synaptic weights even of parts of the brain ) is far too complex to be coded genetically , the detailed structure has to emerge from a self - organizing process , obviously stochastically driven by the sensory information .    </S>",
    "<S> while the qualititative structure of biological maps can successfully be modeled even with the simple kohonen map , there are many variants and modifications leading to qualitatively similar self - organizing topology - preserving maps . the approaches to define and discuss these are as different as the resulting algorithms , but can be categorized roughly as follows . </S>",
    "<S> ( a ) derivation from first principles , as energy or cost functions , mutual information , averaged representation error , distortion measures and every combination of these . </S>",
    "<S> ( b ) argumentation from structure , from a realistic biological model to discussion of optimal technical implementations . </S>",
    "<S> ( c ) extraction of measurable quantitative properties , as magnification laws , properties of fluctuation , ordering , adaptation , error - tolerance , and spatial frequency of singularities upon dimension reduction , as in the retina . </S>",
    "<S> finally ( d ) restriction to the simplest possible models , which is merely a physicists point of view .    </S>",
    "<S> as there are different aims of using feature maps , these may naturally lead to different viewpoints . </S>",
    "<S> for some technical applications , it may be convenient to use any kind of vector quantization , e.  g. the kohonen model _ wihout _ any neighborhood interactions , and to apply some sorting algorithm to set up topological order afterwards . in the brain , hovever </S>",
    "<S> , it is assumed that the topological structure is set up by self - organization , therefore here we focus on self - organizing maps only .    </S>",
    "<S> compared to the elastic net algorithm of durbin and willshaw @xcite and the linsker algorithm @xcite which are performing gradient descent in a certain energy landscape , the kohonen algorithm seems to have no energy function . </S>",
    "<S> although the learning process can be described in terms of a fokker - planck equation @xcite , the expectation value of the learning step is a nonconservative force @xcite driving the process so that it has no associated energy function . furthermore , the relationships of the kohonen model to both alternative models and general principles are still an open field @xcite    in this paper we analyze an algorithm which is a gradient system ( and therefore has an extremal principle ) in terms of the principle of maximizing mutual information . </S>",
    "<S> as one has to assume the existence of a functional that is optimized by evolution , the question of extremal principles is central in theoretical brain research . </S>",
    "<S> mutual information is assumed to play an essential role in any energy landscape that may describe the evolution of the brain . to measure optimality in sense of information theory we consider the magnification law for onedimensional maps . </S>",
    "<S> the magnification factor is defined as the number of synaptic weight vectors ( respectively neurons ) per unit volume of input space . </S>",
    "<S> maps of maximal mutual information show a power law with exponent 1 , but the algorithm given by linsker @xcite requires a complicated learning rule , whereas an exponent 0 corresponds to no adaptation to the stimuli at all . </S>",
    "<S> the exponent 1 is equivalent to that all neurons have the same firing probability .    </S>",
    "<S> the leading question is if there are models that are suitable to describe biological maps and show a sufficient magnification behavior . </S>",
    "<S> the exponents for the kohonen @xcite and the linsker algorithm are known quite a long . </S>",
    "<S> also the elastic net also shows a universal ( i. e. depending only on the local stimulus density ) magnification law which however is not a power law @xcite , and for serial presentation does not allow for both stability and infomax mapping .    with the generalized winner relaxing kohonen algorithm howver , by inverting the ` relaxing ' term an exponent 1 can be acheived , with a minimal computational extension of the algorithm . </S>"
  ]
}