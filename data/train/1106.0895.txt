{
  "article_text": [
    "we start by showing that the sequence @xmath43 is sub additive ; the methodology is similar to gallager s proof in ( * ? ? ?",
    "9.8.1 ) for the case of no feed - forward . then , by showing that the sequence @xmath4 is sub - additive , following ( * ? ? ?",
    "* lemma 4a.2 ) we obtain our main objective , i.e. , @xmath295    to commence , we recall that a sequence @xmath213 is called sub - additive if for all @xmath214 , @xmath215 let @xmath296 be arbitrary positive integers and , for a given @xmath23 , let @xmath297 and @xmath298 be the conditional pmfs that achieve the minimum of the directed information with block length of @xmath1 and @xmath299 , i.e. , that achieve @xmath4 and @xmath300 , respectively .",
    "suppose we transmit @xmath301 samples as follows ; the first @xmath1 samples are transmitted using @xmath302 , and the sequential @xmath299 samples are transmitted using @xmath303 .",
    "hence , the overall conditional pmf is @xmath304 we can see in section [ secalgs ] that the directed information can be written as @xmath305 from the construction of the conditional overall pmf @xmath306 , its clear that @xmath307 furthermore , @xmath308 thus , it follows that @xmath309 since the source is stationary , we can start the input block at any given time index ; thus the pmfs @xmath302 and @xmath303 achieve @xmath310 on the right - hand side of equation ( [ upb ] ) , while the left - hand side is greater than @xmath311 since we attempt to minimize the expression to achieve the rate distortion function .",
    "hence , we obtain @xmath312 using ( * ? ? ?",
    "* lemma 4a.2 ) for sub - additive sequences , we obtain @xmath313",
    "in this appendix we prove lemma [ lemconverse ] , which provides for us that the mathematical expression for the rate distortion feed - forward @xmath211}\\leq d}i(\\hat{x}^n\\to x^n),\\label{rdi2}\\end{aligned}\\ ] ] is a lower bound to the operational definition @xmath35 .",
    "consider any @xmath61 rate distortion with feed - forward code defined by the mappings @xmath314 as given in section [ secprobres ] , equation ( [ mapenddec ] ) , and distortion constraint @xmath3}\\leq d+\\epsilon_n$ ] , where @xmath315 as @xmath1 goes to infinity .",
    "let the message sent be a random variable @xmath316 , and assume that the distortion constraint is satisfied .",
    "then we have the following chain of inequalities : @xmath317 where ( a ) follows from the fact that the alphabet of @xmath26 is @xmath318 , ( b ) follows from the chain rule for mutual information , ( c ) is due to the fact that given @xmath319 , we know @xmath320 , and ( d ) is since conditioning reduces the entropy .",
    "step ( e ) follows the chain rule for directed information .",
    "taking @xmath1 to infinity , we obtain @xmath321 , and the distortion constraint satisfies @xmath71}\\leq d.\\nonumber\\end{aligned}\\ ] ]",
    "in this appendix we provide a proof for theorem [ thrdmax ] .",
    "we recall that theorem [ thrdmax ] states that the rate distortion function can be written as the following optimization problem : @xmath322 where , for some causal conditioned probability @xmath217 , @xmath218 satisfies the inequality constraint @xmath323    we prove this theorem in two ways .",
    "one is similar to berger s proof in @xcite , based on the inequality @xmath220 , for the regular rate distortion function .",
    "the other is using the lagrange duality between the minimization problem we are familiar with and a maximization problem as presented in @xcite and @xcite .",
    "we also provide the connection between the curve of @xmath4 and the parameter @xmath221 ; this is embodied in lemma [ rtag ] .",
    "before we begin , we recall that a step in alg .",
    "[ algs ] is defined by the following equality @xmath324 this equality is the outcome of differentiating the lagrangian when @xmath325 is fixed , as given in section [ secder ] .",
    "we shall use this equality throughout the proof .",
    "as mentioned , the first proof follows the one in @xcite .",
    "first , we show that for every @xmath326 for which the distortion constraint is satisfied , the following chain of inequalities holds @xmath327}-\\sum_{x^n}p(x^n)\\log\\gamma(x^n)\\nonumber\\\\ & = \\sum_{x^n,\\hat{x}^n}p(x^n)r(\\hat{x}^n|x^n)\\log\\frac{r(\\hat{x}^n|x^n)2^{\\lambda      d(x^n,\\hat{x}^n)}}{q(\\hat{x}^n||x^{n-1})\\gamma(x^n)}\\nonumber\\\\ & \\stackrel{(b)}{\\geq}\\sum_{x^n,\\hat{x}^n}p(x^n)r(\\hat{x}^n|x^n )      \\left(1-\\frac{q(\\hat{x}^n||x^{n-1})\\gamma(x^n)}{r(\\hat{x}^n|x^n)2^{\\lambda d(x^n,\\hat{x}^n)}}\\right)\\nonumber\\\\ & = 1-\\sum_{x^n,\\hat{x}^n}q(\\hat{x}^n||x^{n-1})p(x^n)\\gamma(x^n)2^{-\\lambda d(x^n,\\hat{x}^n)}\\nonumber\\\\ & \\stackrel{(c)}{\\geq}1-\\sum_{x^n,\\hat{x}^n}q(\\hat{x}^n||x^{n-1})p'(x^n||\\hat{x}^n)\\nonumber\\\\ & \\stackrel{(d)}{=}0,\\nonumber\\end{aligned}\\ ] ] where ( a ) follows from the fact that the distortion @xmath23 exceeds @xmath328}$ ] for every @xmath326 as has been assumed , ( b ) follows from the inequality @xmath329 , ( c ) is due to the constraint in equation ( [ cons2 ] ) , and ( d ) follows from the fact that @xmath330 is equal to some joint distribution @xmath331 @xcite . since the chain of inequalities is true for every @xmath326 , we can choose the one that achieves @xmath4 , and then divide by @xmath1 to obtain the inequality in equation ( [ lowb3 ] ) in our theorem .    to complete the proof of theorem [ thrdmax ] , we need to show that equality holds in the chain of inequalities above for some @xmath218 that satisfies the constraint .",
    "if so , let us denote by @xmath332 the conditional pmf that achieves @xmath4 .",
    "further , we denote by @xmath333 the corresponding causal conditioned pmf .",
    "now , consider the following chain of equalities .",
    "@xmath334}+\\sum_{x^n}p(x^n)\\log\\gamma(x^n)\\nonumber\\\\ & = -\\lambda d+\\sum_{x^n}p(x^n)\\log\\gamma(x^n),\\nonumber\\end{aligned}\\ ] ] where ( a ) is due to a step in the algorithm given by ( [ step1 ] ) , and by the uniqueness of @xmath332 in the algorithm , as shown in lemma [ runique ] , and ( b ) follows the expression for @xmath218 given by @xmath335    therefore , we are left with verifying that the @xmath218 above satisfies the constraint : @xmath336 where ( a ) follows from equation ( [ step1 ] ) , and ( b ) is due to the causal conditioning chain rule .",
    "hence , we showed that @xmath4 is the solution to the optimization problem given in equation ( [ lowb3 ] ) .",
    "we also present an alternative proof for theorem [ thrdmax ] , this using the lagrange duality , as in @xcite , @xcite .",
    "recall that @xmath4 is the result of @xmath337 where @xmath325 is defined by @xmath338 , subject to the following conditions : @xmath339    let us define the lagrangian as @xmath340 where @xmath341 for all @xmath342 .",
    "differentiating the lagrangian , @xmath343 , over the variable @xmath326 , we obtain @xmath344 solving the equation @xmath345 in order to find the optimum value , yields the following expression @xmath346 where @xmath347 . multiplying both sides by @xmath348",
    "we are left with the constraint @xmath349 where @xmath350 is induced by @xmath351 .    from (",
    "* chapter 5.1.3 ) we know that @xmath352 is a lower bound to @xmath4 . substituting the minimizer @xmath326 using equation ( [ minimizer ] ) , and the condition given by equation ( [ cond ] ) into @xmath353",
    ", we obtain the lagrange dual function @xmath354{l l } $ -\\lambda d+\\sum_{x^n}p(x^n)\\log\\gamma'(x^n),$ & $ p(x^n)\\gamma'(x^n)2^{-\\lambda d(x^n,\\hat{x}^n)}\\leq p(x^n||\\hat{x}^n)$\\\\ $ -\\infty,$ & otherwise.\\end{tabular}\\right.\\label{analsm}\\ ] ]    by making the constraints explicit , and since the minimization problem is convex , we obtain the lagrange dual problem , i.e. , @xmath4 is the solution to @xmath355 subject to @xmath356 for the @xmath350 that is induced by @xmath351 , and @xmath326 is the optimal pmf .",
    "we use the notation of an _ optimal _ pmf if it achieves the optimal value .",
    "for example , the pmf @xmath326 that achieves the minimum of the directed information given the distortion constraint , is optimal .",
    "we say that the pmf , @xmath350 is optimal , if it is induced by the optimal @xmath326 .",
    "another example is the maximization problem in ( [ maxgp ] ) .",
    "we say that @xmath357 are optimal if they achieve the maximum value .",
    "therefore , @xmath350 is optimal as well if it satisfies equation ( [ cond ] ) .",
    "now , we wish to substitute the constraint to @xmath358 for some @xmath217 .",
    "first , note that we always achieve equality in ( [ condsome ] ) since we can increase the value of @xmath218 and thus increase the objective .",
    "this , combined with the fact that for @xmath359 , @xmath360 must be zero , we have equality in ( [ cond ] ) as well ( if @xmath361 , then @xmath362 , and equation ( [ minimizer ] ) holds too ) .",
    "now , let us assume that the maximum in ( [ maxgp ] ) with the constraint in ( [ condsome ] ) is achieved at a _ non - optimal _",
    "@xmath217 , i.e. , one that is not achieved using the optimal @xmath357 .",
    "thus , the value obtained in ( [ maxgp ] ) is larger then the value achieved by @xmath350 , i.e. , @xmath4 ( since the maximization includes @xmath350 ) .",
    "however , from the lagrange duality it should be a lower bound to @xmath4 , thus contradicting the fact that the maximum is achieved at a non - optimal @xmath217 .",
    "note , that we can construct the optimal pmf @xmath326 from the solution to the maximization problem presented here .",
    "consider the parameters @xmath363 that achieve ( [ maxgp ] ) , and calculate @xmath350 according to equation ( [ cond ] ) .",
    "the calculation of @xmath326 is done recursively on @xmath364 . for @xmath365 , calculate @xmath366 using @xmath367 further , calculate @xmath368 using @xmath369 now , once we have @xmath370 for every @xmath371 , calculate @xmath364 using @xmath372      \\frac{\\sum_{x_i}p(x^i)r(\\hat{x}^i|x^i)}{p(x^{i-1})r(\\hat{x}^{i-1}|x^{i-1})},\\nonumber\\end{aligned}\\ ] ] and then @xmath373 do so until @xmath374 , and we obtain our optimal @xmath326 .",
    "another lemma we wish to provide is the connection between the curve of @xmath4 and the parameter @xmath221 .",
    "this lemma is similar to the one given by berger in ( * ? ? ?",
    "2.5.1 ) for the case of no feed - forward .    [ rtag ]",
    "consider the expression for @xmath4 given by @xmath375 where @xmath218 and @xmath221 are the variables that maximize ( [ maxgp ] ) .",
    "we have seen that @xmath218 is of the form @xmath376 hence , the slope at distortion @xmath23 is @xmath377 .    the proof is given simply by differentiating the expression for @xmath4 .",
    "@xmath378\\nonumber\\\\ & = -\\frac{\\lambda}{n}+\\frac{1}{n}\\left[-d+\\sum_{x^n}\\frac{p(x^n)}{\\gamma(x^n)}\\frac{d\\gamma(x^n)}{d\\lambda}\\right]\\frac{d\\lambda}{dd}.\\nonumber\\end{aligned}\\ ] ] now , consider the following expression @xmath379 using the @xmath218 given above , we have @xmath380 and thus @xmath381 .",
    "@xmath382      p(x^n)q^*(\\hat{x}^n||x^{n-1})2^{-\\lambda d(x^n,\\hat{x}^n)}\\nonumber\\\\ & = \\sum_{x^n}\\frac{d\\gamma(x^n)}{d\\lambda}p(x^n)\\sum_{\\hat{x}^n}q^*(\\hat{x}^n||x^{n-1})2^{-\\lambda d(x^n,\\hat{x}^n ) }      -\\sum_{x^n,\\hat{x}^n}p(x^n)q^*(\\hat{x}^n||x^{n-1})2^{-\\lambda d(x^n,\\hat{x}^n)}\\gamma(x^n)d(x^n,\\hat{x}^n)\\nonumber\\\\ & = \\sum_{x^n}\\frac{d\\gamma(x^n)}{d\\lambda}\\frac{p(x^n)}{\\gamma(x^n)}-\\sum_{x^n,\\hat{x}^n}p(x^n)r^*(\\hat{x}^n|x^n)d(x^n,\\hat{x}^n)\\nonumber\\\\ & = \\sum_{x^n}\\frac{d\\gamma(x^n)}{d\\lambda}\\frac{p(x^n)}{\\gamma(x^n)}-d\\nonumber\\\\ & = 0.\\nonumber\\end{aligned}\\ ] ] hence , we can conclude that @xmath383\\frac{d\\lambda}{dd}\\nonumber\\\\ & = -\\frac{\\lambda}{n}.\\nonumber\\end{aligned}\\ ] ]",
    "in this appendix we prove the existence of a sequence of upper and lower bounds to @xmath4 , the rate distortion function with feed - forward .",
    "these bounds correspond to an iteration in alg .",
    "[ algs ] , and both converge to @xmath4 . to this end , we present and prove a few supplementary claims that assist in obtaining our main goal . theorem [ thrdmax ] provides an alternating form ( lagrange dual form ) of an optimization problem achieving @xmath4 , that is proved in app [ apprdmax ] . in lemma [ lemlow2 ]",
    ", we show that in each iteration we can obtain measures that satisfy the constraint in theorem [ thrdmax ] to form a lower bound , and that the bound is tight and achieved as the upper bound converges .",
    "we also provide a proof for the existence of a an upper bound in each iteration .",
    "before we begin , we recall that a step in alg .",
    "[ algs ] is defined by the following equality @xmath384 we shall use this equality throughout the proof .    as mentioned , we use theorem [ thrdmax ] that provides us with the following alternating optimization problem .",
    "@xmath385 where @xmath218 satisfies the inequality constraint @xmath386 for some causal conditioned probability @xmath217 .",
    "we now show that in each iteration in alg .",
    "[ algs ] , choosing @xmath218 appropriately forms a lower bound for @xmath4 .",
    "[ lemlow2 ] in the @xmath272th iteration in alg .",
    "[ algs ] , by letting @xmath387 and @xmath388 and defining @xmath389 the constraint in equation ( [ cons1 ] ) is satisfied , and forms a lower bound given by @xmath390 furthermore , this lower bound is tight , and is achieved as @xmath391 converges to @xmath4 , where @xmath391 is the upper bound .",
    "let us fix the parameter @xmath392 as in ( [ gammak1 ] ) .",
    "hence , @xmath393 where ( a ) follows from the definition of a step in alg .",
    "[ algs ] and given above in equation ( [ step ] ) , and ( b ) follow the chain rule of causal conditioning , and @xmath394 is a causal conditioned pmf .",
    "hence , combined with ( [ gammak3 ] ) , we obtain @xmath395 thus , we can use theorem [ thrdmax ] , and obtain a lower bound for @xmath4 , i.e. , @xmath396\\nonumber\\\\ & = \\frac{1}{n}\\left[-\\lambda d+\\sum_{x^n}p(x^n)\\log\\gamma'^k_{x^n }      -\\sum_{x^n}p(x^n)\\log\\left(\\max_{\\hat{x}^n , x^{n-1}}c^k_{\\hat{x}^n , x^{n-1}}\\right)\\right]\\nonumber\\\\ & = \\frac{1}{n }      \\left[-\\lambda d+\\sum_{x^n}p(x^n)\\log\\gamma'^k(x^n)-\\log\\left(\\max_{\\hat{x}^n , x^{n-1}}c^k_{\\hat{x}^n , x^{n-1}}\\right)\\right].\\label{lowb2}\\end{aligned}\\ ] ]    to complete the proof of this lemma , we are left to show that as @xmath272 increases , i.e. , the upper bound converges to @xmath4 , the lower bound is tight . for that matter",
    ", we note that the pmfs that achieve the optimum value @xmath397 are unique , as shown in lemma [ runique ] .",
    "thus , it is clear that @xmath398 and @xmath399 placing equation ( [ gammak2 ] ) and ( [ ck2 ] ) in equation ( [ lowb2 ] ) , as shown in theorem [ thrdmax ] , achieves equality instead of the chain of inequalities given .",
    "thus @xmath4 is , in fact , the solution to the optimization problem given in equation ( [ lowb1 ] ) , and we have demonstrated the existence of the lower bound    [ lemlow3 ] in the @xmath272th iteration in alg . [ algs ] , the upper bound to the rate distortion is given by @xmath400 where @xmath401}$ ] .",
    "note , that if @xmath402 produces a distortion @xmath23 , then @xmath403}-\\sum_{x^n}p(x^n)\\log\\sum_{\\hat{x}'^n}q^{k-1}(\\hat{x}'^n||x^{n-1})2^{-\\lambda d(x^n,\\hat{x}'^n)}-      \\sum_{x^n,\\hat{x}^n}p(x^n)r^k(\\hat{x}^n|x^n)\\log\\frac{q^{k}(\\hat{x}^n||x^{n-1})}{q^{k-1}(\\hat{x}^n||x^{n-1})}\\nonumber\\\\ & \\stackrel{(b)}{=}-\\lambda d_k+\\sum_{x^n}p(x^n)\\log\\gamma^k(x^n)-      \\sum_{x^n,\\hat{x}^n}p(x^n)r^k(\\hat{x}^n|x^n)\\log{c^k_{\\hat{x}^n , x^{n-1}}},\\label{lastineq}\\end{aligned}\\ ] ] where ( a ) follows from the definition of a step in alg .",
    "[ algs ] and is given above in equation ( [ step ] ) , and ( b ) follows from the definition of @xmath404 .",
    "hence , we have formed an upper bound to the rate distortion as in the lemma .",
    "note that the only inequality is in the first line of the chain , and is due to the fact that @xmath405 .",
    "however , upon convergence , this inequality is tight",
    ".    we can now conclude our main objective in this appendix .",
    "_ proof of lemma [ lembounds ] _ proving this lemma requires us to present upper and lower bounds that converge to @xmath4 .",
    "lemma [ lemlow2 ] provides us with a lower bound and its tightness , whereas lemma [ lemlow3 ] provides us with a tight upper bound as well , as required .",
    "the markov source is presented in fig .",
    "[ stockmod ] above",
    ". we can describe the process @xmath406 using the equation @xmath407 where @xmath408 , @xmath409 .",
    "this allows us to evaluate @xmath410 : @xmath411 where @xmath412 is the stationary distribution of the source .",
    "now , to find the rate distortion of this model , we start with the converse @xmath413 where ( a ) follows from the fact that conditioning reduces entropy , and ( b ) follows the fact that @xmath414 and @xmath415 increases with @xmath23 for @xmath416 .          where @xmath424 must hold for the following equation @xmath425 i.e. , @xmath426 note , that under this construction , the source @xmath11 is still markovian .",
    "further , from fig . [ markex1 ] we can see that @xmath427 forms a markov chain , and @xmath428 .",
    "thus , we obtain equality in ( a ) , ( b ) in the above chain of inequalities , and hence showed that @xmath429 by taking @xmath1 to infinity we obtain @xmath430"
  ],
  "abstract_text": [
    "<S> in this paper we consider the rate distortion problem of discrete - time , ergodic , and stationary sources with feed forward at the receiver . </S>",
    "<S> we derive a sequence of achievable and computable rates that converge to the feed - forward rate distortion . </S>",
    "<S> we show that , for ergodic and stationary sources , the rate @xmath0 is achievable for any @xmath1 , where the minimization is taken over the transition conditioning probability @xmath2 such that @xmath3}\\leq d$ ] . </S>",
    "<S> the limit of @xmath4 exists and is the feed - forward rate distortion . </S>",
    "<S> we follow gallager s proof where there is no feed - forward and , with appropriate modification , obtain our result . </S>",
    "<S> we provide an algorithm for calculating @xmath4 using the alternating minimization procedure , and present several numerical examples . </S>",
    "<S> we also present a dual form for the optimization of @xmath4 , and transform it into a geometric programming problem .    </S>",
    "<S> alternating minimization procedure , blahut - arimoto algorithm , causal conditioning , concatenating code trees , directed information , ergodic and stationary sources , geometric programming , ergodic modes , rate distortion with feed - forward .    </S>",
    "<S> introduction[secintro ] the rate distortion function for memoryless sources is well known and was given by shannon in his seminal work@xcite . </S>",
    "<S> shannon@xcite showed that the rate distortion function is the minimum of mutual information between the source @xmath5 and the reconstruction @xmath6 , where the minimization is over transition probabilities @xmath7 such that the distortion constraint is satisfied , i.e. , @xmath8}\\leq d$ ] . in the case where the source is stationary and ergodic </S>",
    "<S> , gallager@xcite showed that the rate distortion is the limit of the following sequence of rates . </S>",
    "<S> each member of the sequence is the @xmath1th order rate distortion function , which is the solution of the following minimization problem @xmath9 the minimization is over all conditional probabilities @xmath2 such that the distortion constraint is satisfied , i.e. , @xmath3}\\leq d$ ] . </S>",
    "<S> gallager showed that the limit of the sequence @xmath10 exists and is equal to the infimum of the sequence .    </S>",
    "<S> the problem of source coding with feed - forward was introduced by weissman and merhav@xcite and by venataramanan and pradhan@xcite , and is depicted in fig . </S>",
    "<S> [ rdprob ] .    [ ] [ ] [ 1]@xmath11[][][1]decoder[][][1]@xmath12 [ ] [ ] [ 1]@xmath13 [ ] [ ] [ 1]delay @xmath14[][][1]encoder[][][1]delay @xmath14[][][0.8]@xmath15    , and needs to reconstruct the source within the constraint @xmath3}\\leq d$].,width=377 ]    weissman and merhav@xcite named the problem competitive predictions . in their work , they defined a set of functions that predict the following @xmath16 given the previous @xmath17 . after defining the _ loss function _ between @xmath16 and the prediction , the objective was to minimizing the expected loss over all sets of predictors of size @xmath18 . an important result in @xcite </S>",
    "<S> is that in the case where the innovation process @xmath19 is i.i.d . </S>",
    "<S> the distortion - rate with feed - forward function is the same as the distortion - rate function of @xmath20 , where there is no feed - forward . </S>",
    "<S> in particular , if @xmath16 is an i.i.d . process , then @xmath21 and thus the distortion - rate with feed - forward for the source @xmath16 is the same as if there is no feed - forward .    </S>",
    "<S> venkataramana and pradhan@xcite gave an explicit definition of the rate distortion feed - forward for an arbitrary normalized distortion function and a general source . </S>",
    "<S> their goal was to provide the rate @xmath22 of a source given a distortion @xmath23 using causal conditioning and directed information . the source of information </S>",
    "<S> is modeled as the process @xmath24 and is encoded in blocks of length @xmath1 into a message @xmath25 . the message @xmath26 ( after @xmath1 time units ) </S>",
    "<S> is sent to the decoder that has to reconstruct the process @xmath27 using the message @xmath26 and causal information of the source with some delay @xmath14 as in fig . </S>",
    "<S> [ rdprob ] .    for that purpose , venkataramanan and pradhan@xcite defined the measures @xmath28 and @xmath29 the limsup in probability of a sequence of random variables @xmath27 is defined as the smallest extended real number @xmath30 such that @xmath31 , @xmath32=0,\\nonumber\\end{aligned}\\ ] ] and the liminf in probability is the largest extended real number @xmath33 such that @xmath31 , @xmath34=0.\\nonumber\\end{aligned}\\ ] ]    the main result in @xcite is that for a general source @xmath27 and distortion @xmath23 , the rate distortion with feed - forward @xmath35 is given by @xmath36 where the infimum is evaluated over the set @xmath37 of probabilities @xmath38 that satisfy the distortion constraint . </S>",
    "<S> moreover , if @xmath39 venkataramana and pradhan showed in @xcite , that @xmath40    the work of venkataramanan and pradhan has made a significant contribution since it gives a multi - letter characteristic for the rate distortion function with feed - forward . in @xcite </S>",
    "<S> , they evaluated these formulas for a stock - market example and provided an analytical expression for the rate distortion function . </S>",
    "<S> however , these types of formulas are still very hard to evaluate for the general case . in this paper </S>",
    "<S> we show that assuming ergodicity and stationarity of the source , the rate distortion function with feed - forward and delay @xmath41 is upper bounded by @xmath4 , where @xmath42}\\leq d } i(\\hat{x}^n\\rightarrow x^n).\\label{rlim}\\end{aligned}\\ ] ] we further show that the limit of the sequence @xmath43 exists , is equal to @xmath44 , and is the rate distortion feed - forward function @xmath35 . </S>",
    "<S> these expressions for @xmath4 are computable using a blahut - arimoto - type algorithm or using geometric programming , as demonstrated here .    in most models with causal constraints , such as feedback channels or feed - forward rate distortion , the causal conditioning probability , as well as the directed information characterizes the fundamental limits . in order to address these models , </S>",
    "<S> the causal conditioning probability was introduced by massey@xcite and kramer@xcite and is defined as @xmath45 the difference between regular and causal conditioning is that in causal conditioning the dependence of @xmath46 on future @xmath47 is not taken into account . </S>",
    "<S> following the causal conditioning probability , massey @xcite ( who was inspired by marko s work @xcite on bidirectional communication ) introduced the directed information , defined as @xmath48    the directed information was used by tatikonda and mitter@xcite , permuter , weissman , and goldsmith @xcite , and kim @xcite to characterize the point - to - point channel capacity with feedback . </S>",
    "<S> it is shown that the capacity of such channels is characterized by the maximization of the directed information over the input probability @xmath49 . in a previous paper@xcite , we used these results and obtained bounds to estimate the feedback channel capacity using a blahut - arimoto - type algorithm ( baa ) for finding the global optimum of the directed information .    </S>",
    "<S> the main contribution of this work lies in extending the achievability proof given by gallager in @xcite to the case where feed - forward with delay @xmath41 exists . </S>",
    "<S> the extension is done by using the causal conditioning distribution , @xmath50 , rather than the regular reconstruction distribution @xmath51 , in order to construct the codebook . </S>",
    "<S> the proof given is for @xmath41 , but can be extended straightforwardly to any delay @xmath52 . </S>",
    "<S> the difficulty in this modification is that while in @xcite the codebook was an ensemble of sequences ( code words ) from the reconstruction alphabet using @xmath51 , our codebook is an ensemble of code trees using @xmath50 . </S>",
    "<S> this induced a major problem while showing that the probability of error is small , as discussed in section [ secachieve ] . </S>",
    "<S> these difficulties were overcome by appropriate modification to gallager s proofs .    </S>",
    "<S> another contribution of this paper is the development of two optimization methods for obtaining @xmath4 ; a ba - type algorithm and a geometric programming(gp ) form . </S>",
    "<S> the gp form is given as a maximization problem , which can be solved using standard convex optimization methods . </S>",
    "<S> further , this maximization problem gives us a lower bound to the rate distortion with feed - forward , which helps us decide when to terminate the algorithm .    </S>",
    "<S> the remainder of the paper is organized as follows . in section [ secprobres ] </S>",
    "<S> we describe the problem model , provide the operational definition of the rate distortion function with feed - forward , and state our main theorems . in section [ secachieve ] we show that @xmath4 is an achievable rate for all @xmath1 and any distortion @xmath23 , and in section [ secoperational ] we show that the limit of @xmath4 exists and is equal to the operational rate distortion function . in section [ secalggp ] </S>",
    "<S> we present an alternative optimization problem for @xmath4 in a standard geometric programming form that can be solved numerically using convex optimization tools . in section </S>",
    "<S> [ secalgs ] we give a description of the baa for calculating @xmath4 and present the algorithm s complexity and the memory required , and in section [ secder ] we derive the baa and prove its convergence to the optimum value . </S>",
    "<S> numerical examples are given in section [ secex ] to illustrate the performance of the suggested algorithms .    </S>",
    "<S> problem statement and main results[secprobres ] in this section we present notation , describe the problem model and summarize the main results of the paper . </S>",
    "<S> we first state the definitions of a few quantities that we use in our coding theorems . </S>",
    "<S> we denote by @xmath53 the vector @xmath54 . </S>",
    "<S> usually we use the notation @xmath55 for short . </S>",
    "<S> further , when writing a probability mass function ( pmf ) we simply write @xmath56 . </S>",
    "<S> an alphabet of any type is denoted by a calligraphic letter @xmath57 , and its size is denoted by @xmath58 .    in the rate distortion problem with feed - forward of delay @xmath41 , as shown in fig . </S>",
    "<S> [ rdprob ] , we consider a general discrete , stationary , and ergodic source @xmath27 , with the @xmath1th order probability distribution @xmath49 , alphabet @xmath57 and reconstruction alphabet @xmath59 . the normalized bounded distortion measure is defined as @xmath60 on pairs of sequences .    a @xmath61 source code with feed - forward of block length @xmath1 and rate @xmath22 consists of an encoder mapping @xmath62 , </S>",
    "<S> @xmath63 and a sequence of decoder mappings @xmath64 , @xmath65 the encoder maps a sequence @xmath66 to an index in @xmath67 . at time @xmath68 , the decoder has the message that was sent and causal information of the source , @xmath69 , and reconstructs the @xmath68th symbol sent , @xmath46 .    </S>",
    "<S> a rate distortion with feed - forward pair @xmath70 is achievable if there exists a sequence of @xmath61-rate distortion codes with @xmath71}\\leq d.\\nonumber\\end{aligned}\\ ] ]    the rate distortion with feed - forward function @xmath35 is the infimum of rates @xmath22 such that @xmath70 is achievable .    in this paper , we define the mathematical expression for the rate distortion function as the following limit @xmath72 where @xmath4 is the @xmath1th order rate distortion function given by @xmath42}\\leq d}i(\\hat{x}^n\\to x^n).\\nonumber\\end{aligned}\\ ] ] we show that the limit in ( [ limoper ] ) exists , @xmath4 is achievable and upper bounds @xmath73 for all @xmath1 . </S>",
    "<S> further , we show that the rate distortion feed - forward function , @xmath35 , is equal to @xmath73 . </S>",
    "<S> we also provide two ways to calculate numerically the value @xmath4 ; using a ba - type algorithm and a geometric programming form .    </S>",
    "<S> we now state our main theorems .    </S>",
    "<S> [ thrd ] for a discrete , stationary , ergodic source , and for any @xmath23 , any @xmath1 and delay @xmath41 , @xmath4 is an achievable rate .    </S>",
    "<S> [ thoperat ] for any distortion @xmath23 , the operational rate distortion function @xmath35 is equal to the mathematical expression , @xmath73 , where @xmath73 is given by ( [ limoper ] ) .    </S>",
    "<S> [ thrdmaxgp ] the @xmath1th order rate distortion function @xmath4 can be written in a geometric programming standard form as the following maximization problem @xmath74 subject to the constraints : @xmath75    [ ths ] for a fixed source distribution @xmath49 , there exists an alternating minimization procedure in order to compute @xmath42}\\leq d}i(\\hat{x}^n\\rightarrow x^n).\\label{rd1}\\end{aligned}\\ ] ]    proofs to theorem [ thrd ] and [ thoperat ] are given in section [ secachieve ] and section [ secoperational ] , respectively . </S>",
    "<S> the proof for theorem [ thrdmaxgp ] is in section [ secalggp ] , the algorithm in theorem [ ths ] is described in section [ secalgs ] and proved in section [ secder ] .    </S>",
    "<S> achievability proof ( theorem [ thrd]).[secachieve ] in this section we show that if the source is stationary and ergodic , then @xmath4 as given in ( [ rd1 ] ) is achievable for any @xmath1 . in order to do so </S>",
    "<S> , we first assume that the source is ergodic in blocks of length @xmath1 , and show achievability . a source that is </S>",
    "<S> ergodic in blocks is one that , by looking at each @xmath1 letters as a single letter from a super alphabet , we obtain an ergodic super source ( presented in ( * ? ? </S>",
    "<S> * chapter 9.8 ) ) . </S>",
    "<S> then , for the general ergodic sources , we follow a claim given in @xcite about ergodic modes , as explained further on . </S>",
    "<S> the distortion is assumed to be normalized , finite , and of the form @xmath76 for some @xmath77 . </S>",
    "<S> an example for such a distortion can be found in @xcite and in section [ secex ] , in an example called the stock - market .    [ thrdn ] consider a discrete stationary source that is ergodic in blocks of length @xmath1 . for any distortion @xmath23 such that @xmath78 and @xmath79 , and for any @xmath80 sufficiently large , there exists a codebook of trees @xmath81 of length @xmath80 with @xmath82 code trees for which the average distortion per letter satisfies @xmath83}\\leq d+\\delta$ ] .    </S>",
    "<S> let @xmath2 be the transition probability that achieves the minimum @xmath4 and let @xmath84 be the causal conditioning probability that corresponds to @xmath85 .    </S>",
    "<S> * for any @xmath80 , consider the ensemble of codes @xmath81 with @xmath86 code trees of length @xmath80 , where each code tree @xmath87 is a concatenation of @xmath88 sub - code trees of length @xmath1 . each sub - code tree is generated independently according to @xmath84 as in fig . </S>",
    "<S> [ codetree ] . </S>",
    "<S> + [ ] [ ] [ 1]@xmath89 [ ] [ ] [ 1]@xmath90 [ ] [ ] [ 1]@xmath91 [ ] [ ] [ 1]@xmath92 [ ] [ ] [ 1]@xmath93 [ ] [ ] [ 1]@xmath94 [ ] [ ] [ 1]@xmath95 [ ] [ ] [ 1]@xmath96 [ ] [ ] [ 1]@xmath96 [ ] [ ] [ 1]@xmath97 [ ] [ ] [ 1]@xmath98 [ ] [ ] [ 1]code tree 1[][][1]code tree 2 + . </S>",
    "<S> the upper branches are for @xmath99 , and the lower branches are for @xmath100 . </S>",
    "<S> ] * the encoder assigns a code tree @xmath87 for every @xmath101 such that @xmath102 is minimal . </S>",
    "<S> the sequence @xmath103 is determined by walking on tree @xmath104 , and following the branch @xmath105 . * at time @xmath68 , the decoder possesses the index of the tree @xmath104 and causal information of the source @xmath69 , and returns the symbol @xmath106 that it produces .    </S>",
    "<S> let us define a test channel as the conditional probability @xmath107 and the causal conditional probability @xmath108 where the distribution is according to @xmath109 moreover , we define for every code tree @xmath104 of length @xmath80 the measure @xmath110 where @xmath111 . </S>",
    "<S> note that @xmath112 is not the directed information between the sequences @xmath113 , but simply a measure between a source sequence @xmath101 and the output , @xmath114 of the test channel @xmath115 , as defined in ( [ testchan ] ) .    </S>",
    "<S> let @xmath116 be the set of all code trees of length @xmath80 , and consider the following set , @xmath117 and let @xmath118 be the probability of the set @xmath119 on the test channel ensemble .    </S>",
    "<S> let us use the notation @xmath120 where @xmath121 is the ensemble of code trees as described in the coding scheme . </S>",
    "<S> now , let @xmath122 be the probability over the ensemble of codes @xmath121 and source sequences such that the distortion exceeds @xmath123 . </S>",
    "<S> we wish to give an upper bound to the probability @xmath122 ; for this we use the following lemma .    for a given source @xmath124 and test channel </S>",
    "<S> , we have the following inequality @xmath125 where the set @xmath126 is described in ( [ testa ] ) .    _ proof . _ </S>",
    "<S> we first write @xmath127 as @xmath128 for every @xmath101 , let us define the set @xmath129 as the set of all code trees @xmath130 for which @xmath131 , @xmath132    we observe that @xmath133 for a given @xmath101 only if @xmath134 for every @xmath135 . </S>",
    "<S> thus , @xmath133 only if @xmath136 for every @xmath137 . </S>",
    "<S> since @xmath104 are independently chosen , @xmath138 where @xmath139 is the complement set of @xmath129 . </S>",
    "<S> we note that the probability that tree @xmath104 being in @xmath139 depends only on the branch associated with @xmath140 . in other words , if a tree @xmath141 , then all other trees with the same branch associated with @xmath101 is in @xmath139 as well ; the same goes for @xmath129 . </S>",
    "<S> hence , we can divide the set of all code trees @xmath142 into disjoint subsets @xmath143 that have the same branch associated with @xmath105 , i.e. , @xmath144 where @xmath145 is a walk on tree @xmath104 over the branch @xmath105 . clearly , the probability of each subset @xmath143 is @xmath146 since the left hand side is a summation of the probabilities of all trees with the same branch associated with @xmath101 , and we are left with the probability of that one branch .    </S>",
    "<S> now , for every @xmath147 , and due to the definition of @xmath139 , we have @xmath148 therefore , @xmath149 and we obtain that @xmath150 where ( a ) follows the inequality in equation ( [ ineqach ] ) .    using the inequality @xmath151 , and taking @xmath152 , @xmath153 </S>",
    "<S> , we find @xmath154 by taking a sum over @xmath101 we remain with @xmath155 note , that @xmath156 where ( a ) follows the fact that if @xmath157 , then @xmath114 is determined by the tree @xmath104 and the branch @xmath101 . now , </S>",
    "<S> continuing from equation ( [ sumprob ] ) , we obtain @xmath158    we now use the result in ( [ per ] ) in order to complete the proof of the theorem . </S>",
    "<S> furthermore , we can see that the average distortion of the code satisfies @xmath159}(\\leq ( d+\\delta/2)+p_c\\left(d(x^l,\\hat{x}^l({\\cal{t}}_c , x^{l-1}))>l(d+\\delta/2)\\right)\\cdot\\sup_{x^l,\\hat{x}^l}{d(x^l,\\hat{x}^l)}.\\nonumber\\end{aligned}\\ ] ] this arises , as in ( * ? ? ? </S>",
    "<S> * th . </S>",
    "<S> 9.3.1 ) , from upper bounding the distortion by @xmath160 when the @xmath161 , and by @xmath162 otherwise . by choosing @xmath86 , the last term in ( [ per ] ) </S>",
    "<S> goes to zero with increasing @xmath80 . </S>",
    "<S> furthermore , the first term is bounded by @xmath163    note that @xmath164 as assumed , the source is ergodic in blocks of length @xmath1 . </S>",
    "<S> furthermore , the test channel is defined to be memoryless for blocks of length @xmath1 , and hence the joint process is ergodic in blocks of length @xmath1 . </S>",
    "<S> thus , with probability 1 , @xmath165}\\nonumber\\\\ & = r_n(d).\\nonumber\\end{aligned}\\ ] ] therefore , the probability of the first term in ( [ pabound ] ) goes to zero as @xmath80 goes to infinity , and the same goes to the second term due to the definition of the distortion . </S>",
    "<S> in order to finish the proof , and due to the fact that @xmath166 goes to zero with increasing @xmath80 and the fact that the distortion is finite , we can choose @xmath80 large enough such that @xmath167 in this case , we obtain @xmath168 , and hence the rate @xmath4 is achievable for sources that are ergodic in blocks of length @xmath1 .    much like in gallager s proof for the case where there is no feed - forward , we note that not all ergodic sources are also ergodic in blocks , and we need to address these cases as well . for that purpose , </S>",
    "<S> we need ( * ? ? ? </S>",
    "<S> * lemma 9.8.2 ) for ergodic sources . </S>",
    "<S> we recall , that a discrete stationary source is ergodic if and only if every invariant set of sequences under a shift operator @xmath26 is of probability 1 or 0 . in ( </S>",
    "<S> * ? ? ? </S>",
    "<S> * chapter 9.8 ) , the author looks at the operator @xmath169 , i.e. , a shift of @xmath1 places , and considers an invariant set @xmath170 , @xmath171 , with respect to @xmath169 . in lemma 9.8.2 in @xcite , it is stated that one can separate the source @xmath172 to @xmath173 invariant subsets @xmath174 , @xmath175 , with regard to @xmath169 , such that @xmath173 divides @xmath1 and the sets @xmath176 are disjoint except , perhaps , an intersection of zero probability . </S>",
    "<S> these subsets are called _ ergodic modes _ , due to the fact that each invariant subset of them under the operator @xmath169 is of probability 0 or @xmath177 . in other words , </S>",
    "<S> conditional on an ergodic mode @xmath178 each invariant subset of it with respect to @xmath169 , is of probability 0 or 1 .    </S>",
    "<S> recall , that by definition , @xmath179 where the right - hand side is the average directed information between the source and reconstruction , determined according to @xmath2 that achieves @xmath4 . </S>",
    "<S> let @xmath180 be the average directed information between a source sequence from the @xmath68th ergodic mode and the ensemble of codes , using the probability @xmath2 which achieves @xmath4 . </S>",
    "<S> note that the directed information can be written as @xmath181 which is convex over the input probability @xmath49 . </S>",
    "<S> thus , @xmath182 we observe that @xmath183 is an upper bound to the @xmath1th order rate distortion function conditional on the @xmath68th ergodic mode . from theorem [ thrdn ] , we know that there exists a codebook @xmath184 with @xmath185 code trees of length @xmath80 such that the average distortion constraint holds . </S>",
    "<S> another observation is that if a codebook @xmath184 satisfies the distortion constraint , conditional on the ergodic mode @xmath178 , then it has the same effect conditional on the ergodic mode @xmath186 . in other words </S>",
    "<S> , we can encode not only a source sequence from @xmath187 with @xmath188 , but also a shift of the a source sequence in @xmath189 with @xmath190 . </S>",
    "<S> we use these observations while constructing the codebook </S>",
    "<S> .    we can now prove theorem [ thrd ] , i.e. , the achievability of @xmath4 , where the source is ergodic and stationary . </S>",
    "<S> an equivalent version of theorem [ thrd ] is the following : let @xmath4 be the @xmath1th order rate distortion function for a discrete , stationary , and ergodic source . for any @xmath23 such that @xmath78 , and @xmath79 , and any @xmath80 sufficiently large , there exists a codebook of trees @xmath81 of length @xmath80 with @xmath82 code trees for which the average distortion per letter satisfies @xmath191}\\leq d+\\delta$ ] .    </S>",
    "<S> let @xmath2 be the transition probability that achieves @xmath4 and let @xmath84 be the causal conditioning probability that corresponds to @xmath85 .    </S>",
    "<S> * for any @xmath80 and any ergodic mode @xmath178 , @xmath192 , construct an ensemble of codes @xmath184 , with @xmath185 little code trees of length @xmath80 , where each little code tree is generated according to @xmath193 , as in fig . </S>",
    "<S> [ codetree ] in theorem [ thrdn ] above . </S>",
    "<S> now , for every @xmath192 , the @xmath68th codebook is an ensemble of big code trees , which are concatenation of @xmath173 little code trees , starting from one in @xmath184 , and followed by one from @xmath194 to one from @xmath195 , where the index is calculated modiolus @xmath173 . in the example of a big code tree in fig . </S>",
    "<S> [ ncodetree ] we see additional letters at the end of each little code tree , i.e. , in positions @xmath196 , that are fixed . </S>",
    "<S> the purpose of the fixed letters is to shift the sequence and encode it with a codetree from the sequential codebook . </S>",
    "<S> note , that the overall length of a code tree sums up to @xmath197 . </S>",
    "<S> + [ ] [ ] [ 1]codetree from @xmath184 [ ] [ ] [ 1]codetree from @xmath194 [ ] [ ] [ 1]codetree from @xmath198 [ ] [ ] [ 1]fixed letters [ ] [ ] [ 1]code tree 1[][][1]code tree 2 + th codebook , @xmath199 , @xmath200 . ] </S>",
    "<S> * for every @xmath68 , the encoder assigns for every source sequence @xmath201 a code tree @xmath202 from the @xmath68th codebook , such that @xmath203 is minimal . </S>",
    "<S> the sequence @xmath204 is determined by walking on tree @xmath202 , and following the branch @xmath205 . * </S>",
    "<S> the decoder receives a tree @xmath202 and causal information of @xmath206 and returns the sequence @xmath207 that it produces .    since the distortion constraint for every ergodic mode </S>",
    "<S> is satisfied due to theorem [ thrdn ] , the overall distortion is satisfied as well . </S>",
    "<S> the additional fixed letters are of unknown distortion , but due to the face that the distortion is bounded , their contribution is negligible for large @xmath80 . </S>",
    "<S> moreover , note that for every @xmath68 , the @xmath68th codebook is of the same size . </S>",
    "<S> thus , the overall size of the codebook is @xmath208 recall that @xmath197 , and by letting @xmath209 we conclude that @xmath4 is an achievable rate for the general ergodic source , as required .    </S>",
    "<S> proof that @xmath210 ( theorem [ thoperat]).[secoperational ] in this section we show that the operational description of the rate distortion with feed - forward is equal to the mathematical one given in ( [ rdi ] ) . </S>",
    "<S> this will be done first by showing that the mathematical expression @xmath73 is achievable , and then by showing that it is a lower bound to the rate distortion function . </S>",
    "<S> we recall that @xmath211}\\leq d}i(\\hat{x}^n\\to x^n).\\label{rdi}\\end{aligned}\\ ] ]    to show that @xmath73 is achievable we first need to show that the limit of the sequence @xmath43 exists . for this purpose , </S>",
    "<S> we use the following lemma .    </S>",
    "<S> [ lemsubadd ] the sequence @xmath4 , @xmath42}\\leq d}i(\\hat{x}^n\\rightarrow x^n),\\nonumber\\end{aligned}\\ ] ] is a sub - additive sequence , and thus @xmath212    note , that a sequence @xmath213 is called sub - additive if for all @xmath214 , @xmath215 the proof for lemma [ lemsubadd ] is given in app . </S>",
    "<S> [ appsubadd ] .    </S>",
    "<S> we now state a lemma for the achievability of @xmath73 .    </S>",
    "<S> [ lemach ] the mathematical expression for the rate distortion feed - forward @xmath73 is achievable , and thus upper bounds @xmath35 .    </S>",
    "<S> we showed in theorem [ thrd ] that for any @xmath1 , @xmath4 is achievable . </S>",
    "<S> further , in lemma [ lemsubadd ] we show that the limit exists and equal to the infimum , and hence is achievable too . </S>",
    "<S> therefore , we conclude that the mathematical expression @xmath73 is achievable , and forms an upper bound to the operational description @xmath35 .    to show that @xmath73 is a lower bound to the rate distortion function , we provide the following lemma    [ lemconverse ] the mathematical expression @xmath73 is a lower bound to the operational rate distortion function .    for the completeness of the paper , we provide the proof of lemma [ lemconverse ] , this in app . </S>",
    "<S> [ appconv ] . </S>",
    "<S> however , similar proof was presented by venkataramana and pradhan in @xcite , and their expressions involved limit in probability of the entropy and directed information as described in section [ secintro ] .    </S>",
    "<S> combining lemmas [ lemach ] , [ lemconverse ] provides us with the proof for our fundamental theorem , stated in section [ secprobres ] , i.e. , the operational rate distortion function @xmath35 is equal to the mathematical one , @xmath73 .    geometric programming form to @xmath4 ( theorem [ thrdmaxgp])[secalggp ] in this section </S>",
    "<S> we show that the @xmath1th order rate distortion function with feed - forward @xmath4 can be given as a maximization problem , written in a standard form of geometric programming . for this purpose we first state the following theorem .    </S>",
    "<S> [ thrdmax ] the @xmath1th order rate distortion function , @xmath4 , can be written as the following maximization problem @xmath216 where , for some causal conditioned probability @xmath217 , @xmath218 satisfies the inequality constraint @xmath219    in app . </S>",
    "<S> [ apprdmax ] we provide two proofs for theorem [ thrdmax ] ; the first is similar to berger s proof in @xcite for the regular rate distortion function based on the inequality @xmath220 , and the second uses the lagrange duality as presented in @xcite and @xcite that transforms a minimization problem to a maximization one .. app . </S>",
    "<S> [ apprdmax ] also includes the connection between the rate distortion function and the parameter @xmath221 , which states that the slope of @xmath4 in point @xmath23 is @xmath222 .    considering the theorem above </S>",
    "<S> , our interest now is to adjust the constraints in order to obtain a geometric programming form . </S>",
    "<S> we note that the optimization problem in ( [ lowb4 ] ) does not change if we maximize over @xmath217 as well , and the constraint ( [ cons3 ] ) is no longer for some @xmath223 , i.e. , @xmath224 where @xmath225 satisfy the inequality constraint @xmath226 the above statement is true since , on the one hand , the maximization in ( [ lowb4 ] ) increases upon maximizing over another variable , @xmath217 , as in ( [ lowb5 ] ) ; on the other hand , the variable @xmath227 that achieves ( [ lowb5 ] ) satisfy the constraint ( [ cons3 ] ) in theorem [ thrdmax ] , and hence the maximization problem in ( [ lowb5 ] ) can not be greater than the one in ( [ lowb4 ] ) .    to obtain a geometric programming standard form </S>",
    "<S> we transform the constraint in ( [ cons4 ] ) , such that @xmath228 taking the @xmath229 of both sides , we obtain @xmath230    note that maximizing over @xmath217 is the same as maximizing over its products @xmath231 ( * ? ? ? </S>",
    "<S> * lemma 3 ) . </S>",
    "<S> therefore , we can conclude that the rate distortion with feed - forward @xmath4 can be given as a geometric programming maximization form , @xmath232 subject to @xmath75 hence , we obtain a standard form of geometrical programming . </S>",
    "<S> this gp problem can be solved using standard convex optimization tools .    </S>",
    "<S> extension of the baa for rate distortion with feed - forward[secalgs ] in this section we describe an algorithm for calculating @xmath4 , where @xmath233}\\leq d}i(\\hat{x}^n\\rightarrow x^n),\\label{rlim3}\\end{aligned}\\ ] ] using the alternating minimization procedure . </S>",
    "<S> this method was first used by blahut and arimoto @xcite , @xcite to obtain a numerical solution for the i.i.d . source rate distortion and for the memoryless channel capacity . </S>",
    "<S> recently , in @xcite we extended this method for finding the global maximum of the following optimization problem- @xmath234 and we apply similar methods here .    </S>",
    "<S> before we describe the algorithm , let us denote by @xmath235 the pmfs that are participating in the minimization . </S>",
    "<S> further , let us consider the double optimization problem given by @xmath236,\\label{rlimdouble}\\end{aligned}\\ ] ] where @xmath237},\\nonumber\\end{aligned}\\ ] ] and @xmath238 is the directed information that can be written as @xmath239 in section [ secder ] we show that the double optimization problem given in ( [ rlimdouble ] ) is equal to the one given in ( [ rlim3 ] ) . equations ( [ rlimdouble ] ) , ( [ directed4 ] ) allow us to apply the alternating minimization procedure </S>",
    "<S> .    description of the algorithm in algorithm [ algs ] we present the steps required to minimize the directed information where the input pmf @xmath49 is fixed .    </S>",
    "<S> * fix a value of @xmath240 that determines a point on the @xmath4 curve . + * start from a random causally conditioned point @xmath241 . </S>",
    "<S> usually we start from a uniform distribution , i.e. , @xmath242 for every @xmath243 . + * set @xmath244 . + * compute @xmath245 using the formula @xmath246 * calculate the joint probability @xmath247 , and deduce the causal conditioned pmf @xmath248 as in ( [ causalpr ] ) . + * calculate the parameter @xmath249 * calculate @xmath250 * if @xmath251 , set @xmath252 , and return to ( d ) . + </S>",
    "<S> * the rate distortion function , with distortion @xmath253 , is @xmath254    the parameter @xmath221 is used in the lagrangian with which we optimize the directed information . </S>",
    "<S> the value of @xmath255 and hence @xmath256 depends on @xmath221 ; thus choosing @xmath221 appropriately sweeps out the @xmath256 curve . </S>",
    "<S> the algorithm stops when @xmath257 . in app . </S>",
    "<S> [ bounds ] we provide upper and lower bounds , used show that if @xmath257 , we ensure that @xmath258 .    </S>",
    "<S> now , let us present a special case and a few extensions for algorithm [ algs ] .    </S>",
    "<S> * _ regular baa , i.e. , the delay @xmath259_. for delay @xmath259 , the algorithm suggested here meets the original baa , where instead of step ( d ) we have @xmath260 and in step ( e ) , @xmath261 corresponds to the joint probability @xmath262 as well . </S>",
    "<S> moreover , the expression for @xmath263 is reduced to @xmath264 and the termination of the algorithm in step ( g ) is defined by @xmath265 as in the regular blahut - arimoto algorithm @xcite . * </S>",
    "<S> _ function of the feed - forward with general delay @xmath14_. we present a generalization of the algorithm , where the feed - forward is a deterministic function of the source with some delay @xmath14 , @xmath266 . in that case , </S>",
    "<S> step ( d ) is replaced by @xmath267 and in step ( e ) we have @xmath268 where we calculate @xmath269 from the joint distribution @xmath270 . </S>",
    "<S> the algorithm is terminated in the same way , where @xmath271    complexity and memory needed computation complexity and memory needed for the algorithm above is presented in table [ complexmemory2 ] .    </S>",
    "<S> .memory and operations needed extended baa for source coding with feed - forward . [ </S>",
    "<S> cols=\"^,^,^\",options=\"header \" , ]     [ tt3 ]    it was shown in @xcite that the rate - distortion function of a general markov - chain source with @xmath272 states , is given by @xmath273 where @xmath274 $ ] is the stationary distribution of the markov chain , and @xmath275 .    in our special case </S>",
    "<S> we have @xmath276 , i.e. , @xmath277 states for the markov chain , and transition probabilities @xmath278 as illustrated in fig . </S>",
    "<S> [ stockmod ] . </S>",
    "<S> the stationary distribution of such a source is @xmath279 $ ] , and we are left with @xmath280 since the rate can not be less than zero , and is a descending function of the distortion , the rate - distortion function is as above when @xmath281 , i.e. , when @xmath282 , and thus we obtain @xmath283{l l } $ 0.6(h_b(0.2)-h_b(\\frac{d}{0.6})),$ & $ d\\leq0.12$\\\\$0,$ & otherwise.\\end{tabular}\\right.\\label{analsm}\\ ] ]    in fig . </S>",
    "<S> [ smfig](a ) we present the graphs of @xmath4 for @xmath284 up to @xmath285 with the distortion described here and where @xmath286 has the stationary distribution @xmath287 $ ] . </S>",
    "<S> we can see that @xmath4 decreases as @xmath1 increases as expected and converges to the analytical calculation . in fig . </S>",
    "<S> [ smfig ] ( b ) we present the directed information rate estimator only for @xmath285 , where the vector of the distortion is interpolated , i.e. , @xmath288 . </S>",
    "<S> we can see that this estimator is much more accurate than the one in fig . </S>",
    "<S> [ smfig ] ( a ) .    </S>",
    "<S> [ ] [ ] [ 0.8]@xmath35 [ ] [ ] [ 0.8]@xmath23 [ ] [ ] [ 0.8]@xmath1 [ ] [ ] [ 0.8 ] [ ] [ ] [ 0.8]@xmath35 [ ] [ ] [ 0.8]@xmath23    \\(a ) graph of @xmath4 ; the arrow marks the way @xmath4 responds to @xmath1 increasing . </S>",
    "<S> the dashed line is the analytical calculation . </S>",
    "<S> + ( b ) graph of @xmath288 . </S>",
    "<S> the circles represent the performance of alg . </S>",
    "<S> [ algs ] . </S>",
    "<S> [ smfig ]    the effects of the delay on @xmath4 in this example we use the markov source ( fig . </S>",
    "<S> [ stockmod ] ) example with a single letter distortion . </S>",
    "<S> we run alg . </S>",
    "<S> [ algs ] with delays @xmath289 and block length @xmath290 , where @xmath286 has the stationary distribution . </S>",
    "<S> we expect the rate distortion function to increase with the delay @xmath14 . </S>",
    "<S> this is expected because as the delay @xmath14 increases the value of the directed information increases as well . due to the fact that for @xmath291 all graphs are close together , we present @xmath4 only for @xmath292 , and the results are shown in fig . </S>",
    "<S> [ markdelay ] .    </S>",
    "<S> [ ] [ ] [ 0.8]@xmath35 [ ] [ ] [ 0.8]@xmath23 [ ] [ ] [ 0.8]@xmath293     for a markov source as a function of the delay.,width=226 ]    conclusions in this paper we considered the rate distortion problem of discrete - time , ergodic , and stationary sources with feed forward at the receiver . </S>",
    "<S> we first derived a sequence of achievable rates , @xmath294 , that converge to the feed - forward rate distortion . by showing that the sequence is sub - additive </S>",
    "<S> , we proved that the limit of @xmath4 exists and thus equals to the feed - forward rate distortion . </S>",
    "<S> we provided an algorithm for calculating @xmath4 using the alternating minimization procedure , and also presented a dual form for the optimization of @xmath4 , and transformed it into a geometric programming maximization problem . </S>"
  ]
}