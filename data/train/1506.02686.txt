{
  "article_text": [
    "modeling spatio - temporal data , such as high resolution video , is hard .",
    "the sheer dimensionality of the data often makes global inference methods difficult .",
    "similarly curses of dimensionality for textual and time - series data have been met with great success by hmms @xcite , using localized models for prediction and tractable inference on sequences .",
    "inspired by this example , we look to localized models for modeling of spatio - temporal data , like video and fmri data . _",
    "light cone _",
    "methods , such as mixed licors @xcite , successfully reduce the global inference task to iterating a tractable , localized one .",
    "these methods can be used for both regression ( point predictions of @xmath0-valued outputs from input variables ) and computing probability densities .",
    "the latter property allows one to tractably compute distributions over spaces of events , e.g. , over the space of all possible videos , @xmath1 , just as hmms induce probability distributions over the set @xmath2 of all possible sequences ( figure  [ fig : video ] ) .",
    "this ability could make light cone decompositions as general and useful for modeling spatio - temporal data as hmms are for textual and time - series data .    , and the space of all videos , @xmath1.,title=\"fig : \" ] , and the space of all videos , @xmath1.,title=\"fig : \" ]    the goals of this manuscript are thus : ( 1 ) showing how light cone decompositions help make spatio - temporal modeling tasks tractable ; ( 2 ) introducing three easy - to - implement light cone algorithms , allowing others to begin experimenting with light cone methods ; ( 3 ) assessing the predictive accuracy of light cones methods on two video prediction tasks ; and ( 4 ) providing a finite sample guarantee on the error of predictive state light cone methods .",
    "we begin with some preliminaries .",
    "given a random field @xmath3 , observed for each point @xmath4 on a regular spatial lattice @xmath5 at discrete time instants @xmath6 , we seek to approximate a joint likelihood over the observations of the spatio - temporal process , and to accurately forecast the future of the process .",
    "since causal influences in physical systems only propagate at finite speed ( denoted @xmath7 ) , we follow @xcite and adopt the concept of _ light cones _ , which are defined as the set of events that could influence @xmath8 . formally , a past light cone ( plc ) is the set of all past variables that could have affected @xmath3 : @xmath9 similarly , a future light cone ( flc ) is the set of all future events that could be affected by @xmath8 . as a practical matter ,",
    "not all past ( or future ) events are equally informative , since more recent events tend to exert greater causal influence .",
    "thus , in practice , we can approximate the true past light cone with a much smaller subset light cone , improving tractability without incurring much predictive error .",
    "furthermore , we adopt the conditional independence assumption for light cones given in @xcite , which allows for the factorization of the joint likelihood into the product of conditional likelihoods . indexing each @xmath3 by a single integer @xmath10 for simplicity of notation ,",
    "the joint pdf of @xmath11 factorizes as @xmath12 where the proportionality accounts for incompletely observed light cones along the edge of the field .    given this factorization , it becomes natural to seek equivalence classes of light cones , namely , i.e. , to cluster light cones into sets based on the similarity of their conditional distributions .",
    "such equivalence classes of past light cones are _ predictive states _ @xcite , and",
    "our immediate goals become twofold : first , to discover these latent predictive states ( i.e. , learn a mapping @xmath13 from plcs to predictive states ) , and second , to estimate the conditional distribution over @xmath14 given its predictive state . @xcite",
    "introduced _ licors _ as a nonparametric method of predictive state reconstruction , followed by _ mixed licors _ @xcite as a mixture model extension of licors , where each future light cone is forecast using a mixture of extremal predictive states .",
    "mixed licors has predictive advantages over the original licors , but requires finding an @xmath15 matrix of weights ( where @xmath16 is the number of light cones and @xmath17 the number of predictive states ) using a form of em , where each weight is determined using a kernel density estimate on all points .",
    "each em iteration takes @xmath18 steps , slowing mixed licors considerably for large @xmath16 .",
    "almost equally daunting , the original algorithms are quite complex , difficult to implement and debug , inhibiting their adoption .",
    "we review the use of light cones for localized spatio - temporal prediction .",
    "we introduce two simplified nonparametric methods for the predictive state reconstruction task and a simple regression light cone method for fast and accurate forecasting . the first predictive state method , * moonshine * , is a simple meta - algorithm consisting of basic clustering steps combined with dimensionality reduction and nonparametric density estimation .",
    "moonshine is instance - based and requires no iterative likelihood maximization , yet retains many of the qualities of the more complex mixed licors method .",
    "the second predictive state algorithm , * one hundred proof ( ohp ) * , simplifies the moonshine approach further and consists of clustering in the space of future light cones , using the clusters to obtain state - specific nonparametric density estimates over the space of plcs and flcs .",
    "these simple algorithms are much easier to implement than the licors algorithms , being simplified approximations of the mixed licors system , yet retain many of their forecasting and modeling strengths .",
    "we further conduct two sets of empirical experiments showing the predictive power of light cone methods for predicting video - like data , and report results .",
    "lastly , we give a large sample theoretical guarantee for light cone predictive state systems .",
    "the remainder is structured as follows . ",
    "[ sec : methods ] describes the moonshine , one hundred proof and light cone linear regression algorithms . ",
    "[ sec : results ] describes the experimental setup for two real - world spatio - temporal prediction tasks , and gives the results of the algorithms and baselines . ",
    "[ sec : theory ] gives an upper bound on the estimation error of our methods .",
    " [ sec : related - future ] reviews related and future work , while  [ sec : conclusion ] summarizes our findings .",
    "our simple predictive state reconstruction methods build upon the principles introduced in @xcite for mixed licors .",
    "both new methods reconstruct a set of predictive states and a soft mapping @xmath13 from past light cones to states , through use of nonparametric density estimation over the space of light cones .",
    "that is , for all past light cones @xmath19 the methods compute @xmath20^\\top,\\ ] ] where @xmath21 is the normalized weight of state @xmath22 for light cone @xmath19 . unlike mixed licors ,",
    "the new methods avoid having to explicitly construct an @xmath15 matrix , yet retain the benefits of soft membership mixture modeling .    after describing the reconstruction algorithms ,",
    "we discuss how one can determine the conditional probability density of an observation given its past light cone , and how to use this conditional density in forecasts .",
    "we then describe an additional pure regression light cone method , useful for fast and accurate forecasting without state reconstruction .",
    "appendix 2 describes parameter settings and practical implementation issues that arise when using the algorithms .",
    "decompose spatio - temporal process into light cone ( plc , flc ) observation tuples .",
    "cluster plcs using density based clustering .",
    "compute cluster - conditioned density estimates for @xmath23 random points . merge clusters in the space of reduced dimension . map original light cones to final clusters .",
    "moonshine begins by decomposing the random field into its component light cones , shown at far left in figure  [ fig : main ] .",
    "the algorithm then proceeds through two successive stages of clustering , separated by a dimension - reduction step .",
    "the main steps of moonshine are given in algorithm  [ alg : moonshine ] .",
    "the output of the procedure is a set of predictive states , each of which consists of a set of plcs and flcs .",
    "the predictive states are used to create a pair of nonparametric density estimates , one over plcs and one over flcs , which jointly identify each state .",
    "* initial clustering : * for the first clustering step , moonshine uses a density - based clustering approach @xcite to cluster the light cones in the space of plcs , which assumes that similar plcs have similar predictive consequences .",
    "such clustering methods need a specified local - neighborhood size , so we begin with small neighborhoods , progressively increase until 90% of all points are clustered , and assign the remaining points to the nearest cluster center ( effectively hybridizing density - based clustering with @xmath24-means ) .",
    "this allows for good coverage while avoiding formation of a single , all - encompassing cluster .",
    "( alternative clustering algorithms , e.g. , @xcite , would also work . )",
    "* density estimation and dimensionality reduction : * the flcs associated with each cluster ( mapped through their respective light cones ) are used to form kernel density estimates over the space of flcs . in other words ,",
    "each cluster consists of some set of associated flcs and these flcs are then used to estimate densities over the flc space .",
    "we estimate the densities of @xmath25 randomly selected points , where @xmath17 is a parameter that affects the degree of dimensionality reduction .",
    "the log - probability ratio is taken between the first point and the remaining @xmath26 points .",
    "this vector of log probability ratios forms the `` signature '' of the cluster , following the construction of a canonical sufficient statistic for exponential family distributions @xcite .    *",
    "merging clusters : * if the number of clusters is greater than the maximum number of predictive states specified for the model , we cluster again to reduce the number .",
    "we cluster the low - dimensional signature vectors with @xmath24-means++ @xcite , to form the final predictive states .",
    "the original light cones are then assigned to the resulting states , so each predictive state has a unique set of plcs and flcs with which to form nonparametric density estimates over both the plc and flc spaces .",
    "decompose spatio - temporal process into light cone ( plc , flc ) observation pairs .",
    "cluster flcs using @xmath24-means++ clustering .",
    "map original light cone pairs to final clusters .",
    "ohp simplifies moonshine , with a single clustering step and subsequent mapping of light cones to clusters .",
    "the main difference is the space in which the clustering occurs : moonshine clusters in the space of plcs , but ohp clusters in the space of flcs .",
    "clustering in flc space effectively groups past light cones by their predictive consequences , learning a geometry of our space where points with similar futures are `` near '' each other regardless of differences in their histories .",
    "this results in predictive states with expected near - minimal - variance future distributions @xcite , such that once we are sure of which state a new plc maps to , we are highly certain of what outcome the state will generate .    to motivate this choice ,",
    "imagine that all pasts map to some small set of distinct futures , such as to the letters of a discrete finite alphabet .",
    "given input past @xmath19 we want to estimate a probability function over output @xmath14 , so one way to do this is to group all occurrences of future @xmath27 , and use that cluster to estimate the distribution , using bayes theorem , namely , @xmath28 using nonparametric density estimation over the points observed with outcome @xmath29 , we can estimate the first quantity on the right hand side , and taking the normalized number of member outcomes allows one to estimate the second .",
    "this example can easily extend to continuous quantities , by clustering in the space of observed future outcomes and substituting predictive states for the finite alphabet , which is the motivation for the ohp algorithm .",
    "the two steps of ohp are ( algorithm [ alg : ohp ] ) :    1 .",
    "* cluster flcs : * after decomposing our spatio - temporal process into light cones , we cluster the flcs using @xmath24-means++ .",
    "the number of clusters ( which will become the number of predictive states ) is a user - defined parameter .",
    "* map light cones : * we then map the original light cones to our clusters , and produce our final predictive states , which consist of unique sets of plcs and flcs .    as in the case of moonshine ,",
    "the flcs and plcs for each state @xmath22 are used to compute nonparametric density estimates over the space of flcs and plcs , providing estimators for @xmath30 and @xmath31 respectively .",
    "algorithm  [ alg : ohp ] outlines the process of state reconstruction for ohp .      given the states reconstructed by moonshine or ohp , we can estimate predictive distributions as follows .",
    "the conditional probability ( or probability density ) of @xmath14 given plc @xmath19 is obtained by mixing over the predictive states , namely @xmath32 where the second equality follows from the conditional independence of @xmath14 and @xmath33 given the predictive state @xmath22 .",
    "the @xmath34 terms serve as the mixture weights , and bayes s theorem yields @xmath35 all of the quantities in ( [ eq : dist - over - x ] ) and ( [ eq : conditional - of - state ] ) can be estimated using our reconstructed predictive states , which are each associated with unique sets of plcs and flcs .",
    "we estimate @xmath36 by @xmath37 , where @xmath16 is the total number of light cone observations and @xmath38 is the number of light cones assigned to state @xmath22 .",
    "the two state - conditioned densities @xmath30 and @xmath31 are estimated using nonparametric density estimation techniques ( such as kernel density estimation ) based on their associated flcs and plcs .",
    "thus we get @xmath39 where @xmath40 and @xmath41 denote the nonparametric density estimates of the two corresponding conditional densities .",
    "when we need a point prediction of @xmath14 , we use the conditional mean : @xmath42 & = \\mathbb{e}\\left[\\mathbb{e}\\left [ x | \\ell^{- } , s\\right ] \\ell^{- } \\right ] \\\\              & = \\mathbb{e}\\left[\\mathbb{e}\\left [ x | s\\right ] \\ell^{- } \\right ] \\\\              & = \\sum_{j=1}^{k}{p(s_j | \\ell^{- } ) \\mathbb{e}[x|s_j]}.\\end{aligned}\\ ] ] replacing @xmath34 with ( [ eq : conditional - of - state ] ) , plugging in the estimated densities and probabilities , and using the mean future value for state @xmath22 ( denoted @xmath43 ) to estimate @xmath44 $ ] , we obtain the final prediction rule @xmath45 which is simply a suitably weighted mixture of the mean predictions for each state .      if only predictive regression is needed and not a full generative model , one can perform linear regression directly using light cones .",
    "light cone linear regression uses the same light cone decomposition as the licors , moonshine and ohp methods , but learns a regression rule directly from past light cones to future light cone values .",
    "this has the advantages of extremely fast prediction and good forecasting accuracy , along with simple implementation .",
    "we evaluate the performance of light cone linear regression on two real - world forecasting tasks , in  [ sec : results ] .",
    "in order to evaluate the effectiveness of light cone methods , we attempt spatio - temporal forecasting on real - world data .      for the first task , the data come from a set of experiments measuring electrostatic potential changes in organic electronic materials @xcite .",
    "we learn a common set of predictive states across experiments , and do frame - by - frame prediction on a single held - out experiment , effectively cross - validating across experiments .",
    "each experiment consists of 710 time slices , or _",
    "frames_. each frame is a 256-by-256 matrix of scalar measurements , which we call _ pixels _ , since the data resembles video in structure .",
    "predictions are performed for 254-by-254 pixels in each frame after the first , which allows for each pixel to be predicted based on a full light cone , thus excluding marginal light cones .      for the second task",
    ", we predict the next frame of a full - resolution video from a recording of a human speaker , used in generating an intelligent avatar agent . in this task ,",
    "we perform leave - one - frame - out predictions , cross - validating across video frames .",
    "each frame consists of 440-by-330 pixels , of which predictions are performed on the 428-by-328 interior pixels , again excluding marginal light cones .",
    "every fifth frame from the video is retained , and light cones are extracted from roughly one hundred skip frames .",
    "forty - thousand light cones are subsampled for tractability .",
    "these light cones are used for cross - validation .",
    "we compare the performance of predictive state reconstruction and forecasting systems with some simple baseline methods .",
    "for all light cone methods , the same set of light cones were extracted from the data , with @xmath46 , @xmath47 , and @xmath48 , resulting in plcs of dimension @xmath49 and flcs with dimension @xmath50 .",
    "we evaluate the performance of the mixed licors system , implemented by the authors following @xcite . for tractability ,",
    "only twenty thousand light cones were used in training each fold for the first task , and forty thousand for the second .",
    "kernel density estimators were used for both plc density estimation as well as flc density estimation , to improve predictive performance .",
    "initialization was performed using @xmath24-means++ and the iteration delta was set to @xmath51 . for light cone linear regression ,",
    "we use linear regression implemented in the scikit - learn package for python @xcite , version 15.2 .",
    "the simplest method we compare against is the `` predict the value from the last frame '' method that simply takes the previous value of a pixel and uses that as the prediction for the pixel in the current frame .",
    "the @xmath24-nearest neighbor regressor takes as input a past light cone and finds the @xmath24-nearest plcs in euclidean space , then takes the weighted average of their individual future light cone values and outputs that as the current prediction .",
    "below , we report results from the scikit - learn implementation of kneighborsregressor with default parameter settings .      we compared performance in terms of mean - squared - error ( mse ) and correlation ( pearson @xmath52 ) with the ground truth .",
    "additionally , for the three distributional methods ( mixed licors , moonshine and ohp ) we measured the average per pixel log - likelihood ( avg .",
    "ll ) of the predictions , an estimate of the ( negative ) cross - entropy between the model and the truth , and the perplexity ( @xmath53 ) , with lower perplexity being better . for the distributional methods , we tested performance both for a large maximum number of states ( @xmath54 ) and a small number of states ( @xmath55 ) .    to avoid negative infinities appearing when model likelihoods are sufficiently close to zero , we apply smoothing to the three distributional models for all likelihood estimates mapping to zero , converting them to likelihoods of @xmath56 .",
    "light cone systems compare favorably to state - of - the - art deep learning methods , such as @xcite ( seen in figure  [ fig : comparison ] ) , which improves on earlier work by @xcite .",
    "the amount of blurring and structural aberration becomes noticeable in their prediction examples , reproduced here . compare with figure  [ fig : abby - pred ] , where a light cone system ( mixed licors ) is used to predict the next frame of human video .",
    "the light cone predictions maintain strong structural consistency and minimal blurring , at the cost of some quantization effects ( due to predictive state clustering ) .    for the electrostatic potentials prediction task , fig .",
    "[ fig : moonshine - pred ] and [ fig : ohp - pred ] show three frames of predictions each for moonshine and ohp , respectively .",
    "the next frame ( top to bottom ) is predicted using models trained on the remaining six experiments , given plcs from the previous frame .",
    "error percentage was calculated as a proportion of the maximum dynamic range of the actual values or predictions , namely , @xmath57 where @xmath58 is the set of true testing frames , @xmath59 is the set of predicted frames , @xmath60 is the true value at a pixel , @xmath61 is the predicted value of a pixel and @xmath62 is the @xmath63 norm .",
    "qualitatively , both methods do well , capturing much of the changing dynamics in each frame .",
    "the methods have trouble representing the extreme values at the two `` hotspots '' ( visible in the error plots in the third columns ) , giving instead over - smoothed predictions .",
    "other than those extreme regions , the error residuals lack obvious structure and are relatively small .",
    "[ tab : real - world - regression ]    table  [ tab : real - world - regression ] shows how well each method did at predicting electrostatic potentials ( task 1 ) .",
    "mixed licors and moonshine have the lowest mse , with 95% confidence intervals disjoint from the intervals of other methods .",
    "mixed licors also has the highest ( pearson ) correlation with the true values .",
    "lastly , of the generative methods ( i.e. , mixed licors , moonshine and one hundred proof ) , moonshine and ohp have the highest average log - likelihood and lowest perplexity",
    ". thus , mixed licors and moonshine provide the best overall performance on the dataset .    restricting ourselves to the generative methods for a compact number of states ( @xmath55 ) , mixed licors",
    "has the lowest average mse , while moonshine and one hundred proof have the best probabilistic performance , giving the highest likelihoods and lowest perplexities for the data .",
    "[ tab : abby ]    table  [ tab : abby ] gives the results from video prediction ( task 2 ) .",
    "light cone linear regression has the strongest overall performance , with low error and high correlation to the ground truth .",
    "however , the strong temporal consistency of this dataset allows even the fltp method to perform remarkably well , outperforming the predictive state light cone methods . while forecasting is relatively easy for this task , being able to estimate a likelihood model for such data gives the predictive state methods an edge over pure regression methods .",
    "in this manuscript , we have tested an existing light cone method ( mixed licors ) , qualitatively comparing it to deep learning methods , and introduced three new light cone methods ( light cone linear regression , moonshine , ohp ) .",
    "the two latter predictive state methods are successive approximations of the approach used by mixed licors , with ohp pushing the limit of how simplified we could make the approximation .",
    "ohp is demonstrated to be one approximation too far , since the increased simplification comes at the cost of degraded performance .    on the first real - world spatio - temporal regression task , we find that the three licors - inspired methods ( mixed licors , moonshine and one hundred proof ) are able to accurately forecast the changing dynamics of the underlying spatio - temporal system .",
    "furthermore , being generative methods , they can be used to compute the likelihood of spatio - temporal data .",
    "moonshine and one hundred proof ( ohp ) are conceptually simple , easy to implement alternatives to the full mixed licors system , which give comparable performance for likelihood estimation and forecasting on this task . although ohp is the simplest method , it fails to perform well in some contexts , such as the second video prediction task , showing a trade - off between method simplicity and forecasting performance .",
    "light cone linear regression is a fast and simple method , and is able to perform well on both prediction tasks . it does not estimate likelihoods over data as do the other predictive - state methods , but moving to generalized linear models",
    "would allow this .",
    "it shows the effectiveness of light cone decompositions and remains a useful approach .",
    "overall , the best performance on all tasks was achieved or shared by the three new methods , with moonshine having the best probabilistic modeling performance on both tasks , light cone linear regression having the best forecasting performance on the second task , and ohp having good modeling performance under the constrained setting of limited number of states .",
    "moonshine has better probabilistic modeling performance than mixed licors on these tasks , and has statistically indistinguishable forecasting capability ( see tables  [ tab : real - world - regression ] ( 100 state case ) and [ tab : abby ] ) . while it might be argued that the improved performance was not improved _ enough _ , we have to remind ourselves that these are approximations  that they improve performance at all is surprising .    although ohp does have limited forecasting ability , it does manage to model at least one of the datasets well , showing that its simplified form is not entirely without merit .",
    "this , at very least , shows when approximations become too simplified to accomplish complex tasks .",
    "negative results are important , especially when detecting boundaries .",
    "we state a result for light cone predictive state systems , with proof given in appendix i.    we wish to bound the error of our estimated distribution over futures given pasts , namely , the error of @xmath64 . for a fixed random sample of data ,",
    "let @xmath65 denote the optimal estimate for @xmath66 constructable from the sample .",
    "we begin by noting @xmath67    the second summand on the right - hand side is the gap between the optimal estimate and truth , which we assume to shrink in probability with the sample size ( as in @xcite ) .",
    "we focus on first term , which is the gap between our light - cone based nonparametric estimator and the optimal estimate . for this quantity ,",
    "we state our main result :    for a fixed data sample of size @xmath16 , let @xmath65 denote the optimal estimator based on that sample and @xmath64 be the light cone estimator based on the same sample .",
    "let @xmath68 be bounded by a constant @xmath69 for all @xmath70 . if @xmath71 for all @xmath72 , then for any @xmath14 , @xmath73 , @xmath74 , and sufficiently large @xmath16 , @xmath75 where @xmath76 is the ( smallest ) sum of weights for the predictive states and @xmath77 is a bandwidth @xmath78 kernel .    _ proof sketch _",
    "( see appendix for details ) : for the quantity @xmath79 , we first mix over states , and use the chain rule to condition .",
    "then we add and subtract @xmath80 , and split the sum into two parts , one multiplied by @xmath81 and the other multiplied by @xmath68 . by the assumptions stated , the second sum is bounded and decreasing to zero , so that for sufficiently large @xmath16 it is smaller than any @xmath82 .",
    "the first sum is less than @xmath83 , which we bound with high probability , using a hoeffding bound for dependant data @xcite .",
    "the result follows directly from application of the hoeffding bound .",
    "our debt to @xcite needs no elaboration .",
    "we share the same general framework , but aim at simpler algorithms , even if it costs some predictive power .",
    "the work on licors grows out of earlier work on predictive markovian representations of non - markovian time series @xcite , whose transfer to spatio - temporal data was originally aimed at unsupervised pattern analysis in natural systems @xcite ; our qualitative results suggest moonshine and ohp remain suitable for this , as well as for prediction .",
    "the formalism used in this line of work is mathematically equivalent to the `` predictive representations of state '' introduced by @xcite , and lately the focus of much interest in conjunction with spectral estimation methods @xcite .",
    "both formalisms are also equivalent to observable operator models @xcite and to `` sufficient posterior '' representations @xcite ; our approach may suggest new estimation algorithms within those formalisms .",
    "light cone methods , such as the three described here , hold promise for the prediction of dynamical systems . given the flexibility and generality of light cone decompositions , one can easily extend such methods to handle full - color video ( e.g. , figure  [ fig : iva - pred ] ) , and kinect - sensor depth video .",
    "these applications are the focus of current and future research .",
    "the `` rate limiting step '' for approximate light cone methods like moonshine and ohp is the speed of nonparametric density estimation .",
    "methods that scale poorly in the number of observations are of limited use . towards that end , future research into fast approximate nonparametric density estimation",
    "will improve the computational efficiency of the methods presented .",
    "the theoretical properties of the two predictive state methods will be further explored in a future paper , especially with regard to the trade - offs in their approximation to what licors or mixed licors would do , and the influence of the new algorithms internal randomness .",
    "faced with the task of learning to accurately model video - like data , we explore the strengths and drawbacks of light cone decomposition methods and propose new simplified nonparametric predictive state methods inspired by the mixed licors @xcite algorithm .",
    "the methods , moonshine and one hundred proof , do not require costly iterative em training or the memory intensive formation of an explicit @xmath15 matrix , yet retain the generative modeling capabilities and are competitive in predictive performance to the original mixed licors method .",
    "the methods are shown to perform well on one real - world data task , effectively capturing spatio - temporal structure and outperforming baseline methods , while a light cone version of linear regression performs well on the remaining task .",
    "overall , we see that light cone decompositions of complex spatio - temporal data can open opportunities to tractably estimate probability densities and accurately forecast the changing systems . by introducing simplified versions of light cone algorithms , we hope to encourage further exploration and application of this general technique .",
    "[ lem:1 ] let @xmath84 denote the density for state @xmath72 under the true assignment matrix @xmath85 and let @xmath86 .",
    "given an isolated change in @xmath13 in the weight @xmath87 , the difference between density estimate @xmath88 and @xmath84 is bound by @xmath89    @xmath90 - \\sum_{k=1}^{n } \\frac{w_{kj}}{\\sum_{l=1}^{n}w_{lj}}k_h(\\|x_k - x\\|)\\right|\\\\          & = \\left|\\left[\\sum_{k\\not = i}^{n } \\frac{w_{kj}}{\\epsilon + { n}^*_j}k_h(\\|x_k - x\\| ) + \\frac{\\epsilon + w_{ij}}{\\epsilon + { n}^*_j}k_h(\\|x_i - x\\|)\\right ] - \\sum_{k=1}^{n } \\frac{w_{kj}}{{n}^*_j}k_h(\\|x_k - x\\|)\\right|\\\\          & = \\left|\\left[\\sum_{k\\not = i}^{n } \\frac{{n}^*_jw_{kj}}{{n}^*_j(\\epsilon + { n}^*_j)}k_h(\\|x_k - x\\| ) + \\frac{{n}^*_j(\\epsilon + w_{ij})}{{n}^*_j(\\epsilon + { n}^*_j)}k_h(\\|x_i - x\\|)\\right ] - \\sum_{k=1}^{n } \\frac{(\\epsilon + { n}^*_j)w_{kj}}{{n}^*_j(\\epsilon + { n}^*_j)}k_h(\\|x_k - x\\| ) \\right|\\\\          & = \\left|\\frac{\\epsilon}{\\left({n}_j^{*}\\right)^2 + \\epsilon{n}^*_j}\\left[{n}^*_jk_h(\\|x_i - x\\| ) - \\sum_{k=1}^{n}w_{kj}k_h(\\|x_k - x\\|)\\right]\\right|\\\\          & = \\left|\\frac{\\epsilon}{\\left({n}_j^{*}\\right)^2 + \\epsilon{n}^*_j}\\left[\\sum_{k=1}^{n}w_{kj}k_h(\\|x_i - x\\| ) - \\sum_{k=1}^{n}w_{kj}k_h(\\|x_k - x\\|)\\right]\\right| \\\\          & = \\left|\\frac{\\epsilon}{\\left({n}_j^{*}\\right)^2 + \\epsilon{n}^*_j}\\sum_{k=1}^{n}w_{kj}\\left[k_h(\\|x_i - x\\| ) - k_h(\\|x_k - x\\|)\\right]\\right|.\\end{aligned}\\ ] ]    furthermore , we can bound this quantity by @xmath91\\right| & = \\left|\\frac{\\epsilon}{{n}^*_j + \\epsilon}\\sum_{k=1}^{n}\\frac{w_{kj}}{{n}^*_j}\\left[k_h(\\|x_i - x\\| ) - k_h(\\|x_k - x\\|)\\right]\\right| \\\\          & \\leq \\left|\\frac{\\epsilon}{{n}^*_j + \\epsilon}\\max_{k}\\left[k_h(\\|x_i - x\\| ) - k_h(\\|x_k - x\\|)\\right]\\right|\\\\          & \\leq \\left|\\frac{\\epsilon}{{n}^*_j + \\epsilon}k_h(0)\\right|.\\end{aligned}\\ ] ]    [ lem:2 ] let @xmath92 be defined as in lemma  [ lem:1 ] .",
    "given a fixed data sample of size @xmath16 , for all @xmath29 , @xmath93 and @xmath94 we have @xmath95    once the sample is fixed , @xmath96 becomes a deterministic function of the sample , and @xmath97 becomes a deterministic constant .",
    "following @xcite , we define @xmath98 , \\\\      l_i & = 0,\\\\      u_i & = \\frac{1}{1+n^{*}_{j}}k_h(0),\\end{aligned}\\ ] ] where @xmath99 denotes that the two functions only differ at the @xmath100th matrix entry , @xmath101 and @xmath102 are constant ( degenerate ) random variables for a fixed sample and @xmath103 then , for all @xmath29 , @xmath93 and @xmath94 , we have @xmath104 \\geq a , c_n^2 \\leq c^2 \\text { for some $ n$}\\right ) + \\notag{}\\\\          \\phantom { { } } & \\phantom { { } \\leq { } } { \\mathbb{p}}\\left(\\sum_{i=1}^n [ -\\hat{f}_{ij}(x ) + f_{ij}^*(x ) ] \\geq a , c_n^2 \\leq c^2 \\text { for some $ n$}\\right ) \\\\          & = { \\mathbb{p}}(s_n \\geq a , c_n^2 \\leq c^2 \\text { for some $ n$ } ) + \\notag{}\\\\          \\phantom { { } } & \\phantom { { } \\leq { } } { \\mathbb{p}}(-s_n \\geq a , c_n^2 \\leq c^2 \\text { for some $ n$ } ) \\\\          & \\leq 2 \\exp\\left\\{-\\frac{2a^2}{c^2}\\right\\}.\\end{aligned}\\ ] ]    given a fixed sample of size @xmath16 , choose @xmath105 such that @xmath106 for all @xmath107 .",
    "then @xmath108 because @xmath106 for all @xmath107 , we have @xmath109 having already establish that @xmath110 , we set @xmath111 and obtain @xmath112    for a fixed data sample of size @xmath16 , let @xmath65 denote the optimal estimator based on that sample and @xmath64 be the light cone estimator based on the same sample .",
    "let @xmath68 be bounded by a constant @xmath69 for all @xmath70 . if @xmath113 for all @xmath72 , then for any @xmath14 , @xmath73 , @xmath74 , and sufficiently large @xmath16 , @xmath75 where @xmath76 is the ( smallest ) sum of weights for the predictive states , and @xmath77 is a kernel of bandwidth @xmath78 .",
    "@xmath114\\right| \\\\          & \\leq \\sum_{j=1}^{k } \\left|\\left[\\widehat{p}(x|s_j)\\widehat{p}(s_j|\\ell^- ) - p^*(x|s_j)p^*(s_j|\\ell^- ) \\right]\\right| \\\\                  & = \\sum_{j=1}^{k } \\left|\\left[\\left(\\widehat{p}(x|s_j ) - p^*(x|s_j)\\right)p^*(s_j|\\ell^- ) + \\left(\\widehat{p}(s_j|\\ell^- ) - p^*(s_j|\\ell^-)\\right)\\widehat{p}(x|s_j ) \\right]\\right| \\\\          & \\leq \\sum_{j=1}^{k } \\left|\\widehat{p}(x|s_j ) - p^*(x|s_j)\\right|p^*(s_j|\\ell^- ) + \\sum_{k}^{j=1}\\left|\\widehat{p}(s_j|\\ell^- ) - p^*(s_j|\\ell^-)\\right|\\widehat{p}(x|s_j ) \\\\          & \\leq \\max_{j}\\left\\{\\left|\\widehat{p}(x|s_j ) - p^*(x|s_j)\\right|\\right\\ } + \\max_{j}\\left\\{\\left|\\widehat{p}(s_j|\\ell^- ) - p^*(s_j|\\ell^-)\\right|\\widehat{p}(x|s_j)\\right\\ } \\\\          & : = a + b. \\\\\\end{aligned}\\ ] ]    therefore , @xmath115    for sufficiently large @xmath16 , @xmath116 and @xmath117 , given that @xmath68 is bounded and @xmath118 .",
    "therefore , given @xmath16 sufficiently large , @xmath119 where the penultimate inequality follows from lemma  [ lem:2 ] .",
    "we now discuss the choosing of various parameter settings for the two algorithms , as well as some computational techniques used to improve runtime performance .      in both mixed licors and moonshine",
    "a user must specify the maximum number of predictive states for the model , which effectively controls the complexity of the model . in ohp",
    ", one must specify the exact number of predictive states , since the number is determined by a @xmath24-means++  @xcite clustering step . in all cases ,",
    "this number can be chosen based on user preference for simpler models , or cross - validation may be used to find the number of states that gives the best predictive performance on held - out data .",
    "another parameter that must be chosen is the degree of dimensionality reduction when forming distribution signatures in moonshine .",
    "data can guide this choice ( through cross - validation ) , or user preference for more compact models can guide the choice for greater degrees of dimensionality reduction . the fewer the number of dimensions , the less discriminative the signatures , and thus , the higher the likelihood of merging clusters .      when using density based clustering such as dbscan  @xcite , two issues arise .",
    "first , a suitable local neighborhood size must be chosen ( controlled by an @xmath13 parameter ) .",
    "second , such methods can be computationally expensive and thus slow . to address the first issue",
    ", we take an iterative search approach by beginning with very small neighborhood sizes , then increase them until a significant portion of the data is clustered , but keep the proportion below 100% . to address the second issue",
    ", we use dbscan to cluster only a seed portion of all observations , then assign remaining observations to nearest cluster centers , which greatly improves runtime . controlling the proportion of data used for seeding versus the portion assigned to cluster centers affects the degree of forced convexity of resulting clusters , and also determines the total runtime of the clustering .",
    "fewer seed points results in faster clustering , but with more convex - shaped ( e.g. , @xmath24-means - like ) clusters .",
    "since moonshine and ohp cluster based on distances , it becomes important to normalize the scaling of all axes and dynamic ranges of all experiments .",
    "additionally , if the scale of training light cones differs from the scale of test light cones predictive performance will suffer .",
    "nonparametric density estimation techniques are instance based and slow with increasing numbers of observations .",
    "our algorithms use kernel density estimators  @xcite , for which we only retain a randomly chosen subsample of five hundred points in each cluster to compute the densities .",
    "the resulting systems still perform well , as shown in  [ sec : results ] , while being computationally tractable ."
  ],
  "abstract_text": [
    "<S> spatio - temporal data is intrinsically high dimensional , so unsupervised modeling is only feasible if we can exploit structure in the process . when the dynamics are local in both space and time , this structure can be exploited by splitting the global field into many lower - dimensional `` light cones '' . </S>",
    "<S> we review light cone decompositions for predictive state reconstruction , introducing three simple light cone algorithms . </S>",
    "<S> these methods allow for tractable inference of spatio - temporal data , such as full - frame video . </S>",
    "<S> the algorithms make few assumptions on the underlying process yet have good predictive performance and can provide distributions over spatio - temporal data , enabling sophisticated probabilistic inference . </S>"
  ]
}