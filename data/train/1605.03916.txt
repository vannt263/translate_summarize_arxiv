{
  "article_text": [
    "in their seminal work @xcite , berg and purcell ( bp ) have explored the fundamental limits to the accuracy with which cellular organisms can perform concentration measurements with receptors via time averaging .",
    "later , it has then been discovered that a receptor performing a maximum likelihood ( ml ) measurement can reduce the uncertainty of a measurement even further by a factor of two @xcite . in an information theoretic language",
    "the ml estimate `` uses '' 0.5bit more information from the history of the receptor occupancy than the bp estimate .",
    "further studies have examined ml estimates for concentration ramps @xcite , for spatial gradients @xcite , and for competing signals @xcite ( see @xcite for recent reviews ) .",
    "most prominently , it has been elucidated that this additional information can not be reached within the class of linear models @xcite , which illustrates the inherent loss of information for this specific class of inference strategies .",
    "thermodynamic constraints for the information acquisition of biological networks have attracted much interest recently @xcite .",
    "specifically , chemical networks may only reach the highest sensory performance at diverging thermodynamic costs @xcite . using the optimal strategy to be able to come close to such bounds",
    "is , therefore , of fundamental importance .    in this paper",
    ", we will show how the maximum sensory performance depends on the strategy used for inferring the signal .",
    "thereby , we compare different classes of inference strategy , for example time averaging and counting transitions @xcite as well as their joint correlations . for binary sensors responding to a signal ,",
    "we illustrate how the thermodynamic local detailed balance relation determines the sensory limits for time averaging mechanisms but does not generally determine the maximal information attainable by counting transitions",
    ". in particular , we will show under which conditions time averaging mechanisms loose information ( e.g. , up to 1/2 bit per measurement ) . the same loss of information can also be identified in the linear noise approximation , which is a powerful tool to approximate large chemical networks with an effective brownian motion @xcite .",
    "we will show under which circumstances these approximation schemes are almost accurate on the trajectory level and when they loose information up to about @xmath0bit .",
    "the paper is organized as follows . in sec .",
    "[ sec : model ] , we define the model system of a single binary sensor measuring a stochastic signal . in sec .",
    "[ sec : main_results ] , we derive a thermodynamic expression eq . for the information loss of inference strategies that are solely based on time averaging . in sec . [",
    "sec : lna ] , we discuss the implication for approximation schemes that involve a continuous brownian motion ( linear noise approximation for weak signals ) .",
    "we conclude in sec .",
    "[ sec : conclusion ] .",
    "appendix [ sec : information_vs_error ] shows basic relations between information and error . in appendix",
    "[ sec : gen_func ] we introduce the generating function that can be used to calculate all quantities of interest .",
    "appendix [ sec : robust ] explains the robustness of our main result , eq . , against non - linearities arising from strong signals . a comparison to refs .",
    "@xcite can be found in appendix [ sec : method ] . in appendix",
    "[ sec : filter ] , we calculate the mutual information for coarse - grained processes within the linear noise approximation .     of the free energy difference between the the states due to the signal is shown on the right . ]",
    "we assume a sensor @xmath1 which , at time @xmath2 , can be in two different states , the `` empty state '' @xmath3 or the `` occupied state '' @xmath4 .",
    "these states are initially separated by a free energy difference @xmath5 .",
    "the signal then changes the free energy difference to @xmath6 such that a positive signal ( @xmath7 ) favors the occupied state @xmath8 .",
    "more precisely , as illustrated in fig .",
    "[ fig : level ] , the transition rates @xmath9 from @xmath10 to @xmath11 satisfy the local detailed balance relation @xcite @xmath12 where we set @xmath13 throughout . in equilibrium , the probability of being in state @xmath8 is then given by @xmath14 $ ] .",
    "for an illustration , let us consider the paradigmatic case of a single receptor measuring a concentration @xmath15 , where the binding of a ligand to the receptor occurs with a rate @xmath16 and ligands are released from the receptor with a rate @xmath17 .",
    "one can then identify @xmath18 and @xmath19 , where @xmath20 is some reference concentration , i.e. , @xmath21 corresponds to a change of the external chemical potential .",
    "the mean occupation level then becomes @xmath22 .",
    "we will use this example of a receptor measuring a concentration later , since it allows us to compare our results with the findings in , e.g. , refs .",
    "@xcite . as a non - biological example",
    ", one can consider as sensor a single electron transistor @xcite , trying to infer a signal @xmath21 , which is a change of gate voltage @xcite .",
    "the local detailed balance relation provides one constraint for two rates .",
    "hence , it does not generally determine both rates @xmath23 and @xmath24 individually . the asymmetry parameter @xmath25 , a central quantity of this paper , defined through @xmath26 accounts for this freedom of choice . for `` normal '' values @xmath27 , the signal @xmath21 influences the rates in such a way that one rate increases while the respective reverse rate decreases . for @xmath28",
    "the signal @xmath21 has a symmetric influence on both rates , whereas for the above example with the receptor we have @xmath29 , i.e. , only one rate is affected by the signal , since , @xmath30 and @xmath31 . moreover , for fermionic rates as relevant to the single electron box @xmath32 and @xmath33 $ ] ( see , e.g. , ref .",
    "@xcite ) , the asymmetry parameter becomes @xmath34 .",
    "similarly , in the experiment reported in @xcite , @xmath35 corresponds to a single electron box for which the mean number of electrons is about 1/2 , as can be deduced from the supporting information ( fig .",
    "s1 ) of ref . @xcite .",
    "we first define the measurement problem as illustrated in fig . [ fig : level ] .",
    "we assume the sensor is initially equilibrated with free energy difference @xmath5 at time @xmath36 . at @xmath37",
    "the signal @xmath21 changes the free energy difference to @xmath6 , where we assume that @xmath21 is normally distributed with zero mean and variance @xmath38 , i.e. , @xmath39 $ ] . for weak signals the condition @xmath40 holds .",
    "the goal is to determine the mutual information that the time series @xmath41 of the sensor contains about the value @xmath21 .",
    "since there is a direct relation between estimation error and information @xcite , see also appendix [ sec : information_vs_error ] , the former can be inferred from the latter .",
    "moreover , there is a direct link between mutual information and thermodynamics @xcite .",
    "the mutual information is given by @xcite @xmath42 \\equiv{\\big\\langle\\ln\\frac{p(\\{y_{t'}\\}_{0\\le t'\\le t}|x)}{p(\\{y_{t'}\\}_{0\\le t'\\le t})}\\big\\rangle } ,   \\label{eq : def_i}\\ ] ] where @xmath43 is the conditional probability of the sensor trajectory given the signal value and @xmath44 is its marginal distribution .",
    "the brackets @xmath45 denote the average over all possible realizations @xmath46 and @xmath21 , weighted by the corresponding joint distribution @xmath47 .",
    "we will later use @xmath48 for the conditional average over @xmath46 weighted by @xmath43 .",
    "the stochastic time evolution of a discrete markov process in continuous time consists of a sequence of exponential decays .",
    "hence , the path probability reads ( see , e.g. , @xcite ) @xmath49   \\label{eq : def_pathprob}\\end{gathered}\\ ] ] where @xmath50 is the number of transitions from @xmath1 to @xmath51 and @xmath52 is the time spent in state @xmath1 along the trajectory @xmath46 .",
    "since @xmath53 and , since each jump @xmath54 must be followed by the reverse transition , the path weight is fully determined through @xmath55 and @xmath56 , i.e. , @xmath57 $ ] .",
    "we now discuss the important role of the asymmetry parameter @xmath25 from for this information acquisition problem in the long time limit . for @xmath58 , the total number of jumps satisfies @xmath59 .",
    "most prominently , for @xmath28 the first terms of the path probability do not depend on the signal , i.e. , @xmath60^{\\hat{n}_\\mathrm{tot}/2}$ ] . in this specific case ,",
    "the total number of jumps @xmath61 does not contain additional information about @xmath21 .",
    "consequently , the coarse grained statistics @xmath62 that contains the information @xmath63={\\big\\langle\\ln\\frac{p(y_0,\\hat{\\tau}_1|x)}{p(y_0,\\hat{\\tau}_1)}\\big\\rangle } \\label{eq : itilde}\\ ] ] about the signal , leads to @xmath64 for @xmath28 , whereas , in general , it satisfies the inequality @xmath65 .",
    "( a),(c ) or the symmetrically @xmath28 ( b ) , ( d ) influenced by the signal @xmath21 .",
    "the upper panel ( a),(b ) shows the single sensor case ( @xmath66 ) .",
    "the lower panel ( c),(d ) displays the results for a sensor array with @xmath67 binary sensors , where @xmath68 and @xmath69 .",
    "@xmath70 is obtained numerically by simulating @xmath71 individual trajectories to estimate [ or ] .",
    "for the coarse grained mutual information @xmath72 we use the approximation from eq . .",
    "parameters : rates @xmath73 , @xmath74 , @xmath29 ( left panel ) , @xmath28 ( right panel ) , @xmath66 ( upper panel ) , @xmath67 ( lower panel ) , signal standard deviation @xmath75 ( @xmath76 ) , @xmath77 , and @xmath78 . ]    in fig .",
    "[ fig:4plots](a),(b ) , we compare the full information @xmath70 from the trajectory ( blue open circles ) with the information @xmath72 that excludes the knowledge from the number of jump events ( red solid line ) . for symmetric weight ( @xmath28 ) ,",
    "we find @xmath79 , i.e. , the number of jump events @xmath61 does not provide any additional information about the signal .",
    "remarkably , for the asymmetric weight ( @xmath80 ) , as it applies to the receptor model discussed above ( @xmath29 ) , the number of jump events @xmath61 contributes to an additional amount of information up to @xmath81 .",
    "the same result is obtained for an array of @xmath67 receptors [ see fig . [ fig:4plots](c),(d ) ] .",
    "since the measurement error ( variance ) is proportional to @xmath82 ( see , e.g. , @xcite or appendix [ sec : information_vs_error ] ) this additional information corresponds to an error reduction by a factor of 2 .",
    "this enhanced measurement accuracy for ml estimates was first found in @xcite for the above discussed specific receptor model ( @xmath29 ) .",
    "we conclude that for binary sensors measuring a signal an improved accuracy occurs whenever the signal has a non - symmetric impact on the transition rates , i.e. , for @xmath83 .",
    "note that each subfigure in fig .",
    "[ fig:4plots ] shows results for distinct models or sensory systems .    using the method of generating function ,",
    "see appendix [ sec : gen_func ] , we obtain more generally @xmath84we present a simple derivation of this formula for weak signals @xmath85 in appendix [ sec : gen_func ] . in appendix [ sec : robust ] we show that holds even for strong and/or non - gaussian signals .",
    "this relation constitutes our first main result , namely , the asymmetry parameter @xmath25 significantly determines the additional information content of the number of jump events . for @xmath27",
    ", the number of jump events contributes to an additional information up to 1/2bit , which can qualitatively be understood as follows . for a symmetric effect of the signal @xmath21 on the rates ( @xmath28 ) , one rate increases and its reverse rate decreases such that the overall number of transitions @xmath61 remains the same without establishing correlations with @xmath21 , whereas in the extreme case , e.g. , @xmath29 ,",
    "only the transition @xmath23 is affected by the signal resulting in a monotonic correlation between @xmath61 and @xmath21 .    from a more technical point of view",
    ", we note that the method of generating functions necessary to derive our main result , eq .",
    ", can also be used to describe in finite time the joint dispersion of different classes of random variables , as for example , jump variables @xmath50 and persistence time @xmath52 @xcite .",
    "we explain in appendix [ sec : method ] how one can adopt the methods from @xcite to such a joint dispersion in the long time limit .",
    "the expression for the information gain @xmath86 holds even for an arbitrary number of sensors , which we show by generalizing the path weight .",
    "we denote for the @xmath87-th sensor the time spent in state @xmath1 by @xmath88 and the number of transitions from @xmath1 to @xmath51 by @xmath89 , which results in the conditional path weight @xmath90   \\label{eq : def_n_pathprob}\\end{gathered}\\ ] ] where @xmath91 , @xmath92 , @xmath93 , @xmath94 and @xmath95 .",
    "the marginal path weight is given by @xmath96 .",
    "the full information between the time series of an array of @xmath97 sensors and the random signal @xmath21 then formally reads @xmath98 which can also be written in the form @xmath99 $ ] using .",
    "note that @xmath100 holds in general . the mutual information between signal and the coarse - grained statistics @xmath101 then formally generalizes to @xmath102 .",
    "\\label{eq : def_itilde_n}\\ ] ] remarkably , in the long time limit @xmath58 , we find the same result as for the single sensor case @xmath103 which holds for arbitrarily distributed signals as shown in appendices [ sec : gen_func ] and [ sec : robust ] .",
    "this result agrees with fig .",
    "[ fig:4plots ] , where we observe the same loss of information between @xmath29 ( asymmetric weight ) and @xmath28 ( symmetric weight ) for a single sensor as well as for an array of sensors with @xmath67 .",
    "assuming a weak signal as it applies to the results shown in fig .",
    "[ fig:4plots ] , we can obtain in a simpler way by using the following approximations .",
    "as shown in appendix [ sec : gen_func ] for weak signals ( @xmath104 ) the approximations @xmath105 ,   \\label{eq : i_n}\\ ] ] and @xmath106 ,   \\label{eq : itilde_n}\\ ] ] hold , where @xmath107 .",
    "the approximations are fairly accurate for @xmath108 as can be deduced from fig .",
    "[ fig:4plots](b),(d ) ; see also fig .",
    "[ fig:0.3vs2.0].(a ) in appendix [ sec : robust ] . comparing the approximations and immediately leads to .",
    "it should be that the additional information @xmath109 can be also be reached with a longer observation time @xmath110 , since @xmath111 for @xmath112 .",
    "complex chemical networks , for example the signaling networks in individual cells , typically contain of the order of @xmath113 signaling molecules or receptors .",
    "for such systems , it is often reasonable not to try to resolve each reaction of any individual component , but rather to use an approximation with deterministic rate equations or with a stochastic brownian motion , see e.g. , @xcite . for the present inference problem , the approximation of the system with a stochastic brownian motion @xcite is especially interesting , since it considers fluctuations arising from the discreteness of the system .",
    "we will see that this approximation scheme , as illustrated in fig .",
    "[ fig : sampletraj ] , looses information about the signal @xmath21 compared to the original system that remarkably is precisely given by as well .",
    "second , a linear noise approximation @xmath114 . ]    before applying the approximation one has to consider the total sensor activity @xmath115 of the sensor array .",
    "since the sensors are operating independently , one can verify that @xmath116 $ ] , i.e. , @xmath117 contains the same amount of information about the signal @xmath21 as @xmath118 . the linear noise approximation @xmath119 is now used to approximate @xmath120 with a langevin equation .",
    "the approximation , sketched in fig .",
    "[ fig : sampletraj ] , is typically quite accurate for @xmath121 and weakly changing signals @xmath21 ( @xmath85 ) .",
    "one can identify three core properties that determine the dynamics of @xmath119 .",
    "first , the correct mean behavior requires @xmath122 $ ] .",
    "a series expansion of the right hand side up to first order yields @xmath123 where we have used @xmath124 $ ] and .",
    "second , @xmath125 decays exponentially to @xmath126 with rate @xmath107 that also characterizes the relaxation speed of the system .",
    "third , the steady state variance satisfies @xmath127 . adopting these three properties",
    "leads to the effective langevin equation @xcite @xmath128+\\xi_t , \\label{eq : langevin1}\\ ] ] where @xmath129 is standard white noise with @xmath130 and @xmath131 with noise strength @xmath132 .",
    "the process is the well known linear ornstein - uhlenbeck process .",
    "for such a linear process optimal filtering strategies exist , known as kalman - bucy filters , that infer a normally distributed stochastic value @xmath133 from the observation @xmath134 with least variance @xmath135 $ ] ; see e.g. @xcite or appendix [ sec : filter ] .",
    "hence , due to the gaussian nature of the process this linearly approximated process contains information about the signal , which is given by @xmath136 .",
    "\\label{eq : ilin}\\end{aligned}\\ ] ] using and , the information loss of this approximation @xmath119 in relation to the original @xmath137 process is given by @xmath138 for @xmath58 .",
    "in this limit the information of the linearized process satisfies @xmath139 . thus the continuous linear noise approximation compared to original system dynamics looses the information about the discontinuous binding events , which can be explained as follows .",
    "the linear noise approximation correctly describes the dynamical behavior of the mean @xmath140 and the second order covariance @xmath141 , i.e. , it captures the persistence time @xmath142 up to its second moment .",
    "since the long time limit renders @xmath56 gaussian , the linear noise approximation contains the information from @xmath56 .",
    "hence , @xmath143 , defined in , is precisely the amount of information that is lost by the continuous linear noise approximation .",
    "this insight constitutes our second main result : the asymmetry parameter @xmath25 determines whether the linear noise approximation does loose information on the trajectory level or not .",
    "specifically , for @xmath28 , as it applies to the model from fig .",
    "[ fig:4plots](d ) , there is no information loss , which makes the linear noise approximation accurate on the trajectory level .",
    "for all `` natural '' choices @xmath27 , the lost information is bounded by 1/2bit per measurement , irrespective of the total number of sensors @xmath97 .",
    "we have shown that the asymmetry parameter , defined in , plays a crucial role for the information acquisition of binary sensory networks , since it determines whether the number of jump events contain additional information about a signal .",
    "specifically , for a symmetric weight @xmath144 , the number of jump events do not provide additional information about an external signal @xmath21 , whereas under `` natural conditions '' ( @xmath27 ) the number of jump events contain up to 1/2bit .",
    "most remarkably , this additional information in the number of jump events is independent of the number of copies @xmath97 of binary sensors sensors measuring the same signal @xmath21 .",
    "furthermore , we have shown that the linear noise approximation , a continuous brownian motion useful for systems with large copy number ( @xmath145 ) , describes the probability distribution and linear correlations of the original system up to second order in persistence time",
    ". however , discontinuous ( and nonlinear ) jump events are not covered by this continuous linear noise approximation . consequently , it looses information compared to the original system that is precisely the additional information from the number of jump events .",
    "hence , the asymmetry parameter @xmath25 also determines whether the linear noise approximation looses information about the original system .",
    "analogously , for @xmath27 information loss due to the linear noise approximation is less or equal 1/2bit , whereas for @xmath28 no information is lost , making the linear noise approximation accurate on the trajectory level .    for future work , it will be interesting to study implications to competing signals @xcite , where a sensor computes not just a single signal value .",
    "furthermore , non - equilibrium affinities in multi - state systems may result in non - vanishing currents , such that associated jumps may provide additional information even in the case of a symmetric influence of the signal .",
    "specifically , it has been found that investing non - equilibrium free energy may reduce the dispersion of jump related variables @xcite , see also @xcite .",
    "moreover , it will be interesting to study the implications for sensory devices measuring continuously changing signal ramps @xcite , which requires more than a single measurement discussed here .",
    "finally , an experimental realization of , e.g. , a single electron transistor @xcite , may be a promising non - biological device to study the information gain @xmath86 contained in the number of jump events .",
    "we thank a.c .",
    "barato for fruitful interactions and s. goldt for useful discussions .",
    "in this appendix , we briefly review important aspects of differential entropy and mutual information from @xcite .",
    "assume we have a stochastic signal @xmath21 with probability density @xmath146 .",
    "moreover , @xmath147 is some measurement that is correlated with @xmath21 resulting in a conditional distribution @xmath148 .",
    "specifically , one can identify @xmath147 with the sensor state @xmath1 at time @xmath2 or with the complete path @xmath46 .",
    "the conditional entropy that quantifies the uncertainty of the signal @xmath21 given the measurement @xmath147 is defined as @xmath149&\\equiv-{\\big\\langle\\ln p(x|m)\\big\\rangle}\\nonumber\\\\   & = -\\int{{\\rm d}}x\\int { { \\rm d}}m p(m|x)p(x)\\ln p(x|m ) ,   \\label{eq : a_hcond}\\end{aligned}\\ ] ] where we have used bayes formula to get @xmath150 , where @xmath151 .",
    "note that @xmath152 is a path integral if one identifies @xmath147 with the path @xmath46 .",
    "it can be shown that any estimator @xmath153 that is only a function of @xmath147 ( i.e. that uses only the knowledge of @xmath147 ) trying to optimally estimate @xmath21 satisfies the inequality @xcite",
    "@xmath154 ^ 2\\big\\rangle}\\nonumber\\\\ & = \\int{{\\rm d}}x\\int{{\\rm d}}m [ x-\\hat{x}(m)]^2p(m|x)p(x)\\nonumber\\\\ & \\ge \\frac{1}{2\\pi{{\\rm e}}}{{\\rm e}}^{2h[x|m ] } \\label{eq : a_error_hcond}\\end{aligned}\\ ] ] which can saturate only if @xmath21 and @xmath147 are jointly normal distributed .",
    "the uncertainty of the signal before ( or without ) the measurement is @xmath155=-{\\big\\langle\\ln p(x)\\big\\rangle}=-\\int{{\\rm d}}x p(x)\\ln p(x ) .",
    "\\label{eq : a_hx}\\ ] ] specifically , in the main text we have used a normal distributed signal @xmath21 with zero mean @xmath156 and variance @xmath38 . inserting the normal distribution into yields @xmath155=\\frac{1}{2}\\ln(2\\pi{{\\rm e}}\\,\\mathcal{e}_{{\\rm x}}^2 ) .",
    "\\label{eq : a_h}\\ ] ] since side information @xmath147 always reduces the uncertainty , i.e. , @xmath157\\le h[x]$ ] one can define the non - negative mutual information @xcite @xmath158\\equiv h[x]-h[x|m]={\\big\\langle\\ln\\frac{p(x|m)}{p(x)}\\big\\rangle } , \\label{eq : a_def_ixm}\\ ] ] which is the reduction of uncertainty of the signal @xmath21 due to the measurement @xmath147 . using , and one obtains the relation between error and information @xmath159}.\\ ] ] note that this inequality saturates if @xmath160 is normally distributed .",
    "the mutual information is symmetric @xmath161=i[m{:}x]$ ] .",
    "moreover , for jointly gaussian distribution of @xmath162 with covariance @xmath163 the mutual information reads @xmath164=\\frac{1}{2}\\ln\\frac{\\mathcal{e}_{{{\\rm x}}}^2}{\\mathcal{e}_{{{\\rm x}}|{\\mathrm{m}}}^2 }   = \\frac{1}{2}\\ln\\frac{\\det({\\boldsymbol{\\sigma}}_{\\mathrm{m}})}{\\det({\\boldsymbol{\\sigma}}_{{\\mathrm{m}}|x } ) } ,   \\label{eq : a_igauss}\\ ] ] where the conditional variances can be calculated with the schur complements of , see @xcite , @xmath165 it will convenient to express the mutual information in terms of @xmath166 and @xmath167 .",
    "therefore , the mutual information between jointly gaussian variables reads @xmath168&=\\frac{1}{2}\\ln\\left[\\frac{\\det({\\boldsymbol{\\sigma}}_{{\\mathrm{m}}|{{\\rm x}}}+{\\boldsymbol{b}}\\frac{1}{\\mathcal{e}_{{\\rm x}}^2}{\\boldsymbol{b}}^{\\top})}{\\det({\\boldsymbol{\\sigma}}_{{\\mathrm{m}}|{{\\rm x}}})}\\right]\\nonumber\\\\ & = \\frac{1}{2}\\ln\\left[\\det\\left(\\boldsymbol{1}+{\\boldsymbol{\\sigma}}_{{\\mathrm{m}}|{{\\rm x}}}^{-1}{\\boldsymbol{b}}\\frac{1}{\\mathcal{e}_{{\\rm x}}^2}{\\boldsymbol{b}}^{\\top}\\right)\\right]\\nonumber\\\\ & = \\frac{1}{2}\\ln\\left(1+\\frac{{\\boldsymbol{b}}^{\\top}{\\boldsymbol{\\sigma}}_{{\\mathrm{m}}|{{\\rm x}}}^{-1}{\\boldsymbol{b}}}{\\mathcal{e}_{{\\rm x}}^2}\\right ) .",
    "\\label{eq : a_iformula}\\end{aligned}\\ ] ] in the first step we have used and . according to the matrix determinant lemma , the determinant of the identity matrix plus a dyadic product can be written as scalar product , which has been used in the final step .",
    "in this appendix , we will define the generating function @xmath169 , which is related to the conditional path weight @xmath43 defined in eq . of the main text .",
    "we will calculate conditional expectation values @xmath170 that will all depend on the signal value @xmath21 and satisfy @xmath171 .",
    "the binary system with states @xmath172 , which evolves stochasticly in time , satisfy the master equation @xmath173 where @xmath174 is the probability for being in state @xmath1 at time @xmath2 .",
    "note that @xmath175 holds .",
    "the master equation can be written in matrix notation @xmath176 where @xmath177 for later convenience , we have dropped notationally the explicit dependence of the generator @xmath178 on @xmath21 . with initial distribution @xmath179 the formal time dependent solution reads @xmath180 , which can be associated with the path weight @xmath181 see eq . in the main text .",
    "specifically , @xmath182 can be obtained from by summing over all paths @xmath46 with @xmath10 .",
    "the generating function is defined as @xmath183\\boldsymbol{p}_0 , \\label{eq : a_gs}\\ ] ] where @xmath184 and the modified generator reads @xmath185 , i.e. , @xmath186 note that @xmath187 and @xmath188 .",
    "from now on `` @xmath189 '' means @xmath190 for all @xmath191 .",
    "specifically , @xmath192 .",
    "the aim of introducing this generating function is to calculate moments of @xmath50 and @xmath52 , which are functionals of the path @xmath46 .",
    "since one can interpret as the path integral over the path weight with replaced generator @xmath193 , we are able to derive the formulas for the first moments by comparing eqs .  , which read @xmath194 and @xmath195 similarly , the second moments are given by @xmath196 in the limit @xmath197 $ ] , the maximum eigenvalue ( maximum real part ) of the modified generator dominates , which is @xmath198 ^ 2}{4}}. \\label{eq : a_eig}\\end{gathered}\\ ] ] in this limit , the first moments become @xmath199 analogously , the signal dependent conditional covariance matrix , where @xmath200 , is defined as @xmath201 which , for @xmath202 , reads @xmath203 ^ 3}\\\\\\times   \\begin{pmatrix }    1 &   w_{10}(x)-w_{01}(x)\\\\   w_{10}(x)-w_{01}(x)&2[w_{01}(x)^2+w_{10}(x)^2 ]   \\end{pmatrix}.   \\label{eq : a_sigma_m|x}\\end{gathered}\\ ] ] for weak signals ( @xmath85 ) , the conditional variance can then be obtained by @xmath204 where for our purposes @xmath205 suffices for eq . .",
    "note that we show in appendix [ sec : method ] how one can perform the calculations for more complicated systems with more than two states , where it maybe difficult to determine the eigenvalue .      in the following two subsections ,",
    "we derive the weak signal approximation @xmath104 for the mutual information used in the main text , i.e. , each approximation `` @xmath206 '' will be exact up to order @xmath207 .",
    "all our approximations are fairly accurate for @xmath108 as used in the main text , which corresponds to a `` concentration change '' of about 30% . in appendix",
    "[ sec : robust ] we will show that higher order corrections vanish for our main result [ eq . in the main text ] in the long time limit .    for the mutual information , we need @xmath208 , which is given by and @xmath167 from , which is @xmath209 using the definition of the asymmetry parameter eq . in the main text and eqs . and , we obtain @xmath210 ^ 2 }   \\begin{pmatrix } 1   \\\\",
    "2 ( 1-\\alpha)w_{10}(0)-2\\alpha w_{01}(0 )   \\end{pmatrix}.   \\label{eq : a_b}\\ ] ]    the mutual information between jointly gaussian variables then reads @xmath211\\approx \\frac{1}{2}\\ln\\left(1+\\frac{{\\boldsymbol{b}}^{\\top}\\boldsymbol{\\sigma}_{{\\mathrm{m}}|x}(0)^{-1}{\\boldsymbol{b}}}{\\mathcal{e}_{{\\rm x}}^2}\\right)\\nonumber\\\\ & = \\frac{1}{2}\\ln\\left[1+(2\\alpha^2 - 2\\alpha+1 ) \\frac{\\mathcal{e}_{{\\rm x}}^2w_{01}(0)w_{10}(0)t}{w_{01}(0)+w_{10}(0 ) } \\right ] , \\label{eq : a_iges}\\end{aligned}\\ ] ] where we have inserted and in .",
    "similarly for @xmath72 , where @xmath212 , one obtains the coarse grained mutual information by using the upper right component of and the first component of @xmath213 \\approx \\frac{1}{2}\\ln\\left(1+\\frac{({\\boldsymbol{b}}_{1})^2}{[{\\boldsymbol{\\sigma}}_{{\\mathrm{m}}|x}(0)]_{11}\\mathcal{e}_{{\\rm x}}^2}\\right)\\nonumber\\\\ & = \\frac{1}{2}\\ln\\left[1+\\frac{1}{2 } \\frac{\\mathcal{e}_{{\\rm x}}^2w_{01}(0)w_{10}(0)t}{w_{01}(0)+w_{10}(0 ) } \\right ] .",
    "\\label{eq : a_itilde}\\end{aligned}\\ ] ] comparing and leads immediately to eq . in the main text , for weak signals .",
    "we show in appendix [ sec : robust ] that higher order corrections for the difference between and cancel .      for a sensor array @xmath214 of @xmath97 independent sensors , see top panel of fig .",
    "[ fig : sampletraj ] in the main text for an illustration , the path weight becomes @xmath215 where each path weight @xmath216 is given by eq . in the main text with respective individual functionals @xmath89 and @xmath88 with @xmath217 .",
    "the resulting total path weight is given by eq .",
    "in the main text . the functional of the path @xmath118 @xmath218 fully determines the path weight that is given by eq . in the main text .",
    "most importantly , the first moments satisfy @xmath219 and similarly , the variance is given by @xmath220\\\\   = n\\left[{\\langle\\boldsymbol{f}^{(1)}\\boldsymbol{f}^{(1)}{}^{\\top}\\rangle}_x-{\\langle\\boldsymbol{f}^{(1)}\\rangle}_x{\\langle\\boldsymbol{f}^{(1)}{}^{\\top}\\rangle}_x\\right ] ,    \\label{eq : a_fsecond}\\end{gathered}\\ ] ] where we have used that @xmath221 for @xmath222 and that each moment of @xmath223 coincides with the respective moment of @xmath224 .    using similar arguments as for the single sensor case one can see that each forward jump must be compensated by a reverse jump , i.e. , the coarse grained statistics @xmath225 satisfies @xmath226\\approx i[x:\\boldsymbol{m}].\\ ] ] first , from it follows @xmath227 , where @xmath167 is given from the single sensor solution .",
    "second , from it follows that @xmath228 . substituting @xmath229 and @xmath230 in yields the mutual information @xmath231 \\label{eq : a_iges_n}\\end{aligned}\\ ] ] between the time series @xmath118 of an array of @xmath97 sensors and the signal @xmath21 for @xmath232 , which is precisely equation in the main text , where @xmath233 and @xmath234 $ ] .",
    "similarly , one obtains @xmath235 \\approx \\frac{1}{2}\\ln\\left[1+\\frac{n}{2 } \\frac{\\mathcal{e}_{{\\rm x}}^2w_{01}(0)w_{10}(0)t}{w_{01}(0)+w_{10}(0 ) } \\right ] .",
    "\\label{eq : a_itilde_n}\\ ] ]    comparing and results in @xmath236 which does not depend on @xmath97 and is precisely the loss of information @xmath109 of a single sensor that is given by in the main text .",
    "in this appendix , we show that our main result is still valid if @xmath21 is broadly distributed with probability @xmath237 .",
    "we derive an approximation @xmath238 similar to [ eq . in the main text ] that converges more slowly to @xmath70 but",
    "can be used for strong signals ( e.g. , @xmath239 .",
    "we discuss @xmath240 for @xmath241 with @xmath242 and @xmath243 .",
    "the mean @xmath244 is determined by and .",
    "the covariance matrix then determines the conditional probability , which reads ( here @xmath241 ) @xmath245 } } ,   \\label{eq : a_pmx_last}\\ ] ] where @xmath246^{\\top}{\\boldsymbol{\\sigma}}_{{\\mathrm{m}}|{{\\rm x}}}(x)^{-1}[\\boldsymbol{m}-{\\boldsymbol{\\mu}}(x ) ] .",
    "\\label{eq : a_lmx_last}\\ ] ] since @xmath247 holds , we obtain a substantial weight only for @xmath248 in the limit @xmath58 .",
    "( green dashed lines ) and conditional distribution @xmath249 ( blue dotted ellipse ) . ]",
    "we will now use fig .",
    "[ fig : a_trace ] as a blue print to derive an approximation for the marginal distribution of @xmath250 , which can be used to determine the mutual information between @xmath250 and @xmath21 in the long time limit exactly",
    ". the green solid curve corresponds to the one dimensional line @xmath251 where @xmath252 .",
    "the dashed line illustrates the width of the marginal distribution of @xmath250 . the blue point with",
    "the dotted ellipse corresponds to a single point @xmath253 , where the ellipse emphasizes the width @xmath254 of the conditional distribution , where @xmath21 is given .",
    "it is important to note that an increase of variance and mean that increases linearly in time implies @xmath255 and @xmath256 , leading to @xmath257    for the marginal path weight one has to integrate over @xmath21 . using saddle point approximation we obtain for the path weight @xmath258\\frac{{\\partial}{\\boldsymbol{\\mu}}}{{\\partial}x}^{\\top}{\\boldsymbol{\\sigma}}_{{\\mathrm{m}}|{{\\rm x}}}(x^*)^{-1}\\frac{{\\partial}{\\boldsymbol{\\mu}}}{{\\partial}x } } } ,   \\label{eq : a_pm_last}\\end{aligned}\\ ] ] where @xmath259 maximizes for a given @xmath250 , as indicated with two points connected via a red solid line in fig .",
    "[ fig : a_trace ] .     versus exact result with asymmetric rates ( @xmath29 ) .",
    "the numerical solution of the exact mutual information @xmath70 is indicated as blue open circles .",
    "the red solid line indicates the approximation from eq . of the main text , which is here equivalent to , and assumes a narrow - width gaussian distribution of @xmath21 .",
    "parameter : @xmath260 , @xmath29 .",
    "( a ) signal standard deviation @xmath108 [ same as in fig .",
    "[ fig:4plots](a ) in the main text ] .",
    "( b ) @xmath261 . ]",
    "we can approximate the mutual information between @xmath21 and @xmath250 , @xmath168&\\equiv\\int { { \\rm d}}\\boldsymbol{m}\\int{{\\rm d}}x p_{{\\rm x}}(x)p_{{\\mathrm{m}}|{{\\rm x}}}(\\boldsymbol{m}|x)\\ln\\frac{p_{{\\mathrm{m}}|{{\\rm x}}}(\\boldsymbol{m}|x)}{p_{{\\mathrm{m}}}(\\boldsymbol{m})},\\end{aligned}\\ ] ] by using eqs . , and the relation @xmath262 which becomes @xmath263 after averaging over @xmath264 . finally , we set @xmath265 and find @xmath168   & \\approx\\int { { \\rm d}}x p_{{\\rm x}}(x)\\ln\\bigg[\\frac{1}{p_{{\\rm x}}(x)}\\sqrt{\\frac{\\frac{{\\partial}{\\boldsymbol{\\mu}}}{{\\partial}x}^{\\top}{\\boldsymbol{\\sigma}}_{{\\mathrm{m}}|{{\\rm x}}}(x)^{-1}\\frac{{\\partial}{\\boldsymbol{\\mu}}}{{\\partial}x}}{2\\pi { { \\rm e}}}}\\bigg ]   \\nonumber\\\\   & \\equiv\\mathcal{i}^\\infty .   \\label{eq : a_iinfty}\\end{aligned}\\ ] ]",
    "we note that eq .",
    "becomes exact in the long time limit ( @xmath58 ) .    with fig .",
    "[ fig:0.3vs2.0 ] we confirm that is indeed accurate for sufficient long time . for strong signals ( @xmath261 ) the approximation from eq . , which is eq .",
    "in the main text , fails . in this case one",
    "has to use instead .",
    "moreover , one can see that for a narrow - width gaussian the approximation from eq . in the main text",
    "is quite accurate even for `` finite time '' .",
    "note that the parameters from fig .",
    "[ fig:0.3vs2.0](a ) are the same as in fig .",
    "[ fig:4plots](a ) in the main text .     versus exact result with asymmetric rates ( @xmath29 ) .",
    "the signal is uniformly distributed within @xmath266 .",
    "parameter : @xmath260 , @xmath29 . ]",
    "this approximation can also be used for non - gaussian distributed signals .",
    "one example of a uniformly distributed signal is illustrated in fig .",
    "[ fig : rectangle ] , where @xmath267 is used for @xmath266 . for @xmath58 ,",
    "the approximation @xmath238 approaches @xmath70 .",
    "the convergence is more slowly than for a gaussian distributed signal with the same width .    for @xmath268 and @xmath269 , the coarse - grained version of reads @xmath270&\\approx\\int { { \\rm d}}x p_{{\\rm x}}(x)\\ln\\bigg[\\frac{1}{p_{{\\rm x}}(x)}\\sqrt{\\frac{\\left|\\frac{{\\partial}\\mu_1(x)}{{\\partial}x}\\right|^2 } { 2\\pi{{\\rm e}}[{\\boldsymbol{\\sigma}}_{{\\mathrm{m}}|{{\\rm x}}}(x)]_{11}}}\\bigg ]   \\nonumber\\\\   & \\equiv\\tilde{\\mathcal{i}}^\\infty ,   \\label{eq : a_icginfty}\\end{aligned}\\ ] ] with equality for @xmath58 .",
    "note that from follows @xmath271_{11}=2w_{01}(x)w_{10}(x)t/[w_{01}(x)+w_{10}(x)]^3 $ ] .",
    "the difference between and yields an information gain of the jump events @xmath272_{11}\\frac{{\\partial}{\\boldsymbol{\\mu}}}{{\\partial}x}^{\\top}{\\boldsymbol{\\sigma}}_{{\\mathrm{m}}|{{\\rm x}}}(x)^{-1}\\frac{{\\partial}{\\boldsymbol{\\mu}}}{{\\partial}x}}{\\left|\\frac{{\\partial}\\mu_1(x)}{{\\partial}x}\\right|^2}\\right ] .",
    "\\label{eq : a_iinfdiff}\\end{gathered}\\ ] ] the same calculation as in by using , and eq . from the main text can be used to obtain @xmath273 ^ 2 }   \\begin{pmatrix } 1   \\\\    2 ( 1-\\alpha)w_{10}(x)-2\\alpha w_{01}(x )   \\end{pmatrix},\\end{gathered}\\ ] ] which together with finally yields @xmath274 this result implies eq . from the main text , since the approximations and are accurate for @xmath58 .",
    "finally , the derivation from appendix [ sec : multi ] can be adopted to an array of @xmath97 independent sensors .",
    "since mean and covariance is proportional to @xmath97 , the expression does not depend on the number of sensors .",
    "in the limit @xmath58 , the generating function has the form @xmath275 with equality for @xmath58 .",
    "moreover , the conservation of probability determines @xmath276 and the existence of a steady state requires @xmath277 .",
    "let @xmath278 and @xmath279 .",
    "thus one obtains @xmath280 similarly , the dispersion is related to the second derivative @xmath281    following the same procedure as in @xcite ( see also @xcite ) one can obtain the derivatives of the maximal eigenvalue directly from the characteristic polynomial @xmath282=\\prod_i(\\lambda_i({\\boldsymbol{s}})-\\lambda ) \\\\   = \\overbrace{\\prod_i\\lambda_i({\\boldsymbol{s}})}^{\\equiv c_0({\\boldsymbol{s } } ) } -\\lambda\\overbrace{\\sum_{j}\\prod_{j\\neq i}\\lambda_i({\\boldsymbol{s}})}^{\\equiv c_1({\\boldsymbol{s } } ) }   + \\lambda^2\\overbrace{\\sum_{i",
    "< j}\\prod_{k\\neq i , j}\\lambda_k({\\boldsymbol{s}})}^{\\equiv c_2({\\boldsymbol{s}})}+\\mathrm{o}(\\lambda)^3 , \\hidewidth\\\\\\end{gathered}\\ ] ] where @xmath283 are the eigenvalues of @xmath284 , which need not to be known explicitly .",
    "since @xmath285 implies @xmath286 and @xmath277 , we find @xmath287.\\end{aligned}\\ ] ] hence , one can identify @xmath288_{{\\boldsymbol{s}}=1}.   \\label{eq : sophisticated1}\\ ] ] with and one is able to calculate the first moments and .",
    "similarly , after some extended algebra we obtain @xmath289_{{\\boldsymbol{s}}=1}.\\\\   \\label{eq : sophisticated2}\\end{gathered}\\ ] ] with this method , similar to @xcite , one obtains for @xmath222 with @xmath278 and @xmath290 the coupled dispersion between a jump variable @xmath291 and the persistence time @xmath292 .",
    "the procedure is as follows .",
    "one needs and to calculate , , which in turn with - can be used to calculate the covariances of persistence time and/or jump events .    to illustrate the procedure ,",
    "let us use our specific two state model with modified generator that leads to the characteristic polynomial @xmath293\\lambda+\\lambda^2,\\end{aligned}\\ ] ] where we can identify @xmath294 in the first line , @xmath295 in minus the brackets @xmath296 $ ] , and @xmath297 . specifically , for the first moment @xmath298 , we set @xmath299 to get @xmath300 $ ] and @xmath301 .",
    "using implies @xmath302 $ ] . inserting the result into and using , which states that @xmath303 $ ] , yields the first moment @xmath304 which agrees with .    for the variance @xmath305",
    ", we use with @xmath306 , @xmath307 , @xmath308 , and @xmath309 $ ] , and @xmath310 , to obtain @xmath311 ^ 2}{-[w_{01}(x)+w_{10}(x)]^3}+\\frac{2w_{01}(x)w_{10}(x)^2}{[w_{01}(x)+w_{10}(x)]^2}\\nonumber\\\\   & = \\frac{2w_{01}(x)w_{10}(x)^3}{[w_{01}(x)+w_{10}(x)]^3}.\\end{aligned}\\ ] ] with eqs . and",
    ", we obtain @xmath312 ^ 3},\\ ] ] which agrees with .",
    "similarly , one can determine all remaining entries in the covariance matrix .",
    "from the linear noise approximation , we have the langevin equation @xmath313 where @xmath314 , @xmath315 with @xmath316 , and finally , the white noise @xmath129 satisfies @xmath317 with @xmath132 . the goal of filtering is to estimate the signal @xmath21 such that @xmath318 becomes minimal , where @xmath319 is a functional of the trajectory @xmath134 . for a gaussian process like",
    ", one can verify that @xmath319 must be a linear projection of @xmath21 on @xmath134 , i.e. , @xmath320 for @xmath321 @xcite .    after rewriting as an ito differential equation @xmath322 where @xmath323 is the increment of a wiener process with @xmath324 , one obtains , by applying a gram - schmidt procedure for the projection @xmath319 , the differential equation @xcite @xmath325,\\end{aligned}\\ ] ] with @xmath326 .",
    "this in turn leads to the so - called riccati equation @xcite @xmath327 with @xmath328 .",
    "one can immediately see that the solution is given by @xmath329 calculating the mutual information between the signal @xmath21 and the trajectory @xmath134 , which according to reads @xmath330=i[x{:}\\hat{x}_t]=   \\frac{1}{2}\\ln\\frac{\\mathcal{e}_{{\\rm x}}^2}{\\tilde{\\mathcal{e}}_t^2}\\\\   = \\frac{1}{2}\\ln\\left[1+\\frac{n}{2}p_0(1-p_0)\\mathcal{e}_{{\\rm x}}^2\\omega_{{\\rm y}}t\\right]\\approx\\tilde{\\mathcal{i}}_n,\\quad\\end{gathered}\\ ] ] where we set @xmath331 and identified the result finally with @xmath332 given in .",
    "note that all intermediate steps are exact due to the gaussian nature of the coarse grained process @xmath119 ."
  ],
  "abstract_text": [
    "<S> we study the information loss of a class of inference strategies that is solely based on time averaging . for an array of independent binary sensors ( e.g. , receptors , single electron transistors ) measuring a weak random signal ( e.g. , ligand concentration , gate voltage ) this information loss is up to 0.5bit per measurement irrespective of the number of sensors . </S>",
    "<S> we derive a condition related to the local detailed balance relation that determines whether or not such a loss of information occurs . </S>",
    "<S> specifically , if the free energy difference arising from the signal is symmetrically distributed among the forward and backward rates , time integration mechanisms will capture the full information about the signal . as an implication , for the linear noise approximation , we can identify the same loss of information , arising from its inherent simplification of the dynamics . </S>"
  ]
}