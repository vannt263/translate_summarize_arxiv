{
  "article_text": [
    "suppose we observe a single noisy matrix @xmath8 , generated by adding noise @xmath4 to an unknown matrix @xmath2 , so that @xmath3 , where @xmath4 is a noise matrix .",
    "we wish to recover the matrix @xmath2 with some bound on the mean squared error ( mse ) .",
    "this is hopeless when @xmath2 is a completely general matrix , and the noise @xmath4 is arbitrary ; but when @xmath2 happens to be of relatively low rank , and the noise matrix is i.i.d .",
    "standard gaussian , one can indeed guarantee quantitatively accurate recovery .",
    "this paper provides explicit formulas for the best possible guarantees obtainable by a popular , computationally practical procedure .",
    "specifically , let @xmath8 , @xmath2 and @xmath4 be @xmath18-by-@xmath19 real matrices ( a set we denote by @xmath20 ) , and suppose that @xmath4 has i.i.d .",
    "entries , @xmath21 .",
    "consider the following nuclear - norm penalization ( nnp ) problem : @xmath22 where @xmath23 denotes the sum of singular values of @xmath24 , also known as the nuclear norm , @xmath25 denotes square root of the sum of squared matrix entries , also known as the frobenius norm and @xmath26 is a penalty factor .",
    "a solution to ( nnp ) is efficiently computable by modern convex optimization software @xcite ; it shrinks away from @xmath8 in the direction of smaller nuclear norm .",
    "measure performance ( risk ) by mean - squared error ( mse ) .",
    "when the unknown @xmath2 is of known rank @xmath27 and belongs to a matrix class @xmath28 , the minimax mse of nnp is @xmath29 namely the worst - case risk of @xmath30 , where @xmath31 is the threshold for which this worst - case risk is the smallest possible .",
    "here , @xmath32 denotes expectation with respect to the random noise matrix @xmath4 , conditional on a given value of the signal matrix @xmath2 , and @xmath33 denotes the denoiser @xmath34 acting on the matrix @xmath35 .",
    "note that the symbol @xmath36 denotes a matrix class , not a particular matrix . for square matrices , @xmath37",
    ", we write @xmath38 instead of @xmath39 . in a very clear sense @xmath40",
    "gives the best possible guarantee for the mse of nnp , based solely on the rank and problem size , and not on other properties of the matrix @xmath2 .      in this paper",
    ", we calculate the minimax mse @xmath41 for two matrix classes @xmath42 :    _ general matrices _ : @xmath43 : the signal @xmath2 is a real matrix @xmath44 ( @xmath45 ) .    _ symmetric matrices _ :",
    "@xmath46 : the signal @xmath2 is a real , symmetric positive semidefinite matrix , a set we denote by @xmath47 .    in both cases , the asymptotic mse ( amse ) in the `` large @xmath19 '' asymptotic setting admits considerably simpler and more accessible formulas than the minimax mse for finite @xmath19 .",
    "so in addition to the finite-@xmath19 minimax mse , we study the asymptotic setting where a sequence of problem size triplets @xmath48 is indexed by @xmath49 , and where , along this sequence @xmath50 and @xmath51 .",
    "we think of @xmath52 as the matrix shape parameter ; @xmath53 corresponds to a square matrix , and @xmath16 to a matrix wider than it is tall .",
    "we think of @xmath54 as the fractional rank parameter , with @xmath55 implying low rank relative to matrix size . using these notions",
    "we can define the asymptotic minimax mse ( amse ) @xmath56    we obtain explicit formulas for the asymptotic minimax mse in terms of incomplete moments of classical probability distributions : the quarter - circle and semi - circle laws ( square case @xmath53 ) and the marenko ",
    "pastur distribution ( nonsquare case @xmath16 ) .",
    "figures  [ amse_mat : fig ] and [ amse_mat_sym : fig ] show how the amse depends on the matrix class @xmath42 , the rank fraction @xmath54 and the shape factor @xmath52 .",
    "we also give explicit formulas for the optimal regularization parameter @xmath57 , also as a function of @xmath54 ; see figures  [ mmx_lambda_mat : fig ] and [ mmx_lambda_mat_sym : fig ] .",
    "these minimax mse results constitute best possible guarantees , in the sense that for the procedure in question , the mse is actually attained at some rank @xmath27 matrix , so that no better guarantee is possible for the given tuning parameter @xmath57 ; but also , no other tuning parameter offers a better such guarantee .      we see four reasons to develop these bounds .",
    "several important problems in modern signal and image processing , in network data analysis and in computational biology can be cast as recovery of low - rank matrices from noisy data , and nuclear norm minimization has become a popular strategy in many cases ; see , for example , @xcite and references therein .",
    "our results provide sharp limits on what such procedures can hope to achieve , and validate rigorously the idea that _ low rank alone _ is enough to provide some level of performance guarantee ; in fact , they precisely quantify the best possible guarantee",
    ".      one might wonder whether some other procedure offers even better guarantees than nnp .",
    "consider then the minimax risk _ over all procedures _ , defined by @xmath58 where @xmath59 is some measurable function of the observations , and its corresponding minimax amse @xmath60 where the sequences @xmath10 and @xmath9 are as above . here one wants to find the best possible procedure , without regard to efficient computation .",
    "we also prove a lower bound on the minimax mse over all procedures , and provide an asymptotic evaluation @xmath61 in the square case ( @xmath53 ) , this simplifies to @xmath62 .",
    "the nnp - minimax mse is by definition larger than the minimax mse , @xmath63 .",
    "while there may be procedures outperforming nnp , the performance improvement turns out to be limited . indeed ,",
    "our formulas show that @xmath64 while @xmath65 for square matrices ( @xmath53 ) , this simplifies to @xmath66 in words , the potential improvement in minimax amse of _ any _ other matrix denoising procedure over nnp is at most a factor of @xmath67 ; and if any such improvement were available , it would only be available in extreme low - rank situations . actually obtaining such an improvement in performance guarantees",
    "is an interesting research challenge .      the low - rank matrix denoising problem stands in a line of now - classical problems in minimax decision theory .",
    "consider the sparse vector denoising problem , where an unknown vector @xmath68 of interest yields noisy observations @xmath69 with noise @xmath70 ; the vector @xmath71 is sparsely nonzero@xmath72with @xmath73 and @xmath71 independent . in words , a vector with a fraction @xmath74 of nonzeros is observed with noise . in this",
    "setting , consider the following @xmath7-norm penalization problem : @xmath75 the sparse vector denoising problem exhibits several striking structural resemblances to low - rank matrix denoising :    * _ thresholding representation_. for a scalar @xmath76 , define the soft thresholding nonlinearity by @xmath77 in words , values larger than @xmath78 are shifted toward zero by @xmath78 , while those smaller than @xmath78 are set to zero .",
    "the solution vector @xmath79 of ( @xmath80 ) obeys @xmath81 ; namely , it applies @xmath82 coordinate wise .",
    "similarly , the solution of ( nnp ) applies @xmath82 coordinate wise to the singular values of the noisy matrix @xmath8 .",
    "+ by this observation , ( @xmath80 ) can also be called `` soft thresholding '' or `` soft threshold denoising , '' and in fact , these other terms are the labels in common use . similarly , nnp amounts to `` soft thresholding of singular values . '' this paper will henceforth use the term _ singular value soft thresholding _ ( svst ) . * _",
    "sparsity / low rank analogy .",
    "_ the objects to be recovered in the sparse vector denoising problem have sparse entries ; those to be recovered in the low - rank matrix denoising problem have sparse singular values . thus the fractional sparsity parameter @xmath83 is analogous to the fractional rank parameter @xmath54 .",
    "it is natural to ask the same questions about behavior of minimax mse in one setting ( say , asymptotics as @xmath84 ) as in the other setting ( @xmath85 ) .",
    "in fact , such comparisons turn out to be illuminating .",
    "* _ structure of the least - favorable estimand . _ among sparse vectors @xmath68 of a given fixed sparsity fraction @xmath83 , which of these is the hardest to estimate ?",
    "this should maximize the mean - squared error of soft thresholding , even under the most clever choice of @xmath78 .",
    "this least - favorable configuration is singled out in the minimax amse @xmath86 in this min / max , the least favorable situation has all its nonzeros , in some sense , `` at infinity '' ; that is , all sparse vectors which place large enough values on the nonzeros are nearly least favorable , that is , essentially make the problem maximally difficult for the estimator , even when it is optimally tuned . in complete analogy , in low - rank matrix",
    "denoising we will see that all low - rank matrices , which are in an appropriate sense `` sufficiently large , '' are thereby almost least favorable .",
    "* _ structure of the minimax smoothing parameter .",
    "_ in the sparse vector denoising amse ( [ eq : sparseminmax ] ) the @xmath87 achieving the infimum is a type of optimal regularization parameter , or optimal threshold .",
    "it decreases as @xmath83 increases , with @xmath88 as @xmath89 . paralleling this ,",
    "we show that the low - rank matrix denoising amse  ( [ svt - mmx : eq ] ) has minimax singular value soft threshold @xmath90 decreasing as @xmath54 increases , and @xmath91 as @xmath92 .    despite these similarities ,",
    "there is one major difference between sparse vector denoising and low - rank matrix denoising . in the sparse vector denoising problem ,",
    "the soft - thresholding minimax mse was compared to the minimax mse over all procedures by donoho and johnstone @xcite .",
    "let @xmath93 denote the soft thresholding amse and define the minimax amse over all procedures via @xmath94 where here @xmath95 denotes _ any _ procedure which is measurable in the observations . in the limit of extreme sparsity ,",
    "soft thresholding is _ asymptotically minimax _",
    "@xcite , @xmath96 breaking the chain of similarities , we are not able to show a similar asymptotic minimaxity for svst in the low rank matrix denoising problem .",
    "although equation ( [ eq : bndcompare ] ) says that soft thresholding of singular values is asymptotically not more than a factor of 3 suboptimal , we doubt that anything better than a factor of @xmath67 can be true ; specifically , we conjecture that svst suffers a _",
    "minimaxity gap_. for example , for @xmath53 , we conjecture that @xmath97 we believe that interesting new estimators will be found improving upon singular value soft thresholding by essentially this factor of @xmath67",
    ". namely , there may be substantially better guarantees to be had under extreme sparsity , than those which can be offered by svst . settling the minimaxity gap for svst seems a challenging new research question .",
    "evaluating the minimax mse of svst has an intriguing new motivation @xcite , arising from the newly evolving fields of compressed sensing and matrix completion .",
    "consider the problem of recovering an unknown matrix @xmath2 from _ noiseless _ , _ indirect _ measurements .",
    "let @xmath98 be a linear operator , and consider observations @xmath99 in words , @xmath100 contains @xmath101 linear measurements of the matrix object @xmath2 .",
    "see the closely related _ trace regression _",
    "model @xcite which also includes measurement noise .",
    "can we recover @xmath2 ?",
    "it may seem that @xmath102 measurements are required , and in general this would be true ; but if @xmath2 happens to be of low rank , and @xmath103 has suitable properties , we may need substantially fewer measurements .",
    "consider reconstruction by _ nuclear norm minimization _",
    ", @xmath104    recht and co - authors found that when the matrix representing the operator @xmath103 has i.i.d .",
    "@xmath105 entries , and the matrix is of rank @xmath27 , the matrix @xmath2 is recoverable from @xmath106 measurements for certain combinations of @xmath101 and @xmath27 @xcite . the operator @xmath103 offers so - called _ gaussian measurements _ when the representation of the operator as a matrix has i.i.d .",
    "gaussian entries .",
    "empirical work by recht , xu and hassibi  @xcite , fazel , parillo and recht @xcite , tanner and wei @xcite and oymak and hassibi @xcite documented for gaussian measurements a _ phase transition _ phenomenon , that is , a fairly sharp transition from success to failure as @xmath27 increases , for a given @xmath101 .",
    "putting @xmath107 and @xmath108 it appears that there is a critical sampling rate @xmath109 , such that , for @xmath110 , nnm is successful for large  @xmath111 , while for @xmath112 , nnm fails .",
    "@xmath113 provides a sharp `` sampling limit '' for low rank matrices , that is , a clear statement of how many measurements are needed to recover a low rank matrix , by a popular and computationally tractable algorithm .    in very recent work , @xcite , it has been shown empirically that the precise location of the phase transition _ coincides with the minimax mse _ @xmath114 a key requirement for discovering and verifying ( [ eq : ptformula ] ) empirically was to obtain an explicit formula for the right - hand side ; that explicit formula is derived and proven in this paper .",
    "relationship ( [ eq : ptformula ] ) connects two seemingly unrelated problems : matrix denoising from direct observations and matrix recovery from incomplete measurements .",
    "both problems are attracting a large and growing research literature .",
    "equation ( [ eq : ptformula ] ) demonstrates the importance of minimax mse calculations even in a seemingly unrelated setting where there is no noise and no statistical decision to be made !",
    "we start by identifying the least - favorable situation for matrix denoising by svst .",
    "[ lf : thmm ] define the risk function of a denoiser @xmath115 at @xmath44 by @xmath116 let @xmath117 , @xmath118 and @xmath119 .",
    "for the worst - case risk of @xmath34 on @xmath120 matrices of rank at most @xmath121 , we have @xmath122 where @xmath123 is any fixed matrix of rank exactly @xmath121 .",
    "let @xmath124 denote the marginal distribution of the @xmath125th largest eigenvalue of a standard central wishart matrix @xmath126 , namely , the @xmath125th largest eigenvalue of the random matrix @xmath127 where @xmath128 has i.i.d . @xmath129 entries .",
    "define for @xmath130 and @xmath131 @xmath132 \\\\[-8pt ] \\nonumber & & { } + \\alpha\\frac{({n}-{r})}{{m}{n } } \\sum_{i=1}^{{m}-{r}}w_i ( \\lambda;{m}-{r};{n}-{r } ) , \\end{aligned}\\ ] ] where @xmath133 is a combination of the complementary incomplete moments of standard central wishart eigenvalues @xmath134 for @xmath135 .",
    "[ finite - n - mmx : thmm ] the minimax mse of svst over @xmath18-by-@xmath19 matrices of rank at most @xmath121 is given by @xmath136 where the minimum on the right - hand sides is unique .",
    "in fact , we will see that @xmath137 is convex in @xmath138 . as the densities of the standard central wishart eigenvalues @xmath124 are known @xcite , this makes it possible , in principle , to tabulate the finite-@xmath1 minimax risk .",
    "a more accessible formula is obtained by calculating the large-@xmath1 asymptotic minimax mse , where @xmath139 and @xmath140 both grow proportionally to @xmath19 .",
    "let us write _ minimax amse _ for asymptotic minimax mse .",
    "for the case @xmath141 we assume a limiting rank fraction @xmath142 and limiting aspect ratio @xmath143 and consider @xmath144 \\\\[-8pt ] \\nonumber & = & \\lim_{{n}\\to\\infty}\\inf_{\\lambda } \\mathop{\\sup _ { x_0\\in m_{\\lceil\\beta{n}\\rceil\\times n}}}_{\\operatorname{rank}(x_0)\\leq \\rho\\beta{n } } \\frac{1}{{m}{n}}{\\mathbb{e}}\\vert \\hat{x}_\\lambda - x_0 \\vert_f^2.\\end{aligned}\\ ] ] similarly , for the case @xmath145 , we assume a limiting rank fraction @xmath146 and consider @xmath147 \\\\[-8pt ] \\nonumber & = & \\lim_{n\\to\\infty } \\inf_{\\lambda } \\mathop{\\sup _ { { x_0\\in s^{n}_+}}}_{\\operatorname{rank}(x_0)\\leq\\rho{n } } \\frac{1}{{n}^2}{\\mathbb{e}}\\vert \\hat{x}_\\lambda - x_0 \\vert_f^2.\\end{aligned}\\ ] ]    the marcenko  pastur distribution @xcite gives the asymptotic empirical distribution of wishart eigenvalues .",
    "it has density @xmath148}(t),\\ ] ] where @xmath149 .",
    "define the complementary incomplete moments of the marcenko ",
    "pastur distribution @xmath150 finally , let @xmath151 \\\\[-8pt ] \\nonumber & & \\qquad\\quad{}\\times \\biggl [ \\rho\\lambda^2 \\\\ & & \\hspace*{16pt}\\qquad\\quad{}+\\alpha(1-\\rho ) \\biggl ( p_\\gamma\\bigl ( \\lambda^2 ; 1 \\bigr ) - 2\\lambda p_\\gamma\\biggl ( \\lambda^2 ; \\frac{1}{2}\\biggr ) + \\lambda^2 p_\\gamma\\bigl ( \\lambda^2 ; 0\\bigr ) \\biggr ) \\biggr ] , \\nonumber\\end{aligned}\\ ] ] with @xmath152 .    [ asymp - mmx : thmm ] for the minimax amse of svst we have @xmath153 with @xmath154 , where the minimum on the right - hand sides is unique .",
    "moreover , for any @xmath155 , the function @xmath156 is continuous and increasing on @xmath157 $ ] , with @xmath158 and @xmath159 .",
    "the same is true for @xmath160 .",
    "the curves @xmath161 , for different values of @xmath52 , are shown in figure  [ amse_mat : fig ] .",
    "the curves @xmath161 and @xmath162 are shown in figure  [ amse_mat_sym : fig ] .    ,",
    "defined in ( [ asymp - mmx - mat : eq ] ) , for a few values of @xmath52 . ]     with @xmath53 and case @xmath163 . ]      to compute @xmath164 and@xmath160 we need to minimize ( [ proxy : eq ] ) .",
    "define @xmath165    [ proxy - minimizer : thmm ] for any @xmath131 and @xmath166 $ ] , the function @xmath167 is decreasing on @xmath157 $ ] with @xmath168 \\lim_{\\rho\\to1}\\lambda_*(\\rho,\\beta,\\alpha ) & = & \\lambda_*(1,\\beta , \\alpha ) = 0 .",
    "\\label{lim_lambda_up : eq}\\vspace*{2pt}\\end{aligned}\\ ] ] for @xmath169 , the minimizer @xmath170 is the unique root of the equation in @xmath138 @xmath171 where the left - hand side of ( [ argmin - lambda : eq ] ) is a decreasing function of @xmath138 .",
    "the minimizer @xmath170 can therefore be determined numerically by binary search .",
    "[ in fact , we will see that @xmath172 is the unique minimizer of the convex function @xmath173 . ] evaluating @xmath164 and @xmath160 to precision @xmath174 thus requires @xmath175 evaluations of the complementary incomplete marcenko  pastur moments ( [ mp : eq ] ) .    for square matrices ( @xmath53 )",
    ", this computation turns out to be even simpler , and only requires evaluation of elementary trigonometric functions .",
    "[ proxy - sq : thmm ] we have @xmath176 \\\\[-12pt ] \\nonumber & & { } + ( 1-\\rho ) \\bigl [ \\rho\\lambda^2 + \\alpha(1-\\rho ) \\bigl ( q_2 ( \\lambda ) - 2\\lambda q_1 ( \\lambda ) + \\lambda^2 q_0 ( \\lambda ) \\bigr ) \\bigr ] , \\nonumber\\vspace*{1pt}\\end{aligned}\\ ] ] where @xmath177 \\\\[-7pt ] \\nonumber & = & 1 - \\frac{x}{2\\pi}\\sqrt{4-x^2 } - \\frac{2}{\\pi } a\\operatorname{tan}\\biggl(\\frac{x}{\\sqrt { 4-x^2 } } \\biggr ) , \\\\[1pt ] q_{1}(x ) & = & \\frac{1}{\\pi}\\int_x^2 t \\sqrt{4-t^2 } \\,dt = \\frac{1}{3\\pi}\\bigl(4 - x^2 \\bigr)^{3/2 } , \\label{quarter - circle - moments1:eq } \\\\[1pt ] \\label{q2:eq } q_{2}(x ) & = & \\frac{1}{\\pi}\\int_x^2 t^2 \\sqrt{4-t^2 } \\,dt \\nonumber \\\\[-7pt ] \\\\[-7pt ] \\nonumber & = & 1 - \\frac{1}{4\\pi}x \\sqrt{4-x^2}\\bigl(x^2 - 2\\bigr ) - \\frac{2}{\\pi}a\\operatorname{sin } \\biggl(\\frac{x}{2}\\biggr ) \\label{quarter - circle - moments2:eq}\\vadjust{\\goodbreak}\\end{aligned}\\ ] ] are the complementary incomplete moments of the quarter circle law .",
    "moreover , for @xmath131 @xmath178 where @xmath179 $ ] is the unique solution to the transcendental equation @xmath180 the left - hand side of ( [ argmin - lambda - sq : eq ] ) is a decreasing function of @xmath181 .    in @xcite",
    "we make available a matlab script , and a web - based calculator for evaluating @xmath164 and @xmath160 .",
    "the implementation provided employs binary search to solve ( [ argmin - lambda : eq ] ) [ or ( [ argmin - lambda - sq : eq ] ) for @xmath53 ] and then feeds the minimizer @xmath172 into  ( [ proxy : eq ] ) [ or into ( [ proxy - sq : eq ] ) for @xmath53 ] .      the crucial functional @xmath172 , defined in ( [ proxy - minimizer : eq ] ) , can now be explained as the optimal ( minimax ) threshold of svst in a special system of units .",
    "let @xmath183 denote the minimax tuning threshold , namely @xmath184    [ asymp - tuning - mmx : thmm ] consider again a sequence @xmath185 with a limiting rank fraction @xmath142 and a limiting aspect ratio @xmath143 .",
    "for the asymptotic minimax tuning threshold we have @xmath186    the curves @xmath187 , namely the scaled asymptotic minimax tuning threshold for svst , are shown in figure  [ mmx_lambda_mat : fig ] for different values of @xmath52 .",
    "the curves @xmath188 and @xmath189 are shown in figure  [ mmx_lambda_mat_sym : fig ] .",
    ", when @xmath190 and @xmath191 , for a few values of @xmath52 . ]     and @xmath189 , when @xmath191 .",
    "]      for square matrices ( @xmath192 , @xmath53 ) the minimax curves @xmath193 and @xmath160 admit a parametric representation in the @xmath194 plane using elementary trigonometric functions .    [ parametric : thmm ] as @xmath181 ranges over @xmath195 , @xmath196\\end{aligned}\\ ] ] is a parametric representation of @xmath197\\end{aligned}\\ ] ] is a parametric representation of @xmath198 .",
    "[ small - rho - mmx : thmm ] for the behavior of the minimax curves near @xmath199 , we have @xmath200 and in particular @xmath201 moreover , @xmath202    the minimax amse curves @xmath156 for small values of @xmath54 , and the corresponding approximation slopes @xmath203 are shown in figure  [ amse_small_rho : fig ] for several values of @xmath204 .",
    "we find it surprising that asymptotically , _",
    "symmetric positive definite matrices are no easier to recover than general square matrices_. this phenomenon is also seen in the case of sparse vector denoising , where in the limit of extreme sparsity , the nonnegativity of the nonzeros does not allow one to reduce the minimax mse .. ] we note that this first - order amse near @xmath199 agrees with a different asymptotic model for minimax mse of svst over large low - rank matrices @xcite . there",
    ", the asymptotic prediction for amse near @xmath199 is found to be in agreement with the empirical finite-@xmath1 mse .     for small values of @xmath54 ( dashed lines ) and the corresponding approximation slopes @xmath205 ( solid lines ) . ]      in ( [ mmx : eq ] ) we have introduced global minimax mse @xmath206 , namely the minimax risk over _ all _ measurable denoisers @xmath115 . to define the large-@xmath1 asymptotic global minimax mse analogous to ( [ asymp - mmx - svt : eq ] ) , consider sequences where @xmath139 and @xmath140 both grow proportionally to @xmath19 , such that both limits @xmath142 and @xmath143 exist .",
    "define the asymptotic global minimax mse @xmath207    [ global - mmx : thmm ] for the global minimax mse we have @xmath208 for case @xmath209 , and if @xmath37 , for case @xmath163 .    for the asymptotic global minimax mse we have @xmath210 for case @xmath209 , and if @xmath53 , for case @xmath163 . here",
    "@xmath211 .",
    "let @xmath212 denote our lower bound on asymptotic global minimax mse .",
    "then @xmath213 and @xmath214      the body of the paper proves the above results .",
    "section  [ prelim : sec ] introduces notation , and proves auxiliary lemmas . in section  [ lf : sec ]",
    "we characterize the worst - case mse of svst for matrices of a fixed size ( theorem  [ lf : thmm ] ) . in section  [ finite - n - proxy : sec ]",
    "we derive formula ( [ finite - n - proxy : eq ] ) for the worst - case mse , and prove theorem  [ finite - n - mmx : thmm ] . in section  [ asymp - proxy : sec ]",
    "we pass to the large-@xmath19 limit and derive formula ( [ proxy : eq ] ) , which provides the worst - case asymptotic mse in the large-@xmath19 limit ( theorem  [ asymp - mmx : thmm ] ) . in section  [ asymp - minimizer : sec ] we investigate the minimizer of the asymptotic worst - case mse function , and its minimum , namely the minimax amse , and prove theorem  [ proxy - minimizer : thmm ] . in section  [ global : sec ]",
    "we extend our scope from svst denoisers to all denoisers , investigate the global minimax mse and prove theorem  [ global - mmx : thmm ] . in the interest of space ,",
    "theorems [ proxy - sq : thmm ] , [ asymp - tuning - mmx : thmm ] [ parametric : thmm ] and [ small - rho - mmx : thmm ] are proved in the supplemental article @xcite .",
    "the supplemental article also contains a derivation of the stein unbiased risk estimate for svst , which is instrumental in the proof of theorem  [ lf : thmm ] , and other technical auxiliary lemmas .",
    "our main object of interest , the worst - case mse of svst , @xmath215 is more conveniently expressed using a specially calibrated risk function .",
    "since the svst denoisers are scale - invariant , namely @xmath216 we are free to introduce the scaling @xmath217 and define the risk function of a denoiser @xmath115 at @xmath44 by @xmath218 then , the worst - case mse of @xmath219 at @xmath2 is given by @xmath220    to vary the snr in the problem , it will be convenient to vary the norm of the signal matrix @xmath2 instead , namely , to consider @xmath221 with @xmath222 .",
    "vectors are denoted by boldface lowercase letters , such as @xmath223 , and their entries by @xmath224 .",
    "matrices are denoted by uppercase letters , such as @xmath225 , and their entries by @xmath226 . throughout this text",
    ", @xmath8 will denote the data matrix @xmath227 .",
    "we use @xmath20 and @xmath228 to denote the set of real - valued @xmath18-by-@xmath19 matrices , and group of @xmath18-by-@xmath18 orthogonal matrices , respectively .",
    "@xmath25 denotes the frobenius matrix norm on @xmath20 , namely the euclidean norm of a matrix considered as a vector in @xmath229 .",
    "we denote matrix multiplication by either @xmath230 or @xmath231 .",
    "we use the following convenient notation for matrix diagonals : for a matrix @xmath232 , we denote by @xmath233 its main diagonal , @xmath234 similarly , for a vector @xmath235 , and @xmath236 that we suppress in our notation , we denote by @xmath237 the `` diagonal '' matrix @xmath238    we use a `` fat '' singular value decomposition ( svd ) of @xmath232 @xmath239 , with @xmath240 and @xmath241 .",
    "note that the svd is not uniquely determined , and in particular @xmath242 can contain the singular values of @xmath243 in any order .",
    "unless otherwise noted , we will assume that the entries of @xmath71 are nonnegative and sorted in nonincreasing order , @xmath244 .",
    "when @xmath245 , the last @xmath246 columns of @xmath247 are not uniquely determined ; we will see that our various results do not depend on this choice . note that with the `` fat '' svd , the matrices @xmath8 and @xmath248 have the same dimensionality , which simplifies the notation we will need .",
    "when appropriate , we let univariate functions act on vectors entry - wise , namely , for @xmath249 and @xmath250 , we write @xmath251 for the vector with entries @xmath252 .      by orthogonal invariance of the frobenius norm , ( [ svt : eq ] ) is equivalent to @xmath253 through the relation @xmath254 .",
    "it is well known that the solution to ( [ svst - singvals : eq ] ) is given by @xmath255 , where @xmath256 denotes coordinate - wise soft thresholding of @xmath257 with threshold @xmath78 .",
    "the svst estimator ( [ svt : eq ] ) is therefore given by  @xcite @xmath258    note that ( [ svt2:eq ] ) is well defined , that is , @xmath259 does not depend on the particular svd @xmath260 chosen .    in case @xmath163 , observe that the solution to ( [ svt : eq ] ) is constrained to lie in the linear subspace of symmetric matrices .",
    "the solution is the same whether the noise matrix @xmath261 has i.i.d .",
    "standard normal entries , or whether @xmath4 is a symmetric wigner matrix @xmath262 where @xmath263 has i.i.d .",
    "standard normal entries .",
    "below , we assume that the data in case @xmath163 is of the form @xmath264 where @xmath265 and @xmath4 has this wigner form , namely , the singular values @xmath257 are the absolute values of eigenvalues of the symmetric matrix @xmath8 .",
    "we now prove theorem  [ lf : thmm ] , which characterizes the worst - case mse of the svst denoiser @xmath34 for a given @xmath78 .",
    "the theorem follows from a combination of two classical gems of the statistical literature .",
    "the first is stein s unbiased risk estimate ( sure ) from 1981 , which we specialize to the svst estimator ; see also @xcite .",
    "the second is anderson s celebrated monotonicity property for the integral of a symmetric unimodal probability distribution over a symmetric convex set @xcite , from 1955 , and more specifically its implications for monotonicity of the power function of certain tests in multivariate hypothesis testing @xcite . to simplify the proof",
    ", we introduce the following definitions , which will be used in this section only .",
    "[ majorization : def ] let @xmath267 have singular value vectors @xmath268 , respectively , which as usual we assume are sorted in nonincreasing order : @xmath269 and @xmath270 . if @xmath271 for @xmath272 , we write @xmath273 .",
    "we note that by rescaling an arbitrary rank-@xmath121 matrix , it is always possible to majorize any fixed matrix of rank at most @xmath121 ( in the sense of definition  [ majorization : def ] ) .",
    "[ mu : lem ] let @xmath123 be a matrix of rank @xmath121 , and let @xmath232 be a matrix of rank at most @xmath121",
    ". then there exists @xmath274 for which @xmath275 .",
    "let @xmath276 be the vectors of singular values of @xmath277 , respectively , each sorted in nonincreasing order . then @xmath278 .",
    "take @xmath279 .",
    "for @xmath280 we have @xmath281 , and for @xmath282 we have @xmath283 .    the above weak notion of majorization",
    "gives rise to a weak notion of monotonicity :    we say that @xmath284 is an orthogonally invariant function if @xmath285 for all @xmath286 and all orthogonal @xmath287 and @xmath288 .",
    "let @xmath284 be orthogonally invariant .",
    "if , whenever @xmath273 and @xmath289 , @xmath290 satisfies @xmath291 for @xmath292 and @xmath293 , we say that @xmath290 is singular - value - monotone increasing , or _",
    "sv - monotone increasing_.    we now provide a sufficient condition for sv - monotonicity , which follows from anderson s seminal monotonicity result @xcite .",
    "the following lemma is proved in the supplemental article @xcite .",
    "[ sufficient - sv - mon - ii : lem ] assume that @xmath284 can be decomposed as @xmath294 , where for each @xmath295 , @xmath296 is a bounded , orthogonally invariant function .",
    "further assume that for each @xmath297 , @xmath298 is quasi - convex , in the sense that for all @xmath299 , the set @xmath300)$ ] is convex in @xmath20 .",
    "then @xmath290 is sv - monotone increasing .",
    "the second key ingredient in the proof of theorem  [ lf : thmm ] is the stein unbiased risk estimate for svst .",
    "let @xmath219 be a weakly differentiable estimator of @xmath2 from data @xmath301 , where @xmath4 has i.i.d .",
    "standard normal entries .",
    "the stein unbiased risk estimate @xcite is a function of the data , @xmath302 , for which @xmath303 . in our case ,",
    "@xmath304 and @xmath8 are matrices in @xmath20 , and stein s theorem ( @xcite , theorem  1 ) implies that for @xmath305 we have @xmath306    in the supplemental article @xcite , we derive sure for a large class of invariant matrix denoisers . as a result",
    ", we prove :    [ svt - sure : lem ] for each @xmath117 , there exists an event @xmath307 and a function , @xmath308 which maps a matrix @xmath8 with singular values @xmath257 to @xmath309 \\label{sure - svt - rest : eq } \\\\ & & { } -\\frac{2}{{n } } \\sum_{1\\leq i\\neq j\\leq{m } } \\frac{\\min \\ { y_j,\\lambda   \\}y_j-\\min \\",
    "{ y_i,\\lambda   \\}y_j}{y_j^2-y_i^2 } , \\label{sure - svt - div : eq}\\end{aligned}\\ ] ] enjoying the following properties :    @xmath310 , where @xmath311 is the distribution of the matrix @xmath4 with @xmath312 .",
    "@xmath313 is a finite sum of bounded , orthogonally invariant , quasi - convex functions .",
    "denoting as usual @xmath314 , where @xmath315 and @xmath312 , we have @xmath316    putting together lemmas [ sufficient - sv - mon - ii : lem ] and  [ svt - sure : lem ] , we come to a crucial property of svst .    [ svt - mon : lem ] for each @xmath117 , the map @xmath317 is a bounded , sv - monotone increasing function . in particular , let @xmath267 with @xmath318",
    ". then @xmath319    by lemma  [ svt - sure : lem ] , the function @xmath320 satisfies the conditions of lemma  [ sufficient - sv - mon - ii : lem ] and is therefore sv - monotone increasing .",
    "it follows that @xmath321 to see that the risk is bounded , note that for any @xmath232 , we have by lemma  [ svt - sure : lem ] @xmath322    proof of theorem [ lf : thmm ] by lemma  [ svt - mon : lem ] , the map @xmath323 is bounded and monotone nondecreasing in @xmath324 .",
    "hence @xmath325 exists and is finite , and @xmath326 for all @xmath327 . since @xmath328 , obviously @xmath329 and we only need to show the reverse inequality .",
    "let @xmath44 be an arbitrary matrix of rank at most @xmath121 .",
    "by lemma  [ mu : lem ] there exists @xmath330 such that @xmath331 .",
    "it now follows from lemma  [ svt - mon : lem ] and ( [ lim - larger : eq ] ) that @xmath332",
    "let @xmath78 and @xmath333 , and consider them fixed for the remainder of this section .",
    "our second main result , theorem  [ finite - n - mmx : thmm ] , follows immediately from theorem  [ lf : thmm ] , combined with the following lemma , which is proved in the supplemental article @xcite .    [ proxy : lem ] let @xmath44 be of rank @xmath121",
    ". then @xmath334 as defined in ( [ finite - n - proxy : eq ] ) , with @xmath335 for case @xmath209 and @xmath336 for case @xmath163 .    in the supplemental article @xcite",
    "we prove the following lemma :    [ finite - n - mse - convex : lem ] the function @xmath337 , defined in ( [ finite - n - proxy : eq ] ) on @xmath338 , is convex and obtains a unique minimum .",
    "our second main result is an immediate consequence :    proof of theorem [ finite - n - mmx : thmm ] let @xmath123 be an arbitrary fixed matrix of rank  @xmath121 .",
    "for case @xmath209 , by theorem  [ lf : thmm ] and lemma  [ proxy : lem ] , @xmath339 where we have used lemma  [ finite - n - mse - convex : lem ] , which also asserts that the minimum is unique .",
    "now let @xmath340 be an arbitrary , fixed symmetric positive semidefinite matrix of rank @xmath121 . for case @xmath163 , by the same lemmas , @xmath341",
    "toward the proof of our third main result , theorem  [ asymp - mmx : thmm ] , let @xmath78 be fixed . we first show that in the proportional growth framework , where the rank @xmath342 , number of rows @xmath343 and number of columns @xmath19 all tend to @xmath11 proportionally to each other , the key quantity in our formulas can be evaluated by complementary incomplete moments of a marcenko ",
    "pastur distribution , instead of a sum of complementary incomplete moments of wishart eigenvalues .",
    "[ zeta : def ] for a pair of matrices @xmath315 , we denote by @xmath344 the singular values , in nonincreasing order , of @xmath345 where @xmath346 is the projection of @xmath347 on @xmath348 and @xmath349 is the projection on @xmath350 .",
    "similarly , for a pair of matrices @xmath351 , denote by @xmath352 the eigenvalues , in nonincreasing order , of @xmath353    [ mp - asymp : lem ] consider sequences @xmath354 and @xmath355 and numbers @xmath155 and @xmath356 such that @xmath357 and @xmath358 .",
    "let @xmath359 , as in definition  [ zeta : def ] , where @xmath292 has i.i.d .",
    "@xmath105 entries .",
    "define @xmath360 and @xmath149 , and let @xmath361 .",
    "then @xmath362    write @xmath363 , and recall that by the marcenko  pastur law  @xcite , @xmath364 in the sense of weak convergence of probability measures , where @xmath365 is the marcenko  pastur probability distribution with density @xmath366 given by  ( [ mp : eq ] ) .",
    "now , @xmath367 as required .",
    "[ asymp - proxy : lem ] let @xmath343 and @xmath342 such that @xmath368 and@xmath369 , and set @xmath211 . then @xmath370 where the right - hand side is defined in ( [ proxy : eq ] ) , with @xmath371 for case @xmath209 and @xmath336 for case @xmath163 .    for case @xmath209 , let @xmath372 be an arbitrary fixed matrix of rank @xmath121 .",
    "for case @xmath163 , @xmath373 an arbitrary , fixed symmetric positive semidefinite matrix of rank @xmath121 . by theorem  [ lf : thmm ] and",
    "lemma  [ proxy : lem ] , @xmath374 \\\\ & & \\qquad= \\rho+ \\tilde{\\rho } - \\rho\\tilde{\\rho } + ( 1-\\tilde{\\rho } ) \\rho \\lambda^2 \\\\ & & \\qquad\\quad{}+ \\alpha(1-\\rho ) ( 1-\\tilde{\\rho } ) \\int_{\\lambda^2}^{\\gamma_+ } ( \\sqrt{t}-\\lambda)^2 mp_\\gamma(t)\\,dt \\\\ & & \\qquad={\\mathbf{m}}\\biggl(\\frac{\\lambda}{\\sqrt{1-\\tilde{\\rho}}};\\rho , \\tilde { \\rho},\\alpha \\biggr),\\end{aligned}\\ ] ] where we have used lemma  [ mp - asymp : lem ] and set @xmath375 .    in the supplemental article",
    "we prove a variation of lemma  [ finite - n - mse - convex : lem ] for the asymptotic setting :    [ asymp - mse - convex : lem ] the function @xmath376 , defined in ( [ proxy : eq ] ) on @xmath377 $ ] , where @xmath378 , is convex and obtains a unique minimum .",
    "this allows us to the prove our third main result .",
    "proof of theorem [ asymp - mmx : thmm ] by lemma  [ asymp - proxy : lem ] , @xmath379 with @xmath335 for case @xmath209 and @xmath336 for case @xmath163 , where we have used lemma  [ asymp - mse - convex : lem ] , which also asserts that the minimum is unique .",
    "having established that the asymptotic worst - case mse  ( [ proxy : eq ] ) satisfies ( [ asymp - mmx - mat : eq ] ) and ( [ asymp - mmx - sym : eq ] ) , we turn to its minimizer @xmath172 .",
    "the notation follows  ( [ proxy - minimizer : eq ] ) .",
    "proof of theorem [ proxy - minimizer : thmm ] by equation ( 4.2 ) in the supplemental article @xcite , the condition @xmath380 is thus equivalent , for any @xmath157 $ ] , to @xmath381 establishing ( [ argmin - lambda : eq ] ) in particular for @xmath382 . by lemma  [ asymp - mse - convex : lem ] ,",
    "the minimum exists and is unique ; namely this equation has a unique root in @xmath138 .",
    "one directly verifies that @xmath383 . the limits ( [ lim_lambda_down : eq ] ) and ( [ lim_lambda_up : eq ] ) follow from the fact that @xmath384 is decreasing .",
    "to establish this , it is enough to observe that @xmath385 for all @xmath386 , which can be verified directly .",
    "theorem  [ proxy - sq : thmm ] , which provides more a explicit formula for the minimax amse in square matrix case ( @xmath53 ) , is proved in the supplemental article @xcite .",
    "in this section we prove theorem  [ global - mmx : thmm ] , which provides a lower bound on the minimax risk of the family of all measurable matrix denoisers ( as opposed to the family of svst denoisers considered so far ) over @xmath18-by-@xmath19 matrices of rank at most @xmath121 . consider the class of singular - value matrix denoisers , namely all mappings @xmath387 that act on the data @xmath8 only through their singular values . more specifically , consider all denoisers @xmath115 of the form @xmath388 where @xmath389 and @xmath390 .",
    "( note that this class contains svst denoisers but does not exhaust all measurable denoisers . )",
    "the mapping in  ( [ sv - denoiser : eq ] ) is not well defined in general , since the svd of @xmath8 , and in particular the order of the singular values in the vector @xmath257 , is not uniquely determined .",
    "however , ( [ sv - denoiser : eq ] ) is well defined when each function @xmath391 is invariant under permutations of its coordinates .",
    "since the equality @xmath389 may hold for vectors @xmath392 with negative entries , we are led to the following definition .",
    "[ singval - denoiser : def ] by _ singular - value denoiser _ we mean any measurable mapping @xmath115 which takes the form ( [ sv - denoiser : eq ] ) , where each entry of @xmath393 is a function @xmath394 that is invariant under permutation and sign changes of its coordinates .",
    "we let @xmath395 denote the class of such mappings .    for a detailed introduction to real - valued or matrix - valued functions which depend on a matrix argument only through its singular values ,",
    "see @xcite .",
    "the following lemma is proved in the supplemental article @xcite .",
    "[ huntstein : lem ] let @xmath396 be an arbitrary measurable matrix denoiser .",
    "there exists a singular - value denoiser @xmath219 such that @xmath397    proof of theorem [ global - mmx : thmm ] we consider the case @xmath43 . by lemma  [ huntstein : lem ] , it is enough to show that @xmath398 where @xmath399 is an arbitrary singular - value denoiser .",
    "indeed , let @xmath44 be a fixed arbitrary matrix of rank @xmath121 .",
    "the calculation leading to equation ( 3.9 ) in the supplemental article @xcite is valid for any rule in @xmath395 , and implies that @xmath400 , where @xmath401 and @xmath402 write @xmath403 , and let @xmath404 .",
    "we therefore have @xmath405    combining equations ( 3.17 ) and ( 3.15 ) in the supplemental article @xcite , we have @xmath406 a similar argument yields @xmath407 , and the first part of the theorem follows . the second part of the theorem follows since , taking the limit @xmath408 as prescribed , we have @xmath191 , @xmath409 and @xmath410 . for the third part of the theorem",
    ", we have by theorem  [ small - rho - mmx : thmm ] , @xmath411",
    "in the , we pointed out several ways that these matrix denoising results for svst estimation of low - rank matrices parallel results for soft thresholding of sparse vectors .",
    "our derivation of the minimax mse formulas exposed two more parallels :    * _ common structure of minimax mse formulas . _",
    "the minimax mse formula vector denoising problem involves certain incomplete moments of the standard gaussian distribution @xcite .",
    "the matrix denoising problem involves completely analogous incomplete moments , only replacing the gaussian by the marenko ",
    "pastur distribution or ( in the square case @xmath53 ) the quarter - circle law .",
    "* _ monotonicity of sure_. in both settings , the least - favorable estimand places the signal `` at @xmath11 , '' which yields a convenient formula for minimax mse @xcite . in each setting ,",
    "validation of the least - favorable estimation flows from monotonicity , in an appropriate sense , of stein s unbiased risk estimate within that specific setting .",
    "we thank iain johnstone , andrea montanari and art owen for advice at several crucial points , and the anonymous referees for many helpful suggestions ."
  ],
  "abstract_text": [
    "<S> an unknown @xmath0 by @xmath1 matrix @xmath2 is to be estimated from noisy measurements @xmath3 , where the noise matrix @xmath4 has i.i.d . </S>",
    "<S> gaussian entries . </S>",
    "<S> a  popular matrix denoising scheme solves the nuclear norm penalization problem @xmath5 , where @xmath6 denotes the nuclear norm ( sum of singular values ) . </S>",
    "<S> this is the analog , for matrices , of @xmath7 penalization in the vector case . </S>",
    "<S> it has been empirically observed that if @xmath2 has low rank , it may be recovered quite accurately from the noisy measurement @xmath8 .    in a proportional growth framework where the rank @xmath9 , number of rows @xmath10 and number of columns @xmath1 all tend to @xmath11 proportionally to each other ( @xmath12 , @xmath13 ) </S>",
    "<S> , we evaluate the asymptotic minimax mse @xmath14.our formulas involve incomplete moments of the quarter- and semi - circle laws ( @xmath15 , square case ) and the marenko  pastur law ( @xmath16 , nonsquare case ) . for finite @xmath0 and @xmath1 </S>",
    "<S> , we show that mse increases as the nonzero singular values of @xmath2 grow larger . as a result , the finite-@xmath1 worst - case mse , </S>",
    "<S> a quantity which can be evaluated numerically , is achieved when the signal @xmath2 is `` infinitely strong . ''    </S>",
    "<S> the nuclear norm penalization problem is solved by applying soft thresholding to the singular values of @xmath8 . </S>",
    "<S> we also derive the minimax threshold , namely the value @xmath17 , which is the optimal place to threshold the singular values .    </S>",
    "<S> all these results are obtained for general ( nonsquare , nonsymmetric ) real matrices . </S>",
    "<S> comparable results are obtained for square symmetric nonnegative - definite matrices . </S>"
  ]
}