{
  "article_text": [
    "gravitational microlensing is the deflection of light by matter in the regime where multiple - imaging and high magnification events occur , but the angular separation of images is too small to resolve .",
    "the observable effect is a change in brightness of a source over a period of time , with the details of the light curve depending on the complexity of the lens distribution and the surface brightness profile of the source .",
    "gravitational microlensing has been used to study high magnification events on local scales , viz .",
    "the searches for dark matter compact objects in the galactic bulge and halo ( see for example references in and citations to @xcite ; @xcite ; @xcite ; @xcite ; @xcite ) and extrasolar planets ( e.g. @xcite ; @xcite ; @xcite ) , and quasar microlensing on cosmological scales ( e.g. @xcite ; @xcite ; @xcite ; @xcite ; @xcite ) . for recent reviews of microlensing on all scales ,",
    "see @xcite , @xcite , @xcite , @xcite and @xcite .",
    "microlensing on local scales occurs in the low optical depth regime , where the source is affected by at most a few microlenses along the line of sight .",
    "large numbers of stars must therefore be monitored in order to observe a single microlensing event .",
    "conversely , the microlensing optical depth at the location of lensed quasar images is likely to be @xmath0 .",
    "this microlensing signal is therefore more complex , as it is the result of a large number of microlenses acting on the source essentially all the time @xcite . in the remainder of this work",
    ", we focus on high optical depth microlensing .",
    "a key element in many microlensing calculations is the determination of the magnification map ; a typical example is shown in figure [ fig : map ] .",
    "this map provides an estimate of the point - source brightening over a finite region of a two - dimensional source plane .",
    "the magnification of an extended source is found by convolving a source profile ( intensity and geometry ) with the magnification map .",
    "this approach has been used to probe the structure of quasar emission regions , including accretion discs ( e.g. @xcite ; @xcite ; @xcite ) and broad emission line regions ( e.g. @xcite ; @xcite ) .",
    "the limitation in many microlensing calculations is the computational overhead in producing magnification maps with sufficient resolution and high ( statistical ) accuracy .",
    "fitting observed light - curves obtained over long periods of time requires large maps with high pixel resolution to resolve small sources ( such as quasar x - ray emission regions ) .",
    "similarly , fitting multi - wavelength observations requires both large maps and high pixel resolution in order to simultaneously model emission from small optical emission regions and larger infrared emission regions .",
    "the situation is even worse when attempts are made to simultaneously model microlensing of accretion discs and the much larger broad emission line regions , a difference in scale of 23 orders of magnitude .",
    "finally , many maps must be generated in order to ensure statistical independence of simulated measurements .",
    "it is only reasonable to sample any one magnification map as many times as you can place a source on it without overlapping .    in this paper",
    "we compare three techniques for computing magnification maps : a hierarchical tree code for single - core cpus ; a parallel , large data tree code for use on a cpu - based cluster supercomputer ; and a direct , inverse ray - shooting code for use on graphics processing units ( gpus ) .",
    "we consider the accuracy of the codes , to determine whether the approximations inherent in the tree code are valid over a wider range of parameter space than has been feasible to test before , and compare run - times between the single - core tree code and the gpu code over a range of astrophysically - motivated parameters . while the `` brute force '' approach to ray shooting has not been considered a feasible option on cpu architectures",
    ", we show that the gpu implementation is highly competitive , and is indeed faster than the tree code in many circumstances .",
    "the expected increase in gpu speeds in the years ahead bodes well for advances in gravitational microlensing simulations .",
    "our work hints at the potential benefit to other fields of astrophysical computation from the implementation of simple , `` brute force '' algorithms as a means of accelerating the adoption of gpus in astronomy for time - consuming computations .",
    "the remainder of this paper is set out as follows . in section [ sct : magmap ] , we describe the process of generating magnification maps , and summarise key features of three gravitational microlensing codes which can be used for this task . in section [ sct : results ] , we present a series of code comparisons ( accuracy and speed ) . in section [ sct : discussion ] , we discuss the relative merits of hierarchical and direct ray - shooting approaches , and suggest the classes of problems to which each is best - suited .",
    "we present our conclusions in section [ sct : conclusion ] .",
    "in general , magnification maps are calculated using the ( two - dimensional ) gravitational lens equation @xmath1 which relates the locations of the source , @xmath2 , and an image , @xmath3 , via the deflection angle , @xmath4 .",
    "the magnification is : @xmath5 where @xmath6 is the jacobian matrix of equation ( [ eqn : lens ] )",
    ".    equation ( [ eqn : lens ] ) gives a one - to - one relationship between an image location and the corresponding source location ( @xmath7 ) , but a one - to - many relationship for the alternative case of mapping a source location to its multiple images ( @xmath8 ) .",
    "while analytic solutions exist for a small number of simple lens models ( e.g. @xcite ) , in general a numerical approach is used to calculate @xmath9 over a finite region of the source plane .",
    "this can be achieved most easily using inverse ray - shooting ( @xcite ; @xcite ; @xcite ) . here",
    ", light rays are propagated backwards from the observer , through the lens plane , where they are deflected using equation ( [ eqn : lens ] ) , and then mapped to a grid of pixels in the source plane .",
    "this approach avoids the need to determine the location of every image . by counting the number of light rays reaching",
    "each source plane pixel , @xmath10 , compared to the average per pixel if there was no lensing , @xmath11 , an estimate of the per - pixel magnification , @xmath12 , is obtained : @xmath13    for cosmological microlensing problems , the appropriate model for the deflection angle describes @xmath14 compact lens masses in a smooth background mass distribution , acted on by an external ( macro - model ) shear @xmath15 .",
    "this model , an extension of the one proposed by @xcite , is suitable for considering the impact of lensing due to individual stars where the external shear is due to the large - scale galactic mass distribution .",
    "the lens equation in this case is : @xmath16 where the convergence @xmath17 combines contributions from smooth matter @xmath18 , and compact objects @xmath19 .",
    "this is the model for the deflection of light by a screen of mass that we will use throughout this paper .",
    "the relationship between @xmath19 and the number of lenses @xmath20 is : @xmath21 if the deflecting mass distribution were smoothed out , a circle in the image plane would map onto an ellipse with major - to - minor axis ratio @xmath22 in the lens plane .",
    "the area of this ` minimum ' ellipse is set by the size of the image plane . in practice , the graininess of the matter distribution can cause rays to be scattered outside the receiving area .",
    "similarly , rays from a long way outside the shooting region can be scattered into the receiving square in the source plane .",
    "for that reason , the lenses in the codes considered here are distributed in a circle of area @xmath23 with a diameter larger than the long axis of the minimum ellipse .",
    "the number of calculations , @xmath24 , required to obtain a mangification map with @xmath25 pixels scales as : @xmath26 where @xmath27 is the number of floating point operations for each deflection calculation , and @xmath11 is chosen to achieve a desired level of accuracy .",
    "it was quickly realised that the direct ray - shooting approach was not practical on single cpus : a scenario , @xmath28 , with @xmath29 pixels , @xmath30 , @xmath31 lenses , ] and @xmath32 would take months to years to calculate @xcite on hardware that was available at the time .",
    "wambsganss ( 1990 ; 1999 ) introduced a method based on a hierarchical tree code @xcite . here ,",
    "the contribution of each lens depends on its distance from the light ray  lenses that are further away are grouped together into pseudo - lenses with a higher mass .",
    "this approach reduces @xmath14 , thus increasing the speed of the overall calculation , but at the cost of a slight error in the magnification map due to the approximation .",
    "this approach has been used to constrain both the size and temperature profile of quasar accretion discs ( e.g. @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ) , dark matter fractions in the lensing galaxies ( e.g. @xcite ; @xcite ) , and possibly even the orientation of the quasar with respect to the line of sight @xcite .    despite the much - improved speed compared to direct ray - shooting , the hierarchical method is complex to implement , and requires a certain amount of expert knowledge .",
    "these facts make slight modifications or additions to the code quite challenging .",
    "parameters such as the required accuracy ( which determines whether to use a particular lens cell , or its four subcells , based on the distance between the ray and the cell s centre of mass ) must be determined experimentally by repeatedly running the code until acceptable results are obtained .",
    "additionally , the generation of the tree data structure imposes a memory overhead which exceeds typical single cpu memory sizes for large @xmath20 . a parallel hierarchical tree code , suitable for running billion - lens calculations , has now been developed by @xcite .",
    "this approach overcomes the single - cpu memory limitation by distributing the computation over a number of nodes in a cpu - based cluster supercomputer .",
    "alternatives to the hierarchical method include the use of polygonal cells @xcite , optimised to map areas of the image plane to the source plane thus reducing @xmath11 required for a given accuracy , and the contouring method for point sources ( @xcite ; @xcite ) , which maps an infinite line plus @xmath14 closed loops from the image to source plane .",
    "the polygonal cell approach results in its own computational overhead in determining the correct polygonal lattice , and suffers from the need for a substantial number of polygons in the vicinity of caustics , which results in lengthy computation times .",
    "the contouring method has limited applicability to extended sources .",
    "@xcite presented an extension of the contouring method to cover extended sources , however the additional computations required slowed the code to the point where it could no longer compete with the simplicity of inverse ray - shooting .",
    "a recent alternative is the appearance of the gpu as a mathematical co - processor .",
    "originally developed to enhance the rate of generating graphics for the computer game industry , gpus are revolutionising scientific computing ( @xcite ; @xcite ; @xcite ; @xcite ) .",
    "their highly parallel processing architecture , accessed through flexibile programming libraries such as nvidia s cuda or the cross - platform opencl standard created by the khronos group , is enabling speed - ups of @xmath33 to @xmath34 times over a broad range of algorithms .",
    "indeed , the notion of general purpose computing on graphics processing units ( gpgpu ) has motivated a reinvestigation of time - consuming processing tasks .",
    "early adoption of gpgpu in astronomy has included @xmath35-body simulations ( e.g. , @xcite ) , radio - telescope signal correlation ( e.g. , @xcite ) , the solution of kepler s equations @xcite , radiation - transfer models @xcite and adaptive mesh refinement simulations @xcite .    from a conceptual and implementational point of view , direct ray - shooting is a very simple algorithm with a high degree of parallelism : the deflection of each light ray is independent of every other light ray , and the deflection of a light ray due to a single lens is independent of all other lenses .",
    "the existence of such fine - grained parallelism means that ray - shooting can be considered an `` embarassingly parallel '' algorithm ",
    "ideal for gpgpu .",
    "@xcite reported on a cuda implementation of direct ray - shooting , and demonstrated that their code could calculate a magnification map for the @xmath36 scenario in @xmath37 hours using a four - gpu nvidia s1070 tesla unit .",
    "moreover , they were able to calculate a magnification map with one billion lenses ( @xmath38 and @xmath39 ) in 24 hours , with a peak sustained processing rate of 1.28 tflop / sec . while the cuda implementation of direct ray - shooting gave an @xmath34 processing speed - up compared to using the same method on a single cpu , a critical comparison omitted from @xcite was between the tree code and the gpu code .    in this paper , we compare :    * the `` industry standard '' single - core tree code by wambsganss ( 1990 ; 1999 ) , which we refer to as cpu - t ; * the parallel , large - data tree code by @xcite , which we refer to as cpu - p ; and * the cuda direct ray - shooting code by @xcite , which we refer to as gpu - d .",
    "we now describe salient points of these three approaches in more detail , including implementation issues that are relevant for the code comparison in section [ sct : results ] .",
    "the most computationally intensive part of the ray - shooting algorithm is the calculation of the deflection of individual rays by each lens .",
    "the hierarchical tree code attempts to minimise the number of calculations necessary by grouping together lenses that are sufficiently distant from a given light ray .",
    "a group of lenses is treated as a single pseudo - lens with a total mass equal to the sum of the individual lens masses , located at their centre of mass .",
    "the first step in the calculation is to divide up the lenses into a cell structure , starting with the root cell , which is the size of the lens plane .",
    "each cell that contains more than one lens is subdivided into four cells with half the side length of the parent cell .",
    "this process is continued iteratively until only cells containing zero or one lens remain .",
    "this calculation need only be carried out once per magnification map , and typically takes only a small fraction of the code s total run - time .",
    "a determination must be made for each light ray regarding which lenses need to be included individually and which can be grouped together into pseudo - lenses .",
    "the lens equation then becomes : @xmath40 where @xmath41 is the number of lenses to be included directly , and @xmath42 is the number of pseudo - lenses with total mass @xmath43 and centre of mass position @xmath44 .",
    "we note that the actual implementation of the tree code uses higher order multipoles of the mass distribution in order to increase accuracy .",
    "this calculation scales on average as @xmath45 .",
    "additional techniques are used in the standard hierarchical tree code in order to improve the accuracy and speed of the calculation .",
    "for example , `` test rays '' are used to determine the cell structure , and then the same cells are used for a number of actual rays surrounding the test ray .",
    "also , the deflection due to pseudo - lenses is only calculated directly for the test rays ; for the actual rays , pseudo - lens deflections are interpolated between those calculated for the test rays . for full details , see @xcite .",
    "a tree code requires the following data structures to be stored in memory : 2-d magnification map array ; lens array ( mass and 2-d location for each lens ) ; and the cell tree ( stored as an array of indices ) , which provides a significant memory overhead as @xmath20 increases . in order to increase the scale of microlensing simulations beyond those that were achievable with cpu - t , where typical single - core memory permits a maximum of  20 million lenses , @xcite implemented a distributed , parallel version of the tree code ( cpu - p ) .",
    "they take advantage of the inherent parallelism of the ray deflections by distributing rays between multiprocessors of a cpu - based cluster supercomputer , using a simple but effective approach to static load balancing between computing nodes . to handle the large memory requirements for billion - lens simulations ( @xmath46 gb ) , they use a combination of local memory , distribution of data between nodes ( thus sharing the memory requirements ) , and disk - based storage ( although this limits processing speeds somewhat by the requirement of costly file accesss ) .",
    "the cpu - p code implements the same cell - tree and microlensing algorithms as cpu - t , but surrounded by a quite different environment within a running simulation .",
    "the generation of lens masses at the beginning of the run , and the ray - shooting itself , use as much parallelism as is available .",
    "the generation of the cell tree has not been parallelised , but is executed concurrently with another phase in the simulation .",
    "all these operations are logically the same as in the cpu - t version but execute without knowledge that they are running concurrently , or that their data may be contained within huge disk files .",
    "disk files naturally degrade performance and parallel speed - up , but there are various caching schemes which can make up for that .",
    "cpu - p has been built to have the same accuracy as cpu - t , because the same numerical algorithms are used , however due to the reimplementation in a different programming language ( fortran for cpu - t , c++ for cpu - p ) , with different random number generators , and a different precision of integers ( arbitrary precision has been implemented ) , there will naturally be minor differences .",
    "the gpu parallel implementation of direct ray - shooting is achieved by splitting the total number of light rays into @xmath47 batches , which are deflected in parallel . for convenience",
    ", the deflection calculation of equation ( [ eqn : sigma ] ) is recast as : @xmath48 where @xmath49 for @xmath50 parallel threads , and each processing step considers only a single light ray .",
    "the contributions from @xmath15 and @xmath18 are determined at the end of the deflection calculation .",
    "the gpu - d code , described in thompson et al .",
    "( 2010 ) , was implemented in cuda according to the following algorithm :    1 .",
    "@xmath20 lens coordinates are obtained ( see below ) and stored in cpu memory .",
    "coordinates are loaded into gpu device memory as required ; 2 .",
    "@xmath51 light ray coordinates are pseudo - randomly generated on the cpu and loaded into gpu device memory ; 3 .",
    "gpu computation is initialised ; computation is split into groups of @xmath52 threads ; 4 .",
    "each thread group loads @xmath50 lenses and calculates deflection on @xmath50 rays ; this is repeated until all lenses and rays are exhausted ; 5 .",
    "once computation on the gpu is complete , results are copied back from the gpu device memory to system memory ; 6 .",
    "cpu maps the ray locations onto the source pixel grid in order to obtain the magnification map ; 7 .",
    "steps 2 - 6 repeated until an average of @xmath11 rays per source pixel is reached .    the only change in approach from that of thompson et al .",
    "( 2010 ) is the way the @xmath14 lenses coordinates are generated : for this work , we first run a tree code and output lens positions to a data file .",
    "these coordinates are then read from the data file by the gpu - d code , and subsequent processing is carried out as described above . without this modification , we would not be able to make pixel - by - pixel comparisons of the magnification maps , and would be left with just statistical comparisons based on , for example , magnification probability distributions . for full details on choice of optimisation of gpu parameters , especially @xmath47 and @xmath50 ,",
    "see @xcite , as these are somewhat hardware dependent .",
    "in this section , we consider the relative accuracies of cpu - t , cpu - p and gpu - d over a much wider range of parameter space than has been considered previously , and we compare processing times between cpu - t and gpu - d , noting regions of parameter space where each code might currently be preferred .",
    "the former aspect is important in assessing the validity of all microlensing results that have relied on the tree code to date , the latter in motivating a choice of code for future applications .",
    "the choice of model parameters for comparison is somewhat arbitrary .",
    "we attempt to cover the majority of @xmath53 and @xmath15 parameter space typically probed by realistic lens models , as well as typical map sizes and pixel scales used in those analyses ( see for example : @xcite ; @xcite ; @xcite ; @xcite ; @xcite ) .      since the tree code uses an approximation to the lens locations , in some sense the direct approach produces a magnification map that is more accurate for a given configuration of lenses .",
    "an unanswered question is whether the hierarchical approach maintains sufficient accuracy as the number of calculations , @xmath24 , is increased beyond approximately @xmath54 .",
    "this restriction is largely historical , due to the impracticality of using the direct ray - shooting approach on a single - core cpu beyond this limit .",
    "we conducted simulations for a range of parameters , summarised below :    * @xmath55 , @xmath56 and @xmath57 ; * @xmath58 , @xmath59 , @xmath60 , @xmath61 and @xmath62 ; and * @xmath63 in increments of @xmath64 , with @xmath65 and @xmath66 .",
    "this corresponds to @xmath67 , @xmath68 , @xmath69 , @xmath70 , @xmath71 , @xmath72 , @xmath73 , @xmath74 and @xmath75 .",
    "* @xmath76 in increments of @xmath77 , with @xmath78 and @xmath66 .",
    "this corresponds to @xmath79 , @xmath80 , @xmath81 , @xmath82 , @xmath83 , @xmath84 , @xmath85 , @xmath86 , @xmath87 , @xmath88 and @xmath89 .",
    "* @xmath90 in increments of @xmath91 and @xmath92 , where the smooth matter fraction @xmath93 , @xmath94 and @xmath95 .",
    "these parameter choices allow us to probe the range : @xmath96 where we assume that @xmath32 .",
    "we choose all lenses to have the same lens mass as this does not influence the run - time .",
    "the cpu - t / cpu - p accuracy parameter was set to 0.60 for all of our simulations , as this was found to reliably produce acceptable magnification maps .",
    "cpu - p maps were generated using a minimum of 2 compute nodes , to ensure that the parallelisation was not producing inaccurate results .",
    "there is a subtle difference between the way @xmath11 is defined for cpu - t / cpu - p and gpu - d . in both codes ,",
    "the same number of rays are shot across the entire shooting grid .",
    "the cpu - t / cpu - p rays are shot on a regular grid , thus in the absence of lensing exactly @xmath11 are received at each cpu - t / cpu - p pixel .",
    "the gpu - d rays are shot from random positions in the shooting grid , and so _ on average _ @xmath11 are received at each gpu - d pixel .",
    "this will introduce poisson noise into the gpu - d magnification maps , however the total number of rays shot is so large ( @xmath97 to @xmath98 in our simulations ) that any error introduced by shooting rays randomly will be insignificant .",
    "we use two methods for assessing accuracy : the root mean square difference between corresponding magnification maps , and a kolmogorov - smirnov ( k - s ) test on probability distributions for change in magnitude @xmath99 . the k - s test results in a measurement of the significance with which we can reject the null hypothesis , that the two samples were drawn from the same distribution .",
    "change in magnitude is calculated from the ( per pixel ) map magnifications using the following relationship : @xmath100 throughout , we define `` excellent agreement '' as a root mean square difference between maps of less than 0.2 magnitudes ( often much less than 0.2 magnitudes ) , and a k - s test probability that the null hypothesis is not rejected of greater than @xmath101 .",
    ".comparison between the cpu - t , cpu - p and gpu - d codes for magnification maps with @xmath102 , @xmath65 and @xmath103 ( corresponding to @xmath66 ) .",
    "@xmath104 is the root mean square difference between magnification maps ( compared pixel by pixel ) , and @xmath105 is the k - s test probability .",
    "the cpu - t and cpu - p comparisons are virtually indistinguishable .",
    "[ tbl : res1 ] [ cols=\"^,^,^,^,^,^ \" , ]      to begin with , we set both the external shear , @xmath15 , and the smooth matter fraction , @xmath106 , to zero .",
    "this allows us to consider just the effect of compact objects , which are directly related to the number of lenses via equation ( [ eqn : nstar ] ) .",
    "later , we allow both @xmath15 and @xmath107 to vary .    as indicative results , we present the calculated rms values and k - s probabilities , @xmath105 , for the @xmath102 case in table [ tbl : res1 ] . for @xmath108",
    ", we find excellent agreement between the magnification maps from cpu - t and gpu - d when @xmath109 .",
    "the @xmath110 and 0.2 cases differ slightly . here , a low number of lenses ( @xmath111 and @xmath68 respectively ) mean that there is very little variation in magnification across the maps .",
    "the k - s test probabilities are therefore strongly affected by small differences in magnification probability distributions . for @xmath55 and @xmath56 , @xmath112 are sufficient to obtain agreement between cpu - t and gpu - d . for @xmath102",
    "even @xmath113 are not enough . in other words ,",
    "a larger number of rays per pixel are required for agreement between cpu - t and gpu - d when @xmath53 is low ( @xmath114 ) .",
    "we note also that for a given @xmath53 and @xmath11 , the rms error tends to increase with increasing @xmath25 .",
    "for example , when @xmath115 and @xmath116 , we obtain the following rms differences : 0.039 mag ( @xmath117 ) , 0.081 mag ( @xmath118 ) and 0.168 mag ( @xmath119 ) .",
    "in other words , for a fixed accuracy we must increase @xmath11 by a factor of @xmath120 if we increase @xmath25 by a factor of 4 .",
    "for the comparisons between cpu - p and gpu - d , we find the same results .",
    "in fact , the cpu - t and cpu - p maps are virtually indistinguishable down to the accuracy of our comparison .",
    "they therefore behave identically when compared with gpu - d .",
    "we then fixed the convergence at @xmath121 , and examined the impact of varying shear over the range @xmath76 in increments of @xmath122 .",
    "the number of pixels was fixed at @xmath123 . once again , the smooth matter fraction @xmath107 was set to zero .",
    "we find very similar results to the no - shear case discussed above : for @xmath109 , the results of cpu - t and gpu - d are indistinguishable ( rms difference @xmath124 magnitudes and k - s probability @xmath125 ) , and similarly for comparisons between cpu - p and gpu - d .      finally , we test the accuracy of cpu - t and cpu - p when the convergence is split into a continuously distributed component @xmath18 and a compact stellar component @xmath19 .",
    "we set @xmath126 and @xmath127 , and vary @xmath107 in the range @xmath128 in increments of @xmath91 . since there is no microlensing if @xmath129 , we also simulated the @xmath130 case . for convenience ,",
    "we choose @xmath123 .    for @xmath131",
    ", we find the same results as the two cases discussed above : for @xmath109 the results of cpu - t and gpu - d are in excellent agreement .",
    "the @xmath132 case has a slightly higher rms difference at @xmath30 of @xmath133 , although the k - s probability is still @xmath134 . for the @xmath130 case",
    ", however , no agreement is reached between cpu - t and gpu - d .",
    "visual inspection of the maps shows that too few rays have been shot in the cpu - t case .",
    "this suggests that more rays should be shot in cpu - t for higher smooth matter fractions ; @xmath135 should be sufficient for @xmath136 , however more than @xmath137 is required in the @xmath130 case . once again , cpu - p performs identically to cpu - t .    by comparing the accuracy of the two tree codes , which rely on a series of approximations to the lens distribution in order to speed up calculation time , with gpu - d , which conducts the entire brute - force calculation ,",
    "we have verified the accuracy of the standard tool for high optical depth microlensing simulations .",
    "indeed , we have considered a wider range of parameters , as represented through the quantity @xmath24 , than previous work ( @xmath138 ) . while not unexpected , our results confirm that the tree code can indeed be used with confidence , provided a sufficiently large number of rays per pixel are shot ( i.e. typically @xmath139 for the parameter space explored here ) .",
    "having established accuracy of the three codes under consideration , we now turn to timing tests . specifically , we determine the time taken to generate magnification maps with a given set of model parameters , allowing us to determine the regions of @xmath140 parameter space where one of the implementations may be favoured .",
    "we do not consider timing comparisons between cpu - t and cpu - p , as these are reported in garsden & lewis ( 2010 ) . since cpu - p run - times scale with the number of nodes available , it is difficult to establish which configuration provides the most appropriate comparison between cpu - p and gpu - d . using the hardware cost as a factor in choosing between software alternatives , we suggest that the cost of purchasing a suitable high - end graphics card plus computer is much lower than the set - up costs for a modest cpu - based computing cluster , once networking overheads and cluster management are considered .",
    "we reflect further on the implications this has for the future of microlensing computations in section [ sct : discussion ] .",
    "care must be taken when quoting absolute speeds for different hardware  we use our results as a guide to the relative processing speeds of cpu - t and gpu - d , but note that alternative hardware can change timing by factors of a few .",
    "our benchmarks were conducted on the following equipment :    * * cpu - t * intel xeon dual core 3060 cpu ( 2.4 ghz ) with 1 gb ram . *",
    "* gpu - d * nvidia s1070 tesla unit connected via dual pcie x8 , or higher , buses to an intel q6600 quad core cpu ( 2.4 ghz ) with gb ram .",
    "the tesla s1070 comprises four gpus , each with 240 stream processors , running at 1.296 ghz .",
    "each gpu is able to access 4 gb of ram .",
    "the quoted peak processing rate is 2.488 tflop / s , however , for gpu - d , a peak of 1.28 tflop / s was obtained using all four gpus ( thompson et al .",
    "we note that the majority of our timing benchmarks in the present work were conducted using only two of the four gpus available on the tesla s1070 .    since we have determined that the magnification maps produced by cpu - t are accurate for @xmath141 across the majority of parameter space , we conduct our timing tests by comparing the total time taken by cpu - t and gpu - d to generate identical maps with @xmath142 , for each combination of parameters",
    ". the results of these tests are discussed below .",
    "timing results for @xmath55 , @xmath56 and @xmath57 , with @xmath143 and @xmath144 , are plotted in figure [ fig : convergence ] .",
    "each panel shows both @xmath53 ( along the top axis ) , and the logarithm of @xmath20 ( along the bottom axis ) .",
    "note that as @xmath144 for these simulations , @xmath145 .",
    "the performance of cpu - t is essentially constant as @xmath53 increases .",
    "the same is not true for gpu - d ; it performs better for lower @xmath53 ( lower @xmath20 ) values .",
    "this result is as expected ; as the number of lenses is increased , the direct code is required to perform correspondingly more calculations , whereas the approximations in the tree code ( specifically the depth to which the cell tree is accessed ) mean it performs roughly the same number of calculations at each step .",
    "the shape of the gpu - d timing curves is the same as those presented in figure 2 of @xcite . for low numbers of lenses ( @xmath146 ) , overheads induced by the need to transfer data between gpu and cpu",
    "affect the performance of gpu - d . for @xmath147 , near the limit of our comparison , the performance of the code scales linearly with @xmath20 .",
    "while this linear growth suggests that there is a cross - over in run - time between cpu - t and gpu - d at around @xmath148 , it must be recalled that this is approaching the maximum lens limit of cpu - t , due to the memory requirements for storing the cell tree .",
    "we note that although the performance of gpu - d is decreasing as @xmath53 ( or equivalently , @xmath20 ) is increased , it is still faster than cpu - t for all of our @xmath149 simulations . using all four gpus on the tesla s1070 unit , instead of the two we use for this detailed timing comparison , results in factor of two speed - up .",
    "real lensing galaxies are unlikely to be perfectly spherically symmetric , and so realistic lens models do not contain zero external shear .",
    "we therefore also conducted timing tests for varying @xmath15 .",
    "equation [ eqn : sigma ] shows that the actual calculation of the effect of external shear should not significantly add to computation time ",
    "it simply adds one extra operation to the calculation of the deflection of each ray .",
    "however , the side lengths @xmath150 of the rectangular shooting grid that is projected on to a square magnification map with side length @xmath151 depends on the shear in the following way :    @xmath152    when @xmath153 or @xmath154 , one side of the shooting grid can be very large .",
    "this leads to a large area for the lens plane , and hence a correspondingly large number of lenses ( see equation [ eqn : nstar ] and the discussion following it ) .",
    "we conducted timing tests for the case where @xmath135 , @xmath123 , @xmath144 and @xmath155 , with @xmath156 and @xmath122 .",
    "the results of these tests are presented in figure [ fig : gamma ] .",
    "the figure shows @xmath15 versus the logarithm of the time taken to generate the map .",
    "we also plot time against the theoretically predicted average magnification @xmath157 in that figure , where @xmath157 is given by : @xmath158 a negative magnification is interpreted as a parity flip in the image .    for low magnifications",
    "( when @xmath159 and @xmath160 are very different from zero ) , gpu - d is faster than cpu - t .",
    "there is a crossover point at a magnification of @xmath161 where cpu - t begins to out - perform gpu - d . for high magnifications ,",
    "cpu - t is significantly faster than gpu - d .",
    "we emphasise again that these timing tests were conducted using only two of the available four gpus . running on all four gpus",
    "should improve the performance of gpu - d by approximately a factor of two .",
    "the timing behaviour of gpu - d with the addition of a non - zero shear is exactly the same as the zero shear case .",
    "initially , overheads dominate the code s performance , but as the number of lenses is increased to @xmath162 , the code begins to scale linearly with @xmath20 .",
    "the same is not true for cpu - t  the timing behaviour in the zero shear and varying shear cases seems to be quite different .",
    "this is due to the way in which the shooting grid is stretched by the addition of a non - zero shear .",
    "equation [ eqn : square ] shows that for certain combinations of @xmath53 and @xmath15 the shooting grid will be significantly stretched in one direction .",
    "a ray shot from one corner of this stretched grid is close to fewer lenses than a ray shot from a corner of a square with the same area and thus a greater number of lenses can be grouped together into pseudo - lenses .",
    "although @xmath20 goes up for these cases , the number of lenses that need to be included directly in the calculation goes _ down _ for the tree code .",
    "we expect this behaviour to be generic as @xmath53 is varied ",
    "cpu - t will be faster than gpu - d in the high magnification regime on current - generation hardware , whereas gpu - d will out - perform cpu - t in the low magnification regime .",
    "however , as we discuss in section [ sct : discussion ] , the former situation may only be a limit in the short - term .",
    "to provide a physical context for these results , the @xcite model for image @xmath23 in the lensed quasar has @xmath163 and @xmath164 , and a corresponding theoretical magnification @xmath165 .",
    "this is in line with most models for image a , which typically have @xmath166 , @xmath167 and @xmath168 ( e.g. wyithe , agol & fluke 2002 ) . in comparison , the @xcite model for the high magnification image @xmath169 in",
    "has @xmath170 , @xmath171 and a magnification of @xmath172 .      finally , we present the results of the timing tests for varying smooth matter fraction in figure [ fig : smooth ] .",
    "tests were conducted for @xmath123 , @xmath173 , @xmath95 , and @xmath142 .",
    "this combination of @xmath53 and @xmath15 corresponds to the `` worst case '' magnification for gpu - d timing , and the `` best case '' magnification for cpu - t timing ( see the previous section ) .",
    "increasing the smooth matter fraction for fixed @xmath53 corresponds to decreasing @xmath20 , as mass is transferred from the compact stellar component to the continuously distributed component .",
    "we therefore see exactly the timing behaviour we expect : cpu - t takes roughly the same amount of time for each @xmath107 , whereas gpu - d is faster for fewer lenses .    at the distances from the centre of lensing galaxies where",
    "lensed images are typically observed ( @xmath174 kpc ) , we expect the smooth matter fraction to be quite high .",
    "indeed , a few rough microlensing measurements of smooth matter percentage at lensed image positions exist : 80% to 85% at the location of the @xmath23 and @xmath169 images in @xcite ; @xmath175 in @xcite ; @xmath176 in @xcite ; and @xmath177 in @xcite . for smooth matter fractions in this range , we find that gpu - d already performs comparably to cpu - t in high magnification systems , and out - performs it in low magnification systems .",
    "we note that our accuracy tests in section [ sub : smooth_acc ] showed that even @xmath178 were not sufficient for cpu - t to obtain results indistinguishable from gpu - d at @xmath130 , at least with the cpu - t accuracy parameter used throughout this paper .",
    "the @xmath130 point in the figure [ fig : smooth ] cpu - t results is therefore likely to be lower than the actual run - time required to produce a useable map .",
    "our intention in this work is not to imply that the tree code is excessively inaccurate .",
    "however , its reliance on an approximation to the lens distribution to reduce the number of calculations per magnification map warrants further investigation that has not been feasible until now .",
    "the standard tree code ( wambsganss 1990 ; 1999 ) has been in widespread use throughout the lensing community since the early 1990s .",
    "though it is unsurprising , it is nevertheless reassuring to know that cpu - t remains accurate over an extended range of @xmath24-space .",
    "our comparison here demonstrates that researchers can , and should , continue to use the tree code for microlensing simulations with confidence . additionally , we have demonstrated that this accuracy is also maintained by the cpu - p code , which enables exploration of problems orders of magnitude beyond those that can be achieved on a single - core cpu .",
    "however , we stress that the direct inverse ray - shooting code _ is _ more accurate than its tree code counterparts by simple virtue that it includes contributions from all lenses without approximations  the entire calculation is done by brute force , taking advantage of the massively parallel gpu architecture to complete the computation in a reasonable timeframe . while cpu - t can indeed be used for a wide range of astrophysically relevant scenarios , gpu - d is now a highly competitive alternative .",
    "given that we have verified the accuracy of the tree codes , though , it is reasonable to ask in which areas of parameter space each code should be used",
    ". the larger the number of lenses , the better the tree codes perform relative to gpu - d . for the values of @xmath25 and @xmath11 we examined in our benchmarking , the crossover point where cpu - t begins to out - perform",
    "its gpu - based counterpart is @xmath179 lenses .",
    "this can occur either when @xmath180 or @xmath181 are close to 1 . for scenarios with @xmath182 and @xmath183",
    ", there are advantages in moving to a distributed cpu - based computing cluster , although this can be a financially costly approach .",
    "one of the perceived benefits of gpu computing is the heavily reduced dollar per tflop / s they offer , which makes them an attractive alternative as `` desktop '' supercomputers ( see @xcite ) .",
    "the downside of gpus , at this point in time , is the paucity of astronomy codes that have been adapted to run on them , whereas cpu - based computing clusters can support a much wider range of existing codes .    since about 2005 , single - core cpu processing speeds have plateaued .",
    "additional computing performance has been achieved , and indeed moore s law growth in processing power has been maintained , by moving to multi - core cpus .",
    "a growing range of applications across a variety of scientific disciplines are now taking advantage of the `` many - core '' processing architecture of gpus , which are providing a preview of the future of cpu architectures . yet",
    "if gpus do indeed point towards a future where many - core processing is mainstream , the need to convert existing trusted , tested , accepted codes will become more pressing .",
    "a question that many researchers will have to answer is which code should be adapted ?",
    "we suggest that decisions to move other standard astronomy computations to gpu may benefit from a similar approach to thompson et al .",
    "( 2010 ) . rather than attempting to directly port an existing advanced code ( i.e. cpu - t ) to gpu",
    ", they considered a brute force approach that was not feasible for single - core cpu , but which exhibited the type of fine - grained , massive parallelism that is ideal for gpgpu .",
    "this resulted in much faster code development time , despite the learning curve associated with adopting cuda and the specifics of gpu parallel programming .",
    "moreover , we have shown that in most astrophysically - motivated cases , we are already _ no worse off _ using gpu - d compared with cpu - t , with the added benefit of greater intrinsic accuracy .",
    "the @xmath20-crossover in timing tests that we have identified does depend on the hardwared used .",
    "for example , our benchmarking was performed using two of the four gpus available on the tesla s1070 , while the cpus used for cpu - t were a few years old .",
    "the key point , however , is that the gpu - d is already parallel  gpu performance will increase , and more gpu cores will be placed in individual machines , yet no further code development is required .",
    "the single - core tree code , however , is more heavily restricted ",
    "cpu - t can no longer make significant processing gains by simply relying on moore s law to make single - core cpu hardware faster .",
    "in this work , we have examined the accuracy of three highly competive alternatives for generating microlensing magnification maps : a widely - used hierachical tree code for single - core cpus ; a parallel , large data implementation for a distributed cpu - based computing cluster ; and a brute force solution that utilises the massive parallelism of modern graphics processing units .",
    "we have demonstrated that these three codes do indeed present comparable accuracy , and choice of approach will depend on considerations relating to the scale and nature of the problem to be investigated .    while considered impractical as a calculating approach even a few years ago",
    ", our timing tests show that direct , inverse ray - shooting is now a viable , and indeed more accurate , option . moving a simple algorithm",
    "to gpu , rather than attempting to directly implement a more advanced solution  originally developed to overcome a hardware restriction of single - core cpus  to achieve run - times that are already similar , suggests an approach that other early adopters of gpus for astronomy computation may wish to consider .",
    "we wish to thank alex thompson for the original gpu - d code development .",
    "we are extremely grateful to joachim wambsganss for providing his microlensing tree code , which continues to be of great value to the gravitational microlensing community .",
    "cjf thanks david barnes for valuable discussions relating to the philosophy of gpu programming .",
    "this research was supported under the australian research council s discovery projects funding scheme ( project number dp0665574 ) .",
    "alcock , c. , akerlof , c.w . ,",
    "allsman , r.a . , axelrod , t.s .",
    ", bennett , d.p . , chan , s. , cook , k.h . ,",
    "freeman , k.c . ,",
    "griest , k. , marshall , s.l . , park , h.s . ,",
    "perlmutter , s. , peterson , b.a . ,",
    "pratt , m.r . ,",
    "quinn , p.j . , rodgers , a.w . , stubbs , c.w .",
    ", sutherland , w. , 1993 , nature , 365 , 621                                                                                                              wyrzykowski , l. , kozowski , s. , skowron , j. , belokurov , v. , smith , m.  c. , udalski , a. , szymaski , m.  k. , kubiak , m. , pietrzyski , g. , and soszyski , i. , szewczyk , o. , ebru , k. , 2009 , mnras , 397 , 1228"
  ],
  "abstract_text": [
    "<S> to assess how future progress in gravitational microlensing computation at high optical depth will rely on both hardware and software solutions , we compare a direct inverse ray - shooting code implemented on a graphics processing unit ( gpu ) with both a widely - used hierarchical tree code on a single - core cpu , and a recent implementation of a parallel tree code suitable for a cpu - based cluster supercomputer . </S>",
    "<S> we examine the accuracy of the tree codes through comparison with a direct code over a much wider range of parameter space than has been feasible before . </S>",
    "<S> we demonstrate that all three codes present comparable accuracy , and choice of approach depends on considerations relating to the scale and nature of the microlensing problem under investigation . on current hardware , </S>",
    "<S> there is little difference in the processing speed of the single - core cpu tree code and the gpu direct code , however the recent plateau in single - core cpu speeds means the existing tree code is no longer able to take advantage of moore s law - like increases in processing speed . instead </S>",
    "<S> , we anticipate a rapid increase in gpu capabilities in the next few years , which is advantageous to the direct code . </S>",
    "<S> we suggest that progress in other areas of astrophysical computation may benefit from a transition to gpus through the use of `` brute force '' algorithms , rather than attempting to port the current best solution directly to a gpu language  for certain classes of problems , the simple implementation on gpus may already be no worse than an optimised single - core cpu version .    </S>",
    "<S> gravitational lensing , methods : numerical </S>"
  ]
}