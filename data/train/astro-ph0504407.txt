{
  "article_text": [
    "large - scale @xmath0-body simulations , in which the equations of motion of @xmath0 particles are integrated numerically , have been extensively used in the studies of galaxies and cosmological structure formations . at present ,",
    "such simulations are in most cases performed with fast and approximate algorithms that reduce the calculation cost from @xmath1 of the direct summation .",
    "examples of such algorithms include p@xmath2 m method ( efstathiou and eastwood 1981 ) and hierarchical tree algorithm ( barnes and hut 1986 ) , and their derivatives ( couchman 1991 , xu 1995 , bagla and ray 2003 ) . in order to achieve the best performance , these algorithms have been implemented on massively parallel supercomputers ( e.g. pearce and couchman 1997 , springel , yoshida , white 2001 ) or beowulf - type pc - clusters ( e.g. dubinski et al .",
    "2003 , diemond et al 2004 ) .",
    "an alternative approach to achieve high computational speed is to use a specialized hardware for gravitational interaction , grape ( gravity pipe ) ( sugimoto et al .",
    "1991 , makino and taiji 1998 ) .",
    "a grape hardware has specialized pipelines for gravitational force calculation , which is the most expensive part of most of @xmath0-body simulation algorithms .",
    "all other calculations , such as the time integration of orbits , are performed on a standard pc or workstation ( host computer ) connected to grape .",
    "compared to the calculation without using grape , it offers a good speedup both for @xmath1 direct summations and @xmath3 fast algorithms .",
    "the speedup factor for the latter is about 10 - 100 times , depending on accuracy and other factors .",
    "it seems obvious that the combination of the fast algorithm , parallel computer and grape hardware would offer a very high performance .",
    "kawai and makino ( 2003 , also see makino 2004 ) reported implementations of tree algorithms on pc - grape cluster , which is a pc cluster with each pc connected to a grape hardware .",
    "as expected , such parallel pc - grape system achieved very high performance .",
    "for example , the largest simulation for a single virialized @xmath0-body system ( 31 million particles ) in the literature was performed using 8-node grape-5 cluster and parallel tree algorithm ( fukushige , kawai , makino 2004 ) .    in principle , by constructing a larger pc - grape cluster we can increase the size of the system we can handle . however , current hardwares , namely grape-5 ( kawai et al . 2000 ) or grape-6 ( makino et al . 2003 ) ,",
    "are not suited for the construction of very large grape - clusters .",
    "the reason is that they are too fast and also too expensive .",
    "the reproduction cost of the smallest unit of grape-6 is significantly higher than the cost of one node of a pc cluster . on the other hand , as discussed in makino et al .",
    "( 2003 ) , the speed of the host computer limits the speed of the tree algorithm .",
    "thus , if we can increase the number of host computers while keeping the money we spend for grape , we can achieve better performance for the tree algorithm .",
    "this means we should develop a less expensive , small grape hardware compared to what is currently available .",
    "we developed grape-6a to achieve this goal .",
    "it utilizes the same custom processor chip as was used for grape-6 .",
    "however , a single grape-6a card houses only four grape-6 chips , compared to up to 32 chips of single grape-6 processor board . by limiting the number of chips to four , we were able to design grape-6a as a single pci card , which can fit directly into the host pci bus .",
    "thus , we successfully reduce the cost of the smallest grape-6 system by nearly a factor of ten .",
    "the plan of this paper is as follows . in sections 2 and 3 ,",
    "we describe the grape-6a hardware and software . in section 4 , we discuss the performance of a single grape-6a hardware for three algorithms : tree , treepm , and individual timestep . in section 5 we describe a medium - scale ( 24-node ) pc - grape cluster which we constructed . in section 6 ,",
    "we discuss the performance of parallel algorithms on this pc - grape cluster .",
    "section 7 is for discussion .",
    "grape-6a calculates the gravitational force , its time derivative and potential , given by @xmath4 \\\\",
    "\\phi_i & = & \\sum_j { m_j \\over ( r_{ij}^2 + \\varepsilon^2)^{1/2}}\\end{aligned}\\ ] ] where @xmath5 and @xmath6 are the gravitational acceleration and the potential of particle @xmath7 , @xmath8 , @xmath9 , and @xmath10 are the position , velocity and mass of particle @xmath7 , @xmath11 and @xmath12 .",
    "it also calculates predictor polynomials of position and velocity for the individual timestep algorithm , evaluates the nearest neighbor particle and distance , and constructs the list of neighbor particles .",
    "( 15 cm,12 cm)./g6a.eps    figure [ g6a : fig1 ] shows the overall structure of grape-6a .",
    "it is a standard pci short card onto which one grape-6 processor module , an interface unit , and a power supply unit are integrated .",
    "the processor module comprises the custom processor chips for the functions described in section [ sec : func ] .",
    "the interface unit handles data transfer between the host pci bus and grape-6 processor module .",
    "the power supply unit converts the input voltage level to that of the grape-6 processor module .",
    "we describe each unit in the following subsections .",
    "( 10 cm,10 cm)./g6mod.eps    a grape-6 processor module consists of four grape-6 processor chips , eight ssram chips and one fpga chip . figure [ g6mod : fig ] shows its structure .",
    "a grape-6 processor chip is a custom lsi chip dedicated to the functions in section [ sec : func ] .",
    "it consists of six force calculation pipelines , a predictor pipeline , a memory interface , a control unit and i / o ports .",
    "more details about grape-6 processor chip are described in makino et al .",
    "two ssram chips are attached to each grape-6 processor chip .",
    "they are used to store the particle data .",
    "the fpga chip realizes a 4-input , 1-output reduction network for the data transfer from grape-6 processor chip to the host computer .",
    "four grape-6 processor chips calculate the forces on the same set of ( @xmath7-)particles , but from a different set of ( @xmath13-)particles ( what is called @xmath13-parallelization ) .",
    "the maximum number of @xmath7-particles is 48 , since each chip has six real force calculation pipelines and each real pipeline serves as eight virtual multiple pipelines ( vmp , makino et al .",
    "with vmp , one pipeline acts as if it is multiple pipelines operating at a slower speed , each of which calculates the force on its own @xmath7 particles , but from the same @xmath13 particles .",
    "thus , we can reduce the required memory bandwidth for @xmath13 particles .",
    "the forces calculated by the four grape-6 processor chips are summed up in the fpga chip on grape-6 processor module and sent back to the host computer .",
    "maximum numbers of @xmath13-particles that can be stored in the memory is 16384 per chip , and 65536 for the grape-6 processor module we used .",
    "a newer version of the processor module which can keep up to 131072 particles is commercially available .",
    "the peak speed of one grape-6 processor module , @xmath14 , grape-6a , is 131.3 gflops ( force and its derivative)/ 87.5 gflops ( force only ) .",
    "it has 24 ( real ) pipelines and operates at 96 mhz .",
    "here we count operations for gravitational force and its time derivatives and those for force only as 57 and 38 floating point operations , respectively .",
    "the clock signal for grape-6 processor module is generated and supplied by a crystal oscillator on board .",
    "input clock frequency for the processor chip is 24 mhz .",
    "it generates the clock signal with four times higher frequency in on - chip pll ( phase lock loop ) circuit , and all logic in the processor chips and ssram chips operate on this multiplied clock .",
    "the interface unit consists of a pci interface chip and a controller fpga chip . for the pci interface chip",
    ", we used the pci 9080 chip from plx technology inc .",
    ", which we used also for grape-4a , 5 and 6 .",
    "the interface to the host computer is standard 32bit/33mhz pci bus .",
    "the controller fpga chip handles data transfer between the pci interface chip and grape-6 processor module . for the controller fpga chip",
    ", we used altera ep1k100fc256 . the function and performance of the interface unit",
    "are basically the same as those of the pci host interface board ( kawai et al .",
    "1997 ) , which is used for grape-4 and grape-5 .",
    "grape-6a also has the power supply unit on board .",
    "the power supply unit converts the 12v power supplied from the power supply unit of the host pc to 2.5v required by the processor chip and other chips .",
    "the design of the power supply unit is essentially the same as that of ( relatively old ) pc motherboard for x86 processors .",
    "in fact , the circuit design is that same as that of the reference design of the controller chip we used ( ltc1709 - 8 from linear technology ) .",
    "the 12v power supply comes from an additional power connector , which accepts a standard 4-pin connector for hard - disk units .",
    "in addition to the 2.5v supply , grape-6a requires 3.3v and 5v supplies as well . for these , the supply through the pci connector is sufficient .",
    "( 10 cm,8 cm)./g6a_b.ps ( 10 cm,8 cm)./g6a_t.ps    figure [ g6a : fig2 ] shows the top ( upper panel ) and bottom views ( lower panel ) of a grape-6a board . the interface unit and the power supply units are on the left and right sides of the top view , respectively .",
    "the grape-6 processor module mounted on the bottom side .",
    "the board is 8 layer pcb ( printed circuit board ) , and its size is roughly 11 cm by 17 cm ( standard short pci card ) .",
    "the board design began on the autumn of 2001 , and it completed on january , 2003 after once redesign of pcb .",
    "a commercial version of grape-6a ( called microgrape ) is now available .",
    "the primary difference between grape-6a and grape-6 is in the number of grape-6 processor chips .",
    "grape-6a has 4 chips , while a single - board grape-6 can house up to 32 chips , and multi - board configuration connected to a single host is available .",
    "as we stated in the introduction , for grape-6a we intentionally reduced the maximum number of chips , so that the total system of multiple host computers and multiple grape-6a system offer the performance higher than that of a single grape-6 system with the same number of chips , at least for approximate algorithms such as the tree algorithm .",
    "we discuss the performance difference between grape-6 and grape-6a in sections [ perf : tree ] and [ sec : its ] .",
    "the design principle of the interface software of grape-6a is the same as that of previous grape systems ( makino , funato 1993 , makino , taiji 1998 subsection 5.3 , kawai et al .",
    "low - level software that communicates with grape-6a is encapsulated into the user library functions and is hidden to the user .",
    "the user program accesses grape-6a only through the library functions . the api(application program interface ) of the grape-6a library is designed so as to be same as that of the grape-6 software library .",
    "another software library whose api is the same as that of the grape-5 software library is prepared .",
    "the grape-6a software library is available on a web site .",
    "in this section , we describe the implementation and measured performance of three force calculation algorithms , barnes - hut tree algorithm , treepm algorithm and individual timestep algorithm , on a single board of grape-6a . except for the performance measurement in table [ per : tab1 ] and [ its : tab1 ]",
    ", we used a host computer with an intel pentium 4 processor ( 2.8cghz , i865 g ) and 2 gb of pc3200 memories , and gcc complier ( version 3.2.2 ) on redhat 9.0 ( linux kernel 2.4.20 - 8 ) .      the barnes - hut tree algorithm ( barnes , hut 1986 )",
    "reduces the calculation cost from @xmath1 to @xmath3 , by replacing the forces from distant particles by that from a virtual particle at their center of mass ( or multiple expansion ) . in order to use efficiently the grape hardware",
    ", we use the modified tree algorithm developed by barnes ( 1990 ) and first used on grape-1a by makino ( 1991a ) . with this algorithm",
    "the tree traversal is performed for a group of neighboring particles and an interaction list is created .",
    "grape calculates the force from particles and nodes in this interaction list to particle in the group . in the original algorithm ,",
    "tree traversal is performed for each particle .",
    "since grape can not perform the tree traversal algorithm , the tree traversal must be done on the host computer . by reducing the number of tree traversal operations ,",
    "the modified algorithm greatly reduce the work of the host computer and improves the overall performance .",
    "this modified algorithm was originally developed for a vector processor ( cdc cyber 205 ) and has been used on general - purpose mpp ( dubinski 1996 ) .      with the modified tree algorithm , the grape-6a system performs the integration of one time step in the following way .",
    "this procedure is the same as that for other grapes :    * the host computer constructs a tree structure . * repeat steps 3 through 7 until all the forces on all particles are updated . *",
    "identify a group of particles for which the same interaction list is used from remaining particles .",
    "this part is done by traversing the tree structure . *",
    "the host computer creates the interaction list for a group , and sends the data of the particles listed up to grape-6a .",
    "* repeat steps 6 and 7 for all particles in a group . *",
    "the host computer sends particles to be calculated to grape-6a . *",
    "grape-6a calculates the forces exerted on the particles , and then returns the result to the host computer . *",
    "the host computer updates the positions and velocities of all particles using the calculated force .",
    "the modified tree algorithm reduces the calculation cost of the host computer by roughly a factor of @xmath15 , where @xmath15 is the average number of particles in groups . on the other hand , the amount of work on grape increases as we increase @xmath15 , since the interaction list becomes longer .",
    "there is , therefore , an optimal value of @xmath15 at which the total computing time is the minimum ( makino 1991a ) .",
    "the optimal value of @xmath15 depends on various factors , such as the relative speed of grape and its host computer , the opening parameter and the number of particles .",
    "for the present grape-6a , @xmath16 is close to optimal .",
    "( 10 cm,8 cm)./treemon.eps    ( 10 cm,12 cm)./treemon23.eps    figure [ per : fig1 ] shows the measured calculation time per one timestep as a function of the number of particle , @xmath0 .",
    "for the particle distribution , we use an uniform sphere of equal mass particles .",
    "the opening angle , @xmath17 ( dipole expansion ) and @xmath18 .",
    "we set @xmath19 , where @xmath20 is the maximum number of particles in the group .",
    "we can see that the calculation time grows practically linearly as we increase @xmath0 .",
    "figure [ per : fig2 ] shows the measured calculation time and average length of the interaction list as a function of the opening angle @xmath21 for the uniform sphere and the plummer model ( the cutoff radius is 22.8 in a system of units such that @xmath22 , where @xmath23 is the total energy ) .",
    "the number of particles is @xmath24 .",
    "we can see the dependence of @xmath21 is weak for @xmath25 .",
    "this is because the length of the interaction list does not change much for @xmath25 , which is a characteristic of grape implementation of the modified tree algorithm ( makino 1991a ) .",
    "we can also see that the calculation time and the average length of the interaction list for plummer model are longer than those for the uniform sphere .    in the following , we present a theoretical model for the performance . the total calculation time per timestep",
    "is expressed as @xmath26 where @xmath27 , @xmath28 , and @xmath29 are the time spent on the host computer , the time spent on grape-6a , and the time spent for data transfer between the host computer and grape-6a , respectively . the time spent on the host computer",
    "is expressed as @xmath30 where @xmath31 and @xmath32 are the times to construct the tree structure and the interaction lists , respectively . in this equation @xmath33",
    "is the average length of the interaction list .",
    "according to makino ( 1991a ) , @xmath33 can be estimated as follows : @xmath34 the time spent on grape-6a is expressed as @xmath35 where @xmath36 is the time to calculate one pairwise interaction on grape-6a .",
    "the time spent for data transfer is expressed as @xmath37 three terms in the right - hand side indicate the times for data transfer of @xmath13-particles , @xmath7-particles , and calculated forces . here ,",
    "@xmath38 , @xmath39 , and @xmath40 are the times to transfer one byte data between the host computer and grape-6a , for the @xmath13-particle , the @xmath7-particle , and the calculated force , respectively .",
    "these times include not only time for transfer through pci but also that for conversion of data from / to conventional floating format in the host computer to / from number formats in grape-6 chip .",
    "the calculation time estimated using the theoretical model is plotted in figure [ per : fig1 ] .",
    "here , we set @xmath41 and the time constants as @xmath42 ( s ) , @xmath43 ( s ) , @xmath44 ( s ) , and @xmath45 ( s ) .",
    ".calculation times per timestep of the tree algorithm on various host computers ( @xmath46 ) [ cols=\"<,<,^,^,^,^,<\",options=\"header \" , ]     [ ipara : tab1 ]    we can see that for large @xmath0 , our parallel code achieves good efficiency .",
    "however , for small values of @xmath0 such as 16,384 , the speedup is rather marginal and the parallel calculation with @xmath47 is slightly slower than the calculation with single grape-6a . in other words ,",
    "parallel scalability is not very good .",
    "this is mainly because of the relatively short message length for the internode communication .",
    "such short message is inevitable with the individual timestep algorithm .",
    "the relatively long message latency of the mpich implementation we used resulted in this rather poor scaling characteristic .",
    "figure [ ipara : fig2 ] shows the ratio of the sustained calculation speed to the peak speed as a function of @xmath0 . table [ ipara : tab1 ] shows the breakdown of the calculation times per one particle step for some representative @xmath0 and @xmath48 .",
    "here , @xmath49 , @xmath50 and @xmath51 indicate the time spent on grape-6a and for communication between host computer and grape-6a ( step 1 , 5 , 6 , 9 ) , the time spent on the host computer ( step 2 , 4 , 8 , 10 ) , and the time spent for the internode communication ( step 7 ) .",
    "( 10 cm,8 cm)./monpits5.eps    figure [ ipara : fig2 ] shows the calculation speed in gflops as a function of @xmath0 .",
    "we also plot the performance of grape-6 with 16 chips(4 modules ) and 8 chips(2 modules ) .",
    "the performance of the parallel grape-6a system is close to that of grape-6 with the same number of modules for @xmath52k .",
    "we have developed a special - purpose computer for astrophysical @xmath0-body simulations , grape-6a , which is a downsized version of grape-6 suitable for parallel pc cluster configurations . with grape-6a ,",
    "it becomes practical to construct larger - scale pc - grape cluster systems .",
    "various parallel implementation of important algorithms such as treecode and individual timestep are already running on our parallel grape-6a system with very good performance .",
    "for example , using the tree algorithm on our parallel system , we can complete a collisionless simulation with 100 million particles ( 8000 steps ) within 10 days .",
    "several institutes constructed similar parallel grape-6a clusters .",
    "tsukuba university started the project to construct an impressive 256-node cluster for the simulation of first - generation objects ( umemura et al .",
    "http://www.rccp.tsukuba.ac.jp ) .",
    "at present , grape-6a is probably the best solution for construction of pc - grape cluster .",
    "further improvement of grape-6a with up - to - date technology would provide even more powerful computing systems .",
    "as is clear in the breakdown in table [ per : tab1 ] and [ its : tab1 ] , the total performance is limited by the communication speed between the host computer and grape . for the communication",
    ", grape-6a uses pci ( 32bit/33mhz ) interface , which is rather old technology .",
    "faster interfaces , such as pci - x or pci express , are now available .",
    "we are currently developing a new version of grape with pci - x interface .",
    "the peak speed of pci - x ( 64bit/133mhz ) is 1.06gbyte / s",
    ". this speed would be fast enough compared to the speed of the host computer for several years to come .",
    "we are grateful to hiroshi daisaka and eiichiro kokubo for discussions on the implementation on various host computers and to kohji yoshikawa for helping with implementation of the treepm algorithm .",
    "we would like to thank all of those who involved in the grape project .",
    "this research was supported by the research for the future program of japan society for the promotion of science ( jsps - rftf97p01102 ) , the grants - in - aid by the japan society for the promotion of science ( 14740127 ) and by the ministry of education , science , sports , and culture of japan ( 16684002 ) .",
    "aarseth ,  s.  j. 1999 , celest .",
    ", 73 , 127 aarseth ,  s.  j. 2003 , gravitational n - body simulations ( cambridge : cambridge ) bagla ,  j.  s. , & ray , s. 2003 , new astronomy , 8 , 665 barnes ,  j.  e. 1990 , j. comput .",
    "phys , 87 , 161 barnes ,  j.  e. , & hut ,  p. 1986 , nature , 326 , 446 blackstone ,  d & suel ,  t. 1997 , proc .",
    "sc97 ( acm in cd - rom ) brieu ,  p.  p. , & evrard ,  a.  e. 2000 , new astronomy , 5 , 163 couchman ,  h.  m.  p. 1991",
    ", , 368 , l23 diemond ,  j. moore ,  b. , & stadel ,  j. 2004 , , 353 , 624 dubinski ,  j. 1996 , new astronomy , 1 , 133 dubinski ,  j. , kim ,  j. , park ,  c. , & humble ,  r. 2003 , new astronomy , 9 , 111 .",
    "efstathiou ,  g. , & eastwood ,  j.  w. 1981 , , 194 , 503 fukushige ,  t. , kawai ,  a. , & makino ,  j. 2004 , , 606 , 625 heggie ,  d.  c. , & mathieu ,  r.  d. in the use of supercomputer in stellar dynamics , ed .",
    "p.hut & s.mcmillan ( new york : springer ) , 233 kawai ,  a. , fukushige ,  t. , makino ,  j. , & taiji ,  m. 2000 , , 52 , 659 kawai ,  a. , fukushige ,  t. , taiji ,  m. , makino ,  j. , & sugimoto ,  d. 1997 , , 49 , 607 kawai ,  a. , & makino ,  j. 2003 , in iau symp .",
    "208 , astrophysical supercomputing using particle simulations , ed .",
    "j.makino and p.hut ( san francisco : asp ) , 305 makino ,  j. 1991a , , 43 , 621 makino ,  j. 1991b , , 43 , 859 makino ,  j. 1991c , , 369 , 200 makino ,  j. 2004 , , 56 , 521 makino ,  j. & aarseth ,  s.  j. 1992 , , 44 , 141 makino ,  j. , fukushige ,  t. , koga ,  m. , & namura ,  k. 2003 , , 55 , 1163 makino ,  j. , funato ,  y. 1993 , , 45 , 279 makino ,  j. , & taiji ,  m. 1998 , scientific simulations with special - purpose computers  the grape systems ( chichester : john wiley and sons ) makino ,  j. , taiji ,  m. , ebisuzaki ,  t. , & sugimoto ,  d. 1997 , , 480 , 432 mcmillan ,  s.  l.  w. 1986 , in the use of supercomputer in stellar dynamics , ed .",
    "p.hut & s.mcmillan ( new york : springer ) , 156 pearce ,  f.  r. , & couchman ,  h.  m.  p. 1997",
    ", new astronomy , 2 , 441 springel ,  v. , yoshida ,  n. , & white ,  s.  d.  m. 2001 , new astronomy , 6 , 79 sugimoto ,  d. , chikada ,  y. , makino ,  j. , ito ,  t. , ebisuzaki ,  t. , & umemura ,  m. 1990 , nature , 345 , 33 warren ,  m.  s. , & salmon ,  j.  k. 1993 , in supercomputing 93 ( los alamitos : ieee comp .",
    "soc . press ) , 570 xu ,  g. 1995 , , 98 , 355 yoshikawa ,  k. & fukushige ,  t. 2005 , in preparation"
  ],
  "abstract_text": [
    "<S> in this paper , we describe the design and performance of grape-6a , a special - purpose computer for gravitational many - body simulations . it was designed to be used with a pc cluster , </S>",
    "<S> in which each node has one grape-6a . </S>",
    "<S> such configuration is particularly effective in running parallel tree algorithm . </S>",
    "<S> though the use of parallel tree algorithm was possible with the original grape-6 hardware , it was not very cost - effective since a single grape-6 board was still too fast and too expensive . </S>",
    "<S> therefore , we designed grape-6a as a single pci card to minimize the reproduction cost and optimize the computing speed . </S>",
    "<S> the peak performance is 130 gflops for one grape-6a board and 3.1 tflops for our 24 node cluster . </S>",
    "<S> we describe the implementation of the tree , treepm and individual timestep algorithms on both a single grape-6a system and grape-6a cluster . using the tree algorithm on our 16-node grape-6a system </S>",
    "<S> , we can complete a collisionless simulation with 100 million particles ( 8000 steps ) within 10 days .    # </S>",
    "<S> 1*[jm : # 1 ] * </S>"
  ]
}