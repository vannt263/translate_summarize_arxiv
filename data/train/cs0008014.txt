{
  "article_text": [
    "when we look at natural language parsing from a memory - based point of view , one might say that a sentence is analyzed by looking up the most similar structure for the different analyses of that sentence in memory .",
    "the parsing system described in this paper tries to mimic this behavior by interpreting the dop - model as a memory - based model , in which analyses are being matched with syntactic _ patterns _ recorded in memory .",
    "similarity between the proposed analysis and the patterns in memory is computed according to :    * the number of patterns needed to construct a tree ( to be minimized ) * the size of the patterns that are used to construct a tree ( to be maximized )    the _ nearest neighbor _ for a given analysis can be defined as the derivation that shares the largest amount of common nodes .",
    "10-fold cross - validation was used to appropriately evaluate the algorithms , as the dataset ( see section [ corpus ] ) is rather small .",
    "like dop1 the system is trained and tested on part - of - speech tag sequences . in a first phase , a simple bottom - up chart parser , trained on the training partitions , was used to generate parse forests for the part - of - speech tag sequences of the test partition .",
    "next , the parse forests were sent to the 3 algorithms ( henceforth the _ disambiguators _ ) to order these parse forests , the first parse of the ordered parse forest being the one proposed by the disambiguator .    in this paper , 3 disambiguators",
    "are described :    * pcfg : simple probabilistic context - free grammar * pmpg : the dop approximation , pattern - matching probabilistic grammar * pcfg+pmpg : a combined system , integrating pcfg and pmpg    the evaluation metric used is parse accuracy , but also the typical parser evaluation metric _ f - measure _ ( precision / recall ) is given as a means of reference to other systems .",
    "the experiments were conducted on an edited version of the atis - ii - corpus @xcite , which consists of 578 sentences .",
    "quite a lot of errors and inconsistencies were found , but not corrected , since we want our ( probabilistic ) system to be able to deal with this kind of noise .",
    "semantically oriented flags like -tmp and -dir , most often used in conjunction with pp , have been removed , since there is no way of retrieving this kind of semantic information from the part - of - speech tags of the atis - corpus .",
    "syntactic flags like -sbj , on the other hand , have been maintained .",
    "internal relations ( denoted by numeric flags ) were removed and for practical reasons , sentence - length was limited to 15 words max .",
    "the edited corpus retained 562 sentences .",
    "as a first phase , a bottom - up chart parser parsed the test set .",
    "this proved to be quite problematic , since overall , 106 out of 562 sentences ( 19% ) could not be parsed , due to the sparseness of the grammar , meaning that the appropriate rewrite rule needed to construct the correct parse tree for a sentence in the test set , was nt featured in the induced grammar .",
    "np - annotation seemed to be the main cause for unparsability .",
    "an np like _ restriction code ap/57 _ is represented by the rewrite rule :    np @xmath0 nn nn sym sym sym cd cd    highly specific and flat structures like these are scarce and are usually not induced from the training set when needed to parse the test set .",
    "on - going research tries to implement grammatical smoothing as a solution to this problem , but one might also consider generating parse forests with an independent grammar , induced from the entire corpus ( training set+testset ) or a different corpus . in both cases , however , we would need to apply probabilistic smoothing to be able to assign probabilities to unknown structures / rules .",
    "neither grammatical , nor probabilistic smoothing was implemented in the context of the experiments , described in this paper .",
    "the sparseness of the grammar proves to be a serious bottleneck for parse accuracy , limiting our disambiguators to a maximum parse accuracy of 81% .",
    "a pcfg constructs parse trees by using simple rewrite - rules .",
    "the probability of a parse tree can be computed by multiplying the probabilities of the rewrite - rules that were used to construct the parse .",
    "note that a pcfg is identical to dop1 when we limit the maximum substructures size to 1 , only allowing derivations of the type found at the right - hand side of figure [ deriv ] .",
    "the first line of table [ results ] shows the results for the pcfg - experiments : 66.4% parse accuracy is an adequate result for this baseline model .",
    "we also look at parse accuracy for parsable sentences ( an estimate of the parse accuracy we might get if we had a more suited parse forest generator ) and we notice that we are able to achieve a 81.8% parse accuracy .",
    "this is already quite high , but on examining the parsed data , serious and fundamental limitations to the pcfg - model can be observed      figure [ pcfg - eran ] , displays the most common type of mistake made by pcfg s .",
    "the correct parse tree could represent an analysis for the sentence :    this example shows that a pcfg has a tendency to prefer flatter structures over embedded structures .",
    "this is a trivial effect of the mathematical formula used to compute the probability of a parse - tree : embedded structure require more rewrite rules , adding more factors to the multiplication , which will almost inevitably result in a lower probability .",
    "it is an unfortunate property of pcfg s that the number of nodes in the parse tree is inversely proportionate to its probability .",
    "one might be inclined to normalize a parse tree s probability relative to the number of nodes in the tree , but a more linguistically sound alternative is at hand : the enhancement of context sensitivity through the use of larger syntactic context within parse trees can make our disambiguator more robust .",
    "the pattern - matching probabilistic grammar is a memory - based interpretation of a dop - model , in which a sentence is analyzed by matching the largest possible chunks of syntactic structure on the sentence . to compile parse trees into patterns , all substructures in the training set",
    "are encoded by assigning them specific indexes , np@345 e.g. denoting a fully specified np - structure .",
    "this approach was inspired by , in which goodman unsuccessfully uses a system of indexed parse trees to transform dop into an equivalent pcfg .",
    "the system of indexing ( which is detailed in ) used in the experiments described in this paper , is however specifically geared towards encoding contextual information in parse trees .    given an indexed training set , indexes can then be matched on a test set parse tree in a bottom - up fashion . in the following example ,",
    "boxed nodes indicate nodes that have been retrieved from memory .    in this example we can see that an np , consisting of a fully specified embedded np and pp , has been completely retrieved from memory , meaning that the np in its entirety can be observed in the training set .",
    "however , no vp was found that consists of a vbp and that particular np .",
    "disambiguating with pmpg consequently involves pruning all nodes retrieved from memory :    finally , the probability for this pruned parse tree is computed in a pcfg - type manner , not adding the retrieved nodes to the product :      the results for the pmpg - experiments can be found on the second line of table [ results ] . on some partitions ,",
    "pmpg performed insignificantly better than pcfg , but table [ results ] shows that the results for the context sensitive scheme are much worse .",
    "58.2% overall parse accuracy and 71.7% parse accuracy on parsable sentences indicates that pmpg is not a valid approximation of dop s context - sensitivity .      the dramatic drop in parsing accuracy calls for an error analysis of the parsed data .",
    "figure [ pmpg - eran ] is a prototypical mistake pmpg has made .",
    "the correct analysis could represent a parse tree for a sentence like :    the pmpg analysis would never have been considered a likely candidate by a common pcfg .",
    "this particular sentence in fact was effortlessly disambiguated by the pcfg . yet",
    "the fact that large chunks of tree - structure are retrieved from memory , make it the preferred parse for the pmpg .",
    "we notice for instance that a large part of the sentence can be matched on an sbar structure , which has no relevance whatsoever .    clearly , pmpg overestimates substructure size as a feature for disambiguation .",
    "it s interesting however to see that it is a working implementation of context sensitivity , eagerly matching patterns from memory . at the same time , it has lost track of common - sense pcfg tactics .",
    "it is in the combination of the two that one may find a decent disambiguator and accurate implementation of context - sensitivity .",
    "table [ results ] showed that 81.8% of the time , a pcfg finds the correct parse ( for parsable sentences ) , meaning that the correct parse is at the first place in the ordered parse forest .",
    "* 99% * of the time , the correct parse can be found among the 10 most probable parses in the ordered parse forest .",
    "this opens up a myriad of possibilities for optimization .",
    "one might for instance use a _ best - first _ strategy to generate only the 10 best parses , significantly reducing parse and disambiguation time .",
    "an optimized disambiguator might therefore include a preparatory phase in which a common - sense pcfg retains the most probable parses , so that a more sophisticated follow - up scheme need not bother with senseless analyses .    in our experiments , we combined the common - sense logic of a pcfg and used its output as the pmpg s input .",
    "this is a well - established technique usually referred to as _ system combination _",
    "( see for an application of this technique to part - of - speech tagging ) :    c    we are also presented with the possibility to assign a weight to each algorithm s decision .",
    "the probability of a parse can the be described with the following formula :    @xmath1    the weight of each algorithm s decision , as well as the number of most probable parses that are extrapolated for the pattern - matching algorithm , are parameters to be optimized",
    ". future work will include evaluation on a validation set to retrieve the optimal values for these parameters .",
    "the third line in table [ results ] shows that the combined system performs better than either one , with a parse accuracy of 71.5% and close to 90% parse accuracy on parsable sentences , which we can consider an approximation of results reported for dop1 .",
    "error analysis shows that the combined system is indeed able to overcome difficulties of both algorithms .",
    "the example in figure [ pcfg - eran ] as well as the example in figure [ pmpg - eran ] were disambiguated correctly using the combined system    p14 cm ( a ) correct analysis +   + ( b ) pmpg analysis +",
    "even though the pmpg shows a lot of promise in its parse accuracy , the following extensions need to be researched :    * optimizing pmpg+pcfg for computational efficiency : the graph in section [ combi ] shows a possible optimized parsing system , in which a pre - processing pcfg generates the _ n _ most likely candidates to be extrapolated for the actual disambiguator .",
    "full parse forests were generated for the experiments described in this paper , so that the efficiency gain of such a system can not be properly estimated .",
    "* pmpg+pcfg as an approximation needs to be compared to actual dop , by having dop parse the data used in this experiment , and by having pmpg+pcfg parse the data used in the experiments described in . *",
    "the bottleneck of the sparse grammar problem prevents us from fully exploiting the disambiguating power of the pattern - matching algorithm .",
    "the grael - system ( grammar adaptation , evolution and learning ) that is currently being developed , tries to address the problem of grammatical sparseness by using evolutionary techniques to generate , optimize and complement grammars .",
    "even though dop1 exhibits outstanding parsing behavior , the efficiency of the model is rather problematic .",
    "the introduction of multiple derivations causes a considerable amount of computational overhead .",
    "neither is it clear how the concept of multiple derivations translates to a psycholinguistic context : there is no proof that language users consider different instantiations of the same parse , when deciding on the correct analysis for a given sentence .",
    "a pattern - matching scheme was presented that tried to disambiguate parse forests by trying to maximize the size of the substructures that can be retrieved from memory . this straightforward memory - based interpretation yields sub - standard parsing accuracy .",
    "but the combination of common - sense probabilities and enhanced context - sensitivity provides a workable parse forest disambiguator , indicating that language users might exert a complex combination of memory - based recollection techniques and stored statistical data to analyze utterances .",
    "scha , r. 1990 .",
    "taaltheorie en taaltechnologie : competence en performance . in q.",
    "a.  m. de  kort and g.  l.  j. leerdam , editors , _ computertoepassingen in de neerlandistiek _ , lvvn - jaarboek .",
    "landelijke vereniging van neerlandici .    , h. , j.  zavrel , and w.  daelemans .",
    "1998 . improving",
    "data - driven wordclass tagging by system combination . in _ proceedings of the 36th annual meeting of the association for computational linguistics , montreal , quebec , canada _ , pages 491497 ,",
    "montreal , canada , august 10 - 14 ."
  ],
  "abstract_text": [
    "<S> data - oriented parsing ( dop ) ranks among the best parsing schemes , pairing state - of - the art parsing accuracy to the psycholinguistic insight that larger chunks of syntactic structures are relevant grammatical and probabilistic units . parsing with the dop - model </S>",
    "<S> , however , seems to involve a lot of cpu cycles and a considerable amount of double work , brought on by the concept of multiple derivations , which is necessary for probabilistic processing , but which is not convincingly related to a proper linguistic backbone . </S>",
    "<S> it is however possible to re - interpret the dop - model as a pattern - matching model , which tries to maximize the size of the substructures that construct the parse , rather than the probability of the parse . by emphasizing this memory - based aspect of the dop - model , </S>",
    "<S> it is possible to do away with multiple derivations , opening up possibilities for efficient viterbi - style optimizations , while still retaining acceptable parsing accuracy through enhanced context - sensitivity .    </S>",
    "<S> [ cols= \" < , < , < , < , < , < \" , ] </S>"
  ]
}