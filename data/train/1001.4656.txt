{
  "article_text": [
    "in this chapter we give an overview of bayesian data analysis , emphasising that it is _ a method for summarising uncertainty and making estimates and predictions using probability statements conditional on observed data and an assumed model _",
    "@xcite  which makes it valuable and useful in statistics , econometrics , and biostatistics , among other fields .",
    "we first describe the basic elements of bayesian analysis . in the following ,",
    "we refrain from embarking upon philosophical discussions about the nature of knowledge ( * ? ? ?",
    "* chapter 10 ) and the possibility of induction @xcite , opting instead for a mathematically sound presentation of a statistical methodology .",
    "we indeed believe that the most convincing arguments for adopting a bayesian version of data analyses are in the versatility of this tool and in the large range of existing applications .",
    "recall that , given a set of observations @xmath0 , a statistical model is defined as a family of probability distributions on @xmath1 , say @xmath2 and the aim of statistical inference is to derive quantitative information about the unknown _ parameter _ @xmath3 .",
    "this information can be about explanatory features of the model , like the impact of the increase by one point of interest rates over inflation rate or the relevance of culling strategies during the latest foot - and - mouth epidemics in the uk or yet the amount of cold dark matter in the universe , or about predictive features , like the value of a particular stock the next day or the chances for a given individual of catching the h5n1 flu over the coming three mouths .",
    "inference is quantitative in that it provides numerical values for the quantities of interest and numerical evaluations of the uncertainty surrounding those values as well .",
    "since all models are approximations of the real world , the choice of a sampling model is wide - open for criticisms : _ bayesians promote the idea that a multiplicity of parameters can be handled via hierarchical , typically exchangeable , models _ @xcite .",
    "this is however a type of criticism that goes far beyond bayesian modelling and questions the relevance of completely built models for drawing inference or running predictions .",
    "the central idea behind bayesian modelling is that the uncertainty on the unknown parameter @xmath3 is better modelled as randomness and consequently a probability distribution @xmath4 is constructed on @xmath5 .",
    "in particular @xmath6 then represents the probability distribution of the observation @xmath7 given that the parameter is equal to @xmath3 , i.e. the conditional probability distribution of @xmath7 given @xmath3 .",
    "if @xmath4 is a probability on @xmath5 , with density @xmath8 with respect to some measure @xmath9 on @xmath5 , then we can define a joint distribution for the observation and the parameter @xmath10 @xmath11 for the sake of simplicity we consider only models @xmath12 that allow for a dominating measure , @xmath13 ( say the lebesgue measure ) , and we denote by @xmath14 the density of @xmath6 with respect to @xmath13 ( the likelihood ) .",
    "then the joint distribution of @xmath10 has density @xmath15 with respect to @xmath16 . using bayes theorem we can define the distribution of the parameter @xmath3 given the observations by its density with respect to @xmath9 : @xmath17 and denote the denominator by @xmath18 the probability @xmath4 ( @xmath8 , respectively ) on @xmath5 is called the _ prior distribution _ ( density , respectively ) , the conditional probability ( [ def : posterior ] ) of @xmath3 given @xmath7 is called the _ posterior distribution _ ( density , respectively ) and @xmath19 is the _ marginal density _ of the observation @xmath7 . then",
    ", bayesian analysis is based entirely on the posterior distribution ( [ def : posterior ] ) , for all inferential purposes , e.g.  to draw conclusions on the parameter @xmath3 or on some functions of the parameter @xmath3 , to make predictions , to test the plausibility of a hypothesis or to check the fit of the model .",
    "there are many arguments which make such an approach compelling .",
    "without entering into philosophical and epistemological arguments on the nature of science @xcite , we briefly state what we view as the main practical appealing features of introducing a prior probability on @xmath3 .",
    "first such an approach allows to incorporate prior information in a natural way in the model , as explained in section [ sec : witch ] ; second , by defining a probability measure on the parameter space @xmath5 , the bayesian approach gives a proper meaning to notions such as _ the probability that _",
    "@xmath3 _ belongs to a specific region _ which are particularly relevant when constructing measures of uncertainty like confidence regions or when testing hypotheses .",
    "furthermore , the posterior distribution can be interpreted as the actualisation of the knowledge ( uncertainty ) on the parameter after observing the data .",
    "we stress that the bayesian paradigm does not state that the model within which it operates is the  truth \" , no more that it believes that the corresponding prior distribution it requires has a connection with the  true \" production of parameters ( since there may even be no parameter at all ) .",
    "it simply provides an inferential machine that has strong optimality properties under the right model and that can similarly be evaluated under any other well - defined alternative model .",
    "furthermore , the bayesian approach is such that _ techniques allow prior beliefs to be tested and discarded as appropriate _",
    "@xcite , in agreement that the overall principle that a _ bayesian data analysis has three stages : formulating a model , fitting the model to data , and checking the model fit _",
    "@xcite , so there seems to be little reason for not using a given model at an earlier stage even when dismissing it as  un - true \" later ( always in favour of another model ) .    in the above formulation ,",
    "note that @xmath5 can be endowed with quite different features : it can be a finite dimensional set ( as in parametric models ) , an infinite dimensional set ( as in most semi / non parametric models ) or a collection of various sets with no fixed dimension ( as in model choice ) .    as an example , consider the following contingency table on survival rate for breast - cancer patients with or without malignant tumours , extracted from @xcite , the ultimate goal being to distinguish between both types of tumour in terms of survival probability :    ....                    survival      age   malignant yes no   under 50         no   77 10                 yes   51 13     50 - 69         no   51 11                 yes   38 20 above 70         no    7   3                 yes    6   3 ....     representation of two gamma posterior distributions differentiating between malignant _ ( dashes ) _ versus non - malignant _ ( full ) _ breast cancer survival rates . ]",
    "then if we assume that both groups ( malignant versus non - malignant ) of survivors are poisson distributed @xmath20 , where @xmath21 is the total number of patients in this age group , i.e. @xmath22 then we obtain a likelihood @xmath23 which , under an exponential @xmath24 prior  whose rate @xmath25 is chosen here for illustration purposes , leads to the posterior @xmath26 i.e.  a gamma @xmath27 distribution .",
    "the choice of the exponential parameter corresponds to a @xmath28 survival probability . in the case of the non - malignant breast cancers ,",
    "the parameters of the gamma distribution are @xmath29 and @xmath30 , while , for the malignant cancers , they are @xmath31 and @xmath32 .",
    "figure [ fig : cancer ] shows the difference between those posteriors .      in many situations , it is useful to extend the above setup to prior measures that are not probability distributions but @xmath33-finite measures with infinite mass , i.e. @xmath34 since , provided that @xmath35 almost everywhere ( in @xmath7 ) , the quantity ( [ def : posterior ] ) is still well - defined as a probability density as when using a regular posterior probability as prior @xcite .",
    "such extensions are justified for a variety of reasons , ranging from topological coherence ",
    "limits of bayesian procedures often partake of their optimality properties @xcite and should therefore be included in the range of possible procedures  to robustness ",
    "a measure with an infinite mass is much more robust than a true probability distribution with a large variance  and improper priors are typically encountered in situations where there is little or no prior information , inducing flat , i.e.  uniform , distributions on the parameter space ( or on some transforms of the parameter space ) .",
    "indeed it is quite common , for complex models , to have little or no information on some of the parameters present in the model and using improper priors for such parameters has many advantages .",
    "note however that in such cases the marginal density @xmath19 does not define a probability on @xmath1 ( and that the existence condition needs to be checked ) .",
    "this drawback has an importance consequence for bayesian model comparison as explained in section [ seq : test ] .",
    "note also that some improper priors never allow for well - defined posteriors , no matter how many observations there are in the sample .",
    "one such example is when the prior is @xmath36 and the observations are iid cauchy .",
    "another and less anecdotic example occurs in mixture models , under exchangeable improper priors on the components @xcite .      as a general _ modus vivendi _",
    ", let us first stress that inference as a whole is meaningless unless it is evaluated .",
    "the evaluation of a statistical procedure , i.e. determining how well or how bad the inference performs , requires the definition of a comparison criterion , called a loss function .",
    "set @xmath37 the set of all possible results of the inference ( corresponding to the decision set in game theory ) .",
    "an estimator is then a function from @xmath1 into @xmath37 .",
    "( with an obvious abuse of notation , we will also use @xmath37 for the set of estimators . ) for instance , the aim is to estimate @xmath3 , then @xmath38 ; if the aim is to test for some hypothesis , then @xmath39 , and @xmath40 the set of a future observation if the aim is to predict a future observation .",
    "a loss function @xmath41 is a function on @xmath42 , expressing what the loss ( cost ) is for considering a decision @xmath43 when @xmath3 is the _ true _ value .",
    "typical ( formal ) loss functions used for estimation and test are quadratic losses ( @xmath44 ) and 0 - 1 losses ( 1 if decision is wrong , 0 if it is right ) , respectively .",
    "other loss functions can ( should ) be constructed , depending of the problem at hand , and they are strongly related to the notion of utility function encountered in economy and game theory @xcite .    given a statistical model @xmath12 on @xmath45 , a prior @xmath8 on @xmath46 and a loss function @xmath47 , the ( optimal ) bayesian procedure ( estimator ) is then defined as the decision function @xmath43 minimising the integrated risk @xmath48 : @xmath49 where @xmath50such a procedure is called a bayes estimator . using the fact @xcite , that such estimators can be computed pointwise as minimising the posterior risk : @xmath51 , @xmath52 it is possible to derive explicit expression of bayes estimates for many common loss functions . in particular ,",
    "the bayes estimator associated with the quadratic loss and the posterior distribution @xmath53 is the posterior mean @xmath54    note that the integrated risk @xmath48 can also be expressed as @xmath55 , where @xmath56 is the frequentist risk , so that bayes estimates are also often optimal in the frequentist sense .",
    "( it can be shown in particular that any admissible estimator is the limit of bayes estimators , see @xcite or @xcite ) .",
    "a critical aspect is the determination of the prior distribution @xmath8 and its clear influence on the inference .",
    "it is straightforward to come up with examples where a particular choice of the prior leads to absurd decisions .",
    "hence , for a bayesian analysis to be sound the prior distribution needs to be well - justified . before entering into a brief description of the various ways of constructing prior distributions ,",
    "note that as part of model checking , it is necessary in every bayesian analysis to assess the influence of the choice of the prior , for instance through a sensitivity analysis . since the prior distribution models the knowledge ( or uncertainty ) prior to the observation of the data , the sparser",
    "the prior information is , the flatter the prior should be .",
    "the advantage of incorporating prior information via a prior distribution is rather universally accepted and we therefore first describe ways of eliciting prior distributions from prior knowledge .",
    "the elicitation of prior distributions from prior knowledge consists in the construction of the prior probability @xmath57 using all items of prior information available to the modeller .",
    "this prior information may come from expert opinions or from bibliographic data or yet from earlier analyses , as in meta - analysis .",
    "there exists a vast literature on prior elicitation based on expert opinions , which is a much more complex process than is usually acknowledged in most bayesian statistical notebooks , see section 2 of this book for a more complete discussion on prior elicitation based on expert opinions .",
    "in particular the prior information is rarely rich enough to entirely define a prior distribution , therefore it is customary to choose a prior distribution within a parametric class of possible distributions : @xmath58 , where @xmath59 is called a hyperparameter . in such cases the prior information is summarised through the choice of @xmath60 .",
    "for instance , @xcite use bibliographic prior information to construct a prior distribution on the probability of cross - contamination from a contaminated broiler in a household , say @xmath61 . the prior distribution of @xmath61 is assumed to be a beta @xmath62 distribution , @xmath63 and the parameters @xmath64 of the beta distribution are assessed using two cross  contamination models in the literature which lead to a probability of transfer between @xmath65 and @xmath66 , which was translated into a @xmath67 prior on @xmath61 , as it corresponds to a prior mean of @xmath68 and to a @xmath69 prior confidence interval equal to @xmath70 .",
    "see also @xcite for an example of expert elicitation of the beta parameters on some capture and survival probabilities in a lizard population , or the chapter of bcker , crimmi and fink in this volume where beta priors are elicited to model correlations between risk types .      among the possible parametric families",
    "@xmath58 , @xmath71 , conjugate priors form appealing parametric families , merely for computational reasons @xcite .",
    "a family of distribution prior distributions @xmath58 , is said to be conjugate to the likelihood @xmath72 if the posterior also belongs to the same family , i.e. when the prior is equal to @xmath73 then there exists a @xmath74 such that the posterior is equal to @xmath75 .",
    "the actualisation of the information due to observing the data @xmath7 is then modelled as a change of hyperparameter from @xmath76 to @xmath77 .",
    "exponential families ( as models for the observation @xmath7 ) are almost in one - to - one correspondence with sampling distributions allowing for conjugate priors . as an example , @xcite consider an observed random variable @xmath78 that is the number of pregnant women arriving at a given hospital to deliver their babies within a given month , which they model as a poisson @xmath79 distribution with parameter @xmath80 .",
    "a conjugate family of priors for the poisson model is the collection of gamma distributions @xmath81 , since @xmath82 leads to the posterior distribution of @xmath3 given @xmath83 being the gamma distribution @xmath84 .",
    "the computation of estimators , of confidence regions  called credible regions within the bayesian literature to distinguish",
    "the fact that those regions are evaluated on the parameter space rather than on the observation space @xcite  or of other types of summaries of interest on the posterior distribution then becomes straightforward .",
    "for instance in the above poisson ",
    "gamma example , the bayesian estimator of the average number of arrivals , associated with the quadratic loss , is given by @xmath85 , the posterior mean .",
    "the apparent simplicity of conjugate priors should however not make them excessively appealing , since there is no strong justification to their use .",
    "one of the difficulties with such families of priors is the influence of the hyperparameter @xmath76 .",
    "if the prior information is not rich enough to justify a specific value of @xmath60 , fixing @xmath86 arbitrarily is problematic , since it does not take into account the prior uncertainty on @xmath76 itself . to improve on this aspect of conjugate priors ,",
    "a usual fix is to consider a _",
    "hierarchical prior _ , i.e.  to assume that @xmath87 itself is random and to consider a probability distribution with density @xmath88 on @xmath60 , leading to @xmath89 as a joint prior on @xmath90 .",
    "the above is equivalent to considering , as a prior on @xmath3 @xmath91 obviously @xmath88 may also depend on some hyperparameters @xmath92 .",
    "higher order levels in the hierarchy are thus possible , even though the influence of the hyper(-hyper-)parameter @xmath92 on the posterior distribution of @xmath3 is usually smaller than that of @xmath60 . but multiple levels are nonetheless useful in complex populations as those found in animal breeding @xcite",
    ".    in many applications prior information is quite vague or at least vague enough on some parts of the model , in which case it is important to derive priors that have desirable properties and that are as little arbitrary or subjective as possible .",
    "such constructions are commonly called _ non informative_. while this denomination is misleading , and should be replaced by the less judgemental _ reference prior _ denomination , we nonetheless follow suit and use it in the following subsections , since it is the most common denomination found in the literature @xcite .",
    "non informative priors are expected to be flat distributions , possibly improper . an apparently natural way of constructing such priors would be to consider a uniform prior , however this solution has many drawbacks , the worst one being that it is not invariant under a change of parameterisation . to understand this",
    "consider the example of a binomial model : the observation @xmath7 is a @xmath93 random variable , with @xmath94 unknown .",
    "the uniform prior @xmath95 could then sound like the most natural non informative choice ; however , if , instead of the mean parameterisation by @xmath61 , one considers the logistic parameterisation @xmath96 then the uniform prior on @xmath61 is transformed into the logistic density @xmath97 by the jacobian transform , which obviously is not uniform .    to circumvent this lack of invariance per reparameterisation , @xcite proposed the following choice now known as _",
    "jeffreys prior _",
    "@xmath98 where @xmath99 is the fisher - information matrix and @xmath100 denotes its determinant .",
    "the above construction is obviously invariant per reparameterisation and has many other interesting features specially in one - dimensional setups ( see @xcite for a reassessment of jeffreys impact on bayesian statistics ) .",
    "in particular , in the one - dimensional parameter case , the jeffreys prior is also the matching prior ( see @xcite , chapters 3 and 8) , and the reference prior defined by bernardo @xcite . for instance , when @xmath6 is a location family , i.e.  when @xmath101 , the fisher information is constant and thus the jeffreys prior is @xmath102 . note that in many cases like the above the jeffreys prior is improper .    in multivariate setups , jeffreys construction is not so well - justified and it may lead to not - so - well - behaved priors .",
    "a famous example is the neyman ",
    "scott problem where two groups of observations are such that in each group all observations are distributed from @xmath103 , @xmath104 , @xmath105 . in this case",
    "jeffreys prior is given by @xmath106 , and the bayes estimator of @xmath107 associated with the quadratic loss function is equal to @xmath108 = \\sum_{i=1}^n \\frac { ( x_{i,1}- x_{i,2})^2 } { 4(n-1 ) } \\,,\\ ] ] which converges to @xmath109 as @xmath110 goes to infinity , thus leading to an inconsistent estimate .",
    "although this seems like an artificial example it is actually of wider interest , since in the normal linear regression model jeffreys prior is proportional to @xmath111 where @xmath61 is the number of covariates .",
    "this dependence on @xmath61 makes it rather unappealing , even though the alternative @xmath112-prior of @xcite discussed below suffers from the same drawback .",
    "another standard example discussed further in section [ sec : nuisance ] is when estimating @xmath113 when @xmath3 is the @xmath110-dimensional mean of an @xmath110-dimensional normal vector .",
    "the ultimate attempt to define a non informative prior is in our opinion bernardo s ( @xcite ) definition through the information theoretical device of kullback divergence ( see also @xcite or @xcite ) .",
    "the idea is to split the parameter into groups say @xmath114 where @xmath115 is more interesting than @xmath116 , which is more interesting than @xmath117 and so on .",
    "this can be seen as a generalisation of the usual splitting into a parameter of interest and a nuisance parameter .",
    "then the bernardo s reference prior is constructed iteratively as some sorts of jeffreys priors in each of the submodels , see also @xcite for a more precise description of the iterative construction .",
    "quite obviously , this is not the unique possible approach , it depends on a choice of information measure , does not always lead to a solution , requires an ordering of the model parameters that involves some prior information ( or some subjective choice ) but , as long as we do not _ think of those reference priors as representing ignorance _",
    "@xcite , they can indeed be _ taken as reference priors , upon which everyone could fall back when the prior information is missing _",
    "@xcite .",
    "a well - known phenomenon is the decrease of influence of the prior as the sample size ( or the information in the data ) increases .",
    "we shall recall here these results in the simpler case of i.i.d observations , however these results can be extended to non i.i.d .",
    "cases such as dependent observations under stationary and mixing properties , gaussian processes and so on . generally speaking in most parametric cases",
    ", the posterior distribution concentrates towards the true parameter value as @xmath110 goes to infinity so that posterior estimates will converge to the true values , as @xmath110 goes to infinity .",
    "this first type of results ensures that point estimates are satisfactory , as far as asymptotic convergence is concerned .",
    "another important aspect of the asymptotic analysis of bayesian procedures is to understand how the measures of uncertainty derived from the posterior can be related to frequentist measures of uncertainty .",
    "such a relation can be deduced from the bernstein von mises property , which can be stated in the following way : assume that the vector of observations @xmath118 is made of i.i.d observations from a distribution @xmath14 , which is regular , see for instance @xcite for more precise conditions , and let @xmath8 be a prior density , which is positive and continuous on @xmath5 , then the posterior distribution can be approximated in the following way , when @xmath110 goes to infinity : for all @xmath119 @xmath120 \\approx p\\left [ \\mathcal n(0 , i_1(\\hat{\\theta})^{-1 } ) \\in a\\right],\\end{aligned}\\ ] ] where @xmath121 is the maximum likelihood estimator and @xmath122 is the fisher information matrix per observation calculated at @xmath123 . in other words",
    "the posterior distribution resembles a gaussian distribution centred at @xmath121 with covariance matrix @xmath124 , when @xmath110 is large .",
    "this result has many interesting implications .",
    "the first consequence is that , to first order , the influence of the prior disappears as @xmath110 goes to infinity .",
    "it also allows for quick approximate computations in the case of large samples , and it implies that to first order bayesian and frequentist inference ( based on the likelihood ) essentially give the same answers .",
    "although devising procedures giving the same answers as frequentist procedures is not an ultimate aim of the bayesian analysis , it is of importance to ensure that bayesian procedures ultimately have also good frequentist properties . the asymptotic equivalence between the bayesian and the frequentists answers ( to first order ) hold in wide generality for finite dimensional models .",
    "when the dimension of the parameter grows with the number of observations or is infinite , then this is often not true anymore , see for instance @xcite and @xcite .",
    "although these asymptotic results have a strong frequentist flavour , in the sense that they are obtained by assuming that there is a fixed true parameter @xmath125 and as new data comes in the posterior concentrates around the true parameter like a gaussian distribution , they are also appealing from the subjectivists points of view where _ probabilities represent degrees of belief and there are no objective probability model _ , see @xcite for a more precise discussion on this issue .",
    "recall that the whole inference about @xmath3 is deduced from the posterior distribution , @xmath126 , including estimates as major summaries , but the posterior distribution gives us much more information than simply point estimates . in particular , different measures of uncertainty can be derived from the posterior and among the various measures credible regions are the most popular .",
    "a set @xmath127 is an @xmath128 - credible region if and only if @xmath129 \\geq 1 -\\alpha.\\ ] ] contrariwise to frequentist confidence regions , the notion of coverage probability is directly understood as a probability on @xmath3 and is therefore straightforward to interpret . among all credible regions defined by , those having minimal volume are particularly interesting .",
    "it turns out , see @xcite , that they are defined as highest posterior density ( hpd ) regions : @xmath130 where @xmath131 is the largest value such that @xmath132 \\geq 1-\\alpha.\\end{aligned}\\ ] ] ( note that we define the bound @xmath133 in terms of the product prior@xmath134likelihood in order to bypass the difficulty with the normalising constant @xmath19 . )",
    "although the analytic determination of @xmath133 is often challenging , the approximation of this bound based on a sample from @xmath126 , @xmath135 , can be easily derived from an ordering of the values @xmath136 as the corresponding @xmath137-th quantile .",
    "for instance , if a poisson @xmath138 count is associated with a gamma @xmath81 prior , the posterior @xmath84 leads to the hpd region @xmath139 whose determination requires a numerical construct . on the other hand , if a sample @xmath135 from the posterior @xmath84 is available , then the hpd bound @xmath133 can be estimated as the @xmath137-th quantile of the values @xmath140^{a+x-1}\\,\\exp(-(b+1)\\theta^{(i)})$ ] s .",
    "figure [ fig : hpd ] illustrates a similar derivation in the case of a normal @xmath141 model with both parameters unknown .",
    "representation of a gibbs sample of @xmath142 values of @xmath143 for the normal model , @xmath144 with @xmath145 , @xmath146 and @xmath147 , under jeffreys prior , along with the pointwise approximation to the @xmath148 hpd region _",
    "( in darker hues ) _ ( _ source : _",
    "@xcite ) . ]",
    "credible regions have nice interpretations and are optimal under a volume criterion , as bayesian estimators of the confidence sets @xmath149 . in a wide generality",
    ", they further attain good frequentist coverage in the sense that @xmath150 for most prior distributions @xmath8 , where @xmath110 denotes the sample size ( * ? ? ?",
    "* ; * ? ? ?",
    "* chapter 5 ) .",
    "credible regions however suffer from a lack of invariance to changes of parameterisation , i.e. if @xmath3 is a given parameterisation of interest and @xmath151 is the hpd region constructed as above , then if @xmath152 is another parameterisation , @xmath153 is not necessarily the hpd region for the @xmath92 parameterisation ( see @xcite for a detailed analysis of this phenomenon ) .",
    "in many applied problems , one is only interested in some components of the parameter , the remaining part of the parameter being then called the nuisance parameter . this distinction opposes the parameter of interest , say @xmath154 within @xmath155 , where @xmath156 is the parameter of interest and @xmath157 is the nuisance parameter . dealing with nuisance parameters is quite problematic in a frequentist framework , whether one is interested in parameter estimation , in confidence regions determination or in testing .",
    "likelihood approaches need to define _ proper _ likelihoods for @xmath154 , which in complete generality is not possible .",
    "hence , they use approximations and modifications of proper likelihoods such as partial likelihoods or modified profile likelihoods , see @xcite for a more complete discussion on these issues .    on the opposite ,",
    "the bayesian framework offers is a most natural way of dealing with nuisance parameters and for defining proper profile likelihoods : integrating out the nuisance parameter . in other words",
    "the bayesian marginal likelihood for @xmath154 under the prior @xmath158 is given by @xmath159 this approach offers many advantages : ( 1 ) if the conditional prior @xmath160 is proper , then @xmath161 as defined in ( [ margin : likeli ] ) is a proper likelihood , in the sense that it is the density of @xmath7 under some model parameterised by @xmath154 alone ; ( 2 ) integrating @xmath157 out implicitly takes into account the uncertainty on @xmath157 , contrary to the profile likelihood , or to any other kind of plug - in likelihood defined by @xmath162 , where @xmath163 is some _ estimate _ of @xmath157 given @xmath154 . in particular uncertainty measures derived from @xmath161",
    "are not biased downwards due to the replacement of @xmath157 by @xmath163 .",
    "hence there is no need to correct further for this uncertainty , which is usually necessary when dealing with plug - in likelihoods , leading to penalised likelihoods .",
    "this is of particular interest in model selection , when the parameter of interest is the model itself , as discussed in section [ seq : test ] .    however , if @xmath160 is an improper prior , then @xmath161 is not necessarily a _ likelihood _ , in particular @xmath164 may occur .",
    "a well - known example of such misbehaviour is the case of the so - called marginalisation paradoxes , see for instance robert ( 2001 , chapter 3 ) . as another example of badly behaved marginal likelihood , consider the case presented in robert ( 2001 , chapter 3 ) and @xcite where the observations @xmath165 , @xmath166 , are independent and where the parameter of interest is @xmath167 and the nuisance parameter is @xmath168 , the direction of the vector @xmath13 .",
    "a natural flat prior on @xmath157 is the uniform distribution on the @xmath61-dimensional sphere for @xmath157 and the scale prior @xmath169 , leading to a well - behaved marginal likelihood , see @xcite for precise calculations .",
    "however if one considers instead the jeffreys prior on @xmath13 , i.e. @xmath170 , then the posterior distribution of @xmath154 is a chi - square distribution with @xmath61 degrees of freedom and non - centrality parameter @xmath171 , which is not a well - behaved posterior .",
    "in particular the posterior mean of @xmath157 is equal to @xmath172 and satisfies @xmath173 as @xmath61 goes to infinity .",
    "the above examples do not imply that one should not use improper priors on nuisance parameters , since in most cases little information is known on those parameters .",
    "rather they show that one needs to be quite careful in selecting improper priors in such cases .",
    "the construction of bernardo s ( @xcite ) reference priors is particularly relevant in such frameworks .    in the following section ,",
    "we describe bayesian testing and bayesian model comparison or model selection . it is to be noted that model selection can be viewed as a specific example of nuisance parameter framework , where the parameter of interest is the model and the nuisance parameters are the parameters in each model .",
    "the most standard bayesian answer to a testing problem for hypotheses written as @xmath174 for the null and as @xmath175 for the alternative , is the bayesian estimate corresponding to the 01 loss function , i.e.  to the procedure accepting @xmath176 if and only if @xmath177 > p^\\pi\\left[\\theta_1|x\\right]\\,.\\ ] ] in less formal terms , the null hypothesis is accepted if it is more probable under the posterior distribution than under the alternative , which is a very intuitive answer . to constrain the impact of the prior probabilities , a different quantity is usually adopted , namely the bayes factor @xcite , which is defined by @xcite as @xmath178 note that the posterior odds can be recovered from the bayes factor by assigning the appropriate prior probabilities on each of both models , contradicting the criticism of @xcite that the bayes factor is not scaled in probability terms .",
    "interestingly @xmath179 , hence there is no asymmetry in the definition and construction of bayes factor , contrariwise to the neyman ",
    "pearson approach .",
    "we do not believe that this is a drawback and would rather question the interest in forcing such an asymmetry in the neyman  pearson tests .",
    "the bayes factor , a monotonic transform of the posterior probability of @xmath176 which eliminates the influence of the prior weight @xmath180 , has a similar interpretation to the classical likelihood ratio . as noted in the previous section , by integrating out the parameters within each hypothesis , the uncertainty on each parameter is taken into account , which induces a natural penalisation for richer models , as intuited by @xcite ( _ variation is random until the contrary is shown ; and new parameters in laws , when they are suggested , must be tested one at a time , unless there is specific reason to the contrary _ ) .",
    "although we strongly dislike using the term because of its undeserved weight of academic authority , the bayes factor acts as a natural _",
    "ockham s razor . _ the well - known connection with the bic ( bayesian information criterion ,",
    "see @xcite , chapter 5 ) , with a penalty term of the form @xmath181 , makes explicit the penalisation induced by bayes factors in regular parametric models .",
    "however it goes beyond this class of models , and in much greater generality , the bayes factor corresponds asymptotically to a likelihood ratio with a penalty of the form @xmath182 where @xmath183 and @xmath184 can be viewed as the effective dimension of the model and the effective number of observations , respectively , see @xcite .",
    "the bayes factor therefore offers the major interest that it does not require to compute a complexity measure ( or penalty term)in other words , to define what is @xmath183 and what is @xmath184 , which often is quite complicated and may depend on the true distribution",
    ".      the inferential problems of bayesian model selection and of bayesian testing are clearly those for which the most vivid criticisms can be found in the literature : witness @xcite who states that _ the jeffreys - subjective synthesis betrays a much more dangerous confusion than the neyman - pearson - fisher synthesis as regards hypothesis tests_. we find this suspicion rather intriguing given that the bayesian approach is the only one giving a proper meaning to the probability of a null hypothesis , @xmath185 , since alternative methodologies can at best specify a probability value on the _ sampling _ space , i.e.  on the wrong dual space .",
    "if we consider the special case of point null hypotheses  which is not such limited a scope since it includes all variable selection setups , there is a difficulty with using a standard prior modelling in this environment",
    ". as put by @xcite , when _ considering whether a location parameter @xmath128 is @xmath186 [ when ] the prior is uniform , we should have to take @xmath187 and @xmath188 would always be infinite_. this is therefore a case when the inferential question implies a modification of the prior , justified by the information contained in the question . while avoiding the whole issue is a solution , as with gelman ( @xcite ) having _ no patience for statistical methods that assign positive probability to point hypotheses of the @xmath189 type that can never actually be true _",
    ", considering the null and the alternative hypotheses as two different models allows for a bayes factor representation and corresponds to assigning a positive probability to the null hypothesis .    in our view ,",
    "one of the major drawbacks of bayes factors - or even posterior odds - is that they can not be used under improper priors , for lack of proper normalising constants .",
    "this is even more acute a difficulty than what is described in section [ sec : nuisance ] , because the bayes factor is simply not defined under improper priors , for any sample size .",
    "solutions have been proposed , akin to cross - validation techniques in the classical domain @xcite , but they are somehow too ad - hoc to convince the entire community ( and obviously beyond ) . in some situations , when parameters shared by both models have the same meaning in each of the models , an improper prior can be used on these parameters , in both models .",
    "for instance , when considering variable selection in a regression model , @xmath190 e.g.  when deciding whether or not the null hypothesis @xmath191 holds , the relevant non informative prior distribution is zellner s ( @xcite ) @xmath112-prior , where @xmath192 corresponds to a normal @xmath193 distribution on @xmath194 and a  marginal \" improper prior on @xmath107 , @xmath195 .",
    "this means that , when considering the submodel corresponding to the null hypothesis @xmath191 , with parameters @xmath196 and @xmath33 , we can also use the  same \" @xmath112-prior distribution @xmath197 where @xmath198 denotes the regression matrix missing the column corresponding to the first regressor , and @xmath199 .",
    "since @xmath33 is a nuisance parameter in this case , we may use the improper prior on @xmath107 as _ common _ to all submodels and thus avoid the indeterminacy in the normalising factor of the prior when computing the bayes factor @xmath200 figure [ fig : from : core ] reproduces an output from @xcite that illustrates how this default prior and the corresponding bayes factors can be used in the same spirit as significance levels in a standard regression model , each bayes factor being associated with the test of the nullity of the corresponding regression coefficient .",
    "for instance , only the intercept and the coefficients of @xmath201 are significant .",
    "this output mimics the standard lm r function outcome in order to show that the level of information provided by the bayesian analysis goes beyond the classical output , not to show that we can get similar answers to those of a least square analysis since , else , _ if the bayes estimator has good frequency behaviour then we might as well use the frequentist method _ @xcite .",
    "( while computing issues are addressed in the following chapter , we stress that all items in the table of figure [ fig : from : core ] are obtained via closed form formulae . )    [ cols= \" < , < , < , < \" , ]",
    "evidence against h0 : ( * * * * ) decisive , ( * * * ) strong , ( * * ) substantial , ( * ) poor    the major criticism addressed to the bayesian approach to testing is therefore that it is not interpretable on the same scale as the neyman - pearson - fisher solution , namely in terms of probability of type i error and of power of the tests . in other words ,",
    "_ frequentist methods have coverage guarantees ; bayesian methods do nt ; 95 percent frequentist intervals will live up to their advertised coverage claims _ @xcite .",
    "a natural question is then to question the appeal of such frequentist properties when considering a single dataset , i.e.  in jeffreys ( 1939 ) famous words , _ a hypothesis that may be true may be rejected because it had not predicted observable results that have not occurred _ , especially when considering that @xmath61-values may be inadmissible estimators @xcite .",
    "from a decisional perspective  with which the frequentist properties should relate , a classical neyman - pearson - fisher procedure is never evaluated in terms of the consequences of rejecting the null hypothesis , even though the rejection must imply a subsequent action towards the choice of an alternative model .",
    "therefore , complaining that _ having a high relative probability does not mean that a hypothesis is true or supported by the data _",
    "@xcite , simply because the bayesian approach is relative in that it _ posits two or more alternative hypotheses and tests their relative fits to some observed statistics _",
    "@xcite , is missing the main purpose of tests , which is not to validate or invalidate a golden model _",
    "per se _ but rather to infer a working model that allows for acceptable predictive properties .      for model choice , i.e.  when several models are under comparison for the same observation @xmath202 where @xmath203 can be finite or infinite , the usual bayesian answer is similar to the bayesian tests as described above .",
    "the most coherent perspective ( from our viewpoint ) is actually to envision the tests of hypotheses as particular cases of model choices , rather than trying to justify the modification of the prior distribution criticised by @xcite .",
    "this also also to incorporate within model choice the alternative solution of model averaging , proposed by @xcite , which strives to keep all possible models when drawing inference .",
    "the idea behind bayesian model choice is to construct an overall probability on the collection of models @xmath204 in the following way : the parameter is @xmath205 , i.e. the model index and given the model index equal to @xmath206 , the parameter @xmath207 in model @xmath208 , then the prior measure on the parameter @xmath3 is expressed as @xmath209 where both the @xmath210 s and @xmath211 s are part of the prior modelling , hence chosen by the experimenter .",
    "( the @xmath210 s have the natural interpretation of the traditional prior under model @xmath208 , while the @xmath211 s correspond to the prior assessment of the models under comparison . ) as a consequence , the bayesian model selection associated with the 01 loss function and the above prior is the model that maximises the posterior probability @xmath212 across all models .",
    "contrary to classical pluggin likelihoods , the marginal likelihoods involved in the above ratio do compare on the same scale and do not require the models to be nested : the criticism that _ complicating dimensionality of test statistics is the fact that the models are often not nested , and one model may contain parameters that do not have analogues in the other models and vice versa _",
    "@xcite is not founded . as mentioned in section [ sec : nuisance ] integrating out the parameters @xmath207 in",
    "each of the models takes into account their uncertainty thus the marginal likelihoods @xmath213 are naturally penalised likelihoods . in many setups , the bayesian model selector as defined above is consistent , i.e. as the number of observations increases the probability of choosing the right model goes to 1 .",
    "the computational requirements related to handling a collection of marginal likelihoods will be addressed in the following chapter , in connection with the review of classical solutions in @xcite .",
    "interestingly enough , the most accurate approximation technique for marginal likelihoods is , when applicable , directly derived from bayes theorem , via chib s ( @xcite ) rendering : @xmath214 where @xmath215 is a simulation - based approximation to the posterior density based on simulated latent variables .",
    "@xcite illustrate this method in the setting of mixtures and @xcite in the alternative case of a probit model , respectively , both of which demonstrate the precision of this approximation .",
    "posterior odds and bayes factors are the most common bayesian approaches to testing , however they are not the only ones . in particular the choice of the 01 loss function",
    "is not necessarily relevant or the most relevant .",
    "in some situations it might be more interesting to penalise the loss with the distance to the null hypothesis for instance , see @xcite where such ideas are applied to goodness of fit tests or @xcite .",
    "bayesian analysis has long been derided for providing optimal answers that could not be computed . with the advent of early monte carlo methods , of personal computers , and , more recently , of more powerful monte carlo methods @xcite , the pendulum appears to have switched to the other extreme and _ bayesian methods seem to quickly move to elaborate computation _",
    "@xcite , a feature that does not make them less suspicious : _ a simulation method of inference hides unrealistic assumptions _",
    "the simulation techniques that have done so much to promote bayesian analysis in the past decades are detailed in the next chapter and thus not described here .",
    "we nonetheless want to point out that , while simulation methods can be misused  as about any other methodology  and while _ bayesian simulation seems stuck in an infinite regress of inferential uncertainty _",
    "@xcite , there exist enough convergence assessment techniques @xcite to ensure a reasonable confidence about the approximation provided by those simulation methods .",
    "thus , as rightly stressed by @xcite , _ the discussion of computational issues should not be allowed to obscure the need for further analysis of inferential questions_.    the field of bayesian computing is therefore very much alive and , while its diversity can be construed as a drawback by some , we do see the emergence of new computing methods adapted to specific applications as most promising , because it bears witness to the growing involvement of new communities of researchers in bayesian advances ."
  ],
  "abstract_text": [
    "<S> this introduction to bayesian statistics presents the main concepts as well as the principal reasons advocated in favour of a bayesian modelling . we cover the various approaches to prior determination as well as the basis asymptotic arguments in favour of using bayes estimators . </S>",
    "<S> the testing aspects of bayesian inference are also examined in details .    </S>",
    "<S> * keywords : * bayesian inference , bayes model choice , foundations , testing , non - informative prior , bayesian nonparametrics , bayes factor </S>"
  ]
}