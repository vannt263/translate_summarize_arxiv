{
  "article_text": [
    "we consider variable selection problem for the varying coefficient model defined by @xmath1 where @xmath2 is a scalar response variable , @xmath3 , @xmath4 are the candidate covariates , @xmath5 is the random error , and @xmath6 $ ] .",
    "the coefficient functions @xmath7 , @xmath8 , are assumed to vary smoothly with @xmath9 , and are non - zero for only a subset of the @xmath10 candidate covariates .",
    "the variable @xmath9 is an influential variable , such as age or income in econometric studies , and is sometimes called the index variable .",
    "the varying coefficient model is a popular and useful semiparametric approach to modeling data that may not obey the restrictive form of traditional parametric models .",
    "in particular , while it retains the nice interpretability of the linear models , it allows good flexibility in capturing the dynamic impacts of the relevant covariates on the response @xmath2 .",
    "in addition , in practical applications , some of the true covariates may have simply constant effects while the others have varying effects . such situations can be easily accommodated by a variant , the so called semi - varying coefficient model @xcite .",
    "furthermore , model ( [ eqn : e101 ] ) has been generalized to modeling various data types including count data , binary response , clustered / longitudinal data , time series , and so on .",
    "we refer to @xcite for a comprehensive review and the extensive literature .",
    "due to recent rapid developments in technology for data acquisition and storage , nowadays a lot of high - dimensional data sets are collected in various research fields where varying coefficient models find meanings and applications , such as medicine , marketing and so on . in such situations ,",
    "the model used to analyze the data is usually sparse , that is , the number of true covariates is not large even when the dimension is very large .",
    "therefore , under the sparsity condition , some effective variable selection procedures are necessary in order to carry out meaningful statistical estimation and inference . in this regard ,",
    "the penalized variable selection approach emerged as the mainstream in the recent decade .",
    "existing general penalty functions for sparse ( ultra-)high - dimensional models include the lasso @xcite , group lasso @xcite , adaptive lasso @xcite , scad @xcite and dantzig selector @xcite .    in ultra - high dimensional cases where the dimensionality @xmath10 is very large , selection consistency becomes challenging and nearly impossible for existing variable selection methods to achieve , however .",
    "thus , an additional independence screening step is usually necessary before variable selection is carried out .",
    "for example , sure independence screening ( sis ) methods are introduced by @xcite and @xcite for linear models and generalized linear models respectively , and nonparametric independence screening ( nis ) is suggested for additive models by @xcite . under general parametric models , @xcite suggested using the lasso at the screening stage before implementing a local linear approximation to the scad ( or general folded concave ) penalty at the second stage . in all of the above mentioned variable selection and independence screening methods , some tuning parameter or threshold value is involved which needs to be determined by the user or by some elaborated means . under the considered varying coefficient model ( [ eqn : e101 ] ) , there are some existing work on penalized variable selection in several different setups of the dimensionality @xmath10 , using the lasso or folded concave penalties such as the scad @xcite . in ultra - high dimensional cases , for the independence screening purpose ,",
    "the lasso is recommended by @xcite and nis is considered by several authors @xcite .",
    "again , all of these methods require selection of some tuning parameter or threshold value .    more recently , an alternative forward variable selection approach receives increasing attention for linear regression . the literature along this line includes the least angle regression ( lar ) @xcite , the forward iterative regression and shrinkage technique ( first ) @xcite , the forward lasso adaptive shrinkage ( flash ) @xcite , and the sequential lasso ( slasso ) @xcite .",
    "such methods enjoy desirable theoretical properties , including selection consistency , and have advantages from numerical aspects .",
    "motivated by the above observations , we propose and investigate thoroughly a forward variable selection procedure for the considered varying coefficient model in ultra - high dimensional covariate cases , where the dimensionality can be much larger than the sample size .",
    "the proposed method is constructed in a spirit similar to the slasso @xcite , which employes lasso in the forward selection and uses the ebic @xcite as the stopping criterion . however , the selection criterion of our method is based on the reduction in the sum of squared residuals , instead of the lasso .",
    "this is because our preliminary simulation studies suggested that the proposed one performs better than the analogue of the lasso for the varying coefficient model considered here .",
    "the stopping rule of the proposed forward selection procedure is based on the analogue of the ebic @xcite , or alternatively the bic , for the varying coefficient model .",
    "the consistency result of the ebic for model selection in ultra - high dimensional additive models is established by @xcite when the number of true covariates @xmath11 is bounded .",
    "the paper also assumes some knowledge of the number of true covariates , which may be unrealistic or difficult to obtain in some cases . on the other hand , without this kind of knowledge , the number of all possible subsets of the candidate variables to be considered is too large and there is no guarantee that ebic - based model selection will perform properly .",
    "therefore , it makes sense to consider a forward selection procedure , which does not require such prior knowledge , and use the ebic as the stopping criterion .",
    "suppose we have @xmath12 i.i.d .",
    "observations @xmath13 , where @xmath14 , taken from the varying coefficient model ( [ eqn : e101 ] ) : @xmath15 in our theoretical study , we deal with the ultra - high dimensional case where @xmath16 here , @xmath17 is a positive constant and @xmath18 is the dimension of the b - spline basis used in the estimation of the coefficient functions",
    ". we will give more details on the b - spline basis and specify more conditions on @xmath10 later in sections [ sec : procedure ] and [ sec : theorems ] ; especially see assumptions b(2 ) and b(3 ) for the conditions on @xmath10 . throughout this paper",
    ", @xmath19 denotes the number of elements of a set @xmath20 , and @xmath21 is the complement of @xmath20 .",
    "we write @xmath22 for the set of indexes of the true covariates in model ( [ eqn : e101 ] ) , that is , @xmath23 for @xmath24 and @xmath25 for @xmath26 .",
    "in addition , we write @xmath11 for the number of true covariates , i.e. @xmath27 , and consider the case that @xmath28 for some positive constant @xmath29 . here , condition ( [ eqn : e105 ] ) on @xmath11 is imposed for simplicity of presentation ; it can be relaxed at the expense of restricting slightly the order of the dimension @xmath10 specified in ( [ eqn : p ] ) .    under some assumptions we establish the selection consistency of our forward variable selection method when @xmath10 can be larger than @xmath12 and @xmath11",
    "can grow slowly with @xmath12 , as specified in ( [ eqn : p ] ) and ( [ eqn : e105 ] ) . importantly",
    ", this means that no independence screening is required before the proposed variable selection procedure .",
    "this nice property may be intuitively correct when dealing with sparse parametric models using methods like the slasso @xcite .",
    "but it is not obvious for varying coefficient models ; in model ( [ eqn : e101 ] ) each of the coefficient functions is modeled nonparametrically and involves @xmath18 parameters in its spline estimation .",
    "we exploit desirable properties of b - spline bases to drive these strong theoretical results .",
    "note also that our selection consistency results hold when either the ebic or the bic is used in the stopping rule .",
    "interestingly , contradictory to what is suggested for linear models , our simulation results indicate that for the considered varying coefficient model ( [ eqn : e101 ] ) the bic outperforms the ebic when they are used as the stopping criterion in the forward selection procedure . in fact , the ebic stopping rule tends to stop the forward selection too early and make it miss some important variables .",
    "the reason behind this is that the penalty on adding another variable is too large .",
    "some adjustments may be helpful in coping with this issue , but fortunately we can circumvent it by using simply the bic and our simulation results show it works very well .",
    "another problem worth of further study is whether the ebic is really better in forward selection ; it is to account for the large number of possible choices in model selection , but this issue vanishes in forward selection .    as mentioned earlier ,",
    "there exist some useful procedures for variable selection in varying coefficient modeling .",
    "nonetheless , the proposed method has many merits compared to them , from both practical and theoretical viewpoints .",
    "first , since the important variables are selected sequentially , the final model has good _ interpretability _ in the sense that we can rank the importance of the variables according to the order they are selected .",
    "second , in practice we may have some _ a priori _ knowledge that certain relevant variables should be included in the model . in this case , we always have the _ flexibility _ to start from any subset that contains them .",
    "third , our method employs reasonable sequential selection and stopping rules , and no tuning parameters or threshold parameters are present , meaning that the implementation and the computation are _ simple and fast_. fourth , there is a drastic gain in terms of _ numeric stability _ as no inversion of large matrices is necessary , as long as the number of true covariates @xmath11 is not large . by comparison ,",
    "existing variable selection methods all require independence screening in advance , but the nis and the group lasso tend to choose many covariates in order not to miss any true covariates ; thus inversion of large matrices is inevitable .",
    "( notice that the spline estimation of each of the coefficient functions involves @xmath18 number of parameters , which has to diverge to infinity with @xmath12 , and we have only one observation for each subject in the present setup . )",
    "fifth , same as @xcite , we improve on the order of @xmath10 as compared with the conditions in @xcite . in other words , the forward procedure can reduce the dimensionality more effectively .",
    "finally , our method requires _ milder regularity conditions _ than the sparse riesz condition @xcite and the restricted eigenvalue conditions @xcite for the lasso , which are related to all the candidate covariates ( then , there may be a large set of `` ill - behaved '' covariates with indexes outside of @xmath22 , especially when @xmath10 is very large ) .",
    "the assumptions we impose in section [ sec : theorems ] for the selection consistency of our method may fail to hold in some cases .",
    "nevertheless , in that case we can still use the proposed procedure for the purpose of independence screening , under a less restrictive setup specified in section [ screening ] .",
    "then , we will successfully reduce the number of covariates to a moderate order .",
    "this allows us to identify consistently the true covariates in the next stage , by applying the group scad or the adaptive group lasso procedure to the variables that pass the screening .",
    "see sections [ screening ] and [ sec : theorems ] for the details . besides",
    ", some of the coefficient functions may be constant i.e. @xmath30 for some @xmath31 .",
    "under such circumstances , we can carry out some group scad or adaptive lasso procedures to detect both the constant coefficients and the varying coefficients , as suggested in section 3 of @xcite .",
    "we refer to @xcite for such a two - stage approach , i.e. screening and then structure identification , and the theoretical and numerical justifications .",
    "note that , there are indeed some advantages in using the proposed forward procedure as a screening tool . in particular , it tends to remove more irrelevant variables than nis approaches do , and thus reducing the dimensionality more effectively .",
    "see section [ num : compare ] for some numerical comparisons .",
    "this paper is organized as follows . in section [ sec :",
    "procedure ] , we describe the proposed forward variable selection procedure .",
    "at each step , it uses the residual sum of squares resulted from spline estimation of an extended marginal model to determine the next candidate feature , and it uses the ebic or the bic to decide whether to stop or to include the newly selected feature and continue .",
    "we state the assumptions and theoretical results in section [ sec : theorems ] .",
    "results of simulation and empirical studies are presented in section [ sec : simulation ] .",
    "proofs of all the theoretical results are given in section [ sec : proofs ] .",
    "in this section , we describe the proposed forward feature selection procedure . before that",
    ", we introduce some notation . we write @xmath32 and @xmath33 for the @xmath34 and sup norm of a function @xmath35 on @xmath36 $ ] , respectively . when @xmath37 is a function of some random variable(s ) , we define the @xmath34 norm of @xmath37 by @xmath38^{1/2}$ ] . for a @xmath39-dimensional vector @xmath40 , @xmath41 stands for the euclidean norm and @xmath42",
    "is the transpose .",
    "we use the same symbol for transpose of matrices .",
    "recall @xmath22 is the set of true covariates in the varying coefficient model ( [ eqn : e101 ] ) .",
    "suppose that we have selected covariates sequentially and obtain index sets @xmath43 as follows : @xmath44 that is , @xmath45 is the index set of the selected covariates upon the completion of the @xmath46th step , for @xmath47 . note that @xmath48 can be the empty set @xmath49 , @xmath50 which corresponds to the intercept function , or some non - empty subset of @xmath22 given according to some _ a priori _ knowledge .",
    "then , at the current @xmath51th step , we need to choose another candidate from @xmath52 , and then we need to decide whether we should stop or add it to @xmath53 and go to the next step .",
    "our forward feature selection criterion is defined in ( [ eqn : e253 ] ) , and we employ a version of the ebic , given in ( [ eqn : e219 ] ) , as the stopping rule .",
    "see @xcite for more details about the ebic .      in this section",
    ", we consider spline estimation of the extended marginal model when we add another index to the current index set @xmath53 , which we will make use of in deriving our forward selection criterion .",
    "hereafter we write @xmath54 for @xmath55 for any @xmath56 .",
    "temporarily we consider the following extended marginal model for @xmath57 : @xmath58 here , the coefficient functions @xmath59 , @xmath60 , are defined in terms of minimizing the following mean squared error with respect to @xmath61 , @xmath62 where the minimization is over the set of @xmath34 integrable functions on @xmath36 $ ] .",
    "note that @xmath63 should be larger when @xmath64 than when @xmath65 .",
    "we will impose some assumptions on these coefficient functions later in this section and in section [ sec : theorems ] .",
    "first , we introduce some more notation related to the b - spline basis used in estimating the extended marginal model ( [ eqn : e201 ] ) .",
    "let @xmath66 denote the @xmath18-dimensional equi - spaced b - spline basis on @xmath36 $ ] .",
    "we assume that @xmath67 where @xmath68 .",
    "the order of the b - spline basis should be taken larger than or equal to two , under our smoothness assumptions on the coefficient functions in model ( [ eqn : e201 ] ) .",
    "assumptions b(4)-(5 ) given in section [ sec : theorems ] ensure that we can approximate the coefficient functions with the b - spline bases . see @xcite for the definition of b - spline bases .",
    "we write @xmath69 note that @xmath70 is a vector of regressors in the spline estimation of @xmath71 in model ( [ eqn : e201 ] ) , and @xmath72 and @xmath73 are respectively @xmath74 and @xmath75 matrices .",
    "based on the b - spline basis , we can approximate the varying coefficient model ( [ eqn : e103 ] ) by the following approximate regression model : @xmath76 where @xmath77 and @xmath78 , @xmath8 .",
    "similarly , the spline approximation model when the data come from the extended marginal model ( [ eqn : e201 ] ) is given by @xmath79 where @xmath80 and @xmath81 , @xmath82 , are defined by minimizing with respect to @xmath83 , @xmath82 , the following mean squared spline approximation error : @xmath84 with @xmath85 .",
    "note that @xmath86 should be close to the coefficient function @xmath87 in the extended marginal model ( [ eqn : e201 ] ) .",
    "in particular , when @xmath88 , @xmath89 should be large enough , and thus @xmath90 should be also large enough .",
    "we can estimate the vector parameters @xmath81 , @xmath82 , in model ( [ eqn : e207 ] ) by the ordinary least squares estimates , denoted by @xmath91 , @xmath82 .",
    "let @xmath92 and @xmath93 denote respectively the orthogonal projections of @xmath94 and @xmath95 onto the linear space spanned by the columns of @xmath73 , that is , @xmath96 note that @xmath97 is an @xmath74 matrix .",
    "then the ordinary least square estimate of @xmath98 , denoted by @xmath99 , can be expressed as @xmath100 where @xmath101 and @xmath102 .",
    "note that @xmath103 is the spline estimate of the coefficient function @xmath104 in the extended marginal model ( [ eqn : e201 ] ) .      recall that at the current step we are given @xmath53 , the index set of the covariates already selected , and the job is to choose from @xmath52 another candidate and then decide whether we should add it to @xmath53 or we should not and stop . for the purpose of forward feature selection",
    ", we consider the reduction in the sum of squared residuals , or equivalently the difference in the variance estimation , when adding @xmath105 to @xmath53 .",
    "specifically , we compute @xmath106 , where @xmath107 is the variance estimate for a subset of covariates indexed by @xmath108 given as @xmath109 using ( [ eqn : e217 ] ) , we can rewrite @xmath106 as @xmath110 where @xmath111 and @xmath112 is the projection of @xmath113 to @xmath114 with respect to the @xmath34 norm @xmath115 .    as noted earlier , if @xmath116 then @xmath117 will be large enough .",
    "furthermore , @xmath118 will have desirable properties under assumption x(2 ) given in section [ sec : theorems ] ; see lemma [ lem : lem1 ] for the details .",
    "hence , following from expression ( [ eqn : e251 ] ) and recalling that @xmath103 is the spline estimate of @xmath104 , we choose the candidate index as @xmath119 then , we have high confidence that @xmath120 belongs to @xmath121 provided that the latter is non - empty , and we take @xmath122 as the next candidate feature . at first , instead of ( [ eqn : e253 ] ) , we considered choosing @xmath123 as the next candidate index , as motivated by the sequential lasso for linear models proposed by @xcite .",
    "however , after some simulation studies we found that , contrary to the nice properties of its counterpart in linear models , ( [ eqn : e253 ] ) performs better for the varying coefficient model we study .    to determine whether or not to include the candidate feature @xmath122 in the set of selected ones , we employ the ebic criterion .",
    "specifically , we define the ebic of a subset of covariates indexed by @xmath108 as the following : @xmath124 where @xmath125 is a fixed constant and @xmath107 is given in ( [ eqn : sig ] ) .",
    "then , at the current @xmath51th step , we should select the new covariate @xmath122 with @xmath120 defined in ( [ eqn : e253 ] ) , provided that the ebic decreases when we add @xmath120 to @xmath53 and form @xmath126 . otherwise , if the ebic increases , we should not select any more covariates and stop at the @xmath39th step .",
    "note that the ebic defined in ( [ eqn : e219 ] ) reduces to the bic when @xmath125 is taken as 0 . and",
    ", the theoretical results given in section [ sec : theorems ] , in particular the consistency results given in theorem [ thm : thm2 ] , hold when either the ebic or the bic is used as the stopping criterion in the proposed method . in the following , we define formally the proposed forward feature selection algorithm . + * forward feature selection algorithm .",
    "*    _ initial step : _",
    "specify @xmath48 , which can be taken as the empty set @xmath49 , @xmath50 , or some non - empty subset of @xmath22 chosen based on some _ a priori _ knowledge , and compute @xmath127 .",
    "_ sequential selection : _ at the @xmath51th step , compute @xmath128 for every @xmath129 , and find @xmath130 then , let @xmath131 and compute @xmath132 . stop and declare @xmath133 as the set of selected covariate indexes if @xmath134 ; otherwise , change @xmath39 to @xmath135 and continue to search for the next candidate feature .    the forward procedure with the ebic stopping rule tends to stop a little too early and miss some relevant variables , and we need some kind of modification when we implement it . for example , some adjustment of the degrees of freedom will be helpful .",
    "all the details are given in section [ sec : simulation ] .",
    "we need some assumptions to establish consistency of the proposed procedure , especially assumption b(1 ) given below .",
    "when conditions b(1)-(2 ) are not fulfilled , another setup in which we can use the proposed method as a screening approach is given in section [ screening ] .",
    "in this paper , @xmath136 , @xmath137 , @xmath138 are generic positive constants and their values may change from line to line .",
    "recall that @xmath22 is the index set of the true variables in model ( [ eqn : e101 ] ) .",
    "* assumption b(1)-(2 ) *    b(1 ) for some large positive constant @xmath139 , @xmath140 uniformly in @xmath141 .",
    "note that @xmath139 should depend on the other assumptions on the covariates , specifically assumptions * x * and * t * given in section [ sec : theorems ] .",
    "b(2 ) set @xmath142 .",
    "we assume @xmath143 for some small positive constant @xmath144 .",
    "in addition , if @xmath145 in ( [ eqn : e219 ] ) i.e. if bic is used , we require that @xmath146    an assumption similar to assumption b(1 ) is imposed in @xcite and such assumptions are inevitable in establishing the selection consistency of forward procedures .",
    "these assumptions ensure that the chosen index @xmath120 , given in ( [ eqn : e253 ] ) , will be from @xmath121 .",
    "when such assumptions fail to hold , our method may choose some covariates from @xmath147 .",
    "however , these covariates will be removed at the second stage mentioned in the introduction .",
    "see section [ screening ] for more details .",
    "the first condition in assumption b(2 ) is related to the convergence rate of @xmath99 , and it ensures that the signals are large enough to be detected .",
    "if @xmath148 for some positive constants @xmath136 and @xmath137 , this condition is simply @xmath149 for some small positive constant @xmath144 , which is fulfilled by assumption ( [ eqn : p ] ) on @xmath10 .",
    "a few more assumptions on the coefficient functions @xmath87 will be given in section [ sec : theorems ] .",
    "the last condition in assumption b(2 ) is to ensure that , when the bic is used as the stopping criterion , our method can deal with ultra - high dimensional cases .",
    "for example , if @xmath18 is taken of the optimal order @xmath150 then @xmath10 can be taken as @xmath151 for any @xmath152 .",
    "some of the assumptions we impose in section [ sec : theorems ] may not hold .",
    "for example , assumption b(1 ) may not hold if some of the irrelevant variables have strong correlation with the true covariates indexed by @xmath22 .",
    "thus , such assumptions may be too restrictive in practice , in particular when @xmath10 is very large and @xmath11 is much smaller than @xmath10 as specified in ( [ eqn : p ] ) and ( [ eqn : e105 ] ) . in that case , the proposed forward selection procedure may be still used as a forward screening method under certain less restrictive conditions .",
    "then , although some unimportant variables may pass the forward screening , we can utilize some variable selection method to remove them at the next stage . in this section",
    "we discuss the details .",
    "suppose there is a subset of indexes , denoted by @xmath153 , that contains @xmath22 , and the covariates in @xmath153 do not have much correlation with those in @xmath154 . to be clear , we specify the conditions as follows :    ( a ) : :    @xmath155 and    @xmath156 for some positive constant @xmath157 .",
    "( b ) : :    @xmath158 uniformly for @xmath159 satisfying    @xmath160 and    @xmath161 .",
    "( c ) : :    assumption b(2 ) holds with @xmath162 replaced with    @xmath163 , where @xmath164 is defined    by @xmath165 with @xmath53 satisfying the    same conditions as in ( b ) .    if we replace conditions b(1 ) and b(2 ) with conditions ( b ) and ( c ) , respectively , and if condition ( a ) holds , then our procedure given in section [ algorithm ] can be used as a forward independence screening procedure with an effective stopping rule .",
    "that is , it will effectively select all the true covariates indexed by @xmath22 , possibly along with some irrelevant ones from those indexed by @xmath166 . see proposition [ prop : prop1 ] given in section [ sec : theorems ] for the theoretical justifications .",
    "those remaining irrelevant covariates will be removed when we apply at the second stage the group scad or adaptive group lasso @xcite .",
    "in this section , we describe technical assumptions , and we present desirable theoretical properties of the proposed forward procedure in theorems [ thm : thm1 ] and [ thm : thm2 ] . note that we treat the ebic and the bic ( @xmath145 ) in a unified way .",
    "the proofs are given in section [ sec : proofs ] .",
    "first we describe assumptions on the index variable @xmath9 in the varying coefficient model ( [ eqn : e101 ] ) .",
    "the following assumption is a standard one when we employ spline estimation .",
    "* assumption t. * the index variable @xmath9 has density function @xmath167 such that @xmath168 uniformly in @xmath169 $ ] , for some positive constants @xmath170 and @xmath171 .",
    "we define some more notation before we state our assumptions on the covariates .",
    "let @xmath172 consist of @xmath173 and then @xmath172 is a @xmath174-dimensional random vector .",
    "note that @xmath175 is a @xmath176-dimensional random vector . for a symmetric matrix @xmath177 ,",
    "we denote the maximum and minimum eigenvalues respectively by @xmath178 and @xmath179 , and we define @xmath180 as @xmath181    * assumption x. *    x(1 ) there is a positive constant @xmath182 such that @xmath183 , @xmath184 .",
    "x(2 ) uniformly in @xmath185 and @xmath186 , @xmath187 for some positive constants @xmath188 and @xmath189 .",
    "we use the second assumption x(2 ) when we evaluate eigenvalues of the matrix @xmath190",
    ". we can relax assumption x(1 ) slightly by replacing @xmath182 with @xmath191 for some positive constant @xmath192 .",
    "these are standard assumptions in the variable selection literature .",
    "assumption * e * below is about the error term @xmath5 in our varying coefficient model ( [ eqn : e101 ] ) .",
    "the second condition e(2 ) requires that @xmath5 should have the sub - gaussian property .",
    "we use it when we prove the latter half of theorem [ thm : thm2 ] .",
    "this is a standard assumption in the lasso literature , for example , see @xcite and @xcite .",
    "* assumption e. *    e(1 ) there are positive constants @xmath193 and @xmath194 such that @xmath195    e(2 ) there is a positive constant @xmath196 such that @xmath197 for any @xmath198 .",
    "we need some additional assumptions on the coefficient functions @xmath59 in the extended marginal model ( [ eqn : e201 ] ) in order to approximate them by the b - spline basis .",
    "note that , in assumptions b(4)-(5 ) below , @xmath199 for all @xmath31 and @xmath200 for all @xmath65 when @xmath201 .",
    "* assumption b(3)-(5)*.    b(3 ) @xmath202 and @xmath203 , where @xmath204 is defined in assumption b(2 ) .",
    "b(4 ) @xmath59 is twice continuously differentiable for any @xmath60 for @xmath205 and @xmath206 .",
    "b(5 ) there are positive constants @xmath207 and @xmath208 such that @xmath209 and @xmath210 uniformly in @xmath205 and @xmath206 .",
    "theorem [ thm : thm1 ] given below suggests that the forward selection procedure using criterion ( [ eqn : e253 ] ) can pick up all the relevant covariates in the varying coefficient model ( [ eqn : e101 ] ) when @xmath139 in assumption b(1 ) is large enough .",
    "[ thm : thm1 ] assume that assumptions * t * , * x * , b(1)-(5 ) , and e(1 ) hold , and define @xmath120 as in ( [ eqn : e253 ] ) for any @xmath211 .",
    "then , with probability tending to 1 , there is a positive constant @xmath212 such that @xmath213 uniformly in @xmath53 , and thus we have @xmath214 for any @xmath211 when @xmath139 in assumption b(1 ) is larger than @xmath215 .",
    "theorem [ thm : thm2 ] given next implies that the proposed forward procedure will not stop until all of the relevant variables indexed by @xmath22 have been selected , and it does stop when all the true covariates in model ( [ eqn : e101 ] ) have been selected .",
    "note that in the second result , we have to replace assumption e(1 ) with e(2 ) in order to evaluate a quadratic form of error terms in the proof .",
    "[ thm : thm2 ] assume that assumptions * t * , * x * , b(1)-(5 ) , and e(1 ) hold .",
    "then we have the following results .",
    "\\(i ) for @xmath120 as in theorem [ thm : thm1 ] , we have @xmath216 uniformly in @xmath141 , with probability tending to 1 .",
    "\\(ii ) if we replace assumption e(1 ) with assumption e(2 ) , then we have @xmath217 uniformly in @xmath218 , with probability tending to 1",
    ".    the forward method may also choose some irrelevant covariates if assumption b(1 ) fails to hold .",
    "in that case , proposition [ prop : prop1 ] provides some theoretical results in the setup described in section [ screening ] .",
    "note that some conformable changes to assumptions b(3)-(5 ) and x(2 ) and the proofs are needed .",
    "see section [ sec : proofs ] for the changes in the proofs .",
    "[ prop : prop1 ] consider the setup given in section [ screening ] . under the same conditions in theorem",
    "[ thm : thm1 ] ( or theorem [ thm : thm2 ] ) , with conformable changes to assumptions b(3)-(5 ) and x(2 ) , we have the following results .",
    "\\(i ) the selected index @xmath120 comes only from @xmath153 with probability tending to 1 , as in theorem [ thm : thm1 ] .",
    "\\(ii ) with probability tending to 1 , the proposed forward selection procedure continues the feature selection until all the covariates indexed by @xmath22 are selected , and it stops the selection when all the covariates indexed by @xmath22 have been selected .",
    "proposition [ prop : prop1 ] implies that the proposed forward selection procedure can be used as a forward screening method with an effective stopping rule .",
    "note that , in this setup , we may select some irrelevant covariates from those indexed by @xmath219 .",
    "however , the number of potential covariates will be sufficiently reduced after the forward screening stage .",
    "thus , we will be able to remove those remaining irrelevant covariates at the next stage , by using the group scad or the adaptive group lasso @xcite .",
    "we carried out two simulation studies and a real data analysis based on the well - known boston housing data to assess the performance of the proposed forward feature selection method with bic or ebic as the stopping criterion .",
    "for simplicity , we denote these two variants by fbic and febic respectively . at the initial step of the forward selection , we let @xmath220 i.e. we start with the model with only the intercept function . note that it may happen that the bic / ebic drops in one iteration , then increases in the next iteration , and then drops again . to avoid interference caused by such small fluctuations , we continued the fbic / febic forward selection until the bic / ebic continuously increases for five consecutive iterations .",
    "the value of the parameter @xmath125 in the definition ( [ eqn : e219 ] ) of ebic was taken as @xmath221 , as suggested by @xcite .",
    "since the ebic uses a much larger penalty than the bic does , it is expected that the febic will select a smaller model than that selected by the fbic .",
    "we could modify the penalty term by adjusting the degrees of freedom or change the value of @xmath125 to a smaller one , but it becomes complicated .    in the simulation studies , we generated data from the two varying coefficient models studied by @xcite .",
    "following the paper , we used the cubic b - spline with @xmath222 , we set the sample size and the number of covariates as @xmath223 and @xmath224 respectively , and we repeated each of the simulation configuration for @xmath225 times .      in this section , we compare the finite sample performance of the fbic and the febic using the two varying coefficient models studied by @xcite .",
    "[ exp1 ] following example 3 of @xcite , we generated @xmath226 samples from the following varying coefficient model : @xmath227 where @xmath228 , and @xmath229 , with @xmath230 , and @xmath231 being all mutually independent with each other .",
    ".correlations between the covariates @xmath232 s and the index variable @xmath9 . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     from this example , it is seen that the fbic approach or its modified version is very useful in scientific discoveries based on high - dimensional data with complex structure .",
    "it can select a parsimonious close - to - truth model , and can reveal interesting relationship between the response variable and the important covariates .",
    "first , we define some notation related to the approximate regression models ( [ eqn : e205 ] ) and ( [ eqn : e207 ] ) . let @xmath233 then , the parameter vector @xmath234 in model ( [ eqn : e207 ] ) can be expressed as @xmath235 , where @xmath236 denotes the @xmath237 zero matrix and @xmath238 is the @xmath18-dimensional identity matrix .    before we prove theorems [ thm : thm1 ] and [ thm : thm2 ] , we present lemmas [ lem : lem1]-[lem : lem3 ] .",
    "we verify these lemmas at the end of this section . in lemma",
    "[ lem : lem1 ] we evaluate the minimum and maximum eigenvalues of some matrices .    [ lem : lem1 ] assume that assumptions * t * , * x * , and e(1 ) hold .",
    "then , with probability tending to 1 , there are positive constants @xmath239 , @xmath240 , @xmath241 , and @xmath242 such that @xmath243 uniformly in @xmath185 and @xmath206 .          [",
    "lem : lem3 ] assume that assumptions * t * , * x * , and b(4)-(5 ) hold .",
    "then , for any @xmath249 , there are positive constants @xmath250 , @xmath251 , @xmath252 , and @xmath253 such that @xmath254 uniformly in @xmath185 and @xmath206 , with probability @xmath255    [ [ proofs - of - theorems - thmthm1-and - thmthm2-and - proposition - propprop1 ] ] proofs of theorems [ thm : thm1 ] and [ thm : thm2 ] , and proposition [ prop : prop1 ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    now we prove theorems [ thm : thm1 ] and [ thm : thm2 ] by employing lemmas [ lem : lem1]-[lem : lem3 ] . + * proof of theorem [ thm : thm1 ] .",
    "* consider the case that @xmath185 and @xmath206 .",
    "note we can write @xmath256 lemma [ lem : lem1 ] implies we should deal with @xmath257 on the right - hand side of ( [ eqn : e501 ] ) when we evaluate @xmath258 given in equation ( [ eqn : e251 ] ) . for this purpose , assumption b(2 )",
    "suggests that we should take @xmath259 in lemma [ lem : lem3 ] as @xmath260 tending to @xmath261 .",
    "recall the definition of @xmath204 in assumption b(2 ) .",
    "then we have that @xmath262    by ( [ eqn : e503 ] ) , ( [ eqn : e505 ] ) , and lemma [ lem : lem3 ] , @xmath263 is negligible compared to @xmath264 on the right - hand side of ( [ eqn : e501 ] ) , with probability tending to 1 . therefore lemmas [ lem : lem1 ] and [ lem : lem2 ] and assumption b(3 ) imply that we should focus on @xmath265 in evaluating @xmath266 in ( [ eqn : e251 ] ) .",
    "hence the desired result follows from assumption b(1 ) .",
    "@xmath267    * proof of theorem [ thm : thm2 ] . * to prove result ( i ) , we evaluate @xmath268 since @xmath269 we have @xmath270 then lemma [ lem : lem1 ] and ( [ eqn : e507 ] ) imply that we have for some positive @xmath157 , @xmath271 uniformly in @xmath211 and @xmath206 , with probability tending to 1 .",
    "here we use the fact that @xmath272 is uniformly bounded with probability tending to 1 .",
    "then as in the proof of theorem [ thm : thm1 ] , we should consider @xmath273 in evaluating the right - hand side of ( [ eqn : e509 ] ) . since assumption b(2 ) implies that @xmath274 we have from ( [ eqn : e509 ] ) that @xmath275 uniformly in @xmath211 and @xmath206 satisfying @xmath276 , with probability tending to 1 .",
    "hence the proof of result ( i ) is complete .    to prove result ( ii ) , recall that we replace assumption e(1 ) with assumption e(2 ) .",
    "we should evaluate @xmath277 for @xmath218 .",
    "it is easy to prove that @xmath278 converges to @xmath279 in probability and the details are omitted .",
    "we denote @xmath280 by @xmath281 , which is an orthogonal projection matrix .",
    "thus , from ( [ eqn : e511 ] ) we have for some positive @xmath157 , @xmath282 uniformly in @xmath218 , with probability tending to 1 .",
    "now we evaluate @xmath283 on the right - hand side of ( [ eqn : e513 ] ) . from the definition of @xmath284",
    ", we have @xmath285 for any @xmath286 . therefore we obtain @xmath287 where @xmath288 and @xmath289 is some @xmath12-dimensional vector of spline approximation errors satisfying @xmath290 uniformly in @xmath218 . by applying proposition 3 of @xcite , we obtain @xmath291 where @xmath292 .",
    "we take @xmath293 with @xmath294 tending to @xmath295 sufficiently slowly . then from the above inequality",
    ", we have @xmath296 uniformly in @xmath218 .",
    "thus we have @xmath297 uniformly in @xmath218 .",
    "hence the desired result follows from ( [ eqn : e513 ] ) , ( [ eqn : e515 ] ) , and the assumption that @xmath67 with @xmath68 . note that , here we use the condition that @xmath298 when @xmath299 , which is stated in assumption b(2 ) .",
    "@xmath267    * proof of proposition [ prop : prop1 ] . *",
    "the first result follows from almost the same arguments as in the proof of theorem [ thm : thm1 ] , thus we omit the proof .",
    "we just comment on proof of the second one , which corresponds to result ( ii ) of theorem [ thm : thm2 ] .",
    "we should deal with @xmath53 such that @xmath300 in the proof .",
    "then we replace @xmath301 in ( [ eqn : e511 ] ) with @xmath302 and replace @xmath281 with @xmath303 everywhere .",
    "nevertheless , we still have @xmath304 uniformly in @xmath53 and @xmath206 by exploiting ( [ eqn : e514 ] ) .",
    "there is no change about the b - spline approximation .",
    "thus we obtain the version of ( [ eqn : e513 ] ) and ( [ eqn : e515 ] ) with @xmath22 replaced by @xmath53 , and the modified ( [ eqn : e513 ] ) and ( [ eqn : e515 ] ) hold uniformly in @xmath53 .",
    "hence the latter half of proposition [ prop : prop1 ] is established .",
    "note that some minor conformable changes to the assumptions are necessary .",
    "we use the following inequalities in the proofs of lemmas [ lem : lem1]-[lem : lem2 ] .",
    "@xmath305 where @xmath306 and @xmath307 are positive constants independent of @xmath18 .",
    "see @xcite for the proof of ( [ eqn : e517 ] ) . + * proof of lemma [ lem : lem1 ] .",
    "* write @xmath308 where @xmath309 is the @xmath310th sample version of @xmath175 and @xmath311 is the kronecker product . note that ( [ eqn : e517 ] ) , ( [ eqn : e519 ] ) , and assumption x(2 ) imply that , for any @xmath249 , @xmath312 for some positive @xmath136 and @xmath137 .",
    "in addition , by exploiting the band - diagonal property of @xmath313 and @xmath314 and an exponential inequality , we can demonstrate that @xmath315 uniformly in @xmath211 and @xmath206 with probability @xmath316 where @xmath317 , @xmath318 , and @xmath319 are positive constants independent of @xmath11 , @xmath18 , @xmath12 , @xmath10 , and @xmath259 .",
    "when we take @xmath320 , the probability in ( [ eqn : e523 ] ) tends to 0 and the former result follows since @xmath321 .",
    "the latter result follows from the following relationship between @xmath322 and @xmath323 : @xmath324 @xmath267 * proof of lemma [ lem : lem2 ] .",
    "* let @xmath325 be a set of square integrable functions on @xmath36 $ ] .",
    "then assumption x(2 ) implies that @xmath326 besides , assumption * t * implies @xmath327 for any square integrable function @xmath328 .",
    "in addition , due to assumptions b(4 ) and b(5 ) , we can choose some positive constant @xmath136 and a set of @xmath18-dimensional vectors @xmath329 such that @xmath330 where @xmath136 depends only on the assumptions .    by exploiting ( [ eqn : e525])-([eqn : e529 ] )",
    ", we obtain @xmath331 therefore , there is a positive constant @xmath137 such that @xmath332 this implies that @xmath333 the desired result follows from ( [ eqn : e517 ] ) and ( [ eqn : e531 ] ) .",
    "@xmath267      we have @xmath336 from the definition of the b - spline basis . as in the proof of lemma 2 of @xcite",
    ", we have @xmath337 uniformly in @xmath211 and @xmath206 with probability @xmath338 where @xmath137 , @xmath317 , and @xmath318 are positive constants independent of @xmath11 , @xmath18 , @xmath12 , @xmath10 , and @xmath259 .    by combining the above results , ( [ eqn : e522 ] ) , and lemma [ lem : lem1 ]",
    ", we obtain @xmath339 uniformly in @xmath211 and @xmath206 , with probability given in the lemma .",
    "note that @xmath319 is independent of @xmath11 , @xmath18 , @xmath12 , and @xmath259 .",
    "hence the desired result follows from ( [ eqn : e533 ] ) and ( [ eqn : e535 ] ) . @xmath267      a. antoniadis , i. gijbels and a .verhasselt , variable selection in varying - coefficient models using p - splines , journal of computational and graphical statistics 21 ( 2012 ) 638 - 661 .",
    "p. j. bickel , y. a. ritov and a. b. tsybakov , simultaneous analysis of lasso and dantzig selector , annals of statistics 37 ( 2009 ) 1705 - 1732 .",
    "e. candes and t. tao , the dantzig selector : statistical estimation when p is much larger than n , annals of statistics 35 ( 2007 ) 2313 - 2351 .",
    "j. chen and z. chen , extended bayesian information criteria for model selection with large model spaces , biometrika 95 ( 2008 ) 759 - 771 .",
    "cheng , t. honda , j. li and h. peng , nonparametric independence screening and structural identification for ultra - high dimensional longitudinal data , forthcoming in annals of statistics and at arxiv preprint arxiv:1308.3942 ( 2014 ) .",
    "b. efron , t. hastie , i. johnstone and r. tibshirani , least angle regression ( with discussions ) , annals of statistics 32 ( 2004 ) 407 - 499 .",
    "j. fan , y. feng and r. song , nonparametric independence screening in sparse ultra - high - dimensional additive models , journal of the american statistical association 106 ( 2011 ) 544 - 557 . j. fan and r. li , variable selection via nonconcave penalized likelihood and its oracle properties , journal of the american statistical association 96 ( 2001 ) 1348 - 1360 .",
    "j. fan and j. lv , sure independence screening for ultrahigh dimensional feature space , journal of the royal statistical society : series b 70 ( 2008 ) 849 - 911 .",
    "j. fan , y. ma and w. dai , nonparametric independence screening in sparse ultra - high dimensional varying coefficient models , forthcoming in journal of the american statistical association",
    ". j. fan and r. song , sure independence screening in generalized linear models with np - dimensionality , annals of statistics 38 ( 2010 ) 3567 - 3604 .",
    "j. fan , l. xue and h. zou , ( 2014 ) .",
    "strong oracle optimality of folded concave penalized estimation , annals of statistics 42 ( 2014 ) 819 - 849 .",
    "j. fan and w. zhang , statistical methods with varying coefficient models , statistics and its interface , 1 ( 2008 ) , 179 - 195 .",
    "d. harrison and d. rubinfeld , hedonic housing prices and the demand for clean air , journal of environmental econometrics and management 5 , 81 - 102 .",
    "j. z. huang , c. o. wu and l. zhou , polynomial spline estimation and inference for varying coefficient models with longitudinal data , statistica sinica 14 ( 2004 ) 763 - 788 .",
    "w. y. hwang , h. h. zhang and s. ghosal , first : combining forward iterative selection and shrinkage in high dimensional sparse linear regression , statistics and interface 2 ( 2009 ) 341 - 348 .",
    "h. lian , variable selection for high - dimensional generalized varying - coefficient models , statistica sinica 22 ( 2012 ) 1563 - 1588 .",
    "h. lian , semiparametric bayesian information criterion for model selection in ultra - high dimensional additive models , journal of multivariate analysis 123 ( 2014 ) 304 - 310 .",
    "j. liu , r. li , r. wu , feature selection for varying coefficient models with ultrahigh dimensional covariates , journal of the american statistical association 109 ( 2014 ) 266 - 274 .",
    "s. luo and z. chen , sequential lasso cum ebic for feature selection with ultra - high dimensional feature space , forthcoming in journal of the american statistical association .",
    "l. meier , s. van de geer and p. bhlmann , the group lasso for logistic regression , journal of the royal statistical society : series b 70 ( 2008 ) 53 - 71 . h. s. noh and b. u. park , sparse variable coefficient models for longitudinal data , statistica sinica 20 ( 2010 ) 1183 - 1202 .",
    "p. radchenko and g. m. james , improved variable selection with forward - lasso adaptive shrinkage , annals of applied statistics 5 ( 2011 ) 427 - 448",
    ". l. l. schumaker , spline functions : basic theory 3rd ed , cambridge university press , cambridge , 2007 .",
    "r. song , f. yi and h. zou , on varying - coefficient independence screening for high - dimensional varying - coefficient models , statistica sinica 24 ( 2014 ) 1735 - 1752 .",
    "y. tang , h. j. wang , z. zhu , x. song , a unified variable selection approach for varying coefficient models , statistica sinica 22 ( 2012 ) 601 - 628 .",
    "r. j. tibshirani , regression shrinkage and selection via the lasso , journal of the royal statistical society : series b 58 ( 1996 ) 267 - 288 .",
    "l. wang , h. li and j. z. huang , variable selection in nonparametric varying - coefficient models for analysis of repeated measurements , journal of the american statistical association 103 ( 2008 ) 172 - 183 .",
    "f. wei , j. huang and h. li , variable selection and estimation in high - dimensional varying - coefficient models , statistica sinica 21 ( 2011 ) 1515 - 1540 .",
    "l. xue and a. qu , variable selection in high - dimensional varying - coefficient models with global optimality , journal of machine learning research 13 ( 2012 ) 1973 - 1998 .",
    "y. xia , w. zhang and h. tong , efficient estimation for semivarying - coefficient models , biometrika 91 ( 2004 ) 661 - 681 m. yuan and y. lin , model selection and estimation in regression with grouped variables , journal of the royal statistical society : series b 68 ( 2006 ) 49 - 67 .",
    "c. h. zhang , ( 2010 ) .",
    "nearly unbiased variable selection under minimax concave penalty , annals of statistics 38 ( 2010 ) 894 - 942 .",
    "w. zhang , s. lee and x. song , local polynomial fitting in semivarying coefficient model , journal of multivariate analysis 82 ( 2002 ) 166 - 188 .",
    "p. zhao and l. xue , variable selection in semiparametric regression analysis for longitudinal data , annals of the institute of statistical mathematics 64 ( 2012 ) 213 - 231 .",
    "h. zou , the adaptive lasso and its oracle properties , journal of the american statistical association 101 ( 2006 ) 1418 - 1429 ."
  ],
  "abstract_text": [
    "<S> varying coefficient models have numerous applications in a wide scope of scientific areas . </S>",
    "<S> while enjoying nice interpretability , they also allow flexibility in modeling dynamic impacts of the covariates . </S>",
    "<S> but , in the new era of big data , it is challenging to select the relevant variables when there are a large number of candidates . </S>",
    "<S> recently several work are focused on this important problem based on sparsity assumptions ; they are subject to some limitations , however . </S>",
    "<S> we introduce an appealing forward variable selection procedure . </S>",
    "<S> it selects important variables sequentially according to a sum of squares criterion , and it employs an ebic- or bic - based stopping rule . clearly it is simple to implement and fast to compute , and it possesses many other desirable properties from both theoretical and numerical viewpoints . </S>",
    "<S> we establish rigorous selection consistency results when either ebic or bic is used as the stopping criterion , under some mild regularity conditions . </S>",
    "<S> notably , unlike existing methods , an extra screening step is not required to ensure selection consistency . even if the regularity conditions fail to hold , our procedure is still useful as an effective screening procedure in a less restrictive setup . </S>",
    "<S> we carried out simulation and empirical studies to show the efficacy and usefulness of our procedure .    * forward variable selection for sparse ultra - high dimensional varying coefficient models *    * ming - yen cheng , toshio honda , and jin - ting zhang *    * keywords * : b - spline ; ebic ; independence screening ; marginal model ; semi - varying coefficient models ; sub - gaussion error ; structure identification .    </S>",
    "<S> @xmath0ming - yen cheng is professor , department of mathematics , national taiwan university , taipei 106 , taiwan ( email : cheng@math.ntu.edu.tw ) . </S>",
    "<S> toshio honda is professor , graduate school of economics , hitotsubashi university , 2 - 1 naka , kunitachi , tokyo 186 - 8601 , japan ( email : t.honda@r.hit-u.ac.jp ) . </S>",
    "<S> jin - ting zhang is associate professor , department of statistics & applied probability , national university of singapore , 3 science drive 2 , singapore 117546 ( email : stazjt@nus.edu.sg ) . </S>",
    "<S> this research is partially supported by the hitotsubashi international fellow program and the mathematics division , national center of theoretical sciences ( taipei office ) . </S>",
    "<S> cheng is supported by the ministry of science and technology grant most101 - 2118-m-002 - 001-my3 . </S>",
    "<S> honda is supported by the jsps grant - in - aids for scientific research ( a ) 24243031 and ( c ) 25400197 . </S>",
    "<S> zhang is supported by the national university of singapore research grant r-155 - 000 - 128 - 112 . </S>"
  ]
}