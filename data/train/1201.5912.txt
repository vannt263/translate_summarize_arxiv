{
  "article_text": [
    "the problem of maximum likelihood ( ml ) estimation consists of finding a solution of the form @xmath0 where @xmath1 is an observed sample of a random variable @xmath2 defined on a sample space @xmath3 and @xmath4 is the log - likelihood function defined by @xmath5 defined on the parameter space @xmath6 , and @xmath7 denotes the density of @xmath2 at @xmath1 parametrized by the vector parameter @xmath8 .",
    "the expectation maximization ( em ) algorithm is an iterative procedure which is widely used for solving ml estimation problems . the em algorithm was first proposed by dempster , laird and rubin and has seen the number of its potential applications increase substantially since its appearance .",
    "the book of mclachlan and krishnan gives a comprehensive overview of the theoretical properties of the method and its applicability .",
    "the convergence of the sequence of em iterates towards a maximizer of the likelihood function was claimed in the original paper but it was later noticed that the proof contained a flaw .",
    "a careful convergence analysis was finally given by wu @xcite based on zangwill s general theory @xcite ; see also .",
    "zangwill s theory applies to general iterative schemes and the main task when using it is to verify that the assumptions of zangwill s theorems are satisfied . since the appearance of wu s paper , convergence of the em algorithm",
    "is often taken for granted in many cases where the necessary assumptions were sometimes not carefully justified .",
    "as an example , an often neglected issue is the behavior of em iterates when they approach the boundary of the domain of definition of the functions involved .",
    "a different example is the following .",
    "it is natural to try and establish that em iterates actually converge to a single point @xmath9 , which involves proving uniqueness of the cluster point .",
    "wu s approach , reported in is based on the assumption that the euclidean distance between two successive iterates tends to zero .",
    "however such an assumption is in fact very hard to verify in most cases and should not be deduced solely from experimental observations .",
    "the goal of the present paper is to propose an analysis of em iterates and their generalizations in the framework of kullback proximal point algorithms .",
    "we focus on the geometric conditions that are provable in practice and the concrete difficulties concerning convergence towards boundaries and cluster point uniqueness .",
    "the approach adopted here was first proposed in in which it was shown that the em algorithm could be recast as a proximal point algorithm . a proximal scheme for maximizing the function @xmath4 using the distance - like function @xmath10 is an iterative procedure of the form @xmath11 where @xmath12 is a sequence of positive real numbers often called relaxation parameters .",
    "proximal point methods were introduced by martinet @xcite and rockafellar @xcite in the context of convex minimization .",
    "the proximal point representation of the em algorithm is obtained by setting @xmath13 and @xmath14 to the kullback distance between some well specified conditional densities of a complete data vector .",
    "the general case of @xmath15 was called the kullback proximal point algorithm ( kpp ) .",
    "this approach was further developed in where convergence was studied in the twice differentiable case with the assumption that the limit point lies in the interior of the domain .",
    "the main novelty of was to prove that relaxation of the kullback - type penalty could ensure superlinear convergence which was confirmed by experiment for a poisson linear inverse problem .",
    "this paper is an extension of these previous works that addresses the problem of convergence under general conditions .",
    "the main results of this paper are the following .",
    "firstly , we prove that all the cluster points of the kullback proximal sequence which lie in the interior of the domain are stationary points of the likelihood function @xmath16 under very mild assumptions that are easily verified in practice . secondly ,",
    "taking into account finer properties of @xmath10 , we prove that every cluster point on the boundary of the domain satisfies the karush - kuhn - tucker necessary conditions for optimality under nonnegativity constraints .",
    "to illustrate our results , we apply the kullback - proximal algorithm to an estimation problem in animal carcinogenicity introduced in in which an interesting nonconvex constraint is handled . in this case",
    ", the m - step can not be obtained in closed form .",
    "however , the kullback - proximal algorithm can be analyzed and implemented .",
    "numerical experiments are provided which demonstrate the ability of the method to significantly accelerate the convergence of standard em .",
    "the paper is organized as follows . in section",
    "[ kull ] , we review the kullback proximal point interpretation of em .",
    "then , in section [ conv ] we study the properties of interior cluster points .",
    "we prove that such cluster points are in fact global maximizers of a certain penalized likelihood function .",
    "this allows us to justify using a relaxation parameter @xmath17 when @xmath17 is sufficiently small to permit avoiding saddle points .",
    "section [ conv2 ] pursues the analysis in the case where the cluster point lies on a boundary of the domain of @xmath10 .",
    "in this section , we review the em algorithm and the kullback proximal interpretation discussed in .",
    "the em procedure is an iterative method which produces a sequence @xmath18 such that each @xmath19 maximizes a local approximation of the likelihood function in the neighborhood of @xmath20 .",
    "this point of view will become clear in the proximal point framework of the next subsection .    in the traditional approach",
    ", one assumes that some data are hidden from the observer .",
    "a frequent example of hidden data is the class to which each sample belongs in the case of mixtures estimation .",
    "another example is when the observed data are projection of an unkown object as for image reconstruction problems in tomography .",
    "one would prefer to consider the likelihood of the complete data instead of the ordinary likelihood .",
    "since some parts of the data are hidden , the so called complete likelihood can not be computed and therefore must be approximated . for this purpose , we will need some appropriate notations and assumptions which we now describe .",
    "the observed data are assumed to be i.i.d .",
    "samples from a unique random vector @xmath2 taking values on a data space @xmath3 .",
    "imagine that we have at our disposal more informative data than just samples from @xmath2 .",
    "suppose that the more informative data are samples from a random variable @xmath21 taking values on a space @xmath22 with density @xmath23 also parametrized by @xmath8 .",
    "we will say that the data @xmath21 is more informative than the actual data @xmath2 in the sense that @xmath2 is a compression of @xmath21 , i.e. there exists a non - invertible transformation @xmath24 such that @xmath25 .",
    "if one had access to the data @xmath21 it would therefore be advantageous to replace the ml estimation problem ( [ ml ] ) by @xmath26 with @xmath27 . since @xmath28 the density @xmath29 of @xmath2 is related to the density @xmath30 of @xmath21 through @xmath31 for an appropriate measure @xmath32 on @xmath22 . in this setting ,",
    "the data @xmath1 are called _ incomplete data _ whereas the data @xmath33 are called _",
    "complete data_.    of course the complete data @xmath33 corresponding to a given observed sample @xmath1 are unknown . therefore , the complete data likelihood function @xmath34 can only be estimated . given the observed data @xmath1 and a previous estimate of @xmath8 denoted @xmath35 , the following minimum mean square error estimator ( mmse ) of the quantity @xmath34 is natural @xmath36,\\ ] ] where , for any integrable function @xmath37 on @xmath22 , we have defined the conditional expectation @xmath38=\\int_{h^{-1}(\\{y\\ } ) } f(x ) k(x| y;\\bar{\\theta } ) d\\mu(x)\\ ] ] and @xmath39 is the conditional density function given @xmath1 @xmath40    having described the notions of complete data and complete likelihood and its local estimation we now turn to the em algorithm .",
    "the idea is relatively simple : a legitimate way to proceed is to require that iterate @xmath19 be a maximizer of the local estimator of the complete likelihood conditionally on @xmath1 and @xmath20 .",
    "hence , the em algorithm generates a sequence of approximations to the solution ( [ mlx ] ) starting from an initial guess @xmath41 of @xmath42 and is defined by @xmath43 \\text{\\hspace{1cm}\\bf e step}\\nonumber\\ ] ] @xmath44      consider the general problem of maximizing a concave function @xmath45 .",
    "the original proximal point algorithm introduced by martinet @xcite is an iterative procedure which can be written @xmath46 the quadratic penalty @xmath47 is relaxed using a sequence of positive parameters @xmath48 . in @xcite , rockafellar showed that superlinear convergence of this method is obtained when the sequence @xmath48 converges towards zero .",
    "it was proved in that the em algorithm is a particular example in the class of proximal point algorithms using kullback leibler types of penalties .",
    "one proceeds as follows .",
    "assume that the family of conditional densities @xmath49 is regular in the sense of ibragimov and khasminskii , in particular @xmath50 and @xmath51 are mutually absolutely continuous for any @xmath8 and @xmath35 in @xmath52 .",
    "then the radon - nikodym derivative @xmath53 exists for all @xmath54 and we can define the following kullback leibler divergence : @xmath55.\\ ] ] we are now able to define the kullback - proximal algorithm . for this purpose , let us define @xmath56 as the domain of @xmath16 , @xmath57 the domain of @xmath58 and @xmath59 the domain of @xmath60 .",
    "let @xmath12 be a sequence of positive real numbers .",
    "then , the kullback - proximal algorithm is defined by @xmath61    the main result on which the present paper relies is that em algorithm is a special case of ( [ kullprox ] ) , i.e. it is a penalized ml estimator with proximal penalty @xmath14 .",
    "[ equiem ] the em algorithm is a special instance of the kullback - proximal algorithm with @xmath62 , for all @xmath63 .",
    "the previous definition of the kullback proximal algorithm may appear overly general to the reader familiar with the usual practical interpretation of the em algorithm .",
    "however , we found that such a framework has at least the three following benefits :    * to our opinion , the convergence proof of our em is more natural , * the kullback proximal framework may easily incorporate additional constraints , a feature that may be of crucial importance as demonstrated in the example of section [ prob2 ] below , * the relaxation sequence @xmath12 allows one to weight the penalization term and its convergence to zero implies quadratic convergence in certain examples .",
    "the first of these three arguments is also supported by our simplified treatment of the componentwise em procedure proposed in and the remarkable recent results of @xcite based on a special proximal entropic representation of em for getting precise estimates on the convergence speed of em algorithms , however , with much more restrictive assumptions than the ones of the present paper .",
    "although our results are obtained under mild assumptions concerning the relaxation sequence @xmath12 including the case @xmath64 , several precautions should be taken when implementing the method . however , one of the key features of em - like procedures is to allow easy handling of positivity or more complex constraints , such as the ones discussed in the example of section [ prob2 ] . in such cases",
    "the function @xmath10 behaves like a barrier whose value increases to infinity as the iterates approach the boundary of the constraint set .",
    "hence , the sequence @xmath12 ought to be positive in order to exploit this important computational feature . on the other hand ,",
    "as proved under twice differentiability assumptions in when the cluster set reduces to a unique nondegenerate maximizer in the interior of the domain of the log - likelihood and @xmath65 converges to zero , quadratic convergence is obtained .",
    "this nice behavior is not satisfied in the plain em case where @xmath13 for all @xmath63 . as a drawback ,",
    "one problem in decreasing the @xmath65 s too quickly is possible numerical ill conditioning .",
    "the problem of choosing the relaxation sequence is still largely open .",
    "we have found however that for most `` reasonable '' sequences , our method was at least as fast as the standard em .",
    "finally , we would like to end our presentation of kpp - em by noting that closed form iterations may not be available in the case @xmath66 .",
    "if this is the case , solving ( [ kullprox ] ) becomes a subproblem which will require iterative algorithms . in some interesting examples ,",
    "e.g. the case presented in section [ prob2 ] . in this case",
    ", the standard em iterations are not available in closed form in the first place and kpp - em provides faster convergence while preserving monotonicity and constraint satisfaction .",
    "the notation @xmath67 will be used to denote the norm on any previously defined space without more precision .",
    "the space on which it is the norm should be obvious from the context .",
    "for any bivariate function @xmath68 , @xmath69 will denote the gradient with respect to the first variable . in the remainder of this paper",
    "we will make the following assumptions .",
    "[ ass1 ] ( i ) @xmath16 is differentiable on @xmath56 and @xmath4 tends to @xmath70 whenever @xmath71 tends to @xmath72 .",
    "+ ( ii ) the projection of @xmath59 onto the first coordinate is a subset of @xmath56 .",
    "+ ( iii ) @xmath73 is a convergent nonnegative sequence of real numbers whose limit is denoted by @xmath74 .",
    "we will also impose the following assumptions on the distance - like function @xmath10 .",
    "[ ass2 ] ( i ) there exists a finite dimensional euclidean space @xmath75 , a differentiable mapping @xmath76 and a functional @xmath77 such that @xmath78 where @xmath79 denotes the domain of @xmath80 .",
    "+ ( ii ) for any @xmath81 there exists @xmath82 such that @xmath83 .",
    "moreover , we assume that @xmath84 for any bounded set @xmath85 . + for all @xmath86 in @xmath87 , we will also require that + ( iii ) ( positivity ) @xmath88 , + ( iv ) ( identifiability ) @xmath89 , + ( v ) ( continuity ) @xmath80 is continuous at @xmath90 + and for all @xmath91 belonging to the projection of @xmath87 onto its second coordinate , + ( vi ) ( differentiability ) the function @xmath92 is differentiable at @xmath91 .",
    "assumptions [ ass1](i ) and ( ii ) on @xmath16 are standard and are easily checked in practical examples , e.g. they are satisfied for the poisson and additive mixture models . notice that the domain @xmath59 is now implicitly defined by the knowledge of @xmath56 and @xmath87 .",
    "moreover @xmath10 is continuous on @xmath59 .",
    "the importance of requiring that @xmath10 has the prescribed shape comes from the fact that @xmath10 might not satisfy assumption [ ass2](iv ) in general .",
    "therefore assumption [ ass2 ] ( iv ) reflects the requirement that @xmath10 should at least satisfy the identifiability property up to a possibly injective transformation . in both examples discussed above",
    ", this property is an easy consequence of the well known fact that @xmath93 implies @xmath94 for positive real numbers @xmath95 and @xmath96 .",
    "the growth , continuity and differentiability properties [ ass2 ] ( ii ) , ( v ) and ( vi ) are , in any case , nonrestrictive .    for the sake of notational convenience , the regularized objective function with relaxation parameter @xmath17 will be denoted @xmath97    finally we make the following general assumption .    [ ass3 ] the kullback proximal iteration ( [ kullprox ] ) is well defined , i.e. there exists at least one maximizer of @xmath98 at each iteration @xmath99 .    in the em case ,",
    "i.e. @xmath100 , this last assumption is equivalent to the computability of m - steps .",
    "a sufficient condition for this assumption to hold would be , for instance , that @xmath101 be sup - compact , i.e. the level sets @xmath102 be compact for all @xmath103 , @xmath104 and @xmath105 .",
    "however , this assumption is not usually satisfied since the distance - like function is not defined on the boundary of its domain . in practice",
    "it suffices to solve the equation @xmath106 , to prove that the solution is unique .",
    "then assumption [ ass1](i ) is sufficient to conclude that we actually have a maximizer .      using assumptions [ ass1 ] , we easily deduce monotonicity of the likelihood values and boundedness of the proximal sequence .",
    "the first two lemmas are proved , for instance , in .",
    "we start with the following monotonicity result .",
    "[ truit ] for any iteration @xmath107 , the sequence @xmath18 satisfies",
    "@xmath108    from the previous lemma , we easily obtain the boundedness of the sequence .",
    "[ boundu ] the sequence @xmath18 is bounded .",
    "the next lemma will also be useful .",
    "[ yal ] assume that there exists a subsequence @xmath109 belonging to a compact set @xmath110 included in @xmath56 .",
    "then , @xmath111    * proof*. since @xmath16 is continuous over @xmath110 , @xmath112 and @xmath113 is therefore bounded from above .",
    "moreover , lemma [ truit ] implies that the sequence @xmath114 is monotone nondecreasing .",
    "therefore , the whole sequence @xmath114 is bounded from above and convergent .",
    "this implies that @xmath115 . applying lemma [ truit ] again , we obtain the desired result . @xmath116",
    "the convergence analysis of kullback proximal algorithms is split into two parts , the first part being the subject of this section .",
    "we prove that if the accumulation points @xmath9 of the kullback proximal sequence satisfy @xmath117 they are stationary points of the log - likelihood function @xmath16 .",
    "it is also straightforward to show that the same analysis applies to the case of penalized likelihood estimation .",
    "we start with the following useful lemma .",
    "[ asymreg ] let @xmath118 and @xmath119 be two bounded sequences in @xmath87 satisfying @xmath120 assume that every couple @xmath121 of accumulation points of these two sequences lies in @xmath87 .",
    "then , @xmath122    * proof*. first , one easily obtains that @xmath123 is bounded ( use a contradiction argument and assumption [ ass2 ] ( ii ) ) .",
    "assume that there exits a subsequence @xmath124 such that @xmath125 for some @xmath126 and for all large @xmath99 .",
    "since @xmath124 is bounded , one can extract a convergent subsequence .",
    "thus we may assume without any loss of generality that @xmath127 is convergent with limit @xmath128 .",
    "using the triangle inequality , we have @xmath129 .",
    "since @xmath124 converges to @xmath130 , there exists a integer @xmath131 such that @xmath132 implies @xmath133 .",
    "thus for @xmath132 we have @xmath134 .",
    "now recall that @xmath135 is bounded and extract a convergent subsequence @xmath136 with limit denoted by @xmath137 .",
    "then , using the same arguments as above , we obtain @xmath138 .",
    "finally , recall that @xmath139 .",
    "we thus have @xmath140 , and , due to the fact that the sequences are bounded and @xmath141 is continuous in both variables , we have @xmath142 .",
    "thus assumption [ ass2 ] ( iv ) implies that @xmath143 and we obtain a contradiction .",
    "hence , @xmath144 as claimed .",
    "@xmath116      the main results of this section are the following .",
    "first , we prove that under the assumptions [ ass1 ] , [ ass2 ] and [ ass3 ] , any cluster point @xmath9 is a global maximizer of @xmath145 .",
    "we then use this general result to prove that such cluster points are stationary points of the log - likelihood function .",
    "this result motivates a natural assumption under which @xmath9 is in fact a local maximizer of @xmath16 .",
    "in addition we show that if the sequence @xmath146 converges to zero , i.e. @xmath147 , then @xmath9 is a global maximizer of log - likelihood .",
    "finally , we discuss some simple conditions under which the algorithm converges , i.e. has only one cluster point .",
    "the following theorem states a result which describes the stationary points of the proximal point algorithm as global maximizers of the asymptotic penalized function .    [ the ] assume that @xmath148 .",
    "let @xmath9 be any accumulation point of @xmath18 .",
    "assume that @xmath149 .",
    "then , @xmath9 is a global maximizer of the penalized function @xmath150 over the projection of @xmath59 onto its first coordinate , i.e. @xmath151 for all @xmath8 such that @xmath152 .",
    "an informal argument is as follows .",
    "assume that @xmath153 .",
    "from the definition of the proximal iterations , we have @xmath154 for all subsequence @xmath109 converging to @xmath9 and for all @xmath155 .",
    "now , assume we can prove that @xmath156 also converges to @xmath9 , we obtain by taking the limit and using continuity , that @xmath157 which is the required result .",
    "there are two major difficulties when one tries to transform this sketch into a rigorous argument . the first one is related to the fact that @xmath16 and @xmath10 are only defined on domains which may not to be closed . secondly",
    ", proving that @xmath156 converges to @xmath9 is not an easy task .",
    "this issue will be discussed in more detail in the next section .",
    "the following proof overcomes both difficulties .",
    "* proof*. without loss of generality , we may reduce the analysis to the case where @xmath158 for a certain @xmath17 .",
    "the fact that @xmath9 is a cluster point implies that there is a subsequence of @xmath159 converging to @xmath9 . for @xmath99",
    "sufficiently large , we may assume that the terms @xmath160 belong to a compact neighborhood @xmath161 of @xmath162 included in @xmath59 . recall that @xmath163 for all @xmath8 such that @xmath164 and a fortiori for @xmath165 .",
    "therefore , @xmath166    let us have a precise look at the `` long term '' behavior of @xmath10 .",
    "first , since @xmath167 for all @xmath99 sufficiently large , lemma [ yal ] says that @xmath168 thus , for any @xmath126 , there exits an integer @xmath169 such that @xmath170 for all @xmath171 .",
    "moreover , lemma [ asymreg ] and continuity of @xmath91 allows to conclude that @xmath172 since @xmath80 is continuous , for all @xmath126 and for all @xmath99 sufficienlty large we have @xmath173    on the other hand , @xmath174 is continuous in both variables on @xmath161 , due to assumptions [ ass1](i ) and [ ass2](i ) . by continuity in the first and second arguments of @xmath175 , for any @xmath176 there exists @xmath177 such that for all @xmath178 @xmath179 using ( [ eq ] ) , since @xmath16 is continuous , we obtain existence of @xmath180 such that for all @xmath181 @xmath182 combining equations ( [ eq1 ] ) and ( [ eq2 ] ) with ( [ eq0 ] ) , we obtain @xmath183 now , since @xmath184 , there exists an integer @xmath185 such that @xmath186 for all @xmath187 .",
    "therefore for all @xmath188 , we obtain @xmath189 since @xmath10 is continuous and @xmath190 is bounded , there exists a real constant @xmath131 such that @xmath191 , for all @xmath192 .",
    "thus , for all @xmath99 sufficiently large @xmath193 finally , recall that no assumption was made on @xmath8 , and that @xmath161 is any compact neighborhood of @xmath9 .",
    "thus , using the assumption [ ass1](i ) , which asserts that @xmath4 tends to @xmath70 as @xmath71 tends to @xmath72 , we may deduce that ( [ maj ] ) holds for any @xmath8 such that @xmath194 and , letting @xmath195 tend to zero , we see that @xmath9 maximizes @xmath196 for over all @xmath8 such that @xmath197 belongs to @xmath59 as claimed .",
    "@xmath116    using this theorem , we may now deduce that certain accumulation points on the strict interior of the parameter s space are stationary points of the log - likelihood function .",
    "[ stat ] [ corolun ] assume that @xmath148 .",
    "let @xmath9 be any accumulation point of @xmath18 .",
    "assume that @xmath198 .",
    "then , if @xmath16 is differentiable on @xmath56 , @xmath9 is a stationary point of @xmath4 .",
    "moreover , if @xmath16 is concave , then @xmath9 is a global maximizer of @xmath16 .",
    "* proof*. since under the required assumptions @xmath16 is differentiable and @xmath199 is differentiable at @xmath9 , theorem [ the ] states that @xmath200 since @xmath201 is minimum at @xmath9 , @xmath202 and we thus obtain that @xmath9 is a stationary point of @xmath16 .",
    "this implies that @xmath9 is a global maximizer in the case where @xmath16 is concave .",
    "@xmath116 .",
    "theorem [ the ] seems to be much stronger than the previous corollary .",
    "the fact that accumulation points of the proximal sequence may not be global maximizers of the likelihood is now easily seen to be a consequence of fact that the kullback distance - like function @xmath10 perturbs the shape of the likelihood function when @xmath8 is far from @xmath9 .",
    "this perturbation does not have serious consequence in the concave case . on the other hand",
    ", one may wonder whether @xmath9 can not be proved to be at least a local maximizer instead of a mere stationary point .",
    "the answer is given in the following corollary .",
    "let @xmath9 be an accumulation point of @xmath203 such that @xmath204 .",
    "in addition , assume that @xmath16 and @xmath201 are twice differentiable in a neighborhood of @xmath9 and that the hessian matrix @xmath205 at @xmath9 is not the null matrix .",
    "then , if @xmath74 is sufficiently small , @xmath9 is a local maximizer of @xmath16 over @xmath56 .",
    "* proof*. assume that @xmath9 is not a local maximizer .",
    "since @xmath206 is not the null matrix , for @xmath74 sufficiently small , there is a direction @xmath207 in the tangent space to @xmath56 for which the function @xmath208 has positive second derivative for @xmath91 sufficiently small .",
    "this contradicts the fact that @xmath9 is a global maximizer of @xmath150 and the proof is completed .",
    "@xmath116    the next theorem establishes global optimality of accumulation points in the case where the relaxation sequence converges to zero .",
    "let @xmath9 be any accumulation point of @xmath203 .",
    "assume that @xmath149 .",
    "then , without assuming differentiability of either @xmath16 or of @xmath10 , if @xmath73 converges to zero , @xmath9 is a global maximizer of @xmath16 over the projection of @xmath59 along the first coordinate .",
    "* proof*. let @xmath190 be a convergent subsequence of @xmath209 with limit denoted @xmath9 .",
    "we may assume that for @xmath99 sufficiently large , @xmath160 belongs to a compact neighborhood @xmath161 of @xmath9 .",
    "by continuity of @xmath16 , for any @xmath126 , there exists @xmath210 such that for all @xmath132 , @xmath211 on the other hand , the proximal iteration ( [ proxdef ] ) implies that @xmath212 for all @xmath213 .",
    "fix @xmath213 .",
    "thus , for all @xmath132 , @xmath214 since @xmath10 is a nonnegative function and @xmath12 is a nonnegative sequence , we obtain @xmath215 recall that @xmath209 is bounded due to lemma [ boundu ] .",
    "thus , since @xmath10 is continuous , there exists a constant @xmath110 such that @xmath216 for all @xmath99 .",
    "therefore , for @xmath99 greater than @xmath131 , @xmath217 passing to the limit , and recalling that @xmath73 tends to zero , we obtain that @xmath218 using the same argument as at the end of the proof of theorem [ the ] , this latter equation holds for any @xmath8 such that @xmath197 belongs to @xmath59 , which concludes the proof upon letting @xmath195 tend to zero .",
    "@xmath116      one question remains open in the analysis of the previous section : does the sequence generated by the kullback proximal point converge ?",
    "in other words : are there multiple cluster points ? in wu s",
    "paper @xcite , the answer takes the following form . if the euclidean distance between two successive iterates tends to zero , a well known result states that the set of accumulation points is a continuum ( see for instance ( *",
    "* theorem 28.1 ) ) and therefore , it is connected . therefore ,",
    "if the set of stationary points of @xmath16 is a countable set , the iterates must converge .",
    "[ corle ] let @xmath219 denote the set of accumulation points of the sequence @xmath18 .",
    "assume that @xmath220 and that @xmath4 is strictly concave in an open neighborhood @xmath221 of an accumulation point @xmath9 of @xmath203 and that @xmath162 is in @xmath222 .",
    "then , for any relaxation sequence @xmath12 , the sequence @xmath18 converges to a local maximizer of @xmath4 .",
    "* proof*. we obtained in corollary [ stat ] that every accumulation point @xmath9 of @xmath18 in @xmath223 and such that @xmath224 is a stationary point of @xmath4 . since @xmath4 is strictly concave over @xmath225 , the set of stationary points of @xmath16 belonging to @xmath225 reduces to singleton .",
    "thus @xmath9 is the unique stationary point in @xmath221 of @xmath16 , and _ a fortiori _ , the unique accumulation point of @xmath18 belonging to @xmath221 . to complete the proof , it remains to show that there is no accumulation point in the exterior of @xmath221 .",
    "for that purpose , consider an open ball @xmath226 of center @xmath9 and radius @xmath195 included in @xmath221 . then",
    ", @xmath227 is the unique accumulation point in @xmath226 .",
    "moreover , any accumulation point @xmath228 , lying in the exterior of @xmath221 must satisfy @xmath229 , and we obtain a contradiction with the fact that @xmath219 is connected . thus every accumulation point lies in @xmath221 , from which we conclude that @xmath9 is the only accumulation point of @xmath203 or , in other words , that @xmath18 converges towards @xmath9 .",
    "finally , notice that the strict concavity of @xmath4 over @xmath221 implies that @xmath9 is a local maximizer .",
    "@xmath116    before concluding this section ,",
    "let us make two general remarks .",
    "* proving _ a priori _ that the set of stationary points of @xmath16 is discrete may be a hard task in specific examples . * in general",
    ", it is not known whether @xmath220 holds .",
    "in fact , lemma [ asymreg ] could be a first step in this direction . indeed if we could prove in any application that the mapping @xmath91 is injective , the desired result would follow immediately .",
    "however , injectivity of @xmath91 does not hold in many of the standard examples ; in the case of gaussian mixtures , see for instance .",
    "thus we are now able to clearly understand why the assumption that @xmath220 is not easily deduced from general arguments .",
    "this problem has been overcome in where it is shown that @xmath91 is componentwise injective and thus performing a componentwise em algorithm is a good alternative to the standard em .",
    "the goal of this section is to extend the previous results to the case where some cluster points lie on the boundary of the region where computation of proximal steps is well defined .",
    "such cluster points have rarely been analyzed in the statistical literature and the strategy developed for the interior case can not be applied without further study of the kullback distance - like function .",
    "notice further that entropic - type penalization terms in proximal algorithms have been the subject of an intensive research effort in the mathematical programming community with the goal of handling positivity constraints ; see @xcite and the references therein for instance .",
    "the analysis proposed here applies to the more general kullback distance - like functions @xmath10 that occur in em .",
    "our goal is to show that such cluster points satisfy the well known karush - kuhn - tucker conditions of nonlinear programming which extend the stationarity condition @xmath230 to the case where @xmath8 is subject to constraints .",
    "as before , it is straightforward to extend the proposed analysis to the case of penalized likelihood estimation .    in the sequel , the distance - like function will be assumed to have the following additional properties .",
    "[ ass4 ] the kullback distance - like function @xmath10 is of the form @xmath231 where for all @xmath232 and @xmath233 , @xmath234 is continuously differentiable on its domain of definition , @xmath235 is a function from @xmath3 to @xmath236 , the set of positive real numbers , and the function @xmath237 is a non negative convex continuously differentiable function defined for positive real numbers only and such that @xmath238 if and only if @xmath239 .    if @xmath240 and @xmath241 for all @xmath232 and all @xmath233 , the function @xmath10 is the well known @xmath237 divergence defined by csiszr in @xcite . assumption [ ass4 ] is satisfied in most standard examples ( for instance gaussian mixtures and poisson inverse problems ) with the choice @xmath242 .",
    "the main property that will be needed in the sequel is that under assumption [ ass4 ] , the function @xmath10 satisfies the same property as the one given in lemma [ asymreg ] above , even on the boundary of its domain @xmath59 .",
    "this is the result of proposition [ nondegphi ] below .",
    "we begin with one elementary lemma .",
    "[ convone ] under assumptions [ ass4 ] , the function @xmath237 is decreasing on @xmath243 , is increasing on @xmath244 and @xmath245 converges to @xmath72 when @xmath246 converges to @xmath72 .",
    "we have @xmath247 if and only if @xmath248 .",
    "* proof*. the first statement is obvious . for the second statement , the `` if '' part is trivial , so we only prove the `` only if '' part .",
    "first notice that the sequence @xmath249 must be bounded .",
    "indeed , the level set @xmath250 is bounded for all @xmath251 and contains the sequence @xmath252 for @xmath131 sufficiently large .",
    "thus , the bolzano - weierstass theorem applies .",
    "let @xmath253 be an accumulation point of @xmath249 .",
    "since @xmath237 is continuous , we get that @xmath254 and thus we obtain @xmath255 . from this",
    ", we deduce that the sequence has only one cluster point , which is equal to 1",
    ". therefore , @xmath248 . @xmath116    using these lemmas , we are now in position to state and prove the main property of @xmath10 .",
    "[ nondegphi ] the following statements hold .",
    "\\(i ) for any sequence @xmath18 in @xmath236 and any bounded sequence @xmath256 in @xmath236 , the fact that @xmath257 implies @xmath258 for all @xmath232,@xmath233 such that @xmath259 .",
    "\\(ii ) if one coordinate of one of the two sequences @xmath18 and @xmath256 tends to infinity , so does the other s same coordinate .    *",
    "proof*. fix @xmath232 in @xmath260 and @xmath233 in @xmath261 and assume that @xmath259 .",
    "\\(i ) we first assume that @xmath262 is bounded away from zero .    since @xmath263 , then @xmath264 and lemma [ convone ] implies that @xmath265 .",
    "thus , @xmath266 and since @xmath91 is continuous , @xmath267 is bounded .",
    "this implies that @xmath268 .",
    "next , consider the case of a subsequence @xmath269 which tends towards zero . for contradiction ,",
    "assume the existence of a subsequence @xmath270 which remains bounded away from zero , i.e. there exists @xmath271 such that @xmath272 for @xmath99 sufficiently large .",
    "thus , for @xmath99 sufficiently large we get @xmath273 and due to the fact that @xmath237 is increasing on @xmath244 , we obtain @xmath274 on the other hand , lemma [ convone ] says that for any @xmath275 , @xmath276 . since @xmath237 is convex , we get @xmath277 take @xmath278 in this last expression and combine with ( [ toto ] ) to obtain @xmath279 passing to the limit , we obtain @xmath280 which gives the required contradiction .",
    "\\(ii ) if @xmath281 then @xmath282 is a direct consequence of part ( i ) . indeed",
    ", if @xmath267 remains bounded , part ( i ) says that @xmath258 , which contradicts divergence of @xmath283 .",
    "now , consider the case where @xmath282 . then",
    ", a contradiction is easily obtained if we assume that at least a subsequence @xmath284 stays bounded from above .",
    "indeed , in such a case , we have @xmath285 and thus , @xmath286 for some @xmath287 since we know that @xmath237 is decreasing on @xmath243 and @xmath288 .",
    "this implies that @xmath289 which is the required contradiction .",
    "@xmath116      the main result of this section is the property that any cluster point @xmath9 such that @xmath162 lies on the boundary of @xmath59 satisfies the karush - kuhn - tucker necessary conditions for optimality on the domain of the log - likelihood function . in the context of assumptions [ ass4 ] , @xmath59 is the set @xmath290    we have the following theorem .",
    "[ bord ] let @xmath9 be a cluster point of the kullback - proximal sequence .",
    "assume that all the functions @xmath234 are differentiable at @xmath9 .",
    "let @xmath291 be the set of all couples of indices @xmath292 such that the constraint @xmath293 is active at @xmath9 , i.e. @xmath294 . if @xmath9 lies in the interior of @xmath56 , then @xmath9 satisfies the karush - kuhn - tucker necessary conditions for optimality , i.e. there exists a family of reals @xmath295 , @xmath296 such that @xmath297    * proof*. let @xmath298 denote the bivariate function defined by @xmath299 let @xmath300 be a convergent subsequence of the proximal sequence with limit equal to @xmath9 . the first order optimality condition at iteration @xmath99 is given by @xmath301 we have @xmath302 for all @xmath232 and @xmath233 .",
    "* claim a*. _ for all @xmath292 such that @xmath303 , we have @xmath304 _    * proof of claim a*. two cases may occur . in the first case , we have @xmath294 .",
    "since the sequence @xmath305 is bounded due to lemma [ boundu ] , continuous differentiability of @xmath237 and the @xmath234 proves that @xmath306 is bounded from above .",
    "thus , the desired conclusion follows . in the second case , @xmath307 and applying lemma [ yal ]",
    ", we deduce that @xmath308 tends to zero .",
    "hence , @xmath309 , which implies that @xmath310 . from this and assumptions",
    "[ ass4 ] , we deduce that @xmath311 . since @xmath300 converges to @xmath9 and that @xmath307 , we obtain that the subsequence @xmath312 is bounded from above .",
    "moreover , @xmath313 is also bounded by continuous differentiability of @xmath234 .",
    "therefore , the fact that @xmath311 establishes claim a. @xmath116    using this claim , we just have to study the remaining right hand side terms in ( [ frst ] ) , namely the expression @xmath314 .",
    "let @xmath315 be a subset of the active indices @xmath316 such that the family @xmath317 is linearly independent .",
    "this linear independence is preserved under small perturbations , we may assume without loss of generality that the family @xmath318 is linearly independent for @xmath99 sufficiently large .",
    "for such @xmath99 , we may rewrite equation ( [ frst ] ) as @xmath319    * claim b*. _ the sequence @xmath320 is bounded . _    * proof of claim b*. using the previous claim and the continuous differentiability of @xmath16 and @xmath234 , equation ( [ frstbis ] ) expresses that @xmath321 are proportional to the coordinates of the projection on the span of the @xmath322 of a vector converging towards @xmath323 . since @xmath322 , for @xmath324 , form a linearly independent family for @xmath99 sufficiently large , none of the coordinates can tend towards infinity .",
    "we are now in position to finish the proof of the theorem .",
    "take any cluster point @xmath325 of @xmath326 . using claim b , we know that @xmath327 lies in a compact set .",
    "let @xmath328 be a cluster point of this sequence . passing to the limit",
    ", we obtain from equation ( [ frst ] ) that @xmath329 for every cluster point @xmath74 of @xmath330 . for",
    "all @xmath331 , set @xmath332 .",
    "this equation is exactly the karuch - kuhn - tucker necessary condition for optimality .",
    "@xmath116    if the family @xmath333 is linearly independent for @xmath99 sufficiently large , theorem [ bord ] holds and in addition the @xmath334 are nonnegative , which proves that @xmath9 satisfies the karush - kuhn - tucker conditions when it lies in the closure of @xmath335 .",
    "the goal of this section is to illustrate the utility of the previous theory for a nonparametric survival analysis with competing risks proposed by ahn , kodell and moon in .",
    "this problem can be described as follows .",
    "consider a group of @xmath336 animals in an animal carcinogenecity experiment .",
    "sacrifices are performed at certain prescribed times denoted by @xmath337 in order to study the presence of the tumor of interest .",
    "let @xmath338 be the time to onset of tumor , @xmath339 the time to death from this tumor and @xmath340 be the time to death from a cause other than this tumor .",
    "notice that @xmath338 , @xmath339 and @xmath340 are unobservable .",
    "the quantities to be estimated are @xmath341 , @xmath342 and @xmath343 , the survival function of @xmath338 , @xmath339 and @xmath340 respectively . it is assumed that @xmath338 and @xmath339 are statistically independent of @xmath340 .    a nonparametric approach to estimation of @xmath75 , @xmath344 and @xmath345 is proposed in : observed data @xmath346 are the number of deaths on every interval @xmath347 $ ] which can be classified into the following four categories ,    * death with tumor ( without knowing cause of death ) * death without tumor * sacrifice with tumor * sacrifice without tumor    this gives rise to a multinomial model whose probability mass is parametrized by the values of @xmath75 , @xmath344 and @xmath345 at times @xmath348 .",
    "more precisely , for each time interval @xmath347 $ ] denote by @xmath349 the number of deaths with tumor present , @xmath350 the number of deaths with tumor absent , @xmath351 the number of sacrifices with tumor present and @xmath352 the number of sacrifices with tumor absent .",
    "let @xmath353 be the number of live animals in the population at @xmath354 , it is shown in that the corresponding log - likelihood is given by @xmath355 where @xmath356 is a constant @xmath357 , @xmath358 and @xmath359 , @xmath360 , @xmath361 and the parameter space is specified by the constraints @xmath362 where the last nonconvex constraint serves to impose monotonicity of @xmath75 .",
    "note that monotonicity of @xmath344 and @xmath345 is a direct consequence of the constraints on the @xmath363 s and the @xmath364 s , respectively .",
    "define the complete data @xmath365 as a measurement that indicates the cause of death in addition to the presence of absence of a tumor in the dead animals .",
    "specifically , @xmath365 should fall into one of the following categories    * death caused by tumor and death with incidental tumor * death without tumor * sacrifice with tumor * sacrifice without tumor    to each time interval @xmath347 $ ] among those animals dying of natural causes , there correspond the numbers @xmath366 of deaths caused by tumor and the number @xmath367 of deaths with incidental tumor , neither of which are observable .",
    "the associated complete log - likelihood function is given by @xmath368 now , we have to compute the expectation @xmath369 of the log - likelihood function of the complete data conditionally to the parameter @xmath35 .",
    "the random variables @xmath366 and @xmath367 are binomial with parameter @xmath370 and @xmath371 where @xmath370 is the probability that the death was caused by the tumor conditioned on the presence of the tumor . conditioned on @xmath35 , we have @xmath372 ( see for details ) . from this",
    ", we obtain that the conditional mean values of @xmath366 and @xmath367 are given by @xmath373=\\lambda_j c_j \\hspace{.4 cm } \\text { and } \\hspace{.4 cm } { \\rm e}[a_{1j } \\mid y;\\bar{\\theta}]=(1-\\lambda_j ) c_j.\\ ] ] therefore @xmath374 from this , we can easily compute the associated kullback distance - like function : @xmath375 with @xmath376 and @xmath237 is defined by @xmath377 .",
    "it is straightforward to verify that assumptions [ ass1 ] , [ ass2 ] , [ ass3 ] and [ ass4 ] are satisfied .",
    "the main computational problem in this example is to handle the difficult nonconvex constraints entering the definition of the parameter space @xmath378 .",
    "the authors of and use the complex method proposed by box in @xcite to address this problem . however , the theoretical convergence properties of box s method are not known as reported in article mr0184734 in the math . reviews .",
    "using our proximal point framework , we are able to easily incorporate the nonconvex constraints into the kullback distance - like function and obtain an efficient algorithm with satisfactory convergence properties . for this purpose ,",
    "let @xmath379 be defined by @xmath380 where @xmath381 using this new function , the nonconvex constraints @xmath382 are satisfied for all proximal iterations and assumptions [ ass4 ] still hold .",
    "we implemented the kullback proximal algorithm with different choices of relaxation sequence @xmath12 , @xmath383 .",
    "the m - step of the em algorithm does not have a closed form solution , so that nothing is lost by setting @xmath65 to a constant not equal to one .",
    "we attempted to supplement the kpp - em algorithm with the newton method and other built - in methods available in scilab but they were not even able to find local maximizers due to the explosive nature of the logarithms near zero , leading these routines to repetitive crashes . to overcome this difficulty",
    ", we found it convenient to use the extremely simple simulated annealing random search procedure ; see @xcite for instance .",
    "this random search approach avoids numerical difficulties encountered using standard optimization packages and easily handles nonconvex constraints .",
    "convergence of this procedure is well established and recent studies such as @xcite confirm the good computational efficiency for convex functions optimization .",
    "some of our results for the data of table 1 of are given in figures 1 to 4 . in the reported experiments , we chose three constant sequences with respective values @xmath384 .",
    "we observed the following phenomena    * 1 . * after one hundred iterations the increase in the likelihood function is less than @xmath385 except for the case @xmath386 ( figure [ fig4 ] ) where the algorithm had not converged .    *",
    "2 . * for @xmath386",
    "we often obtained the best initial growth of the likelihood    * 3 . * for @xmath387",
    "we always obtained the highest likelihood when the number of iterations was limited to 50 ( see figure [ fig3 ] for the case mcl male al ) .",
    "it was shown in that penalizing with a parameter sequence @xmath388 converging towards zero implies superlinear convergence in the case where the maximum likelihood estimator lies in the interior of the constraint set .",
    "thus , our simulations results seem to confirm observation 3 .",
    "the second observation was surprising to us but this phenomenon occured repeatedly in our experiments .",
    "this behavior did not occur in our simulations for the poisson inverse problem in for instance .    in conclusion",
    ", this competing risks estimation problem is an interesting test for our kullback - proximal method which shows that the proposed framework can provide provably convergent methods for difficult constrained nonconvex estimation problems for which standard optimization algorithms can be hard to tune .",
    "the relaxation parameter sequence @xmath389 also appeared crucial for this problem although the choice @xmath390 could not really be considered unsatisfactory in practice .",
    "evolution of the log - likelihood versus iteration number : mcl female al case ]",
    "the goal of this paper was the study of the asymptotic behavior of the em algorithm and its proximal generalizations .",
    "we clarified the analysis by making use of the kullback - proximal theoretical framework .",
    "two of our main contributions are the following .",
    "firstly we showed that interior cluster points are stationary points of the likelihood function and are local maximizers for sufficiently small values of @xmath17 .",
    "secondly , we showed that cluster points lying on the boundary satisfy the karush - kuhn - tucker conditions .",
    "such cases were very seldom studied in the literature although constrained estimation is a topic of growing importance ; see for instance the special issue of the journal of statistical planning and inference @xcite which is devoted to the problem of estimation under constraints . on the negative side ,",
    "the analysis from the kullback - proximal viewpoint allowed us to understand why uniqueness of the cluster point is hard to establish theoretically . on the positive side",
    ", we were able to implement a new and efficient proximal point method for estimation in the difficult tumor lethality problem involving nonlinear inequality constraints .",
    "h. ahn , h. moon and r.l .",
    "kodell , `` attribution of tumour lethality and estimation of the time to onset of occult tumours in the absence of cause - of - death information '' .",
    "c _ , vol .",
    "2 , 157169 , 2000 .",
    "h. moon , h. ahn , r. kodell and b. pearce `` a comparison of a mixture likelihood method and the em algorithm for an estimation problme in animal carcinogenicity studies , '' computational statistics and data analysis , 31 , no .",
    "2 , pp . 227238 , 1999 ."
  ],
  "abstract_text": [
    "<S> in this paper , we analyze the celebrated em algorithm from the point of view of proximal point algorithms . </S>",
    "<S> more precisely , we study a new type of generalization of the em procedure introduced in and called kullback - proximal algorithms . </S>",
    "<S> the proximal framework allows us to prove new results concerning the cluster points . </S>",
    "<S> an essential contribution is a detailed analysis of the case where some cluster points lie on the boundary of the parameter space . </S>"
  ]
}