{
  "article_text": [
    "the group testing problem was introduced by dorfman @xcite , as described in ( * ? ? ?",
    "* chapter 1.1 ) . while a large variety of problem setups have been considered , they all share common features , and can be considered in a wider class of sparse inference problems which includes compressed sensing @xcite .",
    "group testing has since been applied in a wide variety of contexts , for example including biology @xcite , anomaly detection in networks @xcite , signal processing and data analysis @xcite , and communications @xcite , though this list is very far from being exhaustive . in this paper , we prove rigorous performance bounds on algorithms for the nonadaptive noiseless group testing problem .",
    "we consider a testing design in which each item is in a fixed number of tests , and study practical algorithms for detecting the defective items .",
    "we shall see that we can detect items with fewer tests compared to a more commonly - considered design in which items are placed in tests independently at random .",
    "let us fix some notation .",
    "suppose we have a large number of items @xmath0 , of which @xmath1 are ` defective ' in some sense .",
    "we assume that the defectives are rare , with @xmath2 as @xmath3 ; moreover , for concreteness we follow @xcite by taking @xmath4 for some fixed parameter @xmath5 .",
    "we follow the ` combinatorial model ' and suppose that @xmath6 , the true set of defective items , is uniformly random from the @xmath7 sets of this size .",
    "we perform a sequence of tests to form an estimate @xmath8 of @xmath6 , and study the tradeoff between maximising the success probability @xmath9 and minimising the number of tests @xmath10 .",
    "we could simply take @xmath11 , and test each item one by one .",
    "however , dorfman s key insight @xcite is that since the problem is sparse ( in the sense that @xmath12 ) , the outcomes of the tests are zero with high probability , so these tests are not optimally informative .",
    "a better procedure considers a series of subsets ( ` pools ' ) of items which are tested together , where ( in an idealized testing procedure ) the outcome of each test is positive if and only if it contains at least one defective item . more formally :    we store the testing pools in a ( possibly random ) binary matrix @xmath13 , where @xmath14 if test @xmath15 includes item @xmath16 and @xmath17 otherwise .",
    "the rows of @xmath18 correspond to tests , and the columns correspond to items .",
    "the outcomes of each test are stored in a binary vector @xmath19 , where a positive outcome @xmath20 occurs if and only if @xmath14 for some @xmath21 .",
    "we estimate the defective set by @xmath22 , and write @xmath23 where the probability is over the random test design @xmath18 .",
    "we have freedom to design the test matrix , and in this paper , we focus on the nonadaptive case , where the entire matrix is fixed in advance of the tests . in the adaptive case ( where the members of each test are chosen using the outcomes of the previous tests ) ,",
    "hwang s generalized binary splitting algorithm @xcite recovers the defective set @xmath6 using @xmath24 tests .",
    "this can be seen to be essentially optimal by a standard argument based on fano s inequality ( see for example @xcite ) , a strengthened version of which ( see @xcite ) implies that any algorithm using @xmath10 tests has success probability bounded above by @xmath25 this means that any algorithm with success probability @xmath26 tending to 1 requires at least @xmath27 ( see ( * ? ? ?",
    "* lemma 25 ) for details of the asymptotic behaviour of the binomial coefficient ) .",
    "this motivates the following definition @xcite of the rate of an algorithm .",
    "[ def : rate ] for any algorithm using @xmath10 tests , we define the _ rate _ to be @xmath28 given a random matrix design , we say that @xmath29 is an _ achievable rate _ if for any @xmath30 , there exists a group testing algorithm with rate at least @xmath29 and success probability at least @xmath31 for @xmath0 sufficiently large .",
    "we define the capacity of a design to be the supremum of the achievable rates for that design .",
    "intuitively , one can think of @xmath29 as being the number of bits of information learned per test when the recovery is successful . in this language ,",
    "hwang s adaptive algorithm @xcite has achievable rate @xmath32 in the regime @xmath4 ( and is therefore optimal , since no algorithm can learn more than 1 bit per test ) .",
    "it is an interesting question to consider whether there exists a matrix design and an algorithm with achievable rate @xmath32 in the _ nonadaptive _ case .",
    "indeed , it appears to be difficult even to design a class of matrices with non - zero achievable rate using combinatorial constructions ( see @xcite for reviews of the extensive literature on this subject , with key early contributions coming from @xcite ) .",
    "hence , much recent work on nonadaptive group testing has considered bernoulli random designs , where each item appears in each test independently with fixed probability ; see for example @xcite .",
    "in particular :    [ thm : berncap ] the capacity for bernoulli nonadaptive group testing with @xmath33 defectives is @xmath34 where @xmath35 is the binary entropy function . in particular , for @xmath36 ,",
    "the capacity of bernoulli designs is 1 .",
    "the curve is illustrated in figure [ fig : ratecomparison ] below .",
    "for @xmath37 , the paper @xcite showed that the capacity is achieved by a simple algorithm , which we refer to as dd ( see definition [ def : dd ] for a description ) .",
    "however , for @xmath38 , the algorithms known to achieve the capacity are based on maximising the likelihood or solving other difficult combinatorial problems , and can not be considered as practical in a computational sense  see section [ sec : feasible ] for more detail . for example , we describe the sss algorithm in definition [ def : sss ] , which achieves the capacity but is impractical for large values of @xmath0 and @xmath1 .",
    "the structure of the remainder of the paper is as follows . in section [ sec : results ] , we summarise the main results of the paper , and provide simulation results to illustrate the performance of various algorithms . in section [ sec : algorithms ] , we describe the main algorithms used and introduce some key quantities that control their performance . in section [ sec : maintheorem ] , we state some distributional results ( proved in the appendices ) and deduce the main theorem of the paper .",
    "our main result provides strict improvements on theorem [ thm : berncap ] in a broad range of sparsity regimes , as well as strict improvements over existing practical algorithms in all scaling regimes . to do this",
    ", we make use of the following class of test designs , following our initial work in @xcite .",
    "[ def : cc ] we define the constant - column weight testing design via a testing matrix @xmath18 in which @xmath39 entries of each column of are selected uniformly at random with replacement and set to @xmath40 for some parameter @xmath41 , with independence between columns .",
    "the remaining entries of @xmath18 are set to @xmath42 .",
    "we refer to these as ` constant column weight ' designs despite the fact that some columns will have weight slightly less than @xmath43 . since the weight of a column is the number of tests an item is in , these designs are also known as ` constant tests - per - item ' .    in @xcite",
    ", we showed that @xmath44 is optimal with respect to all of the bounds derived therein , and we will also use this value throughout the present paper .",
    "this choice ensures that each test is equally likely to be positive or negative , and can thus be thought of as being maximally informative .    in our previous paper @xcite",
    ", we proved the two following results .",
    "[ thm : previous ] using a constant column weight design in the regime where there are @xmath33 defectives :    1 .",
    "( _ ( * ? ? ?",
    "* theorem 1 ) _ ) the comp algorithm of chan _ _ e__t al .",
    "@xcite ( see section [ sec : comp ] ) has achievable rate @xmath45 ( improving by @xmath46 on the rate of @xmath47 proved by ( * ? ? ?",
    "* theorem 11 ) for comp with bernoulli designs ) .",
    "( _ ( * ? ? ?",
    "* theorem 2 ) _ ) regardless of the choice of @xmath41 , no algorithm can have a rate greater than @xmath48    for completeness , we provide a proof of theorem [ thm : previous ] in section [ sec : maintheorem ] , albeit with @xmath44 in the second statement as opposed to a general choice .",
    "the main result of this paper is the following theorem , which is proved in the same section .",
    "[ thm : ddrate ] for constant column weight designs in the regime where there are @xmath33 defectives , the dd algorithm has success probability tending to 1 if @xmath49 and hence has achievable rate @xmath50     and [ thm : ddrate ] . in blue",
    ", we plot the rate bounds for bernoulli designs from theorem [ thm : berncap ] and @xcite .",
    "the green line represents the bound arising from .",
    ", scaledwidth=95.0% ]    we illustrate the above bounds in figure [ fig : ratecomparison ] , with the various rates for constant column weight designs marked in red , and corresponding rates for bernoulli designs marked in blue .",
    "note that in particular , a comparison of theorems [ thm : berncap ] , [ thm : previous ] and [ thm : ddrate ] shows the following .",
    "[ rem : highlights ]    1 .",
    "dd with constant column weight designs offers the best rate currently proved for ` practical ' algorithms for all @xmath51 .",
    "dd with constant column weight designs outperforms any possible algorithm ( practical or not ) for bernoulli designs , for all values of @xmath52 .",
    "if we use a constant column weight design , the dd algorithm gives the optimal performance for @xmath37 .    to complement our lower bound , in theorem [ ubthm ] , we provide an upper bound for the rate of dd with constant column weight designs .",
    "although our upper bound does not reveal whether the lower bound is tight at low values of @xmath51 , it does reveal that some amount of gap to the rate of @xmath40 is unavoidable .",
    "although the rates are asymptotic as @xmath53 , in figure [ fig : finite ] we illustrate the performance of these algorithms in a finite blocklength sense , in an illustrative sparse case ( @xmath54 , @xmath55 ) and a denser case ( @xmath56 , @xmath57 ) . for the sparse case ,",
    "in addition to plotting performance of comp and dd , we plot the performance of the sss algorithm , which ( see ( * ? ? ?",
    "* corollary 4 ) ) achieves the capacity bounds of theorem [ thm : berncap ] , though is not practical for larger problems .",
    "because of this issue of practicality , we do not consider sss for the denser case .",
    "instead , we plot the performance of a related algorithm called scomp , which is described in @xcite , so we omit a description in this paper for the sake of brevity . essentially , it amouts to performing dd followed by greedy refinements .    ,",
    "@xmath58 and @xmath56 , @xmath57.,scaledwidth=85.0% ]    , @xmath58 and @xmath56 , @xmath57.,scaledwidth=85.0% ]    constant column weight designs are not new in the literature , as we briefly describe below . our key contribution is a _ rigorous _ analysis of such designs in the regime @xmath33 , including in particular the first such analysis for the dd algorithm , and the first results to attain the benefits described in remark [ rem : highlights ] .    at an intuitive level",
    ", we believe the improvement over bernoulli designs arises due to the fact that the latter can result in some items appearing in considerably fewer tests than average , meaning that it is harder for any algorithm to infer their defectivity status .",
    "kautz and singleton @xcite observed that good group testing peformance is obtained by matrices corresponding to constant weight codes with a high minimum distance .",
    "however , ensuring the minimum distance is sufficiently large is not easy in practice .",
    "@xcite considered randomized designs with both fixed row and column weights , and with fixed column weights only .",
    "they suggested that such designs can beat bernoulli designs ; however , their ` short - loops ' assumption is shown to be rigorous only for @xmath59 , and in fact fails for small @xmath51 .",
    "@xcite considered constant row - weight designs and find no improvement over bernoulli designs .",
    "wadayama @xcite analysed constant row and column weight designs in the @xmath60 regime , and demonstrates close - to - optimal asymptotic performance for certain ratios of parameter sizes .",
    "dyachkov _ et al . _",
    "@xcite used exactly - constant column weights , and setting their list size to one corresponds to insisting that comp succeeds .",
    "however they only considered the case that @xmath61 ; in the limit as @xmath1 gets large , the rate @xmath62 obtained ( * ? ?",
    "? * claim 2 ) matches the rate for comp given in theorem [ thm : previous].1 as @xmath63 .",
    "here we describe the comp , dd and sss algorithms , as introduced in @xcite and @xcite , and discuss the conditions under which they succeed .",
    "we make one further key definition .",
    "[ def : masked ] given an item @xmath16 and a set of items @xmath64 , we say that @xmath16 is _ masked _ by @xmath64 if every test which includes @xmath16 also includes at least one member of @xmath64 .",
    "the comp algorithm @xcite is based on a simple inference : any negative test only contains non - defective items , so any item in a negative test can be marked as non - defective . given enough negative tests , we might hope to correctly infer every member of @xmath65 in this way .",
    "formally speaking , the comp algorithm proceeds as follows :    [ def : comp ]    1 .",
    "mark each item which appears in a negative test as non - defective , and refer to every other item as a possible defective ( pd )  we write @xmath66 for the set of such items .",
    "2 .   mark every item in @xmath66 as defective .",
    "clearly , the first step will not make any mistakes ( every item marked as non - defective will indeed be non - defective ) , so errors will only occur in the second step . as a result comp",
    "will always estimate @xmath6 by a set @xmath67 with @xmath68 .    as in @xcite , a particular quantity of interest",
    "is @xmath69 , the number of non - defective items masked by the defective set @xmath6 ; that is , non - defective items which do not appear in any negative test .",
    "it is clear that comp succeeds ( recovers the defective set exactly ) if and only if @xmath70 , so that @xmath71 we use this in the proof of theorem [ thm : previous].1 in section [ sec : maintheorem ] .",
    "the dd ( ` definite defectives ' ) algorithm builds on the first step of comp , as follows :    [ def : dd ]    1 .",
    "mark each item which appears in a negative test as non - defective , and refer to every other item as a possible defective ( @xmath66 ) .",
    "2 .   for each positive test which contains a single possible defective item , mark that item as defective . 3 .   mark all remaining items as non - defective .",
    "again , the first step will not make any mistakes , and since every positive test must contain at least one defective item , the second step is also certainly correct .",
    "hence , any errors due to dd come from marking a true defective as non - defective in the third step , meaning that the estimate @xmath72 .",
    "the choice to mark all remaining items as non - defective is motivated by the sparsity of the problem , since _ a priori _ an item is much less likely to be defective than non - defective .",
    "we analyse dd rigorously in section [ sec : maintheorem ] , using the following notation , used in @xcite and illustrated in figure [ fig : itemrvs ] . for each @xmath21 , we write :    * @xmath73 for the number of tests containing defective item @xmath16 and no other defective ; * @xmath74 for the number of tests containing defective item @xmath16 and no other possible defective item ( no other member of @xmath66 ) .    in the terminology of definition [ def : masked ] , we see that dd succeeds if and only if no defective item @xmath21 is masked by @xmath75 .",
    "further , since item @xmath16 is masked by @xmath75 if and only if @xmath76 , we can write : @xmath77    for a given defective item @xmath21 , we write @xmath78 for the set of defectives with @xmath16 removed . for",
    "a given set @xmath79 , we write @xmath80 for the total number of tests containing at least one item from @xmath79 .",
    "the random variable @xmath81 ( the total number of tests containing at least one item in @xmath82 ) , henceforth denoted by @xmath83 , will be of particular interest .",
    "( 400,450)(0,120 )    ( 200,180 ) ( 210,200)(0,1)30 ( 140,140 ) ( 100,160)(1,0)130 ( 100,160)(0,1)10 ( 230,160)(0,1)10 ( 270,160 ) ( 100,450 )    [ cols=\"^,^,^,^,^,^ \" , ]     ( 270,268 )    c 0 + 0 +   +   + 0    ( 300,268 )    c 0 + 0 +   +   + 0    ( 320,268 )    c 0 + 0 +   +   + 0    ( 350,268 )    c 0 + 0 +   +   + 0    ( 270,350 )    c 0 + 0 +   +   + 0    ( 300,350 )    c 0 + 1 +   +   + 1    ( 320,350 )    c 0 + 0 +   +   + 1    ( 350,350 )    c 0 + 1 +   +   + 0    ( 280,180)(0,1)40 ( 310,180)(0,1)40 ( 330,180)(0,1)40 ( 360,180)(0,1)40 ( 10,450 ) ( 75,395)(0,1)110 ( 75,395)(1,0)10 ( 75,505)(1,0)10 ( 10,350 ) ( 75,310)(0,1)80 ( 75,310)(1,0)10 ( 75,390)(1,0)10 ( 0,270 ) ( 75,235)(0,1)70 ( 75,235)(1,0)10 ( 75,305)(1,0)10 ( 400,385)@xmath84 ( 395,388)(-1,0)25    to understand the distributions of the quantities illustrated in figure [ fig : itemrvs ] , as in @xcite , it is helpful to think of the process by which elements of the columns are sampled as a _ coupon collector _",
    "problem , where each coupon corresponds to one of the @xmath10 tests .",
    "recall also that that distinct columns of @xmath18 are sampled independently .",
    "hence , for a single item , @xmath85 is the number of distinct coupons selected when @xmath43 coupons are chosen uniformly at random from a population of @xmath10 coupons . in general , for a set @xmath79 of size @xmath86 , the independence of distinct columns means that @xmath87 is the number of distinct coupons chosen when choosing @xmath88 coupons uniformly at random from a population of @xmath10 coupons .",
    "hence , as described in more detail in section [ sec : maintheorem ] , we can first give a concentration of measure result for @xmath83 ( see lemma [ lem : mcdiarmid ] ) , then characterize the distribution of @xmath73 given @xmath83 ( see proposition [ prop : wdef ] ) .",
    "following this , we can state the distribution of @xmath89 conditioned on @xmath90 ( see lemma [ lem : gdef ] ) , and finally deduce the distribution of @xmath74 conditioned on @xmath89 and @xmath91 ( see lemma [ lem : plkzero ] ) .",
    "this allows us to deduce bounds on .",
    "we describe one more algorithm , which we refer to as sss , in terminology taken from @xcite .",
    "[ def : sss ] say that a putative defective set @xmath92 is ` satisfying ' if :    1 .",
    "no negative test contains a member of @xmath92 .",
    "2 .   every positive test contains at least one member of @xmath92 .",
    "the sss algorithm simply finds the smallest satisfying set ( breaking ties at random ) , and takes that as an estimate @xmath93 .",
    "note that the true defective set @xmath6 is certainly a satisfying set , hence sss is guaranteed to return a set of no larger size ( that is @xmath94 ) .",
    "however , it may well not be the case that @xmath95 .",
    "we can , however , see a possible failure event for sss : if a defective item @xmath21 is masked by the other defective items @xmath96 ( in the sense of definition [ def : masked ] ) then @xmath96 will be a satisfying set in the sense above , so sss is certain to fail .",
    "hence , writing @xmath97 for the event that item @xmath16 is masked by @xmath98 , we can use the bonferroni inequality to obtain a lower bound on the sss error probability @xmath99 of the form @xmath100 this serves as a starting point for upper bounding the rate of the sss algorithm .",
    "we refer to comp and dd as practical algorithms , since they can be implemented with low run - time . for example",
    ", comp simply requires us to take one pass through the test matrix and outcomes , requiring no more than @xmath101 storage beyond the matrix itself , and @xmath102 runtime .",
    "similarly , dd builds on comp , requiring two passes through the test matrix and outcomes and can be performed with the same amount of storage and runtime .",
    "in contrast , we can think of sss as a @xmath42-@xmath40 linear programming problem , meaning that it is unlikely to be practical to run in practice for large problems .",
    "however , we think of it as a ` best possible ' algorithm ( this is made more rigorous in @xcite ) , to deduce overall performance bounds . note that although the sss algorithm may be considered to be infeasible in practice , the paper @xcite shows that a relaxation of the @xmath42-@xmath40 linear programming problem to the reals can give good performance .",
    "further , the decoding algorithms we consider here do not require exact , or even approximate , knowledge of @xmath1 .",
    "this compares to the optimal maximum likelihood decoder of @xcite , which requires the exact value of @xmath1 .",
    "note , however , that optimal choice of the parameter @xmath103 in the _ design _ stage does require @xmath1 .",
    "the ultimate goal of this section is to prove our achievable rate for the dd algorithm , but along the way , we will also prove the comp rate and the sss upper bound , since they will essentially come ` for free ' .",
    "recall that @xmath87 corresponds to total number of tests in which items from @xmath79 are placed .",
    "the following lemma shows that this quantity is concentrated around its mean .",
    "[ lem : mcdiarmid ] when making @xmath104 draws with replacement from a total of @xmath10 coupons , the total number of distinct coupons @xmath87 satisfies @xmath105    this result was stated as ( * ? ? ? * lemma 1 ) .",
    "the key insight is that the value of @xmath87 changes by at most 1 when the value of any particular coupon is changed , so the result follows using mcdiarmid s inequality @xcite .      for completeness",
    ", we give a proof of theorem [ thm : previous].2 ( sss rate ) using the above concentration result , albeit for @xmath44 instead of general @xmath41 in definition [ def : cc ] ( the latter is proved similarly ) .",
    "the upper bound of @xmath40 is well - known for arbitrary test designs , following for example from , so we choose @xmath10 according to the other term , setting @xmath106 with @xmath107 .",
    "hence , @xmath108 .",
    "the key is to observe that lemma [ lem : mcdiarmid ] above shows that for @xmath109 or @xmath110 , choosing @xmath111 reveals that @xmath87 is exponentially concentrated around @xmath112 .",
    "we consider the two terms of the rhs of separately .",
    "fixing @xmath16 , conditional on @xmath113 , the event @xmath97 occurs if each test that item @xmath16 occurs in is contained in the @xmath114 ` already hit ' tests .",
    "hence , for any @xmath115 , we can write @xmath116 similarly , for any @xmath117 , we can write the following for any @xmath118 : @xmath119 combining and , we obtain that is at least @xmath120 which we can bound away from zero using lemma [ lem : mcdiarmid ] by taking @xmath121 and taking @xmath122 for @xmath123 sufficiently small .      for any integers @xmath124 , define @xmath125 for the stirling number of the second kind ( see for example ( * ? ? ?",
    "* eq . ( 8) ) ) .",
    "recall that @xmath73 denotes the number of tests containing defective item @xmath16 but no other defectives items .",
    "the following proposition gives the distribution of this quantity conditioned on @xmath83 , the number of tests covered by those other defectives .",
    "[ prop : wdef ]    1 .",
    "we can write the distribution of @xmath126 explicitly as @xmath127 where @xmath128 denotes the falling factorial .",
    "2 .   there exists an explicit constant @xmath129 such that @xmath130 that is , a multiple of the @xmath131 mass function",
    ".    see appendix [ sec : mkdist ] .",
    "next , we observe that ( see figure [ fig : itemrvs ] ) we can write @xmath90 . recall that @xmath89 is the number of non - defectives masked by the defective set @xmath6 , and observe that since an item is only counted in @xmath89 if each of the tests appearing in the corresponding column are in the set of size @xmath91 , we have the following .    [",
    "lem : gdef ] conditional on @xmath132 we have @xmath133      for completeness , we give a proof of theorem [ thm : previous].1 using the above results .",
    "we consider the regime where @xmath134 , where @xmath135 .",
    "as mentioned in , comp succeeds if and only if @xmath136 . using lemma [ lem : gdef ]",
    ", we know that @xmath137 which is a decreasing function in @xmath138 . hence , given @xmath139 , for all @xmath140 , we have @xmath141 next , using the fact that @xmath142 we find that for any @xmath143 , we can choose @xmath139 sufficiently small that @xmath144 , and hence @xmath145 we deduce that the success probability of comp satisfies @xmath146,\\end{aligned}\\ ] ] which is seen to tend to @xmath40 by taking @xmath147 in lemma [ lem : mcdiarmid ] ( since we collect a total of @xmath148 coupons ) .",
    "recalling that @xmath74 denotes the number of tests containing defective item @xmath16 and no other `` possible defective '' ( item from @xmath66 ) , we have the following .    [",
    "lem : plkzero ] for any @xmath149 , @xmath114 , @xmath150 , we have @xmath151 where @xmath152    see appendix [ sec : mkdist ] .",
    "note that the function @xmath153 also appeared in @xcite ; however , our analysis here requires using it very differently .",
    "in particular , we make use of the following properties , the proofs of which are deferred to appendix [ sec : phipropproofs ] .",
    "[ lem : phiupdown ] for all values of @xmath150 , @xmath154 and @xmath155 , the function @xmath153 introduced in has the properties that :    1 .",
    "@xmath153 is increasing in @xmath154 , 2 .",
    "@xmath153 is increasing in @xmath155 , 3 .",
    "@xmath153 is decreasing in @xmath150 .",
    "[ prop : phibd ] if @xmath156 , then @xmath157      we put the above results together to prove theorem [ thm : ddrate ] , giving a lower bound on the achievable rate of the dd algorithm .",
    "as in @xcite , the key is to express the success probability @xmath158 in terms of an expectation of the function @xmath159 , and to show that this expectation is concentrated in a regime where @xmath159 takes favourable values .",
    "recall that we consider the regime where @xmath160 , with @xmath161 and @xmath162 .",
    "it is useful to know that in this regime , @xmath43 satisfies @xmath163 .    as in @xcite , writing @xmath158 for the success probability of dd and applying the union bound to we know that @xmath164 so that @xmath158 will tend to 1 ( as required ) if , for a particular defective item @xmath21 , @xmath165 since symmetry means that @xmath166 is equal for each @xmath21 .",
    "the stated value for the rate then follows on substituting the value of @xmath10 and into .    in order to characterize @xmath167",
    ", we define @xmath168 and @xmath169 , for some @xmath170 , @xmath171 and @xmath172 to be chosen shortly . using lemma [ lem : plkzero ] , we can write @xmath173 where :    * follows because , by lemma [ lem : phiupdown ] , on the event @xmath174 the bound @xmath175 holds and everywhere else @xmath176 ( since @xmath159 represents a probability ) ; * folllows since @xmath177 .",
    "we consider the terms of separately , taking @xmath178 , @xmath179 , @xmath180 , where @xmath181 .    1 .",
    "the first term of can be bounded as follows .",
    "combining @xmath182 and @xmath178 gives @xmath183 , and recalling that @xmath180 and @xmath184 , it follows that @xmath185 where the second line follows by combining @xmath186 and @xmath187 , and the third line follows since @xmath188 under the above choice @xmath181 . + we claim that implies @xmath189 for all @xmath190 . indeed",
    ", we have @xmath191 and @xmath192 , so that @xmath193 , whereas implies that @xmath194 decays to zero strictly faster than @xmath195 .",
    "this implies that @xmath196 since the conditions of proposition [ prop : phibd ] are satisfied under these arguments .",
    "+ writing @xmath197 ( which is decreasing in @xmath150 by lemma [ lem : phiupdown].3 ) , we can bound @xmath1 times the inner sum of using , as follows : @xmath198 here : * follows from the second part of proposition [ prop : wdef ] .",
    "* uses the following argument : the bracketed term in is easily verified to be increasing in @xmath150 by substituting the binomial mass function and noting @xmath199 , and we already know from lemma [ lem : phiupdown ] that @xmath200 is decreasing .",
    "hence , is the expectation of the product of an increasing and decreasing function , and is therefore bounded above by the product of the expectations of those functions .",
    "( this uses ` chebyshev s other inequality '  see ( * ? ? ? * eq .",
    "( 1.7 ) ) ) .",
    "in fact , ( * ? ? ?",
    "* eq .  ( 1.7 ) ) concerns @xmath201 $ ] for two _ increasing _ functions , but we can transform this to @xmath202 $ ] for decreasing @xmath203 by simply defining @xmath204 . ) * follows by upper bounding @xmath205 and @xmath206 ( as established above ) , and then evaluating the summation explicitly . *",
    "follows by substituting @xmath179 .",
    "* follows from @xmath207 , along with the fact that @xmath208 by @xmath209 and @xmath4 ( and hence @xmath210 for some @xmath211 ) .",
    "* follows by again using @xmath209 . + we conclude that acts as an upper bound on @xmath1 times the first term of .",
    "overall tends to zero for @xmath139 sufficiently small , since @xmath212 tends to 1 in this regime .",
    "2 .   the second term of decays to zero exponentially fast in @xmath0 , using lemma [ lem : mcdiarmid ] . in this case , we make @xmath213 draws with replacement , so that @xmath214 , meaning that we can take @xmath215 in lemma [ lem : mcdiarmid ] to obtain @xmath216 since @xmath217 and @xmath4 ( and hence @xmath210 for some @xmath211 ) .",
    "we conclude that this term tends to zero , since the exponent behaves as @xmath218 .",
    "3 .   to control the third term in ,",
    "observe that if @xmath219 , then @xmath220 where the first inequality holds since @xmath221 and @xmath179 , the equality holds since @xmath182 , and the final inequality holds for @xmath1 sufficiently large .",
    "hence , and defining @xmath222 , lemma [ lem : gdef ] gives that @xmath223 where the second line follows from bernstein s inequality ( * ? ? ?",
    "* eq .  ( 2.10 ) ) , the third line follows from @xmath222 and @xmath224 , and the final line follows since the ratio of @xmath225 to @xmath226 tends to zero as @xmath3 ( and hence @xmath227 , since @xmath193 ) .",
    "+ finally , since @xmath228 , we find that @xmath226 behaves as @xmath229 for some @xmath230 that can be made arbitrarily close to @xmath162 by choosing @xmath139 and @xmath143 sufficiently small . by definition , @xmath231 , and",
    "the bound in is exponential in @xmath232 , yielding the desired result .",
    "the following theorem provides an upper bound on the rate of dd with constant column weight designs .",
    "[ ubthm ] the maximum achievable rate of dd with a constant column weight design and @xmath233 defectives is upper bounded by @xmath234    this bound is illustrated in figure [ fig : ub ] .    , and the constant column weight achievability is theorem [ thm : ddrate ] . ,",
    "scaledwidth=75.0% ]    more simply , and only very slightly worse , we have @xmath235 we get from by optimizing the first term ( numerically ) at @xmath236 , and optimizing the second term ( by differentiating ) at @xmath44 .",
    "the message here is that while is it is unclear whether the low - sparsity rates of dd with constant column weight designs can be improved via different analysis techniques , we know that some amount of gap to the counting bound is unavoidable . more specifically , while we do nt know the maximum achievable rate of dd for @xmath38 , shows the rate is always below @xmath237 , so we can not achieve the counting bound of @xmath40 .",
    "for the fixed choice @xmath44 , one can slightly tighten this upper bound to @xmath238 .",
    "the second term in is the algorithm - independent converse of @xcite with @xmath239 unoptimised .",
    "( specialising to @xmath44 gives the bound of @xcite and theorem [ thm : previous ] part 2 of this paper , and is the second term in the simpler form . ) it remains to prove the first term , for fixed @xmath239 .",
    "then optimising over @xmath239 gives the result .",
    "we follow a similar proof for the bernoulli case in @xcite .",
    "the idea is that the group testing rate is bounded by the entropy rate of the outcomes , @xmath240 .",
    "a simple bound would be to write @xmath241 and note that @xmath242 , to get the counting bound .",
    "however , a key point is that dd makes no use of tests containing two or more defectives ",
    "these necessarily have two or more possible defectives , so can not be used to ` convert ' a possible defective to a definite defective  so we can bound the group testing rate by the entropy rate of the outcomes with only zero or one defectives .",
    "write @xmath243 , @xmath244 , @xmath245 for the probability a given test contains , respectively zero , one , and at least two defectives , and write @xmath246 , @xmath247 , and @xmath248 for the number of such tests",
    ". then @xmath249 here , the expression for @xmath244 comes about since we need a test to be ` hit ' by one or more of the @xmath43 coupons corresponding to one of the @xmath1 defectives , and missed by the @xmath250 coupons corresponding to the other @xmath251 defectives .",
    "further , by linearity of expectation , we have @xmath252 and we have exponential concentration of @xmath253 around these means .",
    "( for @xmath246 this follows immediately form lemma [ lem : mcdiarmid ] ; the argument for @xmath247 and @xmath248 is identical , so we do not repeat it here . )",
    "we fix @xmath30 .",
    "the event that @xmath254 has probability at least @xmath31 for @xmath0 ( and hence @xmath10 ) sufficiently large .",
    "thus , by conditioning on this event , and recalling the above discussion that dd only uses tests with @xmath42 or @xmath40 defectives , we have for sufficiently large @xmath0 that @xmath255 where :    * in , @xmath256 is the vector of outcomes of the tests containing @xmath42 or @xmath40 defectives , and @xmath257 is the entropy function . * in , we have used that the entropy of @xmath256 given @xmath248 is decreasing in @xmath248 .",
    "moreover , we have added @xmath143 to account for the uncertainty in the length of @xmath258 itself , which behaves as @xmath259 since the length concentrates exponentially around its mean . * in , we have used the inequality @xmath260 , and we have defined @xmath261 and @xmath262 to be defined in the same way as @xmath243 and @xmath244 but conditioned on @xmath263 , as well as letting @xmath203 denote the binary entropy function . * in , we have used continuity of @xmath203 to replace @xmath264 with its limit , at the cost of an extra @xmath143 . specifically , we have @xmath265 and @xmath266 as @xmath267 , and these in turn have limiting values as characterized above .    taking @xmath143 arbitrarily small gives the result .",
    "we have shown the simple and practical dd algorithm outperforms the best possible group testing algorithms ( practical or not ) for bernoulli designs for a broad range of ( dense ) sparsity levels , as well as showing that it beats the best known performance bounds for practical algorithms with bernoulli designs at all sparsity levels .",
    "we briefly mention four interesting open problems connected with this paper , which we hope to address in future work :    1 .",
    "it remains open to determine the capacity of constant column weight designs for @xmath268 , in the spirit of theorem [ thm : berncap ] .",
    "we conjecture that the value is given by a similar expression , corresponding to the performance of a maximum likelihood algorithm ( or equivalently the sss algorithm of @xcite ) , and that part 2 of theorem [ thm : previous ] is sharp .",
    "it is an important open problem to decide whether ` practical ' algorithms can improve on the performance of dd .",
    "for example , the scomp algorithm of @xcite and approaches based on linear programming both have a rate at least as large as dd @xcite .",
    "however , we do not know how to determine whether these algorithms or others can have a higher rate than dd .",
    "3 .   it remains of great interest to determine whether a capacity of @xmath40 can be achieved for values of @xmath51 beyond @xmath269 using constant column weights or some other design , as well as determining whether there exists an _",
    "adaptivity gap _ : does there exist @xmath270 such that any non - adaptive design must have rate less than @xmath40 , despite the rate of @xmath40 being achievable when adaptivity is allowed ? 4 .",
    "a design of potential interest is that with both constant column weights ( tests - per - item ) and constant row weights ( items - per - test ) .",
    "on one hand , the nonrigorous work of mzard , tarzia and toninelli @xcite suggests that for @xmath270 there may be no gain over constant column weight designs considered here . on the other hand , in the light of wadayama s work @xcite on constant row - and - column designs in the regime where @xmath1 grows linearly with @xmath0 , and the performance of ldpc codes , this",
    "does seem like a natural place to look for improvements .",
    "either way , the independence of columns of @xmath18 plays a significant role in the proofs of this paper , so new arguments would be required to investigate this .",
    "m.  aldridge was supported by the heilbronn institute for mathematical research .",
    "j.  scarlett was supported by the ` epfl fellows ' programme ( horizon2020 grant 665667 ) .",
    "the two parts of the proposition are proved as follows :    1 .",
    "we prove this part directly .",
    "suppose that we pick @xmath43 coupons from a population of @xmath10 coupons , @xmath114 of which were previously chosen .",
    "clearly , the probability of the event that exactly @xmath154 of the coupons picked were previously chosen is @xmath271 .",
    "+ conditioning on this event , we calculate the probability that we pick @xmath272 coupons out of a population of @xmath273 coupons and obtain exactly @xmath150 distinct new coupons .",
    "clearly we require @xmath274 , or @xmath275 . by a standard counting argument , we can choose these @xmath150 coupons @xmath276 ways , then @xmath277 ways of placing the @xmath272 coupons into @xmath150 unlabelled bins such that none of them are empty ( see ( * ? ? ?",
    "* p.204 ) ) , and finally @xmath278 different labellings of the bins .",
    "moreover , overall there are @xmath279 assignments of the coupons .",
    "+ putting this all together and recalling the definition @xmath280 we have @xmath281 as required .",
    "2 .   relabelling @xmath282 and using the fact that @xmath283 ( see @xcite ) , we obtain that the inner sum of is : @xmath284 where the third line follows by explicitly evaluating the summation , and the final line holds with @xmath285 since @xmath286 + this allows us to deduce that the whole of satisfies @xmath287 as required .",
    "the case @xmath288 is trivial , so we assume here that @xmath289 .",
    "we have conditioned on @xmath113 ( i.e. , there are @xmath114 tests containing one or more item from @xmath290 ) , @xmath291 ( i.e. , there are @xmath150 tests that contain item @xmath16 and no member of @xmath290 ) , and @xmath292 ( i.e. , there are @xmath149 items labelled as possibly defective but not in @xmath6 ) .    by relabelling , without loss of generality , we can assume that tests @xmath293 are the ones that contain defective item @xmath16 and no other defective item .",
    "we write @xmath294 for the event that test @xmath154 does not have any element of @xmath295 in it .    if an item is in @xmath295 , then the tests that it appears in are chosen uniformly among those which already contain a defective .",
    "hence , for any set @xmath296 of size @xmath297 , we have @xmath298 , since we require that the @xmath43 coupons of each of @xmath149 items in @xmath295 take values in the set of positive tests ( @xmath299 in total ) , but avoid the specified @xmath297 tests .",
    "thus , @xmath300 and the result follows .",
    "1 .   as in (",
    "* lemma 32 ) , a direct calculation using the fact that @xmath302 gives @xmath303 where the second line uses the fact that @xmath304 and the third line above follows by relabelling @xmath305 .",
    "2 .   again using @xmath302",
    ", we can write @xmath306 where the third line follows from . 3 .   finally , by expanding @xmath307",
    ", we can write @xmath308 again using .          we can expand @xmath311 where the second line can be seen by directly evaluating the summation , the forth line follows by recognising the bracketed inner sum in as a multiple of the stirling number using , and the last line follows since by relabelling @xmath312 and noting that @xmath313 is non - zero only when @xmath314 .",
    "the result now follows by writing @xmath315 and @xmath316 .",
    "using lemma [ lem : phipoly ] , we consider @xmath153 as a sum of the form @xmath319 where @xmath320 by the alternating series test , if @xmath321 is a monotonically decreasing sequence , we can bound @xmath322 , and the result follows .",
    "we can verify that @xmath321 is indeed monotonically decreasing by considering the ratio : @xmath323 the first bracketed term in is trivially decreasing in @xmath324 , and the second bracketed term in is decreasing in @xmath324 by lemma [ lem : stirlc ] .",
    "hence , since the ratio is decreasing in @xmath324 , it is sufficient to verify that @xmath325 .",
    "since @xmath326 and @xmath327 , direct substitution in gives that @xmath328 , so it is sufficient to assume that @xmath329 .      c.  aksoylar , g.  atia , and v.  saligrama .",
    "sparse signal processing with linear and non - linear observations : a unified shannon theoretic approach . in",
    "_ proceedings of the 2013 ieee information theory workshop _ , pages 15 , sept 2013 .",
    "m.  p. aldridge , o.  t. johnson , and j.  scarlett .",
    "improved group testing rates with constant column weight designs . in _ proceedings of the 2016 ieee international symposium on information theory , barcelona spain ,",
    "july 2016 _ , pages 13811385 , 2016 .",
    "l.  baldassini , o.  t. johnson , and m.  p. aldridge . the capacity of adaptive group testing . in _ proceedings of the 2013 ieee international symposium on information theory , istanbul turkey ,",
    "july 2013 _ , pages 26762680 , 2013 .",
    "c.  l. chan , p.  h. che , s.  jaggi , and v.  saligrama .",
    "non - adaptive probabilistic group testing with noisy measurements : near - optimal bounds with efficient algorithms . in _ proceedings of the 49th annual allerton conference on communication , control , and computing _",
    ", pages 1832 1839 , sept . 2011 .",
    "a.  emad and o.  milenkovic .",
    "poisson group testing : a probabilistic model for nonadaptive streaming boolean compressed sensing . in _ 2014 ieee international conference on acoustics , speech and signal processing ( icassp ) _ , pages 33353339 .",
    "ieee , 2014 .    j.  l. gastwirth and p.  a. hammick . estimation of the prevalence of a rare disease , preserving the anonymity of the subjects by group testing : application to estimating the prevalence of aids antibodies in blood donors .",
    ", 22(1):1527 , 1989 .                c.  lo , m.  liu , j.  p. lynch , and a.  c. gilbert .",
    "efficient sensor fault detection using combinatorial group testing . in _ 2013 ieee international conference on distributed computing in sensor systems ( dcoss ) , _ , pages 199206 .",
    "ieee , 2013 .",
    "d.  malioutov and m.  malyutov .",
    "boolean compressed sensing : lp relaxation for group testing . in _",
    "2012 ieee international conference on acoustics , speech and signal processing ( icassp ) _ , pages 33053308 .",
    "ieee , 2012 .",
    "m.  malyutov .",
    "search for sparse active inputs : a review . in _ information theory , combinatorics and search theory _ ,",
    "volume 7777 of _ lecture notes in computer science _ , pages 609647 .",
    "springer , london , 2013 .",
    "s.  wu , s.  wei , y.  wang , r.  vaidyanathan , and j.  yuan .",
    "achievable partition information rate over noisy multi - access boolean channel . in _",
    "2014 ieee international symposium on information theory _",
    ", pages 12061210 .",
    "ieee , 2014 ."
  ],
  "abstract_text": [
    "<S> we consider the nonadaptive group testing problem in the case that each item appears in a constant number of tests , chosen uniformly at random with replacement , so that the testing matrix has ( almost ) constant column weights . </S>",
    "<S> we analyse the performance of simple and practical algorithms in a range of sparsity regimes , showing that the performance is consistently improved in comparison with more standard bernoulli designs . in particular , using a constant - column weight design , the dd algorithm is shown to outperform all possible algorithms for bernoulli designs in a broad range of sparsity regimes , and to beat the best - known theoretical guarantees of existing practical algorithms in all sparsity regimes .    </S>",
    "<S> keywords : algorithms , concentration - of - measure , group testing .    </S>",
    "<S> msc : 60c05 ; 68w40 </S>"
  ]
}