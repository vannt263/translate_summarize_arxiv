{
  "article_text": [
    "[ index : coco - performance - assessment ]      we present ideas and concepts for performance assessment when benchmarking numerical optimization algorithms in a black - box scenario . going beyond a simple ranking of algorithms",
    ", we aim to provide a _ quantitative _ and _ meaningful _ performance assessment , which allows for conclusions like _ algorithm a is seven times faster than algorithm b _ in solving a given problem or in solving problems with certain characteristics . for this end , we record algorithm _",
    "runtimes , measured in number of function evaluations _ to reach predefined target values during the algorithm run .",
    "runtimes represent the cost of optimization .",
    "apart from a short , exploratory experiment , we do not measure the algorithm cost in cpu or wall - clock time .",
    "see for example [ index : id5 ] for a discussion on shortcomings and unfortunate consequences of benchmarking based on cpu time .",
    "in the https://github.com/numbbo/coco[coco ] platform [ index : id6 ] , we display average runtimes ( art , see section ) and the empirical distribution function of runtimes ( ecdf , see section ) . when displaying runtime distributions , we consider the aggregation over target values and over subclasses of problems , or all problems .      in the https://github.com/numbbo/coco[coco ] framework in general ,",
    "a * problem * , or problem instance triplet , @xmath0 , is defined by the search space dimension @xmath1 , the objective function @xmath2 , to be minimized , and its instance parameters @xmath3 for instance @xmath4 .",
    "more concisely , we consider a set of parametrized benchmark functions @xmath5 and the corresponding problems @xmath6 .",
    "different instances vary by having different shifted optima , can use different rotations that are applied to the variables , have different optimal @xmath2-values , etc .",
    "[ index : id9 ] .",
    "the instance notion is introduced to generate repetition while avoiding possible exploitation of artificial function properties ( like location of the optimum in zero ) .",
    "the separation of dimension and instance parameters in the notation serves as a hint to indicate that we never aggregate over dimension and always aggregate over all @xmath3-values .    in the performance assessment",
    "setting , we associate to a problem instance @xmath0 a quality indicator mapping and a target value , such that a problem becomes a quintuple @xmath7 .",
    "usually , the quality indicator remains the same for all problems , while we have subsets of problems which only differ in their target value .",
    "evaluating performance is necessarily based on performance _",
    "measures _ , the definition of which plays a crucial role for the evaluation . here",
    ", we introduce a list of requirements a performance measure should satisfy in general , as well as in the context of black - box optimization specifically . in general , a performance measure should be    * quantitative , as opposed to a simple _ ranking _ of entrants ( e.g. , algorithms ) .",
    "ideally , the measure should be defined on a ratio scale ( as opposed to an interval or ordinal scale ) [ index : id10 ] , which allows to state that `` entrant a is @xmath8 _ times better _ than entrant b '' . * assuming a wide variation of values such that , for example , typical values do not only range between 0.98 and 1.0 , * well interpretable , in particular by having meaning and semantics attached to the measured numbers , * relevant and meaningful with respect to the `` real world '' , * as simple and as comprehensible as possible .    in the context of black - box optimization ,",
    "the * runtime * to reach a target value , measured in number of function evaluations , satisfies all requirements .",
    "runtime is well - interpretable and meaningful with respect to the real - world as it represents time needed to solve a problem .",
    "measuring number of function evaluations avoids the shortcomings of cpu measurements that depend on parameters like the programming language , coding style , machine used to run the experiment , etc .",
    ", that are difficult or impractical to control .",
    "if however algorithm internal computations dominate wall - clock time in a practical application , comparative runtime results _ in number of function evaluations _ can usually be adapted _ a posteri _ to reflect the practical scenario .",
    "this hold also true for a speed up from parallelization .",
    "[ index : sec - verthori ] at each evaluation count ( time step ) @xmath9 of an algorithm which optimizes a problem instance @xmath3 of the function @xmath10 in dimension @xmath1 , we apply a quality indicator mapping . a quality indicator @xmath11 maps the set of all solutions evaluated so far ( or recommended [ index : id16 ] ) to a problem - dependent real value .",
    "then , a runtime measurement can be obtained from each of a ( large ) set of problem instances @xmath12 .",
    "the runtime on this problem instance is defined as the evaluation count when the quality indicator value drops below the target for the first time , otherwise runtime remains undefined .        in the multi - objective case ,",
    "the quality indicator is based on a negative hypervolume indicator of the set of evaluated solutions ( more specifically , the the non - dominated archive ) [ index : id18 ] , while other well- or lesser - known multi - objective quality indicators are possible .",
    "starting from the most basic convergence graphs which plot the evolution of a quality indicator , to be minimized , against the number of function evaluations , there are essentially only two ways to measure the performance .",
    "fixed - budget approach : : :    we fix a maximal budget of function evaluations , and measure the    reached quality indicator value .",
    "a fixed search budget can be pictured    as drawing a _ vertical _ line in the figure ( blue line in figure ) .",
    "fixed - target approach : : :    we fix a target quality value and measure the number of function    evaluations , the _ runtime _ , to reach this target .",
    "a fixed target can    be pictured as drawing a _ horizontal _ line in the figure ( red line in    figure ) .",
    "illustration of fixed - budget view ( vertical cuts ) and fixed - target view ( horizontal cuts ) .",
    "black lines depict the best quality indicator value plotted versus number of function evaluations .",
    "[ index : fig - horizontalvsvertical ]      * the fixed - budget approach ( vertical cut ) does not give _",
    "quantitatively interpretable _ data : the observation that algorithm a reaches a quality indicator value that is , say , two times smaller than the one reached by algorithm b has in general no interpretable meaning , mainly because there is no _ a priori _ way to determine _ how much _ more difficult it is to reach an indicator value that is two times smaller .",
    "this usually depends on the function , the definition of the quality indicator and even the specific indicator values compared . *",
    "the fixed - target approach ( horizontal cut ) _ measures the time _ to reach a target quality value .",
    "the measurement allows conclusions of the type : algorithm a is two ( or ten , or a hundred ) times faster than algorithm b in solving this problem .",
    "the choice of the target value determines the difficulty and often the characteristic of the problem to be solved .",
    "furthermore , for algorithms that are invariant under certain transformations of the function value ( for example under order - preserving transformations , as comparison - based algorithms like de , es , pso [ index : id20 ] ) , fixed - target measures are invariant under these transformations if the target values are transformed accordingly .",
    "that is , only the horizontal line needs to be moved .",
    "fixed - budget measures require the transformation of all resulting measurements individually .      investigating the figure more carefully",
    ", we find that not all graphs intersect with either the vertical or the horizontal line . on the one hand , if the fixed budget is too large , the algorithm might solve the function before the budget is exceeded .",
    "the algorithm performs better than the measurement is able to reflect , which can lead to a serious misinterpretations .",
    "the remedy is to define a _",
    "final _ target value and measure the runtime if the final target is hit .    on the other hand , if the fixed target is too difficult , the algorithm may never hit the target under the given experimental conditions .",
    "the algorithm performs worse than the experiment is able to reflect , while we still get a lower bound for this missing runtime instance .",
    "a possible remedy is to run the algorithm longer .",
    "another possible remedy is to use the final quality indicator value as measurement .",
    "this measurement however should only be interpreted as ranking result , defeating the original objective . a third ( impartial )",
    "remedy is to record the overall number of function evaluations of this run and use simulated restarts , see below .",
    "first , we define for each problem instance @xmath14 a _ reference _ quality indicator value , @xmath15 . in the single - objective case",
    "this is the optimal function value . in the multi - objective case",
    "this is the hypervolume indicator of an approximation of the pareto front [ index : id27 ] .",
    "based on this reference value and a set of target _ precision _ values , which are independent of the instance @xmath3 , we define a target value @xmath16 for each precision @xmath17 , giving rise to the product set of all problems @xmath0 and all @xmath17-values .",
    "runlength - based target values are a novel way to define the target values based on a reference data set . like for _ performance profiles _ [ index : id28 ] ,",
    "the resulting empirical distribution can be interpreted _ relative to a reference algorithm or a set of reference algorithms_. unlike for performance profiles , the resulting empirical distribution _ is _ a data profile [ index : id29 ] reflecting the true ( opposed to relative ) difficulty of the respective problems for the respective algorithm .",
    "we assume to have given a reference data set with recorded runtimes to reach a prescribed , usually large set of quality indicator target values as in the fixed - target approach described above .",
    "the reference data serve as a baseline upon which the runlength - based targets are computed . to simplify wordings",
    "we assume w.l.o.g . that a single reference _ algorithm _ has generated this data set .",
    "now we choose a set of increasing reference _",
    "budgets_. to each budget , starting with the smallest , we associate the easiest ( largest ) target for which ( i ) the average runtime ( taken over all respective @xmath3 instances , @xmath18 , see below ) of the reference algorithm _ exceeds _ the budget and ( ii , optionally ) that had not been chosen for a smaller budget before . if such target does not exist , we take the final ( smallest ) target .",
    "runlength - based targets are used in https://github.com/numbbo/coco[coco ] for the single - objective expensive optimization scenario .",
    "the artificial best algorithm of bbob-2009 ( see below ) is used as reference algorithm with either the five budgets of @xmath19 , @xmath20 , @xmath21 , @xmath22 , and @xmath23 function evaluations , where @xmath1 is the problem dimension , or with 31 targets evenly space on the log scale between @xmath19 and @xmath23 and without the optional constraint from ( ii ) above . in the latter case , the empirical distribution function of the runtimes of the reference algorithm shown in a _ semilogx _",
    "plot approximately resembles a diagonal straight line between the above two reference budgets .",
    "runlength - based targets have the * advantage * to make the target value setting less dependent on the expertise of a human designer , because only the reference _ budgets _ have to be chosen a priori . reference budgets , as runtimes , are intuitively meaningful quantities , on which it is comparatively easy to decide upon .",
    "runlength - based targets have the * disadvantage * to depend on the choice of a reference data set , that is , they depend on a set of reference algorithms .      in the performance assessment context of https://github.com/numbbo/coco[coco ] , a problem instance can be defined by the quintuple search space dimension , function , instantiation parameters , quality indicator mapping , and quality indicator target value , @xmath24 .",
    "for each benchmarked algorithm , a single runtime is measured on each problem instance . from",
    "a _ single _ run of the algorithm on the problem instance triple @xmath6 , we obtain a runtime measurement for _ each _ corresponding problem quintuple @xmath7 , more specifically , one for each target value which has been reached in this run , or equivalently , for each target precision .",
    "this also reflects the anytime aspect of the performance evaluation in a single run .",
    "formally , the runtime @xmath25 is a random variable that represents the number of function evaluations needed to reach the quality indicator target value for the first time . a run or trial that reached the target value",
    "is called _",
    "successful_. for _ unsuccessful trials _ , the runtime is not defined , but the overall number of function evaluations in the given trial is a random variable denoted by @xmath26 . for a single run , the value of @xmath26 is the same for all failed targets .",
    "we consider the conceptual * restart algorithm*. given an algorithm has a strictly positive probability @xmath27 to solve a problem , independent restarts of the algorithm solve the problem with probability one and exhibit the runtime [ index : equation - rtrestart]@xmath28 where @xmath29 is a random variable with negative binomial distribution that models the number of unsuccessful runs until one success is observed and @xmath30 are independent random variables corresponding to the evaluations in unsuccessful trials [ index : id34 ] . if the probability of success is one , @xmath31 equals zero with probability one and the restart algorithm coincides with the original algorithm .",
    "generally , the above equation for @xmath32 expresses the runtime from repeated independent runs on the same problem instance ( while the instance @xmath3 is not given explicitly ) . for the performance evaluation in the https://github.com/numbbo/coco[coco ] framework , we apply the equation to runs on different instances @xmath3 , however instances from the same function , with the same dimension and the same target precision .",
    "different instantiations of the parametrized functions @xmath10 are a natural way to represent randomized repetitions .",
    "for example , different instances implement random translations of the search space and hence a translation of the optimum [ index : id37 ] .",
    "randomized restarts on the other hand can be conducted from different initial points . for translation",
    "invariant algorithms both mechanisms are equivalent and can be mutually exchanged .",
    "we interpret thus runs performed on different instances @xmath33 as repetitions of the same problem .",
    "thereby we assume that instances of the same parametrized function @xmath10 are similar to each other , and more specifically that they exhibit the same runtime distribution for each given @xmath17 .",
    "the runtime of the conceptual restart algorithm as given in is the basis for displaying performance within https://github.com/numbbo/coco[coco ] .",
    "we use the @xmath35 different runs on the same function and dimension to simulate virtual restarts with a fixed target precision .",
    "we assume to have at least one successful run  otherwise , the runtime remains undefined , because the virtual procedure would never stop .",
    "then , we construct artificial , simulated runs from the available empirical data : we repeatedly pick , uniformly at random with replacement , one of the @xmath35 trials until we encounter a successful trial .",
    "this procedure simulates a single sample of the virtually restarted algorithm from the given data .",
    "as given in as @xmath32 , the measured , simulated runtime is the sum of the number of function evaluations from the unsuccessful trials added to the runtime of the last and successful trial .      in practice , we repeat the above procedure between a hundred or even thousand times , thereby sampling @xmath36 simulated runtimes from the same underlying distribution , which then has striking similarities with the true distribution from a restarted algorithm [ index : id40 ] . to reduce the variance in this procedure ,",
    "when desired , the first trial in each sample is picked deterministically instead of randomly as the @xmath37-th trial from the data . picking the first trial data as specific instance @xmath3",
    "could also be interpreted as applying simulated restarts to this specific instance rather than to the entire set of problems @xmath38 .",
    "* simulated restarts allow in particular to compare algorithms with a wide range of different success probabilities by a single performance measure .",
    "conducting restarts is also valuable approach when addressing a difficult optimization problem in practice . *",
    "simulated restarts rely on the assumption that the runtime distribution for each instance is the same .",
    "if this is not the case , they still provide a reasonable performance measure , however with less of a meaningful interpretation for the result .",
    "* the runtime of simulated restarts may heavily depend on * termination conditions * applied in the benchmarked algorithm , due to the evaluations spent in unsuccessful trials , compare .",
    "this can be interpreted as disadvantage , when termination is considered as a trivial detail in the implementation  or as an advantage , when termination is considered a relevant component in the practical application of numerical optimization algorithms . *",
    "the maximal number of evaluations for which simulated runtimes are meaningful and representative depends on the experimental conditions .",
    "if all runs are successful , no restarts are simulated and all runtimes are meaningful . if all runs terminated due to standard termination conditions in the used algorithm , simulated restarts reflect the original algorithm .",
    "however , if a maximal budget is imposed for the purpose of benchmarking , simulated restarts do not necessarily reflect the real performance . in this case and",
    "if the success probability drops below 1/2 , the result is likely to give a too pessimistic viewpoint at or beyond the chosen maximal budget .",
    "see [ index : id44 ] for a more in depth discussion on how to setup restarts in the experiments .",
    "* if only few or no successes have been observed , we can see large effects without statistical significance .",
    "namely , 4/15 successes are not statistically significant against 0/15 successes on a 5%-level .",
    "[ index : averaging - runtime ] the average runtime ( @xmath18 ) , introduced in [ index : id46 ] as enes and analyzed in [ index : id47 ] as success performance and referred to as ert in [ index : id48 ] , estimates the expected runtime of the restart algorithm given in . generally , the set of trials is generated by varying @xmath3 only .",
    "we compute the @xmath18 from a set of trials as the sum of all evaluations in unsuccessful trials plus the sum of the runtimes in all successful trials , both divided by the number of successful trials .        given a data set with @xmath40 successful runs with runtimes @xmath41 and @xmath42 unsuccessful runs with @xmath30 evaluations , the average runtime reads @xmath43 where @xmath27 is the fraction of successful trials , @xmath44 is understood as zero and @xmath45 is the number of function evaluations conducted in all trials before to reach the given target precision .",
    "the average runtime , @xmath18 , is taken over different instances of the same function , dimension , and target precision , as these instances are interpreted as repetitions .",
    "taking the average is meaningful only if each instance obeys a similar distribution without heavy tail .",
    "if one instance is considerably harder than the others , the average is dominated by this instance .",
    "for this reason we do not average runtimes from different functions or different target precisions , which however could be done if the logarithm is taken first ( geometric average ) . plotting the @xmath18 divided by dimension against dimension in a log - log plot is the recommended way to investigate the scaling behavior of an algorithm .",
    "[ index : sec - ecdf ] we display a set of simulated runtimes with the empirical cumulative distribution function ( ecdf ) , aka empirical distribution function .",
    "informally , the ecdf displays the _ proportion of problems solved within a specified budget _",
    ", where the budget is given on the @xmath8-axis .",
    "more formally , an ecdf gives for each @xmath8-value the fraction of runtimes which do not exceed @xmath8 , where missing runtime values are counted in the denominator of the fraction .",
    "empirical cumulative distribution functions are a universal way to display _ unlabeled _ data in a condensed way without losing information .",
    "they allow unconstrained aggregation , because each data point remains separately displayed , and they remain entirely meaningful under transformation of the data ( e.g. taking the logarithm ) .    *",
    "the empirical distribution function from a set of problems where only the target value varies , recovers an upside - down convergence graph with the resolution steps defined by the targets [ index : id50 ] .",
    "* when runs from several instances are aggregated , the association to the single run is lost , as is the association to the function when aggregating over several functions .",
    "this is particularly problematic for data from different dimensions , because dimension can be used as decision parameter for algorithm selection .",
    "therefore , we do not aggregate over dimension . * the empirical distribution function can be read in two distinct ways .",
    "+ @xmath8-axis as independent variable : : :    for any budget ( @xmath8-value ) , we see the fraction of    problems solved within the budget as @xmath46-value , where the    limit value to the right is the fraction of solved problems with the    maximal budget . @xmath46-axis as independent variable : : :    for any fraction of easiest problems ( @xmath46-value ) , we see    the maximal runtime observed on these problems on the    @xmath8-axis . when plotted in _ semilogx _ , a horizontal shift    indicates a runtime difference by the respective factor , quantifiable ,    e.g. , as `` five times faster '' .",
    "the area below the    @xmath46-value and to the left of the graph reflects the    geometric runtime average on this subset of problems , the smaller the    better .",
    "empirical distribution functions over runtimes of optimization algorithms are also known as _",
    "data profiles _ [ index : id51 ] .",
    "they are widely used for aggregating results from different functions and different dimensions to reach a single target precision [ index : id52 ] . in the https://github.com/numbbo/coco[coco ]",
    "framework , we do not aggregation over dimension but aggregate often over a wide range of target precision values .",
    "we display in figure the ecdf of the ( simulated ) runtimes of the pure random search algorithm on the set of problems formed by 15 instances of the sphere function ( first function of the single - objective test suite ) in dimension @xmath47 each with 51 target precisions between @xmath48 and @xmath49 uniform on a log - scale and 1000 bootstraps .",
    "illustration of empirical ( cumulative ) distribution function ( ecdf ) of runtimes on the sphere function using 51 relative targets uniform on a log scale between @xmath48 and @xmath49 .",
    "the runtimes displayed correspond to the pure random search algorithm in dimension 5 .",
    "the cross on the ecdf plots of https://github.com/numbbo/coco[coco ] represents the median of the maximal length of the unsuccessful runs to solve the problems aggregated within the ecdf .",
    "[ index : fig - ecdf ]    we can see in this plot , for example , that almost 20 percent of the problems were solved within @xmath50 function evaluations .",
    "runtimes to the right of the cross at @xmath51 have at least one unsuccessful run .",
    "this can be concluded , because with pure random search each unsuccessful run exploits the maximum budget .",
    "the small dot beyond @xmath52 depicts the overall fraction of all successfully solved functions - target pairs , i.e. , the fraction of @xmath53 pairs for which at least one trial ( one @xmath3 instantiation ) was successful .",
    "we usually divide the set of all ( parametrized ) benchmark functions into subgroups sharing similar properties ( for instance separability , unimodality , ... ) and display ecdfs which aggregate the problems induced by these functions and all targets .",
    "figure shows the result of random search on the first five functions of the _ bbob _ testsuite , separate ( left ) and aggregated ( right ) .         *",
    "left : * ecdf of the runtime of the pure random search algorithm for functions f1 , f2 , f3 , f4 and f5 that constitute the group of separable functions for the testsuite over 51 target values .",
    "* right : * aggregated ecdf of the same data , that is , all functions in one graph . [",
    "index : fig - ecdfgroup ]    finally , we also naturally aggregate over all functions of the benchmark and hence obtain one single ecdf per algorithm per dimension . in figure",
    ", the ecdf of different algorithms are displayed in a single plot .          the thick maroon line with diamond markers annotated as `` best 2009 '' corresponds to the * artificial best 2009 algorithm * : for each set of problems with the same function , dimension and target precision , we select the algorithm with the smallest @xmath18 from the http://coco.gforge.inria.fr/doku.php?id=bbob-2009[bbob-2009 workshop ] and use for these problems the data from the selected algorithm .",
    "the algorithm is artificial because we may use even for different target values the runtime results from different algorithms ."
  ],
  "abstract_text": [
    "<S> we present an any - time performance assessment for benchmarking numerical optimization algorithms in a black - box scenario , applied within the https://github.com/numbbo/coco[coco ] benchmarking platform . </S>",
    "<S> the performance assessment is based on _ runtimes _ measured in number of objective function evaluations to reach one or several quality indicator target values . </S>",
    "<S> we argue that runtime is the only available measure with a generic , meaningful , and quantitative interpretation . </S>",
    "<S> we discuss the choice of the target values , runlength - based targets , and the aggregation of results by using simulated restarts , averages , and empirical distribution functions .    </S>",
    "<S> [ index::doc ] </S>"
  ]
}