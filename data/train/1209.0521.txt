{
  "article_text": [
    "presence of missing values in a dataset often makes it difficult to apply a large class of machine learning algorithms . in many real - world data - mining problems",
    ", databases may contain missing values due directly to the way the data is obtained ( e.g. survey or climate data ) , or also frequently because the gathering process changes over time ( e.g. addition of new variables or merging with other databases ) .",
    "one of the simplest ways to deal with missing data is to discard samples and/or variables that contain missing values .",
    "however , such a technique is not suited to datasets with many missing values ; these are the focus of this paper .    here",
    ", we propose to use a generative model ( a mixture of gaussians with full covariances ) to learn the underlying data distribution and replace missing values by their conditional expectation given the observed variables .",
    "a mixture of gaussians is particularly well suited to generic data - mining problems because :    * by varying the number of mixture components , one can combine the advantages of simple multivariate parametric models ( e.g. a single gaussian ) , that usually provide good generalization and stability properties , to those of non - parametric density estimators ( e.g. parzen windows , that puts one gaussian per training sample  @xcite ) , that avoid making strong assumptions on the underlying data distribution . *",
    "the expectation - maximization ( em ) training algorithm  @xcite naturally handles missing values and provides the missing values imputation mechanism .",
    "one should keep in mind that the em algorithm assumes the missing data are `` missing at random '' ( mar ) , i.e. that the probability of variables to be missing does not depend on the actual value of missing variables . even though this assumption will not always hold in practical applications like those mentioned above",
    ", we note that applying the em algorithm might still yield sensible results in the absence of theoretical justifications .",
    "* training a mixture of gaussians scales only linearly with the number of samples , which is attractive for large datasets ( at least in low dimension , and we will see in this paper how large - dimensional problems can be tackled ) . *",
    "computations with gaussians often lead to analytical solutions that avoid the use of approximate or sampling methods . *",
    "when confronted with the supervised problem of learning a function @xmath0 , a mixture of gaussians trained to learn a joint density @xmath1 directly provides a least - square estimate of @xmath2 by @xmath3 $ ] .",
    "even though such mixtures of gaussians can indeed be applied directly to supervised problems  @xcite , we will see in experiments that using them for missing value imputation before applying a discriminant learning algorithm yields better results .",
    "this observation is in line with the common belief that generative models that are trained to learn the global data distribution are not directly competitive with discriminant algorithms for prediction tasks  @xcite .",
    "however , they can provide useful information regarding the data that will help such discriminant algorithms to reach better accuracy .",
    "the contributions in this paper are two - fold :    1 .",
    "we explain why the basic em training algorithm is not practical in large - dimensional applications in the presence of missing values , and we propose a novel training algorithm that significantly speeds it up .",
    "2 .   we show , both by visual inspection on image data and by for imputed values for classification , how a mixture of gaussians can model the data distribution so as to provide a valuable tool for missing values imputation .",
    "note that an extensive study of gaussian mixture training and missing value imputation algorithms is out of the scope of this paper .",
    "various variants of em have been proposed in the past ( e.g.  @xcite ) , while we focus here on the `` original '' em , showing how it can be solved * exactly * at a significantly lower computational cost . for missing value imputation , statisticians may prefer to draw from the conditional distribution instead of inputing its mean , as the former better preserves data covariance  @xcite . in machine learning ,",
    "the fact that a value is missing may also be by itself a useful piece of information worth taking into account ( for instance by adding extra binary inputs to the model , indicating whether each value was observed ) .",
    "all these are important considerations that one should keep in mind , but they will not be addressed here .    in the following section , we present the standard em algorithm for training mixtures of gaussians in the presence of missing data .",
    "the fast em algorithm is detailed in section  [ sec : scaling ] , while section  [ sec : experiments ] describes experimental results assessing the efficiency of the proposed method for missing value imputation .",
    "finally , section  [ sec : conclusion ] concludes this paper and discusses several possible extensions .",
    "in this section , we present the em algorithm for learning a mixture of gaussians on a dataset with missing values .",
    "the notations we use are as follows .",
    "the training set is denoted by @xmath4 .",
    "each sample @xmath5 may have different missing variables .",
    "a _ missing pattern _ is a maximal set of variables that are simultaneously missing in at least one training sample .",
    "when the input dimension @xmath6 is high , there may be many different missing patterns , possibly on the same order as the number of samples @xmath7 ( since the number of possible missing patterns is @xmath8 ) . for a sample @xmath9 ,",
    "we denote by @xmath10 and @xmath11 the vectors corresponding to respectively the observed and missing variables in @xmath9 .",
    "similarly , given @xmath9 , a symmetric @xmath12 matrix @xmath13 can be split into four parts corresponding to the observed and missing variables in @xmath9 , as follows :    * @xmath14 contains elements @xmath15 where variables @xmath16 and @xmath17 are observed in @xmath9 , * @xmath18 contains elements @xmath15 where both variables @xmath16 and @xmath17 are missing in @xmath9 , * @xmath19 contains elements @xmath15 where variable @xmath16 is observed in @xmath9 , while variable @xmath17 is missing .",
    "it is important to keep in mind that with these notations , we have for instance @xmath20 , i.e. the inverse of a sub - matrix is not the sub - matrix of the inverse .",
    "also , although to keep notations simple we always write for instance @xmath14 , the observed part depends on the sample currently being considered : for two different samples , the @xmath14 matrix may represent a different sub - part of @xmath13 .",
    "it should be clear from the context which sample is being considered .",
    "the em algorithm @xcite can be directly applied in the presence of missing values @xcite . as shown in  @xcite , for a mixture of gaussians",
    "the computations for the two steps ( expectation and maximization ) of the algorithm are :    [ [ expectation ] ] expectation + + + + + + + + + + +    compute @xmath21 , the probability that gaussian @xmath22 generated sample @xmath9 . for the sake of clarity in notations , let us denote @xmath23 and @xmath24 the estimated mean and covariance of gaussian @xmath22 at iteration @xmath25 of the algorithm .",
    "to obtain @xmath21 for a given sample @xmath9 and gaussian @xmath22 , we first compute the density @xmath26 where @xmath27 is the gaussian distribution of mean @xmath28 and covariance @xmath29 : @xmath30 @xmath21 is now simply given by @xmath31 where @xmath32 is the total number of gaussians in the mixture .    [ [ maximization ] ] maximization + + + + + + + + + + + +    first fill - in missing values , i.e. define , for each gaussian  @xmath22 , @xmath33 by @xmath34 and @xmath35 being equal to the expectation of the missing values @xmath11 given the observed @xmath10 , assuming gaussian @xmath22 has generated @xmath9 . denoting again @xmath23 and @xmath24 , this expectation is equal to @xmath36 from these @xmath33 , the maximization step of em yields the new estimates for the mean and covariances of the gaussians : @xmath37 @xmath38 the additional term @xmath39 results from the imputation of missing values by their conditional expectation , and is computed as follows ( for the sake of clarity , we denote @xmath39 by @xmath40 and @xmath41 by @xmath42 ) :    1 .   @xmath40 @xmath43 0 2 .   for each @xmath9 with",
    "observed and missing parts @xmath10 , @xmath11 : @xmath44    the term being added for each sample corresponds to the covariance of the missing values @xmath11 .",
    "regularization can be added into this framework for instance by adding a small value to the diagonal of the covariance matrix @xmath41 , or by keeping only the first @xmath16 principal components of this covariance matrix ( filling the rest of the data space with a constant regularization coefficient ) .",
    "while the em algorithm naturally extends to problems with missing values , doing so comes with a high computational cost .",
    "as we will show , the computational burden may be mitigated somewhat by exploiting the similarity between the covariance matrices between `` nearby '' patterns of missing values . before we delve into the details of our algorithm ,",
    "let us first analyze the computational costs of the em algorithm presented above , for each individual training sample @xmath9 :    * the evaluation of @xmath45 from eq .",
    "[ eq : qij ] and [ eq : gaussian ] requires the inversion of @xmath29 , which costs @xmath46 operations , where @xmath47 is the number of observed variables in @xmath9 . * the contribution to @xmath39 for each gaussian @xmath22 ( eq .  [ eq : gaussian : cjt ] ) can be done in @xmath48 ( computation of @xmath49 ) , or in @xmath50 if @xmath51 is available , because : @xmath52    note that for two examples @xmath9 and @xmath53 with exactly the same pattern of missing variables , the two expensive operations above need only be performed once , as they are the same for both @xmath9 and @xmath53 . but in high - dimensional datasets without a clear structure in the missing values , most missing patterns are not shared by many samples .",
    "for instance , in a real - world financial dataset we have been working on , the number of missing patterns is about half the total number of samples .",
    "since each iteration of em has a cost of @xmath54 , with @xmath55 unique missing patterns , @xmath32 components in the mixture , and input dimension @xmath6 , the em algorithm as presented in section  [ sec : em ] is not computationally feasible for large high - dimensional datasets .",
    "the `` fast '' em variant proposed in  @xcite also suffers from the same bottlenecks , i.e. it is fast only when there are few unique missing patterns .",
    "while the large numbers of unique patterns of missing values typically found in real - world datasets present a barrier to the application of em to these problems , they also motivate a means of reducing the computational cost .",
    "as discussed above , for high - dimensional datasets , the computational cost is dominated by the determination of the inverse covariance of the observed variables @xmath56 and of the conditional covariance of the missing data given the observed data ( eq .",
    "[ eq : cond_covar ] ) .",
    "however , as we will show , these quantities corresponding to one pattern of the missing values may be determined from those of another pattern of missing values at a cost proportional to the distance between the two missing value patterns ( measured as the number of missing and observed variables on which the patterns differ ) .",
    "thus , for `` nearby '' patterns of missing values , these covariance computations may be efficiently computed by chaining their computation through the set of patterns of missing values .",
    "furthermore , since the cost of these updates will be smaller when the missing patterns of two consecutive samples are close to each other , we want to * optimize the samples ordering * so as to minimize this cost .",
    "we present the details of the proposed algorithms in the following sections .",
    "first , we observe in section  [ sec : cholesky ] how we can avoid computing @xmath56 by using the cholesky decomposition of @xmath29 .",
    "then we show in section  [ sec : ivl ] how the so - called inverse variance lemma can be used to update the conditional covariance matrix ( eq .",
    "[ eq : cond_covar ] ) for a missing pattern given the one computed for another missing pattern .",
    "as presented in section  [ sec : optimal ] , these two ideas combined give rise to an objective function with which one can determine an optimal ordering of the missing patterns , minimizing the overall computational cost .",
    "the resulting fast em algorithm is summarized in section  [ sec : algo ] .",
    "computing @xmath45 by eq .",
    "[ eq : qij ] can be done directly from the inverse covariance matrix @xmath56 as in eq .",
    "[ eq : gaussian ] , but , as argued in  @xcite , it is just as fast , and numerically more stable , to use the cholesky decomposition of @xmath29 .",
    "writing @xmath57 with @xmath32 a lower triangular matrix with positive diagonal elements , we indeed have @xmath58 where @xmath59 can easily be obtained since @xmath32 is lower triangular . assuming @xmath32 is computed once for the missing pattern of the first sample in the training set",
    ", the question is thus how to update this matrix for the next missing pattern .",
    "this reduces to finding how to update @xmath32 when adding or removing rows and columns to @xmath29 ( adding a row and column when a variable that was missing is now observed in the next sample , and removing a row and column when a variable that was observed is now missing ) .",
    "algorithms to perform these updates can be found for instance in  @xcite .",
    "when adding a row and column , we always add it as the last dimension to minimize the computations .",
    "these are on the order of @xmath60 , where @xmath47 is the length and width of @xmath29 .",
    "removing a row and column is also on the order of @xmath60 , though the exact cost depends on the position of the row / column being removed .",
    "let us denote by @xmath61 the number of differences between two consecutive missing patterns . assuming that @xmath61 is small compared to @xmath47 , the above analysis shows that the overall cost is on the order of @xmath62 computations .",
    "how to find an ordering of the patterns such that @xmath61 is small will be discussed in section  [ sec : optimal ] .",
    "the second bottleneck of the em algorithm resides in the computation of eq .",
    "[ eq : cond_covar ] , corresponding to the conditional covariance of the missing part given the observed part .",
    "note that we can not rely on a cholesky decomposition here , since we need the full conditional covariance matrix itself .",
    "in order to update @xmath63 , we will take advantage of the so - called inverse variance lemma  @xcite .",
    "it states that the inverse of a partitioned covariance matrix @xmath64 can be computed by @xmath65 where @xmath66 is the covariance of the @xmath67 part , and the matrix @xmath68 and the conditional covariance @xmath69 of the @xmath70 part given @xmath67 are obtained by @xmath71 @xmath72 note that eq .",
    "[ eq : cond_covar_bis ] is similar to eq .",
    "[ eq : cond_covar ] , since the conditional covariance of @xmath70 given @xmath67 verifies @xmath73 where we have also partitioned the inverse covariance matrix as @xmath74    these equations can be used to update the conditional covariance matrix of the missing variables given the observed variables when going from one missing pattern to the next one , so that it does not need to be re - computed from scratch .",
    "let us first consider the case of going from sample @xmath9 to @xmath75 , where we only add missing values ( i.e. all variables that are missing in @xmath9 are also missing in @xmath75 ) .",
    "we can apply the inverse variance lemma ( eq .  [ eq : ivl ] ) with the following quantities :    * @xmath76 is the conditional covariance is a covariance matrix , while we use it here as an inverse covariance : since the inverse of a symmetric positive definite matrix is also symmetric positive definite , it is possible to apply eq .",
    "[ eq : ivl ] to an inverse covariance .",
    "] of the missing variables in @xmath75 given the observed variables , i.e. @xmath63 in eq .",
    "[ eq : cond_covar ] , the quantity we want to compute , * @xmath67 are the missing variables in sample @xmath9 , * @xmath70 are the missing variables in sample @xmath75 that were not missing in @xmath9 , * @xmath77 is the conditional covariance of the missing variables in @xmath9 given the observed variables , that would have been computed previously , * since @xmath78 , then @xmath79 and @xmath80 are simply sub - matrices of the global inverse covariance matrix , that only needs to be computed once ( per iteration of em ) .",
    "let us denote by @xmath61 the number of missing values added when going from @xmath9 to @xmath75 , and by @xmath81 the number of missing values in @xmath9 .",
    "assuming @xmath61 is small compared to @xmath81 , then the computational cost of eq .",
    "[ eq : ivl ] is dominated by the cost @xmath82 for the computation of @xmath68 by eq .",
    "[ eq : b ] and of the upper - left term in eq .",
    "[ eq : ivl ] ( the inversion of @xmath69 is only in @xmath83 ) .    in the case where we remove missing values instead of adding some",
    ", this corresponds to computing @xmath77 from @xmath76 using eq .",
    "[ eq : ivl ] .",
    "this can be done from the partition of @xmath76 since , by identifying eq .",
    "[ eq : ivl ] and [ eq : partition_inv ] , we have : @xmath84 where we have used eq .",
    "[ eq : cond_identity ] to obtain the final result .",
    "once again the cost of this computation is dominated by a term of the form @xmath82 , where this time @xmath61 denotes the number of missing values that are removed when going from @xmath9 to @xmath75 and @xmath81 the number of missing values in @xmath75 .",
    "thus , in the general case where we both remove and add missing values , the cost of the update is on the order of @xmath82 , if we denote by @xmath61 the total number of differences in the missing patterns , and by @xmath81 the average number of missing values in @xmath9 and @xmath75 ( which are assumed to be close , since @xmath61 is supposed to be small ) .",
    "the speed - up is on the order of @xmath85 compared to the `` naive '' algorithm that would re - compute the conditional covariance matrix for each missing pattern .      given an ordering @xmath86 of the @xmath55 missing patterns present in the training set , during an iteration of the em algorithm we have to :    1 .",
    "compute the cholesky decomposition of @xmath29 and the conditional covariance @xmath63 for the first missing pattern @xmath87 .",
    "2 .   for each subsequent missing pattern @xmath88",
    ", find the missing pattern in @xmath89 that allows the fastest computation of the same matrices , from the update methods presented in sections  [ sec : cholesky ] and  [ sec : ivl ] .    since each missing pattern but the first one has a `` parent '' ( the missing pattern from which we update the desired matrices ) , we visit the missing patterns in a tree - like fashion : _ the optimal tree is thus the minimum spanning tree of the fully - connected graph whose nodes are the missing patterns and the weight between nodes @xmath88 and @xmath90 is the cost of computing matrices for @xmath90 given matrices for @xmath88_. note that the spanning tree obtained this way is the exact optimal solution and not an approximation ( assuming we constrain ourselves to visiting only observed missing patterns and we can compute the true cost of updating matrices ) .",
    "for the sake of simplicity , we used the number of differences @xmath61 between missing pattterns @xmath88 and @xmath90 as the weights between two nodes .",
    "finding the `` true '' cost is difficult because ( 1 )  it is implementation - dependent and ( 2 )  it depends on the ordering of the columns for the cholesky updates , and this ordering varies depending on previous updates due to the fact it is more efficient to add new dimensions as the last ones , as argued in section  [ sec : cholesky ] .",
    "we tried more sophisticated variants of the cost function , but they did not decrease significantly the overall computation time .",
    "note it would be possible to allow the creation of `` virtual '' missing patterns , whose corresponding matrices could be used by multiple observed missing patterns in order to speed - up updates even further . finding the optimal tree in this setting corresponds to finding the optimal steiner tree  @xcite , which is known to be np - hard . since we do not expect the available approximate solution schemes to provide a huge speed - up , we did not explore this approach further .    finally , one may have concerns about the numerical stability of this approach , since computations are incremental and thus numerical errors will be accumulated .",
    "the number of incremental steps is directly linked to the depth of the minimum spanning tree , which will often be logarithmic in the number of training samples , but may grow linearly in the worst case .",
    "although we did not face this problem in our own experiments ( where the accumulation of errors never led to results significantly different from the exact solution ) , the following heuristics can be used to solve it : the matrices of interest can be re - computed `` from scratch '' at each node of the tree whose depth is a multiple of @xmath16 , with @xmath16 a hyper - parameter trading accuracy for speed .",
    "we summarize here the previous sections by giving a sketch of the resulting fast em algorithm for gaussian mixtures :    1 .   find all unique missing patterns in the dataset .",
    "2 .   compute the minimum spanning tree , with @xmath55 the number of missing patterns , if @xmath55 is too large it is possible to perform an initial basic clustering of the missing patterns and compute the minimum spanning tree independently in each cluster .",
    "] of the corresponding graph of missing patterns ( see section  [ sec : optimal ] ) .",
    "3 .   deduce from the minimum spanning tree on missing patterns an ordering of the training samples ( since each missing pattern may be common to many samples ) .",
    "[ step : mst ] 4 .",
    "initialize the means of the mixture components by the @xmath91-means clustering algorithm , and their covariances from the empirical covariances in each cluster ( either imputing missing values with the cluster means or just ignoring them , which is what we did in our experiments ) .",
    "iterate through em steps ( as described in section  [ sec : em ] ) until convergence ( or until a validation error increases ) . at each step , the expensive matrix computations highlighted in section  [ sec : scaling ] are sped - up by using iterative updates , following the ordering obtained in step  [ step : mst ] .",
    "to assess the speed improvement of our proposed algorithm over the `` naive '' em algorithm , we trained mixtures of gaussians on the mnist dataset of handwritten digits . for each class of digits ( from 0 to 9 ) , we optimized an individual mixture of gaussians in order to model the class distribution .",
    "we manually added missing values by removing the pixel information in each image from a randomly chosen square of 5x5 pixels ( the images are 28x28 pixels , i.e. in dimension 784 ) .",
    "the mixtures were first trained efficiently on the first 4500 samples of each class , while the rest of the samples were used to select the hyperparameters , namely the number of gaussians ( from 1 to 10 ) , the fraction of principal components kept ( 75% , 90% or 100% ) , and the random number generator seed used in the mean initialization ( chosen between 5 different values ) .",
    "the best model was chosen based on the average negative log - likelihood .",
    "it was then re - trained using the `` naive '' version of the em algorithm , in order to compare execution time and also ensure the same results were obtained .    on average , the speed - up on our cluster computers ( 32 bit p4 3.2  ghz with 2  gb of memory ) was on the order of 8 .",
    "we also observed a larger improvement ( on the order of 20 ) on another architecture ( 64 bit athlon 2.2  ghz with 2  gb of memory ) : the difference seemed to be due to implementations of the blas and lapack linear algebra libraries .",
    "+    we display in figure  [ fig : examples ] the imputation of missing values realized by the trained mixture when provided with sample test images . on each row , images with grey squares have missing values ( identified by these squares ) , while images next to them show the result of the missing value imputation .",
    "although the imputed pixels are a bit fuzzy , the figure shows the mixture was able to capture meaningful correlations between pixels , and to impute sensible missing values .",
    "the abalone dataset from the uci machine learning repository is a standard benchmark regression task .",
    "the official training set ( 3133 samples ) is divided into a training ( 2000 ) and validation set ( 1133 ) , while we use the official test set ( 1044 ) .",
    "this dataset does not contain any missing data , which allows us to see how the algorithms behave as we add more missing values .",
    "we systematically preprocess the dataset after inserting missing values , by normalizing all variables ( including the target ) so that the mean and standard deviation on the training set are respectively 0 and 1 ( note that we do not introduce missing values in the target , so that mean squared errors can be compared ) .",
    "we compare three different missing values imputation mechanisms :    1 .",
    "imputation by the conditional expectation of the missing values as computed by a mixture of gaussians learnt on the joint distribution of the input and target ( the algorithm proposed in this paper ) 2 .",
    "imputation by the global empirical mean ( on the training set ) 3 .",
    "imputation by the value found in the nearest neighbor that has a non missing value for this variable ( or , alternatively , by the mean of the 10 such nearest neighbors ) .",
    "because there is no obvious way to compute the nearest neighbors in the presence of missing values ( see e.g.  @xcite ) , we allow this algorithm to compute the neighborhoods based on the original dataset with no missing value : it is thus expected to give the optimal performance that one could obtain with such a nearest - neighbor algorithm .",
    "one one hand , we report the performance of the mixture of gaussian used directly as a predictor for regression . on another hand ,",
    "the imputed values are also fed to the two following discriminant algorithms , whose hyper - parameters are optimized on the validation set :    1 .   a one - hidden - layer feedforward neural network trained by stochastic gradient descent , with hyper - parameters the number of hidden units ( among 5 , 10 , 15 , 20 , 30 , 50 , 100 , 200 ) , the quadratic weight decay ( among 0 , @xmath92 , @xmath93 , @xmath94 ) , the initial learning rate ( among @xmath94 , @xmath95 , @xmath93 ) and its decrease constant samples is equal to @xmath96 , where @xmath97 is the initial learning rate and @xmath98 the decrease constant . ] ( among 0 , @xmath94 , @xmath93 , @xmath92 ) .",
    "2 .   a kernel ridge regressor , with hyper - parameters the weight decay ( in @xmath99 , @xmath92 , @xmath93 , @xmath94 , 1 ) and the kernel : either the linear kernel , the gaussian kernel ( with bandwidth in 100 , 50 , 10 , 5 , 1 , 0.5 , 0.1 , 0.05 , 0.01 ) or the polynomial kernel ( with degree in 1 , 2 , 3 , 4 , 5 and dot product scaling coefficient in 0.01 , 0.05 , 0.1 , 0.5 , 1 , 5 , 10 ) .",
    "figure  [ fig : imput_compare ] compares the three missing values imputation mechanisms when using a neural network and kernel ridge regression .",
    "it can be seen that the conditional mean imputation obtained by the gaussian mixture significantly outperforms the global mean imputation and nearest neighbor imputation ( which is tried with both 1 and 10 neighbors , keeping the best on the validation set ) .",
    "the latter seems to be reliable only when there are few missing values in the dataset : this is expected , as when the number of missing values increases one has to go further in space to find neighbors that contain non - missing values for the desired variables .",
    "figure  [ fig : discriminant ] illustrates the gain of combining the generative model ( the mixture of gaussian ) with the discriminant learning algorithms : even though the mixture can be used directly as a regressor ( as argued in the introduction ) , its prediction accuracy can be greatly improved by a supervised learning step .",
    "the dataset that motivated this research is a proprietary database from a major financial institution .",
    "the task is learn to predict a company s sales from a number of variables .",
    "after preprocessing of the data , we obtain a dataset that contains 35510 records in 132 dimensions , with a proportion of about 30% missing values . for the purpose of algorithm comparisons ,",
    "this dataset is randomly split in a training , validation and test sets , of respectively 10000 , 10000 and 15510 samples .    .",
    "test mean squared error on financial institution dataset . [ cols=\"<,<,<\",options=\"header \" , ]     here we compare two discriminant learning algorithms ( neural networks and kernel ridge regression ) combined with two missing values imputation methods ( mean imputation , and the imputation mechanism proposed in this paper ) .",
    "hyper - parameters for all algorithms are chosen on the validation set , and the resulting test error is reported in table  [ tab : finance ] .",
    "although the gain between the empirical global mean imputation mechanism and the conditional mean obtained from the mixture of gaussian is small , we have verified that this improvement is statistically significant .",
    "in this paper , we considered the problem of training gaussian mixtures in the context of large high - dimensional datasets with a significant fraction of the data matrix missing . in such situations , the application of em to the imputation of missing values results in expensive matrix computations .",
    "we have proposed a more efficient algorithm that uses matrix updates over a minimum spanning tree of missing patterns to speed - up these matrix computations , by an order of magnitude .",
    "we also explored the application of a hybrid scheme where a mixture of gaussians generative model , trained with em , is used to impute the missing values with their conditional means .",
    "these imputed datasets were then used in a discriminant learning model ( neural networks and kernel ridge regression ) where they were shown to provide significant improvement over more basic missing value imputation methods .",
    "z.  ghahramani and m.  i. jordan , `` supervised learning from incomplete data via an em approach , '' in _ advances in neural information processing systems 6 ( nips93 ) _ , d.  cowan , g.  tesauro , and j.  alspector , eds.1em plus 0.5em minus 0.4emsan mateo , ca : morgan kaufmann , 1994 .",
    "l.  bahl , p.  brown , p.  desouza , and r.  mercer , `` maximum mutual information estimation of hidden markov parameters for speech recognition , '' in _ international conference on acoustics , speech and signal processing ( icassp ) _ , tokyo , japan , 1986 , pp .",
    "4952 .",
    "r.  caruana , `` a non - parametric em - style algorithm for imputing missing values , '' in _ proceedings of the eigth international workshop on artificial intelligence and statistics ( aistats01)_.1em plus 0.5em minus 0.4emsociety for artificial intelligence and statistics , 2001 ."
  ],
  "abstract_text": [
    "<S> in data - mining applications , we are frequently faced with a large fraction of missing entries in the data matrix , which is problematic for most discriminant machine learning algorithms . </S>",
    "<S> a solution that we explore in this paper is the use of a generative model ( a mixture of gaussians ) to compute the conditional expectation of the missing variables given the observed variables . since training a gaussian mixture with many different patterns of missing values </S>",
    "<S> can be computationally very expensive , we introduce a spanning - tree based algorithm that significantly speeds up training in these conditions . </S>",
    "<S> we also observe that good results can be obtained by using the generative model to fill - in the missing values for a separate discriminant learning algorithm .    </S>",
    "<S> delalleau : efficient em training of gaussian mixtures with missing data    gaussian mixtures , missing data , em algorithm , imputation . </S>"
  ]
}