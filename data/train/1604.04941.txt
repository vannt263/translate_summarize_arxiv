{
  "article_text": [
    "as radio telescopes get larger , there is a need to provide digital signal processing electronics that are smaller and less power - hungry than would be implied by the extrapolation of existing designs .",
    "this is especially true of correlation , which grows as @xmath9 for @xmath0 antennas and processed bandwidth @xmath10 .",
    "the alma telescope in chile @xcite , with @xmath5 and @xmath11ghz , currently has the largest correlator by this measure , but much larger ones are planned .",
    "table 1 gives some properties of the correlators of existing and planned telescopes , including @xmath12 .",
    "this paper presents the design of an application - specific integrated circuit ( asic or ic or chip ) that enables construction of power - efficient correlators at large @xmath0 .",
    "the design provides considerable flexibility in that the ic can be used to construct correlators for a wide range of telescope sizes , from @xmath4 to essentially unlimited .",
    "[ table : comparison ]    @llccrrrrrrc@ telescope & status & technology & year * & @xmath13 * * & @xmath0 & @xmath10 , mhz & @xmath14 , w & @xmath15 , hz & @xmath16 , pj & notes + vla & obsolete & asic & 1975 & 1.5 & 27 & 200 & 50,000 & 2.92e+11 & 171,000 & a + jvla & existing & asic 130 nm & 2005 & 4 & 32 & 8,000 & 70,000 & 1.64e+13 & 4,270 & b + alma & existing & asic 250 nm & 2002 & 3 & 64 & 8,000 & 65,000 & 6.55e+13 & 992 & c + leda & existing & gpu 28 nm & 2011 & 4 & 256 & 57.55 & 7,370 & 7.47e+12 & 977 & d + chime & existing & gpu 28 nm & 2013 & 4 & 128 & 400 & 10,080 & 1.31e+13 & 769 & e + ska1-low & proposed & fpga 16 nm & 2017 & 8 & 512 & 300 & 11,600 & 1.57e+14 & 74 & f + ska1-low & & asic 32 nm & 2015 & 4 & 512 & 300 & 752 & 1.57e+14 & 4.8 & g + ska1-mid & proposed & fpga 16 nm & 2017 & 4 & 197 & 5,000 & 40,000 & 3.88e+14 & 103 & h + ska1-mid & & asic 32 nm & 2015 & 4 & 197 & 5,000 & 4,928 & 3.88e+14 & 12.7 & k + ska2 * * * & planned & ( tbd ) & 2021 & tbd & 2000 & 5,000 & tbd & 4.00e+16 & ( tbd ) & m +    all power estimates are for correlation only , at system level , including power supply loss but not including cooling .    design freeze year .    * input number quantization , real or imaginary part .",
    "see section [ sec : parameters ] .    *",
    "* parameters are not yet well defined , so the values here are speculative .",
    "https://public.nrao.edu/gallery/radio-telescopes/image?id=300 .",
    "@xcite and other sources ; see also @xcite .",
    "130 kw system @xcite , approximately half for correlation .",
    "@xcite .",
    "water cooled .",
    "data are for the chime pathfinder ; the full chime telescope will be larger .",
    "preliminary power estimate , probably within a factor of two of the final value .",
    "this work .",
    "not equivalent to the currently proposed implementation due to smaller @xmath17 .",
    "correlator ics : @xmath181.95w = 284w .",
    "support chips , power , control ( est . ) : 468w .",
    "preliminary power estimate , probably within a factor of two of the final value .",
    "this work .",
    "power is for @xmath6 .",
    "correlator ics : @xmath19w = 1292w .",
    "support chips , power , control ( est . ) : 3636w .",
    "https://www.skatelescope.org / projecttimeline/.    figure [ fig : generic ] is a simplified depiction of the signal processing required for a generic multi - antenna telescope .",
    "-0.15 in    there are usually @xmath20 input signals because each antenna collects radiation in two orthogonal polarizations .",
    "each signal is converted to baseband and digitized , forming a time series of complex numbers at rate @xmath10 .",
    "the processing is separated into operations performed on each signal separately and operations used to combine the signals . here",
    "we consider correlation as the combining method .",
    "the rate of operations on individual signals is proportional to @xmath21 , so correlation dominates at sufficiently large @xmath0 .",
    "this paper concerns only correlation .    a correlator performs a complex multiply - accumulate ( cmac ) operation on each sample of each pair of signals .",
    "the total rate of cmac operations is then @xmath22 ( including correlation of each signal with itself ) .",
    "a good figure - of - merit ( fom ) for power efficiency is then @xmath23 , where @xmath14 is the total power consumed .",
    "table 1 shows this fom for each telescope ( where only the power of the correlator proper , as shown in fig .  1",
    ", has been included ) . at the ic level ( including all on - chip overhead and i / o )",
    ", our design achieves 1.78pj at @xmath8 .",
    "table 1 includes estimates of the system - level fom ( including board - level i / o , power supplies , etc . )",
    "that would be achieved if correlators for the proposed ska telescopes were based on this ic .",
    "the design is most efficient when @xmath0 is a multiple of 64 , so it is particularly inefficient for ska1-mid at @xmath24 .",
    "nevertheless , it would provide a substantial improvement over fpga - based designs .",
    "figure [ fig : cmac1 ] is a simplified block diagram of an individual cmac unit .",
    "-0.15 in    it consists of a multiplier , an adder , and an accumulation register .",
    "the product of each pair of samples is added into the accumulator for a fixed number of sample clocks @xmath25 , then the accumulator content is copied to the output and the accumulator is cleared to begin another accumulation cycle or `` integration . ''",
    "although each cmac is straightforward , there are various architectures for organizing many cmacs to produce a large correlator .",
    "the total cmac rate is the same for all architectures , but the rates of other necessary operations ( including memory writing and reading for temporary buffering of data and chip - level i / o ) depend on the architecture .",
    "a detailed study @xcite has shown that the chip - level architecture of figure [ fig : arch ]     signals using an array of @xmath26 cmacs , but it does so for only a fraction of the bandwidth .",
    "the number of cross - correlation products per chip is @xmath27 , which is typically @xmath28 , so the input data are buffered and the cmacs are re - used as many times as necessary .",
    "a filter bank ( not part of the correlator chip ) breaks each input signal into frequency channels narrow enough so that one chip can process all signals.,height=0 ]    -0.15 in    leads to the lowest system - level power consumption .",
    "it achieves this by minimizing the rates of the auxiliary operations .",
    "the architecture includes @xmath26 cmacs arranged as an @xmath29 array and a memory that holds a block of @xmath25 input samples from all @xmath20 signals . when the total number of complex correlations to be computed ( @xmath27 ) is larger than the number of cmacs ( @xmath26 ) , the cmacs are re - used as many times as necessary by re - reading data from the buffer .",
    "we call each computation of @xmath26 correlations one `` sub - integration '' ( si ) , and @xmath30 sis are needed to compute all the correlations of one full integration , where @xmath31 is called the cmac re - use factor .    in practice",
    "this architecture requires that the buffer memory and cmac array be on the same chip because of the high data rate between them .",
    "the memory reading rate is @xmath32 times higher than its writing rate , and this can easily lead to an impractical transfer rate if the cmac array were on a separate chip .",
    "( in our design , the transfer rate is up 352 gb / s . )",
    "even if the transfer rate is practical , several orders of magnitude more energy is required to move a bit between chips than within a chip .",
    "modern telescopes are too large for all correlations to be done in one ic , so the architecture must provide a way to partition the processing among many of them .",
    "if each device has @xmath26 cmacs running at clock rate @xmath33 , then @xmath34 devices are needed .",
    "one way to partition them is to have each device handle a subset of the signals , but then a copy of every signal sample must be delivered to at least @xmath35 devices , leading to high total i / o rate and increased power consumption .",
    "the favored architecture avoids this by having each device process all @xmath20 signals ( even if @xmath0 is large ) but only a subset of the samples of each signal .",
    "the samples could be decimated in time , but this would require another step in which the partial accumulations for the same signal pair are added across devices . instead , the samples are decimated in frequency .",
    "each signal is first processed by a uniform filter bank to separate it into at least @xmath36 independent channels of bandwidth @xmath37 ( fig .",
    "[ fig : arch ] ) .",
    "each device then processes part of the bandwidth .",
    "because of the cmac re - use , each device can handle input sample bandwidth @xmath38 .",
    "the filter bank could provide more than @xmath36 channels ( but preferably an integer multiple of @xmath36 ) , in which case each device can process several channels sequentially so that it still processes bandwidth @xmath37 . for further discussion of the reasons that this architecture is preferred and for descriptions of alternatives , see @xcite .    in this paper we describe a particular ic implementation that follows this architecture and takes additional steps to minimize power consumption .",
    "most existing synthesis telescopes whose correlators were considered large at the time of their design have used asics ( including the original vla , the expanded jvla , the vlba , and alma ) , but in those cases the asic was tailored to the particular requirements of one telescope .",
    "our design strives to be useful for a wide variety of future telescopes by supporting almost any number of antennas and any total bandwidth .",
    "the design targets a 32 nm cmos silicon - on - insulator process from ibm , 32soi @xcite , where it will have a silicon area less than 12 mm@xmath2 .",
    "the logic is specified in code written in verilog , so in principle it could be ported to other processes , but our use of certain hard macros from ibm is not portable . in particular , a high - density dynamic ram is a critical component of the design .",
    "-0.15 in    -0.35 in    figure [ fig : performance ] summarizes the ic s performance over a wide range of @xmath0 .",
    "these results are discussed in more detail in section [ sec : pvsn ] .",
    "results are computed for @xmath4 and each multiple of 64 up to 4096 .",
    "except for @xmath4 , which is a special case , the best performance occurs at @xmath39 , where the device is utilizing its maximum input and output data bandwidths , but good performance is maintained over the full range . in particular , the energy fom achieves very nearly its minimum value of 1.76 pj at many values of @xmath0 out to arbitrarily large values .",
    "there are some unfavorable values of @xmath0 where the device can not be used as efficiently , such as when @xmath40 is a prime number , but even then the performance remains good . for an ideal device ( where the processing rate remains constant and there is negligible overhead )",
    ", the bandwidth decreases as @xmath41 .",
    "[ fig : performance ] shows that the upper envelope of bandwidth vs.  @xmath0 approaches this limit .",
    "the remainder of this paper consists of an overview of the design ( section 2 ) , design details ( 3 ) , synthesis and simulation results ( 4 ) , performance as a function of @xmath0 ( 5 ) , system integration ( 6 ) , and conclusions ( 7 ) .",
    "to create a specific design within the chosen architecture , it is necessary to select the number of cmacs @xmath26 , the memory size , and the maximum input and output rates .",
    "in general , more of everything is better , but there are practical limits to chip size and chip power dissipation , and compromises are needed to control cost . if the ic were being designed for a fixed number of antennas @xmath0 , more optimization would possible . for example , figure [ fig : energy1024 ] shows a model of power consumption per unit bandwidth vs. memory size @xmath42 and number of cmacs @xmath26 when @xmath43 .",
    "-0.22 in    although power decreases monotonically for increasing @xmath42 and @xmath44 , the improvement is slow beyond @xmath45 mb ( at 8b per signal sample ) and @xmath46 . for larger @xmath0 ,",
    "it is better to have more memory , even at the cost of fewer cmacs .",
    "however , it was the objective of our design to support a wide range of @xmath0 .",
    "that is , we wanted the ic to be usable in multiple future telescopes whose sizes are not known in advance .",
    "we chose @xmath47 and @xmath48 mib , largely because this leads to a chip size that can be fabricated in an available process at an acceptable cost and yield .",
    "it is shown in section [ sec : pvsn ] that the resulting design is useful from @xmath4 to @xmath49 .",
    "it is also necessary to choose the digital representations of the input and output data .",
    "each is a complex number , and we represent them as two twos - complement numbers with half of the bits used for the real and imaginary parts .",
    "we choose @xmath50b for the input sample ( 4b real and 4b imaginary ) and @xmath51b for the output result ( 16b and 16b ) .    for the inputs , large words are expensive since the size and power consumption of the cmac multipliers increases as @xmath52 .",
    "conversely , large words provide smaller quantization noise and larger dynamic range . with 4b numbers , quantization adds 1.2% to the noise in measurements of cross - power between gaussian - distributed random processes at the optimum signal levels ( * ? ? ?",
    "* table 8.2 , page 276 ) , and most correlator designs have recognized the diminishing returns of finer quantization . ]",
    "signal levels can be maintained near optimum by level controls in the filter banks or other up - stream circuitry .",
    "dynamic range is usually not a consideration in radio astronomy , since the signals are gaussian - distributed processes with known variance .",
    "an exception is when a signal includes unexpected interference .",
    "nearly all interference is human - generated , and those signals almost always have narrow bandwidth ( @xmath53% ) compared with astronomical observations ( @xmath54% to near 50% in modern telescopes ) .",
    "this can be exploited in fx processing ( fig .",
    "[ fig : arch ] ) where each signal is partitioned by frequency ; those channels that are contaminated with interference can be ignored .",
    "this may require maintaining high dynamic range ( wide data words ) in the per - signal processing , including the filter banks , but not in the correlator .    for the output words , size must be sufficient to avoid overflow during accumulation . in our design ,",
    "the internal accumulators are 20b+20b and 21b wide for cross- and self - correlations , respectively .",
    "the maximum number of accumulations per integration is such that this is sufficient , even if the signal variances are more than 9  db above optimum .",
    "the accumulators are rounded to 16b before delivery to the outputs .",
    "analysis shows that the rounding error is far smaller than the uncertainty caused by the intrinsic noisiness of the signals @xcite .",
    "( see additional discussion in section [ sec : cmac ] . )",
    "figure [ fig : block1 ] is a high - level block diagram of the ic .",
    "-0.15 in    assume that the memory and cmac array operate on the same clock .",
    "on each clock cycle , one sample from each of @xmath44 row signals and @xmath44 column signals is read from the memory and presented to the cmac array , which computes all @xmath26 cross - products and adds them to their respective accumulators .",
    "after @xmath25 cycles , the accumulators are read out , completing one sub - integration .",
    "data are re - read from the memory for subsequent sis until all correlations of the @xmath20 signals have been computed , completing one integration . meanwhile , new data for the next full integration are being written to the memory from the input . because data in the memory are being re - used over @xmath31 sis , the memory s write rate is slower than its read rate .",
    "the data output rate depends on the integration length @xmath25 , which is limited by the size of the memory .",
    "figure [ fig : block2 ] is a more detailed block diagram , showing individual modules of the implementation and all input / output signals .",
    "-0.10 in        -0.15 in    input data are received at datain , which is a 32b bus synchronized to the rising edge of clkin . on each clock , four 8b samples of different signals at the same sampling time are received .",
    "integrate , also synchronized to clkin , marks the start of a new integration .",
    "the input module collects 32 of these 32b input words in a buffer , producing a 1024b word that is delivered to the memory module for writing to the ram .",
    "successive input words supply samples from other signals and other sampling times in a particular order until all @xmath55 samples of one integration have been received and written to the memory .",
    "the address generator module determines the sequencing of write and read cycles and provides the appropriate addresses for each .",
    "it also provides synchronization signals that organize the chip s computations into sub - integrations .",
    "the address generator is logically the most complex part of the design , although it uses very little silicon area ; it is further described in the next section . on memory read cycles , the ram delivers 1024b words to a small buffer which organizes the data into @xmath56-bit words corresponding to the rows and columns of the cmac array ( for details see section [ sec : addrseq ] ) .",
    "the cmac array module performs the correlations as previously described . at the end of an si ,",
    "4096 complex results are available .",
    "these are delivered sequentially by the output module to 16 parallel output pins synchronous with 8192 cycles of clkout .",
    "syncout is asserted during the first clkout cycle of each si .",
    "most of the modules are driven by an internal clock called , synthesized in the clock generator module from clkin using a phase - locked loop ( pll ) .",
    "each cycle of  is either a memory read cycle , memory write cycle , or an idle cycle , as determined by the address generator module .",
    "the frequency of  must be sufficient to keep up with the input data . during read cycles",
    "only , the cmac array computes the appropriate correlations ; otherwise it is idle .",
    "the clock generator can also generate _",
    "outclk _ for use by the output module and as output signal clkout .",
    "alternatively , the output clock can be supplied externally at input usroutclk , in which case that signal is passed through clock generator to _",
    "the control module contains a set of twelve 20b registers that can be written and read by the user over a 4-wire serial peripheral interface ( spi ) bus @xcite .",
    "these registers contain data for programming the pll to set the frequencies of  and , as well as data needed by the address generator to determine the address sequence , including the values of @xmath0 and @xmath25 .      the ability to control the clock frequencies and the address generation parameters provides considerable flexibility , making the chip useful for a wide range of @xmath0 .",
    "the integration length @xmath25 is also determined by a user - controlled register . the frequency - decimation ( fx ) architecture of fig .",
    "[ fig : arch ] allows building a correlator of almost any total bandwidth .    re - use of the cmacs follows a particular pattern , illustrated in figure [ fig : reuse ] for the case of @xmath7 ( 256 signals ) .    . for our design , @xmath47 so @xmath7 in this example.,height=2 ]    -0.2 in    in this figure",
    ", each square represents the same @xmath57 cmacs performing one si .",
    "the sis form a half - matrix with the @xmath20 signals delivered to each row and column in groups of @xmath44 .",
    "there are @xmath58 groups . for the sis along the diagonal , only half of the cmacs",
    "are needed to correlate the signals of a group with each other .",
    "therefore , the other half of the cmacs is used simultaneously for the correlation of another group . during these sis ,",
    "the cmac array operates in  split mode , \" which is further described in section [ sec : cmac ] . in this way",
    ", only @xmath59 sis are needed , not @xmath60 .",
    "it is desirable for @xmath61 to be an integer , and we will see later that it is best if @xmath61 is even . to use the chip with a number of signals @xmath20 that produces non - integer @xmath61 ,",
    "it is necessary to supply dummy signals so that the total number is a multiple of @xmath44 .",
    "although the chip s throughput is only as large as it would be if the dummy signals contained valid values , its power consumption can be smaller .",
    "if the dummy signals are always set to zero , no switching activity occurs within the cmac array when they are being processed so no dynamic power is used",
    ". we will show ( sections [ sec : results ] and [ sec : pvsn ] ) that the majority of the power is used by the cmac array .",
    "the @xmath59 sis can be computed in any order without changing the total computation effort , but the design uses a particular order so as to make efficient use of the memory .",
    "this is illustrated in figure [ fig : subint ] , again for the case of @xmath7 , @xmath62 .",
    ".,height=2 ]    -0.15 in    each of the four groups of signals is placed in a block of contiguous memory addresses , occupying @xmath63 words of 1024 bits .",
    "the sis are computed column - wise , beginning with the diagonal si at the upper left of the diagram and proceeding downward .",
    "the illustration shows computation of the third si , involving signal groups 1 - 64 and 129 - 192 .",
    "note that signal group 1 - 64 is needed for all four sis of the first column , but when that column is complete that group is never needed again .",
    "therefore , as soon as the first column is complete the section of memory used for that group can be overwritten with data for the next full integration . similarly , when the second column is complete , the second memory block can be overwritten with new data .",
    "furthermore , data from the third and fourth groups is not needed for the current integration until sis 3 and 4 , so it can be written during sis 1 - 4 , as illustrated in figure [ fig : addrplot](a ) , which plots the memory addresses for reading column data , for reading row data , and for writing new data as a function of clock cycle number over two complete integrations .",
    "-0.15 in    we see that complete overlap of reading and writing is achieved , with no need for double - buffering in order to have continuous operation .",
    "unfortunately this result can not be extrapolated beyond @xmath62 ; to process more signals continuously , some additional memory is needed to buffer new data so that it does not overwrite data that are not yet fully processed .",
    "for example , figure [ fig : addrplot](b ) shows the situation for @xmath64 , where 9.4% additional memory is needed .",
    "the additional memory requirement increases with @xmath0 but never exceeds 33% , so full double - buffering ( 100% ) is not necessary .",
    "the overlapping of reading and writing also means that the latency in delivering the results of an integration is less than the integration length ; for example , at @xmath64 [ fig .",
    "[ fig : addrplot](b ) ] an integration is complete and all results are delivered to the ouptut 57 sis ( rather than 64 sis ) after the last data for that integration are delivered to the input . in general , the latency is @xmath65 sis .",
    "this arrangement allows each block of memory to use a sequential set of addresses . to compute one si",
    ", we need only know the starting addresses of the row data ( for reading ) , the column data ( for reading ) , and the new data ( for writing ) , each of which is simply incremented throughout the si .",
    "the starting addresses are given in control registers and are incremented automatically in the address generator logic and delivered to the memory in the proper sequence .",
    "the user is required to update the three addresses via the spi bus prior to the start of the next si .",
    "this is easily done with spi clock rates that never exceed 15 mhz , although the chip is designed to support spi rates of at least 25 mhz .",
    "performance is often limited by input or output bandwidth , as shown in section [ sec : pvsn ] .",
    "results given in this paper are based on achieving input and output clock frequencies of 500 mhz , or 16 gb / s over 32 input pins and 8 gb / s over 16 output pins .",
    "spice simulations @xcite show that this is easily achievable to or from a typical modern fpga @xcite via a wire - bond or flip - chip package and a 76-mm - long printed circuit board trace",
    ". considerably higher rates may be achievable if differential signaling is used for clkin and clkout . at i",
    "/ o speeds of 500 mb / s per pin , the best performance is achieved at @xmath39 , where the bandwidth per chip is 1.5625 mhz and the total chip power is 2.26 w , giving a energy figure - of - merit of 1.76 pj per cmac operation .",
    "the selection of dynamic ram ( dram ) for the on - chip memory was an important design choice .",
    "the desired 64 mib could lead to a large and expensive chip unless a high - density memory is used .",
    "the dram available for the ibm 32soi process achieves 2.8 times higher density than static ram ( sram ) in the same process .",
    "even so , it occupies 65.5% of the cell area in our design .",
    "we considered other processes at about the same technology node and found that the available srams had similar or lower density .",
    "few offered dram .",
    "therefore , use of sram was found to be unfeasible for the selected memory size .",
    "power was also a consideration ; the selected dram uses only 14.3% to 19.4% of the chip s power , depending on @xmath0 .",
    "use of a smaller memory was also considered , but it was found that this would require proportionally lower cmac clock speed and bandwidth in order to maintain feasible output data rate . indeed , performance could be improved at large @xmath0 with more memory , so the selected size is already a compromise .",
    "we restricted ourselves to processes for which multi - project wafer runs are available .",
    "it is important to fabricate and test prototype devices before proceeding to production , and it is our judgment that funding for a dedicated prototype run would not be obtainable nor would it be justified .",
    "use of dram implies several design difficulties , including constraints on the sequence of read and write addresses and the need for periodic refreshing .",
    "the dram has a concurrent refresh feature that allows refreshing to be overlapped with reading and writing , so in principle it is possible to avoid any refresh - only cycles .",
    "however , the retention time is finite and this leads to a minimum clock speed of 200 mhz .",
    "in addition , the dram version that we chose has a constraint that the same bank may not be addressed on successive clock cycles .",
    "each bank carries 2048 addresses , so our 65,536-address memory contains 32 banks . on each clock cycle , it is necessary to supply the read or write address along with the refresh bank address ( cra in fig .  [ fig : block2 ] ) ; this is done by the address generator .",
    "every bank must get 256 refresh cycles within the retention time .",
    "we have devised an algorithm that meets all constraints and guarantees refreshing all banks provided that the clock rate is at least 227 mhz .",
    "if the total of the read rate and the write rate is at least this high , then no refresh - only cycles are needed .",
    "otherwise , @xmath66 is set to 227 mhz and the excess cycles are refresh - only . during refresh - only cycles ,",
    "the memory uses much less dynamic energy than during read or write cycles , and the cmacs use no dynamic energy because they are not clocked .",
    "our implementation of an individual cmac is shown in figure [ fig : cmac2 ] .    , output @xmath67 , and internal signals on the data path are complex numbers whose real and imaginary parts are each represented in twos complement with total size @xmath68 bits , as indicated . on each clock",
    ", the product @xmath69 is added to the accumulator .",
    "sync _ is asserted on the first clock of an integration.,height=1 ]    -0.15 in    the inputs @xmath31 and @xmath70 are each 4b+4b complex numbers ( @xmath71 in fig .",
    "[ fig : cmac2 ] ) in twos - complement form , with new values delivered at each clock . to avoid any d.c .",
    "offset in the input data while retaining twos - complement representations , we assume that the range of the real and imaginary parts is [ 7,+7 ] ; that is , 8 is excluded .",
    "the complex multiplier is implemented with four real multipliers and two real adders , producing complex products that are no more than 8b+8b in twos complement ( @xmath72 ) .",
    "each product is added into a 20b+20b accumulator ( @xmath73 ) for @xmath25 clock cycles . on the next clock after that , signal _ sync _ is asserted ; this causes the real and imaginary accumulator contents to be rounded to 16 bits each ( @xmath51 ) and loaded into a holding register until they can be delivered to the output , and at the same time the product of the next pair of input samples is loaded directly to the accumulator , thus starting a new accumulation .",
    "the accumulator size is chosen to avoid overflow in cross - correlation of gaussian - distributed signals .",
    "when correlating such signals , there is an optimum signal strength that minimizes quantization noise in the results ; for 4b numbers , that optimum is a standard deviation of @xmath74 on both the real and imaginary parts ( * ? ? ?",
    "* table 8.2 , page 276 ) .",
    "it has been shown @xcite that even when the standard deviations of both signals are @xmath75 ( 9.5 db above optimum ) overflow does not occur at 20b until @xmath76 , even when the correlation coefficient is unity at the worst - case correlation phase . at @xmath77",
    ", overflow does not occur provided that the correlation coefficient is less than 0.27 , also at @xmath75 and worst - case phase . for our design",
    ", the memory size ensures that @xmath78 for @xmath79 .",
    "high cross - correlation coefficients are rarely seen in radio astronomy ( where @xmath80 is routine ) , but in situations where they can occur the signal level can be reduced upstream of the correlator . if overflow does occur , the design ensures that it is detected .",
    "a check for overflow is done on every clock , and if it ever occurs the final result is set to the maximum positive or negative value , as appropriate .",
    "the rounding logic reduces the results from 20b+20b to 16b+16b by rounding off the 4 least - significant bits .",
    "the mid - point is always rounded away from zero so that no bias is introduced .",
    "rounding is acceptable to the extent that it does not cause loss of significance when the correlation coefficient is small .",
    "analysis shows @xcite that the quantization error due to rounding will be smaller than the intrinsic noise in the result provided that @xmath81 when the signal strengths are 9.5 db below optimum , even if the true correlation coefficient is zero . for our design",
    ", we are limited to @xmath82 at all @xmath0 , otherwise the output data rate would exceed the available output bandwidth .",
    "this means that the rounding never introduces significant quantization noise .",
    "when the cmac array is computing sub - integrations on the diagonal of the si half - matrix ( fig .",
    "[ fig : reuse ] ) , it must operate differently .",
    "the @xmath29 array then contains three versions of the cmac cell : cmacs below the diagonal are computing cross - correlations among the @xmath44 signals of one group ( the  row \" signals ) ; those above the diagonal are computing cross - correlations of another group (  column \" signals ) ; and those on the diagonal are computing two self - correlations at once , for one signal from each group .",
    "the latter is possible because self - correlations accumulate the squared magnitude of the signal , and this can be done using only half the resources of a cmac ( two of the four real multipliers in the complex multiplier , and only the real part of the complex accumulator ) .",
    "the other half of this cmac is then used simultaneously to compute the self - correlation of a signal form the other group , accumulating its real result in the `` imaginary '' part of the accumulator .",
    "this amounts to a re - organizing of the cmac array when a diagonal si is being computed .",
    "we call this the `` split mode '' of the cmac array , and it is controlled by a signal from the address generator called `` cmac mode '' in fig .",
    "[ fig : block2 ] .",
    "self - correlation is equivalent to cross - correlation with a correlation coefficient of unity , so overflow can occur more easily . on the other hand ,",
    "the result is always positive so the accumulator can be considered unsigned ; this allows @xmath25 to be twice as large before overflow occurs . nevertheless , to allow @xmath77 without overflow when the signal strength is 9.5 db above optimum requires 21b",
    "therefore , our implementation provides 21b+21b accumulators for the 64 cmacs on the diagonal , since they are the only ones that do self - correlations ( and 20b+20b accumulators for the other 4032 cmacs ) . in split mode only",
    ", those cmacs round off 5 least - significant bits rather than 4 so that the result is still @xmath83 bits .      to compute all the correlations of one integration , a total of @xmath84 cmac operations",
    "is required , and since @xmath26 of them are done at once , this requires a total of @xmath85 cmac clocks . at the same time , @xmath55 input data samples for the next integration must be received and written to the memory .",
    "each memory word carries @xmath86 samples , so @xmath87 memory write cycles are needed .",
    "it follows that a memory write cycle is needed for every @xmath58 cmac cycles .",
    "since we also need one memory read cycle for each cmac cycle , we need @xmath88 memory cycles ( read+write ) for each @xmath61 cmac cycles .",
    "since 32b@xmath89 samples are received on each input clock , @xmath90 input clocks are needed for each memory write . from this",
    "we find that @xmath91 where @xmath92 is the input clock ( inclk ) rate and @xmath66 is the memory clock ( ) rate . if  is faster than this minimum , the address generator automatically causes the extra cycles to be refresh - only cycles for the memory and idle cycles for the cmacs .",
    "the cmac clock is a gated version of  so that it is active only during memory read cycles .",
    "otherwise all cmacs are idle and consume no dynamic power .",
    "after each si , 8192 cycles of the output clock are needed to read out the 16b+16b complex results from the 4096 cmacs over the 16b dataout bus .",
    "this must be done before the next si is complete , which means that @xmath93 where @xmath94 is the  rate .",
    "both  and  can be synthesized internally from inclk using a pll in the clock generator , but this is subject to some constraints .",
    "there is only one pll ; it contains an internal delay - controlled oscillator in the range 36 ghz locked to @xmath95 , where @xmath67 and @xmath42 are integers determined by user - settable registers .",
    "the oscillator drives two separately - programmable frequency dividers , one of which generates .",
    "then  is either generated by the other divider or supplied by the user at input pin usroutclk .",
    "the two divider ratios and the selection of the  source are configurable via the spi interface .",
    "the memory word size of 1024b or 128 samples was selected so that there is one memory read cycle for each cmac cycle .",
    "however , one cmac cycle requires 64 row samples and 64 column samples , so it would have been more straightforward to have 512b ( 64 sample ) memory words and perform two reads per cmac cycle , one for the row data and one for the column data , which are always at different locations in memory ( fig .",
    "[ fig : subint ] ) . then the maximum cmac rate would be limited to half the maximum memory rate , which would limit performance because the cmacs can operate much faster . to avoid this limitation",
    ", we use wider memory words , but then some data reordering is needed between the memory and the cmac array .",
    "that is the purpose of the buffer module ( fig .",
    "[ fig : block2 ] ) .",
    "each memory word contains simultaneous time samples from @xmath47 different signals along with the next time samples from the same 64 signals .",
    "each subsequent word contains two more time samples , so that @xmath63 words contain all @xmath25 samples for those signals .",
    "similar blocks of @xmath63 words hold @xmath25 samples for other groups of @xmath44 signals , with @xmath58 groups altogether ( fig .",
    "[ fig : subint ] ) .",
    "reading from memory to buffer is done in sets of 4 successive clock cycles , obtaining time samples @xmath96 through @xmath97 for the row signals and the column signals :    _ memory to buffer _",
    "row data containing samples for @xmath96 and @xmath98    row data for @xmath99 and @xmath97    column data for @xmath96 and @xmath98    column data for @xmath99 and @xmath97 .    on the next 4 clock cycles ,",
    "the same data are transferred from the buffer to the cmac array for processing , but in a different order :    _ buffer to cmac array _",
    "row @xmath96 and column @xmath96    row @xmath98 and column @xmath98    row @xmath99 and column @xmath99    row @xmath97 and column @xmath97 .",
    "the buffer size is 8 memory words ( double buffering ) so that memory reading and cmac processing can be overlapped with a latency of 4 clock cycles .",
    "processing uses sets of 4 words rather than 2 in order to ensure that no memory bank is addressed on two successive cycles .",
    "the 16b memory address is structured as-10pt    address[15:0 ] = \\{withinbank[10:0 ] , banknumber[4:0]}.    by always reading two adjacent addresses on successive cycles , we are assured that those cycles access different banks .    as explained earlier , a write cycle for new data must occur for every @xmath61 read cycles . the shortest possible complete sequence",
    "then includes @xmath100 sets of four read cycles ( @xmath101 cycles ) and two write cycles , as illustrated in figure [ fig : addrseq ] .",
    "-0.25 in    the two write cycles use adjacent addresses , so the bank access constraint continues to be satisfied .",
    "the sequence then includes @xmath102 cycles , and @xmath103 such sequences are needed to complete a sub - integration .",
    "this structure introduces two constraints on user parameters .",
    "first , @xmath61 must be even so that the sequence can include @xmath100 sets of read cycles .",
    "this means that @xmath0 must be a multiple of @xmath47 .",
    "second , @xmath25 must be a multiple of @xmath101 because each sequence contains @xmath104 time samples .",
    "the constraint that @xmath0 must be a multiple of 64 is mitigated for small numbers of antennas by a special `` memory bypass mode '' that supports @xmath4 ( 64 signals ) by bypassing the memory entirely and sending input data directly to the cmac array .",
    "two sets of 64 signals are processed in parallel , e.g. from different frequency channels .",
    "bypass mode is selected by a bit in a user - controlled register .",
    "thus , the chip can efficiently support @xmath4 , 64 , 128 , 192 , @xmath105 . for other values of @xmath0 , dummy signals ( preferably always zero )",
    "must be added be even and thus allow @xmath0 to be any multiple of 32 by lengthening the processing sequence . if memory writing were done in sets of 4 addresses rather than 2 , then each sequence would include @xmath61 read sets and @xmath61 could be any integer .",
    "this would exacerbate the constraint on @xmath25 , which would then need to be a multiple of @xmath106 . ] .    on each cycle ,",
    "the memory is given not only an address for reading or writing , but also a concurrent refresh bank number .",
    "the latter must be different from the bank in the current , previous , and next read or write address , as well as different from the current , previous , and next refresh bank .",
    "it is tricky to meet this requirement while ensuring that all banks get refreshed sufficiently often .",
    "our algorithm for this is not necessarily optimum , but it is reasonably efficient in that it requires a clock rate only about 13% higher than an ideal one that distributes all cycles uniformly among the banks .",
    "once the appropriate memory addresses are written to the control registers , the chip processes each sub - integration automatically , delivering the results at the outdata pins over 8192 cycles of clkout .",
    "output signal outsync is asserted when the first word of each si is valid .",
    "the frequency of outclk must be high enough to deliver all 8192 words before the next si is completed .",
    "outclk can be faster , in which case more than 8192 cycles will occur before the next assertion of outsync , but only the first 8192 output words are valid .",
    "the internal processing runs on , whose frequency must be fast enough to keep up with the input data .",
    "it can be faster , in which case idle cycles are automatically inserted by the address generator module . in this way , the overall speed of operation is determined entirely by inclk .",
    "the chip does not automatically keep track of full integrations ; it operates at the si level . to establish the integration - level sequence ( figs .",
    "[ fig : subint ] , [ fig : addrplot ] ) , the user must supply the starting memory addresses of the row data , column data , and input ( write ) data for each si , along with a bit that specifies whether this si uses split mode .",
    "this is done by writing to particular registers over the spi link .",
    "values written during the current si are used during the next one .",
    "writing may be begin immediately after outsync is asserted , and must be completed before the current si is finished , as indicated by the next assertion of outsync .",
    "updating the addresses for each si could have been further automated within the chip rather than putting that burden on the user , but the chosen design ensures that the chip has great flexibility .",
    "it allows a wide range of @xmath0 to be supported , and it allows a different memory address sequence than that shown in fig .  [ fig : addrplot ] to be used if desired . a somewhat different sequence , discussed in section [ sec : largen ] ,",
    "is useful when @xmath0 is sufficiently large ; this permits the chip to support arbitrarily large @xmath0 .",
    "internal synchronization signals are needed to account for the processing latency of each block in the signal flow .",
    "these are shown as blue lines in fig .",
    "[ fig : block2 ] .",
    "the input module requires 32 clkin cycles to collect enough data to write one 1024-b word to the memory . at the beginning of an integration",
    ", it delivers a delayed version of integrate to the address generator module when the first word is ready to be written to the memory .",
    "the address generator then provides a signal marking the start of the first si as well as subsequent sis .",
    "this signal is passed through the memory module , where it is delayed by the memory s read latency ( but not otherwise used ) , and similarly through the buffer module , and then to the cmac array , where it forces each cmac to copy its accumulator to its readout register and to ready the accumulator for the next si ( fig .",
    "[ fig : cmac2 ] ) .",
    "the synchronization signal is then passed to the output module , where it is delayed by the output latency and delivered to the syncout pin at the same time as the first output word of the si is delivered to dataout .",
    "the selection of parallel , synchronous bit streams for the main data input and output is a compromise driven by cost .",
    "simulations @xcite in spice @xcite have shown that at least 500 mb / s can be transferred between each pin and a modern fpga via a wire - bond package and 76 mm - long printed circuit trace .",
    "somewhat higher rates may actually be achieved , but 500 mb / s has been used for the performance calculations of this paper .",
    "the number of pins is limited by package cost and size as well as timing skew .",
    "we selected 32 input pins and 16 output pins , providing 16 gb / s input bandwidth and 8 gb / s output bandwidth .",
    "these bandwidths provide good performance for all @xmath107 , as shown in section [ sec : pvsn ] and fig .",
    "[ fig : performance ] . to achieve higher rates would require multi - gb / s serial transmitters and receivers . such devices are technically feasible , but including them in the present design is cost prohibitive .",
    "( an _ ab initio _ design would be difficult and risky for our small team , and existing designs command high license fees . )",
    "larger input bandwidth would permit a higher processing rate at small @xmath0 , and larger output bandwidth would do so at large @xmath0 .",
    "this would reduce the chip count in a large system , but it would have little effect on the system power consumption .",
    "the design includes 39 input pins and 19 output pins ( fig .",
    "[ fig : block2 ] ) , not including power , ground , and test connections .",
    "all are implemented with the same bi - directional i / o cell from ibm .",
    "it provides a low - voltage cmos interface at a nominal voltage swing of 0.9 v , which is the same as the nominal supply voltage of the internal logic .",
    "each cell is configured internally as either input or output , since none of our pins is bi - directional . for outputs ,",
    "it is configured for maximum slew rate and 50 ohm source resistance .",
    "additional discussion of these cells is given in section [ sec : iopower ] .",
    "the chip is controlled by a set of 12 registers that can be written or read over a 4-wire spi slave interface @xcite .",
    "it is intended to be driven by a corresponding spi master interface in an external controller , such as an fpga .",
    "multiple correlator chips on the same board can be connected to the same controller ; one of the wires ( ssel ) selects the current correlator chip .",
    "each register contains 20 bits .",
    "our protocol uses 25 spi clock cycles per transfer ( 4 address bits , one write enable bit , and 20 data bits ) . on each transfer ,",
    "all bits of the addressed register are read and returned over the spi miso ( master in , slave out ) wire .",
    "data sent on the mosi ( master out , slave in ) wire are then written to the register if the write enable bit is set .",
    "a portion of one register has a set of read - only status bits for reporting error conditions .",
    "once set by the internal logic , the status bits remain set until that register is read , then they are automatically cleared .",
    "register reading and writing are driven entirely by the spi clock ( wire sclk ) , generated by the master interface , independent of any other clocks .",
    "this ensures that they can be set even if the internal pll is not running or is unstable because it was previously misprogrammed or because inclk is not yet being supplied .    at power - up or after assertion of active - low resetn , the registers are loaded with default values which allow operation to proceed .",
    "unless the defaults happen to correspond to the present application , the output data will not be valid until the registers are written with the appropriate values .",
    "these include programming the pll to generate  at a frequency high enough to keep up with the input data , and specifying the values of @xmath61 and @xmath25 , which determine the addressing sequence ( fig .",
    "[ fig : addrseq ] ) and the si length . in most applications , these settings need only be written during initialization .",
    "three registers must be written during each sub - integration .",
    "these give the starting addresses for column data reading , row data reading , and new data writing during the next si , and whether the next si uses split mode .",
    "this requires 75 cycles of sclk , and must be completed during the time of one si , which is @xmath25 cmac clocks , so there is a minimum value of @xmath25 that varies inversely with the spi clock rate and the cmac rate .",
    "the maximum cmac rate is 312.5 mhz at @xmath39 ( see section [ sec : pvsn ] ) . at an sclk clock rate of 15 mhz , this gives @xmath108 .",
    "the memory size is sufficient to support @xmath109 at @xmath39 .",
    "the cmac rate and maximum @xmath25 are such that an sclk rate @xmath110 mhz is sufficient at all @xmath0 , and the chip will support an sclk rate of at least 25 mhz .",
    "the register - transfer - level ( rtl ) design was created in verilog .",
    "three vendor - specific hard - macro cells were instantiated :    ",
    "-2    dram , @xmath111 , a black - box macro from ibm .",
    "two of these are used in parallel inside the memory module to create a @xmath112 memory .",
    "pll , also a black - box macro from ibm , used in the clock generator module .    bi - directional lvcmos input / output cell , from ibm .",
    "the design was synthesized using standard cell libraries from arm by means of synopsys design compiler , with timing constraints based on @xmath113  mhz , @xmath114  mhz , and @xmath115  mhz . except for its use of the above cells from ibm ,",
    "the rtl design is portable and could be synthesized for fabrication in a different process by using an appropriate standard - cell library .",
    "library files giving timing and power data on all standard cells and on the above special cells were available for various process - voltage - temperature corners . during synthesis , we used a `` slow '' corner at drain - source voltage 0.8v ( _ cf . _ 0.9v nominal ) and temperature 40c . the resulting synthesized netlist and the calculated delays of all cells and nets",
    "were then used by modelsim to simulate operation of the chip for a specific scenario ( @xmath7 antennas and integration length @xmath116 ) and a specific set of simulated input signals .",
    "the simulation test bench collected the chip s output data and these were later compared against independently - calculated correct results .",
    "the simulator also calculated the switching activity on each net .",
    "the switching activity was read back into design compiler and its power compiler component was used to calculate the power dissipated by each cell . for the latter calculation",
    ", we used a more practical corner ( 0.9  v and + 50  c ) in order to obtain realistic switching and leakage power estimates .",
    "three versions of the standard cell libraries were available , each containing cells of the same functionality but at different cmos threshold voltages .",
    "the compiler was told that cells from the highest - threshold library are preferred , since this minimizes leakage power , but it was allowed to select up to 10% of the cells from the lower - threshold libraries when optimizing timing . the final result used 6.17% from the low - threshold libraries .",
    "design compiler was used in its topographic mode , where it makes use of data about the process technology ( metal layer stackup ) and physical data about all the cells ( dimensions and port locations ) to perform a rough placement of the cells .",
    "this allows it to make accurate estimates of the delay and capacitive load of each wiring net .",
    "although a complete layout has not yet been done , we believe that this provides reliable power estimates .",
    "the results of this process are summarized in table [ tbl : pcresults ] .",
    "@lrrrrrrrr@ module & & area , mm@xmath2 + & switching & internal & static & total & + cmac array&129.199&179.727&36.500&345.386&2.728296 + memory&2.970&_123.994_&10.000&136.983&6.392190 + input cells ( 39)&_3.702_&_0.331_&0.286&3.257&0.200353 + output cells ( 19)&_33.391_&_6.248_&0.139&39.774&0.131266 + clock generator&0.875&0.013&54.000&54.880&0.078929 + control&0.000&0.005&0.006&0.012&0.001333 + output&12.943&3.766&5.490&22.196&0.102880 + buffer ( dram to cmacs)&2.024&7.860&0.572&10.457&0.034140 + address generator&0.021&0.116&0.016&0.153&0.000704 + input&2.051&6.650&0.286&8.988&0.012865 + misc .",
    "( top module)&3.981&0.000&0.695&4.530&0.07335 + total of all modules & 191.156&327.619&107.989&626.617&9.756312 +    clock buffer trees not included , see section [ sec : clocktrees ]    values in italics are subject to corrections described in section [ sec : corrections ] ; final values are given in table [ tbl : corrected ] .",
    "[ tbl : pcresults ]    as expected , a majority of the power is used by the cmac array ( 55% ) and the memory ( 22% ) .",
    "static power , mostly due to cell leakage currents , is low .",
    "nearly all of the power of the clock generator is static , but in that case it is not leakage but rather is due to the pll s internal microwave oscillator , which runs even without input switching activity .",
    "more than 65% of the cell area is occupied by the memory ; the design would be impractical if lower - density memory ( _ e.g. _ , static ram ) were used .",
    "we have found that several small corrections to these results are necessary .",
    "first , the clock networks were treated as ideal during synthesis , so buffer trees for them were not synthesized .",
    "therefore , we carried out a rough , manual synthesis of the clock trees for the purpose of estimating their power consumption and area .",
    "second , the simulator did not provide enough switching activity detail to determine accurately the internal power of the memory .",
    "this had only a small effect in the simulated scenario , but it is significant when the results are extrapolated to other scenarios , as we do in section [ sec : pvsn ] .",
    "third , the i / o cell power estimates from power compiler are inaccurate for several reasons , so we re - computed those manually .",
    "this produced only a small net change .",
    "details of these corrections are described sections [ sec : clocktrees ] and [ sec : corrections ] and the final results are given in section [ sec : results ] .",
    "the compiler generated a synthesized netlist and a standard delay format @xcite file containing the signal delays through all cells and nets .",
    "these were used with a verilog test bench and simulated input data to compute a detailed simulation under realistic conditions .",
    "the case simulated was @xmath7 antennas ( 256 input signals ) and integration length @xmath116 .",
    "this integration length is the minimum that does not require an output clock faster than 500 mhz .",
    "actual integration lengths can be much longer and @xmath0 can be much larger , but simulations in such cases would have impractical run times .    the simulated input data consisted of quantized gaussian noise , independent among all signals and among all samples of each signal , with standard deviation @xmath117 for the real and imaginary parts .",
    "an additional time series with independent samples and @xmath117 was added to two of the signals , creating a cross - correlation coefficient of 0.5 for that pair but zero for all other pairs .",
    "the test bench generated the inclk signal at 500 mhz and used a simulated spi master controller to load parameters into the chip s registers , just as will be done in the actual device .",
    "these parameters commanded the pll to generate the internal clock ( ) at @xmath118 mhz and the output clock at @xmath119 mhz , and they set @xmath0 and @xmath25 to the selected values .",
    "the test bench then loaded the pre - computed input samples in the appropriate sequence .    under these conditions ,",
    "the average cmac rate is 62.5 mhz , and @xmath120 mhz would be sufficient to keep up with the input data .",
    "the higher @xmath66 rate was used to ensure that the internal dram has sufficient refresh cycles .",
    "( no extra cycles for refresh are needed when @xmath121 . )    for @xmath122 , 8 sub - integrations ( sis ) are needed for each integration .",
    "the simulated input data included 24 sis covering 3 integrations , with each integration repeating the same data values .",
    "the output data are invalid until sufficient input data have been written to the internal memory .",
    "the first integration is read out as the 6th through 13th sis , the second as the 14th through 21st , and the third is only partly read out ( first 3 sis ) because the simulation ends when the input data are exhausted .",
    "the test bench collects all the output data and saves it to a file , recording 24 sis of which the last 19 are valid .",
    "the simulation ends after at 419.793@xmath123s of simulation time . from simulation time 110@xmath123s to 210@xmath123s , the simulator records the switching activity on all nets in a switching activity interchange format ( saif ) file .",
    "this covers the time when sis 6 through 11 were being computed and read out ; it avoids all initialization as well as all invalid sis and thus represents steady - state operation of the chip .",
    "there are three large clock networks that were treated as ideal during synthesis , as listed in table [ tbl : clocks ] .",
    "@cccc@ net & fanout & load , ff + inclk & 3,168 & 1,626 + sysclk & 8,352 & 4,463 + clk_cmac & 311,488 & 167,800 +    [ tbl : clocks ]    in view of the large fanouts , each will need a tree of buffer cells , but these were not synthesized by the compiler .",
    "it is best to create the final clock trees during layout , when their timing can be more precisely optimized .    to estimate the power consumption and area of the three clock trees",
    ", we performed a rough , manual synthesis of each .",
    "the resulting designs are far from optimum , so they represent an upper limit on the required power .",
    "we selected ( somewhat arbitrarily ) two standard cells , an inverting and a non - inverting buffer , from which to build the clock trees .",
    "we adopted the constraint that the delay though any path of the tree should be no more than 20% of the minimum clock period for that net .",
    "details of the calculations are given in appendix a and the results are summarized in table [ tbl : trees ] . the switching power for driving the input pins of all destination cells of each net was included in the power compiler results , so that power is subtracted from the total to determine the net power added by each buffer tree .",
    "@lrrrrrrrr@ net:&clk_cmac & sysclk & inclk & totals + max .",
    "frequency , mhz & 360 & 360 & 500 & + frequency , mhz & 62.5 & 227.273 & 500 & + total delay , ps & 494.6 & 521.2 & 332.2 & + total power , @xmath123w & 44236.23 & 4263.47 & 3674.67 & 52174.36 + load pin switching , @xmath123w & 10599.91 & 1039.17 & 884.53 & + net power added , @xmath123w & 33636.32 & 3224.30 & 2790.14 & 39650.76 + number of tree levels & 3 & 2 & 2 + total number of buffer cells & 2762 & 49 & 27 & 2368 + total cell area , @xmath124 & 3556.1 & 63.1 & 34.7 & 3653.9 +    maximum switching frequency of net over all @xmath0 , for delay constraint .",
    "effective switching frequency in the simulated scenario , for power calculation .",
    "[ tbl : trees ]        for the i / o cells , we have manually estimated the power consumption using data from the cell library files .",
    "these results are shown in table [ tbl : iopwr ] for the simulated scenario .",
    "@lcrrrrr@ pin name & count & toggle rate & + & & mhz & switching & internal & static&total +   + inclk&1&1000.00&0.72576&0.10150&0.00706&0.83432 + indata&32&47.78&0.00642&0.80480&0.22580&1.03702 + integrate&1&0.00&0.00000&0.00000&0.00706&0.00706 + spi_mosi&1&0.00&0.00000&0.00000&0.00706&0.00706 + spi_sclk&1&0.00&0.00000&0.00000&0.00706&0.00706 + spi_ssel&1&0.00&0.00000&0.00000&0.00706&0.00706 + resetn&1&0.00&0.00000&0.00000&0.00706&0.00706 + usroutclk&1&0.00&0.00000&0.00000&0.00706&0.00706 + _ output pins",
    "_ + outclk&1&1000.00&6.73313&2.28490&0.00706&9.02508 + outdata&16&250.00&26.93250&9.13960&0.11290&36.18500 + outsync&1&0.12&0.00082&0.00028&0.00706&0.00816 + spi_miso&1&0.00&0.00000&0.00000&0.00706&0.00706 + totals&58&&34.39863&12.33108&0.40925&47.13896 + input pins & 39 & & 0.73218&0.90630&0.27519&1.91367 + output pins & 19 & & 33.66645&11.42478&0.13407&45.22529 +    [ tbl : iopwr ]    the leakage power is essentially the same for all cells and is about 1% of the total . the dynamic power is dominated by the clock and data pins , since the remaining 8 pins have negligible switching activity .",
    "most of the power is used by output cells for driving the chip s outputs .",
    "we assumed a load of 15.3 pf for each output pin , derived from our model of a flip - chip package , pc board track , and fpga input pin .",
    "pin usroutclk was not used in the simulated scenario ; if it were used at 500mhz , it would add about 1mw .",
    "the total for all cells is in good agreement between the two estimates ( 43 mw in table [ tbl : pcresults ] and 47mw in table [ tbl : iopwr ] ) , but there are significant discrepancies in the details .",
    "power compiler s estimate of the internal power of the cells is about half of our estimate , but the switching power is higher .",
    "this is the result of several issues .",
    "first , power compiler treated each top - level port as bi - directional , so switching power is included for its output function , even if it is really just an input .",
    "second , the internal power of the cell is a strong function of the transition time of its input signal ( in both directions ) . for inputs",
    ", we assumed realistic 500ps transitions , whereas we told design compiler and power compiler that the input sources have infinite drive strength and zero transition time .",
    "for outputs , we assumed 50ps transitions at the cell inputs , which may have been too pessimistic .",
    "we resolve these discrepancies by adopting our estimates of the i / o cell power .",
    "this provides accurate values for switching and leakage power ( which are simple to calculate and comprise 74% of the total ) and conservative estimates of the internal power due to use of pessimistic transition time estimates .",
    "there is one more difficulty ; it affects only the internal power of the memory module , which is 19.8% of the total in table [ tbl : pcresults ] .",
    "the saif file gives , for each net , the total time at logic 0 , the total time at logic 1 , and the number of transitions in the 100@xmath123s analysis period .",
    "for some cells , this is not sufficient for calculating their internal power because the energy dissipated due to a transition on one pin ( such as a clock ) depends on the states of other pins . for almost all cells",
    "this is a negligible effect , but it is important for the dram , where the energy is different in read , write , read+refresh , write+refresh , refresh only , and idle cycles .",
    "modelsim is not capable of reporting state - dependent switching activity . in that case",
    ", power compiler assumes that the switching activity at the various pins is statistically independent with the probabilities given by the logic 0 and 1 times in the saif file . for the particular scenario that we have simulated",
    ", it turns out that this is a good approximation to the actual activity , but we now calculate a correction .",
    "this will be more important when the results are extrapolated to other scenarios ( section [ sec : pvsn ] ) .",
    "the dram has three active - low control pins , readn , wrtn , refn , which specify , respectively , a read cycle , a write cycle and a refresh cycle .",
    "thus in principle there are 8 possible cycle types , but two are excluded because reading and writing on the same cycle is not possible . of the remaining ones , our design uses only three : for the simulated scenario , the fraction of clock cycles in each state and the implied average frequencies are : as expected , the saif file implies probabilities for each control pin ofalthough the pin states are not independent , power compiler assumes , in the absence of any other information , that they are .",
    "there are then 4 possible states with probabilitiesthis implies rates ofif we multiply theses rates by the corresponding energy per clock transition for the appropriate corner , we get a total of 60.953 mw of internal power per dram cell .",
    "additional internal power is dissipated per output pin transition ; since the data are random , those pins should have transitions on 50% of the read cycles ; this is in fact what the saif file shows . applying this to the energy per transition for all 512 output pins gives an additional power of 0.452 mw .",
    "there are two dram cells in the memory module , giving 122.810 mw altogether , compared to 123.994 mw in table [ tbl : pcresults ] .",
    "the difference can be attributed to verilog wrapper logic provided by ibm .    in view of the good agreement in this analysis",
    ", we can proceed to correct the value in table [ tbl : pcresults ] by using the actual rates for each state .",
    "this gives 130.604 mw for the memory module ( including 1.184 mw for the wrappers ) , 5.3% higher than in table [ tbl : pcresults ] .",
    "more importantly , it provides a breakdown of power by the type of memory cycle , enabling us to extrapolate to scenarios where the state probabilities are different .        using the clock tree power and area from table [ tbl : trees ] ,",
    "the i / o cell power from table [ tbl : iopwr ] , and the corrected memory internal power from section [ sec : mempwr ] along with all other results from table [ tbl : pcresults ] gives the final totals in table [ tbl : corrected ] .",
    "[ cols= \" < , > , > , > , > , > \" , ]     power is for the simulated scenario :    @xmath7 , @xmath116 , @xmath125mhz , @xmath126mhz .    for memory , only the internal power estimate is revised .",
    "[ tbl : corrected ]      table [ tbl : corrected ] gives the total area of all cells as 9.760 mm@xmath2 . during layout",
    ", it will be necessary to add area between the cells to accommodate the wiring .",
    "the compiler estimated that the total length of wires will be 40,993.6 mm .",
    "the process provides 13 metal layers that can be used for wiring , but the uppermost layer provides the i / o pads and the next two layers are likely to be dedicated to core logic power and ground , leaving 10 wire routing layers .",
    "these vary in minimum pitch from 0.1 to 0.8  @xmath123 m , with an average of 0.19  @xmath123 m . using the latter number with the estimated length",
    "gives an area of 7.789 mm@xmath2 occupied by wires .",
    "if this is spread uniformly among the 10 layers , it uses 0.7789 mm@xmath2 of chip area .",
    "uniform use of the layers is unrealistic , and congestion in some places will no doubt require that the wiring be further spread out , so we double the latter value and estimate that 1.558 mm@xmath2 of chip area will be needed for wiring . adding this to the cell area gives a total chip area of 11.318 mm@xmath2 .",
    "this could be realized as a 3.4 mm square .",
    "we next consider whether the chip might need to be still larger to accommodate i / o pads .",
    "we are planning to use flip - chip packaging @xcite with c4 bumps at 200 @xmath123 m pitch , which is one of several options available from the foundry",
    ". a dense array of 289 bumps could then be placed on the 3.4 mm square chip .",
    "the design uses 58 pins for operating signals , and additional pins are needed for test signals as well as for power and ground , but the total is unlikely to exceed 100 .",
    "we conclude that the pins need not be densely packed and that no additional area needs to be included .",
    "simulation of the @xmath122 , @xmath127 scenario for the synthesized netlist ( section [ sec : sim ] ) takes 10 to 15 hours on a multi - core linux server with 32 gb of memory . under some circumstances , it can take much longer . with larger numbers of antennas , it is necessary to increase the integration length @xmath25 to avoid excessive output clock rate , and the number of sub - integrations in a full integration is proportional to @xmath1 , increasing the run time proportionally .",
    "it is therefore not practical to simulate large-@xmath0 or large-@xmath25 scenarios .. ]    it is important to understand the performance at larger @xmath0 and @xmath25 because the chip is likely to be useful in those applications and because it then performs more efficiently .",
    "fortunately , the available simulation results can be accurately extrapolated to the larger cases .",
    "the power estimates are broken down by major module and within each module by static and dynamic power .",
    "the modules operate at different clock frequencies , but each has only one clock and we know how the clock rates vary with @xmath0 and @xmath25 . the dynamic power is proportional to clock rate and the static ( leakage ) power is independent of clock rate . the memory module is somewhat more complicated ; it runs on the internal clock ( ) , but it uses different amounts of energy in each cycle depending on whether it is a read cycle , a write cycle or a refresh - only cycle .",
    "these energies are known from the cell library files , so each type of cycle can be extrapolated separately .",
    "table [ tbl : pwrvsn ] shows such extrapolations for several values of @xmath0 from 256 to 2048 .",
    "@lcrrrrrr@ number of antennas , @xmath0 : & & 128&256&512&640&1024&2048 + maximum integration length & @xmath25 & 32768 & 14976 & 6944 & 5480 & 3328 & 1536 + cmac rate , hz & @xmath128 & 6.250e+07 & 1.250e+08 & 2.500e+08 & 3.125e+08 & 2.031e+08 & 9.375e+07 + input clock frequeny , hz & @xmath92 & 5.000e+08 & 5.000e+08 & 5.000e+08 & 5.000e+08 & 2.031e+08 & 4.688e+07 + system clock frequency , hz & @xmath66 & 2.270e+08 & 2.270e+08 & 2.656e+08 & 3.281e+08 & 2.270e+08 & 2.270e+08 + output clock frequency , hz & @xmath94 & 5.000e+08 & 6.838e+07 & 2.949e+08 & 4.672e+08 & 5.000e+08 & 5.000e+08 + memory write rate , hz & @xmath129 & 1.563e+07 & 1.563e+07 & 1.563e+07 & 1.563e+07 & 6.348e+06 & 1.465e+06 + idle ( refresh only ) rate , hz & @xmath130 & 1.489e+08 & 8.638e+07 & 0.000e+00 & 0.000e+00 & 1.753e+07 & 1.318e+08 + bandwidth , hz & @xmath131 & 7.813e+06 & 3.906e+06 & 1.953e+06 & 1.563e+06 & 3.967e+05 & 4.578e+04 + & & _ simulated _ & + cmacs switching + internal & @xmath128 & 308.926 & 617.852 & 1235.704 & 1544.630 & 1004.010 & 463.389 + memory switching & @xmath128 & 2.970 & 5.940 & 11.880 & 14.850 & 9.653 & 4.455 + memory internal - read & @xmath128 & 59.968 & 119.936 & 239.872 & 299.840 & 194.896 & 89.952 + memory internal - write & @xmath129 & 22.224 & 22.224 & 22.224 & 22.224 & 9.029 & 2.084 + memory internal - idle & @xmath130 & 47.230 & 27.402 & 0.000 & 0.000 & 5.560 & 41.808 + memory wrapper internal & @xmath66 & 1.182 & 1.182 & 1.383 & 1.709 & 1.182 & 1.182 + address generator , sw+int & @xmath66 & 0.137 & 0.137 & 0.160 & 0.198 & 0.137 & 0.137 + dram to cmac , sw+int & @xmath66 & 9.884 & 9.884 & 11.566 & 14.287 & 9.884 & 9.884 + input interface , sw+int & @xmath92 & 8.701 & 8.701 & 8.701 & 8.701 & 3.535 & 0.816 + clock generator , sw+int & @xmath92 & 0.888 & 0.888 & 0.888 & 0.888 & 0.361 & 0.083 + control , sw+int & @xmath92 & 0.006 & 0.006 & 0.006 & 0.006 & 0.002 & 0.001 + output interface , sw+int & @xmath94 & 16.709 & 2.285 & 9.856 & 15.611 & 16.709 & 16.709 + misc .",
    "( top module ) , sw+int & @xmath66 & 3.981 & 3.981 & 4.658 & 5.754 & 3.981 & 3.981 + static , all above modules & 1 & 107.565 & 107.565 & 107.565 & 107.565 & 107.565 & 107.565 + input cells ( table [ tbl : iopwr ] ) & @xmath92 & 1.638 & 1.638 & 1.638 & 1.638 & 0.666 & 0.154 + output cells ( table [ tbl : iopwr ] ) & @xmath94 & 45.091 & 6.166 & 26.598 & 42.129 & 45.091 & 45.091 + i / o cell leakage & 1 & 0.409 & 0.409 & 0.409 & 0.409 & 0.409 & 0.409 + clk_cmac buffers ( table [ tbl : trees ] ) & @xmath128 & 33.610 & 67.221 & 134.442 & 168.052 & 109.234 & 50.416 + sysclk buffers ( table [ tbl : trees ] ) & @xmath66 & 3.224 & 3.224 & 3.772 & 4.660 & 3.224 & 3.224 + inclk buffers ( table [ tbl : trees ] ) & @xmath92 & 2.790 & 2.790 & 2.790 & 2.790 & 1.133 & 0.262 + clock buffers leakage & 1 & 0.027 & 0.027 & 0.027 & 0.027 & 0.027 & 0.027 + total , mw & & 677.160 & 1009.385 & 1824.113 & 2255.936 & 1526.254 & 841.528 + energy per cmac op , pj & & 2.65 & 1.97 & 1.78 & 1.76 & 1.83 & 2.19 +    [ tbl : pwrvsn ]    the column for @xmath122 uses results from power compiler ( table [ tbl : pcresults ] ) along with our estimates of the i / o cell power ( table [ tbl : iopwr ] ) and buffer tree power ( table [ tbl : trees ] ) . for the internal power of the memory module ,",
    "the corrected results derived in section [ sec : mempwr ] are used , separately for each cycle type . at the top of the table ,",
    "the average clock frequencies are given for each case .",
    "for the memory , the read rate is the same as the cmac rate ( @xmath128 ) , and the write rate is @xmath132 , where @xmath133 .",
    "the idle ( or refresh - only ) rate is @xmath134 .",
    "for each major module , the relevant clock is listed .",
    "the dynamic power includes internal and switching power , both of which are proportional to the module s clock frequency .",
    "the total static ( leakage ) power of all modules is given separately and is independent of @xmath0 . for the memory , internal and switching power",
    "are handled separately because switching power ( to drive the output pins ) is used only on read cycles .    the power fom , energy per cmac operation ,",
    "is given at the bottom of table [ tbl : pwrvsn ] .",
    "it is minimized at @xmath39 , but it remains low over the entire range .",
    "except at small @xmath0 , the majority of the power is dissipated in the cmacs ; this is desirable , since all else can be considered overhead .",
    "the cmacs themselves use 1.206 pj per operation and 36.5 mw of static power in all cases ( 1.23 to 1.35 pj per operation total ) .",
    "the results plotted in fig .  [ fig : performance ] use the same method to calculate performance for additional values of @xmath0 , including all multiples of 64 up to 4096 , and also including @xmath4 using the special memory bypass mode ( section [ sec : addrseq ] ) , except that the technique described below is used to improve performance for @xmath135 .",
    "[ fig : performance ] also shows the chip input and output rates , making it apparent when performance is limited by the maximum input rate of 16 gb / s or output rate of 8 gb / s ( 500 mb / s per i / o pin ) .",
    "although we could continue the extrapolation of table [ tbl : pwrvsn ] to larger @xmath0 , performance would deteriorate rapidly .",
    "since the on - chip memory is finite , larger @xmath0 means that @xmath25 must be reduced in order to store @xmath55 samples .",
    "shorter integrations increase the output rate , which is @xmath136 products per unit time , so @xmath128 must be reduced to stay within the available output rate capacity . the result for our parameters of @xmath47 and @xmath137 words",
    "is that @xmath128 and hence the overall processing rate is maximized at @xmath39 . below this",
    ", the device is input - rate limited , and above this it is output - rate limited .",
    "this is seen in fig .",
    "[ fig : performance ] .",
    "however , at large @xmath0 there is a way to do better .",
    "this is exploited in the results of fig .",
    "[ fig : performance ] , which shows that @xmath128 and @xmath131 are 1.2 times larger than in table [ tbl : pwrvsn ] at @xmath43 and 2.7 times larger at @xmath138 .",
    "this enables maintaining nearly the same integration length @xmath25 , processing rate @xmath128 , and output rate at arbitrarily large @xmath0 .",
    "otherwise , the processed bandwidth per device would be proportional to @xmath139 rather than @xmath140 .",
    "the method involves using a second level of buffering and loading only part of the @xmath55 samples into the processing chip at a time , as shown in figure [ fig : extmem ] .",
    "-0.15 in    the input buffer s size @xmath141 must be sufficient to hold @xmath55 samples , but we now partition the data into @xmath142 subsets of samples from @xmath143 of the signals , where @xmath144 is an integer . then @xmath145 samples from two of the subsets are loaded into the processing chip .",
    "the processing of these signals completes a _ partial integration _ ( pi ) .",
    "additional size-@xmath143 subsets are then loaded and additional pis are computed in such a way that all correlations among the @xmath20 signals are finally computed , as illustrated in figure [ fig : pi ] . in this way , integration length @xmath25 can be nearly a factor of @xmath144 larger than it could be if we had to load all @xmath55 samples at once ( full - integration processing ) .",
    "this allows the output data rate to be lower , avoiding the output rate limit .",
    "it comes at a cost of increasing the input rate by the same factor , since each subset must be loaded @xmath144 times .",
    "processing via pis is thus useful only at large @xmath0 , where full - integration processing would be output - rate limited and at least a factor of two below the input rate limit . for our design , this happens at @xmath135 .    .",
    "each square represents one pi .",
    "the @xmath20 signals are partitioned into six groups of @xmath143 signals , a through f , and two groups are correlated in each pi . on the diagonal of the matrix , a single pi is able to process all self- and cross - correlatons within each of two groups .",
    "the dashed triangles represent processing that was done earlier .",
    "the numbers in the upper left of each square give the optimum sequence of pis such that only one group is different from those of the previous pi , minimizing the input data rate .",
    "such a sequence exists for any integer value of @xmath144.,height=2 ]    -0.15 in    a complete integration requires @xmath146 partial integrations .",
    "each pi processes two different sets of @xmath143 signals , usually computing the @xmath147 cross - correlations of the first set with the second set , but sometimes computing the correlations among the signals of the first set and also those among the signals of the second set ( see fig .",
    "[ fig : pi ] ) .",
    "just as with full integrations , a pi is broken into sub - integrations within the processing chip .",
    "a total of @xmath148 sis is needed to process either type of pi , or @xmath149 sis altogether , exactly the same as would be needed if all signals were processed at once .",
    "the cmac re - use factor is unchanged , but throughput is higher because the cmacs can run faster without exceeding the ouput rate limit .    to allow pis to be processed continuously , one set of @xmath150 samples must fit in memory space @xmath151 , so that both sets of the current pi fit in @xmath152 .",
    "the remaining @xmath151 is then used to receive the @xmath150 samples that will be needed for the next pi .",
    "this means that the overlap memory is 50% of that holding the data being processed , vs.  a maximum of 33% for full - integration processing .",
    "this limits the increase in @xmath25 for pi processing to a factor of @xmath153 rather than @xmath144 .",
    "the memory address sequence needed to process all sis of one pi is also somewhat different from that shown in figs .  [",
    "fig : subint ] and [ fig : addrplot ] , but the address sequence within an si is the same , so no change to the chip s logic is needed @xcite .",
    "the input buffer memory must be external to the processing chip , but such a memory is also needed for full - integration processing because the data needs to be re - ordered , as explained in section [ sec : integration ] .",
    "the required capacity of the memory is the same for partial - integration and full - integration processing .",
    "the difference is that with pi processing each sample is read @xmath154 times , so the buffer s output rate is higher than its input rate by that factor , but it is never higher than the input bandwith of the processing chip , which is 16 gb / s for our design .",
    "the higher data rate increases the power consumption of the external devices and thus might contribute to a worsening the system - level energy fom .",
    "this is compensated by improved chip - level fom , but more importantly by a reduction in the static ( leakage ) power of all devices since fewer x - units are needed to process a given total bandwidth . whether the overall power consumption is higher or lower with pi processing depends on the parameters of a particular application and the efficiency of the external devices , but the cost will certainly be lower . for a given ( large ) @xmath0 , lower system power consumption could be obtained by using a larger processing chip with more memory , but the use of pis enables a chip with fixed capacity to be used efficiently at arbitrarily large @xmath0 .",
    "in a typical application , multiple correlation ics are used to implement part of an fx correlator , as shown in figure [ fig : arch ] .",
    "the ics are used in the  x \" portion , and each is the main component of one  x unit . \"",
    "the complete correlator also includes an  f \" or frequency analysis portion and a system for interconnecting the f and x portions . if the system bandwidth @xmath10 is greater than the bandwidth @xmath131 that one chip can correlate , then @xmath155 chips ( and x units ) are needed .",
    "normally one or more of the ics is installed on a printed circuit board and integrated with supporting logic , and multiple similar boards are used to build the complete x portion of the correlator . a general block diagram for one such board",
    "is shown in figure [ fig : board ] .",
    "-0.15 in    the supporting logic must provide the board - level input and output interfaces and generate the necessary clocks and control signals .",
    "the control signals include integrate and the memory address sequence that is written to each chip s registers via the spi port .",
    "the board must also provide an input data buffer for putting the samples into the order required by the correlation chips .",
    "in many situations , the supporting logic can be provided by an fpga and one fpga can support multiple correlation ics .",
    "it is necessary for the f portion to implement a filter bank that breaks each signal into channels of width no larger than @xmath131 , and an interconnection network is needed to send the samples for different channels to different correlation chips . in practice",
    "it is often desired to have channels of width @xmath156 , in which case each ic must process @xmath157 channels .",
    "this is possible because the sampling rate for each channel is only @xmath158 , so the ic can process it in a time smaller by factor @xmath159 than the time over which those samples were acquired .",
    "it can therefore keep up with the incoming data by processing the channels sequentially .    the re - ordering buffer is needed because the natural order of the f section outputs is different from that required at the x unit input .",
    "although the specific order needed here is peculiar to the design of our chip , a buffer of the same size would be needed for any fx correlator .",
    "the natural order of incoming data has one sample from all @xmath20 signals followed by the next sample from all signals , etc . , whereas for cross - correlation we need @xmath25 samples for one group of signals ( 64 in our case ) followed by @xmath25 samples for the next group , etc .",
    "the buffer must hold @xmath160 samples , enough for one integration ; in order for the data to be written and read in different orders it must be doubled to @xmath161 samples",
    ". it can be external to the processing chip because ( for full integrations ) its writing and reading rates are the same and each sample is written and read exactly once . with partial integrations ( section [ sec : largen ] )",
    ", the reading rate must be higher , but pi mode is used only at large @xmath0 where the input rate per x unit is lower .",
    "consider an array of @xmath162 dual - polarization antennas producing signals of bandwidth @xmath163mhz . from table [ tbl : pwrvsn ]",
    ", the processing chip can operate at a cmac rate of @xmath164mhz and requires a re - use factor of @xmath165 , so each chip can process a bandwidth of @xmath166mhz .",
    "we therefore need @xmath167 chips to correlate the full bandwidth .",
    "the chip s input and output rates are 16 gb / s and 4.72 gb / s , respectively .",
    "the memory is large enough for @xmath168 , so a re - ordering buffer of at least @xmath169 samples or 108.5mib ( at @xmath50b per sample ) will be needed for each chip . if the signals are broken into 61khz channels , then each ic must process @xmath170 channels and the re - ordering buffer size becomes @xmath171gib per chip .    at the board level , at least four correlation ics can be supported by one currently - available fpga ( e.g. , xilinx virtex 7 , xc7vx330 t ) and one set of ddr3 - 2133 memory chips , as shown in figure [ fig : example ] .    .",
    "four of our ics are supported by one fpga and a set of commodity memory chips . each correlation ic dissipates 1.82w and the total dissipation of the board is 16.5w . to achieve a total bandwidth of @xmath172mhz , 128 of these boards are needed.,height=3 ]    -0.15 in    the fpga provides 8 high - speed serial ports ( 8 gb / s each ) for receiving the input data for all the correlation ics , and 2 similar ports for transmitting the output data . the data could be organized into 10 gb / s ethernet frames or into any of various other standard protocols .    from table",
    "[ tbl : pwrvsn ] , each ic s power dissipation is 1.82w .",
    "the fpga is estimated to dissipate 5.55w @xcite , and the memory chips ( 8 at 8x256k ) 3.7w @xcite , giving 16.5w of dissipation per board for signal - processing devices . allowing 20% for power supplies and other overhead",
    "then gives 19.8w per board .",
    "the system requires 128 boards , resulting in a total x - portion dissipation of 2.54kw .",
    "the system - level energy figure of merit is then 4.84pj per cmac operation .",
    "this example uses 28-nm fpgas and ddr3 memory devices that are readily available now .",
    "more advanced devices of both types are becoming available [ including 20 nm and eventually 14 nm fpgas @xcite , and hybrid memory cubes @xcite ] .",
    "use of these would decrease the power for supporting devices and allow more correlation ics per board , resulting in a system - level power efficiency approaching that of the correlation chips .",
    "the ic design presented here provides a power - efficient means of computing all cross - correlations among many signals . the power efficiency is more than two orders of magnitude better than that of existing large correlators , and about a factor of 10 better than planned correlators based on future - generation fpgas .",
    "the ic is flexible , in that it can be used to construct correlators for almost any number of antennas and any bandwidth , although its efficiency is best if @xmath0 is a multiple of 64 .",
    "the design has been synthesized and subjected to careful post - synthesis simulation and analysis . from this",
    "we know that it will be smaller than 12 mm@xmath2 when fabricated in the ibm 32soi process , and we have accurate estimates of its power consumption .",
    "however , the physical design ( detailed layout ) has not yet been done and devices have not yet been fabricated .    in the radio astronomy community",
    ", there is currently a strong preference for building correlators from off - the - shelf programmable components like fpgas and gpus .",
    "so far , those devices have been used only for relatively small correlators .",
    "technology advances through moore s law continue to increase the size of digital processing machines that can be built that way , but custom - designed devices also benefit from these advances .",
    "the argument is sometimes made that when faced with requirements that can not be reasonably met by today s off - the - shelf devices , it is better to wait a few years until the devices get better than to spend that time developing a custom device . the results presented here belie that argument .",
    "we can do much better with devices fabricated in a technology that has been available for 5 years ( 32 nm ) than with fpgas that are two generations more advanced ( 14 - 16 nm ) and not yet available .",
    "the development cost of an asic is sometimes a deterrent to its adoption . for the large telescopes",
    "now planned or underway , it is a negligible fraction of the total development cost and is recoverable by energy cost savings in the first few years of operation . for smaller projects , several may be able to share the up - front costs provided that the design is flexible enough to be used in all of them . for our design , a substantial part of the development work has already been done .",
    "the design presented here would be a good choice for telescopes whose designs will be frozen in the next few years . for correlators needed further in the future , it would be reasonable to port the design to a more advanced technology , such as the 14 nm finfet processes that are now becoming available .",
    "in such a next - generation design , other improvements could be considered , such as adding high - speed serial i / o ( currently feasible but cost - prohibitive ) and increasing the sizes of the cmac array and memory .",
    "this work was carried out at the jet propulsion laboratory , california institute of technology , under a contract with the national aeronautics and space administration and funded through the internal research and technology development program .",
    "we thank dayton jones , joseph lazio and sander weinreb of jpl / caltech for advice and encouragement throught this project .",
    "jim maryoung of synopsys inc .  provided an exceptional level of assistance in the use of his company s tools .",
    "david hawkins , jonathon kocz , and sander weinreb provided valuable comments on an early draft .",
    "here we describe a manual , non - optimum synthesis of the clock trees for the purpose of estimating their power and area .",
    "final design of clock trees will be done during layout .      @lrrrrrr@ cell name & input cap . & intrinsic delay & delay / load & leakage pwr & internal energy & area + & ff&ps&ps / ff&@xmath123w&fj & @xmath173 + buf_x16 m & 3.80112 & 14.61741 & 0.17408 & 0.022023 & 10.410215 &",
    "2.691 + inv_x9 m & 6.55092 & 2.71712 & 0.58739 & 0.009357 & 2.49423 & 1.287 +      we adopt the constraint that the delay through the tree should be less than 20% of the clock period , and we use the maximum clock rate of each net ( 500 mhz for inclk and 360 mhz for  and clk_cmac ) .",
    "this gives maximum delays of 400 ps for inclk and 555 ps for  and clk_cmac .    to determine the delay",
    ", we adopt the rule - of - thumb that the wire load on a net is 2 ff for each destination cell input ) .",
    "the corresponding value with no wire load , using the synthesized netlist and delays from a compiler run with ideal wires ( no capacitance and no delay ) is 78.4mw .",
    "there are other differences between these synthesized designs , but this indicates that , on average , the total load is about twice the destination pin load . since the average input pin load throughout the design is about 1.9ff , we adopt a conservative estimate of 2ff of wire load per destination cell . ] , and we neglect the network delay because the chip is small ( about 3.4 mm edge - to - edge , giving a network delay of about 19 ps at a dielectric constant of 2.7 ) .",
    "the selection of the two standard cells in table [ tbl : bufcells ] was made by considering various pairs of possible buffer cells and finding the maximum number of one that can be driven by the other while keeping the delay sufficiently low . from this",
    "we find that three levels of buffering are needed for the clk_cmac network ( 311,488 loads ) , but two are sufficient for  and inclk .",
    "the buffer trees constructed from these cells will certainly not be optimum , but we are not attempting to produce the optimum design in this simple exercise ; we merely want to produce a feasible design and evaluate its area and power .",
    "it will then be apparent whether further optimization is worthwhile .    since these are all clock nets , the final destination loads are the clock pins of flip flops",
    ". there are 6 types of d ff in the library , and each has versions with 3 or 4 drive strengths , but among all of them the clock pin capacitance ranges from 0.499 pf to 0.672 pf .",
    "the maximum capacitance was used for our delay calculations .    from the above considerations , we choose the following rough designs : using these designs and the data of table [ tbl : bufcells ] , table",
    "[ tbl : bufresults ] shows the results of calculating the area , power , and delay of the three buffer trees .",
    "the frequency of each clock is the effective frequency in the scenario that was simulated in section [ sec : sim ] , not the maximum used for timing .",
    "the switching power of nets driving the destination cells was already included in the power compiler results ( table [ tbl : pcresults ] ) , so it is excluded from the `` added power '' in table [ tbl : bufresults ] .",
    "the total power added by these buffer trees is then about 39.65 mw .",
    "optimized buffer trees will use less .",
    "the added cell area is 3654 @xmath173 .",
    "the calculated delay through each buffer tree is also given , showing that each is less than the limit we specified .",
    "@lrrrrrrrr@ net & & & & totals + max .",
    "mhz & & & & + frequency , mhz & & & & + & & & + cell&inv_x9m&inv_x9m&buf_x16m&inv_x9m&inv_x9m&inv_x9m&inv_x9 m & + number&2686&75&1&48&1&26&1 & + fanout per cell&116&36&75&175&48&125&26 & + load per cell , ff&309.953&307.833&641.319&467.601&410.444&334.001&222.324 & + leakage , @xmath123w&25.13&0.70&0.02&0.45&0.01&0.24&0.01&26.57 + internal , @xmath123w&837.44&23.38&1.30&54.42&1.13&64.85&2.49&985.02 + switching , @xmath123w&42146.98&1168.80&32.47&4131.90&75.56&3517.03&90.04&51162.78 + delay , ps&184.7793&183.5343&126.2569&277.3799&243.8067&198.9048&133.3073 & + & & & + total power , @xmath123w & & & & 52174.36 + load pin sw . , @xmath123w & & & & + added power , @xmath123w & & & & 39650.76 + total delay , ps & & & & + total cell area , @xmath124 & & & & 3653.9100 +                                denman , n. , amiri , m. , bandura , k. , _ et al .",
    "_ , `` a gpu - based correlator x - engine implemented on the chime pathfinder . ''",
    "ieee 26th international conference on application - specific systems architectures and processors ( asap ) , toronto , 27 - 29 july 2015 , pp 3540 .",
    "elenius , p. , and levine , l. [ 2000 ] , `` comparing flip - chip and wire - bond interconnection technologies . ''",
    "_ chip scale review _ , * 4 * , 6 ( july / august 2000 ) .",
    "available at : http://processsolutionsconsulting.com / pdf / flip_bump / csr-7 - 00.pdf .",
    "[ 2009 ] , `` ibm announces industry s densest , fastest on - chip dynamic memory in 32-nanometer , silicon - on - insulator technology . ''",
    "news release dated 18 sep 2009 .",
    "available at : https://www-03.ibm.com / press / us / en / pressrelease/28428.wss .",
    "ieee computer soc .",
    "[ 2001 ] , _ 1497 - 2001ieee standard for standard delay format ( sdf ) for the electronic design process_. ieee std 1497 - 2001 , doi : 10.1109/ieeestd.2001.93359 , latest version dated 06 aug 2002 .",
    "kocz , j. , greenhill , l. j. , barsdell , b. r. , _ et al . _",
    "[ 2015 ] , `` digital signal processing using stream high performance computing : a 512-input broadband correlator for radio astronomy . '' _",
    "j. of astronomical instrumentation _ , * 4 * , 1550003 .",
    "morales , m. [ 2011 ] , `` enabling next - generation dark energy and epoch of reionization radio observatories with the moff correlator . '' _ pub .",
    "astronon .",
    "soc .  of the pacific _ , vol .",
    "123 , no . 909 , pp . 12651272 .",
    "mortola , inc .",
    "[ 2003 ] , _ spi block guide v03.06_. doc .",
    "s12spiv3/d , revised 04 feb .",
    "available at : www.ee.nmt.edu / teare / ee308l / datasheets / s12spiv3.pdf.see also http://www.byteparadigm.com/applications/introduction-to-i2c-and-spi-protocols .",
    "nagel , l. w. and pederson , d. o. [ 1973 ] , `` spice ( simulation program with integrated circuit emphasis ) .",
    "'' memorandum no .",
    "erl - m382 , university of california , berkeley , april 1973 .",
    "current version available as open - source software at http://bwrcs.eecs.berkeley.edu/classes/icbook/spice/ .",
    "synopsys , inc .",
    "[ 2015 ] , `` dc ultra : concurrent timing , area , power , and test optimization . ''",
    "design compiler ultra datasheet .",
    "available at : http://www.synopsys.com / tools / implementation / rtlsynthesis / dcultra / documents / dcultra - ds.pdf .        xilinx corp .",
    "[ 2012 ] , `` xpower estimator ( xpe)14.1 . ''",
    "xilinx power estimator spreadsheet for 7-series devices , released 8 may 2012 .",
    "current version available at http://www.xilinx.com/products/technology/power/xpe.html ."
  ],
  "abstract_text": [
    "<S> radio telescopes that employ arrays of many antennas are in operation , and ever larger ones are being designed and proposed . </S>",
    "<S> signals from the antennas are combined by cross - correlation . while the cost of most components of the telescope is proportional to the number of antennas @xmath0 , the cost and power consumption of cross - correlation are proportional to @xmath1 and dominate at sufficiently large @xmath0 . here </S>",
    "<S> we report the design of an integrated circuit ( ic ) that performs digital cross - correlations for arbitrarily many antennas in a power - efficient way . </S>",
    "<S> it uses an intrinsically low - power architecture in which the movement of data between devices is minimized . in a large system </S>",
    "<S> , each ic performs correlations for all pairs of antennas but for a portion of the telescope s bandwidth ( the so - called  fx \" structure ) . in our design , </S>",
    "<S> the correlations are performed in an array of 4096 complex multiply - accumulate ( cmac ) units . </S>",
    "<S> this is sufficient to perform all correlations in parallel for 64 signals ( @xmath0=32 antennas with 2 opposite - polarization signals per antenna ) . </S>",
    "<S> when @xmath0 is larger , the input data are buffered in an on - chip memory and the cmacs are re - used as many times as needed to compute all correlations . the design has been synthesized and simulated so as to obtain accurate estimates of the ic s size and power consumption . </S>",
    "<S> it is intended for fabrication in a 32 nm silicon - on - insulator process , where it will require less than 12mm@xmath2 of silicon area and achieve an energy efficiency of 1.76 to 3.3pj per cmac operation , depending on the number of antennas . </S>",
    "<S> operation has been analyzed in detail up to @xmath3 . </S>",
    "<S> the system - level energy efficiency , including board - level i / o , power supplies , and controls , is expected to be 5 to 7pj per cmac operation . </S>",
    "<S> existing correlators for the jvla ( @xmath4 ) and alma ( @xmath5 ) telescopes achieve about 5000 pj and 1000 pj respectively using application - specific ics in older technologies . to our knowledge , </S>",
    "<S> the largest-@xmath0 existing correlator is leda at @xmath6 ; it uses gpus built in 28 nm technology and achieves about 1000pj . </S>",
    "<S> correlators being designed for the ska telescopes ( @xmath7 and @xmath8 ) using fpgas in 16 nm technology are predicted to achieve about 100pj .    ; </S>",
    "<S> ; ; published 2016 march 18 . </S>"
  ]
}