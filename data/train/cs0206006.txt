{
  "article_text": [
    "the _ mutual information _ @xmath0 ( also called _ cross entropy _ or _ information gain _ ) is a widely used information - theoretic measure for the stochastic dependency of discrete random variables @xcite .",
    "it is used , for instance , in learning _",
    "bayesian nets _",
    "@xcite , where stochastically dependent nodes shall be connected ; it is used to induce classification trees @xcite .",
    "it is also used to select _ features _ for classification problems @xcite , i.e.  to select a subset of variables by which to predict the _ class _ variable .",
    "this is done in the context of a",
    "_ filter approach _ that discards irrelevant features on the basis of low values of mutual information with the class _  _ @xcite .",
    "the mutual information ( see the definition in section [ dmi ] )  can be computed if the joint chances @xmath1 of two random variables @xmath2 and @xmath3 are known . the usual procedure in the common case of unknown chances @xmath1 is to use the _ empirical probabilities _ @xmath4 ( i.e. the sample relative frequencies : @xmath5 ) as if they were precisely known chances .",
    "this is not always appropriate .",
    "furthermore , the _ empirical mutual information _ @xmath6 does not carry information about the reliability of the estimate . in the bayesian framework one can address these questions by using a ( second order ) prior distribution @xmath7 , which takes account of uncertainty about @xmath8 . from the prior @xmath9 and the likelihood one can compute the posterior @xmath10 , from which the distribution @xmath11 of the mutual information can in principle be obtained .",
    "this paper reports , in section [ mr ] , the _ exact _ analytical mean of @xmath12 and an analytical @xmath13-approximation of the variance . these are reliable and quickly computable expressions following from @xmath14 when a _ dirichlet _ prior is assumed over @xmath8 .",
    "such results allow one to obtain analytical approximations of the distribution of @xmath0 .",
    "we introduce asymptotic approximations of the distribution in section [ ba ] , graphically showing that they are good also for small sample sizes .",
    "the distribution of mutual information is then applied to feature selection .",
    "section [ tpf ] proposes two new filters that use _ credible intervals _ to robustly estimate mutual information .",
    "the filters are empirically tested , in turn , by coupling them with the _ naive bayes classifier _ to incrementally learn from and classify new data .",
    "on ten real data sets that we used , one of the two proposed filters outperforms the traditional filter : it almost always selects fewer attributes than the traditional one while always leading to equal or significantly better prediction accuracy of the classifier ( section [ ea ] ) .",
    "the new filter is of the same order of computational complexity as the filter based on empirical mutual information , so that it appears to be a significant improvement for real applications .",
    "the proved importance of the distribution of mutual information led us to extend the mentioned analytical work towards even more effective and applicable methods .",
    "section [ ta ] proposes improved analytical approximations for the tails of the distribution , which are often a critical point for asymptotic approximations .",
    "section [ etis ] allows the distribution of mutual information to be computed also from incomplete samples .",
    "closed - form formulas are developed for the case of feature selection .",
    "consider two discrete random variables @xmath2 and @xmath3 taking values in @xmath15 and @xmath16 , respectively , and an i.i.d .",
    "random process with samples @xmath17 drawn with joint chances @xmath1 .",
    "an important measure of the stochastic dependence of @xmath2 and @xmath3 is the mutual information : @xmath18 where @xmath19 denotes the natural logarithm and @xmath20 and @xmath21 are marginal chances .",
    "often the chances @xmath22 are unknown and only a sample is available with @xmath23 outcomes of pair @xmath24 .",
    "the empirical probability @xmath25 may be used as a point estimate of @xmath1 , where @xmath26 is the total sample size .",
    "this leads to an empirical estimate @xmath27 for the mutual information .",
    "unfortunately , the point estimation @xmath28 carries no information about its accuracy . in the bayesian approach to this problem",
    "one assumes a prior ( second order ) probability density @xmath7 for the unknown chances @xmath1 on the probability simplex . from this one",
    "can compute the posterior distribution @xmath29 ( the @xmath23 are multinomially distributed ) and define the posterior probability density of the mutual information : denotes the mutual information for the specific chances @xmath8 , whereas @xmath0 in the context above is just some non - negative real number .",
    "@xmath0 will also denote the mutual information _ random variable _ in the expectation @xmath30 $ ] and variance @xmath31 $ ] .",
    "expectations are _ always _ w.r.t .  to the posterior distribution @xmath32 .",
    "] @xmath33 with sharp upper bound @xmath34 , the integral may be restricted to @xmath35 , which shows that the domain of @xmath11 is @xmath36.$]]the @xmath37 distribution restricts the integral to @xmath8 for which @xmath38 . for large sample size @xmath39 , @xmath10",
    "is strongly peaked around @xmath40 and @xmath14 gets strongly peaked around the frequency estimate @xmath41 .",
    "many _ non - informative _ priors lead to a dirichlet posterior distribution @xmath42 with interpretation @xmath43 , where @xmath44 are the number of samples @xmath24 , and @xmath45 comprises prior information ( @xmath46 for the uniform prior , @xmath47 for jeffreys prior , @xmath48 for haldane s prior , @xmath49 for perks prior @xcite ) . in principle",
    "this allows the posterior density @xmath11 of the mutual information to be computed .",
    "we focus on the mean @xmath30=\\int_{0}^{\\infty } ip(i|{\\bf n})\\,di=\\int i({\\bf % \\pi } ) p({\\bf \\pi } |{\\bf n})d^{rs}{\\bf \\pi } $ ] and the variance @xmath50=e[(i - e[i])^{2}]$ ] .",
    "( [ miexex2 ] ) reports the exact mean of the mutual information : @xmath51 & = & \\frac{{1}}{n}\\sum_{ij}n_{ij}[\\psi ( n_{ij}+1)-\\psi ( n_{i+}+1 ) \\nonumber \\\\ & & -\\psi ( n_{+j}+1)+\\psi ( n+1)]\\text { , }   \\label{miexex2}\\end{aligned}\\ ] ] where @xmath52 is the @xmath52-function that for integer arguments is @xmath53 , and @xmath54 is euler s constant .",
    "the approximate variance is given below : @xmath55 & = & \\stackrel{o\\left ( n^{-1}\\right ) } { \\overbrace{\\frac{k - j^{2}}{% n+1}}}+\\stackrel{o\\left ( n^{-2}\\right ) } { \\overbrace{\\frac{m+\\left ( r-1\\right ) \\left ( s-1\\right ) \\left ( { \\textstyle}\\frac{1}{2}-j\\right ) -q}{% \\left ( n+1\\right ) \\left ( n+2\\right ) } } }   \\nonumber \\\\ & & + o(n^{-3 } )   \\label{varappr}\\end{aligned}\\ ] ] where @xmath56    the results are derived in @xcite .",
    "the result for the mean was also reported in @xcite , theorem 10 .",
    "we are not aware of similar analytical approximations for the variance . @xcite",
    "express the exact variance as an infinite sum , but this does not allow a straightforward systematic approximation to be obtained .",
    "@xcite used heuristic numerical methods to estimate the mean and the variance .",
    "however , the heuristic estimates are incorrect , as it follows from the comparison with the analytical results provided here ( see @xcite ) .",
    "let us consider two further points .",
    "first , the complexity to compute the above expressions is of the same order @xmath57 as for the empirical mutual information ( [ mi ] ) .",
    "all quantities needed to compute the mean and the variance involve double sums only , and the function @xmath52 can be pre - tabled .",
    "secondly , let us briefly consider the quality of the approximation of the variance .",
    "the expression for the exact variance has been taylor - expanded in @xmath58 to produce ( [ varappr ] ) , so the relative error @xmath59_{approx}-\\mbox{\\scriptsize var}[i]_{exact}}{\\mbox{\\scriptsize var}% [ i]_{exact}}}$ ] of the approximation is of the order @xmath60 , _ if _ @xmath2 and @xmath3 are dependent . in the opposite case , the @xmath61 term in the sum drops itself down to order @xmath62 resulting in a reduced relative accuracy @xmath63 of ( [ varappr ] ) .",
    "these results were confirmed by numerical experiments that we realized by monte carlo simulation to obtain `` exact '' values of the variance for representative choices of @xmath1 , @xmath64 , @xmath65 , and @xmath66 .",
    "let us now consider approximating the overall distribution of mutual information based on the formulas for the mean and the variance given in section [ mr ] .",
    "fitting a normal distribution is an obvious possible choice , as the central limit theorem ensures that @xmath11 converges to a gaussian distribution with mean @xmath30 $ ] and variance @xmath31 $ ] . since @xmath0 is non - negative , it is also worth considering the approximation of @xmath67 by a gamma ( i.e. , a scaled @xmath68 ) .",
    "even better , as @xmath69 can be normalized in order to be upper bounded by 1 , the beta distribution seems to be another natural candidate , being defined for variables in the @xmath70 $ ] real interval .",
    "of course the gamma and the  beta are asymptotically correct , too .",
    "we report a graphical comparison of the different approximations by focusing on the special case of binary random variables , and on three possible vectors of counts .",
    "figure [ fig1 ] compares the exact distribution of mutual information , computed via monte carlo simulation , with the approximating curves .",
    "the figure clearly shows that all the approximations are rather good , with a slight preference for the beta approximation .",
    "the curves tend to do worse for smaller sample sizes  as it is was expected. higher moments computed in @xcite may be used to improve the accuracy .",
    "a method to specifically improve the tail approximation is given in section [ ta ] .",
    "classification is one of the most important techniques for knowledge discovery in databases @xcite .",
    "a classifier is an algorithm that allocates new objects to one out of a finite set of previously defined groups ( or _ classes _ )  on the basis of observations on several characteristics of the objects , called _ attributes _ or _",
    "features_. classifiers can be learnt from data alone , making explicit the knowledge that is hidden in raw data , and using this knowledge to make predictions about new data .",
    "feature selection is a basic step in the process of building classifiers @xcite .",
    "in fact , even if theoretically more features should provide one with better prediction accuracy ( i.e. , the relative number of correct predictions ) , in real cases it has been observed many times that this is not the case @xcite .",
    "this depends on the limited availability of data in real problems :  successful models seem to be in good balance of model complexity and available information . in facts",
    ", feature selection tends to produce models that are simpler , clearer , computationally less expensive and , moreover , providing often better prediction accuracy .",
    "two major approaches to feature selection are commonly used @xcite : _ filter _ and _ wrapper _ models .",
    "the filter approach is a preprocessing step of the classification task .",
    "the wrapper model is computationally heavier , as it implements a search in the feature space .      from now on",
    "we focus our attention on the filter approach .",
    "we consider the well - known filter ( f )  that computes the empirical mutual information between features and the class , and discards low - valued features @xcite .",
    "this is an easy and effective approach that has gained popularity with time .",
    "cheng reports that it is particularly well suited to jointly work with bayesian network classifiers , an approach by which he won the _ 2001 international knowledge discovery competition _ @xcite .",
    "the `` weka '' data mining package implements it as a standard system tool ( see @xcite , p. 294 ) .",
    "a problem with this filter is the variability of the empirical mutual information with the sample .",
    "this may allow wrong judgments of relevance to be made , as when features are selected by keeping those for which mutual information exceeds a fixed threshold @xmath71  in order for the selection to be robust , we must have some guarantee about the actual value of mutual information .",
    "we define two new filters .",
    "the _ backward filter _ ( bf ) discards an attribute if its value of mutual information with the class is less than or equal to @xmath72 with given ( high )  probability @xmath73 .",
    "the _ forward filter _ ( ff ) includes an attribute if the mutual information is greater than @xmath72 with given ( high )  probability @xmath73 .",
    "bf  is a conservative filter , because it will only discard features after observing substantial evidence supporting their irrelevance .",
    "ff instead will tend to use fewer features , i.e. only those for which there is substantial evidence about them being useful in predicting the class .",
    "the next sections present experimental comparisons of the new filters and the original filter f.",
    "for the following experiments we use the naive bayes classifier @xcite . this is a good classification model  despite its simplifying assumptions , see @xcite , which often competes successfully with the state - of - the - art classifiers from the machine learning field , such as c4.5 @xcite .",
    "the experiments focus on the incremental use of the naive bayes classifier , a natural learning process when the data are available sequentially : the data set is read instance by instance ; each time , the chosen filter selects a subset of attributes that the naive bayes uses to classify the new instance ; the naive bayes then updates its knowledge by taking into consideration the new instance and its actual class .",
    "the incremental approach allows us to better highlight the different behaviors of the empirical filter ( f )  and those based on credible intervals on mutual information ( bf and ff ) .",
    "in fact , for increasing sizes of the learning set the filters converge to the same behavior .    for each filter , we are interested in experimentally evaluating two quantities : for each instance of the data set , the average number of correct predictions ( namely , the prediction accuracy ) of the naive bayes classifier up to such instance ; and the average number of attributes used . by these quantities",
    "we can compare the filters and judge their effectiveness .",
    "the implementation details for the following experiments include : using the beta approximation ( section [ ba ] ) to the distribution of mutual information ; using the uniform prior for the naive bayes classifier and all the filters ; using natural logarithms everywhere ; and setting the level @xmath73 of the posterior probability to @xmath74 .",
    "as far as @xmath75 is concerned , we can not set it to zero because the probability that two variables are independent ( @xmath76 ) is zero according to the inferential bayesian approach .",
    "we can interpret the parameter @xmath75 as a degree of dependency strength below which attributes are deemed irrelevant .",
    "we set @xmath75 to @xmath77 , in the attempt of only discarding attributes with negligible impact on predictions .",
    "as we will see , such a low threshold can nevertheless bring to discard many attributes .",
    "table [ tab1 ] lists the 10 data sets used in the experiments .",
    "these are real data sets on a number of different domains .",
    "for example , shuttle - small reports data on diagnosing failures of the space shuttle ; lymphography and hypothyroid are medical data sets ; spam is a body of e - mails that can be spam or non - spam ; etc .",
    "._data sets used in the experiments , together with their number of features , of instances and the relative frequency of the majority class .",
    "all but the spam data sets are available from the uci repository of machine learning data sets @xcite .",
    "the spam data set is described in @xcite and available from androutsopoulos s web page.[tab1 ] _ [ cols=\"^,^,^,^\",options=\"header \" , ]     the remaining cases are described by means of the following figures .",
    "figure [ fig2 ] shows that ff allowed the naive bayes to significantly do better predictions than f for the greatest part of the chess data set . the maximum difference in prediction accuracy",
    "is obtained at instance 422 , where the accuracies are 0.889 and 0.832 for the cases ff and f , respectively .",
    "figure [ fig2 ] does not report the bf case , because there is no significant difference with the f curve .",
    "the good performance of ff was obtained using only about one third of the attributes ( table [ tab2 ] ) .",
    "figure [ fig3 ] compares the accuracies on the spam data set .",
    "the difference between the cases ff and f is significant in the range of instances 32413 , with a maximum at instance 59 where accuracies are 0.797 and 0.559 for ff and f , respectively .",
    "bf is significantly worse than f from instance 65 to the end .",
    "this excellent performance of ff is even more valuable considered the very low number of attributes selected for classification . in the spam case ,",
    "attributes are binary and correspond to the presence or absence of words  in an e - mail and the goal is to decide whether or not the e - mail is spam .",
    "all the 21611 words found in the body of e - mails were initially considered .",
    "ff shows that only an average of about123 relevant words is needed to make good predictions .",
    "worse predictions are made using f and bf , which select , on average , about 822 and 13127 words , respectively .",
    "figure [ fig4 ] shows the average number of excluded features for the three filters on the spam data set .",
    "ff suddenly discards most of the features , and keeps the number of selected features almost constant over all the process .",
    "the remaining filters tend to such a number , with different speeds , after initially including many more features than ff .    in summary ,",
    "the experimental evidence supports the strategy of only using the features that are reliably judged as carrying useful information to predict the class , provided that the judgment can be updated as soon as new observations are collected .",
    "ff almost always selects fewer features than f , leading to a prediction accuracy at least as good as the one f leads to .",
    "the comparison between f and bf is analogous , so ff appears to be the best filter and bf the worst .",
    "however , the conservative nature of bf might turn out to be successful when data are available in groups , making the sequential updating be not viable . in this case , it does not seem safe to take strong decisions of exclusion that have to be maintained for a number of new instances , unless there is substantial evidence against the relevance of an attribute .",
    "the expansion of @xmath11 around the mean can be a poor estimate for extreme values @xmath78 or @xmath79 and it is better to use tail approximations .",
    "the scaling behavior of @xmath11 can be determined in the following way : @xmath80 is small iff @xmath81 describes near independent random variables @xmath2 and @xmath3 .",
    "this suggests the reparameterization @xmath82 in the integral ( [ midistr ] ) .",
    "only small @xmath83 can lead to small @xmath80 .",
    "hence , for small @xmath0 we may expand @xmath80 in @xmath84 in expression ( [ midistr ] ) . correctly taking into account the constraints on @xmath84",
    ", a scaling argument shows that @xmath85 .",
    "similarly we get the scaling behavior of @xmath11 around @xmath86 .",
    "@xmath80 can be written as @xmath87 , where @xmath88 is the entropy . without loss of generality @xmath89 .",
    "if the prior @xmath90 converges to zero for @xmath91 sufficiently rapid ( which is the case for the dirichlet for not too small @xmath92 ) , then @xmath93 gives the dominant contribution when @xmath94 .",
    "the scaling behavior turns out to be @xmath95 .",
    "these expressions including the proportionality constants in case of the dirichlet distribution are derived in the journal version @xcite .      in the following",
    "we generalize the setup to include the case of missing data , which often occurs in practice .",
    "for instance , observed instances often consist of several features plus class label , but some features may not be observed , i.e.  if @xmath96 is a feature and @xmath97 a class label , from the pair @xmath98 only @xmath97 is observed .",
    "we extend the contingency table @xmath23 to include @xmath99 , which counts the number of instances in which only the class @xmath97 is observed (= number of @xmath100 instances ) .",
    "it has been shown that using such partially observed instances can improve classification accuracy @xcite .",
    "we make the common assumption that the missing - data mechanism is ignorable ( missing at random and distinct ) @xcite , i.e.the probability distribution of class labels @xmath97 of instances with missing feature @xmath96 is assumed to coincide with the marginal @xmath101 .",
    "the probability of a specific data set @xmath102 of size @xmath103 with contingency table @xmath104 given @xmath8 , hence , is @xmath105 . assuming a uniform prior @xmath106 bayes rule leads to the posterior @xmath107 .",
    "the mean and variance of @xmath0 in leading order in @xmath108 can be shown to be @xmath109 & = & i({\\bf \\hat{\\pi}})+o(n^{-1 } ) , \\\\",
    "\\mbox{var}[i ] & = & \\frac{1}{n}[\\tilde{k}-\\tilde{j}^{2}/\\tilde{q}-\\tilde{p}% ] + o(n^{-2}),\\end{aligned}\\ ] ] where @xmath110    the derivation will be given in the journal version @xcite .",
    "note that for the complete case @xmath111 , we have @xmath112 , @xmath113 , @xmath114 , @xmath115 , @xmath116 , and @xmath117 , consistently with ( [ varappr ] ) .",
    "preliminary experiments confirm that ff outperforms f also when feature values are partially missing .",
    "all expressions involve at most a double sum , hence the overall computation time is @xmath57 . for the case of missing class labels , but",
    "no missing features , symmetrical formulas exist . in the general case of missing features and missing class labels estimates for @xmath118 have to be obtained numerically , e.g.  by the em algorithm @xcite in time @xmath119 , where @xmath120 is the number of iterations of em .",
    "in @xcite we derive a closed form expression for the covariance of @xmath121 and the variance of @xmath0 to leading order which can be evaluated in time @xmath122 .",
    "this is reasonably fast , if the number of classes is small , as is often the case in practice .",
    "note that these expressions converge for @xmath123 to the exact values .",
    "the missingness needs not to be small .",
    "this paper presented ongoing research on the distribution of mutual information and its application to the important issue of feature selection . in the former case , we provide fast analytical formulations that are shown to approximate the distribution well also for small sample sizes .",
    "extensions are presented that , on one side , allow improved approximations of the tails of the distribution to be obtained , and on the other , allow the distribution to be efficiently approximated also in the common case of incomplete samples . as far as feature selection",
    "is concerned , we empirically showed that a newly defined filter based on the distribution of mutual information outperforms the popular filter based on empirical mutual information .",
    "this result is obtained jointly with the naive bayes classifier .",
    "more broadly speaking , the presented results are important since reliable estimates of mutual information can significantly improve the quality of applications , as for the case of feature selection reported here .",
    "the significance of the results is also enforced by the many important models based of mutual information .",
    "our results could be applied , for instance , to _ robustly _ infer classification trees .",
    "bayesian networks can be inferred by using credible intervals for mutual information , as proposed by @xcite .",
    "the well - known chow and liu s approach @xcite to the inference of tree - networks might be extended to credible intervals ( this could be done by joining results presented here and in past work @xcite ) .",
    "i.  androutsopoulos , j.  koutsias , k.  v. chandrinos , g.  paliouras , and d.  spyropoulos .",
    "an evaluation of naive bayesian anti - spam filtering . in g.",
    "potamias , v.  moustakis , and m.  van someren , editors , _ proc . of the workshop on",
    "machine learning in the new information age _ , pages 917 , 2000 .",
    "11th european conference on machine learning .",
    "g.  h. john , r.  kohavi , and k.  pfleger .",
    "irrelevant features and the subset selection problem . in w.",
    "w. cohen and h.  hirsh , editors , _ proceedings of the eleventh international conference on machine learning _ , pages 121129 , new york , 1994 .",
    "morgan kaufmann ."
  ],
  "abstract_text": [
    "<S> mutual information is widely used in artificial intelligence , in a descriptive way , to measure the stochastic dependence of discrete random variables . in order to address questions such as the reliability of the empirical value </S>",
    "<S> , one must consider sample - to - population inferential approaches . </S>",
    "<S> this paper deals with the distribution of mutual information , as obtained in a bayesian framework by a second - order dirichlet prior distribution . </S>",
    "<S> the exact analytical expression for the mean and an analytical approximation of the variance are reported . </S>",
    "<S> asymptotic approximations of the distribution are proposed . </S>",
    "<S> the results are applied to the problem of selecting features for incremental learning and classification of the naive bayes classifier . </S>",
    "<S> a fast , newly defined method is shown to outperform the traditional approach based on empirical mutual information on a number of real data sets . </S>",
    "<S> finally , a theoretical development is reported that allows one to efficiently extend the above methods to incomplete samples in an easy and effective way .    </S>",
    "<S> robust feature selection , naive bayes classifier , mutual information , cross entropy , dirichlet distribution , second order distribution , expectation and variance of mutual information . </S>"
  ]
}