{
  "article_text": [
    "the frank - wolfe ( fw ) optimization algorithm  @xcite , also known as the conditional gradient method  @xcite , is a first - order method for smooth constrained optimization over a compact set .",
    "it has recently enjoyed a surge in popularity thanks to its ability to cheaply exploit the structured constraint sets appearing in machine learning applications  @xcite .",
    "a known forte of fw is that it only requires access to a _ linear minimization oracle _ ( lmo ) over the constraint set , i.e. , the ability to minimize linear functions over the set , in contrast to projected gradient methods which require the minimization of _ quadratic _ functions or other nonlinear functions . in this paper",
    ", we extend the applicability of the fw algorithm to solve the following convex - concave saddle point problems : @xmath0 @xmath1 where @xmath2 is a smooth ( with @xmath3-lipschitz continuous gradient ) _ convex - concave function _ ,",
    "that is , @xmath4 is convex for all @xmath5 and @xmath6 is concave for all @xmath7 .",
    "we also assume that @xmath8 is a convex compact set such that its lmo is cheap to compute .",
    "saddle point solution _ to   is a pair @xmath9  ( * ? ? ? *",
    "vii.4 ) such that : @xmath10 @xmath11    [ [ examples - of - saddle - point - problems . ] ] examples of saddle point problems .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    @xcite cast the maximum - margin estimation of structured output models as a bilinear saddle point problem @xmath12 , where @xmath13 is the regularized set of parameters and @xmath14 is an encoding of the set of possible structured outputs .",
    "they considered settings where projection on @xmath13 and @xmath14 were efficient but one can imagine many situations where only lmo s are efficient .",
    "for example , we could use a structured sparsity inducing norm  @xcite for the parameter @xmath15 , such as the overlapping group lasso for which the projection is expensive  @xcite , while @xmath14 could be a combinatorial object such as a the ground state of a planar ising model ( without external field ) which admits an efficient oracle  @xcite but has potentially intractable projection .",
    "similarly , two - player games  @xcite can often be solved as bilinear minimax problems .",
    "when a strategy space involves a polynomial number of constraints , the equilibria of such games can be solved efficiently  @xcite .",
    "however , in situations such as the colonel blotto game or the matching duel  @xcite the strategy space is intractably large and defined by an exponential number of linear constraints .",
    "fortunately , some linear minimization oracles such as the blossom algorithm  @xcite can efficiently optimize over matching polytopes despite an exponential number of linear constraints .",
    "robust learning is also often cast as a saddle point minimax problem  @xcite .",
    "once again , a fw implementation could leverage fast linear oracles while projection methods would be plagued by slower or intractable sub - problems .",
    "for instance , if the lmo is max - flow , it could have almost linear runtime while the corresponding projection would require cubic runtime quadratic programming  @xcite .",
    "[ [ related - work . ] ] related work .",
    "+ + + + + + + + + + + + +    the standard approaches to solve smooth constrained saddle point problems are projection - type methods ( surveyed in  @xcite ) , with in particular variations of korpelevich s extragradient method  @xcite , such as  @xcite which was used to solve the structured prediction problem  @xcite mentioned above .",
    "there is surprisingly little work on fw - type methods for saddle point problems , although they were briefly considered for the more general _ variational inequality _ problem ( vip ) : @xmath16 where @xmath17 is a lipschitz mapping from @xmath18 to itself and @xmath19 . by using @xmath20 and @xmath21",
    ", the variational inequality problem   reduces to the equivalent optimality conditions for the saddle point problem  .",
    "@xcite showed that a fw algorithm with a step size of @xmath22 converges for the vip   when the set @xmath23 is strongly convex , while fw with a generalized line - search on a saddle point problem is sometimes non - convergent when @xmath23 is a polytope ( see also  @xcite ) .",
    "she conjectured though that using a step size of @xmath22 was also convergent when @xmath23 is a polytope  a problem left open up to this point .",
    "more recently , @xcite ( see also  @xcite ) proposed a method to transform a vip on @xmath23 where one has only access to a lmo , to a `` dual '' vip on which they can use a projection - type method .",
    "@xcite proposes to solve the saddle point problem   by running fw on @xmath13 on the _ smoothed _ version of the problem @xmath24 , thus requiring a projection oracle on @xmath14 .",
    "in contrast , in this paper we study simple approaches that do not require any transformations of the problem   nor any projection oracle on @xmath13 or @xmath14 .",
    "[ [ contributions . ] ] contributions .",
    "+ + + + + + + + + + + + + +    in   [ sec : algorithms ] , we extend several variants of the fw algorithm to solve the saddle point problem   that we think could be of interest to the machine learning community . in   [",
    "sec : sp - fw_strong_convexity ] , we give a first proof of ( geometric ) convergence for these methods over polytope domains under the assumptions of sufficient strong convex - concavity of  @xmath2 , giving a partial answer to the conjecture from  @xcite . in   [ sec : strongly_set ] , we extend and refine the previous convergence results when @xmath13 and @xmath14 are strongly convex sets and the gradient of @xmath2 is non - zero over @xmath25 , while we survey the pure bilinear case in ",
    "[ sec : bilinear ] .",
    "we finally present illustrative experiments for our theory in ",
    "[ sec : experiments ] , noticing that the convergence theory is still incomplete for these methods .",
    "[ [ par : the_algorithms ] ] the algorithms . + + + + + + + + + + + + + + +    this article will explore three sp extensions of the classical _ frank - wolfe _ algorithm ( algorithm [ fw ] ) which are summarized in algorithm [ alg : sp - fw ] , [ alg : afw ] and [ alg : pfw ] . in the following , the point computed by these algorithms after @xmath26 steps",
    "will be noted @xmath27 .",
    "we first obtain the _ saddle point fw _ ( sp - fw ) algorithm ( algorithm  [ alg : sp - fw ] ) by simultaneously doing a fw update on both convex functions @xmath28 and @xmath29 with a properly chosen step size .",
    "hence the point @xmath30 has a sparse representation as a convex combination of the points previously given by the oracle .",
    "this set of points is called the _ active set_. if we assume that @xmath13 and @xmath14 are the convex hulls of two finite sets of points @xmath31 and @xmath32 , we can also extend the _ away - step frank - wolfe _ ( afw ) algorithm  @xcite to saddle point problems .",
    "as for afw , this new algorithm is able to remove mass from  bad \" atoms in the active set , ( see l[algline : drop_step ] of algorithm  [ alg : afw ] and in appendix [ par : active_set ] ) to avoid the zig - zagging problem that slows down standard fw  @xcite .",
    "because of the special product structure of the domain , we consider more away directions than proposed in  @xcite for afw .",
    "namely , for every corner @xmath33 and @xmath34 already picked , @xmath35 is a feasible directions in @xmath13 and @xmath36 is a feasible direction in @xmath14 .",
    "thus the combination @xmath37 is a feasible direction even if the particular corners @xmath38 and @xmath39 have never been picked together .",
    "we thus maintain the iterates on @xmath13 and @xmath14 as independent convex combination of their respective active sets of corners ( line  [ algline : activeset ] of algorithm  [ alg : afw ] ) , i.e. , @xmath40 finally , a straightforward saddle point generalization for the _ pairwise frank - wolfe _ ( pfw ) algorithm is given in algorithm  [ alg : pfw ] .",
    "the proposed algorithms preserve several nice properties of previous fw methods ( in addition to only requiring lmo s ) : simplicity of implementation , affine invariance  @xcite , gap certificates computed for free , sparse representation of the iterates and the possibility to have adaptive step sizes using the gap computation .",
    "we next analyze the convergence of these algorithms .",
    "let @xmath41 compute @xmath42 compute @xmath43 compute @xmath44 * if * @xmath45 * then * * return * @xmath46 let @xmath47 ( or do line - search ) update @xmath48    let @xmath49 compute @xmath50 [ algline : rk ] compute @xmath51 compute @xmath52 * if * @xmath53 * then * * return * @xmath54 let @xmath55 * or * @xmath47 update @xmath56    let @xmath57 , @xmath58 and @xmath59 let @xmath60 and @xmath61   _ ( @xmath62 as defined in l[algline : rk ] in algorithm  [ alg : sp - fw ] ) _ let @xmath63 and @xmath64 _ ( the away direction ) _ * if * @xmath65 * then * * return * @xmath54 _ ( fw gap is small enough , so return ) _ [ algline : gapfw ] [ algline : begafw ] @xmath66 , and @xmath67 _",
    "( choose the fw direction ) _",
    "@xmath68 , and @xmath69 _ ( maximum feasible step size ; a _ drop step _ is when @xmath70 ) _ [ algline : drop_step ] [ algline : endafw ] let @xmath71 * and * @xmath72 _ ( @xmath73 and @xmath74 set as in thm .",
    "[ thm : conv ] ) _ [ algline : gappfw ] update @xmath75 _ ( and accordingly for the weights @xmath76 , see  @xcite ) _ update @xmath77 and @xmath78 [ algline : activeset ]       in alg .",
    "[ alg : afw ] , replace l[algline : begafw ] to  [ algline : endafw ] by : @xmath79 , and @xmath80 .",
    "[ [ par : the_subopti ] ] the suboptimality error and the gap .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to establish convergence , we first define several quantities of interest . in classical convex optimization ,",
    "the suboptimality error @xmath81 is well defined as @xmath82 .",
    "this quantity is clearly non - negative and proving that @xmath81 goes to 0 is enough to establish convergence .",
    "unfortunately , in the saddle point setting the quantity @xmath83 is no longer non - negative and can be equal to zero for an infinite number of points @xmath84 while @xmath85 .",
    "for instance , if @xmath86 with @xmath87 $ ] , then @xmath88 and @xmath89 .",
    "but for all @xmath7 and @xmath5 , @xmath90 .",
    "the saddle point literature thus considers a non - negative gap function ( also known as a merit function @xcite and ( * ? ? ?",
    "* sec 4.4.1 ) ) which is zero only for optimal points , in order to quantify progress towards the saddle point .",
    "we can define the following _ suboptimality error _",
    "@xmath81 for our saddle point problem : @xmath91     \\text{and }   \\quad { \\widehat{\\bm{y}}^{(t)}}:= \\operatorname*{\\arg\\max}_{{\\bm{y}}\\in { \\mathcal{y } } } { \\mathcal{l}}({\\bm{x}^{(t)}},{\\bm{y } } ) .",
    "\\end{aligned}\\ ] ] this is an example of _ primal - dual _ gap function by noticing that @xmath92 where @xmath93 is the convex primal function and @xmath94 is the concave dual function . by convex - concavity , @xmath81 can be upper - bounded by the following fw linearization gap @xcite : @xmath95 this gap is easy to compute and gives a stopping criterion since @xmath96 .",
    "[ [ par : the_compensation_phenomenon ] ] compensation phenomenon and difficulty for sp .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    even when equipped with a suboptimality error and a gap function ( as in the convex case ) , we still can not apply the standard fw convergence analysis .",
    "the usual fw proof sketch uses the fact that the gradient of @xmath97 is lipschitz continuous to get @xmath98 which then provides a rate of convergence .",
    "roughly , since @xmath99 by convexity , if @xmath100 is small enough then @xmath101 will decrease and converge . for simplicity , in the main paper , @xmath102 will refer to the @xmath103 norm of @xmath104 . the partial lipschitz constants and the diameters of the sets",
    "are defined with respect to this norm ( see   in appendix  [ sub : the_lipschitz_constants ] for more general norms ) .    using the @xmath3-lipschitz continuity of @xmath2 and letting @xmath105 as a shorthand , we get @xmath106 where @xmath107 and @xmath108 .",
    "then @xmath109 unfortunately , the quantity @xmath110 does _ not _ appear above and we therefore can not control the oscillation of the sequence ( the quantity @xmath111 can make the sequence increase or decrease ) . instead , we must focus on more specific sp optimization settings and introduce other quantities of interest in order to establish convergence .",
    "[ [ par : the_asymmetry_of_the_sp ] ] the asymmetry of the sp .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    @xcite showed the divergence of the sp - fw algorithm with an extended line - search step - size on some bilinear objectives .",
    "she mentioned that the difficulty for sp optimization is contained in this bilinear coupling between @xmath15 and @xmath112 . more generally ,",
    "most of the examples of sp functions cited in the introduction can be written in the form : @xmath113 in this setting , the bilinear part @xmath114 is the only term preventing us to apply theorems on standard fw .",
    "@xcite also conjectured that the sp - fw algorithm with @xmath115 performed on an uniformly strongly convex - concave objective function ( see  ) over a polytope should converge .",
    "we give a partial answer to this conjecture in the following section .",
    "[ [ par : strong_convex_concavity ] ] uniform strong convex - concavity .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this section , we will assume that @xmath2 is uniformly @xmath116-strongly convex - concave , which means that the following function is convex - concave : @xmath117    [ [ par : the_new_quantity ] ] a new merit function .",
    "+ + + + + + + + + + + + + + + + + + + + +    to prove our theorem , we use a different quantity @xmath118 which is smaller than @xmath81 but still a valid merit function in the case of _ strongly convex - concave _ sps ( where @xmath119 is thus unique ) ; see   below . for @xmath119 a solution of  , we define the non - negative quantity @xmath118 : @xmath120 notice that @xmath121 and @xmath122 are non - negative , and that @xmath123 since : @xmath124 in general , @xmath118 can be zero even if we have not reached a solution .",
    "for example , with @xmath125 and @xmath87 $ ] , then @xmath126 , implying @xmath127 for any @xmath128 .",
    "but for a uniformly strongly convex - concave  @xmath2 , this can not happen and we can prove that @xmath118 has the following nice property ( proposition  [ prop : relation_primal ] in appendix  [ sub : relation_between_the_primal_errors ] ) : @xmath129 where @xmath130    [ [ par : pyramidal_width ] ] pyramidal width and distance to the border .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we now provide a theorem that establishes convergence in two situations : when the sp belongs to the interior of @xmath131 ; when the set is a polytope , i.e. when there exist two finite sets such that @xmath132 and @xmath133 ) .",
    "our convergence result holds when ( roughly ) the strong convex - concavity of  @xmath2 is big enough in comparison to the cross lipschitz constants @xmath134 , @xmath135 of @xmath136 ( defined in   below ) multiplied by geometric `` condition numbers '' of each set . the condition number of @xmath13 ( and similarly for @xmath14 )",
    "is defined as the ratio of its diameter @xmath137 over the following appropriate notions of `` width '' : @xmath138 the pyramidal width   is formally defined in  eq .",
    "9 of  @xcite .",
    "given the above constants , we can state below a non - affine invariant version of our convergence theorem ( for simplicity ) .",
    "the affine invariant versions of this theorem are given in thm .",
    "[ thm : conv_affine_invariant ] and  [ thm : conv_sublin ] in appendix  [ subsec : proof_main_thm ] ( with proofs ) .",
    "[ thm : conv ] let @xmath2 be a convex - concave function and @xmath131 a convex and compact set .",
    "assume that the gradient of @xmath2 is @xmath3-lipschitz continuous , that @xmath2 is @xmath116-strongly convex - concave , and that we are in one of the two following situations : @true@leqno @xmath139      & \\parbox{7cm}{the sets $ { \\mathcal{x}}$ and $ { \\mathcal{y}}$ are polytopes .",
    "in this case , set $ { g_{t}}= { g_{t}}^{{{\\hspace{0.05em}\\textnormal{pfw}}}}$ ( as in l\\ref{algline : gappfw } of alg.~\\ref{alg : afw } ) , $ \\delta_\\mu : = \\sqrt{\\min(\\mu_{\\mathcal{x}}\\delta_{\\mathcal a}^2 , \\mu_{\\mathcal{y}}\\delta_{\\mathcal b}^2)}$ and $ a:=   \\frac{1}{2}$. `` algorithm '' then refers to sp - afw . here",
    "$ \\delta_\\mu$ needs to use the euclidean norm for its defining constants .      }",
    "\\label{enu : situation2 }       \\tag{p }    \\end{aligned}\\ ] ] @false@eqno in both cases , if @xmath140 is positive , then the errors @xmath81   of the iterates of the algorithm with step size @xmath141 decrease geometrically as @xmath142 where @xmath143 , @xmath144 and @xmath145 is the number of non - drop step after @xmath26 steps ( see l[algline : drop_step ] in alg .",
    "[ alg : afw ] ) . in case",
    "we have @xmath146 and in case   we have @xmath147 . for both algorithms , if @xmath148 , we also obtain a sublinear rate with the universal choice @xmath149 .",
    "this yields the rates : @xmath150    clearly , the sublinear rate seems less interesting than the linear one but has the added convenience that the step size can be set without knowledge of various constants that characterize @xmath2 .",
    "moreover , it provides a partial answer to the conjecture from  @xcite .",
    "[ [ par : proof_sketch ] ] proof sketch .",
    "+ + + + + + + + + + + + +    strong convexity is an essential assumption in our proof ; it allows us to relate @xmath118 to how close we are to the optimum .",
    "actually , by @xmath151-strong concavity of @xmath152 we have @xmath153 now , recall that we assumed that @xmath136 is lipschitz continuous .",
    "in the following , we will call @xmath3 the _ lipschitz continuity constant _ of @xmath136 and @xmath134 and @xmath135 its ( cross ) _ partial lipschitz constants_. for all @xmath154 , these constants satisfy @xmath155 note that @xmath156 if @xmath157 . then , using lipschitz continuity of the gradient , @xmath158 furthermore , setting @xmath159 and @xmath160 in equation  , we have @xmath161 finally , combining and , we get @xmath162 a similar argument on @xmath163 gives a bound on @xmath122 much like  .",
    "summing both yields : @xmath164 we now apply recent developments in the convergence theory of fw methods for strongly convex objectives .",
    "@xcite crucially upper bound the square root of the suboptimality error on a convex function with the fw gap if the optimum is in the interior , or with the pfw gap if the set is a polytope ( lemma  [ lemme : lingap ] in appendix  [ app : gapinequalities ] ) .",
    "we continue our proof sketch for case   only : @xmath165 we can also get the respective equation on @xmath112 with @xmath166 and sum it with the previous one to get : @xmath167 plugging this last equation into gives us @xmath168 the recurrence   is typical in the fw literature .",
    "we can re - apply standard techniques on the sequence  @xmath118 to get a sublinear rate with @xmath169 , or a linear rate with @xmath170 ( which minimizes the rhs of   and actually guarantees that @xmath118 will be _ decreasing _ ) .",
    "finally , thanks to strong convexity , a rate on @xmath118 gives us a rate on @xmath81 ( by  ) .",
    "[ [ par : strongly_convex_set ] ] strongly convex set .",
    "+ + + + + + + + + + + + + + + + + + + +    one can ( roughly ) define strongly convex set as sublevel sets of strongly convex functions  ( * ? ? ?",
    "4.14 ) . in this section",
    ", we replace the strong convex - concavity assumption on  @xmath2 with the assumption that @xmath13 and @xmath14 are @xmath171-strongly convex _",
    "sets_.    [ def : strong_set ] a convex set @xmath13 is said to be @xmath171-strongly convex with respect to  @xmath172 if for any @xmath173 and any @xmath174 $ ] , @xmath175 where @xmath176 is the @xmath172-ball of radius @xmath177 centered at @xmath178 .",
    "frank - wolfe for convex optimization over strongly convex sets has been studied by @xcite and @xcite , amongst others .",
    "they all obtained a linear rate for the fw algorithm if the norm of the gradient is lower bounded by a constant .",
    "more recently , @xcite proved a sublinear rate @xmath179 by replacing the lower bound on the gradient by a strong convexity assumption on the function . in the vip setting ,",
    "the linear convergence has been proved if the optimization is done under a strongly convex set but this assumption does _ not _ extend to @xmath25 which _ can not _ be strongly convex if @xmath13 or @xmath14 is not reduced to a single element . in order to prove the convergence , we first prove the lipschitz continuity of the _ fw - corner _",
    "function @xmath180 defined below . a proof of this theorem",
    "is given in appendix [ sec : strong_conv_proof ] .",
    "[ thm : s_lip ] let @xmath13 and @xmath14 be @xmath171-strongly convex sets . if @xmath181 for all @xmath182 , then the oracle function @xmath183 is well defined and is @xmath184-lipschitz continuous ( using the norm @xmath185 ) , where @xmath186 .    [ [ par : convergence_rate ] ] convergence rate .",
    "+ + + + + + + + + + + + + + + + +    when the fw - corner function  @xmath180 is lipschitz continuous ( by theorem  [ thm : s_lip ] ) , we can actually show that the fw gap is decreasing in the fw direction and get a similar inequality as the standard fw one  , but , in this case , on the _ gaps _ : @xmath187 .",
    "moreover , one can show that the fw gap on a strongly convex set @xmath13 can be lower - bounded by @xmath188 ( lemma  [ lemma : lower_gap ] in appendix  [ sec : strong_conv_proof ] ) , by using the fact that  @xmath13 contains a ball of sufficient radius around the midpoint between @xmath189 and @xmath190 .",
    "from these two facts , we can prove the following linear rate of convergence ( _ not _ requiring any _ strong _ convex - concavity of  @xmath2 ) .",
    "[ thm : conv_strong ] let @xmath2 be a convex - concave function and @xmath13 and @xmath14 two compact @xmath171-strongly convex sets .",
    "assume that the gradient of @xmath2 is @xmath3-lipschitz continuous and that there exists @xmath191 such that @xmath192 .",
    "set @xmath193 .",
    "then the gap @xmath110   of the sp - fw algorithm with step size @xmath194 converges linearly as @xmath195 , where @xmath196 .",
    "[ [ par : the_fictitious_play _ ] ] fictitious play . + + + + + + + + + + + + + + + +    in her thesis , @xcite pointed out that for the bilinear setting : @xmath197 where @xmath198 is the probability simplex on @xmath199 elements , the sp - fw algorithm with step size @xmath200 is equivalent to the fictitious play ( fp ) algorithm introduced by @xcite .",
    "the fp algorithm has been widely studied in the game literature .",
    "its convergence has been proved by @xcite , while @xcite showed that one can deduce from robinson s proof a @xmath201 rate . around the same time",
    ", @xcite conjectured that the fp algorithm converged at the better rate of @xmath202 , though this conjecture is still open and shapiro s rate is the only one we are aware of .",
    "interestingly , @xcite recently showed that shapiro s rate is also a lower bound if the tie breaking rule gets the worst pick an infinite number of times .",
    "nevertheless , this kind of adversarial tie breaking rule does not seems realistic since this rule is a priori defined by the programmer . in practical cases ( by setting a fixed prior order for ties or picking randomly for example ) , karlin s conjecture  @xcite is still open .",
    "moreover , we always observed an empirical rate of at least @xmath202 during our experiments , we thus believe the conjecture to be true for realistic tie breaking rules .",
    "[ [ par : rate for sp - fw . ] ] rate for sp - fw .",
    "+ + + + + + + + + + + + + + +    via the affine invariance of the fw algorithm and the fact that every polytope with @xmath199 vertices is the affine transformation of a probability simplex of dimension @xmath199 , any rate for the fictitious play algorithm implies a rate for sp - fw .",
    "[ cor : bilin ] for polytopes @xmath13 and @xmath14 with @xmath199 and @xmath203 vertices respectively and @xmath204 , the sp - fw algorithm with step size @xmath205 converges at the rate @xmath206    this ( very slow ) convergence rate is mainly of theoretical interest , providing a safety check that the algorithm actually converges .",
    "moreover , if karlin s strong conjecture is true , we can get a @xmath207 worst case rate which is confirmed by our experiments .",
    "0.32    0.32    0.32     +    0.32    0.32    0.32    [ [ par : experiments_on_toy_examples ] ] toy experiments .",
    "+ + + + + + + + + + + + + + + +    first , we test the empirical convergence of our algorithms on a simple saddle point problem over the unit cube in dimension  @xmath208 ( whose pyramidal width has the explicit value @xmath209 by lemma  4 from @xcite ) .",
    "thus @xmath210^d$ ] and the linear minimization oracle is simply @xmath211 .",
    "we consider the following objective function : @xmath212 for which we can control the location of the saddle point @xmath213 .",
    "we generate a matrix @xmath114 randomly as @xmath214^{d\\times d})$ ] and keep it fixed for all experiments . for the interior point setup  , we set  @xmath215^{2d})$ ] , while we set  @xmath216 and @xmath217 to some fixed random vertex of the unit cube for the setup  . with all these parameters fixed ,",
    "the constant @xmath218 is a function of @xmath219 only .",
    "we thus vary the strong convexity parameter @xmath219 to test various @xmath218 s .",
    "we verify the linear convergence expected for the sp - fw algorithm for case   in figure  [ fig : conv_int ] , and for the sp - afw algorithm for case   in figure  [ fig : conv_geom ] . as the adaptive step size ( and rate )",
    "depends linearly on  @xmath218 , the linear rate becomes quite slow for small @xmath218 . in this regime ( in red ) , the step size @xmath220 ( in orange ) can actually perform better , despite its theoretical sublinear rate .",
    "finally , figure [ fig : better_step_size ] shows that we can observe a linear convergence of sp - afw even if @xmath218 is negative by using a different step size . in this case",
    ", we use the heuristic adaptive step size @xmath221 where @xmath222 . here",
    "@xmath223 takes into account the coupling between the concave and the convex variable and is motivated from a different proof of convergence that we were not able to complete .",
    "the empirical linear convergence in this case is not yet supported by a complete analysis , highlighting the need for more sophisticated arguments .",
    "[ [ graphical - games . ] ] graphical games .",
    "+ + + + + + + + + + + + + + + +    we now consider a bilinear objective @xmath12 where exact projections on the sets is intractable , but we have a tractable lmo .",
    "the problem is motivated from the following setup .",
    "we consider a game between two universities ( @xmath224 and @xmath225 ) that are admitting @xmath226 students and have to assign pairs of students into dorms .",
    "if students are unhappy with their dorm assignments , they will go to the other university .",
    "the game has a payoff matrix @xmath114 belonging to @xmath227 where @xmath228 is the expected tuition that @xmath225 gets ( or @xmath224 gives up ) if @xmath224 pairs student @xmath229 with @xmath230 and @xmath225 pairs student @xmath231 with @xmath232 . here",
    "the actions @xmath15 and @xmath112 are both in the marginal polytope of all perfect unipartite matchings .",
    "assume that we are given a graph @xmath233 with vertices @xmath234 and edges @xmath235 .",
    "for a subset of nodes @xmath236 , let the induced subgraph @xmath237 .",
    "@xcite showed that any subgraph forming a triangle can contain at most one edge of any perfect matching .",
    "this forms an exponential set of linear equalities which define the matching polytope @xmath238 as @xmath239 while this strategy space seems daunting , the lmo can be solved in @xmath240 time using the blossom algorithm  @xcite .",
    "we run the sp - fw algorithm with @xmath241 on this problem with @xmath242 students for @xmath243 with results given in figure  [ fig : graphical ] ( @xmath244 in the legend represents the dimensionality of the @xmath15 and @xmath112 variables ) .",
    "the order of the complexity of the lmo is then @xmath245 . in figure",
    "[ fig : graphical ] , the observed empirical rate of the sp - fw algorithm ( using @xmath241 ) is @xmath179 .",
    "empirically , faster rates seem to arise if the solution is at a corner ( a pure equilibrium , to be expected for random payoff matrices in light of  @xcite ) .",
    "[ [ par : structured_svm ] ] sparse structured svm .",
    "+ + + + + + + + + + + + + + + + + + + + + +    we finally consider a challenging optimization problem arising from structured prediction .",
    "we consider the saddle point formulation  @xcite for a @xmath246-regularized structured svm objective that minimizes the primal cost function @xmath247 , where @xmath248 is the structured hinge loss ( using the notation from  @xcite ) .",
    "we only assume access to the linear oracle computing @xmath249 .",
    "let @xmath250 have @xmath251 as columns .",
    "we can rewrite the minimization problem as a bilinear saddle point problem : @xmath252 projecting onto @xmath253 is normally intractable as the size of @xmath254 is exponential , but the linear oracle is tractable by assumption .",
    "we performed experiments with 100 examples from the ocr dataset ( @xmath255 )  @xcite .",
    "we encoded the structure @xmath256 of the @xmath257 word with a markov model : its @xmath258 character @xmath259 only depends on @xmath260 and @xmath261 . in this case , the oracle function is simply the viterbi algorithm  @xcite .",
    "the average length of a word is approximately 8 , hence the dimension of @xmath256 is @xmath262 leading to a large dimension for @xmath14 , @xmath263 .",
    "we run the sp - fw algorithm with step size @xmath264 for which we have a convergence proof ( corollary  [ cor : bilin ] ) , and with @xmath265 , which normally gives better results for fw optimization .",
    "we compare with the projected subgradient method ( projecting on the @xmath246-ball is tractable here ) with step size @xmath266 ( the subgradient of @xmath249 is @xmath267 ) .",
    "following  @xcite , we also implement a block - coordinate ( sp - bcfw ) version of sp - fw and compare it with the stochastic projected subgradient method ( ssg ) . as some of the algorithms only work on the primal and to make our result comparable to  @xcite",
    ", we choose to plot the primal suboptimality error @xmath268 for the different algorithms in figure  [ fig : beta_small ] and  [ fig : beta_big ] ( the @xmath269 iterates for the sp approaches are thus ignored in this error ) .",
    "the performance of sp - bcfw is similar to ssg when we regularize the learning problem heavily ( figure  [ fig : beta_small ] ) .",
    "however , under lower regularization ( figure  [ fig : beta_big ] ) , ssg ( with the correct step size scaling ) is faster .",
    "this is consistent with the fact that @xmath270 implies larger errors on the primal suboptimality for the sp methods , but we note that an advantage of the sp - fw approach is that the scale of the step size is automatically chosen .",
    "[ [ sec : conclusion ] ] conclusion .",
    "+ + + + + + + + + + +    we proposed fw - style algorithms for saddle - point optimization with the same attractive properties as fw , in particular only requiring access to a lmo .",
    "we gave the first convergence result for a fw - style algorithm towards a saddle point over polytopes by building on the recent developments on the linear convergence analysis of afw .",
    "however , our experiments let us believe that the condition @xmath271 is not required for the convergence of fw - style algorithms .",
    "we thus conjecture that a refined convergence analysis could yield a linear rate for the general uniformly strongly convex - concave functions in both cases   and  , paving the way for further theoretical work .",
    "thanks to n. ruozzi and a. benchaouine for helpful discussions .",
    "work supported in part by darpa n66001 - 15 - 2 - 4026 , n66001 - 15-c-4032 and nsf iii-1526914 , iis-1451500 , ccf-1302269 .",
    "appendix    [ [ par : outline ] ] outline .",
    "+ + + + + + + +    appendix  [ sec : away_step_frank_wolfe ] provides more details about the saddle point away - step frank - wolfe ( sp - afw ) algorithm .",
    "appendix  [ sec : affine_invariant ] is about the affine invariant formulation of our algorithms , therein , we introduce some affine invariant constants and prove relevant bounds .",
    "appendix  [ sec : relations_gaps ] presents some relationships between the primal suboptimalities and dual gaps useful for the convergence proof .",
    "appendix  [ appendix : analysis ] gives the affine invariant convergence proofs of sp - fw and sp - afw in the strongly convex function setting introduced in section  [ sec : sp - fw_strong_convexity ] .",
    "appendix  [ sec : strong_conv_proof ] gives the proof of linear convergence of sp - fw in the strongly convex set setting as defined in section  [ sec : strongly_set ] .",
    "finally , appendix  [ sec : details_on_the_experiments ] provides details on the experiments .",
    "in this section , we describe our algorithms sp - afw and sp - pfw with a main focus on how the away direction is chosen .",
    "we also rigorously define a _",
    "drop step _ and prove an upper bound on their number . in this section , we will assume that there exist two finites sets @xmath31 and @xmath32 such that @xmath272 and @xmath273 .",
    "[ [ par : active_set ] ] active sets .",
    "+ + + + + + + + + + + +    our definition of _ active set _ is an extension of the one provided in @xcite , we follow closely their notation and their results .",
    "assume that we have the current expansion , @xmath274 and a similar one for @xmath275 .",
    "then , the current iterate has a sparse representation as a convex combination of all possible pairs of atoms belonging to @xmath276 and @xmath277 , i.e. @xmath278 the set @xmath279 is the current ( implicit ) _ active set_. note that after @xmath26 iteration , the current iterate @xmath30 is @xmath26-sparse whereas the size of the active set @xmath279 defined in   can be @xmath280 .",
    "fortunately , we only need to track at most @xmath26 corners in @xmath31 and @xmath26 ones in @xmath32 to get this bigger active set . we can now define the maximal step size for an away direction .",
    "[ [ par : maximal_step size ] ] maximal step size .",
    "+ + + + + + + + + + + + + + + + + +    for the standard afw algorithm , @xcite suggest to use the maximum step size @xmath281 when using the away direction @xmath282 , to guarantee that the next iterate stays feasible .",
    "because we have a product structure of two blocks , we actually consider more possible away directions by maintaining a separate convex combination on each block in our algorithm  [ alg : afw ] ( sp - afw ) and  [ alg : pfw ] ( sp - pfw ) .",
    "more precisely , suppose that we have @xmath283 and @xmath284 , then the following maximum step size @xmath285 ( for afw ) ensures that the iterate @xmath286 stays feasible : @xmath287      \\quad \\text{and } \\quad       \\gamma_{\\max } : = \\min\\left\\{\\frac{\\alpha^{(t)}_{{\\bm{v}^{(t)}}_x}}{1 -\\alpha^{(t)}_{{\\bm{v}^{(t)}}_x}},\\frac{\\alpha^{(t)}_{{\\bm{v}^{(t)}}_y}}{1 -\\alpha^{(t)}_{{\\bm{v}^{(t)}}_y } } \\right\\}.\\ ] ] a larger @xmath288 makes one of the coefficients in the convex combination for the iterate negative , thus no more guaranteeing that the iterate stays feasible . a similar argument",
    "can be used to derive the maximal step size for the pfw direction in algorithm  [ alg : pfw ] .",
    "[ [ par : drop_steps ] ] drop steps .",
    "+ + + + + + + + + + +    a _ drop step _ is when @xmath289 for the away - step update   @xcite . in this case",
    ", at least one corner is removed from the active set .",
    "we show later in lemma  [ lemme : lin ] that we can still guarantee progress for this step , i.e. @xmath290 , but this progress be arbitrarily small since @xmath291 can be arbitrarily small .",
    "@xcite shows that the number of drop steps for afw is at most half of the number of iterations . because we are maintaining two independent active sets in our formulation",
    ", we can obtain more drop steps , but we can still adapt their argument to obtain that the number of drop steps for sp - afw is at most two thirds the number of iterations ( assuming that the algorithm is initialized with only one atom per active set ) . in the sp - afw algorithm , either a fw step is jointly made on both blocks , or an away - step is done on both blocks . let us call  @xmath292 the number of fw steps ( which potentially adds an atom in @xmath293 and @xmath294 ) and @xmath295 ( resp @xmath296 ) the number of steps that removed at least one atom from @xmath293 ( @xmath294 ) . finally , we call @xmath297 the number of _ drop steps _ ,",
    "i.e. , the number of _ away steps _ where at least one atom from @xmath293 or @xmath294 have been removed ( and thus @xmath70 for these ) .",
    "because a step is either a fw step or an away step , we have : @xmath298 we also have that @xmath299 by definition of @xmath297 . because a fw step adds at most one atom in an active set",
    "while a drop step removes one , we have ( supposing that @xmath300 ) : @xmath301 adding these two relations , we get : @xmath302 using the fact that each active set as at least one element",
    ". we thus obtain @xmath303 . combining with  ,",
    "we get : @xmath304 as claimed .",
    "in this section , we define the affine invariant constants of a convex function @xmath97 and their extension to a convex - concave function @xmath2 . these constants are important as the fw - type algorithms are affine invariant if their step size are defined using affine invariant quantities .",
    "we can upper bound these constants using the non affine invariant constants defined in the main paper .",
    "hence a convergence rate with affine invariant constants will immediately imply a rate with the constant introduced in the main paper .",
    "we define the lipschitz constant  @xmath3 of the gradient of the function @xmath97 with respect to the norm  @xmath305 by using a dual pairing of norms , i.e. @xmath3 is a constant such that @xmath306 where @xmath307 is the dual norm of @xmath308 . for a convex - concave function ,",
    "we also consider the partial lipschitz constants with respect to different blocks as follows .    for more generality , we consider the dual pairing of norms @xmath309 on @xmath13 , and similarly @xmath310 on @xmath14 .",
    "we also define the norm on the product space @xmath25 as the @xmath246-norm on the components : @xmath185 .",
    "we thus have that the dual norm of @xmath25 is the @xmath311-norm of the dual norms : @xmath312 .",
    "the _ partial _ lipschitz constants @xmath313 of the gradient of the function @xmath2 with respect to these norms are the constants such that for all @xmath314 and @xmath315 , @xmath316 note that the cross partial lipschitz constants @xmath134 and @xmath135 do not necessarily use a dual pairing as @xmath13 and @xmath14 could be very different spaces . on the other hand , as the possibilities in   are special cases of   when considering the @xmath246-norm of this product domain , one can easily deduce that the partial lipschitz constants can always be taken to be smaller than the full lipschitz constant for the gradient of @xmath2 , i.e. , we have that @xmath317 .      to prove the convergence of the frank - wolfe algorithm , the typical affine invariant analysis proof in the fw literature assumes that the curvature of the objective function is bounded , where the curvature is defined by  @xcite for example .",
    "we give below a slight generalization of this curvature notion in order to handle the convergence analysis of fw with away-steps.the change is to consider the more general directions @xmath318 instead of just @xmath319 , and also any feasible positive step size .",
    "see also footnote  8 in   for a related discussion . a different ( bigger ) constant",
    "was required in  @xcite for the analysis of afw because they used a line - search . ]",
    "it has the same upper bound as the traditional curvature constant ( see proposition  [ prop : c_fbounded ] ) .",
    "[ [ curvature . ] ] curvature .",
    "+ + + + + + + + + +    [ slight generalization of  @xcite ] [ par : curvature ] let @xmath320 be a convex function , we define the curvature @xmath321 of @xmath97 as @xmath322 note that only the _ feasible _ step sizes  @xmath323 are considered in the definition of  @xmath324 , i.e. , @xmath323 such that @xmath325 . if the gradient of the objective function is lipschitz continuous , the curvature is upper bounded .",
    "[ prop : c_fbounded ] let @xmath97 be a convex and continuously differentiable function on  @xmath13 with its gradient @xmath326 l - lipschitz continuous w.r.t .",
    "some norm @xmath172 in dual pairing over the domain @xmath13 .",
    "then @xmath327 where @xmath328 is the diameter of @xmath13 .",
    "let @xmath329 , set @xmath330 and @xmath331 for some @xmath332 such that @xmath325 . then by the fundamental theorem of calculus , @xmath333 hence , we can write @xmath334 thus for all @xmath335 and @xmath336 for @xmath332 such that @xmath325 , we have @xmath337 the supremum is then upper bounded by the claimed quantity .",
    "illustrate well the importance of the affine invariant curvature constant for frank - wolfe algorithms in their paragraph titled  lipschitz and curvature constants \" .",
    "they provide a concrete example where the wrong choice of norm for a specific domain  @xmath13 can make the upper bound of proposition  [ prop : c_fbounded ] extremely loose , and thus practically useless for an analysis .",
    "we will therefore extend the curvature constant to the convex - concave function @xmath2 by simply defining it as the maximum of the curvatures of the functions belonging to the family @xmath338 ( see section  [ sub : curvature_and_interior_strong_convexity_constant_for_a_convex_concave_function ] ) .",
    "but before that , we review affine invariant analogues of the strong convexity constants that will be useful for the analysis .      in this section ,",
    "we review two affine invariant measures of strong convexity that were proposed by  @xcite for the affine invariant linear convergence analysis of the standard frank - wolfe algorithm ( using the `` interior strong convexity constant '' ) or the away - step frank - wolfe algorithm ( using the `` geometric strong convexity constant '' ) .",
    "we will re - use them for the affine invariant analysis of the convergence of sp - fw or sp - afw algorithms . in a similar way as the curvature constant  @xmath324 includes information about the constraint set  @xmath13 and the lipschitz continuity of the gradient of  @xmath97 together , these constants both include the information about the constraint set  @xmath13 and the strong convexity of a function  @xmath97 together .    [",
    "[ par : interior_strong_convexity_constant _ ] ] interior strong convexity constant .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    [ based on ] let @xmath339 be a point in the relative interior of @xmath13 .",
    "the _ interior strong convexity constant _ for  @xmath97 with respect to the reference point  @xmath339 is defined as @xmath340 , \\\\                    { \\bm{z}}= { \\bm{x}}+ \\gamma({\\bm{s}}-{\\bm{x } } )          \\end{matrix}$          } } }         \\frac{2}{\\gamma^2 } \\left(f({\\bm{z}})-f({\\bm{x}})- { \\left\\langle{\\bm{z}}-{\\bm{x } } , \\nabla f({\\bm{x}})\\right\\rangle}\\right).\\ ] ] here , we follow the notation of and take the point @xmath341 to be the point where the ray from @xmath15 to the reference point @xmath339 pinches the boundary of the set @xmath13 , i.e. @xmath342 , where @xmath343 is the boundary of the convex set  @xmath13 .",
    "we note that in the original definition  , @xmath339 was the ( unique ) optimum point for a strongly convex function  @xmath97 over  @xmath13 .",
    "the optimality of  @xmath339 is actually not needed in the definition and so we generalize it here to any point  @xmath339 in the relative interior of  @xmath13 , as this will be useful in our convergence proof for sp - fw .    for completeness ,",
    "we include here the important lower bound from   on the interior strong convexity constant in terms of the strong convexity of the function  @xmath97 .",
    "[ prop : delta ] let @xmath97 be a convex differentiable function and suppose that @xmath97 is strongly convex w.r.t . to some arbitrary norm @xmath344 over the domain  @xmath13 with strong - convexity constant @xmath345 . furthermore , suppose that the reference point @xmath339 lies in the relative interior of  @xmath13 , i.e. , @xmath346 .",
    "then the interior strong convexity constant  @xmath347   is lower bounded as follows : @xmath348    let @xmath15 and @xmath349 be defined as in  , i.e. , @xmath350 for some @xmath332 and where @xmath341 intersects the boundary of @xmath13 with the ray going from @xmath15 to @xmath339 . by the strong convexity of  @xmath97 , we have @xmath351 from the definition of @xmath341 , we have that @xmath339 lies between @xmath15 and @xmath341 and thus : @xmath352 . combining with",
    ", we conclude @xmath353 and therefore @xmath354 we now present the affine invariant constant used in the global linear convergence analysis of frank - wolfe variants when the convex set  @xmath13 is a polytope .",
    "the _ geometric strong convexity constant _ was originally introduced by   and  @xcite . to avoid any ambiguity , we will re - use their definitions verbatim in the rest of this section , starting first with a few geometrical definitions and then presenting the affine invariant constant . in these definitions , they assume that a _ finite _",
    "set  @xmath31 of vectors ( that they call _ atoms _ ) is given such that  @xmath272 ( which always exists when  @xmath13 is a polytope ) .",
    "[ [ directional - width . ] ] directional width .",
    "+ + + + + + + + + + + + + + + + + +    [ @xcite ] the _ directional width _ of a set @xmath31 with respect to a direction @xmath355 is defined as @xmath356 .",
    "the _ width _ of  @xmath31 is the minimum directional width over all possible directions in its affine hull .",
    "[ [ pyramidal - directional - width . ] ] pyramidal directional width .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    [ @xcite ] we define the _ pyramidal directional width _ of a set @xmath31 with respect to a direction @xmath355 and a base point @xmath357 to be @xmath358 where @xmath359 such that @xmath15 is a proper convex combination of all the elements in @xmath360 , and @xmath361 is the fw atom used as a summit , when using the convention in this section that @xmath362 .    [ [ pyramidal - width . ] ] pyramidal width .",
    "+ + + + + + + + + + + + + + + +    [ @xcite ] to define the pyramidal width of a set , we take the minimum over the cone of possible _ feasible _ directions  @xmath355 ( in order to avoid the problem of zero width ) .",
    "+ a direction  @xmath355 is _ feasible _ for @xmath31 from @xmath15 if it points inwards @xmath363 , ( i.e. @xmath364 ) .",
    "+ we define the _ pyramidal width _ of a set @xmath31 to be the smallest pyramidal width of all its faces , i.e. @xmath365    [ [ geometric - strong - convexity - constant . ] ] geometric strong convexity constant .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    [ @xcite ] [ par : strong _ ] the _ geometric strong convexity constant _ of  @xmath97 ( over the set of atoms @xmath31 which is left implicit ) is : @xmath366 where @xmath367 and @xmath272 .",
    "the quantity @xmath368 represents the fw corner picked when running the fw algorithm on  @xmath97 when at  @xmath15 ; while @xmath369 represents the worst - case possible away atom that afw could pick ( and this is where the dependence on @xmath31 appears ) .",
    "we now define these quantities more precisely .",
    "recall that the set of possible active sets is @xmath359 such that @xmath15 is a proper convex combination of all the elements in @xmath360 . for a given",
    "set  @xmath370 , we write @xmath371 for the away atom in the algorithm supposing that the current set of active atoms is @xmath370 .",
    "finally , we define @xmath372 to be the worst - case away atom ( that is , the atom which would yield the smallest away descent ) .",
    "an important property coming from this definition that we will use later is that for @xmath373 and @xmath374 being possible fw and away atoms ( respectively ) appearing during the afw algorithm ( consider algorithm  [ alg : afw ] ran only on  @xmath13 ) , then we have : @xmath375    the following important theorem from  @xcite lower bounds the geometric strong convexity constant of  @xmath97 in terms of both the strong convexity constant of @xmath97 , as well as the pyramidal width of  @xmath376 defined as @xmath377  .",
    "[ thm : mufdirwinterpretation2 ] let @xmath97 be a convex differentiable function and suppose that @xmath97 is @xmath219-_strongly convex _ w.r.t . to the euclidean norm @xmath378 over the domain @xmath379 with strong - convexity constant @xmath380 . then @xmath381    the pyramidal width   is a geometric quantity with a somewhat intricate definition .",
    "its value is still unknown for many sets ( though always strictly positive for finite sets ) , but ( * ? ? ?",
    "* lemma  4 ) give its value for the unit cube in @xmath104 as @xmath382 .",
    "in this subsection , we propose simple convex - concave extensions of the definitions of the affine invariant constants defined introduced in the two previous sections .",
    "to define the convex - concave curvature , we introduce the sets @xmath383 and @xmath384 of the marginal convex functions .",
    "@xmath385 let @xmath386 a convex - concave function , we define the curvature pair @xmath387 of @xmath2 as @xmath388 and the curvature of @xmath2 as @xmath389 an upper bound on this quantity follows directly from the upper bound on the convex case ( lemma  7 of @xcite , repeated in our proposition  [ prop : c_fbounded ] ) :    [ prop : cbounded ] let @xmath386 be a differentiable convex - concave function . if @xmath13 and @xmath14 are compact and @xmath390 is lipschitz continuous , then the curvature of @xmath2 is bounded by @xmath391 , where @xmath392 ( resp @xmath393 ) is the largest lipschitz constant respect to @xmath15 ( @xmath112 ) of @xmath394 ( @xmath395 ) .",
    "let @xmath97 in @xmath383 , @xmath396 similarly , let @xmath397 in @xmath398 , @xmath399 consequently , @xmath400 where @xmath401 and @xmath402 are the respective diameter of @xmath13 and @xmath14 .",
    "note that @xmath392 and @xmath393 are upper bounded by the global lipschitz constant of @xmath136 .",
    "similarly , we define various notions of strong convex - concavity in the following .    [ [ par : uniform_strong_convex_concavity_constant ] ] uniform strong convex - concavity constant .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the uniform strong convex - concavity constants is defined as @xmath403 where @xmath404 is the strong convexity constant of @xmath97 and @xmath405 the strong convexity of @xmath397 .    under some assumptions",
    "this quantity is positive .",
    "[ prop : strongpos ] if the second derivative of @xmath2 is continuous , @xmath13 and @xmath14 are compact and if for all @xmath406 , then @xmath407 and @xmath151 are positive .",
    "let us introduce @xmath408 the hessian of the function @xmath409 .",
    "we want to show that the smallest eigenvalue is uniformly bounded on @xmath131 .",
    "we know that the smallest eigenvalue lower bounds @xmath407 , @xmath410 but @xmath411 is continuous ( because @xmath412 is continuous by assumption ) and then the function @xmath413 is continuous .",
    "hence since @xmath131 and the unit ball are compact , the infimum is a minimum which ca nt be 0 by assumption .",
    "hence @xmath407 is positive .",
    "doing the same thing with the smallest eigenvalue of @xmath414 , we get that @xmath415 .",
    "a common family of saddle point objectives is of the form @xmath416 . in this case , we get simply that @xmath417 an equivalent definition for the uniform strong convex - concavity constant is : @xmath2 is @xmath116-uniform strongly convex - concave function if @xmath418 is convex - concave .",
    "the following proposition relates the distance between the saddle point and the values of the function .",
    "it is a direct consequence from the uniform strong convex - concavity definition  .",
    "[ prop : strongl ] let @xmath2 be a uniformly strongly convex - concave function and @xmath119 the saddle point of @xmath2",
    ". then we have for all @xmath15 in @xmath13 and @xmath5 , @xmath419    the saddle point @xmath119 is the optimal point of the two strongly convex functions @xmath420 and the function @xmath421 , so we can use the property of strong convexity on each function and the fact that @xmath407 lower bounds the strong convexity constant of @xmath422 ( and similarly for @xmath151 with @xmath423 ) as per the definition  , to get the required conclusion .",
    "now we will introduce the uniform strong convex - concavity constants relatively to our saddle point .",
    "[ [ par : interior_strong_convex_concavity ] ] interior strong convex - concavity .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the sp - fw interior strong convex - concavity constants ( with respect to the reference point @xmath424 ) are defined as : @xmath425 where @xmath347 is the interior strong convexity constant of @xmath97 w.r.t to the point @xmath339 and @xmath426 is the interior strong convexity constant w.r.t to the point @xmath427 .",
    "the sets @xmath428 and @xmath384 are defined in  .",
    "we also define the smallest quantity of both ( with the reference point @xmath424 implicit ) : @xmath429 we can lower bound this constant by a quantity depending on the uniform strong convexity constant and the distance of the saddle point to the boundary . the propositions on the strong convex - concavity directly follow from the previous definitions and the analogous proposition on the convex case ( proposition  [ prop : delta ] )    [ prop : deltal ] let @xmath2 be a convex - concave function .",
    "if the reference point @xmath430 belongs to the relative interior of @xmath25 and if the function @xmath2 is strongly convex - concave with a strong convex - concavity constant @xmath431 , then @xmath432 is lower bounded away from zero .",
    "more precisely , define @xmath433 and @xmath434 .",
    "then we have , @xmath435    using the proposition  [ prop : delta ] we have , @xmath436 and , @xmath437 when the saddle point is not in the interior of the domain , we define next a constant that takes in consideration the geometry of the sets .",
    "if the sets are polytopes , then this constant is positive .",
    "[ [ par : geometric_strong_convex_concavity ] ] geometric strong convex - concavity .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the sp - fw _ geometric _ strong convex - concavity constants are defined analogously as the interior strong convex - concavity constants , @xmath438 where @xmath439 is the geometric strong convexity constant of @xmath440 ( over @xmath31 ) as defined in   ( and similarly @xmath441 is the geometric strong convexity constant of @xmath442 over @xmath32 ) .",
    "it is straightforward to notice that the lower bound on the geometric strong convexity constant ( proposition  [ thm : mufdirwinterpretation2 ] ) can be extended to the geometric strong convex - concavity constants ( where @xmath407 and @xmath151 are now assumed to be defined with respect to the euclidean norm ) : @xmath443      in our proof , we need to relate the gradient at the point @xmath444 with the one at the point @xmath445 .",
    "we can use the lipschitz continuity of the gradient for this .",
    "we define below affine invariant quantities that can upper bound this difference .    [",
    "[ par : bilinearity_coefficients ] ] bilinearity coefficients .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath2 be a strongly convex - concave function , and let @xmath119 be its unique saddle point .",
    "we define the bilinearity coefficients @xmath446 as , @xmath447 and , @xmath448 we also define the global bilinearity coefficient as @xmath449 we can upper bound these affine invariant constants with the lipschitz constant of the gradient , the uniform strong convex - concavity constants and the diameters of the sets .",
    "[ mbounded ] if @xmath13 and @xmath14 are compact , @xmath136 is lipschitz continuous and @xmath2 is uniformly strongly convex - concave with constants @xmath450 , then @xmath451 where @xmath134 and @xmath135 are the _ partial _ lipschitz constants defined in equation  . the quantity @xmath401 is the diameter of the compact set @xmath13 and @xmath402 is the diameter of @xmath14 .",
    "@xmath452 then using the relation between @xmath453 and @xmath454 due to strong convexity ( proposition  [ prop : strongl ] ) @xmath455 we use a similar argument for @xmath456 which allows us to conclude .      in this section",
    ", we are going to show that if the objective function @xmath2 is uniformly strongly convex - concave , then we have a relation between @xmath81 and @xmath118 .",
    "first let us introduce affine invariant constants to relate these quantities ( in the context of a given saddle point @xmath457 ) : @xmath458 where @xmath459 and @xmath460 .",
    "we also define : @xmath461 these constants can be upper bounded by easily computable constants .    for any @xmath116-uniformly convex - concave function @xmath2 , [ prop : upper_bound_p_l ] @xmath462",
    "let us start from the definition of @xmath463 , let @xmath7 , @xmath464 the same way we can get @xmath465 it concludes our proof .",
    "one way to compute an upper bound on the supremum of the gradient is to use any reference point @xmath466 of the set : @xmath467 we recall that @xmath392 is the largest ( with respect to @xmath112 ) lipschitz constant of @xmath394 .",
    "note that @xmath392 is upper bounded by the global lipschitz constant of @xmath136 .",
    "we can compute an upper bound on the supremum of the norm of @xmath468 the same way .    with these above defined affine invariant constants",
    ", we can finally relate the two primal suboptimalities as @xmath469 .",
    "[ prop : relation_primal ] for a @xmath116-uniformly strongly convex - concave function @xmath2 , @xmath470    we will first work on @xmath471 : @xmath472 we can do the same thing for @xmath473 and @xmath122 , thus @xmath474 where the last inequality uses @xmath475 . finally , the inequality on @xmath476 is from proposition  [ prop : upper_bound_p_l ] .",
    "recall that we introduced @xmath477 and similarly @xmath478 then the primal suboptimality is the positive quantity @xmath479 to get a convergence rate , one has to upper bound the primal suboptimality defined in  , but it is hard to work with the moving quantities @xmath480 and @xmath481 in the analysis .",
    "this is why we use in our analysis a different merit function that uses the ( fixed ) saddle point @xmath119 of @xmath2 in its definition .",
    "we recall its definition below .    [",
    "[ par : second_primal_gap ] ] second primal suboptimality .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    we define the second primal suboptimality for @xmath2 of the iterate @xmath482 with respect to the saddle point @xmath119 as the positive quantity : @xmath483 it follows from @xmath484 and @xmath485 that @xmath123 .",
    "furthermore , under the assumption of uniform strong convex - concavity , we proved in proposition  [ prop : relation_primal ] that the square root of @xmath118 upper bounds @xmath81 up to a constant .      in this section",
    ", we will prove the crucial inequalities relating suboptimalities and the gap function .",
    "let s recall the definition of @xmath373 and @xmath374 : @xmath486 where @xmath487 . also , the following various gaps are defined as @xmath488 where @xmath489 and @xmath490 . the direction",
    "@xmath491 is the direction chosen by the algorithm at step @xmath26 : it is always @xmath492 for sp - fw , and can be either @xmath492 or @xmath493 for sp - afw . even if the definitions of these gaps are different , the formalism for the analysis of the convergence of both algorithms is going to be fairly similar .",
    "it is straightforward to notice that @xmath494 and one can show that the current gap @xmath495 is lower bounded by half of @xmath496 :    [ lemma : gap_fw_pw ] for the sp - afw algorithm , the current gap @xmath495 can be bounded as follows : @xmath497    first let s show the rhs of the inequality , @xmath498 because both @xmath499 and @xmath500 from their definition . for the lhs inequality , we use the fact that @xmath501 for sp - afw and thus : @xmath502 in the following , we will assume that we are in one of the two following cases : @true@leqno @xmath503 @xmath504 @false@eqno    then either @xmath505 ( case  [ enu : situation1 ] ) or @xmath506 ( case  [ enu : situation2 ] ) .",
    "let s write the gap function as the sum of two smaller gap functions : @xmath507 because of the convex - concavity of @xmath2 , this scalar product bounds the differences between the value of @xmath2 at the point @xmath444 and the value of @xmath2 at another point .",
    "hence this gap function upper - bounds @xmath81 and @xmath118 defined in   and  .",
    "more concretely , we have the following lemma .    [ lemme : g ] for all @xmath26 in @xmath508 , @xmath7 and @xmath509 @xmath510 and ,",
    "furthermore , @xmath511    first let s show the lhs of , @xmath512 because one can easily derive that @xmath499 from the definition of the away direction @xmath513 .",
    "it follows from convexity of @xmath514 that for all @xmath15 in @xmath13 , @xmath515 a similar inequality emerges through the convexity of @xmath516 , @xmath517 which gives us @xmath518 which shows  . by using @xmath519 and @xmath520 in  , we get @xmath521 .",
    "we also know that @xmath522 for sp - afw . so combining with @xmath523 that we already knew , we get  .",
    "next , we recall two lemmas , one from   and the other one from @xcite .",
    "these lemmas upper bound the primal suboptimality with the square of the gap times a constant depending on the geometric ( or the interior ) strong convexity constant .",
    "[ lemme : lingap ] if @xmath97 is strongly convex , then for any @xmath524 , @xmath525 and @xmath526 where @xmath527 , @xmath528 and @xmath529 .",
    "notice once again that in this lemma  _ we do not need @xmath339 to be optimal_. let @xmath530 . using the definition of interior strong convexity   and choosing @xmath531 such that @xmath532 ,",
    "we get @xmath533 the last line of this derivation is obtained through the inequality : @xmath534 . if @xmath535 the inequality is just the positivity of the gap .    for the second statement",
    ", we will use the definition of the geometric strong convexity constant ( equation  ) at the point @xmath536 and @xmath537 . recall that @xmath538 . @xmath539",
    "lemma  [ lemme : lingap ] is useful to understand the following lemma and its proof which is just an extension to the convex - concave case .",
    "[ lemme : gap ] if @xmath2 is a strongly convex - concave function , then for any @xmath540 , @xmath541 where the gaps are defined in , @xmath542 ( i.e. using the reference points @xmath543 in the definition  ) and @xmath544 is the geometric strong convex - concavity of @xmath2 over @xmath545 , as defined in  .    for :    let the function @xmath97 on @xmath13 be defined by @xmath546 , and the function @xmath397 on @xmath14 be @xmath547 .",
    "then using the lemma  [ lemme : lingap ] on the function @xmath97 with the reference point @xmath216 , and on @xmath397 with reference point @xmath217 , we get @xmath548 as @xmath432 is smaller than both @xmath549 and @xmath550 by the definition  , we can use it in the denominator of the above two inequalities . as we saw from section  [ app : gapinequalities ] in  , the gap can be split as sum of the gap of the block @xmath13 and the gap of the block @xmath14 , i.e. @xmath551 . then , using the inequality : @xmath552 for @xmath553 , we obtain @xmath554    for :    using the lemma  [ lemme : lingap ] for case   on the same functions @xmath97 and @xmath397 defined above , we get @xmath555 using a similar argument as the one to get  , using that @xmath544 is smaller than both @xmath556 and @xmath557 , and referring to the separation of the gap  , we get @xmath558",
    "in this section , we are going to show two important lemmas .",
    "the first one shows that under some assumptions we can get a frank - wolfe - style induction scheme relating the second suboptimality of the potential update @xmath559 , the current value of the second suboptimality @xmath118 , the gap  @xmath495 and any step size @xmath560 $ ] .",
    "the second lemma will relate the gap and the square root of @xmath118 ; this relation enables us to get a rate on the gap after getting a rate on @xmath118 .      the first lemma in this section is inspired from the standard fw progress lemma , such as lemma  c.2 in  @xcite , though it requires a non - trivial change due to the compensation phenomenon for  @xmath2 mentioned in the main text in  . in the following , we define the possible updated iterate @xmath561 for @xmath562 $ ] : @xmath563 for a fw step @xmath564 and for an away step @xmath565 .",
    "we also define the corresponding new suboptimality for @xmath566 : @xmath567    [ lemme : ineg ] let @xmath2 be strongly convex - concave , if we are in case   and @xmath568 is a fw direction , we have for any @xmath569 $ ] : @xmath570 if we are in case   and @xmath491 is defined from a step of sp - afw ( algorithm  [ alg : afw ] ) , we have for any @xmath562 $ ] : @xmath571    the beginning of the argument works with any direction @xmath491 . recall that @xmath572 and @xmath573 . now writing @xmath574 and using the definition of the curvature @xmath321   for the function @xmath575 , we get @xmath576 since @xmath577 by definition  . recall that the gap function @xmath495 can be decomposed by   into two smaller gap functions @xmath578 and @xmath579 .",
    "we define @xmath580 to be the sequence representing the error between the gradient used for the minimization and the gradient at the point @xmath445 .",
    "then , @xmath581 now , as @xmath582 is finite ( under lipschitz gradient assumption ) , we can use the definition of the bilinearity constant   to get @xmath583 combining equations and we finally obtain @xmath584 we can get an analogous inequality for @xmath585 , @xmath586 then adding @xmath587 and @xmath585 and using @xmath588 ( coming from the concavity of @xmath589 ) , we get @xmath590 we stress that the above inequality   is valid for any direction @xmath491 , using @xmath591 , and for any feasible step size @xmath323 such that @xmath592 ( the last condition was used in the definition of @xmath321 ; see also footnote  [ foot : cfcomment ] for more information ) .",
    "to finish the argument , we now use the specific property of the direction @xmath491 and use the crucial lemma  [ lemme : gap ] that relates  @xmath118 with the square of the appropriate gap .    for the case   of interior saddle point",
    ", we consider @xmath568 and thus @xmath593 . then combining lemma  [ lemme : gap ] ( using the interior strong convexity constant ) with  , we get @xmath594    for the case   of polytope domains , we consider @xmath491 as defined by the sp - afw algorithm",
    ". we thus have @xmath595 by lemma  [ lemma : gap_fw_pw ] . then combining lemma  [ lemme : gap ] ( using the geometric strong convexity constant ) with",
    ", we get @xmath596 which finishes the proof . still in the case",
    ", we also present an inequality in terms of the direction gap @xmath495 ( which yields a better constant that will be important for the sublinear convergence proof in theorem  [ thm : conv_sublin ] ) by using instead the inequality @xmath494 ( lemma  [ lemma : gap_fw_pw ] ) with lemma  [ lemme : gap ] and  : @xmath597    the above lemma uses a specific update direction  @xmath491 to get a potential new suboptimality  @xmath559 . by using the property that @xmath598 always , we can actually derive an upper bound on the gap in terms of @xmath118 _ irrespective of any algorithm _",
    "( i.e. this relationship holds for any possible feasible point @xmath444 ) .",
    "more precisely , for sp - fw algorithm ( case  ) the only thing we need to set is a feasible point @xmath30 but for the sp - afw algorithm ( case  ) we also need an active set expansion for @xmath30 for which the maximum away step size is larger than @xmath599 ( which can potentially not be the active set calculated by an algorithm ) .",
    "this is stated in the following theorem , which is a saddle point generalization of the gap upper bound given in theorem  2 of  @xcite .",
    "[ thm : subopt_gap ] if @xmath2 is strongly convex - concave and has a finite curvature constant then    * case  : for any @xmath600 , @xmath601 since @xmath30 is fixed , this statement is algorithm free .",
    "* case  : for any @xmath600 , if there exists an active set expansion for @xmath30 for which @xmath602 or @xmath603 ( see   for the definition of @xmath291 ) then , @xmath604    both statement are algorithm free but @xmath496 depends on a chosen expansion of @xmath30 : @xmath605 because , @xmath606       & \\text{where } \\quad { \\bm{d}^{(t)}}_{{\\hspace{0.05em}\\textnormal{a}}}:= { \\bm{z}^{(t)}}- \\operatorname*{\\arg\\max}_{{\\bm{v}}\\in { \\mathcal{s}}_x \\times { \\mathcal{s}}_y } { \\langle { { \\bm{r}}^{(t ) } } , { \\bm{v}}\\rangle } , \\\\       & \\text{and } \\quad { \\bm{d}^{(t)}}_{{\\hspace{0.05em}\\textnormal{fw}}}:= \\operatorname*{\\arg\\min}_{{\\bm{v}}\\in { \\mathcal{a}}\\times { \\mathcal{b } } } { \\langle { { \\bm{r}}^{(t ) } } , { \\bm{v}}\\rangle } - { \\bm{z}^{(t)}}.      \\end{aligned}\\ ] ]",
    "the maximum step size associated with the active set expansion described in equation   is @xmath607    in this proof , we let @xmath608 to stand respectively for @xmath609 for case   or @xmath610 for case  .",
    "we will start from the inequalities   and   in lemma  [ lemme : ineg ] .",
    "equation   is valid considering a fw direction @xmath611 , equation   is valid if we consider the direction that would have be set by the sp - afw algorithm if it was run at point @xmath30 with the active set expansion described in the theorem statement . since @xmath612 ,",
    "for both cases become : @xmath613 then we can put the gap on the lhs , @xmath614 this inequality is valid for any @xmath615 $ ] . in order to get the tightest bound between the gap and the suboptimality",
    ", we will maximize the lhs .",
    "it can be maximized with @xmath616 .",
    "now we have two cases :    if @xmath617 $ ] , then we get : @xmath618    and if @xmath602 and @xmath619 , then setting @xmath620 we get : @xmath621 . by taking the maximum between the two options ,",
    "we get the theorem statement",
    ". the previous theorem guarantees that the gap gets small when @xmath118 gets small ( only for the non - drop steps if in situation  ) . as we use the gap as a stopping criterion in the algorithm ,",
    "this is a useful theorem to provide an upper bound on the number of iterations needed to get a certificate of suboptimality .",
    "the following corollary provides a better bound on @xmath81 than the inequality @xmath622 previously shown when the situation is .",
    "it will be useful later to get a better rate of convergence for @xmath81 under hypothesis  .",
    "[ cor : h_w ] suppose that @xmath2 is strongly convex - concave and has a finite curvature constant , and that the domain is a product of polytopes ( i.e. we are in situation  ) .",
    "let @xmath600 be given .",
    "if there exists an active set expansion for @xmath30 for which the maximum step size is larger than @xmath599 ( see theorem   for more details ) , @xmath623 where @xmath624 is defined in  .    by lemma  [ lemme : gap ] in situation",
    ", we have : @xmath625 the last inequality is obtained by applying the upper bound on the gap given in the previous theorem  [ thm : subopt_gap ] .      in this section",
    ", we will prove that under some conditions on the constant defined in subsections  [ sub : curv ] and  [ sub : affine_invariant_measures_of_strong_convexity ] , the suboptimalities @xmath626 vanish linearly with the adaptive step size @xmath627 or sublinearly with the universal step size @xmath628 .",
    "[ lemme : lin ] let @xmath2 be a strongly convex - concave function with a smoothness constant @xmath629 , a positive interior strong convex - concavity constant @xmath432   or a positive geometric strong convex - concavity @xmath544  .",
    "let us also define the rate multipliers  @xmath218 as @xmath630 let the tuple @xmath631 refers to either @xmath632 for case   where the algorithm is sp - fw , or @xmath633 for case   where the algorithm is sp - afw ,    if @xmath271 , then at each non - drop step ( when @xmath634 or @xmath635 ) , the suboptimality @xmath118 of the algorithm with step size @xmath636 decreases geometrically as @xmath637 where @xmath638 .",
    "moreover , for case there is no drop step and for case   the number of drop step ( when @xmath639 ) is upper bounded by two third of the number of iteration ( see section  [ par : drop_steps ] , equation  ) , while when we have a drop step , we still have : @xmath640    the bulk of the proof is of a similar form for both sp - fw and sp - afw , and so in the following , we let @xmath641 to stand respectively for @xmath642 for sp - fw ( case  ) or @xmath643 for sp - afw ( case  ) . as @xmath644 , we can apply the important lemma  [ lemme : ineg ] with @xmath645 ( the actual step size that was taken in the algorithm ) to get : @xmath646 we note in passing that the adaptive step size rule @xmath647 was specifically chosen to minimize the rhs of   among the feasible step sizes .",
    "if @xmath648 , then we have @xmath649 and so   becomes :",
    "@xmath650 applying the fact that the square of the appropriate gap upper bounds @xmath118 ( lemma  [ lemme : gap ] with a similar form for both cases   and  ) , we directly obtain the claimed geometric decrease @xmath651 if @xmath652 , then we have @xmath653 and so   becomes : @xmath654 if @xmath655 ( either we are taking a fw step or an away step with a big step size ) , then the geometric rate is at least @xmath656 , which is a better rate than @xmath657 since @xmath658 as @xmath659 , and one can show that @xmath660 always ( see remark  7 in appendix d of  @xcite for case   and use a similar argument for case  ) .",
    "thus @xmath657 is valid both when @xmath661 or @xmath655 , as claimed in the theorem .",
    "when @xmath662 , we can not guarantee sufficient progress as @xmath291 could be arbitrarily small ( this can only happen for an away step as @xmath602 for a fw step ) .",
    "these are the problematic _ drop steps _ , but as explained in appendix  [ par : drop_steps ] with equation  , they can not happen too often for sp - afw .    finally , to show that the suboptimality can not increase during a drop step ( @xmath663 ) ,",
    "we point out that the function @xmath664 is a convex function that is minimized by @xmath665 and so is decreasing on @xmath666 $ ] . when @xmath70 , we have that @xmath667 , and thus the value for @xmath668 is lower than the value for @xmath669 , i.e. @xmath670 the previous lemma ( lemma  [ lemme : lin ] ) , the fact that the gap upper bounds the suboptimality ( lemma  [ lemme : g ] ) and the primal suboptimalities analysis lead us directly to the following theorem .",
    "this theorem is the affine invariant formulation with adaptive step size of theorem  [ thm : conv ] .",
    "[ thm : conv_affine_invariant ]",
    "let @xmath2 be a strongly convex - concave function with a finite smoothness constant @xmath629 , a positive interior strong convex - concavity constant @xmath432   or a positive geometric strong convex - concavity @xmath544  .",
    "let us also define the rate multipliers  @xmath218 as @xmath671 let the tuple @xmath631 refers to either @xmath632 for case   where the algorithm is sp - fw , or @xmath633 for case   where the algorithm is sp - afw ,    if @xmath672 , then the suboptimality @xmath81 of the iterates of the algorithm with step size @xmath636 decreases geometrically to get the better rate on @xmath81 losing the square root but with a potentially worse constant @xmath673 .",
    "] as , @xmath674 where @xmath675 and @xmath145 is the number of non - drop step after @xmath26 steps . for sp - fw , @xmath146 and for sp - afw , @xmath147 .",
    "moreover we can also upper bound the minimum gap observed , for all @xmath676 @xmath677 the theorem  [ thm : conv ] statement can be deduced from this theorem using the lower and upper bounds on the affine invariant constant of this statement .",
    "more precisely , one can upper bound @xmath629 , @xmath678 , @xmath476 respectively with propositions  [ prop : cbounded ] , [ mbounded ] and [ prop : upper_bound_p_l ] and lower bound @xmath432 and @xmath544 respectively with proposition  [ prop : deltal ] and equation  . in  theorem  [ thm :",
    "conv ] for case   requires to use the euclidean norm ( because inequality   with the pyramidal width only holds for the euclidean norm ) . on the other hand",
    ", any norm could be used for ( separately ) bounding @xmath629 , @xmath678 and @xmath476 . ]",
    "if we apply these bounds to the rate multipliers in  , it gives the bigger rate multipliers @xmath218 stated in theorem  [ thm : conv ] .",
    "we uses the lemma  [ lemme : lin ] giving a geometric scheme , with a straightforward recurrence we prove that , @xmath679 where @xmath145 is the number of non - drop step steps .",
    "this number is equal to @xmath26 for the sp - fw algorithm and it is lower bounded by @xmath680 for the sp - afw algorithm ( see section  [ par : drop_steps ] equation  ) .",
    "then by using proposition relating @xmath81 and the square root of @xmath118 we get the first statement of the theorem , @xmath681 to prove the second statement of the theorem we just use theorem  [ thm : subopt_gap ] for the last _ non - drop step _",
    "after @xmath682 iterations ( let us assume it was at step @xmath683 ) , @xmath684 the minimum of the gaps observed is smaller than the gap at time @xmath683 then , @xmath685 the affine invariant formulation with the universal step size @xmath686 of theorem  [ thm : conv ] also follows from lemma  [ lemme : lin ] by re - using standard fw proof patterns .",
    "[ thm : conv_sublin ] let @xmath2 be a strongly convex - concave function with a finite smoothness constant @xmath629 , a positive interior strong convex - concavity constant @xmath432   or a positive geometric strong convex - concavity @xmath544  .",
    "let us also define the rate multipliers  @xmath218 as @xmath687 let @xmath218 refers to either @xmath688 for case   where the algorithm is sp - fw , or @xmath689 for case   where the algorithm is sp - afw ,    if @xmath690 , then the suboptimality @xmath118 of the iterates of the algorithm with universal step size @xmath691 ( see equation   for more details about @xmath291 ) has the following decreasing upper bound : @xmath692 where @xmath693 and @xmath145 is the number of non - drop step after @xmath26 steps . for sp - fw , @xmath146 and for sp - afw , @xmath147 .",
    "moreover we can also upper bound the minimum fw gap observed for @xmath694 , @xmath695 note that in this theorem the constant @xmath696  is slightly different from the constant @xmath73 in theorem  [ thm : conv_affine_invariant ] .",
    "we can put both the recurrence   for the sp - fw algorithm and the recurrence   for the sp - afw algorithm ( from the proof of lemma  [ lemme : ineg ] ) in the following form by using our unified notation introduced in the theorem statement : @xmath697 note that the gap @xmath495 is the one defined in equation   and depends on the algorithm .",
    "let @xmath698 to stand respectively for @xmath699 for sp - fw ( case  ) or @xmath700 for sp - afw ( case  ) . with this notation",
    ", the inequality @xmath701 leads to , @xmath702 our goal is to show by induction that @xmath703 let us first define the convex function @xmath704 .",
    "we will show that under  , the function @xmath705 has the following property : @xmath706 this property is due to a simple inequality on integers ; let @xmath707 , from the crucial induction assumption , we get : @xmath708,\\ ] ] but @xmath709 and @xmath710 for any @xmath231 , thus @xmath711 equation   is crucial for the inductive step of our recurrence .    * hypothesis   is true for @xmath712 because @xmath713 . * now let us assume that is true for a @xmath714 .",
    "we set the stepsize @xmath715 .",
    "+ if @xmath716 , it means that @xmath717 and then by   and  , @xmath718 + if @xmath719 , then it means that @xmath720 .",
    "hence , the convexity of the function @xmath705 leads us to the inequality @xmath721 where we used   and the induction hypothesis   to get the penultimate inequality  .",
    "since we assumed that @xmath719 , we get @xmath722 completing the induction proof for  .    in case , @xmath723 and in case ,",
    "@xmath724 ( see equation  ) , leading us to the first statement of our theorem .",
    "the proof of the second statement is inspired by the proof of theorem c.3 from  @xcite .    with the same notation as the proof of lemma  [ lemme : ineg ]",
    ", we start from equation   where we isolated the gap @xmath495 to get the crucial inequality @xmath725 since the gap @xmath495 is the one depending on the algorithm defined by @xmath726 , we have @xmath727 for sp - fw and @xmath728 for sp - afw .",
    "thus , @xmath729 in the following in order not to be too heavy with notation we will work with de fw gap and note @xmath495 for @xmath730 .",
    "the proof idea is to take a convex combination of the inequality   to obtain a new upper - bound on a convex combination of the gaps computed from step @xmath731 to step @xmath682 .",
    "let us introduce the convex combination weight @xmath732 where @xmath145 is the number of non - drop steps after @xmath26 steps and @xmath733 is the normalization factor .",
    "let us also call @xmath734 .",
    "taking the convex combination of  , we get @xmath735 by regrouping the terms and ignoring the negative term , we get @xmath736 by definition @xmath737 and notice that @xmath738 .",
    "we now consider two possibilities : if @xmath100 is a drop step , then @xmath739 and so @xmath740 if @xmath288 is a non - drop step , then @xmath741 and thus we have @xmath742 as @xmath743 , we also have @xmath744 .",
    "the normalization factor @xmath733 to define a convex combination is equal to @xmath745 plugging this and   in the inequality   with the rate   shown by induction gives us , @xmath746 by using @xmath747 .",
    "finally , the minimum of the gaps is always smaller than any convex combination , so we can conclude that ( for @xmath748 ) : @xmath749",
    "in this section , we are going to prove that the function @xmath180 is lipschitz continuous when the sets @xmath13 and @xmath14 are strongly convex and when the norm of the two gradient components are uniformly lower bounded .",
    "we will also give the details of the convergence rate proof for the strongly convex sets situation .",
    "our proof uses similar arguments as  ( * ? ? ?",
    "* theorem  3.4 and  3.6 ) .",
    "thm : s_lip let @xmath13 and @xmath14 be @xmath171-strongly convex sets . if @xmath181 for all @xmath182 , then the oracle function @xmath750 is well defined and is @xmath184-lipschitz continuous ( using the norm @xmath185 ) , where @xmath751",
    ".    first note that since the sets are strongly convex , the minimum is reached at a unique point .",
    "then , we introduce the following lemma which can be used to show that each component of the gradient is lipschitz continuous irrespective of the other set .",
    "[ lemma : fxlip ] let @xmath752 be a @xmath3-lipschitz continuous function ( i.e. @xmath753 ) and @xmath13 a @xmath171-strongly convex set . if @xmath754 , then @xmath755 is @xmath756-lipschitz continuous .",
    "let @xmath757 and let @xmath758 , then @xmath759 now   holds for any @xmath760 as this set is included in @xmath13 by @xmath171-strong convexity of  @xmath13 .",
    "then since @xmath761 is the center of @xmath762 , we can choose a @xmath15 in this ball such that @xmath763 is in the direction which achieves the dual norm of @xmath764 .",
    "proportional to @xmath765 ; but for general norms , it could be a different direction . ]",
    "more specifically , we have that : @xmath766 as we are in finite dimensions , this supremum is achieved by some vector @xmath767 .",
    "so choose @xmath768 and plug it in  :    @xmath769    switching @xmath349 and @xmath770 and using a similar argument , we get , @xmath771 hence summing and , @xmath772 and finally @xmath773 to prove our theorem , we will notice that for the saddle point setup , the oracle function @xmath774 can be decomposed as @xmath775 where @xmath776 and @xmath777 .",
    "then applying our lemma , the function @xmath778 is lipschitz continuous . the same way @xmath779 is lipschitz continuous .",
    "then , for all @xmath780 in @xmath131 @xmath781 which gives the definition of the lipschitz continuity of our function and proves the theorem . in this theorem",
    ", we introduced the function @xmath782 .",
    "this function is monotone in the following sense : @xmath783 actually this property follows directly from the convexity of @xmath4 and the concavity of @xmath6 .",
    "we can also prove that when the sets @xmath13 and @xmath14 are strongly convex and when the gradient is uniformly lower bounded , we can relate the gap and the distance between @xmath30 and @xmath373 .",
    "[ lemma : lower_gap ] if @xmath13 is a @xmath171-strongly convex set and if @xmath784 is uniformly lower bounded by @xmath785 on @xmath13 , then @xmath786 where @xmath787 .",
    "let @xmath15 and @xmath788 be in @xmath13 .",
    "we have @xmath789 by @xmath171-strong convexity .",
    "so as in the proof of lemma  [ lemma : fxlip ] , let @xmath767 be the vector such that @xmath790 and @xmath791 .",
    "let @xmath792 then @xmath793 which leads us to the desired result . from this lemma , under the assumption that @xmath794 @xmath795 , it directly follows that @xmath796 now we recall the convergence theorem for strongly convex sets from the main text , theorem  [ thm : conv_strong ] :    thm",
    ": conv_strong let @xmath2 be a convex - concave function and @xmath13 and @xmath14 two compact @xmath171-strongly convex sets .",
    "assume that the gradient of @xmath2 is @xmath3-lipschitz continuous and that there exists @xmath191 such that @xmath797 .",
    "set @xmath193 .",
    "then the gap @xmath110   of the sp - fw algorithm with step size @xmath194 converges linearly as @xmath798 where @xmath799 .",
    "the initial gap @xmath800 is cheaply computed during the first step of the sp - fw algorithm .",
    "alternatively , one can use the following upper bound to get uniform guarantees : @xmath801    we compute the following relation on the gap : @xmath802 where in the last line we used the fact that the function @xmath803 is lipschitz continuous . then using that @xmath804 ( by definition of @xmath373 )",
    ", we get @xmath805 the last line uses the fact that @xmath782 is monotone by convexity ( equation  ) . finally , using once again the lipschitz continuity of @xmath782 and the one of @xmath180 ( by theorem  [ thm : s_lip ] ) , we get @xmath806 combining   with  , we get @xmath807 thus by setting the step size @xmath808 , we get @xmath809 using the fact that the gap is lower bounded by a constant times the square of the distance between @xmath373 and @xmath30 ( equation  ) .",
    "note that the bound in this theorem is not affine invariant because of the presence of lipschitz constants and strong convexity constants of the sets .",
    "the algorithm is not affine invariant either because the step size rule depends on these constants as well as on @xmath810 .",
    "deriving an affine invariant step size choice and convergence analysis is still an interesting open problem in this setting .",
    "[ [ par : graphical_games ] ] graphical games . + + + + + + + + + + + + + + + +      1 .",
    "university  1 ( respectively university  2 ) has benefit @xmath811 ( @xmath812 ) to get student @xmath229 .",
    "2 .   student @xmath229 ranks the possible roommates with a permutation @xmath813 .",
    "let @xmath814 represents the rank of @xmath230 for @xmath229 ( first in the list is the preferred one ) .",
    "they go to the university that matched them with their preferred roommate , in case of equality the student chooses randomly .",
    "4 .   supposing that @xmath15 encodes the roommate assignment proposed by university  1 ( and @xmath112 for university  2 ) , then the expectation of the benefit of university  1 is @xmath815 , with the following definition for the payoff matrix @xmath114 indexed by pairs of matched students .",
    "for the pairs @xmath816 with @xmath817 and @xmath818 with @xmath819 with elements in @xmath820 , we have : 1 .   @xmath821 2 .",
    "@xmath822 3 .",
    "@xmath823 4 .",
    "@xmath824 5 .",
    "@xmath825      for our experiments , in order to get a realistic payoff matrix , we set @xmath826 $ ] the _ true _ value of student @xmath229",
    ". then we set @xmath827 the value of the student @xmath229 _ observed _ by university  @xmath828 .",
    "to solve the perfect matching problem , we used blossom v by .",
    "we give here more details on the derivations of the objective function for the structured svm problem .",
    "we first recall the structured prediction setup with the same notation from  @xcite . in structured prediction ,",
    "the goal is to predict a structured object @xmath829 ( such as a sequence of tags ) for a given input @xmath7 . for the structured svm approach , a structured feature map @xmath830 encodes the relevant information for input / output pairs , and a linear classifier with parameter @xmath831 is defined by @xmath832 .",
    "we are also given a task - dependent structured error @xmath833 that gives the loss of predicting @xmath112 when the ground truth is @xmath834 .",
    "given a labeled training set @xmath835 , the standard @xmath103-regularized structured svm objective in its non - smooth formulation for learning as given for example in equation  ( 3 ) from @xcite is : @xmath836 where @xmath837 is the structured hinge loss , and the following notational shorthands were defined : @xmath838 , @xmath839 and @xmath840 .    in our setting",
    ", we consider a sparsity inducing @xmath246-regularization instead .",
    "moreover , we use the ( equivalent ) constrained formulation instead of the penalized one , in order to get a problem over a polytope .",
    "we thus get the following challenging problem : @xmath841 to handle any type of structured output space @xmath14 , we use the following generic encoding . enumerating the elements of @xmath256 , we can represent the @xmath842 element of @xmath256 as @xmath843 .",
    "let @xmath250 have @xmath251 as columns and let @xmath844 be a vector of length @xmath254 with @xmath845 as its entries .",
    "the functions @xmath249 can then be rewritten as the maximization of linear functions in @xmath112 : @xmath846 .",
    "as the maximization of linear functions over a polytope is always obtained at one of its vertex , we can equivalently define the maximization over the convex hull of @xmath256 , which is the probability simplex in @xmath847 that we denote @xmath253 : @xmath848 thus our equivalent objective is @xmath849 which is the bilinear saddle point formulation given in the main text in  ."
  ],
  "abstract_text": [
    "<S> we extend the frank - wolfe ( fw ) optimization algorithm to solve constrained smooth convex - concave saddle point ( sp ) problems . </S>",
    "<S> remarkably , the method only requires access to linear minimization oracles . </S>",
    "<S> leveraging recent advances in fw optimization , we provide the first proof of convergence of a fw - type saddle point solver over polytopes , thereby partially answering a 30 year - old conjecture . </S>",
    "<S> we also survey other convergence results and highlight gaps in the theoretical underpinnings of fw - style algorithms . </S>",
    "<S> motivating applications without known efficient alternatives are explored through structured prediction with combinatorial penalties as well as games over matching polytopes involving an exponential number of constraints .    </S>"
  ]
}