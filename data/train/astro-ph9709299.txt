{
  "article_text": [
    "one of the key issues of contemporary cosmology is to understand the formation and evolution of cosmic structures ; from the pattern of clusters and galaxies on the largest scales down to the formation of galaxies themselves . in",
    "the standard model cosmic structure grew from a spectrum of low amplitude fluctuations present at the epoch of recombination .",
    "the form of this spectrum is at present unknown , although a number of experiments are placing constraints on the fluctuation amplitude at recombination on large scales .",
    "early next century satellite missions , such as cobras / samba and map , promise to tightly constrain the recombination - epoch spectrum as well as a number of other significant cosmological parameters .",
    "it is the spectrum of fluctuations at recombination which forms the initial conditions for studies of structure formation .    whilst the fluctuation amplitude is small post - recombination growth is linear ;",
    "spectral modes grow independently and proportionately to the universal expansion ( assuming that the universe is close to flat at the relevant epoch ) .",
    "presently - observed structures range from the quasi - linear such as the large scale network of filaments and voids and superclusters to the highly non - linear such as galaxies and galaxy clusters . whilst linear growth of small fluctuations",
    "is well understood and analytically tractable , only approximate methods and perturbation theory or simple `` toy '' models are available for studying non - linear growth . making the connection between the initial fluctuation spectrum and presently observed structures , a task central to our understanding of the post - recombination universe , requires numerical simulation .",
    "the need for numerical simulation becomes even more pressing when we add the hydrodynamic component necessary for an understanding of gas clusters and , especially , dissipation in galaxies .",
    "the paper is laid out as follows .",
    "i begin with a pictorial description of the growth of the fluctuation spectrum and the transition from linear to non - linear growth for a generic class of spectra in which bound objects progress from small to large .",
    "i then discuss the requirements of a numerical simulation in terms of the way in which we model the universe and the force and mass resolution necessary for various aspects of structure formation .",
    "following a description of the motivation for the use of particle methods , the various different algorithms that have been developed are summarized . a brief historical overview of the achievements of simulation methods",
    "is then presented .",
    "the push towards the use of supercomputers for numerical simulation is considered and i will present a case study of this endeavour .",
    "i conclude with a view of the problems posed by very large simulations and speculate about the future .",
    "many popular contemporary cosmological models consider spectra in which the mass variance is a decreasing function of scale .",
    "such spectra can explain a number of observational features , such as , for example , the fact that clusters have formed more recently than galaxies .",
    "= 3.7truein    figure  1 shows a pictorial view of the growth of the mass variance as a function of scale .",
    "( the precise form of the spectrum is unimportant only the decrease to large scales is significant for this discussion . )",
    "the figure illustrates in a generic way at what scales and epochs we encounter non - linear gravitational evolution .",
    "detailed understanding of structures on scales for which @xmath0 at any epoch requires numerical simulation .",
    "key questions which will benefit from a numerical approach include :    * the large - scale distribution of galaxies and their `` bias '' relative to the underlying gravitationally dominant dark matter component . * the formation and evolution of clusters ; the behaviour of the intra - cluster medium and the distribution and evolution of the galaxy population . * understanding the structure of the universe out to the redshifts of quasars ; lyman@xmath1 forest etc . * the formation and influence of the first bound objects at redshifts between 10 and 40 . * galaxy formation itself  the `` holy grail '' of post - recombination cosmology .",
    "the value of numerical simulation is that it allows us to surmount the obstacle of non - linear gravitational and hydrodynamic evolution . not only will we be able to investigate the formation and evolution of presently observed structure and probe its immediate precursors , but we will be able to make the crucial connection with the initial fluctuation spectrum .",
    "cosmological simulations have considered a number of different aspects of structure formation . these have ranged from studies of the large scale distribution of matter , without specific reference to galaxies , to studies of individual clusters and galaxies .",
    "i will focus on the so - called `` grand challenge '' problem of simulating the formation and distribution of galaxies in the cosmological context .",
    "this study is well motivated : galaxies form the fundamental building blocks of the universe . with the advent of huge galaxy redshift surveys such as the sloan digital sky survey and the two degree field we will have an enormous observational database with which to interpret and constrain theoretical models .",
    "the relationship of the galaxies that we observe to the overall matter distribution in the universe is , however , uncertain .",
    "it is crucial that we understand the nature of any bias that may be present .",
    "it is only through detailed numerical simulation of the formation and evolution of galaxies ( which are highly dissipated objects ) that we can hope to achieve a detailed understanding of cosmic structure .    broadly speaking the requirements of such a simulation are that we model a representative piece of the universe over a sufficient range of scales with accurate forces and time integration schemes .",
    "these are , of course , very general requirements which apply to any well conceived numerical simulation .",
    "one of these desirable qualities , accurate time integration , i will mention only in passing .",
    "typically second order leapfrog or low order runge  kutta or predictor  corrector schemes are employed .",
    "this is largely a consequence of the need to minimize the storage of extra quantities for a huge number of particles whose orbits must be integrated , coupled with the expense of the force calculation . that it is acceptable to use such low order schemes",
    "is a result of studying gravitational collapse .",
    "because of the instability of the system we can not hope to follow individual particle orbits accurately but are only concerned that we adequately model the properties of bound objects in a statistical sense .",
    "note that this is in stark contrast to the extreme care which is exercised in the integration of orbits in the solar system as described elsewhere in this volume .    the issue of the boundary conditions to apply to the simulation will of course depend upon the precise nature of the problem being considered .",
    "i will return briefly to this issue towards the end of the paper .",
    "it suffices at present to note that , for the study of the large - scale distribution of galaxies , it is convenient to chose a cubic simulation volume with triply periodic boundary conditions which is large enough , subject to the limitations of available resolution , to represent a fair sample of the universe .    for",
    "the remainder of this section i will concentrate on two aspects of resolution ; force and mass resolution . gaining",
    "higher and higher resolution has been a primary focus of much of the effort expended in cosmological simulations over the last decade .",
    "i will attempt to justify this trend and at the same time give the motivation for the use of particle methods .",
    "the `` top hat '' model is a useful simplification of the collapse of an initially overdense region .",
    "an overdense spherically symmetric region within an otherwise homogeneous universe will have an expansion and contraction corresponding to the evolution of a super - critical universe . in practice",
    ", inhomogeneities and external tides will generate non - radial motions which will prevent the region collapsing to a singularity and cause it to virialize at roughly half its turnaround radius . at virialization , in a flat universe",
    ", the overdensity of the object will be nearly 200 , and this will increase as the background continues to expand .",
    "an object formed at a redshift of 4 would have an overdensity of 10@xmath2 at redshift zero , even without dissipation .",
    "if we are to properly model cosmic structure it is essential to follow and retain bound objects at increasing density contrast .",
    "an instance of the importance of this would be in an investigation of substructure ; galaxies within a cluster for example . since a cosmological simulation typically models a comoving region of the universe these very large density contrasts are present in the simulation .",
    "popular choices for modelling the wide range of densities and varied geometries that occur are lagrangian particle methods .",
    "although some early simulations represented each galaxy with a single particle it is now much more common to model the matter density as a collisionless fluid which is approximated by an ensemble of softened particles .",
    "an important question is then the choice of softening .",
    "to strictly maintain the fluid limit the softening should never be less than the mean interparticle separation , and yet this precludes accurate tracking of highly over - dense regions within a comoving volume .",
    "it is more realistic to demand that the softening be of order the mean interparticle separation _ within a bound object _ , and thus maintain the collisionless fluid approximation locally . since objects collapsing at different epochs have different physical densities a spatially variable softening is indicated .",
    "most workers , however , chose a fixed spatial softening for the gravitational interaction , although as the dynamic range of simulations increases this question will need to be revisited .    the art of choosing the appropriate softening lies in negotiating between the unwanted effects of two - body relaxation if the value is too small , and over - merger of `` fluffy '' substructure if the value is too large .",
    "typically , realistic simulations of galaxies in a cosmological context , for example , require force - softenings at least an order of magnitude smaller than the mean interparticle separation .",
    "as noted above , a huge amount of effort has been expended in running simulations with ever larger particle number .",
    "i give a simple order - of - magnitude estimate for a suitable particle number for the large - scale galaxy distribution grand challenge problem .",
    "to model a fair sample of the universe requires that we simulate a cube of at least 100mpc on a side .",
    "a volume of this size will contain roughly 10@xmath2 bright galaxies .",
    "the fraction of the overall matter density residing in galaxies is perhaps one percent .",
    "allowing for 100 particles per galaxy halo suggests that a simulation should contain at least 10@xmath3 resolution elements or particles .",
    "only now is it becoming possible to run simulations with this particle number and typically the simulations which have been run have not had high resolution forces . the above argument , whilst simplistic",
    ", demonstrates clearly the need for very high resolution .",
    "a further leap in the required resolution becomes necessary when one wishes to model the dissipative component of galaxies .",
    "suppose that one uses smoothed particle hydrodynamics ( sph ; gingold and monaghan  1977 ) to model the gaseous content of galactic haloes .",
    "sph , which fits well with the particle codes , requires at least 10@xmath4 particles per galaxy to reliably follow the cooling and collapse of gas in dark haloes , and likely considerably more .",
    "this implies a simulation with @xmath5 particles .",
    "not all investigations require modelling of huge volumes of the universe .",
    "two examples will illustrate the requirements in other situations .    * * galaxy formation * @xmath6 particles in a 10mpc region @xmath7 1kpc resolution . * * clusters * @xmath8 particles in a 50mpc region @xmath7 3kpc resolution .",
    "simulations with such huge numbers particles are not without drawbacks of course .",
    "the storage for positions and velocities alone for 10@xmath3 particles is 2.4 gb for each time - slice ( 32bit words ) , and roughly 50% greater for the storage of all quantities in an sph simulation . nonetheless , the problem is well motivated and , at least for the problem under consideration , these sorts of particle numbers are being considered and the first such runs performed .",
    "various alternatives to the straightforward approach of greater particle number are mentioned in the final section .",
    "the task which must be accomplished then , is the computation of the gravitational interaction of a very large number of particles .",
    "whilst conceptually simple this problem has consumed a vast amount of effort and programming skill and has led to a number of different approaches .",
    "the primary popular approaches are described in outline below .",
    "all of these , apart from the first , calculate the gravitational potential by dividing the field into two components ; the near field which has high frequency content and the far field which is approximated to some order by a given basis expansion .",
    "assuming that the force at an arbitrary point is required to a specified accuracy , it can be argued heuristically that these methods all scale as @xmath9 , where @xmath10 is the particle number .    * * particle  particle ( pp ) * forces on each particle are accumulated by directly summing over all neighbours . whilst this method is very robust",
    "it is @xmath11 and is hence limited to fewer than @xmath12 particles .",
    "( it is worth noting that this method has received a new lease of life in cosmology as a result of the development of application - specific chips which compute the newtonian force in hardware . ) * * tree methods * these methods are the logical successors to the pp method although historically they appeared after the standard grid - based techniques described next .",
    "these methods take account of the fact that to a given accuracy a group of particles distant from the force calculation point may be approximated by a low order multipole expansion thus avoiding the expensive sum over all particles in the group .",
    "various types of tree have been used to organize the data in such a way that the decision as to whether or not a cell of particles need be subdivided for a particular force calculation may be made efficiently .",
    "for details see barnes & hut  ( 1986 ) , hernquist & katz  ( 1989 ) .",
    "a number of variants exist .",
    "the fast multipole method of greengard & rokhlin  ( 1987 ) is similar in spirit to this approach . it will not be described further here as its use in cosmology has been very limited . * * grid - based methods * these methods sample the density field with a ( usually ) uniform grid .",
    "poisson s equation is then solved on this grid using one of a number of the fast solvers that are available , usually either an fft or multigrid .",
    "the potential obtained corresponds to the far field component as the grid can not represent frequencies higher than the nyquist limit .",
    "the particle - mesh ( pm ) force is typically augmented by a short range contribution summed over near neighbours , thus allowing high resolution spatial forces .",
    "the method can become grossly inefficient as clustering develops , however , and the short - range direct sum becomes dominant .",
    "this problem has been alleviated in the adaptive p@xmath4 m ( couchman  1991 ; ap@xmath4 m ) code by using a hierarchy of adaptive meshes in regions of high particle density .",
    "most workers now use either a version of the tree code or a grid - based method ( primarily p@xmath4 m , or variant , using fast fourier transforms ) , although a number of other techniques have been , or are being developed .",
    "these include grid - tree hybrids in which a tree replaces the expensive direct sum in p@xmath4 m and other grid techniques in which a true grid refinement strategy is used .",
    "there are a number of pros . and cons .",
    "for each of the primary methods .",
    "tree codes are robust , the cycle time does not vary much between light and heavy clustering and they provide a flexible data structure which makes implementation of individual particle timesteps , parallelization and inclusion of sph relatively straightforward .",
    "their primary drawbacks are that they use a substantial amount of memory , @xmath13 words per particle , and , for cosmology , the necessary implementation of periodic boundary conditions via the ewald  ( 1921 ) summation method can be cumbersome .",
    "p@xmath4 m has the advantage that it is very fast under conditions of light clustering , has automatically periodic boundary conditions when using an fft to solve for the grid potential and uses relatively little memory ; @xmath14 words per particle .",
    "a severe disadvantage is that clustering can dramatically slow the algorithm unless an efficient scheme for relieving the pp work in highly clustered regions is implemented .",
    "although , the use of regular uniform grids throughout the calculation leads to efficiency in terms of memory use and execution speed under light particle clustering , it results in a less general data structure than a tree , for example , which makes inclusion of individual particle timesteps , as well as some aspects of parallelization , more difficult .",
    "the addition of a hydrodynamic component to gravitational cosmological codes permits the investigation of many new aspects of structure formation .",
    "of particular importance has been the study of the hot intra - cluster medium .",
    "the number density and distribution of clusters provide important constraints on the primordial spectrum .",
    "more recently several ambitious attempts have being made upon the prize of galaxy formation itself , in which correct modelling of dissipative processes is crucial .",
    "perhaps the most straightforward technique for adding hydrodynamics to a particle code is smoothed particle hydrodynamics ( sph ) . in this method",
    "thermodynamic quantities are carried by particles and the value of the fluid is approximated at any point by interpolating values from nearby particles . because of the ease of integrating this method with gravitational particle methods it was the first widely used hydrodynamic method in cosmology .",
    "implementations of sph in both tree codes ( hernquist & katz  1989 ) and p@xmath4 m ( evrard  1990 ) exist .",
    "more recently a number of alternative schemes have been imported into cosmological codes .",
    "in particular eulerian codes have become popular .",
    "these offer a number of potential advantages over sph , in particular efficient shock capturing schemes greatly improve on the number of resolution elements necessary to model a shock ; in 1-d two or three cells for an eulerian code and 6 particles for sph .",
    "these methods are also well suited to the addition of magneto - hydrodynamics and radiation fields . in order for these codes to follow",
    "the large density contrasts that arise , adaptive mesh refinement techniques are essential , and these are now beginning to be used .",
    "other schemes include semi - lagrangian methods and very recently an unstructured finite element method has been described ( xu  1996 ) . in all of these codes particles carry the gravitational mass .",
    "in this section i will summarise the improvements that have occurred in the field of cosmological simulations over the last decade , very briefly list the areas in which they have played a significant role in advancing the subject , and outline the present status of the field .",
    "although it is clear that cosmological simulations are an essential tool for cosmologists studying structure formation , they have not proved to be the central mechanism for generating new ideas or insight about the post - recombination universe .",
    "the physical ideas are simple and well understood ; provided that we can quantify the collective behaviour of particles , simulations are helpful in understanding the behaviour and interaction of these physical processes in highly complicated situations . in this respect",
    "cosmological simulations have a different focus and utility than simulations of say , galactic dynamics . in the latter",
    "( well studied ) case the galaxy may be idealised to a degree which allows detailed investigation of , for example , modes of a galactic disc . in cosmological simulations",
    "the focus is rather different .",
    "first we are typically trying to model a realization of an initial random field ( albeit with known initial spectrum ) which lacks the symmetries of the galaxy study .",
    "second , analytic theory beyond first order , so far , has quite limited predictive power .",
    "a reasonable expectation is that simulation and theory will play a mutually supportive role in improving our knowledge in this area .",
    "a convenient milepost to the development of what might be termed `` modern '' cosmological simulation methods is the pioneering work of davis , efstathiou , frenk and white in the mid 1980s ( see efstathiou et al .",
    "1995 for a description of numerical methods ) .",
    "this work , following from earlier work by efstathiou and eastwood drew the p@xmath4 m code of hockney and eastwood  ( 1988 ) from the plasma physics arena into the cosmological community and represented what might be termed the first high resolution cosmological simulations with @xmath15 particles .    with the availability of efficient algorithms , increases in computer speed and memory size led rapidly to higher resolution simulations . by the late 80s @xmath16 particle p@xmath4 m simulation were run and this number increased to @xmath17 by 1990 .",
    "at present it is possible to run routinely @xmath18 particle ap@xmath4 m simulations .",
    "differing memory and cpu requirements mean that at a given time somewhat larger pm , or smaller tree - code , simulations could be performed .",
    "hydrodynamic simulations both because of their larger memory requirement and the necessity of using a smaller timestep are generally at present limited to a few million particles .",
    "a common criticism of cosmological simulation efforts has been that they are merely a method of generating attractive pictures .",
    "it is certainly true that , so far , there have been only a few useful statistical measures used to express the collective behaviour of the large number of particles in a cosmological simulation .",
    "it is worth bearing in mind the comments made above , however , concerning the likely role that simulations will play in improved understanding of structure formation .",
    "the following items of progress are essentially all cases in which simulations have checked analytic models or predictions .",
    "this is only a sample of the areas in which simulations have played a key role but it is indicative of the way in which simulations have been , and are likely to continue to be , employed .",
    "* checks of perturbation theory . *",
    "testing the validity of the press ",
    "schechter  ( 1974 ) model , which attempts to model the could - in - cloud problem . * seeking a universal scaling function to describe the non - linear mass auto - correlation as a function of the initial linear auto - correlation ( hamilton et al .",
    "* determining cluster distributions and abundances * attempts to understand galaxy bias through an investigation of the distribution of putative galaxy haloes in non - hydrodynamic simulations . *",
    "understanding the intra - cluster gas .",
    "* modelling lyman-@xmath1 clouds and lyman-@xmath1 forest observations . * verifying the broad outlines of the white",
    " rees  ( 1978 ) picture for baryon condensation within dark haloes .",
    "the challenge of simulating the formation of galaxies in the cosmological context requires huge computational resources .",
    "the estimates given above suggest 10@xmath19 particles .",
    "both the computer time and the amount of memory required for such large simulations pose severe computational constraints and , at present , are available only on massively parallel supercomputers .",
    "a number of groups are working in this area with parallel codes .",
    "i will describe the `` virgo '' collaboration in which i am involved ( pearce et al .",
    "this is a group of eight , primarily uk researchers , using the uk national supercomputer ( a cray t3d ) in edinburgh as well as a cray t3e in munich .",
    "we have ported the ap@xmath4 m code to the edinburgh 512 processor cray t3d using craft , cray s proprietary software .",
    "this software creates a global address space which appears flat to the programmer vastly easing the difficulty of parallelizing code for a distributed memory architecture .",
    "the penalty , however , is a certain amount of inefficiency and lack of portability . for this reason an explicit message passing ( mpi ) version is now being developed .",
    "the primary difficulty with parallel n - body codes is to achieve an efficient data and work distribution . with grid codes",
    "there is the added difficulty that the mapping from a perhaps highly clustered particle distribution to a regularly spaced grid may lead to a large communications overhead .",
    "tree codes tend to be somewhat better behaved in this regard as they avoid the potential many - to - one mapping of the lagrangian particles to the eulerian grid and because of the use of a more flexible data structure . in general ,",
    "however , care must be taken with particle codes to minimize the communication overhead .",
    "the particle is the fundamental element and essentially no work is done without reference to its neighbours , therefore data must be carefully distributed to avoid costly inter - processor communication .",
    "this behaviour can be compared with that in finite element codes in which a large amount of work is done internally to the element and the ratio of computation to communication is much higher .",
    "the parallel ap@xmath4 m refined grid code scales well up to 256 processors , although some load imbalance occurs for heavy clustering .",
    "we have used a strategy in which we distribute data in one spatial direction over the processors .",
    "provided there are a number of particle clusters present in the simulation volume this is sufficient to average out the worst load imbalances .",
    "we are now achieving performances of 2500 particles / s / t3d node for collisionless simulations and 700 particles / s / t3d node for the ap@xmath4m - sph code `` hydra '' ( couchman et al .",
    "a @xmath20 particle run with 5000 steps would thus take approximately one week on 256 nodes of a cray t3d ; corresponding to 5 years total processing time .",
    "this number of particles uses about half the 64 mb available per node on 256 nodes .",
    "a cray t3e has 3 times faster processors and can have up to 2 gb of memory per node ; 10@xmath3 particles should be feasible .",
    "i will conclude by making a few remarks about the future of cosmological simulations .",
    "it is clear that the trend towards larger simulations will continue .",
    "as the dynamic range of a simulation increases so does the range of density contrasts that can in principle be modelled ; alternatively , the first structures which we can confidently identify in a simulation form at earlier epochs .",
    "accurate modelling demands that the simulation can deal with the wide range of densities and timescales which arise .",
    "both spatial and temporal adaptivity will become more important .",
    "tree codes frequently have both whilst the ap@xmath4 m code lags , for example , in having only spatial adaptivity .",
    "one of the most obvious problems with large supercomputer applications is the vast amount of data that is generated .",
    "cosmological n - body codes are no exception .",
    "each timeslice of a @xmath20 particle run occupies 960 mb ( 64bit ) for position and velocity storage .",
    "the output of a hydrodynamic code would be nearly 50% greater@xmath21 gb per timeslice .",
    "an approach which helps avoid the data explosion is to concentrate on selected volumes .",
    "this has generated a great deal of interest recently with a large number of workers pursuing different avenues of attack .",
    "there is now a general appreciation that external fields must be carefully applied to the selected volumes to properly mimic the tidal effects of the external universe .",
    "this general approach is very promising .",
    "it remains to be seen whether the information gained from these studies will be sufficient to allow us to predict the locations of galaxies in larger - volume ( perhaps collisionless ) simulations . if we are to properly assess the upcoming large sky surveys , and hence to be able to understand galaxy distributions , morphologies , bias etc .",
    ", we must be able to generate reliable theoretical galaxy catalogues from simulations .",
    "pearce , f.r . ,",
    "thomas , p.a . ,",
    "hutchings , r.m . , couchman , h.m.p . ,",
    "jenkins , a.r .",
    ", frenk , c.s . ,",
    "white , s.d.m . and",
    "colberg , j.m .",
    ", 1996 , ieee computer society press .",
    "proceedings of 5th euromicro workshop on parallel and distributed computing , ed .",
    "hans p. zima"
  ],
  "abstract_text": [
    "<S> cosmic structure simulations have improved enormously over the past decade , both in terms of the resolution which can be achieved , and with the addition of hydrodynamic and other techniques to formerly purely gravitational methods . </S>",
    "<S> this is an informal , and perhaps idiosyncratic , overview of the state and progress of cosmological simulations over this period .    </S>",
    "<S> i will discuss the strategies that are used to understand cosmic structure formation and the requirements of a successful simulation . </S>",
    "<S> the computational demands of cosmological simulations in general will be highlighted as will specific features of the various different algorithms that are used . </S>",
    "<S> the incessant push for greater resolution has lead to an increasing use of parallel computers . </S>",
    "<S> the desirability  or otherwise  of this trend will be discussed together with alternative techniques which sidestep the nave drive for greater resolution .    </S>"
  ]
}