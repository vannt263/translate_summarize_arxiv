{
  "article_text": [
    "word sense disambiguation is the process of identifying the correct sense of words in particular contexts .",
    "the solving of wsd seems to be ai complete ( that means its solution requires a solution to all the general ai problems of representing and reasoning about arbitrary ) and it is one of the most important open problems in nlp @xcite,@xcite,@xcite , @xcite,@xcite,@xcite . in the electronical on - line dictionary wordnet ,",
    "the most well - developed and widely used lexical database for english , the polysemy of different category of words is presented in order as : the highest for verbs , then for nouns , and the lowest for adjectives and adverbs .",
    "usually , the process of disambiguation is realized for a single , target word .",
    "one would expect the words closest to the target word to be of greater semantical importance for it than the other words in the text .",
    "the context is hence a source of information to identify the meaning of the polysemous words .",
    "the contexts may be used in two ways : a ) as _ bag of words _ , without consideration of relationships with the target word in terms of distance , grammatical relations , etc . ;",
    "b ) with relational information .",
    "the _ bag of words _",
    "approach works better for nouns than verbs but is less effective than methods that take other relations in consideration .",
    "studies about syntactic relations determined some interesting conclusions : verbs derive more disambiguation information from their objects than from their subjects , adjectives derive almost all disambiguation information from the nouns they modify , and nouns are best disambiguated by directly adjacent adjectives or nouns @xcite .",
    "all these advocate that a global approach ( disambiguation of all words ) helps to disambiguate each pos .",
    "in this paper we propose a global disambiguation algorithm called * chain algorithm * for disambiguation , chad , which presents elements of both points of view about a context : because this algorithm is @xmath0 it belongs to the class of algorithms which depend of relational information ; in the same time it does nt require syntactic analysis and syntactic parsing .",
    "in section 2 of this paper we review lesk s algorithm for wsd . in section 3",
    "we present `` triplet '' algorithm for three words and chad algorithm . in section 4",
    "we describe some experiments and evaluations with chad .",
    "section 5 introduces some conclusions of using the chad for translation ( here from romanian language to english ) and for text entailment verification .",
    "section 6 draws some conclusions and further work .",
    "work in wsd reached a turning point in the 1980s when large - scale lexical resources , such as machine readable dictionaries , became widely available .",
    "one of the best known dictionary - based method is that of lesk ( 1986 ) .",
    "it starts from the idea that a word s dictionary definition is a good indicator for the senses of this word and uses the definition in the dictionary directly .    let us remember basic algorithm of lesk @xcite :    suppose that for a polysemic target word @xmath1 there are in a dictionary @xmath2 senses    @xmath3 given in an equal number of definitions @xmath4 .",
    "here we mean by @xmath5 the set of words contained in the @xmath6-th definition .",
    "consider that the new context to be disambiguated is @xmath7 .",
    "the * reduced form * of lesk s algorithm is :    @xmath8    the score of a sense is the number of words that are shared by the different sense definitions ( glosses ) and the context .",
    "a target word is assigned that sense whose gloss shares the largest number of words .",
    "the algorithm of lesk was successfully developed in @xcite by using wordnet dictionary for english .",
    "it was created by hand in 1990s and includes definitions ( glosses ) for individual senses of words , as in a dictionary .",
    "additionally it defines groups of synonymous words representing the same lexical concept ( synset ) and organizes them into a conceptual hierarchy .",
    "the paper @xcite uses this conceptual hierarchy for improving the original lesk s method by augmenting the definitions with non - gloss information : synonyms , examples and glosses of related words ( hypernyms , hyponyms ) . also , the authors introduced a novel overlap measure between glosses which favorites multi - word matching .",
    "first of all we present an algorithm for disambiguation of a triplet . in a sense , our triplet algorithm is similar with global disambiguation algorithm for a window of two words around a target word given @xcite .",
    "instead , our chad realizes disambiguation of all - words in a text with any length , ignoring the notion of `` window '' and `` target word '' and target word in similar studies , all that without increasing the computational complexity .",
    "the algorithm for disambiguation of a triplet of words @xmath9 for dice measure is the following :    begin    for each sense @xmath10 do    for each sense @xmath11 do    for each sense @xmath12 do    @xmath13    endfor    endfor    endfor    @xmath14 / * sense of @xmath15 is @xmath16 , sense of @xmath17 is @xmath18 , sense of @xmath19 is @xmath20 * /    end    for the overlap measure the score is calculated as : @xmath21 for the jaccard measure the score is calculates as : @xmath22    shortly , chad begins with the disambiguation of a triplet @xmath23 and then adds to the right the following word to be disambiguated .",
    "hence it disambiguates at a time a new triplet , where first two words are already associated with the best senses and the disambiguation of the third word depends on these first two words .",
    "chad algorithm for disambiguation of the sentence @xmath24 is :    begin    disambiguate triplet @xmath23    @xmath25    while @xmath26 do    calculate @xmath27    calculate @xmath28    @xmath29    endwhile    end    due to the brevity of definitions in wn many values of @xmath30 are 0 .",
    "we attributed the first sense in wn for @xmath31 in this cases .",
    "in this section we shortly describe some experiments that we have made in order to validate the proposed chain algorithm * chad*.      we have developed an application that implements * chad * and can be used to :    * disambiguate words ( [ res ] ) ; * translate words into romanian language ( [ app2 ] ) ; * text entailment verification ( 5.2 ) .",
    "the application is written in jdk 1.5.0 . and uses _ httpunit _ 1.6.2 api @xcite . written in java",
    ", httpunit is a free software that emulates the relevant portions of browser behavior , including form submission , javascript , basic http authentication , cookies and automatic page redirection , and allows java test code to examine returned pages either as text , an xml dom , or containers of forms , tables , and links @xcite .",
    "we have used _ httpunit _ in order to search wordnet through the dictionary from @xcite .",
    "more specifically , the following java classes from @xcite are used :    * _ webconversation_. it represents the context for a series of http requests .",
    "this class manages cookies used to maintain session context , computes relative urls , and generally emulates the browser behavior needed to build an automated test of a web site . * _",
    "webresponse_. this class represents a response to a web request from a web server . * _",
    "webform_. this class represents a form in an html page . using this class",
    "we can examine the parameters defined for the form , the structure of the form ( as a dom ) , and the text of the form .",
    "we have used _ webform _ class in order to simulate the submission of the form with corresponding parameters .",
    "we tested our chad on 10 files of brown corpus , which are pos tagged .",
    "recall that wn stores only stems of words .",
    "so , we first preprocessed the glosses and the input files , replacing inflected words with their stems .",
    "the reason for choosing brown corpus was the possibility offered by semcor corpus ( the best known publicly available corpus hand tagged with wn senses ) to evaluate the results .",
    "the correct disambiguated words means the disambiguated words as in semcor .",
    "we ran separately chad for : 1 . nouns , 2 .",
    "verbs , and 3 .",
    "nouns , verbs , adjectives and adverbs . in the case of chad",
    "addressed to nouns , the output is the sequence of nouns tagged with senses .",
    "the tag @xmath32 means that for noun @xmath33 the wn sense @xmath6 was found .",
    "analogously for the case of disambiguation on verbs and of all pos .",
    "the results are presented in tables 1 and 2 .",
    "as our chad algorithm is dependent on the length of glosses , and as nouns have the longest glosses , the highest precision is obtained for nouns . in figure 3 , the precision progress can be traced . by dropping and rising",
    ", the precision finally stabilizes to value 0.767 ( for the file br - a01 ) .",
    "the most interesting part of this graph is that he shows how this chain algorithm works and how the correct or incorrect disambiguation of first two words from the first triplet influences the disambiguation of the next words .",
    "it is known that , at senseval 2 contest , only 2 out of the 7 teams ( with the unsupervised methods ) achieved higher precision than the wordnet @xmath34 sense baseline .",
    "we compared in figures 1 , 2 and 3 the precision of chad for 10 files in brown corpus , for dice , overlap and jaccard measures with wordnet @xmath34 sense .    comparing the precision obtained with the overlap measure and the precision given by the wordnet @xmath34 sense for 10 files of brown corpus ( br - a01 , br - a02 , br-11 , br-12 , br-13 , br-14 , br - a15 , br - b13 , br - b20 and br - c01 ) , we obtained the following results :    * for nouns , the minimum difference was 0.0077 , the maximum difference was 0.0706 , the average difference was 0.0338 ; * as a whole , for 4 files difference was greater or equal to 0.04 , and for 6 files was lower ; * in case of all parts of speech , the minimum difference was 0.0313 , the maximum difference was 0.0681 , the average difference was 0.0491 ; * as a whole , for 7 files difference was greater or equal to 0.04 , and for 3 files was lower ; * relatively to verbs , the minimum difference was 0.0078 , the maximum difference was 0.0591 , the average difference was 0.0340 ; * as a whole , for 4 files difference was greater or equal to 0.04 , and for 6 files was lower .",
    "let us remark that in our chad the standard concept of windows better size parameter @xcite is not working : simply , a window is the variable space between the previous and the following word in respect to the current word .",
    "file    &    words    &    dice    &    jaccard    &    overlap    &    wn1     +    bra01    & 486&0.758&0.758&0.767&0.800 +    bra02    & 479&0.735&0.731&0.758&0.808 +    bra14    & 401&0.736&0.736&0.754&0.769 +    bra11    & 413&0.724&0.726&0.746&0.773 +    brb20    & 394&0.740&0.740&0.743&0.751 +    bra13    & 399&0.734&0.734&0.739&0.746 +    brb13    & 467&0.708&0.708&0.717&0.732 +    bra12    & 433&0.696&0.696&0.710&0.781 +    bra15    & 354&0.677&0.674&0.682&0.725 +    brc01    & 434&0.653&0.653&0.661&0.728 +            file    &    words    &    dice    &    jaccard    &    overlap    &    wn1     +    bra14    & 931&0.699&0.701&0.711&0.742 +    bra02    & 959&0.637&0.685&0.697&0.753 +    brb20    & 930&0.672&0.674&0.693&0.731 +    bra15    & 1071&0.653&0.651&0.684&0.732 +    bra13    & 924&0.667&0.673&0.682&0.735 +    bra01    & 1033&0.650&0.648&0.674&0.714 +    brb13    & 947&0.649&0.650&0.674&0.722 +    bra12    & 1163&0.626&0.622&0.649&0.717 +    bra11    & 1043&0.634&0.639&0.648&0.708 +    brc01    & 1100&0.625&0.627&0.638&0.688 +",
    "wsd is only an intermediate task in nlp . in machine translation wsd",
    "is required for lexical choise for words that have different translation for different senses and that are potentially ambiguous within a given document .",
    "however , most machine translation models do not use explicit wsd @xcite ( in introduction ) .    the algorithm implemented by us",
    "consists in the translation word by word of a romanian text ( using dictionary at http://lit.csci.unt.edu/  rada / downloads / ronlp / r.e .",
    "tralexand ) , then the application of chain algorithm to the english text .",
    "as the translation of a romanian word in english is multiple , the disambiguation of a triplet is modified as following .",
    "let be the word @xmath15 with @xmath35 translations @xmath36 , the word @xmath17 with @xmath37 translations @xmath38 and the word @xmath19 with @xmath39 translations @xmath40 .",
    "each triplet @xmath41 is disambiguated with the triplet disambiguation algorithm and then the triplet with the maxim score is selected :    begin    for @xmath42 do    for @xmath43 do    for @xmath44 do    disambiguate triplet @xmath45 in @xmath46    calculate @xmath47    endfor    endfor    endfor    calculate @xmath48    optimal translation of triplet is @xmath49    end    let us remark that @xmath50 , for example , is a synset which corresponds to the best translation for @xmath15 produced by chad algorithm .",
    "however , since in romanian are used many words linked by different spelling signs , these composed words are not found in the romanian - english dictionary .",
    "accordingly , not each romanian word produces an english correspondent as output of the above algorithm .",
    "however , many translations are still correct .",
    "for example , the translation of expression _ vreme trece _ ( in the poem `` glossa '' of our national poet mihai eminescu ) , is _ word : ( rom)vreme ( eng)age@xmath51 , word : ( rom)trece ( eng)@xmath52 _ . as another example from the same poem , where the synset of a word occurs ( as an output of our application ) , _ ine toate minte _ ,",
    "is translated in _ word : ( rom ) tine ( eng ) @xmath53 : \\{keep , maintain } , word : ( rom ) toate ( eng ) @xmath54 : \\{wholly , entirely , completely , totally , all , altogether , whole } , word : ( rom ) minte ( eng ) @xmath55 : \\{judgment , judgement , assessment}_.      the recognition of text entailment is one of the most complex task in natural language understanding @xcite .",
    "thus , a very important problem in some computational linguistic applications ( as question answering , summarization , segmentation of discourse , and others ) is to establish if a text _ follows _ from another text .",
    "for example , a qa system has to identify texts that entail the expected answer .",
    "similarly , in ir the concept denoted by a query expression should be entailed from relevant retrieved documents . in summarization",
    ", a redundant sentence should be entailed from other sentences in the summary .",
    "the application of wsd to text entailment verification is treated by authors in the paper `` text entailment verification with text similarity '' in this volume .",
    "in this paper we presented a new algorithm of word sense disambiguation .",
    "the algorithm is parametrized for : 1 .",
    "all words ( that means nouns , verbs , adjectives , adverbs ) ; 2 .",
    "all nouns ; 3 .",
    "all verbs .",
    "some experiments with this algorithm for ten files of brown corpus are presented in section 4.2 .",
    "the stemming was realized using the list from http://snowball.tartarus.org/algorithms/porter/diffs.txt .",
    "the precision is calculated relative to the corresponding annotated files in semcor corpus .",
    "some details of implementation are given in 4.1 .",
    "we showed in section 5 how the disambiguation of a text helps in automated translation of a text from a language into another language : each word in the first text is translated into the most appropriated word in the second text .",
    "this appropriateness is considered from two points of view : 1 .",
    "the point of view of possible translation and 2 .",
    "the point of view of the real sense ( disambiguated sense ) of the second text .",
    "some experiments with romanian - english translations and text entailment verification are given ( section 5 ) .",
    "another problem which we intend to address in the further work is that of optimization of a query in information retrieval . finding whether a particular sense is connected with an instance of a word is likely the ir task of finding whether a document is relevant to a query .",
    "it is established that a good wsd program can improve performance of retrieval .",
    "as ir is used by millions of users , an average of some percentages of improvement could be seen as very significant ."
  ],
  "abstract_text": [
    "<S> a large class of unsupervised algorithms for word sense disambiguation ( wsd ) is that of dictionary - based methods . </S>",
    "<S> various algorithms have as the root lesk s algorithm , which exploits the sense definitions in the dictionary directly . </S>",
    "<S> our approach uses the lexical base wordnet for a new algorithm originated in lesk s , namely _ chain algorithm for disambiguation _ of all words ( chad ) . </S>",
    "<S> we show how translation from a language into another one and also text entailment verification could be accomplished by this disambiguation . </S>"
  ]
}