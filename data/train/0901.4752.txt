{
  "article_text": [
    "in order to introduce our method , we will need some preliminary notions on mixtures of regressions and the lasso algorithm for sparse variable selection in regression models .",
    "the gaussian regression mixture assumes that the observations are couples of the form @xmath24 where @xmath25 takes its values in @xmath26 and conditionally on @xmath25 , the real valued random variable @xmath27 follows the mixture density @xmath28 such mixture models are frequent in econometrics and chemometrics as described in the introduction of @xcite .",
    "estimation in these models can be performed using likelihood maximization as in @xcite using the em algorithm or a bayesian methodology as studied in @xcite using monte carlo markov chain techniques .",
    "an important problem in the study of regression mixtures is the one of variable selection .",
    "this problem has been recently addressed in @xcite and several other works using various penalties based on nondifferentiable norm - like functions of the regression vectors like the @xmath23 norm as in the lasso @xcite or scad introduced by @xcite .",
    "the extension of the lasso technique to the present situation consists of penalizing the log - likelihood like function @xmath29 given by @xmath30 by the sum of the @xmath23-norms of the regression vectors in order to obtain the following estimator @xmath31 where @xmath32 is a parameter to be adapted to the sample size . in the case of one regression with known variance @xmath33 instead of a mixture , the recent theory of the lasso prescribes to take @xmath32 proportional to @xmath34 even in the case where @xmath35 ;",
    "see for instance @xcite . on the other hand ,",
    "the paper @xcite studies the case of proper mixtures with unknown variances but restricts the analysis to @xmath36 and @xmath37 is constant as a function of @xmath38 .",
    "a lot of work still remains to be done in the case where @xmath35 and the variance is unknown .        in all what follows",
    ", we will assume that the data have been centered .",
    "our proposal relies on the following simple idea : if the cluster probabilities @xmath39 s , the class indices @xmath4 s and the variances @xmath40 s where known ahead of time , the estimators of the @xmath19 s could be chosen , in the small sample setting , as linear combinations of the data vectors themselves , in the same spirit as the median is preferable to the mean based on the theory of robust statistics .",
    "however , chosing the right sample vectors to estimate the mean seems a priori to be a very hard task .",
    "the main ingredient in our proposal is to force our estimator to be very sparsely represented as a linear combination of the data .",
    "thus , the estimation of the mean vectors can be seen as a estimating a mixture of sparse regressions .",
    "the problem of estimating the right covariates being np - hard in general , the estimation can be performed using with a penalization enforcing sparsity just as the @xmath23 penalty in the lasso .    in the more general case",
    "where the indices @xmath4 are unobserved , and the cluster probabilities @xmath39 s and the variances @xmath40 s are unknown , one can consider maximizing the @xmath23-penalized log - likelihood like function given by @xmath41 under the data - driven constraints @xmath42 for @xmath43 where the matrix @xmath27 is given by @xmath44 $ ] .",
    "in other words , we would like to maximize the @xmath23-penalized likelihood like function @xmath45 where @xmath46      optimizing the @xmath23-penalized function ( [ like ] ) can be performed using an em - type algorithm .",
    "the expectation step consists of computing the conditional expectation of the complete @xmath23-penalized likelihood like function given the observations @xmath0 where the distribution of the latent variables is taken to be their marginal density parametrized by the approximation @xmath47 of the true parameter @xmath12 . the resulting quantity is traditionally denoted by @xmath48 and we will use the same notation in our @xmath23-penalized context .",
    "we will use the general form of covariance matrices instead of @xmath20 in order to present a more general form of the method .",
    "more precisely , the complete @xmath23-penalized log - likelihood like function @xmath49 , i.e. the penalized log - likelihood like function of the complete data @xmath50 ,  , @xmath51 is given by @xmath52 thus , we obtain @xmath53 where we used the standard notation @xmath54 .",
    "the maximization step consists of maximizing @xmath55 in order to make the computational part easy , the @xmath22 s , @xmath56 s and @xmath21 s can be optimized alternatively in the manner of the gauss - seidel approach .",
    "in fact , the separability of the problem into two subproblems , the first being optimization over the @xmath22 s and the second being optimization over the @xmath56 s and @xmath21 s is already well known and the solution of the first subproblem is of the form @xmath57 on the other hand , joint optimization in @xmath56 s and the @xmath21 s is not separable and space alternating option is necessary to keep the computational complexity of each step at a low level . in order to address this problem",
    ", we need a generalization of the em algorithm allowing for componentwise optimization at each step .",
    "such penalized em algorithms have been recently studied in the broader framework of space alternating kullback proximal point algorithms in @xcite .",
    "optimizing successively over the @xmath56 s at one iteration and over the @xmath21 s at the next iteration should be sufficiently efficient in practice . here",
    ", we will also optimize one cluster at a time in order to obtain the injectivity conditions which are needed in the theoretical analysis of the algorithm .",
    "a simple way to accelerate this extreme version of the gauss - seidel methodology would be to average the new iterates @xmath58 and @xmath59 with the previous respective iterates in order to smooth the algorithm s trajectory .",
    "@xmath60 choose intial iterate @xmath61 @xmath62 ( e - step ) compute the conditional probabilities @xmath63 given the observations @xmath0 for @xmath64 and @xmath65 using the following formula @xmath66 * compute *    *either * the @xmath67 s by the formula @xmath68",
    "*or * @xmath69 as the solution of the lasso - like optimization problem @xmath70 for the index @xmath71 updated in cyclic order along iterations .",
    "* @xmath72 using the formula @xmath73 for one index @xmath71 updated in cyclic order along iterations .",
    "@xmath74 , @xmath75 and @xmath76 for @xmath65 .",
    "when using a maximum likelihood approach , incorporation of a nondifferentiable penalty in the em algorithm may cause some technical difficulties . a rigourous analysis has been proposed in @xcite in the case of general nondifferentiable penalties and space alternating optimization versions of the em algorithm .",
    "the convergence analysis is made easier after interpreting the em algorithm as a proximal point algorithm which was first done in @xcite ( see also @xcite for more precise results ) .    in our special case ,",
    "we only need to show that our space - alternating l1-em is a space - alternating kullback proximal point algorithm of the form studied in @xcite .",
    "the space alternating l1-em algorithm is a space alternating kullback proximal point algorithm as defined in definition 2.1.1 . of @xcite ,",
    "i.e. the iterations can be written @xmath77 where @xmath78 : @xmath79 be a continuously differentiable mapping , @xmath32 be a positive real vector in @xmath80 , @xmath81 be a possibly nonsmooth penalty function with bounded clarke subdifferential on compact sets and the parameter space is decomposed into subspaces @xmath82 ,",
    "@xmath83 where @xmath84 are subspaces of @xmath26 and @xmath85 .",
    "* proof*. first , we adopt the decomposition of the parameter space into the cartesian product of the @xmath22 s space , the @xmath56 s space and the @xmath21 s space .",
    "more precisely @xmath86 is the simplex in @xmath87 and @xmath88 , @xmath89 , and @xmath90 and @xmath91 for @xmath65 . thus @xmath92 takes its values in the list @xmath93 .",
    "then the mappings @xmath94 are just the orthogonal projections onto @xmath95 for @xmath96 . moreover @xmath97 and @xmath98 for @xmath65 because the class probabilities and the variances are not penalized . moreover",
    "@xmath99 for @xmath65 .",
    "next , the @xmath100-function can be written @xmath101 with @xmath102 where @xmath103 thus , the space alternating lasso - em algorithm is a special case of the space alternating kullback proximal point algorithm for which the sequence @xmath104 is constant and the terms are all equal to one .",
    "@xmath105    in the following , we will denote the list of values for the parameter @xmath92 by @xmath106 .",
    "we then have the following theorem .    [ bordp ]",
    "let @xmath12 be a cluster point of the space alternating penalized kullback proximal sequence .",
    "if @xmath12 lies in the interior of @xmath107 , then @xmath12 satisfies the following property : there exists a set of subsets @xmath108 where @xmath109 denotes the index of the active constraints at @xmath12 , i.e. @xmath110 , and there is a family of real numbers @xmath111 , @xmath112 , @xmath113 such that the following karush - kuhn - tucker condition for optimality holds at cluster point @xmath12 : @xmath114    * proof*. we start by verifying that assumptions 2.2.1 , 2.2.3 and assumptions 2.2.4 of @xcite hold in our case .",
    "the differentiability requirement in assumptions 2.2.1.(i ) .",
    "is obvious .",
    "however , if one @xmath56 belongs to the kernel of @xmath25 , it may be of any arbitrary large norm without leading the log - likelihood towards @xmath115 .",
    "however , note that , as is well known is gaussian mixture models , @xmath116 tends to @xmath18 only at finite number of degenerate points .",
    "thus , since , the penalization term @xmath81 tends to @xmath18 as the norm of any @xmath56 tends to @xmath18 , the difference @xmath117 tends to @xmath115 if the norm of any @xmath56 goes to @xmath18 .",
    "since @xmath29 also tends to @xmath115 as any variance tends to @xmath18 , the term @xmath118 tends to @xmath115 when the norm of @xmath119 tends to @xmath18 .",
    "the domain @xmath120 is defined by the fact that the term inside the log in ( [ lkhd ] ) must be positive . on the other hand , for any @xmath47 in @xmath121 , the domain @xmath122 is the set of the @xmath119 s for which the @xmath123 are positive , and therefore , does not depend on @xmath47 .",
    "moreover , the set of @xmath119 s for which the @xmath123 are positive is @xmath120 .",
    "thus , the projection of @xmath124 onto the first coordinate is @xmath120 and assumptions 2.2.1.(ii ) .",
    "are satisfied .",
    "assumptions 2.2.1.(iii ) . is immediate since here the relaxation sequence ( denoted here by @xmath125 ) is constant .",
    "assumptions 2.2.1.(iv ) . is also straightforward since the mappings @xmath94 are orthogonal projections onto @xmath95 , @xmath126 .    in our context , based on ( [ toto ] )",
    ", we have @xmath127 and assumptions 2.2.3.(i)-(iii ) . are easily verified .",
    "injectivity of the mapping @xmath128 when restricted to @xmath129 is proved in @xcite and thus , injectivity holds on each @xmath130,  ,@xmath131 and assumption 2.2.3.(iv ) holds .",
    "moreover , since @xmath132 implies that @xmath133 and @xmath133 implies @xmath134 for all @xmath135 and @xmath136 and @xmath137 it follows that @xmath138 if @xmath95 is the vector space generated by the probability vectors @xmath139 and @xmath140 otherwise .",
    "let @xmath12 be a cluster point in the interior of @xmath120 .",
    "since the @xmath141 are clearly continuously differentiable around such a @xmath12 , corollary 1 in @xcite gives that @xmath12 satisfies the following property : there exists a set of subsets @xmath142 and a family of real numbers @xmath111 , @xmath143 , @xmath113 such that the following karush - kuhn - tucker condition for optimality holds at cluster point @xmath12 : @xmath144 which is the desired result .",
    "@xmath105    the meaning of this theorem is simply that a karush - kuhn - tucker condition is satisfied at any cluster point in the domain of definition of the log - likelihood .",
    "in this section , we address the question of testing the algorithm on simulated and datasets .",
    "the space alternating @xmath23-em was first tested on simulated data sets .",
    "the experiments were built as follows : 10 samples in @xmath145 were generated from three different gaussian distributions with the objective to recover the index of the distribution they were drawn from up to some index permutation .",
    "the class probabilities were taken as @xmath146 , @xmath147 and @xmath148 and the variances as @xmath149 , @xmath150 and @xmath151 without change through all the simulation experiments .",
    "various experiments were performed using different values for the expectation vectors @xmath152 , @xmath153 and @xmath154 since it could be easily suspected that the distance between them would play a major role in the class index recovery problem . the results presented below were obtained using the following monte carlo scheme : the expectations were isotropic dilations of three points in @xmath145 drawn uniformly at random in the cube @xmath155 ^ 3 $ ] .",
    "we ran the code for dilation factors @xmath156 going from 10 to 100 by steps of 10 .",
    "an example of the type of result we obtained is given in figure [ ex1 ] below where the 10 points were correctly classified .",
    "here is another example when the expectation vectors are chosen closer to each other and 8 points over 10 were correctly classified .",
    "our monte carlo experiments are given in figure [ mc ] .",
    "for each dilation parameter @xmath156 , 1000 monte carlo experiments were performed and the number of points correctly classified was computed by finding the best permutation of the set @xmath157 matching the class indices obtained by the output of the space alternating @xmath23 algorithm .",
    "these preliminary results show that the number of correctly recovered class indices increases with the dilation factor of the unit cube into which the expectation vector were drawn uniformly at random .",
    "moreover , the larger the dilation facteur , i.e. the better separated the gaussians are , the closer to 80% of correctly identified class indices the space alternating @xmath23 em with data - driven constraint provides .",
    "the number of correctly recovered class indices for 1000 monte carlo experiments in the case of 10 points as a function of the box into which the expectation vectors have been uniformly drawn are given in table [ avnb ] below .",
    "[ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     as table [ avnb ] shows , the class index recovery rate is still quite good in dimension 50 for well separed mixtures and confirm our primary intuition .",
    "a look at table [ gavnb ] shows that our method compares quite well with the standard likelihood approach for gaussian mixtures estimation , especially in the higher dimensions where the average number of well classified data is better by often more than one unit .",
    "giving a rigourous argument justifying these observations is currently under investigation but more experiments should be performed in order to explore in finer details the behavior of the method in more realistic context .",
    "the goal of this paper was to propose a robust version of the maximum likelihood strategy for the estimation of finite gaussian mixtures .",
    "our approach is based on self - regression and sparse variable selection .",
    "sparsity was promoted by using an @xmath23 penalty as in the lasso .",
    "we developed a space alternating version of the penalized em algrithm and proved that the interesting cluster points satisfy the karush - kuhn - tucker optimality conditions .",
    "our method was then tested on simulated datasets .",
    "in particular , the monte carlo experiments showed that cluster identification was more robust with our approach than by using the standard maximum likelihood estimator .",
    "theoretical justifications of these observations ought to be investigated in a near future in order to increase our understanding of the strengths and weaknesses of this approach .        c. biernacki , g. celeux , g. govaert and f. langrognet , ( 2006 ) `` model - based cluster and discriminant analysis with the mixmod software '' , computational statistics and data analysis , vol .",
    "51 , 2 , 587600 .",
    "s. chrtien , a. hero and h. perdry ( 2008 ) `` space alternating penalized kullback proximal point algorithms for maximing likelihood with nondifferentiable penalty '' , submitted .",
    "available at http : //arxiv.org"
  ],
  "abstract_text": [
    "<S> many experiments in medicine and ecology can be conveniently modelled by finite gaussian mixtures but face the problem of dealing with small data sets . </S>",
    "<S> we propose a robust version of the estimator based on self - regression and sparsity promoting penalization in order to estimate the components of gaussian mixtures in such contexts . </S>",
    "<S> a space alternating version of the penalized em algorithm is obtained and we prove that its cluster points satisfy the karush - kuhn - tucker conditions . </S>",
    "<S> monte carlo experiments are presented in order to compare the results obtained by our method and by standard maximum likelihood estimation . in particular </S>",
    "<S> , our estimator is seen to perform well better than the maximum likelihood estimator .    </S>",
    "<S> finite gaussian mixtures , maximum likelihood estimation , kullback proximal point algorithms , em algorithm , l1 penalization , lasso , sparsity , regression mixtures , model based clustering    finite gaussian mixture models are widely used in a great number of application fields as a means to perform model based classification . from pattern recognition to biology , from quality control to finance , many examples have shown the pertinence of the gaussian mixture model approach . the book @xcite is the most comprehensive reference for finite non necessarily gaussian mixture models with many application examples . in gaussian mixture models , </S>",
    "<S> the data @xmath0 are assumed i.i.d . and to be drawn from the density @xmath1 where the vector @xmath2 is an unknown multidimensional parameter . to this model , we traditionally associate an extended model using the notion of complete data . in mixture models , </S>",
    "<S> the complete data are independent and identically distributed couples of the form @xmath3 where @xmath4 is a multinomial random variable taking values in @xmath5 with @xmath6 and which represents the index of the mixture component from which observation @xmath7 was drawn . </S>",
    "<S> we assume that conditionally on the event @xmath8 , @xmath9 has density @xmath10 . </S>",
    "<S> the variables @xmath11 being unobserved , they are usually called latent variables .    the standard approach for estimating @xmath12 is the maximum likelihood methodology which consists of finding @xmath13 which maximizes the log - likelihood function @xmath14 over the set @xmath15 where @xmath16 denotes the set of all symmetric positive semidefinite matrices .    </S>",
    "<S> interestingly enough , the supremum of the log - likelihood function over @xmath17 is equal to @xmath18 and is obtained for singular covariance matrices . </S>",
    "<S> a study of the one dimensional case was made in @xcite . </S>",
    "<S> thus , exact likelihood maximization is not the good approach for this problem . </S>",
    "<S> however , many researchers and practitioners have noticed that some local maximizer of the log - likelihood function is in fact consistent in practice . from the numerical viewpoint , local maximizers of the log - likelihood function </S>",
    "<S> are usually obtained using the expectation - maximization ( em ) algorithm of dempster laird and rudin @xcite . </S>",
    "<S> this algorithm is a nice procedure with closed form expression of each iteration in the gaussian mixture case . </S>",
    "<S> the em algorithm for mixture models is available in the mixmod package @xcite within matlab or scilab for instance .    beside the question of finding the right local optimizer of the likelihood function , one of the main problems for estimating @xmath12 relies in having a sufficiently great sample size . </S>",
    "<S> usually , large sample sizes may sometimes be available in a number of applications such as pattern recognition or financial time series analysis but in ecology for instance the sample size may be very small in situations where finite mixture models are suspected to be very pertinent due to the biological context . </S>",
    "<S> the goal of this paper is to remedy this problem by proposing a new methodology for gaussian mixture model estimation in the case where the sample size is extremely small . </S>",
    "<S> our approach needs to provide a certain amount of robustness . in the same spirit as for the median in the one dimensional case , </S>",
    "<S> the main idea is to express the estimators of the @xmath19 s as a combination of a small number of data in the middle of each cluster . </S>",
    "<S> this is simply done by restricting the search to the data s span , i.e. to obtain the @xmath19 s as a regression with covariates the data themselves and to impose an additional sparsity constraint on the regression vectors . in order to simplify the analysis </S>",
    "<S> , we will assume the covariance matrices to be of the form @xmath20 . the @xmath21 s and the @xmath22 s </S>",
    "<S> can also be estimated using for instance a maximum likelihood approach conditioned on the estimated value of the @xmath19 s .    </S>",
    "<S> the wole procedure is formally equivalent to joint variable selection and estimation in a mixture of regression model . </S>",
    "<S> variable selection and estimation are performed using @xmath23-penalized em steps which reduce the complexity of the regression model just as for the lasso @xcite . </S>",
    "<S> encouraging simulations results show that the proposed approach correctly estimates the class of 8 over 10 points on average for a mixture of 3 gaussians in dimension two . </S>",
    "<S> monte carlo experiments are performed for samples sizes of 10 points and dimension growing up to to 50 showing a good behavior of the method which outperforms the standard maximum likelihood estimator . </S>"
  ]
}