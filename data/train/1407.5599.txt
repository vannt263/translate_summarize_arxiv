{
  "article_text": [
    "the general perception is that kernel methods are not scalable . when it comes to large - scale nonlinear learning problems , the methods of choice so far are neural nets where theoretical understanding remains incomplete .",
    "are kernel methods really not scalable ? or is it simply because we have not tried hard enough , while neural nets have exploited sophisticated design of feature architectures , virtual example generation for dealing with invariance , stochastic gradient descent for efficient training , and gpus for further speedup ?    a bottleneck in scaling up kernel methods is the storage and computation of the kernel matrix , @xmath3 , which is usually dense . storing the matrix requires @xmath4 space , and computing it takes @xmath5 operations , where @xmath6 is the number of data points and @xmath7 is the dimension .",
    "there have been many great attempts to scale up kernel methods , including efforts from numerical linear algebra , functional analysis , and numerical optimization perspectives .",
    "a common numerical linear algebra approach is to approximate the kernel matrix using low - rank factors , @xmath8 , with @xmath9 and rank @xmath10 .",
    "this low - rank approximation usually requires @xmath11 operations , and then subsequent kernel algorithms can directly operate on @xmath12 .",
    "many works , such as greedy basis selection techniques  @xcite , nystrm approximation  @xcite and incomplete cholesky decomposition  @xcite , all followed this strategy . in practice",
    ", one observes that kernel methods with approximated kernel matrices often result in a few percentage of losses in performance .",
    "in fact , without further assumption on the regularity of the kernel matrix , the generalization ability after low - rank approximation is typically of the order @xmath13",
    "@xcite , which implies that the rank needs to be nearly linear in the number of data points ! thus , in order for kernel methods to achieve the best generalization ability , the low - rank approximation based approaches quickly become impractical for big datasets due to their @xmath14 preprocessing time and @xmath4 memory requirement .",
    "random feature approximation is another popular approach for scaling up kernel methods  @xcite .",
    "instead of approximating the kernel matrix , the method directly approximates the kernel function using explicit feature maps .",
    "the advantage of this approach is that the random feature matrix for @xmath6 data points can be computed in time @xmath15 using @xmath16 memory , where @xmath17 is the number of random features .",
    "subsequent algorithms then only operate on an @xmath16 matrix .",
    "similar to low - rank kernel matrix approximation approach , the generalization ability of random feature approach is of the order @xmath13",
    "@xcite , which implies that the number of random features also needs to be @xmath18 .",
    "another common drawback of these two approaches is that it is not easy to adapt the solution from a small @xmath17 to a large @xmath19 .",
    "often one is interested in increasing the kernel matrix approximation rank or the number of random features to obtain a better generalization ability .",
    "then special procedures need to be designed to reuse the solution obtained from a small @xmath17 , which is not straightforward .",
    "another approach that addresses the scalability issue rises from optimization perspective .",
    "one general strategy is to solve the dual forms of kernel methods using coordinate or block - coordinate descent  ( ,  @xcite ) . by doing so",
    ", each iteration of the algorithm only incurs @xmath15 computation and @xmath16 memory , where @xmath17 is the size of the parameter block .",
    "a second strategy is to perform functional gradient descent by looking at a batch of data points at a time  ( ,  @xcite ) .",
    "thus , the computation and memory requirements are also @xmath15 and @xmath16 respectively in each iteration , where @xmath17 is the batch size .",
    "these approaches can easily change to a different @xmath17 without restarting the optimization and has no loss in generalization ability since they do not approximate the kernel matrix or function .",
    "however , a serious drawback of these approaches is that , without further approximation , all support vectors need to be kept for testing , which can be as big as the entire training set ! ( , kernel ridge regression and non - separable nonlinear classification problems . )    in summary , there exists a delicate trade - off between computation , memory and statistics if one wants to scale up kernel methods . inspired by various previous efforts , we propose a simple yet general strategy to scale up many kernel methods using a novel concept called `` _ _ doubly stochastic functional gradients _ _ '' .",
    "our method relies on the fact that most kernel methods can be expressed as convex optimization problems over functions in reproducing kernel hilbert spaces ( rkhs ) and solved via functional gradient descent .",
    "our algorithm proceeds by making _",
    "two unbiased _ stochastic approximations to the functional gradient , one using random training points and the other one using random features associated with the kernel , and then descending using this noisy functional gradient .",
    "the key intuitions behind our algorithm originate from    * the property of stochastic gradient descent algorithm that as long as the stochastic gradient is unbiased , the convergence of the algorithm is guaranteed  @xcite ; and * the property of pseudo - random number generators that the random samples can in fact be completely determined by an initial value ( a seed ) .",
    "we exploit these properties and enable kernel methods to achieve better balances between computation , memory and statistics .",
    "our method interestingly combines kernel methods , functional analysis , stochastic optimization and algorithmic trick , and it possesses a number of desiderata : + * generality and simplicity . *",
    "our approach applies to many kernel methods , such as kernel ridge regression , support vector machines , logistic regression , two - sample test , and many different types of kernels , such as shift - invariant kernels , polynomial kernels , general inner product kernels , and so on .",
    "the algorithm can be summarized in just a few lines of code ( algorithm 1 and 2 ) . for a different problem and kernel ,",
    "we just need to adapt the loss function and the random feature generator . +",
    "* flexibility .",
    "* different from previous uses of random features which typically prefix the number of features and then optimize over the feature weightings , our approach allows the number of random features , and hence the flexibility of the function class , to grow with the number of data points .",
    "this allows our method to be applicable to data streaming setting , which is not possible for previous random feature approach , and achieve the full potential of nonparametric methods . +",
    "* efficient computation .",
    "* the key computation of our method is evaluating the doubly stochastic functional gradient , which involves the generation of the random features with specific random seeds and the evaluation of these random features on the small batch of data points .",
    "for iteration @xmath0 , the computational complexity is @xmath20 . +",
    "* small memory . *",
    "the doubly stochasticity also allows us to avoid keeping the support vectors which becomes prohibitive in large - scale streaming setting .",
    "instead , we just need to keep a small program for regenerating the random features , and sample previously used random feature according to pre - specified random seeds . for iteration @xmath0",
    ", the memory needed is @xmath21 independent of the dimension of the data .",
    "+ * theoretical guarantees . *",
    "we provide a novel and nontrivial analysis involving hilbert space martingale and a newly proved recurrence relation , and show that the estimator produced by our algorithm , which might be outside of the rkhs , converges to the optimal rkhs function . more specifically , both in expectation and with high probability , our algorithm can estimate the optimal function in the rkhs in the rate of @xmath1 , which are indeed optimal  @xcite , and achieve a generalization bound of @xmath2 .",
    "the variance of the random features , introduced during our second approximation to the functional gradient , only contributes additively to the constant in the final convergence rate .",
    "these results are the first of the kind in kernel method literature , which can be of independent interest .",
    "+ * strong empirical performance .",
    "* our algorithm can readily scale kernel methods up to the regimes which are previously dominated by neural nets .",
    "we show that our method compares favorably to other scalable kernel methods in medium scale datasets , and to neural nets in big datasets such as 8 million handwritten digits from mnist , 2.3 million materials from molecularspace , and 1 million photos from imagenet using convolution features .",
    "our results suggest that kernel methods , theoretically well - grounded methods , can potentially replace neural nets in many large scale real - world problems where nonparametric estimation are needed .",
    "+ in the remainder , we will first introduce preliminaries on kernel methods and functional gradients .",
    "we will then describe our algorithm and provide both theoretical and empirical supports .",
    "kernel methods owe their name to the use of kernel functions , @xmath22 , which are symmetric positive definite ( pd ) , meaning that for all @xmath23 , and @xmath24 , and @xmath25 , we have @xmath26 .",
    "there is an intriguing duality between kernels and stochastic processes which will play a crucial role in our later algorithm design .",
    "more specifically ,    if @xmath27 is a pd kernel , then there exists a set @xmath28 , a measure @xmath29 on @xmath28 , and random feature @xmath30 from @xmath31 , such that @xmath32    essentially , the above integral representation relates the kernel function to a random process @xmath33 with measure @xmath34 .",
    "note that the integral representation may not be unique .",
    "for instance , the random process can be a gaussian process on @xmath35 with the sample function @xmath36 , and @xmath27 is simply the covariance function between two point @xmath37 and @xmath38 .",
    "if the kernel is also continuous and shift invariant ,  ,  @xmath39 for @xmath40 , then the integral representation specializes into a form characterized by inverse fourier transformation ( ,  ( * ? ? ?",
    "* theorem 6.6 ) ) , +    a continuous , real - valued , symmetric and shift - invariant function @xmath41 on @xmath42 is a pd kernel if and only if there is a finite non - negative measure @xmath34 on @xmath42 , such that @xmath43 } 2 \\ , \\cos(\\omega^\\top x + b)\\ , \\cos(\\omega^\\top x ' + b)\\ , d \\rbr{\\pp(\\omega ) \\times \\pp(b ) } ,    $ ] where @xmath44 is a uniform distribution on @xmath45 $ ] , and @xmath46 .",
    "+    for gaussian rbf kernel , @xmath47 , this yields a gaussian distribution @xmath34 with density proportional to @xmath48 ; for the laplace kernel , this yields a cauchy distribution ; and for the martern kernel , this yields the convolutions of the unit ball  @xcite .    similar representation where the explicit form of @xmath36 and @xmath34 are known can also be derived for rotation invariant kernel , @xmath49 , using fourier transformation on sphere  @xcite . for polynomial kernels , @xmath50 , a random tensor sketching approach",
    "can also be used  @xcite .",
    "explicit random features have been designed for many other kernels , such as dot product kernel  @xcite , additive / multiplicative class of homogeneous kernels  @xcite , , hellinger s , @xmath51 , jensen - shannon s and intersection kernel , as well as kernels on abelian semigroups  @xcite .",
    "we summarized these kernels with their explicit features and associated densities in table  [ table : explicit_features ] .",
    "ll|c|c|c & kernel & @xmath52 & @xmath36 & @xmath53 + & gaussian & @xmath54 & @xmath55 & @xmath56 + & laplacian & @xmath57 & @xmath55 & @xmath58 + & cauchy & @xmath59 & @xmath55 & @xmath60 + & matrn & @xmath61 & @xmath55 & @xmath62 + & dot product & @xmath63 & @xmath64 & @xmath65 = \\frac{1}{p^{n+1}}$ ] + & & & & @xmath66 + & polynomial & @xmath67 & @xmath68 & @xmath69 + & & & & @xmath70 + & hellinger & @xmath71 & @xmath72 & @xmath73 + & @xmath51 & @xmath74 & @xmath75_{j=1}^d$ ] & @xmath76 + & intersection & @xmath77 & @xmath78_{j=1}^d$ ] & @xmath79 + & jensen - shannon & @xmath80 & @xmath78_{j=1}^d$ ] & @xmath81 + & & @xmath82 & @xmath83 & @xmath84 + & & @xmath85 & @xmath83 & @xmath86 + & & @xmath87 & @xmath88 & @xmath89 + & & @xmath90 & @xmath88 & @xmath91 + & arc - cosine & @xmath92 & @xmath93 & @xmath94 +     +   +    instead of finding the random process @xmath34 and function @xmath36 given a kernel , one can go the reverse direction , and construct kernels from random processes and functions ( , @xcite ) .",
    "[ thm : inverse_dual ] if @xmath95 for a nonnegative measure @xmath34 on @xmath28 and @xmath96 , each component from @xmath31 , then @xmath27 is a pd kernel .    for instance , @xmath97 , where @xmath98 can be a random convolution of the input @xmath37 parametrized by @xmath99 , or @xmath100 $ ] , where @xmath101 denote the random feature for kernel @xmath102 .",
    "the former random features define a hierachical kernel  @xcite , and the latter random features induce a linear combination of multiple kernels .",
    "it is worth to note that the hellinger s , @xmath51 , jensen - shannon s and intersection kernels in  @xcite are special cases of multiple kernels combination . for simplicity ,",
    "we assume @xmath103 following , and our algorithm is still applicable to @xmath104 .",
    "another important concept is the reproducing kernel hilbert space ( rkhs ) .",
    "an rkhs @xmath105 on @xmath35 is a hilbert space of functions from @xmath35 to @xmath106 .",
    "@xmath105 is an rkhs if and only if there exists a @xmath107 such that @xmath108 if such a @xmath27 exist , it is unique and it is a pd kernel .",
    "a function @xmath109 if and only if @xmath110 , and its @xmath111 norm is dominated by rkhs norm @xmath112",
    "many kernel methods can be written as convex optimizations over functions in the rkhs and solved using the functional gradient methods  @xcite . inspired by these previous works",
    ", we will introduce a novel concept called `` _ _ doubly stochastic functional gradients _ _ '' to address the scalability issue .",
    "let @xmath113 be a scalar ( potentially non - smooth ) loss function convex of @xmath114 .",
    "let the subgradient of @xmath115 with respect to @xmath116 be @xmath117 . given",
    "a pd kernel @xmath27 and the associated rkhs @xmath105 , many kernel methods try to find a function @xmath118 which solves the optimization problem @xmath119 + \\frac{\\nu}{2}\\nbr{f}_{\\hcal}^2 \\quad \\longleftrightarrow\\quad \\argmin_{\\nbr{f}_{\\hcal}\\leqslant b(\\nu)}~~ \\ee_{(x , y)}[l(f(x ) , y)]\\end{aligned}\\ ] ] where @xmath120 is a regularization parameter , @xmath121 is a non - increasing function of @xmath122 , and the data @xmath123 follow a distribution @xmath124 . the functional gradient @xmath125 is defined as the linear term in the change of the objective after we perturb @xmath126 by @xmath127 in the direction of @xmath128 ,  , @xmath129 for instance , applying the above definition , we have @xmath130 , and @xmath131 .    * stochastic functional gradient . * given a data point @xmath132 and @xmath109 , the stochastic functional gradient of @xmath133 $ ] with respect to @xmath109 is @xmath134 which is essentially a single data point approximation to the true functional gradient . furthermore , for any @xmath135 , we have @xmath136 .",
    "inspired by the duality between kernel functions and random processes , we can make an additional approximation to the stochastic functional gradient using a random feature @xmath36 sampled according to @xmath34 .",
    "more specifically ,    * doubly stochastic functional gradient .",
    "* let @xmath137 , then the doubly stochastic gradient of @xmath133 $ ] with respect to @xmath109 is @xmath138    r0.5        note that the stochastic functional gradient @xmath139 is in rkhs @xmath105 but @xmath140 may be outside @xmath105 , since @xmath141 may be outside the rkhs .",
    "for instance , for the gaussian rbf kernel , the random feature @xmath142 is outside the rkhs associated with the kernel function .",
    "however , these functional gradients are related by @xmath143 , which lead to unbiased estimators of the original functional gradient ,  , @xmath144 we emphasize that the source of randomness associated with the random feature is not present in the data , but artificially introduced by us .",
    "this is crucial for the development of our scalable algorithm in the next section .",
    "meanwhile , it also creates additional challenges in the analysis of the algorithm which we will deal with carefully .",
    "' '' ''    ' '' ''     +    [ alg : ksup ] sample @xmath145",
    ". sample @xmath146 with seed @xmath147 . @xmath148 . @xmath149 . @xmath150 for @xmath151 .    ' '' ''       ' '' ''    ' '' ''     +    [ alg : testing ] set @xmath152 .",
    "sample @xmath146 with seed @xmath147 . @xmath153 .    ' '' ''    the first key intuition behind our algorithm originates from the property of stochastic gradient descent algorithm that as long as the stochastic gradient is unbiased , the convergence of the algorithm is guaranteed  @xcite . in our algorithm , we will exploit this property and introduce _ two _ sources of randomness , one from data and another artificial , to scale up kernel methods .",
    "the second key intuition behind our algorithm is that the random features used in the doubly stochastic functional gradients will be sampled according to _ pseudo - random number generators _ , where the sequences of apparently random samples can in fact be completely determined by an initial value ( a seed ) .",
    "although these random samples are not the `` true '' random sample in the purest sense of the word , however they suffice for our task in practice .",
    "more specifically , our algorithm proceeds by making two unbiased stochastic approximation to the functional gradient in each iteration , and then descending using this noisy functional gradient .",
    "the overall algorithms for training and prediction is summarized in algorithm 1 and 2 .",
    "the training algorithm essentially just performs random feature sampling and doubly stochastic gradient evaluation , and maintains a collection of real number @xmath154 , which is computationally efficient and memory friendly .",
    "a crucial step in the algorithm is to sample the random features with `` _ _ seed @xmath147 _ _ '' .",
    "the seeds have to be aligned between training and prediction , and with the corresponding @xmath155 obtained from each iteration .",
    "the learning rate @xmath156 in the algorithm needs to be chosen as @xmath1 , as shown by our later analysis to achieve the best rate of convergence .",
    "for now , we assume that we have access to the data generating distribution @xmath124 .",
    "this can be modified to sample uniformly randomly from a fixed dataset , without affecting the algorithm and the later convergence analysis .",
    "let the sampled data and random feature parameters be @xmath157 and @xmath158 respectively after @xmath0 iteration , the function obtained by algorithm 1 is a simple additive form of the doubly stochastic functional gradients @xmath159 where @xmath160 are deterministic values depending on the step sizes @xmath161 and regularization parameter @xmath122 .",
    "this simple form makes it easy for us to analyze its convergence .",
    "we note that our algorithm can also take a mini - batch of points and random features at each step , and estimate an empirical covariance for preconditioning to achieve potentially better performance .",
    "our algorithm is general and can be applied to most of the kernel machines which are formulated in the convex optimization  ( [ eq : primal ] ) in a rkhs @xmath105 associated with given kernel @xmath52 .",
    "we will instantiate the doubly stochastic gradients algorithms for a few commonly used kernel machines for different tasks and loss functions , , regression , classification , quantile regression , novelty detection and estimating divergence functionals / likelihood ratio .",
    "interestingly , the gaussian process regression , which is a bayesian model , can also be reformulated as the solution to particular convex optimizations in rkhs , and therefore , be approximated by the proposed algorithm . +",
    "* kernel support vector machine  ( svm ) .",
    "* hinge loss is used in kernel svm where @xmath162 with @xmath163 .",
    "we have @xmath164 and the step 5 in algorithm .  1 .",
    "becomes @xmath165    * remark : * @xcite used squared hinge loss , @xmath166 , in @xmath167-svm . with this loss function , we have @xmath168 and the step 5 in algorithm .  1 .",
    "becomes @xmath169    * kernel logistic regression .",
    "* log loss is used in kernel logistic regression for binary classification where @xmath170 with @xmath171 .",
    "we have @xmath172 and the step 5 in algorithm .  1 .",
    "becomes @xmath173    for the multi - class kernel logistic regression , the @xmath174 where @xmath175 is the number of categories , @xmath176 , @xmath177 and @xmath178 only if @xmath179 , otherwise @xmath180 .",
    "in such scenario , we denote @xmath181 $ ] , and therefore , the corresponding @xmath182 $ ] . the update rule for @xmath183 in algorithm .  1 . is    @xmath184    * kernel ridge regression . * square loss is used in kernel ridge regression where @xmath185 .",
    "we have @xmath186 and the step 5 in algorithm .  1 .",
    "becomes @xmath187    * kernel robust regression .",
    "* huber s loss is used for robust regression  @xcite where @xmath188 we have @xmath189 and the step 5 in algorithm",
    ".  1 . becomes @xmath190    * kernel support vector regression  ( svr ) .",
    "* @xmath127-insensitive loss function is used in kernel svr where @xmath191 .",
    "we have @xmath192 and the step 5 in algorithm .  1 .",
    "becomes @xmath193    * remark : * note that if we set @xmath194 , the @xmath127-intensitive loss function will become absolute deviatin , , @xmath195 .",
    "therefore , we have the updates for * kernel least absolute deviatin regression*.    * kernel quantile regression . *",
    "the loss function for quantile regression is @xmath196 .",
    "we have @xmath197 and the step 5 in algorithm .  1 .",
    "becomes @xmath198    * kernel novelty detection . *",
    "the loss function @xmath199  @xcite is proposed for novelty detection . since @xmath200 is also a variable which needs to be optimized ,",
    "the optimization problem is formulated as @xmath201 + \\frac{\\nu}{2}\\|f\\|_{\\hcal}^2",
    "- \\nu \\tau,\\end{aligned}\\ ] ] and the gradient of @xmath202 is @xmath203    the step 5 in algorithm",
    ".  1 . becomes @xmath204    * kernel density ratio estimation . *",
    "based on the variational form of ali - silvey divergence , , @xmath205 $ ] , where @xmath206 is a convex function with @xmath207 , @xcite proposed a nonparametric estimator for the logarithm of the density ratio , @xmath208 , which is the solution of following convex optimization , @xmath209 + \\ee_{p}[r^*(-\\exp(f ) ) ] + \\frac{\\nu}{2}\\|f\\|_\\hcal^2\\end{aligned}\\ ] ] where @xmath210 denotes the fenchel - legendre dual of @xmath17 , @xmath211 . in kullback - leibler  ( kl )",
    "divergence , the @xmath212 .",
    "its fenchel - legendre dual is @xmath213 specifically , the optimization becomes @xmath214 - \\ee_{x\\sim p}[f(x ) ] + \\frac{\\nu}{2}\\|f\\|_\\hcal^2 \\\\ & = &   2\\ee_{z , x , y}\\bigg[\\delta_1(z)\\exp(f(y ) ) - \\delta_{0}(z)f(x)\\bigg ] + \\frac{\\nu}{2}\\|f\\|_\\hcal^2 .\\end{aligned}\\ ] ] where @xmath215 .",
    "denote @xmath216 , we have @xmath217 and the the step 5 in algorithm .  1 .",
    "becomes @xmath218 in particular , the @xmath219 and @xmath220 are not sampled in pair , they are sampled independently from @xmath221 and @xmath222 respectively .",
    "@xcite proposed another convex optimization based on @xmath223 whose solution is a nonparametric estimator for the density ratio .",
    "@xcite designed @xmath224 for novelty detection .",
    "similarly , the doubly stochastic gradients algorithm is also applicable to these loss functions .",
    "* gaussian process regression . * the doubly stochastic gradients can be used for approximating the posterior of gaussian process regression by reformulating the mean and variance of the predictive distribution as the solutions to the convex optimizations with particular loss functions .",
    "let @xmath225 where @xmath226 and @xmath227 , given the dataset @xmath228 , the posterior distribution of the function at the test point @xmath229 can be derived as @xmath230 where @xmath231 , @xmath232 , @xmath233^\\top$ ] and @xmath234 is the identity matrix .",
    "obviously , the posterior mean of the gaussian process for regression can be thought as the solution to optimization problem  ( [ eq : primal ] ) with square loss and setting @xmath235 .",
    "therefore , the update rule for approximating the posterior mean will be the same as kernel ridge regression .",
    "to compute the predictive variance , we need to evaluate the @xmath236 .",
    "following , we will introduce two different optimizations whose solutions can be used for evaluating the quantity .    1",
    ".   denote @xmath237 $ ] , then @xmath238 where the second equation based on identity @xmath239 .",
    "therefore , we just need to estimate the operator : @xmath240 + we can express @xmath241 as the solution to the following convex optimization problem @xmath242 where @xmath243 is the hilbert - schmidt norm of the operator .",
    "we can achieve the optimum by @xmath244 , which is equivalent to eq .",
    "[ eq : variance_operator ] .",
    "+ based on this optimization , we approximate the @xmath245 using @xmath246 by doubly stochastic functional gradients .",
    "the update rule for @xmath99 is @xmath247 please refer to appendix  [ appendix : gp_update_rule ] for the details of the derivation .",
    "2 .   assume that the testing points , @xmath248 , are given beforehand , instead of approximating the operator @xmath241 , we target on functions @xmath249^\\top$ ] where @xmath250 , @xmath251 $ ] and @xmath252^\\top$ ] . estimating @xmath253 can be accomplished by solving the optimization problem  ( [ eq : primal ] ) with square loss and setting @xmath254 , @xmath235 , leading to the same update rule as kernel ridge regression .",
    "after we obtain these estimators , we can calculate the predictive variance on @xmath255 by either @xmath256 or @xmath257 .",
    "we conduct experiments to justify the novel formulations for approximating both the mean and variance of posterior of gaussian processes for regression , and the doubly stochastic update rule in section.([sec : experiments ] ) .",
    "note that , to approximate the operator @xmath241 , doubly stochastic gradient requires @xmath258 memory . although we do not need to save the whole training dataset , which saves @xmath259 memory cost , this is still computationally expensive .",
    "when the @xmath260 testing data are given , we estimate @xmath260 functions and each of them requires @xmath21 memory cost , the total cost will be @xmath261 by the second algorithm .",
    "in this section , we will show that , both in expectation and with high probability , our algorithm can estimate the optimal function in the rkhs with rate @xmath1 , and achieve a generalization bound of @xmath2 .",
    "the analysis for our algorithm has a new twist compared to previous analysis of stochastic gradient descent algorithms , since the random feature approximation results in an estimator which is outside the rkhs . besides the analysis for stochastic functional gradient descent",
    ", we need to use martingales and the corresponding concentration inequalities to prove that the sequence of estimators , @xmath262 , outside the rkhs converge to the optimal function , @xmath263 , in the rkhs .",
    "we make the following standard assumptions ahead for later references :    1 .",
    "there exists an optimal solution , denoted as @xmath264 , to the problem of our interest ( [ eq : primal ] ) .",
    "loss function @xmath265 and its first - order derivative is @xmath266-lipschitz continous in terms of the first argument .",
    "3 .   for any data @xmath267 and any trajectory @xmath268 ,",
    "there exists @xmath269 , such that @xmath270 .",
    "note in our situation @xmath271 exists and @xmath272 since we assume bounded domain and the functions @xmath273 we generate are always bounded as well .",
    "4 .   there exists @xmath274 and @xmath275 , such that @xmath276 for example , when @xmath277 is the gaussian rbf kernel , we have @xmath278 , @xmath279 . +    we now present our main theorems as below . due to the space restrictions , we will only provide a short sketch of proofs here .",
    "the full proofs for the these theorems are given in the appendix  [ appendix : proof_details]-[appendix : suboptimality ] .",
    "[ thm : expectation ] when @xmath280 with @xmath281 such that @xmath282 , @xmath283 where @xmath284 , with @xmath285 , and @xmath286 .",
    "[ thm : probability ] when @xmath280 with @xmath281 such that @xmath287 and @xmath288 , for any @xmath289 , we have with probability at least @xmath290 over @xmath291 , @xmath292 where @xmath175 is as above and @xmath293 , with @xmath294 .    * sketch : * we focus on the convergence in expectation ; the high probability bound can be established in a similar fashion .",
    "the main technical difficulty is that @xmath262 may not be in the rkhs @xmath105 .",
    "the key of the proof is then to construct an intermediate function @xmath295 , such that the difference between @xmath262 and @xmath295 and the difference between @xmath295 and @xmath264 can be bounded .",
    "more specifically , @xmath296 where @xmath297 $ ] .",
    "then for any @xmath37 , the error can be decomposed as two terms @xmath298    for the error term due to random features , @xmath295 is constructed such that @xmath299 is a martingale , and the stepsizes are chosen such that @xmath300 , which allows us to bound the martingale .",
    "in other words , the choices of the stepsizes keep @xmath262 close to the rkhs . for the error term due to random data , since @xmath301",
    ", we can now apply the standard arguments for stochastic approximation in the rkhs . due to the additional randomness ,",
    "the recursion is slightly more complicated , @xmath302 where @xmath303 $ ] , and @xmath304 and @xmath305 depends on the related parameters . solving this recursion then leads to a bound for the second error term .     stands the error due to random features , and @xmath306 stands for the error due to random data.,scaledwidth=30.0% ]    [ thm : risk ] let the true risk be @xmath307 . then with probability",
    "at least @xmath290 over @xmath291 , and @xmath175 and @xmath308 defined as previously @xmath309    by the lipschitz continuity of @xmath310 and jensen s inequality , we have @xmath311 again , @xmath312 can be decomposed as two terms @xmath313 and @xmath314 , which can be bounded similarly as in theorem  [ thm : probability ] ( see corollary  [ cor : l2 ] in the appendix ) .",
    "* remarks . *",
    "the overall rate of convergence in expectation , which is @xmath1 , is indeed optimal .",
    "classical complexity theory ( see , e.g. reference in  @xcite ) shows that to obtain @xmath127-accuracy solution , the number of iterations needed for the stochastic approximation is @xmath315 for strongly convex case and @xmath316 for general convex case .",
    "different from the classical setting of stochastic approximation , our case imposes not one but two sources of randomness / stochasticity in the gradient , which intuitively speaking , might require higher order number of iterations for general convex case .",
    "however , the variance of the random features only contributes _",
    "additively _ to the constant in the final convergence rate .",
    "therefore , our method is still able to achieve the same rate as in the classical setting .",
    "notice that these bounds are achieved by adopting the classical stochastic gradient algorithm , and they may be further refined with more sophisticated techniques and analysis .",
    "for example , techniques for reducing variance of sgd proposed in  @xcite , mini - batch and preconditioning  @xcite can be used to reduce the constant factors in the bound significantly .",
    "theorem [ thm : expectation ] also reveals bounds in @xmath317 and @xmath111 sense as in appendix  [ appendix : l2 ] .",
    "the choices of stepsizes @xmath156 and the tuning parameters given in these bounds are only for sufficient conditions and simple analysis ; other choices can also lead to bounds in the same order .",
    "to investigate computation , memory and statistics trade - off , we will fix the desired @xmath111 error in the function estimation to @xmath127 ,  , @xmath318 , and work out the dependency of other quantities on @xmath127 .",
    "these other quantities include the preprocessing time , the number of samples and random features ( or rank ) , the number of iterations of each algorithm , and the computational cost and memory requirement for learning and prediction .",
    "we assume that the number of samples , @xmath6 , needed to achieve the prescribed error @xmath127 is of the order @xmath319 , the same for all methods .",
    "furthermore , we make no other regularity assumption about margin properties or the kernel matrix such as fast spectrum decay .",
    "thus the required number of random feature ( or ranks ) , @xmath17 , will be of the order @xmath320  @xcite .",
    "we will pick a few representative algorithms for comparison , namely , _ ( i ) _ norma  @xcite : kernel methods trained with stochastic functional gradients ; _ ( ii ) _ k - sdca  @xcite : kernelized version of stochastic dual coordinate ascend ; _ ( iii ) _ r - sdca : first approximate the kernel function with random features , and then run stochastic dual coordinate ascend ; _ ( iv ) _ n - sdca : first approximate the kernel matrix using nystrm s method , and then run stochastic dual coordinate ascend ; similarly we will combine pegasos algorithm  @xcite , stochastic block mirror descent ( sbmd )  @xcite , and random block coordinate descent ( rbcd )  @xcite with random features and nystrm s method , and obtain _ ( v ) _ r - pegasos , _ ( vi ) _ n - pegasos , _ ( vii ) _ r - sbmd , _ ( viii ) _ n - sbmd , _ (",
    "ix ) _ r - rbcd , and _ ( x ) _ n - rbcd , respectively .",
    "the comparisons are summarized below in table .",
    "[ table : tradeoff ]    .comparison of computation and memory requirements [ cols=\"^,^,^,^,^,^ \" , ]     we also compare our algorithm with the state - of - the - art neural network . in these experiments ,",
    "the block size is set to be @xmath321 .",
    "compared to the number of samples , @xmath322 , this block size is reasonable .",
    "[ [ mnist-8 m . ] ] * mnist 8 m . * + + + + + + + + + + +    in this experiment , we compare to a variant of lenet-5  @xcite , where all tanh units are replaced with rectified linear units .",
    "we also use more convolution filters and a larger fully connected layer . specifically , the first two convolutions layers have 16 and 32 filters , respectively , and the fully connected layer contains 128 neurons .",
    "we use kernel logistic regression for the task .",
    "we extract features from the last max - pooling layer with dimension 1568 , and use gaussian rbf kernel with kernel bandwidth @xmath323 equaling to four times the median pairwise distance .",
    "the regularization parameter @xmath122 is set to be @xmath324 .",
    "the result is shown in figure  [ fig : results](1 ) .",
    "as expected , the neural net with pre - learned features is faster to train than the jointly - trained one .",
    "however , our method is much faster compared to both methods . in addition , it achieves a lower error rate ( 0.5% ) compared to the 0.6% error provided by the neural nets .",
    "[ [ cifar-10 . ] ] * cifar 10 . *",
    "+ + + + + + + + + + +    in this experiment , we compare to a neural net with two convolution layers ( after contrast normalization and max - pooling layers ) and two local layers that achieves 11% test error on cifar 10  @xcite .",
    "the features are extracted from the top max - pooling layer from a trained neural net with 2304 dimension .",
    "we use kernel logistic regression for this problem .",
    "the kernel bandwidth @xmath323 for gaussian rbf kernel is again four times the median pairwise distance .",
    "the regularization parameter @xmath122 is set to be @xmath324 .",
    "we also perform a pca ( without centering ) to reduce the dimension to 256 before feeding to our method .",
    "the result is shown in figure  [ fig : results](2 ) .",
    "the test error for our method drops significantly faster in the earlier phase , then gradually converges to that achieved by the neural nets .",
    "our method is able to produce the same performance within a much restricted time budget .",
    "[ [ imagenet . ] ] * imagenet .",
    "* + + + + + + + + + + +    in this experiment , we compare our algorithm with the neural nets on the imagenet 2012 dataset , which contains 1.3 million color images from 1000 classes .",
    "each image is of size 256 @xmath325 256 , and we randomly crop a 240 @xmath325 240 region with random horizontal flipping .",
    "the jointly - trained neural net is alex - net  @xcite .",
    "the 9216 dimension features for our classifier and fixed neural net are from the last pooling layer of the jointly - trained neural net .",
    "the kernel bandwidth @xmath323 for gaussian rbf kernel is again four times the median pairwise distance .",
    "the regularization parameter @xmath122 is set to be @xmath324 .",
    "test error comparisons are shown in figure  [ fig : results](3 ) .",
    "our method achieves a test error of 44.5% by further max - voting of 10 transformations of the test set while the jointly - trained neural net arrives at 42% ( without variations in color and illumination ) . at the same time , fixed neural net can only produce an error rate of 46% with max - voting",
    ". there may be some advantages to train the network jointly such that the layers work together to achieve a better performance .",
    "although there is still a gap to the best performance by the jointly - trained neural net , our method comes very close with much faster convergence rate .",
    "moreover , it achieves superior performance than the neural net with pre - learned features , both in accuracy and speed .",
    "we test our algorithm for kernel ridge regression with neural network proposed in  @xcite on two large - scale real - world regression datasets , ( 9 ) and ( 10 ) in table  [ table : datasets ] .",
    "to our best knowledge , this is the first comparison between kernel ridge regression and neural network on the dataset molecularspace .",
    "[ [ quantummachine . ] ] * quantummachine . * + + + + + + + + + + + + + + + + +    in this experiment , we use the same binary representations converted based on random coulomb matrices as in  @xcite .",
    "we first generate a set of randomly sorted coulomb matrices for each molecule . and",
    "then , we break each dimension of the coulomb matrix apart into steps and convert them to the binary predicates .",
    "predictions are made by taking average of all prediction made on various coulomb matrices of the same molecule .",
    "the procedure is illustrated in figure .",
    "[ fig : molecular_nn ] . for this experiment ,",
    "40 sets of randomly permuted matrices are generated for each training example and 20 for each test example .",
    "we use gaussian kernel with kernel bandwidth @xmath326 obtained by median trick .",
    "the batch size is set to be @xmath327 and the feature block is @xmath328 .",
    "the total dimension of random features is @xmath329 .",
    "the results are shown in figure  [ fig : results](4 ) . in quantummachine",
    "dataset , our method achieves mean absolute error ( mae ) of @xmath330 kcal / mole , outperforming neural nets results , @xmath331 kcal / mole . note that this result is already close to the @xmath332 kcal / mole required for chemical accuracy .",
    "[ [ molecularspace . ] ] * molecularspace . *",
    "+ + + + + + + + + + + + + + + + +    in this experiment , the task is to predict the power conversion efficiency ( pce ) of the molecule .",
    "this dataset of 2.3 million molecular motifs is obtained from the clean energy project database .",
    "we use the same feature representation as for `` quantummachine '' dataset  @xcite .",
    "we set the kernel bandwidth of gaussian rbf kernel to be @xmath333 by median trick .",
    "the batch size is set to be @xmath334 and the feature block is @xmath328 .",
    "the total dimension of random features is @xmath329 .",
    "the results are shown in figure  [ fig : results](5 ) .",
    "it can be seen that our method is comparable with neural network on this 2.3 million dataset .",
    "our work contributes towards making kernel methods scalable for large - scale datasets .",
    "specifically , by introducing artificial randomness associated with kernels besides the random data samples , we propose doubly stochastic functional gradient for kernel machines which makes the kernel machines efficient in both computation and memory requirement .",
    "our algorithm successfully reduces the memory requirement of kernel machines from @xmath335 to @xmath18 . meanwhile",
    ", we also show that our algorithm achieves the optimal rate of convergence , @xmath1 , for strongly convex stochastic optimization .",
    "we compare our algorithm on both classification and regression problems with the state - of - the - art neural networks as well as some other competing algorithms for kernel methods on several large - scale datasets . with our efficient algorithm",
    ", kernel methods could perform comparable to sophisticated - designed neural network empirically .",
    "the theoretical analysis , which provides the rate of convergence _ independent _ to the dimension , is also highly non - trivial .",
    "it twists martingale techniques and the vanilla analysis for stochastic gradient descent and provides a new perspective for analyzing optimization in infinite - dimensional spaces , which could be of independent interest .",
    "it should be pointed out that although we applied the algorithm to many kernel machines even with non - smooth loss functions , our current proof relies on the lipschitz smoothness of the loss function . extending the guarantee to non - smooth loss function will be one interesting future work .",
    "another key property of our method is its simplicity and ease of implementation which makes it versatile and easy to be extened in various aspects .",
    "it is straightforward to replace the sampling strategy for random features with fastfood  @xcite which enjoys the efficient computational cost , or quasi - monte carlo sampling  @xcite , data - dependent sampling  @xcite which enjoys faster convergence rate with fewer generated features .",
    "meanwhile , by _ back - propogation _ trick , we could refine the random features by adapting their weights for better performance  @xcite .",
    "m.b . is supported in part by nsf grant ccf-1101283 , afosr grant fa9550 - 09 - 1 - 0538 , a microsoft faculty fellowship , and a raytheon faculty fellowship .",
    "is supported in part by nsf iis-1116886 , nsf / nih bigdata 1r01gm108341 , nsf career iis-1350983 , and a raytheon faculty fellowship",
    ".    10    a.  j. smola and b.  schlkopf .",
    "sparse greedy matrix approximation for machine learning . in _ icml _ , pages 911918 , san francisco , 2000 .",
    "morgan kaufmann publishers .    c.  k.  i. williams and m.  seeger . using the nystrom method to speed up kernel machines . in t.",
    "g. dietterich , s.  becker , and z.  ghahramani , editors , _ nips _ , 2000 .",
    "s.  fine and k.  scheinberg .",
    "efficient svm training using low - rank kernel representations .",
    ", 2:243264 , 2001 .",
    "p.  drineas and m.  mahoney . on the nystrom method for approximating a gram matrix for",
    "improved kernel - based learning .",
    ", 6:21532175 , 2005 .",
    "corinna cortes , mehryar mohri , and ameet talwalkar .",
    "on the impact of kernel approximation on learning accuracy . in _ aistats _ , pages 113120 , 2010 .",
    "a.  rahimi and b.  recht .",
    "random features for large - scale kernel machines . in j.c .",
    "platt , d.  koller , y.  singer , and s.  roweis , editors , _ nips_. mit press , cambridge , ma , 2008 .",
    "le , t.  sarlos , and a.  j. smola .",
    "fastfood  computing hilbert space expansions in loglinear time . in _",
    "icml _ , 2013 .",
    "ali rahimi and benjamin recht . weighted sums of random kitchen sinks : replacing minimization with randomization in learning . in _ nips _ , 2009 .",
    "david lopez - paz , suvrit sra , a.  j. smola , zoubin ghahramani , and bernhard schlkopf .",
    "randomized nonlinear component analysis . in _ icml _ , 2014 .",
    "john  c. platt .",
    "sequential minimal optimization : a fast algorithm for training support vector machines .",
    "technical report msr - tr-98 - 14 , microsoft research , 1998 .",
    "t.  joachims . making large - scale svm learning practical . in b.",
    "schlkopf , c.  j.  c. burges , and a.  j. smola , editors , _ advances in kernel methods  support vector learning _ , pages 169184 , cambridge , ma , 1999 . mit press .",
    "shai shalev - shwartz and tong zhang .",
    "stochastic dual coordinate ascent methods for regularized loss .",
    ", 14(1):567599 , 2013 .",
    "j.  kivinen , a.  j. smola , and r.  c. williamson .",
    "online learning with kernels . , 52(8 ) , aug 2004 .",
    "s.  s. keerthi and d.  decoste . a modified finite newton method for fast solution of large scale linear svms .",
    "_ j. mach .",
    "_ , 6:0 341361 , 2005 .",
    "n.  ratliff and j.  bagnell .",
    "kernel conjugate gradient for fast kernel machines . in _ ijcai _ ,",
    "volume  20 , january 2007 .",
    "a.  nemirovski , a.  juditsky , g.  lan , and a.  shapiro . robust stochastic approximation approach to stochastic programming .",
    ", 19(4):15741609 , january 2009 .",
    "a.  devinatz .",
    "integral representation of pd functions .",
    ", 74(1):5677 , 1953 .",
    "m.  hein and o.  bousquet .",
    "kernels , associated structures , and generalizations . technical report 127 ,",
    "max planck institute for biological cybernetics , 2004 .",
    "h.  wendland .",
    ". cambridge university press , cambridge , uk , 2005 .",
    "bernhard schlkopf and a.  j. smola . .",
    "press , cambridge , ma , 2002 .    n.  pham and r.  pagh .",
    "fast and scalable polynomial kernels via explicit feature maps . in _",
    "kdd_. acm , 2013 .",
    "mller , a.  j. smola , g.  rtsch , b.  schlkopf , j.  kohlmorgen , and v.  vapnik .",
    "predicting time series with support vector machines . in w.",
    "gerstner , a.  germond , m.  hasler , and j .- d .",
    "nicoud , editors , _ artificial neural networks icann97 _ , volume 1327 of _ lecture notes in comput .",
    "_ , pages 9991004 , berlin , 1997 .",
    "springer - verlag .",
    "b.  schlkopf , j.  platt , j.  shawe - taylor , a.  j. smola , and r.  c. williamson . estimating the support of a high - dimensional distribution .",
    "_ neural computation",
    "_ , 130 ( 7):0 14431471 , 2001 .    x.l .",
    "nguyen , m.  wainwright , and m.  jordan . estimating divergence functionals and the likelihood ratio by penalized convex risk minimization . in _ advances in neural information processing systems",
    "20 _ , pages 10891096 .",
    "mit press , cambridge , ma , 2008 .",
    "alex  j smola , le  song , and choon  h teo . relative novelty detection . in _ international conference on artificial intelligence and statistics _ ,",
    "pages 536543 , 2009 .",
    "shai shalev - shwartz , yoram singer , and nathan srebro .",
    "pegasos : primal estimated sub - gradient solver for svm . in _ icml _ , 2007 .",
    "g.  loosli , s.  canu , and l.  bottou .",
    "training invariant support vector machines with selective sampling . in l.",
    "bottou , o.  chapelle , d.  decoste , and j.  weston , editors , _ large scale kernel machines _ , pages 301320 .",
    "mit press , 2007 .",
    "a.  krizhevsky .",
    "learning multiple layers of features from tiny images .",
    "technical report , university of toronto , 2009 .",
    "a.  krizhevsky , i.  sutskever , and g.  hinton .",
    "imagenet classification with deep convolutional neural networks . in _ nips _ , 2012 .",
    "grgoire montavon , katja hansen , siamac fazli , matthias rupp , franziska biegler , andreas ziehe , alexandre tkatchenko , anatole von lilienfeld , and klaus - robert mller . learning invariant representations of molecules for atomization energy prediction . in _",
    "nips _ , pages 449457 , 2012 .",
    "alexander rakhlin , ohad shamir , and karthik sridharan . making gradient descent optimal for strongly convex stochastic optimization . in _ icml _ ,",
    "pages 449456 , 2012 .",
    "y.  lecun , l.  bottou , y.  bengio , and p.  haffner .",
    "gradient - based learning applied to document recognition .",
    ", 86(11):22782324 , november 1998 .",
    "purushottam kar and harish karnick .",
    "random feature maps for dot product kernels . in neil",
    "d. lawrence and mark  a. girolami , editors , _",
    "aistats-12 _ , volume  22 , pages 583591 , 2012 .",
    "andrea vedaldi and andrew zisserman .",
    "efficient additive kernels via explicit feature maps . , 34(3):480492 , 2012 .",
    "jiyan yang , vikas sindhwani , quanfu fan , haim avron , and michael  w. mahoney .",
    "random laplace feature maps for semigroup kernels on histograms . in _ cvpr _ , 2014 .",
    "zichao yang , marcin moczulski , misha denil , nando de  freitas , alexander  j. smola , le  song , and ziyu wang .",
    "deep fried convnets .",
    "_ corr _ , abs/1412.7149 , 2014d .",
    "url http://arxiv.org/abs/1412.7149 .",
    "rie johnson and tong zhang .",
    "accelerating stochastic gradient descent using predictive variance reduction . in _ nips _ , pages 315323 , 2013 .",
    "cong  d. dang and guanghui lan .",
    "stochastic block mirror descent methods for nonsmooth and stochastic optimization .",
    "technical report , university of florida , 2013 .",
    "yurii nesterov .",
    "efficiency of coordinate descent methods on huge - scale optimization problems .",
    ", 22(2):341362 , 2012 .",
    "andrew cotter , shai shalev - shwartz , and nati srebro . learning optimally sparse support vector machines . in _ proceedings of the 30th international conference on machine learning , icml 2013 ,",
    "atlanta , ga , usa , 16 - 21 june 2013 _ , pages 266274 , 2013 .",
    "a.  agarwal , s.  kakade , n.  karampatziakis , l.  song , and g.  valiant .",
    "least squares revisited : scalable approaches for multi - class prediction . in _ international conference on machine learning ( icml ) _ , 2014 .",
    "tianbao yang , rong jin , and shenghuo zhu . on data preconditioning for regularized loss minimization . , 2014 .",
    "jiyan yang , vikas sindhwani , haim avron , and michael  w. mahoney .",
    "quasi - monte carlo feature maps for shift - invariant kernels . in _ proceedings of the 31th international conference on machine learning , icml 2014 , beijing , china , 21 - 26",
    "june 2014 _ , pages 485493 , 2014 .",
    "gaurav pandey and ambedkar dukkipati .",
    "learning by stretching deep networks . in _ proceedings of the 31th international conference on machine learning , icml 2014 , beijing , china , 21 - 26",
    "june 2014 _ , pages 485493 , 2014 .",
    "youngmin cho and lawrence  k. saul .",
    "kernel methods for deep learning . in y.",
    "bengio , d.  schuurmans , j.d .",
    "lafferty , c.k.i .",
    "williams , and a.  culotta , editors , _ advances in neural information processing systems 22 _ , pages 342350 , 2009 .    c.  e. rasmussen and c.  k.  i. williams . .",
    "mit press , cambridge , ma , 2006 .",
    "francis  r. bach . on the equivalence between quadrature rules and random features .",
    "_ corr _ , abs/1502.06800 , 2015 .",
    "url http://arxiv.org/abs/1502.06800 .",
    "we first provide specific bounds and detailed proofs for the two error terms appeared in theorem [ thm : expectation ] and theorem [ thm : probability ] .",
    "[ lem : random_feature ] we have    1 .   for any @xmath289 , @xmath336\\leqslant b^2_{1,t+1}:=4m^2(\\kappa+\\phi)^2\\sum_{i=1}^t|a_t^i|^2.$ ] 2 .   for",
    "any @xmath289 , with probability at least @xmath337 over @xmath291 , @xmath338    let @xmath339 . since @xmath340 is a function of @xmath341 and @xmath342 we have that @xmath343 is a martingal difference sequence .",
    "further note that @xmath344 then by azuma s inequality , for any @xmath345 , @xmath346 which is equivalent as @xmath347 moreover , @xmath348 since @xmath349 , we immediately obtain the two parts of the lemma .    [ lem : coeff ] suppose @xmath350 and @xmath351 .",
    "then we have    1 .",
    "consequently , @xmath353 2 .",
    "@xmath354 .",
    "@xmath355 follows by induction on @xmath147 .",
    "@xmath356 is trivially true .",
    "we have @xmath357 when @xmath358 , @xmath359 for any @xmath360 , so @xmath361 . when @xmath362 , if @xmath363 , then @xmath361 ; if @xmath364 , then @xmath365 . for @xmath366 , when @xmath367 , @xmath368 when @xmath287 and @xmath369 @xmath370      [ lem : random_data ] assume @xmath371 is @xmath266-lipschitz continous in terms of @xmath372",
    "let @xmath264 be the optimal solution to our target problem",
    ". then    1 .   if we set @xmath280 with @xmath99 such that @xmath282 , then @xmath373 where @xmath374 particularly , if @xmath375 , we have @xmath376 .",
    "2 .   if we set @xmath280 with @xmath99 such that @xmath287 and @xmath288 , then with probability at least @xmath377 over @xmath291 , @xmath378 where @xmath379 particularly , if @xmath375 , we have @xmath380 .    for the sake of simple notations , let us first denote the following three different gradient terms , which are @xmath381 note that by our previous definition , we have @xmath382 .",
    "+ denote @xmath383 .",
    "then we have @xmath384 because of the strongly convexity of  ( [ eq : primal ] ) and optimality condition , we have @xmath385 hence , we have @xmath386    _ proof for @xmath387 _ : let us denote @xmath388 , @xmath389 , @xmath390 .",
    "we first show that @xmath391 are bounded . specifically , we have for @xmath392 ,    1 .",
    "@xmath393 , where @xmath394 for @xmath395 and @xmath396 ; 2 .",
    "@xmath397=0 $ ] ; 3 .",
    "@xmath398\\leqslant \\kappa^{1/2}lb_{1,t}\\sqrt{\\ee_{\\dcal^{t-1},\\omegab^{t-1}}[a_t]}$ ] , where @xmath399 for @xmath395 and @xmath400 ;    we prove these results separately in lemma [ lem : bounds ] below .",
    "let us denote @xmath401 $ ] , given the above bounds , we arrive at the following recursion , @xmath402    when @xmath403 with @xmath99 such that @xmath404 , from lemma [ lem : coeff ] , we have @xmath405 .",
    "consequently , @xmath406 and @xmath407 .",
    "applying these bounds leads to the refined recursion as follows @xmath408 that can be further written as @xmath409 where @xmath410 and @xmath411 . invoking lemma [ lem : rec ] with @xmath412",
    ", we obtain @xmath413 where @xmath414 , and @xmath415    _ proof for @xmath416 _ : cumulating equations ( [ eqn : rec ] ) with @xmath417 , we end up with the following inequality @xmath418 let us denote @xmath419 , the above inequality is equivalent as @xmath420 we first show that    1 .   for any @xmath421 and @xmath422 , with probability @xmath423 over @xmath424 , @xmath425 where @xmath426 .",
    "+ 2 .   for any @xmath427 , with probability @xmath423 over @xmath424 , @xmath428 where @xmath429 .",
    "again , the proofs of these results are given separately in lemma [ lem : bounds ] . applying the above bounds",
    "leads to the refined recursion as follows , @xmath430 with probability @xmath377 .",
    "when @xmath403 with @xmath99 such that @xmath287 , with similar reasons in lemma [ lem : coeff ] , we have @xmath431 and also we have @xmath432 , and @xmath433 . therefore , we can rewrite the above recursion as @xmath434 where @xmath435 , @xmath436 , @xmath437 , @xmath438 . invoking lemma [ lem : rec2 ] , we obtain @xmath439 with the specified @xmath308 .",
    "[ lem : bounds ] in this lemma , we prove the inequalities ( 1)(5 ) in lemma [ lem : random_data ] .",
    "given the definitions of @xmath391 in lemma [ lem : random_data ] , we have    1 .   @xmath440 ; + this is because @xmath441 we have @xmath442 and + @xmath443 2 .   @xmath397=0 $ ] ; + this is because @xmath389 , @xmath444&=&\\ee_{\\dcal^{t-1},\\omegab^t}\\sbr{\\ee_{d_t}\\sbr{\\langle h_t - f_\\ast , \\bar g_t-\\hat g_t\\rangle_\\hcal|\\dcal^{t-1},\\omegab^t}}\\\\ & = & \\ee_{\\dcal^{t-1},\\omegab^t}\\sbr{\\langle h_t - f_\\ast , \\ee_{d_t}\\sbr{\\bar",
    "g_t-\\hat g_t}\\rangle_\\hcal}\\\\ & = & 0.\\end{aligned}\\ ] ] + 3 .   @xmath398\\leqslant \\kappa^{1/2}lb_{1,t}\\sqrt{\\ee_{\\dcal^{t-1},\\omegab^{t-1}}[a_t]}$ ] , where @xmath445 + this is because @xmath390 , @xmath446&=&\\ee_{\\dcal^t,\\omegab^t}\\sbr{\\langle h_t - f_\\ast , \\hat g_t -g_t   \\rangle_\\hcal}\\\\ & = & \\ee_{\\dcal^t,\\omegab^t}\\sbr{\\langle h_t - f_\\ast , [ l'(f_t(x_t ) , y_t ) - l'(h_t(x_t ) , y_t)]k(x_t , \\cdot )   \\rangle_\\hcal } \\\\ & \\leqslant & \\ee_{\\dcal^t,\\omegab^t}\\sbr{|l'(f_t(x_t ) , y_t ) - l'(h_t(x_t ) , y_t)|\\cdot \\nbr{k(x_t , \\cdot ) } _ \\hcal\\cdot\\nbr{h_t - f_\\ast}_\\hcal } \\\\ & \\leqslant&\\kappa^{1/2}l\\cdot\\ee_{\\dcal^t,\\omegab^t}\\sbr { |f_t(x_t ) - h_t(x_t)| \\nbr{h_t - f_\\ast}_\\hcal   } \\\\ & \\leqslant & \\kappa^{1/2}l\\sqrt{\\ee_{\\dcal^t,\\omegab^t}|f_t(x_t ) - h_t(x_t)| ^2}\\sqrt{\\ee_{\\dcal^t,\\omegab^t}\\nbr{h_t - f_\\ast}_\\hcal^2}\\\\ & \\leqslant & \\kappa^{1/2}lb_{1,t}\\sqrt{\\ee_{\\dcal^{t-1},\\omegab^{t-1}}[a_t]}\\end{aligned}\\ ] ] where the first and third inequalities are due to cauchy  schwarz inequality and",
    "the second inequality is due to @xmath266-lipschitz continuity of @xmath447 in the first parameter , and the last step is due to lemma  [ lem : random_feature ] and the definition of @xmath448 .",
    "+ 4 .   for any @xmath421 and @xmath449 , with probability at least @xmath423 over @xmath424 , @xmath450 where @xmath426 .",
    "+ this result follows directly from lemma 3 in  @xcite .",
    "let us define @xmath451 , we have * @xmath452 is martingale difference sequence since @xmath453 .",
    "* @xmath454 , with @xmath426 , @xmath455 . * @xmath456 .",
    "+ plugging in these specific bounds in lemma 3 in [ alexander et.al . ,",
    "2012 ] , which is , @xmath457 where @xmath458 and @xmath459 , we immediately obtain the above inequality as desired .",
    "+ 5 .   for any @xmath427 , with probability",
    "at least @xmath423 over @xmath424 , @xmath460 where @xmath429 .",
    "+ this is because , for any @xmath461 , recall that from analysis in ( 3 ) , we have @xmath462 , therefore from lemma [ lem : random_data ] , @xmath463 taking the sum over @xmath147 , we therefore get @xmath464    applying these lemmas immediately gives us theorem [ thm : expectation ] and theorem [ thm : probability ] , which implies pointwise distance between the solution @xmath465 and @xmath466 .",
    "now we prove similar bounds in the sense of @xmath467 and @xmath111 distance .",
    "[ cor : linf ] theorem [ thm : expectation ] also implies a bound in @xmath317 sense , namely , @xmath468 consequently , for the average solution @xmath469 , we also have @xmath470    this is because @xmath471 , where @xmath472 always exists since @xmath35 is closed and bounded .",
    "note that the result for average solution can be improved without log factor using more sophisticated analysis ( see also reference in @xcite ) .",
    "[ cor : l2 ] with the choices of @xmath156 in lemma [ lem : random_data ] , we have    1 .",
    "@xmath473 2 .",
    "@xmath474 with probability at least @xmath290 over @xmath291 .    _",
    "( i ) _ follows directly from theorem [ thm : expectation ] . _",
    "( ii ) _ can be proved as follows .",
    "first , we have @xmath475 from lemma [ lem : random_data ] , with probability at least @xmath377 , we have @xmath476 from lemma [ lem : random_feature ] , for any @xmath477 , we have @xmath478 since @xmath479 , the above inequality can be writen as @xmath480 which leads to @xmath481 by fubini s theorem and markov s inequality , we have @xmath482 from the analysis in lemma [ lem : random_feature ] , we also have that @xmath483 .",
    "therefore , with probability at least @xmath423 over @xmath291 , we have @xmath484\\leqslant \\frac{c^2\\ln(\\frac{2}{\\epsilon})}{2t}(1-\\frac{\\epsilon}{\\delta } ) + c^2\\frac{\\epsilon}{\\delta}\\ ] ] let @xmath485 , we have @xmath486\\leqslant \\frac{c^2}{2t}(\\ln(8t/\\delta)+\\frac{1}{2})=\\frac{c^2\\ln(8\\sqrt{e}t/\\delta)}{2t}.\\ ] ] summing up equation ( [ eqn : term1 ] ) and ( [ eqn : term2 ] ) , we have @xmath487 as desired .    from the bound on @xmath111 distance",
    ", we can immediately get the generalization bound . + * theorem *  [ thm : risk ]  * ( generalization bound ) * _ let the true risk be @xmath307 .",
    "then with probability at least @xmath290 over @xmath291 , and @xmath175 and @xmath308 defined as previously @xmath309 _    by the lipschitz continuity of @xmath310 and jensen s inequality",
    ", we have @xmath311 then the theorem follows from corollary  [ cor : l2 ] .",
    "for comprehensive purposes , we also provide the @xmath1 bound for suboptimality .",
    "[ cor : suboptimality ] if we set @xmath280 with @xmath488 , then the average solution @xmath489 satisfies @xmath490)-r(f_*)\\leqslant \\frac{q(\\ln(t)+1)}{t}.\\ ] ] where @xmath491 , with @xmath492 defined as in lemma [ lem : random_data ] .    from the anallysis in lemma [ lem : random_data],we",
    "have @xmath493 invoking strongly convexity of @xmath494 , we have @xmath495 . taking expectaion on both size and",
    "use the bounds in last lemma , we have @xmath496\\leqslant(\\frac{1}{2\\gamma_t}-\\frac{\\nu}{2})e_t-\\frac{1}{2\\gamma_t}e_{t+1}+\\gamma_t \\kappa m^2(1+\\nu c_t)^2+\\kappa^{1/2}lb_{1,t}\\sqrt{e_t}\\ ] ] assume @xmath280 with @xmath497 , then cumulating the above inequalities leads to @xmath498\\leqslant \\sum_{i=1}^{t}\\gamma_i \\kappa m^2(1+\\nu c_i)^2+\\sum_{i=1}^{t}\\kappa^{1/2}lb_{1,i}\\sqrt{e_i}\\ ] ] which can be further bounded by @xmath499&\\leqslant & \\sum_{i=1}^{t}\\gamma_i \\kappa m^2(1+\\nu c_i)^2+\\sum_{i=1}^{t}\\kappa^{1/2}lb_{1,i}\\sqrt{e_i}\\\\ & \\leqslant & \\frac{4\\kappa m^2}{\\nu}\\sum_{i=1}^{t}\\frac{1 } { i}+\\frac{2\\sqrt{2}\\kappa^{1/2}lm(\\kappa+\\phi)}{\\nu}\\sum_{i=1}^{t}\\sqrt{\\frac{e_i}{i}}\\\\ & \\leqslant & \\frac{4\\kappa m^2}{\\nu}(\\ln(t)+1)+\\frac{2\\sqrt{2}\\kappa^{1/2}lm(\\kappa+\\phi)}{\\nu } q_1(\\ln(t)+1)\\\\ & = & \\frac{q(\\ln(t)+1)}{t}\\end{aligned}\\ ] ] by convexity , we have @xmath500\\leqslant \\frac{q(\\ln(t)+1)}{t}$ ] .",
    "the corollary then follows from the fact that @xmath501=\\ee_{\\dcal^t,\\omegab^t}[\\hat h_{t+1}]$ ] and @xmath502)\\leqslant \\ee_{\\dcal^t,\\omegab^t}[r(\\hat h_{t+1})]$ ] .",
    "[ lem : rec ] suppose the sequence @xmath503 satisfies @xmath504 , and @xmath505 @xmath506 where @xmath507 .",
    "then @xmath508 , @xmath509    the proof follows by induction .",
    "when @xmath510 , it always holds true by the definition of @xmath511 .",
    "assume the conclusion holds true for @xmath0 with @xmath392 , i.e. , @xmath512 , then we have @xmath513 where the last step can be verified as follows .",
    "@xmath514 where the last step follows from the defintion of @xmath515 .",
    "[ lem : rec2 ] suppose the sequence @xmath503 satisfies @xmath516 where @xmath517 and @xmath518",
    ". then @xmath519 , @xmath520    the proof follows by induction . when @xmath521 it is trivial",
    ". let us assume it holds true for @xmath522 , therefore , @xmath523 @xmath524 since @xmath525 , we have @xmath526 .",
    "hence , @xmath527 .",
    "as we show in section  [ sec : doubly_sgd ] , the estimation of the variance of the predictive distribution of gaussian process for regression problem could be recast as estimating the operator @xmath241 defined in  ( [ eq : variance_operator ] ) .",
    "we first demonstrate that the operator @xmath241 is the solution to the following optimization problem @xmath528 where @xmath243 is the hilbert - schmidt norm of the operator .",
    "the gradient of @xmath529 with respect to @xmath241 is @xmath530 set @xmath531 , we could obtain the optimal solution , @xmath532 , exactly the same as ( [ eq : variance_operator ] ) .    to derive the doubly stochastic gradient update for @xmath241 , we start with stochastic functional gradient of @xmath529 . given @xmath533 ,",
    "the stochastic functional gradient of @xmath529 is @xmath534 where @xmath535 which leads to update @xmath536      recall @xmath544\\otimes \\ee_{\\omega'}[\\phi_{\\omega'}(x_t)\\phi_{\\omega'}(\\cdot ) ] = \\ee_{\\omega , \\omega'}[\\phi_\\omega(x_t)\\phi_{\\omega'}(x_t)\\phi_\\omega(\\cdot)\\otimes \\phi_{\\omega'}(\\cdot)],\\end{aligned}\\ ] ] where @xmath545 are independently sampled from @xmath34 , we could approximate the @xmath546 with random features , @xmath547 . plug random feature approximation into  ( [ eq : gp_posterior_variance_update ] ) leads to @xmath548 therefore , inductively , we could approximate @xmath549 by @xmath550 @xmath247"
  ],
  "abstract_text": [
    "<S> the general perception is that kernel methods are not scalable , and neural nets are the methods of choice for large - scale nonlinear learning problems . or have we simply not tried hard enough for kernel methods ? </S>",
    "<S> here we propose an approach that scales up kernel methods using a novel concept called `` _ _ doubly stochastic functional gradients _ _ '' . </S>",
    "<S> our approach relies on the fact that many kernel methods can be expressed as convex optimization problems , and we solve the problems by making _ </S>",
    "<S> two unbiased _ stochastic approximations to the functional gradient , one using random training points and another using random features associated with the kernel , and then descending using this noisy functional gradient . </S>",
    "<S> our algorithm is simple , does _ not _ need to commit to a preset number of random features , and allows the flexibility of the function class to grow as we see more incoming data in the streaming setting . </S>",
    "<S> we show that a function learned by this procedure after @xmath0 iterations converges to the optimal function in the reproducing kernel hilbert space in rate @xmath1 , and achieves a generalization performance of @xmath2 . </S>",
    "<S> our approach can readily scale kernel methods up to the regimes which are dominated by neural nets . </S>",
    "<S> we show that our method can achieve competitive performance to neural nets in datasets such as 2.3 million energy materials from molecularspace , 8 million handwritten digits from mnist , and 1 million photos from imagenet using convolution features . </S>"
  ]
}