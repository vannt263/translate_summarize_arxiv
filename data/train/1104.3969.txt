{
  "article_text": [
    "there is growing empirical evidence of superiority of aggregated statistical procedures , also referred to as _ blending _ ,",
    "_ stacked generalization _ or _",
    "ensemble methods _ , with respect to `` pure '' ones . since their introduction in the 1990s , famous aggregation procedures such as _ boosting _",
    "@xcite , _ bagging _",
    "@xcite or _ random forest _",
    "@xcite have been successfully used in practice for a large variety of applications . moreover , most recent machine learning competitions such as the pascal voc or netflix challenge have been won by procedures combining different types of classifiers / predictors / estimators .",
    "it is therefore of central interest to understand from a theoretical point of view what kind of aggregation strategies should be used for getting the best possible combination of the available statistical procedures .      in the statistical literature , to the best of our knowledge ,",
    "theoretical foundations of aggregation procedures were first studied by nemirovski ( nemirovski  @xcite , juditsky and nemirovski  @xcite ) and independently by a series of papers by catoni ( see  @xcite for an account ) and yang  @xcite . for the regression model ,",
    "a significant progress was achieved by tsybakov  @xcite with introducing the notion of optimal rates of aggregation and proposing aggregation - rate - optimal procedures for the tasks of linear , convex and model selection aggregation .",
    "this point was further developed in @xcite , especially in the context of high dimension with sparsity constraints and in  @xcite for kullback ",
    "leibler aggregation .",
    "however , it should be noted that the procedures proposed in  @xcite that provably achieve the lower bounds in convex and linear aggregation require full knowledge of design distribution .",
    "this limitation was overcome in the recent work  @xcite .",
    "from a practical point of view , an important limitation of the previously cited results on aggregation is that they are valid under the assumption that the aggregated procedures are deterministic ( or random , but independent of the data used for aggregation ) .",
    "the generality of those results ",
    "almost no restriction on the constituent estimators  compensates to this practical limitation .    in the gaussian sequence model ,",
    "a breakthrough was reached by leung and barron  @xcite .",
    "building on very elegant but not very well - known results by george  @xcite ; cf .",
    "equation ( [ eqdefweights ] ) below for a precise definition of exponential weights .",
    "furthermore , to the best of our knowledge ,  @xcite is the first reference using the stein lemma for evaluating the expected risk of the exponentially weighted aggregate . ] , they established sharp oracle inequalities for the exponentially weighted aggregate ( ewa ) for constituent estimators obtained from the data vector by orthogonally projecting it on some linear subspaces .",
    "dalalyan and tsybakov  @xcite showed the result of  @xcite remains valid under more general ( non - gaussian ) noise distributions and when the constituent estimators are independent of the data used for the aggregation .",
    "a natural question arises whether a similar result can be proved for a larger family of constituent estimators containing projection estimators and deterministic ones as specific examples .",
    "the main aim of the present paper is to answer this question by considering families of affine estimators .",
    "our interest in affine estimators is motivated by several reasons .",
    "first , affine estimators encompass many popular estimators such as smoothing splines , the pinsker estimator  @xcite , local polynomial estimators , nonlocal means @xcite , etc .",
    "for instance , it is known that if the underlying ( unobserved ) signal belongs to a sobolev ball , then the ( linear ) pinsker estimator is asymptotically minimax up to the optimal constant , while the best projection estimator is only rate - minimax .",
    "a second motivation is that  as proved by juditsky and nemirovski  @xcite  the set of signals that are well estimated by linear estimators is very rich .",
    "it contains , for instance , sampled smooth functions , sampled modulated smooth functions and sampled harmonic functions .",
    "one can add to this set the family of piecewise constant functions as well , as demonstrated in  @xcite , with natural application in magnetic resonance imaging .",
    "it is worth noting that oracle inequalities for penalized empirical risk minimizer were also proved by golubev  @xcite , and for model selection by arlot and bach  @xcite , baraud , giraud and huet @xcite .    in the present work ,",
    "we establish sharp oracle inequalities in the model of heteroscedastic regression , under various conditions on the constituent estimators assumed to be affine functions of the data .",
    "our results provide theoretical guarantees of optimality , in terms of expected loss , for the exponentially weighted aggregate .",
    "they have the advantage of covering in a unified fashion the particular cases of frozen estimators considered in  @xcite and of projection estimators treated in  @xcite .",
    "we focus on the theoretical guarantees expressed in terms of oracle inequalities for the expected squared loss .",
    "interestingly , although several recent papers @xcite discuss the paradigm of competing against the best linear procedure from a given family , none of them provide oracle inequalities with leading constant equal to one .",
    "furthermore , most existing results involve some constants depending on different parameters of the setup .",
    "in contrast , the oracle inequality that we prove herein is with leading constant one and admits a simple formulation .",
    "it is established for ( suitably symmetrized , if necessary ) exponentially weighted aggregates  @xcite with an arbitrary prior and a temperature parameter which is not too small .",
    "the result is nonasymptotic but leads to an asymptotically optimal residual term when the sample size , as well as the cardinality of the family of constituent estimators , tends to infinity . in its general form ,",
    "the residual term is similar to those obtained in the pac - bayes setting  @xcite in that it is proportional to the kullback ",
    "leibler divergence between two probability distributions .",
    "the problem of competing against the best procedure in a given family was extensively studied in the context of online learning and prediction with expert advice  @xcite .",
    "a connection between the results on online learning and statistical oracle inequalities was established by gerchinovitz  @xcite .      throughout this work ,",
    "we focus on the heteroscedastic regression model with gaussian additive noise .",
    "we assume we are given a vector @xmath0 obeying the model @xmath1 where @xmath2 is a centered gaussian random vector , @xmath3 where @xmath4 is an unknown function and @xmath5 are deterministic points . here",
    ", no assumption is made on the set @xmath6 .",
    "our objective is to recover the vector @xmath7 , often referred to as _ signal _ , based on the data @xmath8 . in our work , the noise covariance matrix @xmath9 $ ] is assumed to be finite with a known upper bound on its spectral norm @xmath10 .",
    "we denote by @xmath11 the empirical inner product in @xmath12 : @xmath13 .",
    "we measure the performance of an estimator @xmath14 by its expected empirical quadratic loss : @xmath15 $ ] where @xmath16 .",
    "we only focus on the task of aggregating _ affine estimators _",
    "@xmath17 indexed by some parameter @xmath18 .",
    "these estimators can be written as affine transforms of the data @xmath0 .",
    "using the convention that all vectors are one - column matrices , we have @xmath19 , where the @xmath20 real matrix @xmath21 and the vector @xmath22 are deterministic .",
    "it means the entries of @xmath21 and @xmath23 may depend on the points @xmath24 but not on the data @xmath25 .",
    "let us describe now different families of linear and affine estimators successfully used in the statistical literature .",
    "our results apply to all these families , leading to a procedure that behaves nearly as well as the best ( unknown ) one of the family .    _",
    "ordinary least squares_. let @xmath26 be a set of linear subspaces of @xmath12 .",
    "a  well - known family of affine estimators , successfully used in the context of model selection @xcite , is the set of orthogonal projections onto @xmath27 . in the case of a family of linear regression models with design matrices @xmath28 ,",
    "one has @xmath29 , where @xmath30 stands for the moore ",
    "penrose pseudo - inverse of @xmath31 .",
    "_ diagonal filters_. other common estimators are the so - called diagonal filters corresponding to diagonal matrices @xmath32 .",
    "examples include the following :    * ordered projections : @xmath33 for some integer @xmath34 [ @xmath35 is the indicator function ] .",
    "those weights are also called truncated svd ( singular value decomposition ) or spectral cutoff . in this case",
    "a natural parametrization is @xmath36 , indexing the number of elements conserved . * block projections : @xmath37 , @xmath38 , where @xmath39 . here",
    "the natural parametrization is @xmath40 , indexing subsets of @xmath41 .",
    "* tikhonov  philipps filter : @xmath42 , where @xmath43 . in this case ,",
    "@xmath44 , indexing continuously the smoothing parameters .",
    "* pinsker filter : @xmath45 , where @xmath46 and @xmath47 .",
    "_ kernel ridge regression_. assume that we have a positive definite kernel @xmath48 and we aim at estimating the true function @xmath49 in the associated reproducing kernel hilbert space ( @xmath50 ) .",
    "the kernel ridge estimator is obtained by minimizing the criterion @xmath51 w.r.t .",
    "@xmath52 ( see  @xcite , page 118 ) . denoting by @xmath53 the @xmath54 kernel - matrix with element @xmath55 , the unique solution  @xmath14 is a linear estimate of the data , @xmath56 , with @xmath57 , where @xmath58 is the @xmath59 identity matrix .",
    "_ multiple kernel learning_. as described in  @xcite , it is possible to handle the case of several kernels @xmath60 , with associated positive definite matrices @xmath61 . for a parameter @xmath62",
    ", one can define the estimators @xmath63 with @xmath64 it is worth mentioning that the formulation in equation ( [ eqmultiplekernel ] ) can be linked to the group lasso @xcite and to the multiple kernel learning introduced in @xcite ",
    "see @xcite for more details .",
    "_ moving averages_. if we think of coordinates of @xmath65 as some values assigned to the vertices of an undirected graph , satisfying the property that two nodes are connected if the corresponding values of @xmath65 are close , then it is natural to estimate @xmath66 by averaging out the values @xmath67 for indices @xmath68 that are connected to @xmath69 .",
    "the resulting estimator is a linear one with a matrix @xmath70 such that @xmath71 , where @xmath72 is the set of neighbors of the node @xmath69 in the graph and @xmath73 is the cardinality of @xmath72 .      in section  [ secmainresult ]",
    "we introduce ewa and state a pac - bayes type bound in expectation assessing optimality properties of ewa in combining affine estimators .",
    "the strengths and limitations of the results are discussed in section  [ secdiscussion ] .",
    "the extension of these results to the case of grouped aggregation  in relation with ill - posed inverse problems  is developed in section  [ secill - posed ] .",
    "as a consequence , we provide in section  [ secexamples ] sharp oracle inequalities in various setups : ranging from finite to continuous families of constituent estimators and including sparse scenarii . in section [ secmmx ] we apply our main results to prove that combining pinsker s type filters with ewa leads to asymptotically sharp adaptive procedures over sobolev ellipsoids .",
    "section  [ secexperiments ] is devoted to numerical comparison of ewa with other classical filters ( soft thresholding , blockwise shrinking , etc . ) and illustrates the potential benefits of aggregating .",
    "the conclusion is given in section  [ secconclusion ] , while the proofs of some technical results ( propositions  [ prop2][propminimax2 ] ) are provided in the supplementary material  @xcite .",
    "in this section we describe the statistical framework for aggregating estimators and we introduce the exponentially weighted aggregate .",
    "the task of aggregation consists in estimating @xmath65 by a suitable combination of the elements of a family of _ constituent estimators _ @xmath74 .",
    "the target objective of the aggregation is to build an aggregate @xmath75 that mimics the performance of the best constituent estimator , called _ oracle _ ( because of its dependence on the unknown function @xmath76 ) . in what follows",
    ", we assume that @xmath77 is a measurable subset of @xmath78 , for some @xmath79 .    the theoretical tool commonly used for evaluating the quality of an aggregation procedure is the oracle inequality ( oi ) , generally written @xmath80 \\leq c_n \\inf_{\\lambda\\in\\lambda } { \\mathbb{e}}\\bigl[\\|\\hat{\\mathbf { f}}_{\\lambda}-{\\mathbf{f}}\\|^2 _ n \\bigr ] + r_n,\\ ] ] with _ residual _ term @xmath81 tending to zero as @xmath82 , and _ leading constant _ @xmath83 being bounded .",
    "the ois with leading constant one are of central theoretical interest since they allow to bound the excess risk and to assess the aggregation - rate - optimality .",
    "they are often referred to as sharp oi .",
    "let @xmath84 $ ] denote the risk of the estimator @xmath17 , for any @xmath85 , and let @xmath86 be an estimator of @xmath87 .",
    "the precise form of @xmath86 strongly depends on the nature of the constituent estimators . for any probability distribution @xmath88 over @xmath77 and for any @xmath89",
    ", we define the probability measure of exponential weights , @xmath90 , by @xmath91 the corresponding exponentially weighted aggregate , henceforth denoted by @xmath92 , is the expectation of @xmath17 w.r.t .",
    "the probability measure @xmath93 : @xmath94 we will frequently use the terminology of bayesian statistics : the measure @xmath88 is called _ prior _ , the measure @xmath90 is called _ posterior _ and the aggregate @xmath95 is then the _ posterior mean_. the parameter @xmath96 will be referred to as the _ temperature parameter_. in the framework of aggregating statistical procedures , the use of such an aggregate can be traced back to george  @xcite .",
    "the interpretation of the weights @xmath97 is simple : they up - weight estimators all the more that their performance , measured in terms of the risk estimate @xmath98 , is good .",
    "the temperature parameter reflects the confidence we have in this criterion : if the temperature is small ( @xmath99 ) , the distribution concentrates on the estimators achieving the smallest value for @xmath86 , assigning almost zero weights to the other estimators . on the other hand ,",
    "if @xmath100 , then the probability distribution over @xmath77 is simply the prior @xmath88 , and the data do not influence our confidence in the estimators .",
    "in this paper we only focus on _ affine estimators _",
    "@xmath101 where the @xmath20 real matrix @xmath21 and the vector @xmath22 are deterministic .",
    "furthermore , we will assume that an unbiased estimator @xmath102 of the noise covariance matrix  @xmath103 is available .",
    "it is well known ( cf . for details ) that the risk of the estimator ( [ eqaffineestimatorsgal ] ) is given by @xmath104= \\bigl\\|(a_{\\lambda}-i_{n\\times n } ) { \\mathbf{f}}+\\mathbf{b}_{\\lambda } \\bigr\\|_n^2 + \\frac{{\\operatorname{tr}}(a_{\\lambda}\\sigma a_{\\lambda}^\\top ) } { n}\\ ] ] and that @xmath105 , defined by @xmath106,\\ ] ] is an unbiased estimator of @xmath87 . along with @xmath105",
    ", we will use another estimator of the risk that we call the adjusted risk estimate and define by @xmath107}_{\\hat r_\\lambda^{\\mathrm{unb } } } + \\frac{1}{n } { \\mathbf{y}}^\\top \\bigl(a_\\lambda - a_\\lambda^2 \\bigr){\\mathbf{y}}.\\ ] ] one can notice that the adjusted risk estimate @xmath108 coincides with the unbiased risk estimate @xmath105 if and only if the matrix @xmath109 is an orthogonal projector .    to state our main results ,",
    "we denote by @xmath110 the set of all probability measures on @xmath77 and by @xmath111 the kullback ",
    "leibler divergence between two probability measures @xmath112 : @xmath113 we write @xmath114 ( resp . ,",
    "@xmath115 ) for two symmetric matrices @xmath116 and @xmath117 , when @xmath118 ( resp . ,",
    "@xmath119 ) is semi - definite positive .",
    "[ mainthm ] let all the matrices @xmath21 be symmetric and @xmath120 be unbiased and independent of @xmath25 .",
    "assume that for all @xmath121 , it holds that @xmath122 , @xmath123 and @xmath124 .",
    "if @xmath125 ) , ( [ eqdefestimator ] ) and the unbiased risk estimate @xmath126 ( [ equnbiasedrisk ] ) satisfies @xmath127 \\leq \\inf_{p \\in\\mathcal{p}_\\lambda } \\biggl\\ { \\int _ { \\lambda}{\\mathbb{e}}\\bigl[\\|\\hat{\\mathbf{f}}_{\\lambda}-{\\mathbf{f}}\\|_n^2 \\bigr ] p(d\\lambda)+ \\frac{\\beta}n { \\mathcal{k}}(p,\\pi ) \\biggr\\}.\\ ] ]    assume that , for all @xmath85 , @xmath128 and @xmath129 .",
    "if @xmath130 , then the aggregate @xmath131 defined by equations ( [ eqdefweights ] ) , ( [ eqdefestimator ] ) and the adjusted risk estimate @xmath132 ( [ equnbiasedrisk1 ] ) satisfies @xmath133 & \\leq & \\inf_{p \\in\\mathcal{p}_\\lambda } \\biggl\\ { \\int _ { \\lambda}{\\mathbb{e}}\\bigl[\\|\\hat{\\mathbf{f}}_{\\lambda}-{\\mathbf{f}}\\|_n^2 \\bigr ] p(d\\lambda)+ \\frac{\\beta}n { \\mathcal{k}}(p,\\pi ) \\nonumber \\\\ & & \\hspace*{26pt } { } + \\frac1n\\int_{\\lambda } \\bigl({\\mathbf{f}}^\\top \\bigl(a_{\\lambda } -a_{\\lambda}^2 \\bigr ) { \\mathbf{f}}+{\\operatorname{tr}}\\bigl[\\sigma \\bigl(a_{\\lambda}-a_{\\lambda}^2 \\bigr ) \\bigr ] \\bigr)p(d \\lambda ) \\biggr\\}.\\end{aligned}\\ ] ]    the simplest setting in which all the conditions of part ( i ) of theorem  [ mainthm ] are fulfilled is when the matrices @xmath21 and @xmath103 are all diagonal , or diagonalizable in a common base .",
    "this result , as we will see in section  [ secmmx ] , leads to a new estimator which is adaptive , in the exact minimax sense , over the collection of all sobolev ellipsoids .",
    "it also suggests a new method for efficiently combining varying - block - shrinkage estimators , as described in section  [ sec2stein ] .",
    "however , part ( i ) of theorem  [ mainthm ] leaves open the issue of aggregating affine estimators defined via noncommuting matrices .",
    "in particular , it does not allow us to evaluate the mse of ewa when each @xmath21 is a convex or linear combination of a fixed family of projection matrices on nonorthogonal linear subspaces .",
    "these kinds of situations may be handled via the result of part ( ii ) of theorem  [ mainthm ] .",
    "one can observe that in the particular case of a finite collection of projection estimators ( i.e. , @xmath134 and @xmath135 for every @xmath34 ) , the result of part ( ii ) offers an extension of  @xcite , corollary 6 , to the case of general noise covariances ( @xcite deals only with i.i.d .",
    "noise ) .    an important situation covered by part ( ii ) of theorem  [ mainthm ] , but not by part ( i ) ,",
    "concerns the case when signals of interest @xmath65 are smooth or sparse in a basis @xmath136 which is different from the basis @xmath137 orthogonalizing the covariance matrix @xmath103 . in such a context , one may be interested in considering matrices @xmath21 that are diagonalizable in the basis @xmath138 which , in general , do not commute with @xmath103 .",
    "while the results in  @xcite yield a sharp oracle inequality in the case of projection matrices @xmath21 , they are of no help in the case when the matrices @xmath21 are nearly idempotent and not exactly . assertion ( ii ) of theorem  [ mainthm ] fills this gap by showing that if @xmath139\\le\\delta$ ] , then @xmath140 $ ] is bounded by @xmath141 p(d\\lambda)+ \\frac{\\beta}n { \\mathcal{k}}(p,\\pi ) \\biggr\\}+\\delta",
    "\\bigl(\\|{\\mathbf{f}}\\|_n^2+n^{-1}| \\!|\\!|{\\sigma}|\\!|\\!| \\bigr).\\ ] ]    we have focused only on gaussian errors to emphasize that it is possible to efficiently aggregate almost any family of _ affine estimators_. we believe that by a suitable adaptation of the approach developed in  @xcite , claims of theorem  [ mainthm ] can be generalized  at least when @xmath142 are independent with known variances  to some other common noise distributions .",
    "the results presented so far concern the situation when the matrices @xmath21 are symmetric .",
    "however , using the last part of theorem  [ mainthm ] , it is possible to propose an estimator of @xmath65 that is almost as accurate as the best affine estimator @xmath143 even if the matrices @xmath21 are not symmetric .",
    "interestingly , the estimator enjoying this property is not obtained by aggregating the original estimators @xmath19 but the `` symmetrized '' estimators @xmath144 , where @xmath145 .",
    "besides symmetry , an advantage of the matrices @xmath146 , as compared to the @xmath21 s , is that they automatically satisfy the contraction condition @xmath147 required by part ( ii ) of theorem  [ mainthm ] .",
    "we will refer to this method as symmetrized exponentially weighted aggregates ( or sewa ) @xcite .    [ mainthmcor ] assume that the matrices @xmath21 and the vectors @xmath148 satisfy @xmath149 for every @xmath85 .",
    "assume in addition that @xmath120 is an unbiased estimator of @xmath103 and is independent of @xmath25 .",
    "let @xmath150 denote the exponentially weighted aggregate of the ( symmetrized ) estimators @xmath151 with the weights  ( [ eqdefweights ] ) defined via the risk estimate @xmath105 . then",
    ", under the conditions @xmath130 and    @xmath152    it holds that    @xmath153   \\leq\\inf_{p \\in\\mathcal{p}_\\lambda } \\biggl\\ { \\int _ { \\lambda}{\\mathbb{e}}\\bigl[\\|\\hat{\\mathbf{f}}_{\\lambda}-{\\mathbf{f}}\\|_n^2 \\bigr]p(d\\lambda)+ \\frac{\\beta}n { \\mathcal{k}}(p,\\pi ) \\biggr\\}.\\ ] ]    to understand the scope of condition ( [ eqc ] ) , let us present several cases of widely used linear estimators for which this condition is satisfied :    * the simplest class of matrices @xmath21 for which condition ( [ eqc ] ) holds true are orthogonal projections . indeed ,",
    "if @xmath21 is a projection matrix , it satisfies @xmath154 and , therefore , @xmath155 .",
    "* when the matrix @xmath120 is diagonal , then a sufficient condition for ( [ eqc ] ) is @xmath156 .",
    "consequently , ( [ eqc ] ) holds true for matrices having only zeros on the main diagonal . for instance",
    ", the @xmath157nn filter in which the weight of the observation @xmath158 is replaced by zero , that is , @xmath159 satisfies this condition .",
    "* under a little bit more stringent assumption of homoscedasticity , that is , when @xmath160 , if the matrices @xmath21 are such that all the nonzero elements of each row are equal and sum up to one ( or a quantity larger than one ) , then @xmath161 and ( [ eqc ] ) is fulfilled .",
    "a notable example of linear estimators that satisfy this condition are nadaraya ",
    "watson estimators with rectangular kernel and nearest neighbor filters .",
    "before elaborating on the main results stated in the previous section , by extending them to inverse problems and by deriving adaptive procedures , let us discuss some aspects of the presented ois .      in some rare situations , the matrix @xmath103 is known and it is natural to use @xmath162 as an unbiased estimator . besides this not very realistic situation , there are at least two contexts in which it is reasonable to assume that an unbiased estimator of @xmath103 , independent of @xmath25 , is available .",
    "the first case corresponds to problems in which a signal can be recorded several times by the same device , or once but by several identical devices .",
    "for instance , this is the case when an object is photographed many times by the same digital camera during a short time period .",
    "let @xmath163 be the available signals , which can be considered as i.i.d .",
    "copies of an @xmath164-dimensional gaussian vector with mean @xmath65 and covariance matrix @xmath165 . then , defining @xmath166 and @xmath167 , we find ourselves within the framework covered by previous theorems .",
    "indeed , @xmath168 with @xmath169 and @xmath170 is an unbiased estimate of @xmath171 , independent of @xmath25 .",
    "note that our theory applies in this setting for every integer @xmath172 .",
    "the second case is when the dominating part of the noise comes from the device which is used for recording the signal . in this case",
    ", the practitioner can use the device in order to record a known signal , @xmath173 . in digital image processing",
    ", @xmath173 can be a black picture .",
    "this will provide a noisy signal @xmath174 drawn from gaussian distribution @xmath175 , independent of @xmath25 which is the signal of interest .",
    "setting @xmath176 , one ends up with an unbiased estimator of @xmath103 , which is independent of @xmath25 .",
    "all the results stated in this work provide sharp nonasymptotic bounds on the expected risk of ewa",
    ". it would be insightful to complement this study by risk bounds that hold true with high probability .",
    "however , it was recently proved in  @xcite that ewa is deviation suboptimal : there exist a family of constituent estimators and a constant @xmath177 such that the difference between the risk of ewa and that of the best constituent estimator is larger than @xmath178 with probability at least @xmath179 .",
    "nevertheless , several empirical studies ( see , e.g. ,  @xcite ) demonstrated that ewa has often a smaller risk than some of its competitors , such as the empirical star procedure  @xcite , which are provably optimal in the sense of ois with high probability .",
    "furthermore , numerical experiments carried out in section  [ secexperiments ] show that the standard - deviation of the risk of ewa is of the order of @xmath180 .",
    "this suggests that under some conditions on the constituent estimators it might be possible to establish ois for ewa that are similar to ( [ ineqkl ] ) but hold true with high probability .",
    "a step in proving this kind of result was done in  @xcite , theorem  c , for the model of regression with random design .",
    "the oi of the previous section requires various conditions on the constituent estimators @xmath181 .",
    "one may wonder how general these conditions are and is it possible to extend these ois to more general @xmath17 s .",
    "although this work does not answer this question , we can sketch some elements of response .",
    "first of all , we stress that the conditions of the present paper relax significantly those of previous results existing in statistical literature . for instance , kneip @xcite considered only linear estimators , that is , @xmath182 and , more importantly , only ordered sets of _ commuting _ matrices @xmath21 .",
    "the ordering assumption is dropped in  leung and barron @xcite , in the case of projection matrices .",
    "note that neither of these assumptions is satisfied for the families of pinsker and tikhonov  philipps estimators .",
    "the present work strengthens existing results in considering more general , affine estimators extending both projection matrices and ordered commuting matrices .    despite the advances achieved in this work",
    ", there are still interesting cases that are not covered by our theory .",
    "we now introduce a family of estimators commonly used in image processing that do not satisfy our assumptions . in recent years ,",
    "nonlocal means ( nlm ) became quite popular in image processing  @xcite .",
    "this method of signal denoising , shown to be tied in with ewa  @xcite , removes noise by exploiting signals self - similarities .",
    "we briefly define the nlm procedure in the case of one - dimensional signals .",
    "assume that a vector @xmath183 given by ( [ eqmodel ] ) is observed with @xmath184 , @xmath185 , for some function @xmath186\\to{\\mathbb{r}}$ ] .",
    "for a fixed `` patch - size '' @xmath187 , let us define @xmath188}=(f_i , f_{i+1},\\ldots , f_{i+k-1})^\\top $ ] and @xmath189}=(y_i , y_{i+1},\\ldots , y_{i+k-1})^\\top $ ] for every @xmath190 .",
    "the vectors @xmath188}$ ] and @xmath189}$ ] are , respectively , called _ true patch _ and _ noisy patch_. the nlm consists in regarding the noisy patches @xmath189}$ ] as constituent estimators for estimating the true patch @xmath191}$ ] by applying ewa .",
    "one easily checks that the constituent estimators @xmath189}$ ] are affine in @xmath192}$ ] , that is , @xmath193}=a_i{\\mathbf{y } } _ { [ i_0]}+{\\mathbf{b}}_i$ ] with @xmath194 and @xmath195 independent of @xmath192}$ ] . indeed ,",
    "if the distance between @xmath69 and @xmath196 is larger than @xmath157 , then @xmath189}$ ] is independent of @xmath192}$ ] and , therefore , @xmath197 and @xmath198}$ ] .",
    "if @xmath199 , then the matrix @xmath194 is a suitably chosen shift matrix and @xmath195 is the projection of @xmath189}$ ] onto the orthogonal complement of the image of @xmath194 .",
    "unfortunately , these matrices @xmath200 and vectors @xmath201 do not fit our framework , that is , the assumption @xmath202 is not satisfied .    finally , our proof technique is specific to affine estimators .",
    "its extension to estimators defined as a more complex function of the data will certainly require additional tools and is a challenging problem for future research .",
    "yet , it seems unlikely to get sharp ois with optimal remainder term for a fairly general family of constituent estimators ( without data - splitting ) , since this generality inherently increases the risk of overfitting .",
    "as explained in  @xcite , the model of heteroscedastic regression is well suited for describing inverse problems .",
    "in fact , let @xmath203 be a known linear operator on some hilbert space @xmath204 , with inner product @xmath205 . for some @xmath206 , let @xmath207 be the random process indexed by @xmath208 such that @xmath209 where @xmath210 is the noise magnitude and @xmath211 is a white gaussian noise on @xmath204 , that is , for any @xmath212 the vector @xmath213 is gaussian with zero mean and covariance matrix @xmath214 .",
    "the problem is then the following : estimate the element @xmath215 assuming the value of @xmath207 can be measured for any given @xmath216 .",
    "it is customary to use as @xmath216 the eigenvectors of the adjoint @xmath217 of @xmath203 . under the condition that the operator @xmath218 is compact , the svd yields @xmath219 and @xmath220 , for @xmath221 , where @xmath222 are the singular values",
    ", @xmath223 is an orthonormal basis in @xmath224 and @xmath225 is the corresponding orthonormal basis in @xmath226 . in view of  ( [ eqinversepb ] )",
    ", it holds that @xmath227 since in practice only a finite number of measurements can be computed , it is natural to assume that the values @xmath228 are available only for @xmath157 smaller than some integer @xmath164 . under the assumption that @xmath229 , the last equation is equivalent to ( [ eqmodel ] ) with @xmath230 and @xmath231 for @xmath232 . examples of inverse problems to which this statistical model has been successfully applied are derivative estimation , deconvolution with known kernel , computerized tomography ",
    "see  @xcite and the references therein for more applications .",
    "for very mildly ill - posed inverse problems , that is , when the singular values @xmath222 of @xmath203 tend to zero not faster than any negative power of @xmath157 , the approach presented in section  [ secmainresult ] will lead to satisfactory results .",
    "indeed , by choosing @xmath233 or @xmath234 , the remainder term in ( [ ineqkl ] ) and ( [ ineqkl4 ] ) becomes  up to a logarithmic factor ",
    "proportional to @xmath235 , which is the optimal rate in the case of very mild ill - posedness .",
    "however , even for mildly ill - posed inverse problems , the approach developed in the previous section becomes obsolete since the remainder blows up when @xmath164 increases to infinity .",
    "furthermore , this is not an artifact of our theoretical results , but rather a drawback of the aggregation strategy adopted in the previous section . indeed , the posterior probability measure @xmath90 defined by ( [ eqdefweights ] ) can be seen as the solution of the entropy - penalized empirical risk minimization problem : @xmath236 where the @xmath237 is taken over the set of all probability distributions .",
    "it means the same regularization parameter @xmath96 is employed for estimating both the coefficients @xmath238 corrupted by noise of small magnitude and those corrupted by large noise .",
    "since we place ourselves in the setting of known operator @xmath203 and , therefore , known noise levels , such a uniform treatment of all coefficients is unreasonable .",
    "it is more natural to upweight the regularization term in the case of large noise downweighting the data fidelity term and , conversely , to downweight the regularization in the case of small noise .",
    "this motivates our interest in the grouped ewa ( or gewa ) .",
    "let us consider a partition @xmath239 of the set @xmath240 : @xmath241 , for some integers @xmath242 .",
    "to each element @xmath243 of this partition , we associate the data sub - vector @xmath244 and the sub - vector of true function @xmath245 . as in previous sections , we are concerned by the aggregation of affine estimators @xmath181 , but here we will assume the matrices @xmath21 are block - diagonal : @xmath246 \\qquad\\mbox{with } a_{\\lambda}^j\\in { \\mathbb{r}}^{(t_{j+1}-t_j)\\times ( t_{j+1}-t_j)}.\\ ] ] similarly , we define @xmath247 and @xmath248 as the sub - vectors of @xmath17 and @xmath23 , respectively , corresponding to the indices belonging to @xmath243 .",
    "we will also assume that the noise covariance matrix @xmath103 and its unbiased estimate @xmath120 are block - diagonal with @xmath249 blocks @xmath250 and @xmath251 , respectively .",
    "this notation implies , in particular , that @xmath252 for every @xmath253 .",
    "moreover , the unbiased risk estimate @xmath105 of @xmath17 can be decomposed into the sum of unbiased risk estimates @xmath254 of @xmath255 , namely , @xmath256 , where @xmath257 , \\qquad j=1,\\ldots , j.\\ ] ] to state the analogues of theorems  [ mainthm ] and  [ mainthmcor ] , we introduce the following settings .    for all @xmath121 and @xmath258 , @xmath259 are symmetric and satisfy @xmath260 , @xmath261 and @xmath262 .",
    "for a temperature vector @xmath263 and a prior @xmath88 , we define gewa as @xmath264 , where @xmath265 with @xmath266    for every @xmath253 and for every @xmath34 belonging to a set of @xmath88-measure one , the matrices @xmath21 satisfy a.s .",
    "the inequality @xmath267 while the vectors @xmath23 are such that @xmath268 . in this case , for a temperature vector @xmath263 and a prior @xmath88 ,",
    "we define gewa as @xmath269 , where @xmath270 and @xmath271 is defined by ( [ eqdefgweights ] ) .",
    "note that this setting is the grouped version of the sewa .",
    "[ thmgroup ] assume that @xmath120 is unbiased and independent of @xmath25 . under setting  1 ,",
    "if @xmath272 for all @xmath253 , then @xmath273 \\leq\\sum_{j=1}^j \\inf_{p_j } \\biggl\\ { \\int_{\\lambda}{\\mathbb{e}}\\bigl\\|\\hat { \\mathbf{f}}_{\\lambda}^j-{\\mathbf{f}}^j\\bigr\\|_n^2 p_j(d\\lambda)+\\frac{\\beta_j}n { \\mathcal{k}}(p_j,\\pi ) \\biggr\\}.\\ ] ] under setting  2 , this inequality holds true if @xmath274 , this theorem allows us to propose an estimator of the unknown signal which is adaptive w.r.t .",
    "the smoothness properties of the underlying signal and achieves the minimax rates and constants over the sobolev ellipsoids provided that the operator @xmath203 is mildly ill - posed , that is , its singular values decrease at most polynomially .",
    "in this section we discuss consequences of the main result for specific choices of prior measures . for conveying the main messages of this section it is enough to focus on settings 1 and 2 in the case of only one group ( @xmath275 ) .      in order to demonstrate that inequality ( [ ineqklgroup ] ) can be reformulated in terms of an oi as defined by ( [ ineqoracleex ] ) ,",
    "let us consider the case when the prior @xmath88 is discrete , that is , @xmath276 for a countable set @xmath277 , and w.l.o.g @xmath278 .",
    "then , the following result holds true .",
    "[ propdiscret ] let @xmath120 be unbiased , independent of @xmath25 and @xmath88 be supported by  @xmath279 . under setting  1 with @xmath275 and @xmath280 ,",
    "the aggregate @xmath281 satisfies the inequality @xmath282 \\leq\\inf_{\\ell\\in{\\mathbb{n}}\\dvtx\\pi _ \\ell>0 } \\biggl ( { \\mathbb{e}}\\bigl [ \\| \\hat{\\mathbf{f}}_{\\ell}-{\\mathbf{f}}\\|_n^2 \\bigr ] + \\frac{\\beta\\log(1/\\pi_\\ell)}n \\biggr).\\ ] ] furthermore , ( [ ineqkldiscrete ] ) holds true under setting  2 for @xmath130 .",
    "it suffices to apply theorem  [ thmgroup ] and to upper - bound the right - hand side by the minimum over all dirac measures @xmath283 such that @xmath284 .",
    "this inequality can be compared to corollary 2 in  @xcite , section 4.3 .",
    "our result has the advantage of having factor one in front of the expectation of the left - hand side , while in  @xcite a constant much larger than 1 appears .",
    "however , it should be noted that the assumptions on the ( estimated ) noise covariance matrix are much weaker in  @xcite .",
    "it may be useful in practice to combine a family of affine estimators indexed by an open subset of @xmath78 for some @xmath285 ( e.g. , to build an estimator nearly as accurate as the best kernel estimator with fixed kernel and varying bandwidth ) .",
    "to state an oracle inequality in such a `` continuous '' setup , let us denote by @xmath286 the largest real @xmath287 such that the ball centered at @xmath288 of radius @xmath289hereafter denoted by @xmath290is included in @xmath77 .",
    "let @xmath291 be the lebesgue measure in @xmath78 .",
    "[ prop2 ] let @xmath120 be unbiased , independent of @xmath25 .",
    "let @xmath292 be an open and bounded set and let @xmath88 be the uniform distribution on @xmath77 .",
    "assume that the mapping @xmath293 is lipschitz continuous , _ that is _",
    ", @xmath294 , @xmath295 . under setting  1 with @xmath275 and @xmath280 ,",
    "the aggregate @xmath296 satisfies the inequality @xmath297 + \\frac{\\beta m}{n } \\log \\biggl(\\frac{\\sqrt{m}}{2\\min ( n^{-1},d_2(\\llambda,\\partial\\lambda ) ) } \\biggr ) \\biggr\\ } \\nonumber\\hspace*{-30pt } \\\\[-4pt ] \\\\[-12pt ] \\nonumber & & { } + \\frac{l_r+\\beta\\log(\\operatorname{leb}(\\lambda))}{n}.\\hspace*{-30pt}\\end{aligned}\\ ] ] furthermore , ( [ ineqklcont ] ) holds true under setting  2 for every @xmath130 .",
    "it suffices to apply assertion ( i ) of theorem  [ mainthm ] and to upper - bound the right - hand side in inequality ( [ ineqkl ] ) by the minimum over all measures having as density @xmath298 .",
    "choosing @xmath299 such that @xmath300 , the measure @xmath301 is absolutely continuous w.r.t .",
    "the uniform prior @xmath88 and the kullback ",
    "leibler divergence between these two measures equals @xmath302 . using @xmath303 and the lipschitz condition",
    ", we get the desired inequality .",
    "note that it is not very stringent to require the risk function @xmath304 to be lipschitz continuous , especially since this condition needs not be satisfied uniformly in @xmath65 .",
    "let us consider the ridge regression : for a given design matrix @xmath305 , @xmath306 and @xmath124 with @xmath307 $ ] , @xmath308 being a given normalization factor typically set to @xmath164 or @xmath309 , @xmath310 and @xmath311 $ ] .",
    "one can easily check the lipschitz property of the risk function with @xmath312 .",
    "the continuous oracle inequality stated in the previous subsection is well adapted to the problems in which the dimension @xmath313 of @xmath77 is small w.r.t . the sample size @xmath164 ( or , more precisely , the signal to noise ratio @xmath314 ) . when this is not the case , the choice of the prior should be done more carefully . for instance , consider @xmath315 with large @xmath313 under the sparsity scenario : there is a sparse vector @xmath316 such that the risk of @xmath317 is small .",
    "then , it is natural to choose a prior that favors sparse @xmath288 s .",
    "this can be done in the same vein as in  @xcite , by means of the heavy tailed prior , @xmath318 let @xmath120 be unbiased , independent of @xmath25 .",
    "let @xmath319 and let @xmath88 be defined by ( [ eqsparseprior ] ) .",
    "assume that the mapping @xmath293 is continuously differentiable and , for some @xmath320 matrix @xmath321 , satisfies @xmath322 under setting  1 if @xmath323 , then the aggregate @xmath324 satisfies @xmath325 & \\leq&\\inf_{\\llambda\\in{\\mathbb{r}}^m } \\biggl\\{{\\mathbb{e}}\\|\\hat { \\mathbf { f}}_{\\lambda}-{\\mathbf{f}}\\|_n^2 + \\frac{4\\beta}n \\sum_{m=1}^m \\log \\biggl(1 + \\frac{|\\lambda_m|}{\\tau } \\biggr ) \\biggr\\ } \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & { } + { \\operatorname{tr}}(\\mathcal m)\\tau^2.\\end{aligned}\\ ] ] moreover , ( [ ineqklsparse ] ) holds true under setting  2 if @xmath326 .",
    "let us discuss here some consequences of this sparsity oracle inequality .",
    "first of all , consider the case of ( linearly ) combining frozen estimators , that is , when @xmath327 with some known functions @xmath328 .",
    "then , it is clear that @xmath329 , where @xmath330 is the gram matrix defined by @xmath331 .",
    "so the condition in proposition  [ prop3 ] consists in bounding the gram matrix of the atoms @xmath328 .",
    "let us remark that in this case ",
    "see , for instance , @xcite@xmath332 is on the order of @xmath313 and the choice @xmath333 ensures that the last term in the right - hand side of equation  ( [ ineqklsparse ] ) decreases at the parametric rate @xmath180 .",
    "this is the choice we recommend for practical applications .    as a second example , let us consider the case of a large number of linear estimators @xmath334 satisfying conditions of setting  1 and such that @xmath335 .",
    "assume we aim at proposing an estimator mimicking the behavior of the best possible convex combination of a pair of estimators chosen among @xmath336 .",
    "this task can be accomplished in our framework by setting @xmath337 and @xmath338 , where @xmath339 .",
    "remark that if @xmath340 satisfies conditions of setting  1 , so does @xmath341 .",
    "moreover , the mapping @xmath293 is quadratic with hessian matrix @xmath342 given by the entries @xmath343 , @xmath344 .",
    "it implies that inequality  ( [ eqlip2 ] ) holds with @xmath345 .",
    "therefore , denoting by @xmath346 the @xmath69th diagonal entry of @xmath103 and setting @xmath347 , we get @xmath348\\le m [ \\|{\\mathbf{f}}\\|_n^2+\\|{\\bolds{\\sigma}}\\|_n^2 ] $ ] . applying proposition  [ prop3 ] with @xmath333",
    ", we get @xmath349 & \\leq&\\inf_{\\alpha , m , m ' }   { \\mathbb{e}}\\bigl[\\bigl\\|\\alpha \\hat{{\\mathbf{g}}}_{m } + ( 1-\\alpha)\\hat{{\\mathbf{g}}}_{m'}-{\\mathbf{f}}\\bigr\\|_n^2 \\bigr ] \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & { } + \\frac{8\\beta}n \\log \\biggl(1 + \\biggl[{\\frac{mn}{\\beta } } \\biggr]^{1/2 } \\biggr)+ \\frac{\\beta}{n } \\bigl[\\|{\\mathbf{f}}\\|_n^2+\\|{\\bolds{\\sigma}}\\|_n^2 \\bigr],\\end{aligned}\\ ] ] where the @xmath237 is taken over all @xmath350 $ ] and @xmath351 . this inequality is derived from ( [ ineqklsparse ] ) by upper - bounding the @xmath352 by the infimum over @xmath288 s having at most two nonzero coefficients , @xmath353 and @xmath354 , that are nonnegative and sum to one : @xmath355 . to get ( [ ineqklsparse1 ] ) , one simply notes that only two terms of the sum @xmath356 are nonzero and each of them is not larger than @xmath357 .",
    "thus , one can achieve using ewa the best possible risk over the convex combinations of a pair of linear estimators  selected from a large ( but finite ) family  at the price of a residual term that decreases at the parametric rate up to a log factor .",
    "let us consider now the problem of aggregation of two - block shrinkage estimators .",
    "this means that the constituent estimators have the following form : for @xmath358 ^ 2\\times\\{1,\\ldots , n\\}:=\\lambda$ ] , @xmath359 where @xmath360 .",
    "let us choose the prior @xmath88 as uniform on @xmath77 .",
    "[ propshrinkageoi ] let @xmath92 be the exponentially weighted aggregate having as constituent estimators two - block shrinkage estimators @xmath361 . if @xmath103 is diagonal , then for any @xmath362 and for any @xmath363 , @xmath364 \\leq{\\mathbb{e}}\\bigl[\\|\\hat{\\mathbf{f}}_{\\llambda}- { \\mathbf{f}}\\|_n^2 \\bigr]+ \\frac{\\beta}{n } \\biggl\\{1+\\log \\biggl(\\frac{n^2\\|{\\mathbf{f}}\\|_n^2+n{\\operatorname{tr}}(\\sigma)}{12\\beta } \\biggr ) \\biggr\\ } .\\ ] ]    in the case @xmath365 , this result is comparable to  @xcite , page  20 , theorem  2.49 , which states that in the homoscedastic regression model ( @xmath366 ) , ewa acting on two - block positive - part james  stein estimators satisfies , for any @xmath362 such that @xmath367 and for @xmath368 , the oracle inequality @xmath369 \\leq { \\mathbb{e}}\\bigl[\\| \\hat{\\mathbf{f}}_{\\llambda}-{\\mathbf{f}}\\|_n^2 \\bigr]+ \\frac{9}{n } + \\frac{8}{n}\\min_{k>0 } \\biggl\\{k\\vee \\biggl ( \\log \\frac { n-6}{k}-1 \\biggr ) \\biggr\\}.\\hspace*{-35pt}\\ ] ]",
    "pinsker proved in his celebrated paper  @xcite that in the model ( [ eqmodel ] ) the minimax risk over ellipsoids can be asymptotically attained by a linear estimator .",
    "let us denote by @xmath370 the coefficients of the ( orthogonal ) discrete cosine such that @xmath371 . ]",
    "( dct ) transform of @xmath65 , hereafter denoted by @xmath372 .",
    "pinsker s result  restricted to sobolev ellipsoids @xmath373 states that , as @xmath82 , the equivalences @xmath374 & \\sim&\\inf_{a } \\sup_{f\\in\\mathcal f_{\\dst } ( \\alpha , r ) } { \\mathbb{e}}\\bigl[\\|a{\\mathbf{y}}-{\\mathbf{f}}\\|_n^2 \\bigr ] \\\\",
    "& \\sim&\\inf_{w>0}\\sup_{f\\in\\mathcal f_\\dst(\\alpha , r ) } { \\mathbb{e}}\\bigl[\\| a_{\\alpha , w } { \\mathbf{y}}- { \\mathbf{f}}\\|_n^2 \\bigr]\\end{aligned}\\ ] ] hold  @xcite , theorem  3.2 , where the first @xmath237 is taken over all possible estimators  @xmath14 and @xmath375 is the pinsker filter in the discrete cosine basis . in simple words , this implies that the ( asymptotically ) minimax estimator can be chosen from the quite narrow class of linear estimators with pinsker s filter .",
    "however , it should be emphasized that the minimax linear estimator depends on the parameters @xmath376 and @xmath377 , that are generally unknown .",
    "an ( adaptive ) estimator , that does not depend on @xmath378 and is asymptotically minimax over a large scale of sobolev ellipsoids , has been proposed by  efromovich and pinsker  @xcite .",
    "the next result , that is , a direct consequence of theorem  [ mainthm ] , shows that ewa with linear constituent estimators is also asymptotically sharp adaptive over sobolev ellipsoids .",
    "[ propminimax ] let @xmath379 and consider the prior @xmath380 where @xmath381 .",
    "then , in model ( [ eqmodel ] ) with homoscedastic errors , the aggregate @xmath92 based on the temperature @xmath382 and the constituent estimators @xmath383 ( with @xmath384 being the pinsker filter ) is adaptive in the exact minimax sense on the family of classes @xmath385 .",
    "it is worth noting that the exact minimax adaptivity property of our estimator @xmath92 is achieved without any tuning parameter .",
    "all previously proposed methods that are provably adaptive in an exact minimax sense depend on some such as the lengths of blocks for blockwise stein  @xcite and efromovich ",
    "pinsker  @xcite estimators or the step of discretization and the maximal value of bandwidth  @xcite .",
    "another nice property of the estimator @xmath92 is that it does not require any pilot estimator based on the data splitting device  @xcite .",
    "we now turn to the setup of heteroscedastic regression , which corresponds to ill - posed inverse problems as described in section  [ secill - posed ] .",
    "to achieve adaptivity in the exact minimax sense , we make use of @xmath296 , the grouped version of the exponentially weighted aggregate .",
    "we assume hereafter that the matrix @xmath103 is diagonal with diagonal entries @xmath386 satisfying the following property : @xmath387 this kind of problems arises when @xmath203 is a differential operator or the radon transform  @xcite , section 1.3 . to handle such situations",
    ", we define the groups in the same spirit as the weakly geometrically increasing blocks in  @xcite .",
    "let @xmath388 be a positive integer that increases as @xmath82 .",
    "set @xmath389 and define @xmath390 where @xmath391 stands for the largest integer strictly smaller than @xmath392 .",
    "let @xmath393 be the smallest integer @xmath68 such that @xmath394 .",
    "we redefine @xmath395 and set @xmath396 for all @xmath253 .",
    "[ propminimax2 ] let the groups @xmath397 be defined as above with @xmath398 satisfying @xmath399 and @xmath400 as @xmath82 .",
    "let @xmath401 and consider the prior @xmath402 then , in model ( [ eqmodel ] ) with diagonal covariance matrix @xmath403 satisfying condition ( [ cgamma ] ) , the aggregate @xmath296 ( under setting  1 ) based on the temperatures @xmath404 and the constituent estimators @xmath383 ( with @xmath384 being the pinsker filter ) is adaptive in the exact minimax sense on the family of classes @xmath405 .",
    "note that this result provides an estimator attaining the optimal constant in the minimax sense when the unknown signal lies in an ellipsoid .",
    "this property holds because minimax estimators over the ellipsoids are linear . for other subsets of @xmath12 , such as hyper - rectangles , besov bodies and so on ,",
    "this is not true anymore . however , as proved by donoho , liu and macgibbon  @xcite , for orthosymmetric quadratically convex sets the minimax linear estimators have a risk which is within @xmath406 of the minimax risk among all estimates .",
    "therefore , following the approach developed here , it is also possible to prove that gewa can lead to an adaptive estimator whose risk is within @xmath406 of the minimax risk , for a broad class of hyperrectangles .",
    "in this section we present some numerical experiments on synthetic data , by focusing only on the case of homoscedastic gaussian noise ( @xmath407 ) with known variance .",
    "a toolbox is made available freely for download at http://josephsalmon.eu/code/index_codes.php .",
    "additional details and numerical experiments can be found in @xcite .",
    "we evaluate different estimation routines on several 1d signals considered as a benchmark in the literature on signal processing  @xcite .",
    "the six signals we retained for our experiments because of their diversity are depicted in figure  [ figtestedsignal ] .",
    "since these signals are nonsmooth , we have also carried out experiments on their smoothed versions obtained by taking the antiderivative .",
    "experiments on nonsmooth ( resp . , smooth )",
    "signals are referred to as experiment i ( resp .",
    ", experiment  ii ) . in both cases ,",
    "prior to applying estimation routines , we normalize the ( true ) sampled signal to have an empirical norm equal to one and use the dct denoted by @xmath408 .",
    "the four tested estimation routines  including ewa  are detailed below .    _",
    "soft - thresholding  @xcite _ : for a given shrinkage parameter @xmath409 , the soft - thresholding estimator is @xmath410 .",
    "we use the data - driven threshold minimizing the stein unbiased risk estimate  @xcite .",
    "_ blockwise james  stein shrinkage  @xcite _ : the set @xmath411 is partitioned into @xmath412 $ ] blocks @xmath413 of nearly equal size @xmath414 .",
    "the corresponding blocks of true coefficients @xmath415 are then estimated by @xmath416 @xmath417 , with blocks of noisy coefficients @xmath418 , @xmath419 and @xmath420 .    _ unbiased risk estimate minimization with pinsker s filters  @xcite _ : pinsker filter with data - driven parameters @xmath376 and @xmath421 selected by minimizing an unbiased estimate of the risk over a suitably chosen grid for the values of @xmath376 and @xmath421 . here",
    ", we use geometric grids ranging from @xmath422 to @xmath423 for @xmath376 and from @xmath424 to @xmath164 for @xmath421 .",
    "_ ewa on pinsker s filters _ : we consider the same finite family of linear filters ( defined by pinsker s filters ) as in the ure routine described above . according to proposition  [ propdiscret ] , this leads to an estimator nearly as accurate as the best pinsker s estimator in the given family .    to report the result of our experiments ,",
    "we have also computed the best linear smoother , hereafter referred to as the oracle , based on a pinsker filter chosen among the candidates that we used for defining ure and ewa . by",
    "best smoother we mean the one minimizing the squared error ( it can be computed since we know the ground truth ) .",
    "results summarized in table  [ tab1 ] for experiment i and table  [ tab2 ] for experiment ii correspond to the average over 1000 trials of the mean squared error ( mse ) from which we subtract the mse of the oracle and multiply the resulting difference by the sample size .",
    "we report the results for @xmath425 and for @xmath426 .",
    "simulations show that ewa and ure have very comparable performances and are significantly more accurate than soft - thresholding and block james  stein ( cf . table  [ tab1 ] ) for every size @xmath164 of signals considered .",
    "improvements are particularly important when signals have large peaks or discontinuities . in most cases",
    ", ewa also outperforms ure , but differences are less pronounced .",
    "one can also observe that for smooth signals , the difference of mses between ewa and the oracle , multiplied by @xmath164 , remains nearly constant when @xmath164 varies .",
    "this is in agreement with our theoretical results in which the residual term decreases to zero inversely proportionally to @xmath164 .",
    "of course , soft - thresholding and blockwise james  stein procedures have been designed for being applied to the wavelet transform of a besov smooth function , rather than to the fourier transform of a sobolev - smooth function . however , the point here is not to demonstrate the superiority of ewa as compared to st and bjs procedures .",
    "the point is to stress the importance of having sharp adaptivity up to an optimal constant and not simply adaptivity in the sense of rate of convergence .",
    "indeed , the procedures st and bjs are provably rate - adaptive when applied to the fourier transform of a sobolev - smooth function , but they are not sharp adaptive  they do not attain the optimal constant  whereas ewa and ure do attain .",
    "@ld2.3d2.3d2.3d2.3cd2.3d2.3d2.3d2.3@ & & & & & & & & & + & & & + 256&0.051&0.245&9.617&4.846&&0.062&0.212&13.233&6.036 + & ( 0.42)&(0.39)&(1.78)&(1.29)&&(0.35)&(0.31)&(2.11)&(1.23 ) + 512&-0.052&0.302&13.807&9.256&&-0.100&0.205&17.080&12.620 + & ( 0.35)&(0.50)&(2.16)&(1.70)&&(0.30)&(0.39)&(2.29)&(1.75 ) + 1024&-0.050&0.299&19.984&17.569&&-0.107&0.270&21.862&23.006 + & ( 0.36)&(0.46)&(2.68)&(2.17)&&(0.35)&(0.41)&(2.92)&(2.35 ) + 2048&-0.007&0.362&28.948&30.447&&-0.150&0.234&28.733&38.671 + & ( 0.42)&(0.57)&(3.31)&(2.96)&&(0.34)&(0.42)&(3.19)&(3.02 ) + & & & + 256&-0.060&0.247&1.155&3.966&&-0.069&0.248&8.883&4.879 + & ( 0.19)&(0.42)&(0.57)&(1.12)&&(0.32)&(0.40)&(1.76)&(1.20 ) + 512&-0.079&0.215&2.064&5.889&&-0.105&0.237&12.147&9.793 + & ( 0.19)&(0.39)&(0.86)&(1.36)&&(0.30)&(0.37)&(2.28)&(1.64 ) + 1024&-0.059&0.240&3.120&8.685&&-0.092&0.291&15.207&16.798 + & ( 0.23)&(0.36)&(1.20)&(1.64)&&(0.34)&(0.46)&(2.18)&(2.13 ) + 2048&-0.051&0.278&4.858&12.667&&-0.059&0.283&21.543&27.387 + & ( 0.25)&(0.48)&(1.42)&(2.03)&&(0.34)&(0.54)&(2.47)&(2.77 ) + & & & + 256&0.038&0.294&6.933&5.644&&0.017&0.203&12.201&3.988 + & ( 0.37)&(0.47)&(1.54)&(1.20)&&(0.37)&(0.37)&(1.81)&(1.19 ) + 512&0.010&0.293&9.712&9.977&&-0.078&0.312&17.765&9.031 + & ( 0.36)&(0.51)&(1.76)&(1.67)&&(0.35)&(0.49)&(2.72)&(1.62 ) + 1024&-0.002&0.300&13.656&16.790&&-0.026&0.321&23.321&17.565 + & ( 0.30)&(0.45)&(2.25)&(2.06)&&(0.38)&(0.48)&(2.96)&(2.28 ) + 2048&0.007&0.312&19.113&27.315&&-0.007&0.314&31.550&29.461 + & ( 0.34)&(0.50)&(2.68)&(2.61)&&(0.41)&(0.49)&(3.05)&(2.95 ) +    @ld2.3d2.3d2.3d2.3cd2.3d2.3d2.3d2.3@ & & & & & & & & & + & & & + 256&0.387&0.216&0.216&2.278&&0.214&0.237&1.608&2.777 + & ( 0.43)&(0.40)&(0.24)&(0.98)&&(0.23)&(0.40)&(0.73)&(1.04 ) + 512&0.170&0.209&0.650&3.193&&0.165&0.250&1.200&3.682 + & ( 0.20)&(0.41)&(0.25)&(1.07)&&(0.20)&(0.44)&(0.48)&(1.24 ) + 1024&0.162&0.226&1.282&4.507&&0.147&0.229&1.842&5.043 + & ( 0.18)&(0.41)&(0.44)&(1.28)&&(0.19)&(0.45)&(0.86)&(1.43 ) + 2048&0.120&0.220&1.574&6.107&&0.138&0.229&1.864&6.584 + & ( 0.17)&(0.37)&(0.55)&(1.55)&&(0.20)&(0.40)&(1.07)&(1.58 ) + & & & + 256&0.217&0.207&1.399&2.496&&0.269&0.279&2.120&2.053 + & ( 0.16)&(0.42)&(0.54)&(0.96)&&(0.27)&(0.49)&(1.09)&(0.95 ) + 512&0.206&0.221&0.024&3.045&&0.216&0.248&2.045&2.883 + & ( 0.18)&(0.43)&(0.26)&(1.10)&&(0.20)&(0.45)&(1.17)&(1.13 ) + 1024&0.179&0.200&0.113&3.905&&0.183&0.228&1.251&3.780 + & ( 0.18)&(0.50)&(0.27)&(1.27)&&(0.20)&(0.41)&(0.70)&(1.37 ) + 2048&0.162&0.189&0.421&5.019&&0.145&0.223&1.650&4.992 + & ( 0.15)&(0.37)&(0.27)&(1.53)&&(0.19)&(0.42)&(1.12)&(1.42 ) + & & & + 256&0.162&0.200&0.339&2.770&&0.215&0.257&1.486&2.649 + & ( 0.16)&(0.38)&(0.24)&(1.00)&&(0.25)&(0.48)&(0.68)&(1.01 ) + 512&0.150&0.215&0.425&3.658&&0.170&0.243&1.865&3.683 + & ( 0.18)&(0.38)&(0.23)&(1.20)&&(0.20)&(0.46)&(0.84)&(1.20 ) + 1024&0.146&0.211&0.935&4.815&&0.179&0.236&1.547&5.017 + & ( 0.18)&(0.39)&(0.33)&(1.35)&&(0.20)&(0.47)&(1.02)&(1.38 ) + 2048&0.141&0.221&1.316&6.432&&0.165&0.210&2.246&6.628 + & ( 0.20)&(0.43)&(0.42)&(1.54)&&(0.20)&(0.39)&(1.15)&(1.70 ) +",
    "in this paper we have addressed the problem of aggregating a set of affine estimators in the context of regression with fixed design and heteroscedastic noise . under some assumptions on the constituent estimators",
    ", we have proven that ewa with a suitably chosen temperature parameter satisfies pac - bayesian type inequality , from which different types of oracle inequalities have been deduced .",
    "all these inequalities are with leading constant one and rate - optimal residual term . as an application of our results",
    ", we have shown that ewa acting on pinsker s estimators produces an adaptive estimator in the exact minimax sense .",
    "next in our agenda is carrying out an experimental evaluation of the proposed aggregate using the approximation schemes described by dalalyan and tsybakov  @xcite , rigollet and tsybakov  @xcite and alquier and lounici  @xcite , with a special focus on the problems involving large scale data .",
    "although we do not assume the covariance matrix @xmath103 of the noise to be known , our approach relies on an unbiased estimator of @xmath103 which is independent on the observed signal and on an upper bound on the largest singular value of @xmath103 .",
    "in some applications , such information may be hard to obtain and it can be helpful to relax the assumptions on @xmath120 .",
    "this is another interesting avenue for future research for which , we believe , the approach developed by giraud  @xcite can be of valuable guidance .",
    "[ secproofs ]",
    "we develop now the detailed proofs of the results stated in the manuscript .",
    "the proofs of our main results rely on stein s lemma  @xcite , recalled below , providing an unbiased risk estimate for any estimator that depends sufficiently smoothly on the data vector @xmath25 .",
    "[ lmstein ] let @xmath25 be a random vector drawn form the gaussian distribution @xmath427 . if the estimator @xmath14 is a.e .",
    "differentiable in @xmath25 and the elements of the matrix @xmath428 have finite first moment , then @xmath429- \\frac{1}{n}{\\operatorname{tr}}[\\sigma]\\ ] ] is an unbiased estimate of @xmath430 , that is , @xmath431=r$ ] .",
    "the proof can be found in  @xcite , page 157 .",
    "we apply stein s lemma to the affine estimators @xmath181 , with @xmath21 an @xmath20 deterministic real matrix and @xmath432 a deterministic vector .",
    "we get that if @xmath433 is an unbiased estimator of @xmath103 , then @xmath434-\\frac{1}{n}{\\operatorname{tr}}[\\widehat\\sigma]$ ] is an unbiased estimator of the risk @xmath435= \\|(a_{\\lambda}-i_{n\\times n } ) { \\mathbf{f}}+\\mathbf{b}_{\\lambda}\\|_n^2+\\frac{1}{n}{\\operatorname{tr}}[a_{\\lambda}\\sigma a_{\\lambda}^\\top ] $ ] .      prior to proceeding with the proof of the main theorems ,",
    "we prove an important auxiliary result which is the central ingredient of the proofs for our main results .",
    "[ lmaux ] let assumptions of lemma  [ lmstein ] be satisfied .",
    "let @xmath436 be a family of estimators of @xmath65 and @xmath437 a family of risk estimates such that the mapping @xmath438 is a.e .",
    "differentiable for every @xmath85 .",
    "let @xmath105 be the unbiased risk estimate of @xmath17 given by stein s lemma .    for every @xmath439 and for any @xmath89 , the estimator @xmath92 defined as the average of @xmath17 w.r.t .",
    "to the probability measure @xmath440 admits @xmath441 as unbiased estimator of the risk .",
    "if , furthermore , @xmath442 , @xmath443 and @xmath444 for some constant @xmath445 , then for every @xmath446 it holds that @xmath447\\le \\inf_{p\\in\\mathcal{p}_\\lambda } \\biggl\\{\\int _ { \\lambda}{\\mathbb{e}}[\\hat r_\\lambda ] p(d\\lambda)+ \\frac{\\beta{\\mathcal{k}}(p,\\pi)}{n } \\biggr\\}.\\ ] ]    according to the stein lemma , the quantity @xmath448- \\frac { 1}{n}{\\operatorname{tr}}[\\sigma]\\ ] ] is an unbiased estimate of the risk @xmath449 $ ] . using simple algebra , one checks that @xmath450 by interchanging the integral and differential operators , we get the following relation : @xmath451.then , combining this equality with equations ( [ eqsteinhereto ] ) and ( [ eqvariancereversed ] ) implies that @xmath452 \\pi(d\\lambda).\\end{aligned}\\ ] ] after having interchanged differentiation and integration , we obtain that @xmath453 and , therefore , we come up with the following expression for @xmath454 : @xmath455 this completes the proof of the first assertion of the lemma .    to prove the second assertion , let us observe that under the required condition and in view of the first assertion , for every @xmath446 it holds that @xmath456 .",
    "to conclude , it suffices to remark that @xmath457 is the probability measure minimizing the criterion @xmath458 among all @xmath459 .",
    "thus , for every @xmath459 , we have @xmath460 taking the expectation of both sides , the desired result follows .      _ assertion _",
    "what follows , we use the matrix shorthand @xmath461 and @xmath462 .",
    "we apply lemma  [ lmaux ] with @xmath463 .",
    "to check the conditions of the second part of lemma  [ lmaux ] , note that in view of equations ( [ eqaffineestimatorsgal ] ) and ( [ equnbiasedrisk ] ) , as well as the assumptions @xmath464 and @xmath465 , we get @xmath466 recall now that for any pair of commuting matrices @xmath467 and @xmath468 the identity @xmath469 holds true . applying this identity to @xmath470 and @xmath471 ( in view of the commuting property of the @xmath21 s ) , we get the following relation : @xmath472 . when one integrates over @xmath77 with respect to the measure @xmath457 , the term of the first scalar product in the right - hand side of the last equation vanishes .",
    "on the other hand , @xmath473 since positive semi - definiteness of matrices @xmath474 implies the one of the matrix @xmath475 , we also have @xmath476 .",
    "therefore , @xmath477 this inequality implies that @xmath478 therefore , the claim of theorem  [ mainthm ] holds true for every @xmath323 .",
    "_ assertion _ ( ii ) .",
    "let now @xmath479 with symmetric @xmath480 and @xmath481 .",
    "using the definition @xmath482 , one easily checks that @xmath483 for every @xmath34 and that @xmath484 therefore , if @xmath130 , all the conditions required in the second part of lemma  [ lmaux ] are fulfilled . applying this lemma , we get the desired result .",
    "we apply the result of assertion ( ii ) of theorem  [ mainthm ] to the prior @xmath485 replaced by the probability measure proportional to @xmath486}\\pi ( d\\lambda)$ ] .",
    "this leads to @xmath487 & \\leq & \\inf_{p \\in\\mathcal{p}_\\lambda } \\biggl\\ { \\int _ { \\lambda}{\\mathbb{e}}\\bigl[\\|\\hat{\\mathbf{f}}_{\\lambda}-{\\mathbf{f}}\\|_n^2 \\bigr]p(d\\lambda)+\\frac{\\beta}n { \\mathcal{k}}(p,\\pi ) \\biggr\\ } \\\\ & & { } + \\frac{\\beta}n{\\mathbb{e}}\\biggl[\\log\\int_\\lambda e^{(2/{\\beta)}{\\operatorname{tr}}[\\widehat\\sigma(a_{\\lambda}-a_{\\lambda}^\\top a_{\\lambda})]}\\pi(d\\lambda ) \\biggr].\\end{aligned}\\ ] ] condition ( [ eqc ] ) entails that the last term is always nonnegative and the result follows .",
    "let us place ourselves in setting  1 .",
    "it is clear that @xmath488 = \\sum_{j=1}^j { \\mathbb{e}}[\\| { \\hat{\\mathbf{f}}}_\\mathrm{gewa}^j-{\\mathbf{f}}^j\\|_n^2 ] $ ] .",
    "for each @xmath258 , since @xmath272 , one can apply assertion ( i ) of theorem  [ mainthm ] , which leads to the desired result .",
    "the case of setting  2 is handled in the same manner .",
    "the authors thank pierre alquier for fruitful discussions ."
  ],
  "abstract_text": [
    "<S> we consider the problem of combining a ( possibly uncountably infinite ) set of affine estimators in nonparametric regression model with heteroscedastic gaussian noise . </S>",
    "<S> focusing on the exponentially weighted aggregate , we prove a pac - bayesian type inequality that leads to sharp oracle inequalities in discrete but also in continuous settings . </S>",
    "<S> the framework is general enough to cover the combinations of various procedures such as least square regression , kernel ridge regression , shrinking estimators and many other estimators used in the literature on statistical inverse problems . as a consequence , </S>",
    "<S> we show that the proposed aggregate provides an adaptive estimator in the exact minimax sense without discretizing the range of tuning parameters or splitting the set of observations . </S>",
    "<S> we also illustrate numerically the good performance achieved by the exponentially weighted aggregate . </S>"
  ]
}