{
  "article_text": [
    "in recent years , compressive sensing ( cs ) has influenced many fields in signal processing .",
    "basically , the theory states that if an unknown signal @xmath0 can be sparsely represented , only a few @xmath1 linear and non - adaptive measurements @xmath2 of the signal suffice to accurately reconstruct it . denoting the measurement vectors by @xmath3",
    ", the measurement process can be compactly written as @xmath4^{\\top } \\bf{s } + \\bf{z } = \\bf{\\phi } \\bf{s } + \\bf{z},\\ ] ] where @xmath5 is the measurement matrix , and @xmath6 constitutes possible sampling errors . due to the reduced dimensionality ,",
    "reconstructing @xmath7 from the measurements is ill - posed in general , and can not be done by simply inverting @xmath8 . however",
    ", additional model assumptions on @xmath7 may help to find a solution . in this context ,",
    "the _ sparse synthesis - approach _ and the _ co - sparse analysis - approach _ @xcite have proven extremely useful . in the sparse synthesis approach",
    "it is assumed that a signal can be decomposed into a linear combination of only a few columns , called atoms , of a known dictionary @xmath9 with @xmath10 , i.e. @xmath11 with @xmath12 being the sparse coefficient vector .",
    "many algorithms for solving the synthesis problem exist , cf . @xcite for an extensive overview .",
    "the co - sparse analysis approach is a similar looking but yet very different alternative to tackle the cs problem .",
    "its underlying assumption is that a signal multiplied by an _ analysis operator _",
    "@xmath13 with @xmath14 results in a sparse vector @xmath15 .",
    "if @xmath16 denotes a function that measures sparsity , the analysis model assumption is exploited via @xmath17 the analysis model has proven useful in the field of image reconstruction and we thus restrict ourselves to compressively sensed images here .",
    "our approach is motivated by the observation that learning the operator leads to an improved image reconstruction quality @xcite , @xcite compared to applying a finite difference operator that approximates the image gradient , known as total variation ( tv - norm ) regularization @xcite , @xcite .",
    "in contrast to the task of dictionary learning only a few analysis operator learning algorithms have been proposed in the literature so far , cf .",
    "@xcite , @xcite , @xcite , @xcite .",
    "furthermore , from image denoising it is known that the reconstruction accuracy can be further improved when the dictionary or operator is not only learned on some general and representative training set , but rather directly on the specific signal that has to be reconstructed @xcite , @xcite .",
    "these observations prompted us to combine the image reconstruction performance of the analysis approach together with the accuracy improvement capabilities of a learned operator .",
    "the principle of cs relies on the fact that the signal @xmath18 has a sparse representation in a _ given _ basis or dictionary @xmath19 that is universal for the considered signal class of interest . however , such universal dictionaries do not necessarily result in the sparsest possible representation , which is crucial for the recovery success . due to this , in @xcite the concept of blind compressive sensing ( bcs ) has been introduced , which aims at simultaneously learning the dictionary and reconstructing the signal , see also @xcite and @xcite for an extension of this idea .",
    "note that all these methods are based on the synthesis model and consider the problem of finding a suitable dictionary , while in this paper we focus on the analysis model .",
    "in this work we address the problem of signal reconstruction from compressively sensed data regularized by an adaptively learned analysis operator .",
    "the work of hawe _ et al . _",
    "@xcite , which focuses on learning a global patch based analysis operator from noise free training samples , has already shown the superior performance of a learned operator compared to state - of - the - art analysis and synthesis based regularization , like e.g. k - svd denoising , in the context of classical image reconstruction problems .",
    "that is why we extend this idea and build on their work to utilize the learning process to obtain a signal dependent regularization of the inverse problem .",
    "since we are dealing with compressive measurements , our approach can be interpreted as an analysis - based bcs problem with no prior knowledge about the operator .",
    "we extend the algorithm proposed in @xcite , where the operator is learned by a geometric conjugate gradient ( cg ) method on the so - called oblique manifold , to our setting of simultaneous image reconstruction and operator learning .",
    "this approach allows us to compensate for various sampling noise models , i.e. gaussian or impulsive noise , by simply exchanging the data fidelity term .",
    "to summarize , the advantages of our approach are as follows : ( i ) the learning process allows to adaptively find an adequate operator that fits the underlying image structure .",
    "( ii ) there is no necessity to train the operator prior to the reconstruction .",
    "( iii ) different noise types are handled by simply exchanging the data fidelity term .",
    "our goal is to find a local analysis operator @xmath20 with @xmath14 simultaneously to the signal @xmath21 that has to be reconstructed from the compressive measurements . here",
    ", the vector @xmath18 denotes a vectorized image of dimension @xmath22 , with @xmath23 being the width and @xmath24 being the height of the image , respectively , obtained by stacking the columns of the image above each other .",
    "note that the analysis operator has to be applied to local image patches rather than to the whole image .",
    "we denote the binary @xmath25 matrix that extracts the patch centered at the @xmath26 pixel by @xmath27 .",
    "furthermore , practice has shown that the learning process is significantly faster if _ centered _",
    ", i.e. zero mean patches are considered .",
    "this can be easily incorporated by multiplying the vectorized patch with @xmath28 , where @xmath29 and @xmath30 are the identity operator and the matrix with all elements equal to one , respectively .",
    "we employ constant padding at the image borders , i.e. replicating the boundary pixel values . in the end",
    ", we globally promote sparsity with an appropriate function @xmath31 and write for the problem of finding a suitable analysis operator @xmath32 where @xmath33 denotes an admissible set , which implies some constraints on @xmath34 to avoid trivial solutions .",
    "we follow the considerations of the authors in @xcite , demanding that :    the rows of @xmath34 have unit euclidean norm , i.e. @xmath35 , for @xmath36 , where @xmath37 denotes the transposed of the @xmath38-row of @xmath34 .    the analysis operator @xmath34 has full rank , i.e. @xmath39",
    ".    the mutual coherence of the analysis operator should be moderate .",
    "these constraints motivate to consider the set of full rank matrices with normalized columns , which admits a manifold structure known as the oblique manifold @xmath40 here , @xmath41 is the diagonal matrix whose entries on the diagonal are those of @xmath42 .",
    "since we require the rows of @xmath34 to have unit euclidean norm , we restrict @xmath43 to be an element of @xmath44 . to enforce the rank constraint ( ii )",
    "we employ the penalty function @xmath45 furthermore , the mutual coherence of the analysis operator , formulated in constraint ( iii ) , can be controlled via the logarithmic barrier function of the atoms scalar products , namely @xmath46 considerations concerning the usefulness of these penalty functions can be found in @xcite . to measure the sparsity of the analyzed patches , we use the differentiable sparsity promoting function @xmath47 where @xmath48 is a positive constant and @xmath49 represents the @xmath50 standard basis vector with the same length as @xmath51 .",
    "since we are interested in simultaneous operator learning and image reconstruction , we further introduce a data term @xmath52 , which measures the fidelity of the reconstructed signal to the measurements @xmath53 .",
    "the choice of @xmath54 depends on the error model , i.e. by using @xmath55 the error is assumed to be gaussian distributed . if the noise is sparsely distributed over the measurements , we set @xmath56 .",
    "this error model has also been utilized in @xcite to compensate for sparse outliers in the measurements .    finally , combining the data term with the constraints and the sparsity promoting function @xmath57 , the augmented lagrangian optimization problem for adaptively learning the analysis operator with simultaneous image reconstruction consists of minimizing the cost @xmath58 subject to @xmath59 with the measurement matrix @xmath60 .",
    "the scalar @xmath61 denotes the number of extracted image patches .",
    "the parameter @xmath62 weights the fidelity of the solution to the measurements and the parameters @xmath63 control the influence of the two constraints .",
    "since the cost function is restricted to a smooth manifold , we follow @xcite and employ a conjugate gradient on manifolds approach to solve the optimization problem .",
    "the cg approach is scalable and converges fast in practice .",
    "it is thus well - suited to handle the high dimensional problem of simultaneous image reconstruction and operator learning .",
    "the challenges for developing the cg method are the efficient computation of the riemannian gradient , the step - size and the update directions . to that end",
    ", we employ the product manifold structure of @xmath64 considered as a riemannian submanifold of @xmath65 .",
    "to enhance legibility in the remainder of this section we denote the oblique manifold by ob .",
    "we further denote the tangent space at a point @xmath66 as @xmath67 , with @xmath68 being a tangent vector at @xmath69 .    the riemannian gradient at @xmath69 is given by the orthogonal projection of the standard ( euclidean ) gradient onto the tangent space @xmath67 .",
    "the orthogonal projection of a matrix @xmath70 onto the tangent space @xmath67 is obtained by @xmath71 . using the product structure and denoting the partial derivatives of @xmath72 by @xmath73 and @xmath74 , respectively , the riemannian gradient of the cost function is @xmath75    in cg methods the updated search directions @xmath76 are linear combinations of the respective gradient and the previous search directions @xmath77 .",
    "the identification of different tangent spaces is done by the so - called parallel transport @xmath78 , which transports a tangent vector @xmath79 along a _",
    "geodesic _ to the tangent space @xmath80 . in the manifold setting geodesics can be considered as the generalization of straight lines .",
    "we denote the geodesic from @xmath81 along the direction @xmath82 as @xmath83 . regarding the product manifold",
    "the new iterates are computed by @xmath84 where @xmath85 denotes the step size that leads to a sufficient decrease of the cost function .",
    "the parallel transport along the geodesics in the product manifold is then given by @xmath86 we use a hybridization of the hestenes - stiefel ( hs ) and the dai yuan ( dy ) formula as motivated in @xcite to determine the update of the search direction . with the shorthand notations @xmath87 and @xmath88 , as well as @xmath89 and @xmath90 the manifold adaptions of these formulas are @xmath91 where @xmath92 denotes the standard inner product in the respective euclidean spaces .",
    "with the hybrid update formula @xmath93 the new search directions are given by @xmath94 in our implementation we use the well - known backtracking line search which is adapted to the manifold setting until the armijo condition is met .",
    "we name our method analysis blind compressive sensing ( abcs ) and briefly summarize the whole procedure in algorithm 1 . for further details concerning cg - methods on the oblique manifold",
    "the reader is referred to @xcite , @xcite , and @xcite .",
    "* algorithm 1 * abcs    * input : * initial operator @xmath95 , noisy measurements @xmath96 , measurement matrix @xmath8 , parameters @xmath97 + * set : * @xmath98 , @xmath99 , @xmath100 , @xmath101 , @xmath102    perform backtracking line search to get step size @xmath85 update to @xmath103 , cf .",
    "compute @xmath104 compute @xmath105 , cf .",
    "compute new cg - search directions @xmath106",
    "@xmath107 @xmath108 maximum # of iterations    * output : * @xmath109 , @xmath110",
    "to measure the image reconstruction accuracy we use the peak signal - to - noise ratio @xmath111 and the mean structural similarity index ( _ mssim _ ) , with the same set of parameters as originally suggested and implemented in @xcite . throughout our experiments we use a patch size of @xmath112 ,",
    "i.e. @xmath113 and set @xmath114 , as larger values of @xmath115 do not enhance the reconstruction quality .",
    "we initialized @xmath95 to be a random matrix and normalized the rows to unit norm . with this initialization , convergence to a local minimum",
    "was observed in all our experiments .",
    "the parameters for the constraints are set to @xmath116 and @xmath117 .",
    "the constant @xmath48 in the sparsity inducing function is chosen as @xmath118 .",
    "the parameter @xmath119 takes into account the size of the image as well as the operator size and reads @xmath120 , with @xmath121 adjusted according to the noise level as explained below and a normalization factor @xmath122 .",
    "we evaluate our method on the three images _ girl _ ( @xmath123 ) , _ barbara _ ( @xmath124 ) , and _ _ texture _ _ ) ] ( @xmath123 ) .",
    "the measurements are obtained by using the real valued noiselet transformation proposed in @xcite .    in the first experiment",
    "we show the robustness of the abcs algorithm to sampling noise which follows a gaussian distribution . for this purpose ,",
    "the measurements have been artificially corrupted by additive white gaussian noise with standard deviation @xmath125 .",
    "the data term in reads @xmath126 .",
    "we assume the noise level @xmath125 to be known and set @xmath127 . two measurement rates @xmath128 and @xmath129 are considered .",
    "table [ tab : denoising ] shows the reconstruction performance for different noise levels . for comparison we used the algorithm of @xcite ( nesta ) , with tv - norm regularization and optimized parameters .",
    "figure [ fig : res ] shows the reconstructed images from @xmath128 measurements and a noise level of @xmath130 .",
    "we also tested the algorithm proposed in @xcite ( tval3 ) with different parameters , which achieves results comparable to nesta . due to space limitations , detailed results are not listed here . in all settings ,",
    "the same measurements are used .",
    ".image reconstruction from measurements corrupted by additive white gaussian noise with standard deviation @xmath125 .",
    "the measurement rates are @xmath128 ( top ) and @xmath129 ( bottom ) .",
    "achieved psnr in decibels and mssim .",
    "[ cols=\"^,^,^,^,^,^,^,^ \" , ]     both experiments confirm that the adaptively learned operator leads to an accuracy improvement compared to the reconstruction quality obtained with a fixed finite difference operator . in particular , the structures in the _ barbara _ and _ texture _ image are better preserved by abcs .",
    "in this article we proposed an analysis based blind compressive sensing algorithm that simultaneously reconstructs an image from compressively sensed data and learns an appropriate analysis operator .",
    "this process is formulated as an optimization problem , which is tackled via a geometric conjugate gradient approach that updates both the operator and the image as a whole at each iteration .",
    "furthermore , the algorithm can be easily adapted to different noise models by simply exchanging the data fidelity term ."
  ],
  "abstract_text": [
    "<S> in this work we address the problem of blindly reconstructing compressively sensed signals by exploiting the co - sparse analysis model . in the analysis model </S>",
    "<S> it is assumed that a signal multiplied by an analysis operator results in a sparse vector . </S>",
    "<S> we propose an algorithm that learns the operator adaptively during the reconstruction process . </S>",
    "<S> the arising optimization problem is tackled via a geometric conjugate gradient approach . </S>",
    "<S> different types of sampling noise are handled by simply exchanging the data fidelity term . </S>",
    "<S> numerical experiments are performed for measurements corrupted with gaussian as well as impulsive noise to show the effectiveness of our method . </S>"
  ]
}