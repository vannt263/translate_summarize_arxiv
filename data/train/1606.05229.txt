{
  "article_text": [
    "a fundamental challenge of computational neuroscience is to understand how information about the external world is processed and represented in the brain .",
    "each individual neuron aggregates the incoming information into a single sequence of spikes  an output which is too simplistic by itself to capture the full complexity of sensory input . only by combining the signals from massive ensembles of neurons",
    "is it possible to reconstruct our complex representation of the world .",
    "nevertheless , neurons form hierarchies of specialization within neural circuits , which are further organized in various specialized regions of the brain . at the lowest level of the hierarchy  individual neurons , it is possible to infer and interpret the functional relationship between a neuron and stimulus features of interest using single - cell recording technologies . due to the inherent stochasticity of the neural output ,",
    "it is natural to view the neuron as a noisy channel , and use mutual information to quantify how much of the stimulus information is encoded by the neuron . moving up the hierarchy to the the macroscale level of organization in the brain",
    "requires both different experimental methodologies and new approaches for summarizing and inferring measures of information in the brain .",
    "shannon s mutual information @xmath1 is fundamentally a measure of dependence between random variables @xmath2 and @xmath3 , and is defined as @xmath4 various properties of @xmath1 make it ideal for quantifying the information between a random stimulus @xmath2 and the signaling behavior of an ensembles of neurons , @xmath3 [ 1 ] .",
    "a leading metaphor is that of a noisy communications channel ; the mutual information describes the rate at which @xmath3 can communicate bits from @xmath2 .",
    "this framework is well - suited for summarizing the properties of a single neuron coding external stimulus information ; indeed , experiments studying the properties of a single or a small number of neurons often make use of the concept of mutual information in summarizing or interpreting their results [ 2 ] .",
    "see discussions in [ 3 ] .",
    "however , estimating mutual information for multiple channels requires large and over - parameterized generative models .",
    "machine learning algorithms showed a way forward : a seminal work by haxby [ 4 ] proposed to quantify the information in multiple channels by measuring how well the stimulus can be identified from the brain responses , in what is known as `` multivariate pattern analysis '' ( mvpa ) . to demonstrate that a particular brain region responds to a certain type of sensory information ,",
    "one employs supervised learning to build a classifier that classifies the stimulus class from the brain activation in that region .",
    "classifiers that achieve above - chance classification accuracy indicate that information from the stimulus is represented in the brain region . in principle",
    ", one could just as well test the statistical hypothesis that the fisher information or mutual information between the stimulus and the activation patterns is nonzero .",
    "but in practice , the machine learning approach enjoys several advantages : first , it is invariant to the parametric representation of the stimulus space , and is opportunistic in the parameterization of the response space .",
    "this is an important quality for naturalistic stimulus - spaces , such as faces or natural images .",
    "second , it scales better with the dimensionality of both the stimulus space and the responses space , because a slimmer discriminative model can be used rather than a fully generative model .    nevertheless , classification error is problematic for quantifying the strength of the relation between stimulus and outputs due to its arbitrary scale and strong dependence on experimental choices .",
    "classification accuracy depends on the particular choice of stimuli exemplars employed in the study and the number of partitions ( @xmath0 ) used to define the classes for the classification task .",
    "the difficulty of the classification task depends on the number of classes defined : high classification accuracy can be achieved relatively easily by using a coarse partition of stimuli exemplars into classes .",
    "often @xmath0 is an arbitrary design constraint , and researchers try to extrapolate the error for alternative number of classes [ 5 ] . in a meta - analysis on visual decoding , coutanche et al ( 2016 ) [ 6 ] quantified the strength of a classification study using the formula @xmath5 such an approach may compensate for the differences in accuracy due purely to choice of number of classes defined ; however , no theory is provided to justify the formula .",
    "in contrast , mutual information has ideal properties for quantitatively comparing information between different studies , or between different brain regions , subjects , feature - spaces , or modalities .",
    "not only is the mutual information defined independently of the arbitrary definition of stimulus classes ( albeit still dependent on an implied distribution over stimuli ) , it is even meaningful to discuss the difference between the mutual information measured for one system and the mutual information for a second system .",
    "hence , a popular approach which combines the strengths of the machine learning approach and the advantages of the information theoretic approach is to obtain a lower bound on the mutual information by using the confusion matrix of a classifier .",
    "treves [ 7 ] first proposed using the empirical mutual information of the classification matrix in order to obtain a lower bound of the mutual information @xmath1 ; this confusion - matrix - based lower bound has subsequently enjoyed widespread use in the mvpa literature [ 2 ] . even earlier that",
    "this , the idea of linking classification performance to mutual information can be found in the beginnings of information theory .",
    "fano s inequality provides a lower bound on mutual information in relation to the optimal prediction error , or bayes error . in practice ,",
    "the bound obtained may be a vast underestimate [ 8 ] .      in this paper",
    ", we propose a new way to link classification performance to the implied mutual information .",
    "to create this link we need to overcome the arbitrary choice of exemplars , and the arbitrary number of classes k. towards this end , we define a notion of @xmath0-class _ average bayes error _ which is uniquely defined for any given stimulus distribution and stochastic mapping from stimulus to response . the @xmath0-class average bayes error is the expectation of the bayes error ( the classification error of the optimal classifier ) when @xmath0 stimuli exemplars are drawn i.i.d . from the stimulus distribution , and treated as distinct classes .",
    "hence the average bayes error can in principle be estimated if the appropriate randomization is employed for designing the experiment .",
    "specifically , we establish a relationship between the mutual information @xmath1 and the average @xmath0-class bayes error , @xmath6 . in short",
    ", we will identify a function @xmath7 ( which depends on @xmath0 ) , @xmath8 and that this approximation becomes accurate under a limit where @xmath9 is small relative to the dimensionality of @xmath2 , and under the condition that the components of @xmath2 are approximately independent .",
    "the function @xmath7 is given by @xmath10 this formula is not new to the information theory literature : it appears as the error rate of an orthogonal constellation [ 9 ] .",
    "what is surprising is that the same formula can be used to approximate the error rate in much more general class of classification problems  this is precisely the universality result which provides the basis for our proposed estimator .",
    "figure [ fig : pi ] displays the plot of @xmath7 for several values of @xmath0 . for all values of @xmath0 , @xmath11 is monotonically decreasing in @xmath12 , and tends to zero as @xmath13 , which is what we expect since if @xmath1 is large , then the average bayes error should be small .",
    "another intuitive fact is that @xmath14 since after all , an uninformative response can not lead to above - chance classification accuracy .",
    "[ cols=\"^,^ , > , < \" , ]     the estimator @xmath15 is based on fano s inequality , which reads @xmath16 where @xmath17 is the entropy of a bernoulli random variable with probability @xmath18 .",
    "replacing @xmath19 with @xmath20 and replacing @xmath21 with @xmath22 , we get the estimator @xmath23 meanwhile , the confusion matrix estimator computes @xmath24 which is the empirical mutual information of the discrete joint distribution @xmath25 .",
    "it is known that @xmath26 , @xmath27 tend to underestimate the mutual information .",
    "quiroga et al .",
    "[ 2 ] discussed two sources of ` information loss ' which lead to @xmath26 underestimating the mutual information : the discretization of the classes , and the error in approximating the bayes rule",
    ". meanwhile , gastpar et al . [",
    "11 ] showed that @xmath27 is biased downwards due to undersampling of the exemplars : to counteract this bias , they introduce the anthropic correction estimator @xmath28 $ ] , @xmath28 could still vastly underestimate or overestimate the mutual information . ] .",
    "in addition to the sources of information loss discussed by quiroga et al . , an additional reason why @xmath26 and @xmath15 underestimate the mutual information is that they are upper bounded by @xmath29 , where @xmath0 is the number of classes . as @xmath1 exceeds @xmath29 , the estimate @xmath30 can no longer approximate @xmath1 , even up to a constant factor .",
    "in contrast , @xmath31 is unbounded and may either underestimate or overestimate the mutual information in general , but performs well when the high - dimensionality assumption is met .    in figure 2",
    "we show the sampling distributions of the five estimators as @xmath1 is varied in the interval @xmath32 $ ] .",
    "the estimator @xmath33 is a plug - in estimator using @xmath34 , the coefficient matrix estimated via multinomial regression of @xmath3 on @xmath2 ; it recovers the true mutual information within @xmath35 with a probability of 90% .",
    "we see that @xmath26 , @xmath15 , and @xmath27 indeed begin to asymptote as they approach @xmath36 .",
    "in contrast , @xmath31 remains a good approximation of @xmath1 within the range , although it begins to overestimate at the right endpoint .",
    "the reason why @xmath31 loses accuracy as the true information @xmath1 increases is that the multivariate normality approximation used to derive the estimator becomes less accurate when the conditional distribution @xmath37 becomes highly concentrated .",
    "discriminative estimators of mutual information have the potential to estimate mutual information in high - dimensional data without resorting to fully parametric assumptions .",
    "however , a number of practical considerations also limit their usage .",
    "first , one has to find a good classifier @xmath38 for the data : techniques for model selection can be used to choose @xmath38 from a large library of methods .",
    "however , there is no way to guarantee how well the chosen classifier approximates the optimal classification rule .",
    "secondly , one has to estimate the generalization error from test data : the complexity of estimating @xmath39 could become the bottleneck when @xmath39 is close to 0 .",
    "thirdly , for previous estimators @xmath15 and @xmath26 , the ability of the estimator to distinguish high values of @xmath1 is limited by the number of classes @xmath0 .",
    "our estimator @xmath31 is subject to the first two limitations , along with any conceivable discriminative estimator , but overcomes the third limitation under the assumption of stratified sampling and high dimensionality .",
    "it can be seen that additional assumptions are indeed needed to overcome the third limitation , the @xmath29 upper bound .",
    "consider the following worst - case example : let @xmath2 and @xmath3 have joint density @xmath40 on the unit square . under partition - based classification ,",
    "if we set @xmath41 , then no errors are made under the bayes rule .",
    "we therefore have a joint distribution which maximizes any reasonable discriminative estimator but has _",
    "information @xmath42 .",
    "the consequence of this is that under partition - based classification , we can not hope to distinguish distributions with @xmath43 .",
    "the situation is more promising if we specialize to stratified sampling : in the same example , a bayes of zero is no longer likely due to the possibility of exemplars being sampled from the same bin ( ` collisions')we obtain an approximation to the average bayes error through a poisson sampling model : @xmath44 . by specializing further to the high - dimensional regime , we obtain even tighter control on the relation between bayes error and mutual information .",
    "our estimator therefore provides more accurate estimation at the cost of more additional assumptions , but just how restrictive are these assumptions ?",
    "the assumption of stratified sampling is usually not met in the most common applications of classification where the classes are defined _ a priori_. for instance , if the classes consist of three different species of iris , it does not seem appropriate to model the three species as i.i.d",
    ". draws from some distribution on a space of infinitely many potential iris species . yet , when the classes have been pre - defined in an arbitrary manner , the mutual information between a latent class - defining variable @xmath2 and @xmath3 may be only weakly related to the classification accuracy .",
    "we rely on the stratified sampling assumption to obtain the necessary control on how the classes in the classification task are defined .",
    "fortunately , in many applications where one is interested in estimating @xmath1 , a stratified sampling design can be practically implemented .",
    "the assumption of high dimensionality is not easy to check : having a high - dimension response @xmath3 does not suffice , since even then @xmath3 could still lie close to a low - dimensional manifold . in such cases ,",
    "@xmath31 could either overestimate or underestimate the mutual information . in situations",
    "where @xmath45 lie on a manifold , one could effectively estimate mutual information by would be to combining dimensionality reduction with nonparametric information estimation [ 13 ] .",
    "we suggest the following diagnostic to determine if our method is appropriate : subsample within the classes collected and check that @xmath31 does not systematically increase or decrease with the number of classes @xmath0 .",
    "the assumption of approximating the bayes rule is impractical to check , as any nonparametric estimate of the bayes error requires exponentially many observations .",
    "hence , while the present paper studies the ` best - case ' scenario where the model is well - specified , it is even more important to understand the robustness of our method in the more realistic case where the model is misspecified .",
    "we leave this question to future work .    even given a classifier which consistently estimates the bayes error , the estimator @xmath31",
    "can still be improved .",
    "one can employ more sophisticated methods to estimate @xmath6 : for example , extrapolating from learning curves [ 14 ] .",
    "furthermore , depending on the risk function , one may debias or shrink the estimate @xmath31 to achieve a more favorable bias - variance tradeoff .",
    "all of the necessary assumptions are met in our simulation experiment , hence our proposed estimator is seen to dramatically outperform existing estimators .",
    "it remains to assess the utility of our estimation procedure in a real - world example , where both the high - dimensional assumption and the model specification assumption are likely to be violated . in a forthcoming work",
    ", we apply our framework to evaluate visual encoding models in human fmri data .",
    "we thank john duchi , youngsuk park , qingyun sun , jonathan taylor , trevor hastie , robert tibshirani for useful discussion .",
    "cz is supported by an nsf graduate research fellowship .",
    "[ 1 ] borst , a. & theunissen , f. e. ( 1999 ) .",
    "`` information theory and neural coding '' _ nature neurosci . _ , vol .",
    "947 - 957 .",
    "[ 2 ] quiroga , r. q. , & panzeri , s. ( 2009 ) . `` extracting information from neuronal populations : information theory and decoding approaches '' .",
    "_ nature reviews neuroscience _ , 10(3 ) , 173 - 185 .",
    "[ 3 ] paninski l. , `` estimation of entropy and mutual information , '' _ neural comput . _ ,",
    "15 , no . 6 , pp . 1191 - 1253 , 2003 .",
    "[ 4 ] haxby , j.v . , et al .",
    "`` distributed and overlapping representations of faces and objects in ventral temporal cortex . ''",
    "_ science _ 293.5539 : 2425 - 2430 .",
    "[ 5 ] kay , k. n. , et al . `` identifying natural images from human brain activity . '' _ nature _ 452.7185 ( 2008 ) : 352 - 355 .",
    "[ 6 ] coutanche , m.n . ,",
    "solomon , s.h . , and thompson - schill s. l. , `` a meta - analysis of fmri decoding : quantifying influences on human visual population codes . '' _ neuropsychologia _ 82 ( 2016 ) : 134 - 141 .    [ 7 ]",
    "treves , a. ( 1997 ) .",
    "`` on the perceptual structure of face space . '' _ bio systems _ , 40(1 - 2 ) , 189?96 .",
    "[ 8 ] beirlant , j. , dudewicz , e. j. , gyrfi , l. , & der meulen , e. c. ( 1997 ) .",
    "`` nonparametric entropy estimation : an overview . '' _ international journal of mathematical and statistical sciences _ , 6 , 17 - 40 .",
    "[ 9 ] tse , d. , & viswanath , p. ( 2005 ) .",
    "_ fundamentals of wireless communication . _ cambridge university press ,    [ 10 ] friedman , j. , hastie , t. , & tibshirani , r. ( 2008 ) . _ the elements of statistical learning .",
    "springer , berlin : springer series in statistics .",
    "[ 11 ] gastpar , m. gill , p. huth , a. & theunissen , f. ( 2010 ) .",
    "`` anthropic correction of information estimates and its application to neural coding . ''",
    "_ ieee trans .",
    "info . theory _ ,",
    "vol 56 no 2 .",
    "[ 12 ] banerjee , a. , dean , h. l. , & pesaran , b. ( 2011 ) .",
    "`` parametric models to relate spike train and lfp dynamics with neural information processing . '' _ frontiers in computational neuroscience _ 6 : 51 - 51 .",
    "[ 13 ] theunissen , f. e. & miller , j.p .",
    "`` representation of sensory information in the cricket cercal sensory system .",
    "information theoretic calculation of system accuracy and optimal tuning - curve widths of four primary interneurons , ''",
    "_ j. neurophysiol .",
    "1690 - 1703 .",
    "[ 14 ] cortes , c. , et al .",
    "`` learning curves : asymptotic values and rate of convergence . ''",
    "( 1994 ) . _",
    "advances in neural information processing systems .",
    "* lemma 1 . * _ suppose @xmath46 are jointly multivariate normal , with @xmath47= \\alpha$ ] , @xmath48 , @xmath49 , @xmath50 , and @xmath51 for all @xmath52 , such that @xmath53 . then , letting @xmath54}{\\sqrt{\\frac{1}{2}{\\text{var}}(z_i - z_j ) } } = \\frac{\\alpha}{\\sqrt{\\delta - \\epsilon}},\\ ] ] @xmath55 we have @xmath56 & = \\pr[w < m_{k-1 } ] \\\\&= 1 - \\int \\frac{1}{\\sqrt{2\\pi\\nu^2 } } e^{-\\frac{(w-\\mu)^2}{2\\nu^2 } } \\phi(w)^{k-1 } dw,\\end{aligned}\\ ] ] where @xmath57 and @xmath58 is the maximum of @xmath59 independent standard normal variates , which are independent of @xmath60 . _    * proof .",
    "* we can construct independent normal variates @xmath61 , @xmath62 such that @xmath63 @xmath64 such that @xmath65 hence @xmath56 & = \\pr[\\min_{i > 1 } z_1 - z_i < 0 ] .",
    "\\\\&= \\pr[\\min_{i=2}^{k } g_1 + g_i + \\alpha < 0 ] \\\\&= \\pr[\\min_{i=2}^{k } g_i < -\\alpha - g_1 ] \\\\&= \\pr[\\min_{i=2}^{k } \\frac{g_i}{\\sqrt{\\delta - \\epsilon } } < -\\frac{\\alpha - g_1}{\\sqrt{\\delta - \\epsilon}}].\\end{aligned}\\ ] ] since @xmath66 are iid standard normal variates , and since @xmath67 for @xmath12 and @xmath68 given in the statement of the lemma , the proof is completed via a straightforward computation . @xmath69    * theorem 1 . *",
    "let @xmath70}(x , y)$ ] be a sequence of joint densities for @xmath71 as given above .",
    "further assume that    * @xmath72 } ; y^{[d ] } ) = \\iota < \\infty.$ ] * there exists a sequence of scaling constants @xmath73}$ ] and @xmath74}$ ] such that the random vector @xmath75 } + b_{ij}^{[d]})_{i , j = 1,\\hdots , k}$ ] converges in distribution to a multivariate normal distribution .",
    "* there exists a sequence of scaling constants @xmath76}$ ] , @xmath77}$ ] such that @xmath78}u(x^{(1 ) } , y^{(2 ) } ) + b^{[d]}\\ ] ] converges in distribution to a univariate normal distribution . * for all @xmath79 , @xmath80 = 0.\\ ] ]    then for @xmath6 as defined above , we have @xmath81 where @xmath82 where @xmath83 and @xmath84 are the standard normal density function and cumulative distribution function , respectively",
    ".    * proof . *    for @xmath85 ,",
    "define @xmath86 then , we claim that @xmath87 converges in distribution to @xmath88 combining the claim with the lemma ( stated below this proof ) yields the desired result .",
    "to prove the claim , it suffices to derive the limiting moments @xmath89 \\to -2\\iota,\\ ] ] @xmath90 \\to 4\\iota,\\ ] ] @xmath91 \\to 2\\iota,\\ ] ] for @xmath92 , since then assumption a2 implies the existence of a multivariate normal limiting distribution with the given moments .    before deriving the limiting moments ,",
    "note the following identities .",
    "let @xmath93 and @xmath94 .",
    "@xmath95 = \\int p(x ) p(y ) e^{u(x , y ) } dx dy = \\int p(x , y ) dx dy = 1.\\ ] ] therefore , from assumption a3 and the formula for gaussian exponential moments , we have @xmath96-\\frac{1}{2}{\\text{var}}[u(x ' , y ) ] = 0.\\ ] ] let @xmath97 $ ] .",
    "meanwhile , by applying assumption a2 , @xmath98 \\\\&= \\int_{\\mathbb{r } } e^z z \\frac{1}{\\sqrt{2\\pi \\sigma^2 } }   e^{-\\frac{(z + \\sigma^2/2)^2}{2\\sigma^2 } } \\text { ( applying a2 ) } \\\\&= \\int_{\\mathbb{r } } z \\frac{1}{\\sqrt{2\\pi \\sigma^2 } }   e^{-\\frac{(z - \\sigma^2/2)^2}{2\\sigma^2 } } \\\\&= \\frac{1}{2}\\sigma^2.\\end{aligned}\\ ] ] therefore , @xmath99 and @xmath96 = -\\iota.\\ ] ] once again by applying a2 , we get @xmath100   & = \\lim_{d \\to \\infty } \\int ( u(x , y ) - \\iota)^2 p(x , y ) dx dy \\\\&= \\lim_{d \\to \\infty } \\int ( u(x , y ) - \\iota)^2 e^{u(x , y ) } p(x ) p(y ) dx dy",
    "\\\\&= \\lim_{d \\to \\infty } { \\textbf{e}}[(u(x ' , y ) - \\iota)^2 e^{u(x ' , y ) } ]   \\\\&= \\int ( z - \\iota)^2 e^z \\frac{1}{\\sqrt{4\\pi\\iota } } e^{-\\frac{(z+\\iota)^2}{4\\iota } } dz \\text { ( applying a2 ) } \\\\&= \\int ( z - \\iota)^2 \\frac{1}{\\sqrt{4\\pi\\iota } } e^{-\\frac{(z-\\iota)^2}{4\\iota } } dz \\\\&= 2\\iota.\\end{aligned}\\ ] ]    we now proceed to derive the limiting moments .",
    "we have @xmath101   & = \\lim_{d \\to \\infty } { \\textbf{e } } [ \\log p(y|x ' ) - \\log p(y|x ) ] \\\\&=",
    "\\lim_{d \\to \\infty } { \\textbf{e } } [ u(x ' , y ) - u(x , y ) ] = -2\\iota.\\end{aligned}\\ ] ] also , @xmath102   & = \\lim_{d \\to \\infty } { \\text{var } } [ u(x ' , y ) - u(x , y ) ] \\\\&= \\lim_{d \\to \\infty } { \\text{var } } [ u(x ' , y ) ] + { \\text{var } } [ u(x , y ) ] \\text { ( using assumption a4 ) } \\\\&= 4\\iota,\\end{aligned}\\ ] ] and similarly @xmath103 & = \\lim_{d \\to \\infty } { \\text{var } } [ u(x , y)]\\text { ( using assumption a4 ) } \\\\&= 2\\iota.\\end{aligned}\\ ] ] this concludes the proof . @xmath69 .",
    "assumptions a1-a4 are satisfied in a variety of natural models .",
    "one example is a multivariate gaussian model where @xmath104 @xmath105 @xmath106 where @xmath107 and @xmath108 are @xmath109 covariance matrices , and where @xmath2 and @xmath110 are independent .",
    "then , if @xmath111 and @xmath108 have limiting spectra @xmath112 and @xmath113 respectively , the joint densities @xmath114 for @xmath115 satisfy assumptions a1 - a4 .",
    "we can also construct a family of densities satisfying a1 - a4 , which we call an _ exponential family sequence model _ since each joint distribution in the sequence is a member of an exponential family . a given exponential family sequence model is specified by choice of a base carrier function @xmath116 and base sufficient statistic @xmath117 , with the property that carrier function factorizes as @xmath118 for marginal densities @xmath119 and @xmath120 .",
    "note that the dimensions of @xmath121 and @xmath122 in the base carrier function are arbitrary ; let @xmath123 denote the dimension of @xmath121 and @xmath124 the dimension of @xmath122 for the base carrier function .",
    "next , one specifies a sequence of scalar parameters @xmath125 such that @xmath126 for some constant @xmath127 .",
    "for the @xmath128th element of the sequence , @xmath129}$ ] is a @xmath130-dimensional vector , which can be partitioned into blocks @xmath131 } = ( x_1^{[d]},\\hdots , x_d^{[d]})\\ ] ] where each @xmath132}$ ] is @xmath123-dimensional .",
    "similarly , @xmath133}$ ] is partitioned into @xmath134}$ ] for @xmath135 .",
    "the density of @xmath136 } , y^{[d]})$ ] is given by @xmath137}(x^{[d ] } , y^{[d ] } ) = z_d^{-1 } \\left(\\prod_{i=1}^d b(x_i^{[d ] } , y_i^{[d ] } ) \\right )   \\exp\\left[\\kappa_d \\sum_{i=1}^d t(x_i^{[d ] } , y_i^{[d ] } ) \\right],\\ ] ] where @xmath138 is a normalizing constant .",
    "hence @xmath70}$ ] can be recognized as the member of an exponential family with carrier measure @xmath139 } , y_i^{[d ] } ) \\right)\\ ] ] and sufficient statistic @xmath140 } , y_i^{[d]}).\\ ] ]    one example of such an exponential family sequence model is a multivariate gaussian model with limiting spectra @xmath141 and @xmath142 , but scaled so that the marginal variance of the components of @xmath2 and @xmath3 are equal to one .",
    "this corresponds to a exponential family sequence model with @xmath143 and @xmath144    another example is a multivariate logistic regression model , given by @xmath145 @xmath146 this corresponds to an exponential family sequence model with @xmath147 @xmath148 and @xmath149 the multivariate logistic regression model ( and multivariate poisson regression model ) are especially suitable for modeling neural spike count data ; we simulate data from such a multivariate logistic regression model in section x.",
    "multiple - response logistic regression model @xmath150 @xmath151 @xmath152 where @xmath153 is a @xmath154 matrix ."
  ],
  "abstract_text": [
    "<S> multivariate pattern analyses approaches in neuroimaging are fundamentally concerned with investigating the quantity and type of information processed by various regions of the human brain ; typically , estimates of classification accuracy are used to quantify information . </S>",
    "<S> while a extensive and powerful library of methods can be applied to train and assess classifiers , it is not always clear how to use the resulting measures of classification performance to draw scientific conclusions : e.g. for the purpose of evaluating redundancy between brain regions . </S>",
    "<S> an additional confound for interpreting classification performance is the dependence of the error rate on the number and choice of distinct classes obtained for the classification task . </S>",
    "<S> in contrast , mutual information is a quantity defined independently of the experimental design , and has ideal properties for comparative analyses . </S>",
    "<S> unfortunately , estimating the mutual information based on observations becomes statistically infeasible in high dimensions without some kind of assumption or prior .    in this paper </S>",
    "<S> , we construct a novel classification - based estimator of mutual information based on high - dimensional asymptotics . </S>",
    "<S> we show that in a particular limiting regime , the mutual information is an invertible function of the expected @xmath0-class bayes error . </S>",
    "<S> while the theory is based on a large - sample , high - dimensional limit , we demonstrate through simulations that our proposed estimator has superior performance to the alternatives in problems of moderate dimensionality . </S>"
  ]
}