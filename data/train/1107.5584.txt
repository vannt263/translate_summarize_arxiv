{
  "article_text": [
    "classifying interesting objects in a dataset is an essential early step in most analysis tasks . for many objects in astronomy ( stars , galaxies , solar system objects , extragalactic supernovae )",
    ", algorithms can identify and characterize these objects automatically @xcite .",
    "structures in the interstellar medium ( ism ) , however , have proven difficult to classify in this way . these objects  which include , e.g. , molecular clouds , infrared dark clouds , bubbles , jets , radiation - shaped pillars , and filaments  are morphologically complex and heterogeneous . the essential properties of these structures are hard to encode .    to take advantage of increasingly wide - area surveys ,",
    "previous researchers have mainly relied on manual identification of features in the ism @xcite .",
    "there are several drawbacks to this approach : it is time consuming , non - repeatable and , when identifying complex structures , affected by difficult - to - quantify selection effects related to how specific people perceive an image . despite recent advances in the study of specific objects ( e.g. infrared dark clouds , @xcite ; filaments , @xcite ) , automated feature identification in the ism",
    "is an still an open problem .",
    "machine learning algorithms are designed to infer patterns in data which are otherwise difficult to define explicitly .",
    "they are grouped into two classes : supervised ( in which the algorithm is `` trained '' to recognize a pattern via a set of training examples ) and unsupervised ( in which the algorithm identifies groupings within a dataset _ a priori _ )",
    ".    these algorithms can mechanize the process of object identification , and hence address many of the shortcomings of manual classification  they scale easily to other similar data , and their results are repeatable .",
    "a potential drawback of these methods is that , because the classification is not guided by a physical model , they are susceptible to over - fitting and to inheriting selection biases within the training data .",
    "however , because the machine approach extends easily to other similar data , these biases are more readily characterized via classification of test data sets .    in this paper",
    ", we explore whether machine learning algorithms can be used to catalog structures in the ism .",
    "we present as a case study a spectral line data cube of @xmath0co emission towards the m17 star forming region .",
    "emission from this cloud overlaps with g16.05 - 0.57 , a supernova remnant situated behind the cloud .",
    "we use the support vector machine ( svm , @xcite ) algorithm to identify the supernova remnant and , on a pixel - by - pixel basis , classify the original data cube .",
    "we are able to use this classification to derive the mass and momentum of the supernova remnant , which would not otherwise be possible with these data .",
    "g16.05 - 0.57 is a supernova remnant in the inner galaxy first discovered via its cm synchrotron emission by brogan et al .",
    "( @xcite ) . because it is located behind m17",
    ", we serendipitously observed the remnant with the james clerk maxwell telescope ( jcmt ) during an imaging campaign of the latter region .",
    "our observations of g16.05 - 0.57 were taken on the nights of 2009 june 23 - 25 , using the harp heterodyne receiver array @xcite .",
    "the observations target the @xmath0co j=32 line , which traces moderately excited ( @xmath2k ) and dense ( @xmath3 ) gas at a resolution of @xmath4 .",
    "the data were acquired via position - switched raster scans , using a reference position of ( @xmath5)=(@xmath6 ) .",
    "basket - weaving was used to reduce striping artifacts in the final map ( see , e.g. , section 2.1 of @xcite ) .",
    "weather conditions were grade 2 - 3 ( 225 ghz opacity @xmath7 = 0.060.10 ) . to convert from antenna temperature @xmath8 to main beam temperature ( @xmath9 ) we assumed an aperture efficiency of @xmath10 @xcite    g16.05 - 0.57 is visible as a filamentary arc subtending 20 arcminutes on the sky ( figure [ fig : brogan ] ) . to our knowledge , these are the first molecular line observations of this object",
    ". features belonging to the supernova have typical linewidths of 15 km / s fwhm , and are centered on a wide range of velocities from -5  90 km / s .",
    "it is impossible to derive a kinematic distance from such broad and scattered lines , but supernova features are absorbed by overlapping cloud emission at 40 km / s .",
    "this corresponds to a near kinematic distance of 3.4 kpc , coincident with the scutum galactic arm .",
    "the fact that g16.05 - 0.57 emits co line emission suggests that the remnant is in the process of colliding with another molecular cloud , and we are observing the shocked interface of this collision @xcite .",
    "the morphology of structures in this datacube make for an appropriate case study of automated feature classification in molecular line datasets .",
    "a representative position - velocity slice through the data cube ( figure [ fig : slice ] ) shows that the remnant overlaps molecular cloud emission from m17 and the scutum arm .",
    "however , the supernova has markedly different structure ; features from the remnant are spatially compact and extend over tens of km / s in velocity space , while the other structures it overlaps with are more spatially diffuse and kinematically narrow .",
    "this suggests that a learning algorithm may be able to distinguish emission from g16.05 - 0.57 based on its unique morphology .",
    "the support vector machine algorithm is a supervised learning algorithm which attempts to segregate data points into two categories , based on a representative sample of data belonging to each category .",
    "the method has recently been applied to many diverse problems in astronomy , including redshift estimation @xcite , galaxy morphology identification @xcite , and time series analysis @xcite . here",
    "we provide a basic overview of the algorithm , but refer the reader to press et al .",
    "( @xcite ) for a deeper and more precise derivation , and to vapnik ( @xcite ) for a discussion of the algorithm s foundation in statistical learning theory . in",
    "what follows , we use the svm@xmath11 implementation by joachims ( @xcite ) .",
    "while this code is written in c , we have written a set of wrappers to use these tools within idl .    during training ,",
    "the svm algorithm takes as input a feature vector for each training example  a set of @xmath12 quantities that describe the discriminating properties of that object .",
    "these numbers can be thought of as coordinates for a vector in @xmath12-dimensional feature space .",
    "using a training set of pre - classified feature vectors , the algorithm searches for a decision boundary in feature space that optimally separates examples from each category .",
    "new data points are then assigned a classification based on the side of the decision boundary on which they fall .",
    "more specifically , svm seeks a decision boundary @xmath13 that maximizes a fitness function @xmath14 given by @xmath15    here @xmath16 is the margin of the boundary ; svm attempts to separate training data with a large gap between each data point and the boundary , and @xmath16 defines the size of this gap .",
    "@xmath17 is the degree to which training example @xmath18 violates this criterion .",
    "if example @xmath18 is separated by more than @xmath16 from @xmath13 ( and on the correct side ) , @xmath19 . otherwise , @xmath20 is the distance that @xmath18 would have to be moved to satisfy this condition .",
    "the adjustable cost parameter @xmath21 sets a tradeoff between large margins @xmath16 and poor classifications @xmath20 .    for a better understanding of equation [ eq : svm ] , consider first the case where @xmath21 is very large , the feature space is 2-dimensional , and the decision boundary is restricted to a line ( figure [ fig : svm ] , where the different symbols denote training examples from two different classes ) . in this scenario ,",
    "the algorithm first optimizes equation [ eq : svm ] over @xmath16 for a fixed boundary .",
    "since @xmath21 is very large , even a single training example on the wrong side of the margin will heavily penalize equation [ eq : svm ] .",
    "thus , the optimal @xmath16 will be the margin that just touches the training example closest to @xmath13 ( i.e. , the largest m that satisfies @xmath22 ) .",
    "the algorithm repeats this optimization over all boundaries , finding the plane that can accommodate the largest margin .",
    "the final classification is illustrated in figure [ fig : svm ] ; the background shading shows how the algorithm classifies feature space .",
    "the dotted line traces the margin on either side of the boundary .",
    "next , consider the impact of reducing @xmath21 .",
    "individual @xmath20 terms now penalize equation [ eq : svm ] less heavily .",
    "the optimal boundary in this scenario may be one which misclassifies a small number of outliers , but can afford to partition the remaining data with a larger margin @xmath16 .",
    "this is depicted in figure [ fig : svm_c ] .",
    "in addition to @xmath21 , a second set of adjustable parameters characterize a `` kernel function '' .",
    "the kernel function determines the topology of the decision surface . in the simplest case ,",
    "decision boundaries are hyper - planes in feature space . in this paper",
    ", we use the radial basis kernel function ( rbf ) , a popular and effective kernel that allows for non - planar decision boundaries .",
    "the rbf kernel has one adjustable parameter , @xmath23 , that controls the curvature of the decision surface .",
    "for example , figure [ fig : svm_gamma ] shows an svm classification of a data set with three different values of @xmath23 .",
    "low values lead to stiff boundaries , while high values lead to curved boundaries that may over - fit to the training data . in this application , over - fitting refers to the case when the decision boundary conforms too tightly to the individual training examples , and the larger - scale organization of the data is ignored .",
    "a small complication arises when different elements of the feature vector have different scales .",
    "components of the feature vector with the very large numerical values will dominate @xmath24 , and the remaining components will have a negligible effect on the classification . to circumvent this ,",
    "we normalize each element of the feature vectors , such that the dispersions of each element across the data are equal",
    ".         influences the svm classification .",
    "the symbols and colors are the same as in figure [ fig : svm ] .",
    "the left and right figures show classifications with low and high values of @xmath21 .",
    "decreasing @xmath21 decreases the penalty for mis - classifications near the boundary , increasing robustness against outliers . , height=134 ]     influences the svm classification . as @xmath23 increases ,",
    "the decision boundary becomes more curved , and conforms more tightly to individual training examples .",
    "the margin in the rightmost plot wraps very tightly around the xs , and is not drawn.,width=326 ]    from a practical standpoint , then , an svm - based classification task involves four steps :    1 .",
    "manually classify a representative subset of the data (  [ sec : train ] ) .",
    "2 .   for each classified example",
    ", create a feature vector .",
    "this vector should encode the properties of an object that make it identifiable (  [ sec : feature ] ) .",
    "3 .   choose a kernel function .",
    "in this study , we restrict our attention to the radial basis function .",
    "train the algorithm , and optimize the classification by adjusting free parameters ( @xmath21 and @xmath23 in this case ;  [ sec : optimize ] ) .    in what follows",
    "we apply these steps to disentangle overlapping emission from the supernova remnant g16.05 - 0.57 and foreground molecular clouds .",
    "the success of any supervised learning algorithm is limited by how representative the training set is . because our aim is a pixel - by - pixel classification of the data , our training set consists of a subset of these pixels , manually - classified as associated with the snr ( or not ) .",
    "we carried out this manual identification on subsets of four position - velocity slices through the cube .",
    "the classification of one of the planes is shown in figure [ fig : slice ] . in total , the training set explicitly labels @xmath25 of the pixels in the cube ( 5% of the pixels above @xmath26 ) . however , as we show in section [ sec : performance ] , only a small fraction (  5% ) of these examples are ultimately necessary .",
    "each pixel in our training set must be assigned a feature vector  a list of numerical attributes that distinguish between the supernova and unassociated foreground objects . when classifying a pixel in the data by eye , it is sufficient to examine the pixels in the immediate vicinity .",
    "in particular , a @xmath27 pixel sub - cube in ppv space is sufficient for a human to classify the pixel in the center of that cube . at the presumed 3.4 kpc distance to the supernova",
    ", this corresponds to a @xmath28 region . in principle",
    ", one could use the intensities of these @xmath29 pixels as a feature vector for the central point . in practice ,",
    "such a large feature vector is prohibitively slow .",
    "we tested three strategies to compress this information .",
    "each of these strategies defines a different feature vector :    * moment*. for each pixel @xmath30 , we extract the surrounding @xmath27 pixels . we calculate the mean intensity of this cube , and the first and second moments along each direction through the data .",
    "these seven numbers constitute the feature vector for @xmath30 .",
    "relative to other cloud emission , supernova features have large velocity dispersions and small spatial dispersions  this information is encoded in the moments of the data .",
    "* derivative*. spatial derivatives are sensitive to edges in images , and such information can be used to identify filamentary structures in astronomical data @xcite .",
    "we generate a feature vector that encodes this information .",
    "we approximate the gradient in each direction and pixel location using the sobel edge detection operator . to generate the feature vector for pixel @xmath31 , we sample profiles of each derivative along each direction through the pixel : @xmath32 for convenience , we further down - sample @xmath33 to 60 elements , which defines the feature vector .",
    "we determined the degree of downsampling that was appropriate by examining the widths of typical features in the derivative profiles by eye .",
    "nevertheless , this smoothing may lead to worse performance .    * pca*. we approximate the @xmath27 sub - cube around each pixel @xmath30 as a linear combination of 15 representative `` basis cubes '' .",
    "we derive the basis cubes using principal component analysis ( pca , @xcite ) , and these basis cubes capture @xmath34 of the variance in the data .",
    "the 15 weights in the linear combination define our final feature vector for @xmath30 .",
    "this is essentially a ( lossy ) compression of the data and , unlike the first two methods , does not explicitly encode any intuitive , identifying characteristics .",
    "nevertheless , this expression of the data has proved useful in other classification tasks ( e.g. , asteroid taxonomy @xcite , stellar spectral types @xcite , star / galaxy separation @xcite ) .",
    "pca has also been used to decompose and analyze molecular cloud structure @xcite .",
    "as discussed above , two free parameters influence the training process : @xmath23 and @xmath21 .",
    "we use cross - validation to choose optimal values for these parameters .",
    "we first partition our classification examples into two independent sets .",
    "the first ( the training set ) is used to train the classifier using a given value for ( @xmath21 , @xmath23 ) .",
    "we then apply the classifier to the second ( validation ) data set , and measure the accuracy of the identification .",
    "we repeat this process for different values of ( @xmath21 , @xmath23 ) to maximize the performance on the validation set .",
    "this approach provides some protection against over - fitting , since over - fits to the training data will poorly classify the validation set . to maximize the independence of the training and data set , the two samples were drawn from different regions of the cube .",
    "noise level .",
    "all three classifiers are more accurate when classifying brighter structures.,width=326 ]    .",
    "blue pixels are classified as belonging to the supernova , while red pixels denote foreground cloud emission.,width=326 ]            we evaluate the performance of each classifier by comparing the accuracy with which it classifies the validation data described in  [ sec : optimize ] .",
    "the accuracy is simply the fraction of correctly - labeled pixels .",
    "we find that the feature vector which encodes the moments of the intensity achieves the highest performance .",
    "the maximum accuracies achieved using the moment , derivative , and pca feature vectors were 83% , 77% and 75% , respectively ( the y intercept of figure [ fig : optimize ] ) .",
    "however , misclassifications are biased towards low - intensity pixels .",
    "this is to be expected since , as figure [ fig : slice ] shows , pixels corresponding to blank sky have been included in both classes in the classification set .",
    "hence , the proper classification of these faint pixels is not well defined . misclassifying noise is not problematic , however , since simple thresholding later in the analysis can separate signal from background . for most data analysis purposes , it is more important that emission features be correctly classified .",
    "figure [ fig : optimize ] shows the accuracy at which each classifier identifies pixels above a given intensity threshold .",
    "here , the moment - based classifier has an accuracy of 90% for emission detected at 3@xmath35 , and exceeds 95% accuracy for the brightest pixels in the data .",
    "figure [ fig : slice_class ] shows the classification of the data in figure [ fig : slice ] , using the moment feature vectors . figure [ fig : pp_slice ] shows the classification of several position - position channels .",
    "many of the misclassified bright pixels lie near the boundary between supernova and cloud emission .",
    "this is perhaps expected , since the moments of the intensity are measured ( and implicitly smoothed ) over a @xmath27 sampling window .",
    "nevertheless , several supernova lines intersect and , presumably , extend behind cloud material . in most cases",
    ", the svm classifier at least partially follows these transitions from supernova to cloud .         and",
    "@xmath23 at each step .",
    "the dashed line fixes these parameters at their original optimal values .",
    "the dotted line is the trivial case of over - fitting , when each correction example causes the algorithm to correctly identify one additional example in the test set.,width=307 ]    there are three reasons why an svm classifier mis - classifies data .",
    "first , data from two classes may not segregate perfectly in feature space ( due either to noise in the data , errors in the training set , or a poorly - designed feature vector ) .",
    "second , the topology of the decision boundary ( determined by the svm kernel function ) may not be able to conform to the distribution of training data in feature space .",
    "finally , the training examples may insufficiently sample how data are distributed in feature space .",
    "each of these possibilities has implications for how to design and improve classification pipelines .",
    "figures [ fig : learning_rate ] and [ fig : correction_rate ] provide some insight into what limits the performance of our classification using the moment feature vectors . using the optimal values of @xmath21 and @xmath23 found above , we measured the learning rate  the accuracy of the classification as a function of training set size .",
    "figure [ fig : learning_rate ] shows this function , and suggests that most of the meaningful information is contained within the first few hundred examples .",
    "remaining examples contain redundant information , and confer little performance gain .",
    "misclassifications in the validation set may occur because the training set is nt representative enough , and the validation set samples a systematically different region of feature space . to test this possibility , figure",
    "[ fig : correction_rate ] shows the classification accuracy when we re - train using the original training set , augmented with a subset of the validation data that were originally mis - classified . the solid line re - optimizes @xmath21 and @xmath23 at each step .",
    "the dashed line shows the result when we fix these parameters to the values used above .",
    "note that in this experiment , part of the validation data is now explicitly included in the training set .",
    "this increases the risk of over - fitting the training data ( i.e. devising arbitrary rules that fit the training data , but which do not generalize well to new data ) . as a trivial case of over - fitting ,",
    "when a classifier is re - trained with a correction example in the validation set , it corrects the misclassification for that example only . the dotted line in figure [ fig : correction_rate ] depicts this scenario .",
    "any meaningful performance gain should fall above this line , since the classifier should ideally use the information in each correction example to correct many additional mistakes .",
    "the figure shows that , even when presented with additional correction examples , the algorithm shows essentially no performance gain .",
    "this figure rules out the possibility that misclassifications in the validation set are due to the training and validation sets sampling different regions of feature space .",
    "instead , this strongly suggests that the classification task is limited by the partial overlap of the two classes in feature space .",
    "additional training and correction examples are of little help in this situation , and a better feature vector is needed for further performance gain .",
    "we do not claim that these classifiers will necessarily generalize to other data sets or classification tasks .",
    "other applications likely require re - training the svm algorithm using the data at hand , or testing new feature vectors .",
    "however , figure [ fig : optimize ] suggests that the svm algorithm is capable of identifying morphological differences in the ism , and figure [ fig : learning_rate ] implies that this task can be taught efficiently , with little manual classification .",
    "for example , we have started to investigate whether wind - blown bubbles can be identified using _ spitzer _ colors and edge information as a feature vector ( beaumont et al . in prep . )      as mentioned above , the co emission from g16.05 - 0.57 is likely due to the remnant s collision with a molecular cloud , presumably in the scutum galactic arm at 3.4 kpc .",
    "our pixel - level classification of the data allows us to analyze the properties of this emission in isolation from foreground material .",
    "here we derive an estimate of the mass and momentum associated with the cloud / remnant collision .    in the limit that all material along the line of sight can be described by a single excitation temperature @xmath36 , the equation for the observed radiation temperature @xmath37 is @xcite @xmath38 where @xmath39 for co 32",
    ", @xmath40 is the optical depth , and @xmath41 is the beam filling factor ( which we take to be 1 ) .",
    "furthermore , the column density of the j=3 state is given by @xmath42 where @xmath43 is the einstein a coefficient .",
    "equation [ eq : trad ] can be solved for @xmath40 , such that equation [ eq : ncol ] explicitly depends only on @xmath36 and @xmath44 ( see , e.g. , equation a8 of ginsburg et al .",
    "@xcite ) .    to constrain the excitation temperature and opacity in the line centers",
    ", we obtained supplementary @xmath45co j=32 observations towards three bright knots of emission . assuming that the filling factor of the gas and excitation temperature of the two co isotopologues are the same",
    ", their intensity ratio gives the gas opacity : @xmath46}{1 - \\text{exp}[-\\tau_{12 } x_{13/12 } ] } \\label{eq : tau}\\ ] ] where @xmath47 is the abundance ratio of @xmath48 , which we take to be 70 .",
    "figure [ fig : tau ] shows the inferred opacity for all pixels where we detect emission from both isotopes .",
    "the typical optical depth is 35 .",
    "plugging @xmath49 into equation [ eq : trad ] gives an estimate of the excitation temperature along each line of sight , which we find to be @xmath50k .",
    "this in turn allows us to evaluate equation [ eq : ncol ] . finally , we convert from @xmath51 to @xmath52 assuming the population levels are thermalized , and to @xmath53 using an abundance ratio @xmath54 .",
    "the abundance of co within shocks is uncertain . in their study of co and h@xmath55 vibrational lines in the c - type shocks of the orion kl region ,",
    "watson et al .",
    "( @xcite ) measure an @xmath56 of @xmath57 . on theoretical grounds , the value of @xmath56 for dissociative shocks may be enhanced by up to a factor of 100 if the re - formation of @xmath58 in post - shock gas is less efficient than co @xcite .",
    "thus , the actual value of @xmath56 depends on both the type and strength of shocks in g16.05 - 0.57 , as well as the microphysics of grain catalysis in shocked gas .",
    "figure [ fig : ncol ] presents the column density map of the supernova remnant obtained from this analysis .",
    "the angularly - integrated column density of the map is @xmath59@xmath60ster .",
    "if we further assume that the remnant is located within the scutum arm at 3.4 kpc , this implies a total mass of @xmath61 m@xmath62 .",
    "we approximate the velocity of the gas at each point by the observed velocity dispersion  this is a lower limit , since it only accounts for motion along our line of sight .",
    "nevertheless , this implies a total momentum of @xmath63 m@xmath62 km s@xmath64 . for comparison ,",
    "the typical momentum of a supernova explosion is @xmath65 .",
    "the characteristic width of filamentary features in the remnant is 3060 ",
    "= 0.51 pc . taking this to be",
    "the line - of - sight depth of supernova emission implies a volume density of @xmath66 @xmath67 .",
    "this is roughly 12 orders of magnitude lower than the densities van dishoeck et al .",
    "( @xcite ) measured towards the supernova remnant / molecular cloud collision ic 443 .",
    "the most likely explanation of this discrepancy is that the filamentary features in our data consist of unresolved filamentary substructure . if this is the case , then the characteristic depth @xmath68 would be smaller , and the corresponding volume density higher .",
    "however , this would not affect our mass and momentum measurements , since the increase in volume density is cancelled out by the decrease in solid angle .",
    "co opacity inferred from equation [ eq : tau ] , in regions where both @xmath45co and @xmath0co are detected.,width=326 ]",
    "we have presented a case study of a supervised learning task applied to classifying structures in the ism .",
    "the m17 molecular cloud overlaps shocked co emission from g16.05 - 0.57 but , because of the supernova s distinct morphology in position - position - velocity space , the two objects are readily distinguishable by eye .",
    "the svm classification algorithm is able to learn these morphological differences using a representative sample of manually - classified pixels .",
    "we emphasize several important characteristics of this approach :    1 .",
    "machine - based classification of ism structures permits a pixel - level classification of datasets .",
    "this level of refinement is often prohibitively cumbersome via manual identification .",
    "2 .   by using an independent set of manually - classified validation data",
    ", we can characterize the quality of this classification .",
    "we can further use this information to refine and improve the algorithm s performance .",
    "3 .   extracting information about the moments of the intensity distribution in our data produced the most effective classification .",
    "other information ( the weights in a principal component analysis , spatial derivatives ) was less successful .",
    "4 .   only a very small fraction of the data ( @xmath69 ) needs to be categorized to train the algorithm .",
    "an efficient approach for future work may be to evaluate the classifier s performance as the training set is assembled , to better understand when the training set is large ( and representative ) enough .",
    "this case study suggests that automated algorithms are capable of identifying complex structures seen in the ism",
    ". such an approach may be useful in analyses of current and future surveys of the milky way s ism , particularly when identifying morphologically distinct structures like bubbles , pillars , and filaments .",
    "we are grateful to c. brogan for sharing her 20 cm data of g16.05 - 0.57 , and to k. binsted for discussions .",
    "we also thank the anonymous referee , whose careful reading and comments improved the clarity of this paper .",
    "this material is based upon work supported by the national science foundation under grant"
  ],
  "abstract_text": [
    "<S> we apply support vector machines  a machine learning algorithm  to the task of classifying structures in the interstellar medium . as a case study </S>",
    "<S> , we present a position - position velocity data cube of @xmath0co j=32 emission towards g16.05 - 0.57 , a supernova remnant that lies behind the m17 molecular cloud . despite the fact that these two objects partially overlap in position - position - velocity space , the two structures </S>",
    "<S> can easily be distinguished by eye based on their distinct morphologies . </S>",
    "<S> the support vector machine algorithm is able to infer these morphological distinctions , and associate individual pixels with each object at @xmath190% accuracy . </S>",
    "<S> this case study suggests that similar techniques may be applicable to classifying other structures in the ism  a task that has thus far proven difficult to automate . </S>"
  ]
}