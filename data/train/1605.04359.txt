{
  "article_text": [
    "the internet is a web of mostly unstructured knowledge woven around things .",
    "however , these things ; people , places , technologies , movies , products , books etc .",
    "are mostly just mentioned by their name , with other crucial bits of information about them scattered around the point of mention .",
    "the cosmic scale of such unstructured information has stemmed the dream of a semantic web . a web which is aware of the links that make sense , which _ understands _ what the user is looking for and which is gifted with the intelligence of locating the desideratum .",
    "there are several pieces in the puzzle of the semantic web , this report is an attempt to understand one important piece ; entities on the web and their co occurrence statistics .    given a knowledge base such as yago or freebase consisting of entities and relations , and the web , our goal is to attach reliable estimates of the frequency of occurrences on the web of various entities and relations as singletons , pairs ( ordered and unordered ) in a sentence .",
    "the aim is to collect statistics so as to be able to assign prior probabilities to the set of entities and relations that can co - exist in a sentence or a paragraph .",
    "these statistics have applications in query interpretation and language understanding tasks .",
    "we can view it as being analogous to statistics in relational catalogs .      for collecting the statistics about entities on the web ,",
    "we need a method to determine which words in the free flowing interminable text are of interest , i.e. represent entities .",
    "consider the following sentence :    we first want to identify all the * named entities * in the text .",
    "the task is called named entity recognition and is formally defined as :    [ nerdef ] named - entity recognition ( ner ) ( also known as entity identification and entity extraction ) is a subtask of information extraction that seeks to locate and classify atomic elements in text into predefined categories such as the names of persons , organizations , locations , expressions of times , quantities , monetary values , percentages , etc .",
    "but we do not stop at that , we want to link each of the named entities thus recognized to a knowledge base ] ] .",
    "thus , our problem has a 2 step solution :    * step 1 : * identify * entities + is a professor at * step 2 : * link * entities to knowledge bases : + ( http://en.wikipedia.org/wiki/michael_i._jordan ) is a professor at ( http://en.wikipedia.org/wiki/university_of_california,_berkeley )    the stanford ner library is a popular choice for recognizing named entities .",
    "[ [ stanfordner ] ]      in simple terms , disambiguating named entities in the unstructured text imparts a structure to the document .",
    "we need two more data points to further appreciate the power that such a tool provides to us .",
    "the first is the size of the web . as of 31st march 2014",
    ", there are atleast 1.8 billon indexed web pages.[[ws ] ] the second is the number of wikipedia entities .",
    "the wikipedia statistics [ [ wikistats ] ] estimate the number of pages to be around 32 million .",
    "yago , a catalog of entities made from wikipedia has 12 , 727 , 222 entities .",
    "imparting structure to documents at this magnitude has far reaching implications in the information extraction and is a bridge towards the hitherto dream of a semantic web .",
    "+ it is highly recommended that the reader pays http://www.google.co.in/insidesearch/features/search/knowledge.html , the google knowledge graph project , a visit .",
    "the following terms are widely used in the literature on named entity disambiguation and thus in this article .    * * mention , spot * + a piece of text which needs to be disambiguated .",
    "for example , the sentence `` * * amazon * * has attracted a lot of visitors '' . * * entity * + a named entity as defined in the definition [ nerdef ] . * * candidates * + a set of entities which might be the correct disambiguation for a given mention .",
    "for example , possible candidates for the sentence above are `` amazon river '' and `` amazon.com '' .",
    "* * prior * + probability of a mention linking to a particular entity . for example , the mention  amazon  may be used to refer to the website ( say ) 60% of the time .",
    "* * knowledge base * + a catalog of entities where an entity is as defined above . for example ,",
    "wikipedia or yago .",
    "the baseline which presents itself given the above problem is labeling the corpora with the named entities and then collecting the markings , keeping track of which entity was seen when along the way . as intuitive as it seems ,",
    "the method is unlikely to perform well in the present scenario , owing to the mismatch in the training and test distribution [ [ mmd ] ] .",
    "our training data , hand labeled corpora , is paltry in comparison with the massive open web , where such systems are supposed to be deployed . this is true even for large training datasets like the wikipedia .      the observation that we do nt really want the individual labels is a first step towards a better solution",
    ". there are 3 reported methods for direct estimation of class ratios [ [ mmd ] ] .",
    "we are interested in using one of them , maximum mean discrepancy ( mmd ) for solving the problem in hand .",
    "we introduce mmd and propose a formulation for determining class ratios in section [ sec : mmd ] .",
    "* section [ seckb ] * gives an overview of what are knowledge bases .",
    "this is important since the concept of such repositories of structured knowledge is central to the report .",
    "* section [ secned ] * begins with an introduction to the problem of named entity disambiguation , the terminology and applications , and goes on to cover the techniques for named entity disambiguation in some detail .",
    "we give and overview of the two broad categories of disambiguation techniques , local and global disambiguation .",
    "* section [ secagg ] * begins with a discussion on definition of aggregate statistics and some of their applications . finally , in section",
    "[ sec : mmd ] , we discuss maximum mean discrepancy and its application for estimating the aggregate statistics over entities .",
    "before the digital age , encyclopedias , such as the encyclopedia britannica were hailed as the repositories containing all that is known to the mankind .",
    "as the computer age dawned , it did nt take long for people to realize that a lot can be achieved if somehow all this information could be made available in a digital format .",
    "wordnet [ [ wordnet ] ] was perhaps the first such attempt .",
    "as the years passed , the research effort in the field of information extraction and creating structured knowledge got a huge pat on the back from the explosion of the web .",
    "wikipedia catalyzed the community , which motivated development of structured knowledge bases like dbpedia and yago .",
    "we discuss how knowledge bases fit in the context of named entity disambiguation , and give a list of several important knowledge bases , along with links to each for the interested reader .",
    "many named entity disambiguation algorithms exploit large knowledge bases . on the other hand ,",
    "reliable named entity disambiguators will be conducive towards fabrication of gargantuan knowledge bases from the open web .",
    "we thus see a chicken and egg situation here . as is often the case in such standoffs , the cycle is broken with the help of extensive manual effort . in the present case",
    ", wikipedia helps the situation .",
    "we give a brief overview of some of the popular knowledge bases .",
    "* wordnet has a clean , hand crafted type hierarchy .",
    "well documented apis , such as the nltk toolkit ( http://www.nltk.org/howto/wordnet.html ) are available for using wordnet for a plethora of tasks , such as listing all the senses of a word , finding distances between 2 concepts and the likes . *",
    "introduction to wordnet http://wordnetcode.princeton.edu/5papers.pdf      * an attempt to create a knowledge base that combines the clean type hierarchy of wordnet with the huge information that wikipedia provides .",
    "http://www.mpi-inf.mpg.de/yago-naga/yago/ has link to an online interface .",
    "refer [ [ yago ] ] for details .",
    "* dbpedia http://dbpedia.org/about extracts information from the wikipedia into rdf and provides an interface that can be used to ask semantic questions .",
    "users can use sparql to ask complicated queries with results spanning several pages .",
    "amazon also provides a dbpedia machine image for the users of aws .",
    "* patty http://www.mpi-inf.mpg.de/yago-naga/patty/ is a repository of relation patterns .",
    "the aim is to create `` wordnet '' for relations .",
    "the authors also create a subsumption hierarchy for the 350 , 569 pattern synsets .",
    "refer [ [ patty ] ] for details .      * freebase [ [ freebase ] ] relies on crowd sourcing for creation of a rich but clean knowledge base .",
    "the development of freebase follows the same chain as wikipedia , with users flagging issues , and cleaning and augmenting information .",
    "freebase also provides access to itself using web apis .",
    "we have already given an introduction to the problem and the applications in the introduction .",
    "the next section discusses the solutions based on local disambiguation , i.e. , figuring out the correct entity based on just the local evidences .",
    "section 3.2 discusses the intuition behind having a global strategy for disambiguation , and the optimization problem that results from such an objective .",
    "the final section summarizes a recent work which pragmatically selects global and local evidences , to get the best of both worlds .        in local disambiguation",
    ", we collect just local evidences for each mention for its disambiguation .",
    "this was state of the art until the csaw[[thepaper ] ] paper came along .",
    "we start by defining the problem and discussing the general form of solutions .",
    "we then provide a short summary of approach followed in wikify [ [ wikify ] ] and the famous milne and witten paper [ [ mw ] ] . a solution based on machine learning[[thepaper ] ]",
    "concludes the subsection .",
    "we need to disambiguate a mention by collecting the local evidences .",
    "the evidences can be anything , pos tags , gender information , dictionary lookup etc . by local disambiguation ,",
    "we mean that * we can not use the disambiguation information for any other entities for solving the problem .",
    "*      every local disambiguation techniques fall into one of the following two categories[[wikify ] ]    * * knowledge based * + derived from the classical word sense disambiguation literature , this technique depends on the information drawn from the definitions provided by the knowledge base .",
    "( see lesk s algorithm [ [ lesk ] ] ) .",
    "this is based on the overlap of context with the definitions of each of the candidate senses as given in the knowledge base . * * machine learning based * + this method is based on collecting features from the mention and its surroundings , and training a classifier to give a verdict on a particular sense being a likely disambiguation of a mention .",
    "machine learning based local disambiguation was almost unanimously adopted by the ned community as the solution for local disambiguation .",
    "aida changed the scene by introducing a knowledge based local similarity score which works well .",
    "wikify[[wikify ] ] the biggest contribution of this paper is perhaps presenting wikipedia as the catalog against which were supposed to disambiguate .",
    "the paper also identifies two broad methods of doing named entity disambiguation : knowledge based and data based . since the paper dates back to 2007 , when the problem of ned was not as established , there are a lot of references to the problem of word disambiguation .    learning to link with wikipedia[[mw ] ] this paper defined three different features for disambiguation :    * commonness : this is the prior defined in chapter 1 . * relatedness : perhaps the biggest contribution of this paper , the relatedness score , gives a measure for determining how similar the two entities are .",
    "this measure is based on the number of common inlinks to entities in question .",
    "the relatedness measure as defined here has been used in a lot of works .",
    "in fact , all the approaches presented in the subsequent subsections use this relatedness score , popular as the milne - witten score for finding out entity entity similarity .",
    "this score is defined as follows +   + @xmath0 + where * * @xmath1 : set of wikipedia pages that link to @xmath2 * * @xmath3 total number of wikipedia pages * * @xmath4 relatedness of topics @xmath2 and @xmath5    the algorithm selects a few unambiguous links in the document , and uses the similarity of the candidates with these unambiguous links as a criteria for disambiguation .",
    "thus , in some sense , although the technique is not totally local , it shies away from doing anything to maintain coherence among the entities that are unveiled and thus we do not call this method a `` global method '' , which are discussed in the following subsection .",
    "as mentioned , there are primarily two approaches for local disambiguation .",
    "this subsection discusses a machine learning based local disambiguation method in some detail .",
    "this subsection is based on the local disambiguation approach taken in [ [ thepaper ] ] .",
    "definitions we first repeat the definitions for quick reference :    * @xmath6 : spot , an entity to be disambiguated ( christian leader john paul ) * @xmath2 : an entity label value ( http://en.wikipedia.org/wiki/po-pe_john_paul_ii ) * @xmath7 : a feature function that creates a vector of features given a spot and a candidate entity label .    local compatibility : feature design the feature function takes the spot and the candidate as arguments .    *",
    "the following information about a candidate @xmath2 is used * * text from the first descriptive paragraph of @xmath2 * * text from the whole page for @xmath2 * * anchor text within wikipedia for @xmath2 . * * anchor text and 5 tokens around @xmath2 * we now have 4 pieces of information about @xmath2 .",
    "we take each of these , and apply the following operations with one argument as the spot * * dot - product between word count vectors * * cosine similarity in tfidf vector space * * jaccard similarity between word sets    thus , for a candidate - mention pair , we get a total of 12 features ( 3 operations , 4 argument pairs ) .",
    "in addition to these , we also use a sense probability prior as defined in the introduction . a popular way of obtaining the prior is counting the number of times the spot has been linked to a particular entity . for example , the hypertext `` linux '' might be linked to the page for the linux kernel 70% of the times , and to the page for linux based operating systems rest of the times .",
    "compatibility score once we have the features , we train the classifier by using the following optimization objective :    * local compatibility score between a spot @xmath6 and a candidate is given by @xmath8 * @xmath9 is trained using an svm like training objective + @xmath10    finding the best candidate    note that a multi class classifier is not learned for several reasons , all of which can be mapped to the large number of classes .",
    "we have seen several different `` local '' solutions , attempting to solve the problem by collecting evidence around a mention and then using it to disambiguate .",
    "milne and witten [ [ mw ] ] came close to inculcating some sort of coherence , but they could nt totally build up the intuition .",
    "it was after a wait of 2 years that csaw [ [ thepaper ] ] took the game to a whole new level by working on the following key intuition :    * a document is usually about one topic * disambiguating each entity using the local clues misses out on a major piece of information : topic of a page * a page is usually has one topic , you can expect all the entities to be _ related _ to the topic _ somehow _    : 30 disambiguations    : 10 disambiguations    but if they are mentioned on the * same page * , the page is most likely about christianity , a big hint towards disambiguating * both * of them .    since the csaw[[thepaper ] ] paper ,",
    "every work on named entity disambiguation includes a notion of _ topical coherence _ in the solution .",
    "though the notion of topical coherence is very natural and intuitive , there are a lot of challenges involved when it comes to actually mapping these intuitions to an optimization problem .",
    "we present the challenges involved and the solution given by the csaw team .    * capturing local compatibility * *   * inculcating topical coherence in the overall objective * *     out of these two challenges , various solutions to the problem of capturing the local compatibility are presented in chapter 2 . in this subsection ,",
    "we focus on the problem of collective disambiguation .",
    "* need to define a collective score based on pairwise topical coherence of all @xmath11 used for labeling . *",
    "the pairwise topical coherence , @xmath12 is as defined above . * for a page , overall topical coherence : + @xmath13 * can be written as clique potential as in case of node potential + @xmath14      with different notations as above , we would like to maximize the following to get the best results .",
    "@xmath15 + ] ]    in verbose , we want that the entity - entity coherence be maximized , while choosing the disambiguation which is the best .",
    "the authors compare 2 different approaches for solving the optimization objective .",
    "* lp rounding approach + @xmath16 + @xmath17 binary variables were introduced .",
    "the first set of binary variables decide the candidate that each mention takes , and the second set has one binary variable for each possible candidate pair .",
    "the authors relax this integer programming to a linear programming and then used rounding with a threshold of 0.5 to obtain the best solution .",
    "* hill climbing + starting from all assignments set to na , assignments are done based on local potentials only .",
    "the following figure ( from the paper ) illustrates the process .        recall that chapter 2 was about local disambiguation . in subsection 3 , we saw how global disambiguation can be combined with the overall objective .",
    "a recent work , robust disambiguation of named entities in text [ [ aida ] ] , proposes that blindly opting for global disambiguation may not be always right .",
    "consider the sentence : `` manchester will play madrid in barcelona '' .",
    "all the 3 named entities in the sentence are cities as well as football clubs",
    ". collective disambiguation may _ coerce _ all the three mentions to be either football clubs or cities .",
    "the work aims to solve this problem by being selective about when to go for collective disambiguation .",
    "this approach first creates a mention to candidate graph . the sample graph for the sentence",
    "`` they performed kashmir written by page and plant .",
    "page played unusual chord on his gibson . ''",
    "is as shown below :        having created the graph , we need to assign the edge weights .",
    "clearly , there are 2 kinds of edges involved :    * mention - entity edge : the authors used a knowledge based approach to assign this weight .",
    "this is as outlined in subsection 2 .",
    "the details about this score are given in [ [ kpsim ] ] .",
    "* entity - entity edge : milne witten score as defined in subsection 2 is used for this purpose .    with",
    "the graph ready , the authors pluck the in a greedy manner such that there is only one edge between each mention and entity .      for this report , only a small subset of the papers was selected to cover as much ground as possible",
    ". the following list may be valuable to the interested readers .    * * mining evidences for named entity disambiguation * the authors discuss a modified lda model for gathering more words that are important to disambiguate an entity .",
    "li , yang , et al .",
    "`` mining evidences for named entity disambiguation . ''",
    "proceedings of the 19th acm sigkdd international conference on knowledge discovery and data mining .",
    "acm , 2013 .",
    "* * we have emphasized on wikipedia as the catalog .",
    "the following work presents a general approach * + sil , avirup , et al .",
    "`` linking named entities to any database . ''",
    "proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning .",
    "association for computational linguistics , 2012 . *",
    "* large scale named entity disambiguation . * + cucerzan , silviu .",
    "`` large - scale named entity disambiguation based on wikipedia data . '' emnlp - conll .",
    ". 2007 . * * one of the initial works on ned * + bunescu , razvan c. , and marius pasca .",
    "`` using encyclopedic knowledge for named entity disambiguation . ''",
    "* quick entity annotations for short text * + suchanek , fabian m. , gjergji kasneci , and gerhard weikum .",
    "`` yago : a core of semantic knowledge . ''",
    "proceedings of the 16th international conference on world wide web .",
    "acm , 2007 .",
    "once you have a catalog of things , it makes sense to ask which of these `` things '' are more important than the others .",
    "in fact , one might extend the question and ask , `` which pairs ( or triples ) of these things appear together on the open web ? '' .",
    "we define several different statistics one might be interested in over these entity catalogs , discuss some applications , propose a baseline method and finally , prepare the ground for the next section by giving an outline of a solution which is aimed at directly providing us with the statistics we are looking for .        for starters",
    ", we might want to calculate the number of times a particular `` sense '' of an entity is used .",
    "for example , the entity michael jordan has several disambiguations , : the professor , basketballer and the botinist .",
    "we want to find out the distribution of occurrences of these senses .",
    "we call this number the sense prior",
    ".    it is important to note that entity prior is different from mention prior , which is the fraction of times a mention links to a particular entity .",
    "for example , the text `` gingerbread '' might refer to several different concepts ; from perhaps the most famous android 2.3 to the novel .",
    "mention prior is to find out how many _ gingerbreads _",
    "mentions on the web refer to gingerbread the operating system .",
    "entity sense prior would tell us how frequent is gingerbread the os compared with gingerbread the novel .",
    "@xmath18    where @xmath19 is the @xmath20 sense disambiguation in wikipedia parlance ] of the entity e.      a second interesting statistic would be to count how many times do two given entities , taking two given senses appear together .",
    "for example , we might want to know how many times does nokia http://en.wikipedia.org/wiki/nokia appears with gingerbread http://en.wikipedia.org/wiki/gingerbread_(operating_system )    we call these counts entity bi grams .",
    "we note that in contrast to word bi - grams and relational grams [ [ relgram ] ] , entity bi grams are symmetric , and there is no obvious use case where we might need to know the order dependent occurrence count of the entities .",
    "however , such a formulation will lead to a sparse distribution , since each count will have to be normalized by the total number of entity bigrams .",
    "we thus define the entity bi gram count as follows : @xmath21    we propose an application of entity bi grams for finding out important entities motivated by [ [ relgram ] ] .",
    "we list a few applications of the sense prior and outline an application of the entity bigrams .",
    "a prior over the sense will be helpful in many applications related to information retrieval .",
    "* entity querying * knowledge graph based searching      given an entity , we want to find out other important entities that are related to it .",
    "for example , given an entity * barack obama , president of the usa * , we need to provide top 10 entities that are `` close '' to barack obama the president . since the solution is only a slight modification of the solution presented in [ [ cohschemas ] ] for finding out important relations , we only sketch an outline here .    for the entity",
    "we are interested in , say x , create a node . now attach to the node x all the entities e for which @xmath22 where @xmath23 is some threshold .",
    "let the weight of the edge be defined as    @xmath24    we then apply personalized page rank on the x sub graph , starting with x having a page rank of 1 and other nodes having a page rank of 0 .",
    "we can then sort the nodes based on the their page ranks upon convergence .",
    "how do we collect the aforementioned statistics ?",
    "this question should nt be too difficult to answer now .",
    "the whole of part 3 was dedicated towards tagging entity mentions in the text .",
    "we can use any of the methods ( for example , aida can be set up as a rest service ) to tag the corpus , and then iterate over the corpus to collect these statistics in single pass .      while estimating class ratios by doing per mention disambiguation seems pretty intuitive , we are doing more than what we need to do .",
    "we are not interested in what each mention disambiguates to , a count of how many times does a particular entity appears is the desideratum .",
    "there are 3 different methods in the open domain for directly estimating the class ratio[[mmd ] ] , without going through the label and collect route .",
    "in particular , [ [ mmd ] ] discuss a solution based on maximum mean discrepancy and proves some upper bounds on errors .",
    "if mmd really works , we should expect better estimation of the sense prior and the entity grams .",
    "the next section outlines the mmd based solution and how mmd may be used to estimate the sense priors for different entities .",
    "this section discusses the mmd approach for direct estimation of class ratios[[mmd ] ] .",
    "we first provide an intuition for the solution , follow it up with some results      the following hypothetical example is aimed to capture the gist of class ratio estimation using mmd .",
    "suppose that in a factory producing balls , there are 3 different ball production machines , ( say ) a , b and c. since neither of the machines is perfect , they do not produce spherical balls .",
    "rather , the balls are ellipsoids .",
    "thus , for each ball , we have 3 different features corresponding to the three semi - axes . since all the machines are different , they have their own unique view of how balls should look like , and thus we expect that the semi axes are a good way of telling the machine which produced a given ball .",
    "also assume that for all the 3 machines , we also have the most likely ( expected ) semi axes measures of the balls produced by them .",
    "let us call these @xmath25 and @xmath26 .",
    "these are the _ expected feature weights_.    suppose we are given a 150 balls produced from these three machines . for 120 balls out of them , we know the machine from which the ball was produced . for the remaining 30 balls , we are asked to give an estimate of how many balls came from machine a , b and c.    how do we do this ?",
    "of course , we can learn a classifier from the 120 known instances and then learn the label each of the 30 balls and collect counts ( label and collect approach ) .",
    "mmd takes the following route to reach the solution .",
    "suppose we are magically given the true class ratios , say , @xmath27 , @xmath28 and @xmath29 .",
    "let @xmath30 be the average of the semi axes of the 30 balls .",
    "let @xmath31 be defined as @xmath32    clearly , we would expect @xmath30 to match @xmath31 .    note",
    "that we do nt really know the @xmath33 , but all is not lost since we know what to look for ; we look for the thetas that minimize : @xmath34    while ensuring that :    * all the @xmath33 sum to 1 . *",
    "all the @xmath33 are non negative .",
    "this is the motivation behind mmd for class ratio estimation .      with the above example by our side      we reproduce the problem statement from [ [ mmd ] ]    * let @xmath35 be the set of all instances and @xmath36 be the set of all labels . * given a labeled dataset @xmath37 , design an estimator that for any given set @xmath38 can estimate the class ratios @xmath39 $ ] where @xmath40 denotes the fraction of instances with class label y in u      * match two distributions based on the mean of features in the hilbert space induced by a kernel k. * assume that distribution of features is same in both training and test data @xmath41 * thus , the test distribution must equal @xmath42    * let @xmath43 and @xmath44 denote the true means of the feature vectors of the y th class and the unlabeled data * suppose we somehow get the true class ratios @xmath45 .",
    "the true mean of the feature vector of the unlabeled data can then be obtained by @xmath46 .",
    "* so ideally , @xmath47    the objective thus is @xmath48    such that    * @xmath49 * @xmath50    interesting discussion on theoretical bounds on the error in the class ratios thus predicted and methods for learning kernel can be found in [ [ mmd ] ]      given a corpus with mentions identified ( using , say [ [ stanfordner ] ] ) , we want reliable estimates of frequency of each of the entities . in this subsection",
    ", we gloss over the solution .    *",
    "* features * + each mention has several candidate disambiguations .",
    "this gives one way of formulating the features .",
    "for each mention , we can have a ( sparse ) feature vector having non zero scores for the candidates . * * training data * + can be obtained by splicing the named entity disambiguation pipeline of any of the popular named entity disambiguators .",
    "[ [ aidafeature ] ] discusses how to achieve this for aida , a popular named entity disambiguator .",
    "the potential of open web can only be harnessed to its full extent by adding structure to it .",
    "the process involves creating structured repositories derived from the web that can answer interesting questions pertaining to entities that exist on the web .",
    "many such smart applications that rely on structured web will rely on frequencies of occurrence of the former .",
    "the report has been a buildup to achieving that .",
    "we started by briefing what knowledge bases are . in the second part",
    ", we introduced the problem of disambiguating the mentions of named entities and presented solutions roughly spanning last 8 years of research in the field .    in the third part",
    ", we elaborated on what is meant by aggregate statistics and presented several applications of the same .",
    "we presented maximum mean discrepancy approach for class ratio estimation via an example and discussed the problem formulation .",
    "we briefly outlined how mmd can be applied for estimating occurrence statistics of entities .",
    "state of the art approaches for named entity disambiguation brush the figure of 90% accuracy .",
    "it is thus expected that the focus of the community will now shift to making the process of disambiguation faster and integrating the disambiguators in the search pipeline .",
    "it remains to be seen how approaches based on direct estimation of entity occurrence ratios perform in comparison with the standard tools , both in terms of speed and accuracy .",
    "this report is a summary of selected readings undertaken while working under the guidance of prof .",
    "sunita sarawagi on application of mmd for collecting occurrence statistics of entities on the web .",
    "i would like to thank her for the guidance .",
    "it was immensely helpful in gaining the understanding required for writing this report .",
    "[ thepaper ] kulkarni , sayali , et al .",
    "`` collective annotation of wikipedia entities in web text . ''",
    "proceedings of the 15th acm sigkdd international conference on knowledge discovery and data mining .",
    "acm , 2009 .",
    "[ wikify ] mihalcea , rada , and andras csomai .",
    "`` wikify ! : linking documents to encyclopedic knowledge . ''",
    "proceedings of the sixteenth acm conference on conference on information and knowledge management .",
    "acm , 2007 .",
    "[ aida ] hoffart , johannes , et al .",
    "`` robust disambiguation of named entities in text . ''",
    "proceedings of the conference on empirical methods in natural language processing .",
    "association for computational linguistics , 2011 .",
    "[ kpsim ] hoffart , johannes , et al .",
    "`` kore : keyphrase overlap relatedness for entity disambiguation . ''",
    "proceedings of the 21st acm international conference on information and knowledge management .",
    "acm , 2012 .",
    "[ relgram ] balasubramanian , niranjan , stephen soderland , and oren etzioni .",
    "`` rel - grams : a probabilistic model of relations in text . ''",
    "proceedings of the joint workshop on automatic knowledge base construction and web - scale knowledge extraction .",
    "association for computational linguistics , 2012 .",
    "[ cohschemas ] balasubramanian , niranjan , stephen soderland , and oren etzioni mausam .",
    "`` generating coherent event schemas at scale . ''",
    "proceedings of the empirical methods in natural language processing .",
    "acm ( 2013 ) .",
    "[ lesk ] michael lesk .",
    "automatic sense disambiguation using machine readable dictionaries : how to tell a pine cone from an ice cream cone . in proceedings of the 5th annual international conference on systems documentation ( sigdoc 86 ) , virginia debuys ( ed . ) .",
    "acm , new york , ny , usa , 24 - 26 .",
    "doi=10.1145/318723.318728 http://doi.acm.org/10.1145/318723.318728          [ patty ] nakashole , ndapandula , gerhard weikum , and fabian suchanek .",
    "`` patty : a taxonomy of relational patterns with semantic types . ''",
    "proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning .",
    "association for computational linguistics , 2012 .",
    "[ freebase ] bollacker , kurt , et al .",
    "`` freebase : a collaboratively created graph database for structuring human knowledge .",
    "'' proceedings of the 2008 acm sigmod international conference on management of data .",
    "acm , 2008 .",
    "[ mmd ] iyer , arun , saketha nath , and sunita sarawagi .",
    "`` maximum mean discrepancy for class ratio estimation : convergence bounds and kernel selection . ''",
    "proceedings of the 31st international conference on machine learning . 2014 ."
  ],
  "abstract_text": [
    "<S> the problem of collecting reliable estimates of occurrence of entities on the open web forms the premise for this report . </S>",
    "<S> the models learned for tagging entities can not be expected to perform well when deployed on the web . </S>",
    "<S> this is owing to the severe mismatch in the distributions of such entities on the web and in the relatively diminutive training data . in this report </S>",
    "<S> , we build up the case for maximum mean discrepancy for estimation of occurrence statistics of entities on the web , taking a review of named entity disambiguation techniques and related concepts along the way .    ' '' ''    ' '' ''     +   occurrence statistics of entities , relations and types on the web +    ' '' ''    ' '' ''     + submitted in partial fulfillment of requirements for the degree of master of technology    by +    aman madaan + under the guidance of prof . </S>",
    "<S> sunita sarawagi        _ department of computer science and engineering + indian institute of technology bombay _    </S>",
    "<S> april , 2014 + </S>"
  ]
}