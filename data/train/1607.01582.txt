{
  "article_text": [
    "this work focuses on classification trees and how their ensembles can be utilized in order to set up a prediction environment using ecological momentary assessment ( ema ) data from a real - world study .",
    "ema @xcite refers to a collection of methods used in many different disciplines by which a research subject repeatedly reports on specific variables measured close in time to experience and in the subject s natural environment ( e.g. experiencing food craving is measured again and again on the same subject ) .",
    "ema aims to minimize recall bias , maximize ecological validity and allow microscopic analysis of influence behavior in real - world contexts .",
    "ema data has a different structure than normal data and account for several dependencies between them , since e.g. many samples belong to the same subject so they are expected to be correlated . however , most decision trees that deal with ema data do not take these specificities into account .    bagging involves having each tree in the ensemble vote with equal weight while boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis - classified",
    ". major differences between bagging and boosting are that ( a ) boosting changes the distribution of training data based on the performance of classifiers created up to that point ( bagging acts stochastically ) and ( b ) bagging uses equal weight voting while boosting uses a function of the performance of a classifier as a weight for voting .",
    "there are limited studies on combining bagging and boosting ( @xcite , @xcite , @xcite and @xcite ) , however , none of these approaches have been applied to longitudinal data or take into account the ema structure .",
    "efforts to apply decision trees to ema data have been attempted but they are mostly focusing on regression tasks ( @xcite , @xcite , @xcite ) and on the other hand they do not use bagging or boosting for improving performance .",
    "work in current paper aims at bridging this gap by combining bagging and boosting with the longitudinal data structure .",
    "let the training data be @xmath0 and @xmath1 where each @xmath2 is a @xmath3-dimensional vector and @xmath4 is the associated observed class label . to justify generalization ,",
    "it is usually assumed that training data as well as any test data are _ iid _ samples from some population of @xmath5 pairs .",
    "our goal is to as accurately predict @xmath6 given @xmath7 .",
    "the first step to fit a bbt is to select the loss function , which in the case of a classification problem is based on the logistic regression loss .",
    "after some initial parameter selection ( number of trees to be grown in sequence , shrinkage ( or learning ) rate , size of individual trees and fraction of the training data sampled ) we grow bbt ( say using @xmath8 trees ) on the training data using the following process and by growing single boosted trees ( bt ) :    * divide the data into @xmath9 ( typically @xmath10 ) subsets and construct @xmath9 training data sets each of which omits one of the @xmath9 subsets ( the out - of - bag  data ) .",
    "each one of the @xmath9 subsets is created by bootstrap sampling data points from the set of subjects ( @xmath11 ) . to create the learning set",
    "we introduce the strategy @xmath12 according to which one observation is drawn per subject .",
    "this strategy is based on a simple rationale : when only one observation per subject is selected , the probability that different observations are used for the training of different trees is increased , although the same subjects might be selected which further reduces similarity between trees . by this way , we manage to incorporate advantages of subject - based bootstrapping and observation - based bootstrapping into the final bbt ensemble .",
    "also , this approach can be applied to unbalanced data points per subject . *",
    "grow @xmath9 bt ; one for each of the @xmath9 training sets , based on the adaboost algorithm @xcite : first let @xmath13 for all @xmath2 and initialize weights @xmath14 for @xmath15 .",
    "then repeat the following for @xmath16 for each one of the @xmath9 bt : * * fit the decision tree @xmath17 to the training data sample using weights @xmath18 where @xmath17 maps each @xmath2 to -1 or 1 . * * compute : + - the weighted error rate @xmath19 + - half its log - odds and derive @xmath20 * * let @xmath21 * * replace the weights @xmath18 with @xmath22 and then renormalize by replacing each @xmath23 by @xmath24 . *",
    "calculate the pe for each bt for tree sizes @xmath25 to @xmath8 from the corresponding out - of - bag data and pool across the @xmath9 boosted trees .",
    "predictions for new data are computed by first predicting each of the component trees and then aggregate the predictions ( e.g. , by averaging ) , like in bagging . *",
    "the minimum pe estimates the optimum number of trees @xmath26 for the bt .",
    "the estimated pe of the single bt obtained by cross - validation can thus also be used to estimate pe for the bbt .",
    "bbt thus require minimal additional computation beyond estimation of @xmath26 . *",
    "reduce the number of trees for each bt to @xmath26 .    for a classification problem",
    ", we use an estimate @xmath27 of the conditional class probability function ( ccpf ) @xmath28 that can be obtained from @xmath29 through a logistic link function :    @xmath30    classifying at the 1/2 quantile of the ccpf works well for binary classification problems but in the case of ema data , sometimes classification with unequal costs or , equivalently , classification at quantiles other than 1/2 is needed .",
    "strategies about correctly computing the ccpf are considered @xcite by over / under - sampling which convert a median classifier into a q - classifier .",
    "in order to illustrate the effect of bbt , we now apply this method to an ema dataset obtained by a study designed by the authors @xcite .",
    "the ema study followed 100 participants over the course of 14 days using experience & event sampling questionnaires ending up with over 5000 data points containing information about participants eating events , emotions , circumstances , locations , etc .",
    "( in total there are 9 variables ) for several time moments during each day that they participated in the study .",
    "each data point is used to predict whether the next data point ( provided that they both occur on the same day ) will be a healthy or an unhealthy eating moment .",
    "figure [ fig : data ] shows an example of how data points ( belonging to user ) are converted and combined in order to enable early prediction using a classification algorithm ( class can be either or ) . then the bbt algorithm can be applied .    in the comparison between methods , bbt gave a pe of @xmath31 , whereas the single classification tree ( @xmath32 ) , bagged trees ( @xmath33 ) , boosted trees ( adaboost ) ( @xmath34 ) , random forests ( @xmath35 ) and b&b combine method @xcite ( @xmath36 ) have higher pe than the bbt .",
    "table [ tab : results ] summarizes these results and also presents a series of experiments made to demonstrate the effectiveness of bbt when the number of different subjects ( @xmath37 ) involved in the dataset increases .",
    "for relatively small numbers of subjects ( 10 or 20 ) performance of bbt and adaboost is comparable ( although variance increases and the number of data samples is not large enough ) but as @xmath37 increases the performance of bbt is clearly better .",
    "larger @xmath37 means that there are more subjects in the dataset , thus the complexity of longitudinal structure increases and it is imperative to take this into account when classifying longitudinal data .",
    "this is the reason that bbt performs better than all other algorithms as @xmath37 increases .",
    "however , for small @xmath37 the effect of different subjects is smaller and this is the reason that adaboost performs slightly better than all other algorithms .",
    ".prediction error ( variance ) % for different algorithms and different numbers of subjects ( @xmath37 ) [ cols=\">,^,^,^,^\",options=\"header \" , ]     [ tab : results ]",
    "in this paper a combination of bagging and boosting was presented : bagged boosted trees ( bbt ) .",
    "bbt have the advantage of being able to deal with multiple categorical data which raises a scalability issue when dealing with classic models ( like generalized linear models ) that are widely used in ema studies .",
    "moreover , bbt can tackle potential nonlinearities and interactions in the data , since these issues are handled through the combination of many different trees of different sizes .",
    "experimental results of bbt on a real - world ema dataset clearly show improvement with respect to accuracy in prediction compared to other decision tree algorithms .",
    "further work involves the evaluation of the conditional class probability function ( based on over / under sampling of data ) , as well as the application to other ema datasets .",
    "finally , adjustment of boosting in order to implement weights based on subjects ( and not individual observations ) is a direction with promising results ."
  ],
  "abstract_text": [
    "<S> ecological momentary assessment ( ema ) data is organized in multiple levels ( per - subject , per - day , etc . ) and this particular structure should be taken into account in machine learning algorithms used in ema like decision trees and its variants . </S>",
    "<S> we propose a new algorithm called bbt ( standing for bagged boosted trees ) that is enhanced by a over / under sampling method and can provide better estimates for the conditional class probability function . </S>",
    "<S> experimental results on a real - world dataset show that bbt can benefit ema data classification and performance . </S>"
  ]
}