{
  "article_text": [
    "batch optimization for the statistical estimation of continuous - time models can be impractical for large datasets where observations occur over a long period of time .",
    "batch optimization takes a sequence of descent steps for the model error for the entire observed data path .",
    "since each descent step is for the model error for the _ entire observed data path _ , batch optimization is slow ( sometimes impractically slow ) for long periods of time or models which are computationally costly to evaluate ( e.g. , partial differential equations ) .",
    "stochastic gradient descent in continuous time provides a computationally efficient method for statistical learning over long time periods and for complex models .",
    "stochastic gradient descent _ continually _ takes gradient steps _ along the path of the observation _ which results in much more rapid convergence .",
    "parameters are updated online in continuous time , with the parameter updates @xmath0 satisfying a stochastic differential equation . we prove that @xmath1 where @xmath2 is a natural objective function for the estimation of the continuous - time dynamics .",
    "we consider a diffusion @xmath3 : @xmath4 the goal is to statistically estimate a model @xmath5 for @xmath6 where @xmath7 .",
    "@xmath8 is a standard brownian motion .",
    "the diffusion term @xmath9 represents any random behavior of the system or environment . the functions @xmath5 and @xmath6 may be non - convex .",
    "the stochastic gradient descent update in continuous time follows the stochastic differential equation ( sde ) : @xmath10 , \\label{sdemain}\\end{aligned}\\ ] ] where @xmath11 is matrix valued and @xmath12 is the learning rate .",
    "the parameter update ( [ sdemain ] ) can be used for both statistical estimation given previously observed data as well as online learning ( i.e. , statistical estimation in real - time as data becomes available ) .",
    "we assume that @xmath13 is sufficiently ergodic ( to be concretely specified later in the paper ) and that it has some well - behaved @xmath14 as its unique invariant measure . as a general notation ,",
    "if @xmath15 is a generic @xmath16 function , then we define its average over @xmath14 to be @xmath17    let us set @xmath18    heuristically , it is expected that @xmath0 will tend towards the minimum of the function @xmath19",
    ". the stochastic gradient descent update ( [ sdemain ] ) continuously moves the parameters in an _ estimated _ direction of @xmath20 .",
    "that is , the drift of @xmath0 is a _ biased _ estimate of @xmath20 .",
    "the bias will decrease as time increases .",
    "this differs from the standard discrete case of stochastic gradient descent where the descent steps are unbiased since the data is i.i.d . at every step .",
    "the data @xmath21 in the continuous time setting will be correlated over long periods of time , further complicating the analysis .    in this paper",
    "we show that if @xmath12 is appropriately chosen then @xmath22 as @xmath23 with probability 1 ( see theorem [ t : maintheorem ] ) .",
    "results like this have been previously derived in the literature for discrete time and in the absence of the @xmath24 component , see @xcite .",
    "the presence of the @xmath24 term complicates the analysis as one needs to control the speed at which convergence to equilibrium happens .",
    "furthermore , the parameter updates are now biased and the bias is correlated across times .",
    "although stochastic gradient descent for discrete time has been extensively studied , stochastic gradient descent in continuous time has received relatively little attention . in comparison to results available in discrete time , our convergence result requires weaker assumptions .",
    "we refer readers to @xcite and @xcite for a thorough review of the very large literature on stochastic gradient descent .",
    "there are also many algorithms which modify traditional stochastic gradient descent ( stochastic gradient descent with momentum , adagrad , rmsprop , etc . ) . for a review of these variants of stochastic gradient descent ,",
    "see @xcite .",
    "we mention below the prior work which is most relevant to our paper .",
    "as mentioned , @xcite proves convergence of discrete - time stochastic gradient descent in the absence of the @xmath24 process .",
    "the presence of the @xmath24 process is essential for considering a wide range of problems in continuous time , and showing convergence with its presence is considerably more difficult .",
    "the @xmath24 term introduces correlation across times , and this correlation does not disappear as time tends to infinity .",
    "unlike in @xcite where parameter updates are unbiased , the correlation introduced by the @xmath24 process causes parameter updates to be biased .",
    "furthermore , the bias of the parameter updates is correlated across times .",
    "these facts make it challenging to prove convergence in the continuous - time case . in order to prove convergence",
    ", we use an appropriate poisson equation associated with @xmath24 to describe the evolution of the parameters for large times .",
    "@xcite proves , in a setting different than ours , convergence in @xmath25 of projected stochastic gradient descent in discrete time for convex functions . in projected gradient descent",
    ", the parameters are projected back into an a priori chosen compact set .",
    "therefore , the algorithm can not hope to reach the minimum if the minimum is located outside of the chosen compact set .",
    "of course , the compact set can be chosen to be very large for practical purposes .",
    "our paper considers unconstrained stochastic gradient descent in continuous time and proves the almost sure convergence @xmath22 as @xmath23 taking into account the @xmath24 component as well .",
    "recently , @xcite proves asymptotic normality and a finite - sample error bound for discrete - time stochastic gradient descent with implicit updates .",
    "implicit stochastic gradient descent often has greater stability than standard stochastic gradient descent .",
    "another approach for proving convergence of discrete - time stochastic gradient descent is to show that the algorithm converges to the solution of an ode which itself converges to a limiting point ; see @xcite and @xcite .",
    "this method , sometimes called the  ode method \" , requires the strong assumption that the iterates ( i.e. , the model parameters which are being learned ) remain in a bounded set with probability one . proving that the iterates remain in a bounded set with probability one can be challenging to show and , moreover , may not necessarily be true for many models of interest .",
    "it is also noteworthy that it is not clear that the ode method can be used to prove convergence of the stochastic gradient descent algorithm in continuous time , which is the problem that this paper considers .",
    "@xcite studies continuous - time stochastic mirror descent in a setting different than ours .",
    "they prove a bound for the minimization of a convex function . in the framework of @xcite , the objective function is known and descent steps are therefore unbiased .",
    "in contrast , we consider the statistical estimation of the unknown dynamics of a random process ( i.e. the @xmath24 process satisfying ( [ classofeqns ] ) ) .",
    "the descent steps in our case are therefore biased , complicating the analysis . for certain applications , stochastic gradient descent in continuous time may have advantages over stochastic gradient descent in discrete time .",
    "physics and engineering models are typically in continuous time .",
    "it therefore makes sense to also develop the statistical learning updates in continuous time .",
    "for example , statistical learning may be used to estimate coefficients or parameters in the engineering model .",
    "the engineering model may also be enhanced in some cases with the addition of a machine learning model to better fit real - world conditions and data .",
    "continuous - time dynamics are oftentimes simpler to analyze than discrete dynamics at longer time intervals .",
    "for instance , a partial differential equation can be written in a very simple form , but its global dynamics over a long time period can be very complex",
    ". it may therefore be advantageous to learn the continuous - time dynamics .",
    "in addition , the continuous - time framework only requires the estimation of a deterministic function @xmath5 while a discrete - time framework would require estimating a multidimensional density @xmath26 $ ] . estimating",
    "a multidimensional density @xmath27 is typically more computationally challenging than estimating a vector - valued function @xmath5 . in our setting ( [ classofeqns ] )",
    ", the instantaneous dynamics @xmath5 directly yield the density @xmath27 . consequently , even if one s goal is to model the density at a longer time horizon @xmath28 , it may be preferable to instead estimate the continuous - time dynamics @xmath29 instead of directly estimating the density @xmath30 .",
    "although continuous - time stochastic gradient descent must ultimately be discretized for numerical implementation , the continuous - time formulation still has significant numerical advantages .",
    "the continuous - time stochastic gradient descent algorithm allows for the control and reduction of numerical error due to discretization .",
    "in particular , higher - order numerical schemes can be used for the numerical solution of the continuous - time stochastic gradient descent updates .",
    "this will lead to more accurate and more computationally efficient parameter updates .",
    "furthermore , continuous - time stochastic gradient descent can use non - uniform time step sizes . if convergence is slow , the time step size may be adaptively decreased .",
    "continuous - time stochastic gradient descent allows for efficient learning at different time step sizes by providing the appropriate scaling for the learning rate .",
    "where @xmath31 is the time step size . ] in contract , discrete - time stochastic gradient descent operates at fixed discrete steps and does not allow for the adaptive control of the time step size nor higher - order numerical schemes to reduce discretization error .",
    "we numerically study the convergence of continuous - time stochastic gradient descent for a number of applications .",
    "applications include the ornstein - uhlenbeck process , burger s equation , and the classic reinforcement learning problem of balancing a pole on a moving cart .",
    "the ornstein - uhlenbeck process is widely - used in finance , physics , and biology .",
    "burger s equation is a widely used nonlinear partial differential equation which is important to fluid mechanics , nonlinear acoustics , and aerodynamics .",
    "it is extensively used in engineering .",
    "the paper is organized into three main sections .",
    "section [ assumptions ] presents the assumption and the main theorem . in section [ s : proofmaintheorem ]",
    "we prove the main result of this paper for the convergence of continuous - time stochastic gradient descent .",
    "section [ numericalanalysis ] provides numerical analysis of continuous - time stochastic gradient descent for several example applications .",
    "before presenting the main result of this paper , theorem [ t : maintheorem ] , let us elaborate on the standing assumptions . in regards to the learning rate @xmath12 the standing assumption is    [ a : extraassumption ] assume that @xmath32 , @xmath33 and that @xmath34 .    a standard choice for @xmath35 that satisfies condition [ a : extraassumption ] is @xmath36 for some constant @xmath37 .",
    "notice that the condition @xmath34 follows immediately from the other two restrictions for the learning rate if it is chosen to be a monotonic function of @xmath38 .",
    "let us next discuss the assumptions that we impose on @xmath39 , @xmath6 and @xmath40 .",
    "condition [ a : lyapunovcondition ] guarantees uniqueness and existence of an invariant measure for the @xmath24 process .",
    "[ a : lyapunovcondition ] we assume that @xmath39 is non - degenerate bounded diffusion matrix and @xmath41    in addition , with respect to @xmath42 we assume that @xmath43 and we impose the following condition    [ a : assumption1 ]    1 .",
    "we assume that @xmath44 for all @xmath45 , @xmath46 , @xmath47 uniformly in @xmath43 for some @xmath48 and that there exist @xmath49 and @xmath50 such that @xmath51 2 .   for every @xmath52",
    "there exists a constant @xmath53 such that for all @xmath54 and @xmath55 , the diffusion coefficient @xmath56 satisfies @xmath57 moreover , there exists @xmath58 and @xmath59 such that @xmath60 3 .",
    "the function @xmath6 is @xmath61 with @xmath48 .",
    "namely , it has two bounded derivatives in @xmath62 , with all partial derivatives being hlder continuous , with exponent @xmath63 , with respect to @xmath62 .",
    "condition [ a : assumption1 ] allows one to control the ergodic behavior of the x process .",
    "as will be seen from the proof of the main convergence result theorem [ t : maintheorem ] , one needs to control terms of the form @xmath64 .",
    "due to ergodicity of the @xmath24 process one expects that such terms are small in magnitude and go to zero as @xmath23 .",
    "however , the speed at which they go to zero is what matters here .",
    "we treat such terms by rewriting them equivalently using appropriate poisson type partial differential equations ( pde ) .",
    "condition [ a : assumption1 ] guarantees that these poisson equations have unique solutions that do not grow faster than polynomially in the @xmath62 variable ( see theorem [ t : regularitypoisson ] in appendix [ s : regularityresults ] ) .",
    "the main result of this paper is theorem [ t : maintheorem ] .",
    "[ t : maintheorem ] assume that conditions [ a : extraassumption ] , [ a : lyapunovcondition ] and [ a : assumption1 ] hold .",
    "then we have that @xmath65",
    "we proceed in a spirit similar to that of @xcite . however , apart from continuous versus discrete dynamics , one of the main challenges of the proof here is the presence of the ergodic @xmath24 process .",
    "let us consider an arbitrarily given @xmath66 and @xmath67 to be chosen . then set @xmath68 and consider the cycles of random times @xmath69 where for @xmath70 @xmath71 \\text { and } \\int_{\\tau_{k}}^{t}\\alpha_{s}ds\\leq \\lambda\\}.\\end{aligned}\\ ] ]",
    "the purpose of these random times is to control the periods of time where @xmath72 is close to zero and away from zero .",
    "let us next define the random time intervals @xmath73 and @xmath74 .",
    "notice that for every @xmath75 we have @xmath76 .",
    "let us next consider some @xmath77 sufficiently small to be chosen later on and set @xmath78 .",
    "[ l : interval ] assume that conditions [ a : extraassumption ] , [ a : lyapunovcondition ] and [ a : assumption1 ] hold .",
    "choose @xmath79 such that for a given @xmath66 , one has @xmath80 , where @xmath81 is the lipschitz constant of @xmath82 .",
    "for @xmath83 large enough and for @xmath77 small enough ( potentially random depending on @xmath83 ) , one has @xmath84 .",
    "in addition we also have @xmath85 with probability one .",
    "let us define the random variable @xmath86 then , for any @xmath87 we have @xmath88 .",
    "we proceed with an argument via contradiction . in particular",
    "let us assume that @xmath89 and let us choose arbitrarily some @xmath90 such that @xmath91 .",
    "let us now make some remarks that are independent of the sign of @xmath92 .",
    "due to the summability condition @xmath33 , @xmath93 and conditions [ a : extraassumption ] and [ a : assumption1 ] , the martingale convergence theorem applies to the martingale @xmath94 .",
    "this means that there exists a square integrable random variable @xmath95 such that @xmath96 both almost surely and in @xmath97 .",
    "this means that for the given @xmath90 there is @xmath83 large enough such that @xmath98 almost surely .",
    "let us also assume that for the given @xmath83 , @xmath99 is so small such that for any @xmath100 $ ] one has @xmath101 .",
    "then , we obtain the following @xmath102+\\left\\|\\int_{\\tau_{k}}^{\\sigma_{k,\\eta}}\\alpha_{s}\\left(\\nabla_{\\theta } g(x_{s},\\theta_{s})-\\nabla_{\\theta } \\bar{g}(\\theta_{s})\\right)ds\\right\\|.\\end{aligned}\\ ] ]    let us next bound appropriately the euclidean norm of the vector - valued random variable @xmath103    by lemma [ l : boundforextraterm1 ] we have that for the same @xmath104 that was chosen before there is @xmath83 large enough such that almost surely @xmath105    hence , using also the fact that @xmath93 we obtain @xmath106= \\|\\nabla \\bar{g}(\\theta_{\\tau_{k}})\\| \\frac{1}{2 l_{\\nabla \\bar{g } } } .\\end{aligned}\\ ] ]    the latter then implies that we should have @xmath107    the latter statement will then imply that @xmath108 but then we would necessarily have that @xmath84 , since otherwise @xmath109 $ ] which is impossible .",
    "next we move on to prove the second statement of the lemma . by definition",
    "we have @xmath110 .",
    "so it remains to show that @xmath111 .",
    "since we know that @xmath84 and because for @xmath83 large enough and @xmath99 small enough one should have @xmath112 , we obtain that @xmath113 concluding the proof of the lemma .",
    "[ l : boundforextraterm1 ] assume that conditions [ a : extraassumption ] , [ a : lyapunovcondition ] and [ a : assumption1 ] hold .",
    "let us set @xmath103    then , with probability one we have that @xmath114    the idea is to use theorem [ t : regularitypoisson ] in order to get an equivalent expression for the term @xmath115 that we seek to control .",
    "let us consider the function @xmath116 .",
    "notice that by definition and due to condition [ a : assumption1 ] , the function @xmath117 satisfies the centering condition ( [ eq : centeringcondition ] ) of theorem [ t : regularitypoisson ] componentwise .",
    "so , the poisson equation ( [ eq : cellproblem ] ) will have a unique smooth solution that grows at most polynomially in @xmath62 .",
    "let us apply it formula to the vector valued function @xmath118 that is solution to this poisson equation with right hand side @xmath117 .",
    "doing so , we get for @xmath119    @xmath120ds\\nonumber\\\\ & + \\int_{\\tau}^{\\sigma}\\left<\\nabla_{x}u_{i}(s , x_{s},\\theta_{s}),\\sigma dw_{s}\\right > + \\int_{\\tau}^{\\sigma}\\alpha_{s}\\left<\\nabla_{\\theta}u_{i}(s , x_{s},\\theta_{s}),\\nabla_{\\theta}f(x_{s},\\theta_{s})\\sigma^{-1}dw_{s}\\right>,\\end{aligned}\\ ] ]    where @xmath121 and @xmath122 denote the infinitesimal generators for processes @xmath24 and @xmath123 respectively .    recall now that @xmath124 is solution to the given poisson equation and that we can write @xmath125 . using these facts and rearranging the previous it formula",
    ", we get in vector notation @xmath126\\nonumber\\\\ & \\quad-\\int_{\\tau_{k}}^{\\sigma_{k}}\\alpha_{s}\\left[\\mathcal{l}_{\\theta}v ( x_{s},\\theta_{s})+\\alpha_{s } \\text{tr}\\left[\\nabla_{\\theta}f(x_{s},\\theta_{s})\\nabla_{x_{i}}\\nabla_{\\theta}v(x_{s},\\theta_{s})\\right]_{i=1}^{m}\\right]ds\\nonumber\\\\ & \\quad-\\int_{\\tau_{k}}^{\\sigma_{k}}\\alpha_{s}\\left<\\nabla_{x}v(x_{s},\\theta_{s}),\\sigma dw_{s}\\right > -\\int_{\\tau_{k}}^{\\sigma_{k}}\\alpha_{s}^{2}\\left<\\nabla_{\\theta}v(x_{s},\\theta_{s}),\\nabla_{\\theta}f(x_{s},\\theta_{s})\\sigma^{-1}dw_{s}\\right>.\\label{eq : equivalentexpression1}\\end{aligned}\\ ] ]    next step is to treat each term on the right hand side of ( [ eq : equivalentexpression1 ] ) separately . for this purpose , let us first set @xmath127}\\left\\|v(x_{s},\\theta_{s})\\right\\|.\\ ] ]    by theorem [ t : regularitypoisson ] and proposition 2 of @xcite there is some @xmath128 ( that may change from line to line below ) and @xmath129 such that for @xmath38 large enough @xmath130}\\|x_{s}\\|^{q}\\right]= k \\alpha_{t}^{2}\\left[1+\\sqrt{t}\\frac{\\mathbb{e}\\sup_{s\\in[0,t]}\\|x_{s}\\|^{q}}{\\sqrt{t}}\\right]\\nonumber\\\\ & \\leq k \\alpha_{t}^{2}\\left[1+\\sqrt{t}\\right]\\leq k \\alpha_{t}^{2}\\sqrt{t}.\\end{aligned}\\ ] ]    let us now consider @xmath131 such that @xmath132 and for any @xmath133 define the event @xmath134",
    ". then we have for @xmath38 large enough such that @xmath135 @xmath136    the latter implies that @xmath137    therefore , by borel - cantelli lemma we have that for every @xmath133 there is a finite positive random variable @xmath138 and some @xmath139 such that for every @xmath140 one has @xmath141    thus for @xmath142 and @xmath140 one has for some finite constant @xmath143 @xmath144}|v(x_{s},\\theta_{s})|\\leq k \\alpha_{2^{n+1 } } \\sup_{s\\in(0,2^{n+1}]}|v(x_{s},\\theta_{s})| \\leq k \\frac{d(\\omega)}{2^{(n+1)(p-\\delta)}}\\leq k \\frac{d(\\omega)}{t^{p-\\delta}}.\\ ] ]    the latter display then guarantees that for @xmath145 we have with probability one @xmath146    next we consider the term @xmath147_{i=1}^{m}\\right)\\right\\|ds.\\end{aligned}\\ ] ]    by the bounds of theorem [ t : regularitypoisson ] we see that there are constants @xmath128 ( that may change from line to line ) and @xmath129 such that @xmath148    the first inequality follows by theorem [ t : regularitypoisson ] , the second inequality follows by proposition 1 in @xcite and the third inequality follows by condition [ a : extraassumption ] .",
    "the latter display implies that the random variable @xmath149 is finite with probability one , which then implies that there is a finite random variable @xmath150 such that @xmath151    the last term that we need to consider is the martingale term @xmath152 notice that the burkholder - davis - gundy inequality and the bounds of theorem [ t : regularitypoisson ] ( doing calculations similar to the ones for the term @xmath153 ) give us that for some finite constant @xmath143 , we have @xmath154    thus , by doob s martingale convergence theorem there is a square integrable random variable @xmath155 such that @xmath156    let us now go back to ( [ eq : equivalentexpression1 ] ) .",
    "using the terms @xmath157 , @xmath153 and @xmath158 we can write @xmath159    the last display together with ( [ eq : termj1 ] ) , ( [ eq : termj2 ] ) and ( [ eq : termj3 ] ) imply the statement of the lemma .",
    "lemma [ l : boundednessofg ] shows that the function @xmath160 and its first two derivatives are uniformly bounded in @xmath123 .    [ l : boundednessofg ] assume conditions [ a : extraassumption ] , [ a : lyapunovcondition ] and [ a : assumption1 ] .",
    "for any @xmath59 , there is a constant @xmath161 such that @xmath162 in addition we also have that there is a constant @xmath163 such that @xmath164 .    by theorem 1 in @xcite , the density @xmath165 of the measure @xmath166 admits , for any @xmath30 , a constant @xmath167 such that @xmath168 . choosing @xmath30 large enough that @xmath169 , we then obtain    @xmath170    concluding the proof of the first statement of the lemma .",
    "let us now focus on the second part of the lemma .",
    "we only prove the claim for @xmath171 , since due to the bounds in condition [ a : assumption1 ] , the proof for @xmath172 is the same . by condition [ a : assumption1 ] and by the first part of the lemma , we have that there exist constants @xmath173 such that @xmath174 concluding the proof of the lemma .",
    "our next goal is to show that if the index @xmath83 is large enough , then @xmath160 decreases , in the sense of lemma [ lemmauppernegativebound ] .",
    "[ lemmauppernegativebound ] assume conditions [ a : extraassumption ] , [ a : lyapunovcondition ] and [ a : assumption1 ] .",
    "suppose that there are an infinite number of intervals @xmath175 .",
    "there is a fixed constant @xmath176 such that for @xmath83 large enough , one has @xmath177    by it s formula",
    "we have that @xmath178ds \\notag \\notag\\\\ & + & \\int_{\\tau_{k}}^{\\sigma_{k } } \\alpha_{s } \\left<\\nabla_{\\theta } \\bar g(\\theta_{s } ) , \\nabla_{\\theta } \\bar{g}(\\theta_{s})- \\nabla_{\\theta } g(x_{s},\\theta_{s})\\right > ds    \\notag \\\\ & = & \\theta_{1,k}+\\theta_{2,k}+\\theta_{3,k } + \\theta_{4,k}.\\end{aligned}\\ ] ]    let s first consider @xmath179 .",
    "notice that for all @xmath180 $ ] one has @xmath181 .",
    "hence , for sufficiently large @xmath83 , we have the upper bound : @xmath182 since lemma [ l : boundforextraterm1 ] proved that @xmath183 for sufficiently large @xmath83 .",
    "we next address @xmath184 and show that it becomes small as @xmath185 .",
    "first notice that we can trivially write @xmath186    by condition [ a : assumption1 ] and it isometry we have @xmath187 where @xmath188 is defined via ( [ eq : rs ] ) .",
    "hence , by doob s martingale convergence theorem there is a square integrable random variable @xmath95 such that @xmath189 both almost surely and in @xmath97 .",
    "the latter statement implies that for a given @xmath90 there is @xmath83 large enough such that almost surely @xmath190    we now consider @xmath191 . @xmath192",
    "ds   \\right\\|   \\leq   c   \\int_{0}^{\\infty } \\frac{\\alpha_{s}^{2}}{2}\\mathbb{e}\\left(1+\\|x_{s}\\|^{q}\\right )   ds     < \\infty , \\label{theta3infinitya}\\end{aligned}\\ ] ] where we have used condition [ a : assumption1 ] and lemma [ l : boundednessofg ]",
    ". bound ( [ theta3infinitya ] ) implies that @xmath193 ds\\ ] ] is finite almost surely , which in turn implies that there is a finite random variable @xmath194 such that @xmath195 ds \\rightarrow \\theta_3^{\\infty } \\text { as } t \\rightarrow \\infty,\\ ] ] with probability one . since @xmath194 is finite , @xmath196 ds \\rightarrow 0 $ ] as @xmath185 with probability one .",
    "finally , we address @xmath197 .",
    "let us consider the function @xmath198 .",
    "the function @xmath117 satisfies the centering condition ( [ eq : centeringcondition ] ) of theorem [ t : regularitypoisson ] .",
    "therefore , the poisson equation ( [ eq : cellproblem ] ) with right hand side @xmath117 will have a unique smooth solution that grows at most polynomially in @xmath62 .",
    "let us apply it formula to the function @xmath118 that is solution to this poisson equation .",
    "@xmath199ds\\nonumber\\\\ & + \\int_{\\tau}^{\\sigma}\\left",
    "< \\nabla_{x}u(s , x_{s},\\theta_{s}),\\sigma dw_{s}\\right>+\\int_{\\tau}^{\\sigma}\\alpha_{s}\\left<\\nabla_{\\theta}u(s , x_{s},\\theta_{s}),\\nabla_{\\theta}f(x_{s},\\theta_{s})\\sigma^{-1}dw_{s}\\right>.\\end{aligned}\\ ] ]    one can write @xmath125 . using these facts and rearranging the previous it formula yields    @xmath200\\nonumber\\\\ & \\quad-\\int_{\\tau_{k}}^{\\sigma_{k}}\\alpha_{s}\\left[\\mathcal{l}_{\\theta}v ( x_{s},\\theta_{s})+\\alpha_{s } \\text{tr}\\left[\\nabla_{\\theta}f(x_{s},\\theta_{s})\\nabla_{x}\\nabla_{\\theta}v(x_{s},\\theta_{s})\\right]\\right]ds\\nonumber\\\\ & \\quad-\\int_{\\tau_{k}}^{\\sigma_{k}}\\alpha_{s}\\left<\\nabla_{x}v(x_{s},\\theta_{s}),\\sigma dw_{s}\\right > -\\int_{\\tau_{k}}^{\\sigma_{k}}\\alpha_{s}\\left<\\nabla_{\\theta}v(x_{s},\\theta_{s}),\\nabla_{\\theta}f(x_{s},\\theta_{s})\\sigma^{-1}dw_{s}\\right>.\\label{eq : equivalentexpression1}\\end{aligned}\\ ] ]    following the exact same steps as in the proof of lemma [ l : boundforextraterm1 ] gives us that @xmath201 almost surely .",
    "we now return to @xmath202 and provide an upper bound which is negative . for sufficiently large @xmath83 , we have that : @xmath203 choose @xmath204 . on the one hand ,",
    "if @xmath205 : @xmath206 on the other hand , if @xmath207 , then @xmath208 finally , let @xmath209 and the proof of the lemma is complete .",
    "[ jintervalupperbound ] assume conditions [ a : extraassumption ] , [ a : lyapunovcondition ] and [ a : assumption1 ] .",
    "suppose that there are an infinite number of intervals @xmath175 .",
    "there is a fixed constant @xmath210 such that for @xmath83 large enough , @xmath211    first , recall that @xmath212 for @xmath213 $ ] .",
    "similar to before , we have that : @xmath214ds \\notag \\\\ & + & \\int_{\\sigma_{k-1}}^{\\tau_{k } } \\alpha_{s } \\left<\\nabla_{\\theta } \\bar g(\\theta_{s } ) , \\nabla_{\\theta } \\bar{g}(\\theta_{s})- \\nabla_{\\theta } g(x_{s},\\theta_{s})\\right > ds    \\notag \\\\ & \\leq &   \\int_{\\sigma_{k-1}}^{\\tau_{k } }   \\alpha_{s }   \\left<\\nabla \\bar g ( \\theta_s),\\nabla_{\\theta } f(x_s , \\theta_s)\\sigma^{-1 }    d w_s\\right > \\nonumber\\\\   & + & \\int_{\\sigma_{k-1}}^{\\tau_{k } } \\frac{\\alpha_{s}^{2}}{2 } \\text{tr}\\left [ ( \\nabla_{\\theta } f(x_s , \\theta_s)\\sigma^{-1})(\\nabla_{\\theta } f(x_s , \\theta_s)\\sigma^{-1})^{t } \\nabla^{2}_{\\theta }   \\bar g ( \\theta_s ) \\right ] ds \\notag \\\\ & + & \\int_{\\sigma_{k-1}}^{\\tau_{k } } \\alpha_{s } \\left<\\nabla_{\\theta } \\bar g(\\theta_{s } ) , \\nabla_{\\theta } \\bar{g}(\\theta_{s})- \\nabla_{\\theta } g(x_{s},\\theta_{s})\\right > ds . \\label{eqnupperboundj}\\end{aligned}\\ ] ] the right hand side ( rhs ) of equation ( [ eqnupperboundj ] ) converges almost surely to @xmath215 as @xmath185 as a consequence of similar arguments as given in lemma [ lemmauppernegativebound ] .",
    "indeed , the treatment of the second and third terms on the rhs of ( [ eqnupperboundj ] ) are exactly the same as in lemma [ lemmauppernegativebound ] .",
    "it remains to show that the first term on the rhs of ( [ eqnupperboundj ] ) converges almost surely to @xmath215 as @xmath185 .",
    "@xmath216    as shown in lemma [ lemmauppernegativebound ] , @xmath217 as @xmath185 almost surely .",
    "finally , note that @xmath218 ( except when @xmath219 , in which case the interval @xmath220 is length 0 and hence the integral ( [ martintj ] ) over @xmath220 is @xmath215 ) .",
    "then , @xmath221 as @xmath185 almost surely .",
    "therefore , with probability one , @xmath222 for sufficiently large @xmath83 .",
    "choose a @xmath223 .",
    "first , consider the case where there are a finite number of times @xmath224 .",
    "then , there is a finite @xmath225 such that @xmath226 for @xmath227 .",
    "now , consider the other case where there are an infinite number of times @xmath224 and use lemmas [ lemmauppernegativebound ] and [ jintervalupperbound ] . with probability one , @xmath228 for sufficiently large @xmath83",
    "choose a @xmath49 such that ( [ boundsgamma ] ) holds for @xmath229 .",
    "this leads to : @xmath230 } \\leq   \\sum_{k = k}^n ( - \\gamma + \\gamma_1 ) < 0 .\\end{aligned}\\ ] ] let @xmath231 and then @xmath232 .",
    "however , we also have that by definition @xmath233 . this is a contradiction , and",
    "therefore almost surely there are a finite number of times @xmath224 .",
    "consequently , there exists a finite time @xmath225 ( possibly random ) such that almost surely @xmath234 for @xmath227 .",
    "since the original @xmath66 was arbitrarily chosen , this shows that @xmath235 as @xmath236 almost surely .",
    "we implement continuous - time stochastic gradient descent for several applications and numerically analyze the convergence .",
    "section [ ou ] studies continuous - time stochastic gradient descent for the ornstein - uhlenbeck process , which is widely used in finance , physics , and biology .",
    "section [ ou2 ] studies the multidimensional ornstein - uhlenbeck process .",
    "section [ burger ] estimates the diffusion coefficient in burger s equation with continuous - time stochastic gradient descent .",
    "burger s equation is a widely - used nonlinear partial differential equation which is important to fluid mechanics , nonlinear acoustics , and aerodynamics .",
    "burger s equation is extensively used in engineering . in our final example in section [ rlexample ] ,",
    "we show how continuous - time stochastic gradient descent can be used for reinforcement learning .",
    "the ornstein - uhlenbeck ( ou ) process @xmath237 satisfies the stochastic differential equation : @xmath238 we use continuous - time stochastic gradient descent to learn the parameters @xmath239 .    for the numerical experiments , we use an euler scheme with a time step of @xmath240 .",
    "the learning rate is @xmath241 with @xmath242 .",
    "we simulate data from ( [ oueqn1d ] ) for a particular @xmath243 and the stochastic gradient descent attempts to learn a parameter @xmath0 which fits the data well .",
    "@xmath0 is the statistical estimate for @xmath243 at time @xmath38 .",
    "if the estimation is accurate , @xmath0 should of course be close to @xmath243 .",
    "this example can be placed in the form of the original class of equations ( [ classofeqns ] ) by setting @xmath244 and @xmath245 .",
    "we study @xmath246 cases . for each case , a different @xmath243 is generated uniformly at random in the range @xmath247 \\times [ 1,2]$ ] . for each case",
    ", we solve for the parameter @xmath0 over the time period @xmath248 $ ] for @xmath249 .",
    "to summarize :    * for cases n = 1 to 10,500 * * generate a random @xmath243 in @xmath247 \\times [ 1,2]$ ] * * simulate a single path of @xmath21 given @xmath243 and simultaneously solve for the path of @xmath0 on @xmath248 $ ]    the accuracy of @xmath0 at times @xmath250 , and @xmath251 is reported in table [ ouerror ] .",
    "figures [ ouerrorplot ] and [ oumseerrorplot1d ] plot the mean error in percent and mean squared error ( mse ) against time . in the table and figures , the  error \" is @xmath252 where @xmath253 represents the @xmath253-th case .",
    "the  error in percent \" is @xmath254 .",
    "mean error in percent \" is the average of these errors , i.e. @xmath255 .",
    ".[ouerror ] error at different times for the estimate @xmath0 of @xmath243 across @xmath246 cases .",
    "the  error \" is @xmath252 where @xmath253 represents the @xmath253-th case .",
    "the  error in percent \" is @xmath256 [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]",
    "we recall the following regularity result from @xcite on the poisson equations in the whole space , appropriately stated to cover our case of interest .    [",
    "t : regularitypoisson ] let conditions [ a : lyapunovcondition ] and [ a : assumption1 ] be satisfied .",
    "assume that @xmath257 , @xmath258 and that for some positive constants @xmath49 and @xmath50 , @xmath259 let @xmath121 be the infinitesimal generator for the @xmath24 process",
    ". then the poisson equation @xmath260 has a unique solution that satisfies @xmath261 for every @xmath45 , @xmath262 and there exist positive constants @xmath263 and @xmath264 such that @xmath265"
  ],
  "abstract_text": [
    "<S> we consider stochastic gradient descent for continuous - time models . </S>",
    "<S> traditional approaches for the statistical estimation of continuous - time models , such as batch optimization , can be impractical for large datasets where observations occur over a long period of time . </S>",
    "<S> stochastic gradient descent provides a computationally efficient method for such statistical learning problems . </S>",
    "<S> the stochastic gradient descent algorithm performs an online parameter update in continuous time , with the parameter updates @xmath0 satisfying a stochastic differential equation . </S>",
    "<S> we prove that @xmath1 where @xmath2 is a natural objective function for the estimation of the continuous - time dynamics . </S>",
    "<S> the convergence proof leverages ergodicity by using an appropriate poisson equation to help describe the evolution of the parameters for large times . </S>",
    "<S> numerical analysis of the stochastic gradient descent algorithm is presented for several examples , including the ornstein - uhlenbeck process , burger s stochastic partial differential equation , and reinforcement learning . </S>"
  ]
}