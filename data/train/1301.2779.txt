{
  "article_text": [
    "entropy is a very important quantity and plays a key role in many aspects of statistical mechanics and information theory .",
    "the most widely used form of entropy was given by boltzmann and gibbs from the statistical mechanics point of view and by shannon from an information theory point of view .",
    "later certain other generalized measures of entropy like the rnyi entropy [ @xcite ] and the sharma - mittal - taneja entropy [ @xcite,@xcite ] were introduced and their information theoretic aspects were investigated . recently in [ @xcite ] a new expression for the entropy was proposed as a generalization of the boltzmann - gibbs entropy and the necessary properties like concavity , lesche stability and thermodynamic stability were verified .",
    "this entropy has been applied to a wide variety of physical systems , in particular to long - range interacting systems [ @xcite,@xcite ] and nonmarkovian systems [ @xcite ] .",
    "most of the generalized entropies introduced so far were constructed using a deformed logarithm . but two generalized entropies one called the fractal entropy and the other known as fractional entropy were proposed using the natural logarithm .",
    "the fractal entropy which was introduced in [ @xcite ] attempts to describe complex systems which exhibit fractal or chaotic phase space .",
    "similarly the fractional entropy was put forward in [ @xcite ] and later applied to study anomalous diffusion [ @xcite ] . merging these two entropies in our present work we propose a fractional entropy in a fractal phase space .",
    "thus there are two parameters one parameter characterizing the fractional nature of the entropy and the other parameter describing the fractal dimension of the phase space .",
    "thus the functional form of the entropy depends on the natural logarithm .",
    "we give the generalized shannon - kinchinn axioms corresponding to this two parameter entropy and prove that they uniquely characterize our entropy .",
    "the two parameter relative entropy and the jensen shannon divergence measure are also generated .",
    "the generalized fisher information is derived from the relative entropy .",
    "relative fisher information measure and its associated jensen - fisher information measure corresponding to this entropy are also proposed .",
    "the thermodynamic properties like the lesche stability and the thermodynamic stability are also verified .",
    "we notice that the probability distribution which maximizes this entropy is expressible in terms of the lambert s @xmath0-function .",
    "finally we set up a two parameter generalization of the well known complexity measure lmc ( lpez - ruiz , mancini and calbet ) complexity measure [ @xcite ] and apply it to a two level system and an exponential distribution .",
    "after the introduction in section i , we introduce our new two parameter entropy in section ii and investigate its properties . in section",
    "iii the relative entropy and the jensen - shannon divergence measure corresponding to this two parameter entropy is presented and its properties are studied . using the relative entropy the fisher information measure , the relative fisher information and the jensen - fisher information",
    "are also obtained .",
    "the thermodynamic properties are analyzed in section iv . in section",
    "v we present a two parameter generalization of the lmc complexity measure and analyze the complexity of a two level system and a system with continuous probability distribution .",
    "we conclude in section vi .",
    "the boltzmann - gibbs - shannon entropy which is an expectation value of @xmath4 is generally expressed as s_bg = ( 1/p_i ) k  _",
    "i p_i ( - p_i ) , [ entr_bgs ] where @xmath5 represents the probability and @xmath6 is a constant . a new form of entropy based on the natural logarithm was proposed in [ @xcite ] considering @xmath7 as the effective probability i.e. , @xmath8 to take into account incomplete information .",
    "the entropy thus defined s = ( 1/p_i ) _ q _ i p_i^q ( - p_i ) , [ entr_fractal ] makes use of the @xmath9-expectation given below : o _ q = _",
    "i p_i^q o. [ q_expec ] the @xmath9-expectation value ( [ q_expec ] ) characterizes incomplete normalization [ @xcite ] which is known to occur in complex systems . later to account for the mixing which occurs due to interactions between the various states of the system , the same form of the entropy but with the regular conditions on the probabilities i.e. , @xmath10 was discussed in [ @xcite,@xcite ] .",
    "the boltzmann - gibbs - shannon entropy can also be defined through the equation s_bg = - _ i p_i^x_x=1 . replacing the ordinary derivative by the weyl fractional derivative",
    "a new entropy was obtained by ubriaco in [ @xcite ] .",
    "the functional form of the entropy which is an expectation value of @xmath11 reads : s = _",
    "i p_i ( - p_i)^q ( ( 1/p_i))^q .",
    "[ entr_fractional ] a salient feature of the fractal entropy ( [ entr_fractal ] ) and the fractional entropy ( [ entr_fractional ] ) is that they are functions of the ordinary logarithm unlike the other generalized entropies [ @xcite,@xcite,@xcite ] which are defined through the use of deformed logarithms .    inspired by fractal entropy ( [ entr_fractal ] ) and the fractional entropy ( [ entr_fractional ] ) we propose a two parameter generalization of the boltzmann - gibbs - shannon entropy s_q , q^ ( p_i)= k ( ( 1/p_i))^q^ _ q k _",
    "i=1^w p_i^q ( - p_i)^q^ k_i=1^w p_i^q()^q^ , [ 2p_entr ] where @xmath6 is a generalization of the boltzmann constant and @xmath9 and @xmath12 are the parameters which are used to generalize the bgs entropy .",
    "the entropy ( [ 2p_entr ] ) can be considered as a fractional entropy in a fractal phase space in which the parameter @xmath9 comes from the the fractal nature and the parameter @xmath12 is from the fractional aspect .    in the @xmath13",
    "limit the entropy ( [ 2p_entr ] ) reduces to the fractional entropy ( [ entr_fractional ] ) .",
    "similarly we recover the fractal entropy ( [ entr_fractal ] ) in the limit @xmath14 and the bgs entropy when both the parameters attain the value of unity .",
    "a very interesting limiting case of the ( [ 2p_entr ] ) occurs when we set @xmath15 s_q ( p_i)= k _",
    "i=1^w p_i^q(-p_i)^q k _",
    "i=1^w p_i^q()^q k _ i=1^w ( s_i^b)^q , [ ub_fr_entr ] where @xmath16 is the single particle boltzmann entropy .",
    "the one parameter entropy ( [ ub_fr_entr ] ) is the sum of biased single particle boltzmann entropy . at this juncture",
    "we would like to make a remark about the boltzmann entropy and the gibbs entropy .",
    "an explanation in [ @xcite ] states that the boltzmann entropy is the @xmath17 sum of the entropy calculated from the one particle distribution , whereas the gibbs entropy is computed directly from the @xmath17-particle distribution .",
    "this implies that the boltzmann entropy and the gibbs entropy are the same only when the systems are noninteracting .",
    "looking into equation ( [ ub_fr_entr ] ) from this point of view we realize that this entropy can be understood in a similar setting , i.e. , the one parameter entropies are biased by a parameter @xmath9 and this bias may be due to the presence of interactions .",
    "such a behaviour strongly resembles the characteristics of complex systems in which the behaviour of the total system is different from the single particle system due to presence of interactions .",
    "so we assume that the entropy ( [ ub_fr_entr ] ) described above may be a strong candidate in describing complex systems .",
    "below we present the two parameter generalization of the shannon - kinchinn axioms .",
    "let @xmath18 be an @xmath19-dimensional simplex as defined below _",
    "n = \\{(p_1 ,  , p_n ) p_i 0 , _",
    "i=1^n p_i = 1 } , in which the two parameter entropy ( [ 2p_entr ] ) satisfies the following axioms . + _",
    "( i ) continuity : _ the entropy @xmath20 is continuous in @xmath18 . + _",
    "( ii ) maximality : _ for any @xmath21 and any @xmath22 s_q , q^(p_1 , ",
    ", p_n ) s_q , q^ ( ,  , ) .",
    "_ ( iii ) expansibility : _",
    "s_q , q^(p_1, .... ,p_n,0 ) = s_q , q^(p_1, .... ,p_n ) .",
    "_ ( iv ) generalized shannon additivity : _ s_q ,",
    "q^(p_ij ) & = & ( _ i s_q , q^(p_i ) _",
    "j(p(j|i))^q + _ i p_i^q _ js_q , q^(p(j|i ) ) [ gen_shn_2p ] + & & + _ k=1 ( _ i s_q , k(p_i ) _",
    "js_q , q^-k ( p(j|i ) ) + _ i s_q , q^-k(p_i ) _",
    "js_q , k ( p(j|i ) ) ) ) where s_q , q^(p_i)= p_i^q ( -p_i)^q^ s_q , q^(p_i)= s(p_i ) . the factor @xmath23 appearing in ( [ gen_shn_2p ] ) is the conditional probability i.e. , probability of occurence of a @xmath24 event when a particular @xmath25 event has occured .",
    "the two parameter entropic functional ( [ 2p_entr ] ) has an extremum at @xmath26 and the second derivative w.r.t @xmath5 is _ p_i= e^-q / q^ = - q^ ( ) ( ) ^q^-2 . [ 2p_2deriv_p ] from ( [ 2p_2deriv_p ] )",
    "it can be observed that the second derivative of the entropy is uniformly @xmath27ve in the region @xmath28 implying that the entropic functional is uniformly concave for @xmath29 .",
    "the various limiting cases arising out of the two parameter entropy _ viz _ the fractal entropy ( [ entr_fractal ] ) , the fractional entropy ( [ entr_fractional ] ) and the one parameter entropy ( [ ub_fr_entr ] ) are also concave only when the deformation parameters are greater than zero .",
    "we illustrate the concavity of the entropic function through a set of plots shown above .",
    "let @xmath30 be the probability of occurence of a joint event , in which @xmath5 and @xmath31 are probability of occurence of the individual events .",
    "the two parameter entropy corresponding to the joint probability @xmath30 can be written as s_q , q^(p_ij ) = _ i , j p_ij^q(-p_ij)^q^ = _ i , j ( p(j|i))^q p_i^q ( -(p_i  p(j|i ) ) ) ^q^. [ entr_jp ] expanding the logarithm in the above equation using the binomial theorem and isolating the @xmath32 term we arrive at s_q , q^(p_ij ) = _ i s_q , q^(p_i ) _ j ( p(j|i))^q + _ k=1 _ i s_q , q^-k(p_i ) _ j s_q , k(p(j|i ) ) , [ bino_exp_1 ] where @xmath33 denotes the binomial coefficients . since",
    "the system is symmetric in @xmath5 and @xmath34 , the binomial expansion of eqn ( [ entr_jp ] ) can be written in an equivalent form as s_q , q^(p_ij)= _ i p_i^q _ j s_q , q^(p(j|i ) ) + _ k=1 _",
    "i s_q , k(p_i ) _ j s_q , q^-k(p(j|i ) ) .",
    "[ bino_exp_2 ] adding ( [ bino_exp_1 ] ) and ( [ bino_exp_2 ] ) the modified form of shannon additivity is obtained s_q , q^(p_ij ) & = & ( _",
    "i s_q , q^(p_i ) _ j(p(j|i))^q + _",
    "i p_i^q _ js_q , q^(p(j|i ) ) [ gen_shn_addt ] + & & + _ k=1 ( _ i s_q , k(p_i ) _",
    "js_q , q^-k ( p(j|i ) ) + _ i s_q , q^-k(p_i ) _",
    "js_q , k ( p(j|i ) ) ) ) when the two events are independent i.e. , the joint probability obeys the relation + @xmath35 eqn ( [ gen_shn_addt ] ) simplifies into s_q , q(p_ij ) & = & ( s_q , q^(p_i ) + s_q , q^(p_j ) - m_j(q ) s_q , q^(p_i ) - m_i(q ) s_q , q^(p_j ) + & & + _ k=1 ( s_q , k(p_i ) s_q , q^-k(p_j ) + s_q , q^-k(p_i ) s_q , k(p_j ) ) ) .",
    "[ 2p_psd_addt ] where @xmath36 is the mixing between the various states and occurs due to the fractal nature of the phase space .",
    "the shannon additivity relations corresponding to the various limiting cases of our two parameter entropy ( [ 2p_entr ] ) are listed below for the sake of completeness .",
    "+ _ special cases corresponding to the various one parameter entropies : _   + _ ( i ) _ in the @xmath37 limit the generalized shannon additivity corresponding to the fractional entropy ( [ entr_fractional ] ) is obtained s_q^(p_ij ) & = & ( _ is_q^(p_i ) + _ i p_i _ j s_q^",
    "( p(j|i ) ) + & & + _ i _ j_k=1 ( s_k(p_i)s_q^-k ( p(j|i ) ) + s_q^-k(p_i)s_k ( p(j|i ) ) ) ) . [ frac_shn_addt ] when the joint probability of system @xmath30 obeys the relation @xmath35 , we recover the pseudoadditivtiy relation proved in [ @xcite ] . + _",
    "( ii ) _ the generalized shannon additivity corresponding to the fractal entropy ( [ entr_fractal ] ) is recovered in the @xmath14 limit s_q(p_ij)= _",
    "i p_i^q _ j s_q ( p(j|i ) ) + _ i s_q(p_i ) _ j ( p(j|i))^q , [ fractal_shn_addt ] and the pseudoadditivity relation corrresponding to this entropy given in [ @xcite ] can be obtained under the condition @xmath35 . + _ ( iii ) _ the generalized shannon additivity relation corresponding to the one parameter entropy ( [ ub_fr_entr ] ) in the @xmath38 limit has the form s_q(p_ij ) & = & ( _ i s_q(p_i ) _ j ( p(j|i))^q + _ i_j _ k=1 ( p_i^k ( p(j|i))^q - k s_q - k(p_i)s_k ( p(j|i ) ) + & & + p_i^q - k ( p(j|i))^k s_k(p_i)s_q - k ( p(j|i ) ) ) + _ i p_i^q _ j s_q(p(j|i ) ) ) .",
    "[ 1p_shn_addt ] imposing the condition @xmath39 we can get the following pseduoadditivity relation s_q(p_ij ) & = & ( s_q(p_i)+s_q(p_j ) - m_j(q ) s_q(p_i ) - m_i(q ) s_q(p_j ) + & & + _ k=1 ( s_q - k(p_i ) s_k(p_j ) ( 1-m_i(k))(1-m_j(q - k ) ) + & & + s_k(p_i ) s_q - k(p_j)(1-m_i(q - k))(1-m_j(k ) ) ) ) . [ 1p_pseud_addt ]      in this subsection",
    "we prove the uniqueness of the two parameter entropy which obeys the modified form of the shannon additivity given in ( [ gen_shn_addt ] ) . it can be noticed that ( [ gen_shn_addt ] ) is a symmetrized combination of the following two equations _ i , j s_q , q^(p_ij ) & = & _ i , j s_q , q^(p_i ) ( p(j|i))^q + _ k=1 _ i , j s_q , q^-k(p_i ) s_q , k(p(j|i ) ) [ bino_exp_11 ] + _ i , j s_q , q^(p_ij ) & = & _ i , j",
    "p_i^q s_q , q^(p(j|i ) ) + _ k=1",
    "_ i , j s_q , k(p_i ) s_q , q^-k(p(j|i ) ) .",
    "[ bino_exp_21 ] we come to this conclusion , since in ( [ gen_shn_addt ] ) there is a @xmath40 symmetry between the first and the second term and also between the two terms within the @xmath6 summation .",
    "since the _ lhs _ in ( [ bino_exp_11 ] ) and ( [ bino_exp_21 ] ) are equal , the _ rhs _ of these equations should also have matching individual terms .",
    "this implies that @xmath41 and also that the entropic function @xmath42 can be separated in the form of @xmath43 . since we already know that @xmath44 what remains is to find the functional form of @xmath45 . using the separable form of the entropy and the structure of @xmath46 in ( [ bino_exp_11 ] ) we arrive at _ i , j p_ij^q _ q^(p_ij ) = _ i , j",
    "p_ij^q _ q^(p_i ) + _ k=1 _ i , j p_ij^q _ q^-k(p_i ) _ k(p(j|i ) ) .",
    "[ unq_sep_entr ] this can be rewritten in the following form _",
    "i , j p_ij^q _ q^(p_ij ) = _ k=0 _ i , j p_ij^q _ q^-k(p_i ) _",
    "k(p(j|i ) ) .",
    "[ unq_bino_exp ] comparing the coefficients of @xmath47 we get _",
    "q^(p_ij ) = ( ( p_i ) + ( p(j|i)))^q^. [ unq_ln_form ] the only function which satisfies the form shown above is the logarithm . the entropy is a positive function , whereas the probabilities can take only the values @xmath48 , so this leads to the conclusion that @xmath49 .",
    "combining the two parts of the entropy @xmath50 and @xmath45 we get ( [ 2p_entr ] ) the form of the entropy .",
    "finally for the sake of completeness we define the conditional entropy of a pair of discrete random variables @xmath51 with a joint distribution @xmath52 as s_q , q^ ( x y ) = _ x , y p(x , y)^q ( -p(x|y))^q^ , where @xmath53 denotes the conditional probability .",
    "divergence measures play an important role in information theoretic analysis of any entropy , since the probability distribution of a random variable can not always be found exactly and also due to the reason that sometimes it is necessary to find the difference between two distributions .",
    "for the shannon entropy several such measures like the kullback liebler relative entropy , jensen shannon divergence , etc . , have been introduced and investigated in detail . in this section",
    "we define these measures for the two parameter entropy proposed in the previous section .",
    "if @xmath54 and @xmath55 be any two probability distributions , the two parameter relative entropy corresponding to these distributions is defined as d_q , q^ ( ) = _ i _ i^q ( ) ^q^ , [ rel_entr_2p ] where @xmath9 and @xmath12 are the generalizing parameters . in the limit @xmath13",
    "we recover the fractional relative entropy and in the limit @xmath14 the fractal relative entropy can be obtained . in the case where @xmath56 we obtain the kullback relative entropy corresponding to the one parameter entropy described in ( [ ub_fr_entr ] ) .",
    "when both the parameters are set to unity we get back the kullback relative entropy .    below we list the properties of the two parameter entropy and prove them . + _ ( i ) nonnegativity : _",
    "@xmath57 . + _ ( ii ) continuity : _ @xmath58 is a continuous function for the 2n variables .",
    "( iii)symmetry : _ the relative entropy is symmetric under the simultaneous exchange of a pair of variables in the distributions @xmath59 and @xmath60 & & d_q , q^(_1 ,  , _ j ,  , _ k ,  , _",
    "n _ 1 ,  , _ j ,",
    " , _ k ,  , _",
    "n ) + & & = d_q , q^(_1 ,  , _ k ,  , _ j ,  , _",
    "n _ 1 ,  , _ k ,  , _ j ,  , _",
    "n ) . [ relentr_symm ] _ ( iv ) possibility of extension : _",
    "d_q , q^(_1 ,  , _ n,0 _ 1 ,  , _",
    "n,0 ) = d_q , q^(_1 ,  , _ n _ 1 ,  , _",
    "n ) . [ rel_entr_ext ] _ ( v ) pseudoadditivity : _",
    "d_q , q^(^(1 ) ^(2 ) ^(1 ) ^(2 ) ) & = & ( d_q , q^(^(1 ) ^(1 ) ) - m_j(q ) d_q , q^(^(1 ) ^(1 ) ) + & & + d_q , q^(^(2 ) ^(2 ) ) - m_i(q ) d_q , q^(^(2 ) ^(2 ) ) + & & + _ k ( d_q , k(^(1 ) ^(1 ) ) d_q , q^-k(^(2 ) ^(2 ) ) + & & + d_q , q^-k(^(1 ) ^(1 ) ) d_q , k ( ^(2 ) ^(2 ) ) ) ) .",
    "where @xmath61 and + @xmath62 .",
    "+ _ ( vi ) joint @xmath9-convexity : _",
    "d_q , q^(^(1 ) + ( 1 - ) ^(2 ) ^(1 ) + ( 1 - ) ^(2 ) ) ^q d_q , q^ ( ^(1 ) ^(1 ) ) + ( 1 - ) ^q d_q , q^ ( ^(2 ) ^(2 ) ) .",
    "[ q_convex ] _ proof : _ + the convexity of the relative entropy function ( [ rel_entr_2p ] ) proves the first axiom .",
    "axioms _ ( ii ) _ , _ ( iii ) _ and _ ( iv ) _ can be trivially proved . the expression for pseudoadditivity in _ ( v ) _ follows from direct calculation .",
    "the relative entropy satisfies the joint @xmath9-convexity stated in axiom _ ( vi ) _ proposed in reference [ @xcite ] . to prove the joint @xmath9-convexity we use the generalized form of the log - sum inequality _",
    "i=1^n _ i^q ( ) ^q^ ( _ i=1^n _ i)^q ( ) ^q^. [ q_log_sum ] this inequality can be obtained from @xmath9-generalization of jensen inequality proposed in [ @xcite ] .",
    "since the relative entropy ( [ rel_entr_2p ] ) is not symmetric in @xmath59 and @xmath60 , we define the following symmetric measure _ q , q^ ( , ) = ( d_q , q^ ( ) + d_q , q^ ( ) ) , [ gen_jkld ] which shares most of the properties of the relative entropy . it can be noticed that the kullback liebler relative entropy ( [ rel_entr_2p ] ) is undefined if the distribution @xmath63 and @xmath64 .",
    "similarly the symmetric form of the relative entropy ( [ gen_jkld ] ) is undefined if any of the distribution vanishes .",
    "this implies that the distribution @xmath65 has to be continuous with respect to the distribution @xmath66 for the measure ( [ rel_entr_2p ] ) to be defined . for the case of the symmetric measure",
    "the distributions @xmath65 and @xmath66 have to continuous with respect to each other .",
    "to overcome this a modified form of the relative entropy is defined between the distribution @xmath59 and a distribution which is a symmetric sum of both @xmath66 and @xmath65 . the mathematical expression corresponding to the modified relative entropy is _ q , q^ ( , ) = d_q , q^ ( ( + ) /2 ) = _ i _ i^q ( ) ^q^. [ symsm_kl_relentr ] the alternative form of relative entropy proposed in the above equation is defined even when @xmath60 is not absolutely continuous with respect to the distribution @xmath59 . though it satisfies all the properties of the kullback liebler relative entropy it is not a symmetric measure .",
    "so , a symmetric measure based on ( [ symsm_kl_relentr ] ) is defined as follows : _ q , q^ ( , ) = ( _ q , q^ ( ) + _ q , q^ ( ) ) .",
    "[ gen_jkld_mod ] which is a generalization of the jensen shannon divergence measure [ @xcite ] corresponding to the two parameter entropy ( [ 2p_entr ] ) defined in the previous section .    the symmetric measure based on the relative entropy ( [ gen_jkld ] ) and the generalized jensen shannon divergence measure ( [ gen_jkld_mod ] ) satisfies the following properties j_q",
    ", q^ ( , ) & & 0 , + j_q , q^ ( , ) & = & j_q , q^ ( , ) , + j_q , q^ ( , ) & = & 0 = , [ jsd_prop ] where @xmath67 can be either @xmath68 or @xmath69 .",
    "the symmetric form of the relative entropy and the jensen shannon divergence measure are related via the expression _",
    "q , q^ _ q , q^ , [ jsd_modre_rel ] which clearly shows that the upper bound to the jensen shannon divergence is given by the symmetric form of the relative entropy .",
    "a similar relationship also exists between the ( [ symsm_kl_relentr ] ) and ( [ rel_entr_2p ] ) in which the relative entropy defines the upper bound of the modified relative entropy .",
    "the fisher information measure for a continuous random variable @xmath70 with probability distribution @xmath71 is defined as : i = p(x ) ( ) ^2 dx = ( ) ^2 = ( p(x ) ) ^2 .",
    "[ bg_fim ] in reference [ @xcite ] , the fisher information was obtained from the kullback liebler relative entropy in the following manner : the relative entropy between a uniform probability distribution @xmath71 and its shifted measure @xmath72 is constructed .",
    "the integrand is then expanded as a taylor series in the shift @xmath73 upto second order from which the fisher information measure is recognized .",
    "analogously we proceed to derive the @xmath74-generalized fisher information measure using the two parameter generalized relative entropy proposed in the previous subsection .",
    "the two parameter relative entropy between the measure @xmath71 and its shifted measure @xmath75 is d_q , q^ ( p(x ) p(x+ ) ) = ( p(x))^q ( -)^q^ dx . in the above equation",
    ", the function @xmath76 is expanded upto second order in @xmath73 and the resulting expression is written in terms of a binomial series as follows : d_q , q^ ( p(x ) p(x+ ) ) = _ k=0^ ( ) ^q^-k ^k dx .",
    "considering the first two lower order terms in @xmath73 d_q , q^ ( p(x ) p(x+ ) ) & = & ( p(x))^q ( ^q^ ( ) ^q^ + & & + ( ( ) ^q^+1 - ) ) dx , and in comparison with the method adopted in ref .",
    "[ @xcite ] , we obtain the two parameter generalization of the fisher information measure i_q , q^ = ( p(x))^q - q^-1  ( ) ^q^+1 dx . [ 2p_fim ] along the lines of eqn ( [ bg_fim ] ) the above expression can be defined through a @xmath9-expectation value as i_q , q^ = ( ) ^q^+1 _ q = ( p(x ) ) ^q^+1 _ q. [ qexpec_2p_fim ] the fisher information measure ( [ qexpec_2p_fim ] ) make use of the @xmath9-expectation and in the @xmath14 limit it reduces to the expression obtained in [ @xcite ] .      the relative fisher information measure between two probability distributions @xmath77 and @xmath78 is given by _",
    "ri_q , q^ ( p_1,p_2 ) = ( ) ^q^+1 _ q = ( p_1(x))^q ( ) ^q^+1 dx .",
    "[ rel_fi_def ]",
    "the above relation is not symmetric and so we define _ q , q^(p_1,p_2 ) = ( _ ri_q , q^ ( p_1,p_2 ) + _ ri_q , q^ ( p_2,p_1 ) ) , [ rel_gfd ] which is a symmetric extension of ( [ rel_fi_def ] ) . the disadvantage with these measures is the following : equation ( [ rel_fi_def ] ) requires the distribution @xmath78 to be a continuous function of @xmath77 for a given @xmath79 and relation ( [ rel_gfd ] ) requires the distributions @xmath77 and @xmath78 continuous with respect to each other .",
    "to surmount this the relative fisher information is defined between the distribution @xmath77 and a new distribution between @xmath80 and the expression for this modified form reads : _ q , q^(p_1,p_2 ) = _ ri_q , q^ ( p_1 , ) = ( p_1(x))^q ( ) ^q^+1 dx .",
    "[ rel_fi_def_mod ] a two parameter generalization of the recently proposed [ @xcite ] jensen fisher divergence measure , can be constructed via a symmetric combination of the modified form of the relative fisher information .",
    "the form of the generalized jensen fisher divergence measure so constructed is _ q , q^(p_1,p_2 ) = = .",
    "[ rel_gfd_mod ] the jensen fisher measure ( [ rel_gfd_mod ] ) is convex and symmetric and vanishes only when both the probabilities @xmath77 and @xmath78 are identical everywhere .",
    "finally we discuss the relevant limiting cases : all the information measures defined in this section reduce to the corresponding measures obtained through the use of the shannon entropy in the @xmath81 . in the @xmath13",
    "limit the information measures relevant to the fractional entropy is recovered .",
    "the @xmath14 limit leads to the information measures of the fractal entropy .",
    "the canonical probability distribution @xmath5 can be obtained by optimizing the entropy subject to the the norm constraint and the energy constraint . adopting a similar procedure for our two parameter entropy ( [ 2p_entr ] ) we construct the functional l= ( p_i ; q , q^ ) - ( _ ip_i-1 ) -(_ip_i _ i - e ) , ( p_i ; q , q^ ) = p_i^q ( - p_i)^q^ , [ optm_entr ] where @xmath82 and @xmath83 are the lagrange s multiplier and @xmath84 is the energy eigenvalue and @xmath85 is the internal energy . employing the variational procedure we optimize the functional in ( [ optm_entr ] ) with respect to the probability to get = ^ ( p_i;q , q^ ) - ( + _ i ) , [ max_entr ] where @xmath86 is ^ ( p_i;q , q^ ) = q p_i^q-1 ( -p_i)^q^ - q^ p_i^q-1(-p_i)^q^-1 .",
    "[ deriv_entr_func ] when the functional @xmath87 attains a maximum its variation wrt @xmath5 is zero and using this in ( [ max_entr ] ) yields the inverse of the probability distribution q p_i^q-1 ( -p_i)^q^ - q^ p_i^q-1(-p_i)^q^-1 = ( + _ i ) .",
    "[ opt_func_1 ] inversion of the relation ( [ opt_func_1 ] ) to obtain the probability distribution is not analytically feasible , so we adopt a different method to derive the distribution . since we have already set @xmath88 to zero , we can integrate ( [ max_entr ] ) to get ( p_i;q , q^ ) = ( + _ i ) p_i .",
    "[ phi_alpha_beta ] substituting the entropic expression ( [ 2p_entr ] ) in ( [ phi_alpha_beta ] ) and comparing this with the equation for lambert s @xmath0-function @xmath89 we obtain the relation for the probability p_i = ( ) ^ , z =  ( + _ i)^. the factor @xmath90 is the lambert s @xmath0-function also known as the product log function . for real @xmath91 the function contains two branches denoted by @xmath92 and @xmath93 . the branch @xmath92 satisfies the condition that @xmath94 and is generally known as the principal branch of the @xmath0-function .",
    "when @xmath95 we have the @xmath93 branch .",
    "the lambert s @xmath0-function occurs naturally in both classical [ @xcite ] and quantum statistical mechanics [ @xcite,@xcite ] as well as in nonequilibrium statistical mechanics [ @xcite ] . very recently",
    "its connection to the field of generalized statistical mechanics has been established through the following works [ @xcite,@xcite,@xcite ] , which our current result emphasizes .",
    "_ lesche stability : _ + a stability criterion was proposed by lesche [ @xcite,@xcite ] to study the stabilities of rnyi and the boltzmann gibbs entropy .",
    "the motivation for this criterion goes as follows : an infinitesimal change in the probabilities @xmath5 should produce an equally infinitesimal changes in an observable . if @xmath96 and @xmath97 be two probability distributions , lesche stability requires that @xmath98 we can find a @xmath99 such that _",
    "j=1^n p_i - p_j^ < .",
    "[ lesche_stab ] using ( [ lesche_stab ] ) a simple condition was derived in [ @xcite ] for any generalized entropy maximized by a probability distribution .",
    "this condition which is widely used to check the lesche stabilities of generalized entropies reads : < c _",
    "j=1^n p_i - p_j^ , [ ls_chck_cond ] where the constant @xmath100 is c= .",
    "[ ls_const_condt ] the function @xmath101 is the inverse probability distribution obtained in ( [ opt_func_1 ] ) . in order to compute the constant",
    "we integrate the inverse probability distribution with respect to the probability _",
    "0 ^ 1 f^-1(p)dp= _ 0 ^ 1 ( q p^q-1(-p)^q^- q^ p^q-1(-p)^q^-1 ) dp . [ pinv_int ]",
    "the _ rhs _ of ( [ pinv_int ] ) consists of two integrals in @xmath96 and by using the transformation @xmath102 they are obtained in terms of the gamma function .",
    "a simple calculation helps us to see that these two integrals are in fact the same and so @xmath103 .",
    "similarly it can also be noticed that the @xmath104 due to occurence of the natural logarithm .",
    "so we finally get the value of @xmath100 as c= = 1 , [ lsc_fv ] which leads to the conclusion that for our case @xmath105 and so the criterion for lesche stability is satisfied .    _",
    "thermodynamic stability : _ + the thermodynamic stability conditions of the boltzmann - gibbs entropy can be derived from the maximum entropy principle and its corresponding additivity relation . in references [ @xcite,@xcite ] it has been shown that concavity alone does not guarantee thermodynamic stability for the two parameter entropy ( [ 2p_entr ] ) .",
    "we derive the stability conditions for the two parameter entropy _  la _ the method developed in [ @xcite ] .",
    "the pseudoadditive relation for the two parameter entropy reads : s_q , q^(a , b ) & = & ( s_q,0(a ) s_q , q^(b ) + s_q,0(b ) s_q , q^(a ) + _ k=1 ( s_q , k(a ) s_q , q^-k(b ) + & & + s_q , q^-k(a ) s_q , k(b ) ) ) .",
    "[ psd_addt ] considering an isolated system comprising of two identical subsystems of energy @xmath106 in equilibrium , the total entropy of the system would be @xmath107 . allowing for an exchange of energy @xmath108 from one subsystem to the other subsystem the total entropy changes as @xmath109 ,",
    "whose pseudoadditive relation following ( [ psd_addt ] ) is s_q , q^ ( u+u , u - u ) & = & ( s_q,0(u+u ) s_q , q^(u - u ) + & & + s_q,0(u - u ) s_q , q^(u+u ) + & & + _ k=1 ( s_q , k(u+u ) s_q , q^-k(u - u ) + & & + s_q , q^-k(u+u ) s_q , k(u - u ) ) ) .",
    "[ psd_addt_eexch ] similarly , the pseudoadditive relation corresponding to @xmath107 obtained using ( [ psd_addt ] ) is s_q , q^(u , u ) & = & ( s_q,0(u ) s_q , q^(u ) + s_q,0(u ) s_q , q^(u ) + _ k=1 ( s_q , k(u ) s_q , q^-k(u ) + & & + s_q , q^-k(u ) s_q , k(u ) ) ) .",
    "[ psd_addt_entr ] from the maximum entropy principle we know that s_q , q^(u , u ) s_q , q^ ( u+u , u - u ) .",
    "[ max_entr_prp ] expanding ( [ psd_addt_eexch ] ) upto second order in @xmath108 and using the maximum entropy principle ( [ max_entr_prp ] ) , we get the following condition 0 & & s_q,0 - + _",
    "k=1 ( s_q , k + s_q , q^-k + & & - 2  ) .",
    "[ max_ent_con ] the two parameter entropy can be connected to its first and second derivatives via recurrence relations which be substituted in eqn .",
    "( [ max_ent_con ] ) to yield the simplified form 0 - ( 1-q ) ( ( 2^q^ - q ) s_q , q^ + q^ ( 2^q^ + 2q + 1 )  s_q , q^-1 + ( q^^2+q^ + 2^q^ ) s_q , q^-2 ) .",
    "[ therm_stab_con ] from ( [ therm_stab_con ] ) we notice that in the _ rhs _ , the first term is negative in the region @xmath110 due to the concavity conditions imposed on the entropy . from the rest of the terms we notice that the stability conditions will be respected when either @xmath111 and @xmath112 or @xmath113 and @xmath114 . under the limiting conditions of @xmath115",
    ", we recover the concavity condition for the boltzmann entropy which is also the thermodynamic stability condition for the boltzmann gibbs entropy .    _ generic example in the microcanonical ensemble : _ + an isolated system in thermodynamic equilibrium can be described via the microcanonical ensemble . in a microcanonical picture",
    "all the microstates are equally probable . under conditions of equiprobability",
    "i.e. , @xmath116 the two parameter entropy ( [ 2p_entr ] ) becomes s_q , q^ = k w^1-q ( w)^q^ , [ ub_entr_eqp ] where @xmath0 is the total number of microstates . in the limit",
    "@xmath13 the above expression ( [ ub_entr_eqp ] ) reduces to the microcanonical entropy derived from the fractal entropy ( [ entr_fractal ] ) . similarly in the @xmath14 limit we can obtain the entropic expression corresponding to the fractional entropy ( [ entr_fractional ] ) .",
    "when we set @xmath117 we can get the microcanonical entropy corresponding to the entropy in ( [ ub_fr_entr ] ) . for the entropy ( [ ub_entr_eqp ] ) the temperature is defined through the relation = = k  w^-q  ( w)^q^-1 ( q^ + ( 1-q ) w + q^ ) .",
    "[ temp_def_entr ] the definition of temperature corresponding to a generic class of systems for which the density of states @xmath0 is related to the energy via the expression @xmath118 is found to be = k f c^1-q",
    "e^(1-q)f-1 ( ce^f)^q^-1 ( q^+ ( 1-q ) ce^f ) . [ temp_gc ] an analytic inversion of ( [ temp_gc ] ) to obtain the energy as a function of temperature is not feasible .",
    "though the above illustration is given only for the microcanonical ensemble a direct extension of this method to include other kinds of adiabatic ensembles can be easily achieved .",
    "physical systems in which the behaviour of the total system can not be constructed from the properties of the individual components is generally defined as complex systems .",
    "several measures were proposed to quantify complexity of physical systems [ @xcite,@xcite,@xcite ] .",
    "one such measure is the disequilibrium based statistical measure of complexity popularly referred to as lmc ( lpez - ruiz , mancini and calbet ) complexity measure which was introduced in [ @xcite ] .",
    "this measure is based on the logic that there are two extreme situations in which we can find the simple systems , one is the perfect crystal in which the constituent atoms are symmetrically arranged and the other limit is the completely disordered system which is best characterized by an ideal gas in which the system can be found in any of the accessible states with the same probability .",
    "the available information is very little in the case of a perfect crystal and is maximum for the ideal gas .",
    "the amount of information in the system can be found using the boltzmann gibbs entropy @xmath119 .",
    "a new quantity called the disequilibrium @xmath120 was proposed which is the distance from the equiprobable distribution and is maximum in the case of the crystal and zero for an ideal gas .",
    "the product of these quantities was defined as the measure of complexity .",
    "this measures vanishes for both perfect crystal and the ideal gas .    for a system consisting of @xmath17 accessible states with a set of probabilities @xmath121 , obeying the normalization condition @xmath122 the complexity measure reads : c = s  , = _ i=1^n(p_i - ) ^2 .",
    "[ stat_compl ] the lmc measure of complexity was found to be a nonextensive quantity .",
    "a generalized measure of complexity was proposed in [ @xcite ] based on tsallis entropy with a view to absorb the nonadditive features of the entropy .",
    "similarly a statistical measure of complexity corresponding to the two parameter entropy ( [ 2p_entr ] ) is defined as follows : c_q , q^ = s_q , q^   ( k _ i^n p_i^q ( -p_i)^q^ ) ( _ i=1^n(p_i - ) ^2 ) .",
    "[ q_stat_compl ] in the limit @xmath81 we recover the lmc complexity measure proposed in [ @xcite ] .",
    "the lmc complexity measure corresponding to the fractal entropy and the fractional entropy are obtained in the @xmath14 and @xmath13 limits .",
    "when we let @xmath56 , the complexity measure corresponding to the entropy ( [ ub_fr_entr ] ) is recovered .    as an example",
    "let us consider a two level system with probabilities @xmath96 and @xmath123 .",
    "the expression for the entropy and the disequilibrium measure are as follows : s_q , q^(p ) = - ( p p + ( 1-p ) ( 1-p ) ) , ( p ) = 2 ( p-1/2)^2 .",
    "[ entr_diseq_cal ] the statistical complexity computed from these quantities c_q , q^(p ) = - 2 ( p p + ( 1-p ) ( 1-p ) ) ( p-1/2)^2 , [ compl_cal ] is plotted below for the sake of analysis .    from the plots we notice that the complexity measured using the boltzmann gibbs entropy , fractal entropy , fractional entropy and the two parameter entropy for the perfectly ordered state ( crystal ) goes to zero uniformly .",
    "the largest complexity is achieved for the two parameter entropy followed by the fractal entropy , fractional entropy and the boltzmann gibbs entropy .",
    "also , the entropic value for which the maximum value of complexity is reached differs for the different entropies . the complexity zero corresponding to the disordered state occurs at various values of the entropy for the different entropies and the bg entropy reaches the zero first , followed by the fractional , the fractal and the two parameter entropies . for the two parameter entropy , the largest complexity is achieved more quickly by varying the fractal index @xmath9 rather than the fractional index @xmath12 .",
    "also the zero complexity state corresponding to the disordered system is attained more quickly when the fractional index @xmath12 is greater than one .",
    "this measure can be extended to continuous probability distribution @xmath71 obeying the normalization condition @xmath124 . for these distributions",
    "the summation over the states in the entropic definition is replaced by a integration over @xmath79 .",
    "similarly the disequilibrium measure is @xmath125 .",
    "since there is a continuum of states , the number of states is very large and so the disequilibrium measure becomes @xmath126 .",
    "the two parameter generalization of the lmc complexity measure for the continuum case is c_q , q^ = s_q , q^   ( k _ - ^ ( p(x))^q ( -p(x))^q^ dx ) ( _ - ^ ( p(x))^2 dx ) .",
    "[ com_cont_dist ] for the purpose of illustration we consider the exponential distribution + @xmath127 and calculate the corresponding two parameter entropy and its disequilibrium measure s_q , q^ = ( q^+1,q ) , = .",
    "[ entr_deq_cont ] the lmc complexity found by substituting ( [ entr_deq_cont ] ) in ( [ com_cont_dist ] ) reads : c_q , q^ = ( q^+1,q ) , [ com_cont_fe ] where @xmath128 , is the incomplete gamma function in which the lower limit is replaced by a positive number . in the limit @xmath129",
    ", statistical complexity reduces to + @xmath130 .",
    "a new two parameter entropy based on the natural logarithm and generalizing both the fractal entropy and the fractional entropy is introduced .",
    "this encompasses an interesting limiting case , where the @xmath17-particle entropy can be expressed in terms of a sum of single particle biased boltzmann entropies .",
    "the generalized form of the shannon - khinchin axioms are proposed and verified for this new two parameter entropy .",
    "these axioms uniquely characterize our new entropy . the corresponding kullback liebler relative entropy is proposed and its properties are investigated . utilizing the relative entropy a generalization of the jensen - shannon divergence",
    "is also achieved . exploiting the relative entropy between a probability measure and its shift , we derive the generalized fisher information .",
    "also , we obtain generalized forms of the relative fisher information and the jensen - fisher information . the lesche stability , and the thermodynamic stability are verified for our entropy and we also find that the canonical probability distribution which optimizes our entropy can be expressed via the lambert s @xmath0-function . finally we introduce a generalization of the lmc complexity measure making use of our two parameter entropy and apply it to measure the complexity of a two level system .",
    "the results obtained indicate that there is a change in the complexity value with the dominant contribution coming from the fractal index @xmath9 .",
    "finally we also examine an exponential distribution as an example of a continuous probability distribution and compute the complexity measure .",
    "though we have introduced a new two parameter entropy based on natural logarithm , we do not know the specific systems where this can be applied .",
    "but from our investigations we assume that it will be of use in measuring complexity in fractal systems and systems which exhibit fractional dynamics in phase space . towards this end",
    "investigating the complexity of probability distributions corresponding to the fractional diffusion equation [ @xcite ] will be worth pursuing .",
    "one of the authors cr would like to acknowledge the use of library facilities at the institute of mathematical sciences ( imsc ) .",
    "99 a. rnyi , proceedings of the @xmath131 berkeley symposium on mathematics , statistics and probability , 547 ( 1960 ) .",
    "mittal , metrika * 22 * , 35 ( 1975 ) .",
    "b.d . sharma and i.j .",
    "taneja , metrika * 22 * , 205 ( 1975 ) . c. tsallis , j. stat .",
    "phys . * 52 * , 479 ( 1988 ) .",
    "b.j.c . cabral and c. tsallis , phys . rev .",
    "* e 66 * , 0615101(r ) ( 2002 ) .",
    "a. pluchino , v. latora and a. rapisarda , continuum mech . thermodyn . * 16 * , 245 ( 2004 ) .",
    "a.m. mariz and c. tsallis , _ long memory constitutes a unified mesoscopic mechanism consistent with nonextensive statistical mechanics _",
    ", arxiv no : 1106.3100 [ cond-mat.stat-mech ] q.a .",
    "wang , entropy * 5 * , 220 ( 2003 ) .",
    "ubriaco , phys . lett . * a 373 * , 2516 ( 2009 ) .",
    "ubriaco , phys . lett . * a 373 * , 4017 ( 2009 ) .",
    "r. lpez - ruiz , h.l .",
    "mancini and x. calbet , phys .",
    "lett . * a 209 * , 321 ( 1995 ) .",
    "f. shafee , _ a new nonextensive entropy _ , arxiv no : 0406044 [ nlin.ao ] ( 2004 ) .",
    "f. shafee , _ generalized entropy from mixing : thermodynamics , mutual information and symmetry breaking _ , arxiv no : 0906.2458 [ cond-mat.stat-mech ] ( 2009 ) .",
    "g. kaniadakis , physica * a 296 * , 405 ( 2001 ) .",
    "a. lavagno , a.m. scarfone and p. narayanaswamy , j. phys . *",
    "a * : math .",
    "theor * 40 * , 8635 ( 2007 ) .",
    "jaynes , am . j. phys .",
    "* 33 * , 391 ( 1965 ) .",
    "a.f.t martins , n.a .",
    "smith , e.p .",
    "xing , p.m.q .",
    "aguiar and m.a.t .",
    "figueiredo , journal of machine learning research * 10 * , 935 ( 2009 ) .",
    "j. lin , ieee transactions on information theory * 37 * , 145 ( 1991 ) .",
    "vstovsky , phys . rev . *",
    "e51 * , 975 ( 1995 ) .",
    "p. snchez - moreno , a. zarzo and j.s .",
    "dehesa , j. phys . *",
    "a * : math .",
    "* 45 * , 125305 ( 2012 ) .",
    "caillol , j. phys . *",
    "a * : math .",
    "theor . * 36 * 10431 ( 2003 ) .",
    "valluri , m. gil , d.j .",
    "jeffrey and s. basu , j. math . phys . *",
    "50 * , 102103 ( 2009 ) .",
    "j. tanguay , m. gil , d.j .",
    "jeffrey and s. basu , j. math . phys . *",
    "51 * , 123303 ( 2010 ) .",
    "e. lutz , am .",
    "* 73 * , 968 ( 2005 ) .",
    "s. asgarani and b. mirza , physica * a 387 * , 6277 ( 2008 ) .",
    "r. chandrashekar and j. segar _",
    "adiabatic thermostatistics of the two parameter entropy and the role of lambert s @xmath0-function in its applications _ arxiv no : 1210.5499 [ cond-mat.stat-mech ] f. pennini and a. plastino , physica * a 247 * , 559 ( 1997 ) .",
    "b. lesche , j. stat . phys . * 27 * , 419 ( 1982 ) .",
    "b. lesche , phys . rev . * e 70 * , 017102 ( 2004 )",
    ". s. abe , g. kaniadakis , a.m. scarfone , j. phys . *",
    "a * : math .",
    "gen . * 37 * , 10513 ( 2004 ) .",
    "t. wada , physica * a 340 * , 126 ( 2004 ) .",
    "a.m. scarfone and t. wada , phys . rev . *",
    "e 62 * , 026123 ( 2005 ) .",
    "s. lloyd and h. pagels , ann . phys . *",
    "188 * , 186 ( 1988 ) .",
    "j. shiner , m. davison and p.t .",
    "landsberg , phys . rev . * e 59 * , 1459 ( 1999 )",
    ". t. yamano , j. math . phys .",
    "* 45 * , 1974 ( 2004 ) .",
    "r. metzler and j. klafter , j. phys . *",
    "a * : math .",
    "theor . * 37 * r161 ( 2004 ) ."
  ],
  "abstract_text": [
    "<S> a two parameter generalization of boltzmann - gibbs - shannon entropy based on natural logarithm is introduced . </S>",
    "<S> the generalization of the shannon - kinchinn axioms corresponding to the two parameter entropy is proposed and verified . </S>",
    "<S> we present the relative entropy , jensen - shannon divergence measure and check their properties . </S>",
    "<S> the fisher information measure , relative fisher information and the jensen - fisher information corresponding to this entropy are also derived . </S>",
    "<S> the canonical distribution maximizing this entropy is derived and is found to be in terms of the lambert s @xmath0 function . also the lesche stability and </S>",
    "<S> the thermodynamic stability conditions are verified . </S>",
    "<S> finally we propose a generalization of a complexity measure and apply it to a two level system and a system obeying exponential distribution . </S>",
    "<S> the results are compared with the corresponding ones obtained using a similar measure based on the shannon entropy .    a fractional entropy in fractal phase space : properties and characterization + r. chandrashekar@xmath1 , c. ravikumar@xmath2 and j. segar@xmath3    _ @xmath1the institute of mathematical sciences , + c.i.t campus , taramani , + chennai 600 113 , india + _ _ @xmath2 department of theoretical physics , + university of madras , + maraimalai campus , guindy , + chennai 600 025 , india + _ _ @xmath3ramakrishna mission vivekananda college + mylapore + chennai 600 004 , india . </S>",
    "<S> _    pacs number(s ) : + keywords : entropy , relative entropy , fisher information measure , stability conditions , complexity measures . </S>"
  ]
}