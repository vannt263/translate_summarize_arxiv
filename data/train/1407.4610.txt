{
  "article_text": [
    "written texts show the remarkable feature that the rank ordered distribution of word frequencies follows an approximate power law @xmath0 where @xmath1 is the rank that is assigned to every word in the text . for most texts , regardless of language , time of creation , genre of literature , its purpose , etc .",
    ", one finds that @xmath2 , which is referred to as zipf s law @xcite . in fig .",
    "1 the word frequency is shown for darwin s text , _ the origin of species_. the quest for an understanding of the origin of this statistical regularity is going on for almost a century .",
    "zipf himself offered a qualitative explanation based on the efforts invested in communication events by a sender and a receiver @xcite .",
    "these ideas were later formalised within an information - theoretic framework @xcite .",
    "the first quantitative model based on linguistic assumptions about text generation has been proposed by h. simon @xcite .",
    "the model assumes that as context emerges in the generation of a text , words that have already appeared in the text are favoured over others . by the simple assumption that words that have previously appeared are added to the text with a probability proportional to their previous appearance ( preferential attachment ) , and assuming that words that have so far not appeared are added at a constant rate , it is possible to derive zipf s law , given the latter rate is low .",
    "this preferential attachment model has been refined by implementing the empirical fact that the rate of appearance of new words decreases as the length of texts increases @xcite .",
    "it has been shown in classical works that random typewriting models can lead to zipf - like distributions of word frequencies @xcite",
    ". however these works are based on unrealistic assumptions on word - length distributions and lead to unstructured and uninterpretable texts . however",
    ", as we will show , syntactic structure , jointly with discourse generation mechanisms , may play an essential a role in the origin of zipf s law in a realistic context .",
    "it is important to stress that the detailed statistical study of language properties does not end here ; important work beyond zipf s law has been put forward , see e.g. @xcite .",
    "recent studies deal with the detailed dependence of the scaling exponents on the length of the body of text under study @xcite .",
    "zipf s law is not limited to word frequencies but appears in countless , seemingly unrelated , systems and processes @xcite .",
    "just to mention a few , it has been found in the statistics of firm sizes @xcite , city sizes @xcite , the genome @xcite , family names @xcite , income @xcite , financial markets @xcite , internet file sizes @xcite , or human behaviour @xcite , for more examples see @xcite .",
    "there has been tremendous efforts to understand the origin of zipf s law , and more generally the origin of scaling in complex systems .",
    "there are three main routes to scaling : multiplicative processes @xcite , preferential processes @xcite , and self - organised criticality @xcite .",
    "several other mechanisms that are more or less related to these basic routes to scaling have been proposed e.g. in @xcite .",
    "recently a fourth , independent route to scaling has been introduced on the basis of stochastic processes that reduce their potential outcomes ( sample - space ) over time @xcite .",
    "these are history - dependent random processes that have been studied in different contexts in the mathematical literature @xcite , and more recently in the context of scaling laws @xcite .",
    "an example of sample - space reducing processes is the following .",
    "think of a set of @xmath3 dice where die number 1 has 1 face , die number 2 has two faces ( coin ) , die number 3 has three faces , and so on . die number @xmath3 has @xmath3 faces",
    ". start by picking one of the @xmath3 dice at random , say dice number @xmath4 .",
    "throw it and record the obtained face value , which was say @xmath5 .",
    "then take die number @xmath6 throw it , get @xmath7 , record @xmath7 , take die number @xmath8 , throw it , etc .",
    "keep throwing dice in this way until you throw 1 for the first time .",
    "since there is no die with less than 1 faces , the process ends here .",
    "the sequence of recorded face values in the above prescription @xmath9 , is obviously strictly ordered or nested , @xmath10 . in @xcite",
    "it was shown rigorously that if this process is repeated many times , the distribution of outcomes ( face values @xmath11 ) is an exact zipf law , i.e. the probability to observe a face value @xmath12 in the above process ( sequence of throws ) is exactly @xmath13 , given we start with @xmath3 dice .",
    "note that it is necessary to keep @xmath3 fixed during the repetitions of the process to obtain the exact zipf law .",
    "if @xmath3 varies during the repetitions , clearly zipf scaling is present asymptotically for high ranks , however due to the mixing of different @xmath3 , deviations from the exact zipf law will appear for low ranks .",
    "more formally , every die @xmath3 has a sample - space , denoted by @xmath14 , which is the number of potential outcomes , i.e. the number of faces of dice @xmath3 .",
    "throwing these dice in the above way gives rise to a sequence of nested sample - spaces @xmath15 the nestedness of sample - spaces in a history - dependent sequence is at the heart of the origin of scaling laws in this type of processes .",
    "for details see @xcite where it is also shown that if noise is added to the history - dependent processes , the scaling law , @xmath16 is obtained , where @xmath17 , is the noise level .",
    "in this paper we present a derivation of zipf s law of word frequencies , based on a simple model for sentence / discourse formation .",
    "the model is motivated by the observation that the process of forming a sentence  or more generally a discourse  is a history - dependent sample - space reducing process .",
    "words are not randomly drawn from the sample - space of all possible words , but are used in strict relations to each other .",
    "the usage of specific words in a sentence highly restricts the usage for consecutive words , leading to a nesting ( or sample - space reducing ) process , similar to the one described above .",
    "sample - space collapse in texts is necessary to convey meaningful information .",
    "otherwise , any interpretation , even in metaphoric or poetic terms , would become impossible .",
    "let us make the point more concrete with an example for the formation of a sentence , where both grammatical and contextual constraints ( that reduce sample - space ) are at work , fig .",
    "we form the sentence : `` the wolf howls in the night '' . in principle the first word ",
    "the wolf \" ( ignoring articles and prepositions for the moment ) can be drawn from all possible words .",
    "assume there exist @xmath3 possible words , and denote the respective sample - space by @xmath14 , where each number now stands for one word .",
    "this is schematically illustrated in fig .",
    "given that we chose ",
    "the wolf \" from @xmath14 , fig .",
    "2(b ) , the next word will now ( usually ) not be chosen from @xmath14 , but from a subset of it , fig .",
    "imagine that the subset contains @xmath18 words , we have @xmath19 .",
    "typically we expect the subset to contain words that are associated to properties of canines , biological functions , other animals , etc . ,",
    "but not all possible words anymore .",
    "once we specify the second word `` howls''@xmath20 , context , intelligibility , and grammatical structure further restrict sample - space for the third word to @xmath21 , from which we finally draw `` night '' . obviously , the nestedness in the formation of sentences is similar to the example of the nested dice before .",
    "nesting is imposed through grammatical and/or contextual , and/or interpretative constraints .",
    "the role of grammar for nesting is obvious . typically in english",
    "the first word is a noun with the grammatical role of the _ subject_. the fact that the first word is a noun restricts the possibilities for the next word to the subset of _ verbal phrases_. depending on the particular verb chosen , the words that can now follow are typically playing the grammatical role of the _ object _ , and are again more restricted .",
    "we use the terms sample - space reduction and nested hierarchical structure in sentences interchangeably .",
    "it is not only grammatical structure that imposes consecutive restrictions on sample - space of words as the sentence progresses , the need for intelligibility has the same effect . without ( at least partial ) hierarchical structures in the formation of sentences , their _ interpretation",
    "_ would become very hard @xcite .",
    "however , nested structures in sentences will generally not be strictly realised .",
    "otherwise the creative use and flexibility of language would be seriously constrained .",
    "sometimes words can act as a linguistic hinge , meaning that it allows for many more consecutive words , than were available for its preceding word .",
    "one expects that nestedness will be realised only to some degree .",
    "imperfect nestedness allows for a degree of ambiguity in the linguistic code , and is one of the sources of its astonishing versatility @xcite .    in this paper",
    "we quantify the degree of nestedness of a text from its word - transition matrix @xmath22 ( network ) .",
    "to characterize the hierarchical structure of a text with a single number , we define its nestedness @xmath23 as a property of @xmath22 by @xmath24 where the average is taken over all possible word pairs @xmath25 .",
    "nestedness is a number between 0 and 1 , and specifies to what extent sample - space reduction is present on average in the text ) is reasonable only for the case where the probability of two words @xmath26 having the same sample space is very low , @xmath27 .",
    "that is the case for the considered transition matrices . ] . a strictly nested system , like the one shown in eq .",
    "( [ eq : nested ] ) , has @xmath28 . in linguistic terms",
    "strict nestedness is clearly unrealistic .",
    "we use word - transition matrices from actual english texts , which serve as the input to a simple model for sentence formation .",
    "we then study the word frequency distributions of these artificially produced texts , and compare them with the distributions of the original texts .",
    "for the first time we show that it is possible to relate the topological feature of ( local ) nestedness in sentence formation to the global features of word frequency distributions of long texts . in this respect",
    "we propose a way to understand the statistics of word frequencies ",
    "zipfs law in particular  by the actual structural feature of language , nestedness , without the need to resort to previous attempts including multiplicative processes , preferential attachment , or self - organized criticality , which , in the context of language , sometimes seem to rest on strong and implausible assumptions .",
    "we assume a finite vocabulary of @xmath3 words . from any given text",
    "we obtain an empirical word - transition matrix @xmath22 .",
    "words are labeled with latin indices .",
    "@xmath29 means that in the text we find at least one occasion where word @xmath7 directly follows @xmath4 , if @xmath30 word @xmath7 never follows @xmath4 in the entire text .",
    "figure 3(a ) shows the transition matrix for _ the origin of species_. to quantify sample - space for individual words , note that a line @xmath4 in @xmath22 contains the _ set _ of words , @xmath31 , that directly follow word @xmath4 . by @xmath32",
    "we denote the size ( number of elements ) of @xmath33 , which is the number of _ different _ words that can follow @xmath4 .",
    "@xmath33 is an approximation for the sample - space volume that is accessible after word @xmath4 has occurred .",
    "different words have different sample - space volumes , see fig .",
    "3(b ) , where the sample - space profile is shown . we parametrize the profile as @xmath34 , where @xmath35 corresponds to the sample space volume , @xmath32 , and @xmath36 to the sample space index @xmath4 .",
    "we call a system _ linearly nested _ if @xmath37 ( as in eq .",
    "( [ eq : nested ] ) ) , _ weakly nested _ for @xmath38 ( as in fig .",
    "3(b ) ) , and _ strongly nested _ if @xmath39 . an example for a weakly nested profile can be seen in one of the insets of fig .",
    "the parameter @xmath40 has an intuitive interpretation in terms of a measure of structuredness of word - transitions . in the case of a weakly nested profile ( @xmath38 )",
    "there are many words that can be followed by many different words , whereas in a strongly nested profile ( @xmath39 ) there are a few words that are followed by many other words , and many words that can only be followed by a very few . in this sense",
    "@xmath40 measures to what extent word - transitions are effectively constrained .",
    "note that the profile in fig .",
    "3(b ) is actually not well fitted with a power law , the reason for the parametrisation is for a purely theoretical argument that will become clear below .",
    "we exclude words that are followed by less than 2 different words in the entire text , i.e. we remove all lines @xmath4 from @xmath22 for which @xmath41 .",
    "strict nestedness is not to be confused with strong or weak nesting .",
    "the latter are properties of the sample - space profile .    for statistical testing",
    "we construct two randomised versions of @xmath22 , and denote them by @xmath42 and @xmath43 , respectively .",
    "@xmath42 is obtained by randomly permuting the rows of the individual lines of the matrix @xmath22 .",
    "this keeps the number of non - zero entries in every line the same as in the original matrix @xmath22 , but destroys its nestedness and the information which words follow each other .",
    "the second randomized version @xmath43 is obtained by permuting the ( entire ) rows of the matrix @xmath22 .",
    "this keeps the nestedness of the matrix unchanged , but destroys the information on word - transitions .",
    "+   +    given @xmath22 we construct random sentences of length @xmath18 with the following model :    * pick one of the @xmath3 words randomly .",
    "say the word was @xmath4 .",
    "write @xmath4 in a wordlist @xmath44 , so that @xmath45 .",
    "* jump to line @xmath4 in @xmath22 and randomly pick a word from the set @xmath33 .",
    "say the word chosen is @xmath5 ; update the wordlist @xmath46 .",
    "* jump to line @xmath5 , and pick one of the words from @xmath47 ; say you get @xmath7 , and update @xmath48 .",
    "* repeat the procedure @xmath18 times . at this stage a random sentence is formed .",
    "* repeat the process to produce @xmath49 sentences .    in this way we get a wordlist with @xmath50 entries , which is a random book that is generated with the word - transition matrix of an actual book . from the wordlist we obtain the word frequency distribution @xmath51 .",
    "the present model is similar to the one in @xcite but differs in three aspects : it allows for non - perfect nesting @xmath52 , it has no explicit noise component , and it has a fixed sequence ( sentence ) length .",
    "we analyze the model with computer simulations , specifying @xmath53 , and @xmath54 .",
    "we use 10 randomly chosen books ( romeo and juliet ) to @xmath55 ( ulysses ) words . ] from the project gutenberg @xcite . for every book",
    "we determine its vocabulary @xmath3 , its matrix @xmath22 , its @xmath33 for all words , its nestedness @xmath56 , and the exponent of the rank ordered word frequency distribution @xmath57 ( least square fits to @xmath58 , fit range between the @xmath59 ) .",
    "@xmath58 is seen for in fig .",
    "1 ( blue ) the exponent is @xmath60 .",
    "we run the model for the parameters of every individual book to generate a random text . using the empirical @xmath33 for the model",
    "ensures that this random text has exactly the same sample - space profile and the nestedness as the book .",
    "the distribution obtained from the model @xmath51 is clearly able to reproduce the approximate power law exponent for _ the origin of species _ , @xmath61 ( same fit range ) .",
    "moreover it captures details of the distribution @xmath62 . for large values of @xmath1 in @xmath63",
    "a plateau is forming before the exponential finite size cutoff is observed .",
    "both , plateau and cutoff can be fully understood with the randomised model .    in fig .",
    "4(a ) we compare the @xmath57 exponents as extracted from the books with the model results @xmath64 .",
    "the model obviously explains the actual values to a large extent , slightly underestimating the actual exponents .",
    "we get a correlation coefficient of @xmath65 ( @xmath66 ) . in fig .",
    "4(b ) we show that nesting @xmath56 is related to the exponents @xmath57 in an approximately linear way .",
    "we test the hypothesis that by destroying nestedness the exponents will vanish . using the randomised @xmath42",
    "we find @xmath67 ( same fit range ) , which effectively destroys the power law .",
    "using the other randomized version that keeps the nestedness intact , @xmath43 , for low - rank words ( up to approximately rank @xmath68 ) we find similar word frequency distributions as for @xmath22 , however , as expected , the power law tail ( high ranks ) vanishes for @xmath43 due to the noise contribution of the randomization ( not shown ) .",
    "to validate our assumption that word ordering is essential , we computed the model rank distributions by using the transposed matrix @xmath69 , meaning that we reverse the time flow in the model .",
    "we find two results .",
    "first , the correlation between the exponents of the books @xmath57 , and the model @xmath70 vanishes , reflected by an insignificant correlation coefficient @xmath71 ( @xmath72 ) .",
    "second , the exponents ( averaged over the 10 books ) are significantly smaller @xmath73 , than for the correct time flow , where we get @xmath74 .",
    "the corresponding @xmath75-value of a t - test is @xmath76 .",
    "finally we try to understand the importance of the sample - space profile on the scaling exponents . for this",
    "we generate a series of @xmath22 matrices that have a profile parametrized with a power @xmath40 . in fig .",
    "4(c ) the model exponents @xmath64 from these artificially generated @xmath22 are shown as a function of @xmath40 , for various sizes of vocabulary @xmath3 . for @xmath38 ( weak nesting )",
    "we find exponents @xmath77 , i.e. no scaling law . for large @xmath3 at @xmath37 a fast transition to @xmath78 ( zipf ) occurs .",
    "for smaller @xmath3 we find a more complicated behaviour of the transition , building a maximum exponent at a @xmath38 .",
    "the range of book exponents @xmath57 ranges between @xmath79 and @xmath80 , which is exactly the observed range for realistic vocabulary sizes @xmath81-@xmath82 .",
    "we verified that variations in sentence length ( with the exception of @xmath83 ) do not change the reported results . for one - word sentences ( @xmath83 )",
    "we obviously get a uniform word frequency distribution , and as a consequence , a flat rank distribution , since most words have almost the same rank .",
    "we varied the number of sentences from @xmath84 to @xmath85 , and find practically no influence on the reported results .",
    "in this paper we focus on the fundamental property of nestedness in any code that conveys meaningful information , such as language .",
    "we argue that if nesting was not present one would easily end up in confusing situations as described in _ la biblioteca de babel _ by j. l. borges , where a hypothetical library owns all books composed of _ all _ possible combinations of characters filling 410 pages .",
    "we define and quantify a degree of nestedness in the linguistic code .",
    "low degrees of nestedness typically imply a less strict hierarchy on word usage or a more _ egalitarian _ use of the vocabulary , than texts with high nestedness . as expected , texts have a well defined , but not strictly nested structure , which might arise from a compromise of specificity ( to convey unambiguous messages ) and flexibility ( to allow a creative use of language ) .",
    "we find that nestedness varies between different texts , suggesting that different ways of using the vocabulary and grammar are at work .",
    "our sample of texts included three plays by shakespeare , three scientific texts , and four novels .",
    "we find that the plays , maybe closest to spoken language , show a lower nestedness than the science books .",
    "the novels show the highest levels of nestedness .",
    "the sample is too small to draw conclusions on whether different types of texts are characterized by typical values of nestedness , however it is remarkable that nestedness is correlated with the variations of the scaling exponents of word frequencies on a book - by - book basis .",
    "the main finding of this paper is that a simple sample - space reducing model can show that nestedness indeed explains the emergence of scaling laws in word frequencies , in particular , zipf s law .",
    "more precisely , we were able to relate the emergence of scaling laws with topological structure of the word transition matrix , or `` phasespace '' .",
    "the result is remarkable since the matrix does not encode any information about how often word @xmath7 follows word @xmath4 , it just tells that @xmath7 followed @xmath4 at least once in the entire text .",
    "random permutations of the matrix that destroy its nestedness can not explain the scaling anymore , while permutations that keep nesting intact , do indicate the existence of the power laws .",
    "it is further remarkable that no ( non - local ) preferential , multiplicative , or self - organized critical assumptions are needed to understand the observed scaling , and that no parameters are needed beyond the word transition matrices .",
    "the fact that the simple model is so successful in reproducing the detailed scaling property in word frequency statistics might point to an important aspect of language , that has not been noted so - far ; the fact that overall word - use is statistically strongly influenced by the use of local hierarchical structures and constraints that we use in generating sentences .",
    "we believe that the close relation between nestedness and the scaling exponent opens the door for an interpretation of word frequency distributions as a statistical observable that strongly depends on the usage of the vocabulary and grammar within a language .",
    "accordingly we conjecture that zipf s law might not be universal , but that word - use statistics depends on local structures which may be different across texts and even within sentences .",
    "further research is needed to clarify this point .",
    "finally , it is worthwhile to note that the class of sample - space reducing processes provide an independent route to scaling that might have a wide range of applications for history - dependent and ageing processes @xcite . in statistical physics",
    "it is known that processes that successively reduce their phasespace as they unfold are characterised by power law or stretched exponential distribution functions .",
    "these distributions generically arise as a consequence of phasespace collapse @xcite .        * author contributions*@xmath86 st designed the research , performed numerical analysis and wrote the manuscript .",
    "rh and bc - m performed numerical analysis and wrote the manuscript .",
    "bl did preprocessing the books , and performed numerical analysis .",
    "f. font - clos , g. boleda , a. corral ( 2013 ) a scaling law beyond zipf s law and its relation to heaps law .",
    "_ new journal of physics _ * 15 * 093033 .",
    "yan , p. minnhagen ( 2014 ) comment on a scaling law beyond zipf s law and its relation to heaps law. arxiv:1404.1461 .",
    "a. saichev , y. malevergne , d. sornette ( 2008 ) theory of zipf s law and of general power law distributions with gibrat s law of proportional growth .",
    "_ lecture notes in economics and mathematical systems_. berlin , heidelberg , new york : springer ."
  ],
  "abstract_text": [
    "<S> the formation of sentences is a highly structured and history - dependent process . </S>",
    "<S> the probability of using a specific word in a sentence strongly depends on the history of word - usage earlier in that sentence . </S>",
    "<S> we study a simple history - dependent model of text generation assuming that the sample - space of word usage reduces along sentence formation , on average . </S>",
    "<S> we first show that the model explains the approximate zipf law found in word frequencies as a direct consequence of sample - space reduction . </S>",
    "<S> we then empirically quantify the amount of sample - space reduction in the sentences of ten famous english books , by analysis of corresponding word - transition tables that capture which words can follow any given word in a text . </S>",
    "<S> we find a highly nested structure in these transition tables and show that this ` nestedness ' is tightly related to the power law exponents of the observed word frequency distributions . with the proposed model it is possible to understand that the nestedness of a text can be the origin of the actual scaling exponent , and that deviations from the exact zipf law can be understood by variations of the degree of nestedness on a book - by - book basis . </S>",
    "<S> on a theoretical level we are able to show that in case of weak nesting , zipf s law breaks down in a fast transition . </S>",
    "<S> unlike previous attempts to understand zipf s law in language the sample - space reducing model is not based on assumptions of multiplicative , preferential , or self - organised critical mechanisms behind language formation , but simply used the empirically quantifiable parameter nestedness to understand the statistics of word frequencies . </S>"
  ]
}