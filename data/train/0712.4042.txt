{
  "article_text": [
    "the main goals of reactor neutrino experiments are to detect @xmath0 oscillation and precisely measure the mixing angle of neutrino oscillation @xmath1 .",
    "the experiment is designed to detect reactor @xmath2 s via the inverse @xmath3-decay reaction    @xmath4    the signature is a delayed coincidence between @xmath5 and the neutron captured signals .",
    "it is very important to reconstruct the energy and the vertex of a signal detected in the experiments .",
    "the standard algorithm of the event reconstruction in the experiments is a maximum likelihood method ( mld from now on ) .",
    "but the method of bayesian neural networks ( bnn from now on)@xcite is more suitable than mld for the event reconstruction of reactor neutrino experiments .",
    "bnn is an algorithm of the neural networks trained by bayesian statistics .",
    "it is not only a non - linear function , but also controls model complexity .",
    "so its flexibility makes it possible to discover more general relationships in data than the traditional statistical methods and its preferring simple models make it possible to solve the over - fitting problem better than the general neural networks@xcite . in this paper ,",
    "bnn is applied to the event reconstruction of the electron samples from the monte - carlo simulation of a toy detector of reactor neutrino experiments . and",
    "the result of the event reconstruction using bnn is compared with the one using mld .",
    "the idea of bnn is to regard the process of training a neural network as a bayesian inference .",
    "bayes theorem is used to assign a posterior density to each point , @xmath6 , in the parameter space of the neural networks .",
    "each point @xmath6 denotes a neural network . in the method of bnn ,",
    "one performs a weighted average over all points in the parameter space of the neural network , that is , all neural networks .",
    "the methods make use of training data \\{(@xmath7,@xmath8 ) , ( @xmath9,@xmath10), ... ,(@xmath11,@xmath12 ) } , where @xmath13 is the known target value associated with data @xmath14 , which has @xmath15 components if there are @xmath15 input values in the regression .",
    "that is the set of data @xmath16(@xmath7,@xmath9, ...",
    ",@xmath11 ) which corresponds to the set of target @xmath17(@xmath8,@xmath10, ...",
    ",@xmath12 ) .",
    "the posterior density assigned to the point @xmath6 , that is , to a neural network , is given by bayes theorem    @xmath18    where data @xmath19 do not depend on @xmath6 , so @xmath20 .",
    "we need the likelihood @xmath21 and the prior density @xmath22 , in order to assign the posterior density @xmath23to a neural network defined by the point @xmath6 .",
    "@xmath24 is called evidence and plays the role of a normalizing constant , so we ignore the evidence . that is ,    @xmath25    we consider a class of neural networks defined by the function    @xmath26    .",
    "the neural networks have @xmath15 inputs , a single hidden layer of @xmath27 hidden nodes and one output . in the particular bnn described here , each neural network has the same structure .",
    "the parameter @xmath28 and @xmath29 are called the weights and @xmath30 and @xmath31 are called the biases .",
    "both sets of parameters are generally referred to collectively as the weights of the bnn , @xmath6 .",
    "@xmath32 is the predicted target value .",
    "we assume that the noise on target values can be modeled by the gaussian distribution .",
    "so the likelihood of @xmath33 training events is    @xmath34=exp[-\\sum_{i=1}^{n}(t_{i}-y\\left(x_{i},\\bar{\\theta}\\right)/2\\sigma^{2})]\\ ] ]    where @xmath13 is the target value , and @xmath35 is the standard deviation of the noise .",
    "it has been assumed that the events are independent with each other .",
    "then , the likelihood of the predicted target value is computed by eq . ( 4 ) .",
    "we get the likelihood , meanwhile we need the prior to compute the posterior density . but the choice of prior is not obvious",
    ". however , experience suggests a reasonable class is the priors of gaussian class centered at zero , which prefers smaller rather than larger weights , because smaller weights yield smoother fits to data . in the paper , a gaussian prior",
    "is specified for each weight using the bayesian neural networks package of radford neal .",
    "however , the variance for weights belonging to a given group(either input - to - hidden weights(@xmath28 ) , hidden -biases(@xmath30 ) , hidden - to - output weights(@xmath29 ) or output - biases(@xmath31 ) ) is chosen to be the same : @xmath36 , @xmath37 , @xmath38 , @xmath39 , respectively . however , since we do nt know , a priori , what these variances should be , their values are allowed to vary over a large range , while favoring small variances .",
    "this is done by assigning each variance a gamma prior    @xmath40    where @xmath41 , and with the mean @xmath42 and shape parameter @xmath43 set to some fixed plausible values .",
    "the gamma prior is referred to as a hyperprior and the parameter of the hyperprior is called a hyperparameter .",
    "then , the posterior density , @xmath23 , is gotten according to eqs .",
    "( 2),(4 ) and the prior of gaussian distribution . given an event with data @xmath44 , an estimate of the target value",
    "is given by the weighted average    @xmath45    currently , the only way to perform the high dimensional integral in eq .",
    "( 6 ) is to sample the density @xmath23 with the markov chain monte carlo ( mcmc ) method@xcite . in the mcmc method ,",
    "one steps through the @xmath6 parameter space in such a way that points are visited with a probability proportional to the posterior density , @xmath23 .",
    "points where @xmath23 is large will be visited more often than points where @xmath23 is small .",
    "( 6 ) approximates the integral using the average    @xmath46    where @xmath47 is the number of points @xmath6 sampled from @xmath23 .",
    "each point @xmath6 corresponds to a different neural network with the same structure .",
    "so the average is an average over neural networks , and is closer to the real value of @xmath48 , when @xmath47 is sufficiently large .",
    "in the paper , a toy detector is designed to simulate central detectors in the reactor neutrino experiments , such as daya bay experiment@xcite and double chooz experiment@xcite , with cern geant4 package@xcite .",
    "the toy detector consists of three regions , and they are the gd - doped liquid scintllator(gd - ls from now on ) , the normal liquid scintillator(ls from now on ) and the oil buffer , respectively . the toy detector of cylindrical shape like the detector modules of daya bay experiment and double chooz experiment is designed in the paper . the diameter of the gd - ls region is 2.4 meter , and its height is 2.6 meter . the thickness of the ls region is 0.35 meter , and the thickness of the oil part is 0.40 meter . in the paper , the gd - ls and ls are the same as the scintillator adopted by the proposal of the chooz experiment@xcite .",
    "the 8-inch photomultiplier tubes ( pmt from now on ) are mounted on the inside the oil region of the detector .",
    "a total of 366 pmts are arranged in 8 rings of 30 pmts on the lateral surface of the oil region , and in 5 rings of 24 , 18 , 12 , 6 , 3 pmts on the top and bottom caps .",
    "the response of the electron events deposited in the toy detector is simulated with geant4 .",
    "although the physical properties of the scintillator and the oil ( their optical attenuation length , refractive index and so on ) are wave - length dependent , only averages@xcite ( such as the optical attenuation length of gd - ls with a uniform value is 8 meter and the one of ls is 20 meter ) are used in the detector simulation .",
    "the program could nt simulate the real detector response , but this wo nt affect the result of the comparison between bnn and mld .",
    "the program allows us to simulate the detector response for the electron events of the different energy and vertex . in the paper , 10000",
    "electron events regarded as the training sample are uniformly generated throughout gd - ls region and their energy is also uniformly generated from 1 mev to 13 mev .",
    "3000 electron events regarded as the 1 mev test sample are generated uniformly throughout gd - ls region .",
    "the test samples from 2 mev to 8 mev are generated in the same way , respectively .",
    "the task of the event reconstruction in the reactor neutrino experiments is to reconstruct the energy and the vertex of a signal .",
    "the maximum likelihood method is a standard algorithm of the event reconstruction in the reactor neutrino experiments .",
    "the likelihood is defined as the joint poisson probability of observing a measured distribution of photoelectrons over the all pmts for given ( @xmath49 ) coordinates in the detector .",
    "the ref.@xcite for the work of the chooz experiment shows the method of the reconstruction in detail .",
    "the algorithm of bnn is also applied to event reconstruction , and its result is compare with the one of mld .      in the paper ,",
    "the event reconstruction with the mld are performed in the similar way with the chooz experiment@xcite , but the detector is different from the detector of the chooz experiment , so compared to ref.@xcite , there are some different points in the paper :    \\(1 ) the detector in the paper consists of three regions , so the path length from a signal vertex to the pmts consist of three parts , and they are the path length in gd - ls region , the one in ls region , and the one in oil region , respectively .",
    "\\(2 ) considered that not all pmts in the detector can receive photoelectrons when a electron is deposited in the detector , the @xmath50 equation is modified in the paper and different from the one in the chooz experiment , that is , @xmath51 , where @xmath52 is the number of photoelectrons received by the j - th pmt and @xmath53 is the expected one for the j - th pmt@xcite .",
    "\\(3 ) @xmath54 and the coordinates of the charge center of gravity for the all visible photoelectrons from a signal are regarded as the starting values for the fit parameters(@xmath49 ) , where @xmath55 is the total numbers of the visible photoelectrons from a signal and @xmath56 is the proportionality constant of the energy @xmath57 , that is , @xmath58 .",
    "@xmath56 is obtained through fitting @xmath55 s of the 1 mev electron events , and is @xmath59 in the paper .",
    "( @xmath49 ) of the all electron events , including the test sample and the training sample , are reconstructed using mld .      in the paper ,",
    "the cartesian coordinates ( @xmath60 ) of the all events , including the test sample and the training sample , are transformed to their cylindrical coordinates ( @xmath61 ) .",
    "the ( @xmath62 ) are used as inputs to the bnn , which have the input layer of 4 inputs , the single hidden layer of 8 nodes and the output layer of a output which is @xmath57 , or @xmath60 , respectively .",
    "the @xmath57 and @xmath60 of the test samples are predicted using the bnn , respectively",
    ". a markov chain of neural networks is generated using the bayesian neural networks package of radford neal , with the training sample , in the process of the event reconstruction .",
    "one thousand iterations , of twenty mcmc steps each , are used in the paper .",
    "the neural network parameters are stored after each iteration , since the correlation between adjacent steps is very high .",
    "that is , the points in neural network parameter space are saved to lessen the correlation after twenty steps .",
    "it is also necessary to discard the initial part of the markov chain because the correlation between the initial point of the chain and the points of the part is very high .",
    "the initial three hundred iterations are discarded in the paper .",
    "fig.1 , fig.2 and fig.3 illustrate the results of the event reconstruction with bnn and mld .",
    "fig.1 shows that the errors of the vertex of 1 mev and 8 mev electrons reconstructed by the bnn are consistent with the ones by mld , that is , they are not obviously different .",
    "fig.2 shows that the energy uncertainty for 1 mev electrons with bnn decreases by 95.0% in comparison with the one using mld .",
    "and the uncertainty in the case of the 8 mev events decreases by 76.3% .",
    "fig.3 shows the energy resolutions using bnn are more significantly improved in comparison with the one using mld while increasing energy .",
    "meanwhile , the relative errors of the energy resolutions are about 2.0% , and are from fit errors ( about 1.5% ) and statistical errors ( about 1.3% ) .",
    "so the difference between results of bnn and mld is not significant in the case of 1 mev events in consideration of the effect of statistical fluctuations .",
    "but the contribution to the difference is mainly from the superiority of bnn for the events from 2 mev to 8 mev .",
    "thus it can be seen that the energy resolutions using bnn are significantly improved for the high energy events in comparison with the one using mld .",
    "therefore , bnn can be well applied to the energy reconstruction in the reactor neutrino experiments , and the better energy resolution is obtained by bnn .",
    "although the discussion in the paper are only for the reactor neutrino experiments , it is expected that the algorithm of bnn can also be applied to the event reconstruction of the other experiments and will find wide application in the experiments of high energy physics .",
    "this work is supported by the national natural science foundation of china(nsfc ) under the contract no .",
    "10605014 .",
    "is the difference between the coordinates of the reconstructed position and the generated ones , respectively .",
    "the event position is reconstruted using bnn and mld , respectively .",
    "( a)(c)(e ) illustrate the difference distribution of the 1 mev electrons , and ( b)(d)(f ) illustrate the one of 8 mev electrons.,width=604,height=604 ]"
  ],
  "abstract_text": [
    "<S> a toy detector has been designed to simulate central detectors in reactor neutrino experiments in the paper . </S>",
    "<S> the electron samples from the monte - carlo simulation of the toy detector have been reconstructed by the method of bayesian neural networks ( bnn ) and the standard algorithm , a maximum likelihood method ( mld ) , respectively . </S>",
    "<S> the result of the event reconstruction using bnn has been compared with the one using mld . </S>",
    "<S> compared to mld , the uncertainties of the electron vertex are not improved , but the energy resolutions are significantly improved using bnn . and the improvement is more obvious for the high energy electrons than the low energy ones .    </S>",
    "<S> department of physics , nankai university , tianjin 300071 , people s republic of china    bayesian neural networks , event reconstruction , neutrino oscillation    pacs numbers : 07.05.mh , 29.85.fj , 14.60.pq </S>"
  ]
}