{
  "article_text": [
    "it is now well established that biological and artificial neural networks propagate information by means of synaptic interactions which may be symmetric or non - symmetric .",
    "the latter appear in biological networks , while either of both have been used in artificial networks .",
    "there are two main classes of neural network models for associative information processing .",
    "one is that of recurrent networks with feed - back of either symmetric or non - symmetric synapses @xcite and the other one is that of layered feed - forward networks , with no feed - back loops , in which the synapses are intrinsically non - symmetric @xcite-@xcite .",
    "models of the first class can be solved by means of equilibrium statistical mechanics in the case of symmetric synapses @xcite , due to the presence of detailed balance , whereas one has to resort to a dynamical procedure in the search for stationary states in the case of non - symmetric synapses @xcite . on the other hand ,",
    "feed - forward networks are already dynamical systems which may reach stationary states in place of true equilibrium states .",
    "a novel dual model of binary neurons which combines recurrent and feed - forward processing , with symmetric synapses in the recurrent part , has been proposed some time ago by coolen and viana @xcite , and the model has been generalized recently @xcite-@xcite .",
    "the network model consists of layers of fully connected neurons in a recurrent architecture that receive inputs from the previous layer ( except the first one ) and pass the processed information by means of feed - forward synaptic interactions to the next layer .",
    "the latter processes further the information and feeds , in turn , the next layer .",
    "every unit in each layer is assumed to be connected to every other unit in that layer as well as to every unit in the next and in the previous layer but the information goes only from one layer to the next one ( the feed - forward nature of the model ) .",
    "it is important to note that each unit in every layer participates of both the recurrent and the feed - forward process .",
    "thus , a single updating procedure ( usually , either sequential or parallel ) has to be applied to every unit in each layer .",
    "the dual model becomes solvable by means of equilibrium statistical mechanics when all layers reach a stationary state and that may eventually be the case in the zero - temperature limit . in the case of finite temperature , thermal fluctuations",
    "destroy the boltzmann form of the stationary states in the layers and one has to go over to a dynamical approach , which has not been done apparently so far .",
    "actually , the model has been solved working out the free energy and the equations for the order parameters for finite temperature , assuming a boltzmann form based on an underlying sequential updating dynamics , and letting the temperature go to zero after the thermodynamic limit .",
    "the close agreement between theory and numerical simulations justified the procedure and the order of these limits despite the absence of detailed balance @xcite .",
    "a feed - forward chain of recurrent networks is interesting as a model of dual information processing in which the feed - forward mode transmits the outcome for the stationary overlaps within a layer to the next one .",
    "so far , only the case of a fully connected network has been considered , and one may ask how does the performance of the model change if the constraint of full connectivity between the units within each layer is relaxed by means of synaptic dilution keeping the full connectivity between units in adjacent layers .",
    "synaptic dilution reduces the outcome from a layer and it could be that the cumulative effect in a long chain of layers would be a vanishingly small storage capacity even for a reduced amount of dilution .",
    "this issue is of interest in statistical mechanics and also for the information processing in biological networks , in particular for the understanding of the role played by the @xmath0 region of the hippocampus in the primate brain where the mean connection between units ( our parameter @xmath1 below ) in the latter is of the order of @xmath2 @xcite , which may be considered as a finite dilution .",
    "finite dilution may also be of interest if one thinks of defects in the connectivity within layers in artificial neural networks .",
    "detailed studies of the dynamics and the equilibrium states that describe the performance have been done in recent works that deal with finite dilution in various neural network models on different architectures @xcite-@xcite following earlier works @xcite-@xcite .",
    "the main purpose of the present paper is to study the stationary states and obtain the phase diagrams that describe the performance of the dual model with finite dilution of the synaptic interactions within layers . in consistency with the procedure , we first take into account the disorder due to dilution for non - zero temperature ,",
    "solve the model in the thermodynamic limit and then let the temperature go to zero .",
    "a second , minor purpose , is to indicate the necessary steps that yield the recursion relation for the parameter that accounts for the input from the non - condensed overlaps which follows an unconventional derivation .",
    "it will be shown that , in addition to the well - known effect of finite dilution in replacing the intra - layer synaptic connection by an effective interaction that is the superposition of a hebbian term for a fully connected network and a gaussian noise , the only further contribution of the dilution on that parameter is an overall factor which can be absorbed by a simple rescaling in the case of dense networks .",
    "a further interest in the procedure to deal with the effects of synaptic dilution is that it may be extended to study the influence of general non - linear synapses @xcite .",
    "the outline of the paper is the following . in sec .",
    "2 we review the model and introduce the alterations due to finite dilution . in sec .",
    "3 we solve the model to obtain the replica symmetric free energy near saturation and derive the recursion relations that describe the evolution of the parameters of the network from one layer to the next .",
    "the phase transitions and some features of the performance are obtained and discussed in sec . 4 and we end with concluding remarks in sec . 5",
    "the network model consists of @xmath3 layers with @xmath4 binary ising units ( neurons ) on each layer @xmath5 in a microscopic state @xmath6 , in which each @xmath7 .",
    "the state @xmath8 represents a firing neuron and the state @xmath9 a neuron at rest .",
    "the microscopic dynamics of the network is assumed to be a glauber sequential stochastic alignment of each neuron @xmath10 to a local field @xmath11 with probability @xmath12\\,\\,\\,\\ , , \\label{1}\\ ] ] where @xmath13 is due to the states of other ( see below ) neurons @xmath14 on the same layer and of neurons @xmath15 on the previous layer , as shown schematically in fig.1 . at each time",
    "step the neuron to be updated is taken at random from the set @xmath16 .",
    "the parameter @xmath17 controls the synaptic noise such that the dynamics of the network becomes deterministic when @xmath18 .",
    "( 140,60)(-20,0 )    ( 22,0)(0,1)52 ( 40,0)(0,1)52 ( 22,0)(1,0)18 ( 22,52)(1,0)18    ( 62,0)(0,1)52 ( 80,0)(0,1)52 ( 62,0)(1,0)18 ( 62,52)(1,0)18    ( 0,5)(4,1)22 ( 0,5)(4,3)22 ( 0,5)(2,3)22    ( 0,26)(1,0)22 ( 0,26)(1,1)22 ( 0,26)(1,-1)22    ( 0,47)(4,-1)22 ( 0,47)(4,-3)22 ( 0,47)(2,-3)22    ( 40,5)(4,1)22 ( 40,5)(4,3)22 ( 40,5)(2,3)22    ( 40,26)(1,0)22 ( 40,26)(1,1)22 ( 40,26)(1,-1)22    ( 40,47)(4,-1)22 ( 40,47)(4,-3)22 ( 40,47)(2,-3)22    ( 80,5)(4,1)22 ( 80,5)(4,3)22 ( 80,5)(2,3)22    ( 80,26)(1,0)22 ( 80,26)(1,1)22 ( 80,26)(1,-1)22    ( 80,47)(4,-1)22 ( 80,47)(4,-3)22 ( 80,47)(2,-3)22    ( 31,-4)(0,0)[c ] ( 71,-4)(0,0)[c ] ( 72,56)(0,0)[c ] ( 32,56)(0,0)[c ] ( 51,54)(0,0)[c ] ( 31,38)(0,0)[c ] ( 71,38)(0,0)[c ]    ( 25,25)(-1,0)1 ( 27,25)(1,0)1 ( 29,25)(1,0)1 ( 31,25)(1,0)1 ( 33,25)(1,0)1 ( 35,25)(1,0)1 ( 37,25)(1,0)1    ( 31,31)(0,1)1 ( 31,29)(0,-1)1 ( 31,27)(0,-1)1 ( 31,25)(0,-1)1 ( 31,23)(0,-1)1 ( 31,21)(0,-1)1 ( 31,19)(0,-1)1    ( 65,25)(-1,0)1 ( 67,25)(1,0)1 ( 69,25)(1,0)1 ( 71,25)(1,0)1 ( 73,25)(1,0)1 ( 75,25)(1,0)1 ( 77,25)(1,0)1    ( 71,31)(0,1)1 ( 71,29)(0,-1)1 ( 71,27)(0,-1)1 ( 71,25)(0,-1)1 ( 71,23)(0,-1)1 ( 71,21)(0,-1)1 ( 71,19)(0,-1)1    a macroscopic set of @xmath19 independent and identically distributed random patterns @xmath20 ; @xmath21 , are stored on the sites of each layer in the learning stage following a hebbian rule @xmath22 where each @xmath23 , with probability @xmath24 , is the component of the pattern @xmath25 on neuron @xmath26 of layer @xmath5 .",
    "thus , @xmath27 is the storage ratio of patterns _ per site _ of the hopfield model @xcite , whether the sites are connected or not within a layer , since they are always connected to the sites of the adjacent layers .",
    "the parameters @xmath28 control the relative strength of the recurrent and feed - forward interactions , respectively , and they are the same for all layers , except on the first layer where @xmath29 , for all @xmath30 , and where one may distinguish between a free relaxation and a clamped operation defined below @xcite .",
    "the recurrent part of the synaptic interaction is symmetrically diluted by means of the set of identically distributed random variables @xmath31 , with @xmath32 , such that @xmath33 with probability @xmath1 and zero with probability @xmath34 . the average number of connected neurons in each layer , the so - called connectivity , is @xmath35 and we focus our attention in this work on dense networks in which @xmath36 implying that @xmath37 within each layer @xcite .",
    "when @xmath38 the network is fully connected and letting @xmath1 go to zero , after the thermodynamic limit @xmath39 , one has the network with strongly ( symmetrical ) diluted layers .",
    "one should expect that the synaptic dilution within the layers will have effects on the feed - forward information processing of the network , as long as @xmath40 , and it is this that we want to study here .",
    "when @xmath41 the model reduces to a set of @xmath3 decoupled symmetrically diluted attractor hopfield - like networks which can be solved by means of equilibrium statistical mechanics using the replica method @xcite . in that case , the intra - layer synaptic interaction becomes an effective interaction which is the sum of a hebbian term and a gaussian noise that will be determined below . in the case of full connectivity ( @xmath38 ) the noise term vanishes and",
    "the critical storage ratio is that of the usual hopfield model , @xmath42 .",
    "in contrast , when @xmath43 , we have a purely feed - forward network with full inter - layer synaptic interactions between neurons on consecutive layers which can be solved exactly for all @xmath44 , including the saturation of stored patterns , in the large @xmath4 limit by means of a signal - to - noise analysis which is a truncated version of a neurodynamic approach developed some time ago @xcite . in that case , the local fields follow a gaussian distribution allowing for the derivation of exact recursion relations for the order parameters in the limit of @xmath45 @xcite with a critical storage capacity @xmath46 .",
    "we expect this capacity , for @xmath43 , to remain the same under exclusive synaptic dilution within the layers but there should be a gradual change of @xmath47 with increasing @xmath48 and an eventual maximum @xmath47 for an intermediate situation between feed - forward and recurrent operation , as in the case of the fully connected network ( @xmath38 ) where @xmath49 @xcite .",
    "coming back to the input at the first layer , one may have either a free relaxation from a specific initialization as a recall cue or a so - called `` clamped '' operation where the recall cue is given by an externally specified but stationary random state vector on that layer .",
    "we will discuss the effects of dilution on both modes .    due to the feed - forward nature of the interactions between consecutive layers the network will attain a stationary state at @xmath50 .",
    "this will be the case for any layer @xmath5 when the input from layer @xmath51 becomes stationary .",
    "the equilibrium states of the network may then be associated with the stationary states of the dynamics with a probability distribution given by a boltzmann form @xmath52 , for any layer , with the hamiltonian @xmath53 which enables one to solve the model first for finite @xmath54 and then taking the @xmath55 limit , as in previous work @xcite .    to study the performance of the network we introduce the macroscopic overlaps between the state and the stored patterns on that layer , @xmath56 and assume , as usual , that in equilibrium only a finite number @xmath57 of the patterns is condensed with overlap of @xmath58 , for @xmath59 , and the remaining @xmath60 overlaps are assumed to be macroscopically small of @xmath61 . the local field @xmath62 will not only have contributions from these overlaps but also from the stationary overlaps of the previous layer @xmath51 , due to the second term in eq.(2 ) , @xmath63 with similarly defined condensed and non - condensed components .",
    "we consider the partition function @xmath64 with @xmath65 given by eq.(4 ) , and calculate the free energy treating , as usual , the non - condensed patterns @xmath66 as quenched disorder over which we average the logarithm of the partition function by means of the replica method to obtain the free energy @xmath67 per site as @xmath68_c\\rangle_{patt}-1 \\ } \\,\\,\\,\\ , .",
    "\\label{8}\\ ] ] here , @xmath69_c\\rangle_{patt}$ ] is the average over the quenched disorder , due to the patterns , of the connected replicated partition function @xmath70_c$ ] obtained by taking the average over the random symmetrical dilution .    with the dilution only of synapses within the layers ,",
    "we may write @xmath71_c=\\sum_{\\{\\bsigma^1\\}\\dots\\{\\bsigma^n\\}}e^{\\beta\\sum_{\\alpha}\\sum_{ij } k_{ij}^l\\sigma_{i\\alpha}^l\\sigma_{j\\alpha}^{l-1}}\\prod_{i < j}\\big[e^{\\beta j_{ij}^l\\sum_{\\alpha}\\sigma_{i\\alpha}^l\\sigma_{j\\alpha}^l}\\big]_c\\,\\,\\,\\ , ,   \\label{9}\\ ] ] where @xmath44 ( and @xmath72 , below ) denotes the replica index , whenever it appears as a subindex , and @xmath73 is the set of state vectors in the replica @xmath74 . using the now standard procedure to perform the configurational average over the symmetric dilution in the case of weak synapses @xmath75 in the large @xmath4 limit one finds @xmath76_c&= & \\exp\\big\\{\\frac{\\beta^2j_0 ^ 2\\alpha ( 1-c)}{4cn}\\sum_{\\alpha\\beta}(\\sum_i\\sigma_{i\\alpha}^l\\sigma_{i\\beta}^l)^2\\big\\ } \\nonumber\\\\ & \\times & \\exp\\big\\{\\frac{\\beta nj_0}{2}\\sum_{\\mu=1}^{p } \\sum_{\\alpha}m_{\\mu , l}^2(\\bsigma_{\\alpha})\\big\\}\\,\\,\\,\\ , .",
    "\\label { 10}\\end{aligned}\\ ] ] thus , the effect of the dilution amounts to the replacement of the intralayer synapses @xmath77 by the effective interaction @xcite @xmath78 for @xmath79 which is the sum of a hebbian term for a fully connected recurrent network and a gaussian noise @xmath80 of mean zero and variance @xmath81_c = j_0 ^ 2\\alpha ( 1-c)/c\\,\\,\\,\\ , .",
    "\\label{12}\\ ] ]    coming back to the averaged partition function we obtain    @xmath82_c\\rangle_{patt}&=&e^{-\\frac{1}{2}j_0\\alpha\\beta nn}\\nonumber\\\\ & \\times&\\big\\langle\\sum_{\\{\\bsigma^1\\}\\dots\\{\\bsigma^n\\ } } \\exp{\\big\\{\\frac{\\beta^2j_0 ^ 2\\alpha(1-c)}{4cn}\\sum_{\\alpha\\beta } ( \\sum_i\\sigma_{i\\alpha}^l\\sigma_{i\\beta}^l)^2\\big\\}}\\nonumber\\\\ & \\times&\\exp{\\big\\{\\beta n\\sum_{\\mu}\\sum_{\\alpha}\\big[\\frac{j_0}{2}m_{\\mu , l}^2(\\bsigma_{\\alpha}^l ) + jm_{\\mu , l}(\\bsigma_{\\alpha}^l)\\tilde m_{\\mu}\\big]}\\big\\}\\big\\rangle_{\\bxi}\\,\\,\\,\\ , \\label{13}\\end{aligned}\\ ] ]    where the average is over the patterns @xmath83 in the overlaps @xmath84 , and @xmath85 are the already stationary overlaps in the previous layer .",
    "the sum over @xmath25 in the exponent is over both , @xmath86 ( condensed ) and @xmath87 ( non - condensed ) patterns .",
    "we follow the extension of the standard procedure @xcite to calculate @xmath69_c\\rangle_{patt}$ ] and obtain the free energy per site in the large-@xmath4 limit by means of the method of steepest descent .",
    "we find the replica - symmetric form @xmath88+\\frac{1}{2}j_0\\sum_{\\mu\\leq k}m_{\\mu}^2 + \\frac{\\alpha}{2\\beta}\\big\\{\\log[1-\\beta j_0(1-q)]\\nonumber\\\\ & -&\\frac{j_0\\beta q}{1-\\beta j_0(1-q)}\\big\\ } -\\frac{\\beta j^2(1-q)}{2\\big[1-\\beta j_0(1-q)\\big]}\\sum_{\\mu > k}\\tilde m_{\\mu}^2-\\frac{1}{4}\\beta \\delta^2(1-q^2)\\nonumber\\\\ & -&\\frac{1}{\\beta}\\big\\langle\\int dz \\log\\big\\ { 2\\cosh\\beta\\big[\\sum_{\\mu\\leq k } \\xi_{\\mu}(j_0m_{\\mu}+j\\tilde m_{\\mu})+z\\sqrt{\\alpha r}\\big]\\big\\}\\big\\rangle_{\\{\\xi_{\\mu}\\}}\\ , , \\label{14}\\end{aligned}\\ ] ] where @xmath89 is given by eq.(12 ) , @xmath90 and all quantities refer to any given layer , except for @xmath85 which refers to the previous layer . here , @xmath91 , @xmath92 and @xmath93 are the replica - symmetric parameters that will be determined as solutions of the saddle - point equations @xmath94",
    ". the condensed overlap @xmath85 for @xmath95 will be taken as an input from the previous layer and the sum over non - condensed patterns , @xmath96 , provides a further link with that layer which will be determined below .",
    "the parameters are interpreted as follows : @xmath97 is the replica symmetric average overlap with pattern @xmath98 for all replicas @xmath44 given by eq.(5 ) , while @xmath99 is the replica symmetric average of the spin - glass order parameter @xmath100 , for @xmath101 , where the brackets stand for a thermal and configurational average over the patterns . as usual ,",
    "@xmath93 accounts for the introduction of the spin - glass order parameter into the replicated partition function and it is given here by @xmath102 for @xmath101 , where @xmath103_{\\alpha\\gamma}^{-1}$ ] in which @xmath104 is the identity matrix and @xmath105 , while @xmath106 and @xmath107 .    in order to determine @xmath96 , consider the formal relationship for the derivative of the free energy with respect to the control parameter @xmath48 , which can be derived directly from eqs.(8 ) , ( 12 ) and ( 13 ) @xmath108 in replica symmetric form .",
    "note that the second term on the right - hand side contains the noise in the form @xmath109 that will be cancelled by a similar term from the last part of the equation . using the explicit expression for the free energy in eq.(14 )",
    ", one finds @xmath110 ^ 2 } \\{\\alpha [ 1-\\beta j_0(1-q)^2]\\nonumber\\\\ & + & \\beta^2 j^2(1-q)^2 \\sum_{\\mu",
    "> k}\\langle m_{\\mu , l-1}^2(\\bsigma^{l-1})\\rangle\\}\\,\\,\\,\\ , , \\label{17}\\end{aligned}\\ ] ] relating the non - condensed overlaps in two consecutive layers , where @xmath92 is the spin - glass order parameter in layer @xmath5 .",
    "a similar relationship can be derived for layer @xmath51 where @xmath111 becomes the stationary input @xmath112 for that layer . using this relationship with the saddle - point equation @xmath113 for layer @xmath51 , in order to eliminate @xmath114 , and applying again the saddle - point equation to layer @xmath5 we obtain the third recursion relation below between the macroscopic order parameters in two consecutive layers @xmath115 .",
    "thus , altogether , the saddle - point equations for the dense network in which @xmath36 yield the recursion relations @xmath116}\\,\\big\\}\\big\\rangle_{\\bxi } \\label{18}\\ ] ] @xmath117}\\,\\big\\}\\big\\rangle_{\\bxi } \\label{19}\\ ] ] @xmath118 ^ 2-j_0 ^ 2q^{\\prime}= \\beta^2 j^2(1-q)^2\\tilde r - j^2q+\\frac{j^2(1+q)}{1-\\beta j_0(1-q)}\\,\\,\\,\\ , , \\label{20}\\ ] ] + where the last equation is formally the same as that for the fully connected recurrent layers in terms of the rescaled variable @xmath119 . here",
    ", @xmath120 is the condensed overlap vector in one layer and @xmath121 in the next layer . given a state of the first layer , eqs.(18)-(20 )",
    "describe the evolution of the states of the network .",
    "we recover the fixed - point equations for a diluted purely recurrent network when @xmath41 for @xmath122 @xcite and the recursion relations for the layered feed - forward network , with full connection of units between layers , when @xmath43 for @xmath123 @xcite .",
    "for @xmath38 we recover the equations of coolen and viana @xcite .",
    "note that the synaptic dilution in the case of the dense network enters only through the variance of the noise @xmath89 in the effective intra - layer interactions .",
    "in contrast , in the extremely dilute limit @xmath124 , it is more appropriate to rescale @xcite @xmath125 , the ratio of stored patterns per mean number of _ connected _ sites , in place of rescaling @xmath93 .",
    "eq.(20 ) becomes then a recursion relation that depends explicitly on @xmath1 .",
    "following earlier work , we consider either a clamped input or a free relaxation in the first layer @xcite . in the first case , the state @xmath126 in that layer may be any randomly chosen configuration with a given condensed overlap @xmath127 .",
    "the first two recursion relations remain the same and the last one is replaced by @xmath128 ^ 2-j_0 ^ 2q^{\\prime}=j^2\\,\\,\\,\\ , .",
    "\\label{21}\\ ] ] on the other hand , in the case of a free relaxation the macroscopic state @xmath129 of the first layer that follows from eqs.(18)-(20 ) when @xmath41 is determined by the equations @xmath130}\\,\\big\\}\\big\\rangle_{\\bxi}\\\\ q&=&\\big\\langle\\int dz \\tanh^{2 } \\beta\\big\\{\\bxi.j_0\\bm + z\\sqrt{\\alpha [ \\tilde r+j_0 ^ 2(1-c)q / c]}\\,\\big\\}\\big\\rangle_{\\bxi}\\\\ \\tilde r&=&\\frac{j_0 ^ 2q}{[1-\\beta j_0(1-q)]^2 } \\,\\,\\,\\ , .",
    "\\label{24}\\end{aligned}\\ ] ] + we recover the known equations for the diluted recurrent network when @xmath122 @xcite .",
    "thus , eqs.(18)-(24 ) extend the model of coolen and viana for the case of symmetric synaptic dilution in the recurrent layers .",
    "we take next the limit @xmath18 and eliminate the parameter redundancy writing @xmath131 and @xmath132 @xcite .",
    "we turn next to the effects of dilution on the phase transitions in the model and consider three situations as in earlier work @xcite .",
    "first , we look for the storage capacity of infinitely long chains in which a stationary state is reached . then we focus attention on the first and the second layer in a search for multiple transitions between replica - symmetric states .",
    "the third transition we discuss is the replica - symmetry breaking transition at the de almeida - thouless ( at ) line .",
    "+   + 4.1 _ saturation transition in an infinitely long diluted chain _",
    "+   + we consider here the transition where the overlap disappears with an increasingly large storage of patterns .",
    "a stationary state is reached along the network when @xmath133 and eqs.(18)-(20 ) become    @xmath134}\\,\\big\\}\\big\\rangle_{\\bxi}\\\\ q&=&\\big\\langle\\int dz \\tanh^{2 } \\beta\\big\\{\\bxi.\\bm + z\\sqrt{\\alpha [ \\tilde r+j_0 ^ 2(1-c)q / c]}\\,\\big\\}\\big\\rangle_{\\bxi}\\\\ \\tilde r&=&\\frac{(1-\\omega)^2+q(1+\\omega)^2 - 2qc\\omega ( 1+\\omega ) } { 4[1-\\frac{1}{2 } c(1+\\omega)][1-c(1+\\omega)+\\omega c^2]}\\,\\,\\,\\ , , \\label{27}\\end{aligned}\\ ] ]     + where @xmath135 . taking the limit @xmath18 and focusing attention on pure states , where @xmath136 , for some @xmath137 , we can do the integrations and the averages and follow the usual procedure @xcite to reduce eqs.(25)-(27 ) to a single equation , now in terms of the scaled overlap @xmath138}\\,\\,\\ , , \\label{28}\\ ] ] where @xmath139 is given by eq . ( 27 ) and @xmath140 in the limit @xmath18",
    ". we also have    @xmath141}\\big]^{\\frac{1}{2}}\\exp(-x^2 ) \\label{29}\\ ] ]    and @xmath142^{\\frac{1}{2}}}{[(1+\\omega^2)\\,erf^2(x)\\,b(x,\\omega ) + \\frac{1}{2}\\,(1+\\omega)^2\\,\\frac{(1-c)}{c}\\,a(x,\\omega)]^{\\frac{1}{2 } } }   \\,\\,\\,\\ , , \\label{30}\\ ] ] where @xmath143[erf(x ) -\\frac{(1+\\omega)x}{\\sqrt{\\pi}}\\exp(-x^2 ) ] \\nonumber \\\\ & \\times&[erf(x)-\\frac{2\\omega x}{\\sqrt{\\pi}}\\exp(-x^2)]\\,\\,\\,\\ , , \\label{31}\\\\ b(x , \\omega)&=&erf(x)-\\frac{(\\omega + \\omega^2)}{(1+\\omega^2)}\\ , \\frac{2\\,x}{\\sqrt{\\pi}}\\exp(-x^2 )   \\label{32}\\,\\,\\,.\\end{aligned}\\ ] ] + when @xmath38 one recovers the relationship for the fully connected dual model @xcite , and when @xmath144 ( @xmath43 ) or @xmath145 ( @xmath41 ) one gets back the result for the layered network @xcite or the result for the diluted recurrent network @xcite , respectively .",
    "the numerical solution of eqs.(30)-(32 ) yields the critical storage capacity @xmath146 , for a given @xmath147 and @xmath1 , as the value of @xmath44 where the solution with finite overlap ( non - zero @xmath148 ) disappears .",
    "the result is shown in fig . 2 for various values of @xmath1 . when @xmath145 ( the purely recurrent network ) one recovers the @xmath149 for the fully connected network with @xmath38 @xcite .",
    "on the other hand , when @xmath144 ( the purely layered network ) @xmath150 for any value of @xmath1 , since the dilution is only in the recurrent part @xcite .",
    "there is a maximum value of @xmath151 for @xmath152 , when @xmath38 , in consistency with a previous result @xcite , and the maximum decreases and shifts towards the behavior of the layered network with increasing dilution .",
    "it has also been checked that although @xmath47 decreases with increasing dilution , the ratio @xmath153 , as one would expect in the extreme diluted limit @xmath124 for @xmath145 .",
    "although @xmath47 is a decreasing function of @xmath1 for all values of @xmath147 , as one would expect , the network still has a relatively large storage capacity for a moderate amount of dilution and a balanced ratio of interaction strengths .",
    "for instance , for @xmath154 and @xmath155 ( @xmath156 ) , @xmath47 is still the same as that for the fully connected network .",
    "thus , the synaptic interactions between layers do not need to be much stronger than the interactions within the layers for a typical good performance with a moderately finite degree of dilution . as pointed out in the introduction",
    ", this may be of use for biological networks .",
    "+   + 4.2 _ multiple phase transitions in the first layers _",
    "+   + we consider now the operation of the first two layers and we study the effects of dilution on multiple phase transitions that already appear at the junction between the first and the second layer @xcite . even limiting the study to pure states as we do here , with @xmath136 for some @xmath137 ,",
    "the overlap is layer dependent and we denote it by @xmath157 and @xmath158 for the first and the second layer , respectively .",
    "we deal first with the clamped operation in the first layer , with a given overlap vector @xmath127 , and take the @xmath18 limit in eqs.(18),(19 ) to obtain @xmath159 , where @xmath160}}\\,\\,\\ , , \\label{33}\\ ] ] is the appropriate scaled overlap and @xmath161 is given by eq.(21 ) . in that limit",
    ", @xmath162 and @xmath163 becomes @xmath164}\\big]^{\\frac{1}{2}}\\exp ( -y^2)\\,\\,\\ , .",
    "\\label{34}\\ ] ] when this is used we obtain the non - linear equation for @xmath165 , @xmath166=y\\sqrt{2\\alpha } \\big[1+\\rho ( \\frac{1-\\omega}{1+\\omega})^2\\big]^{\\frac{1}{2}}\\,\\,\\ , , \\label{35}\\ ] ] in which @xmath167 ^ 2\\,\\,\\ , \\label{37}\\end{aligned}\\ ] ] and @xmath168 . solution of eq.(35 ) with eqs.(36 ) and ( 37 ) yields the overlap @xmath158 for the second layer and the phase diagrams presented below .",
    "we consider next the case of free relaxation of the first layer , and restrict ourselves again to pure states .",
    "first , eqs.(22)-(24 ) have to be solved for the scaled overlap in the limit @xmath18 and when the solution is taken as an input in eqs.(18)-(20 ) , in the same limit , one obtains a new overlap @xmath158 on the second layer for this mode of operation .",
    "thus , we find first @xmath169 , where @xmath170}\\,\\,\\ , \\label{38}\\ ] ] + and @xmath171 is now given by eq.(29 ) with @xmath172 in place of @xmath148 and @xmath173 is given by the same expression as in eq.(36 ) .",
    "we find the equation @xmath174 that yields the scaled overlap @xmath172 for free relaxation of the first layer .",
    "following similar steps as for the clamped operation , with the full expression for @xmath161 in eq.(20 ) , in place of eq.(21 ) , we find formally the result given by eq.(35 ) but now with @xmath175 ^ 2\\,\\,\\ , .",
    "\\label{40}\\ ] ] when @xmath38 , @xmath176 for any @xmath157 , and we recover the results for the fully connected network .",
    "it can be seen from the above equations that , as long as the dilution remains finite , there is a clear distinction between the two modes of operation , as in the fully connected network @xcite .",
    "already when @xmath177 , in the case of a clamped input with @xmath168 , eqs.(35)-(37 ) yield a critical @xmath47 for a bifurcation of a solution with @xmath178 ( meaning @xmath179 ) , for any finite @xmath1 .",
    "in contrast , one finds that @xmath180 for free relaxation of the first layer and in that case the only solution is @xmath181 , that is , @xmath182 .    to obtain all the solutions that appear for general values of @xmath157 ( in the case of clamped input ) and @xmath1",
    ", we look for the bifurcations from eq.(35 ) that are given by this equation and its derivative with respect to @xmath165 which have to be solved simultaneously .",
    "this can be done either for the clamped input , with @xmath168 , or for the case of a free relaxation of the first layer , with @xmath183 given by eq.(40 ) .",
    "the outcome for the case of a clamped input is shown in fig . 3 for @xmath177 and in fig . 4 for @xmath184 , in both cases for various values of @xmath1 , together with the saturation transition for the infinite chain in each case , for reference .",
    "this is not the saturation transition for the chain of two layers we are discussing here , except for @xmath145 , as will be seen below .",
    "the phase diagram for the two - layer chain when @xmath177 has two regions . in region",
    "i , everywhere above the solid phase boundary @xmath182 is the only solution . in region",
    "ii , below the solid line , there are three stable solutions : @xmath185 , @xmath186 and @xmath187 , such that @xmath188 , except for @xmath145 where @xmath189 .",
    "the phase boundary of multiple solutions of region ii meets the saturation transition of the infinitely long chain at @xmath145 .",
    "this is the place where the model becomes a set of purely recurrent networks , with no interaction between layers and , hence , the stationary states of the second layer are already those of the infinite chain .",
    "the solution with overlap zero and @xmath190 , above and below the phase boundary , describes a spin - glass state .    in the case of an input @xmath191 ,",
    "one obtains two further regions of coexisting stable states shown in fig .",
    "4 . now in region ii one has two solutions , @xmath185 and @xmath186 , such that @xmath188 . in region",
    "iii there are two stable retrieval solutions with @xmath185 , @xmath192 smaller than one . in region",
    "iv there are three stable retrieval solutions @xmath185 , @xmath193 and @xmath194 small and positive , such that @xmath188 , except for @xmath145 where @xmath189 . the small solution @xmath194 vanishes precisely at @xmath145 , leaving the known results for the purely recurrent network , regardless of dilution .",
    "the phase boundaries that go up to @xmath145 again meet there the saturation transition of the infinitely long chain .",
    "again , all three regions of retrieval are reduced with increasing dilution , particularly regions iii and iv , but they are still there for a finite dilution of @xmath154 . in the case of extreme dilution only region ii",
    "is left over a tiny part of the phase diagram in terms of @xmath27 .",
    "but in this limit it is more appropriate to consider @xmath195 , which remains finite .",
    "similar results are obtained for other finite values of the fixed input overlap @xmath157 .",
    "we also analyzed the effects of free relaxation of the first layer on the performance of the second layer and found qualitatively similar results to those described here for the case of clamped input .",
    "as one can see , the presence of these regions and the coexistence of various retrieval states is a feature of the competition between the recurrent and the layer information processing , which is considerably diminished with increasing dilution .",
    "when @xmath38 we recover the earlier results for the fully connected network @xcite .",
    "+   + 4.3 _ the replica - symmetry breaking ( rsb ) transition _",
    "+   + we look here for the effect of synaptic dilution on the de almeida - thouless ( at ) transition @xcite where the replica symmetric solution for the saddle point equations ceases to be valid at the at line .",
    "there is rsb at sufficiently low @xmath54 in the fully recurrent network , that is when @xmath145 , and there is no rsb in the purely layered network with @xmath144 . in both ,",
    "the fully connected and the diluted dual model rsb appears as soon as @xmath196 , that is already for an arbitrarily small recurrent interaction @xcite . although the at line is a boundary in the @xmath197 plane and our work , as well as that in that ref . [",
    "11 ] , are restricted to @xmath50 one may get an idea of the size of the rsb region by looking at the deformation of that boundary with a decrease of @xmath147 towards @xmath144 , and that is what we do next .",
    "the at line in the present model is obtained considering the fluctuations around the replica symmetric forms for the spin glass order parameter @xmath198 and the auxiliary parameter @xmath199 given by the right - hand - side of eq.(15 ) for @xmath200 .",
    "proceeding in the usual way on the free energy prior to the assumption of replica symmetry we obtain the at line for the diluted dual model , @xmath201 ^ 2}+j_0 ^ 2(1-c)/c\\}\\big\\langle\\int dz\\,\\,\\nonumber\\\\ & \\times&\\cosh^{-4}\\beta\\big\\{\\bxi.(j_0\\bm+j\\btm)+z\\sqrt{\\alpha",
    "[ \\tilde r+j_0 ^ 2(1-c)q / c]}\\,\\big\\}\\big\\rangle_{\\bxi}\\,\\,\\ , .",
    "\\label{41}\\end{aligned}\\ ] ] we recover the expression for the purely recurrent network when @xmath122 and @xmath41 @xcite and that for the fully connected dual model when @xmath38 @xcite .",
    "the replica symmetric solution eqs.(18)-(20 ) is stable when the right - hand side of eq.(41 ) is less than one .",
    "this corresponds to the regions above the at lines shown in fig . 5 for three different values of @xmath147 when @xmath38 , @xmath154 and @xmath202 .",
    "it can be seen that the domains of rsb below the at lines continue to become smaller with a decrease of @xmath147 ( that is with reduced recurrent interactions ) towards @xmath144 where the at line coincides with the @xmath50 axis as in the fully connected network model @xcite .",
    "the effect of ( finite ) dilution is to increase the region of rsb with a shift of the at lines towards smaller values of @xmath27 .",
    "nevertheless , the effects of rsb are smaller than in the purely recurrent network justifying the analysis in this work based on the assumption of replica symmetry .",
    "we studied in this work the effects of finite symmetric dilution on the performance of a dual model that combines information processing of recurrent and feed - forward networks .",
    "the model consists of a feed - forward chain of recurrent networks and the dilution is in the symmetric synaptic interactions of the recurrent layers .",
    "our analysis extends the original work of coolen and viana with symmetric synaptic interactions between units within the layers and non - symmetric interactions between units in consecutive layers .",
    "the competition between these interactions is responsible for the behavior shown in the phase diagrams . on one hand",
    ", a small inter - layer feed - forward interaction between layers produces the multiple transitions that are found either in the case of clamped operation with finite @xmath157 or in the case of free relaxation . in the other extreme",
    ", an infinitesimal symmetric recurrent interaction already yields a rsb transition .",
    "a stationary state replica analysis was carried out in this work assuming a frozen - in dilution of synaptic interactions in order to study the saturation transition in infinitely long chains and the multiple transitions that already appear at the junction between the first and second layer .",
    "we found that finite synaptic dilution produces a gradual change of the performance of the model reducing all the regions of stable retrieval states , in particular those where multiple solutions appear due to the competition between recurrent and feed - forward processing .",
    "one would expect that the study of further initial layers should yield a performance closer to that of long chains .",
    "although the performance of the dual model is reduced by synaptic dilution , we showed that there can still be a considerable output in a long chain of layers in the case of a finite amount of dilution of the synapses within the layers and pointed out that this feature could be relevant for dual models of biological networks .",
    "there are several extensions that one may conceive of the work presented in this paper",
    ". it would be interesting to consider the effects of non - linear synapses of a general form @xcite and one could also think of relaxing the symmetry of the synaptic interactions within the layers .",
    "a dynamical approach would be necessary in that case .",
    "+   + * acknowledgments * +   + one of the authors ( wkt ) thanks desir boll for informative discussions and for the kind hospitality at the institute of theoretical physics of the catholic university of leuven , belgium .",
    "the work of the same author was financially supported , in part , by cnpq ( conselho nacional de desenvolvimento cientfico e tecnolgico ) , brazil .",
    "grants from cnpq and fapergs ( fundao de amparo  pesquisa do estado de rio grande do sul ) , brazil , to the same author are gratefully acknowledged .",
    "f. l. metz acknowledges a graduate student fellowship from cnpq .",
    "j. j. hopfield , _ proc .",
    "usa _ * 79 * , 2554 ( 1982 ) ; _ proc .",
    "usa _ * 81 * , 3088 ( 1984 ) .",
    "j. hertz , a. krogh and r. palmer , _ introduction to the theory of neural computation _",
    ", addison - wesley , reading , ma ( 1991 ) .",
    "e. domany , r. meir and w. kinzel _ europhys .",
    "_ * 2 * , 175 ( 1986 ) .",
    "r. meir and e. domany , phys .",
    "* 59 * , 359 ( 1987 ) ; _ phys.rev . a _ * 37 * , 608 ( 1988 ) .",
    "r. meir _ j. physique _ * 49 * , 201 ( 1988 ) .",
    "b. derrida and r. meir _ phys.rev .",
    "a _ * 38 * , 3116 ( 1988 ) .",
    "e. domany , w. kinzel and r. meir , j. phys .",
    "a * 22 * , 2081 ( 1989 ) .",
    "d. j. amit , h. gutfreund and h. sompolinsky , _ phys . rev .",
    "letters _ * 55 * , 1530 ( 1985 ) ; _ phys .",
    "a _ * 32 * , 1007 ( 1985 ) .",
    "d. j. amit , h. gutfreund and h. sompolinsky , _ ann . phys . _ * 173 * , 30 ( 1987 ) .",
    "a. c. c. coolen , _ handbook of biological physics , vol . 4 , p. 619",
    "_ ( f. moss and s. gielen , eds . , north - holland 2001 )",
    "a. c. c. coolen and l. viana , _ j. phys . a _ * 29 * , 7855 ( 1996 ) . k. katayama and t. horiguchi , _ physica a _ * 297 * , 532 ( 2001 ) .",
    "k. katayama , y. sakata and t. horiguchi , _ physica a _ * 310 * , 532 ( 2002 ) .",
    "k. katayama , y. sakata and t. horiguchi , _ physica a _ * 317 * , 270 ( 2003 ) .",
    "e. t. rolls , a. treves and c. perez - vicente , _ neural networks _ * 10 * , 1559 ( 1995 ) .",
    "s. r. schultz and e. t. rolls , _ hippocampus _ * 9 * , 582 ( 1999 ) .",
    "p. r. krebs and w. k. theumann _ phys.rev .",
    "e _ * 60 * , 4580 ( 1999 ) ; w. k. theumann and r. erichsen jr . _",
    "e _ * 64 * 061902 ( 2001 ) ; w. k. theumann and r. erichsen jr .",
    "_ physica a _ * 341 * 262 ( 2004 ) .",
    "d. boll and i. prez castillo , _ physica a _ * 39 * , 548 ( 2004 ) .",
    "d. boll , _ advances in condensed matter and statistical mechanics _",
    "e. korutcheva and r. cuerno , nova science publishers , new york , 2004 , p. 319 .",
    "t. verbeiren , `` dilution in recurrent neural networks '' , ph.d .",
    "thesis , k. u. leuven , leuven , belgium , 2005 .",
    "d. boll , r. erichsen jr . and",
    "t. verbeiren , _ cond - mat/0507444 _ h. sompolinsky , _ phys .",
    "a _ * 34 * , 2571 ( 1986 ) . t. l. h. watkin and d. sherrington _ europhys .",
    "* 14 * , 791 ( 1991 ) .",
    "a. canning and j .- p .",
    "i france _ * 2 * , 1791 ( 1992 ) .",
    "s. amari and k. maginu , _ neural networks _ * 1 * , 63 ( 1988 ) .",
    "m. okada , _ neural networks _ * 9 * , 1429 ( 1996 ) .",
    "r. de almeida and d. j. thouless , _ j. phys . a : math .",
    "gen . _ * 11 * , 983 ( 1978 ) .       for an infinitely long chain with various degrees of dilution , as indicated",
    ". the fully connected network , with @xmath38 , is also shown for reference .",
    "increasing dilution is described by a decreasing value of @xmath1.,width=302,height=264 ]     in the first layer for various degrees of dilution , as indicated . the saturation transition ( in dotted lines )",
    "is shown for reference and the phases i and ii are described in the text.,width=302,height=604 ]     in the first layer for various degrees of dilution , as indicated .",
    "the saturation transition ( in dotted lines ) is shown for reference and the phases i - iv are described in the text.,width=302,height=604 ]"
  ],
  "abstract_text": [
    "<S> a stationary state replica analysis for a dual neural network model that interpolates between a fully recurrent symmetric attractor network and a strictly feed - forward layered network , studied by coolen and viana , is extended in this work to account for finite dilution of the recurrent hebbian interactions between binary ising units within each layer . </S>",
    "<S> gradual dilution is found to suppress part of the phase transitions that arise from the competition between recurrent and feed - forward operation modes of the network . despite that , </S>",
    "<S> a long chain of layers still exhibits a relatively good performance under finite dilution for a balanced ratio between inter - layer and intra - layer interactions . </S>",
    "<S> +   + pacs : 87.10.+e , 64.60.cn , 07.05.mh </S>"
  ]
}