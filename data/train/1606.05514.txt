{
  "article_text": [
    "in many applications , such as monitoring or sensing systems , one may be interested in reconstructing a stochastic source @xmath0 that is not directly observable . instead , access is provided to @xmath1 correlated signals @xmath2 that are corrupted versions of @xmath0 .",
    "the goal is to reconstruct @xmath0 from limited information that one can obtain from @xmath3 . while the source coding aspect of the problem",
    "is classical in information theory ( see for instance ( * ? ? ?",
    "3.5 ) ) , its signal processing and sampling aspect has not received much attention . in this paper , we study the sampling aspect of this problem for _ stochastic _ signal @xmath0 and the @xmath1 corrupted versions @xmath4 .",
    "we take into account the fact that the correlation among the signals @xmath3 , can help decrease the sampling rate or improve the signal reconstruction accuracy .",
    "it is known that any _ deterministic _ continuous function @xmath5 defined on the interval @xmath6 $ ] can be expressed in terms of sinusoids as follows : @xmath7 , \\qquad t\\in [ 0,t],\\ ] ] where @xmath8 .",
    "if the number of non - zero coefficients @xmath9 and @xmath10 are limited , the signal @xmath5 is sparse in the frequency domain .",
    "herein , we consider a _",
    "stochastic _ signal @xmath0 , and show its fourier coefficients by random variables @xmath11 and @xmath12 .",
    "we assume that the coefficients @xmath11 and @xmath12 are zero when @xmath13 or @xmath14 for some natural numbers @xmath15 , _",
    "@xmath0 is a bandpass stochastic signal : @xmath16 , \\qquad t\\in [ 0,t],\\label{eqdefs0}\\end{aligned}\\ ] ] where the coefficients @xmath11 and @xmath12 for @xmath17 are mutually independent identically distributed ( i.i.d . )",
    "normal @xmath18 variables , _",
    "i.e. , _ the original signal is white and gaussian .",
    "we can not observe @xmath0 directly .",
    "instead , we have @xmath2 , also defined on @xmath19 $ ] , that are corrupted versions of @xmath0 .",
    "the corrupted versions of the signal can be expressed as @xmath20 , \\qquad t\\in [ 0,t],~~i\\in \\{1,2,\\cdots , k\\},\\label{eqdefs}\\end{aligned}\\ ] ] where @xmath21 and @xmath22 ; here @xmath23 and @xmath24 are independent perturbations that are added to the original signal .",
    "it is assumed that the perturbations @xmath23 and @xmath24 for @xmath25 and @xmath26 are i.i.d .",
    "variables according to @xmath27 .",
    "the perturbations are also mutually independent of the signal coefficients @xmath28 and @xmath29 for @xmath17 .",
    "the statistical model assumed for the coefficients @xmath11 , @xmath12 , @xmath24 , @xmath23 parallels the one for the  gaussian ceo problem \" @xcite .",
    "a summary of the model parameters is given in table  [ table : summary ] .",
    "we are allowed to take @xmath30 samples from the @xmath31th corrupted signal @xmath32 at time instances @xmath33 $ ] of our choice , for @xmath34 .",
    "therefore @xmath35 can be viewed as the sampling rate of the @xmath32 .",
    "we assume that the samples are noisy .",
    "the sampling noise can model quantization noise of an a / d converter , or the noise incurred by transmitting the samples to a fusion center over a communication channel .",
    "the sampling noise of each signal @xmath4 is modeled by an independent zero - mean gaussian random variable with variance @xmath36 .",
    "we use the samples to reconstruct either the remote signal @xmath0 , or the collection of corrupted signals @xmath2 .",
    "the motivation for reconstructing @xmath37 is twofold : firstly , this would parallel the literature on indirect source coding , where the reconstruction distortion of intermediate signals is shown to be equivalent with that of the original signal ( the separation theorem @xcite ) .",
    "secondly , these individual signals @xmath4 may contain some other information of interest besides @xmath0 , e.g. , the differences @xmath38 might be correlated with some other signal of interest .",
    "the reconstruction of the remote signal @xmath0 and the corrupted signal @xmath4 are denoted by @xmath39 and @xmath40 , respectively .",
    "these reconstructions are calculated using the minimum mean square error ( mmse ) criterion , _",
    "@xmath39 is the conditional expectation of the @xmath0 given all the samples .",
    "the goal is to optimize over the sampling times @xmath41 to minimize the distance between the signals and their reconstructions .",
    "more specifically , we consider the minimization @xmath42 for the remote signal , or the minimization @xmath43 for reconstruction of the @xmath1 corrupted signals @xmath4 , @xmath34 . here",
    "@xmath41 is the @xmath44th sampling time of the @xmath31th signal .    [",
    "cols=\"^,^\",options=\"header \" , ]     vectors @xmath45 and @xmath46 are correlated , because they are both corrupted versions of @xmath47 .",
    "their cross covariance can be computed as @xmath48 and @xmath49 for @xmath50 .",
    "one can verify that for any @xmath44 , the covariance matrix for the @xmath1 random variables @xmath51 for @xmath34 is equal to @xmath52    suppose that the @xmath31th signal , @xmath4 , is sampled at time instances @xmath41 for @xmath53 and @xmath54 .",
    "hence , @xmath55.\\ ] ] to represent the problem in a matrix form , we define @xmath56 to be the vector of samples as @xmath57^{\\mathsf{t}}.\\ ] ] therefore , we have @xmath58 , where @xmath45 is defined in and @xmath59 is an @xmath60 matrix of the form @xmath61moreover , the observation vector for the @xmath31th signal is of the following form @xmath62 in which @xmath63 is the @xmath31th noise vector with covariance of @xmath64 .",
    "now , we define the vector of coefficients of all the @xmath1 signals , @xmath65 , the vector of all the samples , @xmath66 , the vector of all the observations , @xmath67 , ... , as follows : @xmath68^{\\mathsf{t } } , \\label{eqn : defnsa1}\\\\ { \\textbf{\\textsf{s}}}&=[\\mathbf{s}_1^{\\mathsf{t } } , \\mathbf{s}_2^{\\mathsf{t } } , \\cdots , \\mathbf{s}_k^{\\mathsf{t}}]^{\\mathsf{t } } , \\label{eqn : defnsa2}\\\\ { \\textsf{\\textbf{y}}}&=[\\mathbf{y}_1^{\\mathsf{t } } , \\mathbf{y}_2^{\\mathsf{t } } , \\cdots , \\mathbf{y}_k^{\\mathsf{t}}]^{\\mathsf{t } } , \\label{eqn : defnsa3}\\\\ { \\textsf{\\textbf{z}}}&=[\\mathbf{z}_1^{\\mathsf{t } } , \\mathbf{z}_2^{\\mathsf{t } } , \\cdots , \\mathbf{z}_k^{\\mathsf{t}}]^{\\mathsf{t } } , \\label{eqn : defnsa4}\\\\ { \\boldsymbol{\\mathsf{\\delta}}}&=[\\mathbf{\\delta}_1^{\\mathsf{t } } , \\mathbf{\\delta}_2^{\\mathsf{t } } , \\cdots , \\mathbf{\\delta}_k^{\\mathsf{t}}]^{\\mathsf{t } } , \\label{eqn : defnsa5}\\\\ { \\mathsf{q}_\\mathsf{a}}&=[{q}_1^{\\mathsf{t } } , { q}_2^{\\mathsf{t } } , \\cdots , { q}_k^{\\mathsf{t}}]^{\\mathsf{t}}. \\label{eqn : defnsa6}\\end{aligned}\\ ] ] furthermore , let @xmath69 be the direct sum of the individual matrices @xmath59 .",
    "then , we can write @xmath70 one can verify that the covariance matrix of the noise is @xmath71 moreover , @xmath72 and @xmath73 , where @xmath74 was given in , and @xmath75 .      here , we first state a lemma , which is frequently used in the formulation and proofs of our problem .",
    "the lemma provides the two alternative forms of the lmmse estimator and the mean squared error .",
    "next we use this lemma to formulate the reconstruction of the remote signal @xmath0 and its corrupted versions @xmath4 in the subsequent subsections .",
    "@xcite[lmmse ] suppose that @xmath76 in which @xmath77 is an observation vector , @xmath78 is a known matrix , @xmath47 is a vector to be estimated and @xmath79 is an additive noise vector .",
    "in the case @xmath47 and @xmath79 are mutually independent gaussian vectors , lmmse is optimal and the estimator and the mean square error , respectively , are given by @xmath80\\right\\}={\\mathtt{tr}}(c_e ) , \\label{eqn : mmsece}\\end{aligned}\\ ] ] where the reconstruction matrix , @xmath81 , and the error covariance matrice , @xmath82 , are of the following forms : @xmath83 or alternatively @xcite , using the matrix identity @xmath84 the matrices @xmath81 and @xmath82 are given by @xmath85      here , the goal is to reconstruct @xmath0 with minimum distortion using the observation vector @xmath67 .",
    "we use the mmse criterion to minimize the average distortion subject to the samples . from the parseval s theorem , we have @xmath86 where @xmath87 is the reconstructed signal and @xmath88 is the mmse reconstruction of the coefficient vector @xmath89 from the observation vector @xmath67 .",
    "since the random variables are jointly gaussian , the mmse estimator is optimal . from the equation @xmath90 where @xmath91 , the error of the linear mmse estiamtor is equal to @xmath92\\right\\}={\\mathtt{tr}}(c^{\\mathsf a}_e ) , \\label{eqn : mmsece1}\\end{aligned}\\ ] ] where @xmath93 has the following two alternative forms @xmath94 in the above formula , the covariance matrix of @xmath95 is @xmath96      here ,",
    "the goal is to reconstruct all the @xmath1 signals with minimum distortion using the observation vector @xmath67 .",
    "again , from the parseval s theorem , we have @xmath97 in which @xmath87 and @xmath98 are the reconstructed signal and the estimated coefficients , respectively . from the equation",
    "@xmath99 the lmmse error is equal to @xmath100\\right\\}={\\mathtt{tr}}(c^{\\mathsf b}_e ) , \\label{eqn : mmsece}\\end{aligned}\\ ] ] where @xmath101 has the following two alternative forms @xmath102      we provide a number of facts about the matrices that we have introduced before .",
    "these facts can be directly verified , and will be repeatedly used in the proofs .",
    "we have listed these facts here to improve the presentation of the proofs .    1 .",
    "[ fact0 ] we have @xmath103 and @xmath104 where @xmath105 .",
    "[ fact1 ] the rows of matrix @xmath106 are vectors of norm @xmath107 .",
    "therefore , the matrix @xmath108 , for @xmath109 , is of size @xmath110 and with diagonal entries equal to @xmath111 regardless of the value of @xmath41 .",
    "therefore , @xmath112 .",
    "3 .   [ fact2 ]",
    "when , @xmath113 and are distinct , the rows of matrix @xmath114 will be perpendicular to each other .",
    "therefore , the matrix @xmath108 will be equal to @xmath115 .",
    "similarly , if @xmath41 are distinct for all @xmath116 , the rows of @xmath106 and @xmath117 for @xmath118 will be perpendicular to each other . therefore , @xmath119 for @xmath118 in this case . hence ,",
    "using the definition of @xmath120 and @xmath121 given in and , both @xmath122 and @xmath123 will become diagonal matrices @xmath124 , where @xmath105 .",
    "[ fact3 ] the diagonal elements of matrices @xmath125 for @xmath126 give us the norm of the column vectors of @xmath59 .",
    "they can be calculated as follows : @xmath127 when we use the uniform sampling strategy , i.e. , @xmath128 , the diagonal entries of @xmath125 will become equal to @xmath129 .",
    "this is because , for instance , @xmath130 where follows from the fact that @xmath131 .",
    "moreover , the off - diagonal entries will be zero .",
    "for example , consider the entry @xmath132 in fact , with uniform sampling , different columns of the matrix @xmath59 will be perpendicular to each other and @xmath125 will become @xmath133 .",
    "in this section , we state the proofs of our results . in the body of the proofs",
    ", we have used some lemmas , which are provided in the appendix .",
    "we start by computing the average distortion using equations , and as follows @xmath134 where results from the cyclic property of the trace and follows from the fact that @xmath72 .",
    "we would like to show that for any arbitrary choice of sampling time instances , @xmath41 , the average distortion will be bounded from below as follows : @xmath135   \\nonumber \\\\&\\geq 2n - n \\sum_{i=1}^{k } \\frac{m_i}{(1+\\eta)n+\\sigma_i^2}.\\label{used}\\end{aligned}\\ ] ] in other words , from , we wish to prove that @xmath136 equivalently , if we use to replace @xmath137 with @xmath138 , we would like to show that @xmath139 this can be derived using theorem [ lemmaab ] ( given in appendix [ app ] ) with matrices @xmath140 , @xmath141 and @xmath142 .",
    "from fact [ fact0 ] of section [ section : helpful - facts ] , observe that the matrices @xmath143 and @xmath144 are of the forms @xmath145 and @xmath146 these matrices satisfy the required properties of theorem [ lemmaab ] , _",
    "i.e. , _ the matrices @xmath143 and @xmath144 are positive semi - definite and @xmath147 , where the matrix @xmath148 has the form of with parameters @xmath149 and @xmath150 .",
    "therefore , we have the following inequality @xmath151 & \\leq { \\mathtt{tr}}\\left [ \\big ( f_{{\\mathsf{diag}}}+ c\\big)^{-1}g_{{\\mathsf{diag}}}\\right ] .\\end{aligned}\\ ] ]    hence , from fact [ fact1 ] of section [ section : helpful - facts ] which states that @xmath152 and @xmath153 , the desired inequality in concludes .    furthermore , when @xmath154 , we would like to show that this lower bound is tight if we take distinct time instances , @xmath41 , from the set @xmath155 .",
    "observe that this is possible since @xmath155 has @xmath111 elements .",
    "fact [ fact2 ] from section [ section : helpful - facts ] states that both the matrices @xmath122 and @xmath123 will become diagonal matrices @xmath124 , and thus @xmath137 given in will be @xmath156 hence , the minimum distortion will be @xmath157 , \\nonumber \\\\&=2n- \\sum_{i=1}^{k } \\frac{nm_i}{(1+\\eta)n+\\sigma_i^2},\\label{eqn : czsubstituted}\\end{aligned}\\ ] ] where is derived using the definition of the diagonal matrix @xmath158 given in .",
    "@xmath159      to compute the minimum average distortion , from @xmath160 , we use the alternative form of lmmse ( given in ) , in which @xmath93 is of the form @xmath161 hence , the average distortion will be @xmath162,\\label{lefthandside521}\\end{aligned}\\ ] ] which results from the facts that @xmath163 ( section [ secreconst ] ) and @xmath164 .    using the matrix identity given in for matrix @xmath78 to be @xmath165 , we have @xmath166    therefore , the matrix in the left - hand side of will be @xmath167 where results from the fact that @xmath168 , and in matrix @xmath169 stands for @xmath170 . moreover , is derived using lemma [ generalizedmean ] for positive definite matrices @xmath171 with the equality if and only if @xmath172 .",
    "notice that @xmath173 .",
    "for any two symmetric positive definite matrices @xmath78 and @xmath174 , the relation @xmath175 implies that @xmath176 .",
    "this is because the function @xmath177 is operator monotone @xcite .",
    "hence , implies that @xmath178^{-1}\\nonumber.\\end{aligned}\\ ] ] let @xmath179 .",
    "then , the relation between the traces of the above matrices is @xmath180^{-1 } \\\\&\\geq   { \\mathtt{tr}}\\left [ i+\\frac{1}{\\eta } \\left({k } i -k^2a_{{\\mathsf{diag}}}^{-1}\\right ) \\right]^{-1}\\label{peilersbi},\\end{aligned}\\ ] ] where results from lemma [ peirelslemma ] for the convex function @xmath181 when @xmath182 and the hermitian matrix @xmath183 . to find @xmath184 , we need to calculate the diagonal entries of matrix @xmath78 .",
    "they are @xmath185 since the diagonal elements of matrices @xmath125 for @xmath126 are of the following forms ( fact [ fact3 ] from section [ section : helpful - facts ] ) @xmath127 substituting the diagonal entries of the matrix @xmath78 in , we obtain @xmath186^{-1}\\nonumber \\\\ & = \\sum_{\\ell = 1}^{n } \\frac{1}{(1+\\frac{k}{\\eta})-\\frac{k^2}{\\eta } a_{\\ell}^{-1}}+\\frac{1}{(1+\\frac{k}{\\eta})-\\frac{k^2}{\\eta } b_{\\ell}^{-1}}\\nonumber \\\\&\\geq \\sum_{\\ell = 1}^{n } \\frac{2}{(1+\\frac{k}{\\eta})-\\frac{k^2}{\\eta } ( k+\\eta \\sum_{i=1}^{k } \\frac{m_i}{2\\sigma_i^2})^{-1 } } \\label{arithmatic } \\\\&=   \\frac{2n}{(1+\\frac{k}{\\eta})-\\frac{k^2}{\\eta } ( k+\\eta \\sum_{i=1}^{k } \\frac{m_i}{2\\sigma_i^2})^{-1 } } , \\label{m1minimizesd}\\end{aligned}\\ ] ] where results from convexity of the function @xmath187 for @xmath188 .",
    "now suppose that each @xmath131 for @xmath189 .",
    "if we uniformly sample the signals , _ i.e. _ , sample @xmath4 at time instances @xmath190 , from fact [ fact3 ] of section [ section : helpful - facts ] we conclude that the equality in the above equations holds , and thus the this lower bound is achieved .    @xmath159      to compute the average distortion , here we use equations , and .",
    "hence , @xmath191 where and are achieved , respectively , by the trace cyclic property and the fact that @xmath192 ( the matrix @xmath74 has been defined in ) .",
    "here we are interested in reconstructing the signals @xmath4 .",
    "following the similar steps from subsection [ proofslower1s ] , we use theorem [ lemmaab ] with the choice of @xmath193 , @xmath194 and @xmath142 . to demonstrate that these matrices have the required properties of theorem [ lemmaab ]",
    ", one can verify ( by explicit evaulation ) that @xmath143 has the same expression as in , _",
    "i.e. _ , @xmath195 is also equal to @xmath196 furthermore to compute @xmath144 , observe that @xmath197 in which @xmath198 is a matrix of the following form @xmath199 for @xmath200 .",
    "then , one can verify that @xmath201 therefore , applying theorem [ lemmaab ] for the matrices @xmath143 , @xmath144 and @xmath142 , we have @xmath202\\nonumber \\\\&\\leq { \\mathtt{tr}}[(f_{\\mathsf{diag}}+c_{{\\textsf{\\textbf{z}}}})^{-1}g_{\\mathsf{diag}}]\\nonumber \\\\&=   { \\mathtt{tr}}\\left[\\big ( ( 1+\\eta)n i + c_{{\\textsf{\\textbf{z}}}}\\big)^{-1}\\alpha",
    "n i \\right]\\label{diagfg } \\\\ & =   \\sum_{i=1}^{k } \\frac{\\alpha n m_i}{(1+\\eta)n+\\sigma_i^2},\\label{qcxqinvqcxq}\\end{aligned}\\ ] ] where results from the fact that the diagonal entries of the matrices @xmath143 and @xmath144 are @xmath203 and @xmath204 , respectively ( fact [ fact1 ] from section [ section : helpful - facts ] and ) .",
    "consequently , the average distortion , for any arbitrary choice of sampling times , can be bounded as @xmath205 + in which @xmath206 is the one defined in .",
    "furthermore , we would like to show that the equality holds when @xmath154 and we take distinct time instances , @xmath41 , from the set @xmath155 . to show that we compute the average distortion when @xmath41 are distinct and belong to @xmath155 for all @xmath116 .",
    "using fact [ fact2 ] from section [ section : helpful - facts ] , the two matrices @xmath143 and @xmath144 will become diagonal matrices of the forms : @xmath207 hence , the average distortion will be @xmath208 where is derived using the diagonal matrix @xmath158 of .",
    "@xmath159      here",
    ", we divide the proof into two parts . in the first part ,",
    "we show that @xmath209)+\\gamma\\right)^{-1},\\ ] ] and then we show that when @xmath131 ( for each @xmath210 ) , the optimal sampling strategy is uniform sampling and the minimum distortion is equal to @xmath211)+\\gamma\\right)^{-1}.\\ ] ] in the second part , we simplify the above equation to obtain the expression given in the statement of the theorem .    * part ( i ) : * to compute the minimum average distortion , we use @xmath212 and the alternative form of lmmse , where @xmath101 is of the form @xmath213 in the above formula , @xmath214 where the matrix @xmath215 is the inverse of the matrix @xmath74 , given in",
    ". therefore , the average distortion will be @xmath216^{-1}.\\label{eqn : l84}\\end{aligned}\\ ] ]    first notice that due to lemma [ hornlemma ] , matrices @xmath217 and @xmath218 are permutation similar , i.e. , there exists a unique permutation matrix @xmath219 of size @xmath220 such that @xmath221 and moreover , @xmath219 has the property @xmath222 using the permutation matrix @xmath219 and the cyclic property of the trace , we have @xmath223^{-1}\\nonumber \\\\ & = { \\mathtt{tr}}\\left(h+i\\otimes \\gamma\\right)^{-1},\\end{aligned}\\ ] ] where matrix @xmath224 denotes the permuted matrix @xmath225 .",
    "notice that the matrix @xmath226 where @xmath227 is a @xmath228 matrix defined as follows : @xmath229 if @xmath230 , and @xmath231 if @xmath232 .",
    "therefore , the matrix @xmath224 can be written as @xmath233 if we partition @xmath224 into @xmath228 submatrices @xmath234 as follows @xmath235 all the @xmath234 submatrices will be diagonal matrices because they are weighted sums of diagonal matrices @xmath227 . more precisely ,",
    "the submatrices @xmath236 for @xmath237 can be computed as follows , using fact [ fact3 ] from section [ section : helpful - facts ] that gives us the diagonal entries of @xmath125 ( the entries of matrix @xmath236 for @xmath238 and @xmath239 are the @xmath240th and the @xmath241th diagonal entries of matrices @xmath242 , given in ) , respectively ) : @xmath243 and for @xmath244 , @xmath245 applying lemma [ peirelslemma ] for the hermitian matrix @xmath78 and the convex function @xmath246 for @xmath247 , we attain a lower bound on the average distortion as : @xmath248 in which the matrix @xmath249 is the block diagonal form of matrix @xmath224 , where all the submatrices other than the @xmath250 block diagonal submatrices , @xmath251 , are zero .",
    "consequently , the matrix @xmath252 is of the form @xmath253 therefore , @xmath254 using lemma [ jointlyconvex ] ( given in appendix [ app ] ) , for any @xmath255 , we obtain @xmath256)+\\gamma\\right)^{-1}.\\end{aligned}\\ ] ] consequently , @xmath257)+\\gamma\\right)^{-1}.\\end{aligned}\\ ] ] therefore , from for any choice of sampling time instances @xmath41 , we obtain @xmath258)+\\gamma\\right)^{-1}. \\label{dblowerupper}\\end{aligned}\\ ] ]    moreover , suppose that each @xmath131 for @xmath189 .",
    "if we employ uniform strategy , the matrices @xmath125 ( @xmath126 ) become diagonal matrices with diagonal entries equal to @xmath129 ( fact [ fact3 ] from section [ section : helpful - facts ] ) .",
    "then , the matrix in the right hand side of will be @xmath259^{-1 } & = \\left({\\mathsf{diag}}([\\frac{m_1}{2\\sigma_1 ^ 2 } , \\frac{m_2}{2\\sigma_2 ^ 2 } , \\cdots , \\frac{m_k}{2\\sigma_k^2}])\\otimes i+\\gamma\\otimes i\\right)^{-1 } \\\\&=\\left({\\mathsf{diag}}([\\frac{m_1}{2\\sigma_1 ^ 2 } , \\frac{m_2}{2\\sigma_2 ^ 2 } , \\cdots , \\frac{m_k}{2\\sigma_k^2}])+\\gamma\\right)^{-1}\\otimes i_{2n\\times 2n}.\\end{aligned}\\ ] ] therefore , from and the fact that @xmath260 , we get @xmath261)+\\gamma\\right)^{-1}.\\ ] ]    therefore , from and the above inequality , we conclude that @xmath262)+\\gamma\\right)^{-1}.\\ ] ]    * part ( ii ) : * [ simplified ] so far we have shown that when @xmath131 for @xmath189 , the minimal distortion is @xmath263)+\\gamma\\right)^{-1}.\\ ] ]    here , we wish to simplify the above equation to obtain observe that @xmath264 can be computed from as follows : @xmath265 in which @xmath266 for simplicity define : @xmath267)+ \\gamma\\end{aligned}\\ ] ] therefore , we need to find @xmath268 which is equal to @xmath269 . to calculate the diagonal entries of @xmath270",
    ", we use the cramer s rule as follows : @xmath271 where @xmath272 is the remaining matrix after removing the @xmath31th row and column of @xmath78 . according to lemma [ det_lemma ] ( given in appendix [ app ] ) ,",
    "we deduce : @xmath273 in which @xmath274 .",
    "hence , @xmath275 in which is derived by replacing in and dividing both numerator and denominator by @xmath276 .",
    "observe that by our definition @xmath277 , we get @xmath278 we get the desired result by replacing @xmath279 .",
    "in this section , we state some lemmas that have been used in the proof section .",
    "a vector @xmath280 is majorized by @xmath281 if after sorting the two vectors in decreasing order , the following inequalities hold : @xmath282 a fundamental result in majorization theory states that for any hermitian matrix @xmath78 of size @xmath283 , the diagonal entries of @xmath78 are majorized by its eigenvalues @xcite .",
    "the extension of the above result to the block hermitian matrices is also true ( e.g. see ( * ? ? ?",
    "1 ) ) :    [ lemmablockmaj ] _ if a hermitian matrix @xmath78 is partitioned into block matrices @xmath284 for matrices @xmath285 , then the eigenvalues of @xmath286 are majorized by the eigenvalues of @xmath78 . _",
    "if a vector @xmath287 is majorized by @xmath288 , then for any convex functions @xmath289 , we have @xmath290  @xcite .",
    "this implies that    [ peirelslemma ] _ let @xmath291 be a closed interval in @xmath292 . for any hermitian matrix @xmath78 with eigenvalues in @xmath291 , and any convex function @xmath293 on @xmath291 , @xmath294 more generally by lemma [ lemmablockmaj ] , for a hermitian matrix @xmath78 partitioned into block matrices @xmath285 , as in , @xmath295 _      _ @xcite[generalizedmean ] let @xmath296 be non - negative weights adding up to one , and let @xmath297 be @xmath298 positive definite matrices .",
    "consider the weighted arithmetic and harmonic means of the matrices @xmath171 @xmath299 then , the following inequality holds , @xmath300 with equality if and only if @xmath301 .",
    "_    _ @xcite a real - valued continuous function @xmath302 on a real interval @xmath303 is called operator monotone if @xmath304 for hermitian matrices @xmath78 and @xmath174 with eigenvalues in @xmath303 .",
    "furthermore , @xmath293 is called operator convex if @xmath305 for any @xmath306 and hermitian matrices @xmath78 and @xmath174 with eigenvalues that are contained in @xmath303 , and @xmath293 is said to be operator concave if @xmath307 is operator convex .",
    "_    _ @xcite [ hornlemma ] let @xmath308 and @xmath309 be given positive integers and matrices @xmath78 and @xmath174 be any square matrices of sizes @xmath310 and @xmath283 , respectively .",
    "then , matrix @xmath311 is permutation similar to matrix @xmath312 , i.e. , there is a unique matrix @xmath313 such that @xmath314 where @xmath315 is the following @xmath316 permutation matrix : @xmath317 in which @xmath318 is an @xmath319 matrix such that only the @xmath320th entry is unity and the other entries are zero .",
    "furthermore , the useful following property holds @xmath321 _    _ [ jointlyconvex ] assume that @xmath322 are positive definite matrices .",
    "let @xmath323 , then @xmath324 _    the lwner - heinz theorem implies that the function @xmath325 for @xmath326 is operator convex @xcite . from the fact that @xmath323 , we conclude the desired inequality .    [ det_lemma ] _ given real non - negative @xmath327 and positive @xmath328 ,",
    "let @xmath329 then , @xmath330 _    the elementary row operations do not change the determinant .",
    "if we first subtract the first row from all the other rows , and then multiply the @xmath31th row of the matrix by @xmath331 for @xmath332 , and add it to the first row , we end up with an upper triangular matrix with diagonal entries @xmath333 , where @xmath334 since the determinant of an upper triangular matrix is equal to product of the diagonal elements , we have @xmath335",
    "[ lemmaab ] _ take two positive semidefinite matrices @xmath143 and @xmath144 of sizes @xmath310 satisfying @xmath336 , where @xmath337 is the hadamard product and @xmath148 is a matrix of the following form : @xmath338 where @xmath339 is a matrix with all one coordinates and @xmath340 and @xmath328 are two positive real numbers , where @xmath341 . then , for any positive definite diagonal matrix @xmath342 , we have @xmath151 & \\leq { \\mathtt{tr}}\\left [ \\big ( f_{{\\mathsf{diag}}}+ c\\big)^{-1}g_{{\\mathsf{diag}}}\\right ] , \\label{diagonalab}\\end{aligned}\\ ] ] where @xmath343 is a diagonal matrix formed by taking the diagonal entries of @xmath143 , and the matrix @xmath344 is defined similarly . _",
    "if the statement of theorem holds for the matrix @xmath143 , it will also hold for the matrix @xmath345 for any positive constant @xmath1 . therefore , without loss of generality , we assume that @xmath346 and hence , @xmath347 . from the hadamard product relation , this implies that @xmath143 and @xmath144 are equal on block matrices on the diagonal .",
    "let @xmath78 denote this common part , _",
    "i.e. , _ @xmath348    one can find matrix @xmath174 such that @xmath349 and @xmath350 .",
    "observe that wherever @xmath78 is non - zero , @xmath174 is zero and vice versa .",
    "substituting @xmath143 and @xmath144 in the left hand side of , one attains @xmath151 & = { \\mathtt{tr}}\\left [ \\big ( a+ b+ c\\big)^{-1 } \\big ( a+b b \\big ) \\right ] \\nonumber \\\\&={\\mathtt{tr}}\\left[(a+ b+ c)^{-1 } ( a+ b)\\right ] + { \\mathtt{tr}}\\left[(a+ b+ c)^{-1 }   ( b-1)b ) \\right ] .",
    "\\label{twoterms}\\end{aligned}\\ ] ] we will show that the first term in the above formula is less than or equal to the right hand side of and the second term is non - positive .    start with the first term of @xmath351   = { \\mathtt{tr}}(i_{m\\times m})- { \\mathtt{tr}}\\left((a+ b+ c)^{-1 } c\\right ) , \\label{abcab}\\end{aligned}\\ ] ] where the second term in can be bounded as follows : @xmath352\\nonumber \\\\ & \\geq { \\mathtt{tr}}\\left[\\big ( c^{\\frac{-1}{2 } } a_{{\\mathsf{diag } } } c^{\\frac{-1}{2 } }   + i\\big)^{-1 } \\right].\\label{piereluse1}\\end{aligned}\\ ] ] the last inequality comes from lemma [ peirelslemma ] . + hence , will be bounded as @xmath351   & \\leq { \\mathtt{tr}}(i_{m\\times m})- { \\mathtt{tr}}\\left[\\big ( c^{\\frac{-1}{2 } } a_{{\\mathsf{diag } } } c^{\\frac{-1}{2 } }   + i\\big)^{-1 } \\right],\\label{term1 } \\\\&= { \\mathtt{tr}}[(a_{{\\mathsf{diag } } } + c)^{-1}(a_{{\\mathsf{diag}}}+c ) ] - { \\mathtt{tr}}[c^{\\frac{1}{2 } } ( a_{{\\mathsf{diag}}}+c)^{-1 } c^{\\frac{1}{2 } } ] \\\\&= { \\mathtt{tr}}[(a_{{\\mathsf{diag } } } + c)^{-1}a_{{\\mathsf{diag } } } ] \\label{term1_b } \\\\ & = { \\mathtt{tr}}[(f_{{\\mathsf{diag } } } + c)^{-1}g_{{\\mathsf{diag } } } ] , \\label{desired}\\end{aligned}\\ ] ] in which comes from the trace interchange property and is derived since @xmath78 is defined to be the common part of the two matrices @xmath143 and @xmath144 .    to complete the proof , it remains to show that the right hand side of is non - positive , i.e. , @xmath353 \\leq 0.\\label{secondterm1}\\end{aligned}\\ ] ] since we have assumed that @xmath354 , we need to show that the trace function is non - positive . for the positive definite matrix @xmath355 , we have @xmath356 in which the inequality follows from the trace interchange property and is derived using lemma [ peirelslemma ] for the hermitian matrix @xmath357 . note that the matrix @xmath358 is a block off - diagonal matrix , since matrices @xmath78 and @xmath174 has been defined to be , respectively , block diagonal and off - diagonal matrices such that wherever @xmath51 is zero , @xmath359 is non - zero and vice versa .",
    "this completes the proof of theorem [ lemmaab ] .",
    "e. mohammadi and f. marvasti ,  sampling and distortion tradeoffs for bandlimited periodic signals , \" sampling theory and applications ( sampta ) , 2015 international conference on , pp . 468 - 472 , arxiv 1405.3980 .",
    "r. masiero , g. quer , d. munaretto , m. rossi , j. widmer , and m. zorzi ,  data acquisition through joint compressive sensing and principal component analysis , \" in ieee globecom 2009 , honolulu , hawaii , us , nov .- dec .",
    "2009 .",
    "we fisrt prove that for fixed values of @xmath30 s , the optimal distortion is attained when @xmath360 are chosen distinctly from the set @xmath155 . with this optimal choice , the matrices @xmath361 and @xmath362 will be diagonal and of the forms @xmath363 and @xmath364 , respectively .",
    "consequently , the minimum distortion will be @xmath365 therefore , we wish to show that for any arbitrary choice of sampling epochs , the distortion will be bounded as follows : @xmath366 or alternatively @xmath367.\\label{qcxqinvqcxq}\\end{aligned}\\ ] ] using matrices @xmath78 and @xmath174 defined in and , we have      substituting the above equations in the right hand side of and using theorem [ lemmaab ] , we attain the desired result as @xmath371 & \\leq   \\alpha n\\cdot { \\mathtt{tr}}\\left[\\big ( ( 1+\\eta)n i + c_{{\\textsf{\\textbf{z}}}}\\big)^{-1 } \\right]\\nonumber \\\\ & = \\alpha n \\sum_{i=1}^{k } \\frac{m_i}{(1+\\eta)n+\\sigma_i^2}.\\label{diagonalized}\\end{aligned}\\ ] ]    it remains to show that for the values of @xmath30 s such that @xmath372 is fixed , we should take the sampling epochs from the less noisy signal .",
    "the proof is the same as the one mentioned in _",
    "part 1 _ and thus we have @xmath373"
  ],
  "abstract_text": [
    "<S> consider a continuous signal that can not be observed directly . </S>",
    "<S> instead , one has access to multiple corrupted versions of the signal . </S>",
    "<S> the available corrupted signals are correlated because they carry information about the common remote signal . </S>",
    "<S> the goal is to reconstruct the original signal from the data collected from its corrupted versions . </S>",
    "<S> known as the indirect or remote reconstruction problem , it has been mainly studied in the literature from an information theoretic perspective . </S>",
    "<S> a variant of this problem for a class of gaussian signals , known as the  gaussian ceo problem \" , has received particular attention ; for example , it has been shown that the problem of recovering the remote signal is equivalent with the problem of recovering the set of corrupted signals ( separation principle ) .    </S>",
    "<S> the information theoretic formulation of the remote reconstruction problem assumes that the corrupted signals are uniformly sampled and the focus is on optimal compression of the samples . </S>",
    "<S> on the other hand , in this paper we revisit this problem from a sampling perspective . more specifically , </S>",
    "<S> assuming restrictions on the sampling rate from each corrupted signal , we look at the problem of finding the best sampling locations for each signal to minimize the total reconstruction distortion of the remote signal . in finding the sampling locations </S>",
    "<S> , one can take advantage of the correlation among the corrupted signals . </S>",
    "<S> the statistical model of the original signal and its corrupted versions adopted in this paper is similar to the one considered for the gaussian ceo problem ; _ i.e. , _ we restrict to a class of gaussian signals .    </S>",
    "<S> our main contribution is a fundamental lower bound on the reconstruction distortion for any arbitrary nonuniform sampling strategy . </S>",
    "<S> this lower bound is valid for any sampling rate . </S>",
    "<S> furthermore , it is tight and matches the optimal reconstruction distortion in low and high sampling rates . </S>",
    "<S> moreover , it is shown that in the low sampling rate region , it is optimal to use a certain nonuniform sampling scheme on all the signals . on the other hand , in the high sampling rate region , it is optimal to uniformly sample all the signals . </S>",
    "<S> we also consider the problem of finding the optimal sampling locations to recover the set of corrupted signals , rather than the remote signal . unlike the information theoretic formulation of the problem in which these two problems were equivalent , </S>",
    "<S> we show that they are not equivalent in our setting . </S>"
  ]
}