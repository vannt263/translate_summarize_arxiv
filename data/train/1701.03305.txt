{
  "article_text": [
    "shannon theoretic information theory originally focuses on the asymptotic performance .",
    "since the block length of any real code is finite , analysis with finite - blocklength is more important in a practical setting .",
    "although the tight analysis is possible in the asymptotic regime , it is almost impossible in the finite - length regime .",
    "hence , we usually take a strategy to find a good upper and lower bounds of the decoding error probability in the finite - length regime . since lower and upper bounds are not unique",
    ", we need several requirements for the bounds to clarify their goodness .",
    "one is the asymptotic tightness .",
    "that is , we impose the first condition that the limit of the bound attains one of the following regimes ; ( 1 ) second order , ( 2 ) moderate deviation , and ( 3 ) large deviation . to satisfy the above requirement",
    ", one may use the minimum value with respect to so many parameters .",
    "if the calculation complexity for the bound is too huge , it can not be used in a practical use because we can not calculate the bound . to estimate the optimal performance for a given blocklength @xmath0",
    ", we need to impose the second condition that its calculation complexity is not so large , e.g. , @xmath1 , @xmath2 , or @xmath3 .",
    "usually , the channel coding is discussed with the message subject to the uniform distribution . however , in the real communication , the message is not necessarily subject to the uniform distribution . to resolve this problem",
    ", we often consider the channel coding with the message subject to the non - uniform distribution .",
    "such a problem is called source - channel joint coding and has been actively studied by several researchers @xcite .    as a simple case",
    ", we often assume that the message is subject to the independent and identical distribution . in this case",
    ", the capacity is given as the ratio of the conventional channel capacity to the entropy of the message .",
    "recently , wang - ingber - kochman @xcite and kostina - verd @xcite discussed the second - order coefficient in this problem , in the same setting , the papers @xcite derived the exponential decreasing rate of the minimum decoding error probability when the information source is subject to an independent and identical distribution and the channel is a discrete memoryless channel . when the information source obeys a markovian process and the channel is additive noisy channel whose additive noise simply obeys markovian process , the paper @xcite derived the exponential decreasing rate of the minimum decoding error probability , and the paper @xcite derived the moderate deviation of the same error probability .",
    "the recent paper @xcite discussed the channel coding when the distribution of the additive noise in the channel is decided by the channel state , and the channel state is observed by the receiver and is subject to markovian process .",
    "for example , gilbert - elliot channel with state - information available at the receiver is written as a special case of the former setting , but can not be written as a special case of the latter setting .",
    "hence , it is needed to treat such a general situation to adopt a more realistic situation . in this paper , we focus on two kinds of assumptions ( assumptions 1 and 2 ) for such generalized additive noise channels . under these assumptions for channels ,",
    "we address joint source - channel coding with markovian source .",
    "the contribution of this paper is the following two points .",
    "one is to derive large and moderate deviation bounds under the above general setting , which are the generalizations of the results by the papers @xcite .",
    "the other is to derive upper and lower bounds of the decoding error probability that match in the large deviation regime in the above general setting .",
    "the results of our paper summarized as follows    .summary of results . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     assumption 1 contains assumption 2 .",
    "`` finite '' , `` ld '' , and `` md '' express the finite - length bound , the large deviation bound , and the moderate deviation bound , respectively .    the remaining part of this paper is organized as follows . in section [ s2 ] , we prepare several information quantities for markovian process .",
    "section [ s3 ] prepares several useful functions for finite - length analysis .",
    "section [ s4 ] explains several useful lemmas under the single shot setting .",
    "section [ s4 ] shows our main results , i.e. , our finite - length bounds and large and moderate deviation bounds .",
    "section [ s6 ] gives our numerical analysis based on our finite - length bounds .",
    "in this section , we introduce some information measures and their properties will be used in latter sections .",
    "since this paper addresses finite - length setting and the large deviation analysis , we need the conditional rnyi entropy .",
    "when the joint distribution is given to be @xmath4 the conditional rnyi entropy relative to @xmath5 is given as @xmath6    dependently of the choice fo the distribution @xmath7 , we have the upper and lower types of conditional rnyi entropy .",
    "@xmath8    where @xmath9^{\\frac{1}{1-\\theta}}}{\\sum_{y'}[\\sum_{x } p_{xy}(x , y')^{1-\\theta}]^{\\frac{1}{1-\\theta}}}. \\end{aligned}\\ ] ] to connect these two types of conditional rnyi entropy , we often focus on the following type of conditional rnyi entropy    @xmath10    for @xmath11 , we define rnyi divergence @xmath12    using rnyi divergence , we introduce two types of rnyi mutual informations @xmath13      since this paper address the markovian information source , we prepare several information measures given in @xcite for an ergodic and irreducible transition matrix @xmath14 on @xmath15 . for this purpose ,",
    "we employ two assumptions on transition matrices , which were introduced by the paper @xcite .",
    "we assume the following condition for a transition matrix @xmath16 : @xmath17 for every @xmath18 and @xmath19 .    when this condition holds , a transition matrix @xmath20 is called non - hidden ( with respect to @xmath21 ) .",
    "we assume one of the following conditions for a transition matrix @xmath20 :    1",
    ".   for every @xmath22 and @xmath23 , @xmath24 is well defined , i.e. , the right hand side of ( [ a2 ] ) is independent of @xmath25 .",
    "+ when this condition holds , a transition matrix @xmath20 is called strongly non - hidden ( with respect to @xmath21 ) .",
    "2 .   @xmath26 .",
    "+ when this condition holds , a transition matrix @xmath20 is called singleton .",
    "assumption 1 is acquired from ( [ a2 ] ) by substituting @xmath27 , so assumption 2 implies assumption 1 . when a transition matrix on @xmath16 satisfies assumption 1 , we define the marginal @xmath28 by @xmath29 .",
    "for the transition matrix @xmath30 on @xmath31 , we also define @xmath32 .",
    "then , when another transition matrix @xmath33 on @xmath31 satisfies @xmath34 , we define    @xmath35    where @xmath36 is the perron - frobenius eigenvalue of @xmath37    then , the lower type of conditional rnyi entropy for the transition matrix @xcite is given as    @xmath38    also , when @xmath20 satisfies assumption 2 , the upper type of conditional rnyi entropy for the transition matrix @xcite is given as    @xmath39    furthermore , we define the information measure which is counterpart of ( [ renyi3 ] ) .",
    "for this purpose , we introduce the following @xmath40 matrix :    @xmath41    let @xmath42 be the perron - frobenius eigenvalue of @xmath43 .",
    "then , we define the two - parameter conditional rnyi entropy @xcite by    @xmath44    for @xmath45 , we define the conditional rnyi entropy for @xmath20 by    @xmath46    also , we define following quantity .",
    "@xmath47 } { \\theta } .",
    "\\label{vw}\\end{aligned}\\ ] ]    according to @xcite , using ( [ hw ] ) and ( [ vw ] ) , we obtain the following two expansions .    @xmath48    around @xmath45 .    under these preparations",
    ", we have three lemmas as follows . for these lemmas",
    ", we define the random variable @xmath49 where @xmath50 are i. i. d. random variables and each random variables @xmath51 obeys the same distribution as the random variable @xmath52 .",
    "also we define the random variable @xmath53 in the same way .",
    "* lemma 9 ) suppose that a transition matrix w satisfies assumption 1 .",
    "let @xmath54 and @xmath55 be the eigenvector of @xmath56 with respect to the perron - frobenius eigenvalue @xmath57 such that @xmath58 .",
    "let @xmath59 .",
    "then , we have @xmath60 where @xmath61    ( * ? ? ?",
    "* lemma 10 ) suppose that a transition matrix w satisfies assumption 2 .",
    "then , we have @xmath62 where @xmath63 and @xmath64 is defined as follows :    for the non - hidden case , we define the @xmath40 matrix @xmath65 so that @xmath66^{\\frac{1}{1-\\theta } } , \\end{aligned}\\ ] ] and @xmath55 be the eigenvector of @xmath67 with respect to the perron - frobenius eigenvalue @xmath68 such that @xmath69 .",
    "let @xmath70 be the @xmath71-dimensional vector defined by @xmath72^{\\frac{1}{1-\\theta}}. \\end{aligned}\\ ] ] then , @xmath63 and @xmath64 are defined as : @xmath73    for the singleton case , let @xmath74 and @xmath55 be the eigenvector of @xmath56 with respect to the perron - frobenius eigenvalue @xmath57 such that @xmath75 .",
    "let @xmath76 .",
    "then , @xmath77 and @xmath78 are define as : @xmath79    ( * ? ? ?",
    "* lemmas 9 and 11 ) suppose that a transition matrix @xmath16 satisfies assumption 2 .",
    "then , we have @xmath80 where @xmath81 and @xmath82 are defined as follows :    for the non - hidden case with respect to @xmath21 , let @xmath83 be the eigenvector of @xmath84 with respect to the perron - frobenius eigenvalue @xmath42 such that @xmath85 .",
    "let @xmath86 be the @xmath71-dimensional vector defined by @xmath87 \\left [ \\sum_x p_{x_1y_1}(x , y)^{1-\\theta ' } \\right ] ^{\\frac{\\theta}{1-\\theta'}}. \\end{aligned}\\ ] ] then , @xmath81 and @xmath82 are defined as : @xmath88 for @xmath89 and @xmath90 for @xmath91 .",
    "for the singleton case , we define @xmath92 and @xmath93 by and independently of @xmath94 .",
    "now , to deal with joint source and channel coding , we newly introduce some functions related with three random variables @xmath95 and @xmath96 . for @xmath97 and @xmath98 ,",
    "we define following function . @xmath99",
    "( \\theta ) : = r \\theta h_{1-\\theta}(m ) + \\theta h_{1-\\theta}(p_{xz}|q_y ) . \\end{aligned}\\ ] ] also we define its derivative @xmath100 ( \\theta ) : = \\frac{d}{d\\theta } u[p_{xz } , q_y ; r ] ( \\theta).\\end{aligned}\\ ] ] since @xmath101 ( \\theta ) $ ] is convex function , @xmath102 ( \\theta)$ ] is monotonically increasing function .",
    "hence , we can define its inverse function @xmath103 ( a)$ ] by @xmath100 ( \\theta[p_{xz } , q_y ; r](a ) ) = a , \\end{aligned}\\ ] ] for @xmath104 , where @xmath105 ( \\theta)$ ] and @xmath106 ( \\theta)$ ] .",
    "when we define @xmath107 ( a ) : = ( 1 - \\theta[p_{xz } , q_y ; r ] ( a))a   + u[p_{xz } , q_y ; r ] ( \\theta[p_{xz } , q_y ; r ] ( a))\\end{aligned}\\ ] ] for @xmath104 , the derivative is calculated to be @xmath108 ( a ) } { da } = ( 1-\\theta(a ) ) .",
    "\\end{aligned}\\ ] ] hence , @xmath109 ( a)$ ] is monotonically increasing function of @xmath104 .",
    "thus , we can define the inverse function @xmath110 ( r)$ ] by @xmath107 ( a[p_{xz } , q_y ; r ] ( r ) ) = r , \\end{aligned}\\ ] ] for @xmath109 ( \\underline{a } ) < r \\le r h_0(m ) + h_0(x|z)$ ] .",
    "we define similar functions for two transition matrices @xmath111 on @xmath112 and @xmath113 on @xmath114 .",
    "suppose that @xmath115 is non - hidden with respect to @xmath116 , i.e. , satisfies assumption 1 .    for @xmath117 and @xmath98",
    ", we define @xmath118 ( \\theta ) : = & r \\theta h_{1-\\theta}^{w_s}(m ) + \\theta h_{1-\\theta}^{w_c , \\downarrow}(x|z ) , \\\\",
    "u[w_s , w_c , \\downarrow ; r ] ( \\theta ) : = & \\frac{d}{d\\theta } u[w_s , w_c , \\downarrow ; r ] ( \\theta).\\end{aligned}\\ ] ] using above two functions , we define @xmath119(a ) & : = ( u[w_s , w_c , \\downarrow ; r])^{-1 } ( a ) , \\\\ r[w_s , w_c , \\downarrow ; r ] ( a )   & : = ( 1 - \\theta[w_s , w_c , \\downarrow ; r ] ( a))a   + u[w_s , w_c , \\downarrow ; r ] ( \\theta[w_s , w_c , \\downarrow ; r ] ( a ) ) , \\end{aligned}\\ ] ] for @xmath104 , where @xmath120 ( \\theta)$ ] and @xmath121 ( \\theta)$ ] . moreover , we define @xmath122 ( r ) : = ( r[w_s , w_c , \\downarrow ; r])^{-1 } ( r ) , \\end{aligned}\\ ] ] for @xmath123 ( \\underline{a } ) < r \\le r h_0^{w_s } ( m )   + h_0^{w_c , \\downarrow}(x|z)$ ] .",
    "now , we suppose that @xmath115 satisfies assumption 2 . for @xmath117 and @xmath124",
    ", we define @xmath125 ( \\theta ) : = & r \\theta h_{1-\\theta}^{w_s}(m ) + \\theta h_{1-\\theta , 1-\\theta'}^{w_c}(x|z ) , \\\\",
    "u[w_s , w_c , \\theta ' ; r ] ( \\theta ) : = & \\frac{d}{d\\theta } u[w_s , w_c , \\theta ' ; r ] ( \\theta).\\end{aligned}\\ ] ] when @xmath126 we also define for @xmath117 and @xmath98 , @xmath127 ( \\theta ) : = & r \\theta h_{1-\\theta}^{w_s}(m ) + \\theta h_{1-\\theta}^{w_c , \\uparrow}(x|z ) , \\\\",
    "u[w_s , w_c , \\uparrow ; r ] ( \\theta ) : = & \\frac{d}{d\\theta } u[w_s , w_c , \\uparrow ; r ] ( \\theta).\\end{aligned}\\ ] ] using above two functions , we define @xmath128(a ) & : = ( u[w_s , w_c , \\uparrow ; r])^{-1 } ( a ) , \\\\",
    "r[w_s , w_c , \\uparrow ; r ] ( a )   & : = ( 1 - \\theta[w_s , w_c , \\uparrow ; r ] ( a))a   + u[w_s , w_c , \\uparrow ; r ] ( \\theta[w_s , w_c , \\uparrow ; r ] ( a ) ) , \\end{aligned}\\ ] ] for @xmath104 , where @xmath129 ( \\theta)$ ] and @xmath130 ( \\theta)$ ] .",
    "moreover , we define @xmath131 ( r ) : = ( r[w_s , w_c , \\uparrow ; r])^{-1 } ( r ) , \\end{aligned}\\ ] ] for @xmath132 ( \\underline{a } ) < r \\le r h_0^{w_s}(m )   + h_0^{w_c \\uparrow}(x|z)$ ] .",
    "we first present the problem formulation by the single shot setting .",
    "assume that the message @xmath133 takes values in @xmath134 and is subject to the distribution @xmath135 .",
    "for a channel @xmath136 with input alphabet @xmath137 and output alphabet @xmath31 , a channel code @xmath138 consists of one encoder @xmath139 and one decoder @xmath140 .",
    "the average decoding error probability is defined by @xmath141 : = \\sum_{m \\in { \\cal m } } p_m(m ) w_{y|x}(\\{b:{\\mathsf{d}}(b ) \\neq m \\}|{\\mathsf{e}}(m ) ) .",
    "\\end{aligned}\\ ] ] for notational convenience , we introduce the minimum error probability under the above condition : @xmath142 .",
    "\\end{aligned}\\ ] ]        we introduce several lemmas for the case when @xmath134 is the set of messages to be sent , @xmath135 is the distribution of the messages , and @xmath143 is the channel from @xmath137 to @xmath31 .",
    "we have the following single - shot lemma for the direct part .",
    "* lemma 3.8.1 ) for any constant @xmath144 and for any @xmath145 , there exists a code @xmath146 such that @xmath141 \\le ( p_m \\times p_x \\times w_{y|x } ) \\ { ( p_m \\times p_x \\times w_{y|x } ) ( m , x , y ) \\le c ( p_x \\times \\bar{w}_{y } ) ( x ,",
    "y ) \\}+ \\frac{1}{c } , \\label{si , di , le1}\\end{aligned}\\ ] ] where @xmath147 and @xmath148",
    ".    from above proposition , we obviously have following corollary .",
    "@xmath149    since the proof of this lemma is crucial for our proof of the next novel lemma , we give a proof of this lemma as follows .",
    "we prove this lemma by using the random coding method .",
    "for the code @xmath150 , we independently choose @xmath151 subject to @xmath152 .",
    "define @xmath153 and define decoding region of message @xmath154 as @xmath155 .",
    "the error probability of this code can be evaluated as : @xmath156 \\nonumber \\\\ \\le &   \\sum_{m}p_m(m )   \\big ( w_{y|x={\\mathsf{e}}(m ) }   \\{p_m(m ) w_{y|x={\\mathsf{e}}(m)}(y ) < c \\bar{w}_{y } ( y ) \\ } \\nonumber \\\\ & \\qquad \\qquad + \\sum_{m'\\neq m } w_{y|x={\\mathsf{e}}(m ) }   \\{p_m(m ' ) w_{y|x={\\mathsf{e } } ( m')}(y ) \\ge c \\bar{w}_{y } ( y ) \\",
    "} \\big).\\label{si , di3}\\end{aligned}\\ ] ] taking the average for the random choice , the first term is @xmath157 and the second term is @xmath158 combining ( [ si , di3 ] ) , ( [ si , di1 ] ) and ( [ si , di2 ] ) , we have @xmath159 \\le ( p_m \\times p_x \\times w_{y|x } ) \\ { ( p_m \\times p_x \\times w_{y|x } ) ( m , x , y ) \\le c ( p_x \\times \\bar{w}_{y } ) ( x , y ) \\}+ \\frac{1}{c}.\\end{aligned}\\ ] ] consequently , there must exist at least one deterministic code @xmath160 satisfying @xmath141 \\le ( p_m \\times p_x \\times w_{y|x } ) \\ { ( p_m \\times p_x \\times w_{y|x } ) ( m , x , y ) \\le c ( p_x \\times \\bar{w}_{y } ) ( x , y ) \\}+ \\frac{1}{c}.\\end{aligned}\\ ] ]    from the above proof , we also find the following single - shot lemma for the direct part .",
    "[ si , di , l1 ] for any constant @xmath144 and for any distribution @xmath161 , we have @xmath162 where @xmath163 is a counting measure on @xmath134 .",
    "the choice @xmath164 gives the minimum upper bound .",
    "we also have following lemma .",
    "[ si , di , l2 ] @xmath165    lemma [ si , di , l1 ] from ( [ ss , di ] ) in the proof of proposition [ si , di , le ] , we can evaluate the second term of ( [ si , di1 ] ) as @xmath166 so , we obtain ( [ 4 ] ) .",
    "next , we prove that the right hand side of ( [ 4 ] ) is minimized when @xmath164 . for any @xmath167",
    ", we can evaluate the right hand side of ( [ 4 ] ) as : @xmath168    lemma [ si , di , l2 ] for any @xmath169 , we have @xmath170    however , even when @xmath171 is subject to the uniform distribution , the upper bound ( [ 5 ] ) is not so tight . in the uniform case , the gallager bound is tighter than the upper bound ( [ 5 ] ) .",
    "so , modifying the derivation of the gallager bound , we derive joint source and channel coding version of the gallager bound as follows .    for any distribution @xmath172",
    ", we have @xmath173 for any @xmath174 $ ] .    for encoder",
    ", we independently choose @xmath175 subject to @xmath152 , and for decoder , we define decoding region of the message @xmath176 as @xmath177    and we also define + @xmath178 then , for any @xmath179 and @xmath180 , @xmath181 and error probability can be represented by @xmath141 = \\sum_{i , y}p_m(i)w_{y|x={\\mathsf{e}}(i)}(y)\\bigtriangleup_{i , mp}(y ) .",
    "\\end{aligned}\\ ] ] so that , @xmath141 & = \\sum_{i , y}p_m(i)w_{y|x={\\mathsf{e}}(i)}(y)\\bigtriangleup_{i , mp}(y)\\\\ & \\le \\sum_{i , y}p_m(i)w_{y|x={\\mathsf{e}}(i)}(y)\\left ( \\sum_{j}\\frac{(p_m(j)w_{y|x={\\mathsf{e}}(i)}(y))^{1-s}}{(p_m(i)w_{y|x={\\mathsf{e}}(i)}(y))^{1-s } } \\right)^t\\\\ & \\le \\sum_{i , y}p_m(i)^{1-t(1-s)}w_{y|x={\\mathsf{e}}(i)}(y)^{1-t(1-s)}\\left ( \\sum_{j } ( p_m(j)w_{y|x={\\mathsf{e}}(i)}(y))^{1-s } \\right)^t.\\end{aligned}\\ ] ] taking the average for the random choice , we have @xmath182 \\nonumber \\\\ & \\le   \\sum_{i , y}p_m(i)^{1-t(1-s)}e_\\phi w_{y|x={\\mathsf{e}}(i)}(y)^{1-t(1-s)}\\left ( \\sum_{j } p_m(j)^{1-s } e_\\phi w_{y|x={\\mathsf{e}}(i)}(y)^{1-s } \\right)^t \\nonumber \\\\ & \\le \\sum_{i , y}p_m(i)^{1-t(1-s)}\\sum_x p_x(x ) w_{y|x}(y|x)^{1-t(1-s)}\\left ( \\sum_{j } p_m(j)^{1-s } \\sum_x p_x(x ) w_{y|x}(y|x)^{1-s } \\right)^t \\nonumber \\\\ & = \\sum_{i}p_m(i)^{1-t(1-s ) } \\sum_{y}\\left ( \\sum_x p_x(x ) w_{y|x}(y|x)^{1-t(1-s)}\\right ) \\left ( \\sum_{j } p_m(j)^{1-s}\\right)^t \\left ( \\sum_x p_x(x ) w_{y|x}(y|x)^{1-s } \\right)^t .",
    "\\label{si , di , l3}\\end{aligned}\\ ] ] by setting @xmath183 in , we have @xmath184 hence , we have @xmath185   \\ge e^{\\frac{s}{1-s } ( h_{1-s}(m)-i_{1-s}^\\uparrow ( x;y|p_x \\times w_{y|x } ) ) } .",
    "\\label{si , di , l5}\\end{aligned}\\ ] ] means that there must exist at least one deterministic code @xmath160 satisfying @xmath141 \\ge e^{\\frac{s}{1-s } ( h_{1-s}(m)-i_{1-s}^\\uparrow ( x;y|p_x \\times w_{y|x } ) ) } .",
    "\\label{si , di , l6}\\end{aligned}\\ ] ] since @xmath180 , @xmath186 is restricted to @xmath187 .",
    "so we obtain ( [ 6 ] ) .",
    "now , we proceed to the case when the channel is conditional additive .",
    "assume that @xmath137 is a module and @xmath31 is given as @xmath188 .",
    "then , the channel @xmath16 is called conditional additive @xcite when there exists a joint distribution @xmath189 such that @xmath190 then we can simplify ( [ 3 ] ) .",
    "we have following lemma .",
    "when the channel is conditional additive channel , it follows that @xmath191    by setting that @xmath152 is the uniform distribution and choosing the random variables @xmath192 and @xmath193 to the right hand side of ( [ 3 ] ) , we have @xmath194 where @xmath195 . hence , ( [ 3 ] ) can be simplified to @xmath196    also we can simplify ( [ 5 ] ) and ( [ 6 ] ) .",
    "we have following lemma .",
    "[ lembc ] when the channel is conditional additive channel , it follows that @xmath197 and @xmath198    firstly , we prove .",
    "@xmath199 is represented as : @xmath200 assume that @xmath201 and its random variable is @xmath202 .",
    "setting @xmath203 , @xmath204 and @xmath205 is uniform distribution , we have    @xmath206    substituting to , we have .    and also we have @xmath207 substituting to , we have .",
    "firstly , combining the idea of meta converse @xcite and ( * ? ? ?",
    "* lemma 4 ) and the general converse lemma for the joint source and channel coding ( * ? ? ?",
    "* lemma 3.8.2 ) , we obtain the following lemma for the single shot setting .",
    "the following lemma is the same as ( * ? ? ?",
    "* lemma 3.8.2 ) when @xmath7 is @xmath208 .    for any constant @xmath144 , any code @xmath146 and any distribution @xmath5 on @xmath21",
    ", we have @xmath209    first , we set @xmath210 and for each @xmath211 , define @xmath212 also , for decoder @xmath213 and each @xmath214 , we define @xmath215 in addition , we define @xmath216 so that @xmath217 using this , we define @xmath218    then , @xmath219 . \\label{sbcl1p1}\\end{aligned}\\ ] ] the last equality follows since the error probability can be written as @xmath141 = \\sum_{(m , x ) \\in { \\cal m , x } } \\sum_{y \\in { \\cal d}^c(m)}p_{mx}(m , x ) w_{y|x}(y|x ) .",
    "\\end{aligned}\\ ] ] we notice here that @xmath220 for @xmath221 . by substituting this into ( [ sbcl1p1 ] ) ,",
    "the first term of ( [ sbcl1p1 ] ) is @xmath222 which implies ( [ 2 ] ) .",
    "now , we proceed to the conditional additive case given in . applying to the conditional additive case ,",
    "we obtain following lemma .",
    "[ sbcl ] for arbitrary distribution @xmath223 , we have @xmath224    for some @xmath225 , we substitute @xmath226 to .",
    "then , the first term of the right hand side of ( [ e ] ) is @xmath227 so , we obtain ( [ e ] ) .",
    "similar to ( * ? ? ?",
    "* theorem 5 ) , using the monotonicity of rnyi divergence , we obtain another type of converse lemma .",
    "[ sbc ] we set @xmath228 .",
    "then , it holds that @xmath229 \\nonumber\\\\      \\ge&\\sup_{s>0 , \\rho \\in \\mathbb{r } , \\sigma \\ge 0 }      \\frac{1+s}{s }      \\left [      -\\frac { u(\\rho ( 1 + s ) ) } { 1+s } + u(\\rho )        + \\log      \\left(1 - 2e^{\\frac{u(\\rho-\\sigma(1-\\rho ) ) - ( 1+\\sigma)u(\\rho ) + \\sigma r}{1+\\sigma } }      \\right )      \\right ] \\label{33}\\\\      \\ge & \\sup_{s>0 , \\theta ( a(r))<\\rho<1 }      \\frac{1+s}{s }      \\left [   -\\frac { u(\\rho ( 1 + s ) ) } { 1+s } + u(\\rho )       + \\log      \\left (       1 - 2e^{(\\rho - \\theta(a(r ) ) ) a(r ) + u ( \\theta(a(r ) ) ) - u ( \\rho ) }      \\right )      \\right ] , \\label{sbc1 }      \\end{aligned}\\ ] ] where @xmath230 ( \\cdot ) , \\label{sbc2}\\\\ \\theta ( \\cdot ) & : = \\theta[p_{xz } , q_z ; 1 ] ( \\cdot ) , \\label{sbc3}\\\\ a ( \\cdot)&:= a[p_{xz } , q_z ; 1 ] ( \\cdot ) .",
    "\\label{sbc4 }      \\end{aligned}\\ ] ]    in this proof , we use the notation defined in - .    for arbitrary @xmath231",
    ", we define following new distributions .",
    "@xmath232 using these , we define following joint distribution .",
    "@xmath233    for arbitrary code @xmath146 , we define @xmath234.\\end{aligned}\\ ] ] and also , when the source distribution is @xmath235 and the channel is conditional additive channel @xmath236 defined by @xmath237 we define @xmath238.\\end{aligned}\\ ] ]    then , for any @xmath239 , by the monotonicity of the rnyi divergence , we have @xmath240\\nonumber \\\\ \\ge & \\log\\beta^{1+s}\\alpha^{-s}. \\end{aligned}\\ ] ] thus , we have @xmath241 for the rnyi divergence , we have @xmath242    in addition , substituting @xmath243 and @xmath244 into ( [ e ] ) , we have @xmath245 for any @xmath246 , the first term of right hand side of can be evaluated as : @xmath247 thus , by setting @xmath248 so that @xmath249 we have @xmath250 for the rnyi divergence in , we have @xmath251 so , we have @xmath252 combining , and , we obtain ( [ 33 ] ) .",
    "now , we restrict the range of @xmath253 so that @xmath254 , and take @xmath255 we obtain the second inequality .",
    "firstly , we give general notations for channel coding when the message obeys markovian process .",
    "we assume that the set of messages is @xmath256 .",
    "then , we assume that the message @xmath257 is subject to the markov process with the transition matrix @xmath258 .",
    "we denote the distribution for @xmath259 by @xmath260 .",
    "now , we consider very general sequence of channels with the input alphabet @xmath261 and the output alphabet @xmath262 . in this case , the transition matrix as @xmath263 .",
    "then , a channel code @xmath138 consists of one encoder @xmath264 and one decoder @xmath265 .",
    "then , the average decoding error probability is defined by @xmath266 : = \\sum_{m^k \\in { \\cal m}^k } p_{m^k}(m^k ) w_{y^n| x^n } ( \\{y^n:{\\mathsf{d}}(y^n ) \\neq m^k \\}|{\\mathsf{e}}(m^k ) ) .",
    "\\end{aligned}\\ ] ] for notational convenience , we introduce the error probability under the above condition : @xmath267 .",
    "\\end{aligned}\\ ] ] when there is no possibility for confusion , we simplify it to @xmath268 .",
    "instead of evaluating the error probability @xmath269 for given @xmath270 , we are also interested in evaluating @xmath271 for given @xmath272 .      in this section",
    ", we address an @xmath0-fold markovian conditional additive channel @xcite .",
    "that is , we consider the case when the joint distribution for the additive noise obeys the markov process . to formulate our channel",
    ", we prepare notations .",
    "consider the joint markovian process on @xmath188 .",
    "that is , the random variables @xmath273 and @xmath274 are assumed to be subject to the joint markovian process defined by the transition matrix @xmath275 .",
    "we denote the joint distribution for @xmath276 and @xmath277 by @xmath278 .",
    "now , we assume that @xmath137 is a module , and consider the channel with the input alphabet @xmath261 and the output alphabet @xmath279 .",
    "the transition matrix for the channel @xmath280 is given as @xmath281 for @xmath282 and @xmath283 .",
    "also , we denote @xmath284 by @xmath285 . in the following discussion ,",
    "we use the channel capacity @xmath286 , which is shown in @xcite . in this case , we denote the average error probability @xmath287 $ ] and the minimum average error probability @xmath288 by @xmath289 $ ] and @xmath290 , respectively . then , we denote the maximum size @xmath291 by @xmath292 . when we have no possibility for confusion , we simplify them to by @xmath293 $ ] , @xmath294 , and @xmath295 , respectively .    in the following discussion , we assume assumption 1 or 2 for the joint markovian process described by the transition matrix @xmath275 . the paper @xcite derives the single - letterized channel capacity under assumption 1 . among author",
    "s knowledge , the class of channels satisfying assumption 1 is the largest class of channels whose channel capacity is known .",
    "when @xmath296 is singleton and the channel is the noiseless channel given by identity transition matrix @xmath297 , our problem is the source coding with markovian source . in this case , the memory size is equal to the cardinality @xmath298 , we denote the minimum error probability @xmath299 by @xmath300 .",
    "now , we assume assumption 1 . combining proposition [ l1 ] and ( [ b ] ) of lemma [ lembc ] , we have an upper bound of the minimum error probability as follows .",
    "[ f1d ] when assumption 1 holds , setting @xmath301 , we have @xmath302(s ) + \\delta(s ) \\right ] , \\label{f1d1}\\end{aligned}\\ ] ] where @xmath303    combining proposition [ l1 ] and ( [ 33 ] ) of lemma [ sbc ] , we have a lower bound of the minimum error probability as follows .",
    "[ f1c ] when assumption 1 holds , setting @xmath301 , we have @xmath304 , \\label{f1c1}\\end{aligned}\\ ] ] where @xmath305 ( \\cdot ) , \\\\",
    "\\theta ( \\cdot ) & : = \\theta[w_s , w_c , \\downarrow ; \\frac{k-1}{n-1 } ] ( \\cdot ) , \\\\ a ( \\cdot)&:= a[w_s , w_c , \\downarrow ; \\frac{k-1}{n-1 } ] ( \\cdot ) ,   \\end{aligned}\\ ] ] and where @xmath306    we first substitute @xmath307 @xmath308 to ( [ 33 ] ) of lemma [ sbc ] and use proposition [ l1 ] .",
    "then , we restrict the range of @xmath253 as @xmath309 and set @xmath310",
    ". then , we have the claim of the theorem .",
    "next , we assume assumption 2 .",
    "combining proposition [ l2 ] and ( [ c ] ) of lemma [ lembc ] , we have an upper bound of the minimum error probability as follows .",
    "[ f2d ] when assumption 2 holds , setting @xmath301 , we have @xmath311 } \\frac{-nsr+(n-1)u [ w_s , w_c , \\uparrow ; \\frac { k-1 } { n-1 } ] ( s)}{1-s}+\\xi(s ) , \\label{f2d1}\\end{aligned}\\ ] ] where @xmath312    combining proposition [ l3 ] and ( [ 33 ] ) , we have a lower bound of the minimum error probability as follows .    [ f2c ] when assumption 2 holds , setting @xmath301 , we have @xmath313 , \\label{g}\\end{aligned}\\ ] ] where @xmath314 and where @xmath315 ( \\cdot),\\\\ a ( \\cdot)&:= a[w_s , w_c , \\uparrow ; \\frac{k-1}{n-1 } ] ( \\cdot),\\\\ u^\\uparrow ( \\cdot ) & : = u[w_s , w_c , \\uparrow ; \\frac{k-1}{n-1 } ] ( \\cdot ) , \\\\",
    "u_{\\theta(a(r ) ) } ( \\cdot ) & : = u[w_s , w_c , \\theta(a(r ) ) ; \\frac{k-1}{n-1 } ] ( \\cdot).\\end{aligned}\\ ] ]    we first substitute @xmath307 @xmath316 to ( [ 33 ] ) of lemma [ sbc ] and use proposition [ l2 ] and [ l3 ] .",
    "then , we restrict the range of @xmath253 as @xmath309 and set @xmath310 .",
    "then , we have the claim of the theorem .      in this section , for some constant @xmath317 , we fix the coding rate @xmath318 to be @xmath319 by using the real number @xmath320 .",
    "now , we assume assumption 1 . using theorem [ f1d ]",
    ", we can upper bound the exponent of the minimum error probability as follows . by setting @xmath321 , taking logarithm and normalizing the both side of ( [ f1d1 ] )",
    ", we obtain following theorem .",
    "[ t5 ] assume that assumption 1 holds and set @xmath301 .",
    "when the rate @xmath322 satisfies @xmath323 , we have @xmath324 where @xmath325 is error exponent function defined as @xmath326(s ) ] .",
    "\\label{ee1d}\\end{aligned}\\ ] ]    this theorem is a conditional additive version of ( * ? ? ?",
    "* proposition 1 ) .    using theorem [ f1c ] , we can lower bound exponent of the minimum error probability as follows . by setting @xmath321",
    ", we obtain following theorem .",
    "[ t6 ] assume that assumption 1 holds and set @xmath301 .",
    "when the rate @xmath322 satisfies @xmath327 , we have @xmath328 where @xmath329 is error exponent function defined as @xmath330 ( \\theta(a(r ) ) ) \\nonumber \\\\ = & \\sup_{\\theta \\le 1 }       \\frac {      \\theta r - u(\\theta ) }      { 1-\\theta},\\label{ee1c}\\end{aligned}\\ ] ] where @xmath331 ( \\cdot ) , \\\\ \\theta ( \\cdot ) : = & \\theta[w_s , w_c , \\downarrow ; r ] ( \\cdot ) , \\\\ a ( \\cdot):= & a[w_s , w_c , \\downarrow ; r ] ( \\cdot ) .",
    "\\end{aligned}\\ ] ]    this theorem is a conditional additive version of ( * ? ? ?",
    "* theorem 2 ) .    from theorem [ f1c ] , we have @xmath332\\nonumber\\\\ & = \\rho   \\frac { u(\\rho ( 1 + s ) ) - u(\\rho ) } { s \\rho }   - u(\\rho ) \\nonumber\\\\ & \\to \\rho u(\\rho ) - u(\\rho ) \\quad ( { \\rm as } \\quad s \\to 0)\\nonumber\\\\ & \\to \\theta(a(r ) ) u(\\theta(a(r ) ) ) - u(\\theta(a(r ) ) )   \\quad ( { \\rm as } \\quad \\rho \\to \\theta(a(r ) ) ) \\nonumber\\\\ & = \\theta(a(r ) ) a(r ) - u(\\theta(a(r ) ) ) , \\end{aligned}\\ ] ] where @xmath333 ( \\cdot)$ ] .    this part will be done similar to ( * ? ? ?",
    "* theorem 21 ) . in this case , the direct part bound does not coincide with the converse part bound , in general . to derive the exact value of the exponent , we need a stronger assumption .      next , we assume assumption 2 , which is stronger than assumption 1 .",
    "using theorem 3 , we can upper bound the exponent of the minimum error probability as follows . by setting @xmath321 , taking logarithm and normalizing the both side of ( [ f2d1 ] )",
    ", we obtain following theorem .",
    "[ ld2d ] assume that assumption 2 holds and set @xmath301 .",
    "when the rate @xmath322 satisfies @xmath334 , we have @xmath335 where @xmath336 is an error exponent function defined as @xmath337}\\frac{sr - u[w_s , w_c , \\uparrow ; r ] ( s)}{1-s}. \\label{ee2d}\\end{aligned}\\ ] ]    using theorem [ f2c ] , we can lower bound the exponent of the minimum error probability as follows . by setting @xmath321 , we obtain following theorem .    [ ld2c ]",
    "assume that assumption 2 holds and set @xmath301 .",
    "when the rate @xmath322 satisfies @xmath338 , we have @xmath339 where @xmath340 is an error exponent function defined as @xmath341 where @xmath342 ( \\cdot ) , \\\\ \\theta ( \\cdot )",
    "& : = \\theta[w_s , w_c , \\uparrow ; r ] ( \\cdot ) , \\\\ a ( \\cdot)&:= a[w_s , w_c , \\uparrow ; r ] ( \\cdot ) .",
    "\\end{aligned}\\ ] ]    from theorem [ f2c ] , we have @xmath343\\nonumber\\\\      & =      \\rho       \\frac { u_{\\theta(a(r ) ) } ( \\rho ( 1 + s ) ) - u_{\\theta(a(r ) ) } ( \\rho ) } { s \\rho }       - u(\\rho ) \\nonumber\\\\      & \\to      \\rho u_{\\theta(a(r ) ) } ( \\rho ) - u_{\\theta(a(r ) ) } ( \\rho ) \\quad ( { \\rm as } \\quad s \\to 0)\\nonumber\\\\      & \\to      \\theta(a(r ) ) u^{\\uparrow}(\\theta(a(r ) ) ) - u(\\theta(a(r ) ) )       \\quad ( { \\rm as } \\quad \\rho \\to \\theta(a(r ) ) ) \\nonumber\\\\      & =      \\theta(a(r ) ) a(r ) - u(\\theta(a(r ) ) ) ,       \\end{aligned}\\ ] ] where @xmath344 ( \\cdot)$ ] and @xmath345 ( \\cdot)$ ] .    combining the above theorems",
    ", we obtain the exact expression of the exponent of the minimum error probability when we define the critical rate @xmath346 as    @xmath347 \\left ( u[w_s , w_c , \\uparrow ; r ]   \\left ( \\frac { 1 } { 2 }   \\right ) \\right ) .",
    "\\end{aligned}\\ ] ]    for @xmath348 , we can rewrite the upper bound in theorem [ ld2c ] as @xmath349 } \\frac{\\theta r - u(\\theta)}{1-\\theta } = \\theta(a(r))a(r ) - u(\\theta(a(r))).\\end{aligned}\\ ] ] thus , the lower bound in theorem [ ld2d ] coincides with the upper bound in theorem [ ld2c ] .",
    "so we have @xmath350 } \\frac{\\theta r - u(\\theta)}{1-\\theta } \\nonumber\\\\ & = \\theta(a(r))a(r ) - u(\\theta(a(r))).\\end{aligned}\\ ] ]    now , we consider the case when @xmath296 is singleton and the transition matrix @xmath113 of the additive noise is the identity matrix @xmath297 , which is the same as the data compression with markovian source . since @xmath351 , we have @xmath352   \\\\ \\lim_{n \\to \\infty } -\\frac{1}{n } \\log p_{s}(nr , n|w_s )   \\ge & \\sup_{\\theta \\le 1 }       \\frac{\\theta",
    "r - r \\theta h^{w_{s}}_{1-\\theta}(m)}{1-\\theta } , \\end{aligned}\\ ] ] which is the same as the result of ( * ? ? ?",
    "* theorem 12 ) .",
    "next , we proceed to the rate is in the moderate deviation regime , in which , the coding rate @xmath353 behaves as @xmath354 with @xmath355 .",
    "then , the minimum error probability can be evaluated as follows .",
    "[ md ] assume that assumption 1 holds .",
    "then , for arbitrary @xmath356 and @xmath357 , it holds that @xmath358}. \\label{md , th}\\end{aligned}\\ ] ]    theorem [ md ] is conditional additive channel version of ( * ? ? ?",
    "* theorem 1 ) .    from theorem [ f1d ] , we obtain @xmath359\\nonumber\\\\ & \\ge \\sup_{s \\in ( 0 , 1)}[nsr-(k-1)sh_{1-s}^{w_{s}}(m)-(n-1)sh_{1-s}^{w_c , \\downarrow}(x|z ) ] + \\inf_{s \\in ( 0 , 1)}[-\\delta(s)]\\nonumber\\\\ & \\ge n[s'r - r_ns'h_{1-s'}^{w_{s}}(m)-s'h_{1-s'}^{w_c , \\downarrow}(x|z ) ] + o(n^{1 - 2 t } ) . \\label{i}\\end{aligned}\\ ] ] by , taylor expansions of @xmath360 and @xmath361 in the neighborhood of @xmath362 are @xmath363 substituting these expansions into ( [ i ] ) , we obtain @xmath364                  + o(n^{1 - 2 t } ) .",
    "\\end{aligned}\\ ] ] now , we set @xmath365 which satisfies @xmath366 $ ] for enough large @xmath0 .",
    "then , we have @xmath367            + o(n^{1 - 2t})\\nonumber\\\\ = & -n^{1 - 2t}\\frac{1}{2}\\cdot\\frac{\\delta ^{2}}{\\frac{1}{(h^{w_{s}}(m))^2 }      \\left [      \\frac{c}{h^{w_{s}}(m)}v^{w_{s}}(m)+v^{w_{c}}(x|z )      \\right ] } + o(n^{1 - 2 t } ) , \\end{aligned}\\ ] ] that is , @xmath368}. \\end{aligned}\\ ] ] on the other hands , by choosing @xmath369 , theorem [ f1c ] implies that @xmath370\\nonumber\\\\ = & \\lim_{n \\rightarrow \\infty }       n^{2 t } \\frac{1+s}{s}\\rho          \\frac{1}{2}(r_nv^{w_{s}}(m)+v^{w_{c}}(x|z ) ) s\\rho \\nonumber\\\\ = & \\lim_{n \\rightarrow \\infty }      n^{2 t } ( 1+s)\\rho^2\\frac{1}{2 }              ( \\frac{c}{h^{w_{s}}(m)}v^{w_{s}}(m)+v^{w_{c}}(x|z)-\\delta n^{-t}v^{w_{s}}(m))\\nonumber\\\\ = & ( 1+s)\\frac{1}{2}\\cdot\\frac{\\delta ^{2}}{\\frac{1}{(h^{w_{s}}(m))^2 }      \\left [      \\frac{c}{h^{w_{s}}(m)}v^{w_{s}}(m)+v^{w_{c}}(x|z )      \\right ] } \\nonumber\\\\ \\rightarrow &   \\frac{1}{2}\\cdot\\frac{\\delta ^{2}}{\\frac{1}{(h^{w_{s}}(m))^2}\\left [      \\frac{c}{h^{w_{s}}(m)}v^{w_{s}}(m)+v^{w_{c}}(x|z )      \\right ] } \\qquad\\qquad\\qquad ( s \\rightarrow 0 ) .",
    "\\end{aligned}\\ ] ]    now , we consider the case when @xmath116 is singleton and the transition matrix @xmath113 of the additive noise is the identity matrix @xmath297 .",
    "when @xmath371 , the minimum error probability @xmath372 is characterized as follows . setting @xmath373 i.e. , @xmath371 , the minimum error probability @xmath372 and using @xmath374 , we obtain    @xmath375^{1 - 2t}}\\log { { \\mathrm{p}}_{{\\mathrm{j}}}}(k , n)\\nonumber\\\\ = & \\lim_{k \\rightarrow \\infty }       -\\left(\\frac{1}{\\frac{c}{h^{w_s}(m)}n}\\right)^{1 - 2 t } \\left(\\frac{1}{1-h^{w_s}(m)(\\frac{c}{h^{w_{s}}(m)})^{-t}{\\delta}'n^{-t}}\\right)^{1 - 2t}\\log { { \\mathrm{p}}_{{\\mathrm{j}}}}(k , n)\\nonumber\\\\ = & \\left(\\frac{h^{w_s}(m)}{c}\\right)^{1 - 2 t } \\lim_{k \\rightarrow \\infty }       -\\frac{1}{n^{1 - 2t}}\\log { { \\mathrm{p}}_{{\\mathrm{j}}}}(k , n)\\left(\\frac{1}{1-h^{w_s}(m)(\\frac{c}{h^{w_{s}}(m)})^{-t}{\\delta}'n^{-t}}\\right)^{1 - 2t}\\nonumber\\\\ = & \\left(\\frac{h^{w_s}(m)}{c}\\right)^{1 - 2 t }      \\frac{1}{2}\\cdot\\frac{\\delta",
    "^{2}}{\\frac{1}{(h^{w_{s}}(m))^2}\\left[\\frac{c}{h^{w_{s}}(m)}v^{w_{s}}(m)\\right]}\\nonumber\\\\ = & \\left(\\frac{h^{w_s}(m)}{c}\\right)^{1 - 2 t }      \\frac{1}{2 } \\cdot \\frac{\\frac{c^2}{h^{w_{s}}(m)^4}(\\frac{c}{h^{w_{s}}(m)})^{-2t}{\\delta'}^2}{\\frac{1}{(h^{w_{s}}(m))^2}\\cdot \\frac{c}{h^{w_{s}}(m)}v^{w_{s}}(m)}\\nonumber\\\\ = & \\frac{{\\delta'}^2}{2v^{w_{s}}(m)}. \\end{aligned}\\ ] ]    this result coincides with .",
    "finally , to demonstrate the advantage of our finite - length bounds , we numerically evaluate the achievability bound in theorem [ f2d ] and the converse bound in theorem [ f2c ] . due to the efficient construction of our bounds",
    ", we could calculate both bounds with huge size @xmath376 because the calculation complexity behaves as @xmath1 .",
    "we employ the following parametrization @xmath377 for the binary transition matrix : @xmath378.\\end{aligned}\\ ] ] we consider the case when @xmath379 .",
    "the optimal transmission rate @xmath380 and the dispersion @xmath381 } $ ] are calculated to be 0.807317 and 6.12809 , respectively .",
    "also , the exponent @xmath382 is calculated to be 0.0002826 , which is approximated by @xmath383 .    when @xmath384 , fig .",
    "[ f1 ] calculates the upper and lower bounds of @xmath385 based on theorems [ f2d ] and [ f2c ] .",
    "also , it shows the comparison them with the approximations @xmath386 and @xmath387 by theorems [ ld2d ] and [ md ] .",
    "[ f2 ] addresses the quantity @xmath388 in the same way .",
    "mh is very grateful to professor vincent y. f. tan and professor shun watanabe for helpful discussions and comments .",
    "the works reported here were supported in part by a mext grant - in - aid for scientific research ( b ) no .",
    "16kt0017 , the okawa research grant and kayamori foundation of informational science advancement .          v. y. f. tan , s. watanabe , and m. hayashi , `` moderate deviations for joint source - channel coding of systems with markovian memory '' , in _ proc .",
    "2014 ieee isit _ ,",
    "honolulu , hi , usa , june 29 - july 4 , 2014 .",
    "pp.1687    v. kostina and s. verd , `` lossy joint source - channel coding in the finite blocklength regime , '' _ proceedings of 2012 ieee international symposium on information theory _ , 1 - 6 july 2012 , cambridge , ma , usa , pp .",
    "1553 - 1557 .",
    "a. t. campo , g. vazquez - vilar , a. g. i fbregas , t. koch and a. martinez , `` achieving csiszr s source - channel coding exponent with product distributions , '' _ proceedings of 2012 ieee international symposium on information theory _ , 1 - 6 july 2012 , cambridge , ma , usa , pp .",
    "1548 - 1552 .",
    "y. zhong , f. alajaji and l. lorne campbell ,  joint source - channel coding error exponent for discrete communication systems with markovian memory , \" _ ieee trans .",
    "inf . theory _ ,",
    "12 , 4457 - 4472 ( 2007 ) .",
    "m. tomamichel , and m. hayashi ,  operational interpretation of renyi information measures via composite hypothesis testing against product and markov distributions , \" _ proceedings of 2016 ieee international symposium on information theory _ , 10 - 15 july 2016 , barcelona , spain , pp . 585 - 589 ; arxiv:1511.04874 ."
  ],
  "abstract_text": [
    "<S> we derive novel upper and lower finite - length bounds of the error probability in joint source - channel coding when the source obeys the ergodic markov process and the channel is a markovian additive channel or a markovian conditional additive channel . </S>",
    "<S> these bounds achieve the tight bounds in the large and moderate deviation regimes .    markov chain , joint source - channel coding , finite - length analysis , large deviation , moderate deviation </S>"
  ]
}