{
  "article_text": [
    "recent approaches for multi - class classification ( e.g. @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ) exploit the representation of a test sample over a redundant basis , formed by the training samples ( or their extracted features ) . this _ collaborative representation _ of the test sample , in which the training samples from different classes _ collaborate _ to approximate the test sample , is later used to decide its class label .",
    "wright et al .",
    "@xcite first demonstrated the impressive potential of this scheme for face recognition .",
    "their approach additionally forces the representation to be sparse ( i.e. it uses only a few vectors from the basis ) .",
    "hence , it is called sparse representation based classification ( src ) .",
    "the success of src was followed up by its variants .",
    "for instance , huang et al .",
    "@xcite proposed a transformation - invariant src .",
    "zhou et al .",
    "@xcite combined markov random fields with src for disguised faces . similarly , wagner et al .",
    "@xcite enhanced src for the misalignment , pose and illumination invariant recognition .",
    "effectiveness of these approaches also boosted significant research in dictionary learning  @xcite based multi - class classification   @xcite , @xcite , @xcite , @xcite , @xcite .",
    "initially , the success of these approaches was attributed to the sparseness of the used representation . however , more recently , researchers have started questioning the role of sparsity in such approaches  @xcite , @xcite ,  @xcite . among them , zhang et al .",
    "@xcite analyzed the working mechanism of src and claimed that it is the _ collaboration and not the sparseness _ of the representation that is the reason behind the effectiveness of src ( and hence the related approaches ) .",
    "this result is rather widely acclaimed as it provides grounds to relinquish the computationally expensive sparsity constraint over the representation without sacrificing the classification accuracy .    in this paper , we first extend the analysis of zhang et al .",
    "@xcite and , in contrast to the original claim , we find that _ sparseness of collaborative representation explicitly contributes to accurate classification _ , hence it should not be completely ignored for computational gains . motivated by this intuition , we propose a sparsity augmented collaborative representation based classification scheme ( sa - crc ) that uses both dense and sparse collaborative representations to decide the class label of a test sample .",
    "sa - crc computes the dense representation using the regularized least squares method and greedily approximates the sparse representation using the orthogonal matching pursuit ( omp )  @xcite .",
    "omp s solution is used to augment the dense representation .",
    "finally , the augmented representation is classified by capitalizing on its enriched discriminative properties . to that end",
    ", we propose an efficient classification method that avoids explicit computation of the reconstruction residuals for each class .",
    "we evaluate the proposed approach on two face databases  @xcite , @xcite , one object category database  @xcite and a dataset for action recognition  @xcite .",
    "extensive experiments show that our approach is not only more accurate than the state - of - the - art collaborative representation based classification approaches , its classification time is also much lower than the approaches that ignore the sparsity altogether .",
    "let @xmath0 denote the training data from @xmath1 distinct classes , such that @xmath2 $ ] .",
    "each sub - matrix @xmath3 pertains to a single class and @xmath4 .",
    "the columns of @xmath5 represent training samples , that are features extracted from images .",
    "our goal is to develop an efficient multi - class classification scheme by collaboratively representing a test sample @xmath6 over the training data is conventionally referred as the _ training _ data  @xcite , @xcite . ] .",
    "a test sample is considered to be a feature vector that can be linearly approximated by the training samples .",
    "that is , @xmath7 , where @xmath8 is the collaborative representation  ( cr ) vector of the test sample .",
    "we allow @xmath5 to be a redundant set of basis vectors in @xmath9 .",
    "furthermore , the subspaces spanned by the sub - matrices @xmath10 are considered to be possibly overlapping , as this is often the case for the multi - class classification problems . following the sparse representation literature  @xcite , @xcite",
    ", we alternatively refer to @xmath5 as the _ dictionary _ and to its columns as the _ dictionary atoms_. furthermore , we generally refer to the _ representation vector _",
    "( e.g. @xmath11 ) as _ representation _ , for brevity .",
    "\\(a ) training data @xmath5 , with samples normalized to have unit @xmath12-norm .",
    "( b ) test sample @xmath13 .",
    "( c ) regularization parameter @xmath14 . _",
    "optimization : _ solve @xmath15 where , @xmath16 denotes a function and @xmath17 represents the @xmath18-norm of a vector . _ residual computation : _ compute class - specific reconstruction residuals @xmath19 , where @xmath20 comprises the coefficients of @xmath11 corresponding to the @xmath21 class . _",
    "labeling : _ label@xmath22 .",
    "label@xmath23 .",
    "[ alg:1 ]    algorithm  [ alg:1 ] presents the base - line scheme used by popular approaches ( e.g. @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ) that exploit collaborative representation in multi - class classification .",
    "the algorithm performs three key steps of ( 1 )  optimizing @xmath13 s representation over a given dictionary , ( 2 )  computing class - specific reconstruction residuals @xmath24 , @xmath25 and ( 3 )  labeling @xmath13 using the computed residuals . in step",
    "( 2 ) , @xmath26 comprises the coefficients of @xmath11 corresponding to the @xmath21 class only .",
    "hence , in step  ( 3 ) , @xmath13 is assigned the label of the class that results in the smallest reconstruction residual .",
    "we can treat different approaches as special cases of the presented algorithm .    in src  @xcite , @xmath27 in eq .",
    "( [ eq : opt1 ] ) , which encourages the computed representation @xmath11 to be sparse . in superposed - src ( ssrc ) , deng et al .",
    "@xcite modified the residual computation step of src .",
    "for ssrc , @xmath5 consists of class centroids and sample - to - centroid differences . while computing the residuals",
    ", ssrc keeps the coefficients of @xmath11 corresponding to the sample - to - centroid differences fixed in each @xmath28 . the cr - based classifier proposed by zhang et al .",
    "@xcite uses @xmath29 and solves eq .",
    "( [ eq : opt1 ] ) using the regularized least squares ( rls ) method , hence denoted as crc - rls .",
    "shi et al .",
    "@xcite used @xmath30 in eq .",
    "( [ eq : opt1 ] ) and solved it as the standard least squares problem for face recognition .",
    "chi and porikli  @xcite used a linear combination of a cr - based classifier and a nearest subspace classifier  @xcite for improved classification performance .",
    "collaborative representation is also commonly used by discriminative dictionary learning techniques , e.g.  @xcite , @xcite .",
    "although such approaches _ learn _ a dictionary instead of directly using the training data as @xmath5 , explicit correspondence between the learned dictionary atoms and the class labels allows them to exploit the cr - based classification scheme .",
    "for instance , the global classifier ( gc ) used by kong and wang  @xcite is the same variant of algorithm  [ alg:1 ] that is used by ssrc  @xcite .",
    "the dictionary learned by the dl - copar algorithm @xcite consists of common atoms for all classes and particular atoms specific to each class . the particular atoms behave like class centroids whereas the common atoms act as centroid - to - sample differences in ssrc .",
    "similarly , the gc used in the fisher discriminant dictionary learning ( fddl )  @xcite is a direct variant of crc - rls  @xcite .",
    "another interesting direction of discriminative dictionary learning techniques , e.g. label consistent k - svd ( lc - ksvd )  @xcite and discriminative k - svd ( d - ksvd )  @xcite , is also related to cr - based classification .",
    "such techniques learn collaborative dictionaries from the training data without enforcing strict correspondence between the class labels and the dictionary atoms . due to the lack of such correspondence ,",
    "the label of a test sample is chosen by maximizing a weighted sum of the coefficients of @xmath11 , where the @xmath31-dimensional @xmath1 weight - vectors are also learned during dictionary optimization . among these weight - vectors ,",
    "the @xmath21 vector generally assigns large weights to the coefficients of @xmath11 corresponding to the dictionary atoms used commonly in representing the training data of the @xmath21 class .",
    "the above mentioned discriminative dictionary learning approaches classify a test sample using its representation over a collaborative set of features , learned directly from the training data .",
    "therefore , they are considered to be instances of cr - based classification .",
    "it is clear from section  [ sec : rw ] that many popular approaches directly exploit collaborative representation @xmath11 in classification . whereas sparse representation based approaches ( e.g. @xcite , @xcite ) associate the discriminative power of @xmath11 to its sparseness",
    ", there is an equal evidence in favor of discriminative abilities of dense representations @xcite , @xcite , @xcite .",
    "in fact , it is also advocated that sparsity of the representation may not even be relevant to classification @xcite , @xcite , @xcite .",
    "zhang et al .",
    "@xcite boosted the popularity of this notion by corroborating their claim with an analysis of the working mechanism of src . in section",
    "[ sec : cr ] , we closely follow this analysis to explain the role of collaboration in cr - based classification .",
    "we extend this analysis on the same lines of reasoning in section  [ sec : crfails ] to show that collaboration alone is not sufficient for accurate classification .",
    "section  [ sec : sr ] discusses how sparseness additionally helps in this regard .",
    "we write the subspace spanned by the columns of @xmath5 as a set @xmath32 .",
    "this subspace is geometrically illustrated as a plane in fig .",
    "[ fig : ill1 ] . since a test sample @xmath13 is approximated by the columns of @xmath5 , we can write the approximation error as @xmath33 , where @xmath34 .",
    ", @xmath35 when @xmath36 and @xmath37 , where @xmath38 .",
    "in that case , we are concerned with @xmath39 only , as @xmath13 is considered to be approximated with a small error of bounded energy , i.e. @xmath40 .",
    "we exaggerate the error vector in figures for clarity .",
    "] let us represent the subspace spanned by the training data of the @xmath21 class by a set @xmath41 , where @xmath42 . without loss of generality , we can decompose @xmath43 into two components , @xmath44 and @xmath45 ( illustrated in fig .  [",
    "fig : ill1a ] ) such that @xmath46 and @xmath47 , where @xmath48 .",
    "similarly , the total approximation error @xmath49 can itself be considered as a component of @xmath50 , where @xmath51 represents the class - specific reconstruction residual @xmath24 , see step  @xmath52 of algorithm  [ alg:1 ] .",
    "0.35     +    0.35    to understand the working mechanism of cr - based classification , let @xmath13 belong to the @xmath53 class . in this case ,",
    "@xmath54 , i.e. @xmath55 in fig .",
    "[ fig : ill1a ] .",
    "a cr - based classifier selects @xmath56 as the label of @xmath13 because @xmath57 is expected to have the smallest length when @xmath58  @xcite , @xcite .",
    "zhang et al .",
    "@xcite noted that this labeling criterion not only considers that the angle between @xmath43 and @xmath59 ( i.e. @xmath60 ) is small , it also considers that the angle between @xmath59 and @xmath61 ( i.e. @xmath62 ) is large .",
    "according to zhang et al .",
    "@xcite , it is this _",
    "double - check with @xmath60 and @xmath62 _ ( not the sparseness of the representation ) that makes cr - based classification robust and effective .",
    "therefore , they solved eq .",
    "( [ eq : opt1 ] ) using a computationally efficient regularized least squares method .",
    "the resulting dense collaborative representation was shown to be effective for face recognition , similar to sparse representation .      in the following text",
    ", we refer to a vector @xmath50 as _ class - specific error vector_. we present lemma  [ lem:1 ] regarding the underlying geometry of the class - specific error vectors involved in cr - based classification :    [ lem:1 ]    for @xmath63 , where @xmath64 , the following holds : @xmath65 such that @xmath66 , while @xmath67 such that @xmath68 .",
    "* proof : * for our problem , the following holds under the law of sines , which can be verified from fig .",
    "[ fig : ill1a ] : @xmath69 also , @xmath70 because @xmath71 . from eq .",
    "( [ eq:2 ] ) , @xmath72 since @xmath73 and @xmath74 become constants once @xmath13 is projected onto @xmath32 , the condition that @xmath67 s.t .",
    "@xmath75 holds when @xmath76 is the minimum .",
    "however , for @xmath77 $ ] there is no unique minima for the given squared ratio .",
    "hence , it is possible that @xmath65 s.t .",
    "@xmath66 , while @xmath67 s.t . @xmath68 .",
    "lemma  [ lem:1 ] , shows the possibility of existence of multiple class - specific error vectors with equal lengths when the length is minimized over the class labels .",
    "figure  [ fig : ill1b ] illustrates this possibility by drawing a circle of radius @xmath78 around point @xmath79 on @xmath32 .",
    "any vector starting from a point on this circle ( e.g. @xmath80 ) and ending at @xmath81 will have the same length .",
    "for the labeling criterion of cr - based classification scheme , collaboration of the representation alone is not sufficient to indicate the best vector among these possible vectors . from lemma  [ lem:1 ]",
    ", it is also evident that the double - check with @xmath60 and @xmath62 mentioned by zhang et al .",
    "@xcite is essentially a _ single - check _ on the squared ratio of the sines of the angles .",
    "thus , _ cr - based classification without considering sparsity may not be as robust and effective as previously thought_.      the above mentioned issue is inherent to cr - based classification scheme , with its roots in the redundancy in @xmath82 . simply computing a unique approximation of the representation , such as in crc - rls  @xcite ,",
    "does not resolve the issue because lemma  [ lem:1 ] still holds for the labeling step in algorithm  [ alg:1 ] .",
    "to truly address the problem , a collaborative representation must be infused with additional information that finally results in using a suitable class - specific error vector in the labeling step .",
    "sparsity constraint over the representation serves this purpose in cr - based classification .",
    "0.35     +    0.35    to support our argument , in fig .",
    "[ fig : ill2 ] , we geometrically illustrate the two jointly exhaustive situations that can occur when two class - specific error vectors @xmath50 and @xmath83 have equal lengths , namely ( a )  @xmath84 and ( b )  @xmath85 . in the figure",
    ", we denote @xmath44 by @xmath86 and @xmath87 by @xmath88 and show these vectors only by their components to avoid cluttering . in fig .",
    "[ fig : ill2a ] , @xmath89 but @xmath66 . in fig",
    "[ fig : ill2b ] , @xmath90 and @xmath66 .",
    "although the class - specific residuals are equal in both cases , @xmath44 and @xmath87 can be distinguished based on their components .",
    "intuitively , @xmath91 ( not @xmath92 ) represents the correct class of the test sample because @xmath44 requires lesser number of components to produce the smallest class - specific residual .",
    "fewer components of @xmath44 implicates a sparser @xmath11 .",
    "hence , the sparsity constraint results in using a better class - specific error vector in the labeling step .",
    "incidentally , the best performance of cr - based classification can be achieved by guaranteeing the representation to be the sparsest possible .",
    "computing the sparsest possible representation is generally np - hard  @xcite .",
    "src  @xcite uses the @xmath93-norm constraint to compute an approximate sparse representation , but the approach remains computationally expensive . on the other hand ,",
    "computing a dense representation , such as in crc - rls  @xcite , resolves the computational issues but it does not offer the advantages of sparsity . in the proposed classification scheme , we augment a dense representation with a greedily obtained approximate sparse representation .",
    "this augmentation enables accurate classification while keeping the approach computationally efficient .",
    "algorithm  [ alg:2 ] presents the proposed scheme . in the first step , the algorithm optimizes two collaborative representations , i.e. @xmath94 and @xmath95 . the dense representation @xmath94 is computed using the regularized least squares method , whereas the sparse representation @xmath95 is obtained by solving eq .",
    "( [ eq : opt2 ] ) using the orthogonal matching pursuit ( omp ) algorithm  @xcite .",
    "omp iteratively selects @xmath96 dictionary atoms to represent @xmath13 , hence , @xmath95 has at most @xmath96 non - zero coefficients , where @xmath96 ( the sparsity threshold ) is determined by cross - validation . in each iteration",
    ", omp chooses a dictionary atom by maximizing its correlation with an error vector .",
    "the error vector is computed as the difference between @xmath13 and its orthogonal projection onto the subspace spanned by the already chosen atoms . for initialization , @xmath13 itself",
    "is considered as the error . as shown in step  @xmath52 of algorithm  [ alg:2 ]",
    ", we add the sparse representation @xmath95 to @xmath94 and normalize the resulting vector to compute the augmented representation  @xmath97 . despite being simple",
    ", this procedure greatly improves the discriminative abilities of the representation .",
    "we defer the discussion on the discriminative properties of @xmath97 to the upcoming paragraphs .",
    "these properties are exploited in step  ( 3 ) of the algorithm to efficiently compute the label of the test sample  @xmath13 .",
    "the labeling step uses a binary matrix @xmath98 , that is provided as an input to the algorithm . for the @xmath21 class",
    ", @xmath99 contains @xmath100 non - zero elements in its @xmath21 row , at the indices corresponding to the columns of @xmath101 .",
    "thus , the @xmath21 coefficient of @xmath102 represents the sum of @xmath103 s coefficients corresponding to @xmath101 .",
    "the label of the test sample is decided by maximizing the coefficients of @xmath104 .",
    "empirical evidence for efficient and accurate classification using the proposed scheme is provided in sections  [ sec : exp ] .",
    "below , we analyze the reasons behind the improved performance of the approach .",
    "\\(a ) training data @xmath5 , with samples normalized in @xmath12-norm .",
    "( b ) test sample @xmath13 .",
    "( c ) regularization parameter  @xmath14 .",
    "( d )  sparsity threshold @xmath96 .",
    "( e ) label matrix @xmath99 .",
    "+ _ optimization : _ + a )  compute @xmath105 where , @xmath106 .",
    "b )  solve the following with greedy pursuit:@xmath107 where , @xmath108 denotes the @xmath109-pseudo norm .",
    "_ augmentation : _ compute @xmath110 _ labeling : _",
    "label@xmath111 , where @xmath112 denotes the @xmath21 coefficient of @xmath113 . label@xmath23 .",
    "[ alg:2 ]        for analysis , let us distribute the coefficient indices of a collaborative representation @xmath11 into two disjoint sets : @xmath114 and @xmath115 , where @xmath116 and @xmath117 with @xmath118 denoting the @xmath119 coefficient of @xmath11 .",
    "the value @xmath120 represents the energy in the @xmath119 coefficient , such that @xmath121 . if @xmath122 , @xmath123 contains the indices of non - zero coefficients of @xmath11 , whereas @xmath124 comprises the indices of zero coefficients .",
    "thus , the cardinality of the set @xmath123 , i.e. @xmath125 , defines the sparsity level of  @xmath11 .",
    "this remains true for @xmath126 .",
    "let @xmath127 denote the sparsest possible representation of @xmath13 over @xmath5 .",
    "we write the aforementioned sets for @xmath127 as @xmath128 and @xmath129 . furthermore , for any @xmath11 , let us now fix @xmath130 , where @xmath131 denotes the lowest energy coefficient of @xmath127 .",
    "hence , @xmath125 now counts the number of coefficients of @xmath11 , each having at least the energy possessed by @xmath131 .",
    "therefore , henceforth , we refer to @xmath125 as the _ effective sparsity _ of the representation .    from section  [ sec : cs ] , we know that @xmath127 is discriminative due to its sparsity . in practice ,",
    "a representation @xmath103 is equally effective for classification if @xmath132 and the coefficients indexed in @xmath133 are discriminative in this argument because the coefficients indexed in @xmath134 can be explicitly forced to zero , once @xmath133 is known . ] . for a dense representation @xmath94 , @xmath135",
    "nevertheless , the representation is globally optimal . on the other hand , @xmath136 for the sparse representation @xmath95 , but the representation is only locally optimal .",
    "however , @xmath95 generally contains large positive coefficients at the indices corresponding to the correct class . for the other classes , most of the coefficients are either negative or have small positive values",
    "this happens because omp greedily assigns large values to the coefficients of @xmath95 corresponding to the dictionary atoms that correlate more to @xmath13 , whereas @xmath13 generally has a strong positive correlation with the samples of its own class .",
    "thus , adding @xmath95 to @xmath94 amplifies the coefficients of the correct class in the globally optimal solution .",
    "figure  [ fig : sc ] illustrate this phenomenon using an actual example of face recognition . in the figure ,",
    "the coefficients of @xmath95 are consistently positive and have relatively large values for the correct class .",
    "this finally results in dominant positive coefficients of @xmath103 for the correct class . for this example",
    ", crc - rls  @xcite is not able to identify the correct label of @xmath13 despite optimized parameter settings , whereas the proposed approach classifies @xmath13 correctly .",
    "notice that , the augmentation in eq .",
    "[ eq : alpha ] also results in @xmath137 , because the procedure reduces the relative energy in the un - amplified coefficients of @xmath103 .",
    "to illustrate the difference between the effective sparsity levels of the dense and the augmented representations , we plot the effective sparsity of the representations as a function of @xmath138 in fig .  [",
    "fig : sparsity ] .",
    "the plot is for actual face recognition task using extended yaleb database  @xcite .",
    "the curve for the augmented representation remains significantly lower than the curve for the dense representation .",
    "moreover , for @xmath139 , @xmath103 is effectively almost as sparse as @xmath95 .    considering the definition of effective sparsity , ideally",
    ", the coefficients of @xmath103 indexed in @xmath134 must be forced to zero before using the representation for classification . however , since @xmath138 is unknown , identifying the exact @xmath134 remains np - hard . to resolve this issue",
    ", we design the labeling criterion that largely remains insensitive to the coefficients indexed in @xmath134 .",
    "that is , instead of deciding the class label of a test sample based on the fidelity of its reconstruction , we directly integrate the coefficients of @xmath103 for each class separately .",
    "the largest integrated value indicates the correct class label . due to the dominance of large values of the coefficients of the correct class in @xmath103",
    ", @xmath134 is not able to strongly influence the classification results .",
    "more precisely , our classification result remains as reliable as that obtained using an accurate representation with sparsity level @xmath140 , under the mild worst - case condition @xmath141 .",
    "here , @xmath142 and @xmath143 denote the largest and the second largest integrated values of the coefficients , respectively , and @xmath144 and @xmath145 are the number of coefficients in @xmath103 contributing to @xmath146 and @xmath147 respectively , such that , each coefficient has energy less than @xmath138 . to exemplify , in fig .",
    "[ fig : sparsity ] , the classification results are as accurate as possible with sparsity level @xmath148 , unless @xmath149 . typically , @xmath150 $ ] , whereas @xmath151 .",
    "since our labeling criterion does not need to compute reconstruction residuals for each class , we directly use the matrix @xmath99 in step ( 3 ) of algorithm  [ alg:2 ] .",
    "the matrix multiplication @xmath152 simultaneously integrates the coefficients for each class .",
    "computationally , this makes our labeling step extremely effective .",
    "we evaluated the proposed approach using two face databases : ar database  @xcite and extended yaleb  @xcite , an object category database : caltech-101  @xcite and an action dataset : ucf sports actions  @xcite .",
    "these datasets are commonly used to benchmark the approaches that use collaborative representation for classification .",
    "we compare the performance of our approach to src  @xcite , crc - rls  @xcite , lc - ksvd  @xcite , d - ksvd  @xcite , fddl  @xcite and dl - copar  @xcite .",
    "unless mentioned otherwise , we performed our own experiments using the same training and testing partitions for all the approaches including the proposed approach .",
    "we carefully optimized the parameter values of the approaches using cross validation . for the existing techniques , these values are generally the same as those reported in the original works",
    "however , for some cases , we used different values to favors these approaches .",
    "we explicitly mention these differences . for the dictionary learning approaches",
    ", the dictionaries are learned using the same training data that is directly used by src , crc - rls and the proposed approach .",
    "we used the _",
    "author - provided codes _ for crc - rls , lc - ksvd , fddl and dl - copar . for src",
    ", we used the spams toolbox  @xcite to solve the @xmath93-norm minimization problem . for d - ksvd",
    ", we modified the public code of lc - ksvd  @xcite .",
    "in all the experiments , the proposed approach uses the implementation of omp made public by elad et al .",
    "the same implementation is used by lc - ksvd and d - ksvd .",
    "the proposed approach uses the sparsity threshold @xmath153 for all the datasets .",
    "the regularization parameter @xmath14 is set to @xmath154 for the face databases , @xmath155 for the object database and @xmath156 for the action database .",
    "experiments are performed on an intel core i7 - 2600 cpu at 3.4 ghz with 8  gb ram .",
    "the ar database  @xcite consists of over @xmath157 face images of @xmath158 subjects . for each subject ,",
    "@xmath159 images are taken during two different sessions with large variations in terms of facial disguise , illumination and expressions . for our experiments ,",
    "a @xmath160 image was projected onto a @xmath161-dimensional vector using a random projection matrix .",
    "thus , the used samples are the random - face features  @xcite .",
    "we followed a common experimental protocol by selecting a subset of @xmath162 images of @xmath163 male and @xmath163 female subjects from the database . for each subject , @xmath164 random images were chosen to create the training data and the remaining images were used for the test data .    in table",
    "[ tab : ar ] , we summarize the results on the ar  database .",
    "the reported accuracies are the means ( and standard deviations ) of ten experiments .",
    "we also report the average time taken by each approach to classify a single test sample . for the parameter values of dl - copar",
    ", we followed the face recognition parameter settings in @xcite , which uses @xmath165 atoms per class to represent class - specific data and @xmath166 atoms to represent the commonalities .",
    "the local classifier  @xcite resulted in the best accuracy for dl - copar .",
    "for lc - ksvd  @xcite and d - ksvd  @xcite we set the sparsity threshold to @xmath163 and the dictionary size to @xmath167 atoms for improved results .",
    "these values are different from the original works because these were found to give the best accuracies .",
    "for fddl , we used the same parameter settings as  @xcite and the global classifier resulted in the best performance .",
    "for src  @xcite , we set the error tolerance @xmath168 , as in the original work .",
    "for crc - rls  @xcite , the regularization parameter @xmath14 is set to @xmath154 .",
    "this value is computed using the formula provided for @xmath14 for the face databases in @xcite .",
    "our cross - validation verified that this value results in the best performance of crc - rls .",
    "table  [ tab : ar ] shows that the best results are achieved by the proposed approach , i.e.  sa - crc .",
    "we have also shown the results of our approach when we use only the regularized least squares ( rls ) or omp in algorithm  [ alg:2 ] .",
    "it is clear that using the augmented vector is better than using any of the two representation vectors alone .",
    "notice that , due to the efficient classification criterion , our approach is much faster than crc - rls even when both omp and rls are used .",
    "the dictionaries used by lc - ksvd and d - ksvd are smaller in size as compared to the one used by sa - crc , which results in slight computational advantage for these approaches .",
    "nevertheless , accuracies of these approaches are much lower than sa - crc .",
    ".recognition accuracies on the ar database @xcite using random - face features . the reported average time ( in milli - seconds )",
    "is for classifying a single test sample .",
    "[ cols=\"<,^,^\",options=\"header \" , ]     [ tab : ucf ]",
    "our approach requires a regularization parameter @xmath14 and sparsity threshold @xmath96 as the input parameters for a given pair of @xmath5 and its label matrix  @xmath99 . in our experiments , we optimized the values of these parameters by cross - vlaidation using the following systematic procedure",
    ". first , @xmath14 was optimized by executing algorithm  [ alg:2 ] without step  1(b ) and considering @xmath95 to be a zero vector in eq .",
    "[ eq : alpha ]",
    ". then @xmath96 was optimized by fixing @xmath14 to the optimized value and executing the complete algorithm .",
    "the parameters were further fine - tuned to nearby values when doing so yielded better performance .    to show the behavior of sa - crc for different parameter values , in fig .",
    "[ fig : para ] , we plot the classification accuracy of sa - crc as a function of @xmath14 and @xmath96 by fixing one parameter and varying the other . we also include results of crc - rls  @xcite for comparison .",
    "plots in fig .",
    "[ fig : para]a , are for ar database  @xcite where we followed the experimental protocol of @xcite .",
    "in the first plot ( from left ) , we fixed @xmath96 to @xmath163 and varied @xmath14 . clearly , sa - crc consistently outperforms crc - rls and the results are less sensitive to the values of @xmath14 once @xmath96 is fixed to an optimized value . in the second plot , we used @xmath169 for both sa - crc and crc - rls and varied @xmath96 for sa - crc . again , for @xmath170 , sa - crc consistently outperforms crc - rls . qualitatively speaking , fig .",
    "[ fig : para]a shows a typical relationship between the performance of crc - rls and sa - crc that was observed in our experiments on face databases . in fig .",
    "[ fig : para]b , we repeated the same experiment for the object dataset , caltech-101  @xcite .",
    "to fix the parameter values , we used @xmath171 and @xmath172 . for this experiment",
    ", we used five samples per class for training and the rest for testing .",
    "again , the obtained results consistently favor sa - crc .",
    "qualitatively similar behavior was observed for all the train / test partitions used in table  [ tab : c101 ] .",
    "in contrast to a popular existing notion , we showed that sparsity of a collaborative representation ( cr ) plays an explicit role in accurate cr - based classification , hence it should not be completely ignored for computational gains .",
    "inspired by this result , we propose a sparsity augmented collaborative representation based classification scheme ( sa - crc ) that augments a dense collaborative representation with an efficiently computed sparse representation .",
    "the resulting representation is classified using a efficient method .",
    "extensive experiments for face , action and object classification establish the effectiveness of sa - crc in terms of accuracy as well as computational efficiency .",
    "this work was supported by arc grants dp110102399 and dp1096801 ."
  ],
  "abstract_text": [
    "<S> many classification approaches first represent a test sample using the training samples of all the classes . </S>",
    "<S> this collaborative representation is then used to label the test sample . </S>",
    "<S> it was a common belief that sparseness of the representation is the key to success for this classification scheme . </S>",
    "<S> however , more recently , it has been claimed that it is the collaboration and not the sparseness that makes the scheme effective . </S>",
    "<S> this claim is attractive as it allows to relinquish the computationally expensive sparsity constraint over the representation . in this paper </S>",
    "<S> , we first extend the analysis supporting this claim and then show that sparseness explicitly contributes to improved classification , hence it should not be completely ignored for computational gains . inspired by this result , we augment a dense collaborative representation with a sparse representation and propose an efficient classification method that capitalizes on the resulting representation . the augmented representation and the classification method work together meticulously to achieve higher accuracy and lower computational time compared to state - of - the - art collaborative representation based classification approaches . </S>",
    "<S> experiments on benchmark face , object and action databases show the efficacy of our approach .    multi - class classification , sparse representation , collaborative representation . </S>"
  ]
}