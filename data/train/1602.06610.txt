{
  "article_text": [
    "mixtures of regression models are commonly used to reveal the relationship among interested variables if the whole population is not homogeneous and consists of several homogeneous subgroups .",
    "it has been widely used in many areas such as econometrics , biology , and epidemiology . recently , many semiparametric mixture models have been proposed .",
    "see , for example , young and hunter ( 2010 ) , huang and yao ( 2012 ) , huang et al .",
    "( 2013 ) , cao and yao ( 2012 ) , xiang and yao ( 2015 ) , among others .    in this article",
    ", we apply the idea of single - index model to mixture of regression models , and propose a mixture of single - index models ( msim ) and a mixture of regression models with varying single - index proportions ( mrsip ) .",
    "huang et al . ( 2013 ) proposed the nonparametric mixture of regression models @xmath1 , and developed an estimation procedure by employing kernel regression .",
    "however , the above model is not very applicable to multivariate predictors due to the so called  curse of dimensionality \" .",
    "the proposed mixture of single - index models can naturally incorporate the multivariate predictors and relax the traditional parametric assumption of mixture of regression models .    in some cases",
    ", we might want to assume linearity in the mean functions .",
    "therefore , the proposed mrsip keeps the easy interpretation of the linear component regression functions while assuming that the mixing proportions are smooth functions of an index @xmath2 .",
    "we show the identifiability of each model under some regularity conditions . to achieve the optimal convergence rate for the global parameters and nonparametric functions , we propose backfitting estimates using the kernel regression technique .",
    "we have shown that the nonparametric functions can be estimated with the same rate as if the parameters were known , and the parameters can be estimated with the same rate of convergence , @xmath3 , that is achieved in a parametric model .",
    "numerical studies are used to demonstrate the effectiveness of the proposed new models , and we discuss the selection of the two models in the real data analysis .    the rest of the paper is organized as follows . in section [ sec : p3-s2 ] , we introduce the msim and study its identifiability result . a one - step estimate and a fully - iterated backfitting estimate",
    "have been proposed , and their asymptotic properties are studied .",
    "section [ sec : p3-s3 ] discusses the mrsip and its identifiability .",
    "a fully - iterated estimate and its asymptotic properties are also studied . in section [ sec : p3-s4 ] , we use monte carlo studies and a real data example to demonstrate the finite sample performance of the proposed estimates . a discussion section ends the paper .",
    "assume that @xmath4 is a random sample from population @xmath5 . throughout this article",
    ", we assume that @xmath6 is @xmath7-dimensional and @xmath8 is univariate .",
    "let @xmath9 be a latent variable , and we assume that conditional on @xmath6 , @xmath9 has a discrete distribution @xmath10 for @xmath11 .",
    "conditional on @xmath12 and @xmath6 , @xmath8 follows a normal distribution with mean @xmath13 and variance @xmath14 .",
    "we assume that @xmath15 , @xmath16 , and @xmath17 are unknown but smooth functions , and therefore , without observing @xmath9 , the conditional distribution of @xmath8 given @xmath6 can be written as : @xmath18 where @xmath19 is the normal density with mean @xmath20 and variance @xmath21 . throughout the paper ,",
    "we assume that @xmath22 is fixed , and refer to model ( [ p3-model ] ) as a finite semiparametric mixture of regression models , since @xmath15 , @xmath16 and @xmath17 are all nonparametric .",
    "when @xmath23 and @xmath15 and @xmath17 are constant , model ( [ p3-model ] ) reduces to a single index model ( ichimura , 1993 ; h@xmath24rdle et al . ,",
    "1993 ) . if @xmath15 and @xmath17 are constant , and @xmath16 are identity functions , then model ( [ p3-model ] ) reduces to a finite mixture of linear regression models ( goldfeld and quandt , 1973 ) . if @xmath6 is a scalar , then model ( [ p3-model ] ) reduces to the nonparametric mixture of regression model proposed by huang et al .",
    "therefore , the proposed model ( [ p3-model ] ) is a natural generalization of many existing popular models .    compared to huang et al .",
    "( 2013 ) , the appeal of the proposed msim is that by focusing on an index @xmath2 , the so - called `` curse of dimensionality '' in fitting multivariate nonparametric regression functions is avoided .",
    "it is of dimension - reduction structure in the sense that , if we can estimate the index @xmath25 efficiently , then we can use the univariate @xmath26 as the covariate and simplify the model ( [ p3-model ] ) to the nonparametric mixture regression model proposed by huang et al .",
    "( 2013 ) , and thus avoid the curse of dimensionality when nonparametric smoothing is employed .",
    "therefore , model ( [ p3-model ] ) is a reasonable compromise between fully parametric and fully nonparametric modeling .",
    "identifiability is a major concern for most mixture models .",
    "some well known results for identifiability of finite mixture models include : mixture of univariate normals is identifiable up to relabeling ( titterington et al .",
    "1985 ) and finite mixture of regression models is identifiable up to relabeling provided that covariates have a certain level of variability ( henning , 2000 ) .",
    "the following theorem gives the result on identifiability of model ( [ p3-model ] ) and its proof is given in section [ p3-sec : proofs ] .",
    "[ p3-thm : iden ] assume that ( i ) @xmath27 , @xmath28 , and @xmath29 are differentiable and not constant on the support of @xmath2 , @xmath11 ; ( ii ) the component of @xmath6 are continuously distributed random variables that have a joint probability density function ; ( iii ) the support of @xmath6 is not contained in any proper linear subspace of @xmath30 ; ( iv ) @xmath31 and the first nonzero element of @xmath25 is positive ; ( v ) any two curves @xmath32 and @xmath33 , @xmath34 , are transversal .",
    "then , model ( [ p3-model ] ) is identifiable .",
    "the transversality of two smooth curves ( huang et al . , 2013 ) implies that the mean and variance functions of any two components can not be tangent to each other .      in this subsection",
    ", we propose a one - step estimate and a fully iterative backfitting estimate to achieve the optimal convergence rate for both the index parameter and nonparametric functions .",
    "let @xmath35 be the log - likelihood of the collected data @xmath4 .",
    "that is : @xmath36 where @xmath37 , @xmath38 , and @xmath39 . since @xmath40 , @xmath41 and @xmath42 consist of nonparametric functions , ( [ p3-lstar ] ) is not ready for maximization .",
    "if @xmath43 is an estimate of @xmath25 , then @xmath40 , @xmath41 and @xmath42 can be estimated locally by maximizing the following local log - likelihood function : @xmath44 where @xmath45 and @xmath46 is a kernel density function .",
    "let @xmath47 , @xmath48 and @xmath49 be the result of maximizing ( [ p3-l1 ] ) .",
    "we can then further update the estimate of @xmath25 by maximizing @xmath50 with respect to @xmath25 .",
    "we now propose two effective algorithms to calculate the estimates .",
    "* one - step estimator ( os ) * + * step 1 : obtain an estimate of the index parameter @xmath25*. + apply sliced inverse regression ( li , 1991 ) to obtain the estimate of @xmath25 , denoted by @xmath43 . +",
    "* step 2 : modified em - type algorithm to maximize @xmath51 in ( [ p3-l1])*. + in step 2 , we propose a modified em - type algorithm to maximize @xmath52 and obtain the estimators @xmath47 , @xmath48 and @xmath49 . in practice , we usually want to evaluate unknown functions at a set of grid points , which in this case , requires us to maximize local log - likelihood functions at a set of grid points .",
    "if we simply imply an em algorithm , the labels in the em algorithm may change at different grid points , and we may not be able to get smoothed estimated curves ( huang and yao , 2012 ) .",
    "therefore , we propose the following modified em - type algorithm , which estimates the nonparametric functions simultaneously at a set of grid points .",
    "let @xmath53 be a set of grid points where some unknown functions are evaluated , and @xmath54 be the number of grid points .",
    "+   + calculate the expectations of component labels based on estimates from @xmath55 iteration : @xmath56 + update the estimates @xmath57 for @xmath58 .",
    "we then update @xmath59 , @xmath60 and @xmath61 , @xmath62 by linear interpolating @xmath63 , @xmath64 and @xmath65 , @xmath66 , respectively .",
    "note that in the m - step , the nonparametric functions are estimated simultaneously at a set of grid points , and therefore , the classification probabilities in the the e - step can be estimated globally to avoid the label switching problem ( yao and lindsay , 2009 ) .",
    "* fully iterative backfitting estimator ( fib ) * + to improve the estimation efficiency , we propose the following _ fully iterative backfitting estimator_. + * step 1 : obtain an initial estimate of the index parameter @xmath25*. + apply sliced inverse regression to obtain an initial estimate of the index parameter @xmath25 , denoted by @xmath43 . +",
    "* step 2 : modified em - type algorithm to maximize @xmath51 in ( [ p3-l1])*. + with @xmath43 , apply the modified em - algorithm proposed above to obtain the estimators @xmath47 , @xmath48 , and @xmath49 . + * step 3 : updating the estimate of @xmath25 by maximizing @xmath67 in ( [ p3-l2])*. + given @xmath47 , @xmath48 , and @xmath49 from step 2 , update the estimate of @xmath25 , denoted by @xmath43 , which maximizes @xmath67 defined in ( [ p3-l2 ] ) using some numerical methods . +",
    "* step 4 : iterate step 2 - 3 until convergence*.      the asymptotic properties of the proposed estimates are investigated below .",
    "let @xmath68 .",
    "define @xmath69 , @xmath70 , @xmath71 and @xmath72 $ ] , @xmath73 $ ] .    under further conditions defined in section [ p3-sec : proofs ] , the properties of the one - step estimator when @xmath25 is estimated to the order of @xmath74 ( i.e. , at the usual parametric rate ) is demonstrated in the following theorem .",
    "[ p3-thm : nonp ] assume that conditions ( c1)-(c7 ) in section [ p3-sec : proofs ] hold .",
    "then , as @xmath75 , @xmath76 and @xmath77 , we have @xmath78 where @xmath79 , with @xmath80 the marginal density function of @xmath2 , @xmath81 and @xmath82 .",
    "@xmath83 the fully iterative backfitting estimator is at least as efficient as the one - step estimator , but the one - step estimator achieves the same efficiency in some important applications with added computational convenience .",
    "this information lower bound turns out to be the same as in huang et al .",
    "thus , the nonparametric functions can be estimated with the same rate of convergence as it would have if the one - dimension quantity @xmath2 were observable .",
    "the next theorem shows that under further conditions , @xmath25 can be estimated at the usual parametric rate using the fully iterated algorithm .",
    "[ p3-thm : index ] assume that conditions ( c1)-(c8 ) in section [ p3-sec : proofs ] hold .",
    "then , as @xmath75 , @xmath84 , and @xmath85 , @xmath86 where @xmath87q_2(z)[\\bx\\btheta'(z)]^t\\right\\}-e\\left[\\bx\\btheta'(z)q_2(z)\\mathcal{i}^{(1)-1}_\\theta(z)e\\{q_2(z)[\\bx\\btheta ' ( z)]^t|z\\}\\right]$ ] .",
    "the mrsip assumes that @xmath10 for @xmath11 , and conditional on @xmath12 and @xmath6 , @xmath8 follows a normal distribution with mean @xmath88 and variance @xmath89 .",
    "that is , @xmath90 since @xmath15 s are nonparametric , model ( [ p41-model ] ) is also a finite semiparametric mixture of regression models .",
    "[ p4-thm : iden ] assume that ( i ) @xmath91 are differentiable and not constant on the support of @xmath2 , @xmath11 ; ( ii ) the component of @xmath6 are continuously distributed random variables that have a joint probability density function ; ( iii ) the support of @xmath6 contains an open set in @xmath30 and is not contained in any proper linear subspace of @xmath30 ; ( iv ) @xmath31 and the first nonzero element of @xmath25 is positive ; ( v ) @xmath92 , @xmath11 , are distinct pairs .",
    "then , model ( [ p41-model ] ) is identifiable .",
    "the log - likelihood of the collected data is : @xmath93 where @xmath37 , @xmath94 , and @xmath95 .",
    "since @xmath40 consists of nonparametric functions , ( [ p41-lstar ] ) is not ready for maximization .    if @xmath96 are estimates of @xmath97 , then @xmath40 can be estimated locally by maximizing the following local log - likelihood function : @xmath98    let @xmath47 be the result of maximizing ( [ p41-l1 ] ) .",
    "we can then further update the estimate of @xmath97 by maximizing @xmath99      * step 1 : obtain an initial estimate of @xmath97*. + * step 2 : modified em - type algorithm to maximize @xmath100 in ( [ p41-l1])*. +   + calculate the expectations of component labels based on estimates from @xmath55 iteration : @xmath101 + update the estimate @xmath102 for @xmath58 .",
    "we then update @xmath59 , @xmath62 by linear interpolating @xmath63 , @xmath66 . +",
    "* step 3 : update @xmath96 by maximizing ( [ p41-l2])*. + * step 3.1 : given @xmath43 , update @xmath103*. +   + calculate the expectations of component identities : @xmath104 + update @xmath105 and @xmath106 : @xmath107 where @xmath11 , @xmath108 , @xmath109 . +",
    "* step 3.2 : given @xmath110 , update @xmath25 . *",
    "+ given @xmath110 , maximize @xmath111 to updates the estimate of @xmath25 , using some numerical methods . +",
    "* step 3.3 : iterate step 3.1 - 3.2 until convergence*. + * step 4 : iterate step 2 - 3 until convergence*.      define @xmath112 , @xmath113 , and @xmath114 .",
    "let @xmath115 , @xmath116 , and similarly , define @xmath117 , @xmath118 , and @xmath119 .",
    "denote @xmath120 $ ] and @xmath121 $ ] .    under further conditions , the properties of the estimator when @xmath122 is estimated to the order of @xmath74 ( i.e. , at the usual parametric rate )",
    "is demonstrated in the following theorem .",
    "[ p4-thm : nonp ] assume that conditions ( c1)-(c4 ) and ( c9)-(c11 ) in section [ p3-sec : proofs ] hold .",
    "then , as @xmath75 , @xmath76 and @xmath77 , we have @xmath123 where @xmath124 .    [",
    "p4-thm : para ] assume that conditions ( c1)-(c4 ) and ( c9)-(c12 ) in section [ p3-sec : proofs ] hold .",
    "then , as @xmath75 , @xmath84 , and @xmath85 , @xmath125 where , + @xmath126 $ ] .",
    "in this section , we conduct simulation studies to test the performance of the proposed methodologies .    the performance of the estimates of the mean functions @xmath16 s in model ( [ p3-model ] )",
    "is measured by the square root of the average square errors ( rase ) @xmath127 ^ 2.\\notag\\ ] ] in this simulation , we set @xmath128 , and take the grid points are on the range .",
    "similarly , we can define the @xmath129 for the variance functions @xmath17 s and proportion functions @xmath15 s , denoted by @xmath130 and @xmath131 , respectively .    to apply the proposed methodologies",
    ", we use cross - validation ( cv ) to select a proper bandwidth for estimating the nonparametric functions .",
    "we conduct a simulation for a 2-component msim :    @xmath132 and @xmath133 , + @xmath134 and @xmath135 , + @xmath136 and @xmath137 .    where @xmath138 , @xmath139 are trivariate with independent uniform ( 0,1 ) components , and the direction parameter is @xmath140 .",
    "the sample sizes @xmath141 , @xmath142 , and @xmath143 are conducted over @xmath144 repetitions . to estimate @xmath25",
    ", we use sliced inverse regression ( sir ) and the fully iterative backfitting estimate ( fib ) . to estimate the nonparametric functions , we apply the one - step estimate ( os ) and fib . for fib",
    ", we use both true value ( t ) and sir ( s ) as the initial values .",
    "we first select a proper bandwidth for estimating @xmath40 , @xmath41 and @xmath42 .",
    "there are ways to calculate theoretical optimal bandwidth , but in practice , data driven methods , such as cross - validation ( cv ) , are popularly used .",
    "let @xmath145 be the full data set , and divide @xmath145 into a training set @xmath146 and a test set @xmath147 .",
    "that is , @xmath148 for @xmath149 .",
    "we use the training set @xmath146 to obtain the estimates @xmath150 .",
    "we then evaluate @xmath40 , @xmath41 and @xmath42 at the data in the corresponding training set .",
    "then , for @xmath151 , we calculate the classification probability as @xmath152 for @xmath11 .",
    "we then consider the regular @xmath153 , which is defined by @xmath154 where @xmath155 .",
    "we set @xmath156 and randomly partition the data .",
    "we repeat the procedure 30 times , and take the average of the selected bandwidth as the optimal bandwidth , denoted by @xmath157 . in the simulation , we consider three different bandwidth , @xmath158 , @xmath157 and @xmath159 , which correspond to the under - smoothing , appropriate smoothing and over - smoothing condition , respectively .",
    "table [ p3-table1 ] reports the mses of @xmath43 ( true value times 100 ) . from table",
    "[ p3-table1 ] , we can see that the fully iterative estimates give better results than sir .",
    "we further notice that fib(s ) provides similar results to fib(t ) , and therefore , sir provides good initial values for other estimates .",
    "table [ p3-table2 ] contains the mean and standard deviation of @xmath131 , @xmath160 , and @xmath130 .",
    "we see that the fully iterative estimate is not sensitive to initial values .",
    "0.05 in    .mse of @xmath43 ( true value times 100 ) [ cols=\"^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ p41-tab2 ]",
    "we illustrate the proposed methodology by an analysis of `` the effectiveness of national basketball association guards '' .",
    "there are many ways to measure the ( statistical ) performance of guards in the national basket association ( nba ) .",
    "of interest is how the height of the player ( height ) , minutes per game ( mpg ) and free throw percentage ( ftp ) affects points per game ( ppm ) ( chatterjee et al . , 1995 ) .",
    "the data set contains some descriptive statistics for all 105 guards for the 1992 - 1993 season . since players playing very few minutes are quite different from those who play a sizable part of the season , we only look at those players playing 10 or more minutes per game and appearing in 10 or more games . we see that michael jordan is an outlier in terms of ppm , so we will also omit him from the data ( chatterjee et al . , 1995 ) .",
    "these excludes 10 players .",
    "we divide each variable by its corresponding standard deviation , so that they have comparable numerical scale .",
    "an optimal bandwidth is selected at 0.344 by cv procedure .",
    "figure [ p3-figure3](a ) contains the estimated mean functions and hard - clustering results , denoted by dots and squares , respectively .",
    "the 95% confidence interval for @xmath43 based on msim are ( 0.134,0.541 ) , ( 0.715,0.949 ) and ( 0.202,0.679 ) , indicating that mpg is the most influential factor on ppm .",
    "to evaluate the prediction performance of the proposed model and compared it to linear regression model and mixture of linear regression models , we used @xmath161-fold cross - validation with @xmath161=5,10 , and also monte - carlo cross - validation ( mccv ) ( shao , 1993 ) . in mccv ,",
    "the data were partitioned 500 times into disjoint training subsets ( with size @xmath162 ) and test subsets ( with size @xmath161 ) .",
    "the mean squared prediction error evaluated at the test data sets over 500 replications are reported as boxplots in figure [ p3-figure3](b ) .",
    "apparently , the msim and the mrsip have superior prediction power than the linear regression model or the mixture of linear regression models , and msim is more favorable then the mrsip for this data set .",
    "in this paper we proposed two finite semiparametric mixture of regression models and the corresponding backfitting estimates .",
    "we showed that the nonparametric functions can be estimated with the same rate as if the parameters were known and the parameters can be estimated with root-@xmath0 convergence rate . in this article , we assume that the number of components is known and fixed , but it requires more research to select the number of components for the proposed semiparametric mixture models .",
    "in addition , it is also interesting to build some formal test to compare the proposed two semiparametric mixture models .",
    "one way is to apply generalized likelihood ratio statistic proposed by fan et al . , ( 2001 ) .",
    "* technical conditions : *    1 .",
    "the sample @xmath4 is independent and identically distributed from its population @xmath5 .",
    "the support for @xmath6 , denoted by @xmath163 , is a compact subset of @xmath164 .",
    "the marginal density of @xmath2 , denoted by @xmath80 , is twice continuously differentiable and positive at the point @xmath165 .",
    "3 .   the kernel function @xmath46 has a bounded support , and satisfies that @xmath166 @xmath167 4 .",
    "@xmath168 , @xmath169 , and @xmath170 as @xmath75 .",
    "the third derivative @xmath171 for all @xmath172 and all @xmath173 in a neighborhood of @xmath174 , and @xmath175<\\infty$ ]",
    "the unknown functions @xmath174 have continuous second derivative . for @xmath11 , @xmath176 , and @xmath91 for all @xmath177 . 7 .   for all @xmath178 and @xmath179 ,",
    "the following conditions hold : @xmath180<\\infty\\hspace{10 mm } e\\left[\\left(\\frac{\\partial^2\\ell(\\btheta(z),y)}{\\partial\\theta_i\\partial\\theta_j}\\right)^2\\right]<\\infty\\notag\\ ] ] 8 .",
    "@xmath181 is continuous at the point @xmath165 .",
    "the third derivative @xmath182 for all @xmath172 and all @xmath183 in a neighborhood of @xmath184 , and @xmath175<\\infty$ ] . 10 .",
    "the unknown functions @xmath184 have continuous second derivative . for @xmath11 , @xmath91 for all @xmath177 . 11 . for all @xmath178 and @xmath179 ,",
    "the following conditions hold : @xmath185<\\infty\\hspace{10 mm } e\\left[\\left(\\frac{\\partial^2\\ell(\\bpi(z),y)}{\\partial\\pi_i\\partial\\pi_j}\\right)^2\\right]<\\infty\\notag\\ ] ] 12 .",
    "@xmath186 is continuous at the point @xmath165 .",
    "@xmath187 + ichimura ( 1993 ) have shown that under conditions ( i)-(iv ) , @xmath25 is identifiable .",
    "further , huang et al . (",
    "2013 ) showed that with condition ( v ) , the nonparametric functions are identifiable .",
    "thus completes the proof .",
    "@xmath188 + let@xmath189 define @xmath190 , @xmath191 , @xmath192 and denote @xmath193 .",
    "let @xmath194 .",
    "let @xmath195 if @xmath196 maximizes ( [ p3-l1 ] ) , then @xmath197 maximizes @xmath198k_h(\\hat{z}_i - z)\\ ] ] with respect to @xmath199 . by a taylor expansion , @xmath200 where",
    "@xmath201 and @xmath202 by wlln , it can be shown that @xmath203 .",
    "therefore , @xmath204 using the quadratic approximation lemma ( see , for example , fan and gijbels ( 1996 ) ) , we have that @xmath205 note that @xmath206 where @xmath207^tk_h(z_i - z)\\right\\}(\\hat{\\balpha}-\\balpha).\\notag\\end{aligned}\\ ] ] since @xmath208 , it can be shown that @xmath209^t]=o_p(1)$ ] , and @xmath210 .",
    "therefore , @xmath211 to complete the proof , we now calculate the mean and variance of @xmath212 .",
    "note that @xmath213\\right]\\notag\\\\ & = \\sqrt{nh}[\\frac{1}{2}f(z)\\lambda_1^{''}(z|z)+f'(z)\\lambda_1^{'}(z|z)]\\kappa_2h^2.\\end{aligned}\\ ] ] similarly , we can show that @xmath214 , where @xmath81 and @xmath82 .",
    "the rest of the proof follows a standard argument .",
    "@xmath215 + denote @xmath216 and @xmath217 .",
    "let @xmath218 . if @xmath219 maximizes ( [ p3-l1 ] ) , then it solves @xmath220 apply a taylor expansion and use the conditions on @xmath221 , we obtain @xmath222(\\hat{\\btheta}(z_0;\\hat{\\balpha})-\\btheta(z_0))\\notag\\\\ & + n^{-1}\\sum_{i=1}^nq_{2i}(z_i)[\\bx_i\\btheta'(z_i)]^tk_h(z_i - z_0)(\\hat{\\balpha}-\\balpha)+o_p(n^{-1/2})+o_p(h^2)\\notag\\end{aligned}\\ ] ] by similar argument as in the previous proof , @xmath223^t|z = z_0\\}(\\hat{\\balpha}-\\balpha)+o_p(n^{-1/2 } ) \\label{p3-a11}\\end{aligned}\\ ] ] note that @xmath224 where the second part is handled by ( [ p3-a11 ] ) .",
    "since @xmath43 maximizes ( [ p3-l2 ] ) , it is the solution to @xmath225 where @xmath226 is the lagrange multiplier . by the taylor expansion and using ( [ p3-a13 ] ) ,",
    "we have that @xmath227+o_p(1)\\notag\\\\ & = \\lambda\\hat{\\balpha}+n^{-1/2}\\sum_{i=1}^n\\bx_i\\btheta'(z_i)q_{1i}(z_i)+n^{-1/2}\\sum_{i=1}^n \\bx_i\\btheta'(z_i ) q_{2i}(z_i)(\\bx_i\\btheta'(z_i))^t(\\hat{\\balpha}-\\balpha)\\notag\\\\ & + n^{-1/2}\\sum_{i=1}^n\\bx_i\\btheta'(z_i)q_{2i}(z_i ) [ \\hat{\\btheta}(z_i)-\\btheta(z_i)])+o_p(1).\\notag\\end{aligned}\\ ] ] define @xmath228q_2(z)[\\bx\\btheta'(z)]^t\\},\\notag\\ ] ] and apply ( [ p3-a11 ] ) , @xmath229^t|z = z_i\\}(\\hat{\\balpha}-\\balpha)\\notag\\\\ & + n^{-1/2}\\sum_{i=1}^n\\bx_i\\btheta'(z_i)q_{2i}(z_i ) n^{-1}f^{-1}(z_i)\\mathcal{i}_\\theta^{-1}(z_i)\\sum_{t=1}^nq_{1t}(z_t)k_h(z_t - z_i)+o_p(1)\\notag\\\\ & = \\lambda\\hat{\\balpha}+n^{-1/2}\\sum_{i=1}^n\\bx_i\\btheta'(z_i)q_{1i}(z_i ) + \\bq_1 n^{1/2}(\\hat{\\balpha}-\\balpha)\\notag\\\\ & + n^{-1/2}\\sum_{i=1}^n\\bx_i\\btheta'(z_i)q_{2i}(z_i ) n^{-1}f^{-1}(z_i)\\mathcal{i}^{(1)-1}_\\theta(z_i)\\sum_{t=1}^nq_{1t}(z_t)k_h(z_t - z_i)+o_p(1).\\end{aligned}\\ ] ] interchanging the summations in the last term , we get @xmath230\\notag\\\\ & = n^{-1/2}\\sum_{i=1}^ne[\\bx\\btheta'(z)q_2(z)|z_i]\\mathcal{i}^{(1)-1}_\\theta(z_i)q_{1i}(z_i)+o_p(1)\\end{aligned}\\ ] ]    let @xmath231 . combining ( [ p3-a14 ] ) and ( [ p3-a15 ] ) , and",
    "multiply by @xmath232 , we have @xmath233\\mathcal{i}^{(1)-1}_\\theta ( z_i)\\}q_{1i}(z_i)+o_p(1)\\end{aligned}\\ ] ] it can be shown that the right - hand side of ( [ p3-a12 ] ) has the covariance matrix @xmath234 , and therefore , completes the proof .",
    "@xmath235 + ichimura ( 1993 ) have shown that under conditions ( i)-(iv ) , @xmath25 is identifiable .",
    "furthermore , huang and yao ( 2012 ) showed that with condition ( v ) , @xmath236 are identifiable . thus completes the proof .",
    "let @xmath238 , @xmath239 , and @xmath190 .",
    "it can be shown that @xmath240 where @xmath241 to complete the proof , notice that @xmath242\\right\\}\\notag\\\\ = & \\sqrt{nh}[\\frac{1}{2}f(z)\\lambda_2''(z|z)+f'(z)\\lambda_2'(z|z)]\\kappa_2h^2.\\notag\\end{aligned}\\ ] ] and cov@xmath243 .",
    "the rest of the proof follows a standard argument .",
    "+ @xmath244 + the proof is similar to the proof of theorem [ p3-thm : index ] .",
    "it can be shown that @xmath245^t|z = z_0\\}(\\hat{\\balpha}-\\balpha)-\\mathcal{i}^{(2)-1}_\\pi(z_0 ) e\\{q_{\\pi\\eta}(z)|z = z_0\\}(\\hat{\\bta}-\\bta)+o_p(n^{-1/2}),\\notag \\label{p4-e1}\\end{aligned}\\ ] ] and therefore , @xmath246 since @xmath247 maximizes ( [ p41-l2 ] ) , it is the solution to @xmath248 where @xmath249 is the lagrange multiplier . by taylor",
    "series and ( [ p4-e2 ] ) @xmath250\\mathcal{i}^{(2)-1}_\\pi(z_i)q_{\\pi i}(z_i)+o_p(1 ) .",
    "\\label{p4-e3}\\end{aligned}\\ ] ] where @xmath251 , and the last equation is the result of interchanging the summations .",
    "let @xmath252 . by ( [ p4-e3 ] ) , and",
    "multiply by @xmath232 , we have @xmath253\\right\\}q_{\\pi i}(z_i)+o_p(1 ) .",
    "\\label{p4-e5}\\ ] ] it can be shown that the right - hand side of ( [ p4-e5 ] ) has the covariance matrix @xmath254 , and thus , completes the proof ."
  ],
  "abstract_text": [
    "<S> in this article , we propose a class of semiparametric mixture regression models with single - index . </S>",
    "<S> we argue that many recently proposed semiparametric / nonparametric mixture regression models can be considered as special cases of the proposed model . </S>",
    "<S> however , unlike existing semiparametric mixture regression models , the new proposed model can easily incorporate multivariate predictors into the nonparametric components . </S>",
    "<S> backfitting estimates and the corresponding algorithms have been proposed to achieve the optimal convergence rate for both the parameters and the nonparametric functions . </S>",
    "<S> we show that nonparametric functions can be estimated with the same asymptotic accuracy as if the parameters were known and the index parameters can be estimated with the traditional parametric root @xmath0 convergence rate . </S>",
    "<S> simulation studies and an application of nba data have been conducted to demonstrate the finite sample performance of the proposed models .    </S>",
    "<S> * key words * : em algorithm , kernel regression , mixture regression model , single - index models . </S>"
  ]
}