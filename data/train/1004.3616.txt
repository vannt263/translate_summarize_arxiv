{
  "article_text": [
    "the cumulative normal distribution , be it univariate or multivariate , has to be evaluated numerically .",
    "there are numerous algorithms available , many of these having been fine - tuned , leading to faster evaluation and higher accuracy but also to lack of mathematical transparency",
    ".    for the univariate case , has proposed a very simple and intuitive but powerful alternative that is based on taylor expansion of mills ratio or similar functions . in this note",
    "we will extend marsaglia s approach to the bivariate case .",
    "this will require two steps : reduction of the evaluation of the cumulative bivariate normal distribution to evaluation(s ) of a univariate function , i.e. , to the cumulative bivariate normal distribution on the diagonal , and taylor expansion of that function . note that a similar approach , but with reduction to the axes instead of the diagonals , has been proposed by .",
    "the resulting algorithm has to be compared with existing approaches . for overview on and discussion of the latter , cf .",
    "@xcite , @xcite , @xcite , and @xcite .",
    "most implementations today will rely on variants of the approaches of or of .",
    "improvements of the latter method have been provided by and .",
    "the method of , although less reliable , is also very common , mainly because it is featured in @xcite and other prevalent books .",
    "it will turn out that the algorithm proposed in this paper is able to deliver near double precision ( in terms of absolute error ) using double arithmetic .",
    "furthermore , implementation of the algorithm using high - precision libraries is straightforward ; indeed , a quad - double implementation has been applied for testing purposes .",
    "performance is competitive , and trade - offs between speed and accuracy may be implemented with little effort .",
    "in this section we are going to develop the algorithm . in order to keep the presentation lean",
    "we will often refer to the author s recent survey @xcite . for further background on normal distributions",
    "the reader is also referred to text books such as @xcite , @xcite and @xcite .",
    "denote by @xmath0 the density and distribution function of the standard normal distribution .",
    "mills ratio is then defined as @xmath1 furthermore , denote by @xmath2 the density and distribution function of the bivariate standard normal distribution with correlation parameter @xmath3 .",
    "we will also write @xmath4 we are going to use the following properties : @xmath5 in the following we will assume that @xmath6 , @xmath7 .",
    "in this case the following bounds apply ( cf .",
    "5.2 ) ) : @xmath8 furthermore , as is proven implicitly in ( * ? ? ?",
    "a.2 ) , @xmath9 now we define @xmath10 starting with @xmath11 we find the recursion @xmath12 which we can use to recursively evaluate the taylor expansion of @xmath13 around zero .",
    "dividing by @xmath14 for convenience , we define @xmath15 using @xmath16 we derive the following recursion scheme : @xmath17 with initial values @xmath18 here we have used that @xmath19 we can now compute @xmath20 numerically via @xmath21 note that it would also have been possible to work with , e.g. , one of the functions @xmath22 instead .",
    "the resulting recursion schemes are in fact easier ( two summands instead of three ) but will be running into numerical problems ( cancellation , or lower accuracy for @xmath23 ) .      in order to apply the results from section [ subsec_diagonal ] to the numerical evaluation of @xmath24 for general @xmath25 , @xmath26 and @xmath27 , we start with the symmetric formula ( cf .",
    "* eq . ( 3.16 ) ) ) @xmath28 where @xmath29 and @xmath30 from the axis @xmath31 to the diagonal @xmath32 we get by applying the formula ( cf .",
    "* eq . ( 3.18 ) ) ) @xmath33 specifically , we obtain @xmath34 with @xmath35 where in an implementation ( [ eq_ax_rho1 ] ) should be used for @xmath36 , and ( [ eq_ax_rhom1 ] ) for @xmath37 , in order to avoid catastrophic cancellation .",
    "note also that @xmath38 in a last step , if necessary to ensure @xmath6 and @xmath7 , we apply the formulas ( cf .",
    "* eq . ( 2.15 ) ) and ( * ? ? ?",
    "* eq . ( 3.27 ) ) ) @xmath39 specifically , we obtain @xmath40 where in an implementation ( [ eq_bx_rho1 ] ) should be used for @xmath36 , and ( [ eq_bx_rhom1 ] ) for @xmath37 , in order to avoid catastrophic cancellation",
    ".    it will be favorable to work with @xmath41 instead of @xmath42 .",
    "if ( [ eq_minusrho ] ) has to be applied ( i.e. , if @xmath43 , which is equivalent with @xmath44 ) , correspondingly we will work with @xmath45",
    "in the following we will discuss implementation of the algorithm derived in section [ sec_theory ] .",
    "the c++ language has been chosen because it is the market standard in quantitative finance , one of the fields frequently requiring evaluation of normal distributions .",
    "source code ( in c++ ) for evaluation of @xmath20 as in section [ subsec_diagonal ] , for @xmath6 and @xmath7 , is provided in figure [ source_diag ] . in the following",
    "we will comment on some details of the implementation .",
    "equations ( [ eq_rec_1 ] ) - ( [ eq_start_6 ] ) show that it is reasonable to provide @xmath46 , instead of @xmath27 , as input for the evaluation of @xmath20 .",
    "moreover , cf .",
    "( [ eq_rho_1 ] ) and ( [ eq_rho_2 ] ) , double inversion ( i.e. , computation of @xmath47 instead of @xmath48 ) is to be avoided in the reduction algorithm .",
    "values for @xmath49 and for @xmath50 are also expected as input parameters .",
    "this makes sense because the values are needed by the reduction algorithm as well ( and hence should not be computed twice ) .",
    "evaluation of @xmath51 is to be avoided for @xmath52 and has been replaced ( without optimization of the cutoff point ) by @xmath53 note that @xmath54 has to be computed anyway .",
    "constants ( all involving @xmath55 ) have been pre - computed in double precision .",
    "the recursion stops if a new term does not change the computed sum .",
    "if the a priori bound for the absolute error , given by ( [ eq_bounds ] ) , is less than @xmath56 , the upper bound is returned ( relative accuracy on the diagonal may be increased by dropping this condition but overall relative accuracy will still be determined by the reduction to the diagonal , cf .",
    "section [ subsec_reduction_imp ] ) , and by the accuracy of the implementation of @xmath57 .",
    "the final result is always checked against the upper and lower bound .",
    "note that @xmath58 and @xmath59 have different sign but comparable order .",
    "bracketing them before summation can therefore reduce cancellation error .    ' '' ''    .... double phi2diag ( const double & x ,                   const double & a ,       // 1 - rho                   const double & px ,      // phi ( x )                   const double & pxs )    //",
    "phi ( lambda ( rho ) * x ) {      if ( a < = 0.0 ) return px ;         // rho = = 1      if ( a > = 1.0 ) return px * px ;    // rho = = 0        double b = 2.0 - a , sqrt_ab = sqrt ( a * b ) ;      double asr = ( a > 0.1 ?",
    "asin ( 1.0 - a ) : acos ( sqrt_ab ) ) ;      double comp = px * pxs ;      if ( comp * ( 1.0 - a - 6.36619772367581343e-001 * asr ) < 5e-17 )          return b * comp ;        double tmp = 1.25331413731550025 * x ;      double a_coeff = a * x * x / b ;      double a_even = -tmp * a ;      double a_odd = -sqrt_ab * a_coeff ;      double b_coeff = x * x ;      double b_even = tmp * sqrt_ab ;      double b_odd = sqrt_ab * b_coeff ;      double d_coeff = 2.0 * x * x / b ;      double d_even = ( 1.0 - a ) * 1.57079632679489662 - asr ;      double d_odd = tmp * ( sqrt_ab - a ) ;        double res = 0.0 , res_new = d_even + d_odd ;      int k = 2 ;      while ( res !",
    "= res_new )      {          d_even = ( a_odd + b_odd + d_coeff * d_even ) / k ;          a_even * = a_coeff / k ;          b_even * = b_coeff / k ;          k++ ;          a_odd * = a_coeff / k ;          b_odd * = b_coeff / k ;          d_odd = ( a_even + b_even + d_coeff * d_odd ) / k ;          k++ ;          res = res_new ;          res_new + = d_even + d_odd ;      }      res * = exp ( -x * x / b ) * 1.591549430918953358e-001 ;      return max ( ( 1.0 + 6.36619772367581343e-001 * asr ) * comp ,                  b * comp - max ( 0.0 , res ) ) ; } ....    ' '' ''      source code ( in c++ ) for evaluation of @xmath20 as in equation ( [ eq_2axis ] ) is provided in figure [ source_phi ] , and source code for evaluation of @xmath60 is provided in figure [ source_help ] . in the following",
    "we will comment on some details of the implementation .",
    "the special cases @xmath61 and @xmath62 are dealt with in phi2 ( ) .",
    "therefore , in phi2help ( ) there is no check against 1.0 - rho = = 0.0 , 1.0 + rho = = 0.0 or s = = 0.0 .",
    "it is assumed that sqr(x ) evaluates x*x .",
    "the cutoff points @xmath63 have been set by visual inspection and might be optimized .    ' '' ''    ....",
    "double phi2help ( const double & x ,                   const double & y ,                   const double & rho ) {      if ( x = = 0.0 ) return ( y > = 0.0 ? 0.0 : 0.5 ) ;        double s = sqrt ( ( 1.0 - rho ) * ( 1.0 + rho ) ) ;        double a = 0.0 , b1 = -fabs ( x ) , b2 = 0.0 ;      if ( rho > 0.99 )      {          double tmp = sqrt ( ( 1.0 - rho ) / ( 1.0 + rho ) ) ;          b2 = -fabs ( ( x - y ) / s - x * tmp ) ;          a = sqr ( ( x - y ) / x / s - tmp ) ;      }      else if ( rho < -0.99 )      {          double tmp = sqrt ( ( 1.0 + rho ) / ( 1.0 - rho ) ) ;          b2 = -fabs ( ( x + y ) / s - x * tmp ) ;          a = sqr ( ( x + y ) / x / s - tmp ) ;      }      else      {          b2 = -fabs ( rho * x - y ) / s ;          a = sqr ( b2 / x ) ;      }        double p1 = phi ( b1 ) , p2 = phi ( b2 ) ;    // cum .",
    "standard normal        double q = 0.0 ;      if ( a < = 1.0 )          q = 0.5 * phi2diag ( b1 , 2.0 * a / ( 1.0 + a ) , p1 , p2 ) ;      else          q = p1 * p2 - 0.5 * phi2diag ( b2 , 2.0 / ( 1.0 + a ) , p2 , p1 ) ;        int c1 = ( y / x > = rho ) ;      int c2 = ( x < 0.0 ) ;      int c3 = c2 & & ( y > = 0.0 ) ;      return ( c1 & & c3 ?",
    "q - 0.5                        : c1 & & c2 ?",
    "q                        : c1 ?",
    "0.5 - p1 + q                        : c3 ?",
    "p1 - q - 0.5                        : c2 ?",
    "p1 - q                        : 0.5 - q ) ; } ....    ' '' ''    ' '' ''    .... double phi2 ( const double & x ,               const double & y ,               const double & rho ) {      if ( ( 1.0 - rho ) * ( 1.0 + rho ) < = 0.0 )    //",
    "|rho| = = 1          if ( rho > 0.0 )              return phi ( min ( x , y ) ) ;          else              return max ( 0.0 , min ( 1.0 , phi ( x ) + phi ( y ) - 1.0 ) ) ;        if ( x = = 0.0 & & y = = 0.0 )          if ( rho > 0.0 )              return phi2diag ( 0.0 , 1.0 - rho , 0.5 , 0.5 ) ;          else              return 0.5 - phi2diag ( 0.0 , 1.0 + rho , 0.5 , 0.5 ) ;        return max ( 0.0 ,             min ( 1.0 ,             phi2help ( x , y , rho ) + phi2help ( y , x , rho ) ) ) ; } ....    ' '' ''",
    "evaluation of @xmath24 as in section [ sec_implement ] will require ( at most ) four calls to an implementation of the cumulative standard normal distribution @xmath57 ( phi ( ) in the code ) .",
    "the actual choice may well determine both accuracy and running time of the algorithm .",
    "for testing purposes i have been using a hybrid method , calling the algorithm from ( * ? ? ?",
    "2 ) for absolute value larger than @xmath64 , and phi ( ) from @xcite else . besides phi ( ) , exp ( ) will be called two times , arcsin ( ) or arccos ( ) two times , and sqrt ( ) six times .",
    "everything else is elementary arithmetic .    due to the reduction algorithm",
    ", the final result will be a sum .",
    "therefore , very high accuracy in terms of relative error can not be expected .",
    "consequently , evaluation of the diagonal aims at absolute error as well .    the phi2diag ( ) function is behaving as it may be expected from an approximation by a taylor series around zero : ( absolute ) error increases with decreasing @xmath25 . for @xmath65 ( or @xmath66 or @xmath36 )",
    "the error bounds from ( [ eq_bounds ] ) are taking over , and absolute error decreases again .",
    "the maximum absolute error is obtained for @xmath67 , @xmath68 ( maximum error of the upper bound is obtained for @xmath69 , cf .",
    "5.2 ) ) .    in general , assuming that all numerical fallacies in the reduction algorithm have been taken care of , the diagonal is expected to provide a worst case because the errors of the two calls to phi2diag ( ) will not cancel . with respect to the reduction algorithm ,",
    "the case @xmath70 , @xmath71 , implying @xmath72 , is most critical .    in order to give an impression of the algorithm s behaviour",
    ", we will discuss the results of a simulation study .",
    "for each @xmath73 , @xmath74 , the value of @xmath75 has been computed via the phi2 ( ) function from figure [ source_phi ] where @xmath76 has been drawn from a uniform distribution on @xmath77 $ ] with @xmath78 , @xmath79 has been drawn from a uniform distribution on @xmath80 $ ] , and @xmath81 where @xmath82 has been drawn from a uniform distribution on @xmath80 $ ] as well .    the c++ implementation from @xcite has been serving as a competitor . both functions have been evaluated against a quad - double precision version of phi2 ( ) , implemented using the qd library @xcite and quad - double precision constants .",
    "the diagram in figure [ diag_error ] is displaying , for @xmath73 , the 99% quantile and the maximum of the absolute difference between the double precision algorithms ( phi2 and west ) and the quad - double precision algorithm .    apart from a shift due to subtractions for positive @xmath25 ,",
    "errors of phi2 are rather symmetric around zero .",
    "the peaks at @xmath83 are due to the taylor expansion around zero ; the peaks at @xmath84 are due to taylor expansion after transformation of the argument .",
    "the characteristics of the @xmath85 quantile , in particular the little peaks at @xmath86 , are already visible in the error of the @xmath57 function used .",
    "the maximum error of west almost always stays below the one of phi2 .",
    "note that the maximum error of west is determined by the case @xmath37 and might be reduced by careful consideration of that case .    in the simulation study ,",
    "phi2 was a little slower than west : it took approximately five minutes and four minutes to perform the @xmath87 evaluations on a fairly standard office pc ( and it took two days to perform the corresponding quad - double precision evaluations ) .",
    "the number of recursion steps used by phi2diag is increasing with @xmath88 . because of the mathematical transparency of the algorithm it should be easy to find an appropriate trade - off between speed and accuracy by replacing the condition terminating the recursion .",
    "wang , m. , kennedy , w.j .",
    "( 1990 ) , _ comparison of algorithms for bivariate normal probability over a rectangle based on self - validated results from interval analysis _ ,",
    "journal of statistical computation and simulation 37(1 - 2 ) , pp ."
  ],
  "abstract_text": [
    "<S> we propose an algorithm for evaluation of the cumulative bivariate normal distribution , building upon marsaglia s ideas for evaluation of the cumulative univariate normal distribution . </S>",
    "<S> the algorithm is mathematically transparent , delivers competitive performance and can easily be extended to arbitrary precision . </S>"
  ]
}