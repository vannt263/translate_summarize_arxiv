{
  "article_text": [
    "on a daily basis , people undergo numerous interactions with objects that barely register on a conscious level . for instance , imagine a person shopping at a grocery store as shown in figure  [ fig : main ] .",
    "suppose she picks up a can of juice to load it in her shopping cart .",
    "the distance of the can is maintained fixed due to the constant length of her arm .",
    "when she checks the expiration date on the can , the distance and orientation towards the can is adjusted with respect to her eyes so that she can read the label easily . in the next aisle",
    ", she may look at a lcd screen at a certain distance to check the discount list in the store .",
    "thus , this example shows that spatial arrangement between objects and humans is subconsciously established in 3d . in other words , even though people do not consciously plan to maintain a particular distance and orientation when interacting with various objects , these interactions usually have some consistent pattern .",
    "this suggests the existence of an egocentric object prior in the person s field of view , which implies that a 3d salient object should appear at a predictable location , orientation , depth , size and shape when mapped to an egocentric rgbd image .",
    "our main conjecture stems from the recent work on human visual perception  @xcite , which shows that _ humans possess a fixed size prior for salient objects_. this finding suggests that a salient object in 3d undergoes a transformation such that people s visual system perceives it with an approximately fixed size . even though , each person s interactions with the objects are biased by a variety of factors such as hand dominance or visual acuity , common trends for interacting with objects certainly exist . in this work",
    ", we investigate whether one can discover such consistent patterns by exploiting egocentric object prior from the first - person view in rgbd frames .",
    "our problem can be viewed as an inverse object affordance task  @xcite . while the goal of a traditional object affordance task is to predict human behavior based on the object locations , we are interested in predicting potential salient object locations based on the human behavior captured by an egocentric rgbd camera .",
    "the core challenge here is designing a representation that would encode generic characteristics of visual saliency without explicitly relying on object class templates  @xcite or hand skin detection  @xcite .",
    "specifically , we want to design a representation that captures how a salient object in the 3d world , maps to an egocentric rgbd image . assuming the existence of an egocentric object prior in the first - person view , we hypothesize that a 3d salient object would map to an egocentric rgbd image with a predictable shape , location , size and depth pattern .",
    "thus , we propose an egoobject representation that represents each region of interest in an egocentric rgbd video frame by its _ shape _ , _ location _ , _ size _ , and _",
    "depth_. note that using egocentric camera in this context is important because it approximates the person s gaze direction and allows us to see objects from a first - person view , which is an important cue for saliency detection . additionally , depth information is also beneficial because it provides an accurate measure of object s distance to a person .",
    "we often interact with objects using our hands ( which have a fixed length ) , which suggests that depth defines an important cue for saliency detection as well . thus assuming the existence of an egocentric object prior",
    ", our egoobject representation should allow us to accurately predict pixelwise saliency maps in egocentric rgbd frames .    to achieve our goals , we create a new egocentric rgbd saliency dataset .",
    "our dataset captures people s interactions with objects during various activities such as shopping , cooking , dining . additionally , due to the use of egocentric - stereo cameras",
    ", we can accurately capture depth information of each scene .",
    "finally we note that our dataset is annotated for the following three tasks : saliency detection , future saliency prediction , and interaction classification .",
    "we show that we can successfully apply our proposed egocentric representation on this dataset and achieve solid results for these three tasks .",
    "these results demonstrate that by using our egoobject representation , we can accurately characterize an egocentric object prior in the first - person view rgbd images , which implies that salient objects from the 3d world map to an egocentric rgbd image with predictable characteristics of shape , location , size and depth .",
    "we demonstrate that we can learn this egocentric object prior from our dataset and then exploit it for 3d saliency detection in egocentric rgbd images .",
    "region proposals . for each of the regions",
    "@xmath1 we then generate a feature vector @xmath2 that captures shape , location , size and depth cues and use these features to predict the 3d saliency of region @xmath1 . ]",
    "* saliency detection in images . * in the past , there has been much research on the task of saliency detection in 2d images .",
    "some of the earlier work employs bottom - up cues , such as color , brightness , and contrast to predict saliency in images  @xcite .",
    "additionally , several methods demonstrate the importance of shape cues for saliency detection task  @xcite . finally , some of the more recent work employ object - proposal methods to aid this task  @xcite .    unlike the above listed methods that try to predict saliency based on contrast , brightness or color cues , we are more interested in expressing an egocentric object prior based on shape , location , size and depth cues in an egocentric rgbd image .",
    "our goal is then to use such prior for 3d saliency detection in the egocentric rgbd images .",
    "* egocentric visual data analysis .",
    "* in the recent work , several methods employed egocentric ( first - person view ) cameras for the tasks such as video summarization  @xcite , video stabilization  @xcite , object recognition  @xcite , and action and activity recognition  @xcite .    in comparison to the prior egocentric approaches we propose a novel problem , which can be formulated as an inverse object affordance problem : our goal is to detect 3d saliency in egocentric rgbd images based on human behavior that is captured by egocentric - stereo cameras .",
    "additionally , unlike prior approaches , we use * egocentric - stereo * cameras to capture egocentric rgbd data . in the context of saliency detection ,",
    "the depth information is important because it allows us to accurately capture object s distance to a person .",
    "since people often use hands ( which have fixed length ) to interact with objects , depth information defines an important cue for saliency detection in egocentric rgbd environment .    unlike other methods , which rely on object detectors  @xcite , or hand and skin segmentation  @xcite",
    ", we propose egoobject representation that is based solely on shape , location , size and depth cues in an egocentric rgbd images .",
    "we demonstrate that we can use our representation successfully to predict 3d saliency in egocentric rgbd images .",
    "based on our earlier hypothesis , we conjecture that objects from the 3d world map to an egocentric rgbd image with some predictable _ shape _ , _ location _ , _ size _ and _ depth_. we encode such characteristics in a region of interest @xmath3 using an egoobject map , @xmath4 \\in \\mathds{r}^{n_s\\times n_l \\times n_b \\times n_d \\times n_c}$ ] where @xmath5 , @xmath6 , @xmath7 , @xmath8 , and @xmath9 are the number of the feature dimension for shape @xmath10 , location @xmath11 , size @xmath12 , depth @xmath13 , and context @xmath14 , respectively .",
    "a shape feature , @xmath15^\\mathsf{t}$ ] captures a geometric properties such as area , perimeter , edges , and orientation of @xmath3 .",
    "* @xmath16 : perimeter divided by the squared root of the area , the area of a region divided by the area of the bounding box , major and minor axes lengths .",
    "* @xmath17 : we employ boundary cues  @xcite , which include , sum and average contour strength of boundaries in region @xmath3 and also minimum and maximum ultrametric - contour values that lead to appearance and disappearance of the smaller regions inside @xmath3  @xcite .",
    "* @xmath18 : eccentricity and orientation of @xmath3 and also the diameter of a circle with the same area as region @xmath3 .    a location feature @xmath19^\\mathsf{t}$ ]",
    "encode spatial prior of objects imaged in an egocentric view :    * @xmath20 : normalized bounding box coordinates and the centroid of a region @xmath3 .",
    "* @xmath21 : we also compute horizontal and vertical distances from the centroid of @xmath1 to the center of an image , and also to the mid - points of each border in the image .    a size feature @xmath22^\\mathsf{t}$ ]",
    "encodes the size of the bounding box and area of the region .    *",
    "@xmath23 : area and perimeter of region @xmath3 . *",
    "@xmath24 : area and aspect ratio of the bounding box corresponding to the region @xmath3 .",
    "@xmath25^\\mathsf{t}$ ] encodes a spatial distribution depth within @xmath3 .",
    "* @xmath26 : minimum , average , maximum , depth and also standard deviation of depth in a region @xmath3 .",
    "* @xmath27 : @xmath28 spatial depth histograms over the region @xmath3 .",
    "* @xmath29 : @xmath30 depth histograms over the region @xmath3 aligned to its major axis . * @xmath31 : @xmath28 spatial _",
    "normalized _ depth histograms over the region @xmath3 . *",
    "@xmath32 : @xmath30 _ normalized _ depth histograms over the region @xmath3 aligned to its major axis .",
    "in addition , we include a context feature @xmath14 that encodes a spatial relationship between near regions in the egocentric image .",
    "given two regions , @xmath33 computes a distance between two features , i.e. , @xmath34^\\mathsf{t}.\\nonumber\\end{aligned}\\ ] ] given a target region , @xmath3 , the context feature @xmath35^\\mathsf{t}$ ] computes the relationship with @xmath36 neighboring regions , @xmath37 :    * @xmath38 : @xmath39 * @xmath40 : @xmath41 * @xmath42 : @xmath43 * @xmath44 : @xmath45    where @xmath46 and @xmath47 are the feature vector constructued by the min - pooling and max - pooling of neighboring regions for each dimension .",
    "@xmath48 takes average of neighboring features and @xmath49 is the feature of the top @xmath50 nearest neighbor .",
    "* summary . * for every region of interest @xmath3 in an egocentric rgbd frame",
    ", we produce a @xmath51 dimensional feature vector denoted by @xmath52 .",
    "we note that some of these features have been successfully used previously in tasks other than 3d saliency detection  @xcite . additionally , observe that we do not use any object - level feature or hand or skin detectors as is done  @xcite .",
    "this is because , in this work , we are primarily interested in studying the idea that salient objects from the 3d world are mapped to an egocentric rgbd frame with a consistent shape , location , size and depth patterns .",
    "we encode these cues with our egoobject representation and show its effectiveness on egocentric rgbd data in the later sections of the paper .",
    "given an rgbd frame as an input to our problem , we first feed rgb channels to an mcg  @xcite method , which generates @xmath53 proposal regions .",
    "then , for each of these regions @xmath3 , we generate our proposed features @xmath52 and use it as an input to the random forest classifier ( rf ) . using a rf",
    ", we aim to learn the function that takes the feature vector @xmath52 corresponding to a particular region @xmath3 as an input , and produces an output for one our proposed tasks for region @xmath3",
    "( i.e. saliency value or interaction classification ) .",
    "we can formally write this function as @xmath54 .",
    "we apply the following pipeline for the following three tasks : 3d saliency detection , future saliency prediction , and interaction classification . however ,",
    "for each of these tasks we define a different output objective @xmath55 and train rf classifier according to that objective separately for each task .",
    "below we describe this procedure for each task in more detail .    * 3d saliency detection . *",
    "we train a random forest _",
    "regressor _ to predict region s @xmath3 intersection over union ( iou ) with a ground truth salient object . to train the rf regressor we sample @xmath56 regions from our dataset , and extract our features from each of these regions .",
    "we then assign a corresponding ground truth iou value to each of them and train a rf regressor using @xmath57 trees .",
    "our rf learns the mapping @xmath58 $ ] where @xmath55 denotes the ground truth iou value between the @xmath3 and the ground truth salient object . to deal with the imbalance issue",
    ", we sample an equal number of examples corresponding to the iou values of @xmath59,[0.25,0.5],[0.5,0.75]$ ] , and @xmath60 $ ] .    at testing time",
    ", we use mcg  @xcite to generate @xmath53 regions of interest .",
    "we then apply our trained rf for every region @xmath3 and predict @xmath61 , which denotes the saliency of a region @xmath3 .",
    "we note that mcg produces the set of regions that overlap with each other .",
    "thus , for the pixels belonging to multiple overlapping regions @xmath62 , we assign a saliency value that corresponds to the maximum predicted value across the overlapping regions ( i.e. @xmath63 ) .",
    "we illustrate the basic pipeline of our approach in fig .",
    "[ fig : method ] .            * future saliency prediction . * for future saliency prediction , given a video frame , we want to predict , which object will be salient ( i.e. used by a person ) after @xmath64 seconds .",
    "we hypothesize that the gaze direction is one of the most informative cues that are indicative of person s future behavior",
    ". however , gaze signal may be noisy if we consider only a single frame in the video .",
    "for instance , this may happen due to the person s attention being focused somewhere else for a split second or due to the shift in the camera .",
    "to make our approach more robust to the fluctuations of person s gaze , we incorporate simple temporal features into our system .",
    "our goal is to use these temporal cues to normalize the gaze direction captured by an egocentric camera and make it more robust to the small camera shifts .",
    "thus , given a frame @xmath65 which encodes time @xmath66 , we also consider frames @xmath67 .",
    "we pair up each of these frames @xmath68 with @xmath65 and compute their respective homography matrix @xmath69 .",
    "we then use each @xmath69 to recompute the image center @xmath70 in the current frame @xmath65 . for every region",
    "@xmath1 we then recompute its distance @xmath71 to the new center @xmath70 for all @xmath72}$ ] and concatenate these new distances to the original features @xmath52 .",
    "such gaze normalization scheme ensures robustness to our system in the case of gaze fluctuations .",
    "seconds , where the same object is salient .",
    "our goal here is to predict an object that will be salient after @xmath66 seconds .",
    "]     seconds , where the same object is salient .",
    "our goal here is to predict an object that will be salient after @xmath66 seconds .",
    "]     seconds , where the same object is salient .",
    "our goal here is to predict an object that will be salient after @xmath66 seconds .",
    "]     seconds , where the same object is salient .",
    "our goal here is to predict an object that will be salient after @xmath66 seconds .",
    "]     seconds , where the same object is salient .",
    "our goal here is to predict an object that will be salient after @xmath66 seconds .",
    "]     seconds , where the same object is salient .",
    "our goal here is to predict an object that will be salient after @xmath66 seconds . ]    * interaction classification .",
    "* most of the current computer vision systems classify objects by specific object class templates ( cup , phone , etc ) .",
    "however , these templates are not very informative and can not be used effectively beyond the tasks of object detection . adding object s function , and",
    "the type of interaction related to that object would allow researchers to tackle a wider array of problems overlapping vision and psychology .    to predict an interaction type at a given frame , for each frame we select top @xmath73 highest ranked regions @xmath74 according to their predicted saliency score .",
    "we then classify each of these regions either as sight or touch . finally , we take the majority label from these @xmath73 classification predictions , and use it to classify an entire frame as sight or touch .",
    "| c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c ? c    & mf & ap & mf & ap & mf & ap & mf & ap & mf & ap & mf & ap & mf & ap & mf & ap & mf & ap + fts  @xcite & 7.3 & 0.6 & 8.9 & 1.3 & 10.6 & 1.2 & 5.8 & 0.5 & 4.5 & 1.1 & 17.2 & 2.0 & 20.0 & 2.5 & 5.0 & 0.7 & 9.9 & 1.2 + mcg  @xcite & 10.4 & 4.7 & 13.8 & 7.0 & 21.1 & 12.7 & 7.1 & 2.9 & 12.5 & 5.7 & 23.6 & 12.2 & 31.2 & 14.9 & 11.1 & 5.1 & 16.4 & 8.1 + gbmr  @xcite & 8.0 & 3.0 & 15.6 & 6.8 & 14.7 & 7.0 & 6.8 & 3.0 & 4.3 & 1.3 & 32.7 & 18.3 & 46.0 & 30.2 & 12.9 & 5.7 & 17.6 & 9.4 + salobj  @xcite & 7.2 & 2.7 & 19.9 & 7.4 & 21.3 & 10.0 & 15.4 & 5.1 & 5.8 & 2.2 & 24.1 & 9.2 & 49.3 & 28.3 & 9.0 & 3.4 & 19.0 & 8.5 + gbvs  @xcite & 7.2 & 3.0 & 21.3 & 11.4 & 20.0 & 10.6 & 16.1 & 8.8 & 4.3 & 1.4 & 23.1 & 13.8 & 50.9 & * 50.2 & 11.6 & 5.7 & 19.3 & 13.1 + objectness  @xcite & 11.5 & 5.6 & * 35.1 & * 24.3 & 39.2 & 29.4 & 11.7 & 6.9 & 4.7 & 1.9 & 27.1 & 17.1 & 47.4 & 42.2 & 13.0 & 6.4 & 23.7 & 16.7 + * ours ( rgb ) & 25.7 & 16.2 & 34.9 & 21.8 & 37.0 & 23.0 & 23.3 & 14.4 & * 28.9 & * 18.5 & 32.0 & 18.7 & * 56.0 & 39.6 & 30.3 & 21.8 & 33.5 & 21.7 + * ours ( rgb - d ) & * 36.9 & * 26.6 & 30.6 & 18.2 & * 55.3 & * 45.4 & * 26.8 & * 19.3 & 18.8 & 10.5 & * 37.9 & * 25.4 & 50.6 & 38.4 & * 40.2 & * 28.5 & * 37.1 & * 26.5 + * * * * * * * * * * * * * * * * * * * *",
    "we now present our egocentric rgbd saliency dataset . our dataset records people s interactions with their environment from a first - person view in a variety of settings such as shopping , cooking , dining , etc .",
    "we use egocentric - stereo cameras to capture the depth of a scene as well .",
    "we note that in the context of our problem , the depth information is particularly useful because it provides an accurate distance from an object to a person .",
    "since we hypothesize that a salient object from the 3d world maps to an egocentric rgbd frame with a predictable depth characteristic , we can use depth information as an informative cue for 3d saliency detection task .",
    "our dataset has annotations for three different tasks : saliency detection , future saliency prediction , and interaction classification .",
    "these annotations enables us to train our models in a supervised fashion and quantitatively evaluate our results .",
    "we now describe particular characteristics of our dataset in more detail .    * data collection . *",
    "we use two stereo gopro hero 3 ( black edition ) cameras with @xmath75 baseline to capture our dataset .",
    "all videos are recorded at @xmath76 with @xmath77 .",
    "the stereo cameras are calibrated prior to the data collection and synchronized manually with a synchronization token at the beginning of each sequence",
    ".    * depth computation . *",
    "we compute disparity between the stereo pair after stereo rectification .",
    "a cost space of stereo matching is generated for each scan line and match each pixel by exploiting dynamic programming in a coarse - to- fine manner .",
    "* sequences . *",
    "we record @xmath78 video sequences that capture people s interactions with object in a variety of different environments .",
    "these sequences include : cooking , supermarket , eating , hotel @xmath79 , hotel @xmath80 , dishwashing , foodmart , and kitchen sequences .",
    "* saliency annotations .",
    "* we use grabcut software  @xcite to annotate salient regions in our dataset .",
    "we generate @xmath81 annotated frames for kitchen , cooking , and eating sequences , @xmath82 and @xmath83 annotated frames for supermarket and foodmart sequences respectively , @xmath84 and @xmath85 annotated frames for hotel @xmath79 and hotel @xmath80 sequences respectively , and @xmath86 annotated frames for dishwashing sequence ( for a total of @xmath87 frames with per - pixel salient object annotations ) . in fig .",
    "[ fig : data_po ] , we illustrate a few images from our dataset and the depth channels corresponding to these images . to illustrate ground truth labels , we overlay these images with saliency annotations .    additionally , in fig .  [",
    "fig : data_stats ] , we provide statistics that capture different properties of our dataset such as the location , depth , and size of annotated salient regions from all sequences . each video sequence from our dataset is marked by a different color in this figure .",
    "we observe that these statistics suggest that different video sequences in our dataset exhibit different characteristics , and captures a variety of diverse interactions between people and objects .",
    "* annotations for future saliency prediction . *",
    "in addition , we also label our dataset to predict future saliency in egocentric rgbd image after @xmath64 frames .",
    "specifically , we first find the frame pairs that are @xmath64 frames apart , such that the same object is present in both of the frames .",
    "we then check that this object is non - salient in the earlier frame and that it is salient in the later frame .",
    "finally , we generate per - pixel annotations for these objects in both frames .",
    "we do this for the cases where the pair of frames are @xmath88 , and @xmath89 seconds apart .",
    "we produce @xmath90 annotated frames for kitchen , @xmath91 for cooking , @xmath92 for eating , @xmath93 for supermarket , @xmath90 for hotel @xmath79 , @xmath94 for hotel @xmath80 , @xmath95 for foodmart , and @xmath90 frames dishwashing sequences .",
    "we present some examples of these annotations in fig .",
    "[ fig : data_fut ] .    * annotations for interaction classification . *",
    "to better understand the nature of people s interactions with their environment we also annotate each interaction either as _ sight _ or as _ touch_.",
    "in this section , we present the results on our egocentric rgbd saliency dataset for three different tasks , which include 3d saliency detection , future saliency prediction and interaction classification .",
    "we show that using our egoobject feature representation , we achieve solid quantitative and qualitative results for each of these tasks .",
    "to evaluate our results , we use the following procedure for all three tasks .",
    "we first train random forest ( rf ) using the training data from @xmath96 sequences .",
    "we then use this trained rf to test it on the sequence that was not used in the training data .",
    "such a setup ensures that our classifier is learning a meaningful pattern in the data , and thus , can generalize well on new data instances .",
    "we perform this procedure for each of the @xmath78 sequences separately and then use the resulting rf model to test on its corresponding sequence .    for the saliency detection and future saliency prediction tasks , our method predicts pixelwise saliency for each frame in the sequence . to evaluate our results we use two different measures : a maximum f - score ( mf ) along the precision - recall curve , and average precision ( ap ) . for the task of interaction classification",
    ", we simply classify each interaction either as sight or as touch .",
    "thus , to evaluate our performance we use the fraction of correctly classified predictions .",
    "we now present the results for each of these tasks in more detail .      detecting 3d saliency in an egocentric rgbd setting",
    "is a novel and relatively unexplored problem .",
    "thus , we compare our method with the most successful saliency detection systems for 2d images .     sequences .",
    "these visualizations demonstrate that in each of these sequences , our method captures an egocentric object prior that has a distinct shape , location , and size pattern . ]",
    "these visualizations demonstrate that in each of these sequences , our method captures an egocentric object prior that has a distinct shape , location , and size pattern . ]",
    "these visualizations demonstrate that in each of these sequences , our method captures an egocentric object prior that has a distinct shape , location , and size pattern . ]",
    "these visualizations demonstrate that in each of these sequences , our method captures an egocentric object prior that has a distinct shape , location , and size pattern . ]",
    "these visualizations demonstrate that in each of these sequences , our method captures an egocentric object prior that has a distinct shape , location , and size pattern . ]",
    "these visualizations demonstrate that in each of these sequences , our method captures an egocentric object prior that has a distinct shape , location , and size pattern . ]    in table  [ po_table ] , we present quantitative results for the saliency detection task on our dataset .",
    "we observe that our approach outperforms all the other methods by @xmath97 and @xmath98 in mf and ap evaluation metrics respectively .",
    "these results indicate that saliency detection methods designed for _ non - egocentric _",
    "images do not generalize well to the _ egocentric _ images .",
    "this can be explained by the fact that in most _ non - egocentric _ saliency detection datasets , images are displayed at a pretty standard scale , with little occlusions , and also close to the center of an image .",
    "however , in the egocentric environment , salient objects are often occluded , they appear at a small scale and around many other objects , which makes this task more challenging .    furthermore , we note that none of these baseline methods use depth information . based on the results , in table  [ po_table ] ,",
    "we observe that adding depth features to our framework provides accuracy gains of @xmath99 and @xmath100 according to mf and ap metrics respectively . finally , we observe that the results of different methods vary quite a bit from sequence to sequence .",
    "this confirms that our egocentric rgbd saliency dataset captures various aspects of people s interactions with their environment , which makes it challenging to design a method that would perform equally well in each of these sequences",
    ". based on the results , we see that our method achieves best results in @xmath96 and @xmath89 sequences ( out of @xmath78 ) according to mf and ap evaluation metrics respectively , which suggests that exploiting egocentric object prior via shape , location , size , and depth features allows us to predict visual saliency robustly across all sequences .    additionally , we present our qualitative results in fig .  [",
    "fig : po_preds ] .",
    "our saliency heatmaps in this figure suggest that we can accurately capture different types of salient interactions with objects . furthermore , to provide a more interesting visualization of",
    "our learned egocentric object priors , we average our predicted saliency heatmaps for each of the @xmath89 selected sequences and visualize them in fig .  [",
    "fig : avg_preds ] .",
    "we note that these averaged heatmaps have a certain shape , location , and size characteristics , which suggests the existence of an egocentric object prior in egocentric rgbd images .      in fig .",
    "[ fig : feats ] , we also analyze , which features contribute the most for the saliency detection task . the feature importance is quantified by the mean squared error reduction when splitting the node by that feature in a random forest . in this case , we manually assign each of our @xmath51 features to one of @xmath78 groups .",
    "these groups include shape , location , size , depth , shape context , location context , size context and depth context features ( as shown in fig .",
    "[ fig : feats ] ) . for each group , we average the importance value of all the features belonging to that group and present it in figure  [ fig : feats ] .    based on this figure , we observe that shape features contribute the most for saliency detection .",
    "additionally , since location features capture an approximate gaze of a person , they are deemed informative as well .",
    "furthermore , we observe that size and depth features also provide informative cues for capturing the saliency in an egocentric rgbd image .",
    "as expected , the context feature are least important .          in this section ,",
    "we present our results for the task of future saliency prediction .",
    "we test our trained rf model under three scenarios : predicting a salient object that will be used after @xmath88 , and @xmath89 seconds respectively .",
    "as one would expect , predicting the event further away from the present frame is more challenging , which is reflected by the results in table  [ fut_table ] .",
    "for this task , we aim to use our egoobject representation to learn the cues captured by egocentric - stereo cameras that are indicative of person s future behavior .",
    "we compare our future saliency detector ( fsd ) to the saliency detector ( sd ) from the previous section and show that we can achieve superior results , which implies the existence and consistency of the cues that are indicative of person s future behavior .",
    "such cues may include person s gaze direction ( captured by an egocentric camera ) , or person s distance to an object ( captured by the depth channel ) , which are both pretty indicative of what the person may do next .    in fig .",
    "[ fig : fut_preds ] , we visualize some of our future saliency predictions .",
    "based on these results , we observe , that even in a difficult environment such as supermarket , our method can make meaningful predictions .",
    ".future saliency results according to max f - score ( mf ) and average precision ( ap ) evaluation metrics .",
    "given a frame at time @xmath66 , our future saliency detector ( fsd ) predicts saliency for times @xmath101 , and @xmath102 ( denoted by seconds ) . as our baseline",
    "we use a saliency detector ( sd ) from section  [ tech_approach ] of this paper .",
    "we show that in every case we outperform this baseline according to both metrics .",
    "this suggests that using our representation , we can consistently learn some of the egocentric cues such as gaze , or person s distance to an object that are indicative of people s future behavior . [",
    "cols=\"^,^,^,^,^\",options=\"header \" , ]      in this section , we report our results on the task of interaction classification . in this case , we only have two labels ( sight and touch ) and so we evaluate the performance as a fraction of correctly classified predictions .",
    "we compare our approach with a depth - based baseline , for which we learn an optimal depth threshold for each sequence , then for a given input frame , if a predicted salient region is further than this threshold , our baseline classifies that interaction as _ sight _ , otherwise the baseline classifies it as _",
    "touch_. due to lack of space , we do not present the full results .",
    "however , we note that our approach outperforms depth - based baseline in @xmath89 out of @xmath78 categories and achieves @xmath103 higher accuracy on average in comparison to this baseline .",
    "we also illustrate some of the qualitative results in fig .",
    "[ fig : po_preds ] .",
    "these results indicate that we can use our representation to successfully classify people s interactions with objects by sight or touch .",
    "in this paper , we introduced a new psychologically inspired approach to a novel 3d saliency detection problem in egocentric rgbd images . we demonstrated that using our psychologically inspired egoobject representation we can achieve good results for the three following tasks : 3d saliency detection , future saliency prediction , and interaction classification .",
    "these results suggest that an egocentric object prior exists and that using our representation , we can capture and exploit it for accurate 3d saliency detection on our egocentric rgbd saliency dataset ."
  ],
  "abstract_text": [
    "<S> on a minute - to - minute basis people undergo numerous fluid interactions with objects that barely register on a conscious level . </S>",
    "<S> recent neuroscientific research demonstrates that humans have a fixed size prior for salient objects . </S>",
    "<S> this suggests that a salient object in 3d undergoes a consistent transformation such that people s visual system perceives it with an approximately fixed size . </S>",
    "<S> this finding indicates that there exists a consistent egocentric object prior that can be characterized by shape , size , depth , and location in the first person view .    in this paper </S>",
    "<S> , we develop an egoobject representation , which encodes these characteristics by incorporating shape , location , size and depth features from an egocentric rgbd image . </S>",
    "<S> we empirically show that this representation can accurately characterize the egocentric object prior by testing it on an egocentric rgbd dataset for three tasks : the 3d saliency detection , future saliency prediction , and interaction classification . </S>",
    "<S> this representation is evaluated on our new egocentric rgbd saliency dataset that includes various activities such as cooking , dining , and shopping . by using our egoobject representation , </S>",
    "<S> we outperform previously proposed models for saliency detection ( relative @xmath0 improvement for 3d saliency detection task ) on our dataset . </S>",
    "<S> additionally , we demonstrate that this representation allows us to predict future salient objects based on the gaze cue and classify people s interactions with objects . </S>"
  ]
}