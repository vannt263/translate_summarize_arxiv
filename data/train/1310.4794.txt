{
  "article_text": [
    "a central task in machine learning is the prediction of unknown values based on ` learning ' from a given set of outcomes .",
    "more precisely , suppose @xmath0 is a non - empty set , the _ input space _ , and @xmath1 is a finite collection of input values @xmath2 together with their corresponding real outputs @xmath3 .",
    "the goal is to predict the value @xmath4 corresponding to a yet unobserved input value @xmath5 .",
    "the fundamental assumption of kernel - based methods such as support vector machines ( svm ) is that the predicted value is given by @xmath6 , where the _ decision _ or _ prediction function _",
    "@xmath7 belongs to a reproducing kernel hilbert space ( rkhs ) @xmath8 over @xmath0 , corresponding to a positive definite function @xmath9 ( see section [ ss : rkhs ] ) . in particular , in _",
    "ridge regression _ , the predictor function @xmath10 is the solution to the minimization problem : @xmath11 where @xmath12 is a _ regularization parameter _ and @xmath13 denotes the norm of @xmath14 in @xmath8 .",
    "the regularization term @xmath15 penalizes functions that ` overfit ' the training data .",
    "the minimization problem has a unique solution , given by a linear combination of the functions @xmath16 . specifically : @xmath17 where @xmath18 is given by : @xmath19 where @xmath20 is the matrix with entries @xmath21_{i j } = k(p_i , p_j)$ ] , @xmath22 is the identity matrix of size @xmath23 and @xmath24 is the vector of outputs . _",
    "we present a geometrical proof of this classic result in theorem _ [ t : svmminim ] .",
    "recently there has been a surge in interest in probabilistic interpretations of kernel - based learning methods ; see for instance @xcite . as we shall see below",
    ", there is a bayesian interpretation and a stochastic process approach to the ridge regression problem when the rkhs is finite - dimensional , but many rkhss used in practice are infinite - dimensional .",
    "the absence of lebesgue measure on infinite - dimensional hilbert spaces poses a roadblock to such interpretations in this case . in this paper :    * we show that there is a valid stochastic interpretation to the ridge regression problem in the infinite - dimensional case , by working within the framework of abstract wiener spaces and gaussian measures instead of working with the hilbert space alone . specifically , we show that the ridge regression solution may be obtained in terms of a conditional expectation and the gaussian radon transform . *",
    "we show that , within the more traditional spline setting , the element of @xmath8 of minimal norm that satisfies @xmath25 for @xmath26 can also be expressed in terms of the gaussian radon transform .",
    "* we propose a way to use the gaussian radon transform for a broader class of prediction problems .",
    "specifically , this method could potentially be used to not only predict a particular output @xmath27 of a future input @xmath28 , but to predict a function of future outputs ; for instance , one could be interested in predicting the maximum or minimum value over an interval of future outputs . * in this work we start with a rkhs @xmath8 and complete it with respect to a measurable norm to obtain a banach space @xmath29 . however , the space @xmath29 does not necessarily consist of functions . in light of recent interest in reproducing kernel banach spaces , which do consist of functions ,",
    "we propose a method to realize @xmath29 as a space of functions .",
    "let us first consider a bayesian perspective : suppose that our parameter space is an rkhs @xmath8 with reproducing kernel @xmath30 .",
    "if @xmath8 is finite - dimensional , we take standard gaussian measure on @xmath8 as our _ prior distribution _ : @xmath31 where @xmath32 is the dimension of @xmath8 .",
    "let @xmath33 denote , for every @xmath34 , the continuous linear functional @xmath35 on @xmath8 .",
    "with respect to standard gaussian measure , every @xmath33 is normally distributed with mean @xmath36 and variance @xmath37 , and cov@xmath38 .",
    "now recall that @xmath8 contains the functions @xmath39 on @xmath0 , and @xmath40 for all @xmath34 , @xmath5 .",
    "then @xmath41 is a centered gaussian process on @xmath8 with covariance function @xmath42 : @xmath43 for all @xmath44 .",
    "if we are given the training data @xmath45 and we assume the measurement contains some error we would like to model as gaussian noise , we choose an orthonormal set @xmath46 such that @xmath47 for every @xmath48",
    ". then @xmath49 are independent centered gaussian processes on @xmath8 , and for every @xmath34 we model our data as arising from the event : @xmath50 where @xmath12 is a fixed parameter .",
    "then for every @xmath34 , @xmath51 are independent gaussian random variables on @xmath8 with mean @xmath52 and variance @xmath53 for every @xmath26 , which gives rise to the statistical model of probability distributions on @xmath54 : @xmath55 for every @xmath56 . replacing @xmath57 with the vector @xmath58 of observed values ,",
    "the _ posterior distribution _ resulting from this is proportional to : @xmath59}.\\ ] ] therefore finding the _ maximum a posteriori _ ( map ) estimator in this situation amounts exactly to finding the solution to the ridge regression problem in .    clearly , this bayesian approach depends on @xmath8 being finite - dimensional ; however , reproducing kernel hilbert spaces used in practice are often infinite - dimensional ( such as those arising from gaussian rbf kernels ) .",
    "the ridge regression svm previously discussed goes through regardless of the dimensionality of the rkhs , and there is still a need for a valid stochastic interpretation of the infinite - dimensional case .",
    "we now explore another stochastic approach to ridge regression , which is equivalent to the bayesian one , but which can be carried over in a sense to the infinite - dimensional case , as we shall see later .",
    "suppose again that @xmath8 is a finite - dimensional rkhs over @xmath60 , with reproducing kernel @xmath42 , and equipped with standard gaussian measure .",
    "recall that @xmath61 is then a centered gaussian process on @xmath8 with covariance function @xmath42 .",
    "if we assume that the data arises from some unknown function in @xmath8 , then the relationship @xmath62 suggests that the random variable @xmath63 is a good model for the outputs .",
    "moreover , the training data @xmath64 provides some previous knowledge of the random variables @xmath65 , which we can use to refine our estimation of @xmath63 by taking conditional expectations . in other words",
    ", our first guess would be to estimate the output of a future input @xmath66 by : @xmath67.\\ ] ] but if we want to include some possible noise in the measurements , we would like to have a centered gaussian process @xmath68 on @xmath8 , with covariance function cov@xmath69 for some parameter @xmath12 , which is also independent of @xmath70 .",
    "let us fix @xmath66 , a ` future ' input whose output we would like to predict . to take measurement error into account",
    ", we choose again an orthonormal set @xmath71 such that : @xmath72^{\\perp},\\ ] ] and @xmath12 , and set : @xmath73 then we estimate the output @xmath74 as the conditional expectation : @xmath75.\\ ] ] as shown in lemma [ l : condexpgauss ] below : @xmath76 where @xmath77 is : @xmath78_{1\\leq j \\leq n},\\ ] ] with : @xmath79_{i , j } = [ k_d + \\lambda i_n]_{i , j},\\ ] ] for all @xmath80 , with @xmath81 being the @xmath82 matrix with entries given by @xmath83 .",
    "moreover : @xmath84 note that this last relationship is why we required that @xmath85 also be orthogonal to @xmath86 .",
    "this yields : @xmath87_j k_{p_j}(p),\\ ] ] showing that the prediction @xmath74 is precisely the ridge regression solution @xmath88 in .",
    "our goal in this paper is to show that the gaussian process approach does go through in infinite - dimensions and , moreover , that it remains equivalent to the svm solution .",
    "since the svm solution in is contained in a finite - dimensional subspace , the possibly infinite dimensionality of @xmath8 is less of a problem in this setting .",
    "however , if we want to predict based on a stochastic model for @xmath14 and @xmath8 is infinite - dimensional , the absence of lebesgue measure becomes a significant problem .",
    "in particular , we can not have the desired gaussian process @xmath89 on @xmath8 itself .",
    "the approach we propose is to work within the framework of abstract wiener spaces , introduced by l. gross in the celebrated work @xcite .",
    "the concept of abstract wiener space was born exactly from this need for a `` standard gaussian measure '' in infinite dimensions and has become a standard framework in infinite - dimensional analysis .",
    "we outline the basics of this theory in section [ ss : gmbs ] and showcase the essentials of the classical wiener space , as the `` original '' special case of an abstract wiener space , in section [ ss : cws ] .",
    "we construct a gaussian measure with the desired properties on a larger banach space @xmath29 that contains @xmath8 as a dense subspace ; the geometry of this measure is dictated by the inner - product on @xmath8 .",
    "this construction is presented in section [ ss : cons ] .",
    "we then show in section [ s : gml ] how the ridge regression learning problem outlined above can be understood in terms of the gaussian radon transform .",
    "the gaussian radon transform associates to a function @xmath14 on @xmath29 the function @xmath90 , defined on the set of closed affine subspaces of @xmath8 , whose value @xmath91 on a closed affine subspace @xmath92 is the integral @xmath93 , where @xmath94 is a gaussian measure on the closure @xmath95 of @xmath96 in @xmath29 obtained from the given gaussian measure on @xmath29 .",
    "this is explained in more detail in section [ s : backg ] . for more on the gaussian radon",
    "transform we refer to the works @xcite .",
    "another area that has recently seen strong activity is learning in banach spaces ; see for instance @xcite .",
    "of particular interest are reproducing kernel banach spaces , introduced in @xcite , which are special banach spaces whose elements are functions .",
    "the banach space @xmath29 we use in section [ s : gml ] is a completion of a reproducing kernel hilbert space , but does not directly consist of functions .",
    "a notable exception is the classical wiener space , where the banach space is @xmath97 $ ] . in section [ s : basfunct ]",
    "we address this issue and propose a realization of @xmath29 as a space of functions .    finally , in appendix [ s : geomform ] we present a geometric view .",
    "first we present a geometric proof of the representer theorem for the svm minimization problem and then describe the relationship with the gaussian radon transform in geometric terms .",
    "in this section we construct a gaussian measure on a banach space along with random variables defined on this space for which the covariance structure is specified in advance . in more detail , suppose @xmath98 is a non - empty set and @xmath99 a function that is symmetric and positive definite ( in the sense that the matrix @xmath100_{p , q\\in{\\mathcal x}}$ ] is symmetric and positive definite ) ; we will construct a measure @xmath101 on a certain banach space @xmath29 along with a family of gaussian random variables @xmath63 , with @xmath28 running over @xmath102 , such that @xmath103 for all @xmath104 . a well - known choice for @xmath105 , for @xmath106 ,",
    "is given by @xmath107 where @xmath108 is a scale parameter .",
    "the strategy is as follows : first we construct a hilbert space @xmath8 along with elements @xmath109 , for each @xmath110 , for which @xmath111 for all @xmath112 .",
    "next we describe how to obtain a banach space @xmath29 , equipped with a gaussian measure , along with random variables @xmath63 that have the required covariance structure .",
    "the first step is a standard result for reproducing kernel hilbert spaces .      for this we simply quote the well - known moore - aronszajn theorem ( see , for example , chapter 4 of steinwart @xcite ) .",
    "[ t : covhilb ] let @xmath98 be a non - empty set and @xmath113 a function for which the matrix @xmath100_{p , q\\in { \\mathcal x}}$ ] is symmetric and positive definite :    * @xmath114 for all @xmath112 , and * @xmath115 holds for all integers @xmath116 , all points @xmath117 , and all @xmath118 .",
    "then there is a unique hilbert space @xmath8 consisting of real - valued functions defined on the set @xmath98 , containing the functions @xmath119 for all @xmath110 , with the inner product on @xmath8 being such that @xmath120 moreover , the linear span of @xmath121 is dense in @xmath8 .",
    "the hilbert space @xmath8 is called the _ reproducing kernel hilbert space _ ( rkhs ) over @xmath0 with _ reproducing kernel _ @xmath42 .    for the mapping",
    "@xmath122 known as the _ canonical feature map _ , we have @xmath123 for all @xmath112 .",
    "hence if @xmath98 is a topological space and @xmath42 is continuous then so is the function @xmath124 given in ( [ e : phipq ] ) , and the value of this being @xmath36 when @xmath125 , it follows that @xmath126 is continuous . in particular , if @xmath98 has a countable dense subset then so does the image @xmath127 , and since this spans a dense subspace of @xmath8 it follows that @xmath8 is separable .      the theory of abstract wiener spaces developed by gross @xcite provides the machinery for constructing a gaussian measure on a banach space @xmath29 obtained by completing a given hilbert space @xmath8 using a special type of norm @xmath128 called a _",
    "measurable norm_. conversely , according to a fundamental result of gross , every centered non - degenerate gaussian measure on a real separable banach space arises in this way from completing an underlying hilbert space called the _ cameron - martin space_. we work here with real hilbert spaces as a complex structure plays no role in the gaussian measure in this context .",
    "a norm @xmath128 on a real separable hilbert space @xmath8 is said to be a",
    "_ measurable norm _",
    "provided that for any @xmath129 , there is a finite - dimensional subspace @xmath130 of @xmath8 such that : @xmath131 for every finite - dimensional subspace @xmath132 of @xmath8 with @xmath133 , where @xmath134 denotes standard gaussian measure on @xmath132 . figure [ fig : aws](a ) illustrates this notion .",
    "we denote the norm on @xmath8 arising from the inner - product @xmath135 by @xmath136 , which is not to be confused with a measurable norm @xmath128 . here",
    "are three facts about measurable norms ( for proofs see gross @xcite , kuo @xcite or eldredge @xcite ) :    1 .",
    "a measurable norm is always weaker than the original norm : there is @xmath137 such that : @xmath138 for all @xmath139 .",
    "if @xmath8 is infinite - dimensional , the original hilbert norm @xmath140 is not a measurable norm .",
    "if @xmath141 is an injective hilbert - schmidt operator on @xmath8 then @xmath142 specifies a measurable norm on @xmath8 .",
    "henceforth we denote by @xmath29 the banach space obtained by completion of @xmath8 with respect to @xmath128 .",
    "any element @xmath143 gives a linear functional @xmath144 that is continuous with respect to the norm @xmath140 .",
    "let @xmath145 be the subspace of @xmath8 consisting of all @xmath14 for which @xmath146 is continuous with respect to the norm @xmath147 .",
    "figure [ fig : aws](b ) illustrates the relationships between @xmath8 , @xmath29 , and @xmath148 .",
    "by fact ( 1 ) above , any linear functional on @xmath8 that is continuous with respect to @xmath128 is also continuous with respect to @xmath140 and hence is of the form @xmath149 for a unique @xmath143 by the traditional riesz theorem . thus @xmath148",
    "consists precisely of those @xmath143 for which the linear functional @xmath149 _ is the restriction to @xmath8 of a ( unique ) continuous linear functional on the banach space @xmath29_. we denote this extension of @xmath149 to an element of @xmath150 by @xmath151 : @xmath152 for all @xmath153 .",
    "moreover , @xmath150 consists exactly of the elements @xmath151 with @xmath154 : for any @xmath155 the restriction @xmath156 is in @xmath157 ( because of the relation ) and hence @xmath158 for a unique @xmath153 .",
    "the fundamental result of gross @xcite is that there is a unique borel measure @xmath101 on @xmath29 such that for every @xmath153 the linear functional @xmath159 , viewed as a random variable on @xmath160 , is gaussian with mean @xmath36 and variance @xmath161 ; thus : @xmath162 for all @xmath163 .",
    "the mapping @xmath164 is a linear isometry .    a triple @xmath165 where @xmath8 is a real separable hilbert space",
    ", @xmath29 is the banach space obtained by completing @xmath8 with respect to a measurable norm , and @xmath101 is the gaussian measure in , is called an _",
    "abstract wiener space_. the measure @xmath101 is known as _ wiener measure _ on @xmath29 .",
    "suppose @xmath166 is orthogonal to @xmath148 ; then for any @xmath155 we have @xmath167 where @xmath14 is the element of @xmath148 for which @xmath168 . since @xmath169 for all @xmath155 it follows that @xmath170",
    "thus , @xmath171 consequently @xmath172 extends uniquely to a linear isometry of @xmath8 into @xmath173 .",
    "we denote this extension again by @xmath172 : @xmath174 it follows by taking @xmath175-limits that the random variable @xmath151 is again gaussian , satisfying ( [ e : mub ] ) , for every @xmath143",
    ".    it will be important for our purposes to emphasize again that _ if @xmath143 is such that the linear functional @xmath149 on @xmath8 is continuous with respect to the norm @xmath128 then the random variable @xmath151 is the unique continuous linear functional on @xmath29 that agrees with @xmath14 on @xmath8 .",
    "_ if @xmath176 is _ not _ continuous on @xmath8 with respect to the norm @xmath128 then @xmath151 is obtained by @xmath173-approximating with elements of @xmath150 .",
    "conversely , one can show that any real separable banach space @xmath29 equipped with a centered , non - degenerate gaussian measure @xmath101 , can be placed in the context of an abstract wiener space .",
    "the corresponding hilbert space is the _ cameron - martin space _ of @xmath160 , the subspace @xmath177 given by : @xmath178 where for every @xmath179 : @xmath180 the norm @xmath140 defined above is a complete inner - product norm on @xmath8 , it is stronger than the banach norm @xmath128 on @xmath8 , and @xmath8 is dense in @xmath29 .",
    "moreover , the norm @xmath128 , restricted to @xmath8 , is a measurable norm - for an interesting proof of this fact , due to stroock , see section viii of driver s notes @xcite .",
    "so @xmath165 is an abstract wiener space and @xmath8 is uniquely determined by @xmath29 and @xmath101 .      as before ,",
    "we assume given a model covariance structure @xmath181 we have seen how this gives rise to a hilbert space @xmath8 , which is the closed linear span of the functions @xmath119 , with @xmath28 running over @xmath98 .",
    "now let @xmath128 be any measurable norm on @xmath8 , and let @xmath29 be the banach space obtained by completion of @xmath8 with respect to @xmath128 .",
    "let @xmath101 be the centered gaussian measure on @xmath29 as discussed above in the context of .",
    "we set @xmath182 for all @xmath5 ; thus @xmath183 . as a random variable on @xmath184",
    "the function @xmath185 is gaussian , with mean @xmath36 and variance @xmath186 , and the covariance structure is given by : @xmath187",
    "thus we have produced gaussian random variables @xmath63 , for each @xmath110 , on a banach space , with a given covariance structure .",
    "let us look at the case of the classical wiener space , as an example of the preceding structures .",
    "we take @xmath188 $ ] for some positive real @xmath189 , and @xmath190 } , 1_{[0,t ] } { { \\rangle}}_{l^2},\\ ] ] for all @xmath191 $ ] , where the superscript refers to brownian motion .",
    "then @xmath192 is @xmath57 for @xmath193 $ ] and is constant at @xmath194 when @xmath195 $ ] .",
    "it follows then that @xmath196 , the linear span of the functions @xmath197 for @xmath198 $ ] , is the set of all functions on @xmath199 $ ] with initial value @xmath36 and graph consisting of linear segments ; in other words , @xmath196 consists of all functions @xmath14 on @xmath199 $ ] whose derivative exists and is locally constant outside a finite set of points .",
    "the inner product on @xmath196 is determined by observing that @xmath200 we can verify that this coincides with @xmath201 for all @xmath202 . consequently , @xmath8 is the hilbert space consisting of functions @xmath14 on @xmath199 $ ] that can be expressed as @xmath203$,}\\ ] ] for some @xmath204 $ ] ; equivalently , @xmath8 consists of all absolutely continuous functions @xmath14 on @xmath199 $ ] , with @xmath205 , for which the derivative @xmath206 exists almost everywhere and is in @xmath207 $ ] .",
    "the sup norm @xmath208 is a measurable norm on @xmath8 ( see gross ( * ? ? ?",
    "* example 2 ) or kuo @xcite ) .",
    "the completion @xmath29 is then @xmath209 $ ] , the space of all continuous functions starting at @xmath36 . if @xmath210 then @xmath151 is gaussian of mean @xmath36 and variance @xmath37 , relative to the gaussian measure @xmath101 on @xmath29 .",
    "we can check readily that @xmath211 is gaussian of mean @xmath36 and variance @xmath212 for all @xmath213 $ ] . the process @xmath214 yields standard brownian motion , after a continuous version is chosen .",
    "the classical radon transform @xmath215 of a function @xmath216 associates to each hyperplane @xmath217 the integral of @xmath14 over @xmath218 ; this is useful in scanning technologies and image reconstruction .",
    "the gaussian radon transform generalizes this to infinite dimensions and works with gaussian measures ( instead of lebesgue measure , for which there is no useful infinite dimensional analog ) ; the motivation for studying this transform comes from the task of reconstructing a random variable from its conditional expectations .",
    "we have developed this transform in the setting of abstract wiener spaces in our work @xcite ( earlier works , in other frameworks , include @xcite ) .",
    "let @xmath8 be a real separable hilbert space , and @xmath29 the banach space obtained by completing @xmath8 with respect to a measurable norm @xmath128 .",
    "let @xmath219 be a closed subspace of @xmath8 .",
    "then @xmath220 is a closed affine subspace of @xmath8 , for any point @xmath221 . in (",
    "* theorem 2.1 ) we have constructed a probability measure @xmath222 on @xmath29 uniquely specified by its fourier transform @xmath223 where @xmath224 denotes orthogonal projection on @xmath219 in @xmath8 .",
    "for our present purposes we formulate the description of the measure @xmath222 in slightly different terms . for every closed affine subspace @xmath92",
    "there is a borel probability measure @xmath94 on @xmath29 and there is a linear mapping @xmath225 such that for every @xmath226 , the random variable @xmath227 satisfies @xmath228 where @xmath229 is the point on @xmath96 closest to the origin in @xmath8 , and @xmath230 is the orthogonal projection operator onto the closed subspace @xmath231 .",
    "moreover , @xmath232 is concentrated on the closure of @xmath96 in @xmath29 : @xmath233 where @xmath95 denotes the closure in @xmath29 of the affine subspace @xmath92 .",
    "( note that the mapping @xmath172 depends on the subspace @xmath96 . for more details about @xmath234 , see corollary 3.1.1 and observation ( v ) following theorem 2.1 in @xcite . )    from the condition ( [ e : grcharih ] ) , holding for all @xmath226 , we see that @xmath227 is gaussian with mean @xmath235 and variance @xmath236 .    the _ gaussian radon transform _",
    "@xmath90 for a borel function @xmath14 on @xmath29 is a function defined on the set of all closed affine subspaces @xmath96 in @xmath8 by : @xmath237 ( of course , @xmath90 is defined if the integral is finite for all such @xmath96 . )",
    "the value @xmath91 corresponds to the conditional expectation of @xmath14 , the conditioning being given by the closed affine subspace @xmath96 . for a precise formulation of this and proof for @xmath96 being of finite codimension see the disintegration formula given in ( * ? ? ?",
    "* theorem 3.1 ) .",
    "the following is an immediate consequence of ( * ? ?",
    "* corollary 3.2 ) and will play a role in section [ s : gml ] :    [ p : condexpgf ] let @xmath165 be an abstract wiener space and linearly independent elements @xmath238 of @xmath8 . let @xmath14 be a borel function on @xmath29 , square - integrable with respect to @xmath101 , and let @xmath239 \\right).\\ ] ] then @xmath240 is a version of the conditional expectation @xmath241.\\ ] ]",
    "consider again the learning problem outlined in the introduction : suppose @xmath0 is a separable topological space and @xmath8 is the rkhs over @xmath0 with reproducing kernel @xmath242 .",
    "let : @xmath1 be our training data and suppose we want to predict the value @xmath4 at a future value @xmath243 .",
    "note that @xmath8 is a real separable hilbert space so , as outlined in section [ ss : gmbs ] , we may complete @xmath8 with respect to a measurable norm @xmath128 to obtain the banach space @xmath29 and gaussian measure @xmath101 .",
    "moreover , recall from section [ ss : cons ] that we constructed for every @xmath244 the random variable @xmath245 where @xmath246 is the isometry in , and @xmath247 for all @xmath248 .",
    "therefore @xmath249 is a centered gaussian process with covariance @xmath42 .    since for every @xmath143 the value @xmath27 is given by @xmath250",
    ", we work with @xmath251 as our desired random - variable prediction .",
    "the first guess would therefore be to use : @xmath252\\ ] ] as our prediction of the output @xmath4 corresponding to the input @xmath28 .    using lemma [ l : condexpgauss ] the prediction in becomes : @xmath253(k_d)^{-1}y\\\\      & = & \\sum_{j=1}^n b_j k_{p_j}(p )      \\end{aligned}\\ ] ] where @xmath254 and @xmath81 is , as in the introduction , the matrix with entries @xmath255 . as we shall see in theorem [ t : splinemin ] , the function @xmath256 is the element @xmath257 of minimal norm such that @xmath258 for all @xmath259 . therefore is the solution in the traditional spline setting , and does not take into account the regularization parameter @xmath53 . combining this with proposition [ p : condexpgf ] ,",
    "we have :    [ t : splinegf ] let @xmath165 be an abstract wiener space , where @xmath8 be the real rkhs over a separable topological space @xmath60 with reproducing kernel @xmath42 .",
    "given : @xmath260 such that @xmath261 are linearly independent , the element @xmath257 of minimal norm that satisfies @xmath262 for all @xmath26 is given by : @xmath263 \\right),\\ ] ] where @xmath264 is the gaussian radon transform of @xmath63 for every @xmath265 .",
    "the next theorem shows that the ridge regression solution can also be obtained in terms of the gaussian radon transform , by taking the gaussian process approach outlined in the introduction .",
    "[ t : fhatascp ] let @xmath8 be the rkhs over a separable topological space @xmath0 with real - valued reproducing kernel @xmath42 and @xmath29 be the completion of @xmath8 with respect to a measurable norm @xmath128 with wiener measure @xmath101 .",
    "let @xmath266 be fixed and @xmath267 .",
    "let @xmath268 be an orthonormal set such that : @xmath269^{\\perp}\\ ] ] and for every @xmath270 let @xmath271 where @xmath272 is the isometry in .",
    "then for any @xmath12 : @xmath273 = \\hat{f}_{\\lambda}(p)\\ ] ] where @xmath10 is the solution to the ridge regression problem in with regularization parameter @xmath53 .",
    "consequently : @xmath274 \\right)\\ ] ] where @xmath264 is the gaussian radon transform of @xmath63 .    note that a completely precise statement of the relation ( [ e : thm1 ] ) is as follows .",
    "let us for the moment write the quantity @xmath275 , involving the vector @xmath4 , as @xmath276 then @xmath277 is a version of the conditional expectation @xmath278.\\ ] ]    by our assumption in : @xmath279 for all @xmath80 .",
    "we note that this covariance matrix is invertible ( see the argument preceding equation ( [ e : f0 tb ] ) below ) .",
    "moreover , @xmath280 for all @xmath270 .",
    "then by lemma [ l : condexpgauss ] : @xmath281 \\\\      & & = \\left [ k(p_1 , p ) , \\ldots , k(p_n , p ) \\right](k_d + \\lambda i_n)^{-1}y\\\\      & & = \\sum_{j=1}^n c_j k_{p_j}(p ) \\text { where } c = ( k_d + \\lambda i_n)^{-1}y\\\\      & & = \\hat{f}_{\\lambda}(p )      \\end{aligned}\\ ] ] finally , follows directly from proposition [ p : condexpgf ] .",
    "the interpretation of the predicted value in terms of the gaussian radon transform allows for quite a broad class of functions that can be considered for prediction . as a simple example , consider the task of predicting the maximum value of an unknown function over a future period using knowledge from the training data .",
    "the predicted value would be @xmath282 where @xmath96 is the closed affine subspace of the rkhs reflecting the training data , and @xmath132 is , for example , a function of the form @xmath283 for some given set @xmath284 of ` future dates ' .",
    "we note that the prediction ( [ e : gmpredictmax ] ) is , in general , _ different _ from @xmath285 where @xmath286 is the svm prediction as in ( [ e : thm2 ] ) ; in other words , the prediction ( [ e : gmpredictmax ] ) is not the same as simply taking the supremum over the predictions given by the svm minimizer .",
    "we note also that in this type of problem the hilbert space , being a function space , is necessarily infinite - dimensional",
    ".      one disadvantage of theorem [ t : fhatascp ] is that the choice of @xmath85 could change with every training set and every future input @xmath5 - specifically , given the training data , one must choose an orthonormal set @xmath46 that is not only orthogonal to each @xmath287 , but also to @xmath86 , for every future input @xmath28 whose outcome we would like to predict .",
    "clearly a set @xmath85 that would `` universally '' work can not be found in @xmath8 , because span@xmath288 is dense in @xmath8 .",
    "we would therefore like to attach to @xmath8 another copy of @xmath8 which would function as a `` repository '' of errors - so the two spaces would need to be in a sense orthogonal to each other .",
    "this is precisely the idea behind direct sums of hilbert spaces : if @xmath289 and @xmath290 are hilbert spaces , their orthogonal direct sum : @xmath291 is a hilbert space with inner - product : @xmath292 for all @xmath293 and @xmath294 .",
    "[ p : awsdirectsum ] let @xmath295 and @xmath296 be abstract wiener spaces , where @xmath289 and @xmath290 are real separable infinite - dimensional hillbert spaces and @xmath297 , @xmath298 are the banach spaces obtained by completing @xmath299 , @xmath300 with respect to measurable norms @xmath301 , @xmath302 , respectively",
    ". then : @xmath303 is an abstract wiener space , where @xmath304 is a separable banach space with the norm @xmath305 , for all @xmath306 , @xmath307 .",
    "it then easily follows that if @xmath308 is the isometry described in , then : @xmath309 where @xmath310 is the corresponding isometry for @xmath311 .",
    "going back to the ridge regression problem , let now @xmath165 be an abstract wiener space , where @xmath8 is the rkhs over a separable topological space @xmath0 with reproducing kernel @xmath42 and @xmath45 be our training data . recall that we would like an `` orthogonal repository '' for the errors - so",
    "let @xmath312 be an abstract wiener space , where @xmath313 is a real separable infinite - dimensional hilbert space .",
    "for every @xmath5 let @xmath314 and : @xmath315 where @xmath316 is the isometry in .",
    "as previously noted : @xmath317 where @xmath318 .",
    "let @xmath12 and @xmath319 be an orthonormal basis for @xmath313 and for every positive integer @xmath320 let : @xmath321 where @xmath322 . then : @xmath323 for all @xmath5 and @xmath324 . then by lemma",
    "[ l : condexpgauss ] and proposition [ p : condexpgf ] , for any @xmath5 : @xmath325\\\\      & = & g\\tilde{k}_p\\left ( \\bigcap_{j=1}^n [ { { \\langle}}(k_{p_j } , \\sqrt{\\lambda}e_j ) , \\cdot { { \\rangle}}= y_j ] \\right ) ,      \\end{aligned}\\ ] ] where @xmath10 is the ridge regression solution and both the conditional expectation and the gaussian radon transform above are with respect to @xmath326 . in this approach , for any number @xmath23 of training points and any future input @xmath5 we may simply work with @xmath327 , and we no longer have to choose this orthonormal set for every training set and every new input .",
    "we return to the proof of proposition [ p : awsdirectsum ] .",
    "every continuous linear functional on @xmath328 is of the form @xmath329 for some @xmath330 and @xmath331 , where : @xmath332 since @xmath301 and @xmath302 are weaker than @xmath333 and @xmath334 on @xmath299 and @xmath300 , respectively , there is @xmath137 such that @xmath335 for all @xmath336 and all @xmath337 .",
    "then : @xmath338 which shows that @xmath339 is a weaker norm than @xmath340 on @xmath341 .",
    "consequently , to every @xmath342 we may associate a unique @xmath343 such that @xmath344 for all @xmath345 , @xmath346 . as can be easily seen , this element is exactly @xmath347 , where @xmath348 for @xmath337 .",
    "the characteristic functional of the measure @xmath349 on @xmath328 is then : @xmath350 for all @xmath351 .",
    "therefore @xmath352 is a centered non - degenerate gaussian measure with covariance operator : @xmath353 for all @xmath354 , @xmath355 .",
    "define for @xmath356 : @xmath357 and the cameron - martin space @xmath8 of @xmath358 : @xmath359 for every @xmath360 and @xmath361 , @xmath362 so : @xmath363 which shows that @xmath364 .",
    "conversely , suppose @xmath365 .",
    "then by letting @xmath366 : @xmath367 so @xmath368 is in the cameron - martin space of @xmath369 - which is just @xmath299 .",
    "similarly , @xmath370 , proving that @xmath371 as sets . to see that the norms also correspond , note that for any @xmath372 and @xmath373 , not both @xmath36 : @xmath374 so : @xmath375 since @xmath376 is dense in both @xmath8 and @xmath341 , proves that @xmath377 for all @xmath378 , so @xmath8 and @xmath341 are the same as hilbert spaces .",
    "thus @xmath341 is the cameron - martin space of @xmath358 , which concludes our proof .",
    "in this section we present some results of a somewhat technical nature to address the question as to whether the elements of the banach space @xmath29 can be viewed as functions .      a general measurable norm on @xmath8 does not ` know ' about the kernel function @xmath42 and",
    "hence there seems to be no reason why the functionals @xmath379 on @xmath8 would be continuous with respect to the norm @xmath128 .",
    "to remedy this situation we prove that there exist measurable norms on @xmath8 relative to which the functionals @xmath380 are continuous for @xmath28 running along a dense sequence of points in @xmath98 :    [ p : kpcontmeas ] let @xmath8 be the reproducing kernel hilbert space associated to a continuous kernel function @xmath381 , where @xmath98 is a separable topological space .",
    "let @xmath382 be a countable dense subset of @xmath98 .",
    "then there is a measurable norm @xmath128 on @xmath8 with respect to which @xmath380 is a continuous linear functional for every @xmath383 .    . as noted in the context of ( [ e : phipq ] ) , the feature map @xmath384 is continuous .",
    "so if @xmath385 then the map @xmath386 is continuous and so if @xmath387 is a point for which @xmath388 then there is a neighborhood @xmath389 of @xmath28 such that @xmath390 for all @xmath391 . since @xmath382 is dense in @xmath98",
    "we conclude that there is a point @xmath392 for which @xmath393 . turning this argument into its contrapositive",
    ", we see that a vector orthogonal to @xmath394 for every @xmath392 is orthogonal to @xmath86 for all @xmath110 and hence is @xmath36 because the span of @xmath395 is dense in @xmath8 .",
    "thus @xmath396 spans a dense subspace of @xmath8 , where @xmath397 . by the gram - schmidt process",
    "we obtain an orthonormal basis @xmath398 of @xmath8 such that @xmath399 is contained in the span of @xmath400 , for every @xmath23 .",
    "now consider the bounded linear operator @xmath401 specified by requiring that @xmath402 for all @xmath23 ; this is hilbert - schmidt because @xmath403 and is clearly injective .",
    "hence , by property ( 3 ) discussed in the context of , latexmath:[\\[\\label{e : deffnormaf }    specifies a measurable norm on @xmath8 .",
    "then @xmath405 from which we see that the linear functional @xmath406 on @xmath8 is continuous with respect to the norm @xmath128 .",
    "hence , by definition of the linear isometry @xmath407 given in ( [ e : ifdef ] ) , @xmath408 is the element in @xmath150 that agrees with @xmath406 on @xmath8 .",
    "in particular each @xmath408 is continuous and hence @xmath409 is a continuous linear functional on @xmath29 for every @xmath383 .",
    "the measurable norm @xmath410 we have constructed in the preceding proof arises from a ( new ) inner - product on @xmath8 .",
    "however , given any other measurable norm @xmath411 on @xmath8 the sum @xmath412 is also a measurable norm ( not necessarily arising from an inner - product ) and the linear functional @xmath413 is continuous with respect to the norm @xmath414 for every @xmath383 .",
    "if a banach space @xmath29 is obtained by completing a hilbert space @xmath8 of functions , the elements of @xmath29 need not consist of functions",
    ". however , when @xmath8 is a reproducing kernel hilbert space as we have been discussing and under reasonable conditions on the reproducing kernel function @xmath42 it is true that elements of @xmath29 can ` almost ' be thought of as functions on @xmath98 . for this",
    "we first develop a lemma :    [ l : bb0 ] suppose @xmath8 is a separable real hilbert space and @xmath29 the banach space obtained by completing @xmath8 with respect to a measurable norm @xmath128 .",
    "let @xmath415 be a closed subspace of @xmath29 that is transverse to @xmath8 in the sense that @xmath416 , and let @xmath417 be the quotient banach space , with the standard quotient norm , given by @xmath418 then the mapping @xmath419 where @xmath420 is the inclusion , is a continuous linear injective map , and @xmath421 specifies a measurable norm on @xmath8 .",
    "the image of @xmath8 under @xmath422 is a dense subspace of @xmath297 .    .",
    "let us first note that by definition of the quotient norm @xmath423 hence @xmath424 let @xmath425 . then since @xmath128 is a measurable norm on @xmath8 there is a finite - dimensional subspace @xmath426 such that if @xmath132 is any finite - dimensional subspace of @xmath8 orthogonal to @xmath427 then , as noted back in , @xmath428 where @xmath134 is standard gaussian measure on @xmath132 .",
    "then @xmath429 where the first inequality holds because whenever @xmath430 we also have @xmath431 .",
    "thus , @xmath301 is a measurable norm on @xmath8 .",
    "the image @xmath432 is the same as the projection of the dense subspace @xmath433 onto the quotient space @xmath434 and hence this image is dense in @xmath297 ( an open set in the complement of @xmath432 would have inverse image in @xmath29 that is in the complement of @xmath8 , and would have to be empty because @xmath8 is dense in @xmath29 ) .",
    "we can now establish the identification of @xmath29 as a function space .",
    "[ p : basfunct ] let @xmath381 be a continuous function , symmetric and non - negative definite , where @xmath0 is a separable topological space , and @xmath382 a countable dense subset of @xmath98 .",
    "let @xmath8 be the corresponding reproducing kernel hilbert space .",
    "then there is a measurable norm @xmath301 on @xmath8 such that the banach space @xmath297 obtained by completing @xmath8 with respect to @xmath301 can be realized as a space of functions on the set @xmath382 .    .",
    "let @xmath29 be the completion of @xmath8 with respect to a measurable norm @xmath128 of the type given in proposition [ p : kpcontmeas ] .",
    "thus @xmath413 is continuous with respect to @xmath128 when @xmath383 ; let @xmath435 be the continuous linear extension of @xmath86 to the banach space @xmath29 , for @xmath383 . now let @xmath436 a closed subspace of @xmath29 .",
    "we observe that @xmath415 is transverse to @xmath8 ; for if @xmath437 is in @xmath8 then @xmath438 for all @xmath383 and so @xmath439 since @xmath440 spans a dense subspace of @xmath8 as noted in theorem [ t : covhilb ] and the remark following it .",
    "then by lemma [ l : bb0 ] , @xmath441 is a banach space that is a completion of @xmath8 in the sense that @xmath442 is continuous linear with dense image and @xmath443 , for @xmath226 , specifies a measurable norm on @xmath8 .",
    "let @xmath444 be the linear functional on @xmath297 induced by @xmath63 : @xmath445 which is well - defined because the linear functional @xmath63 is @xmath36 on @xmath415 .",
    "we note that @xmath446 for all @xmath226 , and so @xmath444 is the continuous linear ` extension ' of @xmath380 to @xmath297 through @xmath447 , viewed as an ` inclusion ' map .    now to each @xmath448 associate the function @xmath449 on @xmath382 given by : @xmath450 we will shows that the mapping @xmath451 is injective ; thus it realizes @xmath297 as a set of functions on the set @xmath382 . to this end , suppose that @xmath452 for some @xmath453 .",
    "this means @xmath454 and so @xmath455 for all @xmath456 .",
    "then @xmath457 thus @xmath458 and so @xmath459 .",
    "thus we have shown that @xmath460 is injective .",
    "we have defined the function @xmath461 on the set @xmath462 , with notation and hypotheses as above . now taking a general point @xmath110 and a sequence of points @xmath463",
    "converging to @xmath28 the function @xmath63 on @xmath29 is the @xmath464-limit the sequences of functions @xmath465 .",
    "thus we can define @xmath466 , with the understanding that for a given @xmath110 , the value @xmath467 is @xmath101-almost - surely defined in terms of its dependence on @xmath468 .",
    "in the theory of gaussian random fields one has conditions on the covariance function @xmath42 that ensure that @xmath469 is continuous in @xmath28 for @xmath101-a.e .",
    "@xmath470 , and in this case the function @xmath461 extends uniquely to a continuous function on @xmath98 , for @xmath101-almost - every @xmath470 .",
    "in this section we present a geometric view of the relationship between the gaussian radon transform and the representer theorem used in support vector machine theory ; thus , this will be a geometric interpretation of theorem [ t : fhatascp ] .    given a reproducing kernel hilbert space @xmath8 of functions defined on a set @xmath98 with reproducing kernel @xmath471",
    ", we wish to find a function @xmath472 that minimizes the functional @xmath473 ^ 2+{\\lambda}\\|f\\|^2,\\ ] ] where @xmath474 are given points in @xmath98 , @xmath475 are given values in @xmath476 , and @xmath477 is a parameter .",
    "our first goal in this section is to present a geometric proof of the following representer theorem widely used in support vector machine theory .",
    "the result has its roots in the work of kimeldorf and wahba @xcite ( for example , ( * ? ? ?",
    "* lemmas 2.1 and 2.2 ) ) in the context of splines ; in this context it is also worth noting the work of de boor and lynch @xcite where hilbert space methods were used to study splines .",
    "[ t : svmminim ] with notation and hypotheses as above , there is a unique @xmath472 such that @xmath478 is @xmath479 .",
    "moreover , @xmath10 is given explicitly by @xmath480 where the vector @xmath481 is @xmath482 , with @xmath81 being the @xmath82 matrix with entries @xmath21_{ij}=k(p_i , p_j)$ ] and @xmath483 .",
    "it will be convenient to scale the inner - product @xmath484 of @xmath8 by @xmath53 .",
    "consequently , we denote by @xmath485 the space @xmath8 with inner - product : @xmath486 we shall use the linear mapping @xmath487 that maps @xmath488 to @xmath489 for @xmath490 , where @xmath491 is the standard basis of @xmath492 : @xmath493 we observe then that for any @xmath494 @xmath495 for each @xmath496 , and so @xmath497 consequently , we can rewrite @xmath498 as @xmath499 and from this we see that @xmath498 has a geometric meaning as the distance from the point @xmath500 to the point @xmath501 in @xmath502 : @xmath503 thus the minimization problem for @xmath504 is equivalent to finding the point on the subspace @xmath505 closest to @xmath501 .",
    "now the subspace @xmath506 is just the graph @xmath507 and it is a closed subspace of @xmath508 because it is the orthogonal complement of a subspace ( as we see below in ) .",
    "hence by standard hilbert space theory there is indeed a unique point on @xmath506 that is closest to @xmath501 , and this point is in fact of the form @xmath509 where the vector @xmath510 is orthogonal to @xmath506 . now",
    "the condition for orthogonality to @xmath506 means that @xmath511 and this is equivalent to @xmath512 for all @xmath143 . therefore @xmath513 thus , @xmath514^\\perp=\\{(-tc , c)\\,:\\,c\\in{{\\mathbb r}}^n\\}.\\ ] ] conversely , we can check directly that @xmath515    [ fig ]            returning to ( [ e : abf0 ] ) we see that the point on @xmath506 closest to @xmath501 is of the form @xmath516 for some @xmath517 .",
    "since the second component is @xmath518 applied to the first , we have @xmath519 and solving for @xmath468 we obtain @xmath520 note that the operator @xmath521 on @xmath492 is invertible , since @xmath522 , so that if @xmath523 then @xmath524",
    ". then from we have @xmath525 given by @xmath526.\\ ] ] now we just need to write this in coordinates .",
    "the matrix for @xmath527 has entries @xmath528_{ij } , \\end{split}\\ ] ] and so @xmath529=t\\left[\\sum_{i , j=1}^n(i_n+\\lambda^{-1}k_d)^{-1}_{ij}y_je_i\\right].\\ ] ] since @xmath530 , we can write this as @xmath531\\lambda^{-1}k_{p_i},\\ ] ] which simplifies readily to ( [ e : f0wahba ] ) .",
    "the observations about the graph @xmath507 used in the preceding proof are in the spirit of the analysis of adjoints of operators carried out by von neumann @xcite .    with @xmath10 being the minimizer as above",
    ", we can calculate the minimum value of @xmath532 : @xmath533 it is useful to keep in mind that our definition of @xmath189 in ( [ e : deft ] ) , and hence of @xmath518 , depends on @xmath53 .",
    "we note that the norm squared of @xmath534 itself is @xmath535    let us now turn to the traditional spline setting .",
    "a function @xmath143 , whose graph passes through the training points @xmath536 , for @xmath490 , of minimum norm has to be found .",
    "we present here a geometrical description in the spirit of theorem [ t : svmminim ] .",
    "this is in fact the result one would obtain by formally taking @xmath537 in theorem [ t : svmminim ] .",
    "[ t : splinemin ] let @xmath8 be a reproducing kernel hilbert space of functions on a set @xmath538 , and let @xmath539 be points in @xmath540 .",
    "let @xmath381 be the reproducing kernel for @xmath8 , and let @xmath541 , for every @xmath542 .",
    "assume that the functions @xmath543 are linearly independent .",
    "then , for any @xmath483 , the element in @xmath544 of minimum norm is given by @xmath545 where @xmath546 , with @xmath81 being the @xmath82 matrix whose @xmath547-th entry is @xmath83 .",
    "the assumption of linear independence of the @xmath287 is simply to ensure that there does exist a function @xmath143 with values @xmath548 at the points @xmath549 .",
    "let @xmath550 be the linear mapping specified by @xmath551 , for @xmath490 .",
    "then the adjoint @xmath552 is given explicitly by @xmath553 and so @xmath554 since the linear functionals @xmath287 are linearly independent , no nontrivial linear combination of them is @xmath36 and so the only vector in @xmath492 orthogonal to the range of @xmath555 is @xmath36 ; thus @xmath556 let @xmath557 be the point on the closed affine subspace in that is nearest the origin in @xmath8 . then @xmath557 is the point on @xmath558 orthogonal to @xmath559 .",
    "now it is a standard observation that @xmath560^\\perp.\\ ] ] ( if @xmath561 then @xmath562 , so that @xmath563^\\perp$ ] ; conversely , if @xmath564^\\perp$ ] then @xmath565 for all @xmath566 and so @xmath567 . ) hence @xmath568 is the closure of @xmath569 .",
    "now @xmath569 is a finite - dimensional subspace of @xmath8 and hence is closed ; therefore @xmath570 returning to our point @xmath557 we conclude that @xmath571 .",
    "thus , @xmath572 for some @xmath166 .",
    "the requirement that @xmath573 be equal to @xmath4 means that @xmath574 , and so @xmath575 we observe here that @xmath576 is invertible because any @xmath577 satisfies @xmath578 , so that @xmath579 , and this is @xmath580 , again by the linear independence of the functions @xmath287 . the matrix for @xmath581 is just the matrix @xmath81 because its @xmath547-th entry is @xmath582 thus , using ( [ e : hatf0t0 t ] ) , @xmath557 works out to @xmath583 .    working in the setting of theorem [ t : splinemin ] , and assuming that @xmath8 is separable , let @xmath29 be the banach space obtained as completion of @xmath8 with respect to a measurable norm .",
    "recall from ( [ e : grcharih ] ) that the gaussian radon transform of a function @xmath132 on @xmath8 is the function on the set of closed affine subspaces of @xmath8 given by @xmath584 where @xmath96 is any closed affine subspace of @xmath8 , and @xmath94 is the borel measure on @xmath29 specified by the fourier transform @xmath585 wherein @xmath557 is the point on @xmath96 closest to the origin in @xmath8 and @xmath218 is the orthogonal projection onto the closed subspace @xmath586 .",
    "let us apply this to the closed affine subspace @xmath587 from equation ( [ e : ftmul ] ) we see that @xmath227 is a gaussian variable with mean @xmath588 and variance @xmath589 .",
    "now let us take for @xmath590 the function @xmath109 , for any point @xmath110 ; then @xmath591 is gaussian , with respect to the measure @xmath94 , with mean @xmath592 where @xmath185 is the random variable @xmath591 defined on @xmath593 .",
    "the function @xmath557 is as given in ( [ e : hatf0 ] ) .",
    "now consider the special case where @xmath594 from the training data .",
    "then @xmath595 because @xmath557 belongs to @xmath96 , by definition",
    ". moreover , the variance of @xmath596 is the norm - squared of the orthogonal projection of @xmath596 onto the closed subspace @xmath597 however , for any @xmath598 we have @xmath599 and so the variance of @xmath596 is @xmath36 .",
    "thus , with respect to the measure @xmath94 , the functions @xmath596 take the constant values @xmath548 almost everywhere .",
    "this is analogous to our result theorem [ t : fhatascp ] ; in the present context the conclusion is @xmath600.\\ ] ]",
    "the following result is a standard one , but we include a proof here for completeness .",
    "[ l : condexpgauss ] suppose that @xmath601 is a centered @xmath602-valued gaussian random variable on a probability space @xmath603 and let @xmath604 be the covariance matrix : @xmath605 and suppose that @xmath141 is invertible .",
    "then : @xmath606 = a_1z_1 + a_2z_2 + \\ldots + a_nz_n\\ ] ] where @xmath607 is given by @xmath608 where @xmath609 is given by @xmath610 for all @xmath611 .",
    "let @xmath612 be the orthogonal projection of @xmath613 on the linear span of @xmath614 ; thus @xmath615 is orthogonal to @xmath614 and , of course , @xmath616 is gaussian .",
    "hence , being all jointly gaussian , the random variable @xmath617 is independent of @xmath618 .",
    "then for any @xmath619 we have @xmath620 & = { { \\mathbb e}}[z_01_s]+{{\\mathbb e}}[y1_s]\\\\ & = { { \\mathbb e}}[z_01_s]+{{\\mathbb e}}[y]{{\\mathbb e}}[1_s]\\\\ & = { { \\mathbb e}}[z_01_s ] .",
    "\\end{split}\\ ] ] since this holds for all @xmath621 , and since the random variable @xmath612 , being a linear combination of @xmath614 , is @xmath622-measurable , we conclude that @xmath623.\\ ] ] thus the conditional expectation of @xmath613 is the orthogonal projection @xmath612 onto the _ linear span _ of the variables @xmath614 .",
    "writing @xmath624 we have @xmath625={{\\mathbb e}}[z_0z_j]=\\sum_{k=1}^n{{\\mathbb e}}[z_jz_k]a_k=(aa)_j,\\ ] ] noting that @xmath626 $ ] since all these variables have mean @xmath36 by hypothesis .",
    "hence we have @xmath627 .",
    "* acknowledgments*. this work is part of a research project covered by nsa grant h98230 - 13 - 1 - 0210 .",
    "i. holmes would like to express her gratitude to the louisiana state university graduate school for awarding her the lsu graduate school dissertation year fellowship , which made most of her contribution to this work possible .",
    "we thank kalyan b. sinha for useful discussions ."
  ],
  "abstract_text": [
    "<S> there has been growing recent interest in probabilistic interpretations of kernel - based methods as well as learning in banach spaces . </S>",
    "<S> the absence of a useful lebesgue measure on an infinite - dimensional reproducing kernel hilbert space is a serious obstacle for such stochastic models . </S>",
    "<S> we propose an estimation model for the ridge regression problem within the framework of abstract wiener spaces and show how the support vector machine solution to such problems can be interpreted in terms of the gaussian radon transform . </S>"
  ]
}