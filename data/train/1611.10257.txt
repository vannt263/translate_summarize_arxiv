{
  "article_text": [
    "the joint analysis of different types of big data should provide a better understanding of the molecular mechanisms underlying cancers @xcite , @xcite , @xcite , @xcite , @xcite and @xcite . in this work",
    "we introduce an approach that involves the use of matrix factorizations such as the singular value decomposition ( svd ) to simultaneously analyze a finite number of matrices by iteratively identifying cross - correlations between them . the iterative singular value decomposition ( isvd )",
    "jointly analyzes a finite number of data matrices by automatically identifying any correlations that may exist among the rows of matrices , or the rows and columns of any one of the matrices . since big data sets contain many distinct signals that associate the measured variables and samples , repeated application of the isvd on re - normalized data iteratively detects these signals as orthogonal correlations among the matrices or within a single matrix",
    "this approach is computationally efficient .",
    "the following notation is used in this work .",
    "* notation : *    * @xmath0 the linear space of real column vectors with @xmath1 coordinates . * @xmath2 the linear space of real @xmath3 matrices . *",
    "@xmath4 the subset of @xmath5 real orthonormal matrices . * @xmath6 the subset of @xmath3 real non - negative diagonal matrices . * @xmath7 the subset of @xmath3 real matrices whose @xmath8 columns are normalized .",
    "the singular value decomposition of a matrix @xmath9 is the decomposition of @xmath10 into the product of three matrices of the form @xmath11 where @xmath12 , @xmath13 , and @xmath14 .",
    "the diagonal entries of @xmath15 are called the singular values of @xmath10 , and columns of @xmath16 and @xmath17 are called left and right singular vectors of @xmath10 respectively .",
    "the left - singular vectors are eigenvectors of @xmath18 and right - singular vectors are eigenvectors of @xmath19 .",
    "non - zero singular values of @xmath10 are same as the square roots of non - zero eigenvalues of @xmath18 ( which are the same as the non - zero eigenvalues of @xmath19 ) . based on the svd , the matrix @xmath10 can be written as an outer product of orthogonal rank one approximations @xmath20 where @xmath21 and @xmath22 are the @xmath23th columns of @xmath16 and @xmath17 respectively ; @xmath24 is the @xmath23th diagonal element of @xmath15 .      given a set of @xmath25 matrices @xmath26 with full column rank .",
    "the svd of stacked matrix @xmath27 will be given by @xmath28 the @xmath29th column of @xmath30 can be written as @xmath31 where @xmath32 .",
    "if @xmath33 represent the @xmath29th ( @xmath34 ) columns of @xmath35 , then @xmath36 can be viewed as correlation detectors of the signal @xmath33 that is common to @xmath37 .",
    "that is , we can detect the rows of @xmath37 that are mutually correlated with the common signal @xmath33 by applying thresholds to the vectors @xmath36 .",
    "note that the above procedure will result in @xmath8 distinct signals that link the rows of @xmath37 via the common signals @xmath33 .",
    "the analysis of the rows of @xmath37 will be based on @xmath35 .",
    "specifically , given the rank one approximation to the data we form the residual data and apply rank one approximation to the residual data pair .",
    "this procedure is iterated until the largest singular value of residue falls below a certain threshold that is near zero .",
    "this algorithm is based on the svd of the stacked matrix @xmath27 .",
    "define @xmath38 ( @xmath39 ) to be the @xmath23th column of matrix @xmath35 ( @xmath30 ) and @xmath24 be the @xmath23th diagonal entry of matrix @xmath15 in ( [ eq : svd ] ) .",
    "the first column of @xmath30 can be written as @xmath40 where @xmath41 .",
    "the rank one approximation to svd of @xmath27 is given as @xmath42 @xmath43 @xmath44 @xmath45 with @xmath46 @xmath47 we are interested to find the support of signals in @xmath37 .",
    "the vectors @xmath48 in the rank one approximation of @xmath37 can be viewed as detectors of the signal in @xmath37 , respectively .",
    "that is , the components of @xmath49 with large absolute magnitude correspond to the rows in @xmath37 that are highly correlated with @xmath50 .",
    "note that @xmath51 is common in the matrix factorization of @xmath37 .",
    "the indices of @xmath49 with large absolute magnitude are the signal support for @xmath37 respectively .",
    "we apply the universal threshold proposed by donoho and johnstone ( @xcite and @xcite ) to detect components of @xmath49 with large absolute magnitude .",
    "now we subtract @xmath52 from @xmath37 respectively and get the rank one approximation to svd of the pair @xmath53 .",
    "it is shown in theorem  @xmath54 that @xmath55 the rank one approximation to svd of @xmath53 is given as @xmath56 @xmath57 @xmath44 @xmath58 with @xmath59 @xmath60 @xmath61 similarly to the first iteration , the signals in @xmath62 are detected .",
    "the sequential application of the isvd is continued until the maximum singular value of @xmath63 drops below a predefined threshold indicating that the noise floor of the data has been reached .",
    "the number of iterations is at most rank of @xmath27 .",
    "therefore , the isvd algorithm is computationally efficient and enhances signal detection by rank one approximation . to get the largest singular value and singular vector of @xmath10 we can use the matlab command @xmath64 . for further information about the algorithm of finding a few eigenvalue of a large matrix",
    "please refer to @xcite .",
    "* theorem 1 : *    let the svd of the stack matrix be @xmath65 then , splitting apart the first column of @xmath30 , we can write @xmath66 satisfying @xmath67 let @xmath68 which latter equality holds since @xmath69 the claim is that    * the nonzero eigenvalues of @xmath70 are @xmath71 , * the nonzero eigenvalues of @xmath72 are @xmath73 , * with the corresponding eigenvalues @xmath74 ,    so that the above representation of @xmath75 is its singular value decomposition .",
    "* proof : * the proof is in the calculation @xmath76 and similarly @xmath77 , etc .    the algorithm proceeds to find @xmath78 satisfying @xmath79    * theorem 2 : * let @xmath80 be an @xmath81 unit vector and @xmath82 be a set of orthonormal vectors .",
    "define two matrices @xmath83 ( @xmath84 ) and @xmath85 ( @xmath86 ) as follows @xmath87 and @xmath88 where the support of @xmath89 and @xmath90 in @xmath10 are @xmath91 and @xmath92 respectively and the support of @xmath89 in @xmath85 is @xmath93 .",
    "define @xmath75 as the stack of two matrices @xmath83 and @xmath85 @xmath94 it is claimed that    * @xmath95 and @xmath92 are the eigenvalues of @xmath96 corresponding to eigenvector @xmath89 and @xmath90 respectively . * if @xmath97 then @xmath98 and @xmath99 are the eigenvectors of @xmath96 corresponding to eigenvalue @xmath97 .    proof : @xmath100 @xmath101 by multiplying @xmath89 to both sides of ( [ eq1 ] ) @xmath102 similarly for @xmath90 .",
    "suppose @xmath97 , by multiplying @xmath89 to both sides of ( [ eq1 ] ) @xmath103 @xmath104 similarly for @xmath99 .",
    "* theorem 3 : * let @xmath105 be an @xmath81 unit vector and @xmath106 be a set of i.i.d .",
    "white noise vectors , in which @xmath107 is a @xmath81 vector .",
    "define the matrices @xmath10 ( @xmath108 ) as follows @xmath109 it is claimed that    * @xmath1 is the approximate eigenvalues of @xmath110 corresponding to eigenvector @xmath105 for a non - trivial range of signal to noise ratio .",
    "proof :    [ eq : eq1111 ] a^ta & =    s+n_1 & & s+n_m & n_m+1 & & n_p    s^t+n_1^t +   + s^t+n_m^t + n_m+1^t +   + n_p^t     + & =  ss^t+n_1s^t+sn_1^t+n_1n_1^t + &   + &  + ss^t+n_ms^t+sn_m^t+n_mn_m^t + &  + n_m+1n_m+1^t++n_pn_p^t    by multiplying @xmath111 from right hand side to both sides of ( [ eq : eq1111 ] )    [ eq : eq2222 ] a^ta = s+    it is easy to see that if    s    or    [ eq : eq22221 ]    then the right hand side of ( [ eq : eq2222 ] ) can be approximated by @xmath105 .",
    "it is noticeable that if @xmath112 , then with a high probability the inequality in ( [ eq : eq22221 ] ) is satisfied . below the convergence behaviour of @xmath113 as @xmath1 grows",
    "is studied .      by taking limit from both sides of ( [ eq : eq2222 ] ) we have    [ eq : eq222222 ] a^ta = s+=s    by law of large number we have    [ eq : eq3333 ] pr(=0)=1    from ( [ eq : eq222222 ] ) and ( [ eq : eq3333 ] ) we can have almost surely    [ eq : eq4444 ] a^ta = s    therefore almost surely for any given @xmath114 there exists an @xmath115 such that for all @xmath116    [ eq : eq5555 ] |a^ta - s| <    and almost surely    [ eq : eq6666 ] m(s- ) < a^tas <",
    "m(s+ ) .    from chebychev s inequality    [ eq : eq6666 ] p(|_i=1^mn_i|",
    "> ) =    let @xmath117 be the weakest signal to noise ratio that thresholding strategy works , @xmath118 .",
    "[ eq : eq6666 ] p(|_i=1^mn_i| > ) = =    what is the critical @xmath117 ?",
    "critical @xmath117 can be achieved from roc curve and donoho johnson threshold . for critical @xmath117",
    "we can find an upper bound on probability of error for any value of error .",
    "the maximum probability of error is a function of signal to noise ratio , because the critical @xmath117 is a function of signal to noise ratio and @xmath1 is the power of signal .",
    "the isvd algorithm is able to detect signals that are common or unique in finite number of data sets .",
    "signals are detected sequentially based on signal strength .",
    "very weak signals with small singular values are detectable since noise background is systematically reduced relative to signal strength . note that the isvd is computationally feasible in situations where the gsvd is not",
    ". it may also be theoretically more meaningful than the generalized singular value decomposition ( gsvd ) @xcite , @xcite , @xcite and @xcite .      a zeinalzadeh , t wenska , g okimoto , integrated analysis of multiple high - dimensional data sets by joint rank-1 matrix approximations , 54th ieee conference on decision and control ( cdc ) , pp . 38523857 , 2015 .",
    "g okimoto , a zeinalzadeh , t wenska , m loomis and etc . ,",
    "joint analysis of multiple high - dimensional data types using sparse matrix approximations of rank-1 with applications to ovarian and liver cancer , biodata mining 9 ( 1 ) , 24 , 2016 .",
    "s. p. ponnapalli , m. a. saunders , c. f. van loan , orly alter , a higher - order generalized singular value decomposition for comparison of global mrna expression from multiple organisms , science , 6(12 ) , 2011 ."
  ],
  "abstract_text": [
    "<S> we develop an iterative version of the singular value decomposition ( isvd ) that jointly analyzes a finite number of data matrices to identify signals that correlate among the rows of matrices . </S>",
    "<S> it will be illustrated how the supervised analysis of a big data set by another complex , multi - dimensional phenotype using the isvd algorithm could lead to signal detection . </S>"
  ]
}