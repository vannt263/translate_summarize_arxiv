{
  "article_text": [
    "quantum walks ( qws ) are a generalization of classical random walks to the quantum regime .",
    "they were first introduced in the discrete - time version  @xcite and later as continuous - time quantum walks ( ctwqs ) in the context of quantum computation and decision trees  @xcite . in this framework",
    ", it has been shown that single - particle quantum walk - based algorithms may outperform the classical counterpart in terms of traveling time through a graph . since then , qws , both in the continuous- and discrete - time versions , have been the subject of extensive studies . besides , qws have been generalized to many - particles quantum walks , where the time evolution of the walkers depends upon their statistics , indistinguishability and kind of interaction  @xcite .",
    "ctqws on more complex structures , e.g. complex graphs , have been also the focus of more recent analysis  @xcite .",
    "overall , ctqws have been proved a useful tool in a variety of contexts , ranging from transport through a graph  @xcite , to quantum search algorithms  @xcite , graph isomorphism testing  @xcite and universal quantum computation  @xcite .    in realistic experimental scenarios , imperfections in the fabrication of the lattice",
    "may induce anderson localization of the walkers  @xcite , whereas stochastic fluctuations of the environment may come into play destroying the quantumness of the system and , in turn , its peculiar propagation features  @xcite .",
    "a more realistic description for noisy quantum walks should therefore take into account the noise that may affect the evolution of the walkers .",
    "a convenient way to describe noise is to introduce suitable stochastic terms in the hamiltonian , in order to model static or dynamical fluctuations that may affect both the on - site energies or the tunneling amplitudes of the walkers  @xcite .",
    "the dynamical evolution of the qw is then obtained as the ensemble average over all possible realizations of the stochastic processes mimicking the noise . in practice ,",
    "the ensemble average is computed numerically as an average over a finite number of realizations : the larger the number of the realizations , the more accurate the simulation of the ctqw .",
    "evaluating the dynamics of a many - particle state over a noisy lattice requires the numerical solution of a set of differential equations which include stochastic terms  @xcite . the total number of equations to solve grows rapidly as long as the numbers of nodes , particles , and realizations increase , thus making the problem more and more computationally demanding with longer execution times .",
    "in fact , codes for simulating many - particle ctqws have been developed for high - performance clusters with distributed memory  @xcite .",
    "on the other hand , the evolution of computer architectures towards multicore processors even in stand - alone workstations enabled important cuts of the execution time by introducing the possibility of running multiple threads in parallel and spreading the workload among cores .",
    "this possibility was boosted up by the general purpose parallel computing architectures of modern graphic cards ( gpgpus ) . in the latter , hundreds or thousands of computational cores in the same single chip",
    "are able to process simultaneously a very large number of data .",
    "it should also be noted that an impressive computational power is present not only in dedicated gpus for high - performance computing , but also in commodity graphic cards , which make modern workstations suitable for numerical analyses . in order to exploit such a huge computational power ,",
    "algorithms must be first redesigned and adapted to the simt ( single instruction multiple thread ) and simd ( single instruction multiple data ) paradigms and translated then into programming languages with hardware - specific subsets of instructions . among them , one of the most diffuse is cuda - c , a c extension for the compute unified device architecture ( cuda ) that represents the core component of nvidia gpus . as a matter of fact , the use of gpus for scientific analysis , which dates back to mid and late 2000 s  @xcite , dramatically boosted with a two - digit yearly increasing rate since 2010 .",
    "just looking at the computational physics realm , several gpu - specific algorithms have been proposed in the last three years , e.g. for stochastic differential equations  @xcite , molecular dynamics simulations  @xcite , fluid dynamics @xcite , metropolis monte carlo  @xcite simulations , quantum monte carlo simulations  @xcite , and free - energy calculations  @xcite .",
    "the evolution of many particles qws in a noisy environment can be classified as an _ embarassing parallel _ problem , since there is little to none communication among realizations .",
    "problems of this kind take great advantage of gpgpu computing , since the solving algorithms can be designed to run directly on the gpu in such a way that communications are implemented via shared memory on the device ( graphic card ) and data transfer between the host ( cpu ) and the device and v.v .",
    "is limited to unavoidable input / output operations .    in this paper , we have compared parallel algorithms for ctqws evolution in a noisy environment based on the exact diagonalization of the hamiltonian , the 4-th order runge - kutta integration method and the taylor - series expansion of the evolution operator .",
    "solutions that avoid the diagonalization of the hamiltionian result in a lower computational cost and pave the way to highly parallelizable algorithms within the simt paradigm , thus leading to a straightforward implementation directly on the gpu .",
    "we have then benchmarked 4 nvidia gpus and 3 quad - core intel cpus for a 2-particle system over a lattice of increasing dimensions and have shown that the gpu speedup with respect to the openmp parallelization fluctuates from 8x to more than 20x , depending on the frequency of post - processing . thus",
    ", gpu - accelerated codes allow the design of simulations involving many particles or large lattices , with the only limit of the memory available on the device .",
    "the paper is organized as follows : in sect .",
    "[ algo ] we discuss and derive efficient algorithms for the dynamics of ctqws ; in sect .  [ impl ] we provide the main details on their implementation and in sect .",
    "[ results ] we compare the performances of the algorithms on different cpus and gpus .",
    "[ conc ] closes the paper with some concluding remarks .",
    "let us consider a @xmath0-dimensional regular lattice hosting @xmath1 quantum particles , and let @xmath2 and @xmath3 be the numbers of mesh elements ( nodes ) and of neighbors to be considered along the @xmath4-th direction .",
    "the system in hand is described by an @xmath5 matrix , storing the elements of the hamiltonian @xmath6 and by a @xmath7 vector for the wave - function @xmath8 , where @xmath9 is the total number of mesh nodes . when @xmath10 , the hamiltonian @xmath6 is largely sparse with a maximum filling factor @xmath11 .",
    "since we are interested in quantum walks in a noisy environment , transitions from node @xmath12 and to node @xmath13 are ruled by @xmath14 deterministic ( @xmath15 ) and stochastic ( @xmath16 ) parameters .",
    "the stochastic terms @xmath16 switch between multiple values at random times during the simulation ( switching times ) in order to describe ( generally time - dependent ) fluctuations induced by lattice imperfections and/or external sources of noise .",
    "thus , the elements of the hamiltonian @xmath17 read @xmath18 the terms @xmath19 quantify the on - site energies of the walkers , whereas the terms @xmath17 with @xmath20 describe the tunneling amplitudes between neighboring sites ( we assume that tunneling occurs only between neighboring nodes ) .",
    "the time evolution of the system is provided by the schrdinger equation @xmath21 where @xmath22 is the reduced planck constant ; the knowledge of @xmath23 at each time step yields the @xmath5 density matrix @xmath24 , which is used to evaluate the average over realizations @xmath25 and eventually further post - processed to calculate any desired observable quantity .    consequently to the introduction of random terms , in order to avoid overweighting of outliers and produce a reliable ensemble average it is required to run a sufficiently large number @xmath26 of simulations ( a.k.a .",
    "realizations , usually @xmath27 ) , and then averaging the density matrix . in order to speed - up the calculation , and significantly cut the execution time , realizations can be run in parallel , as they are independent from each other . however , in the parallel execution memory usage rapidly increases because at least an hamiltonian matrix @xmath28 and a wave - function @xmath29 must be stored for each realization @xmath4 . as a matter of fact , memory occupancy may become quickly an issue when the grid size and/or the number of particles increase .      if we suppose that the hamiltonians @xmath28 do not change significantly within the time - step @xmath30 , eq .",
    "can be solved in the quasi - static approximation .",
    "exact _ time evolution of a qw is provided by the well - known eigenproblem @xmath31 that yields the eigenvalues @xmath32 and the eigenvectors @xmath33 of the @xmath4-th hamiltonian .",
    "the evolution of the wave function is then given by @xmath34 the pseudocode for the parallel implementation is given in algorithm [ algeigen ] .",
    "initialize hamiltonians @xmath28 initialize switching times diagonalize @xmath35 @xmath36 update switching times @xmath37 @xmath38 @xmath39 post - process @xmath40    [ algeigen ]    it is worth noticing that a ) this algorithm requires a large number of computationally intensive events of the order @xmath41 and b ) it is necessary to store @xmath7 eigenvectors of @xmath7 components per realization , which is exactly the same memory space that the dense hamiltonian matrix would occupy . as a matter of fact",
    ", this issue may jeopardize the efficiency of the code , even in the case of a parallel implementation .",
    "going back to the general solution of eq .  , we may directly tackle the time - dependent schrdinger equation as a set of ordinary differential equations for the vector @xmath42 and solve it by means of standard integration techniques that dispose of the calculation of the eigenstates .",
    "a widely - used integration scheme is represented by the 4th - order runge - kutta method .    in this case",
    ", there is no need of allocating a memory space as large as a dense hamiltonian would require .",
    "the hamiltonian topology , i.e. , how nodes are connected to each other , is known _ a - priori _ from the definition of the mesh , and holds true for all of the realizations . in principle , up to @xmath43 non - null elements are present in each row of the hamiltonian . as a consequence , each of the @xmath5 hamiltonians @xmath28",
    "can be stored as a @xmath44 reduced matrix @xmath45 . a common @xmath44 topology matrix holding the indexes of non - null elements also adds . since transitions from node @xmath12 to node @xmath13 and v.v .",
    "share the same rate , the symmetry of @xmath45 allows for further memory savings down to @xmath46 elements .",
    "these relationships hold true for a regular lattice ; in the case of a general graph , where each site is connected to a variable number of other nodes , the approach is still applicable with the only difference that the number @xmath47 of non - null elements per row in the topology matrix is replaced by the number of connections .",
    "the 4th - order runge - kutta procedure lets the wave - functions @xmath42 evolve by means of the linear combination of 4 intermediate states @xmath48 .",
    "the evaluation of any component belonging to the @xmath49-th intermediate state requires only the knowledge of the wave - function at the current time step , the reduced hamiltonian and the @xmath50-th state at the indexes stored in the corresponding row of the topology matrix .",
    "since nodes are topologically equivalent to each other , simd and simt paradigms apply , allowing for a second degree of parallelization over nodes .",
    "the parallelizations over realizations and over nodes can be collapsed into a larger loop ( @xmath51 steps ) , which may better balance the computational burden assigned to each computing unit .",
    "the pseudocode for the implementation of the 4th - order runge - kutta method is reported in algorithm [ algrk4 ] .",
    "define hamiltonian topology initialize reduced hamiltonians @xmath45 initialize switching times @xmath52 @xmath53 check norm of @xmath54 update switching times @xmath55",
    "@xmath38 @xmath39 post - process @xmath40    [ algrk4 ]    the scheme in algorithm [ algrk4 ] requires a single loop of sums and products ; the algorithmic complexity of time evolution is thus reduced to the order @xmath56 , with a large speedup compared to the case discussed in sect .",
    "the most computationally intensive routine is now represented by the calculation of the average density - matrix , order @xmath57 , whose number of calls may vary depending on the desired precision of the output .",
    "the 4-th order runge - kutta method does not conserve the norm , and intermediate checks and corrective actions are required to avoid unphysical outcomes .",
    "it may also happen that the norm of @xmath58 strongly deviates from its theoretical value within a single time step . in order to fix this issue",
    "two strategies may be devised . in the first one , higher - order runge - kutta methods",
    "can similarly be implemented to reach a better accuracy within the same time step , but memory allocation would grow since a larger number of intermediate states are required . on the other hand , one could reduce the time step in such a way that the cumulative error does not drive the simulation far away from its correct path .",
    "the immediate shortcoming is the increase of the running time inversely proportional to the step reduction ; nonetheless , this solution becomes mandatory if it is not possible nor convenient to increase the memory allocation .",
    "algorithm [ algrk4 ] may be modified in order to make the memory allocation independent of the required precision and slightly reduced with respect to the runge - kutta integration method .",
    "upon introducing the evolution operator @xmath59 , such that @xmath60 we may rewrite eq .   in terms of @xmath59 instead of @xmath58 ,",
    "i.e. @xmath61 the formal solution is given by @xmath62 upon expanding @xmath59 in taylor series we have @xmath63 the wave - function can be recast as @xmath64 where @xmath65 and @xmath66    the pseudocode for the evolution of the wave - functions by means of the expansion of the evolution operator in taylor series is shown in algorithm [ algseries ] . in order to understand the similarities and the difference between the two methods , let us remind that the coefficients @xmath67 in the runge - kutta expansion are , in general , determined by an educated fitting of a formal taylor series expansion of the unknown functions in such a way that the truncation error is the same .",
    "due to the exponential form of the evolution operator , there is also a perfect coincidence between the @xmath68-th order taylor series expansion and the @xmath68-th order runge - kutta method  @xcite .",
    "the advantage of the taylor expansion is represented by the progressive updating of @xmath58 with the help of the auxiliary vector @xmath69 , which is overwritten at each step of the expansion loop .",
    "thus , the memory allocation of auxiliary variables does not depend any more on the precision of the calculation , without increasing the algorithmic complexity .",
    "notice that at the same time , all the arguments discussed in sect .",
    "[ rk ] about the heaviest routines ( and the influence of the time step on the results ) still hold true .",
    "define hamiltonian topology initialize reduced hamiltonians @xmath45 initialize switching times @xmath70 @xmath71 @xmath72 @xmath73 check norm of @xmath54 update switching times @xmath55 @xmath38 @xmath39 post - process @xmath40    [ algseries ]",
    "algorithms [ algeigen]-[algseries ] have been implemented to run on multicore shared - memory workstations and graphic accelerators , making use for linear algebra of the blas and lapack or the cublas and cula  @xcite libraries on the host system and on the device , respectively .",
    "we have not tackled any advanced memory optimization : as it will be discussed in sect .",
    "[ results ] , benefits brought in by a highly - optimized code are not expected to further increase the performance gain significantly .    as far as algorithm [ algeigen ] is concerned",
    ", we envisage two workflows for parallel execution . on the one hand , would memory not be an issue , one can split realizations among non - communicating cores in such a way that , even though the single realization is serialized , a number of realizations are handled at the same time . on the other hand , it may be convenient to serialize realizations and decrease the single - realization running time by spreading the matrix diagonalizations and the matrix - matrix products on multiple cooperating cores . in principle",
    ", the latter solution can be pushed farther if a large number of computing cores are available to the programmer , as it is the case of gpus .",
    "the execution times required by the diagonalization of symmetric matrices with single precision data ( ` ssyev ` function of the intel mkl 11.2 library ) have preliminarily been measured for 3 intel processors , then the outcomes have been projected over @xmath74 calls , which is the typical number of diagonalizations required for the problem in hand . as shown in [ fig : tempodiag ] , a simulation may last for years , which is a virtually infinite time for a computational physics problem . according to cula white - papers @xcite , the corresponding routine ported to gpus",
    "may achieve a speedup ranging from 3x to 10x , a condition that still prevents any investigation from completing within an affordable time .",
    "times a symmetric matrix in single precision with the mkl 11.2 library .",
    "the tests have been performed on 4 cpu cores , as this configuration preliminarily proved to maximize the overall performance . ]",
    "algorithms [ algrk4 ] and [ algseries ] have been implemented by means of 15 kernels directly on the gpu , then the corresponding openmp versions have been derived by replacing kernel invocations with loops .",
    "this approach allows for a direct execution time comparison since the number of floating - point operations is basically the same between host and device execution .",
    "the two algorithms share the same 4-stage workflow ( _ 1 .",
    "initialization ; 2 .",
    "wave - function evolution ; 3 .",
    "hamiltonian update ; 4 .",
    "density - matrix calculation and post - processing _ ) and approximately 90% of the code .",
    "contrarily to algorithm [ algeigen ] , where the limiting factor is primarily represented by time , the limiting factor of algorithms [ algrk4 ] and [ algseries ] is given by the memory required to store the ( symmetric , complex ) density - matrix @xmath75 and the wave - functions @xmath58 .",
    "top level , high - performance solutions for gpgpu computing like nvidia tesla k80 offer up 24 gb of gpu - ram , which cap the maximum size around 51000 rows ( e.g. , @xmath76 , @xmath77 , @xmath78 ) .",
    "in order to evaluate the performance of algorithms [ algrk4 ] and [ algseries ] we tested the case of unidimensional , 2-particle , nearest - neighbor ctqws with periodic boundary conditions ( i.e. , @xmath79 , @xmath77 , @xmath80 ) and random noise on the tunneling energies . simulations of 1500 time steps for @xmath81 realizations , with different rates for post - processing ( from 1 out of 1500 to 1 out of 10 time steps ) have been run on the following hardware :    * * intel cpu * : core i5 - 4570r @ 2.7 ghz and 8 gb ram ( 4 cores ) , os",
    "x 10.10.5 * * intel cpu * : core i7 - 3770 @ 3.4 ghz and 4 gb ram ( 4 cores ) , 64-bit linux os * * intel cpu * : xeon e3 - 1241 v3 @ 3.5 ghz and 16 gb ram ( 4 cores ) , 64-bit linux os * * nvidia gpu * : tesla m2050 with 3 gb vram , ecc enabled , compute capability 2.0 , cuda toolkit 5.0 * * nvidia gpu * : tesla k40 with 12 gb vram , ecc enabled , compute capability 3.5 , cuda toolkit 7.5 * * nvidia gpu * : tesla k80 with 24 gb vram , ecc enabled , compute capability 3.7 , cuda toolkit 7.5 * * nvidia gpu * : geforce gtx980 with 4 gb vram , no ecc , compute capability 5.2 , cuda toolkit 6.5    the openmp source code has been compiled with the intel c++ compiler ( icc ) version 15.0.3 for linux and version 15.0.7 for os x ; the cuda source code with the nvidia cuda compiler ( nvcc ) , with no further optimizations other than those provided by default .",
    "preliminary runs on gpus proved that 256 threads per block maximize the efficiency .",
    "the execution times of the 4-th order runge - kutta and of the series expansion methods are basically the same . depending on the hardware , very few seconds in favor of one algorithm or the other are reported ; differences become negligible as long as the size of the mesh increases ( see [ fig : rkvexp ] for tests performed on the tesla k40 ) .",
    "therefore , we proceed in the analysis only with algorithm [ algseries ] and assume that the same conclusions hold true also for algorithm [ algrk4 ] .     and [ algseries ] on a k40 board as a function of the post - processing rate .",
    "the 4-th order runge kutta method is on average 2 seconds faster , which becomes a negligible time as long as the size of the problem increases . ]",
    "[ fig : cpu ] illustrates the execution time of the series expansion algorithms as a function of the problem size for the three cpus under test . a strong performance loss sets in around 12500 rows for the core - i7 and the xeon e3 processors , that , more generally , also share an evident qualitative correlation of the running times as a function of the mesh size .",
    "since these evidences are not confirmed in the core - i5 case , we attribute this underperformance to a compiler issue specific of version 15.0.3 .     +   +    the analogous execution time comparison for the codes running on the four gpus is shown in [ fig : gpu ] .",
    "all tests are completed in less than 1 hour , with running times very similar to each other .",
    "the only exception is represented by the tesla m2050 board that underperforms its competitors , though retaining a substantial gain over any openmp execution . in general , the fastest runs are achieved with the geforce gtx980 board thanks to a superior clock rate .",
    "notice that geforce boards are not certified for gpgpu computing due to lack of ecc memory , and uncontrolled bit - flips or erratic bits in the memory locations devoted to the storage of @xmath58 can jeopardize the reliability of the outcomes of the simulations .",
    "though uncommon , this aspect deserves care and double checks are mandatory in presence of odd results .",
    "+   +    in order to provide an overall comparative review of the performances , we chose the core - i5 as the reference processor and we calculate the simulation speedup as @xmath82 data are shown in [ fig : su ] .",
    "gpu implementation becomes convenient roughly about 1000 rows , when the workload starts to fill completely the computational power of the gpus .",
    "since this is a very common case for many - particle ctqws ( i.e. , a mesh as small as @xmath83 for the two - particle case ) , gpu computing sounds a viable and efficient option to pursue in order to reduce the execution time down to the minute - to - few - hour range .",
    "it is important to stress that the simulation speedup strongly depends on the post - processing rate . for an output generation as frequent as 1 out of 10 time steps ( panel a ) a gain about 5x-7x is obtained ; the gain rises up to 8x-9x for an average post - processing rate of 1 out of 25 time steps ( panel b ) and up to a 10x-13x for a moderate output generation around 1 out of 100 time steps ( panel c ) .",
    "panels d and e refer instead to cases where the calculation of the density matrix is progressively reduced down to a single time per simulation . in other words , this is the the speedup achievable for the pure evolution of the wave - functions , which settles in the 20x range and more . by comparing data reported in panels [ cpu_1500 ] and [ cpu_ser ] of [ fig : cpu ] ,",
    "the openmp parallelization introduces a further 2.5 - 3x gain with respect to the single - core execution , this boosting up the speedup at a minimum gain around 60x for the pure evolution of the wave functions , as shown in [ su_ser ] , and around 15x when a high post - processing frequency is required .",
    "+   +    the speedup depends also on the number of realizations considered during parallel execution . about a + 3x gain",
    "is observed when the number of realizations increases from 500 to 5000 ( [ fig : realizations ] ) , irrespectively of the size of the mesh . while the gpu codes scale with the number of realizations ( as should be according to the discussion of sects .",
    "[ rk ] and [ series ] ) , a performance loss is found for the openmp implementation .",
    "for the sake of truth , we recall that the openmp code was derived from the cuda code with the strict constraint of adhering as much as possible to it and allowing a fair direct comparison of the computational burden , without introducing any further memory nor algorithmic optimizations . since the performance loss does not significantly depend on the size of the problem in hand , but only to the number of realizations , this poor behavior can primarily be ascribed to the larger number of calls to the blas functions .",
    "+    the influence of the post - processing rate in gpu execution is even more evident from the shape of the curves of [ fig : gpu ] that changes from parabolic to linear . though not immediate at first sight",
    ", the same also applies for the curves of [ fig : cpu ] and is validated by numerical regression .",
    "further information stem from code profiling .",
    "we have tracked the execution time of the four stages composing the software for the two opposite cases of very frequent and tiny output generation on the k40 board ( [ fig : profiling ] ) : the _ initialization _ and the _ hamiltonian update _",
    "stages contribute with a negligible running time ( less then 0.3% in total ) , while the _ wave - function evolution _ and the _ density - matrix calculation and post - processing _ stages largely prevail .",
    "the running time in case of a very limited output generation is substantially dictated by the _ wave - function evolution _ stage , which grows linearly with the size of the problem as discussed in sect .",
    "[ algo ] . on the contrary , in case of a frequent output generation , the heaviest stage is represented by _ density - matrix calculation and post - processing _ , whose influence quickly grows up and saturates about 90% of the total execution time . going into details , more than the 99.3% of the time spent for post - processing",
    "is required by the ` cublascher ` library function that builds up the average density - matrix @xmath75 . as a consequence ,",
    "the peaks in panels a , b and c of [ fig : su ] are due to outperforming conditions of the cublas library .",
    "also in the _ wave - function evolution _",
    "stage most of the time is spent in calls to system or library functions ( see [ pies ] ) . as a matter of fact , even for large matrices ( i.e. , @xmath84 ) , only up to approximately one third of the time is dedicated to the series expansion , whereas the remainder is due to device - to - device memcopy and norm evaluation ( ` cublasscnrm2 ` and ` cublascsscal ` ) .",
    "as we already pointed out , memory optimization for speed using , e.g. , shared memory on the device , was not a goal of the present work . from the time profiling above , we do not believe it worth the effort : highly - optimized solutions able to cut the execution time of the computational kernels by a factor of 2 or 3 would only bring a very modest benefit around 1 minute or less . to obtain a further significant speedup",
    "it is instead mandatory to implement new kernels for linear algebra , other than those provided by the cublas library .",
    "the availability of a simulation tool for evolving many - particle ctqws in a noisy environment represents a crucial prerequisite for the investigation of quantum many - body systems and for the implementation of effective quantum algorithms in realistic situations .",
    "in essence , the dynamics of a many - particle state over a noisy lattice can be associated with the solution of a set of stochastic differential equations .",
    "however , the need to post - process a large number of data in order to achieve information for any measurable quantity makes the problem much more resource - demanding .",
    "in fact , as long as the number of particles and/or the dimensionality of the domain increase , limiting factors such as the memory occupancy and the time required to run the simulations quickly become very challenging issues and determine whether a simulation scheme can or can not provide results within the available computational power .",
    "though numerically accurate , the standard hamiltonian diagonalization method is not feasible even for small systems and alternative numerical solutions must be sought . among them , we have shown that the 4-th order runge - kutta integration method and the taylor - series expansion of the evolution operator have a low computational cost and provide reliable data .",
    "moreover , they are highly parallelizable within the simt paradigm , and this allows the straightforward , direct implementation on gpus .    after developing the codes , we have benchmarked four nvidia gpus and three quad - core intel cpus for a 2-particle system over a lattice of increasing dimensions .",
    "gpu execution enables significant cuts of the running time of batches of thousands of simulations down to the minute - to - few - hour range .",
    "the speedup with respect to openmp parallelization stays in the range from 8x to more than 20x , depending on the frequency of post - processing .",
    "our results show that gpu - accelerated codes allow one to overcome concerns about the execution time and make it possible to design simulations involving many particles or large lattices , whose only limit is dictated by the memory available on the device .",
    "this work has been supported by eu through the collaborative project quprocs ( grant agreement 641277 ) and by unimi through the h2020 transition grant 15 - 6 - 3008000 - 625 and by unimore through far2014 .",
    "10 url # 1`#1`urlprefixhref # 1#2#2 # 1#1              x.  qin , y.  ke , x.  guan , z.  li , n.  andrei , c.  lee , statistics - dependent quantum co - walking of two particles in one - dimensional lattices with nearest - neighbor interactions , phys .",
    "a 90 ( 2014 ) 062301 .",
    "a.  crespi , r.  osellame , r.  ramponi , v.  giovannetti , r.  fazio , l.  sansoni , f.  de  nicola , s.  fabio , p.  mataloni , anderson localization of entangled photons in an integrated quantum walk , nat .",
    "( 2013 ) 322328 .",
    "f.  de  nicola , l.  sansoni , a.  crespi , r.  ramponi , r.  osellame , v.  giovannetti , r.  fazio , p.  mataloni , f.  sciarrino , quantum simulation of bosonic - fermionic noninteracting particles in disordered systems via a quantum walk , phys . rev . a 89  ( 3 ) ( 2014 ) 032322 .",
    "j.  glaser , t.  d. nguyen , j.  a. anderson , p.  lui , f.  spiga , j.  a. millan , d.  c. morse , s.  c. glotzer , strong scaling of general - purpose molecular dynamics simulations on , comput .",
    "commun . 192",
    "( 2015 ) 97  107 ."
  ],
  "abstract_text": [
    "<S> many - particle continuous - time quantum walks ( ctqws ) represent a resource for several tasks in quantum technology , including quantum search algorithms and universal quantum computation . in order to design and implement ctqws in a realistic scenario </S>",
    "<S> , one needs effective simulation tools for hamiltonians that take into account static noise and fluctuations in the lattice , i.e. hamiltonians containing stochastic terms . to this aim , </S>",
    "<S> we suggest a parallel algorithm based on the taylor series expansion of the evolution operator , and compare its performances with those of algorithms based on the exact diagonalization of the hamiltonian or a 4-th order runge - kutta integration . </S>",
    "<S> we prove that both taylor - series expansion and runge - kutta algorithms are reliable and have a low computational cost , the taylor - series expansion showing the additional advantage of a memory allocation not depending on the precision of calculation . </S>",
    "<S> both algorithms are also highly parallelizable within the simt paradigm , and are thus suitable for gpgpu computing . in turn , we have benchmarked 4 nvidia gpus and 3 quad - core intel cpus for a 2-particle system over lattices of increasing dimension , showing that the speedup providend by gpu computing , with respect to the openmp parallelization , lies in the range between 8x and ( more than ) 20x , depending on the frequency of post - processing . </S>",
    "<S> gpu - accelerated codes thus allow one to overcome concerns about the execution time , and make it possible simulations with many interacting particles on large lattices , with the only limit of the memory available on the device .    </S>",
    "<S> gpu , cuda , continuous - time quantum walks </S>"
  ]
}