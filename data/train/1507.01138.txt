{
  "article_text": [
    "computers have become an integral part of modern life , and are essential for most academic research  @xcite . since the middle of last century , researchers have invented new techniques to boost their calculation rate , e.g. by engineering superior hardware , designing more effective algorithms , and introducing increased parallelism .",
    "due to a range of physical limitations which constrain the performance of single processing units  @xcite , recent computer science research is frequently geared towards enabling increased parallelism for existing applications .    by definition",
    ", parallelism is obtained by concurrently using the calculation power of multiple processing units . from small to large spatial scales , this is respectively done by : facilitating concurrent operation of instruction threads within a single core , of cores within a single processor , of processors within a node , of nodes within a cluster or supercomputer , and of supercomputers within a distributed supercomputing environment .",
    "the vision of aggregating existing computers to form a global unified computing platform , and to focus that power for a single purpose , has been very popular both in popular fiction ( e.g. , the borg collective mind in star trek or big brother in orwell s 1984 ) and in scientific research ( e.g. , amazon ec2 , projects such as teragrid / xsede and egi , and numerous distributed computing projects  @xcite ) .",
    "although many have tried , none have yet succeeded to link up more than a handful of major computers in the world to solve a major high - performance computing problem .",
    "very few research endeavors aim to do distributed computing at such a scale to obtain more performance .",
    "although it requires a rather large labour investment across several time zones , accompanied with political complexities , it is technically possible to combine supercomputers to form an intercontinental grid .",
    "we consider that combining supercomputers in such a way is probably worth the effort if many machines are involved , rather than a few .",
    "combining a small number of machines is hardly worth the effort of doubling the performance of a single machine , but combining hundreds or maybe even thousands of computers together could increase performance by orders of magnitude  @xcite .    here",
    "we share our experiences , and lessons learned , in performing a large cosmological simulation using an intercontinental infrastructure of multiple supercomputers .",
    "our work was part of the cosmogrid project  @xcite , an effort that was eventually successful but which suffered from a range of difficulties and set - backs .",
    "the issues we faced have impacted on our personal research ambitions , and have led to insights which could benefit researchers in any large - scale computing community .",
    "we provide a short overview of the cosmogrid project , and describe our initial assumptions in section  [ sec : vision ] .",
    "we summarize the challenges we faced , ascending the hierarchy from thread to transcontinental computer , in section  [ sec : parallel ] and we summarize how our insights affected our ensuing research agenda in section  [ sec : after ] .",
    "we discuss the long - term implications of cosmogrid in section  [ sec : future ] and conclude the paper with some reflections in section  [ sec : discuss ] .",
    "the aim of cosmogrid was to interconnect four supercomputers ( one in japan , and three across europe ) using light paths and 10 gigabit wide area networks , and to use them concurrently to run a very large cosmological simulation .",
    "we performed the project in two stages : first by running simulations across two supercomputers , and then by extending our implementation to use four supercomputers concurrently .",
    "the project started as a collaboration between researchers in the netherlands , japan and the united states in october 2007 , and received support from several major supercomputing centres ( sara in amsterdam , epcc in edinburgh , csc in espoo and naoj in tokyo ) .",
    "cosmogrid mainly served a two - fold purpose : to predict the statistical properties of small dark matter halos from an astrophysics perspective , and to enable production simulations using an intercontinental network of supercomputers from a computer science perspective .      for cosmogrid , we required a code to model the formation of dark matter structures ( using @xmath1 particles in total ) over a period of over 13 billion years .",
    "we adopted a hybrid tree / particle - mesh ( treepm ) n - body code named _ greem _",
    "@xcite , which is highly scalable and straightforward to install on supercomputers .",
    "greem uses a barnes - hut tree algorithm  @xcite to calculate force interactions between dark matter particles over short distances , and a particle - mesh algorithm to calculate force interactions over long distances  @xcite . later in the project",
    ", we realized that further code changes were required to enable execution across supercomputers .",
    "as a result , we created a separate version of greem solely for this purpose .",
    "this modified code is named _ sushi _ , which stands for simulating universe structure formation on heterogeneous infrastructures  @xcite .",
    "our case for a distributed computing approach was focused on a classic argument used to justify parallelism : multiple resources can do more work than a single one .",
    "even the world s largest supercomputer is about an order of magnitude less powerful than the top 500 supercomputers in the world combined  @xcite . in terms of interconnectivity",
    "the case was also clear .",
    "our performance models predicted that a 1 gbps wide area network would already result in good simulation performance ( we had 10 gbps links at our disposal ) , and that the round - trip time of about 0.27 s between the netherlands and japan would only impose a limited overhead on a simulation that would require approximately 100 s per integration time step .",
    "world - leading performance of our cosmological n - body integrator was essential to make our simulations possible , and our japanese colleagues optimized the code for single - machine performance as part of the project .",
    "snapshots / checkpoints would then be written distributed across sites , and gathered at run - time . at the start of cosmogrid , we anticipated to run across two supercomputers by the summer of 2008 , and across four supercomputers by the summer of 2009 .",
    "we assumed a number of political benefits : the simulation we proposed required a large number of core hours and produce an exceptionally large amount of data .",
    "these requirements would have been a very heavy burden for a single machine , and by executing a distributed setup we could mitigate the computational , storage and data i / o load imposed on individual machines .",
    "we also were aware of the varying loads of machines at different times , and could accommodate for that by rebalancing the core distribution whenever we would restart the distributed simulation from a checkpoint .",
    "overall , we mainly expected technical problems , particularly in establishing a parallelization platform which works across supercomputers . installing",
    "homogeneous software across heterogeneous ( and frequently evolving ) supercomputer platforms appeared difficult to accomplish , particularly since we did not possess administrative rights on any of the machines .",
    "in addition , the greem code had not been tested in a distributed environment prior to the project .",
    "although we finalized the production simulations about a year later than anticipated , cosmogrid was successful in a number of fundamental areas .",
    "we managed to successfully execute cosmological test simulations across up to four supercomputers , and full - size production simulations across up to three supercomputers  @xcite .",
    "in addition , our astrophysical results have led to new insights on the mass distribution of satellite halos around milky - way sized galaxies  @xcite , on the existence of small groups of galaxies in dark - matter deprived voids @xcite , the structure of voids @xcite and the evolution of barionic - dominated star clusters in a dark matter halo @xcite . however , these results , though valuable in their own right , do not capture some of the most important and disturbing lessons we have learned from cosmogrid about distributed supercomputing . here",
    "we summarize our experiences on engineering a code from the level of threads to that of a transcontinental machine , establishing a linked infrastructure to deploy the code , reserving the required resources to execute the code , and the software engineering and sustainability aspects surrounding distributed supercomputing codes .",
    "we are not aware of previous publications of practical experiences on the subject , and for that reason this paper may help achieve a more successful outcome for existing research efforts in distributed hpc .",
    "the greem code was greatly re - engineered during cosmogrid .",
    "this was necessary to achieve a complete production simulation within the core hour allocations that we obtained from ncf and deisa .",
    "our infrastructure consisted of three cray xt4 machines with little endian intel chips and one ibm machine with big endian power7 chips . at the start of cosmogrid ,",
    "the power7 architecture was not yet in place .",
    "greem had been optimized for the use of sse and avx instruction sets , executing 10 times faster when these instructions are supported .",
    "sse was available in intel chips and avx was expected to be available in power7 chips .",
    "however , the support for avx in power7 never materialized , forcing us to find alternative optimization approaches .",
    "an initial 2-month effort by ibm engineers , leading to a 10% performance improvement , did not speed up the code sufficiently .",
    "we then resorted to manual optimization without the use specialized instruction sets , e.g. , by reordering data structures and unrolling loops .",
    "this effort resulted in a @xmath2 performance increase , which was within a factor of 3 of our original target .",
    "much of the parellelization work on greem was highly successful , as evidenced by the gordon bell prize awarded in 2012 to ishiyama et al .",
    "however , one unanticipated problem arose while scaling up the simulation to larger problem sizes .",
    "greem applies a particle - mesh ( pm ) algorithm to resolve the interactions beyond a preconfigured cutoff limit .",
    "the implementation of is algorithm was initially serial , as the overhead was a negligible component ( @xmath3% ) of the total execution time for smaller problems .",
    "however , the overhead became much larger we scaled up to mesh sizes beyond @xmath4 mesh cells , forcing us to move from a serial implementation to a parallel implementation .",
    "initially we considered executing greem as - is , and using a middleware solution to enable execution across supercomputers . in the years leading up to cosmogrid , a large number of libraries emerged for distributed hpc ( e.g. , mpich - g2  @xcite , openmpi  @xcite , mpig  @xcite and pacx - mpi  @xcite ) .",
    "many of these were strongly recommended by colleagues in the field , and provided mpi layers that allowed applications to pass messages across different computational resources .",
    "although these were well - suited for distributed hpc across self - administered clusters , we quickly found that a distributed supercomputing environment was substantially different .",
    "first , supercomputers are both more expensive and less common than clusters , and the centres managing them are reluctant to install libraries that require administrative privileges , due to risks of security and ease of maintenance ( mpi distributions tend to require such privileges ) .",
    "second , the networks interconnecting the supercomputers are managed by separate organizational entities , and the default configurations at each network endpoint are almost always different and frequently conflicting .",
    "this is not the case in more traditional ( national ) grid infrastructures , where uniform configurations can be imposed by the overarching project team .",
    "the heterogeneity in network configurations resulted in severe performance and reliability penalties when using standard tcp - based middleware ( such as mpi and scp , unless we could find some way to either ( a ) customize the network configuration for individual paths or ( b ) adopt a different protocol ( e.g. , udp ) which ignores these preset configurations . in either case , we realized that using standard tcp - based mpi libraries for the communication between supercomputers was no longer a viable option .",
    "using any other library had the inevitable consequence of modifying the main code , and eventually we chose to customize greem ( the customized version is named sushi  @xcite ) and establish a seperate communication library ( mpwide  @xcite ) for distributed supercomputing .      having a code to run , and computer time to run it on is insufficient to do distributed concurrent supercomputing .",
    "the amount of data to be transported is @xmath5 per integration time step and should not become the limiting factor in measuring performance .",
    "each integration step would take about 100s . when allowing a 10% overhead we would have to require a network speed of @xmath6gb / s .",
    "our collaboration with cees de laat ( university of amsterdam ) and kei hiraki ( tokyo university ) enables us to have two network and data transport specialists at each side of the light path .    at the time",
    ", russia had planned to make their military 10gbps dark fiber available for scientific experiments , but due to their enhanced military use we were unable to secure access to this cable .",
    "the eventual route of the optical cable is presented in fig.[fig : cgnetworktopology ] .",
    "we had a backup network between the ntt - com router at jgn2plus and the starlight interconnect to guarantee that our data stream remained stable throughout our calculations .",
    "one of the interesting final quirks in our network topology was the absence of an optical network interface in the edinburgh machine ( which was installed later ) , and the fact that the optical cable at the japanese side reached the computer science building on the mitaka campus in tokyo next to where the supercomputer at naoj was located .",
    "a person had to go physically to dig a hole and install a connecting cable between the two buildings .    from a software perspective , we present our design considerations on mpwide fully in groen et al .",
    "@xcite . here",
    "we will summarize the main experiences and lessons that we learned from cosmogrid , as well as our experiences in readying the network in terms of software configuration .",
    "we initially attempted to homogenize the network configuration settings between the different supercomputers .",
    "this effort failed , as it was complicated by the presence of over a dozen stakeholder organizations , and further undermined by the lack of diagnostic information available to us .",
    "for example , it was not always possible for us to pinpoint specific configuration errors to individual routers , and to their respective owners .",
    "we also assessed the performance of udp - based solutions such as quanta  @xcite and udt  @xcite , which operate outside of the tcp - specific configuration settings of network devices .",
    "however , we were not able to universally adopt such a solution , as some types of routers filter or restrict udp traffic .",
    "we eventually converged on basing mpwide on multi - stream tcp  @xcite , and combine this with mechanisms to customize tcp settings on each of the communication nodes  @xcite .",
    "our initial tests were marred with network glitches , particularly on the path between amsterdam and tokyo ( see fig .",
    "[ fig : cgglitch ] for an example where packets were periodically stalled ) .",
    "however , later runs resulted in more stable performance once we used a different path and adjusted the mpwide configuration to use very small tcp buffer sizes per stream  @xcite .         particles , run in parallel across the supercomputers in amsterdam and tokyo ( 32 cores per site ) .",
    "we present the total time spent ( blue dotted line ) , time spent on calculation ( green dashed line ) , and the time spent on communication with mpwide ( red solid line ) .",
    "stalls in the communication due to dropped packets resulted in visible peaks in the communication performance measurements .",
    "reproduced from groen et al .",
    "note : the communication time was relatively high ( @xmath7 of total runtime ) ) in this test run due to the small problem size , in  @xcite we also present results from a test run with @xmath1 particles , which had a communication overhead of @xmath8 of total runtime.,width=288 ]      the ability to have concurrent access to multiple supercomputers is an essential requirement for distributed supercomputing . within cosmogrid",
    ", we initially agreed that the four institutions involved would provide so - called `` phone - based '' advance reservation for the purpose of this project .",
    "however , due to delays in the commisioning of the light path , and due to political resistance regarding advance reservation within some of the supercomputing centres ( in part caused due to increasing demand of the machines ) , it was no longer possible to use this means of advance reservation on all sites .",
    "eventually , we ended up with a different `` reservation '' mechanism for each of the supercomputers .",
    "the original approach of calling up was still supported for the huygens machine in the netherlands . for the louhi machine in finland ,",
    "calling up was also possible , but the reservation was established through an exclusively dedicated queue , as no direct reservation system was in place .",
    "this approach had the side effect of locking other users out , and therefore we only opted to use it as a last resort .",
    "for the hector machine in the uk , it was possible to request higher priority , but not to actually reserve resources in advance .",
    "support for advance reservation was provided there shortly after cosmogrid concluded , but at the time the best method to `` reserve '' the machine was to submit a very large high priority job right before the machine maintenance time slot .",
    "we then need to align all other reservations to the end of that maintenance time slot , presumed usually to be 6 hours after the start of the maintenance .",
    "for the cray machine in japan , reservation was no longer possible due to the high work load .",
    "however , some mechanisms of augmented priority could be established indirectly , e.g. by chaining jobs , which allowed for a job to be kept running at all times .",
    "the combination of these strategies made it impossible to perform a large production run using all four sites .",
    "we did perform smaller tests using the full infrastructure ( using regular scheduling queues and hoping for the best ) , but we were only able to do the largest runs using either the three european machines , or huygens combined with the cray in tokyo .",
    "the task of engineering a code for distributed supercomputing is accompanied with a number of unusual challenges , particularly in the areas of software development and testing  @xcite . at the time",
    ", we had to ensure that greem and sushi remained fully compatible with all four supercomputer platforms . with no infrastructure in place to do continuous integration testing on supercomputer nodes ( even today this is still a rarity ) , we performed this testing periodically by hand . in general , testing on a single site is straightforward , but testing across sites less so .",
    "we were able to arrange proof - of - concept tests across 4 sites using very small jobs ( e.g. , 16 cores per site ) by putting these jobs with long runtimes in the queue on each machine and waiting with starting the run until the jobs are running simultaneously . for slightly larger jobs ( e.g. , 64 cores per site )",
    "this became difficult during peak usage hours as the queuing times of each job became longer and less predictable .",
    "however , we have been able to perform a number of tests during more quiet periods of the week ( e.g. , at 2 am ) , without using advance reservation . for yet larger test runs",
    "we were required to use advance reservation , reduce the number of supercomputers involved , or both .",
    "software testing is instrumental to overall development , particularly when developing software in a challenging environment like an intercontental network of supercomputers . here",
    ", the lack of facilities for advance reservation and continuous integration made testing large systems prohibitively difficult , and had an adverse effect on the development progress of sushi .",
    "we eventually managed to get an efficient production calculation running for 12 hours across three sites and with the full system size of @xmath1 particles  @xcite , but with better facilities for testing across sites we could well have tackled larger problems using higher core counts .",
    "more emphasis and investment in testing facilities at the supercomputer centres would have boosted the cosmogrid project , and such support would arguably be of great benefit to increase the user uptake of supercomputers in general .",
    "our experience with cosmogrid changed how we approached our computational research in two important ways .",
    "first , due to the political hardships and the lack of facilities for advance reservation and testing , we changed our emphasis from distributed concurrent supercomputing towards improving code performance on single sites  @xcite .",
    "second , our expertise enabled us the enter the relatively young field of distributed multiscale computing with a head start .",
    "multiscale computing involves the combination of multiple solvers to solve a problem that encompasses several different length and time scales .",
    "each of these solvers may have different resource requirements , and as a result such simulations are frequently well - suited to be run across multiple resources .",
    "drost et al .",
    "applied some of our expertise , combining it with their experience using ibis , to enable the amuse environment to run different coupled solvers on different resources concurrently  @xcite .",
    "our experiences with cosmogrid were an important argument towards redeveloping the muscle coupling environment for the mapper project . in this eu - funded consortium ,",
    "borgdorff et al . developed a successor ( muscle 2 ) , which is optimized for easier installation on supercomputers , and automates the startup of solvers that run concurrently on different sites ( a task that was done manually in cosmogrid ) .",
    "in addition , we integrated mpwide in muscle2  @xcite and used mpwide directly to enable concurrently running coupled simulations across sites  @xcite .",
    "distributed high - performance supercomputing is not for the faint at heart .",
    "it requires excellent programming and pluralization skills , stamina , determination , politics and hard labour .",
    "one can wonder if it is worth the effort , but the answer depends on the available resources and the success of proposal writing . about 2030% of the proposals submitted to incite ( http://www.doeleadershipcomputing.org/faqs/ ) or prace ( http://www.prace-ri.eu/prace-kpi/ ) are successful , but",
    "these success rates are generally lower for the very largest infrastructures ( e.g. , ornl titan ) . in addition , some of the largest supercomputers ( such as tianhe-2 and the k computer ) provide access only to closely connected research groups or to projects that have a native principal investigator .",
    "acquiring compute time on these largest architectures can be sufficiently challenging that running your calculations on a number of less powerful but earlier accessible machines may be easier to accomplish .    accessing several of such machines through one project is even harder , and probably not very realistic .",
    "similarly , for the different architectures , it would be very curious to develop a code that works optimal on k computer and ornl titan concurrently .",
    "achieving 25 pflops on titan alone is already a major undertaking @xcite , and combining such an optimized code with a tofu - type network architecture ( which is present on the k computer ) would make optimization a challenge of a different order .",
    "we therefore do not think that distributed architectures will be used to beef - up the world s fastest computers , nor to connect a number of top-10 to top-100 supercomputers to out - compute the number 1 .",
    "the type of distributed hpc as discussed in this article is probably best applied to large industrial or small academic computer clusters .",
    "these @xmath9petaflop architectures are found in many academic settings or small countries , and are relatively easily accessible , by peer review proposals or via academic license agreements . in this context",
    ", we think it is more feasible to connect 10 to 100 of such machines to outperform a top 1 to 10 computer .",
    "we have presented our experiences from the cosmogrid project in high - performance distributed supercomputing .",
    "plainly put , distributed high - performance supercomputing is a tough undertaking , and most of our initial assumptions were proven wrong .",
    "much of the hardware and software infrastructure was constructed with very specific use - cases in mind , and was simply not fit for purpose to do distributed high - performance supercomputing .",
    "a major reason why we have been able to establish distributed simulations at all is due to the tremendous effort of all the people involved , from research groups , networking departments and supercomputer centres .",
    "it was due to their efforts to navigate the project around the numerous technical and political obstacles that distributed supercomputing became even possible .",
    "cosmogrid was unsuccessful in establishing high - performance distributed supercomputing as the future paradigm for using very large supercomputers .",
    "however , the project did provide a substantial knowledge boost to our subsequent research efforts , which is reflected by the success of projects such as mapper  @xcite and amuse  @xcite . the somewhat different approach taken in these projects ( aiming for more local resource infrastructures , and with a focus on coupling different solvers instead of parallelizing a single one ) resulted in tens of publications which relied on distributed ( super-)computing .",
    "the hpc community has recently received criticism for its conservative approaches and resistance to change ( e.g. ,  @xcite ) . through cosmogrid",
    ", it became obvious to us that resource providers can be subject to tricky dilemmas , where the benefits of supporting one research project need to be weighed against the possible reduced service ( or support ) incurred by other users . in light of that",
    ", we do understand the conservative approaches followed in hpc to some extent . in cosmogrid",
    ", we tried to work around that by ensuring that our software was installable without any administrative privileges , and we recommend that new researchers who wish to do distributed supercomputing do so as well ( or , perhaps , adopt very robust , flexible and well - performing tools for virtualization ) .",
    "in addition , we believe that cosmogrid would have been greatly helped if innovations such as automated advance reservation systems for resources and network links , facilities for systematic software testing and continuous integration , and streamlined procedures for obtaining access to multiple sites had been in place . even today , such facilities make hpc infrastructures more convenient for new types of users and applications , and strengthen the position of the hpc community in an increasingly cloud - dominated computing landscape .",
    "both spz and dg contributed equally to this work .",
    "we are grateful to tomoaki ishiyama , keigo nitadori , jun makino , steven rieder , stefan harfst , cees de laat , paola grosso , steve macmillan , mary inaba , hans blom , jeroen bdorf , juha fagerholm , tomoaki ishiyama , esko kernen , walter lioen , jun makino , petri nikunen , gavin pringle and joni virtanen for their contributions to this work .",
    "this research is supported by the netherlands organization for scientific research ( nwo ) grant # 639.073.803 , # 643.200.503 and # 643.000.803 and the stichting nationale computerfaciliteiten ( project # sh-095 - 08 ) .",
    "we thank the deisa consortium ( eu fp6 project ri-031513 and fp7 project ri-222919 ) for support within the deisa extreme computing initiative ( gbbp project ) .",
    "this paper has been made possible with funding from the uk engineering and physical sciences research council under grant number ep / i017909/1 ( http://www.science.net ) .",
    "s.  hettrick , m.  antonioletti , l.  carr , n.  chue  hong , s.  crouch , d.  de  roure , i.  emsley , c.  goble , a.  hay , d.  inupakutika , m.  jackson , a.  nenadic , t.  parkinson , m.  i. parsons , a.  pawlik , g.  peru , a.  proeme , j.  robinson , and s.  sufi , `` uk research software survey 2014 , '' dec . 2014 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.5281/zenodo.14809      a.  gualandris , s.  portegies zwart , and a.  tirado - ramos , `` performance analysis of direct n - body algorithms for astrophysical simulations on distributed systems . ''",
    "_ parallel computing _ , vol .",
    "33 , no .  3 , pp .",
    "159173 , 2007 .",
    "d.  groen , s.  portegies zwart , t.  ishiyama , and j.  makino , `` high performance gravitational n - body simulations on a planet - wide distributed supercomputer , '' _ computational science and discovery _",
    ", vol .  4 , no .",
    "015001 , jan .",
    "e.  agullo , c.  coti , t.  herault , j.  langou , s.  peyronnet , a.  rezmerita , f.  cappello , and j.  dongarra , `` qcg - ompi : \\{mpi } applications on grids , '' _ future generation computer systems _ , vol .",
    "27 , no .  4 , pp . 357",
    " 369 , 2011 .",
    "[ online ] .",
    "available : http://www.sciencedirect.com/science/article/pii/s0167739x10002359    f.  j. seinstra , j.  maassen , r.  v. van nieuwpoort , n.  drost , t.  van kessel , b.  van werkhoven , j.  urbani , c.  jacobs , t.  kielmann , and h.  e. bal , `` , '' in _ _ , ser .",
    "computer communications and networks , m.  cafaro and g.  aloisio , eds .",
    "1em plus 0.5em minus 0.4emspringer london , 2011 , pp .",
    "167197 .",
    "m.  ben  belgacem , b.  chopard , j.  borgdorff , m.  mamonski , k.  rycerz , and d.  harezlak , `` distributed multiscale computations using the mapper framework , '' _ procedia computer science _ ,",
    "18 , no .  0 , pp . 1106  1115 , 2013 , 2013 international conference on computational science .",
    "[ online ] .",
    "available : http://www.sciencedirect.com/science/article/pii/s1877050913004195    j.  borgdorff , m.  ben  belgacem , c.  bona - casas , l.  fazendeiro , d.  groen , o.  hoenen , a.  mizeranschi , j.  l. suter , d.  coster , p.  v. coveney , w.  dubitzky , a.  g. hoekstra , p.  strand , and b.  chopard , `` performance of distributed multiscale simulations , '' _ philosophical transactions of the royal society a : mathematical , physical and engineering sciences _ , vol .",
    "372 , no . 2021 , 2014 .",
    "g.  hoekstra , a. , s.  portegies zwart , m.  bubak , and p.   sloot , _ towards distributed petascale computing_.1em plus 0.5em minus 0.4empetascale computing : algorithms and applications , by david a. bader ( ed . ) .",
    "chapman & hall / crc computational science series 565pp .",
    "( isbn : 9781584889090 , isbn 10 : 1584889098 ) , 2008 .",
    "s.  portegies zwart , t.  ishiyama , d.  groen , k.  nitadori , j.  makino , c.  de laat , s.  mcmillan , k.  hiraki , s.  harfst , and p.  grosso , `` simulating the universe on an intercontinental grid , '' _ computer _ , vol .",
    "43 , pp . 6370 , 2010 .",
    "t.  ishiyama , t.  fukushige , and j.  makino , `` greem : massively parallel treepm code for large cosmological n - body simulations , '' _ publications of the astronomical society of japan _ ,",
    "61 , no .  6 , pp .",
    "13191330 , 2009 .",
    "d.  groen , s.  rieder , and s.  portegies zwart , `` high performance cosmological simulations on a grid of supercomputers , '' in _ proceedings of infocomp 2011_.1em plus 0.5em minus 0.4emthinkmind.org , sep . 2011 .",
    "t.  ishiyama , s.  rieder , j.  makino , s.  portegies  zwart , d.  groen , k.  nitadori , c.  de  laat , s.  mcmillan , k.  hiraki , and s.  harfst , `` the cosmogrid simulation : statistical properties of small dark matter halos , '' _ the astrophysical journal _ , vol .",
    "767 , no .  2 , p. 146",
    "[ online ] .",
    "available : http://stacks.iop.org/0004-637x/767/i=2/a=146          t.  ishiyama , k.  nitadori , and j.  makino , `` 4.45 pflops astrophysical n - body simulation on k computer : the gravitational trillion - body problem , '' in _ proceedings of the international conference on high performance computing , networking , storage and analysis _",
    "sc 12.1em plus 0.5em minus 0.4emlos alamitos , ca , usa : ieee computer society press , 2012 , pp .",
    "5:15:10 .",
    "n.  karonis , b.  toonen , and i.  foster , `` mpich - g2 : a grid - enabled implementation of the message passing interface , '' _ journal of parallel and distributed computing _ , vol .",
    "63 , no .  5 , pp . 551  563 , 2003 , special issue on computational grids .      c.  coti , t.  herault , and f.  cappello ,",
    "`` , '' in _ _ , ser .",
    "lecture notes in computer science , h.  sips , d.  epema , and h .- x .",
    "lin , eds.1em plus 0.5em minus 0.4emspringer berlin heidelberg , 2009 , vol .",
    "5704 , pp .",
    "466477 .",
    "s.  manos , m.  mazzeo , o.  kenway , p.  v. coveney , n.  t. karonis , and b.  toonen , `` distributed mpi cross - site run performance using mpig , '' in _ proceedings of the 17th international symposium on high performance distributed computing _",
    "hpdc 08.1em plus 0.5em minus 0.4em new york , ny , usa : acm , 2008 , pp .",
    "229230 .",
    "m.  muller , m.  hess , and e.  gabriel , `` grid enabled mpi solutions for clusters , '' in _",
    "cluster computing and the grid , 2003 .",
    "ccgrid 2003 .",
    "3rd ieee / acm international symposium on_.1em plus 0.5em minus 0.4emieee , 2003 , pp .",
    "d.  groen , s.  rieder , p.  grosso , c.  de laat , and s.  portegies zwart , `` a light - weight communication library for distributed computing , '' _ computational science and discovery _ , vol .  3 , no .",
    "015002 , aug . 2010 .",
    "e.  he , j.  alimohideen , j.  eliason , n.  krishnaprasad , j.  leigh , o.  yu , and t.  defanti , `` quanta : a toolkit for high performance data delivery over photonic networks , '' _ future generation computer systems _ , vol .",
    "19 , no .  6 , pp . 919  933 , 2003",
    ".      t.  j. hacker , b.  d. athey , and b.  noble , `` the end - to - end performance effects of parallel tcp sockets on a lossy wide - area network , '' in _ vehicle navigation and information systems conference , 1993 . ,",
    "proceedings of the ieee - iee _ , oct 1993 .",
    "j.  bdorf , e.  gaburov , m.  s. fujii , k.  nitadori , t.  ishiyama , and s.  portegies  zwart , `` 24.77 pflops on a gravitational tree - code to simulate the milky way galaxy with 18600 gpus , '' in _ proceedings of the international conference for high performance computing , networking , storage and analysis _ , ser .",
    "sc 14.1em plus 0.5em minus 0.4em piscataway , nj , usa : ieee press , 2014 , pp .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1109/sc.2014.10    n.  drost , j.  maassen , m.  van meersbergen , h.  bal , f.  pelupessy , s.  portegies  zwart , m.  kliphuis , h.  dijkstra , and f.  seinstra , `` high - performance distributed multi - model / multi - kernel simulations : a case - study in jungle computing , '' in _ parallel and distributed processing symposium workshops phd forum ( ipdpsw ) , 2012 ieee 26th international _ , may 2012 , pp .",
    "150162 .",
    "j.  borgdorff , m.  mamonski , b.  bosak , k.  kurowski , m.  ben  belgacem , b.  chopard , d.  groen , p.  v. coveney , and a.  g. hoekstra , `` distributed multiscale computing with \\{muscle } 2 , the multiscale coupling library and environment , '' _ journal of computational science _",
    ", vol .  5 , no .  5 , pp .",
    "719  731 , 2014 .",
    "[ online ] .",
    "available : http://www.sciencedirect.com/science/article/pii/s1877750314000465    d.  groen , j.  borgdorff , c.  bona - casas , j.  hetherington , r.  nash , s.  zasada , i.  saverchenko , m.  mamonski , k.  kurowski , m.  bernabeu , a.  hoekstra , and p.  coveney , `` flexible composition and execution of high performance , high fidelity multiscale biomedical simulations , '' _ interface focus _",
    ", vol .  3 , no .  2 , p. 20120087 , 2013 .",
    "s.  f. portegies  zwart , s.  l.  w. mcmillan , a.  van elteren , f.  i. pelupessy , and n.  de  vries , `` multi - physics simulations using a hierarchical interchangeable software interface , '' _ computer physics communications _",
    "184 , no .  3 , pp .",
    "456  468 , 2013 ."
  ],
  "abstract_text": [
    "<S> we describe the political and technical complications encountered during the astronomical cosmogrid project . </S>",
    "<S> cosmogrid is a numerical study on the formation of large scale structure in the universe . </S>",
    "<S> the simulations are challenging due to the enormous dynamic range in spatial and temporal coordinates , as well as the enormous computer resources required . </S>",
    "<S> in cosmogrid we dealt with the computational requirements by connecting up to four supercomputers via an optical network and make them operate as a single machine . </S>",
    "<S> this was challenging , if only for the fact that the supercomputers of our choice are separated by half the planet , as three of them are located scattered across europe and fourth one is in tokyo . </S>",
    "<S> the co - scheduling of multiple computers and the gridification of the code enabled us to achieve an efficiency of up to @xmath0 for this distributed intercontinental supercomputer . in this work , we find that high - performance computing on a grid can be done much more effectively if the sites involved are willing to be flexible about their user policies , and that having facilities to provide such flexibility could be key to strengthening the position of the hpc community in an increasingly cloud - dominated computing landscape . given that smaller computer clusters owned by research groups or university departments usually have flexible user policies </S>",
    "<S> , we argue that it could be easier to instead realize distributed supercomputing by combining tens , hundreds or even thousands of these resources . </S>"
  ]
}