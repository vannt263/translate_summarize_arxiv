{
  "article_text": [
    "principal components analysis is one of the most useful statistical tool to extract information by reducing the dimension when one has to analyze large samples of multivariate or functional data ( see _ e.g. _ @xcite or @xcite ) .",
    "when both the dimension and the sample size are large , outlying observations may be difficult to detect automatically .",
    "principal components , which are derived from the spectral analysis of the covariance matrix , can be very sensitive to outliers ( see @xcite ) and many robust procedures for principal components analysis have been considered in the literature ( see @xcite , @xcite and @xcite ) .",
    "the most popular approaches are probably the minimum covariance determinant estimator ( see @xcite ) and the robust projection pursuit ( see @xcite and @xcite ) .",
    "robust pca based on projection pursuit has been extended to deal with functional data in @xcite and @xcite .",
    "adopting another point of view , robust modifications of the covariance matrix , based on projection of the data onto the unit sphere , have been proposed in @xcite ( see also @xcite and @xcite ) .",
    "we consider in this work another robust way of measuring association between variables , that can be extended directly to functional data .",
    "it is based on the notion of median covariation matrix ( mcm ) which is defined as the minimizer of an expected loss criterion based on the hilbert - schmidt norm ( see @xcite for a first definition in a more general @xmath0-estimation setting ) .",
    "it can be seen as a geometric median ( see @xcite or @xcite ) in the particular hilbert spaces of square matrices ( or operators for functional data ) equipped with the frobenius ( or hilbert - schmidt ) norm .",
    "the mcm is non negative and unique under weak conditions .",
    "as shown in @xcite it also has the same eigenspace as the usual covariance matrix when the distribution of the data is symmetric and the second order moment is finite . being a spatial median in a particular hilbert space of matrices ,",
    "the mcm is also a robust indicator of central location , among the covariance matrices , which has a 50 % breakdown point ( see @xcite or @xcite ) as well as a bounded gross sensitivity error ( see @xcite ) .",
    "the aim of this work is twofold .",
    "it provides efficient recursive estimation algorithms of the mcm that are able to deal with large samples of high dimensional data . by this recursive property",
    ", these algorithms can naturally deal with data that are observed sequentially and provide a natural update of the estimators at each new observation .",
    "another advantage compared to classical approaches is that such recursive algorithms will not require to store all the data .",
    "secondly , this work also aims at highlighting the interest of considering the median covariation matrix to perform principal components analysis of high dimensional contaminated data .",
    "different algorithms can be considered to get effective estimators of the mcm .",
    "when the dimension of the data is not too high and the sample size is not too large , weiszfeld s algorithm ( see @xcite and @xcite ) can be directly used to estimate effectively both the geometric median and the median covariation matrix .",
    "when both the dimension and the sample size are large this static algorithm which requires to store all the data may be inappropriate and ineffective .",
    "we show how the algorithm developed by @xcite for the geometric median in hilbert spaces can be adapted to estimate recursively and simultaneously the median as well as the median covariation matrix . then an averaging step ( @xcite ) of the two initial recursive estimators of the median and the mcm permits to improve the accuracy of the initial stochastic gradient algorithms . a simple modification of the stochastic gradient algorithm is proposed in order to ensure that the median covariance estimator is non negative .",
    "we also explain how the eigenelements of the estimator of the mcm can be updated online without being obliged to perform a new spectral decomposition at each new observation .",
    "the paper is organized as follows .",
    "the median covariation matrix as well as the recursive estimators are defined in section 2 . in section 3 ,",
    "almost sure and quadratic mean consistency results are given for variables taking values in general separable hilbert spaces .",
    "the proofs , which are based on new induction steps compared to @xcite , allow to get better convergence rates in quadratic mean even if this new framework is much more complicated because two averaged non linear algorithms are running simultaneously .",
    "one can also note that the techniques generally employed to deal with two time scale robbins monro algorithms ( see @xcite for the multivariate case ) require assumptions on the rest of the taylor expansion and the finite dimension of the data that are too restrictive in our framework . in section 4",
    ", a comparison with some classic robust pca techniques is made on simulated data .",
    "the interest of considering the mcm is also highlighted on the analysis of individual tv audiences , a large sample of high dimensional data which , because of its dimension , can not be analyzed in a reasonable time with classical robust pca approaches .",
    "the main parts of the proofs are described in section 5 .",
    "perspectives for future research are discussed in section 6 .",
    "some technical parts of the proofs as well as a description of weiszfeld s algorithm in our context are gathered in an appendix .",
    "let @xmath1 be a separable hilbert space ( for example @xmath2 or @xmath3 , for some closed interval @xmath4 ) .",
    "we denote by @xmath5 its inner product and by @xmath6 the associated norm .",
    "we consider a random variable @xmath7 that takes values in @xmath1 and define its center @xmath8 as follows : @xmath9 } .",
    "\\label{defmed}\\end{aligned}\\ ] ] the solution @xmath8 is often called the geometric median of @xmath7 .",
    "it is uniquely defined under broad assumptions on the distribution of @xmath7 ( see @xcite ) which can be expressed as follows .",
    "[ eq : supportcdtnmed ] there exist two linearly independent unit vectors @xmath10 , such that @xmath11    if the distribution of @xmath12 is symmetric around zero and if @xmath7 admits a first moment that is finite then the geometric median is equal to the expectation of @xmath7 , @xmath13}$ ] .",
    "note however that the general definition ( [ defmed ] ) does not require to assume that the first order moment of @xmath14 is finite since @xmath15 } | \\leq { \\left\\| u \\right\\|}$ ] .",
    "we now consider the special vector space , denoted by @xmath16 , of @xmath17 matrices when @xmath18 , or for general separable hilbert spaces @xmath1 , the vector space of linear operators mapping @xmath19 . denoting by @xmath20 an orthonormal basis in @xmath1 , the vector space @xmath16 equipped with the following inner product : @xmath21 is also a separable hilbert space . in @xmath22",
    ", we have equivalently @xmath23 where @xmath24 is the transpose matrix of @xmath25 .",
    "the induced norm is the well known frobenius norm ( also called hilbert - schmidt norm ) and is denoted by @xmath26 when @xmath7 has finite second order moments , with expectation @xmath27}=\\mu$ ] , the covariance matrix of @xmath7 , @xmath28}$ ] can be defined as the minimum argument , over all the elements belonging to @xmath16 , of the functional @xmath29 , @xmath30}.\\ ] ] note that in general hilbert spaces with inner product @xmath31 , operator @xmath32 should be understood as the operator @xmath33 .",
    "the mcm is obtained by removing the squares in previous function in order to get a more robust indicator of `` covariation '' . for @xmath34 , define @xmath35 by @xmath36 } .",
    "\\label{def : popriskcov}\\end{aligned}\\ ] ] the median covariation matrix , denoted by @xmath37 , is defined as the minimizer of @xmath38 over all elements @xmath39 .",
    "the second term at the right - hand side of ( [ def : popriskcov ] ) prevents from having to introduce hypotheses on the existence of the moments of @xmath7 . introducing the random variable @xmath40 that takes values in @xmath16 ,",
    "the mcm is unique provided that the support of @xmath41 is not concentrated on a line and assumption 1 can be rephrased as follows in @xmath16 ,    [ eq : supportcdtncov ] there exist two linearly independent unit vectors @xmath42 , such that @xmath43    we can remark that assumption [ eq : supportcdtnmed ] and assumption [ eq : supportcdtncov ] are strongly connected . indeed ,",
    "if assumption [ eq : supportcdtnmed ] holds , then @xmath44 for @xmath45 . consider the rank one matrices @xmath46 and @xmath47 , we have @xmath48 which has a strictly positive variance when the distribution of @xmath7 has no atom .",
    "more generally @xmath49 unless there is a scalar @xmath50 such that @xmath51 = \\mathbb{p}\\left [ \\langle u_1 , x - m \\rangle",
    "= -a\\right ] = \\frac{1}{2}$ ] ( assuming also that @xmath52 = 0 $ ] ) .",
    "furthermore it can be deduced easily that the mcm , which is a geometric median in the particular hilbert spaces of hilbert - schmidt operators , is a robust indicator with a 50% breakdown point ( see @xcite ) and a bounded sensitive gross error ( see @xcite ) .",
    "we also assume that    [ eq : invmomentcov ] there is a constant @xmath53 such that for all @xmath54 and all @xmath39 @xmath55 } \\leq c. \\\\   ( b ) & : \\quad     { { \\mathbb{e}}\\left [ { \\left\\| ( x- h)(x- h)^t - v \\right\\|}^{-2}_f \\right ] } \\leq c. \\\\\\end{aligned}\\ ] ]    this assumption implicitly forces the distribution of @xmath56 to have no atoms .",
    "it is more `` likely '' to be satisfied when the dimension @xmath57 of the data is large ( see @xcite and @xcite for a discussion ) .",
    "note that it could be weakened as in @xcite by allowing points , necessarily different from the mcm @xmath37 , to have strictly positive masses .",
    "considering the particular case @xmath58 , assumption  [ eq : invmomentcov](a ) implies that for all @xmath54 , @xmath59 } \\leq c , \\label{cond : mominv2x}\\end{aligned}\\ ] ] and this is not restrictive when the dimension @xmath57 of @xmath1 is equal or larger than 3 .    under assumption  [ eq : invmomentcov](a )",
    ", the functional @xmath60 is twice frchet differentiable , with gradient @xmath61}. \\label{def : gradv}\\end{aligned}\\ ] ] and hessian operator , @xmath62 , @xmath63}. \\label{def : hev}\\end{aligned}\\ ] ] where @xmath64 , @xmath65 is the identity operator on @xmath16 and @xmath66 for any elements @xmath67 and @xmath68 belonging to @xmath16 .",
    "furthermore , @xmath37 is also defined as the unique zero of the non linear equation : @xmath69 remarking that previous equality can be rewritten as follows , @xmath70}}{{\\mathbb{e}}\\left[\\frac { ( x - m)(x - m)^t } { { \\left\\|   ( x- m)(x - m)^t - \\gamma_m \\right\\|}_f}\\right ] } , \\label{def : baseweiszfled}\\end{aligned}\\ ] ] it is clear that @xmath37 is a bounded , symmetric and non negative operator in @xmath16 .",
    "as stated in proposition  2 of @xcite , operator @xmath37 has an important stability property when the distribution of @xmath7 is symmetric , with finite second moment , _",
    "i.e _ @xmath71 } < \\infty$ ] .",
    "indeed , the covariance operator of @xmath7 , @xmath72}$ ] , which is well defined in this case , and @xmath37 share the same eigenvectors : if @xmath73 is an eigenvector of @xmath74 with corresponding eigenvalue @xmath75 , then @xmath76 , for some non negative value @xmath77 .",
    "this important result means that for gaussian and more generally symmetric distribution ( with finite second order moments ) , the covariance operator and the median covariation operator have the same eigenspaces . note that it is also conjectured in @xcite that the order of the eigenfunctions is also the same .",
    "we suppose now that we have i.i.d .",
    "copies @xmath78 of random variables with the same law as @xmath7 .    for simplicity",
    ", we temporarily suppose that the median @xmath79 of @xmath7 is known .",
    "we consider a sequence of ( learning ) weights @xmath80 , with @xmath81 and @xmath82 and we define the recursive estimation procedure as follows @xmath83 this algorithm can be seen as a particular case of the averaged stochastic gradient algorithm studied in @xcite . indeed",
    ", the first recursive algorithm ( [ def : algormcov ] ) is a stochastic gradient algorithm , @xmath84 } = \\nabla g_m(w_n)\\ ] ] where @xmath85 is the @xmath86-algebra generated by @xmath87 whereas the final estimator @xmath88 is obtained by averaging the past values of the first algorithm .",
    "the averaging step ( see @xcite ) , _ i_.e .",
    "the computation of the arithmetical mean of the past values of a slowly convergent estimator ( see proposition  [ prop : rmvn ] below ) , permits to obtain a new and efficient estimator converging at a parametric rate , with the same asymptotic variance as the empirical risk minimizer ( see theorem  [ theo : asymptnorm ] below ) .    in most of the cases",
    "the value of @xmath79 is unknown so that it also required to estimate the median . to build an estimator of @xmath37",
    ", it is possible to estimate simultaneously @xmath79 and @xmath37 by considering two averaged stochastic gradient algorithms that are running simultaneously . for @xmath89 , @xmath90 where the averaged recursive estimator @xmath91 of the median @xmath79 is controlled by a sequence of descent steps @xmath92 .",
    "the learning rates are generally chosen as follows , @xmath93 , where the tuning constants satisfy @xmath94 $ ] and @xmath95 .",
    "note that by construction , even if @xmath96 is non negative , @xmath97 may not be a non negative matrix when the learning steps do not satisfy @xmath98 projecting @xmath97 onto the closed convex cone of non negative operators would require to compute the eigenvalues of @xmath97 which is time consuming in high dimension even if @xmath97 is a rank one perturbation to @xmath96 ( see @xcite ) .",
    "we consider the following simple approximation to this projection which consists in replacing in ( [ def : gammarm ] ) the descent step @xmath99 by a thresholded one , @xmath100 which ensures that @xmath97 remains non negative when @xmath96 is non negative .",
    "the use of these modified steps and an initialization of the recursive algorithm ( [ def : gammarm ] ) with a non negative matrix ( for example @xmath101 ) ensure that for all @xmath89 , @xmath96 and @xmath102 are non negative .",
    "it is also possible to approximate recursively the @xmath103 eigenvectors ( unique up to sign ) of @xmath37 associated to the @xmath103 largest eigenvalues without being obliged to perform a spectral decomposition of @xmath104 at each new observation .",
    "many recursive strategies can be employed ( see @xcite for a review on various recursive estimation procedures of the eigenelements of a covariance matrix ) .",
    "because of its simplicity and its accuracy , we consider the following one : @xmath105 combined with an orthogonalization by deflation of @xmath106 .",
    "this recursive algorithm is based on ideas developed by @xcite that are related to the power method for extracting eigenvectors .",
    "if we assume that the @xmath103 first eigenvalues @xmath107 are distinct , the estimated eigenvectors @xmath106 , which are uniquely determined up to sign change , tend to @xmath108    once the eigenvectors are computed , it is possible to compute the principal components as well as indices of outlyingness for each new observation ( see @xcite for a review of outliers detection with multivariate approaches ) .      the recursive algorithms ( [ def : gammarm ] ) and ( [ def : gammamedaver ] ) require each @xmath109 elementary operations at each update . with the additional online estimation given in ( [ algo : vectp ] ) of the @xmath103 eigenvectors associated to the @xmath103 largest eigenvalues , @xmath110 additional operations are required .",
    "the orthogonalization procedure only requires @xmath111 elementary operations .",
    "note that the use of classical newton - raphson algorithms for estimating the mcm ( see @xcite ) can not be envisaged for high dimensional data since the computation or the approximation of the hessian matrix would require @xmath112 elementary operations .",
    "the well known and fast weiszfeld s algorithm requires @xmath113 elementary operations for each sample with size @xmath114 .",
    "however , the estimation can not be updated automatically if the data arrive sequentially .",
    "another drawback compared to the recursive algorithms studied in this paper is that all the data must be stored in memory , which is of order @xmath113 elements whereas the recursive technique require an amount of memory of order @xmath109 .",
    "the performances of the recursive algorithms depend on the values of tuning parameters @xmath115 , @xmath116 and @xmath117 . the value of parameter @xmath117 is often chosen to be @xmath118 or @xmath119 .",
    "previous empirical studies ( see @xcite and @xcite ) have shown that , thanks to the averaging step , estimator @xmath120 performs well and is not too sensitive to the choice of @xmath116 , provided that the value of @xmath116 is not too small .",
    "an intuitive explanation could be that here the recursive process is in some sense `` self - normalized '' since the deviations at each iteration in ( [ def : algormcov ] ) have unit norm and finding some universal values for @xmath116 is possible .",
    "usual values for @xmath116 and @xmath115 are in the interval @xmath121 $ ] . when @xmath114 is fixed , this averaged recursive algorithm is about 30 times faster than the weiszfeld s approach ( see @xcite ) .",
    "when @xmath79 is known , @xmath88 can be seen as an averaged stochastic gradient estimator of the geometric median in a particular hilbert space and the asymptotic weak convergence of such estimator has been studied in @xcite .",
    "they have shown that :    ( @xcite , theorem 3.4 ) .",
    "[ theo : asymptnorm ] + if assumptions 1 - 3(a ) hold , then as @xmath114 tends to infinity , @xmath122 where @xmath123 stands for convergence in distribution and @xmath124 is the limiting covariance operator , with @xmath125}.$ ]    as explained in @xcite , the estimator @xmath88 is efficient in the sense that it has the same asymptotic distribution as the empirical risk minimizer related to @xmath38 ( see for the derivation of its asymptotic normality in @xcite in the multivariate case and @xcite in a more general functional framework ) .    using the delta method for weak convergence in hilbert spaces ( see @xcite or @xcite ) , one can deduce , from theorem [ theo : asymptnorm ] , the asymptotic normality of the estimated eigenvectors of @xmath88 .",
    "it can also be proven ( see @xcite ) , under assumptions 1 - 3 , that there is a positive constant @xmath126 such that for all @xmath89 , @xmath127 \\leq \\frac{k}{n}.\\ ] ] note finally that non asymptotic bounds for the deviation of @xmath88 around @xmath37 can be derived readily with the general results given in @xcite .",
    "the more realistic case in which @xmath79 must also be estimated is more complicated because @xmath102 depends on @xmath120 which is also estimated recursively with the same data .",
    "we first state the strong consistency of the estimators @xmath96 and @xmath102 .",
    "[ theops ] if assumptions 1 - 3(b ) hold , we have @xmath128 and @xmath129    the obtention of the rate convergence of the averaged recursive algorithm relies on a fine control of the asymptotic behavior of the robbins - monro algorithms , as stated in the following proposition .",
    "[ theol2l4 ] if assumptions 1 - 3(b ) hold , there is a positive constant @xmath130 , and for all @xmath131 , there is a positive constant @xmath132 such that for all @xmath89 , @xmath133 \\leq \\frac{c'}{n^{\\alpha } } , \\\\   & \\mathbb{e}\\left [ \\left\\| v_{n+1 } -   \\gamma_{m } \\right\\|_{f}^{4}\\right ] \\leq \\frac{c''}{n^{\\beta}}.\\end{aligned}\\ ] ]    the obtention of an upper bound for the rate of convergence at the order four of the robbins - monro algorithm is crucial in the proofs .",
    "furthermore , the following proposition ensures that the exhibited rate in quadratic mean is the optimal one .    under assumptions 1 - 3(b ) , there is a positive constant @xmath134 such that for all @xmath89 , @xmath135 \\geq \\frac{c'}{n^{\\alpha}}. \\end{aligned}\\ ] ] [ prop : rmvn ]    finally , the following theorem is the most important theoretical result of this work .",
    "it shows that , in spite of the fact that it only considers the observed data one by one , the averaged recursive estimation procedure gives an estimator which has a classical parametric @xmath136 rate of convergence in the hilbert - schmidt norm .",
    "[ th : cvgeqm ] under assumptions 1 - 3(b ) , there is a positive constant @xmath137 such that for all @xmath89 , @xmath138 & \\leq \\frac{k'}{n}.\\end{aligned}\\ ] ]    assuming the eigenvalues of @xmath37 are of multiplicity one , it can be deduced from theorem  [ th : cvgeqm ] and lemma 4.3 in @xcite , the convergence in quadratic mean of the eigenvectors of @xmath139 towards the corresponding ( up to sign ) eigenvector of @xmath37 .",
    "a small comparison with other classical robust pca techniques is performed in this section considering data in relatively high dimension but samples with moderate sizes .",
    "this permits to compare our approach with classical robust pca techniques , which are generally not designed to deal with large samples of high dimensional data . in our comparison , we have employed the following well known robust techniques : robust projection pursuit ( see @xcite and @xcite ) , minimum covariance determinant ( mcd , see @xcite ) and spherical pca ( see @xcite ) . the computations were made in the r language ( @xcite ) , with the help of packages ` pcapp ` and ` rrcov ` . for reproductible research , our codes for computing the mcm",
    "have been posted on cran in the ` gmedian ` package .",
    "we will denote by mcm(r ) the recursive estimator @xmath139 defined in ( [ def : gammamedaver ] ) and mcm(r+ ) its non negative modification whose learning weights are defined in ( [ def : gammamodif ] ) .    if the size of the data @xmath140 is not too large , an effective way for estimating @xmath37 is to employ weiszfeld s algorithm ( see @xcite and @xcite as well the supplementary file for a description of the algorithms in our particular situation ) .",
    "the estimate obtained thanks to weiszfeld s algorithm is denoted by mcm(w ) in the following .",
    "note that other optimization algorithms which may be preferred in small dimension ( see @xcite ) have not been considered here since they would require the computation of the hessian matrix , whose size is @xmath141 , and this would lead to much slower algorithms .",
    "note finally that all these alternative algorithms do not admit a natural updating scheme when the data arrive sequentially so that they should be completely ran again at each new observation .",
    "independent realizations of a random variable @xmath142 are drawn , where @xmath143 is a mixture of two distributions and @xmath144 and @xmath145 are independent random variables .",
    "the random vector @xmath7 has a centered gaussian distribution in @xmath146 with covariance matrix @xmath147_{\\ell , j } = \\min ( \\ell , j)/d$ ] and can be thought as a discretized version of a brownian sample path in @xmath148 $ ] .",
    "the multivariate contamination comes from @xmath145 , with different rates of contamination controlled by the bernoulli variable @xmath149 , independent from @xmath7 and @xmath145 , with @xmath150 and @xmath151 .",
    "three different scenarios ( see figure  [ fig : traj ] ) are considered for the distribution of @xmath145 :    * the elements of vector @xmath145 are @xmath57 independent realizations of a student @xmath152 distribution with one degree of freedom .",
    "this means that the first moment of @xmath41 is not defined when @xmath153 . *",
    "the elements of vector @xmath145 are @xmath57 independent realizations of a student @xmath152 distribution with two degrees of freedom .",
    "this means that the second moment of @xmath41 is not defined when @xmath153 . *",
    "the vector @xmath145 is distributed as a `` reverse time '' brownian motion .",
    "it has a gaussian centered distribution , with covariance matrix @xmath154_{\\ell , j } = 2\\min ( d-\\ell , d - j)/d$ ] .",
    "the covariance matrix of @xmath41 is @xmath155 .",
    "trajectories when @xmath156 and @xmath157 for the three different contamination scenarios : student @xmath152 with 1 degree of freedom , student @xmath152 with 2 degrees of freedom and reverse time brownian motion ( from left to right).,width=680,height=340 ]    for the averaged recursive algorithms , we have considered tuning coefficients @xmath158 and a speed rate of @xmath119 .",
    "note that the values of these tuning parameters have not been particularly optimised .",
    "we have noted that the simulation results were very stable , and did not depend much on the value of @xmath116 and @xmath115 for @xmath159 $ ] .",
    "the estimation error of the eigenspaces associated to the largest eigenvalues is evaluated by considering the squared frobenius norm between the associated orthogonal projectors . denoting by @xmath160 the orthogonal projector onto the space generated by the @xmath103 eigenvectors of the covariance matrix @xmath74 associated to the @xmath103 largest eigenvalues and by @xmath161 an estimation , we consider the following loss criterion , @xmath162 \\nonumber \\\\   & = 2 q - 2 \\mbox{tr } \\left [ \\widehat{\\mathbf{p}}_q \\mathbf{p}_q \\right ] .",
    "\\label{def : errvecp } \\end{aligned}\\ ] ] note that we always have @xmath163 and @xmath164 means that the eigenspaces generated by the true and the estimated eigenvectors are orthogonal .      ,",
    "@xmath156 with no contamination ( @xmath165 ) .",
    "mcm(w ) stands for the estimation performed by the weiszfeld s algorithm whereas mcm(r ) denotes the averaged recursive approach and mcm(r+ ) its non negative modification ( see equation [ def : gammamodif]).,width=585,height=377 ]    , @xmath156 and a contamination by a @xmath152 distribution with 2 degrees of freedom with @xmath166 .",
    "mcm(w ) stands for the estimation performed by the weiszfeld s algorithm whereas mcm(r ) denotes the averaged recursive approach and mcm(r+ ) , its non negative modification with learning steps as in ( [ def : gammamodif]).,width=585,height=377 ]    we first compare the performances of the two estimators of the mcm based on the weiszfeld s algorithm and the recursive algorithms ( see ( [ def : gammamedaver ] ) ) with more classical robust pca techniques .",
    "we generated samples of @xmath41 with size @xmath167 and dimension @xmath168 , over 500 replications .",
    "different levels of contamination are considered : @xmath169 . for both dimensions @xmath170 and @xmath171 , the first eigenvalue of the covariance matrix of @xmath7 represents about 81 % of the total variance , and the second one about 9 % .",
    ".median estimation errors , according to criterion @xmath172 with a dimension @xmath173 , for non contaminated samples of size @xmath174 , over 500 monte carlo experiments . [ cols=\"^,^,^,^,^,^,^,^ \" , ]     we can make the following remarks . at first note that even when the level of contamination is small ( 2% and 5% ) , the performances of classical pca are strongly affected by the presence of outlying values in such ( large ) dimensions .",
    "when @xmath156 , the mcd algorithm and the mcm estimation provide the best estimations of the original two dimensional eigenspace , whereas when @xmath57 gets larger ( @xmath175 ) , the mcd estimator can not be used anymore ( by construction ) and the mcm estimators , obtained with weiszfeld s and the non negative recursive algorithm , remain the most accurate .",
    "we can also remark that the recursive mcm algorithms , which are designed to deal with very large samples , performs well even for such moderate sample sizes ( see also figure  [ fig : boxerr ] ) .",
    "the modification of the descent step suggested in ( [ def : gammamodif ] ) , which corresponds to estimator mcm(r+ ) , permits to improve the accuracy the initial mcm estimator , specially when the noise level is not small .",
    "the performances of the spherical pca are slightly less accurate whereas the median error of the robust pp is always the largest among the robust estimators .",
    "when , the contamination is highly structured temporally and the level of contamination is not small ( contamination by a reverse time brownian motion , with @xmath176 ) , the behavior of the mcm is different from the other robust estimators and , with our criterion , it can appear as less effective . however , one can think that we are in presence of two different populations with completely different multivariate correlation structure and the mcd completely ignores that part of the data , which is not necessarily a better behavior .",
    "we now consider an experiment in high dimension , @xmath177 , and evaluate the ability of the recursive algorithms defined in ( [ algo : vectp ] ) to estimate recursively the eigenvectors of @xmath37 associated to the largest eigenvalues .",
    "note that due to the high dimension of the data and limited computation time , we only make comparison of the recursive robust techniques with the classical pca . for this",
    "we generate growing samples and compute , for each sample size the approximation error of the different ( fast ) strategies to the true eigenspace generated by the @xmath103 eigenvectors associated to the @xmath103 largest eigenvalues of @xmath37 .",
    "we have drawn in figure  [ fig : evolr ] , the evolution of the mean ( over 100 replications ) approximation error @xmath178 , for a dimension @xmath179 , as a function of the sample size for samples contaminated by a 2 degrees of freedom student @xmath152 distribution with a rate @xmath180 . an important fact is that the recursive algorithm which approximates recursively the eigenelements behaves very well and we can see nearly no difference between the spectral decomposition of @xmath102 ( denoted by mcm in figure [ fig : evolr ] ) and the estimates produced with the sequential algorithm ( [ algo : vectp ] ) for sample sizes larger than a few hundreds .",
    "we can also note that the error made by the classical pca is always very high and does not decrease with the sample size .    ) ) with @xmath177 and @xmath179 for classical pca , the oracle pca and the recursive mcm estimator with recursive estimation of the eigenelements ( mcm - update ) and with static estimation ( based on the spectral decomposition of @xmath139 ) of the eigenelements ( mcm).,width=491 ]      the last example is a high dimension and large sample case .",
    "individual tv audiences are measured , by the french company mdiamtrie , every minutes for a panel of @xmath181 people over a period of 24 hours , @xmath182 ( see @xcite for a more detailed presentation of the data ) . with a classical pca ,",
    "the first eigenspace represents 24.4% of the total variability , whereas the second one reproduces 13.5% of the total variance , the third one 9.64% and the fourth one 6.79% .",
    "thus , more than 54% of the variability of the data can be captured in a four dimensional space . taking account of the large dimension of the data ,",
    "these values indicate a high temporal correlation .    because of the large dimension of the data , the weiszfeld s algorithm as well as the other robust pca techniques",
    "can not be used anymore in a reasonable time with a personal computer .",
    "the mcm has been computed thanks to the recursive algorithm given in ( [ def : gammamedaver ] ) in approximately 3 minutes on a laptop in the r language ( without any specific c routine ) .        as seen in figure  [ fig1 ] ,",
    "the first two eigenvectors obtained by a classical pca and the robust pca based on the mcm are rather different .",
    "this is confirmed by the relatively large distance between the two corresponding eigenspaces , @xmath183 .",
    "the first robust eigenvector puts the stress on the time period comprised between 1000 minutes and 1200 minutes whereas the first non robust eigenvector focuses , with a smaller intensity , on a larger period of time comprised between 600 and 1200 minutes .",
    "the second robust eigenvector differentiates between people watching tv during the period between 890 and 1050 minutes ( negative value of the second principal component ) and people watching tv between minutes 1090 and 1220 ( positive value of the second principal component ) .",
    "rather surprisingly , the third and fourth eigenvectors of the non robust and robust covariance matrices look quite similar ( see figure  [ fig2 ] ) .",
    "we give in this section the proofs of theorems [ theops ] , [ theol2l4 ] and [ th : cvgeqm ] .",
    "these proofs rely on several technical lemmas whose proofs are given in the supplementary file .",
    "let us recall the robbins - monro algorithm , defined recursively by @xmath184 with @xmath185 .",
    "since @xmath186 , we have @xmath187 = \\nabla g_{\\overline{m}_{n}}(v_{n})$ ] . thus @xmath188 , @xmath189 is a sequence of martingale differences adapted to the filtration @xmath190 . indeed ,",
    "@xmath191 = \\nabla g_{\\overline{m}_{n}}(v_{n } ) - \\mathbb{e}\\left [ u_{n+1}|\\mathcal{f}_{n } \\right ] = 0 $ ] .",
    "the algorithm can be written as follows @xmath192 moreover , it can be considered as a stochastic gradient algorithm because it can be decomposed as follows : @xmath193 with @xmath194 .",
    "finally , linearizing the gradient , @xmath195 with @xmath196    the following lemma gives upper bounds of these remainder terms .",
    "its proof is given in the supplementary file .",
    "[ lem3maj ] under assumptions 1 - 3(b ) , we can bound the three remainder terms .",
    "first ,    @xmath197    in the same way , for all @xmath89 , @xmath198 finally , for all @xmath89 , @xmath199    we deduce from decomposition ( [ decxi ] ) that for all @xmath89 , @xmath200 note that for all @xmath54 and @xmath39 we have @xmath201 .",
    "furthermore , @xmath202 and @xmath203 .",
    "using the fact that @xmath189 is a sequence of martingale differences adapted to the filtration @xmath190 , @xmath204 & \\leq \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2 } -2\\gamma_{n } \\left\\langle v_{n } - \\gamma_{m } , \\nabla_{\\overline{m}_{n}}g \\left ( v_{n } \\right ) - \\nabla_{\\overline{m}_{n}}g\\left ( \\gamma_{m } \\right ) \\right\\rangle_{f } \\\\ & +   28\\gamma_{n}^{2 }   -2\\gamma_{n } \\left\\langle r_{n } , v_{n } - \\gamma_{m } \\right\\rangle_{f}.\\end{aligned}\\ ] ] let @xmath205 , with @xmath206 , we have @xmath207 & \\leq \\left ( 1+\\gamma_{n}\\alpha_{n } \\right ) \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2 } -2\\gamma_{n } \\left\\langle v_{n } - \\gamma_{m } , \\nabla_{\\overline{m}_{n}}g \\left ( v_{n } \\right ) - \\nabla_{\\overline{m}_{n}}g\\left ( \\gamma_{m } \\right ) \\right\\rangle_{f }   \\\\ \\notag & + 28\\gamma_{n}^{2 } + \\frac{\\gamma_{n}}{\\alpha_{n}}\\left\\|   r_{n } \\right\\|_{f}^{2 } .\\end{aligned}\\ ] ]    moreover , applying lemma [ lem3maj ] and theorem 5.1 in @xcite , we get for all positive constant @xmath208 , @xmath209 thus , since @xmath210 , the robbins - siegmund theorem ( see @xcite for instance ) ensures that @xmath211 converges almost surely to a finite random variable and @xmath212 furthermore , by induction , inequality ( [ majvitps ] ) becomes @xmath213 & \\leq   \\left ( \\prod_{k=1}^{\\infty } \\left ( 1 + \\gamma_{k}\\alpha_{k } \\right)\\right ) \\mathbb{e}\\left [ \\left\\| v_{1 } - \\gamma_{m } \\right\\|_{f}^{2 } \\right ] + 28\\left ( \\prod_{k=1}^{\\infty } \\left ( 1 + \\gamma_{k}\\alpha_{k }",
    "\\right ) \\right)\\sum_{k=1}^{\\infty}\\gamma_{k}^{2 } \\\\ &   + \\left ( \\prod_{k=1}^{\\infty } \\left ( 1 + \\gamma_{k}\\alpha_{k } \\right ) \\right )   \\sum_{k=1}^{\\infty } \\frac{\\gamma_{k}}{\\alpha_{k}}\\mathbb{e}\\left [ \\left\\| r_{k } \\right\\|_{f}^{2 } \\right ] .",
    "\\end{aligned}\\ ] ] since @xmath214 , applying theorem 4.2 in @xcite and lemma 6.1 , there is a positive constant @xmath215 such that @xmath216 = c_{0 } \\sum_{k=1}^{\\infty}k^{-\\alpha -1 -\\beta } < + \\infty .\\ ] ] thus , there is a positive constant @xmath0 such that for all @xmath89 , @xmath217 \\leq m$ ] .",
    "since @xmath218 converges almost surely to @xmath79 , one can conclude the proof of the almost sure consistency of @xmath96 with the same arguments as in the proof of theorem 3.1 in @xcite and the convexity properties given in the section b of the supplementary file .",
    "finally , the almost sure consistency of @xmath102 is obtained by a direct application of topelitz s lemma ( see _ e.g. _ lemma 2.2.13 in @xcite ) .",
    "the proof of theorem [ theol2l4 ] relies on properties of the @xmath219-th moments of @xmath220 for all @xmath221 given in the following three lemmas .",
    "these properties enable us , with the application of markov s inequality , to control the probability of the deviations of the robbins monro algorithm from @xmath222 .",
    "[ lemmajordre ] under assumptions 1 - 3(b ) , for all integer @xmath219 , there is a positive constant @xmath223 such that for all @xmath89 , @xmath224 & \\leq m_{p}.\\end{aligned}\\ ] ]    [ lem1 ] under assumptions 1 - 3(b ) , there are positive constants @xmath225 such that for all @xmath89 , @xmath226 & \\leq c_{1}e^{-c_{1}'n^{1-\\alpha } } + \\frac{c_{2}}{n^{\\alpha } }   + c_{3}\\sup_{e ( n/2)+1 \\leq k \\leq n-1}\\mathbb{e}\\left [ \\left\\| v_{k } - \\gamma_{m } \\right\\|^{4}\\right ] , \\end{aligned}\\ ] ] where @xmath227 is the integer part of the real number @xmath228 .",
    "[ lem2 ] under assumptions 1 - 3(b ) , for all integer @xmath229 , there are a rank @xmath230 and positive constants @xmath231 such that for all @xmath232 , @xmath233 & \\leq \\left ( 1-c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p'}}\\right)\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{4}\\right ] + \\frac{c_{1,p'}}{n^{3\\alpha } } + \\frac{c_{2,p'}}{n^{2\\alpha}}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2}\\right ] + \\frac{c_{3,p'}}{n^{3\\alpha -3\\frac{1-\\alpha}{p'}}}.\\end{aligned}\\ ] ]    we can now prove theorem [ theol2l4 ] .",
    "let us choose an integer @xmath234 such that @xmath235 .",
    "thus , @xmath236 , and applying lemma [ lem2 ] , there are positive constants @xmath237 and a rank @xmath230 such that for all @xmath232 , @xmath238 \\leq \\left ( 1-c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p'}}\\right)\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{4}\\right ] + \\frac{c_{1,p'}}{n^{3\\alpha } } + \\frac{c_{2,p'}}{n^{2\\alpha}}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2}\\right ] .\\ ] ]    let us now choose @xmath239 and @xmath234 such that @xmath240 . note that @xmath241 .",
    "one can check that there is a rank @xmath242 such that for all @xmath243 , @xmath244 with the help of a strong induction , we are going to prove the announced results , that is to say that there are positive constants @xmath245 such that @xmath246 and @xmath247 ( with @xmath248 defined in lemma [ lem1 ] ) , such that for all @xmath89 , @xmath135 & \\leq \\frac{c_{p'}}{n^{\\alpha } } , \\\\ \\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{4 } \\right ] & \\leq \\frac{c_{\\beta}}{n^{\\beta } } .\\end{aligned}\\ ] ]    first , let us choose @xmath249 and @xmath132 such that @xmath250 \\right\\rbrace , \\\\ c_{\\beta } &   \\geq \\max_{k \\leq n_{p'}'}\\left\\lbrace k^{\\beta}\\mathbb{e}\\left [ \\left\\| v_{n_{p ' } ' } - \\gamma_{m } \\right\\|_{f}^{4}\\right ] \\right\\rbrace .\\end{aligned}\\ ] ] thus , for all @xmath251 , @xmath252 & \\leq \\frac{c_{p'}}{k^{\\alpha } } , \\\\",
    "\\mathbb{e}\\left [ \\left\\| v_{k } - \\gamma_{m}\\right\\|_{f}^{4 } \\right ] & \\leq \\frac{c_{\\beta}}{k^{\\beta } } .\\end{aligned}\\ ] ] we suppose from now that @xmath243 and that previous inequalities are verified for all @xmath253 . applying lemma [ lemmajordre ] and by induction , @xmath254 & \\leq c_{1}e^{-c_{1}'n^{1-\\alpha } } + \\frac{c_{2}}{n^{\\alpha } } + c_{3}\\sup_{e((n+1)/2 ) + 1 \\leq k \\leq n}\\left\\lbrace\\mathbb{e}\\left [ \\left\\| v_{k } - \\gamma_{m}\\right\\|_{f}^{4}\\right ] \\right\\rbrace \\\\ & \\leq c_{1}e^{-c_{1}'n^{1-\\alpha } } + \\frac{c_{2}}{n^{\\alpha } } + c_{3}\\sup_{e((n+1)/2 ) + 1 \\leq k \\leq n}\\left\\lbrace \\frac{c_{\\beta}}{k^{\\beta } } \\right\\rbrace \\\\ & \\leq c_{1}e^{-c_{1}'n^{1-\\alpha } } + \\frac{c_{2}}{n^{\\alpha } } + c_{3}2^{\\beta}\\frac{c_{\\beta}}{n^{\\beta}}.   \\end{aligned}\\ ] ] since @xmath246 and since @xmath247 , factorizing by @xmath255 , @xmath254 &   \\leq   c_{p'}c_{1}e^{-c_{1}'n^{1-\\alpha } } + c_{p'}2^{-\\alpha -1}\\frac{1}{n^{\\alpha } } + c_{3}2^{\\beta}\\frac{2c_{p'}}{n^{\\beta } } \\\\ & \\leq   \\frac{c_{p}'}{(n+1)^{\\alpha}}(n+1)^{\\alpha}c_{1}e^{-c_{1}'n^{1-\\alpha } } +   2^{-\\alpha}\\left(\\frac{n}{n+1}\\right)^{\\alpha}\\frac{c_{p'}}{2(n+1)^{\\alpha } } + \\frac{c_{3}2^{\\beta + 1}}{(n+1)^{\\beta - \\alpha}}\\frac{c_{p'}}{(n+1)^{\\alpha } } \\\\ & \\leq \\frac{c_{p}'}{(n+1)^{\\alpha}}c_{1}(n+1)^{\\alpha}e^{-c_{1}'n^{1-\\alpha } } +   \\frac{1}{2}\\frac{c_{p'}}{(n+1)^{\\alpha } } + c_{3}2^{\\beta + 1 } \\frac{1}{(n+1)^{\\beta -\\alpha } } \\frac{c_{p'}}{(n+1)^{\\alpha } } \\\\ & \\leq \\left ( ( n+1)^{\\alpha}c_{1}e^{-c_{1}'n^{1-\\alpha } } + \\frac{1}{2 } + c_{3}2^{\\beta + 1}\\frac{1}{(n+1)^{\\beta - \\alpha } } \\right ) \\frac{c_{p'}}{(n+1)^{\\alpha } } .\\end{aligned}\\ ] ] by definition of @xmath256 ,",
    "@xmath257 \\leq \\frac{c_{p'}}{(n+1)^{\\alpha}}.\\ ] ] in the same way , applying lemma [ lem2 ] and by induction , @xmath258   & \\leq \\left ( 1-c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p ' } } \\right ) \\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{4}\\right ] + \\frac{c_{1,p'}}{n^{3\\alpha } } + \\frac{c_{2,p'}}{n^{2\\alpha}}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2}\\right ]   \\\\ & \\leq \\left ( 1-c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p ' } } \\right)\\frac{c_{\\beta}}{n^{\\beta}}+ \\frac{c_{1,p'}}{n^{3\\alpha } } + \\frac{c_{2,p'}}{n^{2\\alpha}}\\frac{c_{p'}}{n^{\\alpha}}.\\end{aligned}\\ ] ] since @xmath259 , factorizing by @xmath260 , @xmath258 & \\leq     \\left ( 1-c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p ' } } \\right)\\frac{c_{\\beta}}{n^{\\beta}}+ \\left ( c_{1,p ' } + c_{2,p'}\\right ) \\frac{c_{\\beta}}{n^{3\\alpha } } \\\\ & \\leq   \\left ( 1-c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p ' } } \\right ) \\left ( \\frac{n+1}{n}\\right)^{\\beta}\\frac{c_{\\beta}}{n^{\\beta } } + 2^{3\\alpha}\\frac{c_{1,p ' } + c_{2,p'}}{(n+1)^{3\\alpha - \\beta}}\\frac{c_{\\beta}}{(n+1)^{\\beta } } \\\\ & \\leq \\left ( \\left ( 1-c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p ' } } \\right ) \\left ( \\frac{n+1}{n}\\right)^{\\beta } + 2^{3\\alpha}\\frac{c_{1,p ' } + c_{2,p'}}{(n+1)^{3\\alpha - \\beta } } \\right ) \\frac{c_{\\beta}}{(n+1)^{\\beta}}.\\end{aligned}\\ ] ] by definition of @xmath256 , @xmath261 \\leq \\frac{c_{\\beta}}{(n+1)^{\\beta}},\\ ] ] which concludes the induction and the proof .      in order to prove theorem [ th : cvgeqm ] , we first recall the following lemma .",
    "[ lemsumg ] let @xmath262 be random variables taking values in a normed vector space such that for all positive constant @xmath103 and for all @xmath263 , @xmath264 < \\infty$ ] .",
    "then , for all real numbers @xmath265 and for all integer @xmath219 , we have @xmath266 \\leq \\left ( \\sum_{k=1}^{n } \\left| a_{k } \\right| \\left ( \\mathbb{e}\\left [ \\left\\| y_{k } \\right\\|^{p } \\right ] \\right)^{\\frac{1}{p } } \\right)^{p}\\ ] ]    we can now prove theorem [ th : cvgeqm ] .",
    "let us rewrite decomposition ( [ decdelta ] ) as follows @xmath267 with @xmath268 . as in @xcite",
    ", we sum these equalities , apply abel s transform and divide by @xmath114 to get @xmath269 we now bound the quadratic mean of each term at the right - hand side of previous equality .",
    "first , we have @xmath270= o \\left ( \\frac{1}{n } \\right)$ ] . applying theorem [ theol2l4 ] ,",
    "@xmath271 & \\leq \\frac{1}{n^{2}}\\frac{c'c_{\\gamma}^{-2}}{n^{-\\alpha } } = o \\left ( \\frac{1}{n}\\right ) .",
    "\\end{aligned}\\ ] ] moreover , since @xmath272 , the application of lemma [ lemsumg ] and theorem  [ theol2l4 ] gives @xmath273 & \\leq \\frac{1}{n^{2}}\\left ( \\sum_{k=2}^{n } \\left| \\gamma_{k}^{-1 } - \\gamma_{k-1}^{-1 } \\right| \\sqrt{\\mathbb{e}\\left [ \\left\\| t_{k } \\right\\|_{f}^{2}\\right ] } \\right)^{2 } \\\\ & \\leq \\frac{1}{n^{2}}4\\alpha^{2}c_{\\gamma}^{-2}c'\\left ( \\sum_{k=2}^{n } \\frac{1}{k^{1-\\alpha /2 } } \\right)^{2 } \\\\ & = o \\left ( \\frac{1}{n^{2-\\alpha}}\\right ) \\\\ & = o \\left ( \\frac{1}{n } \\right ) , \\end{aligned}\\ ] ] since @xmath274 . in the same way , since @xmath275 , applying lemma [ lemsumg ] and theorem  [ theol2l4 ] with @xmath276 , @xmath277 & \\leq \\frac{1}{n^{2}}\\left ( \\sum_{k=1}^{n } \\sqrt{\\mathbb{e}\\left [ \\left\\| \\delta_{k } \\right\\|_{f}^{2}\\right ] } \\right)^{2 } \\\\ & \\leq \\frac{36c^{2}}{n^{2}}\\left ( \\sum_{k=1}^{n } \\sqrt{\\mathbb{e}\\left [ \\left\\| t_{k } \\right\\|_{f}^{4}\\right ] } \\right)^{2 } \\\\ & \\leq \\frac{36c^{2}c_{\\beta}}{n^{2}}\\left ( \\sum_{k=1}^{n } \\frac{1}{k^{\\beta /2 } } \\right)^{2 } \\\\ & = o \\left ( \\frac{1}{n^{\\beta}}\\right ) \\\\ & = o \\left ( \\frac{1}{n}\\right ) , \\end{aligned}\\ ] ] moreover , let @xmath278 .",
    "since @xmath279 , and since there is a positive constant @xmath280 such that for all @xmath281 , @xmath282 \\leq c''n^{-1}$ ] , @xmath283 & \\leq \\frac{1}{n^{2}}\\left ( \\sum_{k=1}^{n } \\sqrt{\\mathbb{e}\\left [ \\left\\| r_{k } \\right\\|_{f}^{2}\\right ] } \\right)^{2 } \\\\ & \\leq \\frac{d^{2}}{n^{2}}\\left ( \\sum_{k=1}^{n } \\sqrt{\\mathbb{e}\\left [ \\left\\| \\overline{m}_{n}-m \\right \\|^{2}\\right ] } \\right ) \\\\ & \\leq \\frac{d^{2}c''}{n^{2}}\\left ( \\sum_{k=1}^{n}\\frac{1}{k^{1/2}}\\right)^{2 } \\\\ & = o \\left ( \\frac{1}{n}\\right ) .\\end{aligned}\\ ] ] since @xmath284 with @xmath285 , cauchy - schwarz s inequality and lemma [ lemsumg ] give @xmath286 & \\leq \\frac{1}{n^{2}}\\left ( \\sum_{k=1}^{n}\\sqrt{\\mathbb{e}\\left [ \\left\\| r_{n } ' \\right\\|_{f}^{2}\\right ] } \\right)^{2 } \\\\ & \\leq \\frac{c_{0}^{2}}{n^{2}}\\left ( \\sum_{k=1}^{n } \\sqrt{\\mathbb{e}\\left [ \\left\\| \\overline{m}_{n } - m \\right\\|^{2}\\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2}\\right]}\\right)^{2 } \\\\ & \\leq \\frac{c_{0}^{2}}{n^{2}}\\left ( \\sum_{k=1}^{n } \\left ( \\mathbb{e}\\left [ \\left\\| \\overline{m}_{n } - m \\right\\|^{4}\\right ] \\right)^{\\frac{1}{4}}\\left ( \\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{4 } \\right ] \\right)^{\\frac{1}{4 } } \\right)^{2 } .\\end{aligned}\\ ] ] applying theorem 4.2 in @xcite and theorem 3.3 , @xmath286 & \\leq \\frac{c_{0}^{2}\\sqrt{c_\\beta}\\sqrt{k_{2}}}{n^{2}}\\left ( \\sum_{k=1}^{n } \\frac{1}{k^{\\beta /4 + 1 /2}}\\right)^{2 } \\\\ & = o \\left ( \\frac{1}{n^{1 + \\beta /2}}\\right ) \\\\ & = o \\left ( \\frac{1}{n}\\right ) , \\end{aligned}\\ ] ] since @xmath287 . finally , one can easily check that @xmath288 \\leq 1 $ ] , and since @xmath189 is a sequence of martingale differences adapted to the filtration @xmath190 , @xmath289 & = \\frac{1}{n^{2}}\\left ( \\sum_{k=1}^{n } \\mathbb{e}\\left [ \\left\\| \\xi_{k+1}\\right\\|_{f}^{2}\\right ] + 2\\sum_{k=1}^{n}\\sum_{k'=k+1}^{n}\\mathbb{e}\\left [ \\left\\langle",
    "\\xi_{k+1},\\xi_{k'+1}\\right\\rangle_{f } \\right ] \\right ) \\\\ & = \\frac{1}{n^{2}}\\left ( \\sum_{k=1}^{n } \\mathbb{e}\\left [ \\left\\| \\xi_{k+1}\\right\\|_{f}^{2}\\right ] + 2\\sum_{k=1}^{n}\\sum_{k'=k+1}^{n}\\mathbb{e}\\left [ \\left\\langle \\xi_{k+1},\\mathbb{e}\\left [ \\xi_{k'+1}\\big| \\mathcal{f}_{k'}\\right ] \\right\\rangle_{f } \\right ] \\right ) \\\\ & = \\frac{1}{n^{2}}\\sum_{k=1}^{n } \\mathbb{e}\\left [ \\left\\| \\xi_{k+1 } \\right\\|_{f}^{2}\\right ] \\\\ & \\leq \\frac{1}{n } .\\end{aligned}\\ ] ] thus , there is a positive constant @xmath126 such that for all @xmath89 , @xmath290 \\leq \\frac{k}{n}.\\ ] ] let @xmath291 be the smallest eigenvalue of @xmath292 .",
    "we have , with proposition b.1 in the supplementary file , that @xmath293 and the announced result is proven , @xmath294 & \\leq \\frac{k}{\\lambda_{\\min}^{2}n}.\\end{aligned}\\ ] ]",
    "the simulation study and the illustration on real data indicate that performing robust principal components analysis via the median covariation matrix , which can bring new information compared to classical pca , is an interesting alternative to more classical robust principal components analysis techniques . the use of recursive algorithms permits to perform robust pca on very large datasets , in which outlying observations may be hard to detect .",
    "another interest of the use of such sequential algorithms is that estimation of the median covariation matrix as well as the principal components can be performed online with automatic update at each new observation and without being obliged to store all the data in memory .",
    "a simple modification of the averaged stochastic gradient algorithm is proposed that ensures non negativeness of the estimated covariation matrices .",
    "this modified algorithms has better performances on our simulated data .",
    "a deeper study of the asymptotic behaviour of the recursive algorithms would certainly deserve further investigations .",
    "proving the asymptotic normality and obtaining the limiting variance of the sequence of estimators @xmath102 when @xmath79 is unknown would be of great interest .",
    "this is a challenging issue that is beyond the scope of the paper and would require to study the joint weak convergence of the two simultaneous recursive averaged estimators of @xmath79 and @xmath37 .",
    "the use of the mcm could be interesting to robustify the estimation in many different statistical models , particularly with functional data .",
    "for example , it could be employed as an alternative to robust functional projection pursuit in robust functional time series prediction or for robust estimation in functional linear regression , with the introduction of the median cross - covariation matrix .",
    "* acknowledgements . *",
    "we thank the company mdiamtrie for allowing us to illustrate our methodologies with their data .",
    "we also thank dr .",
    "peggy cnac for a careful reading of the proofs .",
    "suppose we have a fixed size sample @xmath87 and we want to estimate the geometric median .",
    "the iterative weiszfeld s algorithm relies on the fact that the solution @xmath295 of the following optimization problem @xmath296 satisfies , when @xmath297 , for all @xmath298 @xmath299 where the weights @xmath300 are defined by @xmath301    weiszfeld s algorithm is based on the following iterative scheme .",
    "consider first a pilot estimator @xmath302 of @xmath79 . at step @xmath303 , a new approximation @xmath304 to @xmath79 is given by @xmath305 the iterative procedure is stopped when @xmath306 , for some precision @xmath145 known in advance .",
    "the final value of the algorithm is denoted by @xmath307 .",
    "the estimator of the mcm is computed similarly .",
    "suppose @xmath308 has been calculated at step @xmath303 , then at step @xmath309 , the new approximation @xmath310 to @xmath37 is defined by @xmath311 the procedure is stopped when @xmath312 , for some precision @xmath145 fixed in advance .",
    "note that by construction , this algorithm leads to an estimated median covariation matrix that is always non negative .",
    "in this section , we first give and recall some convexity properties of functional @xmath313 .",
    "the following one gives some information on the spectrum of the hessian of @xmath314 .",
    "[ convexity ] under assumptions 1 - 3(b ) , for all @xmath54 and @xmath39 , @xmath16 admits an orthonormal basis composed of eigenvectors of @xmath315 .",
    "let us denote by @xmath316 the set of eigenvalues of @xmath315 . for all @xmath317 , @xmath318 moreover",
    ", there is a positive constant @xmath319 such that for all @xmath317 , @xmath320 finally , by continuity , there are positive constants @xmath321 such that for all @xmath322 and @xmath323 , and for all @xmath317 , @xmath324    the proof is very similar to the one in @xcite and consequently it is not given here .",
    "furthermore , as in @xcite , it ensures the local strong convexity as shown in the following corollary .",
    "[ corforconv ] under assumptions 1 - 3(b ) , for all positive constant @xmath25 , there is a positive constant @xmath325 such that for all @xmath326 and @xmath327 , @xmath328    finally , the following lemma gives an upper bound on the remainder term in the taylor s expansion of the gradient .",
    "[ lemdelta ] under assumptions 1 - 3(b ) , for all @xmath54 and @xmath39 , @xmath329    let @xmath330 , since + @xmath331 , we have @xmath332 as in the proof of lemma 5.1 in @xcite , under assumptions 1 - 3(b ) , one can check that for all @xmath54 , and @xmath333 $ ] , @xmath334 which concludes the proof .",
    "let us recall that the robbins - monro algorithm is defined recursively by @xmath184 with @xmath185 .",
    "let us remark that @xmath188 , @xmath189 is a sequence of martingale differences adapted to the filtration @xmath190 and the algorithm can be written as follows @xmath193 with @xmath194 . finally , let is consider the following linearization of the gradient , @xmath195 with @xmath196    the bound of @xmath335 is a corollary of lemma [ lemdelta ] .",
    "* bounding @xmath336 *    let us recall that for all @xmath54 , @xmath337 .",
    "we now define for all @xmath54 the random function @xmath338 \\longrightarrow \\mathcal{s}(h)$ ] defined for all @xmath333 $ ] by @xmath339 note that @xmath340   $ ] .",
    "thus , by dominated convergence , @xmath341}\\mathbb{e}\\left [ \\left\\| \\varphi_{\\overline{m}_{n}-m}'(t ) \\right\\|_{f } \\big| \\mathcal{f}_{n } \\right ] .\\ ] ] moreover , one can check that for all @xmath54 , @xmath342 we now bound each term on the right - hand side of previous equality .",
    "first , applying cauchy - schwarz s inequality and using the fact that for all @xmath343 , @xmath344 , @xmath345 & \\leq \\left\\| h \\right\\| \\mathbb{e}\\left [ \\frac{\\left\\| x - m - th \\right\\|}{\\left\\| y(m+th ) - \\gamma_{m } \\right\\|_{f}}\\right ] \\\\ & \\leq \\left\\| h \\right\\|   \\mathbb{e}\\left [ \\frac{\\sqrt{\\left\\| y(m+th ) \\right\\|_{f}}}{\\left\\| y(m+th ) - \\gamma_{m } \\right\\|_{f } } \\right ] \\\\ & \\leq \\left\\| h \\right\\| \\left ( \\mathbb{e}\\left [ \\frac{\\sqrt { \\left\\| \\gamma_{m } \\right\\|_{f}}}{\\left\\| y(m+th ) - \\gamma_{m } \\right\\|_{f } } \\right ] + \\mathbb{e}\\left [ \\frac{1}{\\sqrt{\\left\\| y(m+th ) - \\gamma_{m } \\right\\|_{f } } }   \\right ] \\right ) .",
    "\\end{aligned}\\ ] ] thus , since @xmath346 \\leq c$ ] , @xmath347 \\leq \\left\\| h \\right\\| \\left ( c\\sqrt { \\left\\| \\gamma_{m } \\right\\|_{f } } + \\sqrt{c } \\right ) .\\ ] ] in the same way , @xmath348 \\leq \\left\\| h \\right\\| \\left ( c\\sqrt { \\left\\| \\gamma_{m } \\right\\|_{f } } + \\sqrt{c } \\right ) .\\ ] ] applying cauchy - schwarz s inequality , @xmath349 & \\leq \\mathbb{e}\\left [   \\frac{\\left\\| h\\left ( x - m - th \\right)^{t } \\right\\|_{f}}{\\left\\| y(m+th ) - \\gamma_{m } \\right\\|_{f } } \\right ] \\\\ & \\leq \\left\\| h \\right\\| \\mathbb{e}\\left [ \\frac{\\left\\| x -m -th \\right\\|}{\\left\\| y(m+th ) - \\gamma_{m}\\right\\|_{f}}\\right ] \\\\ & \\leq \\left\\| h \\right\\| \\mathbb{e}\\left [ \\frac{\\sqrt{\\left\\| y(m+th ) \\right\\|_{f } } } { \\left\\| y(m+th ) - \\gamma_{m } \\right\\|_{f}}\\right ] .\\end{aligned}\\ ] ] thus , since @xmath350 \\leq c$ ] , and since for all positive constants @xmath351 , @xmath352 , @xmath353 & \\leq \\left\\| h \\right\\| \\left ( \\mathbb{e}\\left [ \\frac{\\sqrt { \\left\\| \\gamma_{m } \\right\\|_{f}}}{\\left\\| y(m+th ) - \\gamma_{m } \\right\\|_{f } } \\right ] + \\mathbb{e}\\left [ \\frac{1}{\\sqrt{\\left\\| y(m+th ) - \\gamma_{m } \\right\\|_{f } } }   \\right ] \\right ) \\\\ & \\leq \\left\\| h \\right\\| \\left ( c\\sqrt { \\left\\| \\gamma_{m } \\right\\|_{f } } + \\sqrt{c } \\right ) .\\end{aligned}\\ ] ] finally , @xmath354 & \\leq \\left\\| h \\right\\| \\left ( c\\sqrt { \\left\\| \\gamma_{m } \\right\\|_{f } } + \\sqrt{c } \\right ) ,",
    "\\\\ \\label{inequality4}\\mathbb{e}\\left [ \\left| \\left\\langle y(m+th ) - \\gamma_{m } , \\left ( x - m - th \\right)h^{t }   \\right\\rangle_{f}\\right| \\frac{\\left\\| y(m+th ) - \\gamma_{m}\\right\\|_{f}}{\\left\\| y(m+th ) - \\gamma_{m}\\right\\|_{f}^{3 } } \\right ] & \\leq \\left\\| h \\right\\| \\left ( c\\sqrt { \\left\\| \\gamma_{m } \\right\\|_{f } } + \\sqrt{c } \\right ) .\\end{aligned}\\ ] ] applying inequalities ( [ inequality1 ] ) to ( [ inequality4 ] ) with @xmath355 , the announced result is proven , @xmath356    * bounding @xmath357 *    for all @xmath54 and @xmath39 , we define the random function @xmath358 ~\\longrightarrow ~ \\mathcal{s}(h)$ ] such that for all @xmath333 $ ] , @xmath359 note that @xmath360 $ ] . by dominated convergence , @xmath361}\\mathbb{e}\\left [ \\left\\| \\varphi_{\\overline{m}_{n } -m , v_{n } - \\gamma_{m}}'(t)\\right\\|_{f } \\big| \\mathcal{f}_{n } \\right ] .\\ ] ] moreover , as for the bound of @xmath336 , one can check , with an application of cauchy - schwarz s inequality , that for all @xmath54 , @xmath39 , and @xmath333 $ ] , @xmath362 finally , @xmath363 & \\leq \\mathbb{e}\\left [ \\frac{\\left\\| h \\right\\| \\left\\| x - m - th \\right\\|}{\\left\\| y(m+th ) - \\gamma_{m } \\right\\|_{f}^{2}}\\left\\| v \\right\\|_{f } \\right ] \\\\ \\notag & \\leq \\left\\| h \\right\\| \\left\\| v \\right\\|_{f } \\mathbb{e}\\left [ \\frac{\\sqrt{\\left\\|",
    "\\gamma_{m } \\right\\|_{f}}}{\\left\\| y(m+th ) - \\gamma_{m } \\right\\|_{f}^{2 } } \\right ] \\\\",
    "\\notag & + \\left\\| h \\right\\| \\left\\| v \\right\\|_{f}\\mathbb{e}\\left [ \\frac{1}{\\left\\| y(m+th ) - \\gamma_{m } \\right\\|_{f}^{3/2}}\\right ] \\\\ \\label{inequality1 ' } & \\leq \\left ( c\\sqrt{\\left\\| \\gamma_{m}\\right\\|_{f } } + c^{3/4}\\right)\\left\\| h \\right\\| \\left\\| v \\right\\|_{f}. \\end{aligned}\\ ] ] then the announced result follows from an application of inequality ( [ inequality1 ] ) with @xmath364 and @xmath365 , @xmath366",
    "using decomposition ( [ decxi ] ) , @xmath367 note that for all @xmath54 and @xmath39 we have @xmath201 .",
    "moreover , @xmath202 and @xmath203 . since for all @xmath54 , @xmath313 is a convex function , we get with cauchy - schwarz s inequality , @xmath368 let @xmath369 , let us recall that @xmath370 .",
    "we now prove by induction that for all integer @xmath221 , there is a positive constant @xmath223 such that for all @xmath89 , @xmath371 \\leq m_{p}$ ] .",
    "the case @xmath372 has been studied in the proof of theorem 3.2 .",
    "let @xmath373 and suppose from now that for all @xmath374 , there is a positive constant @xmath375 such that for all @xmath89 , @xmath376 \\leq m_{k}.\\ ] ]    * bounding @xmath371 $ ] .",
    "* let us apply inequality ( [ majord2 ] ) , for all @xmath373 and use the fact that @xmath377 is a sequence of martingales differences adapted to the filtration @xmath190 , @xmath378   \\leq \\mathbb{e}\\left [ \\left ( \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2 } + 36\\gamma_{n}^{2 } + 2\\gamma_{n}\\left\\| r_{n } \\right\\|_{f } \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f } \\right)^{p } \\right ] \\\\",
    "+ \\sum_{k=2}^{p } \\binom{p}{k } \\mathbb{e}\\left[\\left ( 2\\gamma_{n } \\left\\langle v_{n } - \\gamma_{m } , \\xi_{n+1 } \\right\\rangle_{f } \\right)^{k } \\left ( \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{2 } + 36 \\gamma_{n}^{2 } + 2\\gamma_{n } \\left\\| r_{n } \\right\\|_{f}\\left\\| v_{n } - \\gamma_{m } \\right\\|_{f } \\right)^{p - k } \\right ] .",
    "\\label{decordp } \\end{gathered}\\ ] ] let us denote by @xmath379 the second term on the right - hand side of inequality ( [ decordp ] ) . applying cauchy - schwarz s inequality and since @xmath203 , @xmath380 \\\\ & \\leq \\sum_{k=2}^{p}\\binom{p}{k}2^{2k}\\gamma_{n}^{k}\\mathbb{e}\\left[\\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{k}\\left ( \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{2 } + 36 \\gamma_{n}^{2 } + 2\\gamma_{n } \\left\\| r_{n } \\right\\|_{f}\\left\\| v_{n } - \\gamma_{m } \\right\\|_{f } \\right)^{p - k}\\right ] .\\end{aligned}\\ ] ] with the help of lemma [ lemtechnique ] , @xmath381 + \\sum_{k=2}^{p}2^{2k}3^{p - k-1}36^{p - k}\\gamma_{n}^{2p - k}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{k}\\right ] \\\\ & + \\sum_{k=2}^{p}2^{p+k}3^{p - k-1}\\gamma_{n}^{p}\\mathbb{e}\\left [ \\left\\| r_{n } \\right\\|_{f}^{p - k}\\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{p } \\right ] .\\end{aligned}\\ ] ] applying cauchy - schwarz s inequality , @xmath382 & = \\sum_{k=2}^{p}2^{2k}3^{p - k-1}\\gamma_{n}^{k } \\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{p-1}\\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{p+1-k}\\right ] \\\\ &",
    "\\leq \\sum_{k=2}^{p}2^{2k}3^{p - k-1}\\gamma_{n}^{k}\\sqrt{\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{2(p-1)}\\right]}\\sqrt{\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{2(p+1-k)}\\right]}. \\\\\\end{aligned}\\ ] ] by induction , @xmath383 & \\leq \\sum_{k=2}^{p}2^{2k}3^{p - k-1}\\gamma_{n}^{k}\\sqrt{m_{p-1}}\\sqrt{m_{p+1-k } } \\\\ & = o \\left ( \\gamma_{n}^{2 } \\right ) .",
    "\\label{eq1}\\end{aligned}\\ ] ] in the same way , applying cauchy - schwarz s inequality and by induction , @xmath384 & = \\sum_{k=2}^{p}2^{2k}3^{p - k-1}36^{p - k}\\gamma_{n}^{2p - k}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}\\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{k-1 } \\right ] \\\\",
    "\\notag & \\leq \\sum_{k=2}^{p}2^{2k}3^{p - k-1}36^{p - k}\\gamma_{n}^{2p - k}\\sqrt{m_{1}}\\sqrt{m_{k-1 } } \\\\ \\label{eq2 } & = o \\left ( \\gamma_{n}^{2 } \\right ) , \\end{aligned}\\ ] ] since @xmath373 .",
    "similarly , since @xmath202 and since @xmath373 , applying cauchy - schwarz s inequality and by induction , @xmath385 & \\leq \\sum_{k=2}^{p}2^{2p}3^{p - k-1}\\gamma_{n}^{p}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{p}\\right ] \\\\",
    "\\notag & \\leq \\sum_{k=2}^{p}2^{2p}3^{p - k-1}\\gamma_{n}^{p}\\sqrt{m_{1}}\\sqrt{m_{p-1 } } \\\\ \\label{eq3 } & = o \\left ( \\gamma_{n}^{2}\\right ) .\\end{aligned}\\ ] ] finally , applying inequalities ( [ eq1 ] ) to ( [ eq3 ] ) , there is a positive constant @xmath386 such that for all @xmath89 , @xmath387 \\leq a_{1}'\\gamma_{n}^{2}.\\ ] ] we now denote by @xmath388 the first term at the right - hand side of inequality ( [ decordp ] ) . with the help of lemma [ lemtechnique ] and applying cauchy - schwarz s inequality",
    ", @xmath389 + \\sum_{k=1}^{p}\\binom{p}{k}\\mathbb{e}\\left [ \\left ( 36\\gamma_{n}^{2 } + 2\\gamma_{n}\\left\\langle r_{n } , v_{n } - \\gamma_{m } \\right\\rangle_{f}\\right)^{k}\\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2p-2k}\\right ] \\\\ & \\leq \\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2p}\\right ] + \\sum_{k=1}^{p}\\binom{p}{k}2^{k-1}\\mathbb{e}\\left [ \\left ( 36^{k}\\gamma_{n}^{2k } + 2^{k}\\gamma_{n}^{k}\\left\\| r_{n } \\right\\|_{f}^{k } \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{k } \\right ) \\left\\| v_{n}-\\gamma_{m}\\right\\|_{f}^{2p-2k } \\right ] .\\end{aligned}\\ ] ] moreover , let @xmath390 \\\\ & = \\sum_{k=1}^{p}\\binom{p}{k}2^{k-1}36^{k}\\gamma_{n}^{2k}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{2p-2k } \\right ] + \\sum_{k=1}^{p}\\binom{p}{k}2^{2k-1}\\gamma_{n}^{k}\\mathbb{e}\\left [ \\left\\| r_{n } \\right\\|_{f}^{k}\\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{2p - k}\\right ] .\\end{aligned}\\ ] ] by induction , @xmath391 & =   \\sum_{k=1}^{p}\\binom{p}{k}2^{k-1}36^{k}\\gamma_{n}^{2k}m_{p - k } \\\\   & = o \\left ( \\gamma_{n}^{2 } \\right ) .\\end{aligned}\\ ] ] moreover , @xmath392   & = \\sum_{k=2}^{p}\\binom{p}{k}2^{2k-1}\\gamma_{n}^{k}\\mathbb{e}\\left [ \\left\\| r_{n } \\right\\|_{f}^{k}\\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{2p - k}\\right ] \\\\ & + 2p\\gamma_{n}\\mathbb{e}\\left [ \\left\\| r_{n } \\right\\|_{f}\\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2p-1}\\right ] .\\end{aligned}\\ ] ] applying cauchy - schwarz s inequality and by induction , since @xmath393 , @xmath394 & \\leq \\sum_{k=2}^{p}\\binom{p}{k}2^{3k-1}\\gamma_{n}^{k}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{2p - k}\\right ] \\\\ & \\leq \\sum_{k=2}^{p}\\binom{p}{k}2^{3k-1}\\gamma_{n}^{k}\\sqrt{m_{p+1-k}}\\sqrt{m_{p-1 } } \\\\ & = o \\left ( \\gamma_{n}^{2 } \\right ) .",
    "\\end{aligned}\\ ] ] moreover , applying theorem 4.2 in @xcite and hlder s inequality , since @xmath395 , @xmath396 & \\leq 2c ' p \\gamma_{n } \\mathbb{e}\\left [ \\left\\| \\overline{m}_{n } - m",
    "\\right\\| \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2p-1}\\right ] \\\\ & \\leq 2c ' p \\gamma_{n } \\left ( \\mathbb{e}\\left [ \\left\\| \\overline{m}_{n } - m \\right\\|^{2p}\\right]\\right)^{\\frac{1}{2p}}\\left",
    "( \\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2p}\\right]\\right)^{\\frac{2p-1}{2p } } \\\\ & \\leq 2c'p\\gamma_{n } \\frac{k_{p}^{\\frac{1}{2p}}}{n^{1/2}}\\left ( \\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2p}\\right]\\right)^{\\frac{2p-1}{2p}}.\\end{aligned}\\ ] ] finally , @xmath397\\right)^{\\frac{2p-1}{2p } } & \\leq 2c'p\\gamma_{n } \\frac{k_{p}^{\\frac{1}{2p}}}{n^{1/2 } } \\max\\left\\lbrace 1 , \\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2p}\\right]\\right\\rbrace \\\\ & \\leq 2c'p\\gamma_{n } \\frac{k_{p}^{\\frac{1}{2p}}}{n^{1/2 } } \\left ( 1 + \\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2p}\\right]\\right ) .\\end{aligned}\\ ] ] thus , there are positive constants @xmath398 such that @xmath399   + a_{1}''\\frac{1}{n^{\\alpha + 1/2 } } .\\ ] ]    finally , thanks to inequalities ( [ maj1 ] ) and ( [ maj2 ] ) , there are positive constants @xmath400 such that @xmath401 & \\leq \\left ( 1+a_{0}'\\frac{1}{n^{\\alpha + 1/2}}\\right)\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2p}\\right ] + a_{1}'\\frac{1}{n^{\\alpha + 1/2 } } \\\\ & \\leq \\prod_{k=1}^{n}\\left ( 1+a_{0}'\\frac{1}{k^{\\alpha + 1/2}}\\right)\\mathbb{e}\\left [ \\left\\| v_{1 } - \\gamma_{m}\\right\\|_{f}^{2p}\\right]+ \\sum_{k=1}^{n}\\prod_{j = k+1}^{n}\\left ( 1+a_{0 } ' \\frac{1}{j^{\\alpha + 1/2}}\\right ) a_{1}'\\frac{1}{k^{\\alpha + 1/2 } } \\\\ & \\leq \\prod_{k=1}^{\\infty}\\left ( 1+a_{0}'\\frac{1}{k^{\\alpha + 1/2}}\\right)\\mathbb{e}\\left [ \\left\\| v_{1 } - \\gamma_{m}\\right\\|_{f}^{2p}\\right]+ \\prod_{j=1}^{\\infty}\\left ( 1+a_{0 } ' \\frac{1}{j^{\\alpha + 1/2}}\\right)\\sum_{k=1}^{\\infty } a_{1}'\\frac{1}{k^{\\alpha + 1/2 } } \\\\ & \\leq m_{p},\\end{aligned}\\ ] ] which concludes the induction and the proof .",
    "let us define the following linear operators : @xmath402 using decomposition ( [ decdelta ] ) and by induction , for all @xmath89 , @xmath403 with @xmath404 we now study the asymptotic behavior of the linear operators @xmath405 and @xmath406 .",
    "as in @xcite , one can check that there are positive constants @xmath407 such that for all integers @xmath408 with @xmath253 , @xmath409 where @xmath410 is the usual spectral norm for linear operators .",
    "we now bound the quadratic mean of each term in decomposition ( [ decbeta ] ) .",
    "* step 1 : the quasi deterministic term @xmath411 . * applying inequality ( [ majbeta ] ) , there is a positive constant @xmath412 such that @xmath413 &   \\leq \\left\\| \\beta_{n-1}\\right\\|_{op}^{2}\\mathbb{e}\\left [ \\left\\| v_{1}-\\gamma_{m}\\right\\|_{f}^{2}\\right ] \\\\",
    "\\notag & \\leq c_{0}e^{-2\\lambda_{\\min}\\sum_{k=1}^{n}\\gamma_{n}}\\mathbb{e}\\left [ \\left\\| v_{1 } - \\gamma_{m } \\right\\|_{f}^{2}\\right ] \\\\ & \\leq c_{0}e^{-c_{0}'n^{1-\\alpha}}\\mathbb{e}\\left [ \\left\\| v_{1}- \\gamma_{m } \\right\\|_{f}^{2 } \\right ] .\\end{aligned}\\ ] ] this term converges exponentially fast to @xmath414 .    * step 2 : the martingale term @xmath415 . * since @xmath189 is a sequence of martingale differences adapted to the filtration @xmath190 , @xmath416 & = \\sum_{k=1}^{n-1}\\mathbb{e}\\left [ \\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\gamma_{k}\\xi_{k+1 } \\right\\|_{f}^{2}\\right ] + 2 \\sum_{k=1}^{n-1}\\sum_{k'=k+1}^{n-1}\\gamma_{k}\\gamma_{k'}\\mathbb{e}\\left [ \\left\\langle \\beta_{n-1}\\beta_{k}^{-1}\\xi_{k+1 } , \\beta_{n-1}\\beta_{k'}^{-1}\\xi_{k'+1 } \\right\\rangle_{f } \\right ] \\\\ & = \\sum_{k=1}^{n-1}\\mathbb{e}\\left [ \\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\gamma_{k}\\xi_{k+1 } \\right\\|_{f}^{2}\\right ] + 2 \\sum_{k=1}^{n-1}\\sum_{k'=k+1}^{n-1}\\gamma_{k}\\gamma_{k'}\\mathbb{e}\\left [ \\left\\langle \\beta_{n-1}\\beta_{k}^{-1}\\xi_{k+1 } , \\beta_{n-1}\\beta_{k'}^{-1}\\mathbb{e}\\left[\\xi_{k'+1}|\\mathcal{f}_{k'}\\right ] \\right\\rangle_{f } \\right ] \\\\ & = \\sum_{k=1}^{n-1}\\mathbb{e}\\left [ \\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\gamma_{k}\\xi_{k+1 } \\right\\|_{f}^{2}\\right ] .\\end{aligned}\\ ] ] moreover , as in @xcite , lemma [ sumexp ] ensures that there is a positive constant @xmath417 such that for all @xmath89 , @xmath418 \\leq \\frac{c_{1}'}{n^{\\alpha}}.\\ ] ]    * step 3 : the first remainder term @xmath419 . * remarking that @xmath420 , @xmath421 & \\leq \\mathbb{e}\\left [ \\left ( \\sum_{k=1}^{n-1 } \\gamma_{k } \\left\\| \\beta_{n-1}\\beta_{k}^{-1 } \\right\\|_{op}\\left\\| r_{k } \\right\\|_{f } \\right)^{2}\\right ] \\\\ & \\leq 16 \\left ( \\sqrt{c } + \\sqrt{\\left\\| \\gamma_{m}\\right\\|_{f}}\\right)^{2 } \\mathbb{e}\\left [ \\left ( \\sum_{k=1}^{n-1 } \\gamma_{k}\\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\right\\|_{op } \\left\\| \\overline{m}_{k}-m \\right\\| \\right)^{2}\\right ] .\\end{aligned}\\ ] ] applying lemma 4.3 and theorem 4.2 in @xcite , @xmath421 & \\leq 16 \\left ( \\sqrt{c } + c\\sqrt{\\left\\| \\gamma_{m}\\right\\|_{f } } \\right)^{2}\\left ( \\sum_{k=1}^{n-1}\\gamma_{k}\\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\right\\|_{op}\\sqrt{\\mathbb{e}\\left [ \\left\\| \\overline{m}_{k } - m \\right\\|^{2}\\right]}\\right)^{2 } \\\\ & \\leq   16 \\left ( \\sqrt{c } + c\\sqrt{\\left\\| \\gamma_{m}\\right\\|_{f } } \\right)^{2}k_{1}\\left ( \\sum_{k=1}^{n-1}\\gamma_{k}\\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\right\\|_{op}\\frac{1}{k^{1/2}}\\right)^{2}.\\end{aligned}\\ ] ] applying inequality ( [ majbeta ] ) , @xmath422 & \\leq   16 \\left ( \\sqrt{c } + c\\sqrt{\\gamma_{m } } \\right)^{2}k_{1}\\left ( \\sum_{k=1}^{n-1}\\gamma_{k}e^{-\\sum_{j = k}^{n}\\gamma_{j}}\\frac{1}{k^{1/2}}\\right)^{2 } \\\\ & \\leq    16 \\left ( \\sqrt{c } + c\\sqrt{\\gamma_{m } } \\right)^{2}k_{1}\\left ( \\sum_{k=1}^{n}\\gamma_{k}e^{-\\sum_{j = k}^{n}\\gamma_{j}}\\frac{1}{k^{1/2}}\\right)^{2}.\\end{aligned}\\ ] ] splitting the sum into two parts and applying lemma [ sumexp ] , we have @xmath422 & \\leq 32 \\left ( \\sqrt{c } + c\\sqrt{\\left\\| \\gamma_{m}\\right\\|_{f } } \\right)^{2}k_{1}\\left ( \\sum_{k=1}^{e ( n/2)}\\gamma_{k}e^{-\\sum_{j = k}^{n}\\gamma_{j}}\\frac{1}{k^{1/2}}\\right)^{2 } \\\\ & + 32 \\left ( \\sqrt{c } + c\\sqrt{\\left\\| \\gamma_{m}\\right\\|_{f } } \\right)^{2}k_{1}\\left ( \\sum_{k = e(n/2 ) + 1}^{n}\\gamma_{k}e^{-\\sum_{j = k}^{n}\\gamma_{j}}\\frac{1}{k^{1/2}}\\right)^{2 } \\\\ & = o \\left ( \\frac{1}{n}\\right ) .\\end{aligned}\\ ] ] thus , there is a positive constant @xmath423 such that for all @xmath89 , @xmath424 \\leq   \\frac{c_{2}'}{n}.\\ ] ]    * step 4 : the second remainder term @xmath425 .",
    "* let us recall that for all @xmath89 , @xmath426 with @xmath427 .",
    "thus , @xmath428 & \\leq \\mathbb{e}\\left [ \\left ( \\sum_{k=1}^{n-1}\\gamma_{k}\\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\right\\|_{op}\\left\\| r_{k } ' \\right\\|_{f } \\right)^{2}\\right ] \\\\ & \\leq 144d^{2}\\mathbb{e}\\left [ \\left ( \\sum_{k=1}^{n-1}\\gamma_{k}\\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\right\\|_{op}\\left\\| \\overline{m}_{k } - m \\right\\| \\left\\| v_{k } - \\gamma_{m}\\right\\|_{f } \\right)^{2}\\right ] .\\end{aligned}\\ ] ] applying lemma 4.3 in @xcite , @xmath429   \\leq 144d^{2}\\left ( \\sum_{k=1}^{n-1}\\gamma_{k}\\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\right\\|_{op}\\sqrt{\\mathbb{e}\\left [ \\left\\| \\overline{m}_{k } - m \\right\\|^{2 } \\left\\|",
    "v_{k } - \\gamma_{m } \\right\\|_{f}^{2 } \\right]}\\right)^{2}.\\ ] ] thanks to lemma 5.2 , there is a positive constant @xmath430 such that for all @xmath89 , @xmath431~\\leq~m_{2}$ ] .",
    "thus , applying cauchy - schwarz s inequality and theorem 4.2 in @xcite , @xmath428 & \\leq 144d^{2}\\left ( \\sum_{k=1}^{n-1}\\gamma_{k}\\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\right\\|_{op}\\left(\\mathbb{e}\\left [ \\left\\| \\overline{m}_{k } - m \\right\\|^{4}\\right]\\right)^{\\frac{1}{4 } } \\left ( \\mathbb{e}\\left [ \\left\\| v_{k } - \\gamma_{m } \\right\\|_{f}^{4 } \\right]\\right)^{\\frac{1}{4}}\\right)^{2 } \\\\ & \\leq 144d^{2}\\sqrt{m_{2}k_{2}}\\left ( \\sum_{k=1}^{n-1}\\gamma_{k}\\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\right\\|_{op } \\frac{1}{k^{1/2 } } \\right)^{2}.   \\end{aligned}\\ ] ] as in step 3 , splitting the sum into two parts , one can check that there is a positive constant @xmath432 such that for all @xmath89 , @xmath433 \\leq \\frac{c_{1}''}{n}.\\ ] ]    * step 5 : the third remainder term : @xmath434 * since @xmath435 , applying lemma 4.3 in @xcite , @xmath436 & \\leq \\mathbb{e}\\left [ \\left ( \\sum_{k=1}^{n-1}\\gamma_{k}\\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\right\\|_{op}\\left\\| \\delta_{k } \\right\\|_{f}\\right)^{2}\\right ] \\\\ & \\leq 36c^{2}\\mathbb{e}\\left [ \\left ( \\sum_{k=1}^{n-1}\\gamma_{k}\\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\right\\|_{op}\\left\\| v_{k } - \\gamma_{m } \\right\\|_{f}^{2}\\right)^{2}\\right ] \\\\ & \\leq 36c^{2 } \\left ( \\sum_{k=1}^{n-1 } \\gamma_{k}\\left\\| \\beta_{n-1}\\beta_{k}^{-1}\\right\\|_{op}\\sqrt{\\mathbb{e}\\left [ \\left\\| v_{k } - \\gamma_{m}\\right\\|_{f}^{4}\\right ] } \\right)^{2 } .\\end{aligned}\\ ] ] thanks to lemma 5.2 , there is a positive constant @xmath430 such that for all @xmath89 , @xmath437~\\leq~m_{2}$ ] .",
    "thus , splitting the sum into two parts and applying inequalities ( [ majbeta ] ) and lemma [ sumexp ] , there are positive constant @xmath438 such that for all @xmath89 , @xmath436 & \\leq 72c^{2}m_{2}^{2}\\left ( \\sum_{k=1}^{e(n/2)}\\gamma_{k}e^{-\\sum_{j = k}^{n}\\gamma_{j } } \\right)^{2 } \\\\ & + 72c^{2}\\sup_{e(n/2)+1 \\leq k \\leq n-1 } \\left\\lbrace \\mathbb{e}\\left [ \\left\\| v_{k } - \\gamma_{m}\\right\\|_{f}^{4}\\right]\\right\\rbrace \\left ( \\sum_{k = e(n/2)+1}^{n}\\gamma_{k}e^{-\\sum_{j = k}^{n}\\gamma_{j } } \\right)^{2 } \\\\ & \\leq    c_{2 } ' \\sup_{e(n/2)+1 \\leq k \\leq n-1}\\left\\lbrace \\mathbb{e}\\left [ \\left\\| v_{k } - \\gamma_{m}\\right\\|_{f}^{4}\\right]\\right\\rbrace   + o \\left ( e^{-2c_{0}'n^{1-\\alpha } } \\right )   .\\end{aligned}\\ ] ] thus , there is a positive constant @xmath439 such that for all @xmath440 , @xmath441 \\leq c_{0}'e^{-2c_{0}'n^{1-\\alpha } } + c_{2}'\\sup_{e(n/2)+1 \\leq k \\leq",
    "n-1}\\left\\lbrace \\mathbb{e}\\left [ \\left\\| v_{k } - \\gamma_{m}\\right\\|_{f}^{4}\\right]\\right\\rbrace .\\ ] ]    * conclusion : * applying lemma [ lemtechnique ] and decomposition ( [ decbeta ] ) , for all @xmath89 , @xmath135   & \\leq 5   \\mathbb{e}\\left [ \\left\\| \\beta_{n-1}\\left ( v_{1}-\\gamma_{m}\\right)\\right\\|_{f}^{2}\\right ] + 5 \\mathbb{e}\\left [ \\left\\| \\beta_{n-1}m_{n}\\right\\|_{f}^{2}\\right ] + 5 \\mathbb{e}\\left [ \\left\\| \\beta_{n-1}r_{n } \\right\\|_{f}^{2}\\right]\\\\ &   + 5\\mathbb{e}\\left [ \\left\\| \\beta_{n-1}r_{n } ' \\right\\|_{f}^{2}\\right ] + 5 \\mathbb{e}\\left [ \\left\\| \\beta_{n-1}\\delta_{n}\\right\\|_{f}^{2}\\right ]   .\\end{aligned}\\ ] ] applying inequalities ( [ eqdec1 ] ) to ( [ eqdec5 ] ) , there are positive constants @xmath225 such that for all @xmath89 , @xmath442 \\leq c_{1}e^{-c_{1}'n^{1-\\alpha } } + \\frac{c_{2}}{n^{\\alpha } }   + c_{3}\\sup_{e(n/2)+1 \\leq k \\leq",
    "n-1}\\mathbb{e}\\left [ \\left\\| v_{k } - \\gamma_{m } \\right\\|_{f}^{4}\\right ] .\\ ] ]    let us define @xmath443 and use decomposition ( [ decxi ] ) , @xmath444 since @xmath445 , @xmath202 and the fact that for all @xmath54 , @xmath39 , @xmath446 , we get with an application of cauchy - schwarz s inequality @xmath447 thus , since @xmath189 is a sequence of martingale differences adapted to the filtration @xmath190 , and since @xmath448 ( this inequality follows from proposition [ convexity ] and from the fact that for all @xmath54 , @xmath313 is a convex application ) , @xmath233 & \\leq \\mathbb{e}\\left [ \\left\\| w_{n } \\right\\|_{f}^{4}\\right ] + 2\\gamma_{n}\\mathbb{e}\\left [ \\left\\| r_{n } \\right\\|_{f } \\left\\| w_{n } \\right\\|_{f}^{2}\\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}\\right ]   \\\\ & + 40 \\left ( 1+c^{2}c_{\\gamma}^{2}\\right)\\gamma_{n}^{2}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2}\\right ] \\\\ &   + 4\\gamma_{n}^{2}\\mathbb{e}\\left [ \\left\\langle \\xi_{n+1 } , v_{n } - \\gamma_{m}\\right\\rangle_{f}^{2}\\right ] + 400\\gamma_{n}^{4 } + 40\\gamma_{n}^{3}\\mathbb{e}\\left [ \\left\\| r_{n } \\right\\|_{f } \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2 } \\right ] \\\\ & + 4\\gamma_{n}^{2}\\mathbb{e}\\left [ \\left\\| r_{n } \\right\\|_{f}^{2}\\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2}\\right ] .\\end{aligned}\\ ] ] since @xmath445 and @xmath202 , applying cauchy - schwarz s inequality , there are positive constants @xmath449 such that for all @xmath281 , @xmath450 \\leq \\mathbb{e}\\left [ \\left\\| w_{n } \\right\\|_{f}^{4}\\right ] + 2\\gamma_{n}\\mathbb{e}\\left [ \\left\\| r_{n } \\right\\|_{f } \\left\\| w_{n } \\right\\|_{f}^{2}\\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}\\right ] + \\frac{c_{1}'}{n^{3\\alpha}}+ \\frac{c_{2}'}{n^{2\\alpha}}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2}\\right].\\ ] ] we now bound the two first terms at the right - hand side of inequality ( [ majoord4 ] ) .    * step 1 : bounding @xmath451 $ ] . * since @xmath452 , applying proposition [ convexity ] , one can check that @xmath453 since for all @xmath54 , @xmath313 is a convex application , @xmath454 .",
    "let @xmath234 be a positive integer .",
    "we now introduce the sequence of events @xmath455 defined for all @xmath89 by @xmath456 with @xmath145 defined in proposition [ convexity ] . for the sake of simplicity",
    ", we consider that @xmath457 defined in proposition [ convexity ] verifies @xmath458 .",
    "applying proposition [ convexity ] , let @xmath459 in the same way , since @xmath460 is convex , let @xmath461 applying proposition [ convexity ] , @xmath462 there is a rank @xmath256 such that for all @xmath243 , we have @xmath463 .",
    "thus , applying inequalities ( [ majbn ] ) and ( [ majcn ] ) , for all @xmath243 , @xmath464 thus , there are a positive constant @xmath465 and a rank @xmath230 such that for all @xmath232 , @xmath466   & \\leq \\left ( 1- \\frac{\\epsilon ' c_{m } } { 2}\\gamma_{n}n^{-\\frac{1-\\alpha}{p'}}\\right)^{2 } \\mathbb{e}\\left [   \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{4}\\mathbf{1}_{a_{n , p'}}\\right ] \\\\ & \\leq \\left ( 1- 2c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p'}}\\right ) \\mathbb{e}\\left",
    "[ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{4}\\right ] .\\end{aligned}\\ ] ] now , we must get an upper bound for @xmath467 $ ] . since @xmath468 and since there is a positive constant @xmath469 such that for all @xmath89 , @xmath470 we have @xmath471 & \\leq \\left ( 1+c_{\\gamma}^{2}c^{2}\\right)^{2}\\mathbb{e}\\left [ \\left\\|",
    "v_{n } - \\gamma_{m}\\right\\|_{f}^{4 } \\mathbf{1}_{a_{n , p'}^{c}}\\right ] \\\\   & \\leq \\left ( 1+c_{\\gamma}^{2}c^{2}\\right)^{2}c_{0}^{4}n^{4 - 4\\alpha}\\mathbb{p}\\left [ a_{n , p'}^{c}\\right ] \\\\ & \\leq \\left ( 1+c_{\\gamma}^{2}c^{2}\\right)^{2}c_{0}^{4}n^{4 - 4\\alpha } \\left ( \\mathbb{p}\\left [ \\left\\| \\overline{m}_{n } - m \\right\\| \\geq \\epsilon \\right ] + \\mathbb{p}\\left",
    "[ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f } \\geq n^{\\frac{1-\\alpha}{p'}}\\right ] \\right ) .\\end{aligned}\\ ] ] applying markov s inequality , theorem 4.2 in @xcite and lemma 5.2 , @xmath471 & \\leq \\left ( 1+c_{\\gamma}^{2}c^{2}\\right)^{2}c_{0}^{4}n^{4 - 4\\alpha } \\left ( \\frac{\\mathbb{e}\\left [ \\left\\| \\overline{m}_{n}-m \\right\\|^{2p''}\\right]}{\\epsilon^{2p '' } } + \\frac{\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2q}\\right]}{n^{2q\\frac{1-\\alpha}{p ' } } } \\right ) \\\\ & \\leq \\frac{k_{p''}}{\\epsilon^{2p''}}\\left ( 1+c_{\\gamma}^{2}c^{2}\\right)^{2}c_{0}^{4}n^{4 - 4\\alpha - p '' } + \\left ( 1+c_{\\gamma}^{2}c^{2}\\right)^{2}c_{0}^{4}m_{q}n^{4 - 4\\alpha - 2q \\frac{1-\\alpha}{p'}}. \\end{aligned}\\ ] ] taking @xmath472 and @xmath473 , @xmath474 = o \\left ( \\frac{1}{n^{3\\alpha}}\\right ) .\\ ] ] thus , applying inequalities ( [ majwnan ] ) and ( [ majwnanc ] ) , there are positive constants @xmath465 , @xmath475 and a rank @xmath230 such that for all @xmath232 , @xmath476 \\leq \\left ( 1- 2c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p'}}\\right ) \\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{4}\\right ] + \\frac{c_{1,p'}}{n^{3\\alpha}}.\\ ] ]    * step 2 : bounding @xmath477 $ ] . * since @xmath468",
    ", applying lemma [ lemtechnique ] , let @xmath478 \\\\",
    "& \\leq 2\\left ( 1+c_{\\gamma}^{2}c^{2}\\right ) \\gamma_{n } \\mathbb{e}\\left [ \\left\\| r_{n } \\right\\|_{f}\\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{3}\\right ] \\\\ & \\leq \\frac{2}{c_{p'}}\\left ( 1+c_{\\gamma}^{2}c^{2}\\right)^{2}\\gamma_{n}n^{\\frac{1-\\alpha}{p'}}\\mathbb{e}\\left [ \\left\\| r_{n } \\right\\|_{f}^{2}\\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2}\\right ] + \\frac{1}{2}c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p'}}\\mathbb{e}\\left",
    "[ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{4}\\right ] \\\\ & \\leq \\frac{2}{c_{p'}^{2}}\\left ( 1+c_{\\gamma}^{2}c^{2}\\right)^{4}\\gamma_{n}n^{3\\frac{1-\\alpha}{p'}}\\mathbb{e}\\left [ \\left\\| r_{n } \\right\\|_{f}^{4}\\right ] + c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p'}}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{4}\\right ] .\\end{aligned}\\ ] ] since @xmath479 and applying theorem 4.2 in @xcite , @xmath480 + c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p'}}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{4}\\right ] \\\\",
    "\\notag & \\leq \\frac{2}{c_{p'}^{2}}k_{2}\\left ( 1+c_{\\gamma}^{2}c^{2}\\right)^{4}\\left ( \\sqrt{c } + c\\sqrt{\\left\\| \\gamma_{m}\\right\\|_{f}}\\right)^{4}\\gamma_{n}n^{3\\frac{1-\\alpha}{p'}}\\frac{1}{n^{2 } } + c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p'}}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{4}\\right ] \\\\ & = c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p'}}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{4}\\right ] + o \\left ( \\frac{1}{n^{2 + \\alpha -3(1-\\alpha)/p'}}\\right ) .\\end{aligned}\\ ] ]    * step 3 : conclusion . * applying inequalities ( [ majoord4 ] ) , ( [ majwn ] ) and ( [ majdn ] ) , there are a rank @xmath230 and positive constants @xmath481 such that for all @xmath232 , @xmath238 \\leq \\left ( 1- c_{p'}\\gamma_{n}n^{-\\frac{1-\\alpha}{p'}}\\right)\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m } \\right\\|_{f}^{4}\\right ] + \\frac{c_{1,p'}}{n^{3\\alpha } } + \\frac{c_{2,p'}}{n^{2\\alpha}}\\mathbb{e}\\left [ \\left\\| v_{n } - \\gamma_{m}\\right\\|_{f}^{2}\\right ] + \\frac{c_{3,p'}}{n^{2+\\alpha -3\\frac{1-\\alpha}{p'}}}.\\ ] ]",
    "first , the following lemma recalls some well - known inequalities .",
    "[ sumexp ] let @xmath487 be non - negative constants such that @xmath488 , and @xmath489 , @xmath490 be two sequences defined for all @xmath89 by @xmath491 with @xmath492 .",
    "thus , there is a positive constant @xmath469 such that for all @xmath89 , @xmath493 where @xmath494 is the integer part function .",
    "we first prove inequality ( [ sumexp1 ] ) . for all @xmath89 , @xmath495 moreover , for all @xmath496 , @xmath497 thus , @xmath498 we now prove inequality ( [ sumexp2 ] ) . with the help of an integral test for convergence , @xmath499",
    "thus , @xmath500 with the help of an integral test for convergence , there is a rank @xmath501 ( for sake of simplicity , we consider that @xmath502 ) such that for all @xmath503 , @xmath504_{e(n/2)+1}^{n } + \\beta\\int_{e(n/2)+1}^{n}e^{t^{1-\\alpha}}t^{-1-\\beta}dt \\\\ & = e^{(n+1)^{1-\\alpha}(n+1)^{-\\beta } } + o \\left ( \\int_{e(n/2)+1}^{n+1}e^{t^{1-\\alpha}}t^{-\\alpha - \\beta } dt \\right ) , \\end{aligned}\\ ] ] since @xmath274 .",
    "thus , @xmath505 as a conclusion , we have @xmath506        cardot , h. , cnac , p. , and chaouch , m. ( 2010 ) .",
    "stochastic approximation to the multivariate and the functional median . in lechevallier , y. and saporta , g. , editors , _ compstat 2010 _ , pages 421428 .",
    "physica verlag , springer .",
    "kemperman , j. h.  b. ( 1987 ) .",
    "the median of a finite measure on a banach space . in _ statistical data analysis based on the @xmath511-norm and related methods ( neuchtel , 1987 ) _ , pages 217230 .",
    "north - holland , amsterdam .",
    "mttnen , j. , nordhausen , k. , and oja , h. ( 2010 ) .",
    "asymptotic theory of the spatial median . in",
    "_ nonparametrics and robustness in modern statistical inference and time series analysis : a festschrift in honor of professor jana jureckov _ , volume  7 , pages 182193 .",
    "ims collection ."
  ],
  "abstract_text": [
    "<S> the geometric median covariation matrix is a robust multivariate indicator of dispersion which can be extended without any difficulty to functional data . </S>",
    "<S> we define estimators , based on recursive algorithms , that can be simply updated at each new observation and are able to deal rapidly with large samples of high dimensional data without being obliged to store all the data in memory . </S>",
    "<S> asymptotic convergence properties of the recursive algorithms are studied under weak conditions . </S>",
    "<S> the computation of the principal components can also be performed online and this approach can be useful for online outlier detection . </S>",
    "<S> a simulation study clearly shows that this robust indicator is a competitive alternative to minimum covariance determinant when the dimension of the data is small and robust principal components analysis based on projection pursuit and spherical projections for high dimension data . </S>",
    "<S> an illustration on a large sample and high dimensional dataset consisting of individual tv audiences measured at a minute scale over a period of 24 hours confirms the interest of considering the robust principal components analysis based on the median covariation matrix . all studied algorithms are available in the r package ` gmedian ` on cran </S>",
    "<S> .    * keywords . * </S>",
    "<S> averaging , functional data , geometric median , online algorithms , online principal components , recursive robust estimation , stochastic gradient , weiszfeld s algorithm . </S>"
  ]
}