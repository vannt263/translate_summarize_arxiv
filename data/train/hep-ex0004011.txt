{
  "article_text": [
    "the progress of experimental sciences to a large extent is due to their practice to assign uncertainties to results .",
    "the information contained in a measurement , or a parameter deduced from it , is incompletely documented and more or less useless unless some kind of error is attributed to the data .",
    "the precision of measurements has to be known i ) to combine data from different experiments , ii ) to deduce secondary parameters from it and iii ) to test predictions of theories .",
    "different statistical methods have to be judged on their ability to fulfill these tasks .",
    "narsky @xcite who compares several different approaches to the estimation of upper poisson limits , states : `` there is no such thing as the best procedure for upper limit estimation .",
    "an experimentalist is free to choose any procedure she / he likes , based on her / his belief and experience .",
    "the only requirement is that the chosen procedure must have a strict mathematical foundation . ''",
    "this opinion is typical for many papers on confidence limits .",
    "however , `` the real test of the pudding is in its eating '' and not in contemplating the beauty of the cooking recipe .",
    "we should not forget that what we measure has practical implications .    in this paper , the emphasis",
    "is put on performance and not on the mathematical and statistical foundation .",
    "the intention is to confront the procedures with the problems to be solved in physics .",
    "simple transparent examples are selected .",
    "important properties are among others consistency , precision , universality , simplicity and objectivity .",
    "consistency is indispensable in any case .",
    "w. f. edwards writes @xcite : `` relative support ( of a hypothesis or a parameter ) must be consistent in different applications , so that we are content to react equally to equal values , and it must not be affected by information judged intuitively to be irrelevant . ''    part of the content of this article has been presented in a comment @xcite to the unified approach @xcite .",
    "classical confidence limits ( ccl ) are based on tail probabilities .",
    "the defining property is _ coverage _ : if a large number of experiments perform measurements of a parameter with confidence level @xmath0 , the fraction @xmath0 of the limits will contain the true value of the parameter inside the confidence limits .",
    "d2limit0.eps    we illustrate the concept of ccl for a measurement ( statistic ) consisting of a two - dimensional observation ( @xmath1 ) and a two dimensional parameter space ( see fig .",
    "1 ) . in a first step we associate to each point",
    "@xmath2 in the parameter space a closed _ probability contour _ in the sample space containing a measurement with probability @xmath0 .",
    "for example , the probability contour labeled @xmath3 in the sample space corresponds to the parameter values of point @xmath4 in the parameter space .",
    "the curve ( _ confidence contour _ ) connecting all points in the parameter space with probability contours in the sample space passing through the actual measurement @xmath1 encloses the _ confidence region _ of confidence level @xmath0 .",
    "figure 1 demonstrates some of the requirements necessary for the construction of an exact confidence region : 1 .",
    "the sample space must be continuous .",
    "( discrete distributions and thus all digital measurements and in principle also poisson processes are excluded . )",
    "the probability contours should enclose a simply connected region .",
    "3 . the parameter space has to be continuous .",
    "the restriction ( 1 ) usually is overcome by relaxing the requirement of exact coverage and by requiring minimum overcoverage .",
    "this is not an elegant solution .",
    "there is considerable freedom in the choice of the probability contours but to insure coverage they have to be defined independently of the result of the experiment .",
    "usually , contours are locations of constant probability density . in one dimension also",
    "central intervals and intervals leading to minimum sized confidence intervals are popular . clearly , there is a lack of standardization .",
    "the unified approach @xcite defines the probability regions through the likelihood ratio .",
    "likelihood intervals enclose a region where the likelihood function decreases by a fixed ratio , equal to @xmath5 for one standard deviation and @xmath6 for two standard deviations etc ..    fig2.eps    bayesians integrate the normalized likelihood function and form either probability regions or moments to define the limits .",
    "i will discuss only uniform prior densities .",
    "this does not restrict the freedom of the scientist because there is the equivalent possibility to choose the parameter .",
    "for example an analysis using the mean life parameter with the prior @xmath7 is equivalent to an analysis of the decay constant @xmath8 with uniform prior .",
    "assume we have two hypothesis characterized by the parameters @xmath9 and @xmath10 . for a measurement @xmath11 the relative support to the two hypothesis",
    "is given by the likelihood ratio @xmath12 another measurement @xmath13 is equivalent to @xmath11 if the likelihood ratios are the same : @xmath14    when we have more than two hypothesis we require that equivalent date provide the same likelihood ratio for all combinations of parameters .",
    "consequently , for a pdf depending on a continuous parameter @xmath15 we have to require that the likelihood functions for the two measurements are proportional to each other .",
    "these considerations correspond to the likelihood principle ( lp ) : the likelihood function contains the full information relative to the parameter .",
    "inference should be based on the likelihood function only .",
    "the lp is due to fisher , birnbaum and others .",
    "proofs and discussions can be found in refs .",
    "@xcite .",
    "methods that provide different results for measurement that have proportional likelihood functions are inconsistent .",
    "[ ptb ]    fgerrors.eps    a physical quantity like the mass of a particle with a resolution following normal distributions is constrained to positive values .",
    "3 shows typical central confidence bounds which extend into the unphysical region . in extreme cases a measurement may produce a 90 % confidence interval which does not cover positive values at all . the unified approach and the bayesian method",
    "avoid unphysical confidence limits .",
    "fig3a.eps      the prescription for the construction of the probability intervals according to the likelihood ratio ordering leads to disconnected interval regions when the pdf has tails and can not produce confidence intervals .",
    "this is shown in fig .",
    "4 top .",
    "the same difficulty arises for the breit - wigner distribution ( see fig",
    ". 4 bottom ) .",
    "the problem is absent if the pdf @xmath16 fulfills the condition @xmath17 .",
    "this condition restricts the application of the unified approach to pdfs similar to gaussians .",
    "let us assume that we have a gaussian resolution in @xmath18 and a physical boundary in @xmath19 ( fig .",
    "the probability contours are deformed in the unified approach as indicated in the sketch . as a consequence",
    "the error in @xmath20 shrinks due to a boundary in @xmath19 even though the two parameters are independent .",
    "one has to be careful in the interpretation of two - dimensional confidence limits as they occur for example in neutrino oscillation experiments .",
    "uc2dim.eps      this is a frequent distribution in particle physics .",
    "a linear distribution is always restricted in the sample and the parameter space to avoid negative probabilities .",
    "we choose @xmath21    as is realized in many asymmetry distributions . for a sample of 100 events following the distribution of equ .",
    "2.4 , a likelihood analysis gives a best value for the slope parameter of @xmath22 ( see figure 6 ) .",
    "there is no simple statistic allowing to compute central classical @xmath23 confidence limits because the parameter is undefined outside the interval [ 1,1 ] .",
    "contrary to the conventional classical approach , the unified approach is able to handle the problem by working in the full sample space ( hundred dimensional in our case ) this requires a considerable computing effort .",
    "likelihood limits are possible - the upper limit would coincide with the boundary - but not well suited to measure the precision .",
    "fig5.eps      a particle track is passing at the unknown position @xmath24 through a proportional wire chamber . the measured coordinate @xmath25 is set equal to the wire location  @xmath26 .",
    "the probability density for a measurement @xmath25@xmath27 is independent of the true location @xmath24 .",
    "thus it is impossible to define a sensible classical confidence or likelihood interval , except a trivial one with full overcoverage .",
    "this difficulty is common to all digital measurements because they violate condition 1 of section 2.1 .",
    "thus a large class of measurements is not handled in classical statistics .",
    "a bayesian treatment with uniform prior is the common solution .",
    "it provides the r.m.s .",
    "error @xmath28 .",
    "[ ptb ]    gnonphys.eps    a particle passes through a small scintillator and another position sensitive detector with gaussian resolution .",
    "both boundaries of the classical error interval are in the region forbidden by the scintillator signal .",
    "( see fig .",
    "7 ) the classical error is twice as large as the r.m.s .",
    "it is meaningless .",
    "the unified classical and the likelihood limits contain the full physical region and thus are useless .",
    "again only the bayesian method gives reasonable results .",
    "a theory , depending on the unknown parameter @xmath29 predicts the gaussian probability density @xmath30 for the time @xmath31 of an earthquake . the classical confidence interval for a measurement at @xmath32 h is @xmath33 .",
    "it is shown together with the likelihood function in fig .",
    "when we look at the two distinct parameter values , predicting the time of an earthquake    earthq.eps    [ cols= \" < , < \" , ]     we realize that the first is excluded by the classical bounds , the second by the likelihood limits . the fig .",
    "8b shows the two probability densities together with the measurement .",
    "clearly , we would rather accept @xmath34 .",
    "this choice is also supported by the likelihood ratio which is in favor of h@xmath35 by a factor 26 .",
    "thus the likelihood limits are intuitively more acceptable than the classical ones .",
    "the preceding example shows that the concept of classical confidence limits for continuous parameters is not compatible with methods based on the likelihood values . we may construct a transition from the discrete case to the continuous one by adding more and more hypothesis but a transition from likelihood based methods to ccl is impossible .",
    "the two classical approaches ccl and neyman - pearson test lack a common bases .",
    "this example was presented by cousins @xcite : markii had measured the number of neutrinos to be @xmath36 and deduced a 95% confidence upper limit of 3.9 excluding 4 neutrino generations .",
    "the likelihood ratio of 7.0 produces a much weaker exclusion of the discrete hypothesis .",
    "a rate measurement may be stopped for reasons like : i ) there are enough events .",
    "ii ) for a long time no event has been observed .",
    "iii ) a `` golden '' event was recorded .",
    "timegap1.eps    these actions do not introduce a bias as has been first realized by barnard and co - workers @xcite .",
    "the reason is that the likelihood function is independent of the stopping rule .",
    "this may be visualized by an infinitely long measurement which is cut in pieces each corresponding to a experiment stopped by the same rule .",
    "the individual experiment can not be biased since the full chain is unbiased .",
    "this is illustrated in fig .",
    "9 where the experiments are stopped whenever 3 events are recorded in a short time interval .",
    "grafik1.eps    the figure 10 shows the likelihood function for an experiment where 4 events are observed in a time interval of one second .",
    "the classical results depend on the stopping condition : a ) the time interval had been fixed , b ) the experiment was stopped after the forth event .",
    "the likelihood principle states that the two data sets are equivalent .",
    "thus the classical limits are inconsistent .",
    "the differences become even larger when we take the example of 1 event recorded in 1 second ( see fig .",
    "10 right ) .",
    "the likelihood functions given by the lifetime distribution and the poisson distribution , respectively are proportional to each other @xmath37      in a garden there are apple and pear trees . usually during night",
    "some pears fall from the trees .",
    "one morning looking from his window , the proprietor who is interested in apples find that no fruit is lying in the grass .",
    "since it is still quite dark he is unable to distinguish apples from pears .",
    "he concludes that the average rate of falling apples per night is less the 2.3 with 90% confidence level .",
    "his wife who is a classical statistician tells him that his rate limit is too high because he has forgotten to subtract the expected pears background .",
    "he argues , `` there are no pears '' , but she insists and explains him that if he ignores the pears that could have been there but were nt , he would violate the coverage requirement . in the meantime it has become bright outside and pears and apples - which both are not there - are now distinguishable .",
    "even though the evidence has not changed , the classical limit has .",
    "the 90% confidence limits for zero events observed and background expectation @xmath38 is @xmath39 .",
    "for @xmath40 it is @xmath41 much lower .",
    "_ ccl are different for two experiments with exactly the same experimental evidence relative to the signal ( no signal event seen)_. this situation is absolutely intolerable .",
    "feldman and cousins consider this kind of objections as `` based on a misplaced bayesian interpretation of classical intervals '' @xcite .",
    "it is hard to detect a bayesian origin in a generally accepted principle in science , namely , two measurements containing the same information should give identical results .",
    "the critics here is not that ccls are inherently wrong but that their application * *  * * to the computation of upper limits when background is expected does not make sense , i.e. these limits do not measure the precision of the experiment .",
    "the effect is less dramatic but also present in the unified approach : an experiment finding no event n=0 with background expectation b=3 produces a 90% confidence limit 1.08 for the signal ( see table 2.1 )",
    ". then the flux is doubled and the background is eliminated . the limit becomes 2.44/2=1.22 , worse than before .",
    "this problem is absent in the versions proposed by roe and woodroofe @xcite and also in that of punzi @xcite .",
    "these methods are however restricted to the poisson case .",
    "[ c]|l|l|l|l|l|l| & n=0 , b=0 & n=0 , b=1 & n=0 , b=2 & n=0 , b=3 & n=2 , b=2 + standard classical & 2.30 & 1.30 & 0.30 & -0.70 & 3.32 + unified classical & 2.44 & 1.61 & 1.26 & 1.08 & 3.91 + uniform bayesian & 2.30 & 2.30 & 2.30 & 2.30 & 3.88 +    to avoid the unacceptable situation , i have proposed a modified frequentist approach to the calculation of the poissonian limits including the information of the limited number of background events @xcite .",
    "there the confidence level is normalized to the probability to observe @xmath42 background events as known from the measurement .",
    "@xmath43 the resulting limits respect the likelihood principle ( see below ) and thus are consistent .",
    "they coincide with those of the uniform bayesian method and provide a frequentist interpretation of the bayesian limits . however , as has been pointed out by highland @xcite",
    ", the limits do not have minimum overcoverage as required by the strict application of the neyman construction .",
    "this is correct @xcite but in my paper no claim relative coverage had been made .",
    "the method has been applied to the a higgs search @xcite .",
    "likpois.eps    often the background expectation is not known precisely since it is estimated from side bands or from other measurements with limited statistics .",
    "so far , there is no classical recipe which allows to incorporate an uncertainty of the background estimate .",
    "likelihood limits also give a sensible description of the data .",
    "whether likelihood limits or bayesian limits obtained from the integration are more sensible depends on the shape of the likelihood function",
    ". ideally both limits should be given .",
    "11 compares the coverage of the unified classical and the bayesian limits . at small signals both overcover strongly . for large signals the bayesian method slightly undercovers and oscillates around the nominal value .",
    "two events are observed from an exponential decay with true mean life @xmath44 .",
    "the maximum likelihood estimate is used either for @xmath45 or @xmath8 .",
    "we assume that an infinite number of identical experiments is performed and that the results are combined . in table 2.2",
    "we summarize the results of different averaging procedures .",
    "there is no prescription for averaging classical intervals .",
    "the unified methods have to explain how they intend to combine their measurements . to compute the classical result given in the table ,",
    "the maximum likelihood estimate and central intervals were used .",
    "[ c]|lll| & @xmath46 & @xmath47 + & @xmath48 & @xmath48 + & @xmath49 & @xmath50 + & @xmath51 & @xmath52 + & @xmath53 & @xmath48 +    [ ptb ]    taugam.eps    in this special example a consistent result is obtained in the bayesian method with uniform prior for the decay constant .",
    "it shows also how critical the choice of the parameter is in the bayesian approach .",
    "it is also clear that an educated choice is also important for the pragmatic procedures .",
    "it is obvious that the decay constant is the better parameter ( see also fig .",
    "methods approximating the likelihood function provide reasonable results unless the likelihood function is very asymmetric .",
    "the weighting procedure of the pdg applied to the likelihood errors gives reasonable results .",
    "as is well known , adding the log - likelihood functions always produces a correct result .",
    "the conventional classical schemes suffer from the following problems :    * there are inconsistencies ( poisson limits , stopping rule , discrete vs. continuous parameters ) .",
    "* there is a lack of precision ( unphysical limits ) .",
    "* they have a restricted range of application ( problems with digital measurements , discrete parameters ) .",
    "* they are not invariant against sample variable transformations ( except central intervals in one dimension ) .",
    "* they are subjective ( coverage requires pre - experimental fixing of cuts and decision to publish ) .",
    "* there are unsolved problems .",
    "( it is not clear how to combine measurements .",
    "the inclusion of background errors in poisson processes is not possible . )",
    "* there is no obvious treatment of nuisance parameters .",
    "* systematic errors can not be included .",
    "compared to the conventional method there are improvements :    * the inconsistencies in poisson processes are weaker ( and absent in the version of roe and woodroofe ) * non - physical limits are avoided .",
    "* it is invariant with respect to variable and parameter transformations",
    ".    however most problems remain ( inconsistencies , lack of precision , background uncertainty in poisson limits ) , and :    * it is restricted to specific pdfs ( gaussian like ) . *",
    "it is complicated and requires considerable computing efforts . *",
    "the combination of measurements is even more unclear . *",
    "artificial error correlations are introduced near boundaries . *",
    "the proposed treatment  @xcite of nuisance parameters ( use best estimate may lead to undercoverage .",
    "likelihood limits have  attractive properties    * they are consistent . *",
    "they provide optimum precision .",
    "* they are invariant against variable and parameter transformations . *",
    "they provide a coherent transition to discrete hypothesis ( likelihood ratio ) * measurements can easily be combined    there are also restrictions in the application :    * digital measurements and uniform distributions can not be handled .",
    "the bayesian philosophy is very general and flexible :    * all problems can be treated .",
    "( nuisance parameters , digital measurements , unphysical boundaries etc . )    but :    * they depend on the parameter choice .",
    "the conventions proposed here represent by no means the only reasonable prescription .    since the complete information is contained in the likelihood function , classical approaches are not considered .",
    "( they can not be computed from the likelihood function alone . )",
    "an even stronger reason for there exclusion are the obvious inconsistencies of this method .",
    "the main objection against bayesian methods is their dependence on the selected parameter .",
    "i find it rather natural to choose a sensible parameter space .",
    "for some application like pattern recognition - which , by the way , can not be done with classical statistics - it is absolutely necessary .      1 .   whenever possible the full likelihood function should be published .",
    "it contains the experimental information and permits to combine the results of different experiments in an optimum way .",
    "this is especially important when the likelihood is strongly non - gaussian ( strongly asymmetric , cut by external bounds , has several maxima etc . ) .",
    "2 .   data are combined by adding the log - likelihoods . when not known , parametrizations are used to approximate it .",
    "3 .   if the likelihood is smooth and has a single maximum the likelihood limits should be given to define the error interval .",
    "these limits are invariant under parameter transformation . for the measurement of the parameter",
    "the value maximizing the likelihood function is chosen .",
    "no correction for biased likelihood estimators is applied .",
    "the errors usually are asymmetric .",
    "these limits can also be interpreted as bayesian one standard deviation errors for the specific choice of the parameter variable where the likelihood of the parameter has a gaussian shape .",
    "nuisance parameters are eliminated by integrating them out using an uniform prior .",
    "a correlation coefficient should be computed .",
    "5 .   for digital measurements",
    "the bayesian mean and r.m.s . should be used .",
    "6 .   in cases where the likelihood function is restricted by physical or mathematical bounds and where there are no good reasons to reject an uniform prior the measurement and its errors defined as the mean and r.m.s . should be computed in the bayesian way .",
    "upper and lower limits are computed from the tails of the bayesian probability distributions .",
    "( in some cases likelihood limits may be more informative .",
    "@xcite ) 8 .",
    "non - uniform prior densities should not be used .",
    "it is the scientist s choice whether to present an error interval or an upper limit .",
    "10 . in any case",
    "the applied procedure has to be documented .",
    "these recipes correspond more or less to our every day practice .",
    "an exception are poisson limits where for strange reasons the coverage principle - though only approximately realized - has gained preference in neutrino experiments .",
    "i would like to thank fred james , louis lyons and yves perrin for having organized this interesting workshop which - for the first time - offered the possibility to high energy physicists to expose and discuss their problems and solutions to statistical problems .",
    "a. l. read , optimal statistical analysis of search results based on the likelihood ratio and its application to the search for the msm higgs boson at @xmath54 = 161 and 172 gev , delphi 97 - 158 phys 737 ( 1997 )"
  ],
  "abstract_text": [
    "<S> classical confidence limits are compared to bayesian error bounds by studying relevant examples . </S>",
    "<S> the performance of the two methods is investigated relative to the properties coherence , precision , bias , universality , simplicity . a proposal to define error limits in various cases </S>",
    "<S> is derived from the comparison . </S>",
    "<S> it is based on the likelihood function only and follows in most cases the general practice in high energy physics . </S>",
    "<S> classical methods are discarded because they violate the likelihood principle , they can produce physically inconsistent results , suffer from a lack of precision and generality . also the extreme bayesian approach with arbitrary choice of the prior probability density or priors deduced from scaling laws </S>",
    "<S> is rejected . </S>"
  ]
}