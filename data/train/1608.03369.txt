{
  "article_text": [
    "the process of freehand sketching has long been employed by humans to communicate ideas and intent in a minimalist yet almost universally understandable manner . in spite of the challenges posed in recognizing them  @xcite",
    ", sketches have formed the basis of applications in areas of forensic analysis  @xcite , electronic classroom systems  @xcite , sketch - based retrieval  @xcite etc .",
    "sketching is an inherently sequential process . the proliferation of pen and tablet based devices today enables us to capture and analyze the entire process of sketching , thus providing additional information compared to passive parsing of static sketched content . yet , most sketch recognition approaches either ignore the sequential aspect or lack the ability to exploit it  @xcite .",
    "the few approaches which attempt to exploit the sequential sketch stroke data do so either in an unnatural manner  @xcite or impose restrictive constraints ( e.g. markov assumption )  @xcite .    in our work , we propose a recurrent neural network architecture for sketch object recognition which exploits the long - term sequential and structural regularities in stroke data in a scalable manner .",
    "we make the following contributions :    * we propose the first deep recurrent neural network architecture which can recognize freehand sketches across a large number ( @xmath0 ) of object categories .",
    "specifically , we introduce a gated recurrent unit ( gru)-based framework ( section [ sec : overview ] ) which leverages deep sketch features and weighted per - stroke loss to achieve state - of - the - art results . *",
    "we show that the choice of deep sketch features and recurrent network architecture _ both _ play a crucial role in obtaining good recognition performance ( section [ sec : results ] ) . * via our experiments on sketches with partial temporal stroke content , we show that our framework recognizes the largest percentage of sketches ( section [ sec : results ] ) .    given the on - line nature of our recognition framework ,",
    "it is especially suited for on - the - fly interpretation of sketches as they are drawn .",
    "thus , our framework can enable interesting applications such as camera - equipped robots playing the popular party game pictionary  @xcite with human players , generating sparsified yet recognizable sketches of objects  @xcite , interpreting hand - drawn digital content in electronic classrooms  @xcite etc .",
    "to retain focus , we review approaches exclusively related to recognition of hand - drawn object sketches . early datasets tended to contain either a small number of sketches and/or object categories  @xcite . in @xmath1 , eitz et al .",
    "@xcite released a dataset containing @xmath2 hand - drawn sketches across @xmath3 categories of everyday objects .",
    "the dataset , currently the largest sketch object dataset available , provided the first opportunity to attempt the sketch object recognition problem at a relatively large - scale . since its release",
    ", a number of approaches have been proposed to recognize freehand sketches of objects .",
    "the initial performance of handcrafted feature - based approaches  @xcite has been recently surpassed by deep feature - based approaches  @xcite , culminating in an custom - designed convolutional neural network dubbed sketchcnn which achieved state - of - the - art results  @xcite .",
    "the approaches mentioned above are primarily designed for static , full - sketch object recognition .",
    "in contrast , another set of approaches attempt to exploit the sequential stroke - by - stroke nature of hand - drawn sketch creation  @xcite . for example ,",
    "arandjelovic and sezgin  @xcite propose a hidden markov model ( hmm)-based approach for recognizing military and crisis management symbol objects .",
    "although mentioned above in the context of static object recognition , a variant of the sketchcnn  @xcite can also handle sequential stroke data .",
    "in fact , the authors demonstrate that exploiting the sequential nature of sketching process improves the overall recognition rate .",
    "however , given that cnns are not inherently designed to preserve sequential `` state '' , better results can be expected from a framework which handles sequential data in a more natural fashion .",
    "the approach we present in our paper aims to do precisely this .",
    "our framework is based on gated recurrent unit ( gru ) networks recently proposed by cho et al .",
    "gru architectures share a number of similarities with the more popular long short term memory networks  @xcite including the latter s ability to perform better  @xcite than traditional models ( e.g. hmm ) for problems involving long and complicated sequential structures . to the best of our knowledge",
    ", recurrent neural networks have not been utilized for online sketch recognition .",
    "sketch creation involves accumulation of hand - drawn strokes over time .",
    "thus , we require our recognition framework to optimally exploit object category evidence being accumulated on a per - stroke basis as well as temporally . moreover , the variety in sketch - based depiction and intrinsic representational complexity of objects results in a large range for stroke - sequence lengths .",
    "therefore , we require our recognition framework to address this variation in sequence lengths appropriately . to meet these requirements ,",
    "we employ gated recurrent unit ( gru ) networks  @xcite .",
    "our choice of gru architecture is motivated by the observation that it involves learning a smaller number of parameters and performs better compared to lstm in certain instances  @xcite including , as shall been seen ( section [ sec : experiments ] ) , our problem of sketch recognition as well .",
    "a gru network learns to map an input sequence @xmath4 to an output sequence @xmath5 .",
    "this mapping is performed by the following transformations which are applied at each time step :    @xmath6    here , @xmath7 and @xmath8 represent the @xmath9-th input and @xmath9-th output respectively , @xmath10 represents the `` hidden '' sequence state of the gru whose contents are regulated by parameterized gating units @xmath11 and @xmath12 represents the elementwise dot - product .",
    "the subscripted @xmath13s , @xmath14s and @xmath15 represent the trainable parameters of the gru .",
    "please refer to chung et al .",
    "@xcite for details .     of sketches at all levels of sketch completion .",
    "best viewed in color . ]    for each sketch , information is available at temporal stroke level .",
    "we use this to construct an image sequence @xmath16 of sketch strokes cumulatively accumulated over time .",
    "thus , @xmath17 represents the full , final object sketch and @xmath18 represents the number of sketch strokes or equivalently , time - steps ( see figure [ fig : overview ] ) . to represent the stroke content for each @xmath19",
    ", we utilize deep features obtained when @xmath20 is provided as input to alexnet  @xcite . the resulting deep feature sequence @xmath21 forms the input sequence to gru ( see figure [ fig : overview ] ) .",
    "the gru unit contains @xmath22 hidden units and its output is densely connected to a final softmax layer for classification . for better generalization , we include a dropout layer before the final classification layer which tends to benefit recurrent networks having a large number of hidden units .",
    "we used a dropout of @xmath23 in our experiments .",
    "our architecture produces an output prediction @xmath8 for every time - step @xmath24 in the sequence . by comparing the predictions @xmath8 with the ground - truth",
    ", we can determine the corresponding loss @xmath25 for a fixed loss function ( shown as a yellow box in figure [ fig : overview ] ) .",
    "this loss is weighted by a corresponding @xmath26 and backpropagated ) for the corresponding time step @xmath9 . for the weighing function ,",
    "we use    @xmath27    thus , losses corresponding to final stages of sequence are weighted more to encourage correct prediction of the full sketch .",
    "also , since @xmath26 is non - zero , our design incorporates losses from all steps of the sequence .",
    "this has the net effect of encouraging correct predictions even in the early stages of the sequence .",
    "overall , this feature enables our recognition framework to be accurate and responsive right from the beginning of the sketching process ( section [ sec : experiments ] ) in contrast with frameworks which need to wait for the sketching to finish before analysis can begin .",
    "we additionally studied variations of the weighing function given in equation  using the final sequence member loss ( i.e. @xmath28 ) and linearly weighted losses ( i.e. @xmath29 ) .",
    "we found that exponentially weighted loss for our experiments .",
    "] gave superior results .",
    "to address the high variation of sequence length across sketches , we create batches of sketches having equal sequence length ( i.e. @xmath18 ( sec .",
    "[ sec : overview ] ) ) .",
    "these batches of varying size are randomly shuffled and delivered to the recurrent network during training .",
    "for each batch , categorical - cross - entropy loss is generated for each sequence by comparing the predictions with the ground - truth .",
    "the resulting losses are weighted ( equation ) on a per - timestep basis as described previously and back - propagated through the corresponding sequence during training .",
    "we used stochastic gradient descent with a learning rate of @xmath30 for training .",
    "suppose for a given input sequence @xmath31 @xmath32 , the corresponding outputs at the softmax layer are @xmath33 .",
    "note that in our case , @xmath34 where @xmath35 is the dimension of deep feature @xmath7 and @xmath36 where @xmath37 is the number of object categories ( @xmath0 ) . to determine the final category label @xmath38",
    ", we perform a weighted sum - pooling of the softmax outputs as @xmath39 where @xmath26 is as given in equation and @xmath40 .",
    "we explored various other softmax output pooling schemes  last sequence member - based prediction ( @xmath41 ) , max - pooling ( @xmath42 $ ] ) , mean - pooling ( @xmath43 ) . from our validation experiments , we found weighted sum - pooling to be the best choice overall .",
    ".average recognition accuracy ( rightmost column ) for various architectures .",
    "# hidden refers to the number of hidden units used in recurrent network .",
    "we obtain state - of - the - art results for sketch object recognition .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]",
    "in addition to obtaining the best results among approaches using handcrafted features , the work of rosalia et al .",
    "@xcite was especially instrumental in identifying a @xmath0-category subset of the tu berlin dataset which could be unambiguously recognized by humans .",
    "consequently , our experiments are based on this curated @xmath0-category subset of sketches .",
    "following rosalia et al .",
    "@xcite , we use @xmath44 sketches from each of the @xmath0 categories . to ensure principled evaluation",
    ", we split the @xmath44 sketches of each category randomly into sets containing @xmath45 , @xmath46 and @xmath47 of sketches to be used for training , validation and testing respectively , @xmath48 and @xmath49 sketches from each category in the training , validation and test sets respectively . ] .",
    "additionally , we utilized the validation set exclusively for making choices related to architecture and parameter settings and performed a one - shot comparative evaluation of ours and competing approaches on the test set .",
    "we compared our performance with the following architectures :    * alexnet - ft : * as a baseline experiment , we fine - tuned alexnet using our @xmath0-category training data .",
    "to ensure sufficient data , we augmented the training data on the lines of sarvadevabhatla et al .",
    "we also used the final fully - connected @xmath50-dimensional layer features as input to our recurrent architectures .",
    "we shall refer to such usage by alexnet - fc .",
    "* sketchcnn : * this is essentially the deep architecture of yu et al .",
    "@xcite but retrained for the categories and splits mentioned in section [ sec : data ] . since cnns do not inherently store  state \" , the authors construct six different sub - sequence stroke accumulation images which comprise the channels of the input representation to the cnns .",
    "it comprises of five different cnns , each trained for five different scaled versions of @xmath51 sketches .",
    "the last fully - connected layer s @xmath52-dimensional features from all the five cnns are processed using a bayesian fusion technique to obtain the final classification .    for our experiments",
    ", we also concatenated the @xmath52 dimensional features from each scale of sketchcnn as the input feature to the recurrent neural network architectures that were evaluated .",
    "however , only the full @xmath51 sketch was considered as the input to cnn ( i.e. single - channel ) . for the rest of the paper",
    ", we refer to the resulting @xmath53-dimensional feature as sketchcnn - sch - fc .",
    "* recurrent architectures : * we experimented with the number of hidden units , the number of recurrent layers , the type of recurrent layers ( i.e. lstm or gru ) , the training loss function ( section [ sec : training ] ) and various pooling methods for obtaining final prediction in terms of individual sequence member predictions ( section [ sec : prediction ] ) .",
    "we built the software framework for our proposed architecture using lasagne  @xcite and theano  @xcite libraries .",
    "we also used matconvnet  @xcite and caffe  @xcite libraries for experiments related to other competing architectures .      *",
    "overall performance * : table 1 summarizes the overall performance in terms of average recognition accuracy for various architectures . as can be seen , our gru - based architecture ( first row ) outperforms sketchcnn by a significant margin even though it is trained on only @xmath45 of the total data .",
    "we believe our good performance stems from ( a ) being able to exploit the sequential information in a scalable and efficient manner via recurrent neural networks ( b ) the superiority of the deep sketch features provided by alexnet  @xcite compared to the sketchcnn - fc features .",
    "the latter can be clearly seen when we compare the first two rows of table 1 with the last two rows . in our case , the performance of gru was better than that of lstm when alexnet features were used .",
    "overall , it is clear that the choice of ( sketch ) features and the recurrent network _ both _ play a crucial role in obtaining state - of - the - art performance for the sketch recognition task .    * on - line",
    "recognition : * we also compared the various architectures for their ability to recognize sketches as they are being drawn ( i.e. on - line recognition performance ) . for each classifier , we determined the fraction of test sketches @xmath54 correctly recognized when only the first @xmath55 of the temporal sketch strokes are available .",
    "we varied @xmath56 between @xmath57 to @xmath58 in steps of @xmath57 and plotted @xmath54 as a function of @xmath56 .",
    "the results can be seen in figure [ fig : onlinerec ] .",
    "intuitively , the higher a curve on the plot , the better its online recognition ability .",
    "as can be seen , our framework consistently recognizes a larger fraction of sketches at all levels of sketch completion ( except for very small @xmath56 ) relative to other architectures .    * semantic information : * to determine the extent to which our architecture captures semantic information",
    ", we examined the performance of the classifier on misclassified sketches .",
    "as can be seen in figure [ fig : semantic ] , most of the misclassifications are reasonable errors ( e.g. ` guitar ` is mistaken for ` violin ` ) and demonstrate that our framework learns the overall semantics of the object recognition problem .",
    "in this paper , we have presented our deep recurrent neural network architecture for freehand sketch recognition .",
    "our architecture has two prominent traits . _",
    "firstly _ , its design accounts for the inherently sequential and cumulative nature of human sketching process in a natural manner .",
    "_ secondly _ , it exploits long - term sequential and structural regularities in stroke data represented as deep features .",
    "these two traits enable our system to achieve state - of - the - art recognition results on a large database of freehand object sketches .",
    "we have also shown that our recognition framework is highly suitable for on - the - fly interpretation of sketches as they are being drawn .",
    "our framework source - code and associated data ( pre - trained models ) can be accessed at https://github.com/val-iisc/sketch-obj-rec .",
    "we thank nvidia for their grant of tesla k40 gpu .",
    "f.  bastien , p.  lamblin , r.  pascanu , j.  bergstra , i.  j. goodfellow , a.  bergeron , n.  bouchard , and y.  bengio .",
    "theano : new features and speed improvements .",
    "deep learning and unsupervised feature learning nips 2012 workshop , 2012 .",
    "b.  meyer , k.  marriott , a.  bickerstaffe , and l.  knipping .",
    "intelligent diagramming in the electronic online classroom . in _ human system interactions , 2009 .",
    "2nd conference on _ , pages 177183 .",
    "ieee , 2009 .",
    "r.  k. sarvadevabhatla and v.  b. radhakrishnan .",
    "eye of the dragon : exploring discriminatively minimalist sketch - based abstractions for object categories . in _ proceedings of the 23rd",
    "acm international conference on multimedia _ , mm 15 , pages 271280 , new york , ny , usa , 2015 .",
    "o.  seddati , s.  dupont , and s.  mahmoudi .",
    "deepsketch : deep convolutional neural networks for sketch recognition and similarity search . in _ content - based multimedia indexing",
    "( cbmi ) , 2015 13th international workshop on _ , pages 16 .",
    "ieee , 2015 .",
    "z.  sun , c.  wang , l.  zhang , and l.  zhang .",
    "query - adaptive shape topic mining for hand - drawn sketch recognition . in _ proceedings of the 20th acm international conference on multimedia _ , pages 519528 .",
    "acm , 2012 ."
  ],
  "abstract_text": [
    "<S> freehand sketching is an inherently sequential process . yet </S>",
    "<S> , most approaches for hand - drawn sketch recognition either ignore this sequential aspect or exploit it in an ad - hoc manner . in our work </S>",
    "<S> , we propose a recurrent neural network architecture for sketch object recognition which exploits the long - term sequential and structural regularities in stroke data in a scalable manner . </S>",
    "<S> specifically , we introduce a gated recurrent unit based framework which leverages deep sketch features and weighted per - timestep loss to achieve state - of - the - art results on a large database of freehand object sketches across a large number of object categories . </S>",
    "<S> the inherently online nature of our framework is especially suited for on - the - fly recognition of objects as they are being drawn . </S>",
    "<S> thus , our framework can enable interesting applications such as camera - equipped robots playing the popular party game pictionary with human players and generating sparsified yet recognizable sketches of objects . </S>"
  ]
}