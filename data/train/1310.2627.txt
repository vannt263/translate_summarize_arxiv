{
  "article_text": [
    "when learning from streams of data to make predictions in the future , how should we handle the timestamp associated with each instance ? ignoring timestamps and assuming data are i.i.d .  is scalable but risks distracting a model with irrelevant `` ancient history . '' on the other hand , using only the most recent portion of the data risks overfitting to current trends and missing important time - insensitive effects . in this paper",
    ", we seek a general approach to learning model parameters that are overall sparse , but that adapt to variation in how different effects change over time .",
    "our approach is a prior over parameters of an exponential family ( e.g. , coefficients in linear or logistic regression ) .",
    "we assume that parameter values shift at each timestep , with correlation between adjacent timesteps captured using a multivariate normal distribution whose precision matrix is restricted to a tridiagonal structure .",
    "we ( approximately ) marginalize the ( co)variance parameters of this normal distribution using a jeffreys prior , resulting in a model that allows smooth variation over time while encouraging overall sparsity in the parameters .",
    "( the parameters themselves are not given a fully bayesian treatment . )",
    "we demonstrate the usefulness of our model on two tasks , showing gains over alternative approaches .",
    "the first is a text regression problem in which an economic variable ( volatility of returns ) is forecast from financial reports @xcite . the second forecasts text by constructing a language model that conditions on highly time - dependent economic variables .",
    "notation is given in ",
    "[ sec : erm ] .",
    "our prior distribution is presented in  [ sec : model ] .",
    "we draw connections to related work in  [ sec : relwork ] . ",
    "[ sec : inference ] presents our inference algorithm and  [ sec : experiments ] our experimental results .",
    "we assume data of the form @xmath0 , where each @xmath1 includes a timestamp denoted @xmath2 .",
    "the aim is to learn a predictor that maps input @xmath3 , assumed to be at timestep @xmath4 , to output @xmath5 . in the probabilistic",
    "setting we adopt here , the prediction is map inference over r.v .",
    "@xmath6 given @xmath7 and a model parameterized by @xmath8 .",
    "learning is parameter estimation to solve : @xmath9 the focus of the paper is on the prior distribution @xmath10 . throughout",
    ", we will denote the task - specific log - likelihood ( second term ) by @xmath11 and assume a generalized linear model such that a feature vector function @xmath12 maps inputs @xmath13 into @xmath14 and @xmath15 is `` linked '' to the distribution over @xmath6 using , e.g. , a logit or identity .",
    "we will refer to elements of @xmath12 as `` features '' and to @xmath16 as `` coefficients . ''",
    "we assume @xmath4 discrete timesteps .",
    "our time - series prior draws inspiration from the probabilistic interpretation of the sparsity - inducing lasso @xcite and group lasso @xcite . in non - overlapping group lasso , features are divided into groups , and the coefficients within each group @xmath17 are drawn according to :    1 .",
    "variance @xmath18 an exponential distribution .",
    "2 .   @xmath19 .",
    "we seek a prior that lets each coefficient vary smoothly over time .",
    "a high - level intuition of our prior is that we create copies of @xmath16 , one at each timestep : @xmath20 . for each feature",
    "@xmath21 , let the sequence @xmath22 form a group , denoted @xmath23 .",
    "group lasso does not view coefficients in a group as explicitly correlated ; they are independent given the variance parameter . given the sequential structure of @xmath23 , we replace the covariance matrix @xmath24 to capture autocorrelation .",
    "specifically , we assume the vector @xmath23 is drawn from a multivariate normal distribution with mean zero and a @xmath25 precision matrix @xmath26 with the following tridiagonal form : for this discussion ; each feature @xmath21 has its own @xmath27 . ]",
    "\\label{eq : lambda}\\end{aligned}\\ ] ] @xmath29 is a scalar multiplier whose role is to control sparsity in the coefficients , while @xmath30 dictates the degree of correlation between coefficients in adjacent timesteps ( autocorrelation ) .",
    "importantly , @xmath30 and @xmath31 ( and hence @xmath32 and @xmath26 ) are allowed to be different for each group @xmath21 .",
    "we need to ensure that @xmath32 is positive definite .",
    "fortunately , it is easy to show that for @xmath33 , the resulting @xmath32 is positive definite .    to show this , since @xmath32 is a symmetric matrix",
    ", we verify that each of its principal minors have strictly positive determinants .",
    "the principal minors of @xmath32 are uniform tridiagonal symmetric matrices , and the determinant of a uniform tridiagonal @xmath34 matrix can be written as @xmath35 ( see , e.g. , @xcite for the proof ) . since @xmath36 $ ] , if @xmath33 , the determinant is always positive .",
    "therefore , @xmath32 is always p.d .  for @xmath33 .      our generative model for the group of coefficients @xmath37",
    "is given by :    1 .",
    "@xmath38 an improper jeffreys prior ( @xmath39 ) .",
    "2 .   @xmath40 a truncated exponential prior with parameter @xmath41 .",
    "this distribution forces @xmath42 to fall in @xmath43 $ ] , so that @xmath44 is p.d . and autocorrelations are always positive : @xmath45 we fix @xmath46 .",
    "3 .   @xmath47 , with the precision matrix @xmath27 as defined in eq .",
    "[ eq : lambda ] .    during estimation of @xmath16 , each @xmath48 and @xmath42",
    "are marginalized , giving a sparse and adaptive estimate for @xmath16 .",
    "our design choice of the precision matrix @xmath27 is driven by scalability concerns . instead of using , e.g. , a random draw from a wishart distribution , we specify the precision matrix to have a tridiagonal structure .",
    "this induces dependencies between coefficients in adjacent timesteps ( first - order dependencies ) and allows the prior to scale to fine - grained timesteps more efficiently .",
    "let @xmath49 denote the number of training instances , @xmath50 the number of base features , and @xmath4 the number of timesteps .",
    "a single pass of our variational algorithm ( discussed in  [ sec : inference ] ) has runtime @xmath51 and space requirement @xmath51 , instead of @xmath52 for both if each @xmath27 is drawn from a wishart distribution .",
    "this can make a big difference for applications with large numbers of features ( @xmath50 ) .",
    "additionally , we choose the off - diagonal entries to be uniform , so we only need one @xmath42 for each base feature .",
    "this design choice restricts the expressive power of the prior but still permits flexibility in adapting to trends for different coefficients , as we will see .",
    "the prior encourages sparsity at the group level , essentially performing feature selection : some feature coefficients @xmath23 may be driven to zero across all timesteps , while others will be allowed to vary over time , with an expectation of smooth changes .",
    "note that this model introduces only one hyperparameter , @xmath41 , since we marginalize @xmath53 and @xmath54 .",
    "our model is related to autoregressive integrated moving average approaches to time - series data @xcite , but we never have access to _ direct _ observations of the time - series . instead , we observe data ( @xmath13 and @xmath55 ) assumed to have been sampled using time - series - generated variables as _ coefficients _ ( @xmath16 ) . during learning , we therefore use probabilistic inference to reason about the variables at all timesteps together . in  [ sec : inference ] , we describe a scalable variational inference algorithm for inferring coefficients at all timesteps , enabling prediction of future data and inspection of trends .    we follow @xcite in creating time - specific copies of the base coefficients , so that @xmath56",
    ". as a prior over @xmath16 , they used a multivariate gaussian imposing non - zero covariance between each @xmath57 and its time - adjacent copies @xmath58 and @xmath59 . the strength of that covariance was set for each base feature by a global hyperparameter , which was tuned on held - out development data along with the global variance hyperparameter .",
    "yogatama et al.s model can be obtained from ours by fixing the same @xmath30 and @xmath31 for all features @xmath21 .",
    "our approach differs in that ( i ) we marginalize the hyperparameters , ( ii ) we allow each coefficient its own autocorrelation , and ( iii ) we encourage sparsity .",
    "there are many related bayesian approaches for time - varying model parameters @xcite , as well as work on time - varying signal estimation @xcite .",
    "each provides a different probabilistic interpretation of parameter generation .",
    "our model has a distinctive generative story in that correlations between parameters of successive timesteps are encoded in a precision matrix .",
    "additionally , unlike these fully bayesian approaches that infer full posterior distributions , we only obtain posterior mode estimates of coefficients , which has computational advantages at prediction time ( e.g. , straightforward map inference and sparsity ) and interpretability of @xmath16 .",
    "as noted , our grouping together of each feature s instantiations at all timesteps , @xmath60 and seeking sparsity , bears clear similarity to _ group lasso _",
    "@xcite , which encourages whole groups of coefficients to collectively go to zero . a probabilistic interpretation for lasso as a two level exponential - normal distribution that generalizes to ( non - overlapping ) group lasso was introduced by @xcite .",
    "he also showed that the exponential distribution prior can be replaced with an improper jeffreys prior for a parameter - free model , a step we follow as well .",
    "our model is also related to the fused lasso @xcite , which penalizes a loss function by the @xmath61-norm of the coefficients and their differences .",
    "our prior has a more clear probabilistic interpretation and adapts the degree of autocorrelation for each coefficient , based on the data .",
    "@xcite proposed a regularization method using a matrix - variate normal distribution prior to model task relationships in multitask learning .",
    "if we consider timesteps as tasks , the technique resembles our regularizer .",
    "their method jointly optimizes the covariance matrix with the feature coefficients ; we choose a bayesian treatment and encode our prior belief to the ( inverse ) covariance matrix , while still allowing the learned feature coefficients to modify the matrix by posterior inference . as a result , our method allows different base features to have different matrices .",
    "we marginalize @xmath62 and @xmath63 and obtain a maximum _ a posteriori _ estimate for @xmath16 , which includes a coefficient for each base feature @xmath21 at each timestep @xmath64 .",
    "specifically , we seek to maximize : @xmath65 exact inference in this model is intractable .",
    "we use mean - field variational inference to derive a lower bound on the above log - likelihood function .",
    "we then apply a standard optimization technique to jointly optimize the variational parameters and the coefficients @xmath16 .",
    "we introduce fully factored variational distributions for each @xmath48 and @xmath42 . for @xmath48",
    ", we use a gamma distribution with parameters @xmath66 as our variational distribution : @xmath67 therefore , we have @xmath68 = a_i b_i$ ] , @xmath69 = ( ( a_i-1)b_i)^{-1}$ ] , and @xmath70 = \\psi(a_i ) + \\log b_i$ ] ( @xmath71 is the digamma function ) .    for @xmath42 , we choose the form of our variational distribution to be the same truncated exponential distribution as its prior , with parameter @xmath72 , denoting this distribution @xmath73 .",
    "we have @xmath74 & = \\int^{0}_{-c } \\alpha_i \\frac{\\kappa_i \\exp(-\\kappa_i(\\alpha_i+c ) ) } { 1-\\exp(-\\kappa_i c ) } d\\alpha_i \\nonumber \\\\ & = \\frac{1}{\\kappa_i}-\\frac{c}{1-\\exp(- \\kappa_i c)}\\end{aligned}\\ ] ] we let @xmath75 denote the set of all variational distributions over @xmath62 and @xmath63 .",
    "the variational bound @xmath76 that we seek to maximize is given in figure  [ fig : varbound ] .",
    "our learning algorithm involves optimizing with respect to variational parameters @xmath77 , @xmath78 , and @xmath79 , and the coefficients @xmath16 .",
    "we employ the l - bfgs quasi - newton method @xcite , for which we need to compute the gradient of @xmath76 .",
    "we turn next to each part of this gradient .",
    "@xmath80   \\fbox{$-\\mathbb{e}_q[\\log \\det{\\mathbf{a}_i^{-1}}]$ } ) - \\mathbb{e}_q[\\lambda_{i}^{-1}]\\frac{1}{2 } { \\boldsymbol{\\beta}}_{i}^{\\top}\\mathbb{e}_q[\\mathbf{a}_i]{\\boldsymbol{\\beta}}_{i } \\right\\ } \\\\ & &   + \\sum_{i=1}^i",
    "\\left\\ { - ( \\mathbb{e}_q[\\alpha_{i}]+c)\\tau - \\mathbb{e}_q[\\log \\lambda_{i } ] \\right\\ } - \\sum_{i = 1}^i \\left\\ { ( a_{i } - 1 ) \\mathbb{e}_q[\\log \\lambda_{i } ] - \\frac{\\mathbb{e}_q[\\lambda_{i}]}{b_{i } } - \\log \\gamma({a_{i } } ) - a_{i } \\log b_{i } \\right\\ }   \\\\ & &   - \\sum_{i = 1}^i \\left\\ { \\log \\kappa_{i } -\\kappa_{i}(\\mathbb{e}_q[\\alpha_{i}]+c ) - \\log ( 1 - \\exp(-\\kappa_{i}c ) ) \\right\\}\\end{aligned}\\ ] ]      for @xmath81 , the first derivative with respect to time - specific coefficient @xmath82 is : @xmath83 \\left ( \\mathbb{e}[\\alpha_{i}](\\beta_{i}^{(t-1)}+\\beta_{i}^{(t+1 ) } ) + 2\\beta_{i}^{(t ) } \\right)\\ ] ] we can interpret the first derivative as including a penalty scaled by @xmath84 $ ] . we rewrite this penalty as : @xmath85 \\left ( \\vphantom{1 - \\mathbb{e}[\\alpha_{i } ] ) 2\\beta_{i}^{(t ) } } \\right . &",
    "( 1 - \\mathbb{e}[\\alpha_{i } ] ) & & \\cdot 2\\beta_{i}^{(t ) } \\\\ & + \\mathbb{e}[\\alpha_{i } ] & & \\cdot ( \\beta_{i}^{(t ) } - \\beta^{(t-1)}_{i } ) \\\\ & + \\mathbb{e}[\\alpha_{i } ] & & \\left .",
    "\\cdot ( \\beta_{i}^{(t ) }   - \\beta^{(t+1)}_{i } ) \\right)\\end{aligned}\\ ] ] this form makes it clear that the penalty depends on @xmath86 and @xmath87 , penalizing the difference between @xmath88 and these time - adjacent coefficients proportional to @xmath89 $ ] .",
    "the form bears strong similarity to the first derivative of the time - series ( log-)prior introduced in @xcite , which depends on fixed , global hyperparameters analogous to our @xmath30 and @xmath31 . because our approach does not require us to specify scalars playing the roles of `` @xmath90 $ ] '' and `` @xmath89 $ ] '' in advance , it is possible for each feature to have its own autocorrelation .",
    "obtaining the same effect in their model would require careful tuning of @xmath91 hyperparameters , which is not practical .",
    "it also has some similarities to the fused lasso penalty @xcite , which is intended to encourage sparsity in the differences between features coefficients across timesteps .",
    "our prior , on the other hand , encourages smoothness in the differences , with additional sparsity at the feature level .",
    "recall that the variational distribution for @xmath48 is a gamma distribution with parameters @xmath92 and @xmath93 .",
    "[ [ precision - matrix - scalar - boldsymbollambda . ] ] precision matrix scalar @xmath62 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the first derivative for variational parameters @xmath77 is easy to compute : @xmath94{\\boldsymbol{\\beta}}_{i}}{2   b_{i}(a_{i}-1)^{2 } } + 1\\ ] ] where @xmath95 is the trigamma function .",
    "we can solve for @xmath78 in closed form given the other free variables : @xmath96{\\boldsymbol{\\beta}}_{i } } {   ( a_{i}-1)t}\\ ] ] we therefore treat @xmath78 as a function of @xmath77 , @xmath79 , and @xmath16 in optimization .    [ [ off - diagonal - entries - boldsymbolalpha . ] ] off - diagonal entries @xmath63 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    first , notice that using jensen s inequality : @xmath97 = \\mathbb{e}[- \\log \\det{\\mathbf{a}_i } ] \\geq - \\log \\det \\mathbb{e}[\\mathbf{a}_i]$ ] due to the fact that @xmath98 is a convex function .",
    "furthermore , for a uniform symmetric tridiagonal matrix like @xmath44 , the log determinant can be computed in closed form as follows @xcite : @xmath99 = & \\log \\left ( \\prod_{t=1}^{t } 1 + 2 \\mathbb{e}[\\alpha_i ]   \\cos\\left (   \\frac{(t+1)\\pi}{t+1 } \\right ) \\right ) \\\\",
    "= & \\sum_{t=1}^{t } \\log \\left ( 1 + 2 \\mathbb{e}[\\alpha_i ]   \\cos\\left ( \\frac{(t+1)\\pi}{t+1 } \\right ) \\right)\\end{aligned}\\ ] ] we therefore maximize a lower bound on @xmath76 , making use of the above to calculate first derivatives with respect to @xmath72 : @xmath100}{\\partial\\kappa_i } -\\frac{1}{\\kappa_{i } } + c + \\mathbb{e}[\\alpha_{i } ] + \\frac{\\partial \\mathbb{e}[\\alpha_{i}]}{\\partial \\kappa_i}\\kappa_{i } \\\\ & + \\frac{c \\exp(-c\\kappa_{i})}{1-\\exp(-c\\kappa_{i } ) } + \\frac{1}{2 }",
    "\\frac { \\partial \\log \\det \\mathbb{e}[\\mathbf{a}_i]}{\\partial \\kappa_i } \\\\ & - \\frac{1}{2}\\mathbb{e}[\\lambda_{i}^{-1 } ] \\frac{\\partial { \\boldsymbol{\\beta}}_{i}^{\\top}\\mathbb{e}[\\mathbf{a}_i]{\\boldsymbol{\\beta}}_{i } } { \\partial \\kappa_i}\\end{aligned}\\ ] ] the partial derivatives @xmath101}{\\partial \\kappa_i}$ ] , @xmath102}{\\partial \\kappa_i}$ ] , and @xmath103{\\boldsymbol{\\beta}}_{i}}{\\partial \\kappa_i } $ ] are easy to compute .",
    "we omit them for space .",
    "a well - known property of numerical optimizers like the one we use ( l - bfgs ; @xcite ) is the failure to reach optimal values exactly at zero .",
    "although theoretically strongly sparse , our prior only produces weak sparsity in practice .",
    "future work might consider a more principled proximal - gradient algorithm to obtain strong sparsity @xcite .    if we expect feature coefficients at specific timesteps to be sparse as well , it is straightforward to incorporate additional terms in the objective function that encode this prior belief ( analogous to an extension from group lasso to _ sparse _ group lasso ) .",
    "for the tasks we consider in our experiments , we found that it does not substantially improve the overall performance .",
    "therefore , we keep the simpler bound given in figure  [ fig : varbound ] .",
    "we report two sets of experiments , one with a continuous @xmath55 , the other a language modeling application where @xmath55 is text .",
    "each timestep in our experiments is one year .      on both tasks ,",
    "we compare our approach to a range of baselines . since this is a forecasting task , at each test year , we only used training examples that come from earlier years .",
    "our baselines vary in how they use this earlier data and in how they regularize .    * * ridge - one * : ridge regression @xcite , trained on only examples from the year prior to the test data ( e.g. , for the 2002 task , train on examples from 2001 ) * * ridge - all * : ridge regression trained on the full set of past examples ( e.g. , for the 2002 task , train on examples from 19962001 ) * * ridge - ts * : the non - adaptive time - series ridge model of @xcite * * lasso - one * : lasso regression @xcite , trained on only examples from the year prior to the test data * * lasso - all * : lasso regression trained on the full set of past examples    in all cases , we tuned hyperparameters on a development data .",
    "note that , of the above baselines , only * ridge - ts * replicates the coefficients at different timesteps ( i.e. , @xmath104 parameters ) ; the others have only @xmath50 time - insensitive coefficients .",
    "the model with our prior always uses all training examples that are available up to the test year ( this is equivalent to a sliding window of size infinity ) . like * ridge - ts * , our model trusts more recent data more , allowing coefficients farther in the past to drift farther away from those most relevant for prediction at time @xmath105 .",
    "our model , however , adapts the `` drift '' of each coefficient separately rather than setting a global hyperparameter .      in the first experiment",
    ", we apply our prior to a forecasting task .",
    "we consider the task of predicting volatility of stock returns from financial reports of publicly - traded companies , similar to @xcite .    [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     quantitative u.s .",
    "macroeconomic data was obtained from the federal reserve bank of st .",
    "louis data repository ( `` fred '' ) .",
    "we used standard measures of economic activity focusing on output ( gdp ) , employment , and specific markets ( e.g. , housing ) .",
    "we use equity market returns for the u.s .",
    "market as a whole and various industry and characteristic portfolios .",
    "they are used as @xmath106 in our model ; in addition to state indicator variables , there are 66 macroeconomic variables in total .",
    "we compare our model to the baselines in  [ sec : baselines ] .",
    "the lasso variants are analogous to the original formulation of sage @xcite , except that our model directly conditions on macroeconomic variables instead of a dirichlet - multinomial compound .",
    "we score models by computing the negative log - likelihood on the test dataset : @xmath107 .",
    "we initialized all the feature coefficients by the coefficients by training a lasso regression on the last year of the training data ( * lasso - one * ) . the first test year",
    "( i.e. , 2003 ) was used as our development data for hyperparameter tuning ( @xmath41 was selected to be @xmath108 ) .",
    "table  [ tbl : results2 ] shows the results for the six models we compared .",
    "similar to the forecasting experiments , at each test year , we trained only on documents from earlier years .",
    "when we collapsed all the training data and ignored the temporal dimension ( ridge - all and lasso - all ) , the background log - frequencies @xmath109 are computed using the entire training data , which is different compared to the background log - frequencies for only the last timestep of the training data .",
    "our model outperformed all ridge and lasso variants , including the one with a time - series penalty @xcite , in terms of negative log - likelihood on unseen dataset .",
    "in addition to improving predictive accuracy , the prior also allows us to discover trends in the feature coefficients and gain insight .",
    "we manually examined the model from the last run ( test year 2006 ) .",
    "examples of temporal trends learned by our model are shown in figure  [ fig : trends ] .",
    "the plot illustrates feature coefficients for words that contain the string ` employ ` . for comparison",
    ", we also included the percentage of unemployment rate in the u.s .",
    "( which was used as one of the features @xmath110 ) , scaled to fit into the plot .",
    "we can see that there is a correlation between feature coefficients for the word ` unemployment ` and the actual unemployment rate . on the other hand ,",
    "the correlations are less evident for other words .",
    "we presented a time - series prior for the parameters of probabilistic models ; it produces sparse models and adapts the strength of temporal effects on each coefficient separately , based on the data , without an explosion in the number of hyperparameters .",
    "we showed how to do inference under this prior using variational approximations .",
    "we evaluated the prior for the task of forecasting volatility of stock returns from financial reports , and demonstrated that it outperforms other competing models .",
    "we also evaluated the prior for the task of modeling a collection of texts over time , i.e. , predicting the probability of words given some observed real - world variables .",
    "we showed that the prior achieved state - of - the - art results as well .",
    "the authors thank several anonymous reviewers for helpful feedback on earlier drafts of this paper .",
    "this research was supported in part by a google research award to the second and third authors .",
    "this research was supported in part by the intelligence advanced research projects activity via department of interior national business center contract number d12pc00347 . the u.s .",
    "government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation thereon .",
    "the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements , either expressed or implied , of iarpa , doi / nbc , or the u.s .  government ."
  ],
  "abstract_text": [
    "<S> we consider the scenario where the parameters of a probabilistic model are expected to vary over time . </S>",
    "<S> we construct a novel prior distribution that promotes sparsity and adapts the strength of correlation between parameters at successive timesteps , based on the data . </S>",
    "<S> we derive approximate variational inference procedures for learning and prediction with this prior . </S>",
    "<S> we test the approach on two tasks : forecasting financial quantities from relevant text , and modeling language contingent on time - varying financial measurements . </S>"
  ]
}