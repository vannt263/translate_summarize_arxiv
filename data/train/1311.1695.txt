{
  "article_text": [
    "the bi - directional link between the relatively new discipline of network science and the well - consolidated field of matrix algebra is intriguing and promising @xcite .",
    "the big challenge is to bridge network science and matrix algebra in a synergy .",
    "can we apply results and methods of matrix algebra to investigate the properties of networks ?",
    "do real networks , with their universal architectures , represent a class of algebraic structures ( matrices ) for which results and methods of matrix algebra can be improved or specialized ? clearly , both network science and matrix algebra would benefit from this synergistic approach .",
    "network science would gain additional insight in the structure of real networks , while matrix algebra would obtain more challenging applications .",
    "networks , in their basic form of graphs of nodes and edges , can be represented as matrices .",
    "the most common representation of a graph consists of the graph adjacency matrix , where the entries of the matrix that are not null represent the edges of the graph . often , it is convenient to represent a graph with its laplacian matrix , which places on the diagonal the degrees of the graph nodes ( the number of connections of the nodes ) and elsewhere information about the distribution of edges among nodes in the graph . the laplacian matrix , and in particular the laplacian matrix , and in particular its smallest eigenpairs ( eigenpairs relative to the smallest eigenvalues ) , turn up in many different places in network science .",
    "examples include random walks on networks , resistor networks , resistance distance on networks , current - flow closeness and betweenness centrality measures , graph partitioning , and network connectivity @xcite .",
    "real networks might be very large ; however , they are typically also very sparse . moreover , generally , not the entire matrix spectrum is necessary , but only a few eigenpairs , either the lowest of the largest , are enough",
    ". a number of iterative procedures , based on a generalization of the well - known power method , have been recently developed to compute a few eigenpairs of a large and sparse matrix .    in this paper",
    ", we experimentally analyze three important iterative methods : ( i ) the implicitly restarted lanczos method , ( ii ) the jacobi - davidson method , and ( iii ) the deflation accelerated conjugate gradient method .",
    "we implement these methods in a uniform programming environment and experimentally compare them on four laplacian matrices of networks arising from realistic applications .",
    "the real networks include a biological network ( a protein - protein interaction network of yeast ) , a technological network ( a snapshot of the internet ) , an information network ( a fragment of the web ) , and a social network ( the whole collaboration network among computer science scholars ) .",
    "the layout of the rest of the paper is the following .",
    "in section [ laplacian ] we describe some applications of the lowest eigenpairs of the laplacian matrix of a graph .",
    "the compared state - of - the - art algorithms for eigenpair computation of large and sparse matrices are reviewed in section [ state - of - the - art ] .",
    "section [ experiments ] is devoted to the discussion of the outcomes of the comparison among the algorithms when they run on real network data .",
    "we draw our conclusions in section [ conclusion ] .",
    "let @xmath1 be a simple ( no multiple edges , no self - loops ) undirected weighted graph with @xmath2 the set of nodes , @xmath3 , @xmath4 the set of edges , @xmath5 , and @xmath6 a vector such that @xmath7 is the positive weight of edge @xmath8 , for @xmath9 .",
    "the weighted laplacian of @xmath10 is the symmetric matrix @xmath11 where @xmath12 is the weighted adjacency matrix of the graph and @xmath13 is the diagonal matrix of the generalized degrees ( the sum of the weights of the incident arcs ) of the nodes . in the following we focus on the spectral properties of the graph laplacian matrix and on their practical importance .    if @xmath14 denotes a vector of ones by definition @xmath15 so that @xmath16 .",
    "thus @xmath14 is an eigenvector of @xmath17 associated to the eigenvalue @xmath18 .",
    "in addition if @xmath19 then @xmath20 this implies that @xmath17 , besides symmetric , is positive semidefinite , and hence it has real and nonnegative eigenvalues that is useful to order @xmath21 .    a basic result states that the multiplicity of @xmath22 as an eigenvalue of @xmath17 coincides with the number of the connected components of @xmath10 .",
    "hence @xmath23 if and only if @xmath10 is connected .",
    "fiedler @xcite was one of the pioneers of the study of the relations between the spectral and the connectivity properties of @xmath10 , and for this reason @xmath24 is called fiedler value or algebraic connectivity .",
    "since @xmath17 is symmetric it admits the spectral decomposition @xmath25 where @xmath26 is the diagonal matrix such that @xmath27 and @xmath2 is orthogonal , i.e. @xmath28 , and its columns are the eigenvectors of @xmath17 . notice that the normalization of the eigenvector of @xmath17 associated with the eigenvalue @xmath18 yields @xmath29 .",
    "in addition @xmath30 , being the eigenvector associated with the fiedler value , will be called fiedler vector .",
    "certain applicative problems require the minimization of ( [ quadratic ] ) under the condition that the entries of the vector @xmath31 belong to some discrete set .",
    "an important example is discussed in @xcite and concerns graph partitioning .",
    "let us partition @xmath2 in two subsets @xmath32 and @xmath33 .",
    "if we set @xmath34 if @xmath35",
    "then ( [ quadratic ] ) is the sum of the weighs of the arcs from one of the subsets to the other , and is called the cut size .",
    "the graph partitioning problem requires to find @xmath32 and @xmath33 of prescribed dimensions @xmath36 and @xmath37 , in such a way the cut size is minimized . actually , all the known methods for finding the minimum are very demanding , since they reduce to an enumeration of all the @xmath38 possible solutions .",
    "however , it is possible to approximate the minimum by relaxing the constraints on the entries of @xmath31 , allowing them to assume real values in such a way that @xmath39 and @xmath40 . observe that @xmath41 if we set @xmath42 we obtain @xmath43 @xmath44 thus , if presented in this spectral form the problem greatly simplifies .",
    "it is easy to find that the minimum is @xmath45 and is obtained when @xmath46 , @xmath47 and @xmath48 for @xmath49 .",
    "hence , the minimum of the original problem is obtained for @xmath50 showing the central role played by the fiedler vector in the problem .",
    "a second example of the same nature is discussed in @xcite . in the case where all the weights are equal to one , the minimization of ( [ quadratic ] ) , with the constraint that the entries of @xmath31 belong to the @xmath51 possible permutations of the integers from @xmath52 to @xmath53 , allows to find an ordering of the nodes of @xmath10 that concentrates the entries of @xmath12 near the main diagonal .",
    "for this reason is known as profile reducing ordering . by relaxing the problem we find as an approximate solution the ordering induced by the entries of the fiedler vector .",
    "this kind of applications do not require an accurate computation of the entries of the vector .",
    "a different but equally important application concerns the problem of the computation of betweenness centrality @xcite .",
    "this centrality index quantifies the quantity of information that passes through a node in order to transit between others . actually , for the computation of betweenness centralities , a linear system in @xmath17 for every couple of nodes of the network has to be solved",
    "this is actually equivalent to the computation of @xmath54 , the moore - penrose generalized inverse of @xmath17 @xcite .",
    "it turns out that @xmath55 the use of approximations of @xmath54 obtained by partial sums @xmath56 has been proposed in @xcite .",
    "clearly this implies the computation a certain number of the smallest eigenpairs of the laplacian .",
    "moreover , if the eigenvalues @xmath57 , for @xmath58 are close to each other it is possible to approximate them by means of a suitable constant @xmath59 ( for example @xmath60 , or simply @xmath61 ) . in @xcite",
    "it has been shown that the use of @xmath62 in the place of @xmath63 leads to improved approximations of the centralities .",
    "it is important to note that in order to use @xmath64 no additional eigenpairs with respect to @xmath63 are requested .",
    "starting from the subspace iteration , which is a generalization of the well - known power method , a number of iterative procedures have been recently developed to compute a few eigenpairs of a large and sparse matrix @xmath12 . in the following ,",
    "we describe three important methods :    * the implicitly restarted lanczos method ( irlm ) . *",
    "the jacobi - davidson method ( jd ) .",
    "* the deflation accelerated conjugate gradient method ( dacg ) .",
    "all the methods are iterative , in the sense that they compute one or more eigenpairs by constructing a sequence of vectors which approximate the exact solution .",
    "they are all based on the following tasks which must be efficiently carried on :    1 .",
    "computation of the the product of the matrix @xmath12 by a vector .",
    "this matrix vector product ( mvp ) has a cost proportional to the number of nonzero entries of @xmath12 .",
    "2 .   computation of a matrix @xmath65 , known as preconditioner , that approximates @xmath66 in such a way that the eigenvalues of @xmath67 are well clustered around @xmath52 and in addition the computations of @xmath68 and @xmath69 , being @xmath70 a generic vector , require comparable cpu time .",
    "it is important to stress that irlm and jd are characterized by an inner - outer iteration , where at every outer iteration a linear system has to be solved .",
    "however , while irlm requires to solve these linear systems to a high accuracy , which is strictly related to the accuracy requested for the eigenpairs , for jd inexact solution of inner linear system is sufficient to achieve overall convergence . on the other hand",
    ", dacg does not require any linear system solution .",
    "the best known method , the implicitly restarted arnoldi method ( iram ) , is implemented within the arpack package @xcite and is also available in the most popular scientific computing packages ( matlab /r ) . for symmetric positive definite matrices",
    ", iram simplifies to irlm , implicitly restarted lanczos method which reduces the computational cost , by taking advantage of the symmetry of the problem .",
    "the idea of the lanczos method is to project the coefficient matrix @xmath12 onto a subspace generated by an arbitrary initial vector @xmath71 and the matrix @xmath12 itself , known as krylov subspace . in particular , a krylov subspace of dimension @xmath72 is generated by the following set of independent vectors : @xmath73 actually it is convenient to work with an orthogonal counterpart of this basis and to organize its vectors as columns of a matrix @xmath74 .",
    "then , a symmetric and tridiagonal matrix @xmath75 can be computed as @xmath76 it is well known that the largest eigenvalues of @xmath77 , @xmath78 converge , as the size of the krylov subspace @xmath72 increases , to the largest eigenvalues of @xmath12 : @xmath79 , while the corresponding eigenvectors of @xmath12 can be computed from the homologous eigenvectors of @xmath77 by @xmath80 .",
    "[ lanczos ] * computation of @xmath77 for @xmath12*. + @xmath81 unitary norm initial vector .",
    "+ @xmath82 + @xmath83 + * for * @xmath84 + @xmath85 + @xmath86 + @xmath87 + @xmath88 + @xmath89 + * end for * +    [ lan_smallest ] * computation of @xmath77 for @xmath66*. + @xmath81 unitary norm initial vector .",
    "+ @xmath82 + @xmath83 + * for * @xmath84 + @xmath90   + @xmath91 + @xmath87 + @xmath88 + @xmath89 + * end for * +    such a convergence in many cases is very fast : roughly @xmath92 matrix - vector products are usually enough to compute a small number @xmath93 of the rightmost eigenpairs to a satisfactory accuracy .",
    "this eigenvalue solver exits whenever the following test is satisfied : @xmath94 with @xmath95 a fixed tolerance .",
    "convergence to the smallest eigenvalues is much slower .",
    "hence , to compute the leftmost part of the spectrum , it is more usual to apply the lanczos process to the inverse of the coefficient matrix @xmath66 . since @xmath12 is expected to be large and sparse , its explicit inversion is not convenient from both cpu time and storage point of view .",
    "algorithm [ lanczos ] , left code , must then be changed since now @xmath96 is computed as the solution of the linear system @xmath97 , as reported in algorithm [ lanczos ] , right code .    as before , a krylov subspace of size roughly @xmath98 is sufficient to have @xmath93 leftmost eigenpairs to a high accuracy .",
    "the complexity of this algorithm is then @xmath98 solutions of linear systems with @xmath12 as the coefficient matrix .",
    "the linear system solution needed at every lanczos step can be solved either by a direct method ( cholesky factorization ) or by an iterative method such as the preconditioned conjugate gradient ( pcg ) method .",
    "the former approach is unviable if the system matrix size is large ( say @xmath99 ) due to the excessively dense triangular factor provided by the direct factorization .",
    "in such a case the pcg method should be used with the aid of a preconditioner , which speeds ups convergence .",
    "we choose the best known multi purpose preconditioner : the incomplete cholesky factorization with no fill - in .",
    "another advantage of the iterative solution is that the iterative procedure to solve the inner linear system is usually stopped when the following test is satisfied : @xmath100 where the tolerance @xmath101 can be chosen proportional to the accuracy required for the eigenvectors .      to compute the smallest eigenvalue this method considers the minimization of the rayleigh quotient @xmath102 which can be accomplished by setting its gradient to 0 , namely @xmath103    equation ( [ nonlin ] ) is a nonlinear system of equations which can be solved by means of the classical newton s method in which the jacobian of ( [ nonlin ] ) ( or the hessian of the rayleigh quotient ) is replaced by a simplified formula : @xmath104 , which is shown to maintain the convergence properties of the newton s method .",
    "the @xmath8th iterate of this newton s method hence reads @xmath105 in practice solution of the system ( [ linsis ] ) is known to produce stagnation in the newton process .",
    "@xcite proposed to use a projected jacobian namely @xmath106 ensuring that the search direction @xmath107 be orthogonal to @xmath108 to avoid stagnation .",
    "even this corrected newton iteration may be slow , especially if a good starting point is not available . in @xcite",
    "it is proposed to perform a rayleigh - ritz step at every newton iteration . in detail , the newton iterates are collected as columns of a matrix @xmath2 ; then a very small matrix @xmath109 is computed .",
    "the leftmost eigenvector @xmath110 is easily computed and a new vector @xmath111 is obtained as @xmath112 . the main consequence of this procedure is the acceleration of the newton s method toward the desired eigenvector .    to compute @xmath24 , @xmath113 , @xmath114 ,",
    "the previous scheme can be used provided that the jacobian matrix is projected onto a subspace orthogonal to the previously computed eigenvectors . in detail ,",
    "if @xmath115 is to be computed , the newton step reads : @xmath116 where @xmath117 $ ] . in order to maintain the dimension of matrix @xmath118 sufficiently small ,",
    "two additional parameters are usually introduced .",
    "if the size of matrix @xmath118 is larger than @xmath119 then only the last @xmath120 columns of matrix @xmath2 are kept .",
    "even more than in the lanczos process , the solution of linear system ( [ jdsis ] ) must be found using an iterative method .",
    "this system is usually solved to a very low accuracy so that in practice few iterations ( 20 @xmath121 30 ) are sufficient to provide a good search direction @xmath107 .",
    "moreover , it has been proved in @xcite that linear system ( [ jdsis ] ) can be solved by the pcg method , despite of the fact that the system matrix is not symmetric positive definite .",
    "the resulting algorithm is very fast in computing the smallest eigenvalue provided that a good preconditioner is available for the matrix @xmath12 in order to solve efficiently the system ( [ jdsis ] ) .    = 0.6em=0.1em    choose unitary starting vector @xmath122 .",
    "initialize empty matrices @xmath123 and",
    "@xmath124 , @xmath125 .    while @xmath126    1",
    ".   @xmath127 .",
    "2 .   orthogonalize @xmath122 against @xmath2 via modified gram - schmidt .",
    "3 .   normalize @xmath70 . compute @xmath128 .",
    "@xmath129              \\qquad w : = [ w | w ] $ ] .",
    "compute the smallest eigenpair @xmath130 of @xmath118 ( with @xmath131 ) .",
    "compute the vector @xmath132 and the associated residual vector @xmath133 .",
    "7 .   if @xmath134   stop 8 .",
    "solve the linear system @xmath135 using the pcg method .",
    "end while      the sketch of the jacobi - davidson algorithm is reported in algorithm [ jd ] .",
    "step 5 implements the rayleigh - ritz projection .",
    "it is a crucial step for the convergence of the algorithm , but requires small cpu time since it consists in the eigensolution of the usually very small matrix @xmath118 .",
    "step 8 is the most relevant one from the viewpoint of computational cost .",
    "a good projected preconditioner should be devised in order to guarantee fast convergence of the pcg method .",
    "we used here as the preconditioner @xmath136 , with @xmath137 the same incomplete cholesky factorization employed by irlm .",
    "+ for the details of this method we refer to the paper  @xcite , as well as to successive works by @xcite who analyze both theoretically and experimentally a number of variants of this well known method .      instead of minimizing @xmath138 by newton s method",
    "the nonlinear conjugate gradient method can be employed . differently from the two methods",
    "just described , this one does not need any linear system solution . like the jd method",
    ", deflation accelerated conjugate gradient ( dacg ) computes the eigenvalues sequentially , starting from the smallest one @xcite .",
    "the leftmost eigenpairs are computed sequentially , by minimizing the rayleigh quotient over a subspace orthogonal to the previously computed eigenvectors .",
    "although not as popular as irlm and jd , this method , which applies only to symmetric positive definite matrices , has been proven very efficient in the solution of eigenproblems arising from discretization of partial differential equations ( pdes ) in @xcite dacg also proved very suited to parallel implementation as documented in @xcite where an efficient parallel matrix vector product has been employed .    convergence of dacg is strictly related to the relative separation between consecutive eigenvalues , namely @xmath139 when two eigenvalues are relatively very close , dacg convergence may be very slow .",
    "also dacg takes advantage of preconditioning which , as in the two previous approaches , can be chosen to be the incomplete cholesky factorization .",
    "= 1.0em=0.0em=0.5em    choose tolerance @xmath140 , set @xmath141 .",
    "do @xmath142    .",
    "= 3.0em=0.2em=-0.5em    choose @xmath143 such that @xmath144 ; set @xmath125 , @xmath145 ;    find the minimum of the rayleigh quotient @xmath146 for every @xmath147 such that @xmath144 by a nonlinear preconditioned conjugate gradient procedure .",
    "stop whenever the following test is satisfied : @xmath148    set @xmath149 .",
    "\\end{displaystyle}$ ]    end do      the dacg procedure is described in algorithm [ dacg ] .",
    "the pcg minimization of the rayleigh quotient ( step 2 ) is carried out by performing a number of iterations .",
    "the main computational burden of a single iteration is represented by :    1 .   one matrix - vector product .",
    "one application of the preconditioner .",
    "orthogonalization of the search direction against the previously computed eigenpairs ( columns of matrix @xmath150 ) .",
    "the cost of this step is increasing with the number of eigenpairs begin sought .",
    "as common in the iterative methods , the number of iterations can not be known in advance .",
    "however , it is known to be proportional to the reciprocal of the relative separation @xmath151 between consecutive eigenvalues ( equation ( [ xi ] ) ) .",
    "in this section we experimentally compare the three previously described solvers in the computation of some of the leftmost eigenpairs of a number of laplacian matrices of graphs arising from the following realistic applications covering all four main categories of real networks , namely biological networks , technological networks , information networks , and social networks :    1 .",
    "matrix protein represents the laplacian of the protein - protein interaction network of yeast @xcite . in a protein - protein interaction network the vertices are proteins and two vertices are connected by an undirected edge if the corresponding protein interact .",
    "2 .   matrix internet a symmetrized snapshot of the structure of the internet at the level of autonomous systems , reconstructed from bgp tables posted by the university of oregon route views project .",
    "this snapshot was created by mark newman and is not previously published .",
    "matrix www  is the laplacian of the web network within nd.edu domain @xcite .",
    "this network is directed but arc direction has been ignored in order to obtain a symmetric laplacian .",
    "matrix dblp is the laplacian of a graph describing collaboration network of computer scientists .",
    "nodes are authors and edges are collaborations in published papers , the edge weight is the number of publications shared by the authors @xcite .    in table",
    "[ char ] we report the number of matrix rows ( n ) , the number of matrix nonzero entries ( nnz ) , the average nonzeros per row ( anzr ) , which account for the sparsity of the matrix , and the ratio @xmath152 ( gap ) , which indicates how the smallest eigenvalues are separated .",
    "note that the number of nonzeros is computed as @xmath153 where @xmath72 is the number of arcs in the graph .",
    ".main characteristics of the sample matrices : size ( n ) , number of nonzero entries ( nnz ) , average nonzeros per row ( anzr ) and ratio between the largest and smallest computed eigenvalues ( gap ) . [",
    "cols=\"<,>,>,^,>\",options=\"header \" , ]     analyzing the results in tables [ internet ] , [ www ] , and [ dblp ] we can make the following observations :    1 .",
    "the irlm is almost always slower than the remaining two .",
    "this occurs since it requires many accurate inner linear system solutions .",
    "actually irlm implicitly computes the largest eigenpairs of @xmath66 .",
    "the latter matrix is not explicitly formed since it would have an excessive number of nonzeros .",
    "only the action of @xmath66 upon a vector is computed as a linear system solution .",
    "solving this system to a low accuracy would mean compute eigenpairs of a matrix different from @xmath12 thus introducing unacceptable errors .",
    "the jd algorithm displays the best performance in terms of number of mvp and cpu time .",
    "in particular on the largest problem , it neatly outperforms both dacg and irlm .",
    "the jd method inherits the nice convergence properties of the newton s method enhanced by the rayleigh - ritz acceleration .",
    "moreover , it allows very inaccurate ( and hence very cheap ) solution of the inner linear system .",
    "the dacg algorithm provides comparable performances with jd for a very small number of eigenpairs ( up to 5 ) and particularly when the eigenvalues are needed to a low accuracy . when the number of eigenvalues is large , the reorthogonalization cost prevails and makes this algorithm not competitive . for the sample tests presented in this paper , the dacg method",
    "is also penalized by the clustering of eigenvalues which results in a very small relative separation between consecutive eigenvalues .",
    "this argument applies also to matrix www  where , apart of the first 10 eigenvalues which are relatively well separated , see figure [ distribution ] , the remaining ones are as clustered as those of the other test problems .",
    "when few eigenpairs are to be computed ( and hence the reorthogonalization cost is not prevailing ) jd does not seem particularly sensitive to eigenvalue accuracy .",
    "this is not surprising as it is based on a newton iteration .",
    "this process is known to converge very rapidly in a neighborhood of the solution .",
    "for this reason , the transition between @xmath154 and @xmath155 tolerance is very fast .",
    "5 .   despite of the favorable distribution of the leftmost part of its eigenspectrum ( see figure [ distribution ] ) ,",
    "the number of iterations to eigensolve matrix www  is high for all the three solvers . for this test problem , the incomplete cholesky factorization with no fill - in preconditioner does not provide a satisfactory acceleration .",
    "the choice of a suitable preconditioner is crucial for the convergence of all iterative methods .",
    "devising a more `` dense '' preconditioner would improve the performance of all the methods described and particularly so for irlm and jd that explicitly require a linear system solution .",
    "we selected to use for the three methods the established implementations without making optimization to any of them .",
    "however , it is worth mentioning that much work is being devoted particularly to the arnoldi method ( the non symmetric counterpart of the lanczos method ) in order to reduce its computational cost and memory storage .",
    "we refer e.g. to the recent work by @xcite .",
    "other methods are efficiently employed for computing a number of eigenpairs of sparse matrices . among these , we mention the rayleigh quotient iteration whose inexact variant has been recently analyzed by  @xcite . a method which has some common features with dacg",
    "is lobpcg ( locally optimal block preconditioned conjugate gradient method ) which has been proposed by @xcite , and is currently available under the hypre package developed in the @xcite .",
    "we experimentally compare three iterative state - of - the - art algorithms for computation of eigenpairs of large and sparse matrices : the implicitly restarted lanczos method , the jacobi - davidson method , and the deflation accelerated conjugate gradient method .",
    "we uniformly implemented the algorithms and ran them in order to compute some of the smallest eigenpairs of the laplacian matrix of real - world networks of different sizes .",
    "the iterative approach followed in this work seems to be particularly suited for the laplacian matrices presented since it fully exploits the sparsity of the matrices involved .",
    "each of our realistic test cases , indeed , has a very high degree of sparsity as accounted for by the very small number of nonzeros per row .",
    "contrary to what observed for matrices arising from discretization of partial differential equations @xcite , where especially the smallest eigenvalues are well separated , here the high clustering of lowest eigenvalues is disadvantageous for the deflation accelerated conjugate gradient algorithm .",
    "as for the implicitly restarted lanczos method , the need to solve the inner linear systems to a high accuracy makes this method less attractive for large eigenproblems.the jacobi - davidson procedure is less sensitive to the clustering thanks to the rayleigh - ritz projection ; moreover , for this method , inexact and hence efficient solution of the inner linear systems is sufficient to achieve overall convergence .",
    "all in all , the jacobi - davidson algorithm is performing the best .",
    "this might be valuable information for popular scientific computing environments ( matlab /r ) , which only implement the implicitly restarted lanczos method .",
    "all the proposed algorithms are well - suited to parallelization on supercomputers .",
    "the most important kernel is represented by the matrix - vector product which can be efficiently implemented in parallel environments .",
    "also application of preconditioner , which is one of the most time - consuming task , in its turn can be devised as a product of sparse matrices as e.g. in the `` approximate inverse preconditioner '' approach ( see the review article by @xcite ) .",
    "the dacg method has been successfully parallelized as documented in  @xcite , however all the iterative solvers described here , being based on the same linear algebra kernels , could be implemented in parallel with the same satisfactory results    in our implementation we used a general purpose preconditioner , obtained by means of incomplete cholesky factorization with no fill in .",
    "certainly , the three methods would greatly benefit from the use of a more specific preconditioner .",
    "this point will be a topic of future research .",
    "25 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , , . . ,",
    "cms books in mathematics / ouvrages de mathmatiques de la smc , 15 . ed .",
    ". . , . , , ,",
    ". . , . , , ,",
    ". . . , , ,",
    ". . , . , ,",
    ". . , . , ,",
    ". . , . , ,",
    ". . , . , , ,",
    ". . , . , ,",
    ". . , . , , ,",
    ". . , . , , ,",
    ". . . , , , ,",
    ". . , . , ,",
    ". . , . , ,"
  ],
  "abstract_text": [
    "<S> the graph laplacian , a typical representation of a network , is an important matrix that can tell us much about the network structure . in particular its eigenpairs ( eigenvalues and eigenvectors ) </S>",
    "<S> incubate precious topological information about the network at hand , including connectivity , partitioning , node distance and centrality . </S>",
    "<S> real networks might be very large in number of nodes ( actors ) ; luckily , most real networks are sparse , meaning that the number of edges ( binary connections among actors ) are few with respect to the maximum number of possible edges . in this paper </S>",
    "<S> we experimentally compare three state - of - the - art algorithms for computation of a few among the smallest eigenpairs of large and sparse matrices : the implicitly restarted lanczos method , which is the current implementation in the most popular scientific computing environments ( matlab @xmath0 r ) , the jacobi - davidson method , and the deflation accelerated conjugate gradient method . </S>",
    "<S> we implemented the algorithms in a uniform programming setting and tested them over diverse real - world networks including biological , technological , information , and social networks . </S>",
    "<S> it turns out that the jacobi - davidson method displays the best performance in terms of number of matrix - vector products and cpu time .    </S>",
    "<S> graph laplacian ; eigenpair computation ; algorithms ; networks . </S>"
  ]
}