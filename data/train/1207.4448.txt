{
  "article_text": [
    "* motivation : * evolutionary algorithms or metaheuristics are efficient stochastic methods for solving a wide range of optimization problems . their performances are often subject to a correct setting of their parameters including the representation of solution , the stochastic operators such as mutation , or crossover , the selection operators , the rate of application of those operators , the stopping criterium , or the population size , etc . from the metaheuristics - user point of view",
    ", it could be a real challenge to choose the parameters , and understand this choice .",
    "moreover , when the algorithms evolve in a distributed environment , comes additional possible designs , or parameters such as the communication between distributed nodes , the migration policy , the distribution of the metaheuristics execution , or when different metaheuristics are used together , their distribution over the environement , etc .",
    "the aim of this paper is the parameter setting in a fully distributed environment .    * background and related work : * following the taxonomy of eiben et al .",
    "@xcite , we distinguish two types of parameter setting : the first one is _ off - line _ , before the actual run , often called _ parameter tuning _ , and the second one is _ on - line _ , during the run , called _",
    "parameter control_. usually , parameter tuning is done by testing all or a subset of possible parameters , and select the combination of parameters which gives the best performances .",
    "obviously , this method is time consuming .",
    "besides , a parameters setting at the beginning of the optimization process could be inappropriate at the end .",
    "several approaches are used for the parameters control .",
    "determinist ones choose an _ a - priori _ policy of modification .",
    "but , the choice of this policy gets more complicated compared to the static setting as the complexity of design increases .",
    "self - adaptive techniques encode the parameters into the solution and evolve them together .",
    "this approach is successfully applied in continuous optimization @xcite .",
    "nevertheless , in combinatorial optimization it often creates a larger search space leading to efficiency loss .",
    "finally , adaptive methods use the information from the history of the search to modify the parameters .",
    "the hyperheuristics are one example @xcite .",
    "they are heuristics that adaptively control ( select , combine , generate , or adapt ) other heuristics .",
    "another example is the adaptive operator selection ( aos ) which controls the application of the variation operator , using probability matching , adaptive pursuit @xcite , or multi - armed bandit @xcite techniques . in this work ,",
    "we focus on parameters control methods in a distributed setting .",
    "in fact , the increasing number of cpu cores , parallel machines like gpgpu , grids , etc . makes more and more the distributed environments the natural framework to design effective optimization algorithms .",
    "parallel evolutionary algorithms ( ea ) are well suited to distributed environments , and have a long history with many success @xcite . in parallel ea , the population is structured : the individuals interact only with their neighbor individuals .",
    "two main parallel ea models can be identified . in the island ( or multi - population ) one , the whole population is divided into several ones . in the cellular model ,",
    "the population is embedded on a regular toroidal grid .",
    "of course , between this coarse grained to fined grained , many variant exist .",
    "this field of research received more and more attention .",
    "recent theoretical and experimental works study the influence of parallelism @xcite , or study the fined grained population structure on performances @xcite .",
    "previous works control the specific parameters of parallel ea , or others parameters . in @xcite , the architecture of workers with a global controller",
    "is used to self - adapt the population size in each island .",
    "population size , and number of crossover points is controlled using the average value in the whole population in @xcite .",
    "those two examples of works use a parallel environment where a global information can be shared . in the work of tongchim",
    "@xcite , a distributed environment is considered for the parameters control .",
    "each island embeds two parameters settings .",
    "each parameters is evaluated on half the population , and the best parameter setting is communicated to the other islands which is used to produce new parameters .",
    "so , parameters are controlled in a self - adaptive way using local comparison between two settings only .",
    "laredo et al .",
    "@xcite propose the gossiping protocol newcast for p2p distributed population ea .",
    "the communication between individuals evolves towards small - world networks .",
    "thus , the selection pressure behaves like in panmictic population , but population diversity and system scalability outperform the fully connected population structure .    in summary ,",
    "four main issues has been studied in parameter control in parallel ea : static strategies where the parameters can be different for each island but does not change during the search ; strategies that use a global controller ; self - adaptive strategies where each node compares possible parameter settings ; and distributed strategies that evolve the communication between nodes . in this work",
    ", we propose a new and intuitive way to control the parameters in distributed environment : during the search , from a set of metaheuristics which correspond to possible parameter settings , each node select one metaheuristic to execute according to local information ( _ e.g. _ a performance measure ) given by the other nodes .",
    "* contributions and outline : * in this paper , we define a new adaptive parameters control method called distributed adaptive metaheuristic selection ( dams ) dedicated to distributed environments .",
    "dams is designed in the manner of heterogeneous island model eas where different metaheuristics live together . in this context , the distributed strategy ( select best and mutate strategy ) which selects either the best metaheuristic identified locally , or one random metaheuristic is defined and studied . for more simplicity , but also to enlighten the main features of this strategy , the sbm is given in the framework of dams .",
    "generally speaking , from an external observer , a distributed environment can be viewed as a unique global system which can be optimized in order to perform efficiently .",
    "however , the existence of such a global observer is not possible in practice nor mandatory . from dams point of view , the optimization process in such a distributed environment is though in a very local manner using only local information to coordinate the global distributed search and guide the optimization process .",
    "the issue of metaheuristic selection in distributed environments is introduced in section  [ sec : dams ] . from that issue",
    ", the dams framework is given in section  [ sec : dams ] bringing out three levels of design .",
    "the select best and mutate strategy ( sbm ) is then addressed in section  [ sec : simpledams ] .",
    "different sbm properties are studied in section  [ sec : expdams ] through extensive experimentations using the onemax problem .",
    "finally , in section  [ sec : conc ] we conclude with some open issues raised by dams .",
    "let us assume that to solve a given optimization problem , we can use a set @xmath0 of _ atomic function _ that we can apply in an _ iterative _ way on a solution or a population of solutions . by atomic function",
    "we mean a black - box having well defined input and output specifications corresponding to the problem being solved . in other words",
    ", we do not have any control on an atomic function once its is executed on some input population of solutions . to give a concrete example",
    ", an atomic function could be a simple ( deterministic or stochastic ) move according to a well defined neighborhood .",
    "it could also consist in applying the execution of a metaheuristic for fixed number of iterations .",
    "hence , an atomic function can be viewed from a very general perspective as a undividable , fixed and fully defined metaheuristic .",
    "so , in the following we call an atomic metaheuristic , or shorter a metaheuristic , the atomic functions .    in this context , where the atomic metaheuristics can be used in a sequential way , designing a strategy to solve an optimization problem turns out to design an iterative algorithm that carefully combines the atomic functions ( metaheuristics ) in a specific order . in this paper",
    ", we additionally assume that we are given a set of computational resources that can be used to run the atomic metaheuristics .",
    "these computational resources could be for instance a set of physical machines distributed over a network and exchanging messages , or some parallel processors having some shared memory to communicate .",
    "having such a distributed environment and the set of atomic metaheuristic at hand , the question we are trying to study in this paper is as following : _ how can we design an efficient distributed strategy to solve the optimization problem ?",
    "_ obviously , the performance of the designed strategy depends on how the _ distributed _ combination of the atomic operations is done . in particular",
    ", one have to _ adapt _ the search at runtime and decide _ distributively _ which atomic function should be applied at which time by which computational entity .",
    "the concept of dams introduced in this paper aims at defining in a simple way a general algorithmic framework allowing us to tackle the latter question .      to give an algorithm following the dams framework ,",
    "let us consider the following simple example .",
    "assume we have a network of @xmath1 nodes denoted by : @xmath2 .",
    "any two nodes , but nodes @xmath3 and @xmath4 ( see fig  [ fig : example ] ) , can communicate together by sending and receiving messages throughout the network .",
    "assume that the atomic operations we are given are actually composed of four population - solution based metaheuristics denoted by : @xmath2 , i.e. , each metaheuristic accepts as input a population of solutions and outputs a new one .",
    "then , starting with a randomly generated population , our goal is to design a distributed strategy that decides how to distribute the execution of our metaheuristics on the network and what kind of information should be exchanged by computational nodes to obtain the best possible solution . to make it simple ,",
    "assume that the best strategy , which can be considered as a distributed oracle strategy , is actually given by distributed algorithm  [ algo : example ] ( see also fig .",
    "[ fig : example ] for an illustration ) .",
    "in other words , any other distributed strategy can not outperforms algorithm  @xmath5 .",
    "generate a random solution on each node @xmath6 with @xmath7 run @xmath8 in parallel on each @xmath6 with @xmath7 exchange best found solution run @xmath9 in parallel on each @xmath6 with @xmath10 run @xmath11 in parallel on each @xmath6 with @xmath12 exchange best found solution run @xmath13 in parallel on each @xmath6 with @xmath7 return best found solution    . at the beginning",
    "all nodes @xmath2 run @xmath8 , then @xmath9 and @xmath11 are used , and finally @xmath13 concludes the distributed search .",
    "[ fig : example],scaledwidth=45.0% ]    having this example in mind , the question we are trying to answer is then : how can we design a distributed strategy which is competitive compared to the oracle of algorithm  [ algo : example ] ? finding such a strategy is obviously a difficult task . in fact , finding the best mapping of our metaheuristics into the distributed environment is in itself an optimization problem which could be even harder than the initial optimization problem we are trying to solve . in this context ,",
    "one possible solution is to map the metaheuristics into the distributed environment in an adaptive way in order to guide the distributed search and operate as close as possible of the oracle strategy ( algorithm  [ algo : example ] in our example ) .",
    "this is exactly the ultimate goal of the adaptive metaheuristic selection in distributed environments . in the following section ,",
    "we introduce a general algorithm , dams , and discuss its different components .",
    "generally speaking , a distributed adaptive metaheuristic selection ( dams ) is an adaptive strategy that allows computational nodes to coordinate their actions distributively by exchanging local information . based only on a local information",
    ", computational nodes should be able to make efficient local decisions that allows them to guide the global search efficiently ( i.e. , to be as efficient as possible compared to a distributed oracle strategy ) .    before going into further details and for the sake of simplicity",
    ", we shall abstract away the nature of the distributed environment under which a dams will be effectively implemented . for that purpose",
    ", we use a simple unweighted graph @xmath14 to model the distributed environment . a node @xmath15 models a computational node ( e.g. , a machine , a processor , a process , etc ) . an edge @xmath16 models a bidirectional direct link allowing neighboring nodes @xmath17 and @xmath18 to communicate together ( e.g. , by sending a message through a physical network , by writing in a shared local / distant memory , etc ) .    a high level overview of dams is given in distributed algorithm  [ algo : dams ] .",
    "notice that the high level code given in algorithm  [ algo : dams ] is executed ( in parallel ) by each node @xmath15 .",
    "the input of each node @xmath18 is a set @xmath0 of atomic metaheuristics .",
    "@xmath19@xmath20 @xmath21 @xmath22 ; +    each node @xmath18 participating into the computations has three _ local _ variables : a population @xmath23 which encodes a set of individuals .",
    "a set @xmath24 which encodes the local state of node @xmath18 .",
    "the local state @xmath24 of node @xmath18 is updated at each computation step and allows node @xmath18 to make local decisions .",
    "finally , each node has a current metaheuristic denoted by @xmath25 .",
    "as depicted in algorithm  [ algo : dams ] , a dams operates in many rounds until some stopping condition is satisfied .",
    "each round is organized in three levels .",
    "* the distributed level * : this level allows nodes to communicate together according to some distributed scheme and to update their local information .",
    "this step is guided by the current local information @xmath26 which encodes the experience of node @xmath18 during the distributed search .",
    "typically , a node can sends and/or receives a message to one or many of its neighbors to share some information about the ongoing optimization process . according to its own local information ( variable @xmath27 ) and",
    "the local information exchanged with neighbors ( variable @xmath28 ) , node @xmath18 can both update its local state @xmath24 and its local population @xmath23 . updating local state @xmath24 is intended to allow node @xmath18 to record the experience of its neighbors for future rounds and make local decisions accordingly . updating local population @xmath23 aims typically at allowing neighboring nodes to share some representative solutions found so far during the search , e.g. best individuals .",
    "this is the standard migration stage of parallel ea .",
    "finally , we remark that since at each round local information @xmath27 of node @xmath18 is updated with the experience of its neighbors , then after some rounds of execution , we may end with a local information @xmath27 that reflects the search experience of _ not _ only @xmath18 s neighbors but of other nodes being farther away in the network graph @xmath29 , e.g. , the best found solution or the best metaheuristic could be spread through the network with a broadcast communication process .",
    "* the metaheuristic selection level * : this level allows a node to locally select a specific metaheuristic @xmath25 to apply from set @xmath0 of available metaheuristics .",
    "first , we remark that this level may _ not _ be independent of the distributed level .",
    "in fact , the decision of selecting a metaheuristic is guided by local information @xmath27 which is updated at the distributed level .",
    "although the metaheuristic choice is executed locally and independently by each node @xmath18 , it may _ not _ be independent of other nodes choices .",
    "in fact , the local communication step shall allow nodes to coordinate their decisions and define distributively a cooperative strategy .",
    "second , since at the distributed level , local information @xmath27 of a a given node @xmath18 could store information about the experience of other nodes in the network , the metaheuristic choice can clearly be adapted accordingly .",
    "therefore , the main challenge when designing an efficient dams is to coordinate the metaheuristic selection level and the distributed level in order to define the best distributed optimization strategy .    * the atomic low level * : at this level a node simply executes the selected metaheuristic @xmath25 using its population @xmath23 as input .",
    "notice that atomic refers to the fact that a node can not control the sequential metaheuristic execution in any way .",
    "however , after applying a given metaheuristic , the population and the local state of each node may be updated .",
    "for instance , one can decide to move to the new population following a given criterion or to simply record some information about the quality of the outputted population using local state @xmath24 .",
    "the originality of dams framework is to introduce a metaheuristic selection level which adaptively select a metaheuristic according to local information shared by the other neighboring nodes . in this section ,",
    "we give a metaheuristic selection strategy , the select best and mutate strategy ( sbm ) , which specify the dams .",
    "for the sake of clarity , we consider the classical message passing model : we consider an @xmath30-node simple graph @xmath14 to model a distributed network .",
    "two nodes @xmath17 and @xmath18 , such that @xmath31 , can communicate together by sending and receiving messages toward edge @xmath32 .",
    "a high level description of sbm is given in algorithm  [ algo : sbm ] and is discussed in next paragraphs .",
    "note that the code of algorihtm  [ algo : sbm ] is to be executed ( in parallel ) on every node @xmath15 .",
    "the algorithm accepts as input a finite set of metaheuristics @xmath33 and a probability parameter @xmath34 .",
    "@xmath35 @xmath36 @xmath37    clearly , sbm follows the general scheme of a dams defined previously in algorithm  [ algo : dams ]",
    ". one can distinguish the three basic levels of a dams and remark their inter - dependency .",
    "+ * sbm distributed level * : the local information defined by a dams is encoded implicitly in sbm using variables @xmath38 , @xmath39 and @xmath23 . for every node @xmath18 ,",
    "variable @xmath39 refers to metaheuristic @xmath40 considered currently by @xmath18 .",
    "variable @xmath38 ( reward ) refers to the quality of metaheuristic @xmath40 .",
    "variable @xmath23 denotes the current population of node @xmath18 .",
    "sbm operates in many rounds until a stopping condition is satisfied . at each round ,",
    "the distributed level consist in sending a message containing triple @xmath41 to neighbors and symmetrically receiving the respective triples @xmath42 sent by neighbors . then after , node @xmath18 constructs two sets @xmath43 and @xmath24 .",
    "set @xmath43 contains information about neighbors populations .",
    "node @xmath18 can then update its current population @xmath23 according to set @xmath43 .",
    "for instance , one may selects the best received individuals , or even apply some adaptive strategy taking into account the search history . as for set @xmath24",
    ", it gathers information about the quality of other metaheuristics considered by neighbors .",
    "this set is used at the next dams level to decide on the new current metaheuristic to be chosen by node @xmath18 . + * sbm metaheuristic selection level * : choosing a new metaheuristic is guided by two ingredients .",
    "firstly , using set @xmath24 , node @xmath18 selects a metaheuristic according to neighbors information .",
    "one can imagine many strategies for selecting a metaheuristic possibly depending not only on @xmath24 but also on received populations @xmath43 .",
    "in sbm , we simply select metaheuristic @xmath44 corresponding to the best received reward @xmath38 .",
    "notice that defining what is the best observed metaheuristic could be guided by different policies . from an exploitation point of view , function @xmath45 has the effect of pushing nodes to execute the metaheuristic with the best observed performance during the search process . obviously , a metaheuristic with good performance at some round could quickly becomes inefficient as the search progresses . to control this issue",
    ", we introduce a _ exploration _ component in the selection level . in sbm",
    ", this component is simply guided by a metaheuristic mutation operator .",
    "in fact , every node decides to select at random another metaheuristic different from @xmath46 with rate @xmath34 .",
    "intuitively , these two ingredients in sbm selection level shall allow distributed nodes to obtain a good tradeoff between exploitation and exploration of metaheuristics , and adapt their decisions according to the search . + *",
    "sbm atomic low level * : once a new metaheuristic @xmath47 is selected , it is used to compute a new population . to evaluate the performance of metaheuristic @xmath40 ,",
    "sbm simply compares the previous population @xmath23 with the new population @xmath48 using a generic @xmath49 function .",
    "for instance , a simple evaluation strategy could be by comparing the best individual fitness , the average population fitness , or even more sophisticated adaptive strategies taking into account the performances observed in previous rounds .",
    "in this section , we study a specific sbm - dams by fully specifying its different levels . we report the results we have obtained by conducting experimental campaigns using the onemax problem .      to conduct experimental study of the sbm - dams , we test the algorithm on a classical problem in ea , the _ onemax _ problem .",
    "this problem was used in recent works of fialho et al .",
    "@xcite which propose a ( sequential ) adaptive operator selection ( aos ) method based on the multi - armed bandits , and a credit assignment method which uses the fitness comparison .",
    "the onemax problem , the `` drosophila '' of evolutionary computation , is a unimodal problem defined on binary strings of size @xmath50 .",
    "the fitness is the number of `` @xmath5 '' in the bit - string . within the aos framework ,",
    "the authors in  @xcite considered @xmath51ea and four mutation operators to validate their approach : the standard @xmath52 bit - flip operator ( every bit is flipped with the binomial distribution of parameter @xmath52 where @xmath50 is the length of the bit - strings ) , and the @xmath53bit , @xmath54bit , and @xmath55bit mutation operators ( the @xmath56bit mutation flips exactly @xmath57 bits , uniformly selected in the parents ) . in the rest of the paper , we shall also consider the same four atomic metaheuristics to study dams .",
    "each atomic function is one iteration of @xmath58-ea using one of the four mutation operators .",
    "unless stated explicitly , parameter @xmath59 is set to @xmath60 as in @xcite .",
    "notice also by studying sbm - dams with these well - understood operators does not undergo any major weakness of our approach since these operators mainly exhibits different exploration degrees that one can encounter in other settings when using other operators .    in all reported experiments ,",
    "the length of the bit strings is set to @xmath61 as in @xcite .",
    "population @xmath23 of sbm - dams is reduced to a single solution @xmath62 .",
    "the initial solution is set to @xmath63 .",
    "the rewards @xmath38 is the fitness gain between parent and offspring solutions : @xmath64 .",
    "the migration policy ( update_population ) is elitist , it replaces the current solution by one of the best received solutions if its fitness is strictly higher .",
    "the algorithm stops when the maximal fitness @xmath50 is reached .",
    "this allows us to compare the performances of sbm - dams according to the number of evaluations as used in sequential algorithms ; but also in terms of rounds ( total number of migration exchange ) as used in parallel frameworks , and in terms of messages cost that is the total number of local communications made through the distributed environment .    in the remainder",
    ", the _ onemax _ experimental protocol is used to first compare the efficiency of sbm - dams to some oracle and naive strategies , then the parallel properties are analyzed with a comparison to the sequential aos method .",
    "three network topologies are studied : complete , grid , and cycle . in the complete one ,",
    "the graph is a clique , i.e. , every node is linked to all other nodes . in the grid topology ,",
    "every node can communicate with four other neighbors except at the edge of the grid ( non - toroidal grid ) . in the circle topology",
    ", nodes are linked to two others nodes to form a circle .    since there exist",
    "no previous _ distributed _ approaches addressing the same issues than dams , we shall compare sbm - dams to the state - of - the - art sequential adaptive approaches ( section  [ sec : parallelism ] ) and also to two simple distributed strategies ( section  [ sec : adaptation ] ) called rnd - dams and seqoracle - dams .",
    "( i )  in rnd - dams , each node selects at random ( independently of node states ) at each round a metaheuristic to be executed .",
    "it allows us to evaluate the efficiency of sbm selection method based on the best instant rewards .",
    "( ii )  in seqoracle - dams , each node executes the sequential oracle which gives the operator with the maximum fitness gain according to the fitness of node current solution .",
    "the sequential oracle is taken from the paper @xcite .",
    "it is used independently by each node without taking into account rewards , or observed performances of others nodes . notice that that seqoracle - dams outperforms a static strategy which use only one metaheuristic , but it is different from a pure distributed oracle which could lead to better performances for the whole distributed system , by taking take into account all nodes to select the metaheuristic of each node .    for the latter two based - line strategies , we use the same elitism migration policy as sbm - dams and evaluate their performances using the same three topologies . for each topology ,",
    "network size is @xmath65 , and the performance measures are reported over @xmath66 independent runs .",
    "notice that for this particular study , the number of evaluations is @xmath67 times the number of rounds .",
    "the number of exchanged messages is @xmath68 times the number of rounds where @xmath68 is the number of communication links in the considered topology .      to tune off - line , the metaheuristics mutation rate of sbm - dams , we perform a design of experiments campain with @xmath69 @xmath70 .",
    "[ fig : rnd - mutation ] shows the average number of rounds to reach the optimum according to mutation rate @xmath34 for different topologies and network sizes . except for small size @xmath71 ,",
    "the range of performances is small according to the mutation rate .",
    "[ cols=\"^ \" , ]",
    "in this paper , we have proposed a new distributed adaptive method , the select best and mutate strategy ( sbm ) , in the generic framework of distributed adaptive metaheuristic selection ( dams ) based on a three - layer architecture .",
    "sbm strategy selects locally the metaheuristic with the best instant reward and mutates it according to a mutation rate parameter .",
    "the conducted experimental study shows sbm robustness and efficiency compared to naive distributed and sequential strategies .",
    "many futures studies and open question are suggested by dams .",
    "firstly , it would be interesting to study dams for other problems and using other eas .",
    "for instance , when considering population - based metaheuristics many issues could be addressed for population migration and population update . in particular",
    ", one can use some distributed adaptive policies to guide population evolution .",
    "adaptive migration taking into account the performance of crossover operators should also be introduced and studied .",
    "dams could also be particularly accurate to derive new algorithms to tackle multi - objective optimization problems , since , for instance , one can use the parallelism properties of dams to explore different regions of the search space by adapting the search process according to different objectives .    for sbm - dams ,",
    "many questions remain open .",
    "for instance , one may ask at what extent one can design a distributed strategy based on multi - armed bandits techniques to select metaheuristics , compute metaheuristic rewards , and adapt the search accordingly .",
    "further work needs also to be conducted to study the impact of the distributed environment .",
    "for instance , asynchrony and low level distributed issues on concrete high performance and large scale computing platforms , such as grid and parallel machines , should be conducted .",
    "another challenging issue is to adapt local communications between nodes , for instance , to use the minimum number of messages and obtain the maximum performances . in particular , it is not clear how to tune the number of nodes and the topology in a dynamic way in order to balance the communication cost and the computation time , i.e. , parallel efficiency . from the theoretical side",
    ", we also plan to study the parallelism of sbm analytically and its relation to a fully distributed oracle that still needs to be defined .",
    "x.  bonnaire and m .- c .",
    "riff . using self - adaptable probes for dynamic parameter control of parallel evolutionary algorithms . in _ foundations of intelligent systems _ ,",
    "volume 3488 of _ lncs _ , pages 237261 . 2005 .",
    "l.  da costa , a.  fialho , m.  schoenauer , and m.  sebag .",
    "adaptive operator selection with dynamic multi - armed bandits . in _",
    "10th acm conf . on genetic and evolutionary computation ( gecco08 ) _ , pages 913920 , 2008 .",
    "j.  l.  j. laredo , j.  j. merelo , c.  fernandes , a.  mora , m.  i.  g. arenas , p.  castillo , and p.  g. sanchez . analysing the performance of different population structures for an agent - based evolutionary algorithm . in _ learning and intelligent optimization ( lion05 )",
    "_ , pages 5054 , 2011 .",
    "j.  lssig and d.  sudholt .",
    "general scheme for analyzing running times of parallel evolutionary algorithms . in _",
    "parallel problem solving from nature ppsn - xi _ ,",
    "volume 6238 of _ lncs _ , pages 234243 ."
  ],
  "abstract_text": [
    "<S> we present a distributed algorithm , select best and mutate ( sbm ) , in the distributed adaptive metaheuristic selection ( dams ) framework . </S>",
    "<S> dams is dedicated to adaptive optimization in distributed environments . given a set of metaheuristics , </S>",
    "<S> the goal of dams is to coordinate their local execution on distributed nodes in order to optimize the global performance of the distributed system . </S>",
    "<S> dams is based on three - layer architecture allowing nodes to decide distributively what local information to communicate , and what metaheuristic to apply while the optimization process is in progress . </S>",
    "<S> sbm is a simple , yet efficient , adaptive distributed algorithm using an exploitation component allowing nodes to select the metaheuristic with the best locally observed performance , and an exploration component allowing nodes to detect the metaheuristic with the actual best performance . </S>",
    "<S> sbm features are analyzed from both a parallel and an adaptive point of view , and its efficiency is demonstrated through experimentations and comparisons with other adaptive strategies ( sequential and distributed ) .    </S>",
    "<S> = 10000 = 10000    [ heuristic methods ] </S>"
  ]
}