{
  "article_text": [
    "in this paper we apply the multi - adaptive galerkin methods @xmath0  and @xmath1 , presented in @xcite , to a variety of problems chosen to illustrate the potential of multi - adaptivity . throughout this paper , we solve the ode initial value problem @xmath2 , \\\\",
    "u(0 ) & = & u_0 , \\end{array } \\right .",
    "\\label{eq : u'=f}\\ ] ] where @xmath3 \\rightarrow { \\mathbb{r}}^n$ ] , @xmath4 \\rightarrow { \\mathbb{r}}^n$ ] is a given bounded function that is lipschitz continuous in @xmath5 , @xmath6 is a given initial condition , and @xmath7 a given final time .",
    "we refer to @xcite for a detailed description of the multi - adaptive methods . here",
    "we recall that each component @xmath8 of the approximate solution @xmath9 is a piecewise polynomial of degree @xmath10 on a partition of @xmath11 $ ] into @xmath12 subintervals of lengths @xmath13 , @xmath14 . on the interval",
    "@xmath15 $ ] , component @xmath8 is thus a polynomial of degree @xmath16 .    before presenting the examples",
    ", we discuss adaptive algorithms for global error control and iterative solution methods for the discrete equations .",
    "we also give a short description of the implementation _",
    "in this section we describe how to use the a posteriori error estimates presented in @xcite in an adaptive algorithm .",
    "the goal of the algorithm is to produce an approximate solution @xmath9 to ( [ eq : u=f ] ) within a given tolerance @xmath17 for the error @xmath18 in a given norm @xmath19 .",
    "the adaptive algorithm is based on the a posteriori error estimates , presented in @xcite , of the form @xmath20 or @xmath21 where @xmath22 are @xmath23 , @xmath24 are @xmath25 ( including interpolation constants ) , @xmath26 is a local measure of the residual @xmath27 of the approximate solution @xmath9 , and where we have @xmath28 for @xmath0  and @xmath29 for @xmath1 .",
    "we use ( [ eq : estimate,2 ] ) to determine the individual time - steps , which should then be chosen as @xmath30 we use ( [ eq : estimate,1 ] ) to evaluate the resulting error at the end of the computation , noting that ( [ eq : estimate,1 ] ) is sharper than ( [ eq : estimate,2 ] ) .    the adaptive algorithm",
    "may then be expressed as follows : given a tolerance @xmath31 , make a preliminary guess for the stability factors and then    1 .",
    "solve the primal problem with time - steps based on ( [ eq : timestep ] ) .",
    "2 .   solve the dual problem and compute stability factors and stability weights .",
    "compute an error estimate @xmath32 based on ( [ eq : estimate,1 ] ) .",
    "4 .   if @xmath33 , then stop , and if not , go back to ( i ) .",
    "although this seems simple enough , there are some difficulties involved . for one thing , choosing the time - steps based on ( [ eq : timestep ] ) may be difficult , since the residual depends implicitly on the time - step .",
    "furthermore , we have to choose the proper data for the dual problem to obtain a meaningful error estimate .",
    "we now discuss these issues .      to avoid the implicit dependence on @xmath34 for @xmath26 in ( [ eq : timestep ] )",
    ", we may try replacing ( [ eq : timestep ] ) with @xmath35 following this strategy , if the time - step on an interval is small ( and thus also is the residual ) , the time - step for the next interval will be large , so that ( [ eq : timestep , alternative ] ) introduces unwanted oscillations in the size of the time - step .",
    "we therefore try to be a bit more conservative when choosing the time - step to obtain a smoother time - step sequence . for ( [ eq : timestep , alternative ] ) to work , the time - steps on adjacent intervals need to be approximately the same , and so we may think of choosing the new time - step as the ( geometric ) mean value of the previous time - step and the time - step given by ( [ eq : timestep , alternative ] ) .",
    "this works surprisingly well for many problems , meaning that the resulting time - step sequences are comparable to what can be obtained with more advanced regulators .",
    "we have also used standard _ pid _",
    "( or just _ pi _ ) regulators from control theory with the goal of satisfying @xmath36 or , taking the logarithm with @xmath37 , @xmath38 with maximal time - steps @xmath39 , following work by sderlind @xcite and gustafsson , lundh , and sderlind @xcite .",
    "this type of regulator performs a little better than the simple approach described above , provided the parameters of the regulator are well tuned .",
    "different choices of data @xmath40 and @xmath41 for the dual problem give different error estimates , as described in @xcite , where estimates for the quantity @xmath42 were derived .",
    "the simplest choices are @xmath43 and @xmath44 for control of the final time error of the @xmath45th component . for control of the @xmath46-norm of the error at final time",
    ", we take @xmath43 and @xmath47 with an approximation @xmath48 of the error @xmath49 .",
    "another possibility is to take @xmath50 and @xmath51 for control of the average error in component @xmath45 .",
    "if the data for the dual problem are incorrect , the error estimate may also be incorrect : with @xmath40 ( or @xmath41 ) orthogonal to the error , the error representation gives only @xmath52 . in practice , however , the dual  or at least the stability factors  seems to be quite insensitive to the choice of data for many problems so that it is , in fact , possible to guess the data for the dual .",
    "in practice , integrals included in the formulation of the two methods @xmath0  and @xmath1  have to be evaluated using numerical quadrature . to control the resulting quadrature error ,",
    "the quadrature rule can be chosen adaptively , based on estimates of the quadrature error presented in @xcite .",
    "the formulations of the methods include the possibility of individual and varying orders for the different components , as well as different and varying time - steps .",
    "the method is thus @xmath53__-adaptive _ _ ( or @xmath54-adaptive ) in the sense that the order can be changed . at this stage , however , lacking a strategy for when to increase the order and when to decrease the time - step , the polynomial orders have to be chosen in an ad hoc fashion for every interval .",
    "one way to choose time - steps and orders could be to solve over some short time - interval with different time - steps and orders , and optimize the choice of time - steps and orders with respect to the computational time required for achieving a certain accuracy .",
    "if we suspect that the problem will change character , we will have to repeat this procedure at a number of control points .",
    "in this section we discuss how to solve the discrete equations that we obtain when we discretize ( [ eq : u=f ] ) with the multi - adaptive galerkin methods .",
    "we do this in two steps .",
    "first , we present a simple explicit strategy , and then we extend this strategy to an iterative method .",
    "as discussed in @xcite , the nonlinear discrete algebraic equations for the @xmath0  method ( including numerical quadrature ) to be solved on every local interval @xmath55 take the form @xmath56 } \\",
    "f_i(u(\\tau_{ij}^{-1}(s_{n}^{[q_{ij}]})),\\tau_{ij}^{-1}(s_n^{[q_{ij } ] } ) ) , \\ m = 1,\\ldots , q_{ij } , \\label{eq : equations}\\ ] ] where @xmath57 are the degrees of freedom to be determined for component @xmath8 on interval @xmath55 , @xmath58}\\}_{m=1,n=0}^{q_{ij}}$ ] are weights , @xmath59}\\}_{n=0}^{q_{ij}}$ ] are quadrature points , and @xmath60 maps @xmath55 to @xmath61 $ ] : @xmath62 .",
    "the discrete equations for the @xmath1  method are similar in structure and so we focus on the @xmath0  method .    the equations are conveniently written in fixed point form , so we may apply fixed point iteration directly to ( [ eq : equations ] ) ; i.e. , we make an initial guess for the values of @xmath57 , e.g. , @xmath63 for @xmath64 , and then compute new values for these coefficients from ( [ eq : equations ] ) , repeating the procedure until convergence .",
    "note that component @xmath8 is coupled to all other components through the right - hand side @xmath65 .",
    "this means that we have to know the solution for all other components in order to compute @xmath8 .",
    "conversely , we have to know @xmath8 to compute the solutions for all other components , and since all other components step with different time - steps , it seems at first very difficult to solve the discrete equations ( [ eq : equations ] ) .    as an initial simple strategy we may try to solve the system of nonlinear equations ( [ eq : equations ] ) by direct fixed point iteration .",
    "all unknown values , for the component itself and all other _ needed _ components , are interpolated or extrapolated from their latest known values . thus ,",
    "if for component @xmath66 we need to know the value of component @xmath67 at some time @xmath68 , and we only know values for component @xmath67 up to time @xmath69 , the strategy is to extrapolate @xmath70 from the interval containing @xmath71 to time @xmath68 , according to the order of @xmath70 on that interval .    in what order should",
    "the components now make their steps ? clearly , to update a certain component on a specific interval , we would like to use the best possible values of the other components .",
    "this naturally leads to the following strategy : @xmath72 this means that we should always make a step with the component that is closest to time @xmath73 .",
    "eventually ( or after one step ) , this component catches up with one of the other components , which then in turn becomes the last component , and the procedure continues according to the strategy ( [ eq : strategy ] ) , as described in figure [ fig : stepping ] .",
    "this gives an explicit time - stepping method in which each component is updated individually once , following ( [ eq : strategy ] ) , and in which we never go back to correct mistakes .",
    "this corresponds to fixed point iterating once on the discrete equations ( [ eq : equations ] ) , which implicitly define the solution .",
    "we now describe how to extend this explicit time - stepping strategy to an iterative process , in which the discrete equations are solved to within a prescribed tolerance .",
    "to extend the explicit strategy described in the previous section to an iterative method , we need to be able to go back and redo iterations if necessary .",
    "we do this by arranging the elements  we think of an element as a component @xmath8 on a local interval @xmath55in a _ time - slab_. this contains a number of elements , a minimum of @xmath74 elements , and moves forward in time . on every time - slab",
    ", we have to solve a large system of equations , namely , the system composed of the element equations ( [ eq : equations ] ) for every element within the time - slab .",
    "we solve this system of equations iteratively , by direct fixed point iteration , or by some other method as described below , starting from the last element in the time - slab , i.e. , the one closest to @xmath73 , and continuing forward to the first element in the time - slab , i.e. , the one closest to @xmath75 .",
    "these iterations are then repeated from beginning to end until convergence , which is reached when the computational residuals , as defined in @xcite , on all elements are small enough",
    ".      the time - slab can be constructed in many ways .",
    "one is by _ dyadic _ partitioning , in which we compute new time - steps for all components , based on residuals and stability properties , choose the largest time - step @xmath76 as the length of the new time - slab , and then , for every component , choose the time - step as a fraction @xmath77 .",
    "the advantage of such a partition are that the time - slab has _ straight edges _ ;",
    "i.e. , for every time - slab there is a common point in time @xmath78 ( the end - point of the time - slab ) which is an end - point for the last element of every component in the time - slab , and that the components have many common nodes .",
    "the disadvantage is that the choice of time - steps is constrained .",
    "another possibility is a _ rational _ partition of the time - slab .",
    "we choose the largest individual time - step @xmath76 as the length of the time - slab , and time - steps for the remaining components are chosen as fractions of this large time - step , @xmath79 , @xmath80 , @xmath81 , and so on . in this way we increase the set of possible time - step selections , as compared to dyadic partitioning , but the number of common nodes shared between different components is decreased .",
    "a third option is to not impose any constraint at all on the selection of time - steps  except that we match the final time end - point .",
    "the time - steps may vary also within the time - slab for the individual components .",
    "the price we have to pay is that we have in general no common nodes , and the edges of the time - slab are no longer straight .",
    "we illustrate the three choices of partitioning schemes in figure [ fig : timeslabs ] .",
    "although dyadic or rational partitioning is clearly advantageous in terms of easier bookkeeping and common nodes , we focus below on unconstrained time - step selection . in this way",
    "we stay close to the original , general formulation of the multi - adaptive methods .",
    "we refer to this as the _ time - crawling _ approach .",
    ".,height=170 ]    .,height=170 ]    .,height=170 ]      the construction of the time - slab brings with it a number of technical and algorithmic problems .",
    "we will not discuss here the implementational and data structural aspects of the algorithm  there will be much to keep track of and this has to be done in an efficient way  but we will give a brief account of how the time - slab is formed and updated .",
    "assume that in some way we have formed a time - slab , such as the one in figure [ fig : timeslab ] .",
    "we make iterations on the time - slab , starting with the last element and continuing to the right .",
    "after iterating through the time - slab a few times , the computational ( discrete ) residuals , corresponding to the solution of the discrete equations ( [ eq : equations ] ) , on all elements have decreased below a specified tolerance for the computational error , indicating convergence .    for the elements at the front of the slab ( those closest to time @xmath75 ) , the values have been computed using extrapolated values of many of the other elements .",
    "the strategy now is to leave behind _ only those elements that are fully covered by all other elements_. these are then cut off from the time - slab , which then decreases in size . before starting the iterations again",
    ", we have to form a new time - slab .",
    "this will contain the elements of the old time - slab that were not removed , and a number of new elements .",
    "we form the new time - slab by requiring that all elements of the previous time - slab be totally covered within the new time - slab . in this way we know that every new time - slab will produce at least @xmath74 new elements .",
    "the time - slab is thus crawling forward in time rather than marching .",
    "an implementation of the method then contains the three consecutive steps described above : iterating on an existing time - slab , decreasing the size of the time - slab ( cutting off elements at the end of the time - slab , i.e. , those closest to time @xmath73 ) , and incorporating new elements at the front of the time - slab .",
    "even if an element within the time - slab is totally covered by all other elements , the values on this element still may not be completely determined , if they are based on the values of some other element that is not totally covered , or if this element is based on yet another element that is not totally covered , and so on . to avoid this , one can impose the requirement that the time - slabs should have straight edges .          for stiff problems",
    "the time - step condition required for convergence of direct fixed point iteration is too restrictive , and we need to use a more implicit solution strategy .    applying a full newton s method",
    ", we increase the range of allowed time - steps and also the convergence rate , but this is costly in terms of memory and computational time , which is especially important in our setting , since the size of the slab may often be much larger than the number of components , @xmath74 ( see figure [ fig : timeslab ] ) .",
    "we thus look for a simplified newton s method which does not increase the cost of solving the problem , as compared to direct fixed point iteration , but still has some advantages of the full newton s method .",
    "consider for simplicity the case of the multi - adaptive backward euler method , i.e. , the @xmath82 method with end - point quadrature . on every element",
    "we then want to solve @xmath83 in order to apply newton s method we write ( [ eq : euler ] ) as @xmath84 with @xmath85 and @xmath86 .",
    "newton s method is then @xmath87 we now simply replace the jacobian with its diagonal so that for component @xmath66 we have @xmath88 with the right - hand side evaluated at @xmath89 .",
    "we now note that we can rewrite this as @xmath90 with @xmath91 so that we may view the simplified newton s method as a damped version , with damping @xmath92 , of the original fixed point iteration .",
    "the individual damping parameters are cheap to compute .",
    "we do not need to store the jacobian and we do not need linear algebra . we still obtain some of the good properties of the full newton s method .    for the general @xmath0  or @xmath1  method , the same analysis applies . in this case , however , when we have more degrees of freedom to solve for on every local element , @xmath93 will be a small local matrix of size @xmath94 for the @xmath0  method and size @xmath95 for the @xmath1  method .",
    "both @xmath0  and @xmath1  are implicit methods since they are implicitly defined by the set of equations ( [ eq : equations ] ) on each time - slab . however , depending on the solution strategy for these equations , the resulting fully discrete scheme may be of more or less explicit character . using a diagonal newton s method as in the current implementation of tanganyika , we obtain a method of basically explicit nature .",
    "this gives an efficient code for many applications , but we may expect to meet difficulties for stiff problems .      in a stiff problem",
    "the solution varies quickly inside transients and slowly outside transients . for accuracy the time - steps",
    "will be adaptively kept small inside transients and then will be within the stability limits of an explicit method , while outside transients larger time - steps will be used . outside the transients the diagonal newton s method handles stiff problems of sufficiently diagonal nature .",
    "otherwise the strategy is to decrease the time - steps whenever needed for stability reasons .",
    "typically this results in an oscillating sequence of time - steps where a small number of large time - steps are followed by a small number of stabilizing small time - steps .",
    "our solver tanganyika thus performs like a modern unstable jet fighter , which needs small stabilizing wing flaps to follow a smooth trajectory .",
    "the pertinent question is then the number of small stabilizing time - steps per large time - step .",
    "we analyze this question in @xcite and show that for certain classes of stiff problems it is indeed possible to successfully use a stabilized explicit method of the form implemented in tanganyika .",
    "there are many `` magic numbers '' that need to be computed in order to implement the multi - adaptive methods , such as quadrature points and weights , the polynomial weight functions evaluated at these quadrature points , etc . in tanganyika ,",
    "these numbers are computed at the startup of the program and stored for efficient access .",
    "although somewhat messy to compute , these are all computable by standard techniques in numerical analysis ; see , e.g. , @xcite .",
    "in addition to solving the primal problem ( [ eq : u=f ] ) , we also have to solve the continuous dual problem to obtain error control .",
    "this is an ode in itself that we can solve using the same solver as for the primal problem .    in order to solve this ode",
    ", we need to know the jacobian of @xmath96 evaluated at a mean value of the true solution @xmath97 and the approximate solution @xmath9 .",
    "if @xmath9 is sufficiently close to @xmath5 , which we will assume , we approximate the ( unknown ) mean value by @xmath9 .",
    "when solving the dual , the primal solution must be accessible , and the jacobian must be computed numerically by difference quotients if it is not explicitly known .",
    "this makes the computation of the dual solution expensive .",
    "error control can , however , be obtained at a reasonable cost : for one thing , the dual problem does not have to be solved with as high a precision as the primal problem ; a relative error of , say , @xmath98 may be disastrous for the primal , whereas for the dual this only means that the error estimate will be off by @xmath98 , which is acceptable .",
    "second , the dual problem is linear , which may be taken into account when implementing a solver for the dual . if we can afford the linear algebra , as we can for reasonably small systems , we can solve the discrete equations directly without any iterations .",
    "we now give a short description of the implementation of the multi - adaptive methods , _ tanganyika _ , which has been used to obtain the numerical results presented below .",
    "the purpose of tanganyika @xcite is to be a working implementation of the multi - adaptive methods .",
    "the code is open - source ( gnu gpl @xcite ) , which means that anyone can freely review the code , which is available at http://www.phi .",
    "/ tanganyika/. comments are welcome .",
    "the solver is implemented as a c / c++ library .",
    "the c++ language makes abstraction easy , which allows the implementation to follow closely the formulation of the two methods .",
    "different objects and algorithms are thus implemented as c++ classes , including ` solution ` , ` element ` , ` cgqelement ` , ` dgqelement ` , ` timeslab ` , ` errorcontrol ` , ` galerkin ` , ` component ` , and so on .",
    "in this section , we present numerical results for a variety of applications .",
    "we discuss some of the problems in detail and give a brief overview of the rest .",
    "a more extensive account can be found in @xcite .      to demonstrate the potential of the multi - adaptive methods , we consider a dynamical system in which a small part of the system oscillates rapidly .",
    "the problem is to accurately compute the positions ( and velocities ) of the @xmath74 point - masses attached with springs of equal stiffness , as in figure [ fig : multiadaptivity - system ] .",
    "masses attached with springs . ]",
    "if we choose one of the masses to be much smaller than the others , @xmath99 and @xmath100 , then we expect the dynamics of the system to be dominated by the smallest mass , in the sense that the resolution needed to compute the solution will be completely determined by the fast oscillations of the smallest mass .    to compare the multi - adaptive method with a standard method , we first compute with constant time - steps @xmath101 using the standard @xmath102 method and measure the error , the cpu time needed to obtain the solution , the total number of steps , i.e. , @xmath103 , and the number of local function evaluations .",
    "we then compute the solution with individual time - steps , using the @xmath104 method , choosing the time - steps @xmath105 for the position and velocity components of the smallest mass , and choosing @xmath106 for the other components ( knowing that the frequency of the oscillations scales like @xmath107 ) . for demonstration purposes",
    ", we thus choose the time - steps a priori to fit the dynamics of the system .",
    "we repeat the experiment for increasing values of @xmath74 ( see figure [ fig : multiadaptivity - results ] ) and find that the error is about the same and constant for both methods . as @xmath74 increases , the total number of time - steps , the number of local function evaluations ( including also residual evaluations ) , and the cpu time increase linearly for the standard method , as we expect . for the multi - adaptive method ,",
    "on the other hand , the total number of time - steps and local function evaluations remains practically constant as we increase @xmath74 .",
    "the cpu time increases somewhat , since the increasing size of the time - slabs introduces some overhead , although not nearly as much as for the standard method . for this particular problem",
    "the gain of the multi - adaptive method is thus a factor @xmath74 , where @xmath74 is the size of the system , so that by considering a large - enough system , the gain is arbitrarily large",
    ".     method ( dashed lines ) and the standard @xmath102 method ( solid lines).,title=\"fig:\",height=370 ]   method ( dashed lines ) and the standard @xmath102 method ( solid lines).,title=\"fig:\",height=370 ]      we consider now the famous lorenz system , @xmath108 with the usual data @xmath109 , @xmath110 , @xmath111 , and @xmath112 ; see @xcite .",
    "the solution @xmath113 is very sensitive to perturbations and is often described as `` chaotic . '' with our perspective this is reflected by stability factors with rapid growth in time .",
    "the computational challenge is to solve the lorenz system accurately on a time - interval @xmath114 $ ] with @xmath115 as large as possible . in figure",
    "[ fig : lorenz - trajectory ] is shown a computed solution which is accurate on the interval @xmath116 $ ] .",
    "we investigate the computability of the lorenz system by solving the dual problem and computing stability factors to find the maximum value of @xmath115 .",
    "the focus in this section is not on multi - adaptivity  we will use the same time - steps for all components , and so @xmath0  becomes @xmath117but on higher - order methods and the precise information that can be obtained about the computability of a system from solving the dual problem and computing stability factors .    ,",
    "computed with the multi - adaptive @xmath118 method .",
    "on the left is a plot of the time variation of the three components.,title=\"fig : \" ]    as an illustration , we present in figure [ fig : lorenz - higher_order ] solutions obtained with different methods and constant time - step @xmath119 for all components . for the lower - order methods , @xmath118 to @xmath120 ,",
    "it is clear that the error decreases as we increase the order .",
    "starting with the @xmath121 method , however , the error does not decrease as we increase the order .",
    "to explain this , we note that in every time - step a small round - off error of size @xmath122 is introduced if the computation is carried out in double precision arithmetic .",
    "these errors accumulate at a rate determined by the growth of the stability factor for the computational error ( see @xcite ) .",
    "as we shall see below , this stability factor grows exponentially for the lorenz system and reaches a value of @xmath123 at final time @xmath124 , and so at this point the accumulation of round - off errors results in a large computational error .",
    "-component of the lorenz system with methods of different order , using a constant time - step @xmath125 .",
    "dotted lines indicate the point beyond which the solution is no longer accurate.,title=\"fig:\",width=472 ] + -component of the lorenz system with methods of different order , using a constant time - step @xmath125 .",
    "dotted lines indicate the point beyond which the solution is no longer accurate.,title=\"fig:\",width=472 ]          we now investigate the computation of stability factors for the lorenz system . for simplicity",
    "we focus on global stability factors , such as @xmath126}(t ) = \\max_{\\|v\\|=1 } \\int_0^t \\| \\varphi^{(q)}(t ) \\| \\ dt , \\label{eq : stabilityfactors}\\ ] ] where @xmath127 is the solution of the dual problem obtained with @xmath128 ( and @xmath43 ) . letting @xmath129 be the fundamental solution of the dual problem , we have @xmath130}(t ) .",
    "\\label{eq : stabilityfactors , estimate}\\ ] ] this gives a bound @xmath131}(t)$ ] for @xmath132}(t)$ ] , which for the lorenz system turns out to be quite sharp and which is simpler to compute since we do not have to compute the maximum in ( [ eq : stabilityfactors ] ) .    in figure",
    "[ fig : lorenz - stabilityfactor_0 ] we plot the growth of the stability factor for @xmath133 , corresponding to computational and quadrature errors as described in @xcite .",
    "the stability factor grows exponentially with time , but not as fast as indicated by an a priori error estimate .",
    "an a priori error estimate indicates that the stability factors grow as @xmath126}(t ) \\sim",
    "\\mathcal{a}^q e^{\\mathcal{a } t},\\ ] ] where @xmath134 is a bound for the jacobian of the right - hand side for the lorenz system .",
    "a simple estimate is @xmath135 , which already at @xmath136 gives @xmath137}(t)\\approx 10^{22}$ ] . in view of this",
    ", we would not be able to compute even to @xmath136 , and certainly not to @xmath124 , where we have @xmath137}(t)\\approx 10^{1000}$ ] .",
    "the point is that although the stability factors grow very rapidly on some occasions , such as near the first flip at @xmath138 , the growth is not monotonic .",
    "the stability factors thus _ effectively _ grow at a moderate exponential  rate .",
    "to predict the computability of the lorenz system , we estimate the growth rate of the stability factors . a simple approximation of this growth rate , obtained from numerically computed solutions of the dual problem ,",
    "is @xmath139}(t ) \\approx 4 \\cdot 10^{(q-3)+0.37 t } \\label{eq : approximation , better}\\ ] ] or just @xmath139}(t ) \\approx 10^{q+t/3}. \\label{eq : approximation , simpler}\\ ] ] from the a posteriori error estimates presented in @xcite , we find that the computational error can be estimated as @xmath140}(t ) \\max_{[0,t ] } \\|\\mathcal{r}^{\\mathcal{c}}\\|,\\ ] ] where the computational residual @xmath141 is defined as @xmath142 with 16 digits of precision a simple estimate for the computational residual is @xmath143 , which gives the approximation @xmath144 with time - steps @xmath145 as above we then have @xmath146 and so already at time @xmath147 we have @xmath148 and the solution is no longer accurate .",
    "we thus conclude by examination of the stability factors that it is difficult to reach beyond time @xmath124 in double precision arithmetic .",
    "( with quadruple precision we would be able to reach time @xmath149 . )",
    "we now consider the solar system , including the sun , the moon , and the nine planets , which is a particularly important @xmath45-body problem of the form @xmath150 where @xmath151 denotes the position of body @xmath66 at time @xmath152 , @xmath153 is the mass of body @xmath66 , and @xmath154 is the gravitational constant .    as",
    "initial conditions we take the values at 00.00 greenwich mean time on january 1 , 2000 , obtained from the united states naval observatory @xcite , with initial velocities obtained by fitting a high - degree polynomial to the values of december 1999 .",
    "this initial data should be correct to five or more digits , which is similar to the available precision for the masses of the planets .",
    "we normalize length and time to have the space coordinates per astronomical unit , au , which is ( approximately ) the mean distance between the sun and earth , the time coordinates per year , and the masses per solar mass . with this normalization ,",
    "the gravitational constant is @xmath155 .",
    "investigating the _ predictability _ of the solar system , the question is how far we can accurately compute the solution , given the precision in initial data . in order to predict the accumulation rate of errors , we solve the dual problem and compute stability factors .",
    "assuming the initial data is correct to five or more digits , we find that the solar system is computable on the order of 500 years .",
    "including also the moon , we can not compute more than a few years .",
    "the dual solution grows linearly backward in time ( see figure [ fig : earth - moon - dual ] ) , and so errors in initial data grow linearly with time .",
    "this means that for every extra digit of increased precision , we can reach 10 times further .          to touch briefly on the fundamental question of the _ computability _ of the solar system , concerning",
    "how far the system is computable with correct initial data and correct model , we compute the trajectories for earth , the moon , and the sun over a period of @xmath156 years , comparing different methods . since errors in initial data grow linearly , we expect numerical errors as well as stability factors to grow quadratically .",
    "years for the earth ",
    "moon  sun system as described in the text . ]    in figure [ fig : earth - moon - computability ] we plot the errors for the @xmath157 components of the solution , computed for @xmath158 with @xmath102 , @xmath159 , @xmath160 , and @xmath161 .",
    "this figure contains much information . to begin with , we see that the error seems to grow linearly for the @xmath162 methods .",
    "this is in accordance with earlier observations @xcite for periodic hamiltonian systems , recalling that the @xmath163 methods conserve energy @xcite .",
    "the stability factors , however , grow quadratically and thus overestimate the error growth for this particular problem . in an attempt to give an intuitive explanation of the linear growth",
    ", we may think of the error introduced at every time - step by an energy - conserving method as a pure phase error , and so at every time - step the moon is pushed slightly forward along its trajectory ( with the velocity adjusted accordingly ) . since a pure phase error does not accumulate but stays constant ( for a circular orbit ) , the many small phase errors give a total error that grows linearly with time .",
    "examining the solutions obtained with the @xmath160 and @xmath161 methods , we see that the error grows quadratically , as we expect . for the @xmath160 solution ,",
    "the error reaches a maximum level of @xmath164 for the velocity components of the moon .",
    "the error in position for the moon is much smaller .",
    "this means that the moon is still in orbit around earth , the position of which is still very accurate , but the position relative to earth , and thus also the velocity , is incorrect .",
    "the error thus grows quadratically until it reaches a limit .",
    "this effect is also visible for the error of the @xmath102 solution ; the linear growth flattens out as the error reaches the limit .",
    "notice also that even if the higher - order @xmath161 method performs better than the @xmath102 method on a short time - interval , it will be outrun on a long enough interval by the @xmath102 method , which has linear accumulation of errors ( for this particular problem ) .",
    "years for the earth ",
    "moon  sun system computed with the @xmath165 method , together with the multi - adaptive time - steps . ]      solving with the multi - adaptive method @xmath165 ( see figure [ fig : earth - moon - computability - multiadaptive ] ) , the error grows quadratically .",
    "we saw in @xcite that in order for the @xmath0 method to conserve energy , we require that corresponding position and velocity components use the same time - steps .",
    "computing with different time - steps for all components , as here , we thus can not expect to have linear error growth .",
    "keeping @xmath166 with @xmath167 as here , the error grows as @xmath168 and we are able to reach @xmath169 .",
    "decreasing @xmath170 to , say , @xmath171 , we could instead reach @xmath172 .",
    "( lower parts ) and @xmath173 ( upper parts).,height=377 ]    we investigated the passing of a large comet close to earth and the moon and found that the stability factors increase dramatically at the time @xmath78 when the comet comes very close to the moon .",
    "the conclusion is that if we want to compute accurately to a point beyond @xmath78 , we have to be much more careful than if we want to compute to a point just before @xmath78 .",
    "this is not evident from the size of residuals or local errors .",
    "this is an example of a hamiltonian system for which the error growth is neither linear nor quadratic .",
    "the system of pdes @xmath174 on @xmath175 $ ] with @xmath176 , @xmath149 , and homogeneous neumann boundary conditions at @xmath177 and @xmath178 models isothermal autocatalytic reactions ( see @xcite ) @xmath179 .",
    "we choose the initial conditions as @xmath180 with @xmath181 and @xmath182 .",
    "an initial reaction where substance @xmath183 is consumed and substance @xmath173 is formed will then occur at @xmath184 , resulting in a decrease in the concentration @xmath185 and an increase in the concentration @xmath186 .",
    "the reaction then propagates to the right until all of substance @xmath183 is consumed and we have @xmath187 and @xmath188 in the entire domain .    solving with the @xmath165 method",
    ", we find that the time - steps are small only close to the reaction front ; see figures [ fig : front - ku ] and [ fig : front - solution ] .",
    "the reaction front propagates to the right as does the domain of small time - steps .",
    "it is clear that if the reaction front is localized in space and the domain is large , there is a lot to gain by using small time - steps only in this area . to verify this",
    ", we compute the solution to within an accuracy of @xmath189 for the final time error with constant time - steps @xmath190 for all components and compare with the multi - adaptive solution . computing on a space grid consisting of @xmath191 nodes on @xmath192",
    "$ ] ( resulting in a system of odes with @xmath193 components ) , the solution is computed in @xmath194 seconds on a regular workstation .",
    "computing on the same grid with the multi - adaptive method ( to within the same accuracy ) , we find that the solution is computed in @xmath195 seconds .",
    "more work is thus required to compute the multi - adaptive solution , and the reason is the overhead resulting from additional bookkeeping and interpolation in the multi - adaptive computation . however , increasing the size of the domain to @xmath193 nodes on @xmath196 $ ] and keeping the same parameters otherwise , we find the solution is now more localized in the domain and we expect the multi - adaptive method to perform better compared to a standard method . indeed",
    ", the computation using equal time - steps now takes @xmath197 seconds , whereas the multi - adaptive solution is computed in @xmath198 seconds . in the same way as previously shown in section [ sec : testproblem ] , adding extra degrees of freedom does not substantially increase the cost of solving the problem , since the main work is done time - stepping the components , which use small time - steps .     and @xmath199 at time @xmath200 as function of space ( above ) and the corresponding time - steps ( below ) . ]      as a final example , we present a computation in which we combine multi - adaptivity with the possibility of moving the nodes in a space discretization of a time - dependent pde . solving burger s equation , @xmath201 on @xmath175 $ ] with initial condition @xmath202 and with @xmath203 , @xmath204 , and @xmath205",
    ", we find the solution is a shock forming near @xmath184 .",
    "allowing individual time - steps within the domain , and moving the nodes of the space discretization in the direction of the convection , @xmath206 , we make the ansatz @xmath207 where the @xmath208 are the individual components computed with the multi - adaptive method , and the @xmath209 are piecewise linear basis functions in space for any fixed  @xmath152 .    solving with the @xmath82 method , the nodes move into the shock , in the direction of the convection , so what we are really solving is a heat equation with multi - adaptive time - steps along the streamlines ; see figures [ fig : burger - solutions ] and [ fig : burger - paths ] .    , @xmath210 , and @xmath75 .",
    "]        [ fig : burger - paths ]",
    "together with the companion paper @xcite ( see also @xcite ) , this paper serves as a starting point for further investigation of the multi - adaptive galerkin methods and their properties .",
    "future work will include a more thorough investigation of the application of the multi - adaptive methods to stiff odes , as well as the construction of efficient multi - adaptive solvers for time - dependent pdes , for which memory usage becomes animportant issue .",
    ", _ multi - adaptive error control for odes _ , preprint 2000 - 03 , chalmers finite element center , chalmers university of technology , gteborg , sweden , 2000 . also available online from http://www.phi.chalmers.se / preprints/.    , _ a multi - adaptive ode - solver _ , preprint 2000 - 02 , chalmers finite element center , chalmers university of technology , gteborg , sweden , 2000",
    "also available online from http://www.phi.chalmers.se / preprints/.      , _ multi - adaptive galerkin methods for odes ii : applications _ , preprint 2001 - 10 , chalmers finite element center , chalmers university of technology , gteborg , sweden , 2001 . also available",
    "online from http://www.phi.chalmers.se / preprints/."
  ],
  "abstract_text": [
    "<S> continuing the discussion of the multi - adaptive galerkin methods @xmath0 and @xmath1 presented in [ a. logg , _ </S>",
    "<S> siam j. sci . </S>",
    "<S> comput . _ , 24 ( 2003 ) , pp .  </S>",
    "<S> 18791902 ] , we present adaptive algorithms for global error control , iterative solution methods for the discrete equations , features of the implementation _ tanganyika _ , and computational results for a variety of odes . </S>",
    "<S> examples include the lorenz system , the solar system , and a number of time - dependent pdes .    </S>",
    "<S> multi - adaptivity , individual time - steps , local time - steps , ode , continuous galerkin , discontinuous galerkin , global error control , adaptivity , @xmath0 , @xmath1 , applications , lorenz , solar system , burger    65l05 , 65l07 , 65l20 , 65l50 , 65l60 , 65l70 , 65l80 </S>"
  ]
}