{
  "article_text": [
    "compositional data are positive multivariate data whose vector elements sum to the same constant usually taken to be @xmath1 for convenience purposes .",
    "data of this type arise in many disciplines , such as geology , ecology , archaeology , economics , geochemistry , biology , bioinformatics , political sciences and forensic sciences among others . in mathematical terms , their sample space , called simplex , is given by @xmath2 where @xmath3 denotes the number of variables ( better known as components ) and @xmath4 .",
    "there are various techniques for regression analysis with compositional data being the response variables .",
    "see for example @xcite who used classical methods on a log - ratio transformed space and @xcite and @xcite who transformed the data on the surface of a unit hyper - sphere using the square root transformation .",
    "dirichlet regression models have been employed by @xcite and @xcite .",
    "when zero values exist in data , dirichlet models and the log - ratio transformation suggested by @xcite and @xcite will not work unless a zero value imputation is applied first .",
    "the square root transformation on the other hand treats them naturally @xcite , but the procedure is not easy to implement .",
    "we propose the use of a newly suggested data based power transformation @xcite to perform regression analysis .",
    "the multivariate logit link function is necessary to ensure that the fitted values lie within the simplex .",
    "the free parameter of the power transformation is chosen such that that the discrepancy between the observed and the fitted values is minimized .",
    "this implies that the use of kullback - leibler divergence will be of great importance in achieving this goal .",
    "the big advantage of this type of regression is that a ) zeros are handled naturally and thus no zero imputation technique prior to the analysis is necessary and b ) more accurate fitted values can be obtained .",
    "the disadvantage on the other hand is that we loose in terms of statistical properties of the estimated coefficients .",
    "hence , this is more a data mining procedure .",
    "the inverse situation , where compositional data are in the predictor variables side has not been looked at , thoroughly , in the literature as far as we are aware of .",
    "one possible solution would be to apply the isometric log - ratio transformation @xcite ( see [ ilr ] ) and then standard techniques , see for example @xcite and @xcite .",
    "but this approach has two drawbacks : first it does not account for any collinearity between the components and second it is not applicable when zero values are present .",
    "collinearity problems can be attacked by principal component regression .",
    "zero values require imputation , so that the isometric log - ratio transformation can be applied .",
    "this however means that the values of the rest components of each vector , which contains at least one zero value , will have to change . in some data sets",
    ", there can be many zeros spread in many observations .",
    "that would mean that many observations would have to change values , even slightly .",
    "this adds extra variation to the data though .    in this paper",
    "we propose a novel parametric regression method whose ultimate purpose is prediction inference . a data based power transformation @xcite involving one free parameter , @xmath0 is employed at first .",
    "we suggest a way to choose the value of @xmath0 which leads to the optimal results .",
    "the results show that prediction can be more accurate when one uses a transformation other than the isometric @xcite or the additive log - ratio @xcite .",
    "a similar approach is exhibited when for the case of compositional data being predictor variables .",
    "thus , this extends the work by @xcite who used the isometric log - ratio transformation only .",
    "however , our work focuses mainly on prediction and not on inference regarding the regression coefficients .",
    "alternatively , if the number of explanatory variables is rather small , standard linear regression analysis can be carried out . in either case",
    ", the @xmath0-transformation handles zero values in the data naturally ( if present ) and principal component or @xmath5-nn regression handles possible collinearity problems .",
    "the key message is that a transformation other than the log - ratio family can lead to better results in terms of prediction accuracy .",
    "regression analysis when compositional data are the response variables is described in section 2 .",
    "simulation studies and a real data example illustrates the methodology . section 3 describes the principal component regression when compositional data are the predictor variables .",
    "a real data example shows interesting results . finally , conclusions close this paper .",
    "the @xmath0-transformation was proposed by @xcite as a more general than the isometric log - ratio transformation @xcite and is a data based power transformation involving one free power parameter , similarly to the box - cox transformation .",
    "the power transformation suggested by @xcite is @xmath6 + and in terms of that define @xmath7 where @xmath8 , @xmath9 is the @xmath10 helmert sub - matrix ( the helmert matrix @xcite without the first row ) and @xmath11 is the @xmath3-dimensional vector of @xmath1s . note that the power transformed vector @xmath12 in ( [ alpha ] ) remains in the simplex @xmath13 , whereas @xmath14 is mapped onto a subset of @xmath15 .",
    "note also that ( [ isoalpha ] ) is simply a linear transformation of ( [ alpha ] ) and moreover as @xmath16 , ( [ isoalpha ] ) converges to the isometric log - ratio transformation @xcite defined as @xmath17    we can clearly see that when there are zero values in the compositional data the isometric log - ratio transformation ( [ ilr ] ) is not applicable because the logarithm of zero is undefined .",
    "for this reason , we will also examine the zero value imputation briefly mentioned in the next section .",
    "we will use the inverse of the additive logistic transformation , combined with the @xmath0-transformation , as a link function .",
    "this is a new regression using the @xmath0-transformation ( [ isoalpha ] ) which allows for more flexibility even in the presence of zero values .",
    "another feature of this method is that the line is always curved ( unless @xmath0 is far away from zero ) and so it can be seen not only as a generalization of the log - ratio regression but also as a flexible type compositional regression in the sense that the curvature of the line is chosen based on some discrepancy criteria , examined later .    in order for the fitted values to satisfy the constraint imposed by the simplex we model the inverse of the additive logistic transformation of the mean response .",
    "hence , the fitted values will always lie within @xmath13 and we also retain the flexibility the @xmath0-transformation offers .",
    "we assume that the conditional mean of the observed composition can be written as a non - linear function of some covariates @xmath18 where @xmath19    then a multivariate linear regression is applied to the @xmath0-transformed data @xmath20,\\end{aligned}\\ ] ] where @xmath21 and @xmath22 are the @xmath0-transformed response and fitted compositional vectors .",
    "we have ignored the jacobian determinant of the @xmath0-transformation since it plays no role in the optimization process and the choice of @xmath0 for each value of @xmath0 we maximize the value of this objective function ( [ loglikreg ] ) .",
    "the @xmath23 needs not be numerically estimated , since @xmath24 , the matrix of the estimates and @xmath23 are statistically independent @xcite .",
    "the maximum likelihood estimator of @xmath25 is @xcite @xmath26 where @xmath27 .",
    "but since this covariance is not unbiased we will use the unbiased estimator @xmath28 where @xmath29 is the design matrix and @xmath30 is the number of independent variables .",
    "the consistency of the estimators of the parameters is not an issue in our case since we focus on prediction inference .",
    "since the estimation of the parameters depends upon the value of @xmath0 , the estimates will not be consistent , unless that is the true assumed model .",
    "the multivariate normal is defined in the whole of @xmath31 but the @xmath0-transformation maps the data onto a subset of @xmath31 . thus ,",
    "unless there is not too much probability left outside the simplex , the multivariate normal distribution might not be the best option .",
    "the @xmath0-transformation what it does essentially is to contract the simplex , center it to the origin and then project it on a subspace of @xmath31 by using the helmert sub - matrix @xcite .",
    "so if the fitted multivariate normal has high dispersion that will lead to probability left outside the simplex .",
    "the multivariate t distribution was used by @xcite as a more robust , in comparison to the multivariate normal , model but even so , it will not be the best option , mainly for two reasons .",
    "even if the multivariate t distribution could provide flatter tails , there would still be some probability ( even less than the normal ) left outside the simplex .",
    "secondly , in a regression setting , the number of parameters we would have to estimate numerically is increased and this would make the maximization process more difficult .",
    "a final key feature we have to note is that when @xmath16 we end up with the additive log - ratio regression ( [ regalr ] ) .",
    "the disadvantage of the profile log - likelihood , for choosing the value of @xmath0 , is that it does not allow zeros . on the other hand",
    ", it provides the maximum likelihood estimates which are asymptotically normal .",
    "furthermore , confidence intervals for the true value of @xmath0 can be constructed .",
    "we suggest an alternative and perhaps better way of choosing the value of @xmath0 .",
    "better in the sense that it is trying to take into account the proximity between the observed and the fitted values .",
    "the criterion is to choose the @xmath0 which minimizes twice the kullback - leibler divergence @xcite @xmath32 where @xmath33 is the observed compositional point and @xmath34 is the corresponding fitted value .",
    "the form of the deviance for the log - linear models and the logistic regression has the same expression as well .",
    "hence , we transfer the same form of divergence to compositional data . for every value of @xmath0 we estimate the parameters of the regression and choose the value of @xmath0 which minimizes ( [ mdiverge ] ) .",
    "the number @xmath35 is there because in the case of @xmath36 we end up with the log - likelihood of the binary logistic regression .",
    "the kullback - leibler divergence ( [ mdiverge ] ) takes into account the divergence or the distance of each of the observed values from the fitted values .",
    "since we are interested in prediction analysis we should use cross - validation to choose the value of @xmath0 .",
    "the reason why we did not choose to go down this road is because of time .",
    "the estimation of the parameters for a single value of @xmath0 requires some seconds in a fine desktop .",
    "the search over many possible values of @xmath0 requires a few minutes .",
    "thus , even a @xmath1-fold cross validation would require a few hours and as the number of independent variables , and/or the sample size increase the computational time will increase as well .",
    "so , for the shake of speed we avoided this way .",
    "the additive log - ratio transformation is defined as @xmath37 where @xmath4 , @xmath38 is the last component playing the role of the common divisor , but by relabelling any component can play this role .",
    "we will now show the additive log - ratio regression .",
    "at first we will see a nice property of the logarithms and its implications on the additive log - ratio regression .",
    "@xmath39 where @xmath40 is a column vector of the design matrix @xmath29 , @xmath3 is the number of components and @xmath41 are the regression coefficients and @xmath30 is the number of independent variables .",
    "we see from ( [ regalr ] ) that when the dependent variable is the logarithm of any component , the logarithm of the common divisor component can be treated as an offset variable ; an independent variable with coefficient equal to @xmath1 .",
    "the main disadvantage of this type of regression is that it does not allow zero values in any of the components , unless a zero value imputation technique @xcite is applied first .",
    "its advantage though is that after the additive log - ratio transformation ( [ alr ] ) we can do the standard multivariate regression analysis . as for the fitted value they are back transformed into the simplex using the inverse of ( [ alr ] ) .",
    "this data set consists of foraminiferal ( marine plankton species ) compositions at @xmath42 different depths ( @xmath1-@xmath42 metres ) and can be found in ( * ? ?",
    "there are @xmath43 compositions with a zero value , either in the third or the fourth component .",
    "the data were analysed by @xcite who performed regression by employing the kent distribution using the logarithm of the depth as the independent variable .",
    "since the data contain zero values , the @xmath0-regression allows only strictly positive values for @xmath0 .",
    "since the composition belongs to @xmath44 we can not use the ternary plot ; we could though use a @xmath45-dimensional pyramid , but it would not show the structure of the data clearly . for this reason we will use a barplot .        a zero value imputation method suggested by @xcite",
    "can be briefly summarised as follows : replace the zero values by @xmath46 of a threshold value using the non parametric method described by @xcite .",
    "the threshold value is different for each component .",
    "we used the minimum , non zero , value of each component as that threshold .",
    "then the isometric log - ratio transformation ( [ ilr ] ) is applied .",
    "an e - m algorithm substitutes the replaced values with a value less than the chosen threshold . in the end , the data are transformed back to the simplex .",
    "this method , like all zero value imputation methods , results in all the components of each compositional vector being changed , even slightly .",
    "a tolerance value ( say @xmath47 ) is used as convergence criterion between successive iterations .",
    "we will use this method in order to allow for the additive log - ratio regression to be performed . in this case the kullback - leibler divergence ( [ mdiverge ] ) will be calculated for the initial , not the zero imputed data .    [ cols=\"^,^ \" , ]     we also used kernel regression and @xmath5-nn regression using many different metrics or distances with and without the @xmath0-transformation but none of them was as successful as the principal component regression .",
    "we also tried robust principal component regression for the second example but the results were not better either .",
    "the best model is the one using all 7 principal components coming from the @xmath0-transformed data with @xmath48 and using the information about the category of glass as we can see in table [ tab1 ] . in mathematical terms",
    "it is written as @xmath49 where @xmath50 , for @xmath51 stands for the scores of each principal component and winf and winnf stand for the window float and window non - float glass respectively .",
    "veh stands for vehicle window glass , con for containers , tabl for tableware .",
    "the vehicle headlamps is the reference glass category .",
    "we do not show the standard errors since the choice of the number of principal components , and the inclusion of the information about the glass categories was based on the mspe ( table [ tab1 ] ) .",
    "the normality of the residuals was rejected according to the shapiro test ( p - value@xmath52 ) . as for the independence assumption by looking at figure [ fig2 ]",
    "we can see that it looks acceptable .",
    "there are a few outliers in the residuals space as we can see from figure [ fig2 ] .",
    "the @xmath0 used in this model was equal to @xmath1 as we mentioned before .",
    "different values of @xmath0 will lead to different values bering detected as potential outliers .",
    "in addition , we detected some outliers in the residuals space and not in the space spanned by the @xmath53 principal components .",
    "there are two key messages this paper tries to convey .",
    "the first is that a transformation other than the isometric of the additive log - ratio transformation should be considered for compositional data analysis .",
    "it was evident from the example that when the data contain zero values the @xmath0-transformation handles the data naturally without changing their values even to the slightest .",
    "the log - ratio transformations require zero value imputation prior to their applications , a requirement not met by the @xmath0-transformation .",
    "one can argue though that in both examples values of @xmath0 close to 1 produced similar results as @xmath48 .",
    "this was something to be expected for the data in the first example , but even then , this discourages the use of log - ratio as the panacea for all compositional data analysis .",
    "we do not disqualify though the use of log - ratio but rather try to show that this very popular transformation for compositional data has some drawbacks and in some cases other transformations could be applied . in both examples , the differences in the kulback - leibler divergence or",
    "the mspe were small , but still indicative .",
    "the second key message is that when compositional data analysis are on the covariates side standard regression techniques after the @xmath0-transformation should be used with caution .",
    "since the data represent percentage allocation , it is very natural that correlations exist even after the transformation and thus , multicollinearity should be examined first .",
    "in addition , if there are many components then variable selection could be applied .",
    "these are some reasons why we preferred to perform principal component regression .",
    "the author would like to express his acknowledgments to the anonymous reviewer for his comments .",
    "0 aitchison , j. , 1982 . the statistical analysis of compositional data . journal of the royal statistical society .",
    "series b , 44 , 139 - 177 .",
    "aitchison , j. and lauder , i.j . , 1985 .",
    "kernel density estimation for compositional data . applied statistics , 44 , 129 - 137 .",
    "aitchison , j. , 2003 .",
    "the statistical analysis of compositional data .",
    "reprinted by the blackburn press , new jersey .",
    "egozcue , j.j . and pawlowsky - glahn , v. and mateu - figueras , g. and barcel - vidal , c. , 2003 .",
    "isometric logratio transformations for compositional data analysis , 35 , 279 - 300 .",
    "egozcue , j.j .",
    ", daunis - i - estadella , j.,pawlowsky - glahn , v. , hron , k. and filzmoser , p. , 2011 .",
    ". the normal model .",
    "journal of applied probability and statistics , 6 , 87 - 108 .",
    "gueorguieva , r. , rosenheck , r. and zelterman , d. , 2008 .",
    "dirichlet component regression and its applications to psychiatric data . computational statistics & data",
    "analysis , 52 , 5344 - 5355 .",
    "hron , k. , filzmoser , p , and thompson , k. , 2012 . linear regression with compositional explanatory variables",
    ". journal of applied statistics , 39 , 11151128 .",
    "jolliffe , i.t . , 2005 .",
    "principal component analysis .",
    "springer - verlag , new york .",
    "kullback , s. , 1997 .",
    "information theory and statistics .",
    "dover publications , new york .",
    "lancaster , h.o . , 1965",
    ". the helmert matrices .",
    "american mathematical monthly , 72 , 4 - 12 .",
    "lange , k.l . ,",
    "little , r.j.a . and taylor , j.m.g . , 1989 .",
    "robust statistical modeling using the t distribution .",
    "journal of the american statistical association , 84 , 881 - 896 .",
    "maier , m.j .",
    "hrefhttp://dirichletreg.r - forge.r - project.org / dirichletreg : dirichlet regression in r. r package version 0.3 - 0 .",
    "mardia , k.v . ,",
    "kent , j.t . and bibby , j.m . , 1979 .",
    "multivariate analysis . academic press ,",
    "maronna , r.a . ,",
    "martin , r.d . and",
    "yohai , v.j . , 2006",
    ". robust statistics .",
    "wiley , chichester .",
    "martn - fernndez , j.a . ,",
    "hron , k. , templ , m. , filzmoser , p. and palarea - albaladejo , j. , 2012 .",
    "model - based replacement of rounded zeros in compositional data : classical and robust approaches .",
    "computational statistics & data analysis , 56 , 2688 - 2704 .",
    "martn - fernndez , j.a .",
    ", barcel - vidal , c. and pawlowsky - glahn , v. , 2012 .",
    "dealing with zeros and missing values in compositional data sets using nonparametric imputation .",
    "mathematical geology , 35 , 253 - 278 .",
    "scealy , j.l . and",
    "welsh , a.h . , 2011 .",
    "regression for compositional data by using distributions defined on the hypersphere .",
    "journal of the royal statistical society .",
    "series b , 73 , 351 - 375 .",
    "stephens , m.a . , 1982 .",
    "use of the von mises distribution to analyse continuous proportions .",
    "biometrika , 69 , 197 - 203 .",
    "tsagris , m.t . ,",
    "preston , s. and wood , a.t.a . , 2011 . a data - based power transformation for compositional data . in proceedings of the 4th compositional data analysis workshop , girona , spain"
  ],
  "abstract_text": [
    "<S> regression analysis , for prediction purposes , with compositional data is the subject of this paper . </S>",
    "<S> we examine both cases when compositional data are either response or predictor variables . a parametric model is assumed but the interest lies in the accuracy of the predicted values . for this reason , </S>",
    "<S> a data based power transformation is employed in both cases and the results are compared with the standard log - ratio approach . </S>",
    "<S> there are some interesting results and one advantage of the methods proposed here is the handling of the zero values . </S>",
    "<S> +   + * keywords * : compositional data , regression , prediction , @xmath0-transformation , principal component regression +   + * mathematics subject classification * : primary 62j02 , secondary 62h99 </S>"
  ]
}