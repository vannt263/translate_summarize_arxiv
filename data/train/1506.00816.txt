{
  "article_text": [
    "resampling methods for dependent data such as time series have been studied extensively over the last decades . for an overview of existing bootstrap methods",
    "see the monograph of @xcite and the review papers by @xcite , @xcite , @xcite , @xcite or the recent review paper by @xcite . among the most popular bootstrap procedures in time series analysis , we mention the autoregressive ( ar ) sieve bootstrap [ cf .",
    "@xcite ( @xcite ) , @xcite , @xcite ] and block bootstrap and its variations ; cf .",
    "@xcite , @xcite , @xcite ( @xcite ) , etc .",
    "a recent addition to the available time series bootstrap methods was the linear process bootstrap ( lpb ) introduced by @xcite who showed its validity for the sample mean for univariate stationary processes without actually assuming linearity of the underlying process .",
    "the main idea of the lpb is to consider the time series data of length @xmath0 as one large @xmath0-dimensional vector and to estimate appropriately the entire covariance structure of this vector .",
    "this is executed by using tapered covariance matrix estimators based on flat - top kernels that were defined in @xcite .",
    "the resulting covariance matrix is used to whiten the data by pre - multiplying the original ( centered ) data with its inverse cholesky matrix ; a modification of the eigenvalues , if necessary , ensures positive definiteness .",
    "this decorrelation property is illustrated in figures  5 and 6 in @xcite .",
    "after suitable centering and standardizing , the whitened vector is treated as having independent and identically distributed ( i.i.d . ) components with zero mean and unit variance .",
    "finally , i.i.d . resampling from this vector and pre - multiplying the corresponding bootstrap vector of residuals with the cholesky matrix itself results in a bootstrap sample that has ( approximately ) the same covariance structure as the original time series .    due to the use of flat - top kernels with compact support , an abruptly dying - out autocovariance structure",
    "is induced to the bootstrap residuals .",
    "therefore , the lpb is particularly suitable for  but not limited to  time series of moving average ( ma ) type . in a sense , the lpb could be considered the closest analog to an ma - sieve bootstrap which is not practically feasible due to nonlinearities in the estimation of the ma parameters .",
    "a further similarity of the lpb to ma fitting , at least in the univariate case , is the equivalence of computing the cholesky decomposition of the covariance matrix to the innovations algorithm ; cf .",
    "@xcite , @xcite and @xcite , the latter addressing the multivariate case .",
    "typically , bootstrap methods extend easily from the univariate to the multivariate case , and the same is true for time series bootstrap procedures such as the aforementioned ar - sieve bootstrap and the block bootstrap . by contrast",
    ", it has not been clear to date if / how the lpb could be successfully applied in the context of multivariate time series data ; a proposal to that effect was described in @xcite  who refer to an earlier preprint of the paper at hand ",
    "but it has been unclear to date whether the multivariate lpb is asymptotically consistent and/or if it competes well with other methods .",
    "here we attempt to fill this gap : we show how to implement the lpb in a multivariate context and prove its validity for the sample mean and for spectral density estimators , the latter being a new result even in the univariate case .",
    "note that the limiting distributions of the sample mean and of kernel spectral density estimators depend only on the second - order moment structure . hence it is intuitive that the lpb would be well suited for such statistics since it generates a linear process in the bootstrap world that mimics well the second - order moment structure of the real world .",
    "furthermore , in the spirit of the times , we consider the possibility that the time series dimension is increasing with sample size and identify conditions under which the multivariate linear process bootstrap ( mlpb ) maintains its asymptotic validity , even in this case .",
    "the key here is to address the subject of consistently estimating the autocovariance sequence ; this is a sequence of matrices that we conveniently stack into one huge matrix .",
    "we are then able to show consistency of an estimator based on the aforementioned flat - top tapers ; most importantly , the consistency holds true even when the time series dimension is allowed to increase with the sample size .",
    "the paper is organized as follows . in section  [ preliminaries ]",
    ", we introduce the notation of this paper , discuss tapered covariance matrix estimation for multivariate stationary time series and state assumptions used throughout the paper ; we then present our results on convergence with respect to operator norm of tapered covariance matrix estimators .",
    "the mlpb bootstrap algorithm and some remarks can be found in section  [ bootstrapscheme ] , and results concerned with validity of the mlpb for the sample mean and kernel spectral density estimates are summarized in section  [ asymptoticresults ] .",
    "asymptotic results established for the case of increasing time series dimension are stated in section  [ asymptoticresultsincreasing ] , where operator norm consistency of tapered covariance matrix estimates and a validity result for the sample mean are discussed .",
    "a finite - sample simulation study is presented in section  [ secsim ] . finally ,",
    "all proofs , some additional simulations and a real data example on the weighted mean of an increasing number of stock prices taken from the german stock index dax can be found at the paper s supplementary material [ @xcite ] , which is also available at http://www.math.ucsd.edu/\\textasciitilde politis / paper / mlpbsupplement.pdf[http://www.math.ucsd.edu/\\textasciitilde politis / paper / mlpbsupplement.pdf ] .",
    "suppose we consider an @xmath1-valued time series process @xmath2 with @xmath3 , and we have data @xmath4 at hand .",
    "the process @xmath5 is assumed to be strictly stationary and its @xmath6 autocovariance matrix @xmath7 at lag @xmath8 is @xmath9 where @xmath10 , and the sample autocovariance @xmath11 at lag @xmath12 is defined by @xmath13 where @xmath14 is the @xmath15-variate sample mean vector .",
    "here and throughout the paper , all matrix - valued quantities are written as bold letters , all vector - valued quantities are underlined , @xmath16 indicates the transpose of a matrix @xmath17 , @xmath18 the complex conjugate of @xmath17 and @xmath19 denotes the transposed conjugate of @xmath17 .",
    "note that it is also possible to use unbiased sample autocovariances , that is , having @xmath20 instead of @xmath0 in the denominator of ( [ samplecovariance ] ) .",
    "usually the biased version as defined in ( [ samplecovariance ] ) is preferred because it guarantees a positive semi - definite estimated autocovariance function , but our tapered covariance matrix estimator discussed in section  [ taperedestimator ] is adjusted in order to become positive definite in any case .",
    "now , let @xmath21 be the @xmath22-dimensional vectorized version of the @xmath23 data matrix @xmath24 $ ] , and denote the covariance matrix of @xmath25 , which is symmetric block toeplitz , by @xmath26 , that is , @xmath27 where @xmath28 is the covariance between the @xmath29th and @xmath30th entry of  @xmath25 .",
    "note that the second order stationarity of @xmath31 does _ not _ imply second - order stationary behavior of the vectorized @xmath22-dimensional data sequence @xmath25 .",
    "this means that the covariances @xmath32 truly depend on both @xmath29 and @xmath30 and not only on the difference @xmath33 .",
    "however , the following one - to - one correspondence between @xmath34 and @xmath35 holds true .",
    "precisely , we have @xmath36 where @xmath37 and @xmath38 with @xmath39 and @xmath40 , and @xmath41 denotes the smallest integer greater or equal to @xmath42 .",
    "if one is interested in estimating the quantity @xmath26 , it seems natural to plug in the sample covariances @xmath43 and @xmath44 in @xmath26 and to use @xmath45 but unfortunately this estimator is _ not _ a consistent estimator for @xmath26 in the sense that the operator norm of @xmath46 does not converge to zero .",
    "this was shown by @xcite , and to dissolve this problem in the univariate case , they proposed a banded estimator of the sample covariance matrix to achieve consistency .",
    "this has been generalized by @xcite , who considered general flat - top kernels as weight functions .    in section  [ taperedestimator ] ,",
    "we follow the paper of @xcite and propose a tapered estimator of @xmath47 and show its consistency in theorem  [ operatornormconvergence1 ] for the case of multivariate processes . moreover , we state a modified estimator that is guaranteed to be positive definite for any finite sample size and show its consistency in theorem  [ operatornormconvergence2 ] and of related quantities in corollary  [ operatornormconvergence3 ] . but prior to this , we state the assumptions that are used throughout this paper in the following .",
    "@xmath31 is an @xmath1-valued strictly stationary time series process with mean @xmath48 and autocovariances @xmath49 defined in ( [ covariance ] ) such that @xmath50 for some @xmath51 to be further specified .",
    "let @xmath52 for some matrix @xmath53 .",
    "there exists a constant @xmath54 such that for all @xmath55 , all @xmath56 with @xmath12 and all @xmath57 , we have @xmath58 where @xmath59",
    ".    there exists an @xmath60 large enough such that for all @xmath61 the eigenvalues @xmath62 of the @xmath63 covariance matrix @xmath26 are bounded uniformly away from zero .",
    "define the projection operator @xmath64 for @xmath65 , and suppose that for all @xmath66 , we have @xmath67 and @xmath68 , respectively , for some @xmath69 to be further specified .    for the sample mean , a clt holds true .",
    "that is , we have @xmath70 where @xmath71 denotes weak convergence , @xmath72 is a normal distribution with zero mean vector and covariance matrix @xmath73 with @xmath74 positive definite .    for kernel",
    "spectral density estimates @xmath75 as defined in ( [ fhat ] ) in section  [ asymptoticresults ] , a clt holds true .",
    "that is , for arbitrary frequencies @xmath76 , we have that @xmath77 converges to an @xmath78-dimensional normal distribution for @xmath79 and @xmath80 such that @xmath81 as @xmath82 , where the limiting covariance matrix is obtained from @xmath83 and the limiting bias from @xmath84 for all @xmath85 , where @xmath86 if @xmath87 and @xmath88 if @xmath89 and zero otherwise , respectively .",
    "therefore , @xmath90 is assumed to be component - wise twice differentiable with lipschitz - continuous second derivatives .",
    "assumption is quite standard , and the uniform convergence of sample autocovariances in ( a2 ) is satisfied under different types of conditions ( cf .  [ remarka2 ] below ) and appears to be a crucial condition here .",
    "the uniform boundedness of all eigenvalues away from zero in ( a3 ) is implied by a nonsingular spectral density matrix @xmath91 of @xmath92 .",
    "this follows with ( [ gammadn ] ) and the inversion formula  from @xmath93 for all @xmath94 , where @xmath95 and @xmath96 denotes the kronecker product .",
    "the requirement of condition ( a3 ) fits into the theory for the univariate autoregressive sieve bootstrap as obtained in @xcite .",
    "similarly , a nonsingular spectral density matrix @xmath91 implies positive definiteness of the long - run variance @xmath97 defined in ( a5 ) .",
    "assumption is , for instance , fulfilled if the underlying process is linear or @xmath98-mixing with summable mixing coefficients by ibragimov s inequality ; cf .",
    ", for example , @xcite , theorem 14.2 . to achieve validity of the mlpb for the sample mean and for kernel spectral density estimates in section  [ asymptoticresults ] , we have to assume unconditional clts in ( a5 ) and ( a6 ) , which are satisfied also under certain mixing conditions [ cf .",
    "@xcite , @xcite ] , linearity [ cf .",
    "@xcite , @xcite ] or weak dependence [ cf .",
    "note also that the condition @xmath81 includes the optimal bandwidth choice @xmath99 , @xmath100 for second - order kernels , which leads to a nonvanishing bias in the limiting normal distribution .",
    "[ remarka2 ] assumption ( a2 ) is implied by different types of conditions imposed on the underlying process @xmath31 .",
    "we present sufficient conditions for ( a2 ) under linearity , mixing or weak dependence type conditions .",
    "more precisely , ( a2 ) is satisfied if the process @xmath31 fulfills one of the following conditions :    _ linearity_. suppose the process is linear ; that is , @xmath101 , @xmath102 , where @xmath103 is an i.i.d .",
    "white noise with finite fourth moments @xmath104 for all @xmath105 and the sequence of @xmath6 coefficient matrices @xmath106 is component - wise absolutely summable .",
    "_ mixing - type condition_. let @xmath107 denote the @xmath108th order joint cumulant of @xmath109 [ cf.@xcite ] , and suppose @xmath110 for all @xmath57 .",
    "note that this is satisfied if @xmath111 is @xmath98-mixing such that @xmath112 and @xmath113 for some @xmath114 ; cf .",
    "@xcite , page 221 .",
    "_ weak dependence - type condition_. suppose for all @xmath57 , we have @xmath115 where @xmath116 and @xmath117 is absolutely summable , that is , @xmath118 cf . @xcite .      to adopt the technique of @xcite , let @xmath119 be a so - called _ flat - top taper _ [ cf .",
    "@xcite ] , where @xmath120 and @xmath121 . the @xmath122-scaled version of @xmath123 is defined by @xmath124 for some @xmath125 .",
    "as @xcite argues , it is advantageous to have a smooth taper @xmath126 , so the truncated kernel that corresponds to @xmath127 for all @xmath128 is not recommended .",
    "the simplest example of a continuous taper function @xmath126 with @xmath129 is the trapezoid @xmath130 which is used in section  [ secsim ] for the simulation study ; the trapezoidal taper was first proposed by @xcite in a spectral estimation setup .",
    "observe also that the banding parameter does not need to be an integer .",
    "the tapered estimator @xmath131 of @xmath26 is given by @xmath132 where @xmath133 and @xmath134 .",
    "the following theorem  [ operatornormconvergence1 ] deals with consistency of the tapered estimator @xmath135 with respect to operator norm convergence .",
    "it extends theorem 1 in @xcite to the multivariate case and does not rely on the concept of physical dependence only .",
    "the operator norm of a complex - valued @xmath6 matrix @xmath17 is defined by @xmath136 and it is well known that @xmath137 , where @xmath138 denotes the largest eigenvalue of a matrix @xmath139 ; cf .",
    "@xcite , page 296 .",
    "[ operatornormconvergence1 ] suppose that assumptions with @xmath140 and are satisfied .",
    "then it holds @xmath141\\\\[-8pt]\\nonumber & & \\qquad \\leq \\frac{4md^2(\\lfloor c_\\kappa l\\rfloor+1)}{\\sqrt{n}}+2\\sum _ { h=0}^{\\lfloor c_\\kappa l\\rfloor}\\frac{{\\vert}h{\\vert}}{n}\\bigl{\\vert}\\mathbf{c}(h)\\bigr{\\vert}_1 + 2\\sum_{h = l+1}^{n-1}\\bigl{\\vert}\\mathbf{c}(h)\\bigr{\\vert}_1.\\end{aligned}\\ ] ]    the second term on the right - hand side of ( [ convergenceeq ] ) can be represented as @xmath142 and vanishes asymptotically due to the kronecker lemma and is of order @xmath143 .",
    "the third one converges to zero for @xmath144 as @xmath82 and the leading first term for @xmath145 . hence , for @xmath146 , the right - hand side of ( [ convergenceeq ] ) vanishes asymptotically .",
    "however , if @xmath147 for @xmath148 for some @xmath149 , setting @xmath150 fixed suffices . in this case",
    ", the expression on the right - hand side of ( [ convergenceeq ] ) is of faster order @xmath151 .    as already pointed out by @xcite , the tapered estimator @xmath152 is not guaranteed to be positive semi - definite or even to be positive definite for finite sample sizes .",
    "however , @xmath152 is at least `` asymptotically positive definite '' under assumption ( a3 ) and due to ( [ convergenceeq ] ) if @xmath146 holds . in the following , we require a consistent estimator for @xmath26 which is positive definite for all finite sample sizes to be able to compute its cholesky decomposition for the linear process bootstrap scheme that will be introduced in section  [ bootstrapscheme ] below .    to obtain an estimator of @xmath26 related to @xmath131 that is assured to be positive definite for all sample sizes , we construct a modified estimator @xmath153 in the following .",
    "let @xmath154 be the diagonal matrix of sample variances , and define @xmath155 .",
    "now we consider the spectral factorization @xmath156 , where @xmath157 is an @xmath158 orthogonal matrix and @xmath159 is the diagonal matrix containing the eigenvalues of @xmath160 such that @xmath161 .",
    "it is worth noting that this factorization always exists due to symmetry of @xmath162 , but that the eigenvalues can be positive , zero or even negative .",
    "now , define @xmath163 where @xmath164 and @xmath165 . here , @xmath166 and @xmath167 are user defined constants that ensure the positive definiteness of @xmath168 .",
    "contrary to the univariate case discussed in @xcite , we propose to adjust the eigenvalues of the ( equivariant ) correlation matrix @xmath162 instead of @xmath152 , which then comes along without a scaling factor in the definition of @xmath169 .",
    "further , note that setting @xmath170 leads to a positive semi - definite estimate if @xmath152 is indefinite , which does not suffice for computing the cholesky decomposition , and also that @xmath168 generally loses the banded shape of @xmath152 .",
    "theorem  [ operatornormconvergence2 ] below , which extends theorem  3 in @xcite , shows that the modification of the eigenvalues does affect the convergence results obtained in theorem [ operatornormconvergence1 ] just slightly .",
    "[ operatornormconvergence2 ] under the assumptions of theorem  [ operatornormconvergence1 ] , it holds @xmath171    in comparison to the upper bound established in theorem  [ operatornormconvergence1 ] , two more terms appear on the right - hand side of ( [ convergenceeq2 ] ) which do converge as well to zero as @xmath0 tends to infinity .",
    "note that the first three summands that the right - hand sides of ( [ convergenceeq ] ) and ( [ convergenceeq2 ] ) have in common , remain the leading terms if @xmath172 .",
    "we also need convergence and boundedness in operator norm of quantities related to @xmath173 .",
    "the required results are summarized in the following corollary .",
    "[ operatornormconvergence3 ] under assumptions with @xmath140 , and , we have :    @xmath174 and @xmath175 are terms of order @xmath176 , where @xmath177 and @xmath178 if @xmath146 .",
    "@xmath179 and @xmath180 are of order @xmath181 and @xmath182 if @xmath183 , and holds for some @xmath184 .",
    "@xmath185 , @xmath186 , @xmath187 , @xmath188 are bounded from above and below .",
    "@xmath189 , @xmath190 and @xmath191 , @xmath192 are bounded from above and below ( in probability ) if @xmath178 and @xmath182 , respectively .",
    "[ individualbanding ] in section  [ taperedestimator ] , we propose to use a global banding parameter @xmath122 that down - weights the autocovariance matrices for increasing lag ; that is , the entire matrix @xmath49 is multiplied with the same @xmath193 in ( [ gammahat ] ) .",
    "however , it is possible to use individual banding parameters @xmath194 for each sequence of entries @xmath195 , @xmath196 as proposed in @xcite , compare also the simulation section .      to get a tapered estimate @xmath152 of the covariance matrix @xmath26",
    ", some parameters have to be chosen by the practitioner .",
    "these are the flat - top taper @xmath126 and the banding parameter @xmath122 , which are both responsible for the down - weighting of the empirical autocovariances @xmath197 with increasing lag @xmath56 .",
    "to select a suitable taper @xmath126 from the class of functions ( [ kappa ] ) , we have to select @xmath121 and the function @xmath198 which determine the range of the decay of @xmath126 to zero for @xmath199 and its form over this range , respectively . for some examples of flat - top tapers ,",
    "compare @xcite ( @xcite ) .",
    "however , the selection of the banding parameter @xmath122 appears to be more crucial than choosing the tapering function @xmath126 among the family of well - behaved flat - top kernels as discussed in @xcite .",
    "this is comparable to nonparametric kernel estimation where usually the bandwidth plays a more important role than the shape of the kernel .",
    "we focus on providing an empirical rule for banding parameter selection that has already been used in @xcite for the univariate lpb .",
    "they make use of an approach primarily proposed in @xcite to estimate the bandwidth in spectral density estimation which has been generalized to the multivariate case in @xcite . in the following , we adopt this technique based on the correlogram / cross - correlogram [ cf .",
    "@xcite ( @xcite , section  6 ) ] for our purposes .",
    "let @xmath200 be the sample ( cross-)correlation between the two univariate time series @xmath201 and @xmath202 at lag @xmath8 .",
    "now , define @xmath203 as the smallest nonnegative integer such that @xmath204 for @xmath205 , where @xmath206 is a fixed constant , and @xmath207 is a positive , nondecreasing integer - valued function of @xmath0 such that @xmath208 .",
    "note that the constant @xmath209 and the form of @xmath207 are the practitioner s choice . as a rule of thumb",
    ", we refer to @xcite ( @xcite ) who makes the concrete recommendation @xmath210 and @xmath211 . after having computed @xmath212 for all @xmath213 ,",
    "we take @xmath214 as a data - driven _ global _ choice of the banding parameter @xmath122 . by setting @xmath215",
    ", we get data - driven _ individual _ banding parameter choices as discussed in remark  [ individualbanding ] . for theoretical justification of this empirical selection of a global cut - off point as the maximum over individual choices and assumptions that lead to successful adaptation",
    ", we refer to theorem 6.1 in @xcite .",
    "note also that for positive definite covariance matrix estimation , that is , for computing @xmath168 , one has to select two more parameters @xmath216 and @xmath217 , which have to be nonnegative and might be set equal to one as suggested in @xcite .",
    "in this section , we describe the multivariate linear process bootstrap ( mlpb ) in detail , discuss some modifications and comment on the special case where the tapered covariance estimator becomes diagonal .",
    "let @xmath218 be the @xmath23 data matrix consisting of @xmath1-valued time series data @xmath219 of sample size @xmath0 .",
    "compute the centered observations @xmath220 , where @xmath221 , let @xmath222 be the corresponding @xmath23 matrix of centered observations , and define @xmath223 to be the @xmath22-dimensional vectorized version of @xmath222 .",
    "compute @xmath224 , where @xmath225 denotes the lower left triangular matrix @xmath226 of the cholesky decomposition @xmath227 .",
    "let @xmath228 be the standardized version of @xmath229 , that is , @xmath230 , @xmath231 , where @xmath232 and @xmath233 .",
    "generate @xmath234 by i.i.d .",
    "resampling from @xmath235 .",
    "compute @xmath236 , and let @xmath237 be the matrix that is obtained from @xmath238 by putting this vector column - wise into an @xmath23 matrix , and denote its columns by @xmath239 .    regarding steps 3 and 4 above and due to the multivariate nature of the data",
    ", it appears to be even more natural to split the @xmath22-dimensional vector @xmath228 in step 3 above in @xmath0 sub - vectors , to center and standardize them and to apply i.i.d .",
    "resampling to these vectors to get @xmath240 .",
    "more precisely , steps 3 and 4 can be replaced by :    let @xmath241 be the standardized version of @xmath229 , that is , @xmath242 , where @xmath243 , @xmath244 and @xmath245 .",
    "generate @xmath246 by i.i.d .",
    "resampling from @xmath247 .",
    "this might preserve more higher order features of the data that are not captured by @xmath168 .",
    "however , comparative simulations ( not reported in the paper ) indicate that the finite sample performance is only slightly affected by this sub - vector resampling .",
    "[ remark2 ] if @xmath248 , the banded covariance matrix estimator @xmath152 ( and @xmath249 as well ) becomes diagonal . in this case and if steps 3@xmath250  and  4@xmath250 are used , the lpb as described above is equivalent to the classical i.i.d .",
    "bootstrap . here , note the similarity to the autoregressive sieve bootstrap which boils down to an i.i.d . bootstrap if the autoregressive order is @xmath251 .",
    "in this section , we establish validity of the mlpb for the sample mean .",
    "the following theorem generalizes theorem 5 of @xcite to the multivariate case under somewhat more general conditions .    [ validitysamplemean ] under assumptions for some @xmath184 , ,",
    ", for @xmath252 , and @xmath183 , the mlpb is asymptotically valid for the sample mean @xmath253 , that is , @xmath254 and @xmath255 , where @xmath256 . the short - hand @xmath257 for @xmath258 is used to denote @xmath259 for all @xmath66 .",
    "here we prove consistency of the mlpb for kernel spectral density matrix estimators ; this result is novel even in the univariate case .",
    "let @xmath260 the periodogram matrix , where @xmath261 is the discrete fourier transform ( dft ) of @xmath262 , @xmath263 .",
    "we define the estimator @xmath264 for the spectral density matrix @xmath265 , where @xmath266 is the integer part of @xmath42 , @xmath267 are the fourier frequencies , @xmath268 is the bandwidth and @xmath269 is a symmetric and square integrable kernel function @xmath270 that satisfies @xmath271 and @xmath272 and we set @xmath273 .",
    "let @xmath274 be the bootstrap analogue of @xmath275 based on @xmath276 generated from the mlpb scheme and let @xmath277 be the bootstrap analogue of @xmath278 .",
    "[ validitykernelspectral ] suppose assumptions with @xmath279 specified below , , , for @xmath280 and are satisfied . if @xmath79 and @xmath80 such that @xmath81 as well as @xmath281 and @xmath282 for some sequence @xmath283 , the mlpb is asymptotically valid for kernel spectral density estimates @xmath284 .",
    "that is , for all @xmath285 and arbitrary frequencies @xmath286 ( not necessarily fourier frequencies ) , it holds @xmath287 where @xmath288 and , in particular , @xmath289 and @xmath290 , for all @xmath85 and all @xmath291 $ ] , respectively .      for statistics @xmath292 contained in the broad class of functions of generalized means ,",
    "@xcite discussed how by using a preliminary blocking scheme tailor - made for a specific statistic of interest , the mlpb can be shown to be consistent .",
    "this class of statistics contains estimates @xmath292 of @xmath293 with @xmath294 such that @xmath295 for some sufficiently smooth functions @xmath296 , @xmath297 and fixed .",
    "they propose to block the data first according to the known function @xmath198 and to apply then the ( m)lpb to the blocked data .",
    "more precisely , the multivariate lpb - of - blocks bootstrap is as follows :    define @xmath298 , and let @xmath299 be the set of blocked data .",
    "apply the mlpb scheme of section  [ bootstrapscheme ] to the @xmath108-dimensional blocked data @xmath300 to get bootstrap observations @xmath301 .",
    "compute @xmath302 .",
    "repeat steps 2 and 3 @xmath303-times , where @xmath303 is large , and approximate the unknown distribution of @xmath304 by the empirical distribution of @xmath305 .",
    "the validity of the multivariate lpb - of - blocks bootstrap for some statistic @xmath292 can be verified by checking the assumptions of theorem [ validitysamplemean ] for the sample mean of the new process @xmath306 .",
    "in this section , we consider the case when the time series dimension @xmath15 is allowed to increase with the sample size @xmath0 , that is , @xmath307 as @xmath82 . in particular , we show consistency of tapered covariance matrix estimates and derive rates that allow for an asymptotic validity result of the mlpb for the sample mean in this case .",
    "the recent paper by @xcite gives a thorough discussion of the estimation of toeplitz covariance matrices for univariate time series . in their setup , that covers also the possibility of having multiple datasets from the same data generating process , @xcite establish the optimal rates of convergence using the two simple flat - top kernels discussed in section  [ taperedestimator ] , namely the truncated ( i.e. , case of pure banding ",
    "no tapering ) and the trapezoid taper . when the strength of dependence is quantified via a smoothness condition on the spectral density ,",
    "they show that the trapezoid is superior to the truncated taper , thus confirming the intuitive recommendations of @xcite .",
    "the asymptotic theory of @xcite allows for increasing number of time series and increasing sample size , but their framework does not contain the multivariate time series case , neither for fixed nor for increasing time series dimension , which will be discussed in this section .",
    "note that theorem 1 in @xcite for the univariate case , as well as our theorem  [ operatornormconvergence1 ] for the multivariate case of fixed time series dimension , give upper bounds that are quite sharp , coming within a log - term to the ( gaussian ) optimal rate found in theorem 2 of @xcite .    instead of assumptions ( a1)(a5 ) that have been introduced in section  [ assumptions ] and used in theorem  [ validitysamplemean ] to obtain bootstrap consistency for the sample mean for fixed dimension @xmath15",
    ", we impose the following conditions on the sequence of time series process @xmath308 of now increasing dimension .",
    "@xmath309 is a sequence of @xmath310-valued strictly stationary time series processes with mean vectors @xmath311 and autocovariances @xmath312 defined as in ( [ covariance ] ) . here",
    ", @xmath313 is a nondecreasing sequence of positive integers such that @xmath314 as @xmath82 and , further , suppose @xmath315 for some @xmath279 to be further specified .",
    "there exists a constant @xmath316 such that for all @xmath55 and all @xmath56 with @xmath12 , we have @xmath317    there exists an @xmath60 large enough such that for all @xmath61 and all @xmath318 the eigenvalues @xmath62 of the @xmath63 covariance matrix @xmath26 are bounded uniformly away from zero and from above .",
    "define the sequence of projection operators @xmath319 for @xmath320 , and suppose @xmath321 and @xmath322    for the sample mean , a cramr  wold - type clt holds true .",
    "that is , for any real - valued sequence @xmath323 of @xmath324-dimensional vectors with @xmath325 for all @xmath55 and @xmath326 , we have @xmath327    assumptions ( a1@xmath250)(a4@xmath250 ) are uniform analogues of ( a1)(a4 ) , which are required here to tackle the increasing time series dimension @xmath15 . in particular , ( a1@xmath250 ) implies @xmath328 observe also that the autocovariances @xmath329 are assumed to decay with increasing lag @xmath56 , that is , in time direction , but they are not assumed to decay with increasing @xmath330 , that is , with respect to increasing time series dimension .",
    "therefore , we have to make use of square summable sequences in ( a5@xmath250 ) to get a clt result .",
    "this technique has been used , for example , by @xcite and @xcite to establish central limit results for the estimation of an increasing number of autoregressive coefficients .",
    "a simple sufficient condition for ( a5@xmath250 ) is , for example , the case of @xmath331 being a sequence of i.i.d .",
    "gaussian processes with eigenvalues of @xmath332 bounded uniformly from above and away from zero .",
    "the following theorem generalizes the results of theorems  [ operatornormconvergence1 ] and  [ operatornormconvergence2 ] and of corollary  [ operatornormconvergence3 ] to the case where @xmath333 is allowed to increase with the sample size .",
    "in contrast to the case of a stationary spatial process on the plane @xmath334 ( where a data matrix is observed that grows in both directions asymptotically as in our setting ) , we do not assume that the autocovariance matrix decays in all directions .",
    "therefore , to be able to establish a meaningful theory , we have to replace ( a1)(a5 ) by the uniform analogues ( a1@xmath335)(a5@xmath250 ) , and due to ( [ rate ] ) , an additional factor @xmath336 turns up in the convergence rate and has to be taken into account .    [ operatornormconvergence3d ] under assumptions with @xmath279 specified below , and",
    ", we have :    @xmath174 and @xmath175 are terms of order @xmath337 , where @xmath338 and @xmath339 if @xmath340 .",
    "@xmath179 and @xmath180 are both terms of order @xmath341 and @xmath342 if @xmath343 .",
    "@xmath185 , @xmath186 , @xmath187 and @xmath344 are bounded from above and below . @xmath345 and",
    "@xmath346 as well as @xmath347 and @xmath348 are bounded from above and below in probability if @xmath339 and @xmath349 , respectively .",
    "the required rates for the banding parameter @xmath122 and the time series dimension @xmath15 to get operator norm consistency @xmath350 can be interpreted nicely .",
    "if @xmath198 is chosen to be large enough , @xmath351 becomes the leading term , and there is a trade - off between capturing more dependence of the time series in time direction ( large @xmath122 ) and growing dimension of the time series in cross - sectional direction ( large @xmath15 ) .      the subsequent theorem is a cramr  wold - type generalization of theorem  [ validitysamplemean ] to the case where @xmath333 is allowed to grow at an appropriate rate with the sample size . to tackle the increasing time series dimension and to prove such a clt result , we have to make use of appropriate sequences of square summable vectors @xmath352 as described in ( a5@xmath250 ) above .",
    "[ validitysamplemeand ] under assumptions with @xmath279 specified below , , , for @xmath252 , as well as @xmath353 and @xmath354 for some sequence @xmath283 , the mlpb is asymptotically valid for the sample mean @xmath355 . that is , for any real - valued sequence @xmath323 of @xmath324-dimensional vectors with @xmath325 for all @xmath55 and @xmath356 , we have @xmath357 and @xmath358 .      in practice",
    ", the computational requirements can become very demanding for large @xmath15 and @xmath0 . in this case",
    ", we suggest to split the data vector @xmath25 in few subsamples @xmath359 , say , and to apply the mlpb scheme to each subsample separately .",
    "this operation can be justified by the fact that dependence structure is distorted only few times . precisely , we suggest the following procedure :    for small @xmath360 , define @xmath361 and @xmath362 such that @xmath363 , and let @xmath364 , @xmath365 , where @xmath366 is filled up with zeros if @xmath367 .",
    "apply the mlpb bootstrap scheme as described in section  [ bootstrapscheme ] separately to the subsamples @xmath368 to get @xmath369 .",
    "put @xmath370 end - to - end together , and discard the last @xmath371 values to get @xmath372 and @xmath373 .    here ,",
    "computationally demanding operations as eigenvalue decomposition , cholesky decomposition and matrix inversion have to be executed only for lower - dimensional matrices , such that the algorithm above is capable to reduce the computation time considerably .",
    "further , to regain efficiency , we propose to use the pooled sample mean @xmath355 for centering and @xmath374 for whitening and re - introducing correlation structure for _ all _ subsamples in step 2 . here",
    ", @xmath375 is obtained analogously to ( [ gammahatepsilon ] ) , but based on the upper - left @xmath376 sub - matrix of @xmath135 .",
    "in this section we compare systematically the performance of the multivariate linear process bootstrap ( mlpb ) to that of the vector - autoregressive sieve bootstrap ( ar - sieve ) , the moving block bootstrap ( mbb ) and the tapered block bootstrap ( tbb ) by means of simulation . in order to make such a comparison ,",
    "we have chosen a statistic for which all methods lead to asymptotically correct approximations .",
    "being interested in the distribution of the sample mean , we compare the aforementioned bootstrap methods by plotting :    root mean squared errors ( rmse ) for estimating the variances of @xmath377 and    coverage rates ( cr ) of 95% bootstrap confidence intervals for the components of @xmath378    for two data generating processes ( dgps ) and three sample sizes in two different setups . first , in section  [ secsimtuning ] , we compare the performance of all aforementioned bootstraps with respect to ( w.r.t . ) tuning parameter choice .",
    "these are the banding parameter @xmath122 ( mlpb ) , the autoregressive order @xmath379 ( ar - sieve ) and the block length  @xmath380 ( mbb , tbb ) .",
    "furthermore , we report rmse and cr for data - adaptively chosen tuning parameters to investigate how accurate automatic selection procedures can work in practice .",
    "second , in section  [ secsimdimension ] , we investigate the effect of the time series dimension @xmath15 on the performance of the different bootstrap approaches .    for each case , we have generated @xmath381 time series and @xmath382 bootstrap replications have been used in each step . for ( a ) , the exact covariance matrix of @xmath383 is estimated by 20,000 monte carlo replications .",
    "further , we use the trapezoidal kernel defined in ( [ trapezoid ] ) to taper the sample covariance matrix for the mlpb and the blocks for the tbb . to correct the covariance matrix estimator @xmath135 to be positive definite , if necessary",
    ", we set @xmath384 and @xmath385 to get @xmath386 .",
    "this choice has already been used by @xcite and simulation results ( not reported in this paper ) indicate that the performance of the mlpb reacts only slightly to this choice .",
    "we have used the sub - vector resampling scheme , that is , steps 3@xmath387 and 4@xmath387 described in section  [ bootstrapscheme ] .",
    "some additional simulation results and a real data application of the mlpb to the weighted mean of an increasing number of german stock prices taken from the dax index can be found in the supplementary material to this paper [ @xcite ] .",
    "the r code is available at http://www.math.ucsd.edu/\\textasciitilde politis / soft / function_mlpb.r[http://www.math.ucsd.edu/\\textasciitilde politis / soft / function_mlpb.r ] .",
    "we consider realizations @xmath388 of length @xmath389 from two bivariate ( @xmath390 ) dgps .",
    "precisely , we study a first - order vector moving average process @xmath391 and a first - order vector autoregressive process @xmath392 where @xmath393 is a normally distributed i.i.d .",
    "white noise process and @xmath394 have been used in all cases .",
    "it is worth noting that ( asymptotically ) all bootstrap procedures under consideration yield valid approximations for both models above . for the vma(1 )",
    "model , mlpb is valid for all ( sufficiently small ) choices of banding parameters @xmath395 , but ar - sieve is valid only asymptotically for @xmath396 tending to infinity at an appropriate rate with increasing sample size @xmath0 .",
    "this relationship of mlpb and ar - sieve is reversed for the var(1 ) model . for the mbb and the tbb",
    ", the block length has to increase with the sample size for both dgps .     and cr of bootstrap confidence intervals for @xmath397 by mlpb ( solid ) , ar - sieve ( dashed ) , mbb ( dotted ) and tbb ( dash - dotted )",
    "are reported vs. the respective tuning parameters @xmath398 for the vma(1 ) model with sample size @xmath399 .",
    "line segments indicate results for data - adaptively chosen tuning parameters .",
    "mlpb with individual ( grey ) and global ( black ) banding parameter choice are reported . ]    , but with var(1 ) model . ]",
    "in addition to the results for tuning parameters @xmath400 , we show also rmse and cr for tuning parameters chosen by automatic selection procedures in figures  [ fig2 ] and  [ fig3 ] . for the mlpb , we report results for data - adaptively chosen global and individual banding parameters as discussed in section  [ selectionsec ] . for the ar - sieve , the order of the var model fitted to the data has been chosen by using the * r * routine var@xmath401 contained in the package * vars * with _ lag_.__max__@xmath402 .",
    "the block length is chosen by using the * r * routine _ b_.__star__@xmath401 contained in the package * np*. in figures  [ fig2 ] and  [ fig3 ] , we report only the results corresponding to the first component of the sample mean , as those for the second component lead qualitatively to the same results . we show them in the supplementary material , which contains also corresponding simulation results for a normal white noise dgp .    for data generated by the vma(1 ) model , figure  [ fig2 ] shows that the mlpb outperforms ar - sieve , mbb and tbb for adequate tuning parameter choice , that is , @xmath403 . in this case",
    ", the mlpb generally behaves superiorly , with respect to rmse and cr , to the other bootstrap methods for all tuning parameter choices of @xmath379 and  @xmath380 .",
    "this was not unexpected since , by design , the mlpb can approximate very efficiently the covariance structure of moving average processes .",
    "nevertheless , due to the fact that all proposed bootstrap schemes are valid at least asymptotically , ar - sieve gets rid of its bias with increasing order @xmath379 , but at the expense of increasing variability and consequently also increasing rmse .",
    "mlpb with data - adaptively chosen banding parameter performs quite well , where the individual choice tends to perform superiorly to the global choice in most cases . in comparison ,",
    "mbb and tbb seem to perform quite well for adequate block length , but they lose in terms of rmse as well as cr performance if the block length is chosen automatically .    the data from the var(1 ) model is highly persistent due to the coefficient @xmath404 near to unity .",
    "this leads to autocovariances that are rather slowly decreasing with increasing lag and , consequently , to large variances of @xmath405 .",
    "figure  [ fig3 ] shows that ar - sieve outperforms mlpb , mbb and tbb with respect to cr for small ar orders @xmath406 .",
    "this is to be expected since the underlying var(1 ) model is captured well by ar - sieve even with finite sample size .",
    "but the picture appears to be different with respect to rmse .",
    "here , mlpb may perform superiorly for adequate tuning parameter choice , but this effect can be explained by the very small variance that compensates its large bias , in comparison to the ar - sieve ( bias and variance not reported here ) leading to a smaller rmse .",
    "this phenomenon is also illustrated by the poor performance of mlpb with respect to cr for small choices of @xmath122 .",
    "however , more surprising is the rather good performance of the mlpb if the banding parameter is chosen data - adaptively , where the mlpb appears to be comparable to the ar - sieve in terms of rmse and is at least close with respect to cr .",
    "further , as observed already for the vma(1 ) model in figure  [ fig2 ] , the individual banding parameter choice generally tends to outperform the global choice here again .",
    "similarly , it can be seen here that the performance of ar - sieve worsens with increasing @xmath379 at the expense of increasing variability .",
    "the block bootstraps mbb and tbb appear to be clearly inferior to mlpb and ar - sieve , particularly with respect to cr , but also with respect to rmse if tuning parameters are chosen automatically .",
    "we consider @xmath15-dimensional realizations @xmath407 with @xmath389 from two dgps of several dimensions .",
    "precisely , we study first - order vector moving average processes @xmath408 and first - order vector autoregressive processes @xmath409 of dimension @xmath410 , where @xmath411 is a @xmath15-dimensional normally distributed i.i.d",
    ". white noise process , and @xmath412 and @xmath413 are such that @xmath414 observe that the vma(1 ) and var(1 ) models considered in section  [ secsimtuning ] are included in this setup for @xmath390 .    in figures  [ fig4 ] and  [ fig5 ] , we compare the performance of mlpb , ar - sieve , mbb and tbb for the dgps above using rmse and cr averaged over all @xmath15 time series coordinates .",
    "precisely , we compute rmse individually for the estimates of @xmath415 , @xmath66 and plot the averages in the upper half of figures  [ fig4 ] and  [ fig5 ] . similarly , we plot averages of individually calculated cr of bootstrap confidence intervals for @xmath416 , @xmath66 in the lower halfs .",
    "all tuning parameters are chosen in a data - based and optimal way , as described in section  [ secsimtuning ] , and to reduce computation time , the less demanding algorithm , as described in section  [ secreduction ] with @xmath417 , is used .    ,",
    "@xmath66 and average cr of bootstrap confidence intervals for @xmath416 , @xmath66 , by mlpb ( solid ) , ar - sieve ( dashed ) , mbb ( dotted ) and tbb ( dash - dotted ) with data - based optimal tuning parameter choices are reported vs. the dimension @xmath418 for the vma@xmath419(1 ) model with sample size @xmath420 .",
    "mlpb with individual ( grey ) and global ( black ) banding parameter choice are reported . ]    , but with var@xmath419(1 ) model . ]    for the vma(1 ) dgps in figure  [ fig4 ] , the mlpb with individual banding parameter choice outperforms the other approaches essentially for all time series dimension under consideration with respect to averaged rmse and cr .",
    "in particular , larger time series dimensions do not seem to have a large effect on the performance of all bootstraps for the vma(1 ) dgps , with the only exception being the mlpb with global banding parameter choice .",
    "in particular , the latter is clearly inferior in comparison to the mlpb with individually chosen banding parameter , which might be explained by sparsity of the covariance matrix @xmath421 .    in figure",
    "[ fig5 ] , for the var(1 ) dgps , the picture is different from the vma(1 ) case above .",
    "the influence of larger time series dimension on rmse ( and less pronounced for cr ) performance is much more pronounced and clearly visible .",
    "in particular , the rmse blows up with increasing dimension @xmath15 for all four bootstrap methods , which is due to the also increasing variance of the process .",
    "note that the zig - zag shape of the rmse curves is due to the back and forth switching from @xmath422 to @xmath423 on the diagonal of @xmath424 . as already observed for the vma(1 ) dgps , the mlpb with individual banding parameter choice again performs best over essentially all time series dimensions with respect to average rmse and average cr .",
    "in particular , mlpb with individual choice is superior to the global choice . here",
    ", the good performance of the mlpb is somewhat surprising as the var(1 ) dgps have rather slowly decreasing autocovariance structure , where we expected an ar - sieve to be more suitable .",
    "the authors thank timothy mcmurry for his helpful advice on the univariate case and three anonymous referees and the editor who helped to significantly improve the presentation of the paper ."
  ],
  "abstract_text": [
    "<S> multivariate time series present many challenges , especially when they are high dimensional . </S>",
    "<S> the paper s focus is twofold . </S>",
    "<S> first , we address the subject of consistently estimating the autocovariance sequence ; this is a sequence of matrices that we conveniently stack into one huge matrix . </S>",
    "<S> we are then able to show consistency of an estimator based on the so - called _ flat - top tapers _ ; most importantly , the consistency holds true even when the time series dimension is allowed to increase with the sample size . </S>",
    "<S> second , we revisit the linear process bootstrap ( lpb ) procedure proposed by mcmurry and politis [ _ j .  </S>",
    "<S> time series anal . _ </S>",
    "<S> * 31 * ( 2010 ) 471482 ] for univariate time series . based on the aforementioned stacked autocovariance matrix estimator , </S>",
    "<S> we are able to define a version of the lpb that is valid for multivariate time series . under rather general assumptions , </S>",
    "<S> we show that our multivariate linear process bootstrap ( mlpb ) has asymptotic validity for the sample mean in two important cases : ( a ) when the time series dimension is fixed and ( b ) when it is allowed to increase with sample size . as an aside , in case ( a ) </S>",
    "<S> we show that the mlpb works also for spectral density estimators which is a novel result even in the univariate case . </S>",
    "<S> we conclude with a simulation study that demonstrates the superiority of the mlpb in some important cases </S>",
    "<S> .    ./style / arxiv - general.cfg </S>"
  ]
}