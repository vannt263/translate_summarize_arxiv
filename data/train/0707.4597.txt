{
  "article_text": [
    "consider the following scenario where a server is to broadcast multimedia data to multiple users with different side informations , however the side informations are not available at the server .",
    "a user may have such strong side information that only minimal additional information is required from the server to satisfy a fidelity criterion , or a user may have barely any side information and expect the server to provide virtually everything to satisfy a ( possibly different ) fidelity criterion .    a naive strategy is to form a single description and broadcast it to all the users , who can decode only after receiving it completely regardless of the quality of their individual side informations .",
    "however , for the users with good - quality side information ( who will simply be referred to as the good users ) , most of the information received is redundant , which introduces a delay caused simply by the existence of users with poor - quality side informations ( referred to as the bad users ) in the network .",
    "it is natural to ask whether an opportunistic method exists , _",
    "i.e. , _ whether it is possible to construct a two - layer description , such that the good users can decode with only the first layer , and the bad users receive both the first and the second layer to reconstruct . moreover , it is of importance to investigate whether such a coding order introduces any performance loss .",
    "we call this coding strategy _ side - information scalable _ ( si - scalable ) source coding , since the scalable coding direction is from the good users to the bad users . in this work ,",
    "we consider mostly two - layer systems , except the quadratic gaussian source for which the solution to the general multi - layer problem is given .",
    "this work is related to the successive refinement problem , where a source is to be encoded in a scalable manner to satisfy different distortion requirement at each individual stage .",
    "this problem was studied by koshelev @xcite , and by equitz and cover @xcite ; a complete characterization of the rate - distortion region can be found in @xcite .",
    "another related problem is the rate - distortion for source coding with side information at the decoder @xcite , for which wyner and ziv provided conclusive result ( now widely known as the wyner - ziv problem ) .",
    "steinberg and merhav @xcite recently extended the successive refinement problem in the wyner - ziv setting ( sr - wz ) , when the second stage side information @xmath0 is better than that of the first stage @xmath1 , in the sense that @xmath2 forms a markov string .",
    "the extension to multistage systems with degraded side informations in such a direction was recently completed in @xcite . also relevant",
    "is the work by heegard and berger @xcite ( see also @xcite ) , where the problem of source coding when side information may be present at the decoder was considered ; the result was extended to the multistage case when the side informations are degraded .",
    "this is quite similar to the problem being considered here and in @xcite@xcite , however without the scalable coding requirement .    ]",
    "both the sr - wz @xcite@xcite and si - scalable problems can be thought as special cases of the problem of scalable source coding with no specific structure imposed on the decoder si ; this general problem appears to be quite difficult , since even without the scalable requirement , a complete solution to the problem has not been found @xcite .",
    "here we emphasize that the sr - wz and the si - scalable problem are quite different in terms of their applications , though they seem similar since only the order of si quality that is reversed . roughly speaking , in the si - scalable problem",
    ", the side information @xmath0 at the later stage is worse than the side information @xmath1 at the early stage , while in the sr - wz problem , the order is reversed . in more mathematically precise terms , for the si - scalable problem , the side informations are degraded as @xmath3 , in contrast to the sr - wz problem where the reversed order is specified as @xmath4 .",
    "the two problems are also different in terms of their possible applications .",
    "the sr - wz problem is more applicable for a single server - user pair , when the user is receiving side information through another channel , and at the same time receiving the description(s ) from the server ; for this scenario , two decoders can be extracted to provide a simplified model . on the other hand ,",
    "the si - scalable problem is more applicable when multiple users exist in the network , and the server wants to provide a scalable description , such that the good user is not jeopardized unnecessarily ( see fig .",
    "[ fig : srwz ] ) .",
    "it is also worth pointing out that heegard and berger showed when the scalable coding requirement is removed , the optimal encoding by itself is in fact naturally progressive from the bad user to the good one ; as such , the si - scalable problem is expected to be more difficult than the sr - wz problem , since the encoding order is reversed from the natural one .",
    "this difficulty is encapsulated by the fact that in the sr - wz ordering the decoder with better si is able to decode whatever message was meant for the decoder with worse si and hence the first stage can be maximally useful .",
    "however , in the si - scalable problem an additional tension exists in the sense that the second - stage decoder will need extra information to disambiguate the information of the first stage .",
    "the problem is well understood for the lossless case .",
    "the key difference from the lossy case is that the quality of the side informations can be naturally determined by the value of @xmath5 . by the seminal work of slepian and wolf @xcite",
    ", @xmath5 is the minimum rate of encoding @xmath6 losslessly with side information @xmath7 at the decoder , thus in a sense a larger @xmath5 corresponds to weaker side information .",
    "if @xmath8 , then the rate @xmath9 is achievable , as noticed by feder and shulman @xcite . extending this observation and a coding scheme in @xcite ,",
    "draper @xcite proposed a universal incremental slepian - wolf coding scheme when the distribution is unknown , which inspired eckford and yu @xcite to design rateless slepian - wolf ldpc code .",
    "for the lossless case , there is no loss of optimality by using a scalable coding approach ; an immediate question is to ask whether the same is true for the lossy case in terms of rate distortion , which we will show to be not so in general . in this rate - distortion setting , the order of goodness by the value of @xmath5 is not sufficient because of the presence of the distortion constraints .",
    "this motivates the markov condition @xmath3 introduced for the si - scalable coding problem . going further along this point of view , the si - scalable problem is also applicable in the single user setting , when the source encoder does not know exactly which side information the receiver has within a given set .",
    "therefore it can be viewed as a special case of the side - information universal rate distortion coding .    in this work",
    ", we formulate the problem of side information scalable source coding , and provide two inner bounds and two outer bounds for the rate - distortion region .",
    "one of the inner - bounds has the same distortion and rate expressions as one of the outer bounds , and they differ in the domain of optimization only by a markov string requirement . though the inner and the outer bounds do not coincide in general , the inner bounds are indeed tight for the case when either the first stage or the second stage requires a lossless reconstruction , as well as for the case when certain deterministic distortion measures are taken . furthermore , a conclusive result is given for the quadratic gaussian source with any finite number of stages and arbitrary correlated gaussian side informations .    with this set of inner and outer bounds , the problem of _ perfect scalability _ is investigated , defined as when both of the layers can achieve the corresponding wyner - ziv bounds ; this is similar to the notion of ( strict ) successive refinability in the sr - wz problem @xcite@xcite .",
    "necessary and sufficient conditions are derived for general discrete memoryless sources to be perfectly scalable under a mild support condition . by using the tool of rate - loss introduced by zamir @xcite",
    ", we further show that the gap between the inner bounds and the outer bounds are bounded by a constant when squared error distortion measure is used , and thus the inner bounds are  nearly sufficient \" , in the sense as given in @xcite .",
    "in addition to the result for the gaussian source , partial result is provided for the doubly symmetric binary source ( dsbs ) with hamming distortion measure when the second stage does not have side information , for which the inner bounds and outer bounds coincide in certain distortion regimes .",
    "it is shown one of the outer bound can be strictly better than the other for this source .",
    "the rest of the paper is organized as follows . in section [ sec : prelim ] we define the problem and establish the notation . in section [ sec : ach ] , we provide inner and outer bounds to the rate - distortion region and show that the bounds coincide in certain special cases . the notion of perfectly scalable",
    "is introduced in section [ sec : perfscalable ] together with the example of a binary source .",
    "the rate loss method is applied in section [ sec : rateloss ] to show the gap between the inner bound and the outer bounds is bounded . in [ sec : gaussian ] , the gaussian source is treated within a more general setting .",
    "we conclude the paper in section [ sec : disc ] .",
    "let @xmath10 be a finite set and let @xmath11 be the set of all @xmath12-vectors with components in @xmath10 . denote an arbitrary member of @xmath11 as @xmath13 , or alternatively as @xmath14 .",
    "upper case is used for random variables and vectors . a discrete memoryless source",
    "( dms ) @xmath15 is an infinite sequence @xmath16 of independent copies of a random variable @xmath6 in @xmath10 with a generic distribution @xmath17 with @xmath18 .",
    "similarly , let @xmath19 be a discrete memoryless three - source with generic distribution @xmath20 ; the subscript will be dropped when it is clear from the context as @xmath21 .",
    "let @xmath22 and @xmath23 be finite reconstruction alphabets .",
    "let @xmath24 , @xmath25 be two distortion measures .",
    "the single - letter distortion extension of @xmath26 to vectors is defined as @xmath27    an @xmath28 rate distortion ( rd ) si - scalable code for source @xmath6 with side information @xmath29 consists of two encoding functions @xmath30 and two decoding functions @xmath31 , @xmath32 : @xmath33 where @xmath34 , such that @xmath35 where @xmath36 is the expectation operation .",
    "a rate pair @xmath37 is said to be @xmath38-achievable for si - scalable encoding with side information @xmath29 , if for any @xmath39 and sufficiently large @xmath12 , there exist an @xmath40 rd si - scalable code , such that @xmath41 and @xmath42 .",
    "denote the collection of all the @xmath38-achievable rate pair @xmath37 for si - scalable encoding as @xmath43 , and we seek to characterize this region when @xmath44 forms a markov string ( see similar but different degradedness conditions in @xcite ) .",
    "the markov condition in effect specifies the _ goodness _ of the side informations .",
    "the rate - distortion function for degraded side - informations was established in @xcite for the non - scalable coding problem . in light of the discussion in section [ sec : intro ] , it gives a lower bound on the sum - rate for any rd si - scalable code .",
    "more precisely , in order to achieve distortion @xmath45 with side information @xmath1 , and achieve distortion @xmath46 with side information @xmath0 , when @xmath3 , the rate - distortion function is @xmath47 , \\end{aligned}\\ ] ] where @xmath48 is the set of all random variable @xmath49 jointly distributed with the generic random variables @xmath50 , such that the following conditions are satisfied was defined as @xmath51 , but it is straightforwardly to verify that they are equivalent .",
    "the cardinality bound is also ignored , which is not essential here . ] : ( i ) @xmath52 is a markov string ; ( ii ) @xmath53 and @xmath54 satisfy the distortion constraints .",
    "notice that the rate distortion function @xmath55 given above suggests an encoding and decoding order from the bad user to the good user .",
    "wyner and ziv @xcite showed that under the following quite general assumption that the distortion measure is chosen in the set @xmath56 defined as @xmath57 then the rate distortion function satisfies @xmath58 , where @xmath59 is the well - known wyner - ziv rate distortion function with side information @xmath7 .",
    "if the same assumption is made on the distortion measure @xmath60 , then we can easily show ( using an argument similar to the remark ( 3 ) in @xcite ) that @xmath61 ,   \\label{eqn : hbcoro}\\end{aligned}\\ ] ] where @xmath62 is the set of all random variable @xmath63 such that @xmath64 is a markov string , and @xmath54 satisfies the distortion constraint .",
    "to provide intuition into the the si - scalable problem , we first examine a simple gaussian source under the mean squared error ( mse ) distortion measure , and describe the coding schemes informally .",
    "let @xmath65 and @xmath66 , where @xmath67 is independent of @xmath6 ; @xmath0 is simply a constant , _ i.e. , _ no side information at the second decoder .",
    "@xmath3 is indeed a markov string . to avoid lengthy discussion on degenerate regimes , assume @xmath68 , and consider only the following extreme cases .",
    "* @xmath69 : it is known binning with a gaussian codebook , generated using a single - letter mechanism ( _ i.e. , _ as an i.i.d .",
    "product distribution of the single - letter form ) as @xmath70 , where @xmath71 is a zero - mean gaussian random variable independent of @xmath6 such that @xmath72 ^ 2 $ ] , is optimal for wyner - ziv coding .",
    "this coding scheme can still be used for the first stage . in the second stage , by direct enumeration in the list of possible codewords in the particular bin specified in the first stage , the exact codeword can be recovered by decoder two , who does not have any side information . since @xmath69 , @xmath73 alone is not sufficient to guarantee a distortion @xmath46 , _",
    "@xmath74 ^ 2 $ ] .",
    "thus a successive refinement codebook , say using a gaussian random variable @xmath63 conditioned on @xmath73 such that @xmath75 ^ 2 $ ] , is needed .",
    "this leads to the achievable rates : @xmath76 * @xmath77 : if we choose @xmath70 such that @xmath72 ^ 2 $ ] and use the coding method in the previous case , then since @xmath78 , @xmath73 is sufficient to achieve distortion @xmath46 ,",
    "@xmath79 ^ 2 $ ] .",
    "the rate needed for the enumeration is @xmath80 , and it is rather wasteful since @xmath73 is more than we need . to solve this problem",
    ", we construct a coarser description using random variable @xmath81 , such that @xmath82 ^ 2 $ ] .",
    "the encoding process has three effective layers for the needed two stages : ( i ) the first layer uses wyner - ziv coding with codewords generated by @xmath83 ( ii ) the second layer uses successive refinement wyner - ziv coding with @xmath84 ( iii ) the third layer enumerates the specific @xmath63 codeword within the first layer bin .",
    "note that the first two layers form a sr - wz scheme with identical side information @xmath7 at the decoder .",
    "for decoding , decoder one decodes the first two layers with side information @xmath7 , while decoder two decodes the first and the third layer without side information . by the markov string",
    "@xmath85 , this scheme gives the following rates : @xmath86    it is seen in the above discussion the specific coding schemes depend on the distortion values , which is not desirable since this usually suggests difficulty in proving the converse .",
    "the two coding schemes can be unified into a single one by introducing an auxiliary random variable , as will be shown in the sequel , however , it appears the converse is indeed quite difficult to prove .    in the rest of this section ,",
    "inner and outer bounds for @xmath43 are provided .",
    "the coding schemes for the above gaussian example are naturally generalized to give the inner bounds .",
    "it is further shown that the inner bounds are in fact tight for certain special cases .",
    "define the region @xmath87 to be the set of all rate pairs @xmath37 for which there exist random variables @xmath88 in finite alphabets @xmath89 such that the following condition are satisfied .    1 .",
    "@xmath90 is a markov string .",
    "2 .   there exist deterministic maps @xmath91 such that @xmath92 3 .",
    "the non - negative rate pairs satisfy : @xmath93 4 .",
    "@xmath94 is a markov string .",
    "the alphabets @xmath95 , @xmath96 and @xmath97 satisfy @xmath98    the last two conditions can be removed without causing essential difference to the region @xmath87 ; with them removed , no specific structure is required on the joint distribution of @xmath99 . to see the last two conditions indeed",
    "do not cause loss of generality , apply the support lemma @xcite as follows . for an arbitrary joint distribution of @xmath99 satisfying the first three conditions",
    ", we first reduce the cardinality of @xmath95 . to preserve @xmath17 and the two distortions and two mutual information values ,",
    "@xmath100 letters are needed . with this reduced alphabet ,",
    "observe that both the distortion and rate expressions depend only on the marginal of @xmath101 and @xmath102 , respectively , hence requiring @xmath103 being a markov string does not cause any loss of generality .",
    "next to reduce the cardinality of @xmath96 , it is seen @xmath104 letters are needed to preserve the joint distribution of @xmath105 , one more is needed to preserve @xmath45 and another is needed to preserve @xmath106 . thus @xmath107 letters suffice .",
    "note that we do not need to preserve the value of @xmath46 and the value of the other mutual information term because of the aforementioned markov string .",
    "a similar argument holds for @xmath108 .",
    "the following theorem asserts that @xmath87 is an achievable region .",
    "[ theorem : achievable ] for any discrete memoryless",
    "stochastic source with side informations under the markov condition @xmath109 , @xmath110    this theorem is proved in appendix [ append : theoremachievable ] , and here we outline the coding scheme for this achievable region in an intuitive manner .",
    "the encoder first encodes using a @xmath111 codebook with a `` coarse '' binning , such that decoder one is able to decode it with side information @xmath112 . a wyner - ziv successive refinement coding ( with side information @xmath112 ) is then added conditioned on the codeword @xmath111 also for decoder one using @xmath113 .",
    "the encoder then enumerates the binning of @xmath111 up to a level such that @xmath111 is decodable by decoder two using the weaker side information @xmath114 . by doing so ,",
    "decoder two is able to reduce the number of possible codewords in the ( coarse ) bin to a smaller number , which essentially forms a  finer \" bin ; with the weaker side information @xmath114 , the @xmath111 codeword is then decoded correctly with high probability .",
    "another wyner - ziv successive refinement coding ( with side information @xmath114 ) is finally added conditioned on the codeword @xmath111 for decoder two using a random codebook of @xmath115 .    as seen in the above argument , in order to reduce the number of possible @xmath111 codewords from the first stage to the second stage , the key idea is to construct a nested binning structure as illustrated in fig .",
    "[ fig : bins ] .",
    "note that this is a fundamentally different from the code structure in sr - wz , where no nested binning is needed .",
    "each of the coarser bin contains the same number of finer bins ; each finer bin holds certain number of codewords .",
    "they are constructed in such a way that given the specific coarser bin index , the first stage decoder can decode in it with the strong side information ; at the second stage , additional bitstream is received by the decoder , which further specifies one of the finer bin in the coarser bin , such that the second stage decoder can decode in this finer bin using the weaker side information .",
    "if we assign each codeword to a finer bin independently , then its coarser bin index is also independent of that of the other codewords .    ]",
    "we note that the coding scheme does not explicitly require that side informations are degraded . indeed as long as",
    "the chosen random variable @xmath116 satisfies @xmath117 as well as the markov condition , the region is indeed achievable . more precisely , the following corollary is straightforward .    for any discrete memoryless",
    "stochastically source with side informations @xmath1 and @xmath0 ( without the markov structure ) , @xmath118 , where @xmath119 is @xmath87 with the additional condition that @xmath117 .",
    "we can specialize the region @xmath87 to give another inner bound .",
    "let @xmath120 be the set of all rate pairs @xmath37 for which there exist random variables @xmath121 in finite alphabets @xmath122 such that the following condition are satisfied .    1 .",
    "@xmath123 or @xmath124 is a markov string .",
    "2 .   there exist deterministic maps @xmath91 such that @xmath92 3 .",
    "the non - negative rate pairs satisfy : @xmath125 4 .",
    "the alphabets @xmath96 and @xmath97 satisfy @xmath126 for any discrete memoryless",
    "stochastically source with side informations under the markov condition @xmath109 , @xmath127    the region @xmath120 is particular interesting for the following reasons .",
    "firstly , it can be explicitly matched back to the coding scheme for the simple gaussian example .",
    "secondly , it will be shown that one of the outer bounds has the same rate and distortion expressions as @xmath120 , only with a relaxed markov string requirement .",
    "we now prove this corollary .",
    "_ proof of corollary [ cor : inner ] _    when @xmath128 , let @xmath129 .",
    "then the rate expressions in theorem [ theorem : achievable ] gives @xmath130 and therefore @xmath131 for this case .",
    "when @xmath132 , let @xmath133 .",
    "then the rate expressions in theorem [ theorem : achievable ] gives @xmath134 and therefore @xmath131 for this case .",
    "the cardinality bound here is larger than that in theorem [ theorem : achievable ] because of the requirement to preserve the markov conditions .",
    "define the following two regions , which will be shown to be two outer bounds .",
    "an obvious outer bound is given by the intersection of the wyner - ziv rate distortion function and the rate - distortion function for the problem considered by heegard and berger @xcite with degraded side information @xmath109 @xmath135    a tighter outer bound is now given as follows : define the region @xmath136 to be the set of all rate pairs @xmath37 for which there exist random variables @xmath121 in finite alphabets @xmath122 such that the following conditions are satisfied .    1 .",
    "2 .   there exist deterministic maps @xmath91 such that @xmath92 3 .",
    "@xmath138 , @xmath139 .",
    "the non - negative rate vectors satisfies : @xmath140    the main result of this subsection is the following theorem .    [",
    "theorem : outer ] for any discrete memoryless",
    "stochastically source with side informations under the markov condition @xmath141 , @xmath142    the first inclusion of @xmath143 is obvious , since @xmath136 takes the same form as @xmath144 and @xmath145 when the rates @xmath146 and @xmath147 are considered individually .",
    "thus we will focus on the latter inclusion , whose proof is given in appendix [ appendix : theorem3 ] .    note that the inner bound @xmath120 and @xmath136 have the same rate and distortion expressions and they differ only by a markov string requirement ( ignoring the non - essential cardinality bounds ) . because of the difference in the domain of optimizations , the two bounds may not produce the same rate - regions .",
    "this is quite similar to the case of distributed lossy source coding problem , for which the berger - tung inner bound requires a long markov string and the berger - tung outer bound requires only two short markov strings @xcite , but their rate and distortion expressions are the same .",
    "since decoder one has better quality side information , it is reasonable for it to require a higher quality reconstruction . alternatively , from the point of view of universal coding , when the encoder does not know the quality of the side information , it might assume the better quality one exists at the decoder and aim to reconstruct with a higher quality , comparing with the case when the poorer quality side information is available .",
    "in the extreme case , decoder one might require a lossless reconstruction .",
    "in this subsection , we consider the setting where either decoder one or decoder two requires lossless reconstruction .",
    "we have the following theorem .    [",
    "theorem : special ] if @xmath148 with @xmath149 , or @xmath150 with @xmath151 ( see [ eq : gammadef ] for @xmath56 ) , then @xmath152 .",
    "more precisely , for the former case , @xmath153 where @xmath154 is the set of random variables satisfying the markov string @xmath155 , and having a deterministic function @xmath156 satisfying @xmath157 .",
    "for the latter case , @xmath158 where @xmath159 is the set of random variables satisfying the markov string @xmath160 , and having a deterministic function @xmath161 satisfying @xmath162 .",
    "_ proof of theorem [ theorem : special ] : _ for @xmath148 , let @xmath163 and @xmath133 . the achievable rate vector implied by theorem [ theorem : achievable ]",
    "is given by @xmath164 it is seen that this rate region is tight by the converse of slepian - wolf coding for rate @xmath146 , and by ( [ eqn : hbcoro ] ) of heegard - berger coding for rate @xmath147 .    for @xmath150 ,",
    "let @xmath165 and @xmath166 . the achievable rate vector implied by theorem [ theorem : achievable ]",
    "is given by @xmath167 it is easily seen that this rate region is tight by the converse of wyner - ziv coding for rate @xmath146 , and the converse of slepian - wolf coding ( or more precisely , wyner - ziv rate distortion function @xmath168 with @xmath151 as given in @xcite ) for rate @xmath147 .",
    "zero distortion under a distortion measure @xmath169 can be interpreted as _ lossless _",
    ", however , it is a weaker requirement than that the block error probability is arbitrarily small",
    ". nevertheless , @xmath170 and @xmath171 in ( [ eqn : lossless1 ] ) and ( [ eqn : lossless2 ] ) still provide valid outer bounds for the more stringent lossless definition . on the other hand ,",
    "it is rather straightforward to specialize the coding scheme for these cases , and show that the same conclusion is true for lossless coding in the this case .",
    "thus we have the following corollary .",
    "the rate region , when the first stage , and respectively the second stage , requires lossless in terms of arbitrary small block error probability is given by ( [ eqn : lossless1 ] ) , respectively ( [ eqn : lossless2 ] ) ,    the key difference from the general case when both stages are lossy is the elimination of the need to generate one of codebooks using an auxiliary random variables , which simplifies the matter tremendously .",
    "for example when @xmath150 , since the first stage encoder guarantees that @xmath172 and @xmath14 are jointly typical , the second stage only needs to construct a codebook of @xmath14 by binning the approximately @xmath173 such @xmath14 vector directly .",
    "subsequently the second stage encoder does not search for a vector @xmath174 to be jointly typical with both @xmath172 and @xmath14 , but instead just sends the bin index of the observed source vector @xmath14 directly .",
    "alternatively , it can be understood as both the encoder and decoder at the second stage have access to a side information vector @xmath172 , and thus a conditional slepian - wolf coding with decoder side information @xmath0 suffices .",
    "another case of interest is when some functions of the source @xmath6 is required to be reconstructed with arbitrary small distortion in terms of hamming distortion ; see @xcite for the corresponding case for the multiple description problem .",
    "more precisely , let @xmath175 , @xmath32 be two deterministic functions and denote @xmath176 .",
    "consider the case that decoder @xmath177 seeks to reconstruct @xmath178 with arbitrarily small hamming distortion .",
    "the achievable region @xmath179 is tight when the functions satisfy certain degradedness condition as stated in the following theorem .",
    "[ theorem : deterministic ] let the distortion measure be hamming distortion @xmath180 for @xmath32 .    1 .",
    "if there exists a deterministic function @xmath181 such that @xmath182 , then @xmath183 . more precisely @xmath184 2 .",
    "if there exists a deterministic function @xmath185 such that @xmath186 , then @xmath183 .",
    "more precisely @xmath187    _ proof of theorem [ theorem : deterministic ] : _ to prove ( [ eq : ratesdetdist1 ] ) , first observe that by letting @xmath188 and @xmath189 , @xmath179 clearly reduces to the given expression .",
    "for the converse , we start from the outer bound @xmath190 , which implies that @xmath71 is a function of @xmath73 and @xmath1 , and @xmath191 is a function of @xmath63 and @xmath0 . for the first stage rate @xmath146",
    ", we have the following chain of equalities @xmath192 for the sum rate , we have @xmath193 where ( a ) is due to the markov string @xmath194 and @xmath191 is function of @xmath6 ; ( b ) is due to the markov string @xmath195 ; ( c ) is due to the markov string @xmath196",
    ".    proof of part 2 ) ( _ i.e. , _ ( [ eq : ratesdetdist2 ] ) relationship ) is straightforward and is omitted .    clearly in the converse proof ,",
    "the requirement that the functions @xmath197 and @xmath198 are degraded is not needed .",
    "indeed this outer bound holds for any general functions , however the degradedness is needed for establishing the achievability of the region .",
    "if the coding is not necessarily scalable , then it can be seen the sum rate is indeed achievable , and the result above can be used to establish a non - trivial special result in the context of the problem treated by heegard and berger @xcite .",
    "let the two function @xmath197 and @xmath198 be arbitrary , and let the distortion measure be hamming distortion @xmath199 for @xmath32 , then we have @xmath200",
    "in this section we introduce the notion of perfect scalability , which is defined as when both the stages operate at the wyner - ziv rates .",
    "we further examine the doubly symmetric binary source and provide a partial characterization and investigate its scalability . the quadratic gaussian source with jointly gaussian side informations",
    "is treated in section [ sec : gaussian ] in a more general setting .",
    "the notion of the ( strict ) successive refinability defined in @xcite for the sr - wz problem with forward degradation in the side - informations ( si ) can be applied to the reversely degraded case considered in this paper .",
    "this is done by introducing the notion of perfect scalability for the si - scalable problem defined below .",
    "a source @xmath6 is said to be _ perfectly scalable _ for distortion pair @xmath38 , with side informations under the markov string @xmath201 , if @xmath202    [ theorem : perfect ] a source @xmath6 with side informations under the markov string @xmath201 , for which @xmath203 such that @xmath204 for each @xmath205 , is perfectly scalable for distortion pair @xmath38 if and only if there exist random variables @xmath121 and deterministic maps @xmath206 such that the following conditions hold simultaneously :    1 .",
    "@xmath207 and @xmath208 , for @xmath25 .",
    "@xmath209 forms a markov string .",
    "3 .   the alphabet @xmath96 and @xmath97 satisfy @xmath138 , and @xmath139 .",
    "the markov string is the most crucial condition , and the substring @xmath210 is the same as one of the condition for successive refinability without side information @xcite@xcite .",
    "the support condition essentially requires the existence of a worst letter @xmath211 in the alphabet @xmath212 such that it has non - zero probability mass for each @xmath213 pair , @xmath205 .",
    "_ proof of theorem [ theorem : perfect ] _",
    "the sufficiency being trivial , we only prove the necessity . without loss of generality ,",
    "assume @xmath214 for all @xmath205 . by theorem [ theorem :",
    "outer ] , if @xmath215 is achievable for @xmath38 , then using the tighter outer bound @xmath136 of theorem [ theorem : outer ] , there exist random variable @xmath216 in finite alphabet , whose sizes is bounded as @xmath138 and @xmath139 , and functions @xmath217 such that @xmath218 is a markov string , @xmath219 for @xmath25 and @xmath220    it follows @xmath221 where ( a ) follows the converse of rate - distortion theorem for wyner - ziv coding . since the leftmost and the rightmost quantities are the same , all the inequalities must be equalities in ( [ eqn : nec ] ) , and it follows @xmath222 .",
    "similarly we have @xmath223 thus ( [ eqn : necr1 ] ) also holds with equality .    notice",
    "that if @xmath128 is a markov string , then we can use corollary [ cor : inner ] to claim the sufficiency and complete the proof .",
    "however , this markov condition is not true in general .",
    "this is where the support condition is needed . for convenience ,",
    "define the set @xmath224 by the markov string @xmath225 , the joint distribution of @xmath226 can be factorized as follows @xmath227 furthermore , @xmath222 implies the markov string @xmath228 , and thus the joint distribution of @xmath226 can also be factorized as follows @xmath229 where ( a ) follows by the markov substring @xmath230 .",
    "fix an arbitrary @xmath231 pair , by the assumption that @xmath232 for any @xmath205 , we have @xmath233 for any @xmath205 .",
    "thus for any @xmath234 ( see definition in ( [ eq : fdef ] ) ) such that @xmath235 is well defined , we have @xmath236 and it further implies @xmath237 for any @xmath234 .",
    "this indeed implies @xmath128 is a markov string , which completes the proof .      consider the following source : @xmath6 is a memoryless binary source @xmath238 and @xmath239 . the first stage side information @xmath7 can be taken as the output of a binary symmetric channel with input @xmath6 , and crossover probability @xmath240 .",
    "the second stage does not have side information .",
    "this source clearly satisfies the support condition in theorem [ theorem : perfect ] .",
    "it will be shown that for some distortion pairs , this source is perfectly scalable , while for others this is not possible .",
    "we next first provide partial results using @xmath241 and @xmath242 previously given .",
    "is the critical distortion in @xcite below which time sharing is not necessary .",
    "[ fig : regions ] ]    an explicit calculation of @xmath145 , together with the optimal forward test channel structure , was given in a recent work @xcite . with this explicit calculation , it can be shown that in the shaded region in fig .",
    "[ fig : regions ] , the outer bound @xmath243 is in fact achievable ( as well as in region ii , iii and iv ; however these three regions are degenerate cases , and will be ignored in what follows ) .",
    "recall the definition of the critical distortion @xmath244 in the wyner - ziv problem for the dsbs source in @xcite @xmath245 where @xmath246 , @xmath247 is the binary entropy function @xmath248 , and @xmath249 is the binary convolution for @xmath250 as @xmath251 .",
    "it was shown in @xcite that if @xmath252 , then @xmath253 . we will use the following result from @xcite .    for distortion pairs",
    "@xmath38 such that @xmath254 and @xmath255 ( _ i.e. , _ region i - d ) , @xmath256    . the crossover probability for the bsc between @xmath6 and @xmath73 is @xmath45 , while the crossover probability @xmath257 for the bsc between @xmath73 and @xmath63 is such that @xmath258.,width=453 ]    this result implies that for the shaded region i - d , the forward test channel to achieve this lower bound is in fact a cascade of two bsc channels depicted in fig .",
    "[ fig : special ] .",
    "this choice clearly satisfies the condition in corollary [ cor : inner ] with the rates given by the outer bound @xmath243 , which shows that this outer bound is indeed achievable .",
    "note the following inequality @xmath259 where the inequality is due to the monotonicity of @xmath260 in @xmath261 , we conclude that in this regime the source is not perfectly scalable .",
    "to see @xmath243 is also achievable in region i - c , recall the result in @xcite that the optimal forward test channel to achieve @xmath262 has the following structure : it is the time - sharing between zero - rate coding and a bsc with crossover probability @xmath244 if @xmath263 , or a single bsc with crossover probability @xmath264 otherwise .",
    "thus it is straightforward to verify that @xmath243 is achievable by time sharing the two forward test channels in fig .",
    "[ fig : specialc ] ; furthermore , an equivalent forward test channel can be found such that the markov condition @xmath265 is satisfied , which satisfies the conditions given in theorem [ theorem : perfect ] .",
    "thus in this regime , the source is in fact perfectly scalable .     and @xmath63 is @xmath46 in both the channels , while the crossover probability @xmath257 for the bsc between @xmath63 and @xmath73 in ( a ) is such that @xmath266 .",
    "note for ( b ) , @xmath73 can be taken as a constant.,width=453 ]     in region i - b of figure [ fig : regions ] .",
    "[ fig : dsbs],width=453 ]    unfortunately , we were not able to find the complete characterization for the regime i - a and i - b .",
    "using an approach similar to @xcite , an explicit outer bound can be derived from @xmath136 .",
    "it can then be shown numerically that for certain distortion pairs in this regime , @xmath136 is strictly tighter than @xmath243 .",
    "this calculation can be found in @xcite and is omitted here .",
    "an example is given in fig .",
    "[ fig : dsbs ] for the two outer bounds with a non - zero gap in between for a specific distortion pair in region i - b .",
    "by using the tool of rate loss introduced by zamir @xcite , which was further developed in @xcite , it can be shown that when both the source and reconstruction alphabets are reals , and the distortion measure is mse , the gap between the achievable region and the out bounds are bounded by a constant .",
    "thus the inner and outer bounds are nearly sufficient in the sense defined in @xcite . to show this result , we distinguish the two cases @xmath267 and @xmath268 .",
    "the source @xmath6 is assumed to have finite variance @xmath269 and finite ( differential ) entropy .",
    "the result of this section is summarized in fig .",
    "[ fig : rateloss ] .     and @xmath136 are given in dashed lines , since it is unknown whether they are indeed the same.[fig : rateloss],width=340 ]      construct two random variable @xmath270 and @xmath271 , where @xmath272 and @xmath273 are zero mean independent gaussian random variables , independent of everything else , with variance @xmath274 and @xmath275 such that @xmath276 and @xmath277 . by letting @xmath278 ,",
    "it is obvious that the following rates are achievable for distortion @xmath38 from theorem [ theorem : achievable ] @xmath279    let @xmath280 be optimal random variable to achieve the wyner - ziv rate at distortion @xmath45 given decoder side information @xmath1 . then it is clear that the difference between @xmath146 and the wyner - ziv rate can be bounded as,@xmath281 where @xmath282 is by applying chain rule to @xmath283 in two different ways ; @xmath284 is true because @xmath285 is the decoding function given @xmath286 , the distortion between @xmath6 and @xmath285 is bounded by @xmath45 , and @xmath287 is independent of @xmath288 .",
    "now we turn to bound the gap for the sum rate @xmath147 .",
    "let @xmath73 and @xmath63 be the two random variables to achieve the rate distortion function @xmath145 .",
    "first notice the following two identities due to the markov string @xmath289 and @xmath288 are independent of @xmath50 @xmath290 next we can bound the difference between the sum - rate @xmath147 ( as given in ( [ eq : nsr1r2 ] ) ) and the heegard - berger sum rate as follows .",
    "@xmath291 to bound the first bracket , notice that @xmath292 where ( a ) is due to the markov string @xmath293 , @xmath294 is the decoding function given @xmath295 , and the other inequalities follow similar arguments as in eqn .",
    "( [ eqn : rateloss ] ) . to bound the second bracket , we write the following @xmath296",
    "thus we have shown that for @xmath267 , the gap between the outer bound @xmath243 and the inner bound @xmath87 is bounded .",
    "more precisely , the gap for @xmath146 is bounded by 0.5 bit , while the gap for the sum rate is bounded by 1.0 bit .      construct random variable @xmath298 and @xmath299 , where @xmath272 and @xmath273 are zero mean independent gaussian random variables , independent of everything else , with variance @xmath274 and @xmath275 such that @xmath300 and @xmath301 . by letting @xmath302",
    ", it is easily seen that the following rates are achievable for distortion @xmath38 @xmath303    clearly , the argument for the first stage @xmath146 still holds with minor changes . to bound the sum - rate gap , notice the following identity @xmath304 next we seek to upper bound the following quantity @xmath305 where again @xmath216 are the r - d optimal random variables for @xmath145 .",
    "for the first bracket , we have @xmath306 where @xmath285 is the decoding function given @xmath307 . for the second bracket , following a similar approach as ( [ eqn : secondbracket ] )",
    ", we have @xmath308 thus we conclude that for both cases the gap between the inner bound and the outer bound is bounded",
    "[ fig : rateloss ] illustrates the inner bound and outer bounds , as well as the gap in between .",
    "the degraded side information assumption , either @xmath44 or @xmath2 , for the quadratic jointly gaussian case is especially interesting , since physically degradedness and stochastic degradedness @xcite do not cause essential difference in terms of the rate - distortion region for the problem being considered @xcite .",
    "moreover , jointly gaussian source - side information is always statistically degraded , these forwardly and reversely degraded cases together provide a complete solution to the jointly gaussian case with two decoders .    in this section we in fact consider a more general setting with an arbitrary number of decoders for jointly gaussian source and multiple side informations .",
    "though the source and side informations can have arbitrary correlation , in light of the discussion above , we will treat only physically degraded side informations .",
    "note that since a specific encoding order is specified , though the side informations are degraded as an unordered set , the quality of side informations may not be monotonic along the scalable coding order .",
    "clearly the solution for the two stage case can be reduced in a straightforward manner from the general solution . recall from theorem [ theorem :",
    "outer ] ( see ( [ eq : intotrbnd ] ) ) that @xmath243 is an outer bound derived from the intersection of the heegard - berger and wyner - ziv bounds .",
    "the generalization of the outer bound @xmath243 to @xmath309 decoders plays an important role , and therefore we take a detour in section [ subsec : gaussianhb ] to start with the characterization of @xmath310 for the jointly gaussian case .",
    "consider the following source @xmath311 , and side informations @xmath312 , where @xmath313 are mutually independent and independent of @xmath6 .",
    "the result by heegard and berger @xcite gives @xmath314 where @xmath315 is the set of all random variable with the markov string @xmath316 , such that deterministic functions @xmath317 , @xmath318 exist which satisfy the distortion constraints . in @xcite , the case @xmath319",
    "was calculated explicitly , however such an explicit calculation appears quite involved for general @xmath309 due to the discussion of various cases when some of the distortion constraints are not tight . in the sequel we approach the problem by showing",
    "a jointly gaussian forward test channel is optimal .",
    "note that if we choose to enforce only a subset of the distortion constraints , the rate for such a restriction gives a lower bound on @xmath310 . by taking all the non - empty subsets of the distortion constraints , labeled by elements of @xmath320 , a total of @xmath321 lower bounds are available and",
    "clearly the maximum of them is also a lower bound .",
    "more precisely , we are interested in @xmath322 , where @xmath323 and @xmath324 is defined in the sequel explicitly in terms of the distortion constraints only ; note that if @xmath325 , @xmath326 is still the distortion constraint for the decoder with side information @xmath327 .",
    "we next derive one of these lower bounds using all the constraints @xmath328 , i.e. @xmath329 ; a similar derivation applies to the case with any subset @xmath330 . using ( [ eq : ndechb ] ) we have , @xmath331- \\ldots -[h(x|y_2w_2^n)-h(x|y_1y_2w_2^n)]\\nonumber\\\\ & = & h(x|y_n)-h(x|y_1w_1^n)-i(x;y_{n-1}|y_nw_n)\\\\ & & \\qquad\\qquad - i(x;y_{n-2}|y_{n-1}w_{n-1}^n)-\\ldots -i(x;y_1|y_2w_2^n)\\\\ & \\stackrel{(b)}{=}&h(x|y_n)-h(x|y_1w_1^n)\\\\ & & \\qquad\\qquad-[h(y_{n-1}|y_nw_n)-h(y_{n-1}|xy_n)]-\\ldots -[h(y_1|y_2w_2^n)-h(y_1|y_2x)]\\\\ & = & h(x|y_n)+\\sum_{k=2}^n h(y_{k-1}|xy_k)-\\sum_{k=2}^nh(y_{k-1}|y_kw_k^n)-h(x|y_1,w_1^n),\\end{aligned}\\ ] ] where ( a ) is because of the markov string @xmath332 , and ( b ) is because of the markov string @xmath333 , both of which are consequences of @xmath334 .",
    "the first two terms depend only on the source and distribution @xmath335 , and we now seek to bound the latter two terms , for which we have @xmath336 where the second inequality is because gaussian distribution maximizes the entropy for a given second moment , and @xmath337 by the existence of the decoding function @xmath161 .",
    "next define @xmath338 and write the following @xmath339\\\\ & = & \\gamma_ky_k+(1-\\gamma_k)x+[\\sum_{i=1}^{k-1}n_i-\\gamma_k\\sum_{i=1}^k n_i]\\end{aligned}\\ ] ] notice that @xmath340=\\sum_{i=1}^{k-1}\\sigma_i^2-\\gamma_k\\sum_{i=1}^k\\sigma_i^2=0,\\end{aligned}\\ ] ] and @xmath341 and @xmath342 are jointly gaussian , which implies that they are independent .",
    "furthermore because @xmath342 is independent of @xmath6 , the markov string @xmath343 implies that it is also independent of @xmath344 .",
    "it follows @xmath345 by the aforementioned independence relation , the variance of term in the bracket is bounded above by @xmath346 define the following quantities @xmath347 summarizing the bounds in ( [ eqn : hx ] ) and ( [ eqn : hyk ] ) , we have @xmath348 where for convenience we define @xmath349 .    to show that @xmath350 is indeed achievable , construct the random variables @xmath351 as follows .",
    "assume that @xmath352 ^ 2 $ ] for each @xmath353 , because otherwise this distortion requirement can be ignored completely .",
    "* [ construction of @xmath351 ] *    1 .   for each @xmath353 , determine the variance @xmath355 of a gaussian random variable @xmath356 such that @xmath357 ^ 2 $ ] .",
    "2 .   rank the variance of @xmath355 in an increasing order , and let @xmath358 denote the rank of @xmath355 .",
    "3 .   calculate @xmath359 , and @xmath360 for @xmath361 .",
    "4 .   construct a set of independent zero - mean gaussian random variables @xmath362 to have variance @xmath363 .",
    "5 .   construct a set of random variables @xmath351 as @xmath364    next we show that this construction of @xmath351 achieves one of aforementioned lower bounds and thus is an optimal forward test channel .",
    "choose the set @xmath365 , and denote the rank ( in increasing order ) of its element @xmath366 as @xmath367 .",
    "clearly by the construction we have @xmath368\\\\ & & -\\ldots -[h(y_{r^{-1}(1)}|y_{r^{-1}(2)}w^*_{r^{-1}(2)})-h(y_{r^{-1}(1)}|xy_{r^{-1}(2)})]\\\\ & = & r_{hb}^*(a^*_d)\\end{aligned}\\ ] ] because of the construction of @xmath351 and the fact that they are jointly gaussian with @xmath369 .",
    "thus , we have proved the following theorem .",
    "the auxiliary random variable @xmath351 constructed above achieves the minimum in the heegard and berger rate distortion function for the jointly gaussian source and side informations .",
    "it is clear that we can determine the set @xmath370 before constructing @xmath371 using the aforementioned procedure , which can simplify the construction .",
    "however , the current construction has the advantage that each @xmath372 is almost individually determined by @xmath373 , and does not substantially depend on the other distortion constraints .",
    "this will prove to be useful for the general scalable coding problem .",
    "it is worth noting that it seemingly requires comparing @xmath321 values of @xmath374 to determine @xmath375 , however , from the forward calculation we see that in fact @xmath376 complexity suffices .",
    "this result can be interpreted using fig .",
    "[ fig : rate ] . on the horizontal axis",
    ", the @xmath309 marks stand for the @xmath309 random variable @xmath377 , and the on the vertical axis , the @xmath309 marks stand for the @xmath309 levels of side informations @xmath378 .",
    "the random variable pairs @xmath379 are then the points of interest on the plane , since if the @xmath366-th decoder has @xmath380 the desired distortion can be achieved ; the @xmath379 pairs are in one - to - one correspondence to the @xmath381 pairs .",
    "next we associate the unit square below and to the right of each integer point @xmath382 is associated with a rate of value @xmath383 where we define @xmath384 , and @xmath385 . for each @xmath353 , if we cover the rectangle below and to the right of @xmath381 , then the sum rate associated with the covered area is exactly @xmath386 .    ,",
    "width=377 ]    with fig .",
    "[ fig : rate ] , the coding scheme can be understood as follows .",
    "the coding proceeds from @xmath387 to @xmath1 , _",
    "i.e. , _ from high to low on the vertical axis ; the @xmath366-th step ( @xmath366-th decoder ) specifies an integer point @xmath381 , which corresponds to a @xmath379 pair , on the figure , and additional rate is required if the area below and to the right of this point induces new area to cover .",
    "this order is illustrated in fig .",
    "[ fig : rate ] along the arrows .",
    "note that @xmath388\\\\ & = & i(w_{\\omega^{-1}(i)};x|w_{\\omega^{-1}(i+1)})-i(w_{\\omega^{-1}(i)};y_k|w_{\\omega^{-1}(i+1)})]\\\\ & = & i(w_{\\omega^{-1}(i)};x|y_kw_{\\omega^{-1}(i+1)}),\\end{aligned}\\ ] ] and it is the rate for a vertical slice of hight @xmath366 between horizontal position @xmath177 and @xmath389 , which is in a quite similar form as ( [ eqn : ratead ] ) . in this example figure , the decoders with side information @xmath390 and @xmath391 do not require additional rates .",
    "more generally , if @xmath381 is inside the area already covered by the previous coding steps @xmath392 , then this stage does not require additional rates .",
    "in fact , the corners of the final covered area specifies the set @xmath370 .",
    "the following observations are essential for the general gaussian scalable coding problem : each unit square in fig .",
    "[ fig : rate ] is not merely associated with rate @xmath393 , it is in fact associated with a fraction of code @xmath394 with the following properties    1 .",
    "the rate of @xmath394 is ( asymptotically ) @xmath393 ; 2 .",
    "if the fractions of code associated with the area below and to the right of @xmath381 are available , then the decoder with side information @xmath341 can decode within distortion @xmath373 ; 3 .   the same set of code @xmath394 can be used to fulfill only subset of the constraints , the rate calculated by the covering area method is the quadratic gaussian heegard and berger rate distortion function .",
    "the first and second observations are straightforward by constructing the nested binning together with conditional codebooks as described in section [ sec : ach ] , _",
    "@xmath395 conditioning stage from @xmath396 to @xmath397 and each conditioned codebook has @xmath309 nested levels from coarse for @xmath1 to fine for @xmath387 .",
    "in fact , it is not necessary to use @xmath309 nested level for each codebook , but we do so for simplicity of understanding .",
    "the last property is due to the inherent markov string among @xmath398 and @xmath6 .",
    "now consider the scalable coding problem where side informations and distortions are given by a permutation @xmath399 of that in the last subsection , _",
    "@xmath400 and @xmath401 .",
    "we next show that the identically permuted set of random variable @xmath351 achieves the heegard - berger rate distortion function for any first @xmath366 stages , thus optimal . in light of pictorial interpretation in fig .",
    "[ fig : rate ] , this reduces to rearranging the coded stream of @xmath394 .",
    "[ fig : rate2 ] shows the effect of changing the scalable coding order .",
    "the denser shaded region gives the incremental rate @xmath402 for the stage with side information @xmath341.,width=377 ]    more precisely , for a certain side information @xmath400 , define the following sets : @xmath403 and the following function @xmath404 , \\end{aligned}\\ ] ] and let @xmath405 .",
    "let the set of integers @xmath406 be ordered increasingly , and the rank of its element @xmath407 be @xmath408 .",
    "denote the set of random variables @xmath409 as @xmath410 for an integer set @xmath411 .",
    "the following @xmath366-th stage rate is achievable for @xmath353 @xmath412 it is clearly this rate corresponds to exactly the dense shaded region in fig .",
    "[ fig : rate2 ] , which is the sum of rates of fraction of codes @xmath413 as described above .",
    "the property of this fraction code @xmath413 thus implies the following .",
    "the gaussian scalable coding achievable rate region for distortion vector @xmath414 is the rate vectors @xmath415 satisfies @xmath416 where the side informations are @xmath417 .",
    "furthermore , it is achievable by a jointly gaussian codebook with nested binning .    an immediate consequence of this result is the following corollary .",
    "a distortion vector @xmath414 is perfectly scalable along side informations @xmath417 for the jointly gaussian source if and only if @xmath418 for each @xmath353 .",
    "this corollary applies to one of the important special cases where @xmath419 and @xmath420 for each @xmath366 , _",
    "i.e. , _ when all the decoders have the same distortion requirement , and the scalable order is along a decreasing order of side information quality .",
    "this implies that at least for the gaussian case , an opportunistic coding strategy does exist when the distortion requirement is the same for all the users .",
    "we studied the problem of scalable source coding with reversely degraded side - information and gave two inner bounds as well as two outer bounds .",
    "these bounds are tight for special cases such as one lossless decoder and under certain deterministic distortion measures .",
    "furthermore we provided a complete solution to the gaussian source with quadratic distortion measure with any number of jointly gaussian side informations .",
    "the problem of perfect scalability is investigated and the gap between the inner and outer bounds are shown to be bounded .",
    "for the doubly symmetric binary source with hamming distortion , we provided partial results of the rate - distortion region .",
    "the result illustrates the difference between the lossless and the lossy source coding : though a universal approach exists with uncertain side informations at the decoder for the lossless case , such uncertainty generally causes loss of performance in the lossy case .",
    "we will follow the definition of typicality in @xcite , but use a slightly different notation to make the small positive quantity @xmath421 explicit ( see @xcite ) .",
    "a sequence @xmath422 is said to be @xmath421-strongly - typical with respect to a distribution @xmath423 on @xmath10 if    1 .   for all @xmath424 with @xmath425 @xmath426 2 .   for all @xmath424 with @xmath427 , @xmath428=0 ,    where @xmath428 is the number of occurrences of the symbol @xmath429 in the sequence @xmath14",
    "the set of sequences @xmath430 that is @xmath421-strongly - typical is called the @xmath421-strongly - typical set and denoted as @xmath431}^{\\delta}$ ] , where the dimension @xmath12 is dropped .",
    "the following properties are well - known and will be used in the proof :    1 .   given a @xmath432}^\\delta$ ] , for a @xmath433 whose component is drawn i.i.d according to @xmath434 and any @xmath435 , we have @xmath436}^{\\delta ' } ] \\leq 2^{-n(i(x;y)-\\lambda_1)}\\end{aligned}\\ ] ] where @xmath437 is a small positive quantity @xmath438 as @xmath439 and both @xmath440 .",
    "similarly , given @xmath441}^{\\delta'}$ ] , for any @xmath442 , let the component of @xmath443 be drawn i.i.d according to the conditional marginal @xmath444 , then @xmath445}^{\\delta '' } ] \\leq 2^{-n(i(x;z|y)-\\lambda_2)}\\end{aligned}\\ ] ] where @xmath446 is a small positive quantity @xmath447 as @xmath439 and both @xmath448 .",
    "_ markov lemma @xcite : _ if @xmath449 is a markov string , and @xmath450 and @xmath451 are such that their component is drawn independently according to @xmath452 . then for all @xmath453 @xmath454}^{|\\mathcal{y}|\\delta}\\left|\\right({\\mbox{\\boldmath$y$}},{\\mbox{\\boldmath$z$}})\\in t_{[yz]}^\\delta]\\rightarrow 1.\\end{aligned}\\ ] ] furthermore , @xmath455}^{\\delta}\\left|\\right({\\mbox{\\boldmath$y$}},{\\mbox{\\boldmath$z$}})\\in t_{[yz]}^\\delta]\\rightarrow 1.\\end{aligned}\\ ] ]",
    "_ codebook generation : _ let a probability distribution @xmath456 , and two reconstruction functions @xmath457 and @xmath458 be given .",
    "first construct @xmath459 coarser bins and @xmath460 finer bins , where @xmath461 and @xmath462 are to be specified later .",
    "generate @xmath463 length-@xmath12 codewords according to @xmath464 , denote this set of codewords as @xmath465 ; assign each of them into one of the finer bins independently . for each codeword @xmath466 , generate @xmath467 length-@xmath12 codewords according to @xmath468 , denote this set of codewords as @xmath469 ; independently assign each codeword to one of the @xmath470 bins .",
    "again for each @xmath111 codeword , independently generate @xmath471 length-@xmath12 codewords according to @xmath472 , denote this set of codewords as @xmath473 ; independently assign each codeword to one of the @xmath474 bins .",
    "reveal this codebook to the encoders and decoders .    _",
    "encoding : _ for a given @xmath14 , find in @xmath465 a codeword @xmath475 such that @xmath476}^{2\\delta}$ ] ; calculate the coarser bin index @xmath477 , and the finer bin index within the coarser bin @xmath478 . then in the @xmath479 codebook , find a codeword @xmath480 such that @xmath481}^{3\\delta}$ ] , and calculate its corresponding bin index @xmath366 . in @xmath482 codebook , find a codeword @xmath483 such that @xmath484}^{3\\delta}$ ] , and calculate its corresponding bin index @xmath485 .",
    "the first - stage encoder sends @xmath177 and @xmath366 , and the second - stage encoder sends @xmath407 and @xmath485 . in the above procedure ,",
    "if there is more than one joint - typical sequence , choose the least ; if there is none , choose a default codeword and declare an error .",
    "_ decoding : _ the first stage decoder finds @xmath486 in the coarser bin @xmath177 , such that @xmath487}^{3|\\mathcal{x}|\\delta}$ ] ; then in the @xmath488 codebook , find @xmath489 such that @xmath490}^{4|\\mathcal{x}|\\delta}$ ] . in the second stage , the decoder finds @xmath486 in the finer bin specified by @xmath382 such that @xmath491}^{3|\\mathcal{x}|\\delta}$ ] ; then in the @xmath492 codebook , find @xmath493 such that @xmath494}^{4|\\mathcal{x}|\\delta}$ ] . in the above procedure ,",
    "if there is none or there are more than one , an error is declared and the decoding stops .",
    "the first decoder reconstructs as @xmath495 and the second decoder as @xmath496 .",
    "_ probability of error : _ first define the encoding errors : @xmath497}^{\\delta}\\}\\cup \\{{\\mbox{\\boldmath$y_1$}}\\notin t_{[y_1]}^{\\delta}\\}\\cup \\{{\\mbox{\\boldmath$y_2$}}\\notin t_{[y_2]}^{\\delta}\\}\\nonumber\\\\ e_1&=&e_0^c\\cap\\{\\forall { \\mbox{\\boldmath$v$}}\\in\\mathcal{c}_v , ( { \\mbox{\\boldmath$x$}},{\\mbox{\\boldmath$v$}})\\notin t_{[xv]}^{2\\delta}\\}\\nonumber\\\\",
    "e_2&=&e_0^c\\cap e_1^c\\cap\\{\\forall { \\mbox{\\boldmath$w_1$}}\\in\\mathcal{c}_{w_1}({\\mbox{\\boldmath$v$}}^ * ) , ( { \\mbox{\\boldmath$w_1$}},{\\mbox{\\boldmath$v$}}^*,{\\mbox{\\boldmath$x$}})\\notin t_{[w_1vx]}^{3\\delta}\\}\\nonumber\\\\ e_3&=&e_0^c\\cap e_1^c\\cap\\{\\forall { \\mbox{\\boldmath$w_2$}}\\in\\mathcal{c}_{w_2}({\\mbox{\\boldmath$v$}}^ * ) , ( { \\mbox{\\boldmath$w_2$}},{\\mbox{\\boldmath$v$}}^*,{\\mbox{\\boldmath$x$}})\\notin t_{[w_2vx]}^{3\\delta}\\}\\nonumber.\\end{aligned}\\ ] ] next define the decoding errors : @xmath498}^{2\\delta}\\}\\nonumber\\\\ e_5&=&e_0^c\\cap e_1^c\\cap\\{({\\mbox{\\boldmath$v$}}^*,{\\mbox{\\boldmath$x$}},{\\mbox{\\boldmath$y_2$}})\\notin t_{[vxy_2]}^{2\\delta}\\}\\nonumber\\\\ e_6&=&e_0^c\\cap e_1^c\\cap\\{\\exists { \\mbox{\\boldmath$v$}}'\\neq{\\mbox{\\boldmath$v$}}^ * : i({\\mbox{\\boldmath$v$}}')=i({\\mbox{\\boldmath$v$}}^ * ) \\ \\text{and } \\ ( { \\mbox{\\boldmath$v$}}',{\\mbox{\\boldmath$y_1$}})\\in t_{[vy_1]}^{3|\\mathcal{x}|\\delta}\\}\\nonumber\\\\ e_7&=&e_0^c\\cap e_1^c\\cap\\{\\exists { \\mbox{\\boldmath$v$}}'\\neq{\\mbox{\\boldmath$v$}}^ * : i({\\mbox{\\boldmath$v$}}')=i({\\mbox{\\boldmath$v$}}^*)\\ \\text{and } \\ j({\\mbox{\\boldmath$v$}}')=j({\\mbox{\\boldmath$v$}}^ * ) \\ \\text{and } \\ ( { \\mbox{\\boldmath$v$}}',{\\mbox{\\boldmath$y_2$}})\\in t_{[vy_2]}^{3|\\mathcal{x}|\\delta}\\}\\nonumber\\\\ e_8&=&e_0^c\\cap e_1^c\\cap e_2^c\\cap e_4^c\\cap e_6^c\\cap\\{({\\mbox{\\boldmath$w^*_1$}},{\\mbox{\\boldmath$v^*$}},{\\mbox{\\boldmath$x$}},{\\mbox{\\boldmath$y_1$}})\\notin t_{[w_1vxy_1]}^{3\\delta } \\}\\nonumber\\\\ e_9&=&e_0^c\\cap e_1^c\\cap e_3^c\\cap e_5^c\\cap e_7^c\\cap\\{({\\mbox{\\boldmath$w^*_2$}},{\\mbox{\\boldmath$v^*$}},{\\mbox{\\boldmath$x$}},{\\mbox{\\boldmath$y_2$}})\\notin t_{[w_2vxy_2]}^{3\\delta } \\}\\nonumber\\\\ e_{10}&=&e_0^c\\cap e_1^c\\cap e_2^c\\cap e_4^c\\cap e_6^c\\cap \\{\\exists { \\mbox{\\boldmath$w'_1$}}\\neq{\\mbox{\\boldmath$w^*_1 $ } } : l({\\mbox{\\boldmath$w'_1$}})=l({\\mbox{\\boldmath$w^*_1 $ } } ) \\ \\text{and } \\ ( { \\mbox{\\boldmath$w'_1$}},{\\mbox{\\boldmath$v^*$ } } , { \\mbox{\\boldmath$y_1$}})\\in t_{[w_1vy_1]}^{4|\\mathcal{x}|\\delta}\\}\\nonumber\\\\ e_{11}&=&e_0^c\\cap e_1^c\\cap e_3^c\\cap e_5^c\\cap e_7^c\\cap \\{\\exists { \\mbox{\\boldmath$w'_2$}}\\neq{\\mbox{\\boldmath$w^*_2 $ } } : l({\\mbox{\\boldmath$w'_2$}})=l({\\mbox{\\boldmath$w^*_2 $ } } ) \\ \\text{and } \\ ( { \\mbox{\\boldmath$w'_2$}},{\\mbox{\\boldmath$v^*$ } } , { \\mbox{\\boldmath$y_2$}})\\in t_{[w_2vy_2]}^{4|\\mathcal{x}|\\delta}\\}\\nonumber\\end{aligned}\\ ] ]    apparently , for any @xmath499 , for @xmath500 , @xmath501 .",
    "we have also @xmath502}^{\\delta})p(\\{\\forall \\ { \\mbox{\\boldmath$v$}}\\in \\mathcal{c}_v,\\ ( { \\mbox{\\boldmath$x$}},{\\mbox{\\boldmath$v$}})\\notin t_{[xv]}^{2\\delta}\\}|{\\mbox{\\boldmath$x$}}\\in t_{[x]}^{\\delta})\\nonumber\\\\ & \\leq&\\sum_{{\\mbox{\\boldmath$x$}}\\in t_{[x]}^{\\delta}}p_{x}({\\mbox{\\boldmath$x$}})(1 - 2^{-n(i(x;v)+\\lambda)})^{nr_1}\\nonumber\\\\ & \\leq&\\exp(-2^{-n(i(x;v)+\\lambda - r_{v})}),\\end{aligned}\\ ] ] where property 1 ) of the typical sequences and @xmath503 are used .",
    "thus @xmath504 , provided that @xmath505 .",
    "@xmath506 and @xmath507 both tends to zero due to the markov lemma ; it requires the condition @xmath508}^{2\\delta}$ ] to hold , which is indeed so given @xmath509 does not happen .",
    "similarly , both @xmath510 and @xmath511 tends to zero for the same reason .",
    "notice that if @xmath512}^{2\\delta}$ ] , then @xmath513}^{3|\\mathcal{x}|\\delta}$ ] , thus @xmath514 can be correctly decoded if there is no other codewords in the same bin satisfying the typicality test .    conditioned on @xmath515",
    ", we have @xmath516}^{2\\delta}$ ] .",
    "thus @xmath517}^{2\\delta}}pr({\\mbox{\\boldmath$x$}},{\\mbox{\\boldmath$v$}})(1 - 2^{-n(i(x;w_1|v)+\\lambda)})^{nr_2}\\nonumber\\\\ & \\leq&\\exp(-2^{-n(i(x;w_1|v)+\\lambda_2-r_2)})\\end{aligned}\\ ] ] where property 2 ) of the typical sequences is used .",
    "thus @xmath518 tends to zero provided @xmath519 .",
    "similarly @xmath520 tends to zero provided @xmath521 .    conditioned on @xmath515 , @xmath522}^{\\delta}$ ] , since codeword in @xmath465",
    "are generated independently according to @xmath523 @xmath524 where we have used property 2 ) of the typical sequences and the fact the bin to which @xmath525 is assigned is independent .",
    "thus @xmath526 provided that @xmath527 .",
    "similarly @xmath528 provided that @xmath529 .    conditioned on @xmath530 , @xmath513}^{2|\\mathcal{x}|\\delta}$ ] .",
    "thus @xmath531 where property 3 ) of the typical sequences is used .",
    "thus @xmath532 tends to zero provided @xmath533 .",
    "similarly , @xmath534 tends to zero provided @xmath535 .",
    "thus the rates only need to satisfy @xmath536 where @xmath537 and @xmath538 are both small positive quantities and vanish as @xmath539 and @xmath540 ; then @xmath541 .",
    "it only remains to show that the distortions constraints are satisfied as well .",
    "when no error occurs , then @xmath542}^{3|\\mathcal{v}|\\delta}$ ] and @xmath543}^{3|\\mathcal{v}|\\delta}$ ] .",
    "by standard argument using the definition of the typical sequences , it can be shown that @xmath544+\\epsilon'\\end{aligned}\\ ] ] where @xmath545 . thus the distortion can be made arbitrarily small by choosing sufficiently small @xmath421 and sufficiently large @xmath12 .",
    "similar arguments holds for the second stage decoder .",
    "this completes the proof .",
    "assume the existence of @xmath28 rd si - scalable code , there exist encoding and decoding functions @xmath30 and @xmath31 for @xmath546 .",
    "denote @xmath547 as @xmath548 .",
    "@xmath549 will be used to denote the vector @xmath550 and @xmath551 to denote @xmath552 ; the subscript @xmath366 will be dropped when it is clear from the context .",
    "the proof follows the same line as the converse proof in @xcite .",
    "the following chain of inequalities is standard ( see page 440 of @xcite ) .",
    "here we omit the small positive quantity @xmath553 for simplicity .",
    "@xmath554    next we bound the sum rate as follows @xmath555.\\nonumber\\end{aligned}\\ ] ] since @xmath556 is independent of @xmath557 , we have @xmath558 the markov condition @xmath559 gives @xmath560 thus we have @xmath561\\nonumber\\\\ & = & \\sum_{k=1}^n[i(x_k;t_1t_2{\\mbox{\\boldmath$y^-_1$}}{\\mbox{\\boldmath$y_2$}}^-{\\mbox{\\boldmath$y^+_2$}}|y_{2,k})+i(x_k;{\\mbox{\\boldmath$y^+_1$}}|t_1t_2{\\mbox{\\boldmath$y_2$}}{\\mbox{\\boldmath$y^-_1$}}y_{1,k})].\\end{aligned}\\ ] ] the degradedness gives @xmath562 , which implies @xmath563.\\end{aligned}\\ ] ] define @xmath564 and @xmath565 , by which we have @xmath566.\\end{aligned}\\ ] ] therefore the markov condition @xmath567 is true .",
    "next introduce the time sharing random variable @xmath568 , which is independent of the multisource , and uniformly distributed over @xmath569 .",
    "define @xmath570 .",
    "the existence of function @xmath571 follows by defining @xmath572 which leads the fulfillment of the distortion constraints .",
    "it only remains to show both the bound can be written in single letter form in @xmath216 , which is straightforward following the approach in ( page 435 of ) @xcite .",
    "this completes the proof for @xmath573 . @xmath574",
    "the discussion with emre telatar is gratefully acknowledged ."
  ],
  "abstract_text": [
    "<S> the problem of side - information scalable ( si - scalable ) source coding is considered in this work , where the encoder constructs a progressive description , such that the receiver with high quality side information will be able to truncate the bitstream and reconstruct in the rate distortion sense , while the receiver with low quality side information will have to receive further data in order to decode . </S>",
    "<S> we provide inner and outer bounds for general discrete memoryless sources . </S>",
    "<S> the achievable region is shown to be tight for the case that either of the decoders requires a lossless reconstruction , as well as the case with degraded deterministic distortion measures . </S>",
    "<S> furthermore we show that the gap between the achievable region and the outer bounds can be bounded by a constant when square error distortion measure is used . </S>",
    "<S> the notion of perfectly scalable coding is introduced as both the stages operate on the wyner - ziv bound , and necessary and sufficient conditions are given for sources satisfying a mild support condition . </S>",
    "<S> using si - scalable coding and successive refinement wyner - ziv coding as basic building blocks , a complete characterization is provided for the important quadratic gaussian source with multiple jointly gaussian side - informations , where the side information quality does not have to be monotonic along the scalable coding order . </S>",
    "<S> partial result is provided for the doubly symmetric binary source with hamming distortion when the worse side information is a constant , for which one of the outer bound is strictly tighter than the other one . </S>"
  ]
}