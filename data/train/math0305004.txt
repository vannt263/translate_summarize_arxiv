{
  "article_text": [
    "-5 mm    we will survey recent progress and describe open problems in the area of accurate floating point computation , in particular for matrix computations .",
    "a very short bibliography would include @xcite .",
    "we consider the evaluation of multivariate rational functions @xmath8 of floating point numbers , and matrix computations on rational matrices @xmath9 , where each entry @xmath10 is such a rational function .",
    "matrix computations will include computing determinants ( and other minors ) , linear equation solving , performing gaussian elimination ( ge ) with various kinds of pivoting , and computing the singular value decomposition ( svd ) , among others .",
    "our goals are _ accuracy _ ( computing each solution component with tiny relative error ) and _ efficiency _ ( the algorithm should run in time bounded by a polynomial function of the input size ) .",
    "we consider three models of arithmetic , defined in the abstract , and for each one we try to classify rational expressions and matrices as to whether they can be evaluated or factored accurately and efficiently ( we will say `` compute(d ) accurately and efficiently , '' or `` cae '' for short ) .    in the traditional `` @xmath11 '' model ( tm ) , we have @xmath12 , @xmath13 and @xmath14 , where @xmath15 is called _ machine precision_. it is the conventional model for floating point error analysis , and means that every floating point result is computed with a relative error @xmath16 bounded in magnitude by @xmath17",
    ". the values of @xmath16 may be arbitrary real ( or complex ) numbers satisfying @xmath18 , so that any algorithm proven to cae in the tm must work for arbitrary real ( or complex ) number inputs and arbitrary real ( or complex ) @xmath14 .",
    "the size of the input in the tm is the number of floating point words needed to describe it , independent of @xmath17 .",
    "the long exponent ( lem ) and short exponent ( sem ) models , which are implementable on a turing machine , make errors that may be described by the tm , but their inputs and @xmath16 s are much more constrained . also , we compute the size of the input in the lem and sem by counting the number of bits , so that higher precision and wider range take more bits .",
    "this will mean that problems we can provably cae in the tm are a strict subset of those we can cae in the lem , which in turn we conjecture are a strict subset of those we can cae in the sem .",
    "in all three models we will describe the classes of rational expressions and rational matrices in terms of the factorization properties of the expressions , or of the minors of the matrices .",
    "the reader may wonder why we insist on accurately computing tiny quantities with small relative error , since in many cases the inputs are themselves uncertain , so that one could suspect that the inherent uncertainty in the input could make even the signs of tiny outputs uncertain .",
    "it will turn out that in the tm , the class we can cae appears to be identical to the class where all the outputs are in fact accurately determined by the inputs , in the sense that small relative changes in the inputs cause small relative changes in the outputs .",
    "we make this conjecture more precise in section 3 below .",
    "there are many ways to formulate the search for efficient and accurate algorithms @xcite .",
    "our approach differs in several ways .",
    "in contrast to either conventional floating point error analysis @xcite or the model in @xcite , we ask that even the tiniest results have correct leading digits , and that zero be exact . in @xcite the model of arithmetic allows a tiny absolute error in each operation , whereas in tm we allow a tiny relative error .",
    "unlike @xcite our lem and sem are conventional turing machine models , with numbers represented as bit strings , and so we can take the cost of arithmetic on very large and very small numbers ( i.e. those with many exponent bits ) into precise account . for these reasons",
    "we believe our models are closer to computational practice than the model in @xcite .",
    "in contrast to @xcite , we ( mostly ) consider the input as given exactly , rather than as a sequence of ever better approximations . finally , many of our algorithms could easily be modified to explicitly compute guaranteed interval bounds on the output @xcite .",
    "-5 mm    we show here how to reduce the question of accurate and efficient matrix computations to accurate and efficient rational expression evaluation .",
    "the connection is elementary , except for the svd , which requires an algorithm from @xcite .",
    "[ prop_caenecessity ] being able to cae the absolute value of the determinant @xmath19 is _ necessary _ to be able to cae the following matrix computations on @xmath9 : lu factorization ( with or without pivoting ) , qr factorization , all the eigenvalues @xmath20 of @xmath9 , and all the singular values of @xmath9 .",
    "conversely , being able to cae _ all _ the minors of @xmath21 is _ sufficient _ to be able to cae the following matrix computations on @xmath9 : @xmath22 , lu factorization ( with or without pivoting ) , and the svd of @xmath9 .",
    "this holds in any model of arithmetic .",
    "* proof * first consider necessity .",
    "@xmath19 may be written as the product of diagonal entries of the matrices @xmath23 , @xmath24 and @xmath25 in these factorizations , or as the product of eigenvalues or singular values .",
    "if these entries or values can be cae , then so can their product in a straightforward way .",
    "now consider sufficiency .",
    "the statement about @xmath22 is just cramer s rule , which only needs @xmath26 different minors .",
    "the statement about lu factorization depends on the fact that each nontrivial entry of @xmath23 and @xmath24 is a quotient of minors .",
    "the svd is more difficult @xcite , and depends on the following two step algorithm : ( 1 ) compute a _",
    "rank revealing _ decomposition @xmath27 where @xmath28 and @xmath29 are `` well - conditioned '' ( far from singular in the sense that @xmath30 is not too large ) and ( 2 ) use a bisection - like algorithm to compute the svd from @xmath31 .",
    "we believe that computing @xmath32 is actually necessary , not just @xmath19 . the sufficiency proof can be extended to other matrix computations like the qr decomposition and pseudoinverse by considering minors of matrices like @xmath33}$ ] . furthermore ,",
    "if we can cae the minors of @xmath34 , and @xmath35 and @xmath36 are well - conditioned , then we can still cae a number of matrix factorizations , like the svd .",
    "the svd can be applied to get the eigendecomposition of symmetric matrices , but we know of no sufficient condition for the accurate and efficient calculation of eigenvalues of nonsymmetric matrices .",
    "-5 mm    we begin by giving examples of expressions and matrix computations that we can cae in the tm , and then discuss what we can not do .",
    "the results will depend on details of the axioms we adopt , but for now we consider the minimal set of operations described in the abstract .    as long as we only do _ admissible operations _ , namely multiplication , division , addition of like - signed quantities , and addition / subtraction of ( exact ! ) input data ( @xmath37 ) , then the worst case relative error only grows very slowly , roughly proportionally to the number of operations .",
    "it is when we subtract two like - signed approximate quantities and significant cancellation occurs , that the relative error can become large .",
    "so we may ask which problems we can cae just using only admissible operations , i.e. which rational expressions factor in such a way that only admissible operations are needed to evaluate them , and which matrices have all minors with the same property .    here",
    "are some examples , where we assume that the inputs are arbitrary real or complex numbers .",
    "( 1 ) the determinant of a cauchy matrix @xmath38 is cae using the classical expression @xmath39 , as is every minor . in fact , changing one line of the classical ge routine will compute each entry of the lu decomposition accurately in about the same time as the original inaccurate version . ( 2 ) we can cae all minors of sparse matrices , i.e. those with certain entries fixed at 0 and the rest independent indeterminates @xmath40 , if and only if the undirected bipartite graph presenting the sparsity structure of the matrix is _ acyclic _ ; a one - line change to ge again renders it accurate . an important special case",
    "are bidiagonal matrices , which arise in the conventional svd algorithm .",
    "( 3 ) the eigenvalue problem for the second centered difference approximation to a sturm - liouville ode or elliptic pde on a rectangular grid ( with arbitrary rectilinear boundaries ) can be written as the svd of an `` unassembled '' problem @xmath41 where @xmath42 and @xmath43 are diagonal ( depending on `` masses '' and `` stiffnesses '' ) and @xmath24 is _ totally unimodular _ , i.e. all its minors are @xmath44 or 0 .",
    "again , a simple change to ge renders it accurate .",
    "in contrast , one can show that it is impossible in the tm to add @xmath7 accurately in constant time ; the proof involves showing that for _ any _ algorithm the rounding errors @xmath16 and inputs @xmath45 can be chosen to have an arbitrarily large relative error .",
    "this depends on the @xmath16 s being permitted to be arbitrary real numbers in our model .",
    "vandermonde matrices @xmath46 are more subtle .",
    "since the product of a vandermonde matrix and the discrete fourier transform ( dft ) is cauchy , and we can compute the svd of a cauchy , we can compute the svd of a vandermonde .",
    "this fits in our tm model because the roots of unity in the dft need only be known approximately , and so may be computed in the tm model . in contrast , one can use the result in the last paragraph to show that the inverse of a vandermonde can not be computed accurately .",
    "similarly , polynomial vandermonde matrices with @xmath47 , @xmath48 a ( normalized ) orthogonal polynomial , also permit accurate svds , but probably not inverses .",
    "-5 mm    if we further restrict the domain of ( some ) inputs to be nonnegative , then much more is possible , @xmath7 as a trivial example .",
    "a more interesting example are weakly diagonally dominant m - matrices , which arise as discretizations of pdes ; they must be represented as offdiagonal entries and the row sums .",
    "more interesting is the class of _ totally positive ( tp ) matrices _ , all of whose minors are positive .",
    "numerous structure theorems show how to represent such matrices as products of much simpler tp matrices .",
    "accurate formulas for the ( nonnegative ) minors of these simpler matrices combined with the cauchy - binet theorem yield accurate formulas for the minors of the original tp matrix , but typically at an exponential cost .    an important class of tp matrices where we can do much better are the tp generalized vandermonde matrices @xmath49 , where the @xmath50 form an increasing nonnegative sequence of integers",
    ". @xmath51 is known to be the product of @xmath52 and a _ schur function _",
    "@xcite @xmath53 , where the sequence @xmath54 is called a _",
    "partition_. schur functions are polynomials with nonnegative integer coefficients , so since their arguments @xmath55 are nonnegative , they can certainly be computed accurately .",
    "however , straightforward evaluation would have an exponential cost @xmath56 , @xmath57 .",
    "but by exploiting combinatorial identities satisfied by schur functions along with techniques of divide - and - conquer and memoization , the cost of evaluating the determinant can be reduced to polynomial time @xmath58 .",
    "the cost of arbitrary minors and the svd remains exponential at this time .",
    "note that the @xmath20 are counted as part of the size of the input in this case .    here is our conjecture generalizing all the cases we have studied in the tm .",
    "we suppose that @xmath59 is a homogeneous polynomial , to be evaluated on a domain @xmath60 .",
    "we assume that @xmath61 , to avoid pathological domains .",
    "typical domains could be all tuples of the real or complex numbers , or the positive orthant .",
    "we say that @xmath6 satisfies condition @xmath62 ( for _ accurate _ ) if @xmath6 can be written as a product @xmath63 where each factor @xmath64 satisfies    * @xmath64 is of the form @xmath55 , @xmath65 or @xmath66 , or * @xmath67 is bounded away from 0 on @xmath60 .",
    "let @xmath6 and @xmath60 be as above . then condition @xmath62 is a necessary and sufficient condition for the existence of an algorithm in the tm model to compute @xmath6 accurately on @xmath60 .",
    "note that we make no claims that @xmath6 can be evaluated efficiently ; there are numerous examples where we only know exponential - time algorithms ( doing ge with complete pivoting on a totally positive generalized vandermonde matrix ) .",
    "-5 mm    so far we have considered the simplest version of the tm , where ( 1 ) we have only the input data , and no additional constants available , ( not even integers , let alone arbitrary rationals or reals ) , ( 2 ) the input data is given exactly ( as opposed to within a factor of @xmath68 ) , and ( 3 ) there is no way to `` round '' a real number to an integer , and so convert the problem to the lem or sem models .",
    "we note that in @xcite , ( 1 ) integers are available , ( 2 ) the input is rounded , and ( 3 ) there is no way to `` round '' to an integer .",
    "changes to these model assumptions will affect the classes of problems we can solve .",
    "for example , if we ( quite reasonably ) were to permit exact integers as input , then we could cae expressions like @xmath69 , and otherwise presumably not .",
    "if we went further and permitted exact rational numbers , then we could also cae @xmath70 .",
    "allowing algebraic numbers would make @xmath71 cae .",
    "if inputs were not given exactly , but rather first multiplied by a factor @xmath68 , then we could no longer accurately compute @xmath37 where @xmath72 and @xmath73 are inputs , eliminating cauchy matrices and most others .",
    "but the problems we could solve with exact inputs in the tm still have an attractive property with inexact inputs : small relative changes in the inputs cause only a small relative change in the outputs , independent of their magnitudes .",
    "the output relative errors may be larger than the input relative error by a factor called a _ relative condition number _ @xmath74 , which is at most a polynomial function of @xmath75 . here",
    "@xmath76 is the _ relative gap _ between inputs @xmath55 and",
    "@xmath77 , and the maximum is taken over all expressions @xmath78 where appearing in @xmath63 .",
    "so if all the input differ in several of their leading digits , all the leading digits of the outputs are determined accurately .",
    "we note that @xmath74 can be large , depending on @xmath6 and @xmath60 , but it can only be unbounded when a relative gap goes to zero .",
    "if a problem has this attractive property , we say that it possesses a relative perturbation theory . in practical situations ,",
    "where only a few leading digits of the inputs @xmath55 are known , this property justifies the use of algorithms that try to compute the output as accurately as we do .",
    "we state a conjecture very much like the last one about when a relative perturbation theory exists .",
    "let @xmath6 and @xmath60 be as in the last conjecture . then condition @xmath62 is a necessary and sufficient condition for @xmath6 to have a relative perturbation theory .",
    "-5 mm    now we consider standard turing machines , where input floating point numbers @xmath3 are stored as the pair of integers @xmath4 , so the size of @xmath72 is @xmath79 .",
    "we distinguish two cases , the long exponent model ( lem ) where @xmath6 and @xmath5 may each be arbitrary integers , and the short exponent model ( sem ) , where the length of @xmath5 is bounded depending on the length of @xmath6 . in the simplest case , when @xmath80 ( or lies in a fixed range )",
    "then the sem is equivalent to taking integer inputs , where the complexity of problems is well understood .",
    "this is more generally the case if @xmath81 grows no faster than a polynomial function of @xmath82 .",
    "in particular it is possible to cae the determinant of an integer ( or sem ) matrix each of whose entries is an independent floating point number @xcite .",
    "this is not possible as far as we know in the lem , which accounts for a large complexity gap between the two models .",
    "we start by illustrating some differences between the lem and sem , and then describe the class of problems that we can cae in the lem .",
    "first , consider the number of bits in an expression with lem inputs can be exponentially larger than the number of bits in the same expression when evaluated with sem inputs .",
    "for example , @xmath83 when @xmath72 and @xmath73 are integers , but @xmath84 when @xmath72 and @xmath73 are lem numbers : @xmath85 has up to @xmath86 different bit positions to store , each @xmath87 , not @xmath88 . in other words",
    ", lem arithmetic can encode symbolic algebra , because if @xmath89 and @xmath90 have no overlapping bits , then we can recover @xmath89 and @xmath90 from the product @xmath91 .",
    "second , the error of many conventional matrix algorithms is typically proportional to the condition number @xmath92 .",
    "this means that a conventional algorithm run with @xmath93 extra bits of precision will compute an accurate answer .",
    "it turns out that if @xmath9 has rational entries in the sem model , then @xmath94 is at most a polynomial function of the input size , so conventional algorithms run in high precision will cae the answer .",
    "however @xmath95 for lem matrices can be exponentially larger , so this approach does not work .",
    "the simplest example is @xmath96 . on the other hand",
    "@xmath97 is a lower bound on the complexity of any algorithm , because this is a lower bound on the number of exponent bits in the answer .",
    "one can show that @xmath97 grows at most polynomially large in the size of the input .",
    "finally , we consider the problem of computing an arbitrary bit in the simple expression @xmath98 .",
    "when the @xmath55 are in the sem , then @xmath99 can be computed exactly in polynomial time .",
    "however when the @xmath55 are in the lem , then one can prove that computing an arbitrary bit of @xmath99 is as hard as computing the permanent , a well - known combinatorially difficult problem .",
    "here is another apparently simple problem not known to even be in np : testing singularity of a floating point matrix . in the sem , we can cae the determinant .",
    "but in the lem , the obvious choice of a `` witness '' for singularity , a null vector , can have exponentially many bits in it , even if the matrix is just tridiagonal .",
    "we conjecture that deciding singularity of an lem matrix is np - hard .",
    "so how do we compute efficiently in the lem ?",
    "the idea is to use _ sparse arithmetic _ , or to represent only the nonzero bits in the number .",
    "( a long string of 1s can be represented as the difference of two powers of 2 and similarly compressed ) . in contrast ,",
    "in the sem one uses _ dense arithmetic _ , storing all fraction bits of a number .",
    "for example , in sparse arithmetic @xmath100 takes @xmath101 bits to store in sparse arithmetic , but @xmath5 bits in dense arithmetic .",
    "this idea is exploited in practical floating point computation , where extra precise numbers are stored as arrays of conventional floating point numbers , with possibly widely different exponents @xcite .",
    "now we describe the class of rational functions that we can cae in the lem .",
    "we say the rational function @xmath8 is in factored form if @xmath102 , where each @xmath103 is an integer , and @xmath104 is written as an explicit sum of nonzero monomials .",
    "we say @xmath105 is the number of bits needed to represent it in factored form .",
    "then by ( 1 ) computing each monomial in each @xmath106 exactly , ( 2 ) computing the leading bits of their sum @xmath106 using sparse arithmetic ( the cost is basically sorting the bits ) , and ( 3 ) computing the leading bits of the product of the @xmath107 by conventional rounded multiplication or division , one can evaluate @xmath8 accurately in time a polynomial in @xmath105 and @xmath108 .",
    "in other words , the class of rational expression that we can cae are those that we can express in factored form in polynomial space .    now we consider matrix computations .",
    "it follows from the last paragraph that if each minor @xmath8 of @xmath9 can be written in factored form of a size polynomial in the size of @xmath9 , then we can cae all the matrix computations that depend on minors .",
    "so the question is which matrix classes @xmath9 have all their minors ( or just the ones needed for a particular matrix factorization ) expressible in a factored form no more than polynomially larger than the size of @xmath9 .",
    "the obvious way to write @xmath8 , with the laplace expansion , is clearly exponentially larger than @xmath9 , so it is only specially structured @xmath9 that will work .",
    "all the matrices that we could cae in the tm are also possible in the lem .",
    "the most obvious classes of @xmath9 that we can cae in the lem that were impossible in the tm are gotten by replacing all the indeterminates in the tm examples by arbitrary rational expressions of polynomial size .",
    "for example , the entries of an m - matrix can be polynomial - sized rational expressions in other quantities .",
    "another class are green s matrices ( inverses of tridiagonals ) , which can be thought of as discretized integral operators , with entries written as @xmath109 .",
    "the obvious question is whether @xmath110 each of whose entries is an independent number in the lem falls in this class .",
    "we conjecture that it does not , as mentioned before .",
    "-5 mm    our goal has been to identify rational expressions ( or matrices ) that we can evaluate accurately ( or on which we can perform accurate matrix computations ) , in polynomial time . accurately means that we want to get a relative error less than 1 , and polynomial time means in a time bounded by a polynomial function of the input size .",
    "we have defined three reasonable models of arithmetic , the traditional model ( tm ) , the long exponent model ( lem ) and the short exponent model ( sem ) , and tried to identify the classes of problems that can or can not be computed accurately and efficiently for each model .",
    "the tm can be used as a model to do proofs that also hold in the implementable lem and sem , but since it ignores the structure of floating point numbers as stored in the computer , it is strictly weaker than either the lem or sem . in other words , there are problems ( like adding @xmath7 ) that are provably impossible in the tm but straightforward in the other two models .",
    "we also believe that the lem is strictly weaker than the sem , in the sense that there appear to be computations ( like computing the determinant of a general , or even tridiagonal , matrix ) that are possible in polynomial time in the sem but not in the lem . in the sem ,",
    "essentially all problems that can be written down in polynomial space can be solved in polynomial time . for the lem ,",
    "only expressions that can be written in _ factored form _ in polynomial space can be computed efficiently in polynomial time .",
    "a number of open problems and conjectures were mentioned in the paper .",
    "we mention just one additional one here : what can be said about the nonsymmetric eigenvalue problem ?",
    "in other words , what matrix properties , perhaps related to minors , guarantee that all eigenvalues of a nonsymmetric matrix can be computed accurately ?",
    "* acknowledgements * the author acknowledges benjamin diament , zlatko drma , stan eisenstat , ming gu , william kahan , ivan slapniar , kresimir veselic , and especially plamen koev for their collaboration over many years in developing this material .",
    "j.  demmel and p.  koev .",
    "necessary and sufficient conditions for accurate and efficient singular value decompositions of structured matrices . in v.  olshevsky , editor , _ special issue on structured matrices in mathematics , computer science and engineering _ , volume 281 of _ contemporary mathematics _ , 117145 , ams , 2001 .",
    "d.  priest .",
    "algorithms for arbitrary precision floating point arithmetic . in p.",
    "kornerup and d.  matula , editors , _ proceedings of the 10th symposium on computer arithmetic _ , 132145 , grenoble , france , june 26 - 28 , 1991 .",
    "ieee computer society press ."
  ],
  "abstract_text": [
    "<S> our goal is to find accurate and efficient algorithms , when they exist , for evaluating rational expressions containing floating point numbers , and for computing matrix factorizations ( like lu and the svd ) of matrices with rational expressions as entries . </S>",
    "<S> more precisely , _ accuracy _ means the relative error in the output must be less than one ( no matter how tiny the output is ) , and _ efficiency _ </S>",
    "<S> means that the algorithm runs in polynomial time . </S>",
    "<S> our goal is challenging because our accuracy demand is much stricter than usual .    </S>",
    "<S> the classes of floating point expressions or matrices that we can accurately and efficiently evaluate or factor depend strongly on our model of arithmetic :    1 .   in the `` traditional model '' ( tm ) , </S>",
    "<S> the floating point result of an operation like @xmath0 is @xmath1 , where @xmath2 must be tiny . </S>",
    "<S> 2 .   in the `` long exponent model '' ( lem ) </S>",
    "<S> each floating point number @xmath3 is represented by the pair of integers @xmath4 , and there is no bound on the sizes of the exponents @xmath5 in the input data . </S>",
    "<S> the lem supports strictly larger classes of expressions or matrices than the tm . </S>",
    "<S> 3 .   in the `` short exponent model '' ( sem ) each floating point number </S>",
    "<S> @xmath3 is also represented by @xmath4 , but the input exponent sizes are bounded in terms of the sizes of the input fractions @xmath6 . </S>",
    "<S> we believe the sem supports strictly more expressions or matrices than the lem .    </S>",
    "<S> these classes will be described by factorizability properties of the rational expressions , or of the minors of the rational matrices . for each such class , we identify new algorithms that attain our goals of accuracy and efficiency . </S>",
    "<S> these algorithms are often exponentially faster than prior algorithms , which would simply use a conventional algorithm with sufficiently high precision .    for example , we can factorize cauchy matrices , vandermonde matrices , totally positive generalized vandermonde matrices , and suitably discretized differential and integral operators in all three models much more accurately and efficiently than before . </S>",
    "<S> but we provably can not add @xmath7 accurately in the tm , even though it is easy in the other models .    </S>",
    "<S> * 2000 mathematics subject classification : * 65f , 65g50 , 65y20 , 68q25 .    * keywords and phrases : * roundoff , numerical linear algebra , complexity . </S>"
  ]
}