{
  "article_text": [
    "in machine learning and data mining , feature selection ( fs ) is the process of selecting a subset of relevant features and removing irrelevant and redundant features from data towards model construction .",
    "it is a very important technique in the era of big data today , and has found applications in a wide range of domains , particularly for scenarios with high - dimensional data .",
    "feature selection has been extensively studied in which various algorithms have been proposed  @xcite .    despite the extensive research efforts in literature , most existing feature selection methods",
    "are restricted to batch learning settings  @xcite , which have many critical drawbacks for big data applications .",
    "one drawback with batch learning is that they often require the entire training data set to be loaded in memory .",
    "this is obviously non - scalable when solving real - world applications with large - scale datasets that exceed memory capacity .",
    "another drawback is that batch learning methods usually assume all training data and their full set of features must be made available prior to the learning task .",
    "this assumption does not always hold in many real - world applications where data arrives sequentially ( e.g. , internet data ) and novel features may appear incrementally ( e.g. , spam email filtering ) .",
    "these drawbacks make traditional batch feature selection techniques non - practical for emerging big data applications .    to overcome the drawbacks of batch feature selection",
    ", online feature selection has been explored recently  @xcite .",
    "one state - of - the - art scheme in @xcite attempts to resolve feature selection by exploring online learning techniques .",
    "although it is far more efficient and scalable than batch feature selection techniques , it still falls short in requiring linear time complexity with respect to feature dimensionality and sometimes failing to achieve satisfying learning accuracy when solving difficult tasks .    in this paper",
    ", we argue that existing solutions are still not feasible due to high time and memory cost in real world applications with large - scale and ultra - high dimensional data .",
    "we propose a simple but smart second order online feature selection algorithm that is extremely efficient , scalable to large scale and ultra - high dimensionality , and effective to address this open challenge .",
    "compared to existing fs methods , the complexity is significantly reduced to be linear to the average number of nonzero features per instance , rather than the full feature dimensionality . in particular , unlike the existing first - order online fs approaches , the proposed algorithm exploits the recent advances of second order online learning techniques  @xcite , trying to select the most confident weights while keeping the distribution close to the non - truncated distribution .",
    "it achieves highly competitive learning accuracy even compared with state - of - the - art batch fs methods .",
    "the rest of this paper is organized as follows : section 2 reviews related work ; section 3 presents the proposed method in detail ; section 4 discusses our empirical studies ; and finally section 5 draws our conclusions .",
    "more extensive results are also included in the appendix section due to space limitation .",
    "our work is related to feature selection and online learning .",
    "we review related work in each below .",
    "feature selection methods have been extensively studied in literature  @xcite , which can be roughly grouped into three categories : _ filter _ , _ wrapper _ , and _ embedded _ methods .",
    "_ filter _ methods rely on characteristics of data such as correlation , distance , and information gain without assuming specific classifiers  @xcite . unlike the _ filter _ methods that ignore the effect of selected features on the performance of the induction algorithm , _ wrapper _ methods employ a predetermined classifier to evaluate the quality of selected features  @xcite .",
    "it searches for a subset of features and then evaluates their classification performances repeatedly .",
    "they often yield better performance for the chosen classifier , but are computationally intensive . _",
    "embedded _ methods integrate feature selection into the model training process  @xcite , aiming to trade off between efficiency of _ filter _ methods and predictive accuracy of _ wrapper _ methods . however , their selected features might not be suitable for other classifiers .",
    "many studies have attempted to address online fs in diverse ways . some aim to handle streaming features arriving sequentially to the classifier @xcite .",
    "although they follow the stream learning setting and return a trained model at each time step given the observed features , they assume all the training instances must be given as a prior , making it unrealistic for many online applications .",
    "our work is more closely related to another online fs setting in  @xcite that follows online learning methodology by assuming training data arrives sequentially . despite its considerable advantages in efficiency and scalability over batch fs methods , it remains slow when being applied to large - scale fs tasks with ultra - high dimensionality .",
    "our work is also related to online learning in machine learning literature  @xcite , where a variety of online algorithms have been proposed , ranging from classical first - order algorithms ( such as passive - aggressive learning  @xcite ) to recent second - order algorithms  @xcite . in general , these algorithms require to access and explore the full set of features .",
    "they are not directly applicable to online fs tasks for selecting a fixed number of active features .",
    "another closely related online learning method is sparse online learning  @xcite , which aims to learn a sparse linear classifier from training data in high - dimensional space . despite the extensive efforts ,",
    "most of these works usually impose a soft constraint , such as @xmath0-regularization , onto the objective function for promoting sparsity , which do not directly solve an online fs task that requires a hard constraint on the number of active dimensions in the learned classifier . in this paper",
    ", we explore recent advances of online learning techniques in both second - order online learning and sparse online learning for advancing the state of the art of online feature selection tasks .",
    "in this section , we present a novel online feature selection method . we first describe the problem setting and then briefly introduce the existing first - order online feature selection methods , followed by presenting the proposed second - order online feature selection method in detail .      without loss of generality ,",
    "this paper first investigates the problem of online feature selection for binary classification tasks .",
    "consider @xmath1 be a sequence of training data instances received sequentially over the training process , where each @xmath2 is a vector of @xmath3 dimensions and @xmath4 .",
    "generally , an online learner will learn a classifier with the same dimensionality @xmath5 . in the setting of online feature selection , we need to select a relatively small number of elements in @xmath6 and set the others to be zero .",
    "in other words , we impose the following constraint @xmath7 where @xmath8 is the predefined constant , and consequently at most @xmath8 features of @xmath9 will be used for prediction . specifically , at each time @xmath10 , a learner receives an incoming example @xmath11 , and then predicts its class label @xmath12 based on its current model , i.e. , a linear weight vector @xmath13 , as @xmath14 after making the prediction , the true label @xmath15 will be revealed , and the learner then can measure the loss @xmath16 suffered with respect to @xmath17 , which is the difference between the prediction outcome and the true label . at the end of each iteration",
    ", the learner will update the weight vector @xmath13 according to some learning rules . throughout the paper ,",
    "we assume @xmath18 .",
    "one of most straightforward approaches to online feature selection is to apply the perceptron algorithm via truncation ( pet ) @xcite .",
    "specifically , at each step , the classifier first predicts the label @xmath19 with @xmath13 .",
    "if @xmath19 is correct , then @xmath20 ; otherwise , the classifier will update @xmath13 by perceptron rule to obtain @xmath21 , which will be further truncated by keeping the largest @xmath8 absolute values of @xmath22 and setting the rest to zero .",
    "the truncated classifier , denoted by @xmath23 or @xmath24 , will be used to predict the next observation .    as analyzed in  @xcite",
    ", the above simple approach does not work well in practice .",
    "in particular , it can not guarantee a small number of mistakes since it fails to ensure the numerical values of truncated elements are sufficiently small , thus leading to a nontrivial loss of accuracy .",
    "consequently , the authors in @xcite proposed a novel first - order online feature selection scheme ( fofs ) by exploring online gradient descent with a sparse projection scheme before truncation , which guarantees the resulting classifier @xmath13 to be restricted into an @xmath0-ball at each step .",
    "algorithm  [ alg : pe - sp ] shows the details of their first - order ofs algorithm .    [ 1 ] * input * : @xmath8,@xmath25 following the similar framework as pet but use constant learning rate @xmath25 @xmath26 @xmath27 , where @xmath28 is a regularization parameter @xmath29      a key limitation of the above online feature selection algorithms is that they only exploit the first - order information of the weight vector during the online feature selection process , which may lead to the loss of potentially informative features . to overcome the limitation , we propose a second - order online feature selection method by exploring the recent advances of second - order online learning techniques .",
    "the confidence - weighted ( cw ) method  @xcite assumes the weight vector of the linear classifier follows a gaussian distribution @xmath30 . confidence of weights are represented by diagonal elements in covariance matrix @xmath31 .",
    "the smaller @xmath31 , the more confidence we have in the mean value of weight @xmath32 . before observing any samples ,",
    "all the weights are of the same confidence or uncertainty . in the cw learning process ,",
    "given an observed training example @xmath17 , cw makes an update by trying to stay close to the previous distribution and ensure that the probability of making correct prediction on @xmath33 is larger than a threshold @xmath25 .",
    "the solution for the update can be cast into the following optimization : @xmath34   \\geq \\eta      \\label{equ : prob}\\ ] ]    the proposed second order online feature selection algorithm sofs takes another step with similar idea to cw . with the goal to reduce the damage to classification ability while selecting features , sofs tries to stay close to the updated distribution and ensure the @xmath35 norm is less than @xmath8 .",
    "the updated weights @xmath22 in equation   follows the distribution @xmath36 .",
    "sofs is cast into the following optimization : @xmath37    in sofs , only diagonal elements of the covariance matrix @xmath38 are considered . this is because maintaining a full covariance matrix requires @xmath39 memory space and @xmath39 computational complexity , which is impractical for handling large - scale ultra - high dimensional data . by writing the kl divergence explicitly with the diagonal covariance matrix assumption , the above optimization is equivalent to : @xmath40    suppose the selected feature indexes of the optimal solution @xmath41 to the optimization are @xmath42 .",
    "thus the rest feature weights with indexes @xmath43 are set to zero .",
    "the kl divergence is : @xmath44 where @xmath45 means the @xmath46-th diagonal element of the covariance matrix at iteration @xmath10 .",
    "as @xmath47 is the smallest among all the possible @xmath48 , it can be drawn that :    @xmath49 $ ] ;    @xmath50 $ ] ;    @xmath51 , j\\in [ b+1,d]$ ] .    note that the covariance matrix represents the confidence of weights .",
    "the above properties of the optimal solution indicate that the @xmath8 most confidence features should be selected by exploiting the second - order information of the classifier .",
    "specifically , in the online learning process , when the loss for a training instance @xmath17 is non - zero , we update the weight vector only for the most confident @xmath8 weight variables whose covariance values @xmath31 are among the @xmath8 smallest , and all the other weights are set to zero .",
    "by contrast , first order online feature selection algorithms select important features based on the magnitudes of the classifier weights .    in this paper",
    ", we adopt the adaptive regularization of weights ( arow ) algorithm  @xcite to solve the optimization problem in  .",
    "it has been shown to be more robust in handling label noises than the original cw algorithms .",
    "the objective function of arow is formulated as : @xmath52 where @xmath53 is a regularization parameter .",
    "the problem in can be solved with closed - form solutions as follows : @xmath54      a common drawback with many existing second - order learning methods is the extra high computational cost incurred for exploiting the second - order information . in this section , we show that it is possible to devise a second - order ofs algorithm that is not only more effective but also considerably more efficient and scalable than the existing first - order approaches .    specifically , one of major time - consuming procedures in the above second - order feature selection method is to select top @xmath8 elements from an array of length @xmath3 ( the diagonal vector of @xmath38 in the second - order ofs ) . instead of sorting all the weights at each step as in the previous study  @xcite , we propose a smart way to implement the proposed second - order online feature selection technique by employing a maxheap - based approach in exploiting the characteristics of sofs , which can significantly reduce computational complexity to be linear with respect to the average number of nonzero features @xmath55 of each example , rather than the original full dimensionality @xmath3 ( @xmath56 ) .",
    "this makes it extremely fast and scalable when handling large - scale sparse high - dimensional data sets .    before presenting the proposed algorithm ,",
    "we first introduce the following proposition for the monotonic decreasing property of @xmath57 , a property that is critical to the proposed algorithm .    given @xmath57 computed by , @xmath58 and @xmath59 $ ] , @xmath60 .",
    "it is not difficult to verify the above by noticing @xmath61 is always non - negative . using this important property",
    ", we can develop a fast algorithm for the second - order ofs method .",
    "specifically , we build a maxheap data structure to store the @xmath8 smallest diagonal values of covariance @xmath57 .",
    "the monotonic decreasing property of @xmath57 implies the heap limit should decrease monotonically .",
    "this leads to two major benefits in saving computational cost : ( i ) we do not need to check those unchanged elements to see if they are smaller than the heap limit ; and ( ii ) when updating elements in the heap , only its child nodes need to be updated .",
    "algorithm  [ alg : fast_arow_fs ] shows the details of the proposed fast algorithm for sofs .",
    "whenever a new feature arrives and its covariance changes , we proceed to update as follows :    if the corresponding covariance exists in the heap , adjust its position in the heap ;    check if it is smaller than the heap limit ; if so , replace the root node of the heap by the current item and set the value of the original root node to be zero ; otherwise ,    simply set the corresponding weight to zero .",
    "* input : @xmath62 , @xmath8 * * initialize : * @xmath63 .",
    "maxheap @xmath64 on @xmath65 with size @xmath8 * for * @xmath66 * if * @xmath67 calculate @xmath68 , @xmath69 by .",
    "* for * @xmath70 @xmath71 , @xmath72 * if * @xmath73 @xmath74 * elseif",
    "* @xmath75 replace @xmath76 by @xmath77 and set the weight value of the original root node to be zero @xmath74 * else * @xmath78 * output : weight vector @xmath79 and confidence @xmath80 *      the above proposed technique significantly improves the efficiency of existing online feature selection techniques .",
    "we now analyze the computational complexity of the above algorithms .",
    "let us denote by @xmath3 the dimensionality of the weight vector , and @xmath55 the average number of nonzero features of each sample . for pet",
    ", each updating step has to calculate the loss ( @xmath81 ) , update the model ( @xmath81 ) , calculate absolute value of the model ( @xmath81 ) , find the largest @xmath8 elements according to their absolute values and then set the rest @xmath82 to zero ( @xmath83 ) .",
    "the overall computational complexity of pet at every step is @xmath84 .",
    "fofs is similar to pet , with an extra normalization and sparse projection .",
    "the extra complexity is @xmath85 .",
    "computational cost of calculating absolute value is also increased to @xmath86 .",
    "thus , the complexity of fofs is @xmath87 , which is much more computationally expensive for high dimensional data .",
    "our sofs only needs to calculate the loss ( @xmath81 ) , update weight vector and the covariance ( @xmath88 ) , and adjust the heap ( @xmath89 ) .",
    "the computational complexity of sofs at each step is reduced to @xmath90 , making it far more efficient and scalable when handling ultra - high dimensional sparse data where @xmath91 and @xmath92 . even in the worst case where @xmath93 , our sofs with complexity @xmath94 is still more efficient than pet ( @xmath95 ) and fofs ( @xmath96 ) , where the improvement even only a constant can still save lots of training time for ultra - high dimensional data .    for space complexity , we only consider the space required by the classifiers .",
    "storages for data loading implementation are excluded here .",
    "both pet and fofs require to keep the weight vector @xmath6 and its absolute vector @xmath97 in memory , and thus have space complexity @xmath85 .",
    "sofs also has space complexity @xmath85 for keeping the weight vector and the diagonal elements of confidence matrix @xmath38 in memory .",
    "thus , sofs shares the same space complexity as the first - order online fs algorithms .",
    "in this section , we conduct extensive experiments to evaluate how the number of selected features affects the test accuracy and the training efficiency of different feature selection algorithms on both synthetic and real data on a large scale",
    ". we also evaluated the proposed algorithm on several public available medium - scale datasets .",
    "the results are shown in the supplementary material .",
    "for the family of online feature selection algorithms , we only run each algorithm by a single pass through the training data if without explicit indication .",
    "we compare the proposed algorithm with a set of state - of - the - art algorithms including both online and batch feature selection as follows :    pet : the baseline of ofs by perceptron with truncation @xcite ;    fofs : the state - of - the - art first - order ofs via sparse projection @xcite ;    mrmr : minimum redundancy maximum relevance feature selection , a state - of - the - art batch feature selection method  @xcite .",
    "liblinear : a famous library for large linear classification  @xcite",
    ". we adopt @xmath981-svm for the _ embedded _ feature selection in our experiments .",
    "fgm : a batch _ embedded _ feature generating method  @xcite .    for online algorithms ,",
    "we use hinge loss as the loss function .",
    "a five - fold cross validation is conducted to identify the optimal parameters .",
    "the experiments were conducted over 10 times with a random permutation of a dataset . for @xmath981-svm in liblinear , we tune parameter @xmath99 to select different number of features . for fgm , we follow the settings in  @xcite and set @xmath100 for simplicity . for mrmr , we first select a specific number of features and then use the perceptron to train a classifier .",
    "we exploited the advantage of online learning that processes data sequentially and implemented the program with two parallel threads , one for data loading and the other for learning .",
    "all experiments were conducted on a pc with intel i7 cpu @ 3.3 ghz , 16 gb ram .",
    "0    ccccccc dataset & # train & # test & dim & idim & ndim & # feat + @xmath101 & 100k & 10k & 20k & 200 & 400 & 60 m + @xmath102 & 1 m & 100k & 1b & 500 & 500 & 1b +    idim is the dimension of informative features per instance    ndim is the dimension of noise features per instance      the goal of this set of experiments is to generate synthetic data with ultra high dimensionality in order to examine different aspects of our algorithm in an effective way .",
    "ccccccc dataset & # train & # test & dim & idim & ndim & # feat + @xmath101 & 100k & 10k & 20k & 200 & 400 & 60 m + @xmath102 & 1 m & 100k & 1b & 500 & 500 & 1b +    idim is the dimension of informative features per instance    ndim is the dimension of noise features per instance    * synthetic data .",
    "* we follow the settings of fgm and generate two types of synthetic data , namely @xmath103 and @xmath104 to test efficacy , efficiency , and scalability of the algorithms for binary classification .",
    "each entry is sampled from the @xmath105 gaussian distribution @xmath106 . to simulate real data ,",
    "each sample is a sparse vector .",
    "the numbers of informative features for the two datasets are @xmath107 and @xmath108 respectively . for each sample",
    ", we randomly select @xmath109 dimensions for @xmath101 and @xmath108 dimensions for @xmath102 as noise . to generate labels , we sample a weight vector @xmath110 from the uniform distribution @xmath111 as the groundtruth weights for features .",
    "the label of each sample is determined by @xmath112 , where @xmath113 is a sample without noise .",
    "table  [ tbl : synthetic_data ] summarizes the synthetic datasets .",
    "figure  [ fig : synthetic ] shows the comparisons of accuracy and time cost",
    ".    * accuracy . *",
    "according to figure  [ fig : synthetic](a ) , the proposed algorithm outperforms other online feature selection algorithms , showing its efficacy in exploiting informative features .",
    "sofs is superior to fofs and pet significantly when the number of selected features exceeds the number of informative features .",
    "mrmr performs the worst , similar to the observations in  @xcite .",
    "batch learning algorithms are superior to online algorithms when number of features is very limited .",
    "however , sofs reaches the best and is comparable to batch feature selection algorithms when the number of selected features exceeds the number of informative feature ( 200 in @xmath101 ) . to conclude , the proposed algorithm is able to identify the groundtruth geometry of the data",
    ".    * time cost .",
    "* although batch fs algorithms are often more effective , they are significantly slower than online fs algorithms . among the algorithms ,",
    "our sofs can achieve comparable test accuracy as batch fs algorithms with the lowest time cost ( only a few seconds ) .",
    "by contrast , liblinear is 10 times slower and fgm is more than 1,000 times slower than sofs on the dataset . among online fs algorithms ,",
    "our method has the best accuracy but requires the least time cost .",
    "* scalability on ultra - high dimensional data . * due to the ultra - high dimensionality and billion - scale features of @xmath102 , we found that it would have to take days to run the existing fs algorithms .",
    "we thus only compare soft with two variants using two kinds of online learning algorithms on full sets of features ( by choosing @xmath114 for simplicity ) : online gradient descent ( ogd ) and arow @xcite .",
    "note that these two baselines were also implemented efficiently using the same framework of sofs with efficient data structure , but without doing feature selection .",
    "table  [ tbl : sofs - ulta - high - d - syn ] also shows the evaluation results on @xmath102 .",
    "as seen from the results , soft has improved the test accuracy as compared to the two baselines without explicit fs , which verifies that removing irrelevant or noisy features can improve predictive performance . not only with higher accuracy",
    ", sofs also uses significantly less features ( only @xmath115 as compared to @xmath116 by ogd and @xmath117 by arow ) . in terms of time cost ,",
    "soft took slightly more time cost due to the extra fs process , but only about 8 minutes to train a classifier on this dataset with billion - scale features .",
    "these encouraging results again validate that soft is efficient , scalable and effective in exploiting informative features on large - scale ultra - high dimensional data .      in this part",
    ", we evaluate the performance of the proposed sofs algorithm for three large - scale text classification tasks , as shown in table  [ tbl : large_dataset ] .",
    "the first dataset  news ",
    "( for news group classification ) is high dimensional , the second  rcv1  ( for text categorization ) is relatively large scale , and the last one  url ",
    "( for suspicious url detection ) is large scale and high dimensional . in this experiment , for simplicity , we compare the proposed sofs algorithm only with pet ( due to its low time complexity ) and fgm ( due to its high accuracy ) .    table  [ tbl : comp_big ] shows the experimental results of test accuracy and time cost of the three algorithms .",
    "we can not show the results of fgm on `` url '' as it was too slow to run ( took days to select 20% features ) .",
    "we observe that the performance of sofs is very close to that of fgm .",
    "both pet and fgm are far more computationally expensive , with fgm even more than an order of magnitude difference .",
    "the results further verify the significant advantage of sofs on large - scale high - dimensional datasets .",
    "this paper addressed an open challenge of large - scale feature selection with large - scale ultra - high dimensional sparse data , and presented a novel scheme of second - order online feature selection ( sofs ) .",
    "in contrast to the existing online fs algorithms whose computational complexity is linear with respect to the total feature dimensions , the proposed new sofs algorithm has a significantly lower computational complexity that is linearly dependent on the average number of nonzero features with each instance .",
    "we extensively evaluated empirical performance of the proposed algorithm by comparing it with state - of - the - art online and batch feature selection algorithms on both synthetic and large - scale real datasets .",
    "the promising results showed that our new method not only achieved highly competitive prediction accuracy , but also significantly improved computational efficiency , making our method practical for handling large - scale sparse data with ultra - high dimensionality .",
    "19 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    crammer , koby , dekel , ofer , keshet , joseph , shalev - shwartz , shai , and singer , yoram .",
    "online passive - aggressive algorithms . _ the journal of machine learning research _",
    ", 7:0 551585 , 2006 .",
    "crammer , koby , kulesza , alex , and dredze , mark .",
    "adaptive regularization of weight vectors .",
    "_ machine learning _ , pp .",
    "133 , 2009 .",
    "dredze , mark , crammer , koby , and pereira , fernando . confidence - weighted linear classification . in _ proceedings of the 25th international conference on machine learning _ , pp .",
    "acm , 2008 .",
    "duchi , john , hazan , elad , and singer , yoram .",
    "adaptive subgradient methods for online learning and stochastic optimization .",
    "_ journal of machine learning research _ , 12:0 21212159 , 2011 .",
    "fan , rong - en , chang , kai - wei , hsieh , cho - jui , wang , xiang - rui , and lin , chih - jen .",
    "liblinear : a library for large linear classification . _ the journal of machine learning research _ , 9:0 18711874 , 2008 .",
    "glocer , karen , eads , damian , and theiler , james .",
    "online feature selection for pixel classification . in _ proceedings of the 22nd international conference on machine learning _ , pp .",
    "acm , 2005 .",
    "hoi , steven c.  h. , wang , jialei , and zhao , peilin .",
    "libol : a library for online learning algorithms . _ the journal of machine learning research _ , 15:0 495499 , 2014 .",
    "url http://libol.stevenhoi.org .",
    "kohavi , ron and john , george  h. wrappers for feature subset selection . _ artificial intelligence _ , 970 ( 1):0 273324 , 1997 .",
    "langford , john , li , lihong , and zhang , tong .",
    "sparse online learning via truncated gradient . _ the journal of machine learning research _",
    ", 10:0 777801 , 2009 .",
    "liu , huan and yu , lei . toward integrating feature selection algorithms for classification and clustering .",
    "_ ieee trans . on knowledge and data engineering _ , 170 ( 4):0 491502 , 2005 .",
    "peng , hanchuan , long , fulmi , and ding , chris .",
    "feature selection based on mutual information criteria of max - dependency , max - relevance , and min - redundancy .",
    "_ pattern analysis and machine intelligence , ieee transactions on _ , 270 ( 8):0 12261238 , 2005 .",
    "perkins , simon and theiler , james .",
    "online feature selection using grafting . in _ icml _ , pp .   592599 , 2003 .",
    "saeys , yvan , inza , iaki , and larraaga , pedro . a review of feature selection techniques in bioinformatics .",
    "_ bioinformatics _ , 230 ( 19):0 25072517 , 2007 .    tan , mingkui , tsang , ivor  w. , and wang , li . towards ultrahigh dimensional feature selection for big data .",
    "_ journal of machine learning research _ , 150 ( 1):0 13711429 , 2014 .",
    "wang , jialei , zhao , peilin , hoi , steven  ch , and jin , rong .",
    "online feature selection and its applications .",
    "_ ieee transactions on knowledge and data engineering _ , 260 ( 3):0 698710 , 2014 .",
    "wu , xindong , yu , kui , wang , hao , and ding , wei . online streaming feature selection . in _ proceedings of the 27th international conference on machine learning ( icml-10 ) _ , pp .   11591166 , 2010 .",
    "xu , zenglin , jin , rong , ye , jieping , lyu , michael  r , and king , irwin .",
    "non - monotonic feature selection . in _ proceedings of the 26th annual international conference on machine learning _ , pp .",
    "acm , 2009 .",
    "yu , lei and liu , huan .",
    "feature selection for high - dimensional data : a fast correlation - based filter solution . in _",
    "icml _ , volume  3 , pp .   856863 , 2003 .",
    "zhao , zheng , wang , lei , liu , huan , and ye , jieping . on similarity",
    "preserving feature selection .",
    "_ knowledge and data engineering , ieee transactions on _ , 250 ( 3):0 619632 , 2013 .",
    "0    ccccccc dataset & # train & # test & dim & idim & ndim & # feat + @xmath101 & 100k & 10k & 20k & 200 & 400 & 60 m + @xmath102 & 1 m & 100k & 1b & 500 & 500 & 1b +    idim is the dimension of informative features per instance    ndim is the dimension of noise features per instance",
    "in this appendix , we give more extensive experimental results of performance evaluations on a variety of medium - scale real - world datasets .      in this section ,",
    "we evaluate the performance of online feature selection algorithms on a number of medium - scale public benchmark datasets , as shown in table  [ tbl : dataset ] .",
    "the datasets can be downloaded either from feature selection website of arizona state university or svmlin ( for sparse datasets ) .",
    "ccccc dataset & feat dim & train no .",
    "& test no . & feat no .",
    "+ relathe & 4,322 & 1,000 & 427 & 87,352 + pcmac & 7,510 & 1,000 & 946 & 55,470 + basehock & 4,862 & 1,500 & 493 & 101,974 + ccat & 47,236 & 13,149 & 10,000 & 994,133 + aut & 20,072 & 40,000 & 22,581 & 1,969,407 + real - sim & 20,958 & 50,000 & 22,309 & 2,560,340 +      figure  [ fig - online - medium - test - others ] shows the test accuracy of different algorithms . by examining the online algorithms",
    ", we found that perceptron (  pet \" ) with a simple truncation does not work well , while fofs is much better than pet in most cases .",
    "however , we observe that performance of fofs is not stable .",
    "the variance of fofs is much larger than those of the other two online algorithms on half of the medium - scale datasets .",
    "the proposed sofs method is able to learn a more compact classification model . with the same number of selected features , sofs is able to achieve the higher test accuracy results .",
    "ccccccc dataset & @xmath8 & 100 & 200 & 300 & 400 & 500 + & mrmr & * 74.19 * & 77.87 & 78.92 & 79.13 & 79.60 + & sofs & 71.38 & * 78.81 * & * 81.34 * & * 82.39 * & * 82.91 * + & mrmr & 87.95 & 90.34 & 89.93 & 91.49 & 91.10 + & sofs & * 89.76 * & * 92.65 * & * 93.28 * & * 93.75 * & * 94.02 * + & mrmr & * 93.78 * & * 95.15 * & 95.03 & 95.25 & 94.89 + & sofs & 90.34 & 94.52 & * 95.86 * & * 96.41 * & * 96.68 * + & mrmr & 82.75 & 85.71 & 86.42 & 86.94 & 87.40 + & sofs & * 82.76 * & * 86.35 * & * 87.94 * & * 89.00 * & * 89.75 * + & mrmr & * 92.41 * & * 93.87 * & * 94.09 * & 94.59 & 94.55 + & sofs & 74.72 & 81.71 & 85.89 & * 97.67 * & * 99.75 * + & mrmr & * 85.44 * & * 88.51 * & * 89.71 * & * 90.84 * & * 94.55 *",
    "+ & sofs & 83.29 & 86.77 & 89.38 & 90.71 & 91.59 +    besides , sofs is comparable to batch fs algorithms when accuracy saturates with number of features .",
    "we find that fgm is able to perform well with rather few features .",
    "liblinear in this case shows a very interesting phenomenon in that the test accuracy first increases rapidly with more selected features , but after a certain stage where the accuracy of other algorithms begins to saturate , the accuracy of liblinear tends to drop considerably .",
    "this implies that liblinear may be more sensitive to irrelevant features or noises .",
    "we show the comparison of sofs with mrmr separately in table  [ tbl : comp_medium_batch ] ( as mrmr was only able to output at most 500 selected features ) . from these results",
    ", we can observe that mrmr is better when the number of features is less .",
    "the accuracy of sofs increases quickly and surpasses mrmr with more selected features .",
    "this is consistent to the above results .",
    "note that mrmr is better than sofs on the dataset `` real - sim '' . in figure  [ fig - online - medium - test - others",
    "] , all online fs algorithms fail to train a good model with only 500 features on `` real - sim '' .",
    "their performance increases quickly and is expected to outperform mrmr with more features .",
    "the comparison again verifies the advantage of batch learning algorithms on very small number of selected features .",
    "however , when more features are selected , the proposed online feature selection becomes more accurate than mrmr .",
    "figure  [ fig - online - medium - time - others ] shows the time cost comparison of feature selection methods on medium - scale data .",
    "first of all , we observe that fofs took slightly higher time cost than pet despite achieving better accuracy .",
    "the extra time cost is more obvious when data dimensions get higher .",
    "further , we observe that the time cost first decreases and then increases with more selected features .",
    "this is due to the fact that when the number of selected features is too small , large number of mistakes are made and the model has to update frequently . with more features , the prediction accuracy can be improved and thus less update is performed , resulting in the decreased time costs .",
    "note the time costs on the later three datasets , which are of relatively high dimension .",
    "it shows the great advantage of our proposed algorithm on high dimensional data .",
    "this is consistent with the analysis in the paper that complexity of sofs is linearly dependent on the number of non - zero features , while pet and fofs are linearly dependent on the feature dimension .      as to batch learning algorithms",
    ", liblinear is quite similar to first - order online algorithms , but is much more than that of sofs .",
    "time cost of fgm is about an order of magnitude higher than liblinear .",
    "mrmr is the most inefficient among all the algorithms .",
    "we show time cost of mrmr and sofs to select 500 features in table  [ tbl : time_mrmr ] . even on the smaller dataset `` relathe ''",
    ", it takes over 1,700 seconds to select 500 features , while sofs requres only 0.03 seconds . to conclude , sofs is the most efficient one among all the algorithms in our experiments ."
  ],
  "abstract_text": [
    "<S> feature selection with large - scale high - dimensional data is important yet very challenging in machine learning and data mining . </S>",
    "<S> online feature selection is a promising new paradigm that is more efficient and scalable than batch feature section methods , but the existing online approaches usually fall short in their inferior efficacy as compared with batch approaches . in this paper </S>",
    "<S> , we present a novel second - order online feature selection scheme that is simple yet effective , very fast and extremely scalable to deal with large - scale ultra - high dimensional sparse data streams . </S>",
    "<S> the basic idea is to improve the existing first - order online feature selection methods by exploiting second - order information for choosing the subset of important features with high confidence weights . </S>",
    "<S> however , unlike many second - order learning methods that often suffer from extra high computational cost , we devise a novel smart algorithm for second - order online feature selection using a maxheap - based approach , which is not only more effective than the existing first - order approaches , but also significantly more efficient and scalable for large - scale feature selection with ultra - high dimensional sparse data , as validated from our extensive experiments . </S>",
    "<S> impressively , on a billion - scale synthetic dataset ( 1-billion dimensions , 1-billion nonzero features , and 1-million samples ) , our new algorithm took only 8 minutes on a single pc , which is orders of magnitudes faster than traditional batch approaches . </S>",
    "<S> http://arxiv.org/abs/1409.7794    0 existing methods are neither effective nor fast enough when handling large - scale ultra - high dimensional sparse data . to overcome this open challenge </S>",
    "<S> , we present a novel second - order online feature selection scheme that is simple yet effective , extremely fast and scalable to deal with large - scale ultra - high dimensional sparse data streams . </S>",
    "<S> the basic idea is to improve existing first - order online feature selection methods by exploiting second - order information for choosing the subset of important features with high confidence weights . </S>",
    "<S> however , unlike many second - order learning methods that often fall short in extra high computational cost , we propose an elegantly designed second - order online feature selection algorithm which is not only more effective than the first - order approaches , but also significantly more efficient and scalable for large - scale feature selection with ultra - high dimensional sparse data , as validated from our extensive experiments . on a billion - scale synthetic dataset ( </S>",
    "<S> 1-billion dimensions , 1-billion nonzero features , and 1-million samples ) , our new algorithm took only eight minutes on a normal pc , which is orders of magnitudes faster than traditional approaches . </S>",
    "<S> http://arxiv.org/abs/1409.7794    0 unlike conventional methods , the proposed algorithm effectively exploits the second - order information , trying to select the most confident weights while keeping the distribution close to the non - truncated distribution . </S>",
    "<S> we conducted extensive experiments by comparing both online and batch feature selection techniques . </S>",
    "<S> our promising results show that our new technique not only outperforms the existing online feature selection algorithms , but also achieves highly competitive accuracy as the state - of - the - art batch feature selection methods while consuming orders of magnitude lower computational cost . </S>",
    "<S> impressively , on a billion - scale synthetic dataset ( 1-billion dimensions , 1-billion nonzero features , and 1-million samples ) , our algorithm took only eight minutes on a normal single machine . </S>"
  ]
}