{
  "article_text": [
    "the artificial neural network ( ann ) technique has been widely used in data analysis of high energy physics experiments in the last decade .",
    "the use of the ann technique usually gives better results than the traditional simple - cut techniques . in this paper , another data classification technique , _ boosting _ , is introduced for data analysis in the miniboone experiment@xcite at fermi national accelerator laboratory .",
    "the miniboone experiment is designed to confirm or refute the evidence for @xmath0 oscillations at @xmath1 found by the lsnd experiment@xcite .",
    "it is a crucial experiment which will imply new physics beyond the standard model if the lsnd signal is confirmed .",
    "based on our studies , particle identification ( pid ) with the boosting algorithm is 20 to 80% better than that with our standard ann pid technique , the boosting performance relative to that of ann depends on the monte carlo samples and pid variables .",
    "although the boosting algorithm was tested in only one experiment , it s anticipated to have wide application in physics , especially in data analysis of particle physics experiments for signal and background events separation .",
    "the boosting algorithm is one of the most powerful learning techniques introduced during the past decade .",
    "the boosting algorithm is a procedure that combines many `` weak '' classifiers to achieve a final powerful classifier .",
    "boosting can be applied to any classification method . in this paper , it is applied to decision trees .",
    "two boosting algorithms , adaboost@xcite and @xmath2-boost@xcite , are considered .",
    "a brief description of boosting algorithms is given in the next section .",
    "our results are presented in section iii , while we summarize our conclusions in section iv .",
    "suppose one is trying to divide events into signal and background and suppose monte carlo samples of each are available .",
    "divide each monte carlo sample into two parts .",
    "the first part , the training sample , will be used to train the decision tree , and the second part , the test sample , to test the final classifier after training .    for each event , suppose there are a number of pid variables useful for distinguishing between signal and background .",
    "firstly , for each pid variable , order the events by the value of the variable . then pick variable one and for each event value see what happens if the training sample is split into two parts , left and right , depending on the value of that variable .",
    "pick the splitting value which gives the best separation into one side having mostly signal and the other mostly background . then repeat this for each variable in turn . select the variable and splitting value which gives the best separation .",
    "initially there was a sample of events at a `` node '' .",
    "now there are two samples called `` branches '' . for each branch , repeat the process , i.e.",
    ", again try each value of each variable for the events within that branch to find the best variable and splitting point for that branch .",
    "one keeps splitting until a given number of final branches , called leaves , are obtained , or until each leaf is pure signal or pure background , or has too few events to continue .",
    "this description is a little oversimplified .",
    "in fact at each stage one picks as the next branch to split , the branch which will give the best increase in the quality of the separation .",
    "a schematic of a decision tree is shown in fig.1 , in which 3 variables are used for signal / background separation : event hit multiplicity , energy , and reconstructed radial position .",
    "what criterion is used to define the quality of separation between signal and background in the split ?",
    "imagine the events are weighted with each event having weight @xmath3 .",
    "define the purity of the sample in a branch by @xmath4 where @xmath5 is the sum over signal events and @xmath6 is the sum over background events .",
    "note that @xmath7 is 0 if the sample is pure signal or pure background . for a given branch let @xmath8 where @xmath9 is the number of events on that branch .",
    "the criterion chosen is to minimize @xmath10    to determine the increase in quality when a node is split into two branches , one maximizes @xmath11    at the end , if a leaf has purity greater than 1/2 ( or whatever is set ) , then it is called a signal leaf and if the purity is less than 1/2 , it is a background leaf .",
    "events are classified signal if they land on a signal leaf and background if they land on a background leaf .",
    "the resulting tree is a _ decision tree_.    decision trees have been available for some time@xcite .",
    "they are known to be powerful but unstable , i.e. , a small change in the training sample can give a large change in the tree and the results .",
    "there are three major measures of node impurity used in practice : misclassification error , the gini index and the cross - entropy .",
    "if we define p as the proportion of the signal in a node , then the three measures are : 1 - max(p , 1-p ) for the misclassification error , 2p(1-p ) for the gini index and -plog(p ) - ( 1-p)log(1-p ) for the cross - entropy .",
    "the three measures are similar , but the gini index and the cross - entropy are differentiable , and hence more amenable to numerical optimization .",
    "in addition , the gini index and the cross - entropy are more sensitive to change in the node probabilities than the misclassification error . the gini index and the cross - entropy are similar .",
    "within the last few years a great improvement has been made@xcite .",
    "start with unweighted events and build a tree as above .",
    "if a training event is misclassified , i.e , a signal event lands on a background leaf or a background event lands on a signal leaf , then the weight of that event is increased ( boosted ) .",
    "a second tree is built using the new weights , no longer equal .",
    "again misclassified events have their weights boosted and the procedure is repeated .",
    "typically , one may build 1000 or 2000 trees this way .",
    "a score is now assigned to an event as follows .",
    "the event is followed through each tree in turn .",
    "if it lands on a signal leaf it is given a score of 1 and if it lands on a background leaf it is given a score of -1 .",
    "the renormalized sum of all the scores , possibly weighted , is the final score of the event .",
    "high scores mean the event is most likely signal and low scores that it is most likely background . by choosing a particular value of the score on which to cut , one can select a desired fraction of the signal or a desired ratio of signal to background .",
    "for those familiar with anns , the use of this score is the same as the use of the ann value for a given event . for the miniboone experiment ,",
    "boosting has been found to be superior to anns .",
    "statisticians and computer scientists have found that this method of classification is very efficient and robust .",
    "furthermore , the amount of tuning needed is rather modest compared with anns .",
    "it works well with many pid variables .",
    "if one makes a monotonic transformation of a variable , so that if @xmath12 then @xmath13 , the boosting method gives exactly the same results .",
    "it depends only on the ordering according to the variable , not on the value of the variable .    in articles on boosting within the statistics and computer science communities ,",
    "it is often recommended that short trees with eight leaves or so be used . for the miniboone monte carlo samples it was found that large trees with 45 leaves worked significantly better .      if there are @xmath14 total events in the sample , the weight of each event is initially taken as @xmath15 .",
    "suppose that there are @xmath16 trees and @xmath17 is the index of an individual tree .",
    "let    * @xmath18 the set of pid variables for the @xmath19th event .",
    "* @xmath20 if the @xmath19th event is a signal event and @xmath21 if the event is a background event . * @xmath22 the weight of the @xmath19th event . *",
    "@xmath23 if the set of variables for the @xmath19th event lands that event on a signal leaf and @xmath24 if the set of variables for that event lands it on a background leaf .",
    "* @xmath25 if @xmath26 and 0 if @xmath27",
    ".    there are at least two commonly used methods for boosting the weights of the misclassified events in the training sample .    the first boosting method is called adaboost@xcite .",
    "define for the @xmath17th tree : @xmath28 @xmath29 @xmath30 is the value used in the standard adaboost method . for the miniboone monte carlo samples ,",
    "@xmath31 has been found to give better results .",
    "change the weight of each event @xmath19 , @xmath32 : @xmath33 each classifier @xmath34 is required to be better than random guessing with respect to the weighted distribution upon which the classifier is trained .",
    "thus , @xmath35 is required to be less than 0.5 , since , otherwise , the weights would be updated in the wrong direction .",
    "next , renormalize the weights , @xmath36 the score for a given event is @xmath37 which is just the weighted sum of the scores over the individual trees , see fig.2 .",
    "the second boosting method is called @xmath2-boost@xcite , or sometimes `` shrinkage '' . after the @xmath17th tree ,",
    "change the weight of each event @xmath19 , @xmath32 : @xmath38 where @xmath2 is a constant of the order of 0.01 .",
    "renormalize the weights , @xmath36 the score for a given event is @xmath39 which is the renormalized , but unweighted , sum of the scores over individual trees .",
    "the adaboost and @xmath40boost algorithms used in this paper try to minimize the expectation value : @xmath41 , where y = 1 for signal , y = -1 for background , @xmath42 , where the classifier @xmath43 if an event lands on signal leaf , and @xmath44 if an event lands on background leaf .",
    "this minimization is closely related to minimizing the binomial log - likelihood@xcite .",
    "it can be shown that @xmath41 is minimized at @xmath45 let @xmath46 .",
    "it is then easy to show that @xmath47 the right - hand side is known as the @xmath48 statistic .",
    "@xmath49 is a quadrative approximation to the log - likelihood , so @xmath48 can be considered a gentler alternative .",
    "it turns out that fitting using @xmath48 is monotone and smooth ; the criteria will continually drive the estimates towards purer solutions .",
    "an ann tries to minimize the squared - error @xmath50 , where y = 1 for signal events , y = 0 for background events , and @xmath51 is the network prediction for training events .",
    "for the @xmath0 oscillation search in the miniboone experiment@xcite , the main backgrounds come from intrinsic @xmath52 contamination in the beam , mis - identified @xmath53 quasi - elastic scattering and mis - identified neutral current @xmath54 production .",
    "since intrinsic @xmath52 events are real @xmath52 events , the pid variables can not distinguish them from oscillation @xmath52 events .",
    "this report concentrates on separating the non-@xmath52 events from the @xmath52 events .",
    "good sensitivity for the @xmath52 appearance search requires low background contamination from all kinds of backgrounds . here",
    ", the ann and the two boosting algorithms are used to separate @xmath52 charged current quasi - elastic ( ccqe ) events from non-@xmath52 background events .",
    "500000 monte carlo @xmath53 events distributed among the many possible final states and 200000 intrinsic @xmath52 ccqe events were fed into the reconstruction package r - fitter@xcite . among these events , 88233 intrinsic @xmath52 ccqe and 162657 background events passed reconstruction and pre - selection cuts .",
    "the signature of each event is given by 52 variables for the r - fitter .",
    "all variables are used in the boosting algorithms for training and testing .",
    "it is a challenge to have agreement between data and monte carlo for all of the pid variables and for the boosting outputs .",
    "the miniboone collaboration is devoting considerable effort to achieve it .",
    "monte carlo samples using 18 different parameter sets have been generated and run through the same reconstruction programs .",
    "the results for both the pid variables and the boosting outputs are consistent .",
    "when the present monte carlo is compared with the real data samples , the shapes of the various pid variables and the boosting outputs match well .",
    "since the recontruction and pid algorithms are still undergoing continuous modifications , relative results rather than absolute percentages are presented in the following plots .    for the adaboost algorithm , the parameter @xmath31 ,",
    "the number of leaves @xmath55 and the number of tree iterations @xmath56 were used .",
    "the relative ratio(defined as the number of background events kept divided by the number kept for 50% intrinsic @xmath52 selection efficiency and @xmath56 ) as a function of @xmath52 selection efficiency for various tree iterations is shown in the top plot of fig.3 and the adaboost output distributions are shown in the bottom plot .",
    "20000 intrinsic @xmath52 ccqe signal and 30000 background events were used for training , 68233 @xmath52 and 132657 background events were used for testing .",
    "all results shown in the paper are for testing samples .    in order to quantify the performance of the boosting algorithm ,",
    "the adaboost results for a particular set of pid variables were compared with ann results .",
    "the results , compared as a function of the intrinsic @xmath52 ccqe selection efficiency , are shown in fig.4 .",
    "for the intrinsic @xmath52 signal efficiency ranging from 40% to 60% , the performances of adaboost were improved by a factor of approximately 1.5 and 1.8 over the ann if trained by the signal and all kinds of backgrounds with 21 ( red dots ) and 52 ( black boxes ) input variables respectively , shown in fig.4.a .",
    "if adaboost and ann were trained by the signal and neutral current @xmath54 background , the performances of adaboost were improved by a factor of approximately 1.3 and 1.6 over the ann for 22 ( red dots ) and 52 ( black boxes ) training variables respectively , shown in fig.4.b .",
    "the best results for the ann were found with 22 variables , while the best results for boosting were found with 52 variables .",
    "comparison of the best ann results and the best boosting results indicates that , when trained by the signal and neutral current @xmath54 background , the ann results kept approximately 1.5 times more background events than were kept by the boosting algorithms for about 50% @xmath52 ccqe efficiencies .    in fig.4.c",
    ", the ratio of the background kept for a 52 variable adaboost to that for a 21(red dots - results for adaboost trained by the signal and all kinds of backgrounds ) / 22(black boxes - results for adaboost trained by the signal and neutral current @xmath54 background ) variables is shown as a function of @xmath52 efficiency .",
    "it can be seen that the adaboost performance is improved by the use of more training variables .",
    "the above ann and adaboost performance comparison with different input variables indicates that adaboost can improve the pid performance significantly by using more input variables , even though many of them have weak discriminant power ; ann , however , seems unlikely to make full use of all input variables because it is more difficult to optimize all the weights between ann nodes , given more nodes in both the input and the hidden layers . for the miniboone monte carlo samples , the ann are optimum for approximately 20 pid variables .",
    "the authors have found a similar number to be true for several other applications .",
    "in general , the optimum number for ann may vary depending on the strength of the pid variables and the correlations between them .    further evidence of this effect comes from the s - fitter@xcite , a second reconstruction ",
    "pid program set for the miniboone .",
    "a systematic attempt was made to find the optimum sets of variables for ann and for boosting classifiers by using @xmath52 ccqe signal and @xmath54 background ( which includes 25 nuance reaction channels ) .",
    "it is found that , for s - fitter , the optimum ann result is achieved by a selected set of 22 variables , while for boosting , no obvious improvement is seen after a selected optimum set of 50 variables are used .",
    "comparison of the best ann results and the best boosting results indicates that , for a given fraction of @xmath52 ccqe events kept , the ann results kept about 1.2 times more @xmath54 background events than were kept by the boosting algorithms within target range of keeping close to 50% of the @xmath52 ccqe events .    as noted in the introduction ,",
    "two boosting algorithms are considered in the present paper .",
    "the comparison of adaboost and @xmath2-boost performance is shown in fig.5 , where parameters @xmath57 and @xmath58 were selected for adaboost and @xmath2-boost training , respectively .",
    "the comparison between small tree size ( 8 leaves ) and large tree size ( 45 leaves ) with a comparable overall number of decision leaves , indicates that large tree size with 45 leaves yields 10 @xmath59 20 % better performance for the miniboone monte carlo samples shown in fig.5.a . increasing",
    "the tree size past 45 leaves did not produce appreciable improvement    comparison of adaboost and @xmath2-boost performance for the background contamination versus the intrinsic @xmath52 ccqe selection efficiency as a function of the number of decision tree iterations is shown in fig.5.b .",
    "a smaller relative ratio implies a better performance for adaboost . the performance of adaboost is better than that of @xmath2-boost if the relative ratio is less than 1 .",
    "boosting performance in the high signal efficiency region is continuously improved for more tree iterations .",
    "adaboost has better performance than @xmath2-boost for less than about 200 tree iterations , but becomes slightly worse than @xmath2-boost for a large number of tree iterations , especially for @xmath52 signal efficiency below @xmath59 60% .",
    "for higher @xmath52 signal efficiency(@xmath60 70% ) , adaboost works slightly better than @xmath2-boost .",
    "pid variables obtained using the r - fitter and the s - fitter event reconstruction programs for the miniboone experiment were used to separate signal events from background events .",
    "the ann and the boosting algorithms were compared for pid . based on these studies with the miniboone monte carlo samples , the boosting algorithms , adaboost and @xmath2-boost , improved pid performance significantly compared with the artificial neural network technique .",
    "this improvement manifested itself when a large number of pid variables was used . for a small number of variables ,",
    "the ann classification was competitive , but as the number of variables was increased , the boosting results proved more efficient and superior to the ann technique .",
    "if more variables are needed , boosting will use them as necessary .",
    "it was also found that boosting with a large tree size of 45 leaves worked significantly better than boosting with a small tree size , 8 leaves , as recommended in some statistics literature .    the boosting technique proved to be quite robust . if a transformation of variables from @xmath61 to @xmath62 is made , then as long as the ordering is preserved , that is if @xmath63 , then @xmath64 , the boosting results are unchanged .",
    "anns must be tuned for temperature , learning rate and other variables , while for boosting , there is much less to vary and it is quite straightforward .",
    "there are certainly applications where anns prove better than boosting . however , for this application boosting appears superior and seems to be exceptionally robust and simple to use .",
    "it is anticipated that boosting techniques will have wide application in physics .",
    "we wish to express our gratitude to the miniboone collaboration for their excellent work on the monte carlo simulation and the software package for physics analysis .",
    "j. friedman , _ greedy function approximation : a gradient boosting machine _ , annals of statistics , 29(5 ) , 1189 - 1232(2001 ) ; j. friedman , t. hastie , r. tibshirani , _ additive logistic regression : a statistical view of boosting _ , annals of statistics , 28(2 ) , 337 - 407(2000 )        yoav freund and robert e. schapire , _ a short introduction to boosting _ , journal of japanese society for artificial intelligence , * 14(5 ) * , 771 - 780 , ( september , 1999 ) . ( appearing in japanese , translation by naoki abe . )"
  ],
  "abstract_text": [
    "<S> the efficacy of particle identification is compared using artificial neutral networks and boosted decision trees . </S>",
    "<S> the comparison is performed in the context of the miniboone , an experiment at fermilab searching for neutrino oscillations . based on studies of monte carlo samples of simulated data , particle identification with boosting algorithms </S>",
    "<S> has better performance than that with artificial neural networks for the miniboone experiment . </S>",
    "<S> although the tests in this paper were for one experiment , it is expected that boosting algorithms will find wide application in physics . </S>"
  ]
}