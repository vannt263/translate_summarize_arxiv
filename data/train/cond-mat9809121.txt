{
  "article_text": [
    "in the context of learning rules by perceptrons , generalization by a neural network is the capability of correctly classify new patterns after some examples being taught to the network ( see , e.g.,@xcite ) . for attractor neural networks , another type of generalization was suggested , the categorization , that emerges from an encoding stage where a hierarchical tree of patterns is stored@xcite . the ability of the network classifying the patterns on a lower level of the tree ( i.e. , the @xmath0 ) , into categories defined by their ancestors ( i.e. , the @xmath1 ) , arises from the hopfield model if the examples are correlated with their concepts@xcite .",
    "a minimal number @xmath2 of examples for each concept is necessary to start the categorization .",
    "an extensive number of concepts is then learned by memorizing finite sets of examples .",
    "this was shown for networks of binary neurons with fully - connected@xcite,@xcite , diluted@xcite or layered@xcite architectures , and for analog@xcite , ternary@xcite , and non - monotonic@xcite neurons , using hebbian synapses .",
    "similar behavior was found for pseudo - inverse synapses@xcite .",
    "categorization is achieved through the appearance of symmetric spurious states .",
    "this ability to categorize start just when the capacity of the network recovering the original examples is lost , because of the interference generated by their correlations .    as in most models for pattern recognition ,",
    "the adequate analysis of the memory capacity of this networks require the tools of the information theory . in the case of non - biased independent patterns",
    ", one can avoid it and measure the performance through the hamming distance @xmath3 between the neuron and the retrieved pattern , and the load capacity @xmath4 .",
    "one scenario where @xmath3 and @xmath4 are not enough to characterize the system is that of sparse coded patterns@xcite .",
    "another one is that of dependent patterns .",
    "this is case for categorization models , since the information conveyed by the examples is not extensive in them .",
    "our goal in this work is to establish a reliable measure for the capacity of retrieving examples and the categorization , based in the information theory . in the next section ,",
    "we define the model and its parameters . after obtaining the expressions for the information capacity in the section iii , in the section iv we study some special cases which present the transition from a retrieval to a categorization phase .",
    "finally we conclude with some remarks in the section v.",
    "consider a network of @xmath5 binary neurons , with states @xmath6 at time @xmath7 .",
    "the neurons states are updated in parallel according to the deterministic rule    @xmath8    where @xmath9 is the local field of neuron @xmath10 at time @xmath7 .",
    "the elements of the hebbian - like synaptic matrix between neurons @xmath10 and @xmath11 are given by    @xmath12    where @xmath13 are the @xmath0 of the @xmath14 @xmath15 .",
    "the concepts are independent identically distributed random variables ( @xmath16 ) , @xmath17 , with equal probability .    in the encoding stage ,",
    "the examples are built from the concepts , according to the stochastic process :    @xmath18    where @xmath19 gives the correlation between the ancestors ( the concepts ) and the descendants ( the examples ) of this tree of patterns .",
    "the second delta of this conditional distribution gives the component of the examples which is independent on the concepts .",
    "this process can equivalently be formulated as @xmath20 , where the @xmath21 iidrv @xmath22 are distributed according to    @xmath23    with @xmath24 .",
    "the macroscopic parameters which describe the state of the network are the @xmath25 and the @xmath26 @xmath27 , respectively :    @xmath28    in the thermodynamic limit , the qualities of the retrieval and of the categorization can be measured by taking the @xmath29 of the overlaps for a single concept , say @xmath30 , which give    @xmath31\\rangle,\\,\\ , m^{1}_{t}\\equiv \\langle\\xi^{1}\\mbox{sign}[h_{t-1}]\\rangle , \\label{2.m1r}\\end{aligned}\\ ] ]    where the brackets mean averages over the set of examples @xmath32 and the local field @xmath33 for a single neuron .    the @xmath34 @xmath35@xcite,@xcite can be defined as @xmath36 , as a function of the categorization overlap .",
    "the stationary states are given by macroscopic overlaps with examples of a given concept , say @xmath37 , and microscopic remaining overlaps @xmath38 , @xmath39 .",
    "the general solution is represented by a retrieval overlap with a single example , say @xmath40 , and the @xmath41 overlaps with the other examples , @xmath42 .",
    "in the retrieval phase one have @xmath43 , @xmath44 , while in the categorization phase the stable state is @xmath45 , which may leads to a large categorization overlap , @xmath46 . in the following",
    "we will consider a situation where the network have relaxed to the equilibrium states , so we can drop the time @xmath7 on the parameters .",
    "in this section we describe a way to measure the storage of information by the network in the retrieving and categorizing regimes .",
    "there are two types of information to be extracted from the patterns in these network : the retrieval information and the categorization information .",
    "the former is that which can be conveyed from the examples to the neurons , while the latter is that which can be conveyed from the concepts . in each case",
    "one must calculate the information entropy of the pattern distributions , @xmath47 = -\\sum_{\\{\\xi^{\\mu}_{i}\\ } } p(\\{\\xi^{\\mu}_{i}\\ } ) \\log[p(\\{\\xi^{\\mu}_{i}\\ } ) ] $ ] , and @xmath48 = -\\sum_{\\{\\eta^{\\mu\\rho}_{i}\\ } } p(\\{\\eta^{\\mu\\rho}_{i}\\ } ) \\log[p(\\{\\eta^{\\mu\\rho}_{i}\\ } ) ] $ ] , where @xmath49 and @xmath50 are the concepts and examples joint probability distributions , respectively .",
    "the categorization information can be easily measured by computing the categorization overlap of a single concept , @xmath51 , and its entropy . since the concepts @xmath52 are iidrv , their probability distribution is factorial , @xmath53 .",
    "thus , the entropy of the concepts is extensive , @xmath54 = \\sum_{\\mu , i}^{p , n } h[\\xi^{\\mu}_{i } ] = pn h[\\xi]$ ] , where the entropy of a single concept on a single neuron is @xmath55=\\log(2)$ ] .",
    "as we study binary patterns , we shall use base-2 logarithm in order to count information in bits , then we have @xmath55=1 $ ] .",
    "the equivocation in the categorization can be evaluated by the square of the overlap , in such a way that no information is transmitted by the concepts if @xmath56 and the information is maximal if @xmath57 , reminding that the information is symmetric in this overlap , because an inverted concept @xmath58 carries the same information than @xmath59 .",
    "therefore , the total categorization information is @xmath60 $ ] , and the categorization information ( per synapse ) is    @xmath61    the retrieval information can be similarly measured , by computing the retrieval overlap and the entropy of the examples , since this entropy can also be factorized as @xmath62 , such that the entropy is extensive in the concepts and in the neurons , @xmath48 = \\sum_{\\mu , i}^{p , n }   h[\\{\\eta^{\\mu\\rho}_{i}\\}_{\\rho}^{s } ]   = pn h[\\{\\eta^{\\rho}\\}_{\\rho}^{s}]$ ] .",
    "thus it is enough to calculate the entropy of a set of examples of a single concept , @xmath63 , on a single neuron , to get the entropy of the whole set @xmath64 .    on the other hand",
    ", @xmath65 is @xmath66 a set of iidrv , so @xmath67 is @xmath66 factorizable in example probabilities , and the entropy is @xmath66 extensive in the examples , @xmath68 \\neq \\sum_{\\rho}^{s } h[\\eta^{\\rho } ] $ ] .",
    "so the retrieval information is not the naive one , @xmath69 .",
    "let @xmath70 be a set of examples of a given concept on a given neuron . in calculating @xmath71 we proceed as it follows : we take the conditional probability of the examples given the concept , @xmath72 , from eq.([2.pem ] ) , and average it on the distribution of @xmath73 ,    @xmath74    where @xmath75 is the probability distribution in eq.([2.pbl ] ) .",
    "after expanding this product , we calculated the entropy of this distribution , obtaining    @xmath76= -\\sum^{s}_{k=0 } c^{s}_{k } a_{k } \\log ( a_{k});\\nonumber\\\\ & & a_{k}= [ b_{+}^{k}b_{-}^{s - k}+b_{-}^{k}b_{+}^{s - k}]/2 , \\label{3.her}\\end{aligned}\\ ] ]    where @xmath77 are the combinatorial numbers .    in evaluating the equivocation in the retrieval",
    ", we here have to multiply this entropy by the square of the retrieval overlap of a single example .",
    "since we have to subtract the information due to the categorization , and the overlaps between examples and their concepts are @xmath78 , we estimate the total retrieval information as @xmath79 $ ] . therefore the retrieval information ( per synapse ) is    @xmath80 . \\label{3.ira}\\ ] ]    although other measures for the informations could be used , they must be monotonous functions of those we consider in the eqs.([3.ica]),([3.ira ] )",
    ". these have , nevertheless , the advantage that both are equivalently scaled and they can be directly compared to each other .",
    "we present now the equilibrium states for the networks which are used to obtain the retrieval and categorization informations .",
    "this states are studied for two systems : an asymptotic network ( @xmath81 ) , for which analytical stationary equations were derived@xcite and finite - sized systems , for which simulations of the dynamics in eq.([2.sit ] ) are carried on . while the information measures obtained in the previous section are functions of asymptotic parameters , @xmath51 and @xmath82 , the results from simulation use the overlaps in eqs.([2.mmr ] ) .",
    "first we study the stationary states of the overlaps in eqs.([2.mmr ] ) , in the thermodynamic limit @xmath81 . using the hebbian synapses in eq.([2.jij ] ) in the dynamics in eq.([2.sit ] ) , taking the local field at the fixed point , and averaging over the distribution of a single example , one get :    @xmath83 , \\nonumber\\\\ m= & & \\sum_{k=0}^{s-1 } p_{s}(k ) \\int_{-\\infty}^{\\infty } dz [ b_{+}g_{+ } + b_{-}g_{- } ] , \\nonumber\\\\ m^{s}= & & \\sum_{k=0}^{s-1 } p_{s}(k )   { x_{s}\\over s-1 } \\int_{-\\infty}^{\\infty } dz [ b_{+ } g_{+ } + b_{- } g_{- } ] , \\label{4.msk}\\end{aligned}\\ ] ]    for the retrieval , categorization and quasi - symmetric overlap , respectively . here    @xmath84 , \\label{4.gpm}\\ ] ]    with @xmath85 . and the averages are over the remaining @xmath86 examples from the first concept , and the remaining @xmath87 concepts .",
    "the first is the binomial variable @xmath88 , distributed according to    @xmath89    the last is a gaussian noise , distributed according to    @xmath90    in the present case of a fully - connected network , there is a strong feedback in the dynamics , but an expression for the variance of the noise can be obtained using a replica symmetric approach@xcite,@xcite ,    @xmath91^{2 } + ( s-1)b^{4 } \\over [ 1-c(1-b^{2})]^{2 } [ 1-c(1-b^{2}+sb^{2})]^{2 } } , \\label{4.rs1}\\ ] ]    with    @xmath92 . \\label{4.c1a}\\ ] ]    we have to solve this eqs.([4.msk])-([4.c1a ] ) , then we introduce the overlaps in the expressions for the informations , eqs.([3.ica]),([3.ira ] ) .",
    "these analytical results for the information are then presented in comparison with the results from simulation .",
    "the simulations we have performed are for networks of @xmath93 and @xmath94 neurons , which are updated in parallel according to the dynamics in eq.([2.sit ] ) , up to @xmath95 time steps , or when the overlaps converge .",
    "thus we have @xmath96 stationary states in most cases , except when a state of non - information is obtained , for which the times of convergence are typically much larger .",
    "the capacity is analized as a function of the two parameters of loading of the network : the rate of loading of concepts , @xmath97 , and the number of examples per concept , @xmath2 .",
    "the sample averages are taken over an interval in @xmath98 or in @xmath4 .",
    "when simulating the information as a function of @xmath2 , we generate first the concepts and then store consecutively the examples of each concept .",
    "when simulating the information as a function of @xmath4 , we generate the @xmath2 examples of the concept generated at each step of the learning .    the network is trained then storing examples , while the retrieval and categorization overlaps are monotorized . for a fixed @xmath4 , it is expected that increasing @xmath2 the network pass from a regime where the retrieving information is large to another where the categorizing information increase up to saturation in a upper bound .",
    "this behavior is seen in fig.1 , where the overlaps , as well as the informations , are plotted as a function of @xmath98 , with a correlation @xmath99 , for a loading of concepts @xmath100 .",
    "when more and more examples are learned , the retrieval information increases until a maximum at @xmath101 , then it falls down . after a while when no information is transmitted ,",
    "the network reach , at @xmath102 , the categorization phase , where the categorization information jumps to a higher value .",
    "it continues to increase untill it saturates in @xmath103 , when the network reach @xmath104 after @xmath105 .",
    "the retrieval information capacity of the network is @xmath106 .",
    "the asymptotic theory for @xmath81 fits quite well the simulation for @xmath94 , except in the region of no information .",
    "this is due to the finite number of steps used in the dynamical simulation , @xmath95 , while the convergence to the fixed - point there is very slow .",
    "a case with a larger load of concepts , @xmath107 , is plotted in fig.2 .",
    "although now the network can only retrieve well the examples up to @xmath108 , it has @xmath109",
    ". then there is a large waiting period where the informations stay close to zero , up to @xmath110 , when the categorization information jumps to @xmath111 , which is much larger than in the case @xmath100 .    comparing this with a network with larger correlation , @xmath112 , plotted in fig.3",
    ", we observe that the network can store with a larger overlap only @xmath113 examples , with a maximal retrieval information @xmath114 , which is somewhat smaller than the naive @xmath115 .",
    "however the categorization information approaches its saturation value @xmath111 much faster , only @xmath116 examples must be learned .",
    "we have checked that for larger load of concepts ( @xmath117 ) the categorization information is larger than the retrieval information .",
    "also we verified the for higher correlations ( @xmath118 ) the categorization information can be the larger one , even for small load @xmath119 , while for smaller correlations ( @xmath120 ) the retrieval information is always the larger one .    for a fixed @xmath2 ,",
    "one expects that increasing @xmath4 the categorization information ( if @xmath121 or @xmath2 are large enough ) increases up to a maximum value after which it decreases until it becomes zero at a critical @xmath4 .",
    "this behavior can be seen in fig.4 , where the case @xmath122 , @xmath123 is plotted .",
    "we verified that the larger the values of @xmath121 , the higher are the maxima of @xmath124 , and less examples are need .",
    "we also observed that the retrieval information have a similar non - monotonic behavior if @xmath121 or @xmath2 are small .",
    "the information conveyed by the categorization model was studied .",
    "it was shown that the transition from the retrieval phase to the categorization phase carries together a transition in the information : the retrieval information decreases when the network is oversaturated with examples , and after a period of resting , the categorization information increases .",
    "it is interesting to note that , although neither the retrieval nor the categorization informations surpasses the usual hopfield model , ( @xmath125 , @xmath126 ) , which is @xmath127 at @xmath128 , the fact that the network can return to behave as an associative memory after a long period of @xmath129 between @xmath130 is an advantage with respect to hopfield network .",
    "it is also of worth of note that the retrieval information can still be relatively large , as we see in fig.2 , a quotation which was not observed before in any work about the categorization model in the literature .",
    "the simulation results fit very well with the theoretical in both retrieval and categorization regimes , showing that almost no effect of finite - size is present , but the time of convergence in the resting period must be much larger than that used in this work .    both expressions for the information of the retrieval and of the categorization in eqs.([3.ira]-[3.ica ] ) are not claimed to be exact .",
    "they are approximations for a more precise measure , the @xmath131 @xmath132@xcite between neuron and patterns , @xmath133 = h[\\xi ] -   \\langle h[\\sigma|\\xi ] \\rangle_{\\xi } $ ] , where @xmath134 $ ] is the conditional entropy . since we know that the conditional probability of the neuron , given the concept state , is @xmath135",
    ", we can replace the categorization information by      this quantity gives the degree of information the neuron can catch from the concept .",
    "however we prefer to use the estimation in eq.([3.ica ] ) to compare with the retrieval information with the same precision .",
    "finally we hope that the present approach to the information content of a neural network of correlated patterns can be used in the context of more general architectures and learning rules .",
    "a more general distribution of the @xmath22@xcite may also deserves some attention ."
  ],
  "abstract_text": [
    "<S> the information conveyed by a hierarchical attractor neural network is examined . </S>",
    "<S> the network learns sets of correlated patterns ( the examples ) in the lowest level of the hierarchical tree and can categorize them at the upper levels . a way to measure the non - extensive information content of the examples </S>",
    "<S> is formulated . </S>",
    "<S> curves showing the transition from a large retrieval information to a large categorization information behavior , when the number of examples increase , are displayed . </S>",
    "<S> the conditions for the maximal information are given as functions of the correlation between examples and the load of concepts . </S>",
    "<S> numerical simulations support the analytical results .    </S>",
    "<S> epsf    2 </S>"
  ]
}