{
  "article_text": [
    "a number of researches have been conducted on prediction of financial time series with neural networks since rumelhart @xcite developed back propagation algorithm in 1986 , which is the most commonly used algorithm for supervised neural network . with this algorithm",
    "the network learns its internal structure by updating the parameter values when we give it training data containing inputs and outputs . we can then use the network with updated parameters to predict future events containing inputs the network has never encountered .",
    "the algorithm is applied in many fields such as robotics and image processing and it shows a good performance in prediction of financial time series .",
    "relevant papers on the use of neural network to financial time series include @xcite , @xcite , @xcite and @xcite .    in these papers authors",
    "are concerned with the prediction of time series and they to not pay much attention to actual investing strategies , although the prediction is obviously important in designing practical investing strategies .",
    "a forecast of tomorrow s price does not immediately tell us how much to invest today .",
    "in contrast to these works , in this paper we directly consider investing strategies for financial time series based on neural network models and ideas from game - theoretic probability of shafer and vovk ( 2001 ) @xcite . in the game - theoretic probability established by shafer and vovk , various theorems of probability theory , such as the strong law of large numbers and the central limit theorem ,",
    "are proved by consideration of capital processes of betting strategies in various games such as the coin - tossing game and the bounded forecasting game . in game - theoretic probability a player `` investor '' is regarded as playing against another player `` market '' . in this framework",
    "investing strategies of investor play a prominent role .",
    "prediction is then derived based on strong investing strategies ( cf .",
    "defensive forecasting in @xcite ) .",
    "recently in @xcite we proposed sequential optimization of parameter values of a simple investing strategy in multi - dimensional bounded forecasting games and showed that the resulting strategy is easy to implement and shows a good performance in comparison to well - known strategies such as the universal portfolio @xcite developed by thomas cover and his collaborators . in this paper",
    "we propose sequential optimization of parameter values of investing strategies based on neural networks .",
    "neural network models give a very flexible framework for designing investing strategies . with simulation and with some data from tokyo stock exchange",
    "we show that the proposed strategy shows a good performance .",
    "the organization of this paper is as follows . in section [ sec : sosnn ]",
    "we propose sequential optimizing strategy with neural networks . in section",
    "[ sec : compare ] we present some alternative strategies for the purpose of comparison . in section [ subsec : back - propagation ] we consider an investing strategy using supervised neural network with back propagation algorithm .",
    "the strategy is closely related to and reflects existing researches on stock price prediction with neural networks . in section [ subsec : markovian ]",
    "we consider markovian proportional betting strategies , which are much simpler than the strategies based on neural networks . in section",
    "[ sec : sim ] we evaluate performances of these strategies by monte carlo simulation . in section",
    "[ sec : real ] we apply these strategies to stock price data from tokyo stock exchange .",
    "finally we give some concluding remarks in section [ sec : remarks ] .",
    "here we introduce the bounded forecasting game of shafer and vovk @xcite in section [ subsec : bounded - forecasting - game ] and network models we use in section [ subsec : design - of - network ] . in section [ subsec : gradient ]",
    "we specify the investing ratio by an unsupervised neural network and we propose sequential optimization of parameter values of the network .",
    "we present the bounded forecasting game formulated by shafer and vovk in 2001 @xcite . in the bounded forecasting game , investor s capital at the end of round @xmath0",
    "is written as @xmath1 ( @xmath2 ) and initial capital @xmath3 is set to be @xmath4 . in each round investor first announces the amount of money @xmath5 he bets ( @xmath6 ) and then market announces her move @xmath7 $ ] .",
    "@xmath8 represents the change of the price of a unit financial asset in round @xmath0 .",
    "the bounded forecasting game can be considered as an extension of the classical coin - tossing game since the bounded forecasting game results in the classical coin - tossing game if @xmath9 . with @xmath10 and @xmath11 , investor s capital after round @xmath0 is written as @xmath12 .",
    "the protocol of the bounded forecasting game is written as follows .",
    "* protocol : * + @xmath13 @xmath141 . + for @xmath15 : + investor announces @xmath16 .",
    "+ market announces @xmath7 $ ] .",
    "+ @xmath17 + end for + we can rewrite investor s capital as @xmath18 , where @xmath19 is the ratio of investor s investment @xmath5 to his capital @xmath20 after round @xmath21 .",
    "we call @xmath22 the investing ratio at round @xmath0 .",
    "we restrict @xmath23 as @xmath24 in order to prevent investor becoming bankrupt .",
    "furthermore we can write @xmath1 as @xmath25 taking the logarithm of @xmath1 we have @xmath26    the behavior of investor s capital in ( [ eq : log - capital ] ) depends on the choice of @xmath27 . specifying a functional form of @xmath28",
    "is regarded as an investing strategy .",
    "for example , setting @xmath29 to be a constant @xmath30 for all @xmath31 is called the @xmath30-strategy which is presented in @xcite . in this paper",
    "we consider various ways to determine @xmath32 in terms of past values @xmath33 of @xmath34 and and seek better @xmath32 in trying to maximize the future capital @xmath1 , @xmath35 .",
    "let @xmath36 denote past @xmath37 values of @xmath34 and let @xmath28 depend on @xmath38 and a parameter @xmath39 : @xmath40",
    ". then @xmath41 is the best parameter value until the previous round . in our sequential optimizing investing strategy",
    ", we use @xmath42 to determine the investment @xmath5 at round @xmath0 : @xmath43 for the function @xmath44 we employ neural network models for their flexibility , which we describe in the next section .",
    "we construct a three - layered neural network shown in figure [ fig:1 ] .",
    "the input layer has @xmath37 neurons and they just distribute the input @xmath45 to every neuron in the hidden layer . also the hidden layer has @xmath46 neurons and we write the input to each neurons as @xmath47 which is a weighted sum of @xmath48 s .",
    "as seen from figure [ fig:1 ] , @xmath47 is obtained as @xmath49 where @xmath50 is called the weight representing the synaptic connectivity between the @xmath51th neuron in the input layer and the @xmath52th neuron in the hidden layer .",
    "then the output of the @xmath52th neuron in the hidden layer is described as @xmath53 as for activation function we employ hyperbolic tangent function . in a similar way , the input to the neuron in the output layer , which we write @xmath54 , is obtained as @xmath55 where @xmath56 is the weight between the @xmath52th neuron in the hidden layer and the neuron in the output layer .",
    "finally , we have @xmath57 which is the output of the network . in the following argument we use @xmath58 as an investment strategy .",
    "thus we can write @xmath59 where @xmath60 investor s capital is written as @xmath61    we need to specify the number of inputs @xmath37 and the number of neurons @xmath46 in the hidden layer .",
    "it is difficult to specify them in advance .",
    "we compare various choices of @xmath37 and @xmath46 in section [ sec : sim ] and section [ sec : real ] .",
    "also in @xmath62 we can include any input which is available before the start of round @xmath0 , such as moving averages of past prices , seasonal indicators or past values of other economic time series data .",
    "we give further discussion on the choice of @xmath62 in section [ sec : remarks ] .      in this section",
    "we propose a strategy which we call sequential optimizing strategy with neural networks ( sosnn ) .",
    "we first calculate @xmath63 that maximizes @xmath64 this is the best parameter values until the previous round .",
    "if investor uses @xmath65 as the investing ratio , investor s capital after round @xmath0 is written as @xmath66    for maximization of ( [ eq : maximize ] ) , we employ the gradient descent method . with this method , the weight updating algorithm of @xmath67 with the parameter @xmath68 ( called the learning constant )",
    "is written as @xmath69 where @xmath70 and the left superscript @xmath31 to @xmath71 indexes the round .",
    "thus we obtain @xmath72 similarly , the weight updating algorithm of @xmath50 is expressed as @xmath73 where @xmath74 thus we obtain @xmath75    here we summarize the algorithm of sosnn at round @xmath0 .    1 .",
    "given the input vector @xmath36 @xmath76 and the value of @xmath77 , we first evaluate @xmath78 and then @xmath79 . also we set the learning constant @xmath68 .",
    "we calculate @xmath80 and then @xmath81 with @xmath82 and @xmath83 of the previous step .",
    "then we update weight @xmath84 with the weight updating formula @xmath85 and @xmath86 .",
    "3 .   go back to step 1 replacing the weight @xmath77 with updated values .",
    "after sufficient times of iteration , @xmath87 in ( [ eq : maximize ] ) converges to a local maximum with respect to @xmath50 and @xmath67 and we set @xmath88 and @xmath89 , which are elements of @xmath90",
    ". then we evaluate investor s capital after round @xmath0 as @xmath91 .",
    "here we present some strategies that are designed to be compared with sosnn . in section [ subsec : back - propagation ] we present a strategy with back - propagating neural network .",
    "the advantage of back - propagating neural network is its predictive ability due to `` learning '' as previous researches show . in section [ subsec : markovian ]",
    "we show some sequential optimizing strategies that use rather simple function for @xmath44 than sosnn does .",
    "in this section we consider a supervised neural network and its optimization by back propagation .",
    "we call the strategy nnbp .",
    "it decides the betting ratio by predicting actual up - and - downs of stock prices and can be regarded as incorporating existing researches on stock price prediction .",
    "thus it is suitable as an alternative to sosnn .    for supervised network , we train the network with the data from a training period , obtain the best value of the parameters for the training period and then use it for the investing period .",
    "these two periods are distinct . for the training period",
    "we need to specify the desired output ( target ) @xmath92 of the network for each day @xmath31 .",
    "we propose to specify the target by the direction of market s current price movement @xmath93 .",
    "thus we set @xmath94 note that this @xmath92 is the best investing ratio if investor could use the current movement @xmath93 of market for his investment .",
    "therefore it is natural to use @xmath92 as the target value for investing strategies .",
    "we keep on updating @xmath95 by cycling through the input - output pairs of the days of the training period and finally obtain @xmath96 after sufficient times of iteration .    throughout the investing period we use @xmath96 and investor s capital after round @xmath0 in the investing period",
    "is expressed as @xmath97    back propagation is an algorithm which updates weights @xmath98 and @xmath99 so that the error function @xmath100 decreases , where @xmath92 is the desired output of the network and @xmath101 is the actual output of the network .",
    "the weight @xmath99 of day @xmath31 is renewed to the weight @xmath102 of day @xmath103 as @xmath104 where @xmath105 also weight @xmath98 is renewed as @xmath106 where @xmath107 at the end of each step we calculate the training error defined as @xmath108 where @xmath109 is the length of the training period .",
    "we end the iteration when the the training error becomes smaller than the threshold @xmath110 , which is set sufficiently small .    here",
    "let us summarize the algorithm of nnbp in the training period .    1",
    ".   we set @xmath111 .",
    "2 .   given the input vector @xmath36 and the value of @xmath95 , we first evaluate @xmath112 and then @xmath113 .",
    "also we set the learning constant @xmath68 .",
    "we calculate @xmath114 and then @xmath115 with @xmath82 and @xmath83 of the previous step .",
    "then we update weight @xmath95 with the weight updating formula @xmath116 and @xmath117 .",
    "4 .   go back to step 2 setting @xmath118 and @xmath119 while @xmath120 . when @xmath121 we set @xmath111 and @xmath122 and continue the algorithm until the training error becomes less than @xmath110 .",
    "in this section we present some sequential optimizing strategies that are rather simple compared to strategies with neural network in section [ sec : sosnn ] and section [ subsec : back - propagation ] .",
    "the strategies of this section are generalizations of markovian strategy in @xcite for coin - tossing games to bounded forecasting games .",
    "we present these simple strategies for comparison with sosnn and observe how complexity in function @xmath44 increases or decreases investor s capital processes in numerical examples in later sections .",
    "consider maximizing the logarithm of investor s capital in ( [ eq : log - capital ] ) : @xmath123 we first consider the following simple strategy of @xcite in which we use @xmath124 , where @xmath125 in this paper we denote this strategy by mkv0 .    as a generalization of mkv0",
    "consider using different investing ratios depending on whether the price went up or down on the previous day .",
    "let @xmath126 when @xmath127 was positive and @xmath128 when it was negative .",
    "we denote this strategy by mkv1 . in the betting on the @xmath0th day we use @xmath129 and @xmath130 , where @xmath131 @xmath132 and @xmath133 here @xmath134 denotes the indicator function of the event in @xmath135 .",
    "the capital process of mkv1 is written in the form of ( [ eq : log - capital ] ) as @xmath136    we can further generalize this strategy considering price movements of past two days .",
    "let @xmath137 and let @xmath138 we denote this strategy by mkv2 .",
    "we will compare performances of the above markovian proportional betting strategies with strategies based on neural networks in the following sections .",
    "in this section we give some simulation results for strategies shown in section [ sec : sosnn ] and section [ sec : compare ] .",
    "we use two linear time series models to confirm the behavior of presented strategies .",
    "linear time series data are generated from the box - jenkins family @xcite , autoregressive model of order 1 ( ar(1 ) ) and autoregressive moving average model of order 2 and 1 ( arma(2,1 ) ) having the same parameter values as in @xcite .",
    "ar(1 ) data are generated as @xmath139 and arma(2,1 ) data are generated as @xmath140 where we set @xmath141 .",
    "after the series is generated , we divide each value by the maximum absolute value to normalize the data to the admissible range @xmath142 $ ] .    here",
    "we discuss some details on calculation of each strategy .",
    "first we set the initial values of elements of @xmath84 as random numbers in @xmath143 $ ] . in sosnn",
    ", we use the first @xmath144 values of @xmath11 as initial values and the iteration process in gradient descent method is proceeded until @xmath145 and @xmath146 with the upper bound of @xmath147 steps . as for the learning constant @xmath68 , we use learning - rate annealing schedules which appear in section 3.13 of @xcite . with annealing schedule called the search - then - converge schedule",
    "@xcite we put @xmath68 at the @xmath0th step of iteration as @xmath148 where @xmath149 and @xmath150 are constants and we set @xmath151 and @xmath152 . in nnbp , we train the network with five different training sets of @xmath153 observations generated by ( [ eq : ar1data ] ) and ( [ eq : armadata ] ) .",
    "we continue cycling through the training set until the training error becomes less than @xmath110 and we set @xmath154 with the upper bound of @xmath155 steps .",
    "also we check the fit of the network to the data by means of the training error for some different values of @xmath68 , @xmath37 and @xmath46 . in markovian strategies ,",
    "we again use the first @xmath144 values of @xmath11 as initial values .",
    "we also adjust the data so that the betting is conducted on the same data regardless of @xmath37 in sosnn and nnbp or different number of inputs among markovian strategies .    in table [ sim ]",
    "we summarize the results of sosnn , nnbp , mkv0 , mkv1 and mkv2 under ar(1 ) and arma(2,1 ) .",
    "the values presented are averages of results for five different simulation runs of ( [ eq : ar1data ] ) and ( [ eq : armadata ] ) . as for sosnn",
    ", we simulate fifty cases ( combinations of @xmath156 and @xmath157 ) , but only report the cases of @xmath158 and some choices of @xmath46 because the purpose of the simulation is to test whether @xmath159 works better than other choices of @xmath37 under ar(1 ) and @xmath160 works better under arma(2,1 ) . for nnbp",
    "we only report the result for one case since fitting the parameters to the data is quite a difficult task due to the characteristic of desired output ( target ) .",
    "also once we obtain the value of @xmath68 , @xmath37 and @xmath46 with training error less than the threshold @xmath154 , we find that the network has successfully learned the input - output relationship and we do not test other choices of the above parameters .",
    "( see appendix for more detail . )",
    "we set @xmath161 , @xmath162 and @xmath163 in simulation with ar(1 ) model and @xmath164 , @xmath165 and @xmath166 in simulation with arma(2,1 ) model .",
    "investor s capital process for each choice of @xmath37 and @xmath46 in sosnn , nnbp and each markovian strategy is shown in three rows , corresponding to rounds @xmath167 , @xmath168 , @xmath153 of the betting ( without the initial 20 rounds in sosnn and markovian strategies ) .",
    "the fourth row of each result for nnbp shows the training error after learning in the training period . the best value among the choices of @xmath37 and @xmath46 in sosnn or among each markovian strategy is written in bold and marked with an asterisk and the second best value is also written in bold and marked with two asterisks .",
    "also calculation results written with ``  '' are cases in that simulation did not give proper values for some reasons .",
    ".log capital processes of sosnn , nnbp , mkv0 , mkv1 , mkv2 under ar(1 ) and arma(2,1 ) [ cols=\"^,^,^,^,^,^,^,^,^ \" , ]     in figure [ fig:2 ] we show the movements of closing prices of each company during the investing period . in figures",
    "[ fig:3]-[fig:5 ] we show the log capital processes of the results shown in table [ real ] to compare the performance of each strategy . figure [ fig:3 ] is for sony , figure [ fig:4 ] is for nomura holdings and figure [ fig:5 ] is for ntt . for sosnn we plotted the result of @xmath37 and @xmath46 that gave the best performance at @xmath169 ( the bottom row of the three rows ) in table [ real ] .",
    "as we see from above figures , nnbp which shows competitive performance for two linear models in section [ sec : sim ] gives the worst result .",
    "thus it is obvious that the network has failed to capture trend in the betting period even if it fits in the training period .",
    "also the results are favorable to sosnn if we adopt appropriate numbers for @xmath37 and @xmath46 .",
    "we proposed investing strategies based on neural networks which directly consider investor s capital process and are easy to implement in practical applications .",
    "we also presented numerical examples for simulated and actual stock price data to show advantages of our method .",
    "in this paper we only adopted normalized values of past market s movements for the input @xmath62 while we can use any data available before the start of round @xmath0 as a part of the input as we mentioned in section [ subsec : design - of - network ] .",
    "let us summarize other possibilities considered in existing researches on financial prediction with neural networks .",
    "the simplest choice is to use raw data without any normalization as in @xcite , in which they analyze time series of athens stock index to predict future daily index . in @xcite",
    "they adopt price of faz - index ( one of the german equivalents of the american dow - jones - index ) , moving averages for 5 , 10 and 90 days , bond market index , order index , us - dollar and 10 successive faz - index prices as inputs to predict the weekly closing price of the faz - index .",
    "also in @xcite they use 12 technical indicators to predict the s`&`p 500 stock index one month in the future . from these",
    "researches we see that for longer prediction terms ( such as monthly or yearly ) , longer moving averages or seasonal indexes become more effective .",
    "thus those long term indicators may not have much effect in daily price prediction which we presented in this paper . on the other hand , adopting data which seems to have a strong correlation with closing prices of tokyo stock exchange such as closing prices of new york stock exchange of the previous day may increase investor s capital processes presented in this paper . since there are numerical difficulties in optimizing neural networks , it is better to use small number of effective inputs for a good performance .    another important generalization of the method of this paper is to consider portfolio optimization .",
    "we can easily extend the method in this paper to the betting on multiple assets .",
    "let the output layer of the network have @xmath170 neurons as shown in figure [ fig:6 ] and the output of each neuron is expressed as @xmath171 , @xmath172 .",
    "then we obtain a vector @xmath173 of outputs . the number of neurons @xmath170 refers to the number of different stocks investor invests .        investor s capital after round @xmath0",
    "is written as @xmath174 where @xmath175 thus also in portfolio cases we see that our method is easy to implement and we can evaluate investor s capital process in practical applications .",
    "here we discuss training error in the training period of nnbp . in this paper",
    "we set the threshold @xmath110 for ending the iteration to @xmath176 , while the value commonly adopted in many previous researches is smaller , for instance , @xmath177 .",
    "we give some details on our choice of @xmath110 .",
    "let us examine the case of nomura holdings in section [ sec : real ] . in figure [ fig:6 ]",
    "we show the training error after each step of iteration in the training period calculated with ( [ eq : te ] ) .",
    "while the plotted curve has a typical shape as those of previous researches , it is unlikely that the training error becomes less than @xmath176 .",
    "also in figure [ fig:7 ] we plot @xmath178 for each @xmath31 calculated with parameter values @xmath84 after learning .",
    "we observe that the network fails to fit for some points ( actually @xmath179 days out of @xmath153 days ) but perfectly fits for all other days .",
    "it can be interpreted that the network ignores some outliers and adjust to capture the trend of the whole data .",
    "99 e.  m.  azoff .",
    "_ neural network time series forecasting of financial markets .",
    "_ wiley , chichester , 1994 .",
    "g.  p.  e.  box and g.  m.  jenkins .",
    "_ time series : analysis forecasting and control_.   holden - day , san francisco , 1970 . t.  m.  cover .",
    "universal portfolios .",
    "_ mathematical finance _ , * 1 * , no.1 , 129 , 1991 .",
    "c.  darken , j.  chang and j.  moody . learning rate schedules for faster stochastic gradient search .",
    "_ ieee second workshop on neural networks for signal processing _ , 312 , 1992 . b.  freisleben .",
    "stock market prediction with backpropagation networks .",
    "_ industrial and engineering applications of artificial intelligence and expert system 5th international conference _ , 451460 , 1992 . m.  hanias , p.  curtis and j.  thalassinos .",
    "prediction with neural networks : the athens stock exchange price indicator .",
    "_ european journal of economics , finance and administrative sciences _ , * 9 * , 2127 , 2007 . s.  s.  haykin .",
    "_ neural networks and learning machines_. 3rd ed . , prentice hall , new york , 2008 . n.  l.  d.  khoa , k.  sakakibara and i.  nishikawa",
    "stock price forecasting using back propagation neural networks with time and profit based adjusted weight factors .",
    "_ sice - icase international joint conference _ , 54845488 , 2006 . m.  kumon , a.  takemura and k.  takeuchi .",
    "sequential optimizing strategy in multi - dimensional bounded forecasting games .",
    "arxiv:0911.3933v1 , 2009 .",
    "d.  e.  rumelhart , g.  e.  hinton and r.  j.  williams .",
    "learning internal representation by backpropagating errors .",
    "_ nature _ , * 323 * , 533536 , 1986 . g.  shafer and v.  vovk .",
    "_ probability and finance : it s only a game!_.   wiley , new york , 2001 . k.  takeuchi , m.  kumon and a.  takemura . multistep bayesian strategy in coin - tossing games and its application to asset trading games in continuous time .",
    "arxiv:0802.4311v2 , 2008 .",
    "conditionally accepted to _",
    "stochastic analysis and applications_. v.  vovk , a.  takemura and g.  shafer .",
    "defensive forecasting .",
    "_ proceedings of the 10th international workshop on artificial intelligence and statistics _ ( r.  g.  cowell and z.  ghahramani editors ) , 365372 , 2005 .",
    "y.  yoon and g.  swales .",
    "predicting stock price performance : a neural network approach .",
    "_ proceedings of the 24th annual hawaii international conference on system _ , * 4 * , 156162 , 1991 . g.  p.  zhang .",
    "an investigation of neural networks for linear time - series forecasting .",
    "_ computers ` & ` operations research _ , * 28 * , no.12 , 11831202 , 2001 ."
  ],
  "abstract_text": [
    "<S> in this paper we propose an investing strategy based on neural network models combined with ideas from game - theoretic probability of shafer and vovk . </S>",
    "<S> our proposed strategy uses parameter values of a neural network with the best performance until the previous round ( trading day ) for deciding the investment in the current round . </S>",
    "<S> we compare performance of our proposed strategy with various strategies including a strategy based on supervised neural network models and show that our procedure is competitive with other strategies . </S>"
  ]
}