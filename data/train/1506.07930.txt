{
  "article_text": [
    "clustering is a widely used unsupervised technique for identifying natural classes within a set of data .",
    "the idea is to group unlabeled data into subsets so the within - group homogeneity is relatively high and the between - group heterogeneity is also relatively high .",
    "the implication is that the groups should reflect the underlying structure of the data generator ( dg ) .",
    "clustering of continuous data has been extensively studied over many decades leading to several conceptually disjoint categories of techniques .",
    "however , the same can not be said for the clustering of categorical data which , by contrast , has not been developed as extensively .",
    "we recall that categorical data are discrete valued measurements . here",
    ", we will assume there are finitely many discrete values and that there is no meaningful ordering on the values or distance between them ( i.e. , nominal ) .",
    "for instance , if the goal is to cluster genomes , the data consist of strings of four nucleotides ( a , t , c , g ) .",
    "typically these strings are high dimensional and have variable length .",
    "there are many other similar examples such as clustering a population based on the presence / absence of biomarkers in specific locations .",
    "we first propose a technique for low dimensional equi - length categorical data vectors as a way to begin addressing the problem of discrete clustering in general .",
    "second , we extend this basic technique to data consisting of high dimensional but equal length vectors .",
    "then , we adapt our technique to permit unequal length vectors .",
    "our approach is quite different from others and there are so many approaches that we defer discussion of a selection of them to sec .",
    "[ review ] .",
    "heuristically , our basic method in low dimensions is as follows .",
    "start with @xmath0 vectors of dimension @xmath1 , say @xmath2 for @xmath3 , where , for @xmath3 and @xmath4 , @xmath5 assumes values in a finite set @xmath6 that can be identified with the first @xmath7 natural numbers . for each @xmath8",
    "we form a @xmath9 membership matrix @xmath10 by treating each of the @xmath1 variables separately .",
    "then the combined membership matrix is @xmath11 of dimension @xmath12 .",
    "using hamming distance on the rows of @xmath13 gives a dissimilarity measure on the @xmath14 s , say , @xmath15 .",
    "now , we can use any hierarchical method to generate a clustering of the @xmath14 s but we argue that adding a second stage by ensembling will improve performance .",
    "we add a second stage by first choosing @xmath16 $ ] for @xmath17 and assume the @xmath18 s are distinct . for each @xmath18",
    ", we use a hierarchical clustering technique based on @xmath15 to produce a clustering with @xmath18 clusters .",
    "each of the @xmath19 clusterings gives a combined membership matrix @xmath20 of the form @xmath13 above .",
    "again , we can use the hamming distance on the rows as a dissimilarity , say @xmath21 . as with @xmath15",
    ", any hierarchical clustering method can be used with the ensemble level dissimilarity @xmath21 .    in our work , we found that using @xmath15 and @xmath21 with the same linkage criterion typically gave better results than using @xmath15 and @xmath21 with different linkage criteria .",
    "thus , we have six categorical clustering techniques : three use the basic method based on @xmath15 ( i.e. , first stage only ) with single linkage ( sl ) , complete linkage ( cl ) , and average linkage ( al ) , while the other three use the ensemble method based on @xmath15 and @xmath21 ( i.e.,first and second stages ) with sl , cl , or al . as a generality , in our comparisons we found that the ensembled clusterings under al or cl typically gave the best results .",
    "we describe two paradigms in which the sort of ensembling described above is likely to be beneficial .",
    "one paradigm uses squared error loss and the familiar jensen s inequality first used in bagging , an ensembling context for classification , see @xcite .",
    "the other paradigm uses a dissimilarity that parallels zero - one loss .",
    "both descriptions rely on the concept of a true clustering @xmath22 of size @xmath23 based on the population defined by @xmath24 , the probability distribution generating the data . for our results ,",
    "we assume @xmath25 is uniquely defined in a formal sense .",
    "we extend this basic method to high dimensional data vectors of equal length by partitioning the vectors into multiple subvectors of a uniform but smaller length , applying our basic method to each of them , and then combining the results with another layer of ensembling .",
    "our second extension , to variable length categorical vectors , is more complicated but can be used to address the clustering of such vectors in genome sciences .",
    "it involves the concept of alignment .",
    "there are various forms of alignment ( local , semi - global , global , etc . )",
    "but the basic idea is to force vectors of categorical data of differing lengths to have the same length by adding an extra symbol e.g. , a @xmath26 , in strategic places .",
    "then the resulting equal length vectors can be clustered as in our first extension .    in the next section ,",
    "we review the main techniques for clustering categorical data and indicate where they differ from the methods we have proposed . in sec .",
    "[ lowdim ] we formally present our technique described above and provide a series of examples , both simulated and with real data , that verify our basic technique works well .",
    "we also present theoretical results that suggest our method should work well in some generality . in sec .",
    "[ highdim ] we present our first extension and in sec .",
    "[ highdimgen ] we present our second extension . in the final sec .",
    "[ conclude ] , we discuss several issues related to the use of our method .",
    "in this section , we review six techniques for clustering categorical data , roughly in the order in which they were first proposed .",
    "@xmath27-modes , see @xcite , is an extension of the familiar @xmath27-means procedure for continuous data to categorical data .",
    "however , there are two essential differences . first ,",
    "since the mean of a cluster does not make sense for categorical data , the modal value of a cluster is used instead ; like the mean , the mode is taken componentwise .",
    "second , in place of the euclidean distance , @xmath27-modes uses the hamming distance , again componentwise .",
    "the initial modes are usually chosen randomly from the observations .",
    "as recognized in @xcite , this leads to instability and frequent inaccuracy in that @xmath27-modes often gives locally optimal clusterings that are not globally optimal .",
    "there have been many efforts to overcome the instability and inaccuracy of @xmath27-modes clustering with categorical data .",
    "indeed , @xcite suggested choosing the initial modes to be as far from each other as possible .",
    "even if this were formalized , it is not clear how it would ensure the resulting @xmath27-modes clustering would be accurate or stable .",
    "a different approach was taken in @xcite .",
    "these authors used the density of a point @xmath14 defined to be @xmath28 so that a point with high density should have many points relatively close to it .",
    "@xcite used a similar density at points .",
    "further attempts to find good initial values are in @xcite who use the algorithm in @xcite with a different distance function . also , @xcite proposed a method for selecting the most relevant attributes and clustering on them individually .",
    "then , they take representatives of the clusterings as the initial values for a @xmath27-modes clustering .",
    "our method is based on ensembling so it is automatically stable",
    ".      there have been several papers using the density - based algorithm , dbscan ( density - based spatial clustering of applications with noise ) . originally proposed for continuous data by @xcite",
    ", it extends to categorical variables because hamming distance can be used to define a dissimilarity matrix .",
    "dbscan defines a cluster to be a maximum set of density - connected points ; two points are density connected if and only if the distance between them is less than a pre - assigned parameter .",
    "this means that every point in a cluster must have a minimum number of points within a given radius .",
    "there are other approaches that are similar to the dbscan such as cactus , see @xcite and clicks , see @xcite . these and other methods are used in @xcite on zoo and soybean data where it is seen they do not outperform @xmath27-modes .",
    "hence , we do not use cactus , clicks or their variants here . note that to use these methods",
    "one must choose a distance parameter that influences the size of the resulting clusters .",
    "our method requires no auxilliary parameter and combines clusterings over randomly selected dimensions avoiding the question of ` density ' in a discrete context .",
    "@xcite presented a robust agglomerative hierarchical - clustering algorithm that can be applied to categorical data .",
    "it is referred to as rock ( robust clustering using links ) .",
    "rock employs ` links ' in the sense that it counts the number of point - to - point hops one must make to form a path from one data point to another .",
    "note that this relies on the number of points there are between two selected points , but not directly on the distance between them .",
    "in essence , rock iteratively merges clusters to achieve high within - cluster similarity .",
    "however , this de facto requires a concept of distance between points by way of hops  two hopes being twice as long as one hop .",
    "our method does not rely on distance  it only counts the number of locations at which two strings differ .      @xcite use the hamming distance ( hd ) from each observation to a reference position @xmath29 .",
    "then , they form the histogram generated by the values @xmath30 and set up a hypothesis test .",
    "let @xmath31 be that there are no clusters in the data and take @xmath32 to be the negation of this statement . under @xmath31",
    ", we expect the histogram to be approximately normal , at least for well chosen @xmath29 . under the null hypothesis ,",
    "lack of clustering , @xcite estimates the frequency of the uniform ` hd ' .",
    "this is compared to the hd vector with respect to @xmath29 by way of a chi - squared statistic .",
    "if the chi - square statistic is too large , this is evidence against the null .",
    "so , the data that make the chi - square large are removed and the process is repeated .",
    "this is an iterative method that relies on testing and the choice of @xmath29 .",
    "it is therefore likely not stable unlike an ensemble method .      model based clustering has attracted a lot of attention , see @xcite .",
    "the main idea is that an overall mixture model for the observations can be identified and the subsets of data can be assigned to components in the mixture .",
    "so , the overall model is of the form @xmath33 where @xmath34 , @xmath35 and the @xmath36 s are the components .",
    "the @xmath37 can be used as indicators of what fraction of data are from each component and the @xmath36 s can be used to assign data to components .",
    "the likelihood of a mixture model is @xmath38 where the parameters can be estimated using the expectation - maximization algorithm .",
    "when the data are categorical , the @xmath36 s should be categorical and @xcite proposed @xmath39 where @xmath40 , @xmath41 and @xmath42 gives the probability of category @xmath43 of the variable @xmath44 in the cluster @xmath45 .",
    "the proposed model is actually the product of @xmath46 conditionally independent categorical distributions .",
    "several versions of this method are implemented in the r package rmixmod .",
    "model based clustering should work well  and does work well for continuous data . for categorical data",
    "it is not at all clear when a mixture model holds or even is a good approximation .",
    "in fact , the mixture model likely does not hold very often and we would only expect good performance from mbc when it does .",
    "by contrast , ensembling should be perform reliably well over a larger range of dg s .",
    "another approach is to regard the categorical data as the result of a clustering procedure .",
    "so , one can form a matrix in which the rows represent the @xmath0 data points and each of the columns represent the values of an attribute of the data point .",
    "if the attributes are taken as cluster labels then the clusterings , one for each attribute , can be used to form a consensus clustering by defining a dissimilarity between data points using hamming distance .",
    "this process is known as ensembling and was first used on categorical data by @xcite via the techniques cspa , hgpa and mcla . from these , @xcite select the clustering with the greatest average normalized mutual information ( anmi ) as the final result .",
    "this seeks to merge information among different clusterings .",
    "however the mutual information is measure of dependence and finding clusterings that are dependent is not the same as finding clusterings that are accurate .",
    "they apply their technique to several data sets but note that it does not perform well on unbalanced data like zoo .",
    "the actual process of ensembling is reviewed in @xcite .",
    "we find that ensembling over the dissimilarities is a better way to ensemble since it seems to give a more accurate assessment of the discrepancy between points .",
    "the idea of evidence accumulation clustering ( eac ) is due to @xcite and is an ensembling approach that initially was used for continuous data .",
    "the central idea is to create many clusterings of different sizes ( by @xmath27-means ) that can be pooled via a ` co - association matrix ' that weights points in each clustering according to their membership in each cluster .",
    "this matrix can then be easily modified to give a dissimilarity so that single - linkage clustering can be applied , yielding a final clustering .",
    "the first use of eac on categorical data seems to be @xcite .",
    "they used the @xmath27-modes technique with random initializations for cluster centers to generate @xmath19 base clusterings .",
    "then they ensembled these by various methods . in section [ num ] , we use this approach for categorical data but using @xmath27-modes instead @xmath27-means ( en - km ) .",
    "the same idea can be used to implement eac on the mbc ( en - mbc ) . again , our method ensembles over dissimilairties rather than clusters directly .",
    "it seems that in catgorical clustering getting a good way to assess discrepancy between data points provides better solutions than trying to merge different clusterings directly .",
    "here we present our ensemble technique for clustering categorical data , provide some justification for why it should perform well , and see how this is borne out in a few examples .      to fix notation",
    ", we assume @xmath0 independent and identical ( iid ) outcomes @xmath47 of a random variable @xmath48 .",
    "the @xmath14 s are assumed @xmath1-dimensional and written as @xmath49 where each @xmath5 is an element of a finite set @xmath50 ; the @xmath50 s do not have to be the same but @xmath51 is required for our method .",
    "we denote a clustering of size @xmath27 by @xmath52 ; often we assume that for each @xmath27 only one clustering will be produced . without loss of generality , we assume each @xmath50 is identified with @xmath53 natural numbers .",
    "we write the @xmath14 s as the rows in an observation matrix and look at its columns : @xmath54 that is , @xmath55 .    now , for each @xmath56 form the @xmath57 membership matrix @xmath10 with elements @xmath58 where @xmath59 and @xmath60 .",
    "the combined membership matrix is @xmath61 the result of concatenating @xmath1 matrices with dimensions @xmath62 .",
    "thus , @xmath13 is @xmath63 where @xmath64 .",
    "we use the transpose of the @xmath10 s so that each row will correspond to a subject . rewriting @xmath13 in terms of its rows i.e. , in terms of subjects 1 through @xmath0",
    ", gives @xmath65 now , consider the dissimilarity @xmath66 where @xmath67 here , @xmath68 if @xmath69 and one otherwise . under this dissimilarity",
    "one can do hierarchical clustering under single linkage ( hcsl ) , average linkage ( hcal ) , and complete linkage ( hccl ) .",
    "this gives three clustering techniques for categorical data .",
    "however , we do not advocate them as we have described because , as will be seen below , we can get better performance by ensembling and treating sets of outlying points representing less than @xmath70 of the data separately ( see @xcite for the basic technique to handle potential chaining problems and outliers under single linkage context ) .",
    "our ensembling procedure is as follows .",
    "choose one of hcsl , hcal , or hccl and draw @xmath71 $ ] for @xmath72 .",
    "for each value of @xmath73 use the chosen method to find a clustering of size @xmath18 .",
    "then form the incidence matrix @xmath74 \\label{incmatrix}\\end{aligned}\\ ] ] note that each column corresponds to a clustering and @xmath75 is the index of the cluster in the @xmath73-th clustering to which @xmath14 belongs .",
    "for any of the three linkages , we can find the dissimilarity matrix @xmath76 in which @xmath77 note that we only compare the entries in a row within their respective columns .",
    "thus , we never compare , say @xmath78 with @xmath79 but only compare @xmath78 to entries of the form @xmath80 for @xmath81 .",
    "now , we can use the same linkages on @xmath21 as before ( sl , al , cl ) with the treatment of small sets of outliers as in @xcite to get a final clustering for any given @xmath27 .",
    "( the choice of @xmath27 can also be estimated by any of a number of techniques ; see @xcite for a discussion and comparison of such techniques . )",
    "consider two iid observations @xmath82 and @xmath83 from the population defined by @xmath24 assumed to have a well defined true clustering @xmath25 .",
    "write the cluster membership function as @xmath84 now define the _ similarity _ of @xmath85 and @xmath86 to be the indicator function @xmath87 this gives the ( random ) dissimilarity @xmath88 .",
    "clearly @xmath89 so that @xmath90 empirically , we use data @xmath91 to form a clustering denoted @xmath92 where @xmath93 is the index of the cluster in @xmath94 containing @xmath95 .",
    "so , @xmath96 in which @xmath82 and @xmath83 are independent of the iid data @xmath91 .",
    "now , given @xmath19 independent replications of the clustering technique we obtain @xmath97 so , we can form the dissimilarity @xmath98 where @xmath99 , @xmath100 and @xmath101 are the minimum and maximum acceptable sizes for a candidate clustering .",
    "note that the expression in reduces to when @xmath102 and @xmath103 .",
    "thus , is a specific instance of so when we do the hierarchical clustering in sec .",
    "[ basictechnique ] we are using a dissimilarity of the same form as .",
    "consequently , any statement involving @xmath104 will lead to a corresponding statement for the ensembling in our clustering technique .",
    "the following proposition shows that the ensemble dissimilarity is closer to the true value of the dissimilarity on average than any individual member of the ensemble when the clusterings in the ensemble are ` mean unbiased ' .",
    "formally , a clustering method is mean unbiased if and only if the expectation of the empirical dissimilarity , conditional on the data and number of clusters , is @xmath105 and so independent of @xmath91 and @xmath106 .",
    "that is , a clustering method is mean unbiased if and only if @xmath107 the intuition behind this definition is that the probability that two data points are not in the same cluster is independent of @xmath106 for any @xmath106 chosen between @xmath108 and @xmath109 for fixed data @xmath91 .",
    "note that , in this expression , the expectation is taken over the collection of possible clusterings holding @xmath91 and @xmath106 fixed .",
    "this requires that some auxiliary random selections , such as the random selection of initial cluster centers in @xmath27-means , is built into the clustering technique .",
    "when a clustering technique is mean unbiased we get that @xmath110 and hence the ensembling preserves the mean unbiasedness . in this case",
    ", we can show that the ensembling behaves like taking a mean of iid variables in the sense that the variance of the average of the @xmath111 s decreases as @xmath112 .",
    "the interpretation of this is that the ensemble dissimilarity is a more accurate representation of the dissimilarity between any two data points than the dissimilarity from any individual clustering in the ensemble .",
    "assume all the clusterings @xmath113 are mean unbiased .",
    "then , for each @xmath21 and set @xmath114 there is a @xmath115 so that @xmath116 implies @xmath117    the expected dissimilarity for any clustering function @xmath118 is @xmath119 i.e. , possibly dependent on @xmath91 and the number @xmath45 of clusters in @xmath118 .",
    "consequently , the conditional probability @xmath120 is well defined for each @xmath91 and @xmath45 . since",
    "@xmath106 is chosen from a finite set of integers , @xmath121 i.e. , the right hand side of is strictly positive .",
    "next observe that in general , for each @xmath106 there is a @xmath122 as in .",
    "since the clusterings in the ensemble are independent over reselected @xmath106 s , and the variance of a bernoulli(@xmath123 ) is uniformly bounded by 1/4 for @xmath124 $ ] , we have that @xmath125    hence , for @xmath19 sufficiently large , implies @xmath126 the difference between this bound and the statement of the result is that this bound uses @xmath127 and @xmath128 in place of @xmath129 .",
    "since @xmath130 is mean unbiased , implies that for @xmath19 large enough gives @xmath131    note that even though mean unbiasedness implies that @xmath129 is independent of @xmath21 and @xmath106 , @xmath104 and @xmath132 need not be .",
    "so , the conditioning in can not be dropped .",
    "if we take expectations over @xmath21 for finite @xmath0 , the proof of the proposition can be modified to give @xmath133 it can also be seen that if @xmath0 increases and the clustering @xmath134 is consistent in the sense that @xmath135 pointwise in @xmath95 , then for an nondecreasing sequence of @xmath136 , the inequality continues to hold and the conditioning on @xmath91 drops out .    instead of arguing that the mean dissimilarity from ensembling behaves well , one can argue that the actual clustering from an ensemble method such as we have presented is close to the population clustering .",
    "that is , it is possible to identify a condition that ensures ensemble clusterings are more accurate on average than the individual clusterings they combine . to see this ,",
    "fix any sequence @xmath114 and let @xmath137 and write @xmath138 since the @xmath106 s are drawn iid from dunif@xmath139 $ ] , the condition @xmath140 is equivalent to @xmath141 moreover , @xmath142 that is , is equivalent to saying the ensemble clustering is more accurate on average and has a smaller variance than any of its constituents .",
    "condition defines a subset of @xmath143^b$ ] on which ensembling good clusterings will improve the overall clustering .",
    "this is a parallel to the set defined in @xcite and quantifies the fact that while ensembling generally gives better results than not ensembling , it is possible that ensembling in some cases can give worse results .",
    "that is , the best clustering amongst the @xmath19 clusterings may be better than the ensemble clustering , but only infrequently .",
    "this is borne out in our numerical analyses below .",
    "although hamming distance is the natural metric to use with discrete , nominal data , it is easier to see that ensembling clusterings gives improved performance using squared error loss .",
    "our result for clustering is modeled on sec 4.1 @xcite for bagging classifiers .",
    "let @xmath144 be the expectation operator for the data generator that produced @xmath21 .",
    "then , the population - averaged clustering is @xmath145 implicitly , this assumes @xmath45 is fixed and not random .",
    "this is an analog to the ensemble based clustering presented in sec .",
    "[ basictechnique ] .",
    "two important squared error ` distances ' between the true clustering and estimates of it are @xmath146 a jensen s inequality gives that the squared error loss from using @xmath147 is smaller than from using @xmath118 .",
    "let @xmath82 be an independent outcome from the same distribution as generated @xmath91 .",
    "then , @xmath148    @xmath149 \\nonumber \\\\          & \\leq & e_{\\mathbb{x}}e_{\\cal{d}}[(c_t(\\mathbb{x})- \\phi(\\mathbb{x},{\\cal{d}}))^2]\\nonumber\\\\          & = & e_{\\cal{d}}ape ( \\phi).\\nonumber\\end{aligned}\\ ] ]    obviously , the inequality might be equality , or nearly so , in which case the averaging provides little to no gain .",
    "on the other hand , the averaging may make the distribution of @xmath147 concentrate around @xmath150 more tightly than the distribution of @xmath118 does , in which case the inequality would be strict and the difference between the two sides could be large representing a substantial gain from the ensembling . in this sense",
    ", the ensembling may stabilize @xmath118 as a way to reduce its variance and hence improve its performance .",
    "note that if the expectation in were taken over @xmath151 , @xmath118 , or both @xmath27 and @xmath118 as well , the jensen s inequality argument would continue to hold with @xmath144 replaced by @xmath152 , @xmath153 , or @xmath154 .",
    "that is , more averaging can only improve the clustering .      to evaluate our proposed methodologies",
    ", we did two numerical analyses , one with simulated data and the other with real data . technically , the real and simulated datasets are classification data but we applied our clustering techniques ignoring the class labels .",
    "thus , the question is how well the clustering techniques replicated the known true classes . to assess this we defined the classification rate ( cr ) to be the proportion of observations from a data set that were correctly assigned to their cluster or class . to calculate the cr , we start by generating the clustering from the data . then",
    ", we order the clusters according to how much they overlap with the correct clusters . for instance , if the estimated clusters are @xmath155 and the correct clusters are @xmath156 then , if necessary , we relabel the estimated clusters so that @xmath157 , @xmath158 , and @xmath159 . in this way",
    ", we ensure that each of the estimated clusters overlap maximally , in a sequential sence , with exactly one of the true clusters .",
    "this can be done using the hungarian algorithm , see @xcite for details .",
    "the simulated data were generated as follows .",
    "first , we fixed the dimension @xmath160 .",
    "then we generated sets of 20-dimensional data points of sizes 50 or 125 assuming different cluster structures .",
    "for instance , in the equi - sized cluster case with five clusters with @xmath161 we generated five clusters of size 25 .",
    "for each such cluster , we proceeded as follows .",
    "choose @xmath162 iid and choose @xmath163 .",
    "then , draw 25 values from @xmath164 where @xmath165 is the number of values the variable giving the first dimension can assume .",
    "do this again for each of @xmath166 for @xmath167 .",
    "taken together these 125 values give the first entries for the 125 vectors of length 20 to be formed .",
    "we proceed similarly to obtain the second entries for the 125 vectors of dimension 20 , but draw a new @xmath168 independent of @xmath165 .",
    "that is , we choose values @xmath169 iid for @xmath170 and again generate 25 values from each @xmath171 for each @xmath44 . doing this",
    "18 more times for iid ranges of length @xmath172 gives 125 vectors of length 20 that represent five clusters each of size 25 .",
    "doing this entire procedure 3000 times gives 3000 such data sets .",
    "we did this sort of procedure for various clustering patterns as indicated in table [ clusterpatterns ] , e.g. , taking nine values in place of 25 values in @xmath173 for the first cluster . the patterns were chosen to test the methods systematically over a reasonable range of true clusterings based on the size of the clusters .",
    ".design of simulated data . [ cols=\"<,^,^,^\",options=\"header \" , ]      + [ simhighdim ]      we consider a population consisting of five varieties of rice ( oryza sativa ) and use clustering on single nucleotide polymorphism ( snp ) data to assess the plausibility of the division of the species into five varieties .",
    "the data , rice , were originally presented and analyzed in @xcite and consist of 391 samples from the five varieties indica ( 87 ) , aus ( 57 ) , temperate japonica ( 96 ) , aromatic ( 14 ) and tropical japonica ( 97 ) where the numbers in parentheses indicate the number of samples from each variety .    the analysis done in @xcite was to measure the genetic similarity between individuals .",
    "essentially , @xcite calculate the proportion of times a pair of nucleotides at the same position differ .",
    "mathematically , this is equivalent to using a version of the average hamming distance .",
    "note that in their analysis they ignored missing values as is permitted in plink , @xcite .",
    "setting @xmath174 we regenerated their analysis and dendrogram .",
    "the result is shown on the left hand panel of fig .",
    "[ fig1:level ] . around the outer ring of the circle",
    "the correct memberships of the data points are indicated .",
    "the cr for this clustering is one .",
    "+ for comparison , the right hand panel in fig .",
    "[ fig1:level ] shows the dendrogram for clustering the rice data using @xmath175 , again setting @xmath174 .",
    "the cr was found to be one .",
    "no dendrogram for mwkm can be shown because it is not a hierarchical method . however , the cr for mwkm was 0.87 , making it second best in performance .",
    "even though plink and @xmath175 have the same cr , visually it is obvious that @xmath175 gives the better dendrogram because the clusters are more clearly separated .",
    "that is , @xmath175 does not perform better in terms of correctness but does provide a better visualization of the data .",
    "this is the effect of ensembling over dissimilarity matrices .",
    "( right ) , for the rice data .",
    "green , blue , red , yellow , and black indicate indica , aus , temperate japonica , aromatic and tropical japonica , respectively . ]      in this example we demonstrate that the performance of @xmath175 can be regarded as robust . consider the gene expression data presented and analyzed in @xcite .",
    "it is actually classification data and analyzed as such in @xcite . here ,",
    "for demonstration purposes we compare some of the clusterings we can generate to the known classes so as to find crs .",
    "the sample size is 62 and there are three classes : diffuse large b - cell lymphomas ( d ) with 42 samples , follicular lymphoma ( f ) with 9 samples , and chronic lymphocytic leukemia ( c ) with 11 samples .",
    "the dimension of the gene expression data after pre - processing is 4026 .",
    "( the pre - processing included normalization , imputation , log transformation , and standardization to zero mean and unit variance across genes . )",
    "first we applied @xmath27-means to the data 1000 times with @xmath176 and random starts , and found an average cr of 0.79 with a standard error of 0.13 .",
    "when we applied @xmath177 to the data , we found its cr to be 0.67 , noticeably worse than @xmath27-means .",
    "now consider the following procedure .",
    "trichotomize all 4026 variables by using their 33% and 66% percentiles and relabel them as ` 1 ' , ` 2 ' , and ` 3 ' . then , apply @xmath178 and @xmath175 to the discretized data . to get crs for hierarchical methods",
    ", we must convert their dendrograms to clusterings by cutting them at some level . to do this",
    ", we used the function cuttree ( with @xmath176 ) in r. the resulting crs for @xmath178 , @xmath175 were 0.71 and 0.84 , respectively .",
    "that is , even when the data are discrete only because they were made that way artificially , @xmath175 handily outperformed three other methods .",
    "the full dendrogram from @xmath175 is given in fig .",
    "[ dendgeneexp ] . along the horizontal axis",
    ", the correct labels of the classes are given . if these were ignored and one were merely to eyeball the data , one could be led to put the two rightmost d s and the first two f s into the cluster of c s , giving an cr of 58/62 = .93  much higher than .84 .",
    "one could just as well put the leftmost d s into one cluster , the next 13 d s into a second cluster , the next eight d s into a third cluster , and the rightmost 18 observations into a fourth cluster , leaving the intervening data points essentially as a fifth cluster that does not cohere . in this case",
    ", the cr would be terrible .",
    "so , even though the data are artificially discretized , using an automated method of @xmath175 on a discretized @xmath175 and cutree gives a result in the midrange of what informal methods would give .",
    "this is evidence that ensemble methods such as @xmath175 are inherently robust .",
    "otherwise put , reading dendrograms informally can be misleading whereas formal methods may be reliably accurate .",
    "we extend our method to clustering categorical vectors of different lengths .",
    "this is an important clustering problem in genomics because it is desirable to be able to cluster strains of organisms , for instance , even though their genomes have different lengths in terms of number of nucleotides .",
    "the first step is to preprocess the data so all the vectors have the same length . this process is called alignment .",
    "the aligned vectors can then be clustered using the technique of sec .",
    "[ highdim ] .",
    "the point of this section is to verify that our clustering method is effective even after alignment .    to be specific , consider sequence data of the form @xmath179 with @xmath3 in which each @xmath5 for @xmath180 is a nucleotide in @xmath181 .",
    "it is obvious that a sufficient condition for our method to apply is that all the @xmath5 s assume values in sets @xmath50 for which @xmath182 is bounded . to find a common value for the @xmath0 sequences we align them using software called mafft-7 ( @xcite ) .",
    "mafft-7 is a multiple alignment program for amino acid or nucleotide sequences .",
    "the basic procedure was first presented in @xcite .",
    "a recent comparison of algorithms and software for this kind of alignment problem was carried out in @xcite who argued that mafft-7 is faster and scales up better than other implementations such as clustal and muscle .    at the risk of excessive oversimplification ,",
    "the basic idea behind alignment procedures is as follows .",
    "suppose two sequences @xmath14 and @xmath183 of different lengths are to be aligned .",
    "then , the alignment procedure introduces place holders represented by @xmath118 so that the two sequences are of the same length and the subsequences that do match are in the same place along the overall sequence .",
    "when more than two sequences must be aligned , a progressive alignment can be used , i.e. , two sequence are aligned and fixed , the third one aligned to previous ones , and the procedure continues until all sequences are aligned .",
    "given that a collection of genomic sequences have been aligned , we can cluster them by applying our technique .    in the absence of established theory for this more complicated case , we present two examples to verify that the procedure gives reasonable results .",
    "both of our examples concern viruses : their genomes are large enough to constitute a nontrivial test of our clustering method and of different enough in lengths from species to species that alignment of some sort is necessary .    as a first simple example , consider the virus family _ filovirdae _ .",
    "this family includes numerous related viruses that form filamentous infectious viral particles ( virions ) .",
    "their genomes are represented as single - stranded negative - sense rnas .",
    "the two members of the family that are best known are _ ebolavirus _ and _ marburgvirus_. both viruses , and some of their lesser known relatives , cause severe disease in humans and nonhuman primates in the form of viral hemorrhagic fevers ( see @xcite ) .",
    "+ there are three genera in _ filoviridae _ , and we chose as our data set all the complete and distinct viral genomes with a known host from this family available from vipr .",
    "there were 103 in total from 3 genera , namely , _",
    "( 1 , cue ) , _ ebolavirus _ ( 80 ) , and _ marburgvirus _ ( 22 , mar ) where the indicators in parentheses show the frequency and the abbreviation . _",
    "ebolavirus _ further subdivides into five species : _ bundibugyo virus _ ( 3 , bun ) , _ reston ebolavirus _ ( 5 , res ) , _ sudan ebolavirus _ ( 6 , sud ) , _ tai forest ebolavirus _ ( 2 , tai ) , and _ zaire ebolavirus _ ( 64 , zai ) .",
    "the hosts are human , monkey , swine , guinea pig , mouse , and bat ( denoted hum , mon , swi , gpi , mou , bat , respectively , on the dendrograms ) .",
    "the minimum and maximum genome lengths are 18623 and 19114 . while we recognize that the genomes in the pathogen virus resource are not drawn independently from a population",
    ", we can nevertheless apply our method and evaluate the results .",
    "we apply only @xmath175 as @xmath178 and @xmath177 performed poorly on high dimensional data .",
    "in addition , we used @xmath184 , @xmath185 , and @xmath186 because we had a dissimilarity that could be used after alignment .",
    "essentially , as long as the distance between @xmath118 and the nucleotides could be omitted from the hamming distance sum , the dissimilarity was well - defined .",
    "it turned out that all three gave nearly identical results although @xmath186 was slightly better .",
    "the important point is that @xmath175 performed better than the three non - ensemble methods because the ensembling over dissimilarity matrices gives a better assessment of the distance between aligned genomes than not ensembling .",
    "the results for @xmath186 are shown in the fig .",
    "[ flohcal ] .",
    "it is seen that the subpopulations are well separated .",
    "virologically speaking , this means that the various species correspond to relatively tight clusters . within the _ marburg _ cluster ( on the left ) it appears that most of the genomes have either humans or bats as hosts suggesting that one organism ( probably the bats ) is transmitting _ marburg _ to the other ( probably human ) .",
    "sudan ebolavirus mainly afflicts humans ( pending more data ) while _ reston ebolavirus _ is known not to be a pathogen for humans . the vast majority of the _ zaire _ and _ bunibugyo _ genomes have human as host .",
    "[ flowr ] shows the corresponding dendrogram using @xmath175 , an ensemble method .",
    "qualitatively the results are the same as for @xmath186 .",
    "the improvement of @xmath175 over @xmath186 is seen in the fact that @xmath175 reveals greater separation between the clusters .",
    "indeed , even if one corrects for the vertical scale , the leaves within a cluster under @xmath175 separate from each other at a much finer level .",
    "that is , the ensembling over the dissimilarities accentuates the differences between genomes in different clusters as discussed in sec .",
    "[ whyensemble ] .     under the same labeling convention as the fig .",
    "[ flohcal ] . ]     under the same labeling convention as the fig .",
    "[ flohcal ] . ]",
    "these dendrograms can be contrasted with a phylogenetic tree for the _ filoviridae _ viruses .",
    "figure [ phylotree_filo ] shows the phylogenetic tree generated by the neighbor - joining ( nj ) method as implemented in the r package ape ( @xcite ) .",
    "the nj method constructs a tree by successive pairing of the neighbors .",
    "the idea behind a phylogenetic tree , as opposed to a dendrogram , is to represent the sequence of evolutionary steps through which organisms mutated as a reasonable way to classify the existing and extinct organisms .",
    "the goals of the two sorts of trees are somewhat different and one would not expect them to agree fully , since clustering only gives a mathematically optimal path to the evolutionary endpoint while phylogenetic trees try to track genomic changes .",
    "for instance , the phylogenetic tree shows that _ zaire ebolavirus _ with a human host separates early into two distinct groups which may or may not be reasonable evolutionarily and is different from fig .",
    "[ flowr ] .",
    "_ tai forest ebolavirus _ and _ bunidbugyo virus _ genomes are seen to be possibly close evolutionary but are not close in fig .",
    "[ flowr ] . on the other hand ,",
    "_ sudan ebolavirus _ and _ reston ebolavirus _ are seen to be close in terms of both clustering and phylogenetics while _ marburg _ is a separate and recent genus , consistent with it being its own cluster .     and [ flowr ] , but differs in some details . ]    as a second and more complicated example , we studied the _",
    "herpesviridae _ family of viruses that cause diseases in humans and animals .",
    "_ herpesviridae _ is a much larger family than _ filoviridae _ and the genomes in _ herpesviridae _ are generally longer as well as more varied in length than those in _ filoviridae_. according to vipr , the family _ herpesviridae _ is divided into three subfamilies ( _ alphaherpesvirinae _ , _ betaherpesvirinae _ and _ gammaherpesvirinae _ ) .",
    "we limited our analysis to the distinct and complete genomes in _ alphaherpesvirinae _ that have known hosts ; _ alphaherpesvirinae _ has more complete genomes than either _ betaherpesviridae _ or _",
    "gammaherpesvidae_. within _ alphaherpesviridae _ there are has five genera : _ iltovirus _ ( iit ) , _ mardivirus _ ( mar ) , _ scutavirus _ , _ simplexvirus _ ( sim ) , and _ varicellovirus _ ( var ) . since _",
    "did not have complete any complete genomes , we disregarded this genus .",
    "the rest remaining genera had 20 , 18 , 20 , 40 genomes , respectively , from different hosts , namely , human , monkey , chicken , turkey , duck , cow , bat ( fruit ) , equidae ( horse ) , boar , cat family , amazona oratrix ( denoted hum , mon , chi , tur , duc , cow , bat , equ , boa , cat , and ora , respectively in the dendrograms ) .",
    "these viral genomes have lengths ranging from 124784 to 178311 base pairs .",
    "parallel to the _ ebolavirus _ example , we present the two dendrograms corresponding to @xmath186 and @xmath175 .",
    "these are in fig.[herphcal ] and [ herpen ] .",
    "the top panel shows that @xmath186 , the non - ensembled version based on hamming distance , is qualitatively the same as the lower panel . as before",
    ", the key difference is that @xmath175 yields a cleaner separation of clusters relative to @xmath186 .",
    "it is important to note that the clusters in the dendrograms correspond to ( genus , host ) pairs .",
    "that is , the clustering corresponds to identifiable physical differences so the clusters have a clear interpretation .     and",
    "[ herpen ] , but differs in some details . ]     and [ herpen ] , but differs in some details . ]     and [ herpen ] , but differs in some details . ]    the same can not be said for the phylogenetic tree generated by nj as before and shown in fig .",
    "[ phylo_herp1 ] .",
    "for instance , _ varicellovirus _ from host equidae are partitioned into two clusters early . also , _",
    "mardivirus _ from chicken and duck hosts are not cleanly separated . on the other hand ,",
    "most other population - host pairs are fairly well separated .",
    "overall , fig .",
    "[ phylo_herp1 ] does not give as good a clustering as the panels in figs .",
    "[ herphcal ] and [ herpen ] .",
    "in this paper we have presented a method for clustering categorical data in low , high , and varying dimensions .",
    "we began with relatively small dimensions , up to 35 for the @xmath187 data , and studied the way our method seemed to improve over other methods . specifically , we ensembled over dissimilarity matrices in an effort to represent the distance between data points more accurately . our theoretical work in sec . [ whyensemble ]",
    "provides some formal justification for why this sort of technique should perform well in some generality .",
    "then we turned to the clustering of high dimensional categorical data , focusing on genomic data .",
    "we extended our ensemble method for low dimensional data to high dimensional categorical vectors of equal length by adding a layer of ensembling : we obtained dissimilarity matrices by ensembling over randomly selected dimensions .",
    "we then used our method on categorical vectors of different lengths by artificially making them the same length through alignment procedures .",
    "again , our ensembling method performed better than the other methods we tested .",
    "in particular , we compared the output of our method in this case to phylogenetic trees . while not strictly scientific , the dendrograms we generated can be interpreted physically and differ in some important respects from phylogenetic trees generated from the same data .    throughout",
    "we have used a large number of simulated and real data examples to buttress the intuition behind the technique and formal results .",
    "we comment that there are many other tests of the general methodology that could be done .",
    "for instance , in our clustering of viral genomes we could have included incomplete genomes .",
    "however , it many cases the incomplete genomes had over 90% of the nucleotides missing and we thought this insufficient for good conclusions .",
    "the authors gratefully acknowledge research support from nsf - dtra grant dms-1120404 .",
    "authors would like to thank daniela witten and liang bai for providing the codes of sparce clustering and mwkm , and mehdi r.m .",
    "bidokhti for his comments of virology part .",
    "( 1996 ) , `` a density- based algorithm for discovering clusters in large spatial databases with noise , '' _ in : proc .",
    "2nd internat .",
    "knowl- edge discovery and data mining ( kdd-96 ) , portland , or _ , pp .",
    "226 - 231 .",
    "( 2009 ) , `` clustering high - dimensional data : a survey on subspace clustering , pattern - based clustering , and correlation clustering , '' _ acm transactions on knowledge discovery from data ( tkdd ) _ , 3(1 ) , 1 .",
    "pickett , b. e. , e. l. sadat , y. zhang , noronha , et al .",
    "( 2012 ) , `` vipr : an open bioinformatics database and analysis resource for virology research , '' nucleic acids research , 40(d1 ) , d593-d598 .",
    "www.viprbrc.org    purcell , s. , b. neale , k. todd - brown , l. thomas , m. ferreira , d. bender , j. maller , p. sklar , p. de bakker , m. daly , and p.c .",
    "sham ( 2007 ) , `` plink : a toolset for whole - genome association and population - based linkage analysis , '' american journal of human genetics , 81 ."
  ],
  "abstract_text": [
    "<S> we present a technique for clustering categorical data by generating many dissimilarity matrices and averaging over them . </S>",
    "<S> we begin by demonstrating our technique on low dimensional categorical data and comparing it to several other techniques that have been proposed . </S>",
    "<S> then we give conditions under which our method should yield good results in general . </S>",
    "<S> our method extends to high dimensional categorical data of equal lengths by ensembling over many choices of explanatory variables . in this context </S>",
    "<S> we compare our method with two other methods . </S>",
    "<S> finally , we extend our method to high dimensional categorical data vectors of unequal length by using alignment techniques to equalize the lengths . we give examples to show that our method continues to provide good results , in particular , better in the context of genome sequences than clusterings suggested by phylogenetic trees .    </S>",
    "<S> * clustering categorical data via ensembling + dissimilarity matrices *    saeid amiri ,  bertrand clarke   and  jennifer clarke    _ department of statistics , university of nebraska - lincoln , lincoln , nebraska , usa _    * keywords * : categorical data ; ensembling methods ; high dimension ; monte carlo investigation . </S>"
  ]
}