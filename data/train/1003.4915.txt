{
  "article_text": [
    "consider the general polynomial program @xmath19 where @xmath20 is a polynomial , @xmath21 is a basic semi - algebraic set , and @xmath22 is the _ global _ minimum of @xmath0 ( as opposed to a local minimum ) .",
    "one way to approximate the global optimum @xmath22 of @xmath0 is to solve a hierarchy of either lp - relaxations or semidefinite relaxations as proposed in e.g. lasserre @xcite . despite practice with the semidefinite relaxations",
    "seems to reveals that convergence is fast , the matrix size in the @xmath23-th semidefinite relaxation of the hierarchy grows up as fast as @xmath24 .",
    "hence , for large size ( and sometimes even medium size ) problems , only a few relaxations of the hierarchy can be implemented ( the first , second or third relaxation ) . in that case ,",
    "one only obtains a lower bound on @xmath22 , and no feasible solution in general .",
    "so an important issue is :    _ how can we use the result of the @xmath23-th semidefinite relaxation to find an approximate feasible solution of the original problem ? _    for some well - known special cases of 0/1 optimization like",
    "e.g. the celebrated maxcut problem , one may generate a feasible solution with guaranteed performance , from a randomized rounding procedure that uses an optimal solution of the first semidefinite relaxation ( i.e. with @xmath25 ) ; see goemans and williamson @xcite .",
    "but in general there is no such procedure .",
    "our contribution is to provide two relatively simple algorithms for polynomial programs which builds up upon the so - called `` joint+marginal '' approach ( in short ( j+m ) ) developed in @xcite for _ parametric _ polynomial optimization .",
    "the ( j+m)-approach for variables @xmath26 and parameters @xmath27 in a simple set @xmath28 , consists of the standard hierarchy of semidefinite relaxations in @xcite where one treats the parameters @xmath27 also as variables .",
    "but now the moment - approach implemented in the semidefinite relaxations , considers a _ joint _ probability distribution on the pair @xmath29 , with the additional constraint that the _ marginal _ distribution on @xmath28 is fixed ( e.g. the uniform probability distribution on @xmath28 ) ; whence the name _ `` joint+marginal''_.    for every @xmath30 , let the compact interval @xmath31\\subset\\r$ ] be contained in the projection of @xmath32 into the @xmath33-coordinate axis . in the context of the ( non - parametric ) polynomial optimization ( [ defpb ] ) , the above ( j+m)-approach can be used as follows in what we call the * ( j+m)-algorithm * :    @xmath34 ( a ) treat @xmath1 as a parameter in the compact interval @xmath35 $ ] with associated probability distribution @xmath6 uniformly distributed on @xmath7 .    @xmath34 ( b ) with",
    "@xmath36 fixed , solve the @xmath23-th semidefinite relaxation of the ( j+m)-hierarchy @xcite applied to problem @xmath4 with @xmath37 variables @xmath38 and parameter @xmath1 , which is problem @xmath0 with the additional constraint that the variable @xmath39 is fixed .",
    "the dual provides a univariate polynomial @xmath40 which , if @xmath23 would increase , would converge to @xmath41 in the @xmath42-norm .",
    "( the map @xmath43 denotes the optimal value function of @xmath44 , i.e. the optimal value of @xmath0 given that the variable @xmath1 is fixed at the value @xmath45 . )",
    "next , compute @xmath46 , a global minimizer of the univariate polynomial @xmath47 on @xmath7 ( e.g. this can be done by solving a single semidefinite program ) .",
    "ideally , when @xmath23 is large enough , @xmath48 should be close to the first coordinate @xmath49of a global minimizer @xmath50 of @xmath0 .",
    "@xmath34 ( c ) go back to step ( b ) with now @xmath51 instead of @xmath1 , and with @xmath52 being the probability measure uniformly distributed on @xmath53 . with the same method ,",
    "compute a global minimizer @xmath54 , of the univariate polynomial @xmath55 on the interval @xmath53 . again , if @xmath23 would increase , @xmath56 would converge in the @xmath57-norm to the optimal value function @xmath58 of @xmath15 ( i.e. the optimal value of @xmath0 given that the variable @xmath16 is fixed at the value @xmath45 . ) iterate until one has obtained @xmath59 .",
    "one ends up wih a point @xmath60 and in general @xmath61 .",
    "one may then use @xmath62 as initial guess of a local optimization procedure to find a local minimum @xmath63 .",
    "the rational behind the ( j+m)-algorithm is that if @xmath23 is large enough and @xmath0 has a unique global minimizer @xmath64 , then @xmath62 as well as @xmath65 should be close to @xmath66 .    the computational complexity before the local optimization procedure is less than solving @xmath67 times the @xmath23-th semidefinite relaxation in the ( j+m)-hierarchy ( which is itself of same order as the @xmath23-th semidefinite relaxation in the hierarchy defined in @xcite ) , i.e. , a polynomial in the input size of @xmath0 .",
    "when the feasible set @xmath32 is convex , one may define the following variant to obtain a _ feasible",
    "_ point @xmath68 .",
    "again , let @xmath7 be the projection of @xmath69 into the @xmath1-coordinate axis . once @xmath46 is obtained in step ( b ) ,",
    "consider the new optimization problem @xmath70 in the @xmath37 variables @xmath38 , obtained from @xmath0 by fixing the variable @xmath39 at the value @xmath48 .",
    "its feasible set is the convex set @xmath71 .",
    "let @xmath53 be the projection of @xmath69 into the @xmath16-coordinate axis .",
    "then go back to step ( b ) with now @xmath72 as parameter and @xmath73 as variables , to obtain a point @xmath54 , etc . until a point @xmath60 is obtained .",
    "notice that now @xmath68 because @xmath32 is convex .",
    "then proceed as before with @xmath62 being the initial guess of a local minimization algorithm to obtain a local minimizer @xmath63 of @xmath0 .",
    "most of the material of this section is taken from @xcite .",
    "let @xmath74 $ ] denote the ring of polynomials in the variables @xmath75 , and the variables @xmath76 , whereas @xmath74_d$ ] denotes its subspace of polynomials of degree at most @xmath77 .",
    "let @xmath78\\subset\\r[\\bx,\\y]$ ] denote the subset of polynomials that are sums of squares ( in short s.o.s . ) .",
    "for a real symmetric matrix @xmath79 the notation @xmath80 stands for @xmath79 is positive semidefinite .",
    "let @xmath81 be a compact set , called the _",
    "parameter _ set , and let @xmath82 $ ] , @xmath83 .",
    "let @xmath84 be the basic closed semi - algberaic set : @xmath85 and for each @xmath86 , let @xmath87 for each @xmath86 , fixed , consider the optimization problem : @xmath88 the interpretation is as follows : @xmath28 is a set of parameters and for each instance @xmath86 of the parameter , one wishes to compute an optimal _ decision _ vector @xmath89 that solves problem ( [ pb1 ] ) .",
    "let @xmath90 be a borel probability measure on @xmath28 , with a positive density with respect to the lebesgue measure on @xmath91 ( or with respect to the counting measure if @xmath28 is discrete ) .",
    "for instance @xmath92 is uniformly distributed on @xmath28 .",
    "sometimes , e.g. in the context of optimization with data uncertainty , @xmath90 is already specified .",
    "the idea is to use @xmath90 ( or more precisely , its moments ) to get information on the distribution of optimal solutions @xmath89 of @xmath93 , viewed as random vectors . in this section",
    "we assume that for every @xmath86 , the set @xmath94 in ( [ set - x ] ) is nonempty .",
    "let @xmath95 be the set of finite borel probability measures on @xmath32 , and consider the following infinite - dimensional linear program @xmath0 : @xmath96 where @xmath97 denotes the marginal of @xmath98 on @xmath91 , that is , @xmath97 is a probability measure on @xmath91 defined by @xmath99 for all @xmath100 .",
    "notice that @xmath101 for any feasible solution @xmath98 of @xmath0 .",
    "indeed , as @xmath90 is a probability measure and @xmath102 one has @xmath103 .",
    "the dual of @xmath0 is the the following infinite - dimensional linear program : @xmath104}&\\displaystyle \\int_\\y p(\\y)\\,d\\varphi(\\y)\\\\ & f(\\bx)-p(\\y)\\,\\geq\\,0\\quad\\forall ( \\bx,\\y)\\in\\k.\\end{array}\\ ] ] recall that a sequence of measurable functions @xmath105 on a measure space @xmath106 converges to @xmath107 , _",
    "@xmath90-almost uniformly _ , if and only if for every @xmath108 , there is a set @xmath109 such that @xmath110 and @xmath111 , uniformly on @xmath112 .",
    "[ th1 ] let both @xmath81 and @xmath32 in ( [ set - xy ] ) be compact and assume that for every @xmath86 , the set @xmath113 in ( [ set - x ] ) is nonempty .",
    "let @xmath0 be the optimization problem ( [ pb2 ] ) and let @xmath114 , @xmath86 . then :    \\(a ) @xmath115 and @xmath0 has an optimal solution .",
    "assume that for @xmath90-almost @xmath86 , the set of minimizers of @xmath116 is the singleton @xmath117 for some @xmath118 .",
    "then there is a measurable mapping @xmath119 such that @xmath120 and for every @xmath121 , and @xmath122 : @xmath123    there is no duality gap between ( [ pb2 ] ) and ( [ dual - lp ] ) , i.e. @xmath124 , and if @xmath125 $ ] is a maximizing sequence of ( [ dual - lp ] ) then : @xmath126 moreover , define the functions @xmath127 as follows : @xmath128 , and @xmath129,\\quad i=1,2,\\ldots\\ ] ] then @xmath130 , @xmath90-almost uniformly .",
    "an optimal solution @xmath131 of @xmath0 encodes _ all _ information on the optimal solutions @xmath89 of @xmath93 .",
    "for instance , let @xmath132 be a given borel set of @xmath133 .",
    "then from theorem [ th1 ] , @xmath134 with @xmath107 as in theorem [ th1](b ) .",
    "moreover from theorem [ th1](c ) , any optimal or nearly optimal solution of @xmath135 provides us with some polynomial lower approximation of the optimal value function @xmath136 that converges to @xmath137 in the @xmath138 norm .",
    "moreover , one may also obtain a piecewise polynomial approximation that converges to @xmath137 , @xmath90-almost uniformly .",
    "in @xcite the first author has defined a ( j+m)-hierarchy of semidefinite relaxations @xmath139 to approximate as closely as desired the optimal value @xmath140 . in particular",
    ", the dual of each semidefinite relaxation @xmath141 provides a polynomial @xmath142 $ ] bounded above by @xmath143 , and @xmath144 converges @xmath90-almost uniformly to the optimal value function @xmath145 , as @xmath146 .",
    "this last property is the rationale behind the heuristic developed below .",
    "let @xmath147 with @xmath148 . with a sequence @xmath149 indexed in the canonical basis @xmath150 of @xmath151 $ ] ,",
    "let @xmath152\\to\\r$ ] be the linear mapping : @xmath153.\\ ] ]      the moment matrix @xmath154 associated with a sequence @xmath149 , @xmath155 , has its rows and columns indexed in the canonical basis @xmath150 , and with entries .",
    "let @xmath157 be the polynomial @xmath158 .",
    "the localizing matrix @xmath159 associated with @xmath160 $ ] and a sequence @xmath149 , has its rows and columns indexed in the canonical basis @xmath150 , and with entries . @xmath161",
    "a sequence @xmath162 is said to have a _ representing _ finite borel measure supported on @xmath32 if there exists a finite borel measure @xmath98 such that @xmath163      with @xmath164 $ ] , let @xmath21 be the basic compact semi - algebraic set @xmath165 and consider the polynomial optimization problem ( [ defpb ] ) .",
    "let @xmath166 be some interval @xmath167 $ ] , assumed to be contained in the orthogonal projection of @xmath32 into the @xmath33-ccordinate axis .",
    "for instance when the @xmath168 s are affine ( so that @xmath32 is a convex polytope ) , @xmath169 ( resp .",
    "@xmath170 ) solves the linear program @xmath171 .",
    "similarly , when @xmath32 is convex and defined by concave polynomials , one may obtain @xmath169 and @xmath170 , up to ( arbitrary ) fixed precision . in many cases , ( upper and lower ) bound constraints on the variables are already part of the problem definition .",
    "let @xmath172 the probability measure uniformly distributed on @xmath173 , hence with moments @xmath174 given by : @xmath175 for every @xmath176 . define the following parametric polynomial program in @xmath37 variables : @xmath177 or , equivalently @xmath178 ,",
    "where for every @xmath179 : @xmath180 observe that by definition , @xmath181 , and @xmath182 whenever @xmath183 , where @xmath173 is the orthogonal projection of @xmath32 into the @xmath33-coordinate axis .      to compute ( or at least approximate ) the optimal value @xmath140 of problem @xmath0 in ( [ pb2 ] ) associated with the parametric optimization problem ( [ pb - param1 ] ) , we now provide a hierarchy of semidefinite relaxations in the spirit of those defined in @xcite .",
    "let @xmath184 , @xmath83 , and for @xmath185 , consider the semidefinite program : @xmath186 where @xmath174 is defined in ( [ mom1 ] ) .",
    "we call ( [ primal ] ) the _ parametric semidefinite relaxation _ of @xmath0 with parameter @xmath187 . observe that without the `` moment '' constraints @xmath188 , @xmath189 , the semidefinite program ( [ primal ] ) is a relaxation of @xmath0 and if @xmath32 is compact , its corresponding optimal value @xmath190 converges to @xmath22 as @xmath191 ; see lasserre @xcite .",
    "letting @xmath192 , the dual of ( [ primal ] ) reads : @xmath193,\\quad 0\\leq j\\leq m;\\\\ & { \\rm deg}\\,\\sigma_jg_j\\leq 2i,\\quad 0\\leq j\\leq m. \\end{array}\\ ] ] equivalently , recall that @xmath194_{2i}$ ] is the space of univariate polynomials of degree at most @xmath195 , and observe that in ( [ dual ] ) , the criterion reads @xmath196 where @xmath197_{2i}$ ] is the univariate polynomial @xmath198 .",
    "then equivalently , the above dual may be rewritten as : @xmath199_{2i};\\:\\sigma_j\\in\\sigma[\\bx],\\quad 0\\leq j\\leq m;\\\\ & { \\rm deg}\\,\\sigma_jg_j\\leq 2i,\\quad 0\\leq j\\leq m. \\end{array}\\ ] ]    [ ass1 ] the family of polynomials @xmath200 $ ] is such that for some @xmath201 , @xmath202 for some @xmath203 and some s.o.s .",
    "polynomials @xmath204 $ ] .",
    "[ th22 ] let @xmath32 be as ( [ setk ] ) and assumption [ ass1 ] hold .",
    "let the interval @xmath166 be the orthognal projection of @xmath32 into the @xmath33-coordinate axis , and let @xmath172 be the probability measure , uniformly distributed on @xmath173 .",
    "assume that @xmath205 in ( [ setky ] ) is not empty , let @xmath206 be as in ( [ pb - param1 ] ) and consider the semidefinite relaxations ( [ primal])-([duall ] ) .",
    "then as @xmath146 :    \\(a ) @xmath207 and @xmath208    \\(b ) let @xmath209 be a nearly optimal solution of ( [ duall ] ) , e.g. such that @xmath210 .",
    "then @xmath211 for all @xmath183 , and @xmath212 moreover , if one defines @xmath128 , and @xmath213,\\quad i=1,2,\\ldots,\\ ] ] then @xmath214 , for @xmath172-almost all @xmath183 , and so @xmath215 , @xmath172-almost uniformly on @xmath173 .",
    "theorem [ th22 ] is a direct consequence of ( * ? ? ?",
    "* corollary 2.6 ) .",
    "theorem [ th22 ] provides a rationale for the following ( j+m)-algorithm in the general case . in what follows we use the primal and dual semidefinite relaxations ( [ primal])-([dual ] ) with index @xmath23 _",
    "fixed_.    * algo 1 : ( j+m)-algorithm : non convex @xmath32 , relaxation @xmath23 *    * set @xmath13 ; * + * step @xmath10 * : * input : * @xmath32 , @xmath20 , and the orthogonal projection @xmath216 $ ] of @xmath32 into the @xmath33-coordinate axis , with associated probability measure @xmath172 , uniformly distributed on @xmath173 . +",
    "* ouput : * @xmath217 .",
    "solve the semidefinite program ( [ duall ] ) and from an optimal ( or nearly optimal ) solution @xmath218 of ( [ duall ] ) , get a global minimizer @xmath219 of the univariate polynomial @xmath220 on @xmath173 .",
    "+ if @xmath221 stop and output @xmath222 , otherwise set @xmath223 and repeat .    of course , in general the vector @xmath18 does not belong to @xmath32 .",
    "therefore a final step consists of computing a local minimum @xmath63 , by using some local minimization algorithm starting with the ( unfeasible ) initial point @xmath62 .",
    "also note that when @xmath32 is not convex , the determination of bounds @xmath169 and @xmath170 for the interval @xmath173 may not be easy , and so one might be forced to use a subinterval @xmath224 with conservative ( but computable ) bounds @xmath225 and @xmath226 .",
    "[ disconnected ] theorem [ th22 ] assumes that for every @xmath183 , the set @xmath205 in ( [ setky ] ) is not empty , which is the case if @xmath32 is connected .",
    "if @xmath227 for @xmath228 in some open subset of @xmath173 , then the semidefinite relaxation ( [ primal ] ) has no solution ( @xmath229 ) , in which case one proceeds by dichotomy on the interval @xmath173 until @xmath230 .      in this section",
    ", we now assume that the feasible set @xmath21 of problem @xmath0 is convex ( and compact ) . the idea is to compute @xmath48 as in * algo 1 * and then repeat the procedure but",
    "now for the @xmath2-variable problem @xmath70 which is problem @xmath0 in which the variable @xmath1 is _ fixed _ at the value @xmath48 .",
    "this alternative is guaranteed to work if @xmath32 is convex ( but not always if @xmath32 is not convex ) .    for every @xmath231 ,",
    "denote by @xmath232 the vector @xmath233 , and by @xmath234 the vector @xmath235 ( and so @xmath236 ) .",
    "let the interval @xmath5 be the orthogonal projection of @xmath32 into the @xmath1-coordinate axis . for every @xmath237 , let the interval @xmath238 be the orthogonal projection of the set @xmath239 into the @xmath16-coordinate axis .",
    "similarly , given @xmath240 , let the interval @xmath241 be the orthogonal projection of the set @xmath242 into the @xmath243-coordinate axis , and etc . in the obvious way .",
    "for every @xmath244 , and @xmath245 , let @xmath246 , and @xmath247 , @xmath83 .",
    "similarly , let @xmath248 and consider the problem : @xmath249 i.e. the original problem @xmath0 where the variable @xmath250 is fixed at the value @xmath251 , for every @xmath252 .",
    "write @xmath253 $ ] , and let @xmath172 be the probability measure uniformly distributed on @xmath254 .",
    "let @xmath255 be a sequence indexed in the monomial basis of @xmath256 $ ]",
    ". with index @xmath23 , fixed , the parametric semidefinite relaxation ( [ primal ] ) with parameter @xmath33 , associated with problem @xmath257 , reads : @xmath258 where @xmath174 is defined in ( [ mom1 ] ) .",
    "its dual is the semidefinite program ( with @xmath259 ) : @xmath260_{2i},\\:\\sigma_j\\in\\sigma[\\bx_k],\\quad j=0,\\ldots , m\\\\ \\nonumber & & { \\rm deg}\\,\\sigma_j\\tilde{g}^k_j\\leq 2i,\\quad j=0,\\ldots , m.\\end{aligned}\\ ] ] the important difference between ( [ primal ] ) and ( [ primalj ] ) is the _ size _ of the corresponding semidefinite programs , since @xmath255 in ( [ primal ] ) ( resp . in ( [ primalj ] ) ) is indexed in the canonical basis of @xmath261 $ ] ( resp . @xmath256 $ ] ) .",
    "recall that the order @xmath23 of the semidefinite relaxation is fxed .",
    "the ( j+m)-algorithm consists of @xmath67 steps . at step @xmath10 of the algorithm ,",
    "the vector @xmath262 ( already computed ) is such that @xmath46 and @xmath263 for every @xmath264 , and so the set @xmath265 is a nonempty compact convex set .    * algo 2 : ( j+m)-algorithm : convex @xmath32 , relaxation @xmath23 *    * set @xmath13 ; * + * step @xmath266 * : * input : * for @xmath13 , @xmath267 , @xmath268 ; @xmath269 , @xmath270 and @xmath271 , @xmath83 . + for @xmath272 , @xmath273 . + * output : * @xmath274 with @xmath275 .",
    "+ consider the parametric semidefinite relaxations ( [ primalj])-([dualj ] ) with parameter @xmath33 , associated with problem @xmath257 in ( [ pbpj ] ) .    * from an optimal solution of ( [ dualj ] ) , extract the univariate polynomial @xmath276 .",
    "* get a global minimizer @xmath219 of @xmath220 on the interval @xmath277 $ ] , and set @xmath278 .",
    "if @xmath221 stop and ouput @xmath68 , otherwise set @xmath223 and repeat .",
    "as @xmath32 is convex , @xmath68 and one may stop .",
    "a refinement is to now use @xmath62 as the initial guess of a local minimization algorithm to obtain a local minimizer @xmath63 of @xmath0 . in view of theorem [ th22 ] , the larger the index @xmath23 of the relaxations ( [ primalj])-([dualj ] ) , the better the values @xmath279 and @xmath280 .",
    "of course , * algo 2 * can also be used when @xmath32 is not convex .",
    "however , it may happen that at some stage @xmath10 , the semidefinite relaxation ( [ primalj ] ) may be infeasible because @xmath281 is infinite for some values of @xmath282 .",
    "this is because the feasible set @xmath283 in ( [ kjx ] ) may be disconnected .",
    "we report on preliminary computational experiments on some non convex np - hard optimization problems .",
    "we have tested the algorithms on a set of difficult global optimization problems taken from floudas et al .",
    "@xcite . to solve the semidefinite programs involved in * algo 1 * and in * algo 2 * , we have used the gloptipoly software @xcite that implements the hierarchy of semidefinite relaxations defined in ( * ? ? ?",
    "* ( 4.5 ) ) .",
    "those problems are taken from @xcite .",
    "the set @xmath32 is a convex polytope and the function @xmath20 is a nonconvex quadratic polynomial @xmath284 for some real symmetric matrix @xmath285 and vector @xmath286 . in table",
    "i one displays the problem name , the number @xmath67 of variables , the number @xmath287 of constraints , the gobal optimum @xmath22 , the index @xmath23 of the semidefinite relaxation in * algo 2 * , the optimal value obtained using the output of * algo 2 * as initial guess in a local minimization algorithm of the matlab toolbox , and the associated relative error . as recommended in gloptipoly @xcite for numerical stability and precision , the problem data have been rescaled to obtain a polytope contained in the box @xmath288^n$ ] .",
    "as one may see , and excepted for problem 2.8c5 , the relative error is very small . for the last problem",
    "the relative error ( about @xmath289 ) is relatively high despite enforcing some extra upper and lower bounds @xmath290 , after reading the optimal solution .",
    "however , using @xmath68 as initial guess of the local minimization algorithm in matlab , one still finds the optimal value @xmath22 .",
    "[ tab1 ]    .*algo 2 * for convex set @xmath32 [ cols=\">,^,^,>,^,>,>\",options=\"header \" , ]",
    "first preliminary results are promising , even with small relaxation order @xmath23 .",
    "when the feasible set is non convex , it may become difficult to obtain a feasible solution and an interesting issue for further investigation is how to proceed when @xmath227 for @xmath228 in some open subinterval of @xmath173 ( proceeding by dichotomy on @xmath173 is one possiblity ) .",
    "floudas et al . , _ handbook of test problems in local and global optimization _ , kluwer academic publishers , dordrecht , 1999 .",
    "goemans , d.p .",
    "williamson , _ improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming _ , journal of the acm 42 , pp.1115 - 1145 , 1995 .",
    "d. henrion , j. b. lasserre , j. lofberg , _ gloptipoly 3 : moments , optimization and semidefinite programming _ , optim .",
    "methods and softw .",
    "761779 , 2009 .",
    "lasserre , _ global optimization with polynomials and the problem of moments _ , siam j. optim.11 , pp .",
    "796817 , 2001 . j.b .",
    "lasserre , _ polynomial programming : lp - relaxations also converge _ , siam j. optim .",
    "15 , pp . 383393 , 2004 .",
    "lasserre , _ a `` joint+marginal '' approach to parametric polynomial optimization _ , siam j. optim .",
    "1995 - 2022 , 2010 ."
  ],
  "abstract_text": [
    "<S> we present a new algorithm for solving a polynomial program @xmath0 based on the recent `` joint + marginal '' approach of the first author for parametric polynomial optimization . </S>",
    "<S> the idea is to first consider the variable @xmath1 as a _ parameter _ and solve the associated @xmath2-variable ( @xmath3 ) problem @xmath4 where the parameter @xmath1 is fixed and takes values in some interval @xmath5 , with some probability @xmath6 uniformly distributed on @xmath7 . </S>",
    "<S> then one considers the hierarchy of what we call `` joint+marginal '' semidefinite relaxations , whose duals provide a sequence of univariate polynomial approximations @xmath8 that converges to the optimal value function @xmath9 of problem @xmath4 , as @xmath10 increases . then with @xmath10 fixed _  priori _ , one computes @xmath11 which minimizes the univariate polynomial @xmath12 on the interval @xmath7 , a convex optimization problem that can be solved via a single semidefinite program . </S>",
    "<S> the quality of the approximation depends on how large @xmath10 can be chosen ( in general for significant size problems @xmath13 is the only choice ) . </S>",
    "<S> one iterates the procedure with now an @xmath14-variable problem @xmath15 with parameter @xmath16 in some new interval @xmath17 , etc . so as to finally obtain a vector @xmath18 . </S>",
    "<S> preliminary numerical results are provided . </S>"
  ]
}