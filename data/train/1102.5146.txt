{
  "article_text": [
    "consider a scenario where a large volume of data is collected on a daily basis : for example , sales records in a retailer , or network activity in a telecoms company .",
    "this activity will be archived in a warehouse or other storage mechanism , but the size of the data is too large for data analysts to keep in memory . rather than go out to the full archive for every query , it is natural to retain accurate summaries of each data table , and use these queries for data exploration and analysis , reducing the need to read through the full history for each query .",
    "since there can be many tables ( say , one for every day at each store , in the retailer case , or one for every hour and every router in the network case ) , we want to keep a very compact summary of each table , but still guarantee accurate answers to any query .",
    "the summary allows approximate processing of queries , in place of the original data ( which may be slow to access or even no longer available ) ; it also allows fast ` previews ' of computations which are slow or resource hungry to perform exactly .",
    "[ eg : motivate ] as a motivating example , consider network data in the form of ip flow records .",
    "each record has a source and destination ip address , a port number , and size ( number of bytes ) .",
    "ip addresses form a natural hierarchy , where prefixes or sets of prefixes define the ranges of interest .",
    "port numbers indicate the generating application , and related applications use ranges of port numbers .",
    "flow summaries are used for many network management tasks , including planning routing strategies , and traffic anomaly detection .",
    "typical ad hoc analysis tasks may involve estimating the amount of traffic between different subnetworks , or the fraction of voip traffic on a certain network .",
    "resources for collection , transport , storage and analysis of network measurements are expensive ; therefore , structure - aware summaries are needed by network operators to understand the behavior of their network .",
    "such scenarios have motivated a wealth of work on data summarization and approximation.there are two main themes : methods based on random sampling , and algorithms that build more complex summaries ( often deterministic , but also randomized ) .",
    "both have their pros and cons .",
    "sampling is fast and efficient , and has useful guaranteed properties .",
    "dedicated summaries can offer greater accuracy for the kind of range queries which are most common over large data , albeit at a greater cost to compute , and providing less flexibility for other query types .",
    "our goal in this work is to provide summaries which combine the best of both worlds : fast , flexible summaries which are very accurate for the all - important range queries . to attain this goal",
    ", we must understand existing methods in detail to see how to improve on their properties .",
    "summaries which are based on random sampling allow us to build ( unbiased ) estimates of properties of the data set , such as counts of individual identifiers ( `` keys '' ) , sums of weights for particular subsets of keys , and so on , all specified after the data has been seen .",
    "having high - quality estimates of these primitives allows us to implement higher - level applications over samples , such as computing order statistics over subsets of the data , heavy hitters detection , longitudinal studies of trends and correlations , and so on .",
    "summarization of items with weights traditionally uses poisson sampling , where each item is sampled independently .",
    "the approach which sets the probability of including an item in the sample to be proportional to its weight ( ipps )  @xcite enables us to use the horvitz - thompson estimator  @xcite , which minimizes the sum of per - item variances .",
    "`` '' samples  @xcite improve on poisson samples in that the sample size is fixed and they are more accurate on subset - sum queries .",
    "in particular  samples have _ variance optimality _ : they achieve variance over the queries that is provably the smallest possible for any sample of that size .    since sampling is simple to implement and flexible to use , it is the default summarization method for large data sets .",
    "samples support a rich class of possible queries directly , such as those mentioned in example  [ eg : motivate ] : evaluating the query over the sampled data ( with appropriately scaled weights ) usually provides an unbiased , low variance estimate of the true answer , while not requiring any new code to be written .",
    "these summaries provide not only estimates of aggregate values but also a representative sample of keys that satisfy a selection criteria .",
    "the fact that estimates are unbiased also means that relative error decreases for queries that span multiple samples or larger subsets and the estimation error is governed by exponential tail bounds : the estimation error , in terms of the number of samples from any particular subset , is highly concentrated around the square root of the expectation .",
    "we observe , however , that traditionally sampling has neglected the inherent structure that is present , and which is known before the data is observed .",
    "that is , data typically exists within a well - understood schema that exhibits considerable structure .",
    "common structures include _ order _ where there is a natural ordering over keys ; _ hierarchy _ where keys are leaves within a hierarchy ( e.g. geographic hierarchy , network hierarchy ) ; and combinations of these where keys are multi - dimensional points in a _",
    "product structure_. over such data , queries are often _ structure - respecting_. for example , on ordered data with @xmath0 possible key - values , although there are @xmath1 possible subset - sum queries , the most relevant queries may be the @xmath2 possible range queries . in a hierarchy",
    ", relevant queries may correspond to particular nodes in the hierarchy ( geographic areas , ip address prefixes ) , which represent @xmath3 possible ranges . in a product structure ,",
    "likely queries are boxes  intersections of ranges of each dimension .",
    "this is observed in example  [ eg : motivate ] : the queries mentioned are based on the network hierarchy .",
    "while samples have been shown to work very well for queries which resemble the sums of _ arbitrary _ subsets of keys , they tend to be less satisfactory when restricted to range queries .",
    "given the same summary size , samples can be out - performed in accuracy by dedicated methods such as ( multi - dimensional ) histograms  @xcite , wavelet transforms  @xcite , and geometric summaries  @xcite including the popular q - digest  @xcite .",
    "these dedicated summaries , however , have inherent drawbacks : they primarily support queries that are sum aggregates over the original weights , and so other queries must be expressed in terms of this primitive .",
    "their accuracy rapidly degrades when the query spans multiple ranges  a limitation since natural queries may span several ( time , geographic ) ranges within the same summary and across multiple summaries .",
    "dedicated summaries do not provide `` representative '' keys of selected subsets , and require changes to existing code to utilize .",
    "of most concern is that they can be very slow to compute , requiring a lot of i / o ( especially as the dimensionality of the data grows ) : a method which gives a highly accurate summary of each hour s data is of little use if it takes a day to build !",
    "lastly , the quality of the summary may rely on certain structure being present in the data , which is not always the case .",
    "while these summaries have shown their value in efficiently summarizing one - dimensional data ( essentially , arrays of counts ) , their behavior on even two - dimensional data is less satisfying : troubling since this is where accurate summaries are most needed .",
    "for example , in the network data example , we are often interested in the traffic volume between ( collections of ) various source and destination ranges .",
    "motivated by the limitations of dedicated summaries , and the potential for improvement over existing ( structure - oblivious ) sampling schemes , we aim to design sampling schemes that are both  and _ structure - aware_. at the same time , we aim to match the accuracy of deterministic summaries on range sum queries and retain the desirable properties of existing sample - based summaries : unbiasedness , tail bounds on arbitrary subset - sums , flexibility and support for representative samples , and good i / o performance .",
    "we introduce a novel algorithmic sampling framework , which we refer to as _",
    "probabilistic aggregation _ , for deriving  samples .",
    "this framework makes explicit the freedom of choice in building a  summary which has previously been overlooked . working within this framework , we design _ structure - aware _  sampling schemes which exploit this freedom to be much more accurate on ranges than their structure - oblivious counterparts .",
    "= 1em    for hierarchies , we design an efficient algorithm that constructs  summaries with bounded `` range discrepancy '' . that is , for any range , the number of samples deviates from the expectation by less than @xmath4 .",
    "this scheme has the minimum possible variance on ranges of any unbiased sample - based summary .    for ordered sets ,",
    "where the ranges consist of all intervals , we provide a sampling algorithm which builds a  summary with range discrepancy less than @xmath5 .",
    "we prove that this is the best possible for any  sample .    for @xmath6-dimensional datasets , we propose sampling algorithms where the discrepancy between @xmath7 , the expected number of sample points in the range @xmath8 , and the actual number is @xmath9 @xmath10 , where @xmath11 is the sample size .",
    "this improves over structure - oblivious random sampling , where the corresponding discrepancy is @xmath12 .",
    "discrepany corresponds to error of range - sum queries but sampling has an advantage over other summaries with similar error bounds : the error on queries @xmath13 which span multiple ranges grows linearly with the number of ranges for other summaries but has square root dependence for samples .",
    "moreover , for samples the expected error never exceeds @xmath14 ( in expectation ) regardless of the number of ranges .",
    "* construction cost .",
    "* for a summary structure to be effective , it must be possible to construct quickly , and with small space requirements .",
    "our main - memory sampling algorithms perform tasks such as sorting keys or ( for multidimensional data ) building a kd - tree .",
    "we propose even cheaper alternatives which perform two read - only passes over the dataset using memory that depends on the desired summary size @xmath11 ( and independent of the size of the data set ) . when the available memory is @xmath15 , we obtain a  sample that with high probability @xmath16 is close in quality to the algorithms which store and manipulate the full data set .    *",
    "empirical study . * to demonstrate the value of our new structure - aware sampling algorithms , we perform experiments comparing to popular summaries , in particular the wavelet transform  @xcite , @xmath17-approximations  @xcite , randomized sketches  @xcite and to structure - oblivious random sampling .",
    "these experiments show that it is possible to have the best of both worlds : summaries with equal or better accuracy than the best - in - class , which are flexible and dramatically more efficient to construct and work with .",
    "this section introduces the `` probabilistic aggregation '' technique . for more background ,",
    "see the review of core concepts from sampling and summarization in appendix  [ prelim : sec ] .",
    "our data is modeled as a set of ( key , weight ) pairs : each key @xmath18 has weight @xmath19 .",
    "a sample is a random subset @xmath20 .",
    "a sampling scheme is ipps when , for expected sample size @xmath11 and derived threshold @xmath21 , the sample includes key @xmath22 with probability @xmath23 .",
    "ipps can be acheived with _",
    "poisson _ sampling ( by including keys independently ) or  sampling , which allows correlations between key inclusions to achieve improved variance and fixed sample size of exactly @xmath11 .",
    "there is not a unique  sampling scheme , but rather there is a large family of  sampling distributions : the well - known `` reservoir sampling '' is a special case of  on a data stream with uniform weights .",
    "classic tail bounds , including chernoff bounds , apply both to  and poisson sampling .",
    "structure is specified as a _ range space _",
    "@xmath24 with @xmath25 being the key domain and _ ranges _ @xmath26 that are subsets of @xmath25 . the _ discrepancy",
    "_ @xmath27 of a sample @xmath28 on a range @xmath29 is the difference between the number of sampled keys @xmath30 and its expectation @xmath7 .",
    "we use @xmath31 to denote the maximum discrepancy over all ranges @xmath26 .",
    "disrepancy @xmath31 means that the error of range - sum queries is at most @xmath32 .",
    "if a sample is poisson or , it follows from chernoff bounds that the expected discrepancy is @xmath12 and ( from bounded vc dimension of our range spaces ) that the maximum range discrepancy is @xmath33 with probability @xmath34 . with structure - aware sampling , we aim for much lower discrepancy .    * defining probabilistic aggregation .",
    "* let @xmath35 be the vector of sampling probabilities .",
    "we can view a sampling scheme that picks a set of keys @xmath28 as operating on @xmath35 .",
    "vector @xmath35 is incrementally modified : setting @xmath36 to 1 means @xmath22 is included in the sample , while @xmath37 means it is omitted . when all entries are set to 0 or 1 ,",
    "the sample is chosen ( e.g. poisson sampling independently sets each entry to 1 with probability @xmath36 ) . to ensure a  sample ,",
    "the current vector @xmath38 must be a _",
    "probabilistic aggregate _ of the original @xmath35 .",
    "a random vector @xmath39^n$ ] is a _",
    "probabilistic aggregate _ of a vector @xmath40^n$ ] if the following conditions are satisfied :    * ( _ agreement in expectation _ ) @xmath41=p^{(0)}_i$ ] , * ( _ agreement in sum _ ) @xmath42 , and * ( _ inclusion - exclusion bounds _ ) @xmath43   & \\quad \\leq \\quad \\prod_{i\\in j } p^{(0)}_i \\\\",
    "\\mbox{(e ) : } & & { \\textsf{e}}[\\prod_{i\\in j } ( 1-p^{(1)}_i ) ] & \\quad \\leq \\quad \\prod_{i\\in j } ( 1-p^{(0)}_i ) \\ .\\end{aligned}\\ ] ]    @xmath44 @xmath45 ; @xmath46 @xmath47 ; @xmath48 @xmath49 ; @xmath50 @xmath51 ; @xmath52 * return * @xmath35    we obtain  samples by performing a sequence of probabilistic aggregations , each setting at least one of the probabilities to 1 or 0 . in appendix  [ probaggapp ]",
    "we show that probablistic aggregations are transitive and that set entries remain set .",
    "thus , such a process must terminate with a  sample .",
    "* pair aggregation . *",
    "our summarization algorithms perform a sequence of simple aggregation steps which we refer to as _ pair aggregations _ ( algorithm  [ pairagg : alg ] ) .",
    "each pair aggregation step modifies only two entries and sets at least one of them to @xmath53 .",
    "the input to _",
    "pair aggregation _ is a vector @xmath35 and a pair @xmath54 with each @xmath55 .",
    "the output vector agrees with @xmath35 on all entries except @xmath54 and one of the entries @xmath54 is set to @xmath56 or @xmath4 .",
    "it is not hard to verify , separately considering cases @xmath57 and @xmath58 , that pair - aggregate@xmath59 correctly computes a probabilistic aggregate of its input , and hence the sample is .",
    "pair aggregation is a powerful primitive .",
    "it produces a sample of size exactly @xmath60 .",
    ". this can be ensured ( deterministically ) by choosing @xmath61 as described in algorithm [ get_tau_k : alg ] . ]",
    "observe that the choice of which pair @xmath54 to aggregate at any point can be arbitrary  and the result is still a  sample .",
    "this observation is what enables our approach .",
    "we harness this freedom in pair selection to obtain  samples that are structure aware : intuitively , by choosing to aggregate pairs that are `` close '' to each other with respect to the structure , we control the range impact of the `` movement '' of probability mass .",
    "we use pair aggregation to make sampling structure - aware by describing ways to pick which pair of items to aggregate at each step . for now , we assume the data fits in main - memory , and our input is the list of keys and their associated ipps probabilities @xmath36 .",
    "we later discuss the case when the data exceeds the available memory . for hierarchy structures ( keys @xmath25",
    "are associated with leaves of a tree and @xmath26 contains all sets of keys under some internal node ) we show how to obtain  samples with ( optimal ) maximum range discrepancy @xmath62 .",
    "there are two special cases of hierarchies : ( i ) _ disjoint ranges _",
    "( where @xmath26 is a partition of @xmath25)captured by a flat 2-level hierarchy with parent nodes corresponding to ranges and ( ii )  _ order _ where there is a linear order on keys and @xmath26 is the set of all prefixes  the corresponding hierarchy is a path with single leaf below each internal node . for order structures where @xmath26 is the set of `` intervals '' ( all consecutive sets of keys )",
    "we show that there is always a  sample with maximum range discrepancy @xmath63 and prove that this is the best possible .",
    "* disjoint ranges : * pair selection picks pairs where both keys belong to the same range @xmath8 .",
    "when there are multiple choices , we may choose one arbitrarily .",
    "only when there are none do we select a pair that spans two different ranges ( arbitrarily if there are multiple choices ) .",
    "* hierarchy : * pair selection picks pairs with lowest @xmath64 ( lowest common ancestor ) .",
    "that is , we pair aggregate @xmath65 if there are no other pairs with an @xmath64 that is a descendant of @xmath66 .           the kd - hierarchy algorithm ( algorithm [ kdhierarchy : alg ] ) aims to minimize the discrepancy within a product space .",
    "figure  [ fig : kdpart_all][fig : kdpart ] shows a two - dimensional set of @xmath67 keys that are uniformly weighted , with sampling probabilities @xmath68 , and the corresponding kd - tree : a balanced binary tree of depth 6 .",
    "the cuts alternate vertically ( red tree nodes ) and horizontally ( blue nodes ) .",
    "right - hand children in tree correspond to right / upper parts of cuts and left - hand children to left / lower parts .",
    "we now analyze the resulting summary , based on the properties of the space partitioning performed by the kd - tree .",
    "we use @xmath69 to refer interchangeably to a node in the tree and the hyperrectangle induced by node @xmath69 in the tree .",
    "a node @xmath69 at depth @xmath6 in the tree has probability mass @xmath70 .",
    "we refer to the set of minimum depth nodes that satisfy @xmath71 as s - leaves ( for _ super leaves _ ) ( @xmath69 is an s - leaf iff @xmath71 and its immediate ancestor @xmath72 has @xmath73 .",
    "the depth of an s - leaf ( and of the hierarchy when truncated at s - leaves ) is at most @xmath74 .",
    "consider the hierarchy level - by - level , top to bottom .",
    "each level where the axis was not perpendicular to the hyperplane at most doubles the number of nodes that intersect the hyperplane .",
    "levels where the partition axis is perpendicular to the hyperplane do not increase the number of intersecting nodes . because axes were used in a round - robin fashion , the fraction of levels that can double the number of intersecting nodes is @xmath76 .",
    "hence , when we reach the s - leaf level , the number of intersecting nodes is at most @xmath77 .",
    "an immediate corollary is that the boundary of an axis - parallel box @xmath8 may intersect at most @xmath78 s - leaves .",
    "we denote by @xmath79 this set of boundary s - leaves .",
    "let @xmath80 be a minimum size collection of nodes in the hierarchy such that no internal node contains a leaf from @xmath79 .",
    "informally , @xmath80 consists of the ( maximal ) hyperrectangles which are fully contained in @xmath8 or fully disjoint from @xmath8 .",
    "figure  [ fig : kdpart_all][fig : kdpart_query ] illustrates a query rectangle @xmath8 ( dotted red line ) over the data set .",
    "the maximal interior nodes contained in @xmath8 ( @xmath81 ) are marked in solid colors ( and green circles in the tree layout ) and the boundary s - leaves @xmath79 in light stripes ( magenta circles in the tree layouts ) .",
    "for example , the magenta rectangle corresponds to the r - l - l - r path .",
    "each node in @xmath83 must have a sibling such that the sibling , or some of its descendants , are in @xmath79 .",
    "if this is not the case , then the two siblings can be replaced by their parent , decreasing the size of @xmath80 , which contradicts its minimality .",
    "we bound the size of @xmath80 by bounding the number of potential siblings .",
    "the number of ancestors of each boundary leaf is at most the depth which is @xmath84 .",
    "thus , the number of potential siblings is at most the number of boundary leaves times the depth . by substituting a bound on @xmath85",
    ", we obtain the stated upper bound .",
    "these lemmas allow us to bound the estimation error , by applying lemma  [ hierarcyunionbound : lemma ] .",
    "that is , for each @xmath86 such that @xmath87 we have a 0/1 random variable that is 1 with probability @xmath88 and is @xmath56 otherwise ( the value is @xmath56 if @xmath69 includes @xmath89 samples and @xmath4 otherwise ) . for each @xmath90 , we have a random variable that is 1 with probability @xmath91 .",
    "this is the probability that @xmath28 contains one key from @xmath92 ( @xmath28 can contain at most one key from each s - leaf ) .",
    "the sample is  over these random variables with"
  ],
  "abstract_text": [
    "<S> in processing large quantities of data , a fundamental problem is to obtain a summary which supports approximate query answering . </S>",
    "<S> random sampling yields flexible summaries which naturally support subset - sum queries with unbiased estimators and well - understood confidence bounds .    </S>",
    "<S> classic sample - based summaries , however , are designed for arbitrary subset queries and are oblivious to the structure in the set of keys . </S>",
    "<S> the particular structure , such as hierarchy , order , or product space ( multi - dimensional ) , makes _ </S>",
    "<S> range queries _ much more relevant for most analysis of the data .    </S>",
    "<S> dedicated summarization algorithms for range - sum queries have also been extensively studied . </S>",
    "<S> they can outperform existing sampling schemes in terms of accuracy on range queries per summary size . </S>",
    "<S> their accuracy , however , rapidly degrades when , as is often the case , the query spans multiple ranges . </S>",
    "<S> they are also less flexible  being targeted for range sum queries alone  and are often quite costly to build and use .    </S>",
    "<S> in this paper we propose and evaluate variance optimal sampling schemes that are _ structure - aware_. these summaries improve over the accuracy of existing _ structure - oblivious _ sampling schemes on range queries while retaining the benefits of sample - based summaries : flexible summaries , with high accuracy on both range queries and arbitrary subset queries .    </S>",
    "<S> [ theorem]lemma [ theorem]definition </S>"
  ]
}