{
  "article_text": [
    "it has been shown that _ planning _ can dramatically improve convergence in reinforcement learning ( rl )  @xcite . however , most rl methods that explicitly use planning that have been proposed are value ( or @xmath0-value ) based methods , such as _ dyna - q _ or _",
    "prioritized sweeping_.    recently , much attention is directed to so - called _ policy - gradient _ methods that improve their policy directly by calculating the derivative of the future expected reward with respect to the policy parameters .",
    "gradient based methods are believed to be more advantageous than value - function based methods in huge state spaces and in pomdp settings .",
    "probably the first gradient based rl formulation is class of reinforce algorithms of    williams  @xcite .",
    "other more recent methods are , e.g. ,  @xcite .",
    "our approach of deriving the gradient has the flavor of  @xcite who derive the gradient using future state probabilities .",
    "our novel contribution in this paper is to combine gradient - based learning with explicit planning .",
    "we introduce `` gradient - based reinforcement planning '' ( grep ) that improves a policy _ before _ actually interacting with the environment .",
    "we derive the formulas for the exact policy gradient and confirm our ideas in numerical experiments .",
    "grep learns the action probabilities of a probabilistic policy for discrete problem .",
    "while we will illustrate grep with a small mdp maze , it may be used for the hidden state in pomdps .",
    "let us denote the discrete space of _ states _ by @xmath1 .",
    "our belief on @xmath2 is decribed by a probability vector @xmath3 of which each element @xmath4 represents the probability of being in state @xmath5 .",
    "we also define a set of actions @xmath6 .",
    "the stochastic policy @xmath7 is represented by a matrix @xmath8 with elements @xmath9 , i.e. the conditional probability of action @xmath10 in state @xmath5 .",
    "is just given by @xmath11 and we do not use reparameterization in order to keep the analysis clear . ] furthermore , let environment @xmath12 be defined by transition matrices @xmath13 ( @xmath14 ) with elements @xmath15 , i.e. the transition probability to @xmath16 in state @xmath4 given action @xmath10 .",
    "now we define the _ projection matrix _ @xmath17 with elements @xmath18 important is that matrix @xmath17 is _ not _ modelling the transition probabilities of the environment , but models the induced transition probability using policy @xmath7 in environment @xmath12 .",
    "the induced transition probability @xmath19 is a weighted sum over actions @xmath10 of the transition probabilities @xmath20 with the policy parameters @xmath11 as the weights .",
    "using the projection matrix @xmath17 , states @xmath21 and @xmath22 are related as @xmath23 and therefore @xmath24 , where @xmath25 is the state probability distribution at @xmath26 .",
    "we can now define the _ expected state occupancy _ as @xmath27     = \\sum_{t=0}^\\infty \\gamma^t { { \\bfs}}_t    = \\sum_{t=0}^\\infty ( \\gamma { { \\bff}})^t { { \\bfs}}_0    = ( { { \\bfi } } - \\gamma { { \\bff}})^{-1 } { { \\bfs}}_0 \\label{eq : occupancy}\\ ] ] where @xmath28 is a discount factor in order to keep the sum finite . in the last step ,",
    "we have recognized the sum as the neumann representation of the inverse .",
    "notice that @xmath29 is a solution of the linear equation @xmath30 which is just the familiar bellman equation for the expected occupancy probability @xmath29 .      in _ reinforcement learning _",
    "( rl ) the objective is to maximize future reward .",
    "we define a reward vector @xmath31 in the same domain as @xmath3 . using the expected occupancy @xmath29",
    "the future expected reward @xmath32 is simply @xmath33 where @xmath34 is the scalar vector product.[multiblock footnote omitted ]    because @xmath29 is a solution of eq .",
    "[ eq : system ] it is dependent on @xmath17 which in turn depends on policy @xmath35 .",
    "given @xmath31 and @xmath25 , our task is to find the optimal @xmath36 such that @xmath32 is maximized , i.e. @xmath37    we can regard the calculation of the future expected reward as a composition of two operators @xmath38 which maps the transition matrix @xmath17 to the expected occupancy probabilities @xmath29 , and @xmath39 which maps the probabilities @xmath29 , given a reward distribution @xmath31 , to an expected reward value @xmath40 .",
    "a variation @xmath41 in the expected occupancy can be related to first order to a perturbation @xmath42 in the ( stochastic ) policy . to obtain the partial derivatives",
    "@xmath43 , we differentiate eq .",
    "[ eq : system ] with respect to @xmath44 and obtain : @xmath45 the right hand side of the equation is zero because we assume that @xmath25 is independent of @xmath44 . rearranging gives : @xmath46 where @xmath47 . from eq .",
    "[ eq : rl - error ] and eq .",
    "[ eq : diff1 ] , together with the chain rule , we obtain the gradient of the rl error with respect to the policy parameters @xmath44 : @xmath48 where @xmath49 means the adjoint operator of @xmath50 defined by @xmath51 .",
    "let us define : @xmath52 . while @xmath53 maps the initial state @xmath25 to the future expected state occupancy @xmath29 , its adjoint , @xmath54 , maps the reward vector @xmath31 back to _",
    "expected reward _",
    "the value of @xmath56 represents the ( pointwise ) expected reward in state @xmath4 for policy @xmath35 .   nor @xmath55 are probabilities because their 1-norm is generally not 1 . ]    finally , differentiating eq .",
    "[ eq : projection ] gives us @xmath57 .",
    "inserting this into eq .",
    "[ eq : gradient ] yields : @xmath58 in words , the gradient of @xmath32 with respect to policy parameter @xmath44 ( i.e. the probability of taking action @xmath59 in state @xmath4 ) is proportional to the expected occupancy @xmath60 times the weighted sum of expected reward @xmath61 over next states @xmath62 weighted by the transition probabilities @xmath63 .",
    "note that the gradient could also have been approximated using finite differences which would need at least @xmath64 field calculations . involves computing @xmath29 for @xmath17 and then perturbing a single @xmath65 in @xmath17 by a tiny amount @xmath66 and subsequently recomputing @xmath67 .",
    "then the derivative is approximated by @xmath68 . for a @xmath69 matrix @xmath17",
    ", one would need to repeat this for every element and would require a total upto @xmath64 calculations of @xmath29 . ]",
    "the adjoint method is much more efficient and needs only _ two _ field calculations .",
    "once we have the gradient @xmath70 , improving policy @xmath35 is now straight forward using gradient ascent or we can also use more sophisticated gradient - based methods such as nonlinear conjugate gradients ( as in  @xcite ) .",
    "the optimization is nonlinear because @xmath29 and @xmath31 themselves depend on the current estimate of @xmath35 .",
    "we will introduce two algorithms that incorporate our ideas of gradient - based reinforcement planning .",
    "the first algorithms describes an off - line planning algorithm that finds the optimal policy but assumes that the environment transition probabilities are known .",
    "the second algorithm is an online version that could cope with unknown environments .",
    "if the environment transition probabilities @xmath63 are known , the agent may improve its policy using grep .",
    "our offline grep planning algorithm consist of two steps :    1 .",
    "_ plan ahead : _ compute the policy gradient @xmath70 in eq .",
    "[ eq : gradient ] and improve current policy @xmath71 where @xmath72 is a suitable step size parameter ; for efficiency we can also perform a linesearch on @xmath72 .",
    "2 .   _ evaluate policy : _ repeat above until policy is optimal .",
    "matrix @xmath35 describes a probabilistic policy .",
    "we define the _ maximum probable policy _ ( mpp ) to be the deterministic policy by taking the maximum probable action at each state .",
    "it is not obvious that the mpp policy will converge to the global optimal solution but we expect mpp at least to be near - optimal .       toy maze with start at left and goal at right side",
    ". center : plot of expected occupancy @xmath29 .",
    "right : plot of expected reward @xmath55 .",
    "white corresponds to higher probability .",
    "[ blurring is due to visualisation only ] .",
    ", title=\"fig:\",scaledwidth=14.7% ]   toy maze with start at left and goal at right side",
    ". center : plot of expected occupancy @xmath29 .",
    "right : plot of expected reward @xmath55 .",
    "white corresponds to higher probability .",
    "[ blurring is due to visualisation only ] .",
    ", title=\"fig:\",scaledwidth=15.5% ]   toy maze with start at left and goal at right side . center : plot of expected occupancy @xmath29 .",
    "right : plot of expected reward @xmath55 .",
    "white corresponds to higher probability .",
    "[ blurring is due to visualisation only ] .",
    ", title=\"fig:\",scaledwidth=15.5% ]        we performed some numerical experiments using offline grep .",
    "our test problem was a pure planning task in a @xmath73 toy maze ( see fig .",
    "[ fig : maze ] ) where the probabilistic policy @xmath35 represents the probability of taking a certain action at a certain maze position .",
    "the same figure also shows typical solutions for the quantities @xmath29 and @xmath55 , i.e. the expected occupancy and expected reward respectively ( for certain @xmath35 ) .    after each grep iteration , i.e. after each gradient calculation and @xmath35 update , we checked the obtained policy by running 20 simulations using the current value of @xmath35 . the probability weighted ( pw ) policy selects action @xmath10 at state @xmath5 proportional to @xmath44 , while the annealed pw policy uses an annealing factor of @xmath74 ; we also simulated the mpp solution .",
    "figure  [ fig : reward ] shows the average simulated path length versus grep iteration of the pw , the annealed pw policy and the derived mpp policy . in the left plot the initial policy @xmath35 was taken uniform .",
    "the right plot in the same figure shows the simulated path lengths from a random policy ; also here the mpp finds the optimal solution but slightly later .",
    "we see from the figure that in both cases the probability - weighted ( pw ) policy is improving during the grep iterations . however , the convergence is very slow which shows the severe non - linearity of the problem .",
    "the annealed pw does perform better than pw .",
    "finally , we see that mpp finds the optimal solution quickly within a few iterations . using dijkstra s method",
    ", we confirmed that the found mpp policy was in agreement with the global shortest path solution .",
    "the account below desribe an idea to use grep when the environment is not known beforehand .",
    "the steps actually interleave `` kalman filter''-like estimation of the unknown environment transition probabilities with the explicit planning of grep .",
    "in fact , it also includes a step to estimate a possibly unknown ( linear ) sensor mapping .",
    "apart from the policy matrix @xmath35 , we need to estimate also the ( environment ) transition probabilities @xmath63 and possibly sensor matrix @xmath75 .",
    "we can optimize for all parameters by iteratively ascending to their conditional mode .",
    "the conditional maximizing steps are easy :    1 .",
    "_ plan ahead : _ compute the policy gradient @xmath70 in eq .",
    "[ eq : gradient ] and improve current policy @xmath71 where @xmath72 is a suitable step size parameter ; for efficiency we can also perform a linesearch .",
    "after , we need to renormalize the columns of @xmath35 .",
    "see note below on policy regularization .",
    "_ select action : _ given state estimate @xmath76 , draw an action @xmath10 from the policy according to : @xmath77 and receive reward @xmath40 and estimate new state @xmath22 .",
    "3 .   _ estimate state : _ observe @xmath78 and estimate @xmath22 using @xmath79 assuming gaussian noise for observations and state estimates we obtain : @xmath80 where @xmath75 is the _ sensor matrix _ that maps internal state @xmath3 to observations @xmath81 .",
    "matrices @xmath82 and @xmath83 are the inverse covariance ( or so called _ precisions _ ) of state @xmath3 and observation @xmath81 respectively .",
    "_ estimate sensors : _ in case also sensor matrix @xmath75 is unknown , we have to perform an additional estimation step for @xmath75 .",
    "this is common step in the standard kalman formulation .",
    "estimate environment : _ given action @xmath10 and reward @xmath40 , we update the reward vector @xmath84 and the environment transition probabilities @xmath85 or if @xmath86 does not exist we can use @xmath87 where @xmath88 , and reestimate the environment transition probabilities @xmath89 after the update , one should set entries in @xmath90 that corresponds to physically impossible transitions to zero .",
    "after , we need to renormalize the columns of @xmath13 .",
    "it is important to note that given @xmath21 and @xmath22 transition matrix @xmath13 is conditionally independent of the policy @xmath35 .",
    "that is to say , we can obtain an accurate model of the environment using , e.g. , just a random walk .",
    "repeat 1 .    to draw a picture of what is happening . in the planning stage , based on the current ( and maybe not accurate ) environment model",
    ", the agent tries to improve its current policy by planning ahead using the gradient in eq .",
    "[ eq : gradient - p ] . remember that the gradient involves simulating paths from the current state and adjoint paths from the goal .",
    "in the action stage the agent samples an action from its policy",
    ". then the agent senses the new state and updates its environment model using this new information .",
    "notice that policy improvement is not done `` backwards '' as traditionally is done in dp methods but `` forward '' by planning ahead .",
    "we have tacitly assumed that @xmath29 and @xmath55 are computed using the same discount factor @xmath28 .",
    "however , we could introduce separate parameters @xmath91 and @xmath92 which effectively assigns a different `` forward time window '' for @xmath29 and a `` backward time window '' for @xmath55 .",
    "in fact when @xmath93 we have a `` one - step - look - ahead '' . alternatively ,",
    "in the limit of @xmath94 we obtain a gradient for a greedy policy that maximize only `` immediate reward '' .",
    "how both parameters affect grep s performance is a topic for future research .",
    "the above suggests that grep can be viewed as a generalization to `` one - step - look - ahead '' policy improvement .",
    "in fact , a `` one - step - look - ahead '' improvement rule using can be obtained for @xmath95 simply by taking @xmath96 in eq .",
    "[ eq : gradient - p ]",
    ". such an approach would be `` policy greedy '' in a sense that it updates the policy only locally .",
    "we expect grep to perform better because it updates the policy more globally ; whether this in fact improves grep is also a remaining issue for future research .",
    "the interleaving of grep with a kalman - like estimation procedure of the environment could handle a variety of interesting problems such as planning in pomdp environments .",
    "we must mention that appropriate reparameterization of the stochastic policy , e.g. using a boltzman distribution , could improve the convergence .",
    "we have not pursued this further .",
    "we have introduced a learning method called `` gradient - based reinforcement planning '' ( grep ) .",
    "grep needs a model of the environment and plans ahead to improve its policy _ before _ it actually acts in the environment .",
    "we have derived formulas for the exact policy gradient .",
    "numerical experiments suggest that the probabilistic policy indeed converges to an optimal policy  but quite slowly .",
    "we found that ( at least in our toy example ) the optimal solution can be found much faster by annealing or simply by taking the most probable action at each state .",
    "further work will be to incorporate grep in online rl learning tasks where the environment parameters , i.e. transition probabilities @xmath63 , are unknown and have to be learned . while an analytical solution for @xmath55 and @xmath29 are only viable for small problem sizes , for larger problems we probably need to investigate monte carlo or dp methods .",
    "baird , l.  c. ( 1998 ) .",
    "gradient descent for general reinforcement learning . .",
    "baxter , j. , & bartlett , p. ( 1999 ) .",
    "( technical report ) .",
    "research school of information sciences and engineering , australian national university .",
    "difilippo , f.  c. , goldstein , m. , worley , b.  a. , & ryman , j.  c. ( 1996 ) .",
    "adjoint monte carlo methods for radiotherapy treatment planning . , _ 74 _ , 1416 .",
    "ng , a. , parr , r. , & koller , d. ( 1999 ) .",
    "policy search via density estimation . .",
    "schmidhuber , j. ( 1990 ) .",
    "an on - line algorithm for dynamic reinforcement learning and planning in reactive environments .",
    "253258 ) .",
    "sutton , r.  s. , & barto , a.  g. ( 1998 ) . .",
    "press , cambridge .",
    "sutton , r.  s. , mcallester , d. , singh , s. , & mansour , y. ( 2000 ) .",
    "policy gradient methods for reinforcement learning with function approximation . .",
    "williams , r.  j. ( 1992 ) .",
    "simple statistical gradient - following algorithms for connectionist reinforcement learning .",
    ", _ 8 _ , 229256 .",
    "in deterministic environments where the state - action pair @xmath97 uniquely leads to a state @xmath16 , i.e. @xmath98 the projection @xmath17 is solely determined by the policy @xmath29 , and _",
    "vice versa_. we refer to this as the case of _ implicit policy _ because the policy is implicitly implied in the induced transition probability @xmath99 .    in such environments we can suffice to solve for @xmath17 directly and omit parameterization through @xmath29 .",
    "from eq .",
    "[ eq : projection ] we see that @xmath100 and using a similar derivation as we have done for @xmath101 , it can be shown that the gradient of @xmath32 with respect to @xmath17 is given by @xmath102 an important point must be mentioned . in most cases",
    "many elements @xmath99 are zero , representing an absent transition between @xmath103 and @xmath104 . naively updating @xmath17 using",
    "the full gradient @xmath70 would incur complete fill - in of @xmath17 which is in most cases not desirable or even physically incorrect .",
    "therefore , one must check the gradient each time and set impossible transition probabilities to zero . we will refer to this `` heuristically corrected '' gradient as @xmath105 .",
    "also , after each update , we have to renormalize the columns of @xmath17 . the rank - one update in eq .",
    "[ eq : rank - one ] is interesting because it provides an efficient means of calculating the inverse in eq .",
    "[ eq : occupancy ] .",
    "in our example , we calculated @xmath29 and @xmath55 in eq.[eq : occupancy ] by linear programming . for large state spaces the matrix inversion quickly becomes too computationally intensive and probably traditonal dynamic programming based methods would be more efficient .",
    "instead , we investigated to use monte carlo ( mc ) simulation .",
    "we use _ forward sampling _ to approximate the expected state occupancies in @xmath29 and use , so - called , _ adjoint monte carlo sampling _  @xcite to estimate the adjoint reward @xmath55 .",
    "adjoint mc simulation of is far more efficient than would we have estimated each @xmath56 by a separate mc run . by performing the simulation backward from @xmath31",
    ", we obtain all values of @xmath55 using only a single mc run .     toy maze .",
    "the agent is at ( 0,1 ) and targets ( 5,4 ) .",
    "( a ) expected occupancy , ( b ) adjoint probability , and ( c ) normalized policy gradient for @xmath106 , @xmath107 , @xmath108 .",
    "each vector is computed as @xmath109 where @xmath110 is the unit vector along the state change induced by action @xmath59.__,title=\"fig:\",scaledwidth=28.0% ]   toy maze .",
    "the agent is at ( 0,1 ) and targets ( 5,4 ) .",
    "( a ) expected occupancy , ( b ) adjoint probability , and ( c ) normalized policy gradient for @xmath106 , @xmath107 , @xmath108 .",
    "each vector is computed as @xmath109 where @xmath110 is the unit vector along the state change induced by action @xmath59.__,title=\"fig:\",scaledwidth=28.0% ]   toy maze .",
    "the agent is at ( 0,1 ) and targets ( 5,4 ) .",
    "( a ) expected occupancy , ( b ) adjoint probability , and ( c ) normalized policy gradient for @xmath106 , @xmath107 , @xmath108 .",
    "each vector is computed as @xmath109 where @xmath110 is the unit vector along the state change induced by action @xmath59.__,title=\"fig:\",scaledwidth=28.0% ] + ( a ) ( b ) ( c )    fig .",
    "[ fig : mc - gradientplots ] shows the mc approximations of @xmath29 and @xmath55 . on the right of the same figure ,",
    "we have plotted the computed policy - gradient based on mc estimates using a minimum number of @xmath111 samples . to compare them with the exact gradient , we calculated the exact values of @xmath29 and @xmath55 by inverting the linear system . for larger number of samples ,",
    "the gradient vector do indeed point more strongly towards the goal .",
    "an important feature of general monte carlo methods is that they automatically concentrate their sampling to the important regions of the parameter space mostly proportional to the posterior or the likelihood . for our purpose of sampling the gradient , to even more concentrate the sampling density towards the regions of large gradient values ,",
    "we have tried to apply _",
    "annealing_. to sample from a density @xmath112 we may sample from the annealed function @xmath113 and reweight each sample with its importance weight @xmath114 . for @xmath115 ,",
    "the set of samples converges to the maximum probable gradient .    in conclusion",
    ", our approach of separately estimating @xmath55 and @xmath29 using mc and _ then _ ( elementwise ) multiply their solutions , does nt really brought clear advantages .",
    "if we could sample from the joint distribution @xmath116 ( i.e. elementwise product ) then mc would clearly turn out to be a very efficient method ."
  ],
  "abstract_text": [
    "<S> we introduce a learning method called `` gradient - based reinforcement planning '' ( grep ) . unlike traditional dp methods that improve their policy backwards in time , </S>",
    "<S> grep is a gradient - based method that plans ahead and improves its policy _ before _ it actually acts in the environment . </S>",
    "<S> we derive formulas for the exact policy gradient that maximizes the expected future reward and confirm our ideas with numerical experiments . </S>"
  ]
}