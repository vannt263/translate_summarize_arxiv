{
  "article_text": [
    "the belle experiment@xcite is the @xmath4-factory project at kek to study @xmath5 violation in @xmath4 meson system .",
    "the kekb accelerator@xcite is an asymmetric energy collider with 8 gev electron to 3.5 gev positron , operating at @xmath3 energies .",
    "the belle group has been taking data since june 1999 and has logged more than 160 fb@xmath1 of data until december 2003 .",
    "this means that we have the largest data sample of @xmath4 meson pairs around the @xmath3 energy region in the world .",
    "the kekb reached its design luminosity of 10@xmath6 @xmath7sec@xmath1 on may 2003 .",
    "the kekb performance is still being improved and we can accumulate integrated luminosity of about 800 pb @xmath1 per day recently .",
    "we have to promptly process all of beam data to provide them for user analyses . to do this , the dst production should be stable and the computing resources have to be used in an efficient way .",
    "the monte carlo ( mc ) data should be generated with statistics large enough to control experimental systematics . as a result , data size which we should handle is extremely huge and a mass storage system has to be used to avoid network traffic , and data management for entire data sets should be carefully done without loosing flexibility , for instance , any modification of data distributions .    provided a large amount of data sample , we have published a variety of physics results related to @xmath4 meson decays , which is highlighted by the first observation of the large @xmath5 violation in @xmath4 meson decays@xcite . the quick and stable data processing greatly contributed to this remarkable achievement .    in this paper",
    ", we will describe a detail of our computing system after a brief sketch of belle software utilities . in the next section ,",
    "how we proceed dst as well as mc productions will be mentioned , and then summary will be given .",
    "in the belle experiment , unique software framework called as b.a.s.f .",
    "( belle analysis framework ) for all phases in event processing from online data - taking to offline user analyses has been employed .",
    "this is `` home - made '' core software developed by the belle group . in this scheme ,",
    "each program , written in c@xmath8 , is compiled as a shared object , and it is treated as a module . when one wants to run a program with this framework , the modules defined in the one s script are dynamically loaded .",
    "the data handling is done with the traditional bank system , named as panther , with a zlib compression capability in this utility , data transfer between different modules is made and data i / o is maniplated .",
    "panther is only software to handle our data in any stage of data processing .",
    "the typical event data size for rawdata is 35 kb , and it is increased up to 60 kb for reconstructed dst data , which contains all of detector information after unpacking , calibration and analysis . for user analyses , compact data set ( `` mini - dst '' ) is produced , which is approximately 12 kb for one hadronic event .      the reconstruction package is built when major update of programs , which can affect final physics analysis , is made .",
    "usually it is built once or twice per year .",
    "once new library is released , we need to process all of events to produce a consistent data set for analysis .    for mc data ,",
    "the detector response is simulated based upon the geant3 library@xcite . here",
    "background events , calculated from beam data , are overlaied onto mc events .",
    "the same reconstruction codes are also applied to mc simulated events .",
    "the detector calibration constants are stored in the database , for which postgresql@xcite is adopted in the belle experiment .",
    "two database servers are running in our computing system , where one is for public usage and the other for dst / mc productions .",
    "the contents of each server are periodically mirrored to the other , and a backup tar file for the contents of the original database server",
    "is archived once per week . for linux users , one can start up own database server in her / his pc , according to a belle instruction .",
    "user , if necessary , can download original database contents from a kek b computer for private purpose .",
    "figure  [ fig : comp_model ] indicates an overview of our computing model for the belle experiment . as can be seen , it comprises of the four major elements .",
    "the first one is the kek b computers which has been operated since february 2001@xcite .",
    "this has been a principal system , where data processing as well as analyses have been performed .",
    "in addition to this , we have equipped a 60 tb disk storage area march 2003 . in this space ,",
    "more beam and mc data can be kept . as for a network inside japan , super - sinet link has been available since 2002@xcite . to enrich cpu s for user analyses , pc farms dedicated to analysis jobs",
    "have been installed in june 2003 .",
    "these components are interlinked each other with a fast network of 1@xmath910 gbps .    [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     the primary purpose to add pc farms is that we have to increase cpu power for dst and mc productions . in 2003 , we expanded usage for our pc s by releasing new pc farms for user analyses , as a possible solution for ever increasing users demand .",
    "the 10 login servers have been arranged with 6 tb local disk area , where user histogram files and analysis codes are located . from each login server ,",
    "job can be submitted to the user pc farm consisting of 84 pc s with dual xeon 2.8 ghz cpu s .",
    "all pc s are managed by the lsf queuing utility . from user pc farms , beam and mc data samples , which are stored in the 60 tb disk mentioned previously ,",
    "can be accessed .",
    "a fast 1 gbps network dedicated to academic activities , super - sinet@xcite , has been available between kek and major japanese universities .",
    "this link enables us to copy a bulk of beam and mc data from kek to other institutions and vice versa .",
    "moreover , computing resources can be shared as seamless system using super - sinet .",
    "for example , one disk connected to pc at nagoya university can be mounted to the kek computer via nfs as if it were located inside the kek site .",
    "then , output data can be written onto the disk directly from the kek computer .",
    "it is possible to make efficient and full use of total resources collaboration - wide .      a large volume of data is comprised of more than 30 k data files including beam and mc events .",
    "basically each user has to go through all of these files to obtain final physics results .",
    "so , it is very important to notify all of file locations to users for their analyses .",
    "these data files are distributed over a bunch of storage disks and sometimes data files have to be moved for various reasons such as disk failure and so on . from administrative point of view , it is necessary to maintain flexibility in data management despite of any change of the file locations . to solve this ,",
    "we have registered all of the attributes of data files such as locations , data type and number of events into our database , and they serves as `` metadata '' to access actual beam data .",
    "the central database contents for data file information is maintained and updated whenever new data is available .",
    "the web - based interface between database and users is prepared to extract necessary information . in user s batch job ,",
    "inquiry to the database is automatically issued and user can easily analyze event data without knowing actual file locations .",
    "the scheme of the dst production is shown in figure  [ fig : prod_scheme ] . in the first step of the dst production ,",
    "one of sun compute host is assigned as a tape server , and two dtf2 tapes , one is for raw data and the other for dst data , are mounted .",
    "then , raw data are read from the tape and are distributed over pc nodes . in each pc node",
    ", event processing is performed in the b.a.s.f . framework .",
    "after the reconstruction is made , event data are sent back to the tape server , where data are written onto the tape as dst .",
    "the next step , the event skimming , is carried out in such a way that dst data are again read from the dst2 tape at the sun compute server , and an appropriate selection criteria is imposed onto the data . in case that an event satisfies selection conditions , it is saved onto disk as a skimmed event .",
    "each event is examined by a set of selections such as @xmath10-pair event for a detector study and hadronic event for a physics analysis .    in our system ,",
    "we can process about 1 fb@xmath1 of beam data per day with 40 pc nodes of quad intel xeon 700 mhz cpu s each , and it can be increased up to 2 fb@xmath1 per day by adding pc nodes .",
    "figure  [ fig : reprocess ] shows a history of our beam data processing from 2001 . in 2001",
    ", we completed the first entire reprocessing with a single version of the reconstruction package , providing 30 fb@xmath1 of data sample for 2000 summer conferences . in 2002 , major update of the software was made in april 2002 and the second reprocessing for 78 fb@xmath1 of data using this library was performed from april to june .",
    "last year , we added another 80 fb@xmath1 by reprocessing them , amounting to 159 fb@xmath1 of total beam data .",
    "we have three types of mc samples , @xmath11 , @xmath12 and continuum events .",
    "they are produced on a run - by - run basis , where each mc sample file corresponds to each beam data file .",
    "after data production for one run beam file is done , run dependent information like beam interaction profile and background hit rate are calculated immediately and , based upon these information , mc sample data corresponding to the beam run file is created for three types .",
    "beam background hits are overlaied with mc generated data to mimic actual events as precisely as possible .",
    "figure  [ fig : mcprocess ] shows how we have produced mc samples recent two years .",
    "we have produced 2.2 billion events in total by the end of 2003 , which is equivalent to 3 times larger statistics for 159 fb@xmath1 real beam data .",
    "another aspect of the mc production is contribution from the remote institutes outside kek .",
    "approximately one quater of mc events have been produced at remote sites and they are transferred to kek via network .",
    "the belle computing system has been successfully working and we have processed a bulk of beam data of more than 250 fb@xmath1 so far . for mc data ,",
    "2.2 billion events have been produced , which corresponds to more than 3 times larger statistics than real data .",
    "those data have been managed via our database and this scheme allows us to provide the data sets steadily and flexibly .    in 2003 summer , new silion vertex detector has been installed it is expected that this detector expands our tracking and vertexing capability further . at present , reconstruction algorithm based upon new belle configuration is being developed .",
    "after it is completed , beam data in 2003 autumn runs will be processed . to do this , we are planninng to double our processing power for beam data by adding more cpu s and storage devices .    9 belle collaboration , a.  abashian _ et al_. , nucl .",
    "instr . and meth .",
    "a*479 * , 117(2002 ) .",
    "s.  kurokawa and e.  kikutani , nucl .",
    "instr . and meth .",
    "a*499 * , 1(2003 ) .",
    "belle collaboration , k.  abe _ et al_. phys .",
    "87 , 091802(2001 ) .",
    "r.  brun _ et al_. , geant3.21 cern report no.dd/ee/84-1(1987 ) . .",
    "i.  adachi _ et al_. , proc . of chep03 , la jolla , california ,",
    "march 24 - 28 , 2003 .",
    "national institute of informatics , japan , http://www.sinet.ad.jp/."
  ],
  "abstract_text": [
    "<S> we describe the present status of the computing system in the belle experiment at the kekb @xmath0 asymmetric - energy collider . </S>",
    "<S> so far , we have logged more than 160 fb@xmath1 of data , corresponding to the world s largest data sample of 170 m @xmath2 pairs at the @xmath3 energy region . </S>",
    "<S> a large amount of event data has to be processed to produce an analysis event sample in a timely fashion . </S>",
    "<S> in addition , monte carlo events have to be created to control systematic errors accurately . </S>",
    "<S> this requires stable and efficient usage of computing resources . </S>",
    "<S> here we review our computing model and then describe how we efficiently proceed dst / mc productions in our system . </S>"
  ]
}