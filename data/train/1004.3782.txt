{
  "article_text": [
    "the problem of `` scaling up for high dimensional data and high speed data streams '' is among the `` ten challenging problems in data mining research''@xcite .",
    "this paper is devoted to estimating entropy of data streams .",
    "mining data streams@xcite in ( e.g. , ) 100 tb scale databases has become an important area of research , e.g. , @xcite , as network data can easily reach that scale@xcite .",
    "search engines are a typical source of data streams@xcite .",
    "consider the _ turnstile _",
    "stream model@xcite .",
    "the input stream @xmath28 , @xmath29 $ ] arriving sequentially describes the underlying signal @xmath30 , meaning @xmath31 = a_{t-1}[i_t ] + i_t,\\end{aligned}\\ ] ] where the increment @xmath32 can be either positive ( insertion ) or negative ( deletion ) . restricting @xmath33\\geq 0 $ ] results in the _ strict - turnstile",
    "_ model , which suffices for describing almost all natural phenomena@xcite .",
    "this study focuses on the _ relaxed strict - turnstile _ model and studies efficient algorithms for estimating the _ @xmath1th frequency moment _ of data streams @xmath34^\\alpha.\\end{aligned}\\ ] ] we are particularly interested in the case of @xmath35 , which is very important for estimating _ shannon entropy_.    the _ relaxed strict - turnstile model _ only requires @xmath33\\geq 0 $ ] at the time @xmath36 one cares about ( e.g. , the end of streams ) ; and hence it is considerably more flexible than the _ strict - turnstile _ model .",
    "a very useful ( e.g. , in web and networks@xcite and neural comptutations@xcite ) summary statistic is the _",
    "shannon entropy _",
    "@xmath37}{f_{(1)}}\\log \\frac{a_t[i]}{f_{(1)}}.\\end{aligned}\\ ] ] various generalizations of the shannon entropy have been proposed .",
    "the rnyi entropy@xcite , denoted by @xmath38 , and the tsallis entropy@xcite , denoted by @xmath39 , are respectively defined as @xmath40^\\alpha}{\\left(\\sum_{i=1}^d a_t[i]\\right)^\\alpha } = \\frac{1}{1-\\alpha } \\log \\frac{f_{(\\alpha)}}{f_{(1)}^\\alpha } , \\\\ & t_\\alpha = \\frac{1}{1-\\alpha } \\left ( \\frac{f_{(\\alpha)}}{f_{(1)}^\\alpha}-1\\right).\\end{aligned}\\ ] ]    as @xmath35 , both rnyi entropy and tsallis entropy converge to shannon entropy : @xmath41 .",
    "thus , both rnyi entropy and tsallis entropy can be computed from the @xmath1th frequency moment ; and one can approximate shannon entropy from either @xmath38 or @xmath39 by letting @xmath42 .",
    "several studies@xcite ) used this idea to approximate shannon entropy , all of which relied critically on efficient algorithms for estimating the @xmath1th frequency moments ( [ eqn_moment ] ) near @xmath43 .",
    "in fact , one can numerically verify that the @xmath1 values proposed in @xcite are extremely close to 1 , for example , @xmath44 ( * ? ? ?",
    "1 ) or @xmath6@xcite are quite likely . , @xmath45 , where @xmath46 is the number of streaming updates . if we let @xmath47 , @xmath48 , @xmath49 , then @xmath50 . if we let @xmath51 , @xmath52 , then @xmath53 .",
    "+ ( * ? ? ?",
    "4.2.2 and 5.2 ) provides some improvements , to allow larger @xmath5 . if @xmath48 and @xmath49 , then @xmath54 . if @xmath55 and @xmath52 , then @xmath56 . ]",
    "+ from the definition of the rnyi and tsallis entropies , it is clear that , in order to achieve a @xmath0-additive guarantee for the shannon entropy , it suffices to estimate the @xmath1th frequency moment with an @xmath57 guarantee ( for sufficiently small @xmath5 ) .",
    "for example , suppose an estimator @xmath58 guarantees ( with high probability ) that @xmath59 , then the estimated rnyi entropy , denoted by @xmath60 would satisfy @xmath61 , assuming @xmath5 is sufficiently small .    another perspective is from the estimation variances .",
    "from the definitions of the rnyi and tsallis entropies , it is clear that we need estimators of the frequency moments with variances proportional to @xmath62 in order to cancel the term @xmath63 .",
    "the estimation variance , of course , is also closely related to the sample complexity .",
    "+ suppose we have an unbiased estimator of @xmath64 whose variance is @xmath65 , where @xmath23 is the sample size .",
    "then the sample complexity is essentially @xmath66 , using the standard argument popular in the theory literature , e.g. , @xcite .",
    "the space complexity ( in terms of bits ) will be @xmath67 .",
    "the drawback of this argument is that it does not fully specify the constants .",
    "+ in a summary , in order to provide a @xmath0 ( e.g. , 0.1 ) additive approximation of the shannon entropy , one should use @xmath68 samples for estimating the @xmath69th frequency moments .",
    "this bound initially appears disappointing , because , if for example , @xmath70 , @xmath71 , @xmath72 , then it requires @xmath73 samples , which is very likely impractical .",
    "well - known algorithms based on _ symmetric stable random projections_@xcite indeed exhibit @xmath70 .",
    "network traffic is a typical example of high - rate data streams .",
    "an effective and reliable measurement of network traffic in real - time is crucial for anomaly detection and network diagnosis ; and one such measurement metric is shannon entropy@xcite .",
    "the _ turnstile _ data stream model ( [ eqn_turnstile ] ) is naturally suitable for describing network traffic , especially when the goal is to characterize the statistical distribution of the traffic . in its empirical form",
    ", a statistical distribution is described by histograms , @xmath33 $ ] , @xmath74 to @xmath75 .",
    "it is possible that @xmath47 ( ipv6 ) if one is interested in measuring the traffic streams of unique source or destination .    the distributed denial of service ( * ddos * )",
    "attack is a representative example of network anomalies .",
    "a ddos attack attempts to make computers unavailable to intended users , either by forcing users to reset the computers or by exhausting the resources of service - hosting sites . for example , hackers may maliciously saturate the victim machines by sending many external communication requests .",
    "ddos attacks typically target sites such as banks , credit card payment gateways , or military sites .    a ddos attack changes the statistical distribution of network traffic .",
    "therefore , a common practice to detect an attack is to monitor the network traffic using certain summary statistics .",
    "since shannon entropy is a well - suited for characterizing a distribution , a popular detection method is to measure the time - history of entropy and alarm anomalies when the entropy becomes abnormal@xcite .",
    "entropy measurements do not have to be `` perfect '' for detecting attacks .",
    "it is however crucial that the algorithm should be computationally efficient at low memory cost , because the traffic data generated by large high - speed networks are enormous and transient ( e.g. , 1 gbits / second ) .",
    "algorithms should be real - time and one - pass , as the traffic data will not be stored@xcite .",
    "many algorithms have been proposed for `` sampling '' the traffic data and estimating entropy over data streams@xcite ,      the recent work@xcite was devoted to estimating the shannon entropy of msn search logs , to help answer some basic problems in web search , such as , _ how big is the web ? _    the search logs can be viewed as data streams , and @xcite analyzed several `` snapshots '' of a sample of msn search logs . the sample used in @xcite contained 10 million @xmath76query , url , ip@xmath77 triples ; each triple corresponded to a click from a particular ip address on a particular url for a particular query .",
    "@xcite drew their important conclusions on this ( hopefully ) representative sample .",
    "alternatively , one could apply data stream algorithms such as cc on the whole history of msn ( or other search engines ) .",
    "a workshop in nips03 was devoted to entropy estimation , owing to the wide - spread use of shannon entropy in neural computations@xcite .",
    "( http://www.menem.com/~ilya/pages/nips03 ) for example , one application of entropy is to study the underlying structure of spike trains .",
    "the problem of approximating @xmath64 has been very heavily studied in theoretical computer science and databases , since the pioneering work of @xcite , which studied @xmath78 , 2 , and @xmath79 .",
    "@xcite provided improved algorithms for @xmath80 .",
    "@xcite provided algorithms for @xmath81 to achieve the lower bounds proved by @xcite .",
    "@xcite suggested using even more space to trade for some speedup in the processing time .",
    "+ note that the first moment ( i.e. , the sum ) , @xmath82 , can be computed easily with a simple counter@xcite .",
    "this important property was recently captured by the method of * * compressed counting ( cc)**@xcite , which was based on the _ maximally - skewed stable random projections_. @xcite provided two algorithms , based on the _ geometric mean _ and _ harmonic mean _ , and proved some important theoretical results :    * the _ geometric mean _",
    "algorithm has the variance proportional to @xmath83 in the neighborhood of @xmath84 , where @xmath85 .",
    "this is the first algorithm that captured the intuition that , in the neighborhood of @xmath84 , the moment estimation algorithms should work better and better as @xmath35 , in a continuous fashion . + * our comments * : the _ geometric mean _ algorithm , unfortunately , did not provide an adequate mechanism for entropy estimation .",
    "as previously discussed , this methods leads to an entropy estimation algorithm with complexity @xmath86 , which is actually quite intuitive from the definitions of the tsallis entropy and rnyi entropy .",
    "both entropies contain the @xmath87 terms , meaning that the variance will blow up as @xmath88 , which can not be canceled by @xmath89 .",
    "note that @xcite did not show the variance of the _ harmonic mean _",
    "algorithm is also proportional to @xmath89 ; this paper will provide the proof . * for fixed @xmath3 , as @xmath90 , the sample complexity bound of the _",
    "geometric mean _",
    "algorithm is @xmath91 with all constants specified .",
    "this result was a major improvement over the well - known @xmath92 bound@xcite .",
    "note that the assumption of fixing @xmath3 and letting @xmath90 is needed for theoretical convenience in order to derive bounds with no unspecified constants .",
    "this study will continue to use this assumption . +",
    "* our comments * : when @xmath84 , the moment estimation problem is trivial and only requires one simple counter . therefore , even intuitively , @xmath91 can not possibly be the true complexity bound .",
    "we consider the _ relaxed strict - turnstile _ model ( [ eqn_turnstile ] ) .",
    "conceptually , we multiply the data stream vector @xmath93 by a random projection matrix @xmath94 . the resultant vector @xmath95 is only of length @xmath23 .",
    "more specifically , the entries of the projected vector @xmath96 are @xmath97_j= \\sum_{i=1}^d r_{ij } a_t[i ] , \\ \\ j = 1 , 2 , ... , k\\end{aligned}\\ ] ]    @xmath98 s are random variables generated from the following ( * non - standard * ) skewed stable distribution@xcite : @xmath99^{1/\\alpha } } \\left[\\frac{\\sin\\left ( v_{ij}\\delta\\right)}{w_{ij } } \\right]^{\\frac{\\delta}{\\alpha } } , \\ \\ \\ \\ \\delta = 1-\\alpha>0,\\end{aligned}\\ ] ] where @xmath100 ( i.i.d . ) and @xmath101 ( i.i.d . ) , an exponential distribution with mean 1 .",
    "we use this formulation to avoid numerical problems and simplify the analysis .",
    "+ of course , in data stream computations , the matrix @xmath102 is never fully materialized . the standard procedure in data stream computations",
    "is to generate entries of @xmath102 on - demand@xcite . in other words ,",
    "whenever an stream element @xmath28 arrives , one updates entries of @xmath96 as @xmath103    the proposed algorithm is defined as follows : @xmath104^\\delta\\end{aligned}\\ ] ]    the following theorem proves that this new estimator is ( asymptotically ) unbiased with the variance proportional to @xmath62 . note that @xmath105 as @xmath90 .",
    "[ thm_f_var ] @xmath106 * proof : * see appendix [ proof_thm_f_var ] .    in this paper , we only consider @xmath107 .",
    "this is because the maximally - skewed stable distributions have good theoretical properties when @xmath108@xcite ; for example , all negative moments exist ; see lemma [ lem_moments ] .",
    "the standard procedure for sampling from skewed stable distributions is based on the chambers - mallows - stuck method@xcite . to generate a sample from @xmath109 ,",
    "i.e. , @xmath1-stable , maximally - skewed ( @xmath110 ) , with unit scale , one first generates an exponential random variable with mean 1 , @xmath111 , and a uniform random variable @xmath112 , then , @xmath113^{1/\\alpha } } \\left[\\frac{\\cos\\left ( u - \\alpha(u + \\rho)\\right)}{w } \\right]^{\\frac{1-\\alpha}{\\alpha } } \\sim s(\\alpha,\\beta=1,1),\\end{aligned}\\ ] ] where @xmath114 when @xmath108 and @xmath115 when @xmath116 .",
    "+ note that @xmath117 as @xmath35 . for convenience ( and avoiding numerical problems )",
    ", we will use @xmath118    in this study , we will only consider   @xmath107 , i.e , @xmath119 .",
    "after simplification , we obtain @xmath120^{1/\\alpha } } \\left[\\frac{\\sin\\left ( v\\delta\\right)}{w } \\right]^{\\frac{\\delta}{\\alpha}},\\end{aligned}\\ ] ] where @xmath121 . this explains ( [ eqn_r_ij ] ) .",
    "+ lemma [ lem_z_order ] shows @xmath122 , which can be accurately represented using @xmath123 bits .",
    "the proof is omitted since it is straightforward .",
    "[ lem_z_order ] for any given @xmath124 , and @xmath125 , as @xmath90 , @xmath126    let @xmath127 , where entries of @xmath102 are i.i.d .",
    "samples of @xmath128",
    ". then by properties of stable distributions , entries of @xmath96 are @xmath129_j = \\sum_{i=1}^d r_{i , j}a_t[i ] \\sim s \\left(\\alpha,\\beta=1 , \\cos\\left(\\frac{\\pi}{2}\\alpha\\right)f_{(\\alpha)}\\right),\\end{aligned}\\ ] ] where @xmath130^\\alpha$ ] as defined in ( [ eqn_moment ] ) .",
    "+ therefore , cc boils down to estimating @xmath64 from @xmath23 i.i.d .",
    "stable samples .",
    "@xcite provided two statistical estimators , the _ geometric mean _ and _ harmonic mean _ estimators , which are derived based on the following basic moment formula .",
    "[ lem_moments ] @xcite .",
    "if @xmath131 , then @xmath132 , and for any @xmath133 , @xmath134      assume @xmath22 , @xmath135 to @xmath23 , are i.i.d .",
    "samples from @xmath136 . after simplifying the corresponding expression in @xcite , we obtain @xmath137^k \\prod_{j=1}^k x_j^{\\alpha / k},\\end{aligned}\\ ] ] which is unbiased and has asymptotic variance @xmath138 as @xmath35 , the asymptotic variance approaches zero at the rate of only @xmath89 , which is not adequate .",
    "@xmath139    which is asymptotically unbiased and has variance @xmath140    @xcite only graphically showed that the _ harmonic mean _ estimator is noticeably better than the _ geometric mean _ estimator .",
    "we prove the following lemma , which says the variance of the _ harmonic mean _ is also proportional to @xmath89 .",
    "thus , the _ harmonic mean _ estimator is not adequate for entropy estimation either .",
    "[ lem_hm ] as @xmath141 , @xmath142 * proof : * see appendix [ proof_lem_hm ] .      this section provides the distribution function of @xmath143 , which will be needed in deriving the proposed estimator ( [ eqn_f ] ) .    [ lem_cdf ] suppose a random variable @xmath143 .",
    "the cumulative distribution function ( cdf ) is @xmath144 where @xmath145^{\\alpha/\\delta } } { \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right ) , \\hspace{0.5 in } \\theta\\in(0,\\pi)\\end{aligned}\\ ] ]    assume @xmath146 , then @xmath147 is monotonically increasing in @xmath148 , with @xmath149 moreover , @xmath147 is a convex function of @xmath150 .",
    "* proof : * see appendix [ proof_lem_cdf ] .",
    "+    note that @xmath151 approaches zero as @xmath90 .",
    "thus , one might be wondering if we replace @xmath152 by @xmath153 , the errors may be quite small .",
    "this conjecture is verified in figure [ fig_cdf ] .      basically , we derive the proposed estimator by `` guessing . ''",
    "we first derive a maximum likelihood estimator ( mle ) for a slightly different distribution based on the intuition from lemma [ lem_cdf ] and figure [ fig_cdf ] .",
    "then we verify that this mle is actually a very good estimator ( in terms of both the variances and tail bounds ) for the stable distribution we care about .    here",
    ", we consider a random variable @xmath154 whose cumulative distribution function ( cdf ) is @xmath155 it is indeed a cdf because it is an increasing function of @xmath156 , @xmath157 , and @xmath158 .",
    "similar to stable random projections , we are interested in estimating @xmath159 from @xmath23 i.i.d .",
    "samples @xmath160 , @xmath135 to @xmath23 .",
    "statistics theory tells us that the maximum likelihood estimator ( mle ) has the ( asymptotic ) optimality .",
    "because the distribution function of @xmath161 is known , we can actually compute the mle in this case .",
    "[ thm_mle ] suppose @xmath161 , @xmath162 to @xmath23 , are i.i.d .",
    "samples from a distribution whose cdf is given by ( [ eqn_cdf_y ] ) .",
    "let @xmath160 , where @xmath163 .",
    "then the maximum likelihood estimator of @xmath159 is given by @xmath164^\\delta\\end{aligned}\\ ] ] * proof : * see appendix [ proof_thm_mle ] .",
    "compared with the proposed estimator @xmath58 in ( [ eqn_f ] ) , the mle solution has the addition term of @xmath165 .",
    "note that , while both @xmath166 and @xmath167 approach 1 , @xmath105 considerably slower than @xmath168 , because @xmath169 for example , when @xmath170 , @xmath171 , @xmath172 ; when @xmath173 , @xmath174 , @xmath175 .",
    "therefore , while @xmath167 may be considered negligible , it may be preferable to keep @xmath166 .",
    "in fact , when proving that the proposed estimator @xmath58 is ( asymptotically ) unbiased ( see appendix [ proof_thm_f_var ] ) , we do need the @xmath166 term .",
    "theorem [ thm_f_var ] has proved that the proposed estimator @xmath176^\\delta\\end{aligned}\\ ] ] is asymptotically unbiased with variance proportional to @xmath177 . using the standard argument , we know that the sample complexity bound must be @xmath178 .",
    "we are , however , very interested in the precise complexity bounds , not just the orders .",
    "normally , we would like to present the tail bounds as , e.g. , @xmath179 , which immediately leads to the statement that :    _ with probability at least @xmath180 , it suffices to use @xmath181 to guarantee @xmath182 . _ + ideally , we hope @xmath183 will be as small as possible .",
    "in fact , in order to achieve a @xmath0-additive algorithm for entropy estimation , we need @xmath4 ( where @xmath6 or even much smaller ) . therefore , we really need @xmath184 . in this sense , it is no longer appropriate to treat @xmath183 as a `` constant . ''",
    "+ theorem [ thm_tail ] presents the tail bounds for @xmath58 .",
    "[ thm_tail ] for any @xmath185 and @xmath186 , we have the right tail bound for the proposed estimator : @xmath187 where @xmath188 is the solution to @xmath189    for any @xmath190 and @xmath186 , we have the left tail bound : @xmath191 where @xmath192 is the solution to @xmath193 * proof : *  see appendix [ proof_thm_tail ] .",
    "+    these bounds appear to be too complicated to gain insightful information .",
    "people may be even wondering about numerical stability of the infinite sums .",
    "first of all , we notice that when @xmath194 ( i.e. , @xmath78 ) , we can compute the tail bounds exactly , as presented in lemma [ lem_tail_d=1 ] .",
    "[ lem_tail_d=1 ] when @xmath194 , i.e. , @xmath78 , @xmath195 * proof : * when @xmath196 ( @xmath197 ) , we have @xmath198 , @xmath199 , @xmath200 , and @xmath201 .",
    "the conclusions follow easily .",
    "@xmath202    next , we re - formulate the tail bounds to facilitate numerical evaluations .",
    "our numerical results show that , when @xmath5 is small , @xmath203 and @xmath204 , for @xmath13 .",
    "thus , we indeed have an algorithm for entropy estimation with complexity @xmath10 . + the tail bounds ( [ eqn_g_r ] ) and ( [ eqn_g_l ] ) contain @xmath205 , which can be written as @xmath206 therefore , @xmath207 according to the stirling s series @xcite @xmath208.\\end{aligned}\\ ] ]    thus , for numerical reasons , we can rewrite ( [ eqn_g_r ] ) and ( [ eqn_g_l ] ) as @xmath209 the infinite series always converge provided @xmath210 and @xmath211 .",
    "in fact , because the bounds hold for any @xmath212 ( not necessarily the optimal values , @xmath188 and @xmath192 ) , we know @xmath213 and @xmath214 if using ( e.g. , ) @xmath215 .",
    "in other words , @xmath216 and @xmath217 , as desired .",
    "we state this as a lemma .",
    "+    [ lem_g ] the tail bound constants ( [ eqn_g_r ] ) and ( [ eqn_g_l ] ) @xmath218 in other words @xmath219 therefore , to estimate @xmath64 within a @xmath220 factor , it suffices to let the sample size @xmath221 , using the proposed estimator @xmath58 .",
    "+    figure [ fig_g1e246 ] presents the values in terms of @xmath222 and @xmath223 for @xmath13 and @xmath224 , @xmath225 , @xmath226 , together with the closed - form expressions for @xmath196 as obtained in lemma [ lem_tail_d=1 ] .",
    "the values are pleasantly small .",
    "thus , at least numerically , we can say , for example , when @xmath5 is small , @xmath227 in other words , with a probability at least @xmath180 , using the proposed estimator , one can achieve @xmath228 by using @xmath229 samples . and",
    "we know the constant 9 could be replaced by 6 if @xmath0 is small .",
    "+    whenever possible , analytical expressions are always more desirable . in fact , when @xmath230 , we can actually obtain the analytical expressions for @xmath231 and @xmath232 .",
    "[ lem_g->0 ] as @xmath230 , @xmath233 * proof * : see appendix [ proof_lem_g->0 ] .",
    "in the previous ( unpublished ) work@xcite , we proposed the * _ sample minimum _ * estimator , which allowed us to prove a much improved sample complexity bound than that in @xcite .",
    "interestingly , the proposed estimator @xmath58 in this paper actually converges to the _ sample minimum _ estimator , denoted by @xmath234 , @xmath235^\\delta\\rightarrow \\hat{f}_{(\\alpha),\\min } =   \\min\\{x_j^\\alpha , j = 1 , 2 , ... , k\\}.\\end{aligned}\\ ] ]    this fact is quite intuitive . as @xmath90 , the smallest one of @xmath22 s is amplified the most by @xmath236 .",
    "this is analogous to the well - know fact that , the @xmath237 norm approaches the @xmath238 norm ( which is the maximum element of the vector ) , as @xmath239 .",
    "+ in @xcite , we proved the following ( closed - form ) sample complexity bound for @xmath234 :    @xcite [ thm_right_bound_min ] as @xmath240 , for any fixed @xmath185 , @xmath241\\right).\\end{aligned}\\ ] ]    basically , in terms of @xmath4 , theorem [ thm_right_bound_min ] is applicable when @xmath0 is large ( @xmath242 ) and @xmath5 is small .",
    "a simulation study in @xcite demonstrated that the bound in theorem [ thm_right_bound_min ] can be very sharp .",
    "real - world data are often dynamic and can be modeled as data streams .",
    "measuring summary statistics of data streams such as the shannon entropy has become an important task in many applications , for example , detecting anomaly events in large - scale networks .",
    "one line of active research is to approximate the shannon entropy using the @xmath1th frequency moments of the stream with @xmath1 very close to 1 ( e.g. , @xmath243 or even much smaller ) .    efficiently approximating the @xmath1th frequency moments of data streams",
    "has been very heavily studied in theoretical computer science and databases . when @xmath244 , it is well - known that efficient @xmath92-space algorithms exist , for example , _ symmetric stable random projections_@xcite , which however are impractical for estimating shannon entropy using @xmath1 extremely close to 1 .",
    "recently , @xcite provided an algorithm to achieve the @xmath91 bound in the neighborhood of @xmath84 , based on the idea of _ maximally - skewed stable random projections _",
    "( also called _ compressed counting ( cc ) _ ) . the algorithms provided in @xcite , however , are still impractical",
    ". + in this paper , we provide a truly practical algorithm for entropy estimation .",
    "we prove that its variance is proportional to @xmath62 whereas previous algorithms for cc developed in @xcite have variances proportional only to @xmath89 .",
    "this new algorithm leads to an @xmath245 algorithm for entropy estimation to achieve @xmath0-additive accuracy , while previous algorithms must use @xmath246 samples @xcite , or @xmath247",
    "samples @xcite . note that because @xmath5 is so small , it is no longer appropriate to treat it as `` constant . ''",
    "we also analyze the precise sample complexity bound of the proposed new estimator , both numerically ( for general @xmath13 ) and analytically ( for small @xmath0 ) , to demonstrate that the sample complexity bound of the new estimator is free of large constants .",
    "this further confirms that our proposed new estimator is practical .",
    "as defined in ( [ eqn_f ] ) , the proposed estimator @xmath176^\\delta = \\left[\\frac{1}{\\hat{j}}\\right]^\\delta ,   \\hspace{0.5 in } \\hat{j } = \\delta\\frac{1}{k}\\sum_{j=1}^k x_j^{-\\alpha/\\delta}\\end{aligned}\\ ] ] where @xmath248 , i.i.d .",
    "denote @xmath249 . according to lemma [ lem_moments ]",
    ", @xmath250 @xmath251 @xmath252 a bit more algebra can show @xmath253    recall @xmath254 .",
    "we will basically proceed by using the `` delta '' method popular in statistics .",
    "we need to be a bit careful here as @xmath5 is small . just to make sure the resultant higher - order terms are indeed negligible",
    ", we carry out the algebra .    by the taylor expansion about @xmath255 , we obtain @xmath256 taking expectations on both sides yields , @xmath257 evaluating the higher - order moments yields @xmath258 ^ 2\\\\\\notag = & e\\left[\\left(\\hat{j}-j\\right)^2\\delta^2 j^{-2\\delta-2}\\right]-",
    "e\\left [ \\frac{(\\hat{j}-j)^3}{2}\\delta^2(\\delta+1)j^{-2\\delta-3}\\right]+ ... \\\\\\notag = & \\frac{f_{(\\alpha)}^2}{k}\\delta^2\\left(3 - 2\\delta + o\\left(\\frac{1}{k}\\right)\\right),\\end{aligned}\\ ] ] and @xmath259      given @xmath23 i.i.d .",
    "samples @xmath160 , the task is to estimate @xmath159 using mle .",
    "the cdf of @xmath161 is given by @xmath285 by taking derivatives , the density function of @xmath22 is given by @xmath286 because @xmath287 solving the mle equation , @xmath288 we obtain @xmath289^\\delta\\end{aligned}\\ ] ]      from the previous results , we know @xmath290^\\delta,\\\\\\notag & x_j \\sim s\\left(\\alpha , \\beta=1 , \\cos\\left(\\frac{\\pi}{2}\\alpha\\right)f_{(\\alpha)}\\right),\\\\\\notag & e\\left(x_j^\\lambda\\right ) = f_{(\\alpha)}^{\\lambda/\\alpha } \\frac { \\gamma\\left(1-\\frac{\\lambda}{\\alpha}\\right ) } { \\gamma\\left(1-\\lambda\\right)},\\\\\\notag & e\\left(\\frac{x_j^{-n\\alpha/\\delta}}{f_{(\\alpha)}^{-n/\\delta}}\\right ) = \\frac{\\gamma\\left(1+\\frac{n}{\\delta}\\right)}{\\gamma\\left(1+\\frac{n\\alpha}{\\delta}\\right)}.\\end{aligned}\\ ] ]    we first study the right tail bound .",
    "@xmath291^\\delta      \\geq ( 1+\\epsilon)f_{(\\alpha)}\\right)\\\\\\notag = & \\mathbf{pr}\\left(\\sum_{j=1}^k x_j^{-\\alpha/\\delta } \\leq \\frac{k}{(1+\\epsilon)^{1/\\delta } \\delta f_{(\\alpha)}^{1/\\delta}}\\right)\\\\\\notag = & \\mathbf{pr}\\left(-t\\sum_{j=1}^k\\frac { x_j^{-\\alpha/\\delta}}{f_{(\\alpha)}^{-1/\\delta } } \\geq -t\\frac{k}{(1+\\epsilon)^{1/\\delta } \\delta } \\right ) \\hspace{0.5 in } ( \\text{any   } t>0)\\\\\\notag \\leq&e\\left(\\exp\\left(-t\\sum_{j=1}^k\\frac { x_j^{-\\alpha/\\delta}}{f_{(\\alpha)}^{-1/\\delta}}\\right)\\right)\\exp\\left ( t\\frac{k}{(1+\\epsilon)^{1/\\delta } \\delta } \\right)\\\\\\notag = & e^k\\left(\\exp\\left(-t\\frac { x_j^{-\\alpha/\\delta}}{f_{(\\alpha)}^{-1/\\delta}}\\right)\\right)\\exp\\left ( t\\frac{k}{(1+\\epsilon)^{1/\\delta } \\delta } \\right)\\\\\\notag = & e^k\\left(\\sum_{n=0}^\\infty \\frac{(-t)^n}{n ! } \\left(\\frac { x_j^{-\\alpha/\\delta}}{f_{(\\alpha)}^{-1/\\delta}}\\right)^n \\right)\\exp\\left ( t\\frac{k}{(1+\\epsilon)^{1/\\delta } \\delta } \\right)\\\\\\notag = & \\left(\\sum_{n=0}^\\infty \\frac{(-t)^n}{n ! } \\frac{\\gamma\\left(1+\\frac{n}{\\delta}\\right)}{\\gamma\\left(1+\\frac{n\\alpha}{\\delta}\\right ) } \\right)^k\\exp\\left ( t\\frac{k}{(1+\\epsilon)^{1/\\delta } \\delta } \\right)\\\\\\notag = & \\exp\\left(k\\left ( \\log \\sum_{n=0}^\\infty \\frac{(-t)^n}{n ! } \\frac{\\gamma\\left(1+\\frac{n}{\\delta}\\right)}{\\gamma\\left(1+\\frac{n\\alpha}{\\delta}\\right ) } + \\frac{t}{(1+\\epsilon)^{1/\\delta } \\delta } \\right)\\right)\\end{aligned}\\ ] ]    we can choose the optimal @xmath36 to minimize this upper bound .",
    "thus , @xmath292 where @xmath188 is the solution to @xmath293    now , we look into the left tail bound .",
    "@xmath294^\\delta      \\leq ( 1-\\epsilon)f_{(\\alpha)}\\right)\\\\\\notag = & \\mathbf{pr}\\left(\\sum_{j=1}^k x_j^{-\\alpha/\\delta }",
    "\\geq \\frac{k}{(1-\\epsilon)^{1/\\delta } \\delta f_{(\\alpha)}^{1/\\delta}}\\right)\\\\\\notag = & \\mathbf{pr}\\left(t\\sum_{j=1}^k\\frac { x_j^{-\\alpha/\\delta}}{f_{(\\alpha)}^{-1/\\delta } } \\geq t\\frac{k}{(1-\\epsilon)^{1/\\delta } \\delta } \\right ) \\hspace{0.5 in } ( \\text{any   } t>0)\\\\\\notag \\leq&e\\left(\\exp\\left(t\\sum_{j=1}^k\\frac { x_j^{-\\alpha/\\delta}}{f_{(\\alpha)}^{-1/\\delta}}\\right)\\right)\\exp\\left ( -t\\frac{k}{(1-\\epsilon)^{1/\\delta } \\delta } \\right)\\\\\\notag = & e^k\\left(\\exp\\left(t\\frac { x_j^{-\\alpha/\\delta}}{f_{(\\alpha)}^{-1/\\delta}}\\right)\\right)\\exp\\left ( -t\\frac{k}{(1-\\epsilon)^{1/\\delta } \\delta } \\right)\\\\\\notag = & e^k\\left(\\sum_{n=0}^\\infty \\frac{t^n}{n ! } \\left(\\frac { x_j^{-\\alpha/\\delta}}{f_{(\\alpha)}^{-1/\\delta}}\\right)^n \\right)\\exp\\left ( -t\\frac{k}{(1-\\epsilon)^{1/\\delta } \\delta } \\right)\\\\\\notag = & \\left(\\sum_{n=0}^\\infty \\frac{t^n}{n ! }",
    "\\frac{\\gamma\\left(1+\\frac{n}{\\delta}\\right)}{\\gamma\\left(1+\\frac{n\\alpha}{\\delta}\\right ) } \\right)^k\\exp\\left ( -t\\frac{k}{(1-\\epsilon)^{1/\\delta } \\delta } \\right)\\\\\\notag = & \\exp\\left(k\\left ( \\log \\sum_{n=0}^\\infty \\frac{t^n}{n ! } \\frac{\\gamma\\left(1+\\frac{n}{\\delta}\\right)}{\\gamma\\left(1+\\frac{n\\alpha}{\\delta}\\right ) } - \\frac{t}{(1-\\epsilon)^{1/\\delta } \\delta } \\right)\\right)\\end{aligned}\\ ] ]    again , we can choose the optimal @xmath295 to minimize this upper bound .",
    "thus , @xmath296 where @xmath192 is the solution to @xmath297",
    "the task is to show that , as @xmath141 , @xmath260    using properties of gamma functions , for example , @xmath261 , we obtain @xmath262 using the infinite product representation of the gamma function@xcite , we obtain @xmath263 therefore , @xmath264      suppose a random variable @xmath143 .",
    "we can show that the cumulative distribution function is @xmath265^{\\alpha/\\delta } } { t^{\\alpha/\\delta } \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right)\\right ) d\\theta , \\hspace{0.5 in } ( \\delta = 1-\\alpha).\\end{aligned}\\ ] ] recall @xmath266^{1/\\alpha } } \\left[\\frac{\\sin\\left ( v\\delta\\right)}{w } \\right]^{\\frac{\\delta}{\\alpha}}$ ] .",
    "@xmath8 is uniform in @xmath267 $ ] and @xmath268 is exponential with mean 1 .",
    "therefore , @xmath269^{1/\\alpha } } \\left[\\frac{\\sin\\left ( v\\delta\\right)}{w } \\right]^{\\frac{\\delta}{\\alpha } }   \\geq t \\right)\\\\\\notag = & \\mathbf{pr}\\left (   w \\leq \\frac {   \\left[\\sin\\left(\\alpha v \\right)\\right]^{\\alpha/\\delta } } { t^{\\alpha/\\delta } \\left[\\sin v \\right]^{1/\\delta } } \\sin\\left ( v\\delta\\right)\\right)\\\\\\notag = & \\text{e}\\left(\\mathbf{pr}\\left ( \\left .",
    "\\frac {   \\left[\\sin\\left(\\alpha v \\right)\\right]^{\\alpha/\\delta } } { t^{\\alpha/\\delta } \\left[\\sin v \\right]^{1/\\delta } } \\sin\\left ( v\\delta\\right)\\right|v\\right)\\right)\\\\\\notag = & 1-\\text{e}\\left(\\exp\\left(- \\frac {   \\left[\\sin\\left(\\alpha v \\right)\\right]^{\\alpha/\\delta } } { t^{\\alpha/\\delta } \\left[\\sin v \\right]^{1/\\delta } } \\sin\\left ( v\\delta\\right)\\right)\\right)\\\\\\notag = & 1-\\frac{1}{\\pi}\\int_0^\\pi   \\exp\\left(- \\frac {   \\left[\\sin\\left(\\alpha \\theta \\right)\\right]^{\\alpha/\\delta } } { t^{\\alpha/\\delta } \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right)\\right ) d\\theta.\\end{aligned}\\ ] ]    for @xmath270 , let @xmath145^{\\alpha/\\delta } } { \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right).\\end{aligned}\\ ] ] it is easy to show that , as @xmath271 , @xmath272^{\\alpha/\\delta } } { \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right)\\\\\\notag = & \\lim_{\\theta\\rightarrow 0 + } \\left(\\frac{\\sin\\left(\\alpha \\theta \\right ) } { \\sin \\theta } \\right)^{1/\\delta } \\frac{\\sin\\left ( \\theta\\delta\\right)}{\\sin\\left(\\alpha \\theta \\right)}\\\\\\notag = & \\alpha^{1/\\delta}\\frac{\\delta}{\\alpha } = \\delta\\alpha^{1/\\delta-1}.\\end{aligned}\\ ] ]    the proof of the monotonicity of @xmath273 is omitted , because it is can be inferred from the proof of the convexity .    to show @xmath147 is a convex function @xmath150",
    ", it suffices to show it is log - convex . since @xmath274^{\\alpha/\\delta}}{[\\sin(\\theta)]^{1/\\delta } } = \\frac{\\sin(\\theta\\delta)}{\\sin(\\alpha\\theta)}\\left[\\frac{\\sin(\\alpha\\theta)}{\\sin(\\theta)}\\right]^{1/\\delta}\\end{aligned}\\ ] ] it suffices to show that both @xmath275 and @xmath276^{1/\\delta}$ ] are log - convex .",
    "@xmath277}{\\partial \\theta } = \\frac{\\cos(\\theta\\delta)}{\\sin(\\theta\\delta)}\\delta - \\frac{\\cos(\\alpha\\theta)}{\\sin(\\alpha\\theta)}\\alpha\\end{aligned}\\ ] ]    @xmath278}{\\partial \\theta^2 } = -\\frac{\\delta^2}{\\sin^2(\\theta\\delta ) } + \\frac{\\alpha^2}{\\sin^2(\\alpha\\theta ) } = \\left(\\frac{\\alpha}{\\sin(\\alpha\\theta)}-\\frac{\\delta}{\\sin(\\theta\\delta)}\\right ) \\left(\\frac{\\alpha}{\\sin(\\alpha\\theta)}+\\frac{\\delta}{\\sin(\\theta\\delta)}\\right)\\end{aligned}\\ ] ]    @xmath279}{\\partial \\theta } = \\delta\\alpha(\\cos(\\theta\\delta)-\\cos(\\alpha\\theta))\\geq 0 \\hspace{0.5 in } ( \\text{because } \\ \\ \\delta<0.5)\\end{aligned}\\ ] ]    therefore , @xmath280 and @xmath275 is convex .",
    "@xmath281}{\\partial \\theta } = \\frac{\\cos(\\alpha\\theta)}{\\sin(\\alpha\\theta)}\\alpha - \\frac{\\cos(\\theta)}{\\sin(\\theta)}\\end{aligned}\\ ] ]    @xmath282}{\\partial \\theta^2 } = -\\frac{\\alpha^2}{\\sin^2(\\alpha\\theta ) } + \\frac{1}{\\sin^2(\\theta ) } = \\left(\\frac{1}{\\sin(\\theta)}-\\frac{\\alpha}{\\sin(\\alpha\\theta)}\\right ) \\left(\\frac{1}{\\sin(\\theta)}+\\frac{\\alpha}{\\sin(\\alpha\\theta)}\\right)\\end{aligned}\\ ] ]    @xmath283}{\\partial \\theta } = \\alpha(\\cos(\\alpha\\theta)-\\cos(\\theta))\\geq 0 \\hspace{0.5 in } ( \\text{because } \\",
    "\\alpha = 1-\\delta>0.5)\\end{aligned}\\ ] ]    therefore , we have proved the convexity of @xmath284 .",
    "we have derived @xmath298 and @xmath299 in theorem [ thm_tail ] .",
    "the task of this lemma is to show that , as @xmath230 , @xmath300 to proceed with the proof , we first assume that , as @xmath301 , we have @xmath302 which can be later verified . with this assumption , we can expand @xmath299 : @xmath303 setting the first derivative to zero , @xmath304 we obtain @xmath305 which verifies that @xmath306 is indeed on the order of @xmath0 .",
    "therefore , @xmath307 + o\\left(\\nu^3\\right)\\\\\\notag = & \\frac{\\nu^2}{6 - 4\\delta } + o\\left(\\nu^3\\right).\\end{aligned}\\ ] ] thus , we have proved that @xmath308 as @xmath230 .",
    "a similar procedure can also prove @xmath309 .",
    "this section demonstrates that the proposed estimator @xmath58 in ( [ eqn_f ] ) for compressed counting ( cc ) is a truly practical algorithm , while the previously proposed _",
    "geometric mean _",
    "algorithm@xcite for cc is inadequate for entropy estimation .",
    "we also demonstrate that algorithms based on _ symmetric stable random projections _",
    "@xcite are not suitable for entropy estimation .",
    "since the estimation accuracy is what we are interested in , we can simply use static data instead of real data streams .",
    "this is because the projected data vector @xmath310 is the same at the end of the stream ( i.e. , time @xmath36 ) , regardless whether it is computed at once ( i.e. , static ) or incrementally ( i.e. , dynamic ) .",
    "eight english words are selected from a chunk of web crawl data .",
    "the words are selected fairly randomly , although we make sure they cover a whole range of data sparsity , from function words ( e.g. , `` a '' ) , to common words ( e.g. , `` friday '' ) to rare words ( e.g. , `` twist '' ) .",
    "thus , as summarized in table [ tab_data ] , our data set consists of 8 vectors and the entries are the numbers of word occurrences in each document .",
    "l l l l l l l + word & sparsity & entropy @xmath311 +   + twist & 0.004 & 5.4873 + friday & 0.034 & 7.0487 + fun & 0.047 & 7.6519 + business & 0.126 & 8.3995 + name & 0.144 & 8.5162 + have & 0.267 & 8.9782 + this & 0.423 & 9.3893 + a & 0.596 & 9.5463 +        we estimate the @xmath1th frequency moments , for @xmath312 ,  0.1 ,   ... ,  @xmath313 , using the proposed new estimator @xmath58 and the _ geometric mean _ estimator @xmath314 , as well as the _ geometric mean _",
    "estimator for _ symmetric stable random projections _ proposed in @xcite .",
    "recall @xmath315^\\delta,\\hspace{0.5 in } \\hat{f}_{(\\alpha),gm } = \\left[\\frac{\\gamma\\left(1-\\frac{\\alpha}{k}\\right)}{\\gamma\\left(1-\\frac{1}{k}\\right)}\\right]^k \\prod_{j=1}^k x_j^{\\alpha / k}.\\end{aligned}\\ ] ]    we find @xmath58 is numerically very stable , if we express it as @xmath316^\\delta \\times f_{(1)}^\\alpha,\\end{aligned}\\ ] ] where @xmath82 , the first moment , can be computed exactly .",
    "using matlab ( the 32-bit version ) , we find no numerical problems with @xmath317 even for very small @xmath5",
    "( e.g. , @xmath318 ; see figure [ fig_twist_f ] ) .",
    "+ however , we could not find a numerically very stable implementation of the _ geometric mean _",
    "estimator @xmath314 , when @xmath319 .",
    "we tried a variety of ways ( including the tricks in implementing @xmath58 ) to implement @xmath314 and the gamma functions ( e.g. , using `` gammaln '' instead of `` gamma '' in matlab ) .",
    "fortunately , we believe @xmath16 is sufficiently small for comparing the two estimators .",
    "we experiment with three @xmath23 values : 10 , 100 , and 1000 ; and we present the estimation errors in terms of the normalized mean square errors ( mse , normalized by the square of the true values ) . as @xmath5 decreases , the mses for the _ symmetric stable random projections _ ( in the right panel of figure [ fig_twist_f ] ) are roughly flat , verifying that algorithms based on _ symmetric stable random projections _ do not capture the fact that the first moment ( @xmath84 ) should be a trivial problem .",
    "+ using compressed counting ( cc ) , the _ geometric mean _ estimator , @xmath314 ( in the left panel of figure [ fig_twist_f ] ) , and proposed new estimator , @xmath58 ( in the middle panel ) , clearly exhibit the desired property that the mses decrease as @xmath5 decreases .",
    "of course , as expected , @xmath58 , has a much faster rate of decreasing than @xmath314 ; the latter is also numerically much less stable when @xmath319 .",
    "after we have estimated the frequency moments , we use them to estimate the shannon entropies using tsallis entropies . for the data vector",
    "`` twist '' , we present results at sample sizes @xmath320 , and 10000 .",
    "for all other vectors , we do not experiment with @xmath321 .",
    "figure [ fig_h1 ] and figure [ fig_h2 ] present the normalized mses .",
    "+ using cc and the proposed estimator @xmath58 ( middle panels ) , only @xmath322 samples already produces fairly accurate estimates .",
    "in fact , for some vectors ( such as `` a '' ) , even @xmath323 may provide reasonable estimates .",
    "we believe the performance of the new estimator is remarkable .",
    "another nice property is that the estimation errors ( mses ) become stable after ( e.g. , ) @xmath324 ( or @xmath225 ) .",
    "+ in comparisons , the performance of the _ geometric mean _ estimator ( left panels ) for cc is not satisfactory .",
    "this is because its variance only decreases only at the rate of @xmath83 , not @xmath177 .",
    "also clearly , using _",
    "symmetric stable random projections _",
    "( right panels ) would not provide good estimates of the shannon entropy ( unless the sample size is extremely large ( @xmath325 ) and one could carefully choose a good @xmath5 to exploit the bias - variance trade - off ) .",
    "laura feinstein , dan schnackenberg , ravindra balupari , and darrell kindred .",
    "statistical approaches to attack detection and response . in _",
    "darpa information survivability conference and exposition _ , pages 303314 , 2003 .",
    "kuai xu , zhi - li zhang , and supratik bhattacharyya .",
    "profiling internet backbone traffic : behavior models and applications . in _",
    "sigcomm 05 : proceedings of the 2005 conference on applications , technologies , architectures , and protocols for computer communications _ , pages 169180 , 2005 ."
  ],
  "abstract_text": [
    "<S> the long - standing problem of shannon entropy estimation in data streams ( assuming the strict turnstile model ) is now an easy task by using the technique proposed in this paper . essentially speaking , in order to estimate the shannon entropy with a guaranteed @xmath0-additive accuracy </S>",
    "<S> , it suffices to estimate the @xmath1th frequency moment , where @xmath2 , with a guaranteed @xmath3-multiplicative accuracy , where @xmath4 . </S>",
    "<S> previous studies have shown that @xmath5 has to be extremely small ( e.g. , @xmath6 or even much smaller ) . </S>",
    "<S> in other words , the sample complexity for entropy estimation is @xmath7 , where @xmath8 is the coefficient essentially determined by the variance of the estimator of frequency moments . in this paper , </S>",
    "<S> the proposed algorithm achieves @xmath9 and hence is a practical technique with complexity @xmath10 ( which is essentially @xmath11 if we consider @xmath12 ) . </S>",
    "<S> we provide the ( small ) complexity bound constants numerically ( for @xmath13 ) and analytically ( for small @xmath0 ) . </S>",
    "<S> + prior well - known algorithms based on _ symmetric stable random projections _ could only achieve @xmath14 , meaning that the sample complexity would be @xmath15 , which will be extremely large . </S>",
    "<S> for example , if @xmath12 and @xmath16 , then @xmath17 . + * * compressed counting ( cc)**@xcite , based on _ maximally skewed stable random projections _ , was recently proposed for estimating the @xmath1th frequency moment of data streams . </S>",
    "<S> @xcite proposed algorithms for cc based on the _ geometric mean _ and _ harmonic mean _ estimators . it was proved that the _ geometric mean _ estimator could achieve @xmath18 , leading to an @xmath19 algorithm , which unfortunately could still be impractical . in this paper , we prove that the _ harmonic mean _ </S>",
    "<S> estimator for cc also could only achieve @xmath20 . + </S>",
    "<S> the proposed new estimator for cc has a simple clean form : @xmath21^\\delta$ ] , where @xmath22 s are the projected data and @xmath23 is the sample size . </S>",
    "<S> we prove that its variance achieves @xmath9 , leading to a practical algorithm with complexity @xmath10 . </S>",
    "<S> in other words , if @xmath16 , the new algorithm improves the prior algorithms based the _ </S>",
    "<S> symmetric stable random projections _ </S>",
    "<S> roughly by a factor of @xmath24 ; and it improves the _ geometric / harmonic mean _ </S>",
    "<S> algorithms for cc roughly by a factor of @xmath25 . </S>",
    "<S> + our extensive experiments ( in the appendix ) verify that , using the proposed algorithm , @xmath26 samples could provide accurate estimates of the shannon entropy . </S>",
    "<S> the proposed algorithm is also numerically very stable , even for @xmath5 as small as @xmath27 . </S>"
  ]
}