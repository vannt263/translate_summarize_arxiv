{
  "article_text": [
    "because a bayesian network is a complete model for the variables and their relationships , it can be used to answer probabilistic queries about them . for example",
    ", the network can be used to find out updated knowledge of the state of a subset of variables when other variables ( the evidence variables ) are observed .",
    "this process of computing the posterior distribution of variables given evidence is called probabilistic inference .",
    "a bayesian network can thus be considered a mechanism for automatically applying bayes theorem to complex problems .    in the application of bayesian networks ,",
    "most of the work is related to probabilistic inferences .",
    "any variable updating in any node of bayesian networks might result in the evidence propagation across the bayesian networks .",
    "how to examine and execute various inferences is the important task in the application of bayesian networks .",
    "this chapter will sum up various inference techniques in bayesian networks and provide guidance for the algorithm calculation in probabilistic inference in bayesian networks .",
    "information systems are of discrete event characteristics , this chapter mainly concerns the inferences in discrete events of bayesian networks .",
    "the key feature of bayesian networks is the fact that they provide a method for decomposing a probability distribution into a set of local distributions .",
    "the independence semantics associated with the network topology specifies how to combine these local distributions to obtain the complete joint probability distribution over all the random variables represented by the nodes in the network .",
    "this has three important consequences .",
    "firstly , naively specifying a joint probability distribution with a table requires a number of values exponential in the number of variables . for systems in which interactions among the random variables are sparse",
    ", bayesian networks drastically reduce the number of required values .",
    "secondly , efficient inference algorithms are formed in that work by transmitting information between the local distributions rather than working with the full joint distribution .",
    "thirdly , the separation of the qualitative representation of the influences between variables from the numeric quantification of the strength of the influences has a significant advantage for knowledge engineering .",
    "when building a bayesian network model , one can focus first on specifying the qualitative structure of the domain and then on quantifying the influences .",
    "when the model is built , one is guaranteed to have a complete specification of the joint probability distribution .",
    "the most common computation performed on bayesian networks is the determination of the posterior probability of some random variables , given the values of other variables in the network . because of the symmetric nature of conditional probability",
    ", this computation can be used to perform both diagnosis and prediction .",
    "other common computations are : the computation of the probability of the conjunction of a set of random variables , the computation of the most likely combination of values of the random variables in the network and the computation of the piece of evidence that has or will have the most influence on a given hypothesis .    a detailed discussion of inference techniques in bayesian networks can be found in the book by pearl @xcite .    *",
    "* probabilistic semantics . * any complete probabilistic model of a domain must , either explicitly or implicitly , represent the joint distribution which the probability of every possible event as defined by the values of all the variables .",
    "there are exponentially many such events , yet bayesian networks achieve compactness by factoring the joint distribution into local , conditional distributions for each variable given its parents .",
    "if @xmath0 denotes some value of the variable @xmath1 and @xmath2 denotes some set of values for @xmath1 s parents @xmath2 , then @xmath3 denotes this conditional distribution .",
    "for example , @xmath4 is the probability of wetness given the values of sprinkler and rain . here",
    "@xmath4 is the brief of @xmath5 .",
    "the set parentheses are omitted for the sake of readability .",
    "we use the same expression in this thesis .",
    "the global semantics of bayesian networks specifies that the full joint distribution is given by the product + @xmath6 + equation [ jpd ] is also called the chain rule for bayesian networks .",
    "+    + in the example bayesian network in figure [ exabn ] , we have + @xmath7 + provided the number of parents of each node is bounded , it is easy to see that the number of parameters required grows only linearly with the size of the network , whereas the joint distribution itself grows exponentially .",
    "further savings can be achieved using compact parametric representations , such as noisy - or models , decision tress , or neural networks , for the conditional distributions @xcite .",
    "+ there are also entirely equivalent local semantics , which assert that each variable is independent of its non - descendants in the network given its parents . for example , the parents of @xmath8 in figure [ exabn ] are @xmath9 and @xmath10 and they render @xmath8 independent of the remaining non - descendant , @xmath11 .",
    "that is , + @xmath12 + the collection of independence assertions formed in this way suffices to derive the global assertion in equation [ jpd - fig1 ] , and vice versa .",
    "the local semantics are most useful in constructing bayesian networks , because selecting as parents the direct causes of a given variable automatically satisfies the local conditional independence conditions .",
    "the global semantics lead directly to a variety of algorithms for reasoning . * * evidential reasoning . * from the product specification in equation [ jpd - fig1 ] , one can express the probability of any desired proposition in terms of the conditional probabilities specified in the network .",
    "for example , the probability that the sprinkler was on , given that the pavement is slippery , is + @xmath13 + these expressions can often be simplified in the ways that reflect the structure of the network itself .",
    "+ it is easy to show that reasoning in bayesian networks subsumes the satisfiability problem in propositional logic and hence reasoning is np - hard @xcite .",
    "monte carlo simulation methods can be used for approximate inference @xcite , given that estimates are gradually improved as the sampling proceeds .",
    "( unlike join - tree methods , these methods use local message propagation on the original network structure . ) alternatively , variational methods @xcite provide bounds on the true probability . *",
    "* functional bayesian networks . *",
    "the networks discussed so far are capable of supporting reasoning about evidence and about actions .",
    "additional refinement is necessary in order to process counterfactual information .",
    "for example , the probability that `` the pavement would not have been slippery had the sprinkler been _ off _ , given that the sprinkler is in fact _ on _ and that the pavement is in fact slippery '' can not be computed from the information provided in figure [ exabn ] and equation [ jpd - fig1 ] .",
    "such counterfactual probabilities require a specification in the form of functional networks , where each conditional probability @xmath14 is replaced by a functional relationship @xmath15 , where @xmath16 is a stochastic ( unobserved ) error term . when the functions @xmath17 and the distributions of @xmath16 are known , all counterfactual statements can be assigned unique probabilities , using evidence propagation in a structure called a `` twin network '' .",
    "when only partial knowledge about the functional form of @xmath17 is available , bounds can be computed on the probabilities of counterfactual sentences @xcite @xcite . *",
    "* causal discovery .",
    "* one of the most exciting prospects in recent years has been the possibility of using bayesian networks to discover causal structures in raw statistical data @xcite @xcite @xcite , which is a task previously considered impossible without controlled experiments .",
    "consider , for example , the following pattern of dependencies among three events : @xmath18 and @xmath19 are dependent , @xmath19 and @xmath20 are dependent , yet @xmath18 and @xmath20 are independent .",
    "if you ask a person to supply an example of three such events , the example would invariably portray @xmath18 and @xmath20 as two independent causes and @xmath19 as their common effect , namely , @xmath21 .",
    "fitting this dependence pattern with a scenario in which @xmath19 is the cause and @xmath18 and @xmath20 are the effects is mathematically feasible but very unnatural , because it must entail fine tuning of the probabilities involved ; the desired dependence pattern will be destroyed as soon as the probabilities undergo a slight change .",
    "+ such thought experiments tell us that certain patterns of dependency , which are totally void of temporal information , are conceptually characteristic of certain causal directionalities and not others . when put together systematically ,",
    "such patterns can be used to infer causal structures from raw data and to guarantee that any alternative structure compatible with the data must be less stable than the one(s ) inferred ; namely , slight fluctuations in parameters will render that structure incompatible with the data . *",
    "* plain beliefs . * in mundane decision making",
    ", beliefs are revised not by adjusting numerical probabilities but by tentatively accepting some sentences as `` true for all practical purposes '' .",
    "such sentences , called plain beliefs , exhibit both logical and probabilistic characters . as in classical logic ,",
    "they are propositional and deductively closed ; as in probability , they are subject to retraction and to varying degrees of entrenchment .",
    "bayesian networks can be adopted to model the dynamics of plain beliefs by replacing ordinary probabilities with non - standard probabilities , that is , probabilities that are infinitesimally close to either zero or one @xcite . *",
    "* models of cognition .",
    "* bayesian networks may be viewed as normative cognitive models of propositional reasoning under uncertainty @xcite .",
    "they handle noise and partial information by using local , distributed algorithm for inference and learning .",
    "unlike feed forward neural networks , they facilitate local representations in which nodes correspond to propositions of interest .",
    "recent experiments @xcite suggest that they capture accurately the causal inferences made by both children and adults .",
    "moreover , they capture patterns of reasoning that are not easily handled by any competing computational model .",
    "they appear to have many of the advantages of both the  symbolic \" and the  subsymbolic \" approaches to cognitive modelling . +",
    "two major questions arise when we postulate bayesian networks as potential models of actual human cognition .",
    "+ firstly , does an architecture resembling that of bayesian networks exist anywhere in the human brain ?",
    "no specific work had been done to design neural plausible models that implement the required functionality , although no obvious obstacles exist .",
    "+ secondly , how could bayesian networks , which are purely propositional in their expressive power , handle the kinds of reasoning about individuals , relations , properties , and universals that pervades human thought ?",
    "one plausible answer is that bayesian networks containing propositions relevant to the current context are constantly being assembled as needed to form a more permanent store of knowledge .",
    "for example , the network in figure [ exabn ] may be assembled to help explain why this particular pavement is slippery right now , and to decide whether this can be prevented .",
    "the background store of knowledge includes general models of pavements , sprinklers , slipping , rain , and so on ; these must be accessed and supplied with instance data to construct the specific bayesian network structure .",
    "the store of background knowledge must utilize some representation that combines the expressive power of first - order logical languages ( such as semantic networks ) with the ability to handle uncertain information .",
    "d - separation is one important property of bayesian networks for inference . before we define d - separation , we first look at the way that evidence is transmitted in bayesian networks .",
    "there are two types of evidence :    * * hard evidence * ( instantiation ) for a node @xmath18 is evidence that the state of @xmath18 is definitely a particular value . *",
    "* soft evidence * for a node @xmath18 is any evidence that enables us to update the prior probability values for the states of @xmath18 .    * d - separation * ( definition ) :    two distinct variables @xmath22 and @xmath23 in a causal network are d - separated if , for all paths between @xmath22 and @xmath23 , there is an intermediate variable @xmath24 ( distinct from @xmath22 and @xmath23 ) such that either    * the connection is serial or diverging and @xmath24 is instantiated or * the connection is converging , and neither @xmath24 nor any of @xmath24 s descendants have received evidence .    if @xmath22 and @xmath23 are not d - separated , we call them d - connected .      based on the definition of d - seperation , three basic structures in bayesian networks are as follows :    1 .",
    "* serial connections * + consider the situation in figure [ serial - connection ] .",
    "@xmath22 has an influence on @xmath25 , which in turn has an influence on @xmath23 .",
    "obviously , evidence on @xmath23 will influence the certainty of @xmath25 , which then influences the certainty of @xmath23 .",
    "similarly , evidence on @xmath23 will influence the certainty on @xmath22 through @xmath25 . on the other hand ,",
    "if the state of @xmath25 is known , then the channel is blocked , and @xmath22 and @xmath23 become independent .",
    "we say that @xmath22 and @xmath23 are d - separated given @xmath25 , and when the state of a variable is known , we say that it is instantiated ( hard evidence ) .",
    "+ we conclude that evidence may be transmitted through a serial connection unless the state of the variable in the connection is known .",
    "+   is instantiated , it blocks the communication between @xmath22 and @xmath23.,width=188 ] 2 .",
    "* diverging connections * + the situation in figure [ diverging - connection ] is called a diverging connection .",
    "influence can pass between all the children of @xmath22 unless the state of @xmath22 is known .",
    "we say that @xmath26 are d - separated given @xmath22 .",
    "+ evidence may be transmitted through a diverging connection unless it is instantiated .",
    "+   is instantiated , it blocks the communication between its children.,width=151 ] 3 .   *",
    "converging connections * +    changes certainty , it opens for the communication between its parents.,width=151 ] + a description of the situation in figure [ converging - connection ] requires a little more care .",
    "if nothing is known about @xmath25 except what may be inferred from knowledge of its parents @xmath27 , then the parents are independent : evidence on one of the possible causes of an event does not tell us anything about other possible causes .",
    "however , if anything is known about the consequences , then information on one possible cause may tell us something about the other causes .",
    "+ this is the explaining away effect illustrated in figure [ exabn ] .",
    "@xmath8 ( pavement is wet ) has occurred , and @xmath10 ( the sprinkler is on ) as well as @xmath9 ( it s raining ) may cause @xmath8 . if we then get the information that @xmath9 has occurred , the certainty of @xmath10 will decrease . likewise ,",
    "if we get the information that @xmath9 has not occurred , then the certainty of @xmath10 will increase .",
    "the three preceding cases cover all ways in which evidence may be transmitted through a variable .",
    "in bayesian networks , 4 popular inferences are identified as :    1 .",
    "forward inference + forward inferences is also called predictive inference ( from causes to effects ) .",
    "the inference reasons from new information about causes to new beliefs about effects , following the directions of the network arcs .",
    "for example , in figure [ serial - connection ] , @xmath28 is a forward inference .",
    "2 .   backward inference + backward inferences is also called diagnostic inference ( from effects to causes ) . the inference reasons from symptoms to cause ,",
    "note that this reasoning occurs in the opposite direction to the network arcs . in figure [ serial - connection ]",
    ", @xmath29 is a backward inference . in figure",
    "[ diverging - connection ] , @xmath30)$ ] is a backward inference .",
    "intercausal inference + intercausal inferences is also called * explaining away * ( between parallel variables ) .",
    "the inference reasons about the mutual causes ( effects ) of a common effect ( cause ) . for example , in figure [ converging - connection ] , if the @xmath25 is instantiated , @xmath1 and @xmath31)$ ] are dependent .",
    "the reasoning @xmath32)$ ] is an intercausal inference . in figure",
    "[ diverging - connection ] , if @xmath22 is not instantiated , @xmath33 and @xmath34)$ ] are dependent .",
    "the reasoning @xmath35)$ ] is an intercausal inference .",
    "mixed inference + mixed inferences is also called combined inference . in complex bayesian networks ,",
    "the reasoning does not fit neatly into one of the types described above .",
    "some inferences are a combination of several types of reasoning .        * in serial connections * * the * forward inference * executes with the evidence forward propagation .",
    "for example , in figure [ sn - inference ] , consider the inference @xmath28 .",
    "is the abbreviation of @xmath36 , @xmath37 is the abbreviation of @xmath38 . for simple expression",
    ", we use @xmath39 to denote @xmath40 by default .",
    "but in express @xmath41 , @xmath22 denotes both situations @xmath42 and @xmath43 . ]",
    "+    + if y is instantiated , x and z are independent , then we have following example : + @xmath44 ; + @xmath45 ; + @xmath46 ; + @xmath47 ; + @xmath48 ; + if y is not instantiated , x and z are dependent , then + @xmath49 + @xmath50 ; + @xmath51 + @xmath52 . * * the * backward inference * executes the evidence backward propagation .",
    "for example , in figure [ sn - inference ] , consider the inference @xmath53 .",
    "if @xmath25 is instantiated ( @xmath54 or @xmath55 , @xmath22 and @xmath23 are independent , then + @xmath56 + @xmath57 ; + @xmath58 .",
    "if @xmath25 is not instantiated , @xmath22 and @xmath23 are dependent ( see the dashed lines in figure [ sn - inference ] ) .",
    "suppose @xmath59 then + @xmath60 ; + @xmath61 ; + @xmath62 ; + @xmath63 + in serial connections , there is no intercausal inference .",
    "* in diverging connections * * the * forward inference * executes with the evidence forward propagation .",
    "for example , in figure [ dn - inference ] , consider the inference @xmath64 and @xmath65 , the goals are easy to obtain by nature .",
    "+    * * the * backward inference * executes with the evidence backward propagation , see the dashed line in figure [ dn - inference ] , consider the inference @xmath66 , @xmath22 and @xmath23 are instantiated by assumption , suppose @xmath67 , @xmath68",
    ". then , @xmath69 * * the intercausal inference executes between effects with a common cause . in figure",
    "[ dn - inference ] , if @xmath25 is not instantiated , there exists intercausal inference in diverging connections .",
    "consider the inference @xmath70 , + @xmath71 ; + @xmath72 . * in converging connections , * * the * forward inference * executes with the evidence forward propagation . for example , in figure [ cn - inference ] , consider the inference @xmath66 , @xmath73 is easy to obtain by the definition of bayesian network in by nature .",
    "+    * * the * backward inference * executes with the evidence backward propagation .",
    "for example , in figure [ cn - inference ] , consider the inference @xmath74 .",
    "+ @xmath75 , + @xmath76 .",
    "+ finally , + @xmath77 , + @xmath78 . * * the * intercausal inference * executes between causes with a common effect , and the intermediate node is instantiated , then @xmath54 or @xmath79 . in figure",
    "[ cn - inference ] , consider the inference @xmath70 , suppose @xmath54 , + @xmath80 ; + @xmath81 ; + @xmath82 ; +",
    "@xmath83      for complex models in bayesian networks , there are single - connected networks , multiple - connected , or event looped networks .",
    "it is possible to use some methods , such as triangulated graphs , clustering and join trees",
    "@xcite @xcite @xcite , etc . , to simplify them into a polytree .",
    "once a polytree is obtained , the inference can be executed by the following approaches .",
    "polytrees have at most one path between any pair of nodes ; hence they are also referred to as singly - connected networks .",
    "suppose @xmath22 is the query node , and there is some set of evident nodes @xmath84 .",
    "the posterior probability ( belief ) is denoted as @xmath85 , see figure [ polytree ] .",
    "@xmath86 can be splitted into 2 parts : @xmath87 and @xmath88 .",
    "@xmath88 is the part consisting of assignments to variables in the subtree rooted at @xmath22 , @xmath87 is the rest of it .",
    "@xmath89    @xmath90 @xmath91    @xmath92 is a constant independent of @xmath22 .    where    @xmath93    @xmath94    1 .",
    "forward inference in polytree + node @xmath22 sends @xmath95 messages to its children .",
    "@xmath96 2 .",
    "backward inference in polytree node @xmath22 sends new @xmath97 messages to its parents . @xmath98\\ ] ]      various types of inference algorithms exist for bayesian networks @xcite @xcite @xcite @xcite .",
    "each class offers different properties and works better on different classes of problems , but it is very unlikely that a single algorithm can solve all possible problem instances effectively .",
    "every resolution is always based on a particular requirement .",
    "it is true that almost all computational problems and probabilistic inference using general bayesian networks have been shown to be np - hard by cooper @xcite .    in the early 1980 s",
    ", pearl published an efficient message propagation inference algorithm for polytrees @xcite @xcite .",
    "the algorithm is exact , and has polynomial complexity in the number of nodes , but works only for singly connected networks .",
    "pearl also presented an exact inference algorithm for multiple connected networks called loop cutset conditioning algorithm @xcite .",
    "the loop cutset conditioning algorithm changes the connectivity of a network and renders it singly connected by instantiating a selected subset of nodes referred to as a loop cutset .",
    "the resulting single connected network is solved by the polytree algorithm , and then the results of each instantiation are weighted by their prior probabilities .",
    "the complexity of this algorithm results from the number of different instantiations that must be considered .",
    "this implies that the complexity grows exponentially with the size of the loop cutest being @xmath99 , where @xmath100 is the number of values that the random variables can take , and @xmath101 is the size of the loop cutset .",
    "it is thus important to minimize the size of the loop cutset for a multiple connected network .",
    "unfortunately , the loop cutset minimization problem is np - hard .",
    "a straightforward application of pearl s algorithm to an acyclic digraph comprising one or more loops invariably leads to insuperable problems @xcite @xcite .",
    "another popular exact bayesian network inference algorithm is lauritzen and spiegelhalter s clique - tree propagation algorithm @xcite .",
    "it is also called a `` clustering '' algorithm .",
    "it first transforms a multiple connected network into a clique tree by clustering the triangulated moral graph of the underlying undirected graph and then performs message propagation over the clique tree .",
    "the clique propagation algorithm works efficiently for sparse networks , but still can be extremely slow for dense networks .",
    "its complexity is exponential in the size of the largest clique of the transformed undirected graph .    in general , the existent exact bayesian network inference algorithms share the property of run time exponentiality in the size of the largest clique of the triangulated moral graph , which is also called the induced width of the graph @xcite .",
    "this chapter summarizes the popular inferences methods in bayesian networks .",
    "the results demonstrates that the evidence can propagated across the bayesian networks by any links , whatever it is forward or backward or intercausal style .",
    "the belief updating of bayesian networks can be obtained by various available inference techniques .",
    "theoretically , exact inferences in bayesian networks is feasible and manageable .",
    "however , the computing and inference is np - hard .",
    "that means , in applications , in complex huge bayesian networks , the computing and inferences should be dealt with strategically and make them tractable . simplifying the bayesian networks in structures , pruning unrelated nodes , merging computing , and approximate approaches might be helpful in the inferences of large scale bayeisan networks .",
    "m. i. jordan , z. ghahramani , t. s. jaakkola and l. k. saul .",
    "an introduction to variational methods for graphical models .",
    "m. i. jordan ( ed . ) , learning in graphical models .",
    "kluwer , dordrecht , the netherlands , 1998 .",
    "jin h. kim and judea pearl . a computational model for combined causal and diagnostic reasoning in inference systems .",
    "in proceedings of the eighth international joint conference on artificial intelligence ( ijcai-83 ) , pages 190 - 193 , 1983 .",
    "morgan kaufmann .",
    "s. l. lauritzen and d. j. spiegelhalter . local computations with probabilities on graphical structures and their application to expert systems .",
    "journal of the royal statistical society , series b 50:157 - 224 , 1988 .",
    "j. pearl and t. verma . a theory of inferred causation .",
    "j. a. allen , r. fikes and e. sandewall ( eds . ) , principles of knowledge representation and reasoning .",
    "proceedings of the second international conference , pages 441 - 452 .",
    "morgan kaufmann , san mateo , ca , 1991 ."
  ],
  "abstract_text": [
    "<S> bayesian network is a complete model for the variables and their relationships , it can be used to answer probabilistic queries about them . </S>",
    "<S> a bayesian network can thus be considered a mechanism for automatically applying bayes theorem to complex problems . in the application of bayesian networks , </S>",
    "<S> most of the work is related to probabilistic inferences . </S>",
    "<S> any variable updating in any node of bayesian networks might result in the evidence propagation across the bayesian networks . </S>",
    "<S> this paper sums up various inference techniques in bayesian networks and provide guidance for the algorithm calculation in probabilistic inference in bayesian networks . </S>"
  ]
}