{
  "article_text": [
    "bootstrap resampling , or sampling with replacement from the given data , is used to mimic the sampling process that produced the original sample in the first place . by randomly drawing items from the original dataset with equal probability",
    ", one is effectively drawing a fresh sample from the `` empirical distribution '' , a multinomial with equal probability assigned to each item in the original sample .",
    "this can be seen as a best guess at the true population distribution .",
    "it has been over 30 years since @xcite introduced the bootstrap to statistics , and now there exist many different applications and related resampling techniques are found in statistics and computer science .",
    "of particular interest to researchers in pattern recognition and machine learning are the bagging ensemble method of @xcite and the .632 + validation scheme of @xcite for supervised algorithms .    in these settings , and others involving the construction of prediction rules on bootstrap samples , an important quantity of interest is the number of unique items from the original sample @xcite . in many cases , this can be seen as a limit on the amount of information carried down from the original sample . for learning algorithms trained on bootstrap samples ,",
    "the reduction in the number of unique items expected can be viewed as an effective reduction in training sample size , particularly in high dimensional problems where the prediction model is unstable with respect to changes in the training data .",
    "while generally it is a desirable property of learning algorithms that they are flexible and responsive to new information , there also exist specific cases where the unique items in a training sample are the sole determinant of the prediction rule . in ( hard - margin )",
    "support vector machine classification where the number of dimensions exceeds the number of items observed , it is always possible to divide the two classes in the training data .",
    "the prediction rule is determined only by the points that lie on the maximum margin separating hyperplane , whose duplication has no effect on its location .",
    "another simple example is found in nearest - neighbour classification and regression in continuous feature spaces where the probability of obtaining any truly identical training items is zero . without competitions between multiple prediction values due to items at exactly the same location , the resulting prediction rule is unaffected by duplications .    as the number of unique items in a bootstrap sample is an important determinant of the behaviours of prediction rules learned on it , the distribution of this quantity should be of interest to researchers working on their development and validation .",
    "while related distributions have long been studied in a purer mathematical context @xcite , and this distribution has been identified before in this setting @xcite , nowhere were we able to find a concise and accessible summary of the relevant information for the benefit of researchers in machine learning .",
    "our aim here is to fill this gap by presenting this distribution along with its key properties , and to make it easier for others who to understand or modify resampling techniques in a machine learning context .    in section [ background ]",
    ", we discuss the relevance of the number of unique items to the bagging ensemble method and the .632 + validation scheme . in section [ present_dist ]",
    "we give a closed form of this distribution together with its notable properties .",
    "we give an empirically derived rule to decide whether normal approximation is permissible , and describe how we produced it . in section [ multi ] , we consider the case where the items in the original sample belong to one of several categories , as is the case in classification problems . here , where the outcome is the vector of the number of unique original items from each category , we show that the limit distribution is multivariate normal , and consider limits to produce a heuristic for the normal approximation of the number of unique items from a single category .",
    "bootstrap aggregation , or bagging , uses the perturbed samples generated by sampling with replacement to produce a diverse set of models which can then be combined to promote stability @xcite . though a given model may be over - fit to its sample , the combination should less reflect those unstable aspects particular to an individual sample @xcite .",
    "perhaps its most important use in pattern recognition has been in random forest classification and regression @xcite , where it is key to allowing the use of flexible decision trees without over - fitting .",
    "while bagging is still most commonly performed by drawing @xmath0 items with replacement , the way in which the samples are drawn is not necessarily fixed .",
    "the number of unique items present in a bootstrap sample has been identified as an important predictor of algorithm performance , and the sampling method can be purposefully modified to control its range and variability @xcite .",
    "an added bonus of bagging is the possibility of producing a performance estimate for the ensemble without doing additional cross validation @xcite .",
    "predictions are made for items in the training sample using only those models which did not use them in training . in this way , it is possible to get a performance estimate for an ensemble without the bias of over - fitting . a pessimistic bias has been observed in high dimensional classification problems that was ameliorated using different sampling techniques @xcite , suggesting the importance of unique items in this phenomenon .",
    "there is no one dominant scheme for the statistical comparison of learning algorithms on small to moderately sized datasets , and researchers must choose between different forms of hold out tests and cross validation .",
    "one option available to them is the .632 + bootstrap of @xcite .",
    "this , and the earlier .632 scheme @xcite , extend the bootstrap to the validation of learned prediction rules . both of these are functions of a quantity called the leave - one - out bootstrap error , which is the average error of the prediction models constructed on many bootstrap samples . to avoid the optimistic bias of testing on the training set ,",
    "the models are tested only on those points not in their respective bootstrap samples .    as in bagging",
    ", the prediction rules created can have an effectively reduced training sample size in line with the lower number of unique original items in their samples .",
    "the expectation of the leave - one - out error then becomes a weighted average of the expected algorithm performance at the different sizes given by the distribution of that quantity . unlike conventional cross validation schemes where the reduced sample size is obvious , users of this technique",
    "may not have the effective reduction in mind . while both the .632 and .632 + methods attempt to correct for the pessimistic bias that may result , the true corrective mapping of this value will be problem specific and can not be known ahead of time .",
    "this is almost certainly the cause of the bias observed by @xcite .",
    "knowledge of the distribution of the effective sample sizes in these schemes should be useful to those interpreting the results of these schemes , particularly when there is concern about bias .",
    "in this section , we first provide a closed form of the distribution of the number of unique items and describe its connection to the family of occupancy distributions . with this information ,",
    "we are then able to derive the integer moments and pormal limit .",
    "finally , we describe the construction of a heuristic to decide when normal approximation is appropriate .",
    "some illustrations of the distribution are given in fig .",
    "[ onedim ] .    where @xmath1 items are taken with replacement from a sample of size @xmath0 , the probability of obtaining @xmath2 unique original items is @xmath3 where @xmath4 , @xmath5 is the falling factorial , and @xmath6 is a stirling number of the second kind .",
    "equivalently , this is the probability of obtaining @xmath2 unique outcomes after @xmath1 categorical trials in a multinomial problem with @xmath0 equally likely outcomes .    because each of the @xmath0 items is equally likely to be drawn at each of the @xmath1 sampling events , there are @xmath7 equally likely bootstrap samples that may be drawn if order is accounted for .",
    "the number of ways to make an ordered selection of @xmath2 unique items from the original @xmath0 to be present in the bootstrap sample is @xmath5 , the falling factorial .",
    "this is defined as follows : @xmath8 having already considered the ordering of the @xmath2 unique items to be included means that we may consider them unlabelled in the final step ; here , we consider the number of ways to take @xmath1 distinguishable draws from @xmath2 unlabelled unique items such that at least one draw is taken from each . in more generic terms",
    ", this is the number of ways to place @xmath1 labelled items into @xmath2 subsets such that none of them are empty .",
    "this quantity is written as @xmath6 , and is called a stirling number of the second kind ( * ? ? ?",
    "* chapter  6 ) . combining these three results",
    "gives us eq . .",
    "the number of unique items present may be viewed as the sum of the set of @xmath0 indicator functions @xmath9 .",
    "the indicator @xmath10 corresponds to original item @xmath11 , taking the value of one if this is present and zero if not .",
    "a simple consideration of the sampling process allows one to find the mean and covariance of the indicator functions .",
    "as all points are equally likely to be selected , the chance of an item not being selected at a given sampling trial is @xmath12 . in order for @xmath10 to be zero ,",
    "its corresponding point must not be selected at all @xmath1 events .",
    "this gives us @xmath13 .",
    "as the indicators are binary variables , this is sufficient to determine their mean and variance :    @xmath14 & = { \\text{p}}(d_i = 1 ) = 1 - { \\text{p}}(d_i = 0 ) \\\\                 & = 1 - \\big ( 1 - \\frac{1}{n } \\big)^a , \\\\        \\text{var}[d_i ] & = { \\text{p}}(d_i = 0 ) \\cdot { \\text{p}}(d_i = 1 ) \\\\                 & = \\big ( 1 - \\frac{1}{n } \\big)^a - \\big ( 1 - \\frac{1}{n } \\big)^{2a}.      \\end{split}\\ ] ]    a similar consideration gives us the probability of two items , @xmath11 and @xmath15 ( where @xmath16 ) both being excluded from a bootstrap sample .",
    "this is given @xmath17 , and is enough to give us the covariance of two indicator functions ,    @xmath18 & = { \\text{p } } ( d_i = 1 , d_j = 1 ) - \\big({\\text{p}}(d_i = 1)\\big)^2 \\\\                       & = \\big(1-\\frac{2}{n}\\big)^a - \\big(1-\\frac{1}{n}\\big)^{2a}.      \\end{split}\\ ] ]    now that we know the mean , variance and covariance of the indicators , we have enough information to determine the same quantities for any linear combination of them , including their sum , @xmath2 .",
    "of particular interest to us is the case where the number of samples drawn is proportional to the number of original items .",
    "for this reason , along with the general formulae , we also provide limits for the mean and variance as @xmath0 and @xmath1 jointly approach infinity :    @xmath19 & = n(1 - \\big(1 - \\frac{1}{n}\\big)^a ) = \\frac { n^a - ( n-1)^a } { n^{a-1 } } \\\\               & \\to n(1 - e^{-\\alpha } ) , \\\\          \\text{var}[k ] & = n(n-1)\\big(1-\\frac{2}{n}\\big)^a + n\\big(1 - \\frac{1}{n}\\big)^a - n^2\\big(1-\\frac{1}{n}\\big)^{2a } \\\\                 & \\to n(e^{-\\alpha } - ( 1+\\alpha)e^{-2\\alpha } ) ,      \\end{split}\\ ] ]    where the limits refer to the case @xmath20 and @xmath21 .",
    ", scaledwidth=55.0% ]      while we came to the problem of item inclusion through machine learning , the study of this problem far predates our field of research @xcite .",
    "if , instead of the number of items included in the bootstrap sample , we were to consider the number of items _ excluded _ , we would be studying one of the family of occupancy distributions @xcite .",
    "these arise in the study of urn problems when @xmath1 balls are independently and randomly placed into @xmath0 urns in such a way that each ball is equally likely to be placed into any one of the urns .",
    "the @xmath22 occupancy distribution then details the probabilities of obtaining different values of @xmath23 , the number of urns containing exactly @xmath11 balls . in our problem ,",
    "the @xmath1 drawing events are the balls of the urn model , and the @xmath0 original items that may be selected are the urns with which the balls / draws can be associated .",
    "the number of empty urns is @xmath24 , and it has the same distribution as the number of excluded items in our sampling problem . as @xmath2 is just @xmath25 , it is easy to drawn upon useful results .      while there exist a great many limit theorems for occupancy distributions ( *",
    "* chapter  6 ) , in our case we need only the earliest .",
    "@xcite proved that the limit distribution of @xmath24 was normal in the case that @xmath20 , and @xmath21 .",
    "as @xmath2 is distributed as @xmath25 , it too must be normally distributed .",
    "the @xmath26 raw moment of the distribution of @xmath2 is @xmath27 a proof for this is provided in appendix [ rawproof ] .",
    "@xcite provides a formula for the central moments of the @xmath28 occupancy distribution , which for us corresponds to the number of excluded items . by simplifying this and adapting",
    "the sign to the distribution of the number of items _ included _ , we can give the @xmath26 central moment as @xmath29    in both of these formulae the sum over @xmath30 and @xmath31 comprises @xmath32 unique locations .",
    "reassuringly , they provide the identity @xmath33 , as well as results for the mean and variance that match up to those derived by the considerations of [ indicators ] .",
    "the stirling numbers quickly overflow conventional precision , making the form of the distribution in eq . hard to work with at reasonable values of @xmath0 and @xmath1 .",
    "while it is known that the distribution does converge to normal , this is not practically useful unless one knows when a normal approximation is appropriate . to allow others to use a normal approximation with confidence",
    ", we decided to build a heuristic based on existing rules of thumb for normal approximation of the binomial distribution .    where the number of bernoulli trials is @xmath34 and the probability of success is @xmath35 at any one , a normal approximation works best when the @xmath34 is high and @xmath35 is not too close to zero or one . while there is a degree of arbitrariness in deciding when two distributions are `` close enough '' , there have long existed conventions for doing so .",
    "two of the most common such rules from applied statistics @xcite are @xmath36 in order to build our own rule for approximation of @xmath37 , we decided first to measure the convergence of the true distribution and its normal approximation to measure the quality of the approximation of the binomial case under the rules of .",
    "we could then experiment with building rules to permit approximation of the distribution of @xmath2 such that convergence was always better than the worst case seen for the binomial approximation . to measure the quality of approximation , we chose a metric closely linked to the concept of convergence in distribution : the maximum absolute difference in cumulative distribution ( madcd ) between the true distribution and its continuity - corrected normal approximation on the set of all possible outcome values .",
    "the continuity correction was performed using a shift of @xmath38 .",
    "computation of the exact value of the true distribution was performed using the symbolic mathematics toolkit of the matlab software package .    to find the worst permissible binomial approximation , we measured the madcd between the binomial distribution and its continuity - corrected normal approximation across the grid of parameters specified by @xmath39 and @xmath40 .",
    "we combined the two rules of eq . to produce a third , stricter rule , and found a maximum madcd of 0.0205 within the resulting acceptance region .",
    "as this value occurred far from the boundary of the allowed set , we think it unlikely that expanding the grid would produce higher values .",
    "we then mapped the madcd for @xmath37 and its continuity - corrected normal approximation across the grid of parameters @xmath41 and @xmath42 , so that we could begin constructing a rule .",
    "the results can be seen in fig .",
    "[ madcd_grid ] . after inspecting this map ,",
    "we decided to construct rules for the minimum permissible @xmath1 at a given @xmath0 ( and vice versa ) , of the form @xmath43 .",
    "while there are areas near @xmath44 and @xmath45 where the madcd value is low , this did not represent convergence to normal , rather , most of the probability mass has collapsed to a single value of @xmath2 .",
    "for this reason , we chose to focus on the central valley of good approximation near the axis @xmath46 .    , scaledwidth=55.0% ]    we were mainly concerned with producing a reliable rule that would not fail at parameter setting outside the mapped grid ; while we experimented with different fitting methods , we chose to select the rule parameters by hand so as to have greater control over features likely to make it more conservative .",
    "this also made it easier to restrict the rule parameters to those fully specified by two digits of decimal precision .",
    "we produced a rule such that    * all madcd values in the acceptance region were below the specified level of 0.0205 , * the boundaries of the acceptance region did not cross the ridges of the distribution as @xmath0 and @xmath1 increase , unless it was to move towards the central valley , and * the values on the boundary had to appear to be decreasing with increasing @xmath0 and @xmath1 .",
    "we excluded the smallest values of @xmath1 and @xmath0 in order to better fit of the general pattern .",
    "the resulting rule is that a normal approximation is permissible when @xmath47 the acceptance region specified and the values of madcd at its boundaries can be seen in fig .",
    "[ madcd_grid ] and [ madcd_grid_bound ] respectively .",
    "in addition to the madcd , we also computed the jensen - shannon divergence between the true probability distribution and the normal approximation .",
    "as this is function of a probability distribution itself , rather than a cumulative distribution , it was necessary to discretise the normal approximation .",
    "this was done by defining the probability mass at a location as the difference in the continuity - corrected cumulative distribution between that location and the next .",
    "we found a maximum value of 0.0444 within the allowed region for the binomial distribution , and 0.0631 for the allowed region for the distribution of @xmath2 .",
    "all the highest divergence values for @xmath37 and its approximation occurred on the boundaries near the very lowest allowed values of @xmath1 and @xmath0 .",
    ", scaledwidth=55.0% ]",
    "in some machine learning tasks , notably in supervised classification , items in the sample belong to one of several categories .",
    "it is then of interest to those studying bootstrap techniques to know the distribution of the balance of the unique items drawn from each category .",
    "we consider the case where there are @xmath48 categories present , the @xmath49 of which contains @xmath50 items in the original sample of @xmath0 . when @xmath1 samples are drawn without replacement , the distribution can be found by marginalising over the number of draws taken from each category , the various @xmath51 .",
    "the probability of obtaining the vector @xmath52 detailing the number of items @xmath53 from each of category is    @xmath54    an example of this distribution in the case of two categories is illustrated in fig .",
    "[ twodim ] .    ,",
    "scaledwidth=55.0% ]      from consideration of the mean and covariance of the individual items participations as in section [ indicators ] , it is straightforward to derive the mean , variance and covariance ; these are the following : @xmath55 & = n_i ( 1 - \\big(1 - \\frac{1}{n}\\big)^a ) , \\\\          \\text{var}[k_i ] & = n_i(n_i-1)\\big(1-\\frac{2}{n}\\big)^a + n_i\\big(1 - \\frac{1}{n}\\big)^a - n_i^2\\big(1-\\frac{1}{n}\\big)^{2a } , \\\\          \\text{cov}[k_i , k_j ] & = n_in_j\\big[\\big(1-\\frac{2}{n}\\big)^a - \\big(1-\\frac{1}{n}\\big)^{2a } \\big ] , \\text { for $ i \\neq j$. }      \\end{split}\\ ] ]      not only is the total number of unique items from the original sample asymptotically normally distributed under appropriate conditions , but so is the number from the subset belonging to each category . to show this , we must turn to limit theorems for dependent variables rather than the study of urn problems .",
    "the covariance between the presence indicators of section [ indicators ] is always negative ( see eq . ) .",
    "as these are binary variables , this means that the joint distribution of any two indicators @xmath10 and @xmath56 ( @xmath16 ) must meet the condition @xmath57 for any values @xmath58 and @xmath59 .",
    "any collection of the indicators can then be termed a pairwise negative quadrant dependent ( pnqd ) sequence @xcite .",
    "weighted sums of pnqd sequences are asymptotically normally distributed as the number of items in the sequence goes to infinity @xcite .",
    "the number of unique items present from the subset of a category is a weighted sum of the indicators , and so its asymptotic distribution is normal as the number of items present and items drawn jointly approach infinity . the number of items in a category must approach infinity too , as the number of possible values that the sum can take must also go to infinity for its discrete distribution to converge to a continuous one .",
    "this is guaranteed in the case where each category contains a fixed fraction of the total number of items .    to show the limit of the joint distribution of the numbers of unique items from the categories is multivariate normal",
    ", we can consider the set of weighted sums that give equal weight to all items within a category . if the weight given to items in category @xmath60 is @xmath61 and we add the constraint @xmath62 , then the distributions of this set still include all possible projections of the multivariate distribution of @xmath63 .",
    "as all projections of this distribution are normal , then it itself must be multivariate normal @xcite .",
    "we now consider when it is appropriate to approximate the distribution of the number of unique items from a subset of the data with a normal distribution .",
    "we consider the case where the original items has @xmath0 items of which @xmath50 belong to a particular subset of fixed size , and where the number of items drawn is @xmath20 . in the case where @xmath64",
    ", we have the heuristic developed in [ heuristic ] .",
    "as @xmath21 while @xmath50 remains fixed , the correlation between the indicator functions of the items of category @xmath60 goes to zero and the distribution of @xmath53 approaches a binomial distribution with @xmath50 trials and a probability of success @xmath65 .",
    "we argue that if both the distribution with @xmath66 and that with @xmath21 are well approximated by the binomial , it is highly likely that the intermediate stages are too .",
    "this suggests the following rule for normal approximation of @xmath53 : if the number of samples drawn is proportional to the number of items ( @xmath20 ) and the expected number drawn from the category @xmath60 is @xmath67 , then if both the rules for approximation of the binomial limit ( with @xmath68 and @xmath69 ) are met , and the rule for the single category case with @xmath0 and @xmath1 substituted for @xmath50 and @xmath70 respectively , then a normal approximation should be appropriate for @xmath53 for any value of @xmath0 .",
    "the resulting rule requires four inequalities to be met , which makes it a little unintuitive and tedious to apply .",
    "as the inequalities due to the consideration of the binomial limit only have a small effect on the acceptance region near the limit of lowest @xmath50 / highest @xmath71 , and eventually become redundant as @xmath50 and @xmath71 increase , it is possible to account for them with a simple offset of the rule for the single category case .",
    "a small strip of @xmath71@xmath50 parameter space is then sacrificed for the sake of simplicity .",
    "we find a normal approximation to be permissible when @xmath72 this has no solutions with @xmath73 or @xmath74 .",
    "we have aimed to make a clear and concise answer to the titular question readily available to machine learning researchers .",
    "we have summarised the key properties of this distribution , and provided practical information about when a normal approximation is appropriate in the form of a heuristic , allowing others to justify its use . with particular consideration to classification problems",
    ", we have considered the generalisation of this distribution to the scenario where items come from multiple categories . in this case , we have provided a theoretical guarantee of asymptotic normality and a considered heuristic rule for approximation based its limit distributions .",
    "we hope our results rovide a useful resource to researchers interested in understanding or modifying the number of unique items to appear under random sampling with that replacement .",
    "this work is funded by ucl ( code elcx ) , a case studentship with the epsrc and ge healthcare , epsrc grants ( ep / h046410/1 , ep/ h046410/1 , ep / j020990/1 , ep / k005278 ) , the mrc ( mr / j01107x/1 ) , the eu - fp7 project vph - dare@it ( fp7-ict-2011 - 9 - 601055 ) , the nihr biomedical research unit ( dementia ) at ucl and the national institute for health research university college london hospitals biomedical research centre ( nihr brc uclh / ucl high impact initiative ) .    10    a.  abadie and g.  w. imbens . on the failure of the bootstrap for matching estimators . , 76(6):15371557 , 2008 .",
    "g.  box , w.  g. hunter , j.  s. hunter , et  al . .",
    "john wiley and sons new york , 1978 .",
    "l.  breiman .",
    "out - of - bag estimation . technical report , citeseer .    l.  breiman . bagging predictors . , 24(2):123140 , 1996 .    l.  breiman . random forests .",
    ", 45(1):532 , 2001 .",
    "p.  bchlmann and b.  yu . analyzing bagging . ,",
    "pages 927961 , 2002 .",
    "b.  efron .",
    "bootstrap methods : another look at the jackknife . , pages 126 , 1979 .",
    "b.  efron .",
    "estimating the error rate of a prediction rule : improvement on cross - validation .",
    ", 78(382):316331 , 1983 .",
    "b.  efron and r.  tibshirani .",
    "improvements on cross - validation : the 632 + bootstrap method .",
    ", 92(438):548560 , 1997 .    a.  gut .",
    "springer , 2009 .",
    "n.  l. johnson and s.  kotz . .",
    "wiley , 1977 .",
    "estimating classification error rate : repeated cross - validation , repeated hold - out and bootstrap .",
    ", 53(11):3735  3745 , 2009 .",
    "d.  e. knuth , r.  graham , and o.  patashnik . .",
    "addison , 1989 .",
    "li and j .- f .",
    "an application of stein s method to limit theorems for pairwise negative quadrant dependent random variables .",
    ", 67(1):110 , 2008 .",
    "m.  w. mitchell .",
    "bias of the random forest out - of - bag ( oob ) error for certain input parameters .",
    ", 1:205 , 2011 .",
    "j.  muoz - garca , r.  pino - mejias , j.  munoz - pichardo , and m.  cubiles - de - la - vega .",
    "identification of outlier bootstrap samples .",
    ", 24(3):333342 , 1997 .",
    "r.  pino - mejas , m .- d .",
    "jimnez - gamero , m .- d .",
    "cubiles - de - la vega , and a.  pascual - acosta . reduced bootstrap aggregating of learning algorithms .",
    ", 29(3):265271 , 2008 .    c.  r. rao , p.  pathak , and v.  koltchinskii",
    ". bootstrap by sequential resampling .",
    ", 64(2):257  281 , 1997 .",
    "m.  schader and f.  schmid .",
    "two rules of thumb for the approximation of the binomial distribution by the normal distribution . , 43(1):2324 , 1989 .",
    "i.  weiss . limiting distributions in some occupancy problems .",
    ", pages 878884 , 1958 .",
    "before proceeding , we shall need two intermediate results .",
    "* add one to the upper argument .",
    "* subtract one from the lower argument and multiply by minus one .",
    "* multiplication by offset between @xmath2 and the second argument of the stirling number ( i.e. , the initial @xmath78 plus the number of @xmath79 operations that have occurred to the number thus far ) .    by multiplying by @xmath80",
    ", we sequentially apply all these operations @xmath75 times . in a way similar to pascal s triangle",
    ", we can see this triplication as carrying a `` charge '' of stirling numbers down to the next level of a pyramid ( see _ a ) _ in fig .",
    "[ pyramid ] .",
    "locations in the pyramid can be described by the number of times each operator has been applied to all charge that reached them .",
    "we shall call these coordinates @xmath81 , @xmath82 and @xmath31 .",
    "any given selection of @xmath75 populates the pyramid down to the level described by @xmath83 . at any point in the pyramid ,",
    "the arguments of the stirling numbers are determined only by the coordinates .",
    "if these operators commuted , we could consider a flow of charge exactly the same as in pascal s triangle , and apply the effects of the operators on the coefficients of the stirling numbers separately at the end .",
    "as @xmath79 and @xmath84 do not commute with one another , we must consider their interaction and how they change the charge passing through them .",
    "@xmath85 commutes with the other operators and does not change the coefficient of the charge , so we can `` factorise '' the pyramid .",
    "that is , we need only consider the number of ways to select @xmath86 non-@xmath85 operations , and then to sum over all possible orders of choosing @xmath82 and @xmath81 operations of @xmath79 and @xmath84 respectively .",
    "the amount of charge at a location in the pyramid will be @xmath87 , where @xmath88 is the coefficient from the charge triangle of the operators @xmath79 and @xmath84 alone . to determine what this is",
    ", we use the fact that the @xmath78 in eq .",
    "is initially zero . see _",
    "b _ in fig . [ pyramid ] for illustration .",
    "this gives us the initial conditions @xmath89 and @xmath90 for @xmath91 .",
    "together with the recurrence @xmath92 , these are enough to uniquely specify @xmath93 .",
    "this then gives us eq . .",
    "we begin with the standard relation @xcite @xmath95 from eq . , it is clear that @xmath96 for values of @xmath2 higher than @xmath0 .",
    "consistent with their recurrence relation and combinatorial meaning , the stirling numbers @xmath6 can be defined as zero for positive values of @xmath2 outside @xmath97 @xcite . as",
    "one of these two factors is zero whenever @xmath2 exceeds either @xmath1 or @xmath0 , we can truncate the sum in [ standardsum ] to the smaller of those without effect .",
    "we can therefore write @xmath98 if we combine this with the knowledge that @xmath99 we can then produce eq . .    if we take the definition of the moment as in the first line of eq .",
    ", apply eq .",
    ", reverse the order of summation , and then apply eq .",
    ", we are then able to derive the final form for the raw integer moments seen in the second line ( of eq . [ moments ] ) ."
  ],
  "abstract_text": [
    "<S> sampling with replacement occurs in many settings in machine learning , notably in the bagging ensemble technique and the .632 + validation scheme . </S>",
    "<S> the number of unique original items in a bootstrap sample can have an important role in the behaviour of prediction models learned on it . indeed , there are uncontrived examples where duplicate items have no effect . </S>",
    "<S> the purpose of this report is to present the distribution of the number of unique original items in a bootstrap sample clearly and concisely , with a view to enabling other machine learning researchers to understand and control this quantity in existing and future resampling techniques . </S>",
    "<S> we describe the key characteristics of this distribution along with the generalisation for the case where items come from distinct categories , as in classification . in both cases </S>",
    "<S> we discuss the normal limit , and conduct an empirical investigation to derive a heuristic for when a normal approximation is permissible .    </S>",
    "<S> * keywords : * bootstrap , resampling , sampling with replacement , bagging </S>"
  ]
}