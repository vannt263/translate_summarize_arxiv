{
  "article_text": [
    "it is well - known that classification of objects in astronomical survey data provides a fundamental characterization of the dataset , and forms a vital step in understanding the ensemble properties of the dataset and the properties of individual objects . in this paper , we apply automated machine learning algorithms to classify the 143 million non - repeat objects in the third data release ( dr3 ; * ? ? ?",
    "* ) of the sloan digital sky survey ( sdss ; york et al .",
    "the sdss is used because the combination of the quality and accuracy of the survey ccd photometry , the approximately concurrent spectroscopy performed with the same telescope , and the large number of objects , is ideally suited to the methods we employ in this work .    as our primary goal is to perform reliable star - galaxy separation ,",
    "we have used three classes : _ galaxy _ , _ star _ , and _ nsng _ ( neither star nor galaxy ) .",
    "the use of this third class is novel , and is included to not only cleanly separate out those objects that clearly are neither stars nor galaxies ( e.g. , quasars ) , but to also improve the extrapolation of our classifications well beyond the nominal magnitude limits of our current training sample .",
    "the latter benefit arises since star - galaxy separation becomes less accurate at fainter magnitudes due to source confusion . in this case",
    ", our algorithm can assign sources to the third category when a clear star - galaxy classification is not feasible , thereby minimizing contamination for lower signal - to - noise sources .    in this paper ,",
    "the learning algorithm we employ is the axis - parallel decision tree .",
    "a decision tree is trained on the subset of data for which spectra are available and is subsequently used to classify the much larger sample for which there is just photometry .",
    "this is the first public release of objects classified using this method for the entire sdss dr3 .",
    "a natural question is `` why provide classification for all objects , most of which are near the photometric limits of the survey , where classifications are less robust ? '' we have three reasons for classifying the entire sdss dr3 .",
    "first , given the speed of applying our classifications , we have found that it is considerably easier to handle large datasets when no artificial cuts are used . with the growth in the number of large astronomical surveys that are accessible from a virtual observatory , we feel this point is becoming increasingly more important , as it greatly simplifies any data federation issues a fellow scientist may need to perform if they wish to use our classifications .",
    "second , classifying all objects allows others with fainter training data to verify the efficacy of our classifications . finally , since our classifications are probabilistic in nature , by providing the classifications for all objects , we allow others to make their own probability cuts , perhaps using combinations of classes , to perform their own science .",
    "for example , our approach simplifies the task of performing follow - up spectroscopy , as a user of our fully classified catalog has an easier task in characterizing any selection effects .",
    "decision tree learning algorithms have been in existence since the 1970s , see , e.g. , @xcite or @xcite .",
    "they are able to classify objects into discrete or continuous categories , scale well to large datasets , are fairly robust to inequitably - sampled training sets , and can cope with values from individual objects that happen to be bad or irrelevant for the training targets being considered .",
    "axis - parallel decision trees , particularly smaller ones , are also easily understood , providing clear criteria for the splitting of the data set into constituent classes or values .",
    "this is in contrast to , for example , artificial neural networks ( e.g. , * ? ? ?",
    "* ) , where the resulting weights are basically a _ black box _ , or oblique decision trees ( e.g. , * ? ? ?",
    "* ) , where linear combinations of parameters , or hyper - planes , are used to subdivide the data .",
    "given the size of our dataset , however , our trees are generally rather large , making it difficult to fully interpret the classification rules , but one can in principle follow the splitting of the population . while oblique trees may be less transparant in their operation , they do have the benefit of generally producing smaller trees .",
    "most astronomical results using neural networks and decision trees show that the two methods give comparable results ",
    "an example is @xcite , described below .",
    "a number of previous efforts have used decision trees to classify objects in astronomical surveys .",
    "@xcite use decision trees to separate stars and cosmic ray hits in hubble space telescope images and are able to do so with an accuracy of over 95% .",
    "their algorithm , known as oc1 , is an oblique decision tree .",
    "@xcite use a version of oc1 to classify galaxy morphologies in the european southern observatory ",
    "uppsala surface photometry catalog @xcite and show their results to be comparable to those of @xcite , who use artificial neural networks on the same data .",
    "@xcite construct a radio - selected sample of quasars using the vla - first survey @xcite and the optical apm - poss - i quasar survey of @xcite .",
    "they show that using decision trees improves on simpler methods and enables quasars to be selected with 85% efficiency .",
    "they also show the completeness and efficiency of the selection as a function of threshold probability from the decision tree that the object is a quasar .",
    "@xcite compare naive bayes , decision tree and neural network classifiers for morphological galaxy classification and show that the latter two are comparable .",
    "@xcite take advantage of the much improved data , both in quality and quantity of objects , available from the sdss .",
    "they apply the oblique decision tree classifier classx , based on oc1 , to the sdss dr2 @xcite to classify objects into star , red star ( type m or later ) , agn photometric redshift and galaxy photometric redshift bins .",
    "three representative samples , each of @xmath3 photometric objects , one of which extends 2 magnitudes fainter than the spectroscopy , are also investigated .",
    "they give a table of percentage correct classifications from which one can calculate completeness and efficiency .",
    "many studies , too numerous to list here , use other machine learning techniques to classify astronomical survey objects , including star - galaxy separation and quasar identification . these previous works , however , do not provide spectroscopically - trained classifications with probabilities on the scale of the 143 million sources given here for the sdss dr3 . as part of our work ,",
    "we make publicly available the full set of our 143 million object classifications for the sdss dr3 and probability cuts to select galaxies and stars for a range of completenesses and efficiencies .",
    "the rest of the paper is as follows . in ",
    "[ sec : data ] we describe the datasets used in the construction and testing of the catalog .",
    " [ sec : algorithms ] describes the decision trees , their optimization using supercomputing resources , and the probability cuts to create subsamples of particular objects . ",
    "[ sec : results ] then describes the optimal learning parameters found and the resulting datasets . in ",
    "[ sec : discussion ] we discuss the efficacy of our star - galaxy separation , the extrapolation of our results to fainter magnitude , and new projects we have undertaken to extend this work .",
    "we conclude in  [ sec : conclusions ] .",
    "the sdss is a project to map @xmath4 steradians of the northern galactic cap in five bands ( @xmath5 , @xmath6 , @xmath7 , @xmath8 and @xmath9 ) from 35008900  .",
    "the final survey will provide photometry for approximately @xmath10 objects , of which a multifiber spectrograph will provide spectra and redshifts for around one million .",
    "the spectroscopic targeting pipeline selects targets for spectroscopy from the imaging @xcite , and a tiling algorithm @xcite optimally assigns spectroscopic fibers to the selected targets .",
    "further details of the survey are given in @xcite and in @xcite for dr3 .",
    "the datasets we use in this work consist of training , testing , blind testing , and application sets .",
    "the training and testing sets are drawn from objects in the ` specobj ` view of the dr3 catalog archive server ( cas ) public database .",
    "the blind testing sets are from the sdss dr3 , 2df galaxy redshift survey ( 2dfgrs ; * ? ? ?",
    "* ) final data release and the 2df qso redshift survey ( 2qz ; * ? ? ?",
    "* ) final data release .",
    "the application set is a locally hosted version of the ` photoprimary ` view of the dr3 cas . as with ` specobj ` , this is the view which contains the primary sdss observations with no duplicates .",
    "the raw ` specobj ` data consists of 477,081 objects and the ` photoprimary ` data 142,705,734 .",
    "the training , testing and application data consist of sdss photometric and spectroscopic attributes available in the ` photoprimary ` and ` specobj ` tables .",
    "we perform no additional image or spectral reduction .",
    "since we classify all sources in the sdss dr3 , this allows a user of our classifications to apply their own sample cuts based on sdss flags or attribute errors .",
    "the photometric attributes used for each object are the colors @xmath11 , @xmath12 , @xmath13 , and @xmath14 , corrected for galactic reddening via the maps of @xcite .",
    "the colors in each of the four magnitude types psf , fiber , petrosian and model are used .",
    "the use of the four magnitude types allows star - galaxy separation to be , to some extent , built - in .",
    "the light profiles of galaxies and stars are very different , which can be quantified by measuring the object flux in different - sized apertures .",
    "this fact is often used to perform star - galaxy separation , including within the sdss photometric pipeline where the difference between @xmath15 and @xmath16 is used ( see   [ sec : algorithms ] for more information ) .",
    "the spectroscopic attribute is the target object type and is given by the ` specclass ` value in ` specobj ` assigned by the sdss spectroscopic pipeline .",
    "the ` specclass ` is discrete and takes the values ` unknown ` , ` star ` , ` galaxy ` , ` qso ` , ` qso_hiz ` or ` star_late ` .",
    "we construct our three training samples : galaxies , stars , and nsng , from these classes according to the mappings shown in table [ table : mappings ] .",
    "resulting numbers of each are 361,502 galaxies , 62,333 stars and 53,246 nsng , giving a total of 477,081 objects available for the training and testing process . given the spectroscopic data available in the sdss , the nsng training sample will predominantly consist of quasars .",
    "decision trees , however , effectively utilize all of their training data . thus , at fainter magnitudes , we can expect the nsng sample to contain sources that are not just quasars , but unknowns in conjunction with the ` unknown ` category in the ` specclass ` .",
    "on the other hand , decision trees are also robust to outlying values in their training data . for the classification training",
    "we simply apply the very broad cuts @xmath17 to remove clearly unphysical values from the data such as -9999 .",
    "these are applied to each of the colors and resulted in the removal of 12 galaxies and 1 quasar , leaving 477,068 for training and testing .    for the decision tree optimization",
    ", sets of objects were randomly drawn from the remaining ` specobj ` set and used in training , with the rest for testing .",
    "thus the testing set , in a strict sense , is not independent of the training set .",
    "however , the large numbers of objects available in the training data means that the random subsamples drawn , except perhaps the smallest ones at a level of 10% or less , are large enough to still be truly representative of the training data and should thus be effectively independent .",
    "the performance results quoted in  [ sec : results ] are , nevertheless , from a tree trained _ and _ tested on 80% of the training data and applied to the other 20% .",
    "the 80% is subdivided as above into non - overlapping training and testing sets , also in the ratio 80:20 .",
    "given the hypothesized easily - adequate size of the training set , this pseudo - blind test should be very similar to the test results during the optimization .",
    "for the final application to the ` photoprimary ` data , the tree was trained on all of the training data to maximize the available information .",
    "thus the results quoted in  [ sec : results ] may be slightly pessimistic , but again they are likely to be very similar to the performance of the tree on ` photoprimary ` , within the spectroscopic regime . for the vast majority of objects in the ` photoprimary ` table , no spectroscopic information is available . in addition",
    ", we do not restrict the application dataset , as each object is classified independently .",
    "therefore , specific cuts , either on sdss flags , attribute errors , or the attributes themselves can be later applied by a user as appropriate to create a particular sample .",
    "for example , one might only want ` best ' photometry , as indicated by the object flags , for comparing object populations , or one might want to apply a color cut to enhance the utility of the provided classifications .",
    "the blind testing sets are drawn from the 2dfgrs and the 2qz .",
    "these test the performance of the decision tree on non - sdss data in a parameter space which extends beyond the sdss dr3 spectroscopic regime .",
    "the 2dfgrs was used to test the best tree s galaxy classifications and the 2qz to test its star and nsng classifications .",
    "the surveys were matched with the sdss using object position with a tolerance of 2 arcseconds and their spectroscopic classifications were taken as the target types . of the 2dfgrs galaxies , 50,191 were matches .",
    "for the 2qz the results were compared to the objects assigned the category _",
    "11 _ ( best classification and redshift ; see @xcite ) objects ( 8,739 quasars , 5,193 stars ) and those assigned _ 12 _ , _ 21 _ or _ 22 _ ( second - best ; 9,273 quasars ) .",
    "star - galaxy separation in the sdss is currently performed within the sdss photometric pipeline @xcite .",
    "an object is classified as extended , and hence a galaxy , if @xmath18 . here ` psfmag ` is the point - spread - function magnitude and ` cmodelmag ` is a combination of the best fitting de vaucouleurs and exponential profiles . the separation is done for each band individually , and for all five bands combined . ` psfmag ` is described further in @xcite and ` cmodelmag ` in @xcite .",
    "additional star - galaxy classifications have been performed on sdss data , e.g. , @xcite perform a bayesian star - galaxy separation via differences between the @xmath7 band ` psfmag ` and ` modelmag ` .    in our approach",
    ", we use an axis - parallel decision tree to assign a probability that a source belongs to each of three classes .",
    "these probabilities always sum to one and reflect the relative degree of certainty about an object s type .",
    "we provide cuts in these probabilities that can be applied to generate catalogs to a required completeness and efficiency of either galaxies , stars , or neither galaxies nor stars ( i.e. , nsng ) . in general , the cuts can be a function of the probabilities , and these functions can have both minimum and maximum values .",
    "we define both completeness and efficiency as functions of the classification probability . for a given sample , therefore , the _ completeness _ is the fraction of all catalog sources of a given type found at a specific probability threshold .",
    "likewise , _ efficiency _ is the fraction of all catalog sources that are correctly classified to a specific probability threshold . in previous astronomical machine learning results ,",
    "the efficiency of a sample has also been called the _ reliability _ or _ purity _ , and",
    "@xmath19__efficiency _ _ has been called the _",
    "contamination_. the quantities are also known ( e.g. , * ? ? ?",
    "* ) as the _ recall _ and _",
    "precision_.    as well as a maximal combination of completeness and efficiency , one may also desire maximal values of these measures separately .",
    "an example of the former is to include interesting or unusual objects of a certain class , for example , candidate objects for gravitational lensing .",
    "an example of the latter is applications such as the 2-point correlation function of extragalactic objects , where one wants to minimize contamination by non - cosmological objects such as galactic stars ( e.g. , * ? ? ?",
    "our decision tree methods are applied within the framework of the data - to - knowledge toolkit @xcite , developed and maintained by the automated learning group at the national center for supercomputing applications ( ncsa ) .",
    "this is a java - based application which allows numerous modules , each of which performs a single data processing task , to be interconnected in a variety of ways .",
    "the framework allows the easy addition of further data to the training and testing process and the application of the trained decision trees to the entire photometric catalog .",
    "the trees are implemented on the xeon linux cluster _ tungsten _ at ncsa .",
    "this nationally allocated supercomputing system is composed of 1280 compute nodes .",
    "each node has available 2 gb of memory and a peak double - precision performance of 6.4 gflops .",
    "our use of the _ tungsten _ supercomputing system was facilitated by a large , peer - reviewed , national allocation submitted by the laboratory for cosmological data mining at ncsa , for which rjb is the principal investigator .",
    "execution tasks were submitted to _ tungsten _ via a shell script that created and tested one or more decision trees whose testable parameters were specified at run time .",
    "one or more decision trees would be created and tested on a single _ tungsten _ node , and when completed the test results for each decision tree was returned for subsequent analysis .",
    "as this process was embarrassingly parallel , we submitted multiple execution tasks simultaneously from this single shell script , with a maximum of 240 submitted at any one time .",
    "as discussed in the next section , we analyzed the results of over seven thousand decision trees , which spanned a range of different decision tree parameters in selecting our optimal parameter values .",
    "decision trees consist first of a root node in which the parameters describing the objects in the training set population are input , along with the classifications .",
    "the tree is usually considered upside - down , with the root node at the top . here",
    "there are three classes , so for each object we have @xmath20,[o_g , o_n , o_s])\\ ] ] for the 16 features @xmath21 ( four colors in four magnitude types ) and the three output classes @xmath22 .",
    "the outputs are [ 1,0,0 ] , [ 0,1,0 ] and [ 0,0,1 ] for galaxy , nsng , and star , respectively .",
    "the outputs are in the form @xmath23,\\ ] ] and the predicted probabilities of class membership always sum to one .    a node population is split into population groups that are assigned to child nodes using the criterion that minimizes the classification error . this is a measure of how good the classification is when the tree that would result from the split is run on the test set .",
    "the process is repeated iteratively , resulting in a number of layers of nodes that form a tree .",
    "the iteration stops when either all nodes reach the minimum allowed population of objects in a node ( the minimum decomposition population ; mdp ) , the maximum number of nodes between the termination node and the root node ( the maximum tree depth ; mtd ) , or the population split no longer decreases the classification error by a minimum set amount ( the minimum error reduction ; mer ) .",
    "the nodes from which no further nodes branch are the leaf nodes .",
    "the computational complexity of the algorithm is @xmath24 , where @xmath25 is the number of examples , @xmath26 is the number of features , and @xmath27 is the depth of the tree .",
    "here @xmath25 is @xmath28 , whereas @xmath26 and @xmath27 are @xmath29 , giving a product of @xmath30 .",
    "the split is tested for each input feature .",
    "we use axis - parallel splits , in which , following @xcite , @xmath31 where @xmath32 is the @xmath8th attribute for example @xmath33 , and @xmath34 is a value to be tested .",
    "an alternative , which we have not utilized in this work , is the oblique split @xmath35 for @xmath27 attributes .",
    "this allows hyperplanes at arbitrary angles in the parameter space . here",
    "we allow the split point to be either the midpoint , the mean , or the median of the values of the parameters for the node population , with the best of these chosen for each individual node . with the removal of extreme non - physical outliers such as -9999 from the training data , selecting the optimal statistic from those measured at each node",
    "splitting provides better results than consistently selecting the same statistic .",
    "the classification error we used is the variance @xmath36 where @xmath8 is the output feature index , @xmath37 is the actual output feature value ( 0 or 1 ) , and @xmath38 is the probability prediction made by the decision tree .",
    "the data is modeled using the mean of all the output vectors , and the result is interpreted as a probability over class space .",
    "we used @xmath25-fold bagging ( bootstrap - aggregating ) in addition to the optimized tree and node parameters . in this procedure , the testing set is a randomly sampled fraction ( bagging fraction ) of the original testing set , and the procedure is repeated @xmath25 times , creating @xmath25 decision trees .",
    "the results are then voted on , using simple majority voting .",
    "similarly , the testing set itself is randomly subsampled from the training set , a procedure known as cross - validation .",
    "we perform 10-fold bagging , but use 1-fold cross - validation , as only the former improved the results .",
    "the parameters of the decision tree , and additional methods such as bagging , can have a substantial effect on the quality of the results produced .",
    "the massively parallel supercomputing resources we used in this work allowed extensive investigation of this parameter space , including : 1 ) the minimum decomposition population , 2 ) the maximum tree depth , 3 ) the minimum error reduction , 4 ) the method of splitting the population at a node , either half - and - half , midpoint , mean or median , 5 ) the ratio of sizes of the training set to the testing set , 6 ) the bagging fraction , and 7 ) the random seed used in selecting the subsamples for the testing set and the bagging .",
    "in addition , the construction of thousands of trees and the use of advanced visualization allowed us to investigate the interaction of these parameters .      once we have constructed an optimal decision tree , given the available training data , we can characterize the best probability thresholds to construct samples from further data to the desired level of completeness or efficiency .",
    "we investigated the basic probabilities @xmath39 , @xmath40 and @xmath41 ( hereafter @xmath42 , @xmath43 , and @xmath44 ) and the ratios @xmath45 for each target type galaxy , star and nsng for binned minima and/or maxima , using the 80:20 pseudo - blind test described in  [ sec : data ] .",
    "small numbers of objects in the leaf nodes resulted in discretization in the output probabilities from the decision tree .",
    "therefore , care was taken when comparing the resulting mixture of exact and floating point values to bin edges to ensure that the counts were correct and not offset due to finite floating point precision .",
    "the accuracy was achieved by using a decimal datatype , which stores numbers as exact values to a specified number of decimal places .",
    "as described in  [ subsec : algorithms - dts ] , a decision tree has a number of parameters that can be adjusted .",
    "the product of these parameters defines an enormous parameter space of possible trees that must be explored to identify what we expect to be the optimal decision tree .",
    "in addition , this parameter space can potentially include many local minima of the classification error , further complicating this process . given the size of the parameter space and the computational time required to construct and test an individual decision tree",
    ", we utilized the ncsa _ tungsten _ cluster ( described in more detail in  [ sec : algorithms ] ) to perform an extensive exploration of this parameter space .",
    "all decision trees were built from the same training set with the 16 input parameters described in ",
    "[ sec : data ] , and the full parameter space we explored is described in table [ table : dt params ] .",
    "first , we studied the interaction of the minimum node decomposition population ( mdp ) , the maximum tree depth ( mtd ) and the minimum error reduction ( mer ) for splitting the population .",
    "the values investigated were @xmath46 , where @xmath47 , @xmath48 , and @xmath49 , where @xmath50 , and @xmath51 .",
    "negative mers do not split the population .",
    "this gave 4,480 combinations , and for each one a decision tree was generated .",
    "figure [ fig : dt params ] shows a partiview  @xcite visualization of the results .",
    "the best tree had an mdp of 2 and an mer of 0 .",
    "mtd was limited to 16 by the combination of the size of the training set and the memory available on tungsten but , as shown in figure [ fig : dt params ] , a deeper tree is unlikely to give a substantial improvement .",
    "we next varied the statistic used to split the population at the nodes : halving the population , using the midpoint of each input , the mean of each input , and the median .",
    "the 16 combinations give very similar results , with the exception of those which allow population halving , which are significantly worse . as a result , we allowed the algorithm to select , when splitting each node , the optimal statistic from the midpoint , mean , and median split values .    in all previous measurements , we had set the level of bagging used when building the decision tree at 20-fold and assumed a ratio of the fraction of the training set used in the bagging to the fraction of the sample used in training of 80:20 ( i.e. , 80% for training to 20% for testing ) .",
    "we conducted a number of tests to verify these assumptions .",
    "when varying the level of bagging , we found that the classification error is substantially reduced when the bagging level is increased from 1- to 10-fold , and continues toward an asymptotic value slightly below the value seen for the 50-fold case . as with our tests involving the maximum tree depth ,",
    "the number of models was limited by the available memory on each node of the tungsten cluster , although given the asymptotic tendency , we do not expect that the results would change substantially for larger levels of bagging .",
    "we cross - tested the training to testing ratios with the n - fold bagging test , finding the best results were obtained using a 50-fold model with an 80% training subsample , as shown in figure [ fig : bagging ] . given memory limitations ,",
    "the actual trees used in our other parameter tests were always limited to a 20-fold bagging . when performing our final classifications , however , we used the more accurate 50-fold bagging . in all subsequent tests we set the training to testing fractions to 80:20 .",
    "we also note that the bagging prevents an overfitting of the training set : for low values of bagging , such as 1-fold , the classification error has a minimum at a particular mtd , and increases substantially for deeper trees .",
    "finally , the effect of the random seed in choosing a same - sized training set was investigated and is shown in figure [ fig : random seed ] . the @xmath52 variation in the classification error is approximately 0.1% . given this variation , we quote the best classification error achieved as being @xmath53 .",
    "this is a robust result as the classification error shows a broad , approximately flat , minimum in several areas of the parameter space .",
    "see , for example , figure [ fig : bagging ] , where the flat area corresponds to approximately 3% classification error .",
    "although our exploration of the parameter space was extensive , the number of _ possible _ trees in the space quickly becomes very large with the variation of all parameters , so it is always possible that a better tree exists than the one we used .",
    "although given the widely observed 3% minimum it is unlikely to be _ significantly _ better , especially for the training set we used .      once the optimal decision tree parameters were quantified , we constructed and subsequently tested this decision tree using unseen data to give a more realistic idea of the quality of our classifications .",
    "we achieved this result by building the tree with the previously described parameters , but training on 64% of the dr3 data and testing on 16% ( i.e. , maintaining the optimal 80:20 ratio ) .",
    "we tested this decision tree in a pseudo - blind fashion by using the remaining and unseen 20% of the dr3 data , which consisted of 95,413 sources . from this classification test , we compared the assigned types galaxy , star and nsng to the true types , in terms of completeness and efficiency as a function of the three probabilities @xmath42 , @xmath44 , and @xmath43 .",
    "overall , this experiment demonstrated that our axis parallel decision tree is very successful at classifying objects into these three types , and , as a result , at star - galaxy separation .",
    "the vast majority of objects are classified correctly and given high probabilities for the appropriate type .",
    "the confusion matrix is shown in figure [ fig : confusion matrix ] and tabulated in table [ table : confusion matrix ] .",
    "figure [ fig : true probability ] compares the true fraction of correct classifications as a function of assigned probability from the tree , demonstrating that these are approximately correct .",
    "we note that there is some discretization in the probabilities due to the potentially small number of objects in a leaf node .",
    "as can be seen in figure [ fig : dt params ] , however , increasing the minimum decomposition population to the point at which the probabilities would not be discretized would worsen the decision tree performance .    in general the function for generating subsamples according to probability is @xmath54 in all subsequent tests , we analyze a subsample in terms of true positives ( tp ) , false positives ( fp ) , true negatives ( tn ) and false negatives ( fn ) for each of the three target types separately .",
    "we define the completeness , @xmath55 , as the number of the target type that are included in the @xmath56 cut compared to the total number of objects of that target type in the whole sample : @xmath57 where @xmath8 indicates the objects to the probability threshold and @xmath58 indicates all the objects in the test set . for example , in our pseudo - blind test",
    "this would correspond to all 95,413 sources .",
    "the efficiency , @xmath59 , is the fraction assigned a particular type that are genuinely of that type : @xmath60 the completeness varies between 0 and 1 and in general increases at the expense of efficiency .",
    "an object can be tp or fn depending on the probability threshold .",
    "we define the best sample for a specific test to be that which maximizes the metric : @xmath61.\\ ] ] all the ratios , but not the simple probabilities , exclude objects for which any of the three probabilities are zero . in our pseudo - blind test , 18,989 of the 95,413 sources , or roughly 20% , were affected in this manner .",
    "we used the logarithm of a specific probability ratio , given the large dynamic range of the ratios of the the three probabilities . in all cases the classification is taken to be the object type assigned the highest probability by our decision tree .",
    "if the highest probability is less than 50% it is possible that two types are assigned an equal probability , which could introduce a systematic effect in our classification .",
    "we do not expect this to be problematic in practice , however , as this effect is small , only affecting one object in our pseudo - blind testing , and excluding these sources is easy after classification has been performed .    to determine the best probability function , i.e. , @xmath56 , and associated probability values to use when assigning classifications , we computed a number of different functions involving the three probabilities assigned by our decision tree .",
    "table  [ table : f(p ) cuts ] provides a compilation of these different functions and a characterization of their distribution sampled across one hundred bins . from the values in table  [ table : f(p ) cuts ] , we find that by using the simple minima in the three class probabilities we obtain the most accurate classification samples in our pseudo - blind testing .",
    "these probability cuts are @xmath62 , which results in 98.9% completeness and 98.3% efficiency for galaxies , @xmath63 , which results in 88.5% completeness and 94.5% efficiency for nsng , and @xmath64 , which results in 91.6% completeness and 93.5% efficiency for stars .",
    "we find that a number of probability cuts near fifty percent produce similar classification accuracies .",
    "this is not surprising , as fifty percent is an ideal threshold , as objects below this percentage are likely to be of a different class since they can have a higher probability of being one of the other two types .    in some cases , ratios of probabilities are able to select better samples when a maximum value is also used as opposed to only using a minimum value . for example , for the probability ratio @xmath65 , the sample is dominated by stars at the low end and galaxies at the high end . so the nsng are only selected by requiring a minimum and a maximum @xmath56 . in the end , however , none of the ratio cuts we have studied provided a better combination of completeness and efficiency than the simple probability cuts .",
    "we interpret this result as further evidence that our decision tree is successful , especially within the regime of spectroscopically confirmed objects used for training ( i.e. , the interpolation regime ) .",
    "the full completeness - efficiency curves , as a function of @xmath42 , @xmath43 , and @xmath44 , are shown in figure [ fig : com eff ] .",
    "we constructed our final decision tree from the full ( i.e. , 100% ) , spectroscopic training sample from the sdss dr3 . by using the data - streaming modules available within the d2k environment , we applied this decision tree to the full sdss dr3 ` photoprimary ` catalog of 142,705,734 sources . with this paper ,",
    "we publicly release the classification probabilities for every one of these sources . following the classification guidelines discussed in the previous section , within the magnitude range @xmath66",
    ", this sample consists of 38,022,597 galaxies , 57,369,754 stars and 47,026,955 nsng .",
    "the sum of these subsample sizes do not match the size of the full sample because of ( a ) the magnitude restriction , which removes 589 objects , and ( b ) source classifications that are ambiguous due to equal - highest probabilities for multiple object types , which removes 285,839 ( 0.2% ) objects .",
    "one of the most important issues with any supervised classification effort is the application of an algorithm trained on one sample of data to a different set of data .",
    "this concern is generally framed in terms of interpolation versus extrapolation , and in our case results from the application of our decision tree that is trained on sources from the sdss spectroscopic sample being applied to the sdss photometric sample , which is , in some cases , considerably fainter than our training set . given the strong dependence on the number counts of galaxies and stars with apparent magnitude",
    ", this affects the vast majority of the sources we have classified . without deeper training data ,",
    "however , we feel that the best approach is to classify all sources .",
    "this approach allows anyone using our classification catalog to create a well - defined sample for further study or follow - up spectroscopy that does not have any selection effects artificially imposed due to our classification algorithm .    while we can not guarantee the accuracy of our classifications across the full sdss photometric sample",
    ", we can provide some guidance on the accuracy of specific sample restrictions .",
    "historically , one of the most powerful techniques for characterizing classifications in the absence of spectroscopic identifications is by analyzing the differential number of sources as a function of apparent magnitude ( i.e. , number count plots ) .",
    "this technique is simple and straightforward ; by comparing the number count distribution of training sources with the number count distribution of classified sources , we gain a statistical estimate of the apparent magnitude limit at which we can no longer expect our extrapolated classification rules to remain robust .",
    "we present , therefore , in figure  [ fig : mag counts ] , the number counts for both the training set and for the classified ` photoprimary ` catalog for our three different classifications : galaxy , nsng , and star . the sdss spectroscopy is limited to @xmath67 for galaxies @xcite , which can clearly be seen in the galaxy training set number counts .",
    "similarly , the nsng training set , which is dominated by quasars , is limited to @xmath68 @xcite , which is reflected in the decline in the @xmath7-band number counts around @xmath69 .",
    "the logarithmic slopes of the galaxies and nsng are approximately 0.5 and 0.6 respectively for the training set .",
    "stars , on the other hand , have a more heterogeneous selection scheme in the sdss survey , with many subpopulations , which is reflected in the number count distribution of the stellar training set .",
    "we see clear evidence for reliable star - galaxy separation in figure [ fig : mag counts ] , where the nsng counts remain considerably lower than the star and galaxy counts until @xmath0 .",
    "given the spectroscopic limits of the sdss , this figure suggests that the ` photoprimary ` classifications are reliable to substantially fainter magnitudes than the spectroscopic sample .",
    "the galaxy counts approximately follow the logarithmic slope for around two magnitudes fainter than the training sample , and the nsng counts extend even fainter .",
    "the inflection points in the counts at @xmath70 suggest that our classifications are reliable to this magnitude and fainter than this the tree becomes less reliable and increasingly assigns objects to nsng , which is one of the primary reasons we included this third classification in our algorithm .",
    "the sdss photometric sample does extend fainter , the 95% detection repeatability limits for point sources being @xmath71 , @xmath72 , @xmath73 , @xmath74 and @xmath75 , which is seen in the number count distributions as the abrupt turnover at @xmath76 . if , as the number counts suggest , our classifications are reliable to @xmath0 , the numbers of reliable classifications are 7,978,356 , 832,993 , and 13,945,621 , totaling 22,756,970 for the galaxy , nsng , and star classes , respectively .",
    "we must emphasize , however , that the number of sources that will actually be scientifically useful will be lower , as these samples have not been cleaned .",
    "for example , before using these data , appropriate cuts should be made on either photometric errors , detection flags , or both , as described more fully on the sdss project website .",
    "since we used colors , as opposed to magnitudes , to classify our sources , we also compared , as shown in figure  [ fig : color counts ] , the @xmath77 distribution of training set sources with the ` photoprimary ` classified sources .",
    "the extrapolation in @xmath77 color in going from the testing set to the classified sources is considerably less than the corresponding extrapolation in magnitude , although it is still present , particularly in the nsng class toward redder colors .",
    "as expected for the training set , galaxies and stars show a bimodal distribution in color and the nsngs are dominated by blue objects ( e.g. , uvx - selected quasars ) .",
    "this bimodality is washed out for galaxies in our classified sample , however , presumably due to the fact that the photometric sample has a higher mean redshift than the spectroscopic sample .",
    "we note that this difference in mean redshift will not affect the apparent magnitude number count distributions , as the redshift difference is too small to allow for significant evolution to occur between the two samples .    in figure",
    "[ fig : p(gns ) ] , we present a visualization of the three classification probabilities for all objects in ` photoprimary ` , demonstrating how the three values interact .",
    "since the three probabilities must always sum to unity , their distribution appears as a triangle , with the height corresponding to the density of objects in that region of probability space .",
    "one can see that galaxies and stars are assigned relatively unambiguously , but that the tree is generally less certain when an object is neither a star nor a galaxy , as the base of the triangle rises noticeably before reaching @xmath78 .",
    "discretization is also seen in this figure , but is insignificant compared to the numbers of each source type with high probability .",
    "the data used to produce the visualization are available to download with the catalog .      in the pseudo - blind test results described earlier (   [ subsec : results - testing ] ) , our decision tree provided robust classifications for data that was not used to train the decision tree , nor to characterize its performance . while this pseudo - blind test did provide a good approximation to a blind test ,",
    "a true blind test is to compare the decision tree classifications to sources that have been assigned a spectroscopic identification from another survey .",
    "we have , therefore , matched the sdss dr3 photometric data to the 2dfgrs and 2qz surveys , by using coordinate position with a tolerance of 2 arcseconds .",
    "the 2dfgrs resulted in 50,191 matches and the 2qz 10,259 matches .",
    "the 2dfgrs contains mainly galaxies and the 2qz stars and quasars with a small number of mainly narrow emission - line galaxies .    by applying the optimal cuts of @xmath62 , @xmath63 and @xmath64 , as detailed in  [ subsec : results - testing ] , we can characterize our classification accuracy in this blind test as a function of apparent magnitude .",
    "we show these results for the 2dfgrs galaxies in figure  [ fig : com eff galaxy ] for the 2qz stars in figure  [ fig : com eff star ] . for galaxies ,",
    "the overall completeness of the 2dfgrs galaxies is 93.8% ( 47,055 of 50,191 ) , with the efficiency undefined as this is the only type of object .",
    "the completeness of the stars in the 2qz match is rather low , at 79.7% ( 4141 of 5193 ) , but the efficiency is 95.4% ( 4141 of 4359 ) .",
    "figure [ fig : com eff star ] shows that the low completeness is due to stars at @xmath79 .",
    "the low value may be due to the fact that the 2qz is uvx - selected , which causes unusual stars that lie off the main - sequence , such as sub - dwarfs , halo stars , and hot young stars , to be preferentially targeted .",
    "we also blind - tested the nsng classifications in a similar manner to the galaxy and star classes , but in this case by using quasars from the 2qz with the same matching criteria as the one we used for stars from the 2qz .",
    "as shown in figure  [ fig : com eff quasar ] , our decision tree does an excellent job of assigning quasars to the nsng class , with a completeness of 94.7% ( 8278 of 8739 ) and an efficiency of 87.4% ( 8278 of 9471 ) for the type _",
    "11 _ best ids and with a completeness of 94.4% and an efficiency of 85% for the _ 12 _ , _ 21 _ and _ 22 _ next - best ids .",
    "the efficiency drops off from around 98% fainter than the sdss spectroscopic limit of @xmath68 , and is about 87% at @xmath70 when the counts reach their limit in the 2qz .",
    "this result is further vindication of adding the third class to our algorithm , as these sources are clearly neither stars nor galaxies , and our algorithm identifies them appropriately .",
    "overall , these blind tests support our assertion , based on the number count distributions , that our classifications remain reliable to @xmath2 .",
    "the axis - parallel decision tree algorithm we have adopted in this work is an example of supervised machine learning .",
    "one of the primary criticisms of supervised techniques is their difficulty in extrapolating past the limits of the actual data used in their construction .",
    "supervised learning algorithms can be viewed as a mapping between the training parameter space and a classification , and are not necessarily a representation of anything physical ( e.g. , a decision tree might correctly classify stars , but it would not necessarily reproduce the hertzsprung - russell diagram ) .",
    "ideally , the training data should sample , perhaps sparsely , the entire parameter space occupied by the data to be classified . in the case of astronomical datasets , such as the star - galaxy separation problem being studied in this paper , this is often impossible .",
    "we do not have a sufficiently large and faint spectroscopic survey to adequately sample either the magnitude or color spaces spanned by the photometric data from the sdss survey .",
    "yet we see that our classifications do reliably extrapolate past the training data .",
    "what is most likely driving this successful extrapolation is that the source colors do not change dramatically prior to @xmath2 . as a result ,",
    "the existing training data is sufficient to classify sources in this area of parameter space . at these fainter magnitudes",
    ", the algorithm increasingly assigns sources to the nsng class , as the distinction between stars and galaxies is breaking down with decreasing signal - to - noise .",
    "we can characterize this effect from the classifications in the final catalog , where the fraction of nsng sources in the full catalog rises from approximately 1.3% for @xmath80 to approximately 3.7% for @xmath0 .",
    "this observation , together with the obvious examples of sources that are neither stars nor galaxies , strengthens the case for introducing a third class into the historical star - galaxy separation problem .    to aid in the utilization of our classification catalog , we have made available classifications for all sources in the sdss dr3 , without regard for photometric errors , or object detection flags . as a result",
    ", anyone wanting to use our catalog must subsequently apply any appropriate flag or photometric error restrictions as appropriate for their science  just as they would when using the sdss databases directly .",
    "we feel that this is an important step forward in the realm of astronomical source classifications . while we are certainly cognizant that a large fraction of our source classifications are not highly reliable , we feel it is important in the new era of large surveys to provide an astronomer with the greatest possible freedom in selecting their own samples .",
    "if we released only those sources that pass a potentially opaque classification cut , we force all users of our catalog to deal with an additional , implicit selection effect when creating their own subsamples from our classification catalog .      in this paper , our supervised learning algorithm utilized photometry and spectra from the sloan digital sky survey .",
    "while the sdss is an incredible resource for this sort of investigation , it is limited to the optical and near - infrared wavelengths , and , consequently , physical processes outside these wavelengths that might distinguish objects otherwise indistinguishable in the optical will not be seen .",
    "given the growth in survey astronomy , a large number of sdss objects have been observed in surveys at other wavelengths , and thus an important next - step is to include these in the training set .",
    "this extension has two stages : 1 ) match the sdss photometry to spectroscopic targets from other surveys and train on the sdss photometry with these augmented training data , and 2 ) add the photometry from other surveys as additional training parameters .",
    "the latter is a substantial task as the cumulative number of photometric objects in surveys such as galex @xcite , 2mass @xcite , swire @xcite and rosat @xcite number in the millions .",
    "we are in the process of matching the sdss to these surveys , and others that may become available such as ukidss , to improve the classification performance achieved here .",
    "of course this technique also extends to subsequent data releases from the sdss itself .",
    "we also would like to investigate alternative mechanisms for selecting sdss photometric parameters for training , including the use of techniques such as forward - selection , backward - elimination , or a hybrid of the two , in which training parameters are respectively added or removed according to their effect on the quality of the classification .",
    "this approach is adopted by @xcite , who uses this technique for morphological galaxy classification .",
    "the training parameters could utilize magnitudes rather than colors , and also morphological parameters such as the @xmath81 measure . in this paper",
    ", we have only used the colors of objects through four different matched apertures , as this work is our first major effort in large - scale machine classification of astronomical datasets . in the future , we anticipate performing a more detailed exploration of different classification parameters , especially as we incorporate additional datasets at other wavelengths into our algorithms",
    ".    one training parameter that was found to not be useful in the current framework is the error on an object attribute , in agreement with previous analyses ( see , e.g. , * ? ? ?",
    "fundamentally , this is because , without additional information , the decision tree can not differentiate between an error and another parameter .",
    "coupled with the different dynamic ranges of the two types of attributes , this effect essentially trivializes the error attributes . since irrelevant attributes dilute the real information content in a training dataset , they reduce the performance of a decision tree . as a result",
    ", we did not include magnitude errors in our analysis .",
    "intellectually , a better technique might be to include the error as part of the appropriate attribute when constructing a machine learning algorithm .",
    "our decision tree implementation , however , does not provide this capability , and we therefore defer this concept to a future paper .",
    "a further decision tree algorithm , supported in the data - to - knowledge framework but not yet extensively investigated , is _ boosting _ , in which the differences between classes are emphasized .",
    "this method has been shown to improve results by many workers in the data mining field and is described further in @xcite .",
    "besides decision trees , d2k supports numerous other algorithms , including nave bayes , artificial neural networks , support vector machines , k - means clustering , and instance - based learning .",
    "the latter is of particular interest because , although it has shown promising results , its full potential has not been realized , due to the computational requirements of calculating the classifications for the full application set .    in this method ,",
    "the distance between each test or application set object ( each instance ) and the nearest @xmath25 neighbors in the training set is measured and a weighted mean is taken to give the new object its assigned type .",
    "thus the training consists of specifying @xmath25 , the weighting , and the distance measure , typically of the form @xmath82 in each dimension where @xmath27 is the distance and @xmath83 would be the euclidean distance .",
    "hence there are a large number of distance calculations involved .",
    "typically the classification of an object takes of the order of a second on a desktop machine , rendering the process computationally intractable for the 143 million objects in the sdss dr3 .",
    "the process is , however , embarrassingly parallel , so is amenable to a significant speedup by using supercomputing resources . in which case",
    ", this technique could potentially be used to not only classify sources , but to characterize the energy generation mechanisms responsible for a source s luminosity .",
    "we anticipate applying this technique , for example , to characterize the fraction of every source s luminosity in terms of the ratio between the thermonuclear fusion powering stars and the accretion powering quasars .",
    "in many applications it has been found that a _ mixture of experts _ , that is , the combination of results from more than one learning method , is best , in an analogous way to the improvement seen here through the use of bagging .",
    "a simple way to combine results is to take a weighted sum of outputs from the different methods .",
    "preliminary tests have shown that instance - based with @xmath84 and @xmath85 in a 0.7:0.3 combination with a decision tree gives improved results .",
    "of course , one must be able to run a full instance - based classification to explore the potential of combining it with the other methods , which heretofore has been impossible due to the necessary computing resources .",
    "another promising future method is semi - supervised classification @xcite , in which the algorithm is given some predefined classes but is able to discover further classes in the data for itself .",
    "this combines prior knowledge from spectra with the large number of objects for which there is available photometry and may perform better where very few spectra are available .    finally , in this work we limited our analysis to three classes as our main interest was robust star - galaxy separation .",
    "given the diversity of both stars and galaxies , and the ignorance of our approach to non - stellar luminosity , we could clearly benefit from adding additional classes to our existing decision tree work ( see , e.g. , * ? ? ?",
    "as it stands , the nsng class is dominated by quasars at bright magnitudes , and could , therefore , serve as the basis for a quasar class that we expect would provide reasonable quasar classifications for relatively bright sources with clean photometry .",
    "ideally , this approach could be extended to include other classes for different types of stars and galaxies , for example , white dwarfs , hot stars , cool dwarfs , starbursts , and active galaxies .",
    "in this paper we have classified all of the 142,705,734 non - repeat ( primary ) objects in the third data release of the sloan digital sky survey ( sdss dr3 ) . the classifications were determined by using machine learning , the algorithm of choice being the axis - parallel decision tree .",
    "the algorithm was trained on 477,068 objects for which spectroscopic classifications were available within dr3 .",
    "this training data consists of 361,490 galaxies , 62,333 stars , 49,545 quasars , and 3,700 unknown objects .",
    "collaboration with domain experts at the national center for supercomputing applications ( ncsa ) and the use of the general machine learning environment data - to - knowledge combined with supercomputing resources enabled extensive investigation of the decision tree parameter space and the associated datasets . to our knowledge , this level of investigation has not previously been published in the astronomy literature .",
    "this is the first public release of objects classified in this way for the whole survey .",
    "the objects are classified as either galaxy , star or nsng , and we provide an associated probability for each class .",
    "these three probabilities always sum to unity for every object classified . by merely assigning the classification with the highest probability",
    ", we find that our full classification sample contains 38,022,597 galaxies , 57,369,754 stars and 47,026,955 nsng in the magnitude range @xmath86 , with 589 outside this range and 285,839 ( 0.2% ) ambiguous due to equal - highest probabilities for object type .",
    "the catalog is available for download at http://quasar.astro.uiuc.edu/rml .",
    "a major issue with this method of classification is the inevitable extrapolation from the spectroscopic regime .",
    "we investigate this by examining the apparent magnitude number counts for all sources and by performing several pseudo - blind or fully blind tests by using sources spectroscopically identified in the sdss , 2df galaxy redshift , and 2df qso redshift ( 2qz ) surveys .",
    "we find that for a sample giving the optimal completeness and efficiency , the completeness values are 98.9% , 91.6% , and 88.5% for galaxies , stars , and nsngs in the sdss , and 94.7% for quasars , which are classified as nsng , in the 2qz .",
    "the corresponding efficiencies are 98.3% , 93.5% , 94.5% , and 87.4% .",
    "the number count distributions suggest that the classifications are reliable for @xmath0 , giving 7,978,356 galaxies , 832,993 nsng , and 13,945,621 stars .",
    "as we have not applied any restrictions to the classification catalog , such as limiting by photometric error or detection flags , the number of reliable sources will be lower ; however , we provide these classifications to eliminate any opaque selection effects from our classification catalog , which simplifies the task of using these classifications to supplement additional analyses , such as as defining a target sample for follow - up spectroscopy .",
    "the assignment of probabilities to each object allows one to investigate the completeness and efficiency of the classifications as a function of these probabilities . while ratios of the probabilities were investigated , we find that the samples with an optimal completeness and efficiency are yielded by the simple cuts @xmath62 , @xmath63 and @xmath64 .",
    "other values close to 0.5 also give very similar results .",
    "the full data describing the completeness and efficiency as a function of these three threshold probabilities is made available with the catalog .",
    "we feel that the application of machine learning algorithms to large , astronomical surveys is a rich area of research .",
    "we are currently augmenting our training data with both additional wavelengths and fainter spectroscopic identifications . to improve our classification results",
    ", we are also performing additional tests to determine the optimal parameters for decision trees built from these data .",
    "given the efficacy of our approach , we plan to increase the number of training classes used in our analysis , first by using just the sdss , and later by using additional photometric and spectroscopic data .",
    "finally , we are also exploring the efficacy of more powerful algorithms , such as instance - based classification , to tackle the fundamental goal of characterizing sources by the fraction of their energy that is derived from fusion and accretion .",
    "these results will presented in subsequent papers in this series .",
    "nmb , rjb and adm acknowledge support from nasa through grants nag5 - 12578 and nag5 - 12580 , microsoft research , and from the nsf paci project .",
    "the authors made extensive use of the storage and computing facilities at the national center for supercomputing applications and thank the technical staff for their assistance in enabling this work .",
    "funding for the sdss and sdss - ii has been provided by the alfred p. sloan foundation , the participating institutions , the national science foundation , the u.s .",
    "department of energy , the national aeronautics and space administration , the japanese monbukagakusho , the max planck society , and the higher education funding council for england .",
    "the sdss web site is http://www.sdss.org/.    the sdss is managed by the astrophysical research consortium for the participating institutions .",
    "the participating institutions are the american museum of natural history , astrophysical institute potsdam , university of basel , cambridge university , case western reserve university , university of chicago , drexel university , fermilab , the institute for advanced study , the japan participation group , johns hopkins university , the joint institute for nuclear astrophysics , the kavli institute for particle astrophysics and cosmology , the korean scientist group , the chinese academy of sciences ( lamost ) , los alamos national laboratory , the max - planck - institute for astronomy ( mpa ) , the max - planck - institute for astrophysics ( mpia ) , new mexico state university , ohio state university , university of pittsburgh , university of portsmouth , princeton university , the united states naval observatory , and the university of washington .",
    "data to knowledge ( d2k ) software , d2k modules , and/or d2k itineraries , used by us , was developed at the national center for supercomputing applications ( ncsa ) at the university of illinois at urbana - champaign .",
    "this research has made use of nasa s astrophysics data system .",
    "ll minimum decomposition population & 32768 16384 8192 4096 2048 1024 512 256 128 64 32 16 8 4 2 1 + maximum tree depth & 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 + minimum error reduction & 999999.0 100000.0 10000.0 1000.0 100.0 10.0 1.0 0.1 0.01 0.001 0.0001 0.00001 0.000001 0.0 +",
    "one half split & false true + midpoint split & false true + mean split & false true + median split & false true + number of repetitions & 1 2 5 10 + fraction train examples & 0.01 0.1 0.3 0.5 0.7 0.8 0.9 0.99 + number of bagging models & 1 10 20 50 + bagging fraction & 0.001 0.01 0.1 0.3 0.5 0.7 0.8 0.9 0.99 0.999 + random seed subsamples & 1 ... 32 + random seed bagging & 1 ... 32 +    lcccc & _ galaxy _ & 71,748 & 608 & 735 + number & _ nsng _ & 397 & 9,514 & 309 + & _ star _ & 345 & 451 & 11,305 +   +   + & _ galaxy _ & 75.2 & 0.637 & 0.770 + percentage & _ nsng _ & 0.416 & 9.97 & 0.324 + & _ star _ & 0.362 & 0.473 & 11.8 +    ccccccc @xmath39 & galaxy & 0.49 & 1.00 & 98.9 & 98.3 & 98.6 + @xmath40 & nsng & 0.54 & 1.00 & 88.5 & 94.5 & 91.5 + @xmath41 & star & 0.46 & 1.00 & 91.6 & 93.5 & 92.6 + @xmath87 & galaxy & 0.821 & 4.82 & 98.2 & 97.4 & 97.8 + @xmath87 & nsng & -4.70 & -0.989 & 76.4 & 86.2 & 81.3 + @xmath87 & star & -2.32 & 2.06 & 94.2 & 35.0 & 64.6 + @xmath88 & galaxy & 0.770 & 4.93 & 97.6 & 97.5 & 97.6 + @xmath88 & nsng & -2.38 & 1.97 & 95.3 & 32.3 & 63.8 + @xmath88 & star & -4.32 & -0.711 & 74.3 & 83.4 & 78.9 + @xmath89 & galaxy & -1.62 & 1.85 & 96.7 & 92.6 & 94.7 + @xmath89 & nsng & 2.19 & 4.02 & 56.5 & 90.0 & 73.3 + @xmath89 & star & -4.23 & 3.15 & 100.0 & 7.74 & 53.9 +"
  ],
  "abstract_text": [
    "<S> we provide classifications for all 143 million non - repeat photometric objects in the third data release of the sloan digital sky survey ( sdss ) using decision trees trained on 477,068 objects with sdss spectroscopic data . </S>",
    "<S> we demonstrate that these star / galaxy classifications are expected to be reliable for approximately 22 million objects with @xmath0 . </S>",
    "<S> the general machine learning environment data - to - knowledge and supercomputing resources enabled extensive investigation of the decision tree parameter space . </S>",
    "<S> this work presents the first public release of objects classified in this way for an entire sdss data release . </S>",
    "<S> the objects are classified as either _ galaxy _ , _ star _ or _ nsng _ </S>",
    "<S> ( neither star nor galaxy ) , with an associated probability for each class . to demonstrate how to effectively make use of these classifications , we perform several important tests . </S>",
    "<S> first , we detail selection criteria within the probability space defined by the three classes to extract samples of stars and galaxies to a given completeness and efficiency . </S>",
    "<S> second , we investigate the efficacy of the classifications and the effect of extrapolating from the spectroscopic regime by performing blind tests on objects in the sdss , 2df galaxy redshift and 2df qso redshift ( 2qz ) surveys for which spectra are available . we find that , for a sample giving a maximal combined completeness and efficiency , the completeness values are 98.9% and 91.6% and the corresponding efficiencies are 98.3% and 93.5% for galaxies and stars , respectively , in the sdss . </S>",
    "<S> we also test our star - galaxy classification by studying the inverse of this sample , finding quasars in the nsng sample with a completeness and efficiency of 88.5% and 94.5% from the sdss and 94.7% and 87.4% from the 2qz . </S>",
    "<S> given the photometric limits of our spectroscopic training data , we effectively begin to extrapolate past our star - galaxy training set at @xmath1 . by comparing the number counts of our training sample with the classified sources , </S>",
    "<S> however , we find that our efficiencies appear to remain robust to @xmath2 . as a result </S>",
    "<S> , we expect our classifications to be accurate for 900,000 galaxies and 6.7 million stars , and remain robust via extrapolation for a total of 8.0 million galaxies and 13.9 million stars . </S>",
    "<S> the latter sample should prove useful in constructing follow - up spectroscopic surveys . characterizing the success of our classifications fainter than @xmath2 will require fainter spectroscopic data . </S>"
  ]
}