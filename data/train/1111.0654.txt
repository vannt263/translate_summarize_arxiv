{
  "article_text": [
    "the distributed source coding ( dsc ) deals with compression of correlated sources which do not communicate with each other @xcite .",
    "lossless dsc ( slepian - wolf coding ) , has been realized by different binary channel codes , including ldpc @xcite and turbo codes @xcite . the wyner - ziv coding problem @xcite , deals with lossy data compression with side information at the decoder , under a fidelity criterion .",
    "current approach in the dsc of a continuous - valued source is to first convert it to a discrete - valued source using quantization , and then to apply slepian - wolf coding in the binary field .",
    "similarly , a practical wyner - ziv encoder is realized by cascading a quantizer and slepian - wolf encoder @xcite . in other words ,",
    "the quantized source is compressed .",
    "there are , hence , source coding ( or quantization ) loss and channel coding ( or binning ) loss .",
    "this approach is based on the assumption that there is still correlation remaining in the quantized version of correlated sources .    in this paper",
    ", we establish a new framework for the wyner - ziv coding .",
    "we propose to first compress the continuous - valued source and then quantize it , as opposed to the conventional approach .",
    "the compression is thus in the real field , aiming at representing the source with fewer samples .    to do compression ,",
    "we generate either syndrome or parity samples of the input sequence using a real - number channel code , similar to what is done to compress a binary sequence of data using binary channel codes .",
    "then , we quantize these syndrome or parity samples and transmit them .",
    "there are still coding ( binning ) and quantization losses ; however , since coding is performed before quantization , error correction is in the real field and quantization error can be corrected when two sources are completely correlated over a block of code .",
    "a second and more important advantage of this approach is the fact that the correlation channel model can be more realistic , as it captures the correlation between continuous - valued sources rather than quantized sources . in the conventional approach",
    ", it is implicitly assumed that quantization of correlated signals results in correlated sequences in the discrete domain which is not necessarily correct due to nonlinearity of quantization operation .",
    "in addition , most of previous works assume that this correlation , in the binary field , can be modeled by a binary symmetric channel ( bsc ) with a known crossover probability . to avoid the loss due to inaccuracy of correlation model , we exploit correlation between continuous - valued sources before quantization .",
    "specifically , we use real bch - dft codes @xcite , for compression in the real field . owing to the dft codes , the loss due to quantization",
    "can be decreased by a factor of @xmath0 for an @xmath1 dft code @xcite , @xcite . additionally , if the two sources are perfectly correlated over one codevector , reconstruction loss vanishes .",
    "this is achieved in view of modeling the correlation between the two sources in the continuous domain .",
    "finally , the proposed scheme seems more suitable for low - delay communication because using short dft codes a reconstruction error better than quantization error is achievable .",
    "the rest of this paper is organized as follows . in section",
    "[ sec : sys ] , we motivate and introduce a new framework for lossy dsc . in section  [ sec : dft ] , we briefly review encoding and decoding in real dft codes . then in section  [ sec : wz ] , we present the dft encoder and decoder for the proposed system , both in the syndrome and parity approaches . these two approaches are also compared in this section .",
    "section  [ sec : sum ] discusses the simulation results .",
    "section  [ sec : con ] provides our concluding remarks .",
    "we introduce the use of real - number codes in lossy compression of correlated signals .",
    "specifically , we use dft codes @xcite , a class of real bose - chaudhuri - hocquenghem ( bch ) codes , to preform compression .",
    "similar to error correction in finite fields , the basic idea of error correcting codes in the real field is to insert redundancy to a message vector of @xmath2 samples to convert it to a codevector of @xmath3 samples ( @xmath4 ) @xcite .",
    "but unlike that , the insertion of redundancy in the real field is performed before quantization and entropy coding .",
    "the insertion of _ soft redundancy _ in the real - number codes has advantages over _ hard redundancy _ in the binary field . by using soft redundancy",
    ", one can go beyond quantization error , and thus reconstruct continuous - valued signals more accurately .",
    "this makes real - number codes more suitable than binary codes for lossy distributed source coding .",
    "the proposed system is depicted in fig .",
    "[ fig : realdsc ] .",
    "although it consists of the same blocks as existing practical wyner - ziv coding scheme @xcite , the order of these blocks is changed here .",
    "that is , we perform slepian - wolf coding before quantization .",
    "this change in the order of the dsc and quantization blocks brings some advantages as described in the following .    * * realistic correlation model : * in the existing framework for lossy dsc , correlation between two sources is modeled after quantization , i.e. , in the binary domain .",
    "more precisely , correlation between quantized sources is usually modeled as a bsc , mostly with known crossover probability .",
    "admittedly though , due to nonlinearity of quantization operation , correlation between the quantized signals is not known accurately even if it is known in the continuous domain .",
    "this motivates investigating a method that exploits correlation between continuous - valued sources to perform dsc . * * alleviating quantization error : * in lossy data compression with side information at the decoder , soft redundancy , added by dft codes , can be used to correct both quantization errors and ( correlation ) channel errors .",
    "the loss due to quantization error thus can be recovered , at least partly if not wholly .",
    "more precisely , if the two sources are exactly the same over a codevector , quantization error can be corrected completely . that is , perfect reconstruction is achieved over corresponding samples .",
    "the loss due to quantization error is decreased even if correlation is not perfect , i.e. , when ( correlation ) channel errors exist . *",
    "* low - delay communication : * if communication is subject to low - delay constraints , we can not use turbo or ldpc codes , as their performance is not satisfactory for short code length . whether low - delay requirement exists or not depends on the specific applications",
    ". however , even in the applications that low - delay transmission is not imperative , it is sometimes useful to consider low - dimensional systems for their low computational complexity .",
    "real bch - dft codes , a subset of complex bch codes @xcite , are linear block codes over the real field .",
    "any bch - dft code satisfies two properties .",
    "first , as a dft code , its parity - check matrix is defined based on the dft matrix .",
    "second , similar to other bch codes , the spectrum of any codevector is zero in a block of @xmath5 cyclically adjacent components , where @xmath6 is the designed distance of that code @xcite . a real bch - dft codes , in addition , has a generator matrix with real entries , as described below .",
    "an @xmath7 real bch - dft code is defined by its generator and parity - check matrices .",
    "the generator matrix is given by @xmath8    in which @xmath9 and @xmath10 respectively are the dft and idft matrices of size @xmath2 and @xmath3 , and is an @xmath11 matrix with @xmath12 zero rows @xcite .",
    "particularly , for odd @xmath2 , has exactly @xmath2 nonzero elements given as @xmath13 , @xmath14 , @xmath15 @xcite , @xcite .",
    "this guarantees the spectrum of any codeword to have @xmath12 consecutive zeros , which is required for any bch code @xcite .",
    "the parity - check matrix @xmath16 , on the other hand , is constructed by using the @xmath12 columns of @xmath10 corresponding to the @xmath12 zero rows of @xmath17 .",
    "therefore , due to unitary property of @xmath10 , @xmath18 .",
    "in the rest of this paper , we use the term dft code in lieu of real bch - dft code . besides",
    ", we only consider odd numbers for @xmath2 and @xmath3 ; thus , the error correction capability of the code is @xmath19 .",
    "for decoding , we use the extension of the well - known peterson - gorenstein - zierler ( pgz ) algorithm to the real field @xcite .",
    "this algorithm , aimed at detecting , localizing , and estimating errors , works based on the syndrome of error .",
    "we summarize the main steps of this algorithm , adapted for a dft code of length @xmath3 , in the following .    1 .",
    "compute vector of syndrome samples 2 .",
    "determine the number of errors @xmath20 by constructing a syndrome matrix and finding its rank 3 .",
    "find coefficients @xmath21 of error - locating polynomial @xmath22 whose roots are the inverse of error locations 4 .",
    "find the zeros @xmath23 of @xmath24 ; the errors are then in locations @xmath25 where @xmath26 and @xmath27 5 .",
    "finally , determine error magnitudes by solving a set of linear equations whose constants coefficients are powers of @xmath28 .    as mentioned , the pgz algorithm works based on the syndrome of error , which is the syndrome of the received codevector , neglecting quantization .",
    "let @xmath29 be the received vector , then @xmath30 where @xmath31^t$ ] is a complex vector of length @xmath12 .",
    "in practice however , the received vector is distorted by quantization ( @xmath32 ) and its syndrome is no longer equal to the syndrome of error because @xmath33 where @xmath34 and @xmath35 . while the `` exact '' value of errors is determined neglecting quantization , the decoding becomes an _ estimation _ problem in the presence of quantization .",
    "then , it is imperative to modify the pgz algorithm to detect errors reliably @xcite . error detection , localization , and also estimation can be largely improved using least squares methods @xcite .",
    "dft codes by construction are capable of decreasing quantization error .",
    "when there is no error , an @xmath7 dft code brings down the mean - squared error ( mse ) , below the level of quantization error , with a factor of @xmath36 @xcite .",
    "this is also shown to be valid for channel errors , as long as channel can be modeled as by additive noise . to appreciate this",
    ", one can consider the generator matrix of a dft code as a tight frame @xcite ; it is known that frames are resilient to any additive noise , and tight frames reduce the mse @xmath0 times @xcite .",
    "hence , dft codes can result in a mse even better than quantization error level whereas the best possible mse in a binary code is obviously lower - bounded by quantization error level .",
    "the concept of lossy dsc and wyner - ziv coding in the real field was described in section [ sec : sys ] . in this section ,",
    "we use dft codes , as a specific means , to do wyner - ziv coding in the real field .",
    "this is accomplished by using dft codes for binning , and transmitting compressed signal , in the form of either syndrome or parity samples .",
    "let @xmath37 be a sequence of i.i.d random variables @xmath38 , and @xmath39 be a noisy version of @xmath37 such that @xmath40 , where @xmath41 is continuous , i.i.d . , and independent of @xmath42 .",
    "since @xmath43 is continuous , this model precisely captures any variation of @xmath44 , so it can model correlation between @xmath37 and @xmath39 accurately .",
    "for example , the gaussian , gaussian bernoulli - gaussian , and gaussian - erasure correlation channels can be modeled using this model @xcite .",
    "these correlation models are practically important in video coders that exploit wyner - ziv concepts , e.g. , when the decoder builds side information via extrapolation of previously decoded frames or interpolation of key frames @xcite . in this paper ,",
    "the virtual correlation channel is assumed to be a bernoulli - gaussian channel , inserting at most @xmath45 random errors in each codeword ; thus , @xmath43 is a sparse vector",
    ".        given , to compress an arbitrary sequence of data samples , we multiply it with to find the corresponding syndrome samples @xmath46 .",
    "the syndrome is then quantized ( @xmath47@xmath48@xmath47@xmath49 ) , and transmitted over a noiseless digital communication system , as shown in fig .",
    "[ fig : wzsynd ] .",
    "note that @xmath50 , @xmath46 are both complex vectors of length @xmath12 .",
    "the decoder estimates the input sequence from the received syndrome and side information @xmath51 . to this end",
    ", it needs to evaluate the syndrome of channel ( correlation ) errors .",
    "this can be simply done by subtracting the received syndrome from syndrome of side information .",
    "then , neglecting quantization , we obtain , @xmath52 and @xmath53 can be used to precisely estimate the error vector , as described in section [ sec : dec ] . in practice , however , the decoder knows @xmath47@xmath48@xmath47@xmath49 rather than .",
    "therefore , only a distorted syndrome of error is available , i.e. , @xmath54 hence , using the pgz algorithm , error correction is accomplished based on .",
    "note that , having computed the syndrome of error , decoding algorithm in dsc using dft codes is exactly the same as that in the channel coding problem .",
    "this is different from dsc techniques in the binary field which usually require a slight modification in the corresponding channel coding algorithm to customize for dsc .",
    "syndrome - based wyner - ziv coding is straightforward but not very efficient because , in a real dft code , syndrome samples are complex numbers .",
    "this means that to transmit each sample we need to send two real numbers , one for the real part and one for the imaginary part .",
    "thus , the compression ratio , using an @xmath7 dft code , is @xmath55 whereas it is @xmath56 for a similar binary code .",
    "this also imposes a constraint on the rate of code , i.e. , @xmath57 or @xmath58 , since otherwise there is no compression . in the sequel",
    ", we explore parity - based approach to the wyner - ziv coding .",
    "to compress @xmath44 , the encoder generates the corresponding parity sequence @xmath59 with @xmath12 samples .",
    "the parity is then quantized and transmitted , as shown in fig .",
    "[ fig : wzparity ] , instead of transmitting the input data .",
    "the first step in parity - based system is to find the systematic generator matrix , as @xmath60 in is not in the systematic form .",
    "let @xmath16 be partitioned as @xmath61 $ ] , where @xmath62 is a matrix of size @xmath63 , and @xmath64 is a square matrix of size @xmath12 .",
    "since @xmath65 is a vandermonde matrix , @xmath66 exist and we can write @xmath67 , \\label{eq : synd5}\\end{aligned}\\ ] ] in which @xmath68 is an @xmath69 matrix , and @xmath70 is an identity matrix of size @xmath71 .    the systematic generator matrix corresponding to @xmath72 is given by @xmath73      = \\left[\\begin{array}{c }        \\bm{i}_k \\\\",
    "-\\bm{h}_2^{-1 } \\bm{h}_1      \\end{array}\\right ] .",
    "\\label{eq : gsys}\\end{aligned}\\ ] ] clearly , @xmath74 .",
    "it is also easy to check that @xmath75 therefore , we do not need to calculate @xmath76 and the same parity - check matrix @xmath16 can be used for decoding in the parity approach .    an even easier way to come up with systematic generator matrix",
    "is to partition @xmath60 as @xmath77 $ ] where @xmath78 is a square matrix of size @xmath2 .",
    "then , from @xmath79 and the fact that @xmath80 is invertible one can see @xmath81 ; thus , we have @xmath82      = \\left[\\begin{array}{c }        \\bm{i}_k \\\\        -\\bm{h}_2^{-1 } \\bm{h}_1      \\end{array}\\right]\\bm{g}_{1}. \\label{eq : gsys2}\\end{aligned}\\ ] ] note that @xmath78 is invertible because using any @xmath83 submatrix of @xmath60 can be represented as product of a vandermonde matrix and the dft matrix @xmath84 .",
    "this is also proven using a different approach in @xcite , where it is shown that any subframe of @xmath60 is a frame and its rank is equal to @xmath2 . hence , since @xmath78 is invertible , the systematic generator matrix is given by @xmath85 again @xmath86 because @xmath79 .",
    "therefore , the same parity - check matrix @xmath16 can be used for decoding in the parity approach .",
    "it is also easy to see that @xmath87 is a real matrix .",
    "the question that remains to be answered is whether @xmath87 corresponds to a bch code ? to generate a bch code ,",
    "@xmath87 must have @xmath12 consecutive zeros in the transform domain .",
    "@xmath88 , the fourier transform of this matrix satisfies this condition because @xmath89 , the fourier transform of original matrix , satisfies that .",
    "note that , since parity samples , unlike syndrome samples , are real numbers , using an @xmath7 dft code a compression ratio of @xmath90 is achieved .",
    "obviously , a compression ratio of @xmath56 is achievable if we use a @xmath91 dft code .",
    "a parity decoder estimates the input sequence from the received parity and side information @xmath51 .",
    "similar to the syndrome approach , at the decoder , we need to find the syndrome of channel ( correlation ) errors . to do this ,",
    "we append the parity to the side information and form a vector of length @xmath3 whose syndrome , neglecting quantization , is equal to the syndrome of error .",
    "that is , @xmath92   = \\left [ \\begin{array}{cc } \\bm{x }    \\\\",
    "\\bm{p }   \\end{array}\\right ]   + \\left [ \\begin{array}{cc } \\bm{e}_k    \\\\   \\bm{0}\\end{array}\\right]= \\bm{g}_{\\mathrm{sys}}\\bm{x}+\\bm{e}_n ,   \\end{aligned}\\ ] ] hence , @xmath93 similarly , when quantization is involved ( @xmath94 ) , we get @xmath95   = \\bm{z } +   \\left [ \\begin{array}{cc } \\bm{0 }    \\\\",
    "\\bm{q}\\end{array}\\right]= \\bm{g}_{\\mathrm{sys}}\\bm{x}+\\bm{e}_n+\\bm{q}_n ,   \\end{aligned}\\ ] ] and @xmath96 in which , @xmath97 .",
    "therefore , we obtain a distorted version of error syndrome . in both cases ,",
    "the rest of the algorithm , which is based on the syndrome of error , is similar to that in the channel coding problem using dft codes .",
    "as we saw earlier , using an @xmath7 code the compression ratio in the syndrome and parity approaches , respectively , is @xmath55 and @xmath90 .",
    "hence , the parity approach is @xmath98 times more efficient than the syndrome approach .",
    "conversely , we can find two different codes that result in same compression ratio , say @xmath56 .",
    "we know that in the parity approach , a @xmath91 code can be used for this matter .",
    "it is also easy to verify that , in the syndrome approach , a code with rate @xmath99 results in the same compression . for odd @xmath3 and @xmath2",
    ", the @xmath100 dft code gives the desired compression ratio .",
    "thus , for a given compression ratio the parity approach implies a code with smaller rate compared to the code required in the syndrome approach .",
    "we evaluate the performance of the proposed systems using a gauss - markov source with zero mean , unit variance , and correlation coefficient 0.9 ; the effective range of the input sequences is thus @xmath101 $ ] .",
    "the sources sequences are binned using a @xmath102 dft code . the compressed vector , either syndrome or parity , is then quantized with a 6-bit uniform quantizer , and transmitted over a noiseless communication media .",
    "the correlation channel randomly inserts one error @xmath103 , generated by a gaussian distribution .",
    "the decoder localizes and decodes errors .",
    "we compare the mse between transmitted and reconstructed codevectors , to measurers end to end distortion . in all simulations ,",
    "we use 20,000 input frames for each channel - error - to - quantization - noise ratio ( ceqnr ) .",
    "we vary the ceqnr and plot the resulting mse .",
    "the result are presented in fig .",
    "[ fig : syndq ] , and compared against the quantization error level in the existing lossy dsc methods .",
    "it can be observed that the mse in the syndrome approach is lower than quantization error except for a small range of ceqnr .",
    "similarly , in the parity approach , the mse is less than quantization error for a wide range of ceqnr .",
    "note that in lossy dsc using binary codes , the mse can be equal to quantization error only if the probability of error is zero .",
    "the performance of both algorithms improves as ceqnr is very high .",
    "this improvement is due to better error localization , since the higher the ceqnr the better the error localization , as shown in fig .",
    "[ fig : poe ] and @xcite . at very low ceqnrs , although error localization is poor , the mse is still very low because , compared to quantization error , the errors are so small that the algorithm may localize and correct some of quantization errors instead . additionally , reconstruction error is always reduced with a factor of @xmath36 , in an @xmath1 dft code .    in terms of compression ,",
    "the parity approach is @xmath104 times more efficient than the syndrome approach , as discussed earlier in section  [ comp ] .",
    "not surprisingly though , the performance of the parity approach is not as good as that of the syndrome approach , because it contains fewer redundant samples . on top of that , in this simulation , @xmath105 of samples are corrupted in the parity approach while this figure is @xmath106 for the syndrome approach .",
    "the parity approach , however , suffers from the fact that dynamic range of parity samples , generated by , could be much higher than that of syndrome samples as @xmath45 increases .",
    "this implies more precision bits to achieve the same accuracy . finally , it is worth mentioning that when data and side information are the same over a block of code , reconstruction error becomes zero in both approaches .",
    "dft code in fig . 2 , 3 .",
    "for both schemes , the virtual correlation channel inserts one error at each channel error to quantization noise ratio . ]",
    "we have introduced a new framework for distributed lossy source coding in general , and wyner - ziv coding specifically . the idea is to do binning before quantizing the continuous - valued signal , as opposed to the conventional approach where binning is done after quantization . by doing binning in the real field",
    ", the virtual correlation channel can be modeled more accurately , and quantization error can be corrected when there is no error . in the new paradigm , wyner - ziv coding",
    "is realized by cascading a slepian - wolf encoder with a quantizer .",
    "we employ real bch - dft codes to do the slepian - wolf in the real field . at the decoder , by introducing both syndrome - based and parity - based systems ,",
    "we adapt the pgz decoding algorithm accordingly . from simulation results , we conclude that our systems , specifically with short codes , can improve the reconstruction error , so that they may become viable in real - world scenarios , where low - delay communication is required .",
    "g.  takos and c.  n. hadjicostis , `` determination of the number of errors in dft codes subject to low - level quantization noise , '' _ ieee transactions on signal processing _ , vol .",
    "56 , pp .  10431054 , march 2008 .",
    "f.  bassi , m.  kieffer , and c.  weidmann , `` source coding with intermittent and degraded side information at the decoder , '' in _ proc .",
    "ieee international conference on acoustics , speech and signal processing ( icassp ) _ , pp .  29412944 , 2008 ."
  ],
  "abstract_text": [
    "<S> we show how real - number codes can be used to compress correlated sources , and establish a new framework for distributed lossy source coding , in which we quantize compressed sources instead of compressing quantized sources . </S>",
    "<S> this change in the order of binning and quantization blocks makes it possible to model correlation between continuous - valued sources more realistically and correct quantization error when the sources are completely correlated . </S>",
    "<S> the encoding and decoding procedures are described in detail , for discrete fourier transform ( dft ) codes . </S>",
    "<S> reconstructed signal , in the mean - squared error sense , is seen to be better than or close to quantization error level in the conventional approach .    </S>",
    "<S> distributed source coding , real - number codes , bch - dft codes , channel coding , wyner - ziv coding . </S>"
  ]
}