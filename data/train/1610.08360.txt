{
  "article_text": [
    "an important tool for making decisions about goodness - of - fit and lack - of - fit is the residual - based empirical distribution function .",
    "this has been studied in many articles .",
    "stute ( 1997 ) and khmaladze and koul ( 2004 , 2009 ) , for example , test parametric hypotheses about the regression function in nonparametric models .",
    "neumeyer and van keilegom ( 2010 ) study additivity tests in heteroskedastic nonparametric regression .",
    "mller , schick and wefelmeyer ( 2012 ) test for normal errors .    in this article",
    "we study the nonparametric regression model @xmath2 with the error @xmath3 independent of the covariate vector @xmath4 .",
    "nonparametric models are particularly useful for residual - based inference because residuals constructed from them are usually consistent .",
    "we are interested in the case where responses @xmath5 are missing , i.e.  we observe the sample @xmath6 , where @xmath7 is an indicator variable which equals one , if @xmath5 is observed , and zero , otherwise . in practical applications , most datasets contain missing responses .",
    "it is important to choose appropriate statistical methods that ensure conclusions are not biased .",
    "we make the assumption that responses are _ missing at random _ ( mar ) .",
    "this means that the probability that @xmath5 is observed depends only on the covariates , @xmath8 we will refer to the model with responses missing at random as the _ mar model_. mar is a common assumption and is reasonable in many situations ( see little & rubin , 2002 , chapter 1 ) . as an example , consider missing responses to a survey question about income .",
    "if additional data ( @xmath4 ) about medical conditions were available , we might see that the response probabilities ( @xmath9 ) are smaller for subjects diagnosed with depression . in this case",
    "the missing mechanism is ignorable since @xmath9 depends only on fully observed data @xmath4 , i.e.  it can be estimated from the data .",
    "more examples of missing data can be found in tsiatis ( 2006 ) , in liang , wang and carroll ( 2007 ) , in molenberghs and kenward ( 2007 ) , and in efromovich ( 2011a , 2011b ) .",
    "we show in this article that the residual - based empirical distribution function @xmath10 given in equation below is an efficient estimator of the unknown error distribution function @xmath11 .",
    "this estimator uses only the complete data pairs @xmath12 , i.e.the available residuals @xmath13 , where @xmath14 is a suitable complete case estimator of the regression function .",
    "demonstrating this requires two steps .",
    "first we show that @xmath10 satisfies the uniform stochastic expansion @xmath15   - f(t ) \\frac{1}{n } \\sj \\delta_j \\ve_j \\bigg| = \\opn.\\ ] ] here @xmath16 is the error density and @xmath17 is the number of complete cases .",
    "then we show that an estimator of @xmath11 that admits this expansion is asymptotically efficient in the sense of hjek and le cam .",
    "this follows from the arguments in section [ eff ] , where we derive , more generally , the efficient influence function for estimating an arbitrary linear functional @xmath18 $ ] , which covers @xmath19 $ ] as a special case .",
    "we conclude that an estimator @xmath10 with expansion is indeed efficient for @xmath11 .",
    "we handle part of the proof that holds by using the _ transfer principle _ for complete case statistics in koul , schick and mller ( 2012 ) .",
    "this principle makes it possible to adapt results for the model where all data are fully observed , the _ full model _ , to missing data models . in particular , we can use the complete case version @xmath14 of an estimator @xmath20 in the fully observed data model ( i.e.  all indicators @xmath21 are equal to one ) .",
    "mller , schick and wefelmeyer ( 2009 ) obtain expansion for the full model using a local polynomial smoother to estimate the regression function @xmath22 , and these authors derive useful results the local polynomial estimator of the regression function that are suitable to the missing data model considered here .",
    "see also neumeyer and van keilegom ( 2010 ) , who consider heteroskedastic nonparametric regression .    in order to summarize the main result by mller , schick and wefelmeyer ( 2009 ) ( theorem [ mswthm1 ] below )",
    ", we introduce some notation .",
    "let @xmath23 be a multi - index and write @xmath24 for the set of multi - indices that satisfy @xmath25 .",
    "mller , schick and wefelmeyer ( 2009 ) estimate @xmath22 by a local polynomial smoother @xmath20 of degree @xmath26 .",
    "it is defined as the component @xmath27 corresponding to the multi - index @xmath28 of a minimizer @xmath29 where @xmath30 @xmath31 is a product of densities , and @xmath32 is a bandwidth sequence",
    ".    the estimator @xmath20 permits the desired expansion , if the assumptions of theorem [ mswthm1 ] ( below ) are satisfied .",
    "this requires , in particular , the regression function @xmath22 belongs to the hlder space @xmath33 , i.e.  it has continuous partial derivatives of order @xmath26 ( or higher ) , and that the partial derivatives of order @xmath26 are hlder with exponent @xmath34 .",
    "the choice of the degree @xmath26 of the local polynomial smoother will also depend on smoothness and moment conditions on the error density , and on the dimension of the covariate vector .",
    "in our simulation study in section [ sims ] , we consider an infinitely differentiable regression function @xmath22 and a one - dimensional covariate @xmath4 , which allows us to use a locally linear smoother .",
    "theorem 1 from mller , schick and wefelmeyer ( 2009 ) is proved under the following assumption on the covariate distribution :    [ assumpg ] the covariate vector @xmath4 is quasi - uniform on the cube @xmath35^m$ ] , i.e.  @xmath4 has a density which is bounded and bounded away from zero on @xmath35^m$ ] .",
    "[ mswthm1 ] let assumption [ assumpg ] be satisfied .",
    "suppose that the regression function @xmath22 belongs to @xmath36 with @xmath37 .",
    "let that the error variable have mean zero , a finite moment of order @xmath38 and a density @xmath16 that is hlder with exponent @xmath39 .",
    "consider the estimator @xmath20 from above with densities @xmath40 that are @xmath41-times continuously differentiable with compact support @xmath42 $ ] .",
    "finally , let the bandwidth sequence satisfy @xmath43 .",
    "then , writing @xmath44 , @xmath45 - \\1\\big[\\ve_j \\leq t\\big ]   - \\ve_j f(t ) \\big\\ } \\bigg|   = \\opn.\\ ] ]    we can apply the transfer principle for asymptotically linear statistics given by koul , mller and schick ( 2012 ) to adapt the results from theorem [ mswthm1 ] for the mar model as follows .",
    "the complete case estimator for @xmath46 is given by @xmath47   = \\frac{1}{n}\\sj \\delta_j\\1\\big[y_j - \\hat r_c(x_j ) \\leq t\\big],\\ ] ] where @xmath14 is the complete case version of @xmath20 , i.e.@xmath14 is given by the component @xmath48 of a minimizer @xmath49 using the transfer principle requires the conditional distribution of @xmath12 given @xmath50 to meet the assumptions on the ( unconditional ) joint distribution of @xmath12 from theorem [ mswthm1 ] . in our case",
    ", it is easy to see how this requirement affects only the covariate distribution @xmath51 : the mar assumption combined with the independence of @xmath4 and @xmath3 yield that @xmath3 and @xmath52 are independent .",
    "this implies the parameters @xmath16 and @xmath22 stay the same when switching from the unconditional to the conditional distribution . in particular ,",
    "the complete case statistic @xmath53 is a consistent estimator for @xmath46 in the mar model ( since @xmath11 remains unchanged ) .",
    "hence , we can keep all but one of our assumptions : only assumption [ assumpg ] must be restated .",
    "[ assumpg1 ] the conditional distribution of the covariate vector @xmath4 given @xmath54 is quasi - uniform on the cube @xmath55^m$ ] , i.e.  it has a density which is bounded and bounded away from zero on @xmath55^m$ ] .",
    "the transfer principle implies the complete case version of the estimator from theorem [ mswthm1 ] has the corresponding expansion .",
    "this expansion is equivalent to @xmath56 - \\1\\big[\\ve_j \\leq t\\big ]   - \\ve_j f(t ) \\bigg\\ } \\bigg|   = \\opn.\\ ] ] hence , we have , uniformly in @xmath57 , @xmath58   + \\opn   = f(t ) + \\avj   b(\\delta_j , \\ve_j , t ) + \\opn,\\ ] ] where @xmath59 - f(t ) + \\ve f(t)\\}$ ] is the influence function .",
    "this is indeed the _ efficient influence function _ for estimating @xmath46 : see corollary [ corfhateffexpan ] in section [ eff ] .",
    "this brings us to the main result of this paper .",
    "[ thmfhatccexpan ] consider the nonparametric regression model with responses missing at random .",
    "suppose the assumptions of theorem [ mswthm1 ] are satisfied , with assumption [ assumpg1 ] in place of assumption [ assumpg ] .",
    "then the _ complete case _",
    "estimator @xmath10 of the error distribution function @xmath11 satisfies the stochastic expansion , @xmath60 - \\1\\big[\\ve_j \\leq t\\big ]   - \\ve_j f(t ) \\big\\ } \\bigg|   = \\opn.\\ ] ] if the error density @xmath16 furthermore fulfills assumption [ assumpf ] , stated in section [ eff ] , then @xmath10 is asymptotically efficient in the sense of hjek and le cam for estimating @xmath11 , with influence function @xmath61 - f(t ) + \\ve f(t ) \\big\\}.\\ ] ]    [ remnottprin ] if the transfer principle were not available , the expansion in theorem [ thmfhatccexpan ] could be derived by mimicking the ( rather elaborate ) proofs of lemma 1 in mller , schick and wefelmeyer ( 2009 ) and of theorem 2.2 in mller , schick and wefelmeyer ( 2007 ) , who estimate the error distribution in a general semiparametric regression model .",
    "the arguments are essentially the same  what is new now is the presence of indicators .",
    "analogously to mller , schick and wefelmeyer ( 2009 ) , derive an approximation @xmath62 of the difference @xmath63 , @xmath64^m } \\big|   \\rhat_c(x ) - r(x ) - \\ahat_c(x ) \\big|   = \\opn;\\ ] ] see equation ( 1.4 ) in that paper .",
    "note , the events @xmath65 and @xmath66 are equivalent .",
    "combining this fact and with replacing the two empirical distribution functions @xmath10 and @xmath67 $ ] in the proof of theorem 2.2 in mller , schick and wefelmeyer ( 2007 ) yields @xmath68 - \\1\\big[\\ve_j \\leq t\\big ]   - f_{\\ahat_c}(t ) - f(t ) \\big\\ } \\bigg|   = \\opn,\\ ] ] writing @xmath69 \\bigg ]   = e\\big[\\1\\big[\\ve \\leq t + \\ahat_c(x)\\big]\\,\\big|\\,\\delta=1\\big ]   = \\int_{[0,\\,1]^m } f\\big(t + \\ahat(x)\\big ) \\ , g_1(dx).\\ ] ] here @xmath70 denotes the conditional distribution of @xmath4 given @xmath71 .",
    "a taylor expansion applied to the difference @xmath72 in the above expansion gives @xmath56 - \\1\\big[\\ve_j \\le t\\big ]   - f(t ) \\int_{[0,\\,1]^m } \\ahat_c(x)\\,g_1(dx ) \\bigg\\ } \\bigg|   = \\opn.\\ ] ] the desired expansion now follows from this combined with @xmath73^m } \\ahat_c(x)\\,g_1(dx )   = \\avj \\frac{\\delta_j}{e\\delta } \\ve_j + \\opn.\\ ] ] the last approximation is the complete case version of equation ( 1.3 ) in mller , schick and wefelmeyer ( 2009 ) .",
    "it can be verified by inspecting the proof of lemma 1 in that paper , where properties of locally polynomial smoothers are derived .",
    "keep in mind that our estimators are constructed from the complete cases ( equation above ) , which explains the indicators in the above formula .",
    "note , the uniform expansion implies @xmath10 satisfies a functional central limit theorem , and the efficiency property of the estimator @xmath10 guarantees that competing estimators will not be able to outperform it in large samples .",
    "this includes estimators based on imputations that attempt to replace the missing responses .",
    "the article is organized as follows .",
    "we provide the efficient influence function for estimating linear functionals of the error distribution function @xmath11 in section [ eff ] , and we specialize these results to estimators of @xmath11 . in section [ sims ] , we illustrate this result with simulations for two examples .",
    "the first example demonstrates the efficiency property of the complete case estimator @xmath10 by comparing it with a ` tuned ' estimator using an imputation technique that is in the spirit of gonzlez - manteiga and prez - gonzlez ( 2006 ) . for our second example",
    ", we perform simulations similar to those in mller , schick and wefelmeyer ( 2012 ) , who use a martingale transform approach to test for normal errors in the full model .",
    "the test statistics involve the estimators from the first example .",
    "in this section we provide the _ efficient influence function _ for estimating the linear functional @xmath18 $ ] using observations @xmath74 , @xmath75 .",
    "we first follow the arguments of mller , schick and wefelmeyer ( 2006 ) , who study efficient estimation of general differentiable functionals with data of the above form .",
    "we summarize their main arguments and refer to that paper for more details .",
    "we then focus on the functional @xmath18 $ ] , which mller , schick and wefelmeyer ( 2004 ) study in the full model .",
    "this allows us to adapt parts of their proofs to the mar model considered here . to begin",
    ", we will require the fisher information for location of the error distribution to be finite :    [ assumpf ] the error density @xmath16 is absolutely continuous with almost everywhere derivative @xmath76 satisfying @xmath77 where @xmath78 is the fisher information for location and @xmath79 is the score function .",
    "we do not assume a parametric model for the regression function or for the distribution of the observations .",
    "the parameter set @xmath80 of the statistical model therefore includes a family of covariate distributions @xmath81 satisfying assumption [ assumpg ] , a family of error distributions @xmath82 satisfying assumption [ assumpf ] , a space of regression functions @xmath83 that belong to @xmath36 , and a family of response probability distributions @xmath84 that are characterized by proportion functions mapping @xmath35^m$ ] to @xmath85 $ ] .",
    "it follows that we can write @xmath86 .",
    "since the construction of the efficient influence function utilizes the directional information in @xmath80 , we will now identify the set @xmath87 of all perturbations related to the statistical model , which may be thought of as directions .",
    "the joint distribution @xmath88 depends on the marginal distribution @xmath89 of @xmath4 , the conditional probability @xmath90 that @xmath7 equals one given @xmath91 , and the conditional distribution @xmath92 of @xmath5 given @xmath91 : @xmath93 where @xmath94 denotes the bernoulli distribution with parameter @xmath95 and @xmath96 is the dirac measure for @xmath97 .",
    "now consider perturbations @xmath98 , @xmath99 and @xmath100 of @xmath51 , @xmath9 and @xmath101 , respectively , that are _ hellinger differentiable _ in the following sense : @xmath102^m } \\bigg\\ {   n^{1/2}\\big\\{dg_{nu}^{1/2}(x ) - dg^{1/2}(x)\\big\\ }   & - \\frac12 u(x)dg^{1/2}(x)\\bigg\\}^2\\,dx   \\rightarrow 0 , \\\\ \\int_{[0,\\,1]^m } \\int_{\\{0,\\,1\\ } } \\bigg\\ {   n^{1/2}\\big\\{db_{\\pi_{nw}}^{1/2}(z\\,|\\,x ) - db_{\\pi}^{1/2}(z\\,|\\,x)\\big\\ }   & - \\frac12 \\{z - \\pi(x)\\}w(x)db_{\\pi}^{1/2}(z\\,|\\,x)\\bigg\\}^2\\,g(dx )   \\rightarrow 0 , \\\\   \\int_{[0,\\,1]^m } \\int_{-\\infty}^{\\infty } \\bigg\\ {   n^{1/2}\\big\\{dq_{nv}^{1/2}(y\\,|\\,x ) - dq^{1/2}(y\\,|\\,x)\\big\\ }   & - \\frac12 v(x , y)dq^{1/2}(y\\,|\\,x)\\bigg\\}^2\\,g_1(dx )   \\rightarrow 0 , \\end{aligned}\\ ] ] writing @xmath70 for the conditional distribution of @xmath4 given that @xmath50 . the perturbed distribution functions @xmath98 , @xmath103 and",
    "@xmath100 must satisfy the original model constraints , which requires their hellinger derivatives to be restricted to suitable function spaces : @xmath104 belongs to @xmath105 , i.e.  @xmath106 and @xmath107^m } u(x)\\,g(dx ) = 0 $ ] ; @xmath108 belongs to @xmath109^m } w^2(x ) \\pi(x)\\{1 - \\pi(x)\\}\\,g(dx ) < \\infty   \\bigg\\},\\ ] ] writing @xmath110 ; and @xmath111 belongs to @xmath112    note that models for @xmath70 , @xmath9 and @xmath101 will imply further restrictions on the perturbations in order to satisfy those model assumptions .",
    "this means that @xmath104 , @xmath108 and @xmath111 must be further restricted to subspaces of @xmath105 , @xmath113 and @xmath114 , respectively . here",
    "no model assumptions on @xmath51 and @xmath9 have been introduced , but model equation does present a structural constraint on the conditional distribution @xmath101 of @xmath5 given @xmath4 .",
    "this implies that we only have to identify the appropriate subspace @xmath115 of @xmath114 to account for this additional structure .    since the covariates and the errors are assumed to be independent , we may write the density function @xmath116 of @xmath101 as @xmath117 . using this notation ,",
    "the constraint on @xmath118 now states that @xmath119 in order to derive the explicit form of the function space @xmath120 , we introduce further respective perturbations @xmath121 and @xmath122 for the unknown functions @xmath16 and @xmath22 , and we can write @xmath123 where @xmath124 and @xmath125 with @xmath126 and @xmath127 .",
    "our assumptions on model require the errors to have mean zero and the perturbed error density @xmath128 must integrate to one .",
    "hence , @xmath129 takes the form @xmath130 we can simply restrict the perturbation @xmath122 to belong to @xmath131 , which follows from the fact that we do not assume a parametric form for @xmath22 .    with the appropriate spaces @xmath129 and @xmath131 identified , we can specify the appropriate form of @xmath115 . in the following arguments we will write `` @xmath132 '' to denote asymptotic equivalence , i.e.  equality up to an additive term of order @xmath133 . as in mller ( 2009 ) , who considers a _ parametric _ ( nonlinear ) regression function , a brief sketch gives @xmath134 hence ,",
    "we can write @xmath135 equation implies that @xmath115 has the form @xmath136    we can see that @xmath87 is the set containing all possible hellinger perturbations of the statistical model parameters : @xmath137 the perturbed distribution @xmath138 , with @xmath139 in @xmath87 , of the observation @xmath140 can be written @xmath141 it then follows that @xmath138 is hellinger differentiable with perturbation function @xmath142 and we have the stochastic expansion , writing @xmath143 for the density function of @xmath144 , @xmath145   + \\op.\\ ] ] since @xmath146 is asymptotically normally distributed with mean zero and variance @xmath147 $ ] by the central limit theorem , it follows for the expansion above to characterize local asymptotic normality in the present situation .",
    "the efficient influence function of a differentiable functional is characterized by its canonical gradient , which takes the form @xmath148 for some @xmath149 .",
    "this gradient is defined as the orthogonal projection of the gradient for the functional @xmath18 $ ] ( to be specified later ) onto the tangent space given by the perturbed distributions @xmath138 .",
    "it then follows from for the tangent space @xmath150 to be equal to the closure of the linear subspace formed by @xmath151 . since @xmath151 is a sum of orthogonal elements we can write @xmath152    we are interested in the linear functional @xmath18 $ ] . in order to specify a gradient for @xmath18 $ ] , we need the directional derivative @xmath153 of @xmath18 $ ] , which is characterized by a limit as follows . as in mller ,",
    "schick and wefelmeyer ( 2004 ) we have , for every @xmath154 , @xmath155 = e\\big[h_0(\\ve)s(\\ve)\\big],\\ ] ] where @xmath156 is the projection of @xmath157 onto @xmath158 : @xmath159   - \\frac{z}{\\sigma^2}e\\big[\\ve h(\\ve)\\big].\\ ] ] here @xmath160 denotes the error variance . hence , @xmath18 $ ] is directionally differentiable , and implies this directional derivative is @xmath161 .",
    "it then follows for @xmath18 $ ] to have the gradient @xmath162 , with @xmath156 given by .    by the convolution theorem ( see , for example , section 2 of schick , 1993 ) , the unique canonical gradient @xmath163 is obtained by orthogonally projecting the gradient @xmath162 of @xmath18 $ ] onto the tangent space @xmath150 .",
    "hence , @xmath164 must be of the form @xmath165 which satisfies @xmath166 = e\\big [ g^*(x , \\delta y , \\delta )   d_\\gamma(x , \\delta   y , \\delta ) \\big]\\end{aligned}\\ ] ] for every @xmath167 .",
    "a straightforward calculation shows the right - hand side of is equal to @xmath168 \\\\ & = e\\big[u^*(x)u(x)\\big ]   + e\\big [ \\delta\\big\\{s^*(\\ve ) + \\ell(\\ve)t(x)\\big\\ }   \\big\\{s(\\ve ) + \\ell(\\ve)t^*(x)\\big\\ } \\big ] \\\\ & \\quad + e\\big[\\{\\delta - \\pi(x)\\}^2w^*(x)w(x)\\big ] \\\\ & = e\\big[u^*(x)u(x)\\big ] + e\\delta \\big\\ {   e\\big[s^*(\\ve)s(\\ve)\\big ]   + e\\big[\\ell_0(\\ve)s^*(\\ve)\\big]e_1\\big[t(x)\\big ] \\\\ &",
    "\\quad + e\\big[\\ell_0(\\ve)s(\\ve)\\big]e_1\\big[t^*(x)\\big ]   + je_1\\big[t^*(x)t(x)\\big ] \\big\\ }   + e\\big[\\pi(x)\\{1 - \\pi(x)\\}w^*(x)w(x)\\big],\\end{aligned}\\ ] ] where @xmath78 is the fisher information given in assumption [ assumpf ] and @xmath169 is the projection of @xmath170 onto @xmath120 , i.e.  @xmath171 .",
    "the notation @xmath172 indicates the expectation is with respect to the conditional distribution @xmath70 . for convenience ,",
    "we introduce the quantity @xmath173 which is calculated analogously to @xmath78 : @xmath174   = e\\bigg[\\bigg\\{\\ell(\\ve ) - \\frac{\\ve}{\\sigma^2}\\bigg\\}^{2}\\bigg ]   = j - \\frac{1}{\\sigma^{2}}.\\ ] ]    with appropriate choices of @xmath34 in @xmath87 , it easily follows from for @xmath175 . now choosing the zero function for the function @xmath104 , becomes @xmath176 = e\\delta \\big\\ {   e\\big[s(\\ve)s^*(\\ve)\\big ]   + e\\big[\\ell_0(\\ve)s(\\ve)\\big]e_1\\big[t^*(x)\\big ] \\big\\},\\end{aligned}\\ ] ] which must hold for all @xmath177 .",
    "this implies @xmath178 ,   \\qquad z \\in \\r.\\ ] ]    following mller , schick and wefelmeyer ( 2004 ) , we can consider @xmath179 written as an orthogonal sum of functions with mean zero and of constants , i.e.  we write @xmath180 $ ] .",
    "this means we can decompose @xmath181 into @xmath182\\ } + e_1[t(x)]$ ] .",
    "finally , choosing the zero function for @xmath121 and inserting @xmath183 from , becomes @xmath184e_1\\big[t(x)\\big ]   + ( j - j_0)e_1\\big[t^*(x)\\big]e_1\\big[t(x)\\big ] \\\\ & \\quad + je_1\\bigg [ \\big\\{t^*(x ) - e_1\\big[t^*(x)\\big]\\big\\ }   \\big\\{t(x ) - e_1\\big[t(x)\\big]\\big\\ } \\bigg ] \\\\ & = \\bigg\\ { \\frac1{e\\delta}e\\big[\\ell_0(\\ve)h_0(\\ve)\\big ]   + \\frac1{\\sigma^2}e_1\\big[t^*(x)\\big ] \\bigg\\}e_1\\big[t(x)\\big ] \\\\ & \\quad +   je_1\\bigg [ \\big\\{t^*(x ) - e_1\\big[t^*(x)\\big]\\big\\ }   \\big\\{t(x ) - e_1\\big[t(x)\\big]\\big\\ } \\bigg],\\end{aligned}\\ ] ] which must hold for every @xmath185 .",
    "this implies @xmath186 is equal to its mean @xmath187 $ ] and @xmath188 = -\\frac{\\sigma^2}{e\\delta }   e\\big[h_0(\\ve)\\ell_0(\\ve)\\big].\\ ] ] therefore , combining with the fact that @xmath186 must be equal to its mean yields @xmath189 $ ] , @xmath190^m$ ] .",
    "combining the fact that @xmath191 with and , we obtain the following result :    [ lemcangradeh ] the canonical gradient of @xmath18 $ ] is @xmath192 given in that characterized by @xmath193 , with @xmath194\\ell_0(z)\\big\\ } ,   \\qquad z \\in \\r , \\\\",
    "t^*(x ) = -\\frac{\\sigma^2}{e\\delta}e\\big[h_0(\\ve)\\ell_0(\\ve)\\big ] ,   \\qquad x \\in",
    "[ 0,\\,1]^m,\\end{gathered}\\ ] ] where @xmath195 $ ] is the error variance , @xmath156 is given in and @xmath171 .",
    "an estimator @xmath196 of @xmath18 $ ] is called _ efficient _ , in the sense of hjek and le cam , when @xmath196 is asymptotically linear with influence function equal to the canonical gradient @xmath197 that characterizes @xmath18 $ ] , i.e.  if the expansion holds : @xmath198\\big\\ }   = n^{-1/2}\\sj g^*(x_j , \\delta_j y_j , \\delta_j ) + o_p(1).\\ ] ] a straightforward calculation combining the result of lemma [ lemcangradeh ] with the display above and formula yields :    [ coreheffif ] consider the nonparametric regression model with responses missing at random",
    ". an efficient estimator @xmath196 of @xmath18 $ ] must satisfy the expansion @xmath198\\big\\ }   = n^{-1/2}\\sj \\frac{\\delta_j}{e\\delta } \\big\\ {   h(\\ve_j ) - e\\big[h(\\ve)\\big ] - e\\big[\\ell(\\ve)h(\\ve)\\big]\\ve_j \\big\\ }   + \\op.\\ ] ]    [ remmswestimates ] mller , schick and wefelmeyer ( 2004 ) construct residual - based estimators @xmath199 for estimating @xmath18 $ ] in the full model . in their section",
    "2 , they give conditions for the i.i.d.representation : @xmath200\\ve_j\\big\\ } + \\op,\\ ] ] which characterizes an efficient estimator .",
    "( for simplicity , we assume in this remark that @xmath157 is differentiable . )",
    "note that @xmath201 = e[\\ell(\\ve)h(\\ve)]$ ] .",
    "hence , using the transfer principle , we see that the complete case versions of their estimators have the expansion from the previous corollary , and , therefore , these estimates are also efficient in the mar model .",
    "the function @xmath202 $ ] is of particular interest because many statistical methods are residual - based and require estimation of the error distribution function .",
    "using corollary [ coreheffif ] with this particular @xmath203 , we obtain an expansion for the residual - based empirical distribution function :    [ corfhateffexpan ] consider the nonparametric regression model with responses missing at random .",
    "an estimator @xmath204 of the error distribution function @xmath11 is efficient , if it satisfies the expansion @xmath205 - f(t ) + f(t)\\ve_j\\big\\ } + \\op.\\ ] ]    note , this is the expansion of the complete case estimator @xmath10 from the previous section , which provide the proof of the second assertion in theorem [ thmfhatccexpan ] .",
    "to conclude the article , we present a brief simulation study of the previous results in two important examples .",
    "the first example compares the efficiency property of @xmath10 , which is constructed using only the completely observed data , to another estimator @xmath206 , which is constructed using an imputation methodology . in the second example , we consider applying a goodness - of - fit test for normal errors to the residuals from the nonparametric regression constructed by only the complete cases and by the same imputation methodology that was implemented in the first example . in both examples , we assume a nonparametric regression model , but choosing @xmath207 which we expect preserves the nonparametric nature of the studies .",
    "data points from a typical simulated dataset._,height=302 ]    the covariates are generated from a uniform distribution and the errors from a normal distribution : @xmath208 and @xmath209 for @xmath210 ; see figure [ figure1 ] for a scatterplot of a typical simulated dataset .",
    "finally , the indicators @xmath21 have a bernoulli distribution with proportion function parameter @xmath211 , which is chosen to be the logistic distribution function with a mean of zero and scale parameter of one : @xmath212 consequently , the average amount of missing data is 50% and ranges between 27% and 73% .",
    "finally , we work with the local linear smoother ( it is easy to see that @xmath22 is lipschitz and , therefore , it belongs to the hlder space @xmath213 ) , and the bandwidth sequence @xmath214 is taken as @xmath215 .",
    "the assumptions of theorem [ thmfhatccexpan ] are then satisfied .",
    "we consider two estimators of the error distribution function .",
    "the first estimator is the proposed complete case estimator @xmath10 and the second is a ` tuned ' version of @xmath10 that utilizes an imputation technique .",
    "similar to gonzlez - manteiga and prez - gonzlez ( 2006 ) , we take the initial local polynomial complete case estimator @xmath14 ( see equation ) to produce the completed sample @xmath216 , @xmath217 .",
    "we chose to fully impute the responses , i.e.  @xmath218 .",
    "this is a variation of the approach of gonzlez - manteiga and prez - gonzlez ( 2006 ) , who work with partially imputed responses @xmath219 .",
    "a new local polynomial estimator @xmath220 is then constructed from the completed sample .",
    "when @xmath5 is observed , we can compute adjusted residuals @xmath221 based on the updated estimator @xmath222 .",
    "this means we still work with complete cases when estimating @xmath11 , but now the estimated regression has changed .    using these residuals we obtain the new tuned estimator @xmath223.\\ ] ]",
    "the results in the previous sections show the complete case estimator @xmath10 is an ( asymptotically ) efficient estimator of @xmath11 .",
    "our discussion in remark [ remnottprin ] also suggests the tuned estimator @xmath206 is also efficient ; i.e.  both estimators are asymptotically equivalent .",
    "we expect that @xmath206 can be expanded in the same way as @xmath10 : @xmath224 - \\1\\big[\\ve_j \\leq t\\big]\\big\\ } -   f(t ) \\int_{[0,\\,1]^m } \\ahat^*(x)\\,g_1(dx ) \\bigg|   = \\opn,\\ ] ] where @xmath225 is now an approximation of the difference @xmath226 ( cf .  equation ( [ approx ] ) in remark [ remnottprin ] ) .",
    "the integral in the display above can be written as @xmath73^m } \\ahat^*(x)\\,g_1(dx )   = \\int_{[0,\\,1]^m } \\ahat_c(x)\\,g_1(dx )   + \\int_{[0,\\,1]^m } \\big\\{\\ahat^*(x ) - \\ahat_c(x)\\big\\}\\,g_1(dx).\\ ] ] since @xmath227 approximates the difference @xmath228 of two consistent estimators of @xmath229 , we expect the last term in the display above to be asymptotically negligible . repeating the arguments from remark [ remnottprin ] would then give the desired expansion : @xmath230 - \\1\\big[\\ve_j \\leq t\\big ]   - \\ve_j f(t ) \\big\\ } \\bigg|   = \\opn,\\ ] ] and , hence , both @xmath10 and @xmath206 should have the same asymptotic expansion , i.e.  both estimators are asymptotically equivalent .    in order to further check the conjecture that both estimators are asymptotically equivalent",
    ", we conducted a simulation study using 1000 trials .",
    "we considered four sample sizes and five different points at which the error distribution function was evaluated .",
    "the findings are summarized in table [ table1 ] .",
    "note , we also implemented another estimator , which uses partial imputation to complete the sample as suggested by gonzlez - manteiga and prez - gonzlez ( 2006 ) , but our approach performed slightly better and so we only report the results for our tuned estimator @xmath206 . for the second smoothing step we chose the same bandwidth as in the first step , @xmath231 .",
    "._simulated and true asymptotic mse of @xmath232 and @xmath233 at the points @xmath234 , @xmath235 , @xmath236 , @xmath237 and @xmath238 . _",
    "[ cols=\">,^,^,^,^,^,^,^,^,^,^ \" , ]     table [ table2 ] shows , when the errors are normally distributed ( and the null hypothesis is true ) , the test using @xmath239 rejects the null hypothesis 2.2% of the time for samples of size 50 , and 3% of the time for samples of size 200 .",
    "this indicates the test using @xmath239 is slightly conservative .",
    "we find similar conservative behavior in the test using @xmath240 , where the hypothesis of normality is rejected 2.5% and 2.8% of the time for sample sizes 50 and 200 , respectively .",
    "when the null hypothesis is not true , the power figures are fairly close for both tests .",
    "the test using @xmath240 seems to be more powerful for low sample sizes , which is expected from the results of the first example .",
    "the differences are less pronounced for the larger sample size of 200 , suggesting that the two approaches are asymptotically equivalent  which is also what we would expect given the discussion and the simulation results in the previous example . summing up",
    ", both test procedures have similar performance .",
    "the test based on @xmath239 appears to be the better choice for moderately large ( or large ) samples because it is easier to implement .",
    "ursula u. mller was supported by nsf grant dms 0907014 .",
    "the authors thank the referees for a number of suggestions that improved the manuscript .",
    "we also thank susan davis for her helpful comments on an earlier draft .",
    "mller , u.u . ,",
    "schick , a.  and wefelmeyer , w.  ( 2006 ) .",
    "imputing responses that are not missing . in _ probability , statistics and modelling in public health _ , eds .",
    "nikulin , m. , commenges , d.  and huber , c. , springer , 350 - 363 ."
  ],
  "abstract_text": [
    "<S> this article considers nonparametric regression models with multivariate covariates and with responses missing at random . </S>",
    "<S> we estimate the regression function with a local polynomial smoother . </S>",
    "<S> the residual - based empirical distribution function that only uses complete cases , i.e.  residuals that can actually be constructed from the data , is shown to be efficient in the sense of hjek and le cam . in the proofs </S>",
    "<S> we derive , more generally , the efficient influence function for estimating an arbitrary linear functional of the error distribution ; this covers the distribution function as a special case . </S>",
    "<S> we also show that the complete case residual - based empirical distribution function admits a functional central limit theorem . </S>",
    "<S> the article concludes with a small simulation study investigating the performance of the complete case residual - based empirical distribution function .     </S>",
    "<S> correspondence may be addressed to either author : + @xmath0 fakultt fr mathematik , lehrstuhl fr stochastik , 44780 bochum , de + email : justin.chown@ruhr-uni-bochum.de + @xmath1 department of statistics , texas a&m university , college station , tx 77843 - 3143 , usa + email : uschi@stat.tamu.edu_ ]    _ keywords : _ efficient estimator , empirical distribution function , local polynomial smoother , martingale transform , missing at random , nonparametric regression , test for normal errors , transfer principle    _ 2010 ams subject classifications : _ primary : 62g05 ; secondary : 62g08 , 62g20 </S>"
  ]
}