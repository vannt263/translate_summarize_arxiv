{
  "article_text": [
    "if we solve linear systems for increasing dimensions , then we observe increasing numerical condition numbers , up to the point where double arithmetic no longer suffices to obtain an accurate solution .",
    "the linear systems we encounter are not random , but originate from the application of newton s method to a polynomial system . for higher precision arithmetic",
    "we use the qd library  @xcite and its gpu version  @xcite .",
    "numerical continuation methods apply newton s method in a predictor - corrector algorithm to track solution paths , see  @xcite for the application to solving polynomial systems .",
    "the focus in this paper is on the acceleration of the tracking of one single path , which requires a fine granularity in the parallel algorithm , because the tracking of one path proceeds step by step , computing points along a path in a strictly sequential order .",
    "the purpose of our investigations is to find thresholds on dimensions and sizes of interesting classes of polynomial systems which clearly benefit from the accelerated path tracking methods .",
    "the goal of acceleration is to offset the computational overhead caused by the higher precision arithmetic .    in our investigations , we selected the reverse method of algorithmic differentiation  @xcite to evaluate the polynomials and their derivatives . even already for systems of modest size , this method leads to a good occupancy of the gpu on many blocks of threads .",
    "for fully expanded polynomials with general coefficients , the cost of evaluation and differentiation agrees with theoretical complexity bounds  @xcite for this problem . for the linear system solving ,",
    "we have chosen to implement the modified gram - schmidt method instead of row reduction with partial pivoting , which is more commonly used in path tracking methods .",
    "because the right - looking algorithm provides more thread - level parallelism ( as pointed out in  @xcite ) , we first implemented a pure right - looking algorithm .",
    "combining left and right looking in a tiled algorithm reduces memory access and yields better speedups .    for very sparse polynomials of low degree ,",
    "the linear algebra will dominate the computational cost of the path tracker . for such problems ,",
    "the number of equations and variables must be at least several hundreds in order to obtain significant speedups from the acceleration . for general polynomials , where the cost of evaluation and differentiation is substantial , already for systems of modest size ( where the dimension is less than a hundred ) acceleration may already compensate for the cost of one extra level of precision",
    ".    * related work . *",
    "the granularity of parallel homotopy algorithms was investigated in  @xcite .",
    "the consideration in  @xcite of fine grained algorithms for architectures with fast processor interconnection and relatively slow processor speeds is relevant for gpus .    in computer algebra , the implementation of massively parallel algorithms for polynomial operations on gpus",
    "are described in  @xcite and  @xcite .",
    "the computation of the smith normal form as needed to solve large systems of binomials ( that is : having exactly two monomials in every equation ) using the nvidia gtx 780 graphics card is reported in  @xcite and in  @xcite .",
    "the modified gram - schmidt method relates to lattice basis reduction algorithms which have been implemented on graphics cards  @xcite .    in algorithmic differentiation",
    ", reports on parallel implementations for multicore architectures can be found in  @xcite and on gpus in  @xcite .    * our contributions . * in  @xcite , we experimentally showed that the cost overhead of double double arithmetic is of a similar magnitude as the cost of complex double arithmetic and that eight cpu cores may suffice to offset this overhead in multithreaded implementations . in  @xcite , we continued this line of investigation on the gpu , based on our gpu implementations of evaluation and differentiation algorithms  @xcite , combining our gpu implementation of the the gram - schmidt orthogonalization method  @xcite . the computational results in  @xcite and  @xcite were on randomly generated data .",
    "the data in this paper comes from relevant polynomial systems , relevant to actual applications . in particular",
    ", the cyclic @xmath0-root problems relate to the construction of complex hadamard matrices  @xcite and the pieri homotopies solve the output placement problem in linear systems control  @xcite .",
    "this paper reports on improvements and the integration of our building blocks ( described in  @xcite ) in an accelerated path tracker .",
    "good speedups relative to the cpu are obtained on benchmark problems , sufficiently large enough to compensate for the computational overhead caused by the double double arithmetic .",
    "several software packages are available to track solution paths defined by polynomial homotopies , e.g. : bertini  @xcite , hom4ps  @xcite , hom4ps-2.0  @xcite , hom4ps-3  @xcite , phom  @xcite , nag4m2  @xcite , pss  @xcite , and hompack @xcite .",
    "while our work is aimed at accelerating the path trackers in phcpack  @xcite , also other software packages may benefit from our work .",
    "the focus in this paper is the acceleration of the tracking of one _ single _ path .",
    "the acceleration of the tracking of _ many _ paths defined by a polynomial homotopy is described in  @xcite .",
    "the gpu that runs our accelerated path tracker also hosts our cloud service  @xcite .",
    "a family of systems in one parameter @xmath1 is called a homotopy and denoted as @xmath2 .",
    "the artificial - parameter homotopy we consider is @xmath3 where @xmath4 , @xmath5 . for random  @xmath6 and with complex arithmetic , the solution paths are free from singularities for all  @xmath7 , see for instance  @xcite .",
    "as done in phcpack  @xcite , the relaxation constant @xmath8 leads to smaller step sizes at the start when @xmath9 and towards the end when @xmath10 .",
    "an input - output specification of the path tracker summarizes the notations :    input : = @xmath11 , @xmath12 : @xmath13 polynomials in @xmath0 variables ; + @xmath14 , target system , must be solved ; + @xmath15 , start system , with a start solution ; + @xmath16 : @xmath17 , a start solution ; + @xmath18 parameters in @xmath2 as in  ( [ eqhomotopy ] ) .",
    "+ output : @xmath16 : @xmath19 , a solution of @xmath14 .",
    "before we apply newton s method as a corrector , a predictor extrapolates the current solution  @xmath20 at the current value for  @xmath1 to the next point on the path .",
    "based on the outcome of the convergence of newton s method , an adaptive step size control strategy increases or decreases the step size @xmath21 .    in one newton step",
    "we compute @xmath22 as the solution of @xmath23 where @xmath24 denotes the matrix of all partial derivatives of the system @xmath14 .",
    "then the current approximation is updated as @xmath25 . to continue with the next newton step , we run the tests @xmath26 where @xmath27 and @xmath28 are respectively the backward and forward error of the problem .",
    "path following in an artificial - parameter homotopy follows an increment - and - fix strategy .",
    "the homotopy continuation parameter  @xmath1 is increased independently of the coordinates of  @xmath20 in the predictor stage , as @xmath29    because the parameter  @xmath1 in our homotopies  ( [ eqhomotopy ] ) is artificial we apply the increment - and - fix predictor - corrector methods to solution paths in complex space . our use of extended precision arithmetic is targeted to overcome the numerical instabilities that arise from working with higher degree polynomials which leads to matrices that may contain numbers of various magnitudes . in table",
    "[ tabruncyclic ] we report several instances where one level of precision does not suffice to successfully track a solution path to the end",
    ".    * problem statement . *",
    "the difficulty to accelerate the tracking of one single path is that the predictor - corrector method is a strictly sequential process .",
    "although we compute many points on a solution path , we can not compute those points in parallel , independently from each other . in order to move to the next point on the path , the correction for the previous point must be completed .",
    "this difficulty requires a fine granularity in the parallel algorithm and a sufficiently high enough threshold on the dimension of the problem .",
    "in this section we explain the implementation of path tracking with predictor - corrector methods in an increment - and - fix approach for artificial - parameter polynomial homotopies .",
    "we start with the high level descriptions and then go into the finer details of the accelerated algorithms .",
    "the input format for a polynomial system is a fully expanded distributed symbolic representation of a list of polynomials .",
    "this representation fits our choice of polynomial evaluation and differentiation algorithms . in the reverse mode of algorithmic differentiation ,",
    "the focus is on a product of variables , which provides a large amount of thread parallelism , suitable for gpus .",
    "our path tracker computes with complex arithmetic in three modes : in double , double double , or quad double precision .    in the first stage ,",
    "the value for the homotopy continuation parameter is incremented as @xmath30 and the corresponding solution  @xmath20 is updated using extrapolation methods . denoting @xmath31 as the degree of extrapolation polynomial , the values for  @xmath1 as @xmath32 , @xmath33 , @xmath34 , @xmath35 , and the corresponding coordinates as @xmath36 , @xmath37 , @xmath34 , @xmath38 , then the predicted value for @xmath39 at the new value for  @xmath1",
    "is obtained by evaluating a @xmath31th degree polynomial interpolating through those points . as the extrapolation occurs independently for each solution coordinate , as many threads as the dimension  @xmath0 are occupied .",
    "but even with complex quad double arithmetic , the predictor stage has cost  @xmath40 , significantly less than the other stages .",
    "figure  [ figparalgnewton ] lists pseudocode for the accelerated corrector , illustrating the synchronizations performed by the kernel launches and the memory transfers from the device to the host to determine the convergence .",
    "= input : = @xmath41 , polynomial instructions + @xmath42 , gpu workspace + @xmath43 , parameters for newton s method + output : = @xmath44 or @xmath45 + updated @xmath46 + @xmath47 = @xmath48 + for @xmath31 from 1 to @xmath49 do + ( @xmath41,@xmath42 ) + launch kernel max_array(@xmath50 , @xmath51 ) + with single block + copy @xmath51 from gpu to host + if ( @xmath52 ) + return @xmath45 + if ( @xmath53 ) + return @xmath44 + ( @xmath42 ) + launch kernel max_array(@xmath54 , @xmath55 ) + with single block + copy @xmath55 from gpu to host + launch kernel update_@xmath20(@xmath46,@xmath54 ) + if ( @xmath56 ) + return @xmath44 + @xmath47 = @xmath51 + return @xmath45    the step control coincides with the launching of new kernels which can be done on the host , as only a constant amount of data ( independent of the size of the problem ) needs to be transferred between host and device . in particular , to determine the success of newton s method , the device computes a norm of the update @xmath57 , the norm the residual vector @xmath58 and sends this number to the host . in reply",
    ", the device receives the new step size from the host before launching the next predictor - corrector kernel .",
    "as the host controls step size , there is a constant amount of data transfer from device to host .",
    "this size of these data does not depend on the size of the problem .",
    "keeping in mind the goal of minimizing the communication between host and device , we make the following arrangements . transferring the polynomial structures , the coefficients , the start solution from the host to the device occurs only once at the start of the path tracking . in newton",
    "s method , @xmath57 and @xmath58 are used to control the newton iteration , sent from device to host .",
    "the host uses these values to check whether the corrector converged or diverged and to determine whether to launch a new corrector kernel .",
    "the step size @xmath21 is controlled by the host .",
    "after each predictor - corrector stage , the host updates @xmath1 by @xmath21 , and sends this update to the device .",
    "then the predictor on the device uses the updated values to predict the next point on the path . when @xmath1 gets to @xmath59 , the final solution @xmath20 is sent from the device to the host .",
    "the computation of the norm of the value of the solution at the system and the magnitude of the update to the solution can happen in ordinary double arithmetic as we are only concerned in the magnitude of the norms .",
    "with communications of these 3 double variables between host and device , then host can control the device to launch the kernels that run the path tracker .",
    "pseudocode for the path tracker is sketched in figure  [ figparalgpath ] .",
    "the gpu_newton call refers to the algorithm in figure  [ figparalgnewton ] .",
    "the decision to execute the step control at the host is motivated by its low computational cost and the desire to monitor the quality of the computations as the tracker advances along a solution path .",
    "although the latest versions of cuda allow for separate kernel launches initiated at the device , the fine granularity of our algorithms allows for the host to monitor the progress of the newton steps .",
    "= input : = @xmath41 , polynomial instructions + @xmath42 , gpu workspace + @xmath43 , parameters for path tracker + output : = @xmath44 or @xmath45 + @xmath46 , solution for @xmath60 if @xmath44 + @xmath61 + @xmath62 + @xmath63 + @xmath64 + while @xmath7 do + if ( @xmath65 ) + return @xmath45 + @xmath66 ) + copy @xmath1 from host to gpu + launch kernel predict(@xmath67 , @xmath68 ) + @xmath69 = gpu_newton(@xmath41,@xmath42,@xmath43 ) + if ( = @xmath69 ) + update pointer of @xmath46 in @xmath67 + @xmath70 = @xmath70 + 1 + if (= @xmath71 ) + @xmath72 + else + @xmath63 + @xmath73 + @xmath74 + return @xmath44      we can evaluate and differentiate a single product of variables ( the so - called example of speelpenning ) in two ways : in the reverse mode or with an arithmetic circuit , organized in a binary tree ( as we described in  @xcite ) .",
    "the latter circuit can use several threads or even occupy an entire block to evaluate a single monomial , whereas the straightforward application of the reverse mode has to be executed by a single thread .",
    "if @xmath0 denotes the number of variables in a product , then the reverse mode writes about @xmath0 values to global memory to store intermediate results , whereas the reorganization in a binary tree keeps intermediate results in shared memory .",
    "the drawback however is that the binary tree structure of the algorithm limits the number of threads that can be used at each level of the tree .",
    "the organization in a binary tree is better for memory bound computations , which is the case when we work in double precision .",
    "but for compute bound computations , as is the case with double double and quad double arithmetic , the reverse mode is better . to balance the work loads for each block , we sort the products of the variables according to the number of variables that appear in the product , so all threads in a single block work on products of similar size . for improved access to global memory ,",
    "the data structures are designed to allow for coalesced reading and writing .",
    "the instructions to evaluate a product include the number of variables that occur in the product and the positions to indicate where each variable occurs in the product . aligning numbers of variables and indices to positions",
    ", all threads are reading from global memory from consecutive locations at the same step .",
    "access to global memory for intermediate and final results is handled in the same way .",
    "table  [ tabevaldiffprod ] illustrates the evaluation and differentiation of a product of variables .",
    "figure  [ figmonspeedup ] illustrates the comparison of the original reverse mode , reverse mode with aligned memory and the tree mode in complex double precision .",
    "the polynomial system in this experiment is defined in  ( [ eqcyclicsys ] ) .",
    "after alignment , the reverse mode improves , especially for larger dimension systems . the tree mode still works better for complex double precision .",
    "but for larger monomials , due to the limits of threads in the tree mode , reverse mode with aligned memory almost catches up .",
    "this new reverse mode also works better for double double and quad double arithmetic .",
    "| c | c | c | c | c | tidx & 0 & 1 & 2 & 3 + @xmath75 & @xmath76 & @xmath77 & @xmath78 & @xmath79 + & & & & + & @xmath80 & @xmath81 & @xmath82 & @xmath83 + & @xmath84 & @xmath85 & @xmath86 & @xmath87 + & & & @xmath88 & @xmath89 + & & & & @xmath90 +     | c | c | c | c | c | tidx & 0 & 1 & 2 & 3 + @xmath75 & @xmath76 & @xmath77 & @xmath78 & @xmath79 + & @xmath87 & @xmath91 & @xmath91 & @xmath92 + & @xmath93 & @xmath94 & @xmath95 & @xmath96 + & @xmath97 & @xmath98 & @xmath99 & @xmath100 + & & & @xmath101 & @xmath102 + & & & & @xmath103 +      we improved the efficiency of our summation routines over those in  @xcite .",
    "we reorganized the summation algorithm so many threads work on one sum .",
    "this led to an increased amount of parallelism and an improved coalesced access to the main memory of the device .",
    "even already in double precision arithmetic , double digits speedups of the accelerated code were obtained .",
    "tables  [ tabsumcyc128 ] and  [ tabsumcyc352 ] illustrate that the best speedups are obtained if four threads are collaborating to perform the summation . working with a finer granularity increase the parallelism , but also leads to a larger number of blocks .",
    "observe that on the gpu all times in the quad double column are all less than the time on the cpu .",
    "with acceleration , we can quadruple the precision and still be about twice faster than without acceleration . the polynomial system in this experiment",
    "is defined in  ( [ eqcyclicsys ] ) .",
    ".summation times in milliseconds for the pieri problem of dimension  103 with an increasing number of threads # th per monomial on the gpu , for complex arithmetic in double ( d ) , double double ( dd ) , and quad double precision ( qd ) . with speedup ( s )",
    "relative to one cpu core .",
    "[ cols=\">,>,>,>,>,>,>,>\",options=\"header \" , ]     the data in table  [ tabruncyclic ] for complex double and complex double double precision is visualized in figure  [ figcyclicspeedups ] .",
    "concerning the data in table  [ tabruncyclic ] , let us compare the accelerated times in double double precision to the times on one cpu core in double precision . for the last line , observe that it takes 93.89 seconds to track one path in double precision without acceleration .",
    "with acceleration tracking one path in double double precision takes 28.75 seconds , so we can double the precision and still be three times faster than in double precision without acceleration .",
    "speedups computed in table  [ tabruncyclic ] are shown in figure  [ figcyclicspeedups ] .",
    "we see that in double double precision , the speedups rise faster as the dimension increases than in double precision .",
    "this paper provides a description for an accelerated path tracker for polynomial homotopies . on two classes of benchmark problems",
    "we illustrate that for sufficiently high dimensions with acceleration we can compensate for the extra cost of high precision arithmetic .",
    "this material is based upon work supported by the national science foundation under grant no .",
    "the microway workstation with the nvidia tesla k20c was purchased through a uic las science award .",
    "we thank the reviewers for their comments and suggestions which improved the quality of this paper .",
    "d.  adrovic and j.  verschelde .",
    "computing puiseux series for algebraic surfaces . in j.",
    "van  der hoeven and m.  van hoeij , editors , _ proceedings of the 37th international symposium on symbolic and algebraic computation ( issac 2012 ) _ , pages 2027 .",
    "acm , 2012 .",
    "d.  adrovic and j.  verschelde .",
    "polyhedral methods for space curves exploiting symmetry applied to the cyclic @xmath0-roots problem . in v.p .",
    "gerdt , w.  koepf , e.w .",
    "mayr , and e.v .",
    "vorozhtsov , editors , _ proceedings of casc 2013 _ , pages 1029 , 2013 .",
    "t.  bartkewitz and t.  gneysu . full lattice basis reduction on graphics cards . in f.",
    "armknecht and s.  lucks , editors , _",
    "weworc11 proceedings of the 4th western european conference on research in cryptology _ , volume 7242 of _ lecture notes in computer science _ , pages 3044 .",
    "springer - verlag , 2012 .        c.  bischof , n.  guertler , a.  kowartz , and a.  walther .",
    "parallel reverse mode automatic differentiation for openmp programs with adol - c . in c.  bischof , h.m .",
    "bcker , p.  hovland , u.  naumann , and j.  utke , editors , _ advances in automatic differentiation _ , pages 163173 .",
    "springer - verlag , 2008 .",
    "n.  bliss , j.  sommars , j.  verschelde , and x.  yu . solving polynomial systems in the cloud with polynomial homotopy continuation .",
    ", accepted for publication in the _ proceedings of the 17th workshop on computer algebra in scientific computing ( casc 2015)_.      t.  chen , t .- l . lee , and t .- y .",
    "li . : a parallel numerical solver for systems of polynomial equations based on polyhedral homotopy continuation methods . in h.",
    "hong and c.  yap , editors , _ mathematical software ",
    "icms 2014 , 4th international conference , seoul , south korea , august 4 - 9 , 2014 , proceedings _ ,",
    "volume 8592 of _ lecture notes in computer science _ , pages 183190 .",
    "springer - verlag , 2014 .",
    "j.  c. faugre .",
    "finding all the solutions of cyclic 9 using grbner basis techniques . in _",
    "computer mathematics - proceedings of the fifth asian symposium ( ascm 2001 ) _ , volume  9 of _ lecture notes series on computing _ , pages 112 .",
    "world scientific , 2001 .",
    "m.  grabner , t.  pock , t.  gross , and b.  kainz .",
    "automatic differentiation for gpu - accelerated 2d/3d registration . in c.  bischof , h.m .",
    "bcker , p.  hovland , u.  naumann , and j.  utke , editors , _ advances in automatic differentiation _ , pages 259269 .",
    "springer - verlag , 2008 .",
    "s.  a. haque , x.  li , f.  mansouri , m.  m. maza , w.  pan , and n.  xie .",
    "dense arithmetic over finite fields with the cumodp library .",
    "in h.  hong and c.  yap , editors , _ mathematical software ",
    "icms 2014 _ , volume 8592 of _ lecture notes in computer science _ , pages 725732 .",
    "springer - verlag , 2014 .",
    "y.  hida , x.  s. li , and d.  h. bailey .",
    "algorithms for quad - double precision floating point arithmetic . in _",
    "15th ieee symposium on computer arithmetic ( arith-15 2001 ) , 11 - 17 june 2001 , vail , co , usa _ , pages 155162 .",
    "ieee computer society , 2001 . shortened version of technical report lbnl-46996 , software at http://crd.lbl.gov/@xmath104dhbailey/ mpdist .",
    ". numerical solution of polynomial systems by homotopy continuation methods . in f.",
    "cucker , editor , _ handbook of numerical analysis .",
    "volume xi .",
    "special volume : foundations of computational mathematics _ , pages 209304 .",
    "north - holland , 2003 .",
    "m.  lu , b.  he , and q.  luo . supporting extended precision on graphics processors . in a.",
    "ailamaki and p.a .",
    "boncz , editors , _ proceedings of the sixth international workshop on data management on new hardware ( damon 2010 ) , june 7 , 2010 , indianapolis , indiana _ , pages 1926 , 2010 .",
    "software at http://code.google.com / p / gpuprec/.          a.  j. sommese , j.  verschelde , and c.  w. wampler .",
    "numerical irreducible decomposition using phcpack . in m.",
    "joswig and n.  takayama , editors , _ algebra , geometry , and software systems _ ,",
    "pages 109130 .",
    "springer - verlag , 2003 .",
    "sommese , j.  verschelde , and c.w .",
    "introduction to numerical algebraic geometry . in _ solving polynomial equations .",
    "foundations , algorithms and applications _ , volume  14 of _ algorithms and computation in mathematics _",
    ", pages 301337 .",
    "springer - verlag , 2005 .",
    "j.  verschelde and g.  yoffe .",
    "polynomial homotopies on multicore workstations . in m.m .",
    "maza and j .- l .",
    "roch , editors , _ proceedings of the 4th international workshop on parallel symbolic computation ( pasco 2010 ) , july 21 - 23 2010 , grenoble , france _ , pages 131140 .",
    "acm , 2010 .",
    "j.  verschelde and g.  yoffe . evaluating polynomials in several variables and their derivatives on a gpu computing processor . in",
    "_ proceedings of the 2012 ieee 26th international parallel and distributed processing symposium workshops ( pdsec 2012 ) _ , pages 13911399 .",
    "ieee computer society , 2012 .",
    "j.  verschelde and g.  yoffe .",
    "orthogonalization on a general purpose graphics processing unit with double double and quad double arithmetic . in _ proceedings of the 2013 ieee 27th international parallel and distributed processing symposium workshops ( pdsec 2013 ) _ , pages 13731380 .",
    "ieee computer society , 2013 .",
    "j.  verschelde and x.  yu .",
    "tracking many solution paths of a polynomial homotopy on a graphics processing unit .",
    ", accepted for publication in the _ proceedings of the 17th ieee international conference on high performance computing and communications ( hpcc 2015)_.    j.  verschelde and x.  yu . acceleration of newton s method for large systems of polynomial equations in double double and quad double arithmetic . in",
    "_ proceedings of the 16th ieee international conference on high performance computing and communication ( hpcc 2014 ) _ , pages 161164 .",
    "ieee computer society , 2014 ."
  ],
  "abstract_text": [
    "<S> numerical continuation methods track a solution path defined by a homotopy . </S>",
    "<S> the systems we consider are defined by polynomials in several variables with complex coefficients . for larger dimensions and degrees , the numerical conditioning worsens and hardware </S>",
    "<S> double precision becomes often insufficient to reach the end of the solution path . with double double and quad double arithmetic </S>",
    "<S> , we can solve larger problems that we could not solve with hardware double arithmetic , but at a higher computational cost . </S>",
    "<S> this cost overhead can be compensated by acceleration on a graphics processing unit ( gpu ) . </S>",
    "<S> we describe our implementation and report on computational results on benchmark polynomial systems </S>",
    "<S> .    * keywords . * double double arithmetic , general purpose graphics processing unit ( gpu ) , massively parallel algorithm , path tracking , predictor - corrector , polynomial homotopy continuation , polynomial system , quad double arithmetic . </S>"
  ]
}