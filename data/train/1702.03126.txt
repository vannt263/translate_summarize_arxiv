{
  "article_text": [
    "statistical inference is of fundamental importance to science .",
    "inference enables the testing of theoretical models against observations , and provides a rational means of quantifying uncertainty in existing models .",
    "modern approaches to statistical inference , based on monte carlo sampling techniques , provide insight into many complex phenomena  @xcite .    inference can be described as follows : suppose we have a set of observations , @xmath0 ; a method of determining the likelihood of these observations , @xmath1 , under the assumption of some model characterized by parameter vector , @xmath2 ; and a prior probability density , @xmath3 .",
    "the posterior probability density , @xmath4 , can be computed using bayes theorem , @xmath5 explicit expressions for likelihood functions are rarely available  @xcite ; motivating the development of likelihood - free methods , such as approximate bayesian computation ( abc )  @xcite .",
    "abc methods approximate the likelihood through evaluating the discrepancy between data generated by a simulation of the model of interest and the observations , yielding an approximate form of bayes theorem , @xmath6 here , @xmath7 is data generated by the model simulation process , @xmath8 , @xmath9 is a discrepancy metric , and @xmath10 is the acceptance threshold . due to this approximation ,",
    "monte carlo estimators based on equation  ( [ eqn : approxbayes ] ) are biased  @xcite .",
    "the most simple implementation of abc is abc rejection  @xcite , see algorithm  [ alg : abc - rej ] .",
    "[ line : sample1]sample prior , @xmath11 .",
    "generate data , @xmath12 .",
    "set @xmath13 .",
    "[ alg : abc - rej ]    this method generates @xmath14 samples @xmath15 by accepting proposals , @xmath11 , when the data generated by the model simulation process @xmath16 is within @xmath17 of the observed data , @xmath0 .",
    "while abc rejection is simple to implement , it can be computationally prohibitive in practice  @xcite .    to improve the efficiency of abc",
    ", one can consider a likelihood - free modification of markov chain monte carlo ( mcmc )  @xcite in which a markov chain is constructed with a stationary distribution identical to the desired posterior .",
    "given the markov chain is in state @xmath18 , a state transition is proposed via a proposal kernel , @xmath19 .    the classical metropolis - hastings  @xcite state transition probability , @xmath20 , can be modified within an abc framework to yield @xmath21 if @xmath22 , or @xmath23 otherwise  @xcite .",
    "the stationary distribution of such a markov chain is the desired approximate posterior , @xmath24  @xcite .",
    "algorithm  [ alg : mcmc - abc ] provides a method for computing @xmath25 iterations of this markov chain .",
    "sample transition kernel , @xmath26 .",
    "generate data , @xmath27 .",
    "set @xmath28 . sample uniform distribution , @xmath29 .",
    "set @xmath13 .",
    "set @xmath30 .",
    "set @xmath30 .",
    "[ alg : mcmc - abc ]    while mcmc - abc sampling can be highly efficient  @xcite , the samples in the sequence , @xmath31 , are not independent .",
    "this can be problematic as it is possible for the markov chain to take long excursions into regions of low posterior probability ; thus incurring additional , and potentially significant , bias  @xcite .",
    "a poor choice in the proposal kernel can also have considerable impact upon the efficiency of mcmc - abc  @xcite .",
    "the question of how to choose the proposal kernel is non - trivial , and typically determined heuristically .",
    "sequential monte carlo ( smc ) sampling was introduced to address these potential inefficiencies  @xcite and later extended within an abc context  @xcite .",
    "a set of samples , referred to as particles , is evolved through a sequence of abc posteriors defined through a sequence of @xmath32 acceptance thresholds , @xmath33  @xcite . at each step ,",
    "$ ] , the current abc posterior , @xmath35 , is approximated by a discrete distribution constructed from a set of @xmath36 particles @xmath37 with importance weights , @xmath38 , that is , @xmath39 for @xmath40 .",
    "the collection is updated to represent the abc posterior of the next time step , @xmath41 , through application of rejection sampling on particles perturbed by a proposal kernel , @xmath42 .",
    "if @xmath35 is similar to @xmath41 , the acceptance rate should be high .",
    "the importance weights of the new family of particles are updated using an optimal backwards kernel  @xcite .",
    "algorithm  [ alg : abc - smc ] outlines the process .",
    "initialize @xmath43 and @xmath44 , for @xmath45 .",
    "set @xmath46 with probability @xmath47 .",
    "sample transition kernel , @xmath48 .",
    "generate data , @xmath49 .",
    "set @xmath50 .",
    "set @xmath51 .",
    "normalize weights so that @xmath52 .",
    "[ alg : abc - smc ]    through the use of independent weighted particles , smc - abc avoids long excursions into the distribution tails that are possible with mcmc - abc .",
    "however , the efficiency of each step still depends on the non - trivial problem of selecting a proposal kernel  @xcite .",
    "multilevel monte carlo ( mlmc ) is a recent development that can significantly reduce the computational burden in the estimation of expectations  @xcite . to demonstrate the basic idea of mlmc ,",
    "consider computing the expectation of a continuous - time stochastic process @xmath53 at time @xmath32 .",
    "let @xmath54 denote a discrete - time approximation to @xmath53 with time step @xmath55 : the expectations of @xmath56 and @xmath57 are related according to @xmath58}~=~{\\mathbb{e}\\left[z_t^\\tau\\right]}~+~{\\mathbb{e}\\left[x_t - z_t^\\tau\\right]}$ ] .",
    "that is , an estimate of @xmath59}$ ] can be treated as a biased estimate of @xmath60}$ ] . by taking a sequence of time steps @xmath61 ,",
    "the indices of which are referred to as _ levels _ , we can arrive at a telescoping sum , @xmath62 } = { \\mathbb{e}\\left[z_t^{\\tau_1}\\right ] } + \\sum_{\\ell = 2}^l { \\mathbb{e}\\left[z_t^{\\tau_\\ell } - z_t^{\\tau_{\\ell-1}}\\right]}.\\ ] ] while computing this form of the expectation returns the same bias as when computing @xmath63}$ ] directly , giles  @xcite demonstrates that a monte carlo estimator for the telescoping sum can be computed more efficiently than directly estimating @xmath63}$ ] in the context of stochastic differential equations ( sdes ) .",
    "this efficiency comes from exploiting the fact that the bias correction terms , @xmath64}$ ] , only measure the expected difference between levels @xmath65 and @xmath66 , hence samples paths need not be independent . in the case of sdes , samples may be generated in pairs driven by the same underlying brownian motion , this introduces a strong coupling that reduces the variance of the monte carlo estimator .",
    "recently , the concept of mlmc  @xcite has been extended to statistical applications with known likelihoods  @xcite .",
    "our work considers mlmc in the context of bayesian inference with intractable likelihoods .",
    "we describe a new algorithm for likelihood - free inference , based on mlmc , that is as general as mcmc - abc and smc - abc . here",
    "we describe the algorithm , its implementation , and we compare the performance of the new method against mcmc - abc and smc - abc for a standard benchmark problem .",
    "the key attraction of our method is that its performance does not depend on any user - specified proposal kernel .",
    "we show that the new method significantly outperforms mcmc - abc and smc - abc in the most practical scenario where parameter proposal kernels are independent .",
    "this is important because choosing an optimal proposal density is non - trivial .",
    "therefore , developing new methods that avoid the need for specifying a proposal density kernel can lead to significant computational savings for practical applications .",
    "in this section , we demonstrate our application of mlmc ideas to the likelihood - free inference problem given in equation  ( [ eqn : approxbayes ] ) .",
    "the aim is to compute an accurate approximation to the joint posterior cumulative distribution function ( cdf ) of @xmath67 using as few data generation steps as possible .",
    "we define a data generation step to be a simulation of the model of interest given a proposed parameter vector .",
    "we apply mlmc methods to likelihood - free bayesian inference by reposing the problem of computing the posterior cdf as an expectation calculation .",
    "this allows the mlmc telescoping sum idea , as in equation  ( [ eqn : mlmc ] ) , to be applied . in this context",
    ", the levels of the mlmc estimator are parameterized by a sequence of @xmath68 acceptance thresholds @xmath69 with @xmath70 for all @xmath71 .",
    "the efficiency of mlmc relies upon computing the terms of the telescoping sum with low variance .",
    "variance reduction is achieved through exploiting features of the telescoping sum for cdf approximation , and further computational gains are achieved by using properties of nested abc rejection samplers .",
    "we first reformulate the bayesian inference problem ( equation  ( [ eqn : bayes ] ) ) as an expectation . to this end , note that , given a @xmath72-dimensional parameter space , @xmath73 , and the parameter random vector , @xmath67 , the joint posterior cdf , @xmath74 , is defined as @xmath75 .",
    "if @xmath74 is differentiable , that is , its probability density function ( pdf ) exists , then @xmath76 where @xmath77 .",
    "this can be expressed as an expectation by noting @xmath78},\\end{aligned}\\ ] ] where @xmath79 is the indicator function : @xmath80 if @xmath81 and @xmath82 otherwise .",
    "now , consider the abc approximation given in equation  ( [ eqn : approxbayes ] ) with discrepancy metric @xmath83 and acceptance threshold @xmath17 .",
    "the abc posterior cdf , denoted by @xmath84 , will be @xmath85}.\\end{aligned}\\ ] ] the marginal abc posterior cdfs , @xmath86 , are @xmath87 .",
    "we now introduce some notation that will simplify further derivations .",
    "we define @xmath88 to be a random vector distributed according to the abc posterior cdf @xmath84 with acceptance threshold @xmath17 as given in equation  ( [ eqn : cdf ] ) .",
    "this provides us with simplification in notation for the abc posterior pdf , @xmath89 , and the conditional expectation , @xmath90 } = { \\mathbb{e}\\left[{\\mathds{1}_{a_{\\mathbf{s}}}({\\boldsymbol{\\theta } } ) } \\mid { d ( { \\mathcal{d } } , { \\mathcal{d}_s } ) } < \\epsilon\\right]}$ ] .",
    "we also use @xmath91 to denote the monte carlo estimator of the expectation , @xmath92 , obtained using @xmath14 samples .",
    "the standard monte carlo integration approach is to generate @xmath14 samples @xmath93 from the abc posterior , @xmath94 , then evaluate the empirical cdf ( ecdf ) , @xmath95 for @xmath96 , where @xmath97 a discretization of the parameter space  @xmath73 . for simplicity , we will consider @xmath97 to be a @xmath72-dimensional regular lattice .",
    "the ecdf is not , however , the only monte carlo approximation to the cdf one may consider . in particular , giles et al .",
    "@xcite demonstrate the application of mlmc to a univariate cdf approximation .",
    "we now present a multivariate equivalent of the mlmc cdf of giles et al .",
    "@xcite in the context of abc posterior cdf estimation . given a strictly decreasing sequence of @xmath68 acceptance thresholds , @xmath98",
    ", we can represent the cdf ( equation  ( [ eqn : cdf ] ) ) using the telescoping sum @xmath99 } = \\sum_{\\ell=1}^{l } y_\\ell({\\mathbf{s}}),\\ ] ] where @xmath100 } & \\text{if } \\ell = 1 , \\\\ { \\mathbb{e}\\left[{\\mathds{1}_{a_{\\mathbf{s}}}({\\boldsymbol{\\theta}}_{\\epsilon_\\ell } ) } - { \\mathds{1}_{a_{\\mathbf{s}}}({\\boldsymbol{\\theta}}_{\\epsilon_{\\ell -1}})}\\right ] } & \\text{if } \\ell > 1 .",
    "\\end{cases}\\ ] ] using our notation , the mlmc estimator for equation  ( [ eqn : mlmccdf ] ) and equation  ( [ eqn : mlmccdfterms ] ) is given by @xmath101 where @xmath102 & \\text{if } \\ell > 1 , \\end{cases}\\ ] ] and @xmath103 is a lipschitz continuous approximation to the indicator function ; this approximation is computed using a tensor product of cubic splines .",
    "such a smoothing is necessary to avoid convergence issues with mlmc caused by the discontinuity of the indicator function  @xcite .    to compute the @xmath104 term ( equation  ( [ eqn : bmcest ] ) )",
    ", we generate @xmath105 samples @xmath106 from @xmath107 ; this represents a biased estimate for @xmath108 . to compensate for this bias ,",
    "correction terms ( equation  ( [ eqn : bmcest ] ) ) , @xmath109 , are evaluated for @xmath110 , each requiring the generation of @xmath111 samples @xmath112 from @xmath113 and @xmath111 samples @xmath114 from @xmath115 .",
    "the goal is to introduce a coupling between levels that controls the variance of the bias correction terms . with an effective coupling ,",
    "the result is an estimator with lower variance , hence the number of samples required to obtain an accurate estimate is reduced .",
    "denote @xmath116 as the variance of the estimator @xmath117 . for @xmath118",
    "this can be expressed as @xmath119 } \\\\",
    "= & \\ , { \\text{var}\\left[g_{\\mathbf{s}}({\\boldsymbol{\\theta}}_{\\epsilon_\\ell})\\right ] } + { \\text{var}\\left[g_{\\mathbf{s}}({\\boldsymbol{\\theta}}_{\\epsilon_{\\ell-1}})\\right ] } - { \\text{cov}\\left[g_{\\mathbf{s}}({\\boldsymbol{\\theta}}_{\\epsilon_\\ell}),g_{\\mathbf{s}}({\\boldsymbol{\\theta}}_{\\epsilon_{\\ell-1}})\\right]}.\\end{aligned}\\ ] ] introducing a positive correlation between the random variables @xmath120 and @xmath121 will have the desired effect of reducing the variance of @xmath109 .    in many applications of mlmc , a positive correlation is introduced through driving samplers at both the @xmath65 and @xmath122 level with the same _ randomness_. properties of brownian motion or poisson processes are typically used for the estimation of expectations involving sdes or markov processes  @xcite . in the context of abc methods , however , simulation of the quantity of interest is necessarily based on rejection sampling .",
    "the reliance on rejection sampling makes a strong coupling , in the true sense of mlmc , a difficult , if not impossible task .",
    "rather , here we introduce a weaker form of coupling through exploiting the fact that our mlmc estimator is performing the task of computing an estimate of the abc posterior cdf .",
    "we combine this with a property of nested abc rejection samplers to arrive at an efficient algorithm for computing @xmath123 .",
    "we proceed to establish a correlation between levels as follows .",
    "assume we have computed , for some @xmath124 , the terms @xmath125 in equation  ( [ eqn : mlmcest ] ) .",
    "that is , we have an estimator to the cdf at level @xmath65 by taking the sum @xmath126 with marginal distributions @xmath127 for @xmath128 .",
    "we can use this to determine a coupling based on matching marginal probabilities when computing @xmath129 . after generating @xmath130 samples @xmath131 from @xmath132 , we compute the ecdf @xmath133 using equation  ( [ eqn : ecdf ] ) and obtain the marginal distributions @xmath134 for @xmath135 .",
    "we can thus generate @xmath130 coupled pairs @xmath136 by choosing the @xmath137 with the same marginal probabilities as the empirical probability of @xmath138 .",
    "that is , the @xmath139th component of @xmath137 is given by @xmath140 , where @xmath141 is the @xmath139th component of @xmath138 and @xmath142 is the inverse of @xmath139th marginal distribution of @xmath143 .",
    "this introduces a positive correlation between the sample pairs , @xmath144 , since an increase in any of the components of @xmath145 will cause an increase in the same component @xmath146 .",
    "this correlation reduces the variance in the bias correction estimator @xmath147 computed according to equation  ( [ eqn : bmcest ] ) .",
    "we can then update the mlmc cdf using @xmath148 and apply an adjustment that ensures monotonicity .",
    "we continue this process iteratively to obtain @xmath149 .",
    "initialize @xmath150 , @xmath151 and prior @xmath3 .",
    "sample @xmath152 restricted to @xmath153 .",
    "generate data , @xmath12 .",
    "set @xmath154 . set @xmath155 .",
    "set @xmath156 . set @xmath157/n_\\ell$ ] .",
    "set @xmath158 .",
    "[ alg : mlmc - abc ]    by computing the bias correction terms in an iterative fashion , we can exploit a further computational gain .",
    "let @xmath159 denote the support of a function @xmath160 , and note that , for any @xmath161 $ ] , @xmath162 .",
    "this follows from the fact that if @xmath163 , @xmath164 since @xmath165 .",
    "therefore , we can truncate the prior to the support of @xmath115 when computing @xmath109 , thus increasing the acceptance rate of level @xmath65 samples . in practice , we approximate the support by the smallest bounding box that contains all generated samples .",
    "we now require the sample numbers @xmath166 that are the optimal trade - off between accuracy and efficiency .",
    "denote @xmath167 as the number of data generation steps required during the computation of @xmath109 and let @xmath168 be the average number of data generation steps per accepted abc posterior sample using acceptance threshold @xmath169 .",
    "given @xmath170}$ ] and @xmath171}$ ] , for @xmath172 , one can construct the optimal @xmath111 using a lagrange multiplier method under the constraint @xmath173}~=~\\mathcal{o}(h^2)$ ] , where @xmath174 is the target variance of the mlmc cdf estimator . with details left to appendix  [ app : optn ] ,",
    "the optimal @xmath175 are given by @xmath176 in practice , the values for @xmath177 and @xmath178 will not have analytic expressions available ; rather , we perform a low accuracy trial simulation with all @xmath179 , for some comparatively small constant , @xmath180 , to obtain the relative scaling of variances and data generation requirements .",
    "once @xmath175 have been estimated , the computation of the mlmc - abc posterior cdf @xmath123 proceeds according to algorithm  [ alg : mlmc - abc ] .",
    "we evaluate the performance of mlmc - abc using the model developed by tanaka et al .",
    "@xcite in the study of tuberculosis transmission rate parameters using dna fingerprint data  @xcite .",
    "this model has been selected due to the availability of published comparative performance evaluations of mcmc - abc and smc - abc  @xcite .    the model proposed by tanaka et al .",
    "@xcite describes the occurrence of tuberculosis infections over time and the mutation of the bacterium responsible , _",
    "myobacterium tuberculosis_. the number of infections , @xmath181 , at time @xmath182 is @xmath183 , where @xmath184 is the number of distinct genotypes and @xmath185 is the number of infections caused by the @xmath186th genotype at time @xmath182 . for each genotype",
    ", new infections occur with rate @xmath187 , infections terminate with rate @xmath188 , and mutation occurs with rate @xmath189 ; causing an increase in the number of genotypes .",
    "this process can be described by a discrete - state continuous - time markov process and simulated using the gillespie algorithm  @xcite . after a realization of the model is completed , either by extinction or when a maximum infection count is reached , a sub - sample of @xmath190 cases is collected and compared against the is@xmath191 dna fingerprint data of tuberculosis bacteria samples  @xcite .",
    "the dataset consists of @xmath192 distinct genotypes ; the infection cases are clustered according to the genotype responsible for the infection .",
    "the collection of clusters can summarized succinctly as @xmath193 where @xmath194 states there are @xmath139 clusters of size @xmath195 .",
    "the discrepancy metric used is @xmath196 where @xmath195 is the size of the population sub - sample ( e.g. , @xmath197 ) , @xmath198 denotes the number of distinct genotypes in the dataset ( e.g. , @xmath199 ) and the genetic diversity is @xmath200/n^2 $ ] , where @xmath201 is the cluster size of the @xmath186th genotype in the dataset .",
    "we perform likelihood - free inference on the tuberculosis model for the parameters @xmath202 with the goal of evaluating the efficiency of mlmc - abc , mcmc - abc and smc - abc .",
    "we use a target posterior distribution of @xmath24 with @xmath83 as defined in equation  ( [ eqn : tb_disc ] ) and @xmath203 .",
    "the improper prior is @xmath204 , @xmath205 and @xmath206  @xcite . for the mcmc - abc and smc - abc algorithms",
    "we apply a typical gaussian proposal kernel , @xmath207 , with covariance matrix @xmath208 such a proposal kernel is reasonable to characterize the initial explorations of an abc posterior as no correlations between parameters are assumed .    , ( b ) @xmath188 and ( c ) @xmath189for the tuberculosis transmission stochastic model .",
    "estimate computed using using mlmc - abc with @xmath209 ( yellow lines ) , mcmc - abc over @xmath210 iterations ( blue lines ) , smc - abc with @xmath211 particles ( red lines ) and high precision solution ( dashed lines ) . ]",
    ".comparison of mlmc - abc against mcmc - abc and smc - abc using a naive proposal kernel . [ cols=\"^,^,^,^,^,^,^,^,^ \" , ]",
    "our results indicate that while smc - abc and mcmc - abc can be heuristically optimized to be highly efficient , an accurate estimation of the parameter posterior can be obtained using our mlmc - abc in a reasonably automatic fashion .",
    "furthermore , the efficiency of mlmc - abc is comparable or improved over mcmc - abc and smc - abc , even in the case of good proposal densities that have been heuristically determined .",
    "mlmc - abc shares some concepts with smc - abc .",
    "for example , both methods consider a sequence of abc posteriors .",
    "furthermore , both use information gained at each step to improve the acceptance rates of the next step while maintaining independence in the set of samples .",
    "mlmc - abc and smc - abc are still , however , distinct methods .",
    "for example , in smc - abc the prior distribution is only sampled at the initial step , whereas in mlmc - abc the prior must be sampled at all levels to ensure the telescoping sum does not introduce bias .",
    "another distinct difference is that mlmc does not evolve particles through the abc posterior sequence , rather each abc posterior is sampled independently to compute the bias correction terms .",
    "the need to estimate the variances of each bias correction term could be considered a limitation of the mlmc - abc approach .",
    "however , we find in practice that these need not be computed to high accuracy , since the relative variance between the levels is the main requirement and this can often be estimated with a relatively small number of samples . there could be examples of bayesian inference problems where mlmc - abc is inefficient on account of this fact , in which case the variance estimation would have to be considered as a more heuristic process .",
    "we have so far , however , failed to find an example for which @xmath212 samples of each bias correction term is insufficient to obtain a good mlmc - abc estimator .",
    "there are many modifications one could consider to further improve mlmc - abc .",
    "currently , we depend on the sequence of acceptance thresholds to be specified in advance and we acknowledge that , in practice , many smc - abc implementations determine these adaptively ; modification of mlmc - abc to allow for adaptive acceptance thresholds would make mlmc - abc even more practical .",
    "other improvements could focus on the discretization used for the ecdf calculations , for example , removing the requirement of a regular lattice would enable mlmc - abc to scale to much higher dimensional parameter spaces .",
    "coupling strategies are also possible improvement areas , our current coupling depends only on the computation of the full posterior cdf and assumes nothing about the underlying model ; can model specific features improve the coupling and obtain further variance reduction ?",
    "the rejection sampling method certainly makes this difficult , but there may still be opportunities for improvement .",
    "we have shown , in a practical way , how mlmc techniques can be applied to abc inference , and demonstrated that superior performance over modern advanced abc methods can be achieved .",
    "furthermore , the performance of our mlmc - abc method is not dependent on user specified functions such as proposal kernels .",
    "this work is supported by the australian research council ( ft130100148 ) .",
    "computational resources were provided by the high performance computing and research support group , qut .",
    "in this appendix , we demonstrate how the sequence of sample numbers @xmath166 is obtained , as stated in the main text .",
    "first , we assume the variances @xmath213}$ ] and @xmath214}$ ] , for @xmath215 , are known quantities .",
    "we also assume that , on average , @xmath216 data generation steps are required to compute the estimator @xmath217 .",
    "the sequence @xmath166 is considered optimal if @xmath218 is minimised subject to the constraint @xmath222 for some constant @xmath223 and target monte carlo error @xmath174 . to determine optimal @xmath175 , we consider the lagrangian @xmath224 and note that solutions to the optimisation problem exist at @xmath225 .",
    "that is , @xmath226{\\mathbf{e}}_\\ell + \\left[e(n_1,\\ldots , n_l ) - kh^2\\right]{\\mathbf{e}}_{l+1 } & = { \\mathbf{0}},\\end{aligned}\\ ] ] where @xmath227 are the standard orthonormal basis vectors of @xmath228-dimensional euclidean space .",
    "we obtain the following system of equations , @xmath229    first we consider the forms of @xmath218 and @xmath221 . by definition of the mlmc estimator given in main text",
    ", we have @xmath230 } = { \\text{var}\\left[\\sum_{\\ell=1}^{l}\\hat{y}^{n_\\ell}_\\ell({\\mathbf{s}})\\right]}. \\end{aligned}\\ ] ] furthermore , since @xmath231 are independent , @xmath232 } = & \\sum_{\\ell=1}^{l}{\\text{var}\\left[\\hat{y}^{n_\\ell}_\\ell({\\mathbf{s}})\\right ] } \\\\ = & { \\text{var}\\left[\\frac{1}{n_1}\\sum_{i=1}^{n_1}g_{\\mathbf{s}}({\\boldsymbol{\\theta}}_{\\epsilon_1}^{i})\\right ] } + \\sum_{\\ell=2}^{l } { \\text{var}\\left[\\frac{1}{n_\\ell}\\sum_{i=1}^{n_\\ell } g_{\\mathbf{s}}({\\boldsymbol{\\theta}}_{\\epsilon_\\ell}^{i } ) -g_{\\mathbf{s}}({\\boldsymbol{\\theta}}_{\\epsilon_{\\ell-1}}^{i})\\right ] } \\\\ = & \\sum_{\\ell=1}^{l } \\frac{1}{n_\\ell^2 } \\sum_{i=1}^{n_\\ell}v_\\ell.\\end{aligned}\\ ] ] that is , @xmath233 the total number of data generation steps is simply @xmath234 substitution of equation  ( [ eqn : const3 ] ) and equation  ( [ eqn : const4 ] ) into equation  ( [ eqn : const1 ] ) yields @xmath235 substitution of equation  ( [ eqn : const3 ] ) and equation  ( [ eqn : const5 ] ) into equation  ( [ eqn : const2 ] ) allows us to obtain @xmath236 , @xmath237 finally , through substitution of equation  ( [ eqn : lambda ] ) back into equation  ( [ eqn : const5 ] ) we find that the optimal @xmath238 is given by , @xmath239 as required .",
    "dodwell tj , ketelsen c , scheichl r , teckentrup al ( 2015 ) a hierarchical multilevel markov chain monte carlo algorithm with applications to uncertainty quantification in subsurface flow .",
    "3(1):10751108 .",
    "small pm , et  al .",
    "( 1994 ) the epidemiology of tuberculosis in san francisco  a population - based study using conventional and molecular methods .",
    "gillespie dt ( 1977 ) exact stochastic simulation of coupled chemical reactions ."
  ],
  "abstract_text": [
    "<S> likelihood - free methods , such as approximate bayesian computation , are powerful tools for practical inference problems with intractable likelihood functions . </S>",
    "<S> markov chain monte carlo and sequential monte carlo variants of approximate bayesian computation can be effective techniques for sampling posterior distributions without likelihoods . </S>",
    "<S> however , the efficiency of these methods depends crucially on the proposal kernel used to generate proposal posterior samples , and a poor choice can lead to extremely low efficiency . </S>",
    "<S> we propose a new method for likelihood - free bayesian inference based upon ideas from multilevel monte carlo . </S>",
    "<S> our method is accurate and does not require proposal kernels , thereby overcoming a key obstacle in the use of likelihood - free approaches in real - world situations . </S>"
  ]
}