{
  "article_text": [
    "model selection aims at choosing , from a set of candidates , a mathematical model that strikes a balance between simplicity and adequacy to the observed data .",
    "traditionally , it is performed by optimizing some information criteria ( ics ) . in particular , the bayesian information criterion ( bic ,  @xcite ) , aic  @xcite , and the minimum message length ( mml ) principle  @xcite , are widely used in different statistical model selection problems .",
    "these criteria have a discrete feasible domain .",
    "their optimization usually involves exhaustive search over all possible models , which is computationally intensive .",
    "when the model is very complex or the space of candidate models is very large , a brute force testing of all possible models causes very high computational costs and becomes impractical . to tackle this problem ,",
    "a lot of efforts have been made to adjust the model complexity continuously .",
    "for instance , for the linear regression problem , lasso  @xcite applies the @xmath0 penalty on the coefficients which could shrinking some coefficients to zero .",
    "various approaches , including adaptive lasso ( alasso ,  @xcite ) , scad  @xcite , and first  @xcite make use of similar but different ways of parameter shrinkage . for finite mixture models ,  entropic prior \"  @xcite or the dirichlet prior  @xcite for the mixing weights could produce sparsity of the mixing weights and hence perform model selection .",
    "however , in these methods , how to select the penalization parameter is usually a crucial issue .",
    "moreover , asymptotic properties of these methods have been well studied , but less attention was paid to their performance on finite samples .",
    "it would be very useful if one can find their relationship to the ic - based approach for finite samples .",
    "we aim to develop an efficient model selection approach which is based on the continuous penalized likelihood and approximately coincides with model selection based on ics , such as bic .",
    "we call this approach quick information criterion - like ( quick - ic ) model selection .",
    "our contributions are mainly two fold .",
    "first , for regular models , we establish a bridge between the penalized likelihood of alasso and ics , and propose to approximate the latter with the former , resulting in convenient model selection ; this can also be considered as a way to directly determine the penalization parameter in alasso to perform ic - like model selection , which avoids the search for the penalization parameter and would save a lot of computation , especially when iterative procedures are needed to find alasso solutions .",
    "specifically , in sec .",
    "[ sec : alasso ] , we give the intuition that the penalty term for each parameter in alasso is closely related to an indicator function showing if this parameter is active .",
    "consequently , one can approximate the number of free parameters in ics in terms of such penalty terms and find continuous approximators to the ics .",
    "this inspires the proposed approach quick - ic in sec .",
    "[ sec : equivalence ] , which is shown to select exactly the same model as the corresponding ic does in the case with a diagonal fisher information matrix .",
    "general cases are also briefly discussed .",
    "the theoretical claims are verified by simulation studies in section 4 .",
    "second , in sec .",
    "[ sec : make_continuous ] , we extend quick - ic to non - regular and complex models , such as factor analysis , the gaussian mixture model and the mixture of factor analyzers  @xcite , whose model selection is traditionally very difficult due to the large candidate model space . by making use of logarithm penalties with data - dependent weights , we provide continuous approximators to the ics suitable for model selection of these models , and make their model selection easy and efficient .",
    "this illustrates the good applicability of the proposed approach .",
    "in this section we assume that the model under consideration satisfies some regularity conditions including identification conditions for the parameters @xmath1 , the consistency of the maximum likelihood estimate ( mle ) @xmath2 when the sample size @xmath3 tends to infinity , and the asymptotic normality of @xmath2 . the penalized likelihood can be written as @xmath4 where @xmath5 is the log - likelihood",
    ", @xmath1 is the parameter vector , @xmath6 is the penalty , and @xmath7 is the penalization parameter .",
    "the maximum penalized likelihood estimate is @xmath8 .",
    "@xmath9 gives the @xmath0-norm penalty .",
    "the @xmath0 penalty produces sparse and continuous estimates  @xcite , and it has been shown to outperform other penalties in some scenarios  @xcite",
    ". however , it also causes bias in the estimate of significant parameters , and it could select the true model consistently only when the data satisfy certain conditions  @xcite .",
    "certain methods , including stability selection with the randomized lasso  @xcite and alasso  @xcite , were proposed to overcome such disadvantages of the @xmath0 penalty .",
    "in particular , alasso uses @xmath10 , with @xmath11 , where @xmath12 , and @xmath2 is a ( initial ) mle of @xmath1 . consequently , the strength for penalizing different parameters depends on the magnitude of their estimate . under some regularity conditions and",
    "the condition @xmath13 and @xmath14 ( the subscript @xmath3 is used in @xmath15 to indicate the dependence of @xmath7 on the sample size @xmath3 ) , the alasso estimator is consistent in model selection .",
    "we are more interested in its behavior on finite samples .",
    "the result of alasso depends on the penalization parameter @xmath7 . for very simple models",
    ", one may use least angle regression ( lars ,  @xcite ) to compute the entire solution path , which gives all possible solutions as @xmath7 changes . among these solutions ,",
    "the best model can then be selected by cross - validation or based on some ics  @xcite .",
    "( the latter approach is compared with our approach in sec .",
    "[ sec : compare ] , and one can see that it may give very different results from the corresponding ic . ) however , for complex models , especially when iterative algorithms are used to find the solution corresponding to a given @xmath7 , it is computationally very demanding and impractical to find the solution path .",
    "one then needs to select the penalization parameter in advance . in the next section",
    "we show that one can simply determine this parameter , while the model selection result approximately coincides with that based on ics .",
    "let us focus on the case @xmath16 , meaning that @xmath17 for such parameters .",
    "on the other hand , with suitable @xmath15 , the alasso estimator @xmath18 is also consistent  @xcite ; roughly speaking , significant parameters are then expected to be changed little by the penalty , when @xmath3 is not very small .",
    "consequently , at convergence , @xmath19 _ approximately indicates whether the parameter @xmath20 is active or not_. suppose that the parameters are not redundant .",
    "@xmath21 is then an approximator of the number of active parameters , denoted by @xmath22 , in the resulting model .",
    "recall that the traditional ics whose minimization enables model selection can be written as @xmath23 the bic and aic criteria are obtained by setting the value of @xmath24 to @xmath25 respectively . relating ( [ eq : ic ] ) to the penalized likelihood ( [ eq : vs ] ) , one can see that in alasso , by setting @xmath26 ( @xmath24 may be @xmath27 , @xmath28 , etc . ) , the maximum penalized likelihood is closely related to the ic ( [ eq : ic ] ) .",
    "this will be rigorously studied next , and in fact _ @xmath29 _ ( instead of @xmath26 ) _ gives interesting results_.",
    "can we make the model selection results of alasso exactly the same as those based on the ics ?",
    "in fact , if the fisher information matrix is diagonal , this can be achieved by simply setting @xmath7 in alasso to @xmath30 , i.e. , maximizing the following penalized likelihood @xmath31 selects the same model as the ic ( [ eq : ic ] ) does , as seen from the following proposition .",
    "[ prop1 ] suppose that the following conditions hold",
    "the log - likelihood @xmath5 is quadratic around the mle @xmath2 , with a non - singular observed fisher information matrix @xmath32 .",
    "@xmath32 is diagonal .",
    "then the non - zero parameters selected by maximizing ( [ eq : proposed_pl ] ) are exactly those selected by minimizing the ic ( [ eq : ic ] ) .    since the mle @xmath2 maximizes @xmath5 , we have @xmath33 .",
    "let @xmath34 . under the assumptions made in the proposition ,",
    "the log - likelihood becomes @xmath35 the penalized likelihood ( [ eq : proposed_pl ] ) then becomes @xmath36 it is easy to show that the solutions maximizing @xmath37 are latexmath:[\\[\\nonumber \\hat{\\theta}_{i , al } = \\textrm{sgn } ( \\hat{\\theta}_{i})\\cdot \\big(|\\hat{\\theta}_{i}| - 2\\lambda_{ic } /(\\mathbf{h}_{ii}\\cdot",
    "@xmath18 estimated by maximizing ( [ eq : proposed_pl ] ) is non - zero if and only if @xmath39    on the other hand , the model selected by minimizing the criterion ( [ eq : ic ] ) has @xmath40 free parameters if @xmath41 and @xmath42 .",
    "according to ( [ eq : ic ] ) , we then have @xmath43 the least change in @xmath5 caused by eliminating a particular parameter has been derived in the optimal brain surgeon ( obs ) technique  @xcite . here , due to the simple form of ( [ eq : like_approx ] ) , the least change in @xmath5 caused by eliminating @xmath20 , denoted by @xmath44 , can be seen directly : @xmath45_{ii }   = \\frac{1}{2 } [ \\mathbf{h}]_{ii } \\cdot \\hat{\\theta}_i^2.\\ ] ] note that @xmath46 in ( [ eq : l_change ] ) is the minimum of @xmath44 for all @xmath40 parameters in the current model .",
    "therefore , one can see that model selection based on the ic ( [ eq : ic ] ) selects @xmath20 if and only if @xmath47 , which is equivalent to the constraint ( [ eq : criterion_pl ] ) .",
    "that is , under the assumptions made in the proposition , non - zero parameters produced by maximizing the penalized likelihood ( [ eq : proposed_pl ] ) are exactly those selected by the corresponding ic ( [ eq : ic ] ) .",
    "@xmath48    this proposition indicates that ( [ eq : proposed_pl ] ) can be considered as a continuous approximator to the ics ( [ eq : ic ] ) , which enables quick - ic ; one can see that the continuous approximator is obtained by simply replacing the maximum likelihood @xmath49 by the data likelihood @xmath5 , and replacing the number of free parameters , @xmath22 , by @xmath50 .",
    "the condition in proposition  [ prop1 ] is rather restrictive ; in the linear regression scenario , it corresponds to the orthogonal design case . in the more general case , where @xmath32 is usually not diagonal ,",
    "the condition for the parameters @xmath20 to be selected by maximizing ( [ eq : proposed_pl ] ) becomes more complex , and ( [ eq : ic ] ) and ( [ eq : proposed_pl ] ) are usually not exactly equivalent .",
    "we give some results on the relationship between quick - ic and model selection based on ics .",
    "[ prop2 ] suppose that condition 1 in proposition  [ prop1 ] holds .",
    "assume that both the ic approach ( [ eq : ic ] ) and quick - ic ( [ eq : proposed_pl ] ) perform model selection in the backword elimination manner , i.e. , the penalization parameter is gradually increased to the target value , such that insignificant parameters are set to zero one by one .",
    "further assume that once a parameter is set to zero , it will not become non - zero again .",
    "let @xmath51 , where @xmath52 and is assumed to be nonsingular . then the ic approach ( [ eq : ic ] ) selects @xmath20 if and only if @xmath53_{ii } < \\frac{1}{2\\lambda_{ic}}$ ] , while quick - ic ( [ eq : proposed_pl ] ) does so if and only if @xmath53_{i\\cdot}\\mathbb{i } < \\frac{1}{2\\lambda_{ic}}$ ] , where @xmath53_{i\\cdot}$ ] donotes the @xmath54th row of @xmath55 and @xmath56 is the vector of 1 s .    from the proof of proposition  [ prop1 ] or  @xcite",
    ", one can see that the ic approach selects @xmath20 if and only if @xmath57_{ii } } > \\lambda_{ic}$ ] , which is equivalent to @xmath53_{ii } < \\frac{1}{2\\lambda_{ic}}$ ] .",
    "let @xmath58 . on the other hand , due to condition 1 in proposition  [ prop1 ] , the penalized likelihood with the penalization parameter @xmath7",
    "is @xmath59 clearly , if @xmath7 is very small such that none of @xmath20 is set to zero , @xmath60 is maximized when @xmath61 , i.e. , @xmath62 which is equivalent to @xmath63 .",
    "consequently , we have @xmath64_{i\\cdot}\\mathbb{i}.\\ ] ] when @xmath7 is gradually increased such that @xmath65_{j.}\\mathbb{i } = 1 $ ] , @xmath66 , or equivalently @xmath67 , is set to zero .",
    "finally , when @xmath7 is increased to @xmath30 , the non - zero parameters @xmath20 selected by quick - ic satisfy @xmath53_{i\\cdot}\\mathbb{i } < \\frac{1}{2\\lambda_{ic}}$ ] .",
    "@xmath48    although in practice one may not adopt backword elimination , the above proposition helps us understand the similarity and difference between the ic approach and quick - ic .",
    "for example , if @xmath68_{ij } = 0 $ ] for all @xmath54 ( which includes proposition  [ prop1 ] as a special case ) , the two approaches give the same results .",
    "of course , for finite samples , in the general case it is theoretically impossible to make parameter shrinkage - based quick - ic exactly identical to the ic approach .",
    "however , their empirical comparisons in various situations presented in sec .",
    "[ sec : compare ] suggest that they usually give the same model selection results for various sample sizes .",
    "we give the following remarks on the proposed model selection approach .",
    "firstly , the result of the proposed approach depends on @xmath2 .",
    "when the model is very large , @xmath2 may be too rough , and it is useful to update @xmath2 using a consistent estimator sometime when a smaller model is derived .",
    "minimization methods ( see , e.g. ,  @xcite ) . in the reweighted methods , in each iteration the penalized estimate given in the previous iteration is used to form the new weight ; in this way , the reweighted alasso penalty provides an approximator to the logarithm penalty , since @xmath69 can be locally approximated by @xmath70 plus some constant , about point @xmath71 . ] secondly , in sec .",
    "[ sec : make_continuous ] the idea of quick - ic is further applied to more complex models , by using data - dependent weights for suitable penalization functions and approximating the number of effective parameters .",
    "for example , in some cases one needs to resort to the logarithm penalty to produce sparsity of parameters , and we suggest using the corresponding data - adaptive penalty @xmath72 with @xmath73 , where @xmath74 is a very small positive number , as the penalty term , as discussed in section  [ sec : make_continuous_mml ] .",
    "correspondingly , to obtain the continuous approximator of the ics , one just simply replaces the number of effective parameters in @xmath20 with @xmath75 .",
    "the proposed approach in section  [ sec : equivalence ] directly applies to model selection of simple models such as regression and vector auto - regression ( var ) .",
    "var provides a convenient way for granger causality analysis  @xcite , and has a lot of applications in economics , neuroscience , etc .",
    "unfortunately it usually involves quite a large number of parameters , making the ic approach impractical , while quick - ic gives efficient model selection .    in this section",
    "we use simulations to investigate the performance of quick - ic . to verify the results in sec .",
    "[ sec : equivalence ] , we consider the simple linear regression problem @xmath76 , where @xmath77 is the target , @xmath78 contains predictors , and @xmath74 is the gaussian noise . we take bic as an example , i.e. , we compare bic - like quick - ic ( or quick - bic , with @xmath79 in ( [ eq : proposed_pl ] ) ) with the original bic ( [ eq : ic ] ) .",
    "we also compare them with the approach of alasso followed by the bic criterion ( alasso+bic ) : one first finds the solution path of alasso using lars , and then selects the  best \" model by evaluating the bic criterion with the maximum likelihood @xmath49 replaced by the likelihood of the parameter values on the solution path ( @xcite , sec .",
    "for this reason , alasso+bic is different from bic . in quick - bic",
    ", the noise variance was estimated from the full model . for bic",
    ", we searched the prediction number between 4 and 8 .",
    "20 predictors @xmath80 were used , i.e. , @xmath81 .",
    "14 entries of @xmath1 were set to zero .",
    "the magnitudes of the others were randomly chosen between 0.2 and 2.5 , and the signs were arbitrary .",
    "we considered three cases .",
    "case i corresponded to an orthogonal design , i.e. , all predictors are uncorrelated . in case ii , the pairwise correlation between @xmath80 and @xmath82 was set to be @xmath83 . in the last case ,",
    "the covariance matrix of @xmath84 was randomly generated as @xmath85 with entries of the square matrix @xmath86 randomly sampled between @xmath87 and @xmath88 . in all cases",
    "we normalized the variance of each @xmath80 .",
    "the noise variance was @xmath88 . to see the sample size effect , we varied the sample size @xmath3 from 100 to 300 .",
    "the simulation was repeated for 100 random trials .",
    "table  [ tab1 ] reports the frequency of the differences in the selected predictor numbers given by different methods .",
    "one can see that in case i , all the three methods almost always select the same number of predictors . in cases",
    "ii and iii , quick - bic still gives rather similar results to bic ; in particular , as the sample size increases , their results tend to agree with each other quickly .",
    "alasso+bic produces different models with a surprisingly noteworthy chance for both sample sizes , especially in case iii .",
    "however , it seems to be still statistically consistent in model selection , like bic ; we found that when @xmath89 , for 56 times it gave the same model as bic .",
    "as for the computational loads , bic took more than 550 times longer than quick - bic as well as alasso+bic .",
    "[ tab1 ]    [ cols=\"^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]",
    "below we focus on other frequently - used statistical models , especially some complex ones , and give continuous approximators to the ics for their model selection by extending quick - ic .",
    "we also give empirical results to illustrate the applicability and efficiency of quick - ic .      for regular statistical models , under a set of regularity conditions , the asymptotic normality of @xmath90 holds . the @xmath0 penalty used in lasso",
    "can then produce sparsity of the parameters and hence perform model selection  @xcite .",
    "the asymptotic properties of the variable selection techniques established in the linear regression scenario also approximately hold for regular models .",
    "for some non - regular models , it is still possible to do so .",
    "if the gradient of the log - likelihood changes slowly around @xmath90 , these penalties will successfully push insignificant parameters to zero . otherwise , one may apply penalization on suitable transformations of the parameters , instead of the original parameters .    in practice ,",
    "the parameters in a model often naturally belong to groups , i.e. , they are selected or discarded simultaneously  @xcite .",
    "one can formalize this by introducing functions @xmath91 which allow computation of the penalties for groups of variables . generally speaking ,",
    "the information criterion of the form ( [ eq : ic ] ) can be approximated by the negative penalized likelihood : @xmath92 where @xmath93 are suitable transformations of the parameters ( or selected parameters ) controlling the complexity of the model , and @xmath94 are the numbers of independent parameters associated with the group @xmath93 .",
    "minimization of the negative penalized likelihood ( [ eq : npl ] ) enables simultaneous model selection and parameter estimation .",
    "when a particular @xmath93 is pushed to zero , @xmath94 free parameters disappear , and the model complexity is reduced . how to choose @xmath93 and to calculate @xmath94 depends on the specific model .",
    "let us first consider model selection of the factor analysis ( fa ) model . in fa , the observed @xmath95-dimensional data vector @xmath96 is modeled as @xmath97 , where @xmath98 is the factor loading matrix , @xmath99 the vector of @xmath100 underlying gaussian factors , and @xmath101 the vector of uncorrelated gaussian errors with the covariance matrix @xmath102 .",
    "the factors @xmath103 and the errors @xmath104 are also mutually independent . here",
    ", we have assumed that @xmath84 is zero - mean and that the factors @xmath103 are normally distributed with zero mean and identity covariance matrix .",
    "given the factor number @xmath100 and a set of observations @xmath105 , the fa model can be fitted by maximum likelihood ( ml ) using the expectation - maximization ( em ) algorithm  @xcite .",
    "but ml estimation could not determine the optimal factor number @xmath106 , since the ml does not consider the complexity of the model and it increases as @xmath100 grows .",
    "a suitable factor number gives the fa model enough capacity and avoids over - fitting .",
    "when the unconditional variances of @xmath107 are fixed , model selection of fa can be achieved by shrinking suitable columns of @xmath98 to zero .",
    "so entries in each column of @xmath98 are grouped .",
    "denote by @xmath108 the @xmath54th column of @xmath98 .",
    "note that @xmath109 is singular at @xmath110 , so penalization on @xmath111 can remove unnecessary columns in @xmath98 and consequently perform model selection .",
    "the negative penalized likelihood for approximating bic is @xmath112 where @xmath113 denotes the number of free parameters in the column of @xmath98 which is to be removed , and @xmath27 is given in ( [ eq : lambda_alasso ] ) .",
    "due to the rotation indeterminacies of the factors @xmath107 , the total number of free parameters in @xmath98 is @xmath114 .",
    "the proposed method removes columns of @xmath98 one by one .",
    "if one insignificant column of @xmath98 is shrinked to zero , the total number of free parameters in @xmath98 reduces from @xmath114 to @xmath115 .",
    "therefore , @xmath116 can be evaluated to equal @xmath117 , as the change of the number of free parameters in @xmath98 when a certain column disappears .",
    "once a column of @xmath98 is removed , @xmath100 is updated accordingly .    the em algorithm for minimizing the negative penalized likelihood ( [ eq : pll_fa ] )",
    "can be derived analogously to the derivation of that for the fa model  @xcite . following  @xcite",
    ", we use the local quadratic approximation ( lqa ) to approximate the penalties @xmath118 . as a great advantage , it admits a closed - form solution for @xmath98 in the m step .",
    "we would like to address the following advantages of adopting the negative penalized likelihood based on alasso , instead of the original bic criterion , for model selection .",
    "the negative penalized likelihood is easy to minimize .",
    "if the log - likelihood function is concave in the neighborhood of the maximum likelihood estimator ( like in the linear regression problem ) , the negative penalized likelihood is _ convex _ , and its minimization does not suffer from multiple local minima .",
    "the gaussian mixture model ( gmm ) models the density of the @xmath95-dimensional variable @xmath84 as a weighted sum of some gaussian densities : @xmath119 , where @xmath120 are gaussian densities with mean @xmath121 and covariance matrix @xmath122 , and @xmath123 are nonnegative weights that sum to one .",
    "bic is not suitable for model selection of mixture models , since not all data are effective for estimating the parameters specifying an individual component . instead",
    ", the mml - based model selection criterion is preferred  @xcite .",
    "the message length to be minimized for model selection of gmm is @xmath124 where @xmath125 denotes the number of non - zero - probability components , and the number of free parameters in each component is @xmath126 .",
    "minimization of the above function is troublesome since it involves the discrete variable @xmath125 .",
    "below we develop an approximator to ( [ eq : mml_gmm ] ) which is continuous in @xmath123 .",
    "gmm is a typical non - regular statistical model .",
    "the expected complete - data log likelihood of gmm ( see  @xcite for its formulation ) , which gives an approximation of the true data likelihood , involves @xmath127 .",
    "hence , its gradient w.r.t .",
    "@xmath123 grows very fast when @xmath128 .",
    "consequently , the @xmath0 penalty could not push insignificant @xmath123 to zero .",
    "fortunately , one can then naturally exploit the @xmath129 penalty to produce sparsity of @xmath123 .",
    "the @xmath129 penalty on @xmath123 also has the advantage of admitting a closed - form update equation for @xmath123 in the em algorithm . to avoid the discontinuity of the objective function when a component with @xmath130 vanishes , we use @xmath131 as the penalty , where @xmath132 is a small enough positive number ( we chose @xmath133 in experiments ) .",
    "let @xmath134 $ ] .",
    "inspired by the idea of adaptive weights in alasso , we can let the penalty term be @xmath135 $ ] .",
    "@xmath125 could then be approximately by 2@xmath136}$ ] .",
    "consequently , ( [ eq : mml_gmm ] ) is approximated by @xmath137\\sum_{i=1}^m\\hat{w}_i\\log\\big(\\frac{\\pi_i+\\epsilon}{\\epsilon } \\big ) - l^{gm}.\\end{aligned}\\ ] ] the em algorithm for minimizing the function above is the same as that for maximizing the gmm likelihood , except that the update equation for @xmath123 is changed to @xmath138}{n-0.5md_f^{gm}- 0.5 [ \\log(n/12 ) + d_f^{gm}+1]\\sum_{j=1}^m\\hat{w}_j } \\big\\},\\end{aligned}\\ ] ] where @xmath139 denotes the posterior probability that the @xmath140th point comes from the @xmath54th component .",
    "when @xmath123 becomes very small , say smaller than @xmath141 , we drop the @xmath54-th component . in practice ,",
    "if the initialized model is very far from the desired one , as the model complexity reduces , it is better to occasionally update @xmath142 with the corresponding maximum likelihood estimator .",
    "now consider the mixture of factor analyzers ( mfa , @xcite ) , which has a lot of applications in pattern recognition .",
    "it assumes that the @xmath95-dimensional observations @xmath84 can be modeled as @xmath143 , where @xmath121 is the mean of the @xmath54th factor analyzer , and local factors @xmath144 , which follow @xmath145 , are independent from @xmath146 , which follow @xmath147 with @xmath148 .",
    "the factor number @xmath149 may vary for different @xmath54 .",
    "following  @xcite , one can find the message length for mfa ( with some constant terms dropped ) : @xmath150 +   \\frac{1}{2}\\sum_{i=1}^{m_{nz}}d_{fi}^{f }   % \\\\ \\label{eq : mml_mfa } & & ~~ + \\frac{m_{nz}}{2}\\big[\\log\\big(\\frac{n}{12}\\big)+1\\big ] - l^{mfa},\\end{aligned}\\ ] ] where @xmath151 denotes the number of free parameters specifying the @xmath54-th factor analyzer , i.e. , @xmath152 .",
    "this function involves integers @xmath125 ( the number of factor analyzers ) and @xmath149 , @xmath153 , ( the number of factors in each factor analyzer ) .",
    "its optimization is computationally highly demanding due to the large search space of @xmath154 . using the @xmath0 and @xmath129 penalties with data - adaptive weights , we can approximate @xmath155 with the following function : @xmath156 -l^{mfa},\\end{aligned}\\]]where @xmath157 and @xmath158 is the alasso - based approximator to the number of free parameters in @xmath159 .",
    "after some derivations , one can see that a reasonable approximator is @xmath160 - d_f \\sum_{j=1}^{k_i } \\big(1 - p_\\lambda(||\\mathbf{a}_{i,(j)}|| ) \\big ) = \\frac{k_i(k_i-1)}{2 } + ( d - k_i+1)\\sum_{j=1}^{k_i } p_\\lambda(||\\mathbf{a}_{i,(j)}||)$ ] .",
    "one can verify that @xmath158 changes very slightly when @xmath149 is reduced by shrinking columns of @xmath161 .",
    "similar to  @xcite , one can derive the em algorithm for minimizing ( [ eq : npl_mml_mfa ] ) .",
    "we note that the proposed model selection methods for gmm and mfa generate new components or split any large component .",
    "for very complex problems , they may converge to local optima .",
    "if necessary , one can perform the split and merge operations  @xcite after certain em iterations to improve the final results .",
    "we generated the data according to the fa model , and compared four model selection schemes , which are bic - like quick - ic given in section  [ sec : fa ] ( or quick - bic ) , bic , aic , and fivefold cross - validation ( cv ) .",
    "the true factor number was @xmath162 , and the data dimension was @xmath163 .",
    "elements in @xmath98 were randomly generated between @xmath164 and @xmath165 , and the error variances @xmath166 were random numbers between 0 and 1 . when using bic , aic , or cv , we let @xmath167 and @xmath168 , while quick - bic was initialized with 8 factors . to investigate the sample size effect ,",
    "we let the sample size @xmath3 be 40 and @xmath169 .",
    "in each case , we repeated all methods for 100 random trials .    when @xmath170 , bic , aic , quick - bic , and cv approximately took 4 , 4 , 1.5 , and 20 seconds , respectively , for each trial .",
    "clearly quick - bic is most computationally appealing , as expected .",
    "[ fig : fa_exp ] plots the histogram of the factor numbers found by the four methods . when @xmath170 , bic ( as well as quick - bic ) seems to over - penalize the model complexity and results in a smaller factor number .",
    "but when @xmath3 is increased to 100 , its performance becomes almost the best . on the contrary",
    ", aic seems to under - penalize the complexity . in both cases , quick - bic is always similar to bic . also considering its light computational load ,",
    "quick - bic is preferred among the four methods .",
    "we also tested the case @xmath171 , and found that quick - bic and bic give clearly the best results .",
    "( 100 trials ) .",
    "the true value is 5 .",
    "top / bottom : @xmath3=40/100 .",
    ", title=\"fig:\",width=499,height=96 ] +  ( a ) bic  ( b ) aic  ( c ) quick - bic  ( d ) cv   +   ( 100 trials ) .",
    "the true value is 5 .",
    "top / bottom : @xmath3=40/100 . , title=\"fig:\",width=499,height=96 ] +  ( e ) bic  ( f ) aic  ( g ) quick - bic  ( h ) cv   +      we compared the approach quick - ic which minimizes the continuous version of mml ( [ eq : mml_gmm_continuous ] ) with the mml - based method proposed in  @xcite ( denoted by fj s method ) , in terms of the chances of finding the preferred component number and the cpu time . here",
    "we present the results on two data sets . for each data",
    "set , we repeated each method for 100 trials . in each trial , the data were randomly generated , and for initialization , the mean of each gaussian component was randomly chosen from the data points .",
    "the results on the  shrinking spiral \" data set  @xcite are given in fig .",
    "[ fig : gmm_exp](a - c ) , and fig .",
    "[ fig : gmm_exp](d - f ) shows the results on the  triangle data \" , which were obtained by rotating and shifting three sets of bivariate gaussian points following @xmath172 .",
    "for the first data set , we set @xmath173 for both methods and @xmath174 for fj s method . the cpu time taken by",
    "quick - ic and fj s method was @xmath175 and @xmath176 seconds , respectively .",
    "for the second data set , we let @xmath177 and @xmath178 for fj s method . the cpu time was about @xmath179 and @xmath180 seconds for the two methods .",
    "[ fig : gmm_exp](b ,",
    "e ) and ( c , f ) give the histograms of the component numbers obtained by fj s method and quick - ic .",
    "one can see that they give similar results . however , fj method seems to produce less robust ( more disperse ) results for the spiral data .",
    "we conjecture that it is caused by the  annihilation \" process in fj s method  @xcite : fj s method annihilates the least probable component ( with the smallest mixing weight @xmath181 ) to obtain a smaller model .",
    "this process is discontinuous , and simply uses the magnitude of the mixing weight to indicate the significance of the corresponding component .",
    "in fact the significance of a particular component also depends on its relationship to other components . as a consequence , when a component that has the least weight but is actually significant is removed , the message length @xmath182 may increase , resulting in a sub - optimal model .     +  ( a ) spiral data  ( b ) fj s  ( c ) quick - ic",
    "+   +  ( d ) triangle data  ( e ) fj s  ( f ) quick - ic",
    "+      quick - ic uses the mml approximator ( [ eq : npl_mml_mfa ] ) to determine both the number of factor analyzers ( @xmath183 ) and the local factor numbers ( @xmath149 ) in the mfa model .",
    "we tested the spiral data ( figure  [ fig : gmm_exp]a ) , and repeated the experiments with 50 trials .",
    "@xmath183 was initialized as @xmath184 and all of @xmath149 were initialized as @xmath185 .",
    "the number of factor analyzers learned by our approach is always between 10 and 13 ( with the chances 10 : 8/50 , 11 : 21/50 , 12 : 14/50 , and 13 : 7/50 ) . in the resulting model ,",
    "most factor analyzers have 1 factor , and occasionally there is one factor analyzer with 2 factors ( with one dominating the other ) or with no factor .",
    "this is consistent with the previous results with @xmath149 _ a prior _ set to 1  @xcite .",
    "we then constructed another synthetic data set in which the local factor number varies for different factor analyzers . fig .",
    "[ fig : mfa_guy](a ) plots the data points without noise , and ( b ) shows the observed noisy data .",
    "the sample size was 5390 .",
    "quick - ic was compared with the variational bayesian method ( vbmfa ,  @xcite ) .",
    "we repeated both methods for 20 trials with different initializations .",
    "quick - ic and vbmfa took about @xmath165 and @xmath186 minutes for each run , and produced @xmath187 and @xmath188 factor analyzers , respectively .",
    "note that the data are clearly non - gaussian , so some factor analyzers may overlap to some extent to model the data well . fig .",
    "[ fig : mfa_guy](c ) and ( d ) show the results of the two method in one run . since quick - ic does not generate new local factor analyzers , it can not separate two factor analyzers which are initialized together .",
    "this can be alleviated by using a large @xmath183 for initialization .",
    "on the other hand , sometimes vbmfa may split one factor analyzer into two ; we found that in 2 trials vbmfa divided the `` arm '' or `` leg '' into two segments .    .",
    "( a ) noiseless points .",
    "( b ) noisy data for analysis . ( c )",
    "a structure learned by quick - ic , with @xmath189 .",
    "( d ) that by vbmfa , with @xmath189 . note that ( c ) plots columns of loading matrices , while ( d ) depicts the contour of the gaussian distribution of each factor analyzer .",
    ", title=\"fig:\",width=299 ] +  ( a )  ( b )   + .",
    "( a ) noiseless points .",
    "( b ) noisy data for analysis . ( c )",
    "a structure learned by quick - ic , with @xmath189 .",
    "( d ) that by vbmfa , with @xmath189 . note that ( c ) plots columns of loading matrices , while ( d ) depicts the contour of the gaussian distribution of each factor analyzer .",
    ", title=\"fig:\",width=288 ] +  ( c )  ( d )   +",
    "we showed that under some conditions , the penalty used in adaptive lasso , which is the @xmath0 penalty with a data - dependent weight , resembles an indicator function showing if this parameter is active .",
    "this motivated us to approximate the traditional model selection criterion by the penalized likelihood with a fixed penalization parameter .",
    "the latter is continuous in the parameters , greatly facilitating the model selection procedure .",
    "we formulated this idea as the quick - ic approach .",
    "we showed that for finite samples , quick - ic produces exactly the same model as the corresponding information criterion when the fisher information matrix is diagonal .",
    "we also investigated more general cases .",
    "furthermore , for some complex and non - regular models , we provided continuous approximators to their model selection criteria , by using suitable penalty forms and data - adaptive weights .",
    "we have demonstrated that for these models , our simple approach is computationally very efficient in model selection , and that its results are similar to those produced by the corresponding ic .",
    "one line of our future research is to investigate the theoretical properties of quick - ic for non - regular models such as finite mixture models ."
  ],
  "abstract_text": [
    "<S> model selection based on classical information criteria , such as bic , is generally computationally demanding , but its properties are well studied . on the other hand , </S>",
    "<S> model selection based on parameter shrinkage by @xmath0-type penalties is computationally efficient . in this paper </S>",
    "<S> we make an attempt to combine their strengths , and propose a simple approach that penalizes the likelihood with data - dependent @xmath0 penalties as in adaptive lasso and exploits a fixed penalization parameter . even for finite samples , </S>",
    "<S> its model selection results approximately coincide with those based on information criteria ; in particular , we show that in some special cases , this approach and the corresponding information criterion produce exactly the same model . </S>",
    "<S> one can also consider this approach as a way to directly determine the penalization parameter in adaptive lasso to achieve information criteria - like model selection . as extensions </S>",
    "<S> , we apply this idea to complex models including gaussian mixture model and mixture of factor analyzers , whose model selection is traditionally difficult to do ; by adopting suitable penalties , we provide continuous approximators to the corresponding information criteria , which are easy to optimize and enable efficient model selection .    </S>",
    "<S> model selection , parameter shrinkage , information criterion , adaptive lasso , factor analysis , gaussian mixture , mixture of factor analysizers </S>"
  ]
}