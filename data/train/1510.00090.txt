{
  "article_text": [
    "multiplication in finite fields is a fundamental operation in arithmetic and finding efficient multiplication methods remains a topical issue .",
    "let @xmath1 be a prime power , @xmath2 the finite field with @xmath1 elements and @xmath3 the degree @xmath4 extension of @xmath2 .",
    "if @xmath5 is a basis of @xmath3 over @xmath6 then for @xmath7 and @xmath8 , we have the product @xmath9 where @xmath10 @xmath11 being some constants .",
    "the complexity of a multiplication algorithm in @xmath3 depends on the number of multiplications and additions in @xmath6 .",
    "there exist two types of multiplications in @xmath6 : the scalar multiplication and the bilinear multiplication .",
    "the scalar multiplication is the multiplication by a constant ( in @xmath12 ) which does not depend on the elements of @xmath13 that are multiplied .",
    "the bilinear multiplication is a multiplication of elements that depend on the elements of @xmath13 that are multiplied .",
    "the bilinear complexity is independent of the chosen representation of the finite field .",
    "for example , the direct calculation of @xmath14 using ( [ calculdirect ] ) requires @xmath15 non - scalar multiplication @xmath16 , @xmath17 scalar multiplications and @xmath18 additions .",
    "more precisely , the multiplication of two elements of @xmath13 is an @xmath6-bilinear application from @xmath19 onto @xmath13",
    ". then it can be considered as an @xmath6-linear application from the tensor product @xmath20 onto @xmath13 .",
    "consequently , it can also be considered as an element @xmath21 of @xmath22 where @xmath23 denotes the dual .",
    "when @xmath21 is written @xmath24 where the @xmath25 elements @xmath26 as well as the @xmath25 elements @xmath27 are in the dual @xmath28 of @xmath13 while the @xmath25 elements @xmath29 are in @xmath13 , the following holds for any @xmath30 : @xmath31 the decomposition ( [ tensor ] ) is not unique .",
    "every expression @xmath32 defines a bilinear multiplication algorithm @xmath33 of bilinear complexity @xmath34 .",
    "the minimal number of summands in a decomposition of the tensor @xmath21 of the multiplication is called the bilinear complexity of the multiplication and is denoted by @xmath35 : @xmath36 where @xmath33 is running over all bilinear multiplication algorithms in @xmath13 over @xmath6 .    the bilinear complexity of the multiplication in @xmath3 over @xmath2 has been widely studied . in particular , it was proved in @xcite that it is uniformly linear with respect to the degree @xmath4 of the extension .",
    "it follows from a clever algorithm performing multiplication : the so - called multiplication algorithm of chudnovsky and chudnovsky .",
    "the original chudnovsky - chudnovsky algorithm was introduced in 1987 by d.v . and",
    "chudnovsky @xcite and is based on the interpolation on some algebraic curves . from now on",
    ", we will denote this algorithm by ccma .",
    "there is benefit having a low bilinear complexity when considering hardware implementations mainly because it reduces the number of gates in the circuit .",
    "in fact , in the so - called non - scalar model ( denoted ns ) , only the bilinear complexity is taken into account and it is assumed that all scalar operations are free . indeed , this model does not reflect the reality and since the bilinear complexity is not the whole complexity of the algorithm , the complexity of the linear part of the algorithm should also be taken into account . in this paper , we consider two other models .",
    "the model s1 , which takes into account the number of multiplications without distinguishing between the bilinear ones and the scalar ones .",
    "the model s2 which takes into account all operations ( multiplications and additions ) in @xmath6 .",
    "notice that so far , practical implementations of multiplication algorithms over finite fields have failed to simultaneously optimize the number of scalar multiplications , additions and bilinear multiplications .",
    "regarding exponentiation algorithms , the use of a normal basis is of interest because the @xmath37 power of an element is just a cyclic shift of its coordinates .",
    "a remaining question is , how to implement multiplication efficiently in order to have simultaneously fast multiplication and fast exponentiation . in 2000 , gao et al .",
    "@xcite show that fast multiplication methods can be adapted to normal bases constructed with gauss periods .",
    "they show that if @xmath13 is represented by a normal basis over @xmath6 generated by a gauss period of type @xmath38 , the multiplication in @xmath13 can be computed with @xmath39 and the exponentiation with @xmath40 operations in @xmath6 ( @xmath1 being small ) .",
    "this result is valuable when @xmath41 is bounded .",
    "however , in the general case @xmath41 is upper - bounded by @xmath42 .    in 2009 ,",
    "couveignes and lercier construct in ( * ? ? ?",
    "* theorem 4 ) two families of basis ( called elliptic and normal elliptic ) for finite field extensions from which they obtain a model @xmath43 defined as follows .",
    "to every couple @xmath44 , they associate a model , @xmath45 , of the degree @xmath4 extension of @xmath6 such that the following holds :    there is a positive constant @xmath46 such that the following are true :    @xmath47 elements in @xmath13 are represented by vectors for which the number of components in @xmath6 is upper bounded by @xmath48    @xmath47 there exists an algorithm that multiplies two elements at the expense of @xmath49 multiplications in @xmath6 .",
    "@xmath47 exponentiation by @xmath1 consists in a circular shift of the coordinates .",
    "therefore , for each extension of finite field , they show that there exists a model which allows both fast multiplication and fast application of the frobenius automorphism .",
    "their model has the advantage of existing for all extensions .",
    "however , the bilinear complexity of their algorithm is not competitive compared with the best known methods , as pointed out in ( * ? ? ?",
    "* section 4.3.4 ) .",
    "indeed , it is clear that such a model requires at least @xmath50 bilinear multiplications .",
    "note that throughout the paper , efficiency of algorithms is described in terms of parallel time ( depth of the circuit , in number of multiplications ) , number of processors ( width ) and total number of multiplications ( size ) .",
    "we have width @xmath51 size @xmath51 depth@xmath52width .",
    "we propose another model with the following characteristics :    - our model is based on ccma method , thus the multiplication algorithm has a bilinear complexity in @xmath53 , which is optimal .",
    "- our model is tailored to parallel computation .",
    "hence , the computation time used to perform a multiplication or any exponentiation can easily be reduced with an adequate number of processors . since our method has a bilinear complexity of multiplication in @xmath53 , it can be parallelized to obtain a constant time complexity using @xmath54 processors . the previous aforementioned works ( @xcite and @xcite ) do not give any parallel algorithm ( such an algorithm is more difficult to conceive than a serial one ) .    - exponentiation by @xmath1 is a circular shift of the coordinates and can be considered free .",
    "thus , efficient parallelization can be done when doing exponentiation .",
    "- the scalar complexity of our exponentiation algorithm is reduced compare to a basic exponentiation using ccma algorithm thanks to a suitable basis representation of the riemann - roch space @xmath55 in the second evaluation map .",
    "more precisely , the normal basis representation of the residue class field is carried in the associated riemann - roch space @xmath56 , and the exponentiation by @xmath1 consists in a circular shift of the @xmath4 first coordinates of the vectors lying in the riemann - roch space @xmath55 .",
    "- our model uses coppersmith - winograd @xcite method ( denoted cw ) or any variants thereof to improve matrix products and to diminish the number of scalar operations .",
    "this improvement is particularly efficient for exponentiation .    in term of complexity",
    ", we can state the following results , depending on the chosen model ( ns , s1 and s2 ) .",
    "[ lns ] in the non - scalar model ns , there exist multiplication and exponentiation algorithms in @xmath13 such that :    * multiplication is done in parallel time in @xmath57 multiplications in @xmath6 with @xmath54 processors , for a total in @xmath54 multiplications . *",
    "exponentiation is done in parallel time in @xmath58 multiplications in @xmath6 with @xmath59 processors , for a total in @xmath60 multiplications .",
    "when considering models s1 and s2 , two cases can be distinguished for the multiplication complexity . we might be interested either by the complexity of one multiplication or by the average ( amortized ) complexity of one multiplication when many multiplications are done simultaneously .",
    "regarding exponentiation , a wise use of cw method allows the complexity to be improved .",
    "we can state the followings :    [ ls1 ] in the model s1 , there exist multiplication and exponentiation algorithms in @xmath13 such that :    * multiplication : * * one multiplication is done in parallel time in @xmath57 multiplications in @xmath6 with @xmath61 processors , for a total in @xmath61 multiplications ; * * in the amortized sense , the parallel time is in @xmath57 multiplications in @xmath6 with @xmath62 processors , for a total in @xmath62 multiplications where the value of @xmath63 is approximately @xmath64 for the best known matrix product methods ; * exponentiation is done in a parallel time of @xmath58 multiplications in @xmath6 with @xmath65 processors , for a total in @xmath66 multiplications .",
    "[ ls2 ] in the model s2 , there exist multiplication and exponentiation algorithms in @xmath13 such that :    * multiplication : * * one multiplication is done in parallel time in @xmath58 operations in @xmath6 with @xmath60 processors , for a total in @xmath61 operations ; * * in the amortized sense , the parallel time is in @xmath58 operations in @xmath6 with @xmath67 processors , for a total in @xmath62 operations ; recall that the value of @xmath63 is approximately @xmath64 for the best matrix product methods ; * exponentiation is done in a parallel time of @xmath68 operations in @xmath6 with @xmath69 processors , for a total in @xmath66 operations .      after some background on ccma algorithm",
    ", we describe in subsection  [ construction ] our method which leads to an effective algorithm that can directly be implemented .",
    "our algorithm reveals the use of matrix - vector products that can easily be parallelized . in section  [ secexp ]",
    ", we use this algorithm to tackle the problem of computing @xmath70 where @xmath71 and @xmath72 and we derive an exponentiation algorithm from the work of von zur gathen @xcite . in section",
    "[ effective ] , we focus on the multiplication in @xmath73 and we explain how to construct our algorithm . a magma  @xcite implementation of the multiplication algorithm in @xmath74 is given in appendix .",
    "first , we present the ccma algorithm on which is based our method .",
    "let @xmath75 be an algebraic function field over the finite field @xmath2 of genus @xmath76 .",
    "we denote by @xmath77 the number of places of degree one of @xmath78 over @xmath2 .",
    "if @xmath79 is a divisor , @xmath80 denotes the riemann - roch space associated to @xmath79 .",
    "we denote by @xmath81 the valuation ring of the place @xmath82 and by @xmath83 its residue class field @xmath84 which is isomorphic to @xmath85 where @xmath86 is the degree of the place @xmath82 . the following theorem that makes effective the original algorithm groups some results of @xcite .",
    "[ mainth ] let @xmath87 be an algebraic function field of genus @xmath76 defined over @xmath6 and @xmath4 an integer .",
    "let us suppose that there exists a place @xmath82 of degree @xmath4 .    then , if @xmath88 there is an effective divisor @xmath79 of degree @xmath89 such that :    1 .",
    "@xmath82 is not in the support of @xmath79 , 2 .",
    "the evaluation map @xmath90 defined by @xmath91 is an isomorphism of vector spaces over @xmath2 , 3 .",
    "there exist @xmath92 places of degree one @xmath93 which are not in the support of @xmath79 such that the multi - evaluation map @xmath21 defined by @xmath94 is an isomorphism .",
    "the chosen framework is the original ccma algorithm , namely using only places of degree one and without derivated evaluation ( cf .",
    "we transform this algorithm in order that it be adapted to both multiplication and exponentiation computations .    in this context , the construction of this algorithm is based on the choice of the place @xmath82 of degree @xmath4 , the effective divisor @xmath79 ( cf .",
    "@xcite ) and the bases @xmath95 and @xmath96 .",
    "recall some notions on normal bases .",
    "the finite field @xmath3 will be considered as a vector space of dimension @xmath4 over the finite field @xmath2 .",
    "let @xmath97 be an element of @xmath3 such that @xmath98 is a basis of @xmath3 over @xmath2 .",
    "such a basis is called a normal basis of @xmath3 over @xmath2 and @xmath97 is called a cyclic element .",
    "thus , a normal basis is composed of all conjugates of a cyclic element @xmath97 .",
    "there is always a normal basis and furthermore , there is always a primitive normal basis .",
    "we call a normal polynomial of degree @xmath4 over @xmath2 , a polynomial in @xmath99 $ ] , irreducible over @xmath2 , and having for roots in @xmath3 the @xmath4 conjugates of a cyclic element @xmath97 .",
    "we refer to @xcite and @xcite for a detailed presentation .",
    "when @xmath13 is represented by a normal basis , the @xmath1th power of an element is just a cyclic shift of its coordinates .",
    "the repeated use of this operation allows exponentiation to be efficiently parallelized . without normal basis @xcite ,",
    "precomputation should be stored for a same base @xmath100 .",
    "this makes sense only when many exponentiation have to be done with this same base and in this case , precomputations are not considered in the running time .",
    "the use of a normal basis has the following benefits :    * substitute lookup table accesses by circular shifts . *",
    "reduce prior storage . *",
    "avoid the constraint of fixing a base .",
    "the construction of the algorithm is based on the choice of the place @xmath82 of degree @xmath4 , the effective divisor @xmath79 of degree @xmath89 ( cf .",
    "@xcite ) , the bases of spaces @xmath101 and @xmath55 and the basis of the residue class field @xmath102 of the place @xmath82 .    in practice , as in @xcite , we take as a divisor @xmath79 one place of degree @xmath89 .",
    "it has the advantage to solve the problem of the support of divisor @xmath79 ( condition ( 1 ) of theorem [ mainth ] ) as well as the problem of the effectivity of the divisor @xmath79 .",
    "furthermore , we require additional properties .      to build the good places , we draw them at random and we check that they satisfy the required conditions namely :    1 .",
    "we draw at random an irreducible polynomial @xmath103 of degree @xmath4 in @xmath104 $ ] and check that this polynomial is : 1 .",
    "2 .   normal .",
    "3 .   totally decomposed in the algebraic function field @xmath87 ( which implies that there exists a place @xmath82 of degree n above the polynomial @xmath103 ) .",
    "we choose a place @xmath82 of degree n among the @xmath4 places lying above the polynomial @xmath103 .",
    "we draw at random a place @xmath79 of degree @xmath89 and check that @xmath105 is a non - special divisor of degree @xmath106 , _ i.e. _ @xmath107 .    in practice",
    ", it is easy to find @xmath82 and @xmath79 satisfying these properties in our context since there exist many such places .",
    "however , it is not true in the general case ( it is sufficient to consider an elliptic curve with only one rational point ) . a sufficient condition for the existence of at least one place of degree @xmath4 is given by the following inequality : @xmath108    when @xmath109 , we are sure of the existence of a non - special divisor of degree @xmath106 @xcite .",
    "the larger @xmath1 , the larger the probability to draw a non - special divisor of degree @xmath106 becomes ( proposition 5.1 @xcite ) but not necessarily as a difference of two places ( this is an open problem ) .",
    "however , in practice , such divisors are easily found .        when we take a place @xmath82 of degree @xmath4 lying above a normal polynomial in @xmath104 $ ] , we mean that the residue class field is the finite field @xmath13 for which we choose as a representation basis the normal basis @xmath110 generated by a root @xmath97 of the polynomial @xmath103 .",
    "[ changebasis ] suppose that the context requires the use of a representation basis of @xmath13 which is not the basis @xmath111 of the residue class field of the place @xmath82 .",
    "then , we can easily avoid the problem by a change of basis .",
    "this requirement may happen when additional properties on the basis @xmath111 ( cf . section [ chudnb ] ) are required .",
    "in particular , it would be the case in our context if we could not find a place @xmath82 of degree @xmath4 above a normal polynomial @xmath112 $ ] .",
    "as the residue class field @xmath102 of the place @xmath82 is isomorphic to the finite field @xmath13 , from now on we identify @xmath13 to @xmath102 .",
    "notice that the choice of @xmath79 and @xmath82 of section [ si ] are such that the map @xmath90 of theorem  [ mainth ] is an isomorphism .",
    "indeed , @xmath114 , @xmath115 yet @xmath116 in particular , we choose for basis of @xmath80 , the reciprocal image @xmath117 of the basis @xmath118 of @xmath102 by the evaluation map @xmath90 , namely @xmath119 .",
    "note that as the divisor @xmath79 is an effective divisor , we have @xmath121 .",
    "let @xmath122 be the map from @xmath55 to @xmath55 defined in the following way : if @xmath123 then @xmath124 is in the residue field @xmath102 of the place @xmath82 ; define @xmath125 where @xmath126 is the injection map from @xmath80 into @xmath55 .",
    "then @xmath122 is a linear map from @xmath55 into @xmath55 whose image is @xmath80 .",
    "more precisely , @xmath122 is a projection from @xmath55 onto @xmath80 .",
    "let @xmath127 be the kernel of @xmath122",
    ". then @xmath128    [ rem_proj ] from the definition of @xmath122 we remark that    1 .",
    "@xmath129 , 2 .   for any @xmath123",
    ", we have @xmath130 .    as @xmath131 ,",
    "the divisor @xmath132 is non - special and @xmath133 hence , we define as basis of @xmath55 , the basis @xmath134 defined by :    @xmath135 where @xmath136 is a basis of @xmath127 and @xmath137 is the basis of @xmath80 defined in section [ baseld ] .",
    "[ mainrem ] as a consequence of the choice of the basis @xmath138 for @xmath55 , if @xmath139 then @xmath140      in this section , we use as representation basis of spaces @xmath102 , @xmath80 , @xmath55 , the basis defined in section [ choixbases ] .",
    "the product of two elements in @xmath3 is computed by the algorithm of chudnovsky and chudnovsky .",
    "let @xmath141 and @xmath142 be two elements of @xmath3 given by their components over @xmath143 relative to the chosen basis @xmath111 . according to the previous notation",
    ", we can consider that @xmath100 and @xmath144 are identified to the following elements of @xmath80 : @xmath145 the product @xmath146 of the two elements @xmath147 and @xmath148 of @xmath80 is their product in the valuation ring @xmath81 .",
    "this product lies in @xmath55 .",
    "we will consider that @xmath100 and @xmath144 are respectively the elements @xmath147 and @xmath148 of @xmath55 where the @xmath89 last components are @xmath149 .",
    "now it is clear that knowing @xmath100 or @xmath147 by their coordinates is the same thing .",
    "let us consider the following hadamard product in @xmath150 : @xmath151    the product of @xmath100 by @xmath144 is such that @xmath152    indeed , from the definition of @xmath21 the following holds @xmath153 then @xmath154 by remark [ rem_proj ] we conclude @xmath155    we can now present the setup algorithm and the multiplication algorithm .",
    "note that the setup algorithm is only done once .",
    "@xmath157    1 .",
    "the elements @xmath100 of the field @xmath3 are known by their components relatively to a fixed basis : @xmath141 ( where @xmath158 ) .",
    "2 .   the function field @xmath159 , the place @xmath82 , the divisor @xmath79 and the points @xmath160 are as in theorem [ mainth ] .",
    "3 .   construct a basis @xmath161 of @xmath55 where @xmath162 is the basis of @xmath80 defined in section [ baseld ] and @xmath163 a basis of @xmath127 .",
    "any element @xmath164 in @xmath3 is identified to the element @xmath165 of @xmath80 .",
    "5 .   compute the matrices @xmath21 and @xmath166 .",
    "@xmath168    1 .   compute @xmath169 2 .",
    "compute @xmath170 where @xmath171 .",
    "3 .   compute @xmath172 .",
    "4 .   return@xmath173 ( remark that in the previous step we just have to compute the @xmath4 first components of @xmath174 ) .    in terms of number of multiplications in @xmath6 , the complexity of this multiplication algorithm is as follows : calculation of @xmath175 and @xmath176 needs @xmath177 multiplications , calculation of @xmath178 needs @xmath92 multiplications and calculation of @xmath174 needs @xmath179 multiplications .",
    "the total complexity is bounded by @xmath180 .",
    "the asymptotic analysis of our method needs to consider infinite families of algebraic function fields defined over @xmath6 with increasing genus ( or equivalently of algebraic curves ) having the required properties .",
    "the existence of such families follows from that of recursive towers of algebraic function fields of type garcia - stichtenoth @xcite reaching the drinfeld - vladut bound .",
    "note that , because of the drinfeld - vladut bound , in the case of small basis fields @xmath181 respectively @xmath182 with @xmath183 , we select the method of the quartic respectively quadratic embedding .",
    "in these cases , instead of the embeddings , it would be possible to use places of degree four and respectively two ( cf .",
    "@xcite and @xcite ) but it requires to generalize our algorithm and it generates significant complications ( cf .",
    "@xcite , @xcite ) .",
    "note also that , in the case of the quartic respectively quadratic embedding of the small fields , the operations are precomputed and do not increase the complexity",
    ".    then it is proved @xcite from a specialization of the original chudnovsky algorithm on these families that the bilinear complexity of the multiplication in any degree @xmath4 extension of @xmath6 is uniformly linear in @xmath1 with respect to @xmath4 .",
    "hence , the number of bilinear multiplications is in @xmath53 and the genus @xmath184 of the required curves also necessarily increases in @xmath53 .",
    "consequently , the total number of multiplications / additions / subtractions of our method is in @xmath61 and the total number of bilinear multiplications is in @xmath54 .    on some occasions in the paper",
    ", the cw algorithm @xcite will be used to decrease the number of scalar operations . given two square matrices of size @xmath4 , the product can be computed in @xmath185 multiplications where @xmath186 .",
    "in fact , if we consider a parallel version of the algorithm in the model s1 , a product can be performed in @xmath57 multiplications in @xmath6 using @xmath185 processors .",
    "this is a consequence of strassen s normal form theorem @xcite . in the model s2 where scalar multiplications and additions / subtractions have same costs",
    ", the depth becomes @xmath58 and a rescheduling technique @xcite allows the reduction of the width in @xmath187 .",
    "now , we focus on the parallel complexity of our multiplication algorithm .",
    "this actually consists of determining the parallel complexity of a constant number of matrix - vector products and the product coordinate - wise of two vectors .",
    "first , let us consider the ns model in which a round represents the time interval for a processor to perform one bilinear multiplication in @xmath182 ( scalar operations are considered as free ) , a multiplication can be carry out in a constant number of rounds with only @xmath54 processors , as stated in theorem  [ lns ] .    regarding the two other models , two cases have to be considered , the non amortized case and the amortized one . in the model s1 ( additions and subtractions in @xmath182",
    "are considered as free ) , if a round represents the time interval for a processor to perform one multiplication , the product can be performed in a constant number of rounds with @xmath61 processors , as stated in theorem  [ ls1]a .",
    "this width corresponds to the asymptotic number of scalar multiplications . in the model s2 where we take into account all scalar operations in @xmath182 ,",
    "a multiplication in @xmath188 can be performed in parallel time @xmath58 operations in @xmath182 using @xmath60 processors , as stated in theorem  [ ls2]a .    if we have @xmath189 multiplications in @xmath188 to perform , the width can be decreased ( in an amortized sense ) by using an optimized matrix multiplication method .",
    "it consists in grouping the operands in order that they become the columns of square matrices allowing the use of an efficient method like the cw one .",
    "more precisely , the method is as follows :    1 .",
    "store the @xmath190 operands ( vectors ) as columns in square matrices of size @xmath92 , denoted @xmath191 ; 2 .",
    "compute the products @xmath192 where @xmath193 represent the columns of @xmath194 , by using the coppersmith - winograd algorithm ( or another efficient method ) ; 3 .",
    "perform the @xmath195 bilinear products @xmath196 ; 4 .",
    "store the @xmath195 results as columns in square matrices , denoted @xmath197 ; 5 .",
    "compute the products @xmath198 with the cw method .",
    "the results are then stored in the columns of the resulting matrices .    in the model s1 , the necessary width to perform step 3 is in @xmath199 processors .",
    "the overall width is in fact dominated by steps 2 and 5 .",
    "all these products are performed in constant time using @xmath200 processors .",
    "this represents , in an amortized sense , @xmath201 processors per multiplication , as stated in theorem  [ ls1]b .    in the model s2 ,",
    "a multiplication in @xmath188 can be performed in parallel time @xmath58 using ( thanks to the cw matrix product ) @xmath202 processors , as stated in theorem  [ ls2]b .",
    "this last result is obtained using a rescheduling technique @xcite which allows the parallel computation of the matrix product to be done in @xmath58 operations in @xmath182 using @xmath203 processors , instead of @xmath204 processors without rescheduling @xcite .",
    "the previous algorithm can be iterated in order to obtain the product of three elements @xmath205 in @xmath80 ( or equivalently in @xmath3 ) .",
    "@xmath207    1 .   compute @xmath208 2 .",
    "compute @xmath209 , 3 .   then compute @xmath210 ( this is the hadamard product in @xmath150 ) , 4 .   and finally the result is @xmath211 .    in terms of number of multiplications in @xmath6 , the complexity of this algorithm",
    "is as follows : the matrix @xmath212 can be precomputed and the matrix - vector product needs @xmath213 multiplications .",
    "the total complexity is then @xmath214 including @xmath215 bilinear multiplications ( this number is almost doubled compare to the preceding algorithm ) .",
    "note that the precomputation of @xmath216 is of interest for the parallel computations .",
    "asymptotically , the complexity is the same as in the previous case .",
    "this algorithm will be used to construct our exponentiation algorithms in section  [ secexp ] .",
    "the form of the involved matrices is analysed in the next subsection .      obviously , the crucial part of the algorithm consuming more time is the iterative call to @xmath216 , namely the iterative call to the composition @xmath217 .",
    "the matrix @xmath21 is @xmath218 the matrix @xmath122 is @xmath219 where @xmath220 is the unit matrix of size @xmath221 .",
    "let us define the following blocks : @xmath222 where @xmath223 is a @xmath221-matrix , @xmath224 is a @xmath225-matrix , @xmath226 is a @xmath227-matrix and @xmath228 is a @xmath229-matrix . in the same way ,",
    "we can write @xmath166 in the following form : @xmath230    then @xmath231 moreover ,",
    "the following relations hold : @xmath232 @xmath233 @xmath234 @xmath235    an important problem , which is out of the scope of this paper , is to choose a basis @xmath236 of @xmath55 and places of degree one @xmath93 in order to obtain a `` simple '' matrix @xmath216 .",
    "indeed , a sparse matrix may reduce the number of multiplications .      having a matrix @xmath21 ( or @xmath216 )",
    ", the product @xmath237 for all possible @xmath238 can be efficiently precomputed .",
    "we choose an integer @xmath239 dividing @xmath4 and we set @xmath240 . for all @xmath241 such that @xmath242 ,",
    "we denote by @xmath243 a variable of the form @xmath244 , where : @xmath245    thus , we have @xmath246 .",
    "we can store in a table @xmath247 $ ] the products @xmath248 for the @xmath249 possible values of @xmath243 . after having stored all the precomputations in tables @xmath250)_{i=0 \\ldots k-1}$ ]",
    ", we can compute @xmath251 by evaluating @xmath252 + tab^t_1[y_{l,1 } ] + \\cdots + tab^t_{k-1}[y_{l , k-1}]$ ] .    as an example , for @xmath253 we can choose @xmath254 , leading to the storage of @xmath255 tables of @xmath256 values plus one table of @xmath257 values .",
    "in this section , our aim is to point out the interest , in terms of parallel running time and number of processors involved , of computing an exponentiation in finite fields ( _ i.e. _ @xmath70 in @xmath3 with @xmath258 ) based on our model .",
    "first , it is natural to consider the well known square and multiply algorithm ( with say , the method `` right - to - left '' ) .",
    "we describe this basic algorithm , showing the use of matrices @xmath21 , @xmath122 and @xmath216 . in a second time",
    ", we consider a more advanced algorithm , based on an idea from von zur gathen @xcite that we embed in our model .",
    "we show that this algorithm reaches optimal depth in terms of operations in @xmath6 .",
    "let @xmath46 be the unidimensional array of length @xmath259 containing the bits of @xmath41 .",
    "this array will be indexed by @xmath260 .",
    "more precisely @xmath2612^i.\\ ] ]    in order to bind operations we must iterate the use of the operator @xmath216 .",
    "we obtain algorithm [ sam_algo ] :    @xmath262 @xmath70 @xmath263 @xmath264 @xmath265 @xmath266 @xmath267 * return * @xmath268    within a same loop turn , operations under the condition `` if '' are not used in the subsequent operations .",
    "we consider two sets of processing units @xmath269 and @xmath270 , the set @xmath270 running the operations under the condition `` if '' while the set @xmath269 deals with the other calculations .",
    "we assume that the amounts of resources of @xmath269 and @xmath270 are the same . as an example , let us describe the steps in the calculation of @xmath271 .",
    "figure  [ exponentiation_wo_precomput ] depicts the operations made in parallel .",
    "we remark that at each step , the set @xmath269 or @xmath270 ( or both ) perform(s ) a vector product in @xmath272 and subsequently a matrix - vector product , except for the first and the last step for which only a matrix - vector product has to be performed . at step 0",
    ", the set @xmath269 ( or @xmath270 ) computes @xmath273 .",
    "the set @xmath269 computes @xmath274 at step @xmath275 . for its part , the set @xmath270 computes @xmath276 at step 2 , @xmath277 at step 3 and @xmath278 at step 4 .",
    "a last step is needed to retrieve the result @xmath279 .",
    "then we can remark that three products in @xmath272 are made in parallel , to which must be added the product performed by @xmath269 at the step 1 , for an overall computation time of four products in @xmath272 .",
    "we now consider operations over @xmath6 and the amount of processing units used . in the model ns ,",
    "the computation time is in @xmath54 with only @xmath54 processors . in the model s1 ,",
    "the computation time is in @xmath54 with @xmath61 processors and in the model s2 , the computation time is in @xmath280 with @xmath60 processors .      with the use of a normal basis",
    ", the base @xmath100 does not have to be fixed anymore .",
    "moreover , the number of multiplications in @xmath13 is reduced and it becomes possible to obtain an `` overall '' parallel time in @xmath58 without prior storage .    to make use of normal bases ,",
    "let us now consider the @xmath1-ary representation @xmath46 of the exponent ( composed of @xmath4 terms according to fermat s little theorem ) by writing @xmath281q^i=\\sum_{i=0}^{n-1 } \\sum_{j=0}^{t } k[i , j]2^jq^i,\\ ] ] where @xmath282 $ ] for @xmath283 are the bits of @xmath284 $ ] .",
    "let @xmath285 be the function such that @xmath286 right shifts @xmath241 times the vector @xmath100 .",
    "then , we can express @xmath70 as : @xmath287q^i } & = & \\prod_{i=0}^{n-1}\\sigma(x , i)^{k[i ] } & = & \\prod_{i=0}^{n-1}\\prod_{j=0}^{t}\\sigma(x , i)^{k[i , j]2^j}\\\\   & = & \\prod_{i=0}^{n-1}\\sigma(x^{k[i]},i ) & = & \\prod_{i=0}^{n-1}\\sigma\\left(\\prod_{j=0}^{t}x^{k[i , j]2^j},i\\right).\\end{aligned}\\ ] ] this gives two ways of rewriting the square and multiply algorithm .",
    "in fact , since we are no longer limited to the use of two ( sets of ) processors , there exist more efficient algorithms , as shown by von zur gathen @xcite .",
    "the idea is that short patterns might occur repeatedly in the @xmath288ary representation of the exponent @xmath41 .",
    "hence , precomputation of all patterns of a given short length @xmath289 allows the overall cost to be lower . by setting @xmath290 and writing @xmath291 with @xmath292 for all @xmath241",
    ", von zur gathen obtains optimal depth for an appropriate choice of @xmath25 .",
    "in particular it is shown that this result is reachable with width in @xmath293 processors .",
    "lee et al .",
    "@xcite introduced a rescheduling technique to reduce the number of processors at the counterpart of near - optimal depth . in",
    "what follows , we describe our own variant of von zur gathen algorithm , achieving the same asymptotical efficiencies than the one from lee et al . , while being simpler .",
    "we set @xmath294 and @xmath295 , and rewrite the exponent in the following form : @xmath296 thus , @xmath290 is in @xmath297 and @xmath298 is in @xmath58 .",
    "the algorithm is divided into five steps :    1 .   for @xmath299 ,",
    "compute @xmath300 , 2 .   for @xmath301 and @xmath302 , compute @xmath303 , 3 .   for @xmath301 ,",
    "compute @xmath304 , 4 .   for @xmath301 ,",
    "compute @xmath305 , 5 .",
    "return @xmath306 .",
    "we first examine the parallel time complexities in terms of multiplication in @xmath188 .",
    "we can use a binary tree of multiplications ( executed from root to leaves ) to compute step 1 in @xmath307 multiplications using @xmath308 processors .",
    "step 2 is free . in step 3 , each @xmath309 for @xmath301 can be computed by distinct processors in @xmath310 multiplications .",
    "step 4 is free .",
    "step 5 can be computed with a binary multiplication tree ( executed from leaves to root ) in @xmath311 multiplications using @xmath312 processors . by summing the times of each step",
    ", the overall depth can be upper bounded by @xmath313 for sufficiently large @xmath4 .",
    "moreover , by using an optimisation from von zur gathen , the parallel execution time of step 1 can be reduced to approximately @xmath314 .",
    "thus , the overall depth is in @xmath58 multiplications .",
    "it can be noticed that at each parallel step , the number of processors involved stays in @xmath297 .",
    "consequently , this overall depth is achieved with a width in @xmath297 processors .    from now on , we scale up the number of processors to optimize the running time in terms of operations in @xmath6 .",
    "when considering this algorithm in our chudnovsky model , we consider @xmath297 sets of @xmath54 processors if the scalar multiplications in @xmath182 are considered free .",
    "otherwise , we consider sets of @xmath61 processors .",
    "@xmath315 @xmath316 for @xmath317 @xmath318 @xmath319 @xmath320 @xmath320 * return * @xmath321    in terms of bilinear multiplications in @xmath12 , step 1 , described in algorithm [ precomputevzg ] , is performed in depth @xmath58 and width @xmath322 .",
    "step 2 and 4 consist in the shift of the first @xmath4 coordinates and are thus considered free .",
    "step 3 , the details of which are left to the reader , is performed in depth @xmath58 and width @xmath322 .",
    "the last step , which consists in computing @xmath323 , is a binary multiplication tree .",
    "it is executed for a cost of @xmath58 bilinear multiplications using @xmath322 processors .",
    "overall , in the model ns , this algorithm is done in depth @xmath58 , width @xmath322 , and size @xmath60 , as stated in theorem  [ lns ] .",
    "we let the reader deduce that , @xmath241 ) in the model s1 , in terms of multiplications in @xmath182 , this algorithm is done in depth @xmath58 , width @xmath324 , and size @xmath325 ; @xmath326 ) in the model s2 , in terms of any operations in @xmath182 , this algorithm is done in depth @xmath68 , width @xmath327 , and size @xmath325 .",
    "+    [ [ reducing - the - number - of - scalar - operations . ] ] * reducing the number of scalar operations . *",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we have seen that in the scalar model , the high number of processors is due to the high number of scalar operations ( in particular the matrix - vector products @xmath237 , @xmath328 and @xmath329 ) . at each step of the computation ,",
    "the number of multiplications in @xmath188 done in parallel is in @xmath297 in the worst case .",
    "thus , at a step of the computation involving multiple parallel matrix - vector products , instead of performing @xmath297 separate matrix - vector products , we write the @xmath297 vectors as columns of a matrix @xmath330 , then complete this matrix with zero columns in order to obtain a square matrix .",
    "now , we have a product of two square matrices that can be performed using the coppersmith - winograd method , thus reducing the number of scalar operations . in the s1 model",
    ", the coppersmith - winograd product can be performed in @xmath57 multiplications in @xmath6 using @xmath204 processors .",
    "this optimization allows the exponentiation to be done in depth @xmath58 , width @xmath204 and size @xmath331 . in the model s2",
    ", the coppersmith - winograd product can be performed in @xmath58 operations in @xmath6 using @xmath203 processors .",
    "the optimization allows the exponentiation to be done in depth @xmath68 , width @xmath203 and size @xmath331 .",
    "remark that the number of zero columns in the matrix @xmath330 may not be negligible .",
    "thus , instead of filling @xmath330 with zero columns in order to obtain a square matrix , we can slightly improve the complexity by using the cw method in the following way : consider a matrix @xmath330 containing only @xmath332 vectors .",
    "let us denote by @xmath333 the number of columns of @xmath330 ( @xmath333 is then in @xmath332 ) .",
    "we partition @xmath330 in square submatrices of size @xmath333 so that @xmath330 is seen as a column @xmath334 of blocks ( with @xmath335 in @xmath68 ) . in the same way , we partition @xmath216 in square submatrices of size @xmath333 so that a row @xmath241 of blocks of @xmath216 is represented as @xmath336 .",
    "the sum of products @xmath337 corresponds to the @xmath241-th block of the resulting column of blocks .",
    "a product @xmath338 is done using cw in @xmath339 scalar multiplications .",
    "since we have @xmath340 such products to compute @xmath341 , the overall number of scalar multiplications is in @xmath342 .",
    "consequently , in the model s1 the product @xmath341 can be computed in @xmath57 multiplications in @xmath182 using @xmath342 processors , whereas in the model s2 , it can be computed in @xmath58 operations in @xmath182 using @xmath343 processors .",
    "we can substitute these last results in the case of the parallel exponentiation to obtain the stated complexities of theorem  [ ls1 ] and theorem  [ ls2 ] , with the current exponent for the best optimized matrix product : in the model s1 , @xmath70 can be computed in @xmath58 multiplications in @xmath182 using @xmath342 processors for a size of @xmath344 multiplications in @xmath182 , whereas in the model s2 , @xmath70 can be computed in @xmath68 operations in @xmath182 using @xmath343 processors for a size of @xmath344 operations in @xmath182 .",
    "set @xmath345 and @xmath346 . from now on",
    ", @xmath87 denotes the algebraic function field associated to the hyper elliptic curve @xmath347 with plane model @xmath348 , of genus two .",
    "this curve has 33 rational points , which is maximal over @xmath6 according to the hasse - weil bound .",
    "we represent @xmath349 as the field @xmath350/(p(x))$ ] where @xmath351 is the irreducible polynomial @xmath352 and @xmath353 denotes a primitive root of @xmath352 .",
    "let us give the projective coordinates @xmath354 of rational points of the curve @xmath347 :    @xmath355        it is sufficient to take a place @xmath356 of degree @xmath4 in the rational function field @xmath357 , which totally splits in @xmath87 .",
    "it is equivalent to choose a monic irreducible polynomial @xmath358 $ ] of degree",
    "n such that its roots @xmath359 in @xmath13 satisfy @xmath360 for @xmath361 where the map @xmath362 denotes the classical function trace over @xmath363 by ( * ? ? ?",
    "* theorem 2.25 ) .",
    "in fact , it is sufficient to verify that this property is satisfied for only one root since a finite field is galois .",
    "moreover , in the context of our method , we require that this irreducible polynomial @xmath103 corresponds to a normal polynomial ( cf .",
    "section [ basefq ] ) .",
    "for example , for the extension @xmath364 , we choose the primitive normal polynomial    @xmath365    let @xmath366 be one primitive root of @xmath103 .",
    "it is easy to check that @xmath367 , hence the place @xmath368 of @xmath369 is totally splitted in the algebraic function field @xmath87 , which means that there exist two places of degree @xmath4 in @xmath87 lying over the place @xmath368 of @xmath369 . for the place @xmath82 of degree @xmath4 in the algebraic function field @xmath87",
    ", we consider one of the two places in @xmath87 lying over the place @xmath368 of @xmath369 , namely the orbit of the @xmath0-rational point @xmath370 where @xmath359 is a root of @xmath103 and @xmath371 for @xmath372 .",
    "notice that the second place is given by the conjugated points @xmath373 for @xmath372 .      for the divisor @xmath79 of degree @xmath89 , we choose a place @xmath79 of degree 14 according to the method used for the place @xmath82 .",
    "we consider the orbit of the @xmath374-rational point @xmath375 where @xmath376 is a root of @xmath377 and @xmath378 for @xmath379 .",
    "notice that the second place is given by the conjugated points @xmath380 for @xmath379 .    the place @xmath82 and the divisor @xmath79 satisfy the good properties since the dimension of the divisor @xmath105 is zero which means that the divisor @xmath105 is non - special of degree @xmath106 .",
    "we choose as basis of the residue class field @xmath102 the normal basis @xmath381 associated to the place @xmath82 obtained in section [ q13 ] .",
    "we choose as basis of the riemann - roch space @xmath113 the basis @xmath382 such that @xmath383 is a normal basis of @xmath102 as in section [ baseld ] .",
    "any element @xmath384 of @xmath385 is such that @xmath386 where @xmath387 $ ] . to simplify , we set @xmath388 .",
    "let us give the elements of @xmath385 :    @xmath389 , + @xmath390 , + @xmath391 , + @xmath392 , + @xmath393 , + @xmath394 , + @xmath395 , + @xmath396 , + @xmath397 , + @xmath398 , + @xmath399 , + @xmath400 , + @xmath401 , +      let @xmath402 be a basis of @xmath120 as defined in section [ basel2d ] . since the divisor @xmath79 is effective , we can complete the basis @xmath403 of @xmath113 in @xmath120",
    ". then any element @xmath404 of @xmath184 is such that @xmath405 for @xmath361 and @xmath406 ( which we will denote @xmath407 ) , with @xmath408 $ ] for @xmath409 .",
    "moreover , we require the completion of @xmath113 by the kernel of the map @xmath122 .",
    "hence , the set @xmath410 is a basis of the kernel of the restriction map over @xmath120 of the canonical projection from the valuation ring of the place @xmath82 in its residue class field @xmath102 .",
    "@xmath411 , + @xmath412 , + @xmath413 , + @xmath414 , + @xmath415 , + @xmath416 , + @xmath417 , + @xmath418 , + @xmath419 , + @xmath420 , + @xmath421 , + @xmath422 , + @xmath423 , + @xmath424 .",
    "....   // construction of the function field   n:=13 ; g:=2 ; q:=16 ; f16<a>:=gf(16 ) ;   kx < x > : = functionfield(f16 ) ;    r < x>:=polynomialring(f16 ) ;   kxy < y > : = polynomialring(kx ) ;   f:=y^2 + y + x^5 ;   f < c > : = functionfield(f ) ;   // construction of the place q and divisor d   q : = x^13+a^6*x^12+a^5*x^11+a^11*x^10+x^9+a^12*x^8+a^7*x^7 +   a^7*x^5+a^2*x^4+a^11*x^3+a^8*x^2+a^6*x+a^14 ;   dd:=x^14+a^9*x^13+a^6*x^12+a^7*x^11+a^11*x^10+a^12*x^9 +    a^10*x^8+a^6*x^7+a^7*x^6+a^10*x^5+a^14*x^4+x^3+x^2+a^3*x+a ;    p:=decomposition(f , zeros(kx!q)[1 ] ) ; q:=p[1 ] ;   d:=decomposition(f , zeros(kx!dd)[1])[1 ] ; d:=1d ;   isspecial(1d ) ; // false   dimension(q - d ) ; //   0   isspecial(d - q ) ; // false   //",
    "construction of the residue class field    // and the degree one places   k < b>:=residueclassfield(q ) ;                                                                                            lp:=places(f,1 ) ;       // construction of the riemann space   ld , h : = riemannrochspace(d ) ;   l2d , h2 : = riemannrochspace(2d ) ;   baseld:=[(h(v))@@h2 : v in basis(ld ) ] ;   base:=extendbasis(baseld , l2d ) ;   l2d:= [ ] ;    for i in[1 .. 2*n+g-1 ] do      l2d:=append(l2d , h2(base[i ] ) ) ;    end for ;   ml2d:=matrix(2*n+g-1,1,l2d ) ;   // construction of e : e = evalf(q )   l:= [ ] ;    for i in [ 1 .. n ] do      l:=append(l , elementtosequence(evaluate(l2d[i],q ) ) ) ;   end for ;   e:=transpose(matrix(l ) ) ;   // we use a normal basis    pp:= [ ] ;    for i:=0 to n-1 do      pp:=append(pp , elementtosequence(b^(q^i ) ) ) ;    end for ;   nc:=transpose(matrix(f16,n , n , pp ) ) ; nci:=nc^-1 ;   //   x.m = fx      m:=matrix(f , e^-1*nc ) ;   ev:=matrix(1,n,[l2d[i ] : i in [ 1 .. n ] ] ) ;   //",
    "construction of l(2d ) with the required properties   el2d:=matrix(2*n+g-1,1 ,         [ evaluate(l2d[i],q ) : i in [ 1 .. 2*n+g-1 ] ] ) ;   bel2d:=matrix(f16,2*n+g-1,n ,          [ elementtosequence(el2d[i][1 ] ) : i in [ 1 .. 2*n+g-1 ] ] ) ;   mm:=parent(zeromatrix(f , n+g-1,2n+g-1 ) ) !",
    "matrix(basis(nullspace(bel2d)))*ml2d ;      for i in [ 1 .. n ] do      l2d[i]:=transpose(evm)[i,1 ] ;    end for ;    //",
    "rows of ml2d form a basis of l(2d )                     ml2d:=matrix(2*n+g-1,1 ,         [ l2d[i ] : i in [ 1 .. n ] ] cat [ mm[i,1 ] : i in [ 1 .. n+g-1 ] ] ) ;   // we can check that evaluation in    //",
    "q of the last n+g-1 gives 0 :   //",
    "[ evaluate(ml2d[i,1],q ) : i in [ 1 .. 2*n+g-1 ] ] ;     // construction of t and t^-1   st:= [ ] ;    for j:=1 to 2*n+g-1 do      for i:=1 to 2*n+g-1 do        st:=append(st , evaluate(ml2d[i,1 ] , lp[j ] ) ) ;     end for ;    end for ;   t:=matrix(2*n+g-1 , st ) ;    ti:=t^-1 ;   // construction of p   p:=verticaljoin(horizontaljoin(scalarmatrix(f16,n,1 ) ,                           zeromatrix(f16,n , n+g-1 ) ) ,   zeromatrix(f16,n+g-1,2*n+g-1 ) ) ;   // matrix t1",
    "t1:=t*p*ti ;   // x and y are the elements to multiply   // represented in a normal basis   x:=matrix(f16,13,1,[a,1,0,0,0,0,0,0,0,0,0,0,0 ] ) ; //",
    "example   y:=matrix(f16,13,1,[1,a , a,0,0,0,0,0,0,0,0,0,0 ] ) ; //",
    "example   fx:=verticaljoin(x , zeromatrix(f16,n+g-1,1 ) ) ;   fy:=verticaljoin(y , zeromatrix(f16,n+g-1,1 ) ) ;   //    u = t(fx)t(fy )    u:= matrix(2*n+g-1,1 ,            [ ( tfx)[i][1]*(tfy)[i][1 ] : i in [ 1 .. 2*n+g-1 ] ] ) ;   //",
    "fz = mm(p*ti*u )    fz:=matrix(n,1,[(p*ti*u)[i][1 ] : i in [ 1 .. n ] ] ) ;   //",
    "fz gives x*y in the normal basis ....",
    "d.  coppersmith and s.  winograd .",
    "matrix multiplication via arithmetic progressions . in _ proceedings of the nineteenth annual acm symposium on theory of computing _",
    ", stoc 87 , pages 16 , new york , ny , usa , 1987 . acm ."
  ],
  "abstract_text": [
    "<S> thanks to a new construction of the so - called chudnovsky - chudnovsky multiplication algorithm , we design efficient algorithms for both the exponentiation and the multiplication in finite fields . they are tailored to hardware implementation and they allow computations to be parallelized while maintaining a low number of bilinear multiplications . </S>",
    "<S> we give an example with the finite field @xmath0 . </S>"
  ]
}