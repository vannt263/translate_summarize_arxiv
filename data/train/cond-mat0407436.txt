{
  "article_text": [
    "strong feedforward , feedback , and lateral connections exist between distinct areas of the cerebral cortex , but such connections are not observed in cerebellar , sensory , or motor output circuits .",
    "the anatomical structure of the cerebral cortex may facilitate a modular approach to solving complex problems @xcite , with different cortical areas being specialized for different information processing tasks . to permit a modular strategy of this sort , coordinated and efficient routing of information",
    "must be maintained between modules , which in turn demands extensive connections throughout the cortex .",
    "it has been proposed @xcite that cortical circuits perform statistical inference , encoding and processing information about analog variables in the form of probability density functions ( pdfs ) .",
    "this hypothesis provides a theoretical framework for understanding diverse results of neurobiological experiments , and a practical framework for the construction of recurrent neural network models that implement a broad variety of information - processing functions @xcite .",
    "probabilistic formulations of neural information processing have been explored along a number of avenues .",
    "one of the earliest such analyses showed that the original hopfield neural network implements , in effect , bayesian inference on analog quantities in terms of pdfs @xcite .",
    "as in the present work , zemel _ et al . _",
    "@xcite have investigated population coding of probability distributions , but with different representations and dynamics than those we will consider here .",
    "several extensions of this representation scheme have been developed @xcite that feature information propagation between interacting neural populations .",
    "additionally , several `` stochastic machines '' @xcite have been formulated , including boltzmann machines @xcite , sigmoid belief networks @xcite , and helmholtz machines @xcite .",
    "stochastic machines are built of stochastic neurons that occupy one of two possible states in a probabilistic manner .",
    "learning rules for stochastic machines enable such systems to model the underlying probability distribution of a given data set .",
    "the putative modular nature of cortical processing fits well in such a probabilistic framework .",
    "cortical areas collectively represent the joint pdf over several variables .",
    "these neural `` problem - solving modules '' can be mapped in a relatively direct fashion onto the nodes of a bayesian belief network , giving rise to a class of neural network network models that we have termed _ neural belief networks _ @xcite .",
    "in contrast , recent work based on population - temporal coding @xcite indicates that the modeling of low - level sensory processing and output motor control do not require such a sophisticated representation : manipulation of mean values instead of pdfs is generally sufficient .",
    "further , the representations can be simplified to deal with vector spaces describing the mean values instead of function spaces describing the probability density functions .    in this work ,",
    "we develop neural networks processing mean values of analog variables as a specialized form of the more general neural belief networks .",
    "we begin with a brief summary of the key relevant properties of bayesian belief networks in section  [ sec : bbns ] .",
    "we describe a procedure for generating and evaluating the neural networks in section  [ sec : mvbns ] , and apply the procedure to several examples in section  [ sec : applications ] .",
    "bayesian belief networks @xcite are directed acyclic graphs that represent probabilistic models ( fig .  [",
    "fig : bbn ] ) .",
    "each node represents a random variable , and the arcs signify the presence of direct causal influences between the linked variables .",
    "the strengths of these influences are defined using conditional probabilities .",
    "the direction of a particular link indicates the direction of causality ( or , more simply , relevance ) ; an arc points from cause to effect .",
    "multiple sources of evidence about the random variables are conveniently handled using bbns .",
    "the belief , or degree of confidence , in particular values of the random variables is determined as the likelihood of the value given evidentiary support provided to the network .",
    "there are two types of support that arise from the evidence : predictive support , which propagates from cause to effect along the direction of the arc , and retrospective support , which propagates from effect to cause , opposite to the direction of the arc .",
    "bayesian belief networks have two properties that we will find very useful , both of which stem from the dependence relations shown by the graph structure .",
    "first , the value of a node @xmath0 is not dependent upon all of the other graph nodes .",
    "rather , it depends only on a subset of the nodes , called a markov blanket of @xmath0 , that separates node @xmath0 from all the other nodes in the graph .",
    "the markov blanket of interest to us is readily determined from the graph structure .",
    "it is comprised of the union of the direct parents of @xmath0 , the direct successors of @xmath0 , and all direct parents of the direct successors of @xmath0 .",
    "second , the joint probability over the random variables is decomposable as @xmath1 where @xmath2 denotes the ( possibly empty ) set of direct - parent nodes of @xmath3 .",
    "this decomposition comes about from repeated application of bayes rule and from the structure of the graph .",
    "we will develop neural networks from the set of marginal distributions @xmath4 so as to best match a desired probabilistic model @xmath5 over the set of random variables , which are organized as a bbn .",
    "one or more of the variables @xmath6 must be specified as evidence in the bbn . to facilitate the development of general update rules",
    ", we do not distinguish between evidence and non - evidence nodes in our notation .",
    "our general approach will be to minimize the difference between a probabilistic model @xmath7 and an estimate of the probabilistic model @xmath8 .",
    "for the estimate , we utilize @xmath9 this is a so - called naive estimate , wherein the random variables are assumed to be independent .",
    "we will place further constraints on the probabilistic model and representation to produce neural networks with the desired dynamics .",
    "the first assumption we make is that the populations of neurons only need to accurately encode the mean values of the random variables , rather than the complete pdfs .",
    "we take the firing rates of the neurons representing a given random variable @xmath10 to be functions of the mean value @xmath11(t ) ( fig .",
    "[ fig : firingrates ] ) @xmath12 where @xmath13 and @xmath14 are parameters describing the response properties of neuron @xmath15 of the population representing random variable @xmath16 .",
    "the activation function @xmath17 is in general nonlinear ; in this work , we take @xmath17 to be the logistic function , @xmath18 we can make use of  ( [ eq : neurresponses ] ) to directly encode mean values into neural activation states , providing a means to specify the value of the evidence nodes in the nbn .    using  ( [ eq : neurresponses ] )",
    ", we derive an update rule describing the neuronal dynamics , obtaining ( to first order in @xmath19 ) @xmath20 thus , if we can determine how @xmath11 changes with time , we can directly determine how the neural activation states change with time .",
    "the mean value @xmath21 can be determined from the firing rates as the expectation value of the random variable @xmath10 with respect to a pdf @xmath22 represented in terms of some decoding functions @xmath23 the pdf is recovered using the relation @xmath24 the decoding functions are constructed so as to minimize the difference between the assumed and reconstructed pdfs ( discussed in detail in @xcite ) .    with representations as given in ( [ eq : decodingrule ] ) , we have @xmath25 where we have defined @xmath26 although we used the decoding functions @xmath27 to calculate the parameters @xmath28 , they can in practice be found directly so that the relations in ( [ eq : neurresponses ] ) and ( [ eq : expectvalues ] ) are mutually consistent .",
    "we take the pdfs @xmath22 to be normally distributed with the form @xmath29 .",
    "intuitively , we might expect that the variance @xmath30 should be small so that the mean value is coded precisely , but we will see that the variances have no significance in the resulting neural networks .",
    "the second assumption we make is that interactions between the nodes are linear : @xmath31 utilizing the causality relations given by the bayesian belief network , we require that @xmath32 only if @xmath33 is a child node of @xmath10 in the network graph . to represent the linear interactions as a probabilistic model , we take the normal distributions @xmath34 for the conditional probabilities .    for nodes in the bbn which have no parents ,",
    "the conditional probability @xmath35 is just the prior probability distribution @xmath36 .",
    "we utilize the same rule to define the prior probabilities as to define the conditional probabilities . for parentless nodes ,",
    "the prior is thus normally distributed with zero mean , @xmath37 .",
    "we use the relative entropy @xcite as a measure of the `` distance '' between the joint distribution describing the probabilistic model @xmath38 and the pdf estimated from the neural network @xmath39 .",
    "thus , we minimize @xmath40 with respect to the mean values @xmath11 . by making use of the gradient descent prescription @xmath41 and the decomposition property for bbns given by ( [ eq : bbndecompose ] )",
    ", we obtain the update rule for the mean values , @xmath42 because the coupling parameters @xmath43 are nonzero only when @xmath10 is a parent of @xmath33 , generally only a subset of the mean values contributes to updating @xmath44 in  ( [ eq : finalupdate ] ) . in terms of the belief network graph structure ,",
    "the only contributing values come from the parents of @xmath45 , the children of @xmath45 , and the parents of the children of @xmath45 ; this is identical to the markov blanket discussed in section  [ sec : bbns ] .",
    "the update rule for the neural activities is obtained by combining ( [ eq : updaterule ] ) , ( [ eq : expectvalues ] ) , and ( [ eq : finalupdate ] ) , resulting in @xmath46 the quantity @xmath47 serves to stabilize the activities of the neurons representing @xmath48 ( similar to neural integrator models @xcite ) , while @xmath49 drives changes in @xmath50 based on the pdfs represented by other nodes of the bbn .",
    "the synaptic weights of the neural network are @xmath51    the foregoing provides an algorithm for generating and evaluating neural networks that process mean values of random variables . to summarize ,    1 .",
    "establish independence relations between model variables .",
    "this may be accomplished by using a graph to organize the variables .",
    "2 .   specify the @xmath43 to quantify the relations between the variables .",
    "3 .   assign network inputs by encoding desired values into neural activities using  ( [ eq : neurresponses ] ) .",
    "4 .   update other neural activities using ( [ eq : finalupdate ] ) .",
    "extract the expectation values of the variables from the neural activities using ( [ eq : expectvalues ] ) .",
    "as a first example , we apply the algorithm to the bbn shown in fig .  [ fig : bbn ] , with firing rate profiles as shown in fig .  [ fig : firingrates ] . specifying @xmath52 and @xmath53 as evidence , we find an excellent match between the mean values calculated by the neural network and the directly calculated values for the remaining nodes ( table  [ table : comparison ] ) .",
    "we next focus on some simpler bbns to highlight certain properties of the resulting neural networks ( which will again utilize the firing rate profiles shown in fig .",
    "[ fig : firingrates ] ) . in fig .",
    "[ fig : trees ] , we present two bbns that relate three random variables in different ways .",
    "the connection strengths are all taken to be unity in each graph , so that @xmath54 .    with the connection strengths so chosen , the two bbns have straightforward interpretations . for the graph shown in fig .",
    "[ fig : trees]a , @xmath55 represents the sum of @xmath56 and @xmath57 , while , for the graph shown in fig .",
    "[ fig : trees]b , @xmath55 provides a value which is duplicated in @xmath56 and @xmath57 .",
    "the different graph structures yield different neural networks ; in particular , nodes @xmath56 and @xmath57 have direct synaptic connections in the neural network based on the graph in fig .",
    "[ fig : trees]a , but no such direct weights exist in a second network based on fig .",
    "[ fig : trees]b .",
    "thus , specifying @xmath58 and @xmath59 for the first network produces the expected result @xmath60 , but specifying @xmath59 in the second network produces @xmath61 regardless of the value ( if any ) assigned to @xmath62 .    to further illustrate the neural network properties",
    ", we use the graph shown in fig .  [",
    "fig : trees]b to process inconsistent evidence . nodes @xmath56 and @xmath57 should copy the value in node @xmath55 , but we can specify any values we like as network inputs .",
    "for example , when we assign @xmath63 and @xmath64 , the neural network yields @xmath65 for the remaining value .",
    "this is a typical and reasonable result , matching the least - squares solution to the inconsistent problem .",
    "we have introduced a class of neural networks that consistently mix multiple sources of evidence .",
    "the networks are based on probabilistic models , represented in the graphical form of bayesian belief networks , and function based on traditional neural network dynamics ( i.e. , a weighted sum of neural activation values passed through a nonlinear activation function ) .",
    "we constructed the networks by restricting the represented probabilistic models by introducing two auxiliary assumptions .",
    "first , we assumed that only the mean values of the random variables need to be accurately represented , with higher order moments of the distribution being unimportant .",
    "we introduced neural representations of relevant probability density functions consistent with this assumption .",
    "second , we assumed that the random variables of the probabilistic model are linearly related to one another , and chose appropriate conditional probabilities to implement these linear relationships .    using the representations suggested by our auxiliary assumptions , we derived a set of update rules by minimizing the relative entropy of an assumed pdf with respect to the pdf decoded from the neural network . in a straightforward fashion ,",
    "the optimization procedure yields neural weights and dynamics that implement specified probabilistic relations , without the need for a training process .",
    "the restricted class of neural belief networks investigated in this work captures many of the properties of both bayesian belief networks and neural networks .",
    "in particular , multiple sources of evidence are consistently pooled based on local update rules , providing a distributed version of a probabilistic model .",
    "this work was supported in part by the u. s. national science foundation under grant phy-0140316 and in part by the portuguese fundao para a cincia e a technologia ( fct ) under bolsa sfrh / bpd/9417/2002 ) .",
    "jwc also acknowledges support received from the fundao luso - americana para o desenvolvimento ( flad ) and from the fct for his participation in madeira math encounters xxiii at the university of madiera , where portions of the work were conducted .",
    "is a parent of @xmath66 and a child of both @xmath67 and @xmath68 . from the structure of the graph",
    ", we can see the conditional independence relations in the probabilistic model .",
    "for example , @xmath66 is independent of @xmath69 and @xmath67 given @xmath68 and @xmath70 . ]",
    "are fully determined by a single input @xmath71 , which we interpret as the mean value of a pdf .",
    "the form of the neuronal transfer functions can be altered without affecting the general result presented in this work . , width=312 ]    .the mean values decoded from the neural network closely match the values directly calculated from the linear relations .",
    "the coefficients for the linear combinations were randomly selected , with values @xmath72 , @xmath73 , @xmath74 , @xmath75 , @xmath76 , and @xmath77 . [ cols=\"^,^,^\",options=\"header \" , ]"
  ],
  "abstract_text": [
    "<S> we introduce a class of neural networks derived from probabilistic models in the form of bayesian belief networks . by imposing additional assumptions about the nature of the probabilistic models represented in the belief networks , we derive neural networks with standard dynamics that require no training to determine the synaptic weights , that can pool multiple sources of evidence , and that deal cleanly and consistently with inconsistent or contradictory evidence . </S>",
    "<S> the presented neural networks capture many properties of bayesian belief networks , providing distributed versions of probabilistic models . </S>"
  ]
}