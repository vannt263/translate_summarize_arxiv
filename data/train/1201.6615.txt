{
  "article_text": [
    "in this paper , we address the problem of approximating the value function under a stationary policy @xmath0 for a continuous state space @xmath1 , @xmath2 using a linear approximation of the form @xmath3 to represent @xmath4 . here",
    "@xmath5 denotes the state , @xmath6 the scalar reward and @xmath7 the discount factor .",
    "given a trajectory of states @xmath8 and rewards @xmath9 sampled under @xmath0 , the goal is to determine weights @xmath10 ( and basis functions @xmath11 ) such that @xmath12 is a good approximation of @xmath4 .",
    "this is the fundamental problem arising in the policy iteration framework of infinite - horizon dynamic programming and reinforcement learning ( rl ) , e.g. see @xcite .",
    "unfortunately , this problem is also a very difficult problem that , at present , has no completely satisfying solution . in particular , deciding which features ( basis functions @xmath11 )",
    "to use is rather challenging , and in general , needs to be done manually : thus it is tedious , prone to errors , and most important of all , requires considerable insight into the domain .",
    "hence , it would be far more desirable if a learning system could automatically choose its own representation . in particular ,",
    "considering efficiency , we want to adapt to the actual difficulties faced , without wasting resources : often , there are many factors that can make a particular problem easier than it initially appears to be , for example , when only a few of the inputs are relevant , or when the input data lies on a low - dimensional submanifold of the input space .",
    "recent work in applying nonparametric function approximation to rl , such as gaussian processes ( gp ) @xcite , or equivalently , regularization networks @xcite , is a very promising step in this direction . instead of having to explicitly specify individual basis functions",
    ", we only have to specify a more general kernel that just depends on a very small number of hyperparameters .",
    "the key contribution of this paper is to demonstrate that feature selection in rl from sample transitions can be automated , using any of several possible model selection methods for these hyperparameters , such as marginal likelihood optimization in a bayesian setting , or leave - one - out ( loo ) error minimization in a frequentist setting . here",
    ", we will focus on the bayesian setting , and adapt marginal likelihood optimization for the gp - based approximate policy evaluation method gptd , introduced without model selection in @xcite .",
    "overall , this will have the following benefits : first , only by automatic model selection ( as opposed to a grid - based search or manual tweaking of kernel parameters ) will we be able to use more sophisticated kernels , which will allow us to uncover the `` hidden '' properties of given problem .",
    "for example , by choosing an rbf kernel with independent lengthscales for the individual dimensions of the state space , model selection will automatically drive those components to zero that correspond to state variables irrelevant ( or redundant ) to the task",
    ". this will allow us to concentrate our computational efforts on the parts of the input space that really matter and will improve computational efficiency .",
    "second , because it is generally easier to learn in `` smaller '' spaces , it may also benefit generalization and thus help us to reduce sample complexity .    despite its many promises , previous work with gps in rl",
    "rarely explores the benefits of model selection : in @xcite , a variant of stochastic search was used to determine hyperparameters of the covariance for gptd using as score function the online performance of an agent . in @xcite , standard gps with marginal likelihood based model selection were employed ; however , since their approach was based on fitted value iteration , the task of value function approximation was reduced to ordinary regression .",
    "the remaining paper is structured as follows : section  2 - 3 contain background information and summarize the gptd framework .",
    "as one of the benefits of model selection is the reduction of computational complexity , section  4 describes how gptd can be solved for large - scale problems using sr - approximation .",
    "section  5 introduces model selection for gptd and derives in detail the associated gradient computation .",
    "finally , section  6 illustrates our approach by providing experimental results .",
    "the overall goal of learning representations and feature selection for linearly parameterized @xmath12 is not new within the context of rl .",
    "roughly , past methods can be categorized along two dimensions : how the basis functions are represented ( e.g. either by parameterized and predefined basis functions such as rbf , or by nonparameterized basis functions directly derived from the data ) and what quantity / target function is considered to guide their construction process ( e.g. either supervised methods that consider the bellman error and depend on the particular reward / goal , or unsupervised graph - based methods that consider connectivity properties of the state space ) . conceptually closely related to our work is the approach described in @xcite , which adapts the hyperparameters of rbf - basis functions ( both their location and lengthscales ) using either gradient descent or the cross - entropy method on the bellman error .",
    "however , because basis functions are adapted individually ( and their number is chosen in advance ) , the method is prone to overfitting : e.g. by placing basis functions with very small width near discontinuities .",
    "the problem is compounded when only few data points are available .",
    "in contrast , using a bayesian approach , we can automatically trade - off model fit and model complexity with the number of data points , choosing always the best complexity : e.g. for small data sets we will prefer larger lengthscales ( less complex ) , for larger data sets we can afford smaller lengthscales ( more complex ) .    other alternative approaches do not rely on predefined basis functions : the method in @xcite is an incremental approach that uses dimensionality reduction and state aggregation to create new basis functions such that for every step the remaining bellman error for a trajectory of states is successively reduced .",
    "a related approach is given in @xcite which incrementally constructs an orthogonal basis for the bellman error .",
    "a graph - based unsupervised approach is presented in @xcite , which derives basis functions from the eigenvectors of the graph laplacian induced from the underlying mdp .",
    "in this section we briefly summarize how gps @xcite can be used for approximate policy evaluation ; here we will follow the gptd formulation of @xcite .",
    "suppose we have observed the sequence of states @xmath13 and rewards @xmath9 , where @xmath14 and @xmath15 . in practice , mdps considered in rl will often be of an episodic nature with absorbing terminal states .",
    "therefore we have to transform the problem such that the resulting markov chain is still ergodic : this is done by introducing a zero reward transition from the terminal state of one episode to the start state of the next episode . in addition to the sequence of states and rewards our training data thus also includes a sequence @xmath16 , where @xmath17 ( the discount factor in eq .  ) if @xmath18 was a non - terminal state , and @xmath19 if @xmath20 was a terminal state ( in which case @xmath18 is the start state of the next episode ) .",
    "assume that the function values @xmath21 of the unknown value function @xmath22 from eq .",
    "form a zero - mean gaussian process with covariance function @xmath23 for @xmath24 ; in short @xmath25 . in consequence ,",
    "the function values for the @xmath26 observed states , @xmath27 , will have a gaussian distribution @xmath28 where @xmath29 $ ] and @xmath30 is the @xmath31 covariance matrix with entries @xmath32_{ij}=k({\\mathbf x}_i,{\\mathbf x}_j)$ ] .",
    "note that the covariance @xmath33 alone fully specifies the gp ; here we will assume that it is a simple ( positive definite ) function parameterized by a number of scalar parameters collected in vector @xmath34 ( see section  4 ) .",
    "however , unlike in ordinary regression , in rl we can not observe samples from the target function @xmath35 directly . instead",
    ", the values can only be observed indirectly : from eq .",
    "we have that the value of one state is recursively defined through the value of the successor state(s ) and the immediate reward . to this end , engel et al .",
    "propose the following generative model : . ]",
    "@xmath36 where @xmath37 is a noise term that may depend on the inputs .",
    "plugging in the observed training data , and defining @xmath38 , we obtain @xmath39 where the @xmath40 matrix @xmath41 is given by @xmath42 and noise @xmath43 has distribution @xmath44 . one first choice for the noise covariance @xmath45 would be @xmath46 , where @xmath47 is an unknown hyperparameter ( see section  4 ) .",
    "however , this model does not capture stochastic state transitions and hence would only be applicable for deterministic mdps . if the environment is stochastic , the noise model @xmath48 is more appropriate , see @xcite for more detailed explanations . for the remainder we will solely consider the latter choice , i.e. @xmath48 .",
    "let @xmath49 be an abbreviation for the training inputs .",
    "using eq .  , it can be shown that the joint distribution of the observed rewards @xmath50 given inputs @xmath51 is again a gaussian , @xmath52 where the @xmath53 covariance matrix @xmath54 is given by @xmath55    to predict the function value @xmath56 at a new state @xmath57 , we consider the joint distribution of @xmath50 and @xmath56 @xmath58{^\\text{\\sffamily t } } & k^ * \\end{bmatrix } \\right)\\ ] ] where @xmath59 vector @xmath60 is given by @xmath61 and scalar @xmath62 by @xmath63 . conditioning on @xmath50 ,",
    "we then obtain @xmath64 where @xmath65 thus , for any given single state @xmath57 , gptd produces the distribution @xmath66 in eq .   over function values .",
    "regarding its implementation , gptd for policy evaluation shares the same weakness that gps have in traditional machine learning tasks : solving eq .",
    "requires the inversion of a dense @xmath67 matrix , which when done exactly would require @xmath68 operations and is hence infeasible for anything but small - scale problems ( say , anything with @xmath69 ) .      in the subset of regressors ( sr )",
    "approach initially proposed for regularization networks @xcite , one chooses a subset @xmath70 of the data , with @xmath71 , and approximates the covariance for arbitrary @xmath72 by taking @xmath73 here @xmath74 denotes @xmath75 , and @xmath76 is the submatrix @xmath77_{ij}=k({\\mathbf{\\tilde x}}_i,{\\mathbf{\\tilde x}}_j)$ ] of @xmath30 . the approximation in eq .",
    "can be motivated for example from the nystrm approximation @xcite .",
    "let @xmath78 denote the submatrix @xmath79_{ij}=k({\\mathbf x}_i,{\\mathbf{\\tilde x}}_j)$ ] corresponding to the @xmath80 columns of the data points in the subset .",
    "we then have the rank - reduced approximation @xmath81 and @xmath82 . plugging these into eq .  , we obtain for the mean @xmath83 where we have defined @xmath84 , @xmath85 and applied the smw identity ] to show that @xmath86 similarly , we obtain for the predictive variance @xmath87 doing this means a huge gain in computational savings : solving the reduced problem in eq .",
    "costs @xmath88 for initialization , requires @xmath89 storage and every prediction costs @xmath90 ( or @xmath89 if we additionally evaluate the variance ) .",
    "this has to be compared with the complexity of the full problem : @xmath68 initialization , @xmath91 storage , and @xmath92 prediction .",
    "thus computational complexity now only depends linearly on @xmath26 ( for constant @xmath80 ) .",
    "note that the sr - approximation produces a degenerate gp . as a consequence , the predictive variance in eq .   will underestimate the true variance .",
    "in particular , it will be near zero when @xmath5 is far from the subset @xmath70 ( which is exactly the opposite of what we want , as the predictive variance should be high for novel inputs ) . the situation can be remedied by considering the projected process approximation @xcite , which results in the same expression for the mean in eq .  , but adds the term @xmath93 to the variance in eq .",
    "selecting the best subset is a combinatorial problem that can not be solved effeciently .",
    "instead , we try to find a compact subset that summarizes the relevant information by incremental forward selection . in every step of the procedure",
    ", we add that element from the set of remaining unselected elements to the active set that performs best with respect to a given specific criterion .",
    "in general , we distinguish between supervised and unsupervised approaches , i.e. those that consider the target variable we regress on , and those that do not . here",
    "we focus on the incomplete cholesky decomposition ( icd ) as an unsupervised approach @xcite .",
    "icd aims at reducing at each step the error incurred from approximating the covariance matrix : @xmath94 .",
    "note that the icd of @xmath30 is the dual equivalent of performing partial gram - schmidt on the mercer - induced feature representation : in every step , we add that element to the active set whose distance from the span of the currently selected elements is largest ( in feature space ) .",
    "the procedure is stopped when the residual of remaining ( unselected ) elements falls below a given threshold , or a given maximum number of allowed elements is exceeded . in @xcite online variants",
    "thereof are considered ( where instead of repeatedly inspecting all remaining elements only one pass over the dataset is made and every element is examined only once ) . in general , the number of elements selected by icd will depend on the effective rank of @xmath30 ( and thus its eigenspectrum ) .",
    "the major advantage of using gp - based function approximation ( in contrast to , say , neural networks or tree - based approaches ) is that both learning of the weight vector and specification of the architecture / hyperparameters / basis functions can be handled in a principled and essentially automated way .      to determine hyperparameters for gptd",
    ", we consider the marginal likelihood of the process , i.e. the probability of generating the rewards we have observed given the sequence of states and a particular setting of the hyperparameter @xmath34 .",
    "we then maximize this function ( its logarithm ) with respect to @xmath34 . from eq .",
    "we see that for gptd we have @xmath95 . thus plugging in the definition for a multivariate gaussian and taking the logarithm",
    ", we obtain @xmath96 optimizing this function with respect to @xmath34 is a nonconvex problem and we have to resort to iterative gradient - based solvers ( such as scaled conjugate gradients , e.g. see @xcite ) . to do this we need to be able to evaluate the gradient of @xmath97 .",
    "the partial derivatives of @xmath97 with respect to each individual hyperparameter @xmath98 can be obtained in closed form as @xmath99 note that @xmath97 automatically incorporates the trade - off between model fit ( training error ) and model complexity and can thus be regarded as an indicator for generalization capabilities , i.e. how well gptd will predict the values of states not in its training set . the first term in eq .",
    "measures the complexity of the model , and will be large for flexible and small for rigid models .",
    "( since the determinant equals the sum of the eigenvalues ) . in general , flexible models are achieved by smaller bandwidths in the covariance , meaning that @xmath30 s effective rank will be large and its eigenvalues will fall off more slowly . on the other hand ,",
    "more rigid models are achieved by larger bandwidths , meaning that @xmath30 s effective rank will be low and its eigenvalues will fall off more quickly .",
    "note that the effective rank of @xmath30 is also important for the sr - approximation ( see section  3 ) , since the effectiveness of sr depends on building a low - rank approximation of @xmath30 spending as few resources as possible . ]",
    "the second term measures the model fit and can shown to be the value of the error function for a penalized least - squares that would ( in a frequentist setting ) correspond to gptd .      a common choice for @xmath33 is to consider a ( positive definite ) function parameterized by a small number of scalar parameters , such as the stationary isotropic gaussian ( or squared exponential ) , which is parameterized by the lengthscale ( bandwidth @xmath100 ) . in the following we will consider three variants of the form @xcite : @xmath101 where hyperparameter @xmath102 denotes the vertical lengthscale , @xmath103 the bias , and symmetric positive semidefinite matrix @xmath104 is given by    * * variant 1 ( isotropic ) : * @xmath105 with hyperparameter @xmath106 . * * variant 2 ( axis - aligned ard ) : * @xmath107 with hyperparameters @xmath108 .",
    "* * variant 3(factor analysis ) : * @xmath109 where @xmath110 matrix @xmath111 is given by @xmath112 $ ] , @xmath113 , and both the entries of @xmath111 , i.e. @xmath114 and @xmath108 are adjustable hyperparameters .",
    "is also determined from model selection : we systematically try different values of @xmath115 , find the corresponding remaining hyperparameters via scg - based likelihood optimization and compare the final scores ( likelihood ) of the resulting models . ]",
    "the first variant ( see figure  [ fig:1 ] ) assumes that every coordinate of the input ( i.e. state - vector ) is equally important for predicting its value .",
    "however , in particular for high - dimensional state vectors , this might be too simple : along some dimensions this will produce too much resolution where it will be wasted , along other dimensions this will produce too little resolution where it would otherwise be needed .",
    "the second variant is more powerful and includes a different parameter for every coordinate of the state vector , thus assigning a different scale to every state variable .",
    "this covariance implements automatic relevance determination ( ard ) : since the individual scaling factors are automatically adapted from the data via marginal likelihood optimization , they inform us about how relevant each state variable is for predicting the value .",
    "a large value of @xmath116 means that the @xmath117-th state variable is important and even small variations along this coordinate are relevant .",
    "a small value of @xmath118 means that the @xmath119-th state variable is less important and only large variations along this coordinate will impact the prediction ( if at all ) .",
    "a value close to zero means that the corresponding coordinate is irrelevant and could be left out ( i.e. the value function does not rely on that particular state variable ) .",
    "the benefit of removing irrelevant coordinates is that the complexity of the model will decrease while the fit of the model stays the same : thus likelihood will increase .",
    "the third variant first identifies relevant directions in the input space ( linear combinations of state variables ) and performs a rotation of the coordinate system ( the number of relevant directions is specified in advance by @xmath115 ) . as in the second variant , different scaling factors",
    "are then applied along the rotated axes .",
    ", i.e. @xmath120.,scaledwidth=99.0% ]      as an example , we will now show how the gradient @xmath121 of eq .",
    "is calculated for the ard covariance .",
    "note that since all hyperparameters in this model , i.e. @xmath122 , must be positive , it is more convenient to consider the hyperparameter vector @xmath34 in log space : @xmath123 .",
    "we start by establishing some useful identities : for any @xmath31 matrix @xmath124 we have @xmath125_{ij}=a_{ij}-\\gamma_i a_{i+1,j}-\\gamma_j a_{i , j+1 }   + \\gamma_i \\gamma_j a_{i+1,j+1}. \\ ] ] furthermore , we have @xmath126_{ij}=\\begin{cases } 1+\\gamma_i^2 & , i = j \\\\",
    "-\\gamma_i     & , i = j-1 \\textrm { or } i = j+1\\\\                                    0 & ,",
    "\\textrm { otherwise }                       \\end{cases}\\ ] ] now write @xmath30 as @xmath127 , where @xmath128_{ij}=\\exp\\left\\{-0.5 \\sum_{d=1}^d a_d\\bigl(x_d^{(i ) } - x_d^{(j)}\\bigr)^2\\right\\}$ ] and @xmath129 is the @xmath31 matrix of all ones .",
    "computing the partial derivative of @xmath30 , we then obtain @xmath130 @xmath131_{ij}=-\\frac{1}{2}v_0c_{ij } \\bigl(x_\\nu^{(i ) } - x_\\nu^{(j)}\\bigr)^2 , \\quad \\nu=1\\ldots d\\ ] ] next , we will compute the partial derivatives of @xmath132 , giving for @xmath133 : @xmath134 { \\mathbf h}{^\\text{\\sffamily t}}= b{\\mathbf h}{\\mathbbm 1_{n , n}}{\\mathbf h}{^\\text{\\sffamily t}}\\\\ & \\rightarrow \\left[\\frac{\\partial { \\mathbf q}}{\\partial \\log b}\\right]_{ij}= b(1-\\gamma_i-\\gamma_j+\\gamma_i\\gamma_j ) .",
    "\\end{split}\\ ] ] for @xmath135 we have @xmath136 { \\mathbf h}{^\\text{\\sffamily t}}= v_0{\\mathbf h}{\\mathbf c}{\\mathbf h}{^\\text{\\sffamily t}}\\\\ \\rightarrow \\left[\\frac{\\partial { \\mathbf q}}{\\partial \\log v_0}\\right]_{ij}&= v_0(c_{ij}-\\gamma_ic_{i+1,j}-   \\gamma_jc_{i , j+1}+\\gamma_i\\gamma_jc_{i+1,j+1 } ) .",
    "\\end{split}\\ ] ] for @xmath47 we have @xmath137={\\sigma_0 ^ 2}{\\mathbf h}{\\mathbf h}{^\\text{\\sffamily t}}\\\\ & \\rightarrow \\left[\\frac{\\partial { \\mathbf q}}{\\partial \\log { \\sigma_0 ^ 2}}\\right]_{ij}= \\begin{cases } { \\sigma_0 ^ 2}(1+\\gamma_i^2 ) & , i = j \\\\                                    -{\\sigma_0 ^",
    "2}\\gamma_i     & , i = j-1 \\textrm { or } i = j+1\\\\                                    0 & , \\textrm { otherwise }                       \\end{cases } \\end{split}\\ ] ] finally , for each of the @xmath138 , @xmath139 we get @xmath140{\\mathbf h}{^\\text{\\sffamily t}}\\\\ \\rightarrow\\left [ \\frac{\\partial { \\mathbf q}}{\\partial \\log a_\\nu}\\right]_{ij}&=-\\frac{1}{2 } a_\\nu v_0 ( c_{ij}d_{ij}^\\nu-\\gamma_i c_{i+1,j}d_{i+1,j}^\\nu \\\\ & -\\gamma_j c_{i , j+1}d_{i , j+1}^\\nu + \\gamma_i \\gamma_j c_{i+1,j+1 } d_{i+1,j+1}^\\nu ) \\end{split}\\ ] ] where we have defined @xmath141 . thus , with @xmath142 we have for eq .",
    "@xmath143_{ij } \\left[\\frac{\\partial { \\mathbf q}}{\\partial \\theta_\\nu}\\right]_{ji } \\\\",
    "{ \\mathbf w}{^\\text{\\sffamily t}}\\frac{\\partial { \\mathbf q}}{\\partial \\theta_\\nu}{\\mathbf w } & = \\sum_{i=1}^{n-1 } \\sum_{j=1}^{n-1 } [ { \\mathbf w}]_i [ { \\mathbf w}]_j \\left[\\frac{\\partial { \\mathbf q}}{\\partial \\theta_\\nu}\\right]_{ij}\\end{aligned}\\ ] ] which can be used to calculate the partial derivates with computational complexity @xmath91 each ( except for @xmath47 , where the matrix of derivatives is tridiagonal ) .",
    "this section demonstrates that our proposed model selection can be used to solve the approximate policy evaluation problem in a completely automated way  without any manual tweaking of hyperparameters .",
    "we will also show some of the additional benefits of model selection , which are improved accuracy and reduced complexity : because we automatically set the hyperparameters we can use more sophisticated covariance functions ( see section  [ sec : choosingthecovariance ] ) that depend on a larger number of hyperparameters , thus better fit the regularities of a particular dataset , and therefore do not waste unnecessary resources on irrelevant aspects of the state - vector .",
    "the latter aspect is particularly interesting for computational reasons ( see section  4 ) and becomes important in large - scale applications .",
    "r0.5          first , we consider the pendulum swing - up task , a common benchmark in rl .",
    "the goal is to swing up an underpowered pendulum and balance it around the inverted upright position ( here formulated as an episodic task ) .",
    "more details and the equations of motion can be found in e.g. @xcite .",
    "since gptd only solves ( approximate ) policy evaluation , to test our model selection approach we chose to generate a sample trajectory under the optimal policy ( obtained from fitted value iteration ) .",
    "we generated a sequence of 1000 state - transitions under this policy ( which corresponds to about 25 completed episodes ) and applied gptd for the three choices of covariance : isotropic ( i ) , axis - aligned ard ( ii ) , and factor analyis ( iii ) . in each case , the best setting of hyperparameters was found from running scaled conjugate gradients on eq .",
    ", giving    [ cols=\"^,^,^,^,^,^,^ \" , ]     as can be seen from figure  [ fig:4 ] ( center and right ) , both obtain a very reasonable approximation .",
    "however , ( ii ) automatically detects that the @xmath144-coordinate of the state is irrelevant and thus assigns a very small weight to it ( @xmath145 ) . with a uniform lengthscale , ( i )",
    "is unable to do that and has to put equal weight on both state variables . as a consequence ,",
    "its estimate is less exact and more wiggly ( mse : ( i ) 0.030 , and ( ii ) 0.019 ) .",
    "additional insight can be gained by looking at the likelihood @xmath97 of the models ( cf .",
    ". here we see that ( ii ) has lower complexity ( cf .",
    "eigenspectrum of @xmath54 in figure  [ fig:5 ] ) , fits the data better and thus has a higher combined likelihood ( note that the values in the table show the negative log likelihood which we minimize ) .",
    "moreover , if we completely remove the @xmath144 state variable ( setting @xmath146 ) , the eigenspectrum of @xmath54 decreases more rapidly ; thus ( ii ) without @xmath144 has an even lower complexity while still having the same fit .",
    "this indicates that state component @xmath144 can be safely ignored in this task / domain .",
    "in addition , as was mentioned before , the lower effective rank of @xmath30 will also allow us to make more efficient use of sr - based approximations .            .",
    "right : eigenvalues of @xmath54.,title=\"fig:\",scaledwidth=49.0%,height=158 ] .",
    "right : eigenvalues of @xmath54.,title=\"fig:\",scaledwidth=49.0%,height=158 ]",
    "it should be noted that the proposed framework for automatic feature generation and model selection should primarily be thought of as a practical tool : despite offering a principled solution to an important problem in rl , ultimately it does not come with any theoretical guarantees ( due to some modeling assumptions from gptd and the way the hyperparameters are obtained ) . for most practical applications",
    "this might be less of an issue , but in general care has to be taken .",
    "the framework can be easily extended to perform policy evaluation over the joint state - action space to learn the model - free q - function ( instead of the v - function ) : we just have to choose a different covariance function , taking for example the product @xmath147,[{\\mathbf x}',a'])=k({\\mathbf x},{\\mathbf x}')k(a , a')$ ] with @xmath148 for problems with a small number of discrete actions @xcite .",
    "this opens the way for model - free policy improvement and thus optimal control via approximate policy iteration .",
    "our next step then is to apply this approach to real - world high - dimensional control tasks , both in batch settings and hybrid batch / online settings ; in the latter case exploiting the gain in computational efficiency obtained through model selection to improve @xcite .",
    "this work has taken place in the learning agents research group ( larg ) at the artificial intelligence laboratory , the university of texas at austin .",
    "larg research is supported in part by grants from the nsf ( cns-0615104 ) , darpa ( fa8750 - 05 - 2 - 0283 and fa8650 - 08-c-7812 ) , the federal highway administration ( dtfh61 - 07-h-00030 ) , and general motors ."
  ],
  "abstract_text": [
    "<S> feature selection in reinforcement learning ( rl ) , i.e. choosing basis functions such that useful approximations of the unkown value function can be obtained , is one of the main challenges in scaling rl to real - world applications . </S>",
    "<S> here we consider the gaussian process based framework gptd for approximate policy evaluation , and propose feature selection through marginal likelihood optimization of the associated hyperparameters . </S>",
    "<S> our approach has two appealing benefits : ( 1 ) given just sample transitions , we can solve the policy evaluation problem fully automatically ( without looking at the learning task , and , in theory , independent of the dimensionality of the state space ) , and ( 2 ) model selection allows us to consider more sophisticated kernels , which in turn enable us to identify relevant subspaces and eliminate irrelevant state variables such that we can achieve substantial computational savings and improved prediction performance . </S>"
  ]
}