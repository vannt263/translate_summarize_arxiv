{
  "article_text": [
    "intercellular communication is one of the most important characteristics of all animal species because it makes the many components of such complex systems operate together . among the many types of intercellular communication , we are interested in the communication among brain cells , the neurons , that exchange information mediated by chemical and electrical synapses @xcite .",
    "the uncovering of the essence of behaviour and perception in animals and human beings is one of the main challenges in brain research .",
    "while the behaviour is believed to be linked to the way neurons are connected ( the topology of the neural network and the physical connections among the neurons ) , the perception is believed to be linked to synchronisation .",
    "this comes from the binding hypothesis @xcite , which states that synchronisation functionally binds neural networks coding the same feature or objects .",
    "this hypothesis raised one of the most important contemporary debates in neurobiology @xcite because desynchronisation seems to play an important role in perception as well .",
    "the binding hypothesis is mainly supported by the belief that a convenient environment for neurons to exchange information appears when they become more synchronous .    despite the explosive growth in the field of complex networks ,",
    "it is still unclear for which conditions synchronisation implies information transmission and it is still unclear which topology favours the flowing of information .",
    "additionally , most of the models being currently studied in complex networks consider networks whose nodes ( such as neurons ) are either linearly or non - linearly connected .",
    "but , recent works have shown that neurons that were believed to make only non - linear ( chemical ) synapses make also simultaneously linear ( electrical ) synapses @xcite . to make the scenario even more complicated",
    ", neurons connect chemically in an excitatory and/or an inhibitory way . in this work ,",
    "we aim to study the relationship between synchronisation and information transmission in such neural networks , whose neurons are simultaneously connected by chemical and electrical synapses .",
    "the electrical synapse is the result of the potential difference between the neurons and causes an immediate physiological response of the latter one , linearly proportional to the potential difference .",
    "the chemical synapse is mediated by the exchange of neurotransmitters from the pre to the postsynaptic neuron and can only be released once the presynaptic neuron membrane achieves a certain action potential .",
    "the chemical interaction is described by a nonlinear function @xcite .    while the electrical synapses between neurons is localised in the neuron cell and therefore it is a local connection , the chemical synapse is in the neuron axon and is therefore mainly responsible for the non - local nature of the synapses .",
    "chemical synapses can be inhibitory and excitatory .",
    "when an inhibitory neuron spikes ( the pre - synaptic neuron ) , a neuron connected to it ( the post - synaptic neuron ) is prevented from spiking .",
    "as shown in ref .",
    "@xcite , inhibition promotes synchronisation .",
    "when an excitatory neuron spikes , it induces the post - synaptic neuron to spike .",
    "several types of synchronisation were found in networks of chaotic neurons coupled with only electrical synapses .",
    "one can have complete synchronisation , generalised synchronisation and phase synchronisation , the latter appearing for small synapse strength @xcite .",
    "complete synchrony strongly depends on the network structure and the number of cells . in networks of chemically coupled neurons",
    "@xcite , the net input a neuron receives from synaptic neurons emitting synchronised spikes is proportional to the number of connected units .",
    "hence , for chemical synapses , if all the nodes in the network have the same degree , synchronisation will be enhanced ; if different nodes have different degrees , synchronisation will be hampered @xcite .",
    "in fact , ref .",
    "@xcite has shown analytically that the stability of the completely synchronous state in such networks only depends on the number of signals each neuron receives , independent of all other details of the network topology",
    ".    the most obvious possible role of electrical synapses within networks of inhibitory neurons is to couple the membrane potential of connected cells , leading to an increase in the probability of synchronised action potentials .",
    "this synchronous firing could coordinate the activity of other cortical cell populations .",
    "for example , it has been reported that the introduction of electrical synapses among gabaergic neurons that are also chemically connected can promote oscillatory rhythmic activity @xcite .",
    "these possibilities have been addressed experimentally by several investigators and have been reviewed recently@xcite .",
    "motivated by these observations and also by the fact that the behaviour of micro - circuitry in the cerebral cortex is not well understood , we analyse the combined effect of these two types of synapses on the stability of the synchronous behaviour and on the information transmission in small neural networks . in order to deal with this problem",
    "analytically we consider idealistic networks , composed of equal neurons with mutual connections of equal strengths ( see sec .",
    "[ network ] ) .",
    "a basic assumption characterising most of the early works on synchronisation in neural networks is that , by adding a relatively small amount of electrical synapse to the inhibitory synapse , one can increase the degree of synchronisation far more than a much larger increase in inhibitory conductance @xcite .",
    "our results agree with this finding in the sense that for larger inhibitory synaptic strengths complete synchronisation can only be achieved if the electrical synapse strength is larger than a certain amount .",
    "but in contrast , we found that for moderate inhibitory synaptic strengths , the larger the chemical synapse strength is the larger the electrical synapse strength needs to be to achieve complete synchronisation . additionally , we introduce in this work analytical approaches to understand when complete synchronisation should be expected to be found and what is the relation of that with the amount of information produced by the network .",
    "information is an important concept @xcite .",
    "it measures how much uncertainty one has about an event before it happens .",
    "it is a measure of how complex a system is .",
    "very complicated and higher dimensional systems might be actually very predictable , and as a consequence the content of information of such a system might be very limited . but",
    "measuring the amount of information is something difficult to accomplish .",
    "normally , there is always some bias or error on the calculation of it @xcite , and one has to rely on alternative approaches .",
    "measuring the shannon entropy of a chaotic trajectory is extremely difficult because one has to calculate an integral of the probability density of a fractal chaotic set .",
    "but for chaotic systems that have absolutely continuous conditional measures , one can calculate shannon s entropy per unit of time , a quantity known as kolmogorov - sinai ( ks ) entropy @xcite , by summing all the positive lyapunov exponents @xcite .",
    "a system that has absolutely continuous conditional measures is a system whose trajectory continuously distribute along unstable directions .",
    "more precisely , systems whose trajectories continuously distribute along unstable manifolds at points that have positive probability measure .",
    "these systems form a large class of nonuniformly hyperbolic systems @xcite : the hnon family ; hnon - like attractor arising from homoclinic bifurcations ; strange attractors arising from hopf bifurcations ( e.g. rssler oscillator ) ; some classes of mechanical models with periodic forcing . the result in ref .",
    "@xcite extends a previous result by pesin @xcite that demonstrated that for hyperbolic maps , the ks entropy is equal to the sum of the positive lyapunov exponents .",
    "we are not aware of any rigorous result proving the equivalence of the ks entropy and the sum of lyapunov exponent for the hindmarsh - rose neural model neither to a network constructed with them .",
    "but the chaotic attractors arising in this neuron model are similar to the ones appearing from homoclinic bifurcations .",
    "additionally , for two coupled neurons , we show in sec .",
    "[ combined_information ] ( using the non - rigorous methods described in appendix [ apendice1 ] ) that a lower bound estimation of the ks entropy is indeed close to the sum of all the positive lyapunov exponents . despite the lack of a rigorous proof , we will assume that the results in refs .",
    "@xcite apply in here in the sense that the sum of the positive lyapunov exponents provide a good estimation for the ks entropy .",
    "the ks entropy for chaotic networks has another important meaning .",
    "it provides one the so called network capacity @xcite , the maximal amount of information that all the neurons in the network can simultaneously process ( per unit of time ) .",
    "a network that produces information at a higher rate is more unpredictable and more complex .",
    "arguably , the network capacity is an upper bound for the amount of information that the network is capable of processing from external stimuli . in ref .",
    "we discuss a situation were that is indeed the case .    to understand the scope of this paper and the methods used , we first justify the chosen network topologies in sec . [ network ] .",
    "then , in sec",
    ". [ network_dynamic ] , we describe the dynamical system of our network and derive the variational equations of it in the eigenmode form , a necessary analytical tool in order to be able to study the onset of complete synchronisation ( cs ) and to calculate the rate of information produced by the network .",
    "complete synchronisation happens when the trajectories of all neurons are equal .",
    "our main results can be summarised as in the following :    * we show ( secs .",
    "[ estabilidade ] and [ rescaling ] ) how one can calculate the synaptic strengths ( chemical and electrical ) necessary for a network of @xmath0 neurons to achieve complete synchronisation when one knows the strengths for which two mutually coupled neurons become completely synchronous .",
    "* we show numerically ( sec . [ combined_synchronous ] ) parameter space diagrams indicating the electrical and chemical synapse strengths responsible to make complete synchronisation to appear in different networks . the analytical derivation from sec .",
    "[ rescaling ] are found to be sufficiently accurate .",
    "there are two scenarios for the appearance of complete synchronisation for inhibitory networks .",
    "if the chemical synapse strength is small , the larger the chemical synapse strength used the larger the electrical synapse strength needed to be to achieve complete synchronisation .",
    "otherwise , if the chemical synapse strength is large , complete synchronisation appears if the electrical synapse strength is lager than a certain value . in excitatory networks both synapses",
    "work in a constructive way to promote complete synchronisation : the larger the chemical synapse strength is the smaller the electrical synapse strength needs to be to achieve complete synchronisation . * we show ( secs .",
    "[ combined_information ] ) that the sum of the positive lyapunov exponents provides a good estimation for the ks entropy .",
    "additionally , we show that there are optimal ranges of values for the chemical and electrical strengths for which the amount of information is large . *",
    "if complete synchronisation is absent , we show ( sec . [ ex_in ] ) that while in inhibitory networks one can typically expect to find high levels of synchronous behaviour , in excitatory networks one is likely to expect desynchronous behaviour .",
    "* we calculate ( sec . [ bounds ] ) an upper bound for the rate of information produced per time unit ( kolmogorov - sinai entropy ) by larger networks using the rate at which information is produced by two mutually coupled neurons .",
    "in order to consider the combined action of these two different types of synapses , we need to consider in our theoretical approach idealistic networks , constructed by nodes possessing equal dynamics and particular coupling topologies such that a synchronisation manifold exists and cs is possible . if we had studied networks whose",
    "neurons were exclusively connected by electrical means , we could have considered networks with arbitrary topologies . on the other hand , if we had studied networks whose",
    "neurons are exclusively connected by chemical means , we would have considered networks whose neurons receive the same number of chemical connections .",
    "these conditions are the same ones being usually made to study complete synchronisation in complex networks @xcite .    in order to analytically study networks formed by neurons that make simultaneously chemical and electrical connections",
    ", we have not only to assume that the neurons have equal dynamics and that every neuron receives the same number of chemical connections coming from other neurons , but also that the laplacian matrix for the electrical synapses ( that provides topology of the electrical connections ) and the laplacian matrix for the chemical synapses commute , as we clarify later in this paper .",
    "naturally , there is a large number of laplacian matrices that commute . in this work",
    "we construct networks that are biologically plausible .",
    "since the electrical connection is local , we consider that neurons connect electrically only to their nearest neighbours .",
    "since neurons connected chemically make a large number of connections ( of the order of 1000 ) , it is reasonable to consider that for small networks the neurons that are chemically connected are fully connected , i.e. , every neuron connects to all the other neurons .",
    "notice however that while reciprocal connections are commonly found in electrically coupled neurons , that is not typical for chemically connected neurons .",
    "since our small networks are composed of no more than 8 neurons , we make an abstract assumption and admit another possible type of network in which neurons that are connected electrically can also make non - local connections , allowing them to become fully connected to the other neurons .",
    "notice , however , that our theoretical approach remains valid for larger networks that admit a synchronisation manifold .",
    "the dynamics of the hindmarsh - rose ( hr ) model for neurons is described by    @xmath1 \\nonumber\\end{aligned}\\ ] ]    where @xmath2 is the membrane potential , @xmath3 is associated with the fast current , @xmath4 or @xmath5 , and @xmath6 with the slow current , for example , @xmath7 .",
    "the parameters are defined as @xmath8 and @xmath9 where the system exhibits a multi - time - scale chaotic behaviour characterised as spike - bursting .",
    "the dynamics of a neural networks of @xmath0 neurons connected simultaneously by electrical ( a linear coupling ) and chemical ( a non - linear coupling ) synapses is described by @xmath10 \\nonumber\\end{aligned}\\ ] ] @xmath11 , where @xmath0 is the number of neurons .    in this work",
    "we consider that @xmath12 .",
    "but we preserve the function @xmath13 in our remaining analytical derivation to maintain generality .",
    "the chemical synapse function is modelled by the sigmoidal function @xmath14 with @xmath15 , @xmath16 and @xmath17 for excitatory and @xmath18 for inhibitory . for the chosen parameters and all the networks",
    "that we have worked @xmath19 and the term @xmath20 is always negative for excitatory networks and positive for inhibitory networks .",
    "if two neurons are connected under an inhibitory ( excitatory ) synapse then , when the presynaptic neuron spikes , it induces the postsynaptic neuron not to spike ( to spike ) .",
    "the matrix @xmath21 describes the way neurons are electrically connected .",
    "it is a laplacian matrix and therefore @xmath22 .",
    "the matrix @xmath23 describes the way neurons are chemically connected and it is an adjacent matrix , therefore @xmath24 , for all @xmath25 . for both matrices , a positive off - diagonal term placed in the line @xmath25 and",
    "column @xmath26 means that neuron @xmath25 perturbs neuron @xmath26 with an intensity given by @xmath27 ( or by @xmath28 ) .",
    "since the diagonal elements of the adjacent matrix are zero , @xmath29 represents the number of connections that neuron @xmath25 receives from all the other neurons @xmath26 in the network .",
    "this is a necessary condition for the existence of the synchronous solution @xcite by the subspace @xmath30 .    under these assumptions and ,",
    "as previously explained , we consider networks with three topologies : * topology i * , when all the neurons are mutually fully ( all - to - all ) connected with chemical synapses and mutually diffusively ( nearest neighbours ) connected with electrical synapses ; * topology ii * , when all the neurons are mutually fully connected with chemical synapses and mutually fully connected with electrical synapses ; * topology iii * , when all the neurons are mutually diffusively ( nearest neighbours ) connected with chemical and electrical synapses .",
    "we consider networks with 2 , 4 and 8 neurons . by nearest neighbours",
    ", we consider that the neurons are forming a closed ring .    the synchronous solutions @xmath31 take the form @xmath32 \\nonumber\\end{aligned}\\ ] ] the variational equation of the network in ( [ neurons02 ] ) [ calculated around the synchronisation manifold ( [ neurons04 ] ) ] is given by @xmath33 the matrix @xmath23 has been transformed to a laplacian matrix by @xmath34 .",
    "@xmath35 represents the derivative of @xmath36 with respect to @xmath2 , which in this work equals 1 .",
    "the term @xmath37 refers to the spatial derivative @xmath38 and equals @xmath39 ^ 2}. \\label{sprime}\\ ] ] notice that if @xmath40 ( what happens for @xmath41 ) , then @xmath42 and if @xmath43 ( @xmath44 ) , then @xmath42 .",
    "@xmath37 is not zero when the value of @xmath45 changes from 1 to 0 ( and vice - versa ) and @xmath46",
    ".    equation ( [ neurons06 ] ) is referred to as the variational equation and is often the starting point for determining whether the synchronisation manifold is stable .",
    "this equation is rather complicated since , given arbitrary synapses @xmath47 and @xmath48 , it can become quite higher dimensional . also the coupling matrices @xmath49 and @xmath50 can be arbitrary making the situation to become even more complicated .",
    "however , assuming that whenever there is a chemical synapse ( and @xmath51 ) , the matrices @xmath49 and @xmath50 commute , then the problem can be simplified by noticing that the arbitrary state @xmath52 ( where @xmath53 is the deviation of the @xmath25th vector state from the synchronisation manifold ) can be written as @xmath54 , with @xmath55 .",
    "the @xmath56 be the eigenvector and @xmath57 and @xmath58 the corresponding eigenvalues for the matrices @xmath49 and @xmath50 respectively .",
    "so , if that is the case , by applying @xmath59 ( with @xmath60 ) , to the left ( right ) side of each term in eq .",
    "( [ neurons06 ] ) one finally obtains the following set of n variational equations in the eigenmode @xmath61 where the term @xmath62 is given by @xmath63 in which @xmath64 ( with @xmath65=0 , and @xmath660 , @xmath67 ) are the eigenvalues of @xmath49 and @xmath68 are the eigenvalues of @xmath50 .",
    "the eigenvalues @xmath64 are negative because the off - diagonal elements of @xmath49 are positive .    for networks with @xmath69",
    "we have that @xmath70 and @xmath71 , meaning that the neurons are connected in an all - to - all fashion . for networks with @xmath72 ,",
    "if the neurons are connected in an all - to - all fashion , we have that @xmath73 and @xmath74 or if the neurons are connected with their nearest neighbours we have that @xmath70 and @xmath75 . for @xmath76 , @xmath77 and @xmath78 ( all - to - all ) and @xmath79 and @xmath75 ( nearest - neighbour ) .",
    "these values are placed in table [ tabela1 ] for further reference .",
    ".values of @xmath80 in absolute value and @xmath29 for the considered networks . [ cols=\"^,^,^\",options=\"header \" , ]     the previous equations are integrated using the 4th - order range - kutta method with a step size of 0.001 .",
    "the calculations of the lyapunov exponents are performed considering a time interval of 600 [ sufficient for a neuron to produce approximately 600 spikes ( @xmath81 ) ] .",
    "we discard a transient time of 300 , corresponding to 300,000 integrations .",
    "the stability of the synchronisation manifold can be seen from the perspective of control @xcite by imagining that the term @xmath62 stabilises eq .",
    "( [ neurons07 ] ) at the origin .",
    "this term can be interpreted as the main gain of a feedback control law @xmath82 such that @xmath83 ( resp . @xmath84 and @xmath85 ) tends to @xmath86 as @xmath87 tends to infinity .",
    "in fact , the controlling force @xmath82 could be designed with no previous knowledge of the system under consideration assuming that it has a parametric dependence .",
    "a drawback of such a general control approach is that it leads to non - feedback control strategy , which have not guaranteed stability margins .",
    "more robust approaches for determining the structural stability of the synchronisation manifold of systems whose equations of motion are partially unknown have been recently developed @xcite .    in this work , however , we determine the stability of the synchronisation manifold from the master stability analysis of refs .",
    "a necessary condition for the linear stability of the synchronised state is that all lyapunov exponents associated with @xmath64 and/or @xmath68 for each @xmath88 ( the directions transverse to the synchronisation manifold ) are negative .",
    "this criterion is a necessary condition for complete synchronisation only locally , i.e. close to the synchronisation manifold .",
    "when working with networks formed by nodes possessing equal dynamical rules , we wish to predict the behaviour of a large network from the behaviour of two coupled nodes .",
    "that can always be done whenever the equations of motion of the network can be rescaled into the form of the equations describing the two coupled nodes .",
    "that means that , given that two mutually coupled neurons completely synchronise for the electrical and chemical synapse strengths @xmath89 and @xmath90 , respectively , then it is possible to calculate the synapse strengths @xmath91 and @xmath92 for which a network composed by @xmath0 nodes completely synchronises .    in order to rescale the equations for the synchronisation manifold and for its stability , eqs .",
    "( [ neurons04 ] ) and ( [ neurons07 ] ) , respectively , we need to preserve the form of these equations as we consider different networks .",
    "concerning eq .",
    "( [ neurons07 ] ) , we need to show under which conditions it is possible to have @xmath93 , where @xmath94 is the term responsible to make the stability of the synchronisation manifold to depend among other things on the topology of the network and on the coupling function @xmath45 .",
    "notice that @xmath45 assumes for most of the time either the value 0 or 1 .",
    "for some short time interval @xmath45 changes its value from 0 to 1 ( and vice - versa ) and at this time @xmath37 is different from zero [ see eqs .",
    "( [ sp ] ) and ( [ sprime ] ) ] .",
    "for that reason we will treat @xmath37 as a small perturbation in our further calculations and will ignore it , most of the times .",
    "that leave us with two relevant terms in both eqs .",
    "( [ neurons04 ] ) and ( [ neurons07 ] ) that need to be taken into consideration in our rescaling analyses .",
    "these terms are @xmath95 and @xmath96 .",
    "while the first term comes from the electrical synapse , the second term comes from the chemical synapse .",
    "the first term depends on the eigenvalues of @xmath21 ( which varies according to the number of nodes and the topology of the network ) and on the synapse strength @xmath48 . if this term assumes a particular value for a given network , for another network one can suitably vary @xmath48 in order for the whole term to assume this same value in the other network .",
    "so , the term @xmath95 can always be rescaled by finding an appropriate value of @xmath48 .    the rescaling of the second term , @xmath96 is more complicated because it depends on the trajectory @xmath97 of the attractor .",
    "naturally , we wish to find a proper rescaling for the function @xmath45 , which implies that the attractors appearing as solutions on the synchronisation manifold should present some kind of invariant property .    in order to find such an invariant property , we study the time average @xmath98 of the function @xmath45 for attractors appearing as solutions of eq .",
    "( [ neurons04 ] ) for 5 network topologies . in fig .",
    "[ mostra_valores ] we show in the boxes ( a - e ) the values of @xmath99@xmath100 , @xmath29 and the type of topology considered in the networks of figs . [ s_p_inib54_58 ] , [ s_p_67_71 ] , [ ps_pecora_inib78_82 ] , and [ ps_pecora_24_28 ] .",
    "the result for excitatory networks can be seen in fig .",
    "[ s_p_inib54_58](a - e ) , which shows this value as a function of @xmath101 .",
    "apart from some small differences , the function @xmath102 remains invariant for the different networks considered .",
    "we identify two relevant values for @xmath103 .",
    "either @xmath104 , for @xmath105 or @xmath106 , for @xmath107 .",
    "@xmath108 .",
    "we also find an invariant curve of @xmath103 for inhibitory networks . in fig .",
    "[ s_p_67_71](a - e ) we show this curve for the same networks of fig .",
    "[ s_p_inib54_58 ] . for these networks",
    ", we define @xmath109 as the value of @xmath47 for which the curve of @xmath103 reaches its maximum . in the considered inhibitory networks",
    ", @xmath110 is a consequence of the fact that the neurons loose their chaotic behaviour and become a stable limit cycle .",
    "notice that the value of @xmath98 does not depend on the value of the electrical synapse strength @xmath48 .",
    "this is due to the fact that @xmath48 is not present in the equations for the synchronisation manifold [ eq .",
    "( [ neurons04 ] ) ] .",
    "let us rescale eq .",
    "( [ neurons04 ] ) .",
    "first notice that the average @xmath111 has the same invariant properties of the average @xmath98 .",
    "then , we assume that both @xmath45 and @xmath112 make small oscillations around their average value .",
    "that implies that @xmath113 . from figs .",
    "[ s_p_inib54_58 ] and [ s_p_67_71 ] we have that the average @xmath114 can be written as a function of @xmath115 , as well as @xmath111 .",
    "therefore , we can write @xmath116 as a function of @xmath115 .",
    "it is clear that the value of this average obtained for @xmath117 should be approximately equal to the value obtained for @xmath118 , and so this average function can be rescaled by @xmath119 .",
    "therefore , eq . ( [ neurons04 ] ) describing a large network can be rescaled into this same equation describing two mutually coupled neurons by    @xmath120    now , we need to show that it is also possible to do the same to eq .",
    "( [ neurons07 ] ) , the equation responsible for the stability of the synchronous solution .    assuming again that @xmath45 make small oscillations around its average value allows us to write @xmath121 as a function of @xmath122 as in @xmath123 .",
    "notice from figs .",
    "[ s_p_inib54_58 ] and [ s_p_67_71 ] that the average @xmath114 can be written as a function of @xmath115 . in order to rescale eq .",
    "( [ neurons07 ] ) , describing a network of @xmath0 nodes in terms of a network of 2 nodes , we need to have that @xmath124 leading to @xmath125 \\rangle - \\gamma_2 g_l(n ) & = & \\nonumber \\\\",
    "g_n(n=2 ) \\langle s[g_n(n=2 ) ]",
    "\\rangle + 2 g_l(n ) \\label{neurons13}\\end{aligned}\\ ] ] where we have considered only the second largest eigenvalue @xmath80 , the one responsible for the stability of the synchronisation manifold ; we have ignored terms that appear together with @xmath126 in @xmath94 .",
    "we make now a reasonable hypothesis that if a stable synchronous solutions for eq .",
    "( [ neurons04 ] ) exists for @xmath127 ( for a two mutually coupled neurons ) , then this same stable synchronous solution exists for @xmath128 ( for a network composed by @xmath0 neurons mutually connected ) .",
    "this hypothesis is constructed from the observation that equivalent attractors can be found in different networks if the rescaling in eq .",
    "( [ neurons081 ] ) is employed .",
    "we are assuming that if @xmath90 represents the chemical synapse strength for which complete synchronisation appears in two mutually coupled neurons , then complete synchronisation would appear in a network of @xmath0 nodes if @xmath129    if the previous hypothesis is satisfied , i.e. eq . ( [ neurons004 ] ) is satisfied , we see from figs . [ s_p_inib54_58 ] and [ s_p_67_71 ] that @xmath130 \\rangle \\approxeq \\langle s[g_n(n=2 ) ] \\rangle$ ] and assuming that these two averages are equal , then eq . ( [ neurons13 ] ) takes us to @xmath131 where @xmath132 represents the electrical synapse strength for which complete synchronisation occurs in a network composed by @xmath0 neurons .    in the following , we analyse two special cases of eq .",
    "( [ neurons13 ] ) when the function @xmath45 is constant and the previous approximations ( expanding @xmath94 around its average and that @xmath130 \\rangle = \\langle s[g_n(n=2 ) ] \\rangle$ ] ) to arrive to eqs .",
    "( [ neurons004 ] ) and ( [ neurons09 ] ) are exact .",
    "_ case 1 _ : a large chemical synapse strength , @xmath133 , with @xmath1341.67 , makes for * all * the time @xmath135 , leading to @xmath136 and @xmath37=0 ( see fig . [ s_p_inib54_58 ] ) .",
    "the neurons become completely synchronous to a stable equilibrium point .",
    "_ case 2 _ : a large chemical synapse strength , @xmath137 , with @xmath138 , makes for * all * the time @xmath139 and as a consequence @xmath140 and @xmath141 ( see fig .",
    "[ s_p_67_71 ] ) .",
    "the neurons become completely synchronous to a limit cycle .",
    "the analytical derivations done in the previous section are approximations , except for some special values of the synaptic strengths ( case 1 and 2 ) . however , as we show in this section , our calculations provide a good estimation of what to expect from parameter spaces of larger networks when the parameter space of two mutually coupled neurons is known .",
    "the parameter space is constructed by considering the synapses @xmath142 and they identify the regions where the state of complete synchronisation is stable .    the stability is determined from eqs .",
    "( [ neurons07 ] ) , by verifying whether there are no lyapunov exponents associated with transversal directions to the synchronisation manifold .",
    "these exponents are numerically obtained , without any approximation .    in fig .",
    "[ ps_pecora_inib78_82 ] , we show in black the synchronous regions ( all transversal conditional exponents are negative ) for the excitatory networks and in fig .",
    "[ ps_pecora_24_28 ] the same network topologies but for inhibitory networks . to simplify the understanding of these two figures , in fig .",
    "[ mostra_valores ] we show in boxes ( a - e ) the values of @xmath0 , @xmath100 , @xmath29 and the type of topology considered in the networks of figs .",
    "[ ps_pecora_inib78_82](a - e ) and [ ps_pecora_24_28](a - e ) .",
    "the values of @xmath48 and @xmath47 were rescaled by using eqs .",
    "( [ neurons004 ] ) and ( [ neurons09 ] ) . as expected , in excitatory networks our rescaling works very well and roughly in inhibitory networks .",
    "so , the vertical axis of figs .",
    "[ ps_pecora_inib78_82](b - e ) and [ ps_pecora_24_28](b - e ) show the quantity @xmath143 and the horizontal axis of these same figures show the quantity @xmath144 .    to assist the analysis of the parameter spaces , imagine a curve @xmath145 that is the border between the regions defining parameters for which the synchronisation manifold is unstable ( white regions ) and regions defining parameters for which the synchronisation manifold is stable ( black regions )",
    ". there are four main characteristics in these two types ( excitatory and inhibitory ) of networks concerning the occurrence of complete synchronisation .    * * in excitatory networks * , the electrical and the chemical synapses act in a combined way to foster synchronisation .",
    "the neurons become completely synchronous to a stable equilibrium point .",
    "the asynchronous neurons ( white regions ) are chaotic .",
    "the curve @xmath145 would look like a diagonal line with a negative slope .",
    "such a curve could be defined by an equation similar to @xmath146 , @xmath147 being a function that is approximately constant ( see fig . [ ps_pecora_inib78_82 ] ) . * * in excitatory networks , with @xmath1481.67 * , neurons are completely synchronous to a stable equilibrium point ( see fig .",
    "[ ps_pecora_inib78_82 ] ) . * * in inhibitory networks , with @xmath1495 * , the larger the chemical synapse strength is the larger the electrical synapse strength needs to be to achieve complete synchronisation .",
    "neurons become completely synchronous to either a limit cycle ( large chemical synapse strength ) or to a chaotic attractor ( small chemical synapse strength ) . the curve @xmath145 would look like a diagonal line with a positive slope . such a curve could be defined by an equation similar to @xmath150 ,",
    "@xmath147 being a function that is approximately constant ( see fig . [ ps_pecora_24_28 ] ) . * * in inhibitory networks , for large values of @xmath118 * , complete synchronisation appears for @xmath151 and neurons become completely synchronous to a stable limit cycle , which is unstable if @xmath152 . the curve @xmath145 would look like a straight vertical line .",
    "such a curve could be defined by an equation similar to @xmath153 .",
    "@xmath147 being a function that is approximately constant ( see fig . [ ps_pecora_24_28 ] ) .    if the neurons are set with different initial conditions , but sufficiently close , complete synchronisation is found for similar synaptic strengths for which the synchronisation manifold is stable .    if the neurons are set with sufficiently different initial conditions , and we construct parameter spaces that represent synaptic strengths for which cs takes place , we would have obtained parameter spaces with similar structure as the one observed in figs .",
    "[ ps_pecora_inib78_82 ] and [ ps_pecora_24_28 ] .",
    "however , the network can become completely synchronous to other synchronous solutions of eq .",
    "( [ neurons04 ] ) , different from the synchronous solutions observed for the parameters used to make figs .",
    "[ ps_pecora_inib78_82 ] and [ ps_pecora_24_28 ] . in other words , parameter spaces that show cs in networks",
    "whose neurons are set with different initial conditions constructed for the same synaptic strengths and networks considered in figs .",
    "[ ps_pecora_inib78_82 ] and [ ps_pecora_24_28 ] would present additional black points in the white areas of figs .",
    "[ ps_pecora_inib78_82 ] and [ ps_pecora_24_28 ] .",
    "first , we calculate the sum of all the positive lyapunov exponents of the attractor obtained from integrating the neural network [ eq .",
    "( [ neurons02 ] ) ] and represent it by @xmath154 .",
    "the lyapunov exponents are calculated from the variational equation of the network in eq .",
    "( [ neurons02 ] ) .",
    "as previously discussed , it is reasonable to assume that @xmath155 , where @xmath156 represents the ks entropy @xcite , which measures the amount of information ( shannon s entropy ) produced per time unit .    in figs .",
    "[ estima_entropia_fig02](a - b ) we show in the thin line @xmath157 for two mutually chemically and electrically coupled neurons ( @xmath48=0.1 ) for excitatory synapse ( a ) and for inhibitory synapse ( b ) . to confirm that the sum of the positive lyapunov exponents have an entropic meaning for the studied hindmarsh - rose neuron model",
    ", we have estimated a lower bound for the ks entropy , represented by the tick line with filled squares ( red online ) in fig .",
    "[ estima_entropia_fig02](a - b ) .",
    "we see that for both cases , as one increases the synaptic strength , @xmath154 decreases . for the excitatory case , for @xmath158",
    ", the neurons trajectories go to an equilibrium point and we obtain @xmath159 . if @xmath160 , that means that there are no positive lyapunov exponents and therefore no chaos .",
    "the maximal value of @xmath154 , calculated varying the synaptic strengths , is almost equal for both types of synapses .",
    "one sees that there is a range of strength values in both figures within which @xmath154 is large . for example , in ( a ) @xmath154 is large for @xmath161 $ ] and in ( b ) @xmath154 is large for @xmath162 $ ] .",
    "this was also observed in 3d parameter space diagrams ( not shown in here ) that show the value of @xmath154 versus @xmath47 and @xmath48 .",
    "these diagrams indicate that there is an optimal range of values for @xmath47 and @xmath48 for which @xmath154 remains large .",
    "the reason we have shown results for two coupled neurons is because for such a configuration a lower bound estimation of the ks entropy can be calculated by encoding the trajectory into a binary symbolic sequence .",
    "since the sequence is binary , this method is only capable of measuring an information rate that is less or equal than 1bit / symbol or 1bit / unit of time . since that for two coupled neurons , @xmath163bit / unit of time , and assuming that @xmath157 is a good estimation for @xmath156 , then the employed method to calculate a lower bound of the ks entropy is appropriate .",
    "the details of this estimation can be seen in appendix [ apendice1 ] .",
    "notice that in fig .",
    "[ estima_entropia_fig02](a - b ) for @xmath1640 ( as well as in ( b ) for @xmath165 ) the estimations of @xmath156 are larger than @xmath154 .",
    "that is the result of a known problem in the estimation of entropic quantities which prevents the estimation to be small .",
    "the problem arises because the symbolic sequences considered are not infinitely long for one to realise that there exists a few or only one symbolic sequence encoding the trajectory .",
    "for example , a long periodic orbit would be encoded by a series of short symbolic sequences making the estimation of @xmath156 to be positive instead of zero as it should be .",
    "to understand the relation between synchronisation ( desynchronisation ) and inhibition ( excitability ) , when _ complete synchronisation is absent _ we do the following .",
    "but notice that the following results are based on a conjecture that is currently not demonstrated .",
    "we calculate the lyapunov exponents along the synchronisation manifold , which are just the lyapunov exponents of the network by assuming that all neurons are completely synchronous .",
    "we call these exponents conditional lyapunov exponents and the sum of all the positive ones is denoted by @xmath166 .",
    "there are two ways for calculating them , either using eq .",
    "( [ neurons06 ] ) or ( [ neurons07 ] ) , eq .",
    "( [ neurons07 ] ) being simpler because of the dimensionality of the orthogonal vectors employed to calculate the lyapunov exponents . while the use of eq .",
    "( [ neurons06 ] ) requires 3n vectors , each one with dimensionality 3n , the use of eq .",
    "( [ neurons07 ] ) requires n vectors each one with dimensionality 3 . additionally ,",
    "once the function that relates the conditional exponents of two mutually coupled neurons with @xmath47 and @xmath48 is known , then one can calculate this function for all the conditional exponents of larger networks as long as eqs .",
    "( [ neurons04 ] ) and ( [ neurons07 ] ) can be rescaled .",
    "we can then classify these neural networks into 2 types .",
    "the types upper or lower .",
    "more specifically ,    @xmath167    to understand what @xmath166 and @xmath154 exactly mean and the reason for such a classification , notice that the networks here considered admit a synchronous solution",
    ". this synchronous solution might be unstable ( an unstable saddle ) and typical initial conditions depart from the neighbourhood of the synchronous solution and asymptotically tend towards a stable solution , the chaotic attractor .",
    "this attractor describes a network whose nodes are not synchronous . in such a situation ,",
    "the network admits at least two relevant solutions : a stable desynchronous one ( the chaotic attractor ) and an unstable synchronous one ( the synchronisation manifold ) .",
    "while @xmath166 can be associated with the amount of information produced by the unstable synchronous solution , @xmath154 can be associated with the amount of information produced by the desynchronous chaotic attractor .",
    "if the complete synchronous state is stable , then , @xmath168 , and the network in eq .",
    "( [ neurons02 ] ) possesses only one stable synchronous solution , for typical initial conditions . the nomenclature in eqs .",
    "( [ conjecture1 ] ) and ( [ conjecture4 ] ) comes from the fact that if @xmath169 then , @xmath166 is an upper bound for @xmath154 , otherwise it is a lower bound @xcite .",
    "assume now that the more information a network produces , the more desynchronisation is observed among pair of neurons @xcite .",
    "if @xmath170 ( upper ) , then @xmath171 is limited . as a consequence ,",
    "the production of information in the network is limited and therefore the level of desynchronisation is small . on the other hand , if @xmath172 ( lower ) , then @xmath171 can be large implying a large level of desynchronisation .",
    "another way of understanding the relationship between synchronisation and information is by using a result from ref .",
    "@xcite , which shows that for two coupled maps ( but this result is trivially extended to networks ) , the largest transversal conditional exponent , when the maps have a lower character , is larger than this exponent for when they have an upper character .",
    "since this exponent provides a necessary condition for the stability of the synchronisation manifold , it can be interpreted as a measure of the level of desynchronisation in the network .",
    "the larger this exponent is , the more desynchronous the network is .",
    "therefore , upper networks should have neurons more synchronous than lower networks .",
    "if @xmath169 ( upper ) , the synapse forces the trajectory to approach the synchronisation manifold and , as a consequence , there is a high level of synchronisation in the network . on the other hand ,",
    "if @xmath172 ( lower ) , the synapse forces the trajectory to depart from the synchronisation manifold and , as a consequence , there is a high level of desynchronisation in the network .",
    "one can check that in fig .",
    "[ fig_hks_hc00_19 ] , which shows as gray , the parameter regions for which @xmath173 and as black the parameter regions for which the synchronisation manifold is stable and there is complete synchronisation ( and therefore , @xmath168 ) for typical initial conditions .",
    "gray points appearing on black regions represent synaptic strengths for which in fact one has @xmath174 , but numerically we obtain that @xmath175 , with @xmath176 being a very small positive constant .",
    "typically , neurons coupled via an excitatory synapse [ ( a - d ) ] present a lower character while via an inhibitory synapse [ ( e - h ) ] present an upper character .",
    "this classification is also important because as it was shown in ref .",
    "@xcite , once two coupled neurons are upper ( or lower ) there is always a synaptic strength range for which a large network is upper ( or lower ) . and these synaptic strength ranges can be calculated using the rescalings in eqs .",
    "( [ neurons004 ] ) and ( [ neurons09 ] ) .    in figs .",
    "[ fig_hks_hc00_19](b - c ) and [ fig_hks_hc00_19](f - h ) , we show that the upper and lower character of two mutually coupled neurons is preserved in networks composed by a number of neurons larger than 2 , if one considers the rescalings of eqs .",
    "( [ neurons004 ] ) and ( [ neurons09 ] ) .",
    "this result is of fundamental importance , specially for synaptic strengths that promote the network to have an upper character because it allows us to calculate an upper bound for the ks entropy of larger networks by knowing the value of @xmath166 for two mutually coupled neurons .",
    "such a situation arises for inhibitory networks for a large range of both synaptic strengths .",
    "one finds an upper character in excitatory networks for a small value of the chemical synapse strength .",
    "the electrical synapse favours the neurons to synchronise . as a consequence",
    ", it is expected that networks with neurons connected exclusively by electrical synapses are of the upper type .",
    "this can be checked in all figures for when @xmath1770 .",
    "we are currently trying to prove the conjecture in ref .",
    "@xcite by studying the relationship between the stability of unstable periodic orbits @xcite embedded in the attractors appearing in complex networks and the stability of the equilibrium points .",
    "all the equilibrium points of a polynomial network can be calculated by the methods in refs .",
    "according to ruelle @xcite , the sum of all the positive lyapunov exponents is an upper bound for the kolmogorov - sinai entropy @xcite .",
    "therefore , whenever @xmath178 ( upper ) it is valid to write that @xmath179 where @xmath180 denotes the kolmogorov - sinai entropy of a network composed of @xmath0 neurons .",
    "as we have previously seen , the upper character of two mutually coupled neurons is preserved in the special larger networks here studied .",
    "in addition to this , if the positive conditional exponents of two mutually coupled neurons are known for a given @xmath47 and @xmath48 , allowing us to calculate @xmath181 $ ] , then one can calculate the positive conditional exponents of a network with @xmath0 neurons , @xmath182 $ ] .",
    "in other words , if the ratio of information production of two mutually coupled neurons that have equal trajectories , @xmath183 , is known and the neurons have an upper character , one can calculate the upper bound for the ratio of information production in larger networks , as long as eqs .",
    "( [ neurons04 ] ) and ( [ neurons07 ] ) can be rescaled .",
    "therefore , in upper networks connected simultaneously with electrical and inhibitory chemical synapses we can always calculate an upper bound for the rate of information production in terms of this quantity in two mutually coupled inhibitory neurons .    consider two mutually coupled neurons .",
    "denote @xmath184 as the sum for the positive lyapunov conditional exponents associated with the synchronisation manifold for a chemical synapse strength @xmath47 and @xmath185 as the sum of the positive lyapunov exponents associated with the only one transversal direction for a chemical synapse strength @xmath47 and an electrical synapse strength @xmath48 .",
    "remind that @xmath186 and @xmath187 are calculated using eq .",
    "( [ neurons07 ] ) for the index @xmath188 and @xmath189 , respectively .",
    "now , consider a network formed by n neurons .",
    "using similar arguments than the ones presented in sec . [ rescaling ] and",
    "based on the conjecture proposed in @xcite , the value of the synapse strengths @xmath190 for which the exponent @xmath191 has the same value of @xmath192 can be calculated by @xmath193 and the value of the synapse strengths @xmath190 for which the sum of the positive conditional exponent @xmath194 ( for @xmath195 ) has the same value of @xmath185 can be calculated by @xmath196    denote @xmath197 and @xmath198 as the maximal values of @xmath184 and @xmath185 with respect to @xmath47 and @xmath48 .    as an example of how to use eqs .",
    "( [ eq000 ] ) , ( [ eq001 ] ) and ( [ eq002 ] ) in order to calculate the upper bound for the rate of information produced in the network , we consider that the neurons in the network with @xmath0 nodes are coupled via electrical and excitatory chemical synapses in an all - to - all configuration ( topology ii ) , then @xmath199 , @xmath200 and @xmath201 .",
    "now , we search for a synapse strength range for which two mutually coupled neurons have an upper character . for example , let us say the range @xmath202 $ ] and @xmath203 $ ] , in fig .",
    "[ fig_hks_hc00_19](e ) , for two inhibitory mutually coupled neurons .    from eqs .",
    "( [ eq001 ] ) and ( [ eq002 ] ) , as long as the network with @xmath0 nodes has @xmath204 and @xmath205 , then @xmath206 and @xmath207 , and therefore for this synapse range , the maximum of @xmath166 is    @xmath208 } = \\lambda^{max}_1(n=2 ) + ( n-1)\\lambda^{max}_2(n=2 ) \\label{upper_network}\\ ] ]    notice that eq .",
    "( [ upper_network ] ) is valid to any network topology as long as eqs .",
    "( [ neurons04 ] ) and ( [ neurons07 ] ) can be rescaled .    for very large networks that are very well connected",
    ", @xmath209 and @xmath115 will be very small , since @xmath29 and @xmath0 are large . as a consequence , @xmath210 , since neurons are equal , and we can write    @xmath208 } = n\\lambda^{max}_2(n=2 ) \\label{upper_network1}\\ ] ]    which means that the rate of information produced by large upper neural networks whose neurons are highly connected has an upper bound that increases linearly with the number of neurons .",
    "a similar result is obtained when the neurons are connected with only electrical synapses @xcite .",
    "we have studied the combined action of chemical and electrical synapses in small networks of hindmarsh - rose ( hr ) neurons in the process of synchronisation and on the rate of information production .",
    "there are mainly two scenarios for the appearance of complete synchronisation for the studied inhibitory networks .",
    "if the chemical synapse strength is small , the larger the chemical synapse strength used the larger the electrical synapse strength needs to be to achieve complete synchronisation .",
    "otherwise , if the chemical synapse strength is large , complete synchronisation appears if the electrical synapse strength is larger than a certain value . in the studied excitatory networks both synapses work in a constructive way to promote complete synchronisation : the larger",
    "the chemical synapse strength is the smaller the electrical synapse strength needs to be to achieve complete synchronisation .    when neurons connect simultaneously by electrical and chemical ways , there is an optimal range of synaptic strengths for which the production of information is large . for strengths larger than values within this optimal range , the larger the electrical and chemical synaptic strengths are the smaller the production of information of coupled neurons .    in the absence of complete synchronisation",
    ", it is intuitive to expect that excitatory networks have neurons that are more desynchronous while inhibitory networks have neurons that are more synchronous .",
    "this intuitive idea can be better formalised by understanding the relationship between excitation ( inhibition ) , synchronisation ( desynchronisation ) and the rate of information production .",
    "for that we classify the network as having an upper or a lower character . in a upper ( lower ) network , the sum of all the positive lyapunov exponents , denoted by @xmath154 , is bounded from above ( below ) by the sum of all the positive conditional lyapunov exponents , denoted by @xmath166 , the lyapunov exponents of the synchronisation manifold and the transversal directions .",
    "networks that have neurons connected simultaneously by inhibitory chemical synapses and electrical synapses can be expected to have an upper character . in such networks",
    ", one should expect to find synchronous behaviour , since the synapses force the trajectory to approach the synchronisation manifold . on the other hand , networks whose chemical synapse are of the excitatory type",
    "might likely have a lower character .",
    "in such networks one should expect to find desynchronous behaviour since the synapses force the trajectory to depart from the synchronisation manifold .",
    "notice that @xmath211 can only be numerically obtained whereas @xmath212 can be calculated from the conditional exponents numerically obtained for two mutually coupled neurons that have equal trajectories . for upper networks , @xmath178 , and by ruelle",
    "@xcite @xmath213 , where @xmath156 is the kolmogorov - sinai entropy , the amount of information ( shannon s entropy ) produced by time unit ; we have then that @xmath166 is an upper bound for @xmath180 .",
    "that can be advantageously used in order to calculate the rate of information produced by a large network , composed of @xmath0 neurons by using only the rate at which information is produced in two mutually coupled neurons that are completely synchronous and have equal trajectories .",
    "we have worked with idealistic networks . however , our results can be extended to more realistic networks @xcite . for upper networks ,",
    "our numerical results show that more realistic networks constructed with non - equal nodes ( or networks of equal nodes but with random synapse strengths @xcite ) have @xmath154 smaller than the networks with equal nodes .",
    "therefore , even though networks with equal nodes might not be realistic , their entropy production per time unit is an upper bound for the entropy production of more realistic networks .",
    "* acknowledgment * msb and fmmk thank the max - planck - institut fr physik komplexer systeme ( dresden ) for the partial support of this research .",
    "msb acknowledges the partial financial support of `` fundao para a cincia e tecnologia ( fct ) , portugal '' through the programmes pocti and posi , with portuguese and european community structural funds .",
    "the authors are deeply grateful for the 4 anonimous referees for their important comments and suggestions that were considered in this new version of the manuscript .",
    "imagine a 2d chaotic system as the one studied in ref . @xcite",
    "( 5 ) and ( 6 ) ] . following the same ideas from there ,",
    "the ks entropy of two coupled maps with variables @xmath214 and @xmath215 can be estimated from the shannon s entropy of the probabilities that a trajectory point makes a given itinerary in the phase space @xmath216 , divided by the time interval for the trajectory to make that itinerary .    in practice , calculating the shannon s entropy @xcite for all possible itineraries on the phase space ( @xmath214,@xmath215 ) of a chaotic trajectory is equivalent to calculating the joint entropy between the probabilities of finding a point following simultaneously an itinerary along the variable the variable @xmath214 and another itinerary along the variable @xmath215 .",
    "since we are unable to make a high resolution partition of the phase space ( nor we do not know the markov partition ) in the neural networks studied in this work , we estimate a lower bound for the ks entropy by calculating the joint entropy between symbolic sequences encoding the trajectory",
    ". such calculation of probabilities involve large matrix operations and for that reason we restrain ourselves to the calculation of the joint entropy between two neurons .",
    "it is a lower bound due to two reasons .",
    "the first one is because the entropy will be measured considering the probabilities of occupation of a projected trajectory in a subspace of the network .",
    "the second one is because we calculate the entropy considering the probabilities of binary symbolic sequences and obviously a binary sequence may contain much less information than the content of a continuous signal @xcite",
    ".      given two symbolic sequences @xmath217 and @xmath218 , generated by neuron 1 and 2 , respectively , a lower bound for the ks entropy can be estimated by @xmath219 with @xmath220 representing the joint entropy between the symbolic sequences @xmath217 and @xmath218 .",
    "to create the symbolic sequences , we represent the time at which the @xmath6-th maxima happens in neuron 1 by @xmath221 , and the time interval between the n - th and the ( n+1)-th maxima , by @xmath222 .",
    "a maxima represents the moment when the action potential reaches its maximal value .",
    "the quantity @xmath223 represents the average time between two spikes .",
    "we then encode the spiking events using the following rule .",
    "the @xmath25-th symbol of the encoding is a `` 1 '' if a spike is found in the time interval @xmath224 , and `` 0 '' otherwise .",
    "we choose @xmath225 $ ] in order to maximise @xmath226 .",
    "each neuron produces a symbolic sequence that is split into small non - overlapping sequences of length @xmath227=8 .",
    "m. galarreta and s. hestrin , nature * 402 * , 72 ( 1999 ) .",
    "j. r. gibson et al .",
    "nature * 402 * , 75 ( 1999 ) .",
    "s. hestrin and m. galarreta , trends in neurosci * 28 * , 304 ( 2005 ) .",
    "m. galarreta and s. hestrin , nat .",
    "neurosci * 2 * , 425 ( 2001 ) .",
    "b. w. connors and m. a. long , annu .",
    "neurosci * 27 * , 393 ( 2004 ) .",
    "s. cosenza , p. crucitti , l. fortuna , m. frasca , m. la rosa , c. stagni , and l. usai , math .",
    "biosciences and engineering * 2 * , 53 ( 2005 ) .",
    "i. belykh , e. de lange and m. hasler , phys .",
    "lett . * 94 * 188101 ( 2005 ) .",
    "r. femat , j. a - ramirez and g. f .- anaya , physica d * 139 * , 231 ( 2000 ) .",
    "f. m. moukam kakmeni , s. bowong , c. tchawoua , and e. kaptouom , phys .",
    "a * 322 * , 263 ( 2004 ) .",
    "s. bowong , f. m. moukam kakmeni and c. tchawoua , phys .",
    "e * 70 * , 066217 ( 2004 ) .",
    "d. ruelle , bol .",
    ". bras . mat .",
    ", * 9 * , 83 ( 1978 ) ."
  ],
  "abstract_text": [
    "<S> in this work we studied the combined action of chemical and electrical synapses in small networks of hindmarsh - rose ( hr ) neurons on the synchronous behaviour and on the rate of information produced ( per time unit ) by the networks . </S>",
    "<S> we show that if the chemical synapse is excitatory , the larger the chemical synapse strength used the smaller the electrical synapse strength needed to achieve complete synchronisation , and for moderate synaptic strengths one should expect to find desynchronous behaviour . otherwise , if the chemical synapse is inhibitory , the larger the chemical synapse strength used the larger the electrical synapse strength needed to achieve complete synchronisation , and for moderate synaptic strengths one should expect to find synchronous behaviours . </S>",
    "<S> finally , we show how to calculate semi - analytically an upper bound for the rate of information produced per time unit ( kolmogorov - sinai entropy ) in larger networks . as an application </S>",
    "<S> , we show that this upper bound is linearly proportional to the number of neurons in a network whose neurons are highly connected . </S>"
  ]
}