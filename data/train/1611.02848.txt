{
  "article_text": [
    "a matrix @xmath0th root ( @xmath3 ) of @xmath4 is defined as a solution of the following matrix equation : @xmath5 while this matrix equation might have infinitely many solutions , the target of this paper is a solution whose eigenvalues lie in the set @xmath6 . if @xmath7 has no nonpositive real eigenvalues , the target solution is unique ( * ? ? ? * theorem 7.2 )  and is referred to as the principal matrix @xmath0th root of @xmath7 , denoted by the symbol @xmath8 . throughout this paper",
    ", @xmath7 is assumed to have no nonpositive real eigenvalues .",
    "the principal matrix @xmath0th root arises in lattice quantum chromodynamics ( qcd ) calculations @xcite and in the computation of the matrix logarithm @xcite that corresponds to the inverse function of the matrix exponential .",
    "therefore , numerical algorithms for computing the principal matrix @xmath0th root have been developed during the past decade .",
    "numerical algorithms for the principal matrix @xmath0th root can be classified roughly into direct methods and iterative methods .",
    "direct methods include , for example , the schur method @xcite , the matrix sign method @xcite , and a method based on repeated eigenvalues of @xmath7 @xcite .",
    "the schur method can be performed in @xmath1 flops , the matrix sign method can be performed in at least @xmath9 flops , and the method based on repeated eigenvalues requires all eigenvalues of @xmath7 .",
    "therefore , in terms of computational cost , the schur method is likely the method of choice for large - scale problems .",
    "iterative methods include newton s method and halley s method for @xmath8 , proposed by iannazzo @xcite , and newton s method for @xmath10 , proposed by guo @xcite . in this paper",
    ", we consider newton s method for @xmath8 , since that method is the most fundamental iterative method . in addition",
    ", it has been reported that newton s method for @xmath8 gives a more accurate solution than the schur method for some ill - conditioned problems @xcite .",
    "now , let us recall several results for newton s method by iannazzo @xcite .",
    "it is known that newton s method for a matrix @xmath0th root can be written as @xmath11 with an initial guess @xmath12 satisfying @xmath13 .",
    "however , it is not always guaranteed that this method converges to the principal @xmath0th root .",
    "iannazzo showed that if both of the following conditions ,    align & a , [ eq : condition1 ] + & x_0=i , [ eq : condition2 ]    are satisfied , then newton s method converges to @xmath8 .",
    "next , iannazzo proposed a preconditioning step , computing @xmath14 , because then @xmath15 satisfies the condition for any @xmath7 .    even if the matrix @xmath7 is preconditioned , newton s iteration could be unstable in the neighborhood of @xmath8 @xcite",
    "then , iannazzo proposed three stable iterations : @xmath16 @xmath17 and @xmath18                 n_{k+1 } = \\qty(\\cfrac{(p-1)i+n_k}{p})^{-p}n_k .",
    "\\end{cases }             \\quad\\qty\\big(x_0 = i,\\ n_0 = a )          \\end{aligned}\\ ] ] in particular , iteration is called incremental newton ( in ) iteration , and iteration is called coupled newton iteration .",
    "it is known that newton s method converges quadratically in a neighborhood of the solution , but global convergence of that method is not guaranteed .",
    "one way to globalize the convergence of newton s method is by using damping.)$ ] , where @xmath19 is a relaxation factor chosen to reduce residuals . ] from this point of view , it might be possible to apply damping to in iteration and iteration . comparing these two iterations , the cost of in iteration is @xmath1 flops per iteration , higher than @xmath20 flops for iteration .",
    "on the other hand , the incremental part of in iteration is computed in the form of @xmath21 , in contrast to iteration .",
    "this characteristic of in iteration might provide a new viewpoint for convergence analysis to confirm that @xmath22 converges to @xmath23 .",
    "that is to say , if @xmath24 explicitly includes @xmath22 , then @xmath24 is represented as @xmath25 , and its convergence behavior might be analyzed using composite mapping @xmath26 and initial matrix @xmath27 .",
    "thus , in iteration is worth considering .",
    "the purpose of this paper is to provide a cost - efficient variant of in iteration whose increment part is computed in the form @xmath28 . in this paper",
    ", we reduce the cost of in iteration by finding a specific matrix polynomial in in iteration and proposing a decomposition of the matrix polynomial .",
    "the remainder of this paper is organized as follows . in section 2 ,",
    "a variant of in iteration is shown , and we numerically estimate its cost at @xmath20 flops per iteration . in section 3 , we present the results of numerical experiments .",
    "we conclude in section 4 .",
    "the computational cost for computing the increment part @xmath29 is the highest in in iteration , because @xmath30 flops are required for eq . , and @xmath31 flops for in iteration .",
    "in this section , without losing the previous matrix @xmath22 , eq . is rewritten to reduce the number of matrix multiplications whose computational costs are @xmath32 flops .      from the definition of in iteration , the increment @xmath22 is equivalent to @xmath33 , and thus @xmath34 substituting this relation into eq .",
    "yields @xmath35h_k\\\\              & = -\\frac{1}{p } \\qty\\bigg {                  \\qty\\big[-(p-1)f_k+pi]\\qty\\big [                      i+f_k + f_k^2 + \\dots + f_k^{p-2 }                      ] -(p-1)i                  } h_k .",
    "\\label{eq : ceinhk }          \\end{aligned}\\ ] ] introducing the matrix polynomial @xmath36 enables eq . to be simplified further to @xmath37p_{p-2}(f_k)-(p-1)i                  } h_k .",
    "\\end{aligned}\\ ] ] the number of matrix multiplications for eq . is equal to the number of matrix multiplications for @xmath38 plus two .",
    "we now define a variant of in iteration as @xmath39p_{p-2}(f_k)-(p-1)i              } h_k .",
    "\\end{cases }          \\end{aligned}\\ ] ] this new expression motivates us to reduce the number of matrix multiplications for computing @xmath38 .",
    "furthermore , this variant is as stable as original in iteration .",
    "we use the following definition of stability to analyze the variant .",
    "[ def : stability ] consider an iteration @xmath40 with a fixed point @xmath41 .",
    "assume that @xmath42 is frchet differentiable at @xmath41 .",
    "the iteration is stable in a neighborhood of @xmath41 if the frchet derivative @xmath43 has bounded powers , that is , there exists a constant c such that @xmath44 for all @xmath45 , where @xmath46 is @xmath47th power of the frchet derivative @xmath48 at @xmath41 , defined as @xmath47-fold composition ; thus @xmath49 .",
    "then , we show that the variant is stable .",
    "the variant is stable .",
    "the iteration function for the variant is @xmath50p_{p-2}(f)-(p-1)i                          } h                      ]                  \\quad \\qty(f = x(x+h)^{-1 } ) ,              \\end{aligned}\\ ] ] and the fixed point is @xmath51 $ ] .",
    "then , the frchet derivative of @xmath52 is @xmath53 ,              \\end{aligned}\\ ] ] where @xmath54f^{i+1 }                  } , \\label{eq : proof01}\\\\                  c_h = & -\\frac{1}{p}\\qty {                      \\qty[-(p-1)f+pi]\\sum_{i=0}^{p-2}f^i - ( p-1)i                      } \\\\                      & + \\frac{1}{p}x^{-1}h\\sum_{i=0}^{p-2}\\qty {                      \\qty [                        -(i+1)(p-1)f + ip                      ] f^{i+1 }                  } .",
    "\\label{eq : proof02 }              \\end{aligned}\\ ] ] on the fixed point of @xmath52 , we find @xmath55 by substituting @xmath56 , and @xmath57 into eqs . and .",
    "hence , @xmath58\\mqty[e_x\\\\e_h ] .",
    "\\end{aligned}\\ ] ] the matrix @xmath59 $ ] is idempotent because @xmath60 ^ 2 = \\mqty[i&i\\\\{o}&{o}].\\ ] ] then , for all @xmath61 , @xmath62 is bounded . from the above ,",
    "the variant is stable .    in the next subsection",
    ", we provide a means of reducing matrix multiplications of @xmath38 .      if @xmath63 , the matrix polynomial @xmath64 can be rewritten in a more efficient form : @xmath65    on the right - hand side of eq .",
    ", there is a new matrix polynomial whose variable is @xmath66 and degree is approximately half of @xmath67 .",
    "this decomposition reduces the number of matrix multiplications by almost a factor of two .",
    "thus , the number of matrix multiplications of @xmath64 is reduced by applying the decomposition to @xmath64 repeatedly .",
    "let us show the example of @xmath68",
    ". appears when calculating the matrix @xmath69th root . ]",
    "@xmath70\\qty[x^4+i]\\qty[x^4+x^2]+i                  } \\qty\\big{x+i}.\\label{eq : ceinexamplefin }          \\end{aligned}\\ ] ] in this example , @xmath71 of eq .",
    "is computed using 56 matrix multiplications by naive implementation . on the other hand , after applying the decomposition to eq .",
    "four times , eq . can be computed with nine matrix multiplications . in detail , five matrix multiplications are required for constructing five intermediate matrices , @xmath72 , and @xmath73 , and another four matrix multiplications are required for multiplication of the subpolynomials .    finally , we combine variant with decomposition into algorithm [ alg : algorithm1 ] for practice .",
    "@xmath4 ( satisfying condition in section 1 ) , @xmath3 @xmath74 decompose @xmath75 by applying the decomposition repeatedly .",
    "@xmath76 ( @xmath77 condition ) @xmath78 @xmath79 @xmath80 compute @xmath38 @xmath81p_{p-2}(f_k)-(p-1)i                              } h_k$ ] @xmath82      we calculated the computational cost of the variant for @xmath83 $ ] numerically and found that cost to be consistent with @xmath84 .    while a proof that the cost of variant is @xmath2 flops per iteration is left for future work , this numerical result agrees with that expectation .",
    "in addition , we calculated the costs of in iteration and the iteration for @xmath83 $ ] to compare them with that of variant . the result is shown in fig .",
    "[ fig : calccost ] .",
    "it is clear from the figure that the computational cost of the variant is lower than that of in iteration and competitive with that of iteration .",
    "for example , when @xmath85 , the computational cost of variant is approximately a quarter of that of in iteration and slightly higher than that of iteration .",
    "this section describes a numerical experiment in which the principal @xmath69th roots of test matrices are calculated .",
    "the test matrices are described in table [ tab : numexptestmat ] .",
    ".test matrices . [ cols=\"<,>,>,>,<\",options=\"header \" , ]     [ tab : numexpcos ]    for this experiment , python 3.5 was used for programming , and intel@xmath86 core@xmath87 i7 2.8ghz cpu and 8 gb ram were used for computation .    first , figure [ fig : numexp1 ] shows the ratios of computation time of these three iterations . from fig .",
    "[ fig : numexp1 ] , the computation time of variant is approximately one fourth of that of in iteration and slightly longer than that of in all cases . here",
    "it can be seen that both the computation time and the computational cost decreased .",
    "next , figure [ fig : numexp2 ] shows the relative residual defined as @xmath88 for these three iterations .",
    "the figure shows that the convergence behavior of variant differs little from that of in iteration and iteration .",
    "since there is some possibility of numerical cancelation of variant , in iteration is slightly better than variant in terms of accuracy .",
    "[ fig : numexp2_1 ]     [ fig : numexp2_2 ]     +   +     [ fig : numexp2_3 ]",
    "in this paper , a variant of in iteration is proposed whose computational cost is estimated at @xmath20 flops per iteration and whose increment part still has the form @xmath28 .",
    "we have learned from the results of the numerical experiment that the variant is competitive with iteration in terms of accuracy and computation time .",
    "the proposed variant therefore becomes a choice for practical application .",
    "future work is to reduce the computation time of newton s method for the principal matrix @xmath0th root by reducing the number of iterations .",
    "however ,    it is not clear how to choose a better initial guess than the conventional initial guess @xmath89 ( the identity matrix )",
    ". it might be easier to find a good initial guess , when considering the damped newton method .",
    "this work has been supported in part by jsps kakenhi ( grant no .",
    "26286088 ) ."
  ],
  "abstract_text": [
    "<S> incremental newton ( in ) iteration , proposed by iannazzo , is stable for computing the matrix @xmath0th root , and its computational cost is @xmath1 flops per iteration . in this paper , a cost - efficient variant of in iteration is presented . </S>",
    "<S> the computational cost of the variant is estimated at @xmath2 flops per iteration .    * keywords * matrix @xmath0th root ; matrix polynomial . + * mr(2010 ) subject classification * 65f30 ; 65f60 ; 65h04 </S>"
  ]
}