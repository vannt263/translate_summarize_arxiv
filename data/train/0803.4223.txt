{
  "article_text": [
    "one of the key concepts behind grid computing is the transparent use of distributed compute and storage resources .",
    "users of the grid should not need to know where their data resides , nor where it is processed and analysed .    when the large hadron collider at cern begins to run at luminosities sufficient for physics studies",
    ", it will produce around 15 petabytes of data a year . in order to analyse such a large quantity of information , the worldwide lhc computing grid ( wlcg )",
    "has been created .",
    "this is an international collaboration of physics laboratories and institutes , spread across three major grids ( egee , osg and nordugrid ) .",
    "the uk s grid for particle physics ( gridpp ) @xcite started in 2001 with the aim of creating a computing grid that would meet the needs of particle physicists working on the next generation of particle physics experiments , such as the lhc . to meet this aim",
    ", participating institutions were organised into a set of tier-2 centres according to their geographical location .",
    "scotgrid @xcite is one such distributed tier-2 computing centre formed as a collaboration between the universities of durham , edinburgh and glasgow . to the grid ,",
    "the three collaborating institutes appear as individual sites .",
    "currently , the close association between them exists only at a managerial and technical support level .",
    "one of the aims of this paper is to study the possibility of having even closer - coupling between the sites of scotgrid ( or other collections of sites on the grid ) from the point of view of accessing storage resources from geographically distributed computing centres .",
    "current estimates suggest that the maximum rate with which a physics analysis job can read data is 2mb / s .",
    "we want to investigate if such a rate is possible using distributed storage when using production quality hardware that is currently operational on the wlcg grid .",
    "physics analysis code will use the posix - like lan access protocols to read data , which for dpm is the remote file i / o protocol ( rfio ) .",
    "data transport using gridftp across the wan access has been considered previously @xcite .",
    "the structure of this paper is as follows .",
    "section [ sec : middleware ] describes the storage middleware technology that we use in this study .",
    "we further motivate this work in section [ sec : wan ] .",
    "section [ sec : hardware ] discusses the storage , compute and networking hardware that is employed to perform the testing .",
    "section [ sec : testing ] explains the reasoning behind our testing methodology .",
    "we present and interpret the results of this testing in section [ sec : results ] , present future work in [ sec : future ] and conclude in section [ sec : conclusions ] .",
    "the disk pool manager ( dpm ) @xcite is a storage middleware product created at cern as part of the egee @xcite project .",
    "it has been developed as a lightweight solution for disk storage management at tier-2 institutes . _ a priori _ , there is no limitation on the amount of disk space that the dpm can handle .",
    "the dpm consists of the following component servers ,    * dpm ( dpm ) : keeps track of all requests for file access . *",
    "dpm name server ( dpnsd ) : handles the namespace for all files under the control of dpm . * drpm rfio ( rfiod ) : handles the transfers for the rfio protocol ( see section [ sec : protocols ] ) . * dpm gridftp ( dpm - gsiftp ) : handles data transfers requiring use of the gridftp protocol ( see section [ sec : protocols ] ) .",
    "* storage resource manager ( srmv1 , srmv2 , srmv2.2 ) : receives the srm requests , passing them on to the dpm server .",
    "the protocols listed above will be described in the section [ sec : protocols ] .",
    "figure [ fig : dpm_arch ] shows how the components can be configured in an instance of dpm . typically at a tier-2 the server daemons ( ` dpm ` , ` dpns ` , ` srm ` )",
    "are shared on one dpm _ headnode _",
    ", with separate large disk servers actually storing and serving files , running ` dpm - gsiftp ` and ` rfiod ` servers .    ]",
    "dpm currently uses two different protocols for data transfer and one for storage management ,    * gridftp : typically used for wide area transfer of data files , e.g. , movement of data from tier-1 to tier-2 storage . * remote file io ( rfio ) : gsi - enabled @xcite protocol which provides posix @xcite file operations , permitting byte - level access to files . *",
    "storage resource manager : this standard interface is used on the wlcg grid to permit the different storage server and client implementations to interoperate .    rfio is the protocol that physics analysis code should use in order to read data stored within a dpm instance and is the protocol used to perform the tests in this paper .",
    "client applications can link against the rfio client library permits byte level access to files stored on a dpm .",
    "the library allows for four different modes of operation ,    * 0 : normal read with one request to the server . *",
    "rfio@xmath0readbuf : an internal buffer is allocated in the client api , each call to the server fills this buffer and the user buffer is filled from the internal buffer .",
    "there is one server call per buffer fill .",
    "* rfio@xmath0readahead : rfio@xmath0readbuf is forced on and an internal buffer is allocated in the client api , then an initial call is sent to the server which pushes data to the client until end of file is reached or an error occurs or a new request comes from the client .",
    "* rfio@xmath0stream ( v3):this read mode opens 2 connections between the client and server , one data socket and one control socket .",
    "this allows the overlap of disk and network operations .",
    "data is pushed on the data socket until eof is reached .",
    "transfer is interrupted by sending a packet on the control socket .",
    "current grid middleware is designed such that analysis jobs are sent to the site where the data resides .",
    "the work presented in this paper presents an alternative use case where analysis jobs can use rfio for access to data held on a dpm which is remote to the location analysis job is processed .",
    "this is of interest due to a number of reasons ,    * data at a site may be heavily subscribed by user analysis jobs , leading to many jobs being queued while remote computing resources remain under used .",
    "one solution ( which is currently used in wlcg ) is to replicate the data across multiple sites , putting it close to a variety of computing centres .",
    "another would be to allow access to the data at a site from remote centres , which would help to optimise the use of grid resources . *",
    "the continued expansion of national and international low latency optical fibre networks suggest that accessing data across the wide area network could provide the dedicated bandwidth that physics analysis jobs will require in a production environment .",
    "* simplification of vo data management models due to the fact that any data is , in essence , available from any computing centre .",
    "the atlas computing model already has the concept of a `` cloud '' of sites which store datasets .",
    "rfio uses the grid security infrastructure ( gsi ) model , meaning that clients using the protocol require x.509 grid certificate signed by a trusted certificate authority .",
    "therefore , within the framework of x.509 , rfio can be used over the wide area network without fear of data being compromised .",
    "additional ports must be opened in the site firewall to allow access to clients using rfio .",
    "they are listed below .",
    "data transfer will use the site defined rfio port range .    * 5001 : for access to the rfio server . * 5010 : for namespace operations via the dpns server .",
    "* 5015 : for access to the dpm server .",
    "yaim @xcite was used to install v1.6.5 of dpm on a dual core disk server with 2 gb of ram .",
    "the server was running sl4.3 32bit with a 2.6.9 - 42 kernel .",
    "vdt1.2 was used @xcite .",
    "all dpm services were deployed on the same server .",
    "a single disk pool was populated with a 300 gb filesystem .      to facilitate the testing",
    ", we had use of the uki - scotgrid - glasgow wlcg grid site @xcite .",
    "the computing cluster is composed of 140 dual core , dual cpu opteron 282 processing nodes with 8 gb of ram each .",
    "being a production site , the compute cluster was typically processing user analysis and experimental monte carlo production jobs while our performance studies were ongoing .",
    "however , observation showed that jobs on the cluster were typically cpu bound , performing little i / o .",
    "as our test jobs are just the opposite ( little cpu , i / o and network bound ) tests were able to be performed while the cluster was still in production .      the clients and servers at uki - scotgrid - glasgow and scotgrid - edinburgh are connected ( via local campus routing ) to the janet - uk production academic network @xcite using gigabit ethernet .",
    "figure [ fig : path ] shows the results of running iperf between the two sites .",
    "this shows that the maximum file transfer rate that we could hope to achieve in our studies is approximately 900mb / s ( 100mb / s ) .",
    "the round trip time for this connection is 12ms .    ]",
    "[ cols= \" < , > \" , ]",
    "as shown in section [ sec : lan ] , use of the production network for the data transfer limited the data throughput relative to that of the lan .",
    "we plan to continue this work by making use of a newly provisioned lightpath between the two glasgow and edinburgh . this will provide dedicated bandwidth with an rtt of around 2ms .",
    "we would like to study more realistic use cases involving real physics analysis code .",
    "in particular , we would like to make use of the root @xcite ttreecache object which has been shown @xcite to give efficient access to root objects across the wan . as root is the primary tool for analysing physics data , it is essential that the performance benefits of this access method are understood .",
    "the tests performed so far have been designed to simulate access to the dpm by clients that operate in an expected manner ( i.e. , open file , read some data , close file ) .",
    "it would be an interesting exercise to perform a quantitative study of the storage element and network when presented unexpected non - ideal use cases of the protocol .",
    "the dpm architecture allows for the core services and disk servers ( section [ sec : arch ] ) to be spread across different network domains .",
    "therefore , rather than having separate dpm instances at each site , we could create a single instance that spans all collaborating institutes .",
    "however , there are disadvantages to this this approach , as an inter - site network outage could take down the entire system .",
    "finally , dpm does not currently have the concept of how `` expensive '' a particular data movement operation would be , which could impact the behaviour and performance of this setup .",
    "in addition to rfio , xrootd has recently been added as an access protocol .",
    "this implementation currently lacks gsi security @xcite , making it unsuitable for use across the wan .",
    "once security is enabled , it would be interesting to perform a similar set of tests to those presented above .",
    "similar tests should be performed when dpm implements v4.1 of the nfs protocol @xcite .",
    "through this work we have shown that it is possible to use rfio to provide byte level access to files stored in an instance of dpm across the wide area network .",
    "this has shown the possibility of unifying storage resources across distributed grid computing and data centres , which is of particular relevance to the model of distributed tier-2 sites found within the uk gridpp project . furthermore , this should be of interest to the data management operations of virtual organisations using grid infrastructure as it could lead to optimised access to compute and data resources , possibly leading to a simpler data management model .    using a custom client application ,",
    "we have studied the behaviour of rfio access across the wan as a function of number of simultaneous clients accessing the dpm ; the different rfio modes ; the application block size and the rfio buffer size on the client side .",
    "we looked at the effect of varying the tcp window size on the data throughput rates and found that it had little effect , particularly when a large number of clients were simultaneously reading data .",
    "further work should be done to explore application optimisations before looking at the networking stack .",
    "our testing has shown that rfio streaming mode leads to the highest overall data transfer rates when sequentially reading data .",
    "the rates achieved were of order 62mib / s on the production janet - uk network between the uki - scotgrid - glasgow and scotgrid - edinburgh sites .",
    "for the case where the client only accesses 10% of the file , rfio mode in normal mode was shown to lead to the best overall throughput as it does not transfer data that is not requested by the client .",
    "for all rfio modes , file open times increase linearly with the number of simultaneous clients , from @xmath1s with small number of clients up to @xmath2s with 64 clients .",
    "this increase is to be expected , but it is unclear at this time how it will impact on actual vo analysis code .    finally , we have shown that it is possible to access remote data using a protocol that is typically only used for access to local grid storage .",
    "this could lead to a new way of looking at storage resources on the grid and could ultimately impact on how data is efficiently and optimally managed on the grid .",
    "the authors would like to thank all those who helped in the preparation of this work .",
    "particular mention should go to dpm developers for very helpful discussions and b garrett at the university of edinburgh .",
    "this work was funded by stfc / pparc via the gridpp project .",
    "g stewart is funded by the eu egee project ."
  ],
  "abstract_text": [
    "<S> the start of data taking at the large hadron collider will herald a new era in data volumes and distributed processing in particle physics . </S>",
    "<S> data volumes of hundreds of terabytes will be shipped to tier-2 centres for analysis by the lhc experiments using the worldwide lhc computing grid ( wlcg ) .    in many countries tier-2 </S>",
    "<S> centres are distributed between a number of institutes , e.g. , the geographically spread tier-2s of gridpp in the uk . </S>",
    "<S> this presents a number of challenges for experiments to utilise these centres efficaciously , as cpu and storage resources may be sub - divided and exposed in smaller units than the experiment would ideally want to work with . in addition </S>",
    "<S> , unhelpful mismatches between storage and cpu at the individual centres may be seen , which make efficient exploitation of a tier-2 s resources difficult .    one method of addressing this is to unify the storage across a distributed tier-2 , presenting the centres aggregated storage as a single system . </S>",
    "<S> this greatly simplifies data management for the vo , which then can access a greater amount of data across the tier-2 . </S>",
    "<S> however , such an approach will lead to scenarios where analysis jobs on one site s batch system must access data hosted on another site .    </S>",
    "<S> we investigate this situation using the glasgow and edinburgh clusters , which are part of the scotgrid distributed tier-2 . </S>",
    "<S> in particular we look at how to mitigate the problems associated with `` distant '' data access and discuss the security implications of having lan access protocols traverse the wan between centres . </S>"
  ]
}