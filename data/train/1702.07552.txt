{
  "article_text": [
    "given i.i.d samples @xmath0 drawn from some unknown probability distribution @xmath1 on @xmath2 , where @xmath3 is an arbitrary set and @xmath4 , the goal to explore the conditional distribution of @xmath5 given @xmath6 beyond the center of the distribution can be achieved by using both quantile and expectile regression .",
    "the well - known quantiles are obtained by minimizing asymmetric least absolute deviation ( alad ) loss function proposed by @xcite , whereas expectiles are computed by minimizing asymmetric least square ( als ) loss function @xmath7 for all @xmath8 and a fixed @xmath9 , see primarily @xcite and also @xcite for further references .",
    "these expectiles have attracted considerable attention in recent years and have been applied successfully in many areas , for instance , in demography @xcite , in education @xcite and extensively in finance @xcite .",
    "in fact , it has recently been shown ( see , e.g.  @xcite , @xcite ) that expectiles are the only risk measures that enjoy the properties of coherence and elicitability , see @xcite , and therefore they have been suggested as potentially better alternative to both value at risk ( var ) and expected shortfall ( es ) , see e.g.  @xcite . in order to see more applications of expectiles , we refer the interested readers to , e.g.  @xcite .",
    "both quantiles and expectiles are special cases of so - called asymmetric @xmath10-estimators ( see @xcite ) and there exists one - to - one mapping between them ( see , e.g.  @xcite , @xcite and @xcite ) , in general , however , expectiles do not coincide with quantiles .",
    "hence , the choice between expectiles and quantiles mainly depends on the application at hand , as it is the case in the duality between the mean and the median .",
    "for example , if the goal is to estimate the ( conditional ) threshold for which @xmath11-fraction of ( conditional ) observations lie below that threshold , then @xmath11-quantile regression is the right choice . on the other hand , if one is interested to estimate the ( conditional ) threshold for which the average of below threshold excess information ( deviations of observations from threshold ) is @xmath12 times larger then above that threshold , then the @xmath11-expectile regression is a preferable choice with @xmath13 , see @xcite . in other words ,",
    "the focus in quantiles is the ordering of observations while expectiles account magnitude of the observations , which makes expectiles sensitive to the extreme values of the distribution and this sensitivity thus play a key role in computing the es in finance . since , estimating expectiles is computationally more efficient than quantiles , one can however use expectiles as a promising surrogate of quantiles in the situation where one is only interested to explore the conditional distribution .",
    "as already mentioned above , @xmath14-expectiles can be computed with the help of asymmetric risks @xmath15 where @xmath16 is the data generating distribution on @xmath17 and @xmath18 is some predictor . to be more precise ,",
    "there exists a @xmath19-almost surely unique function @xmath20 satisfying @xmath21 and @xmath22 equals @xmath11-expectile of the conditional distribution @xmath23 for @xmath19-almost all @xmath24 .",
    "some semiparametric and nonparametric methods for estimating conditional @xmath11-expectiles with the help of empirical @xmath25-risk have already been proposed in literature , see e.g.  @xcite for further details .",
    "recently , @xcite proposed an another nonparametric estimation method that belongs to the family of so - called kernel based regularized empirical risk minimization , which solves an optimization problem of the form @xmath26 here , @xmath27 is a user specified regularization parameter , @xmath28 is a reproducing kernel hilbert space ( rkhs ) over @xmath3 with reproducing kernel @xmath12 ( see , e.g.  @xcite and ( * ? ? ?",
    "* chapter 4.2 ) ) and @xmath29 denotes the empirical risk of @xmath30 , that is @xmath31 since the als loss @xmath32 is convex , so is the optimization problem ( [ svm - expectile ] ) and by ( * ? ? ?",
    "* lemma 5.1 , theorem 5.2 ) there always exits a unique @xmath33 that satisfies ( [ svm - expectile ] ) .",
    "moreover , the solution of @xmath33 is of the form @xmath34 where @xmath35 for all @xmath36 , see @xcite for further details .",
    "learning method of the form ( [ svm - expectile ] ) but with different loss functions have attracted many theoretical and algorithmic considerations , see for instance @xcite for least square regression , @xcite for quantile regression and @xcite for classification with hinge loss . in addition , @xcite recently proposed an algorithm for solving , that is now a part of @xcite , and compared its performance to , see @xcite , which is another algorithm minimizing an empirical @xmath25-risk .",
    "the main goal of this article is to complement the empirical findings of @xcite with a detailed statistical analysis .    a typical way to access",
    "the quality of an estimator @xmath37 is to measure its distance to the target function @xmath20 , e.g.  in terms of @xmath38 . for estimators obtained by some empirical risk minimization scheme , however",
    ", one can hardly ever estimate this @xmath39-norm directly .",
    "instead , the standard tools of statistical learning theory give bounds on the excess risk @xmath40 .",
    "therefore , our first goal of this paper is to establish a so - called calibration inequality that relates both quantities . to be more precise",
    ", we will show in theorem [ self - calibration - inquality ] that @xmath41 holds for all @xmath42 and some constant @xmath43 only depending on @xmath14 .",
    "in particular , provides rates for @xmath38 as soon as we have established rates for @xmath40 .",
    "furthermore , it is common knowledge in statistical learning theory that bounds on @xmath40 can be improved if so - called variance bounds are available .",
    "we will see in lemma [ lemma - supremum and variance bound ] that leads to an optimal variance bound for @xmath32 whenever @xmath5 is bounded .",
    "note that both and the variance bound are independent of the considered expectile estimation method .",
    "in fact , both results are key ingredients for the statistical analysis of any expectile estimation method based on some form of empirical risk minimization .    as already indicated above , however , the main goal of this paper is to provide a statistical analysis of the svm - type estimator @xmath33 given by .",
    "since @xmath44 equals the least squares loss , any statistical analysis of also provides results for svms using the least squares loss .",
    "the latter have already been extensively investigated in the literature .",
    "for example , learning rates for generic kernels can be found in @xcite and the references therein . among these articles ,",
    "only @xcite obtain learning rates in minimax sense under some specific assumptions .",
    "for example , @xcite assumes that the target function @xmath45 , while @xcite establish optimal learning rates for the case in which @xmath28 does not contain the target function .",
    "in addition , @xcite has recently established ( essentially ) asymptotically optimal learning rates for least square svms using gaussian rbf kernels under the assumption that the target function @xmath46 is contained in some sobolev or besov space @xmath47 with smoothness index @xmath48 . a key ingredient of this work is to control the capacity of rkhs @xmath49 for gaussian rbf kernel @xmath50 on the closed unit euclidean ball @xmath51 by an entropy number bound @xmath52 see ( * ? ? ? * theorem 6.27 ) , which holds for all @xmath53 $ ] and @xmath54 $ ] .",
    "unfortunately , the constant @xmath55 derived from ( * ? ? ?",
    "* theorem 6.27 ) depends on @xmath56 in an unknown manner . as a consequence ,",
    "@xcite were only able to show learning rates of the form @xmath57 for all @xmath58 . to address this issue , we use ( * ? ? ?",
    "* lemma 4.5 ) to derive the following new entropy number bound @xmath59 which holds for all @xmath54 $ ] and @xmath53 $ ] and some constant @xmath60 only depending on @xmath61 .",
    "in other words , we establish an upper bound for @xmath55 whose dependence on @xmath56 is explicitly known . using this new bound , we are then able to find improved learning rates of the form @xmath62",
    "clearly these new rates replace the nuisance factor @xmath63 of @xcite by some logarithmic term , and up to this logarithmic factor our new rates are minimax optimal , see @xcite for details . in addition , our new rates also hold for @xmath64 , that is for general expectiles .    the rest of this paper is organized as follows . in section [ sec - head - propertiesals ] , some properties of the als loss function are established including the self - calibration inequality and variance bound",
    ". section [ sec - head - oraclelearningrates ] presents oracle inequalities and learning rates for and gaussian rbf kernels .",
    "the proofs of our results can be found in section [ sec - head - proofs ] .",
    "this section contains some properties of the als loss function i.e.  convexity , local lipschitz continuity , a self - calibration inequality , a supremum bound and a variance bound . throughout this section , we assume that @xmath3 is an arbitrary , non - empty set equipped with @xmath65-algebra , and @xmath4 denotes a closed non - empty set .",
    "in addition , we assume that @xmath1 is the probability distribution on @xmath2 , @xmath23 is a regular conditional probability distribution on @xmath5 given @xmath24 and @xmath66 is a some distribution on @xmath5 .",
    "furthermore , @xmath67 is the als loss defined by and @xmath68 is a measurable function .",
    "it is trivial to prove that @xmath25 is convex in @xmath69 , and this convexity ensures that the optimization problem ( [ svm - expectile ] ) is efficiently solvable .",
    "moreover , by ( * ? ? ?",
    "* lemma 2.13 ) convexity of @xmath25 implies convexity of corresponding risks . in the following , we present the idea of clipping to restrict the prediction @xmath69 to the domain @xmath70",
    "$ ] where @xmath71 , see e.g.  ( * ? ? ?",
    "* definition 2.22 ) .",
    "[ definition - clipping ] we say that a loss @xmath72 can be clipped at @xmath71 , if , for all @xmath73 , we have @xmath74 where @xmath75 denotes the clipped value of @xmath69 at @xmath76 , that is @xmath77\\,,\\\\         m & \\hspace*{6ex } \\text{if } \\hspace*{2ex } t > m\\ , .   \\end{array}\\right . \\end{aligned}\\ ] ] moreover , we say that @xmath78 can be clipped if @xmath69 can be clipped at some @xmath79 .",
    "recall that this clipping assumption has already been utilized while establishing learning rates for svms , see for instance @xcite for hinge loss and @xcite for pinball loss .",
    "it is trivial to show by convexity of @xmath25 together with ( * ? ? ?",
    "* lemma 2.23 ) that @xmath25 can be clipped at @xmath10 and has at least one global minimizer in @xmath80 $ ] .",
    "this also implies that @xmath81 for every @xmath82 . in other words ,",
    "the clipping operation potentially reduces the risks .",
    "we therefore bound the risk @xmath83 of the clipped decision function rather than the risk @xmath84 , which we will see in details in section [ sec - head - oraclelearningrates ] . from a practical point of view , this means that the _ training _",
    "algorithm for ( [ svm - expectile ] ) remains unchanged and the _ evaluation _ of the resulting decision function requires only a slight change . for further details on algorithmic advantages of clipping for svms using the hinge loss and the als loss ,",
    "we refer the reader to @xcite and @xcite respectively .",
    "it is also observed in @xcite that @xmath85-bounds , see section [ sec - head - oraclelearningrates ] , can be made smaller by clipping the decision function for some loss functions .    let us further recall from (",
    "* definition 2.18 ) that a loss function is called locally lipschitz continuous if for all @xmath86 there exists a constant @xmath87 such that @xmath88\\,.\\end{aligned}\\ ] ] in the following we denote for a given @xmath86 the smallest such constant @xmath87 by @xmath89 .",
    "the following lemma , which we will need for our proofs , shows that the als loss is locally lipschitz continuous .",
    "[ lemma - lipschitz - constant ] let @xmath90 $ ] and @xmath91 , then the loss function @xmath92 is locally lipschitz continuous with lipschitz constant @xmath93 where @xmath94 .    for later use note that @xmath25 being locally lipschitz continuous implies that @xmath25 is also a _ nemitski loss _ in the sense of ( * ? ? ?",
    "* definition 18 ) , and by ( * ? ? ?",
    "* lemma 2.13 2.19 ) , this further implies that the corresponding risk @xmath95 is convex and locally lipschitz continuous .",
    "empirical methods of estimating expectile using @xmath32 loss typically lead to the function @xmath37 for which @xmath96 is close to @xmath97 with high probability .",
    "the convexity of @xmath98 then ensures that @xmath99 approximates @xmath20 in a weak sense , namely in probability @xmath19 , see ( * ? ? ?",
    "* remark 3.18 ) .",
    "however , no guarantee on the speed of this convergence can be given , even if we know the convergence rate of @xmath100 .",
    "the following theorem addresses this issue by establishing a so - called calibration inequality for the excess @xmath98-risk .",
    "[ self - calibration - inquality ] let @xmath25 be the als loss function defined by ( [ lossals ] ) and @xmath1 be the distribution on @xmath101",
    ". moreover , assume that @xmath102 is the conditional @xmath11-expectile for fixed @xmath9 .",
    "then , for all @xmath103 , we have @xmath104 where @xmath105 and @xmath106 is defined in lemma [ lemma - lipschitz - constant ] .",
    "note that the calibration inequality , that is the right - hand side of the inequality above in particular ensures that @xmath107 in @xmath108 whenever @xmath100 .",
    "in addition , the convergence rates can be directly translated .",
    "the inequality on the left shows that modulo constants the calibration inequality is sharp .",
    "we will use this left inequality when bounding the approximation error for gaussian rbf kernels in the proof of theorem [ theorem - approximation_error_function - newentropy ] .    at the end of this section , we present supremum and variance bounds of the @xmath25-loss . like the calibration inequality of theorem [ self - calibration - inquality ] these two bounds are useful for analyzing the statistical properties of any @xmath98-based empirical risk minimization scheme . in section [ sec - head - oraclelearningrates ]",
    "we will illustrate this when establishing an oracle inequality for the svm - type learning algorithm .",
    "[ lemma - supremum and variance bound ] let @xmath109 be non - empty set , @xmath110 $ ] be a closed subset where @xmath71 , and @xmath1 be a distribution on @xmath2 .",
    "additionally , we assume that @xmath67 is the als loss and @xmath22 is the conditional @xmath11-expectile for fixed @xmath9 .",
    "then for all @xmath111 $ ] we have    * @xmath112 * @xmath113",
    "in this section , we first introduce some notions related to kernels .",
    "we assume that @xmath114 is a measurable , symmetric and positive definite kernel with associated rkhs @xmath28 .",
    "additionally , we assume that @xmath12 is bounded , that is , @xmath115 , which implies that @xmath28 consists of bounded functions with @xmath116 for all @xmath117 . in practice , we often consider svms that are equipped with well - known gaussian rbf kernels for input domain @xmath118 , see @xcite . recall that the latter are defined by @xmath119 where @xmath120 is called the width parameter that is usually determined in a data dependent way , i.e.  by cross validation . by (",
    "* corollary 4.58 ) the kernel @xmath50 is universal on every compact set @xmath121 and in particular strictly positive definite .",
    "in addition , the rkhs @xmath122 of kernel @xmath123 is dense in @xmath124 for all @xmath125 and all distributions @xmath126 on @xmath3 , see ( * ? ? ? * proposition 4.60 ) .",
    "one requirement to establish learning rates is to control the capacity of rkhs @xmath28 .",
    "one way to do this is to estimate eigenvalues of a linear operator induced by kernel @xmath12 .",
    "to be more precise , given a kernel @xmath12 and a distribution @xmath126 on @xmath3 , we define the integral operator @xmath127 by @xmath128 for @xmath126-almost all @xmath6 . in the following ,",
    "we assume that @xmath129 .",
    "recall ( * ? ? ?",
    "* theorem 4.27 ) that @xmath130 is compact , positive , self - adjoint and nuclear , and thus has at most countably many non - zero ( and non - negative ) eigenvalues @xmath131 . ordering these eigenvalues ( with geometric multiplicities ) and extending the corresponding sequence by zeros ,",
    "if there are only finitely many non - zero eigenvalues , we obtain the _ extended sequence of eigenvalues _ @xmath132 that satisfies @xmath133 ( * ? ? ?",
    "* theorem 7.29 ) .",
    "this summability implies that for some constant @xmath134 and @xmath135 , we have @xmath136 . by @xcite , this eigenvalues assumption can converge even faster to zero , that is , for @xmath137 , we have @xmath138 it turns out that the speed of convergence of @xmath131 influences learning rates for svms . for instance , @xcite used ( [ eigne - value - assumption ] ) to establish learning rates for svms using hinge loss and @xcite for svms using least square loss .",
    "another way to control the capacity of rkhs @xmath28 is based on the concept of _ covering numbers _ or the inverse of covering numbers , namely , _",
    "entropy numbers_. to recall the latter , see ( * ? ?",
    "* definition a.5.26 ) , let @xmath139 be a bounded , linear operator between the banach spaces @xmath140 and @xmath141 , and @xmath135 be an integer",
    ". then the @xmath142-th ( dyadic ) entropy number of @xmath143 is defined by @xmath144 in the hilbert space case , the eigenvalues and entropy number decay are closely related .",
    "for example , @xcite showed that ( [ eigne - value - assumption ] ) is equivalent ( modulo a constant only depending on @xmath56 ) to @xmath145 it is further shown in @xcite that ( [ general - entropy ] ) implies a bound on average entropy numbers , that is , for empirical distribution associated to the data set @xmath146 , the average entropy number is @xmath147 which is used in ( * ? ? ? * theorem 7.24 ) to establish the general oracle inequality for svms .",
    "a bound of the form ( [ general - entropy ] ) was also established by ( * ? ? ?",
    "* theorem 6.27 ) for gaussian rbf kernels and certain distributions @xmath19 having unbounded support . to be more precise ,",
    "let @xmath148 be a closed unit euclidean ball .",
    "then for all @xmath149 $ ] and @xmath150 , there exists a constant @xmath55 such that @xmath151 which has been used by @xcite to establish leaning rates for least square svms .",
    "note that the constant @xmath55 depends on @xmath56 in an unknown manner . to address this issue , we use ( * ? ? ?",
    "* lemma 4.5 ) and derive an improved entropy number bound in the following theorem by establishing an upper bound for @xmath55 whose dependence on @xmath56 is explicitly known .",
    "we will further see in corollary [ corollary - lr_for_entropy_2 ] that this improved bound leads us to achieve better learning rates than the one obtained by @xcite .",
    "[ sec3-theorem - newentropynumber ] let @xmath152 be a closed euclidean ball .",
    "then there exists a constant @xmath153 , such that , for all @xmath137 , @xmath149 $ ] and @xmath135 , we have @xmath154    another requirement for establishing learning rates is to bound the _ approximation error function _ considering rkhs @xmath155 for gaussian rbf kernel @xmath123 .",
    "if the distribution @xmath16 is such that @xmath156 , then the approximation error function @xmath157 is defined by @xmath158 for @xmath159 , the approximation error function @xmath160 quantifies how well an infinite sample @xmath39-svm with rkhs @xmath155 , that is , @xmath161 approximates the optimal risk @xmath97 . by (",
    "* lemma 5.15 ) , one can show that @xmath162 if @xmath155 is dense in @xmath108 .",
    "in general , however , the speed of convergence can not be faster than @xmath163 and this rate is achieved , if and only if , there exists an @xmath164 such that @xmath165 , see ( * ? ? ? * lemma 5.18 ) . in order to bound @xmath166",
    ", we first need to know one important feature of the target function @xmath20 , namely , the _ regularity _ which , roughly speaking , measures the smoothness of the target function .",
    "different function spaces norms e.g.  hlder norms , besov norms or triebel - lizorkin norms can be used to capture this regularity . in this work , following @xcite , we assume that the target function @xmath20 is in a sobolev or a besov space . recall ( * ? ? ?",
    "* definition 5.1 ) and ( * ? ? ? * definition 3.1 and 3.2 ) that for any integer @xmath167 , @xmath168 and a subset @xmath169 with non - empty interior , the sobolev space @xmath170 of order @xmath12 is defined by @xmath171 with the norm @xmath172 where @xmath173 is the @xmath48-th weak partial derivative for multi - index @xmath174 of modulus @xmath175 .",
    "in other words , the sobolev space is the space of functions with sufficiently many derivatives and equipped with a norm that measures both the size and the regularity of the contained functions . note that @xmath170 is a banach space , see ( * ? ? ?",
    "* lemma 5.2 ) .",
    "moreover , by ( * ? ? ?",
    "* theorem 3.6 ) , @xmath170 is separable if @xmath176 , and is uniformly convex and reflexive if @xmath177 .",
    "furthermore , for @xmath178 , @xmath179 is a separable hilbert space that we denote by @xmath180 . despite the underlined advantages ,",
    "sobolev spaces can not be immediately applied when @xmath48 is non - integral or when @xmath181 , however , the smoothness spaces for these extended parameters are also needed when engaging nonlinear approximation .",
    "this shortcoming of sobolev spaces is covered by besov spaces that bring together all functions for which the modulus of smoothness have a common behavior .",
    "let us first recall ( * ? ? ?",
    "* section 2 ) and ( * ? ? ?",
    "* section 2 ) that for a subset @xmath169 with non - empty interior , a function @xmath182 with @xmath183 for all @xmath184 $ ] and @xmath185 , the modulus of smoothness of order @xmath186 of a function @xmath30 is defined by @xmath187 where the @xmath186-th difference @xmath188 given by @xmath189 for @xmath190 , is used to measure the smoothness .",
    "note that @xmath191 as @xmath192 , which means that the faster this convergence to 0 the smoother is @xmath30 .",
    "for more details on properties of the modulus of smoothness , we refer the reader to ( * ? ? ?",
    "* chapter 4.2 ) .",
    "now for @xmath193 , @xmath194 , @xmath195 , the besov space @xmath196 based on modulus of smoothness for domain @xmath169 , see for instance ( * ? ? ?",
    "* section 4.5 ) , ( * ? ? ? * chapter 4.3 ) and ( * ? ? ?",
    "* section 2 ) , is defined by @xmath197 where the semi - norm @xmath198 is given by @xmath199 and for @xmath200 , the semi - norm @xmath198 is defined by @xmath201 in other words , besov spaces are collections of functions @xmath30 with common smoothness . for more general definition of besov - like spaces",
    ", we refer to ( * ? ? ?",
    "* section 4.1 ) .",
    "note that @xmath202 is the norm of @xmath196 , see e.g.  ( * ? ? ?",
    "* section 2 ) and ( * ? ? ?",
    "* section 2 ) .",
    "furthermore , for @xmath203 different values of @xmath204 give equivalent norms of @xmath196 , which remains true for @xmath205 , see ( * ? ? ?",
    "* section 2 ) .",
    "it is well known , see",
    "e.g  ( * ? ? ?",
    "* section 4.1 ) , that @xmath206 for all @xmath168 , @xmath207 , where for @xmath208 the besov space is the same as the sobolev space .    in the next step ,",
    "we find a function @xmath209 such that both the regularization term @xmath210 and the excess risk @xmath211 are small .",
    "for this , we define the function @xmath212 , see @xcite , by    @xmath213    for all @xmath214 , @xmath215 and @xmath216 .",
    "additionally , we assume that there exists a function @xmath217 satisfies @xmath218 and @xmath219 .",
    "then @xmath220 is defined by    @xmath221    with these preparation , we now establish an upper bound for the approximate error function @xmath166 .",
    "[ theorem - approximation_error_function - newentropy ] let @xmath25 be the als loss defined by ( [ lossals ] ) , @xmath1 be the probability distribution on @xmath222 , and @xmath19 be the marginal distribution of @xmath16 onto @xmath223 such that @xmath224 and @xmath225 .",
    "moreover , assume that the conditional @xmath11-expectile @xmath20 satisfies @xmath226 as well as @xmath227 for some @xmath228 .",
    "in addition , assume that @xmath50 is the gaussian rbf kernel over @xmath3 with associated rkhs @xmath122 . then for all @xmath149 $ ] and @xmath27 , we have @xmath229 where @xmath230 is a constant depending on @xmath186 and @xmath11 , and the constant @xmath231",
    ".    clearly , the upper bound of the approximation error function in theorem [ theorem - approximation_error_function - newentropy ] depends on the regularization parameter @xmath232 , the kernel width @xmath120 , and the smoothness parameter @xmath233 of the target function @xmath20 .",
    "note that in order to shrink the right - hand side we need to let @xmath234 .",
    "however , this would let the first term go to infinity unless we simultaneously let @xmath235 with a sufficient speed . now using ( * ? ? ?",
    "* theorem 7.24 ) together with lemma [ lemma - supremum and variance bound ] , theorem [ theorem - approximation_error_function - newentropy ] and the entropy number bound ( [ sec3-eq - newentropynumber ] ) , we establish oracle inequality of svms for @xmath236 in the following theorem .",
    "[ theorem - final_orale_inequlity - newentropy ] consider the assumptions of theorem [ theorem - approximation_error_function - newentropy ] and additionally assume that @xmath237 $ ] for @xmath238 .",
    "then , for all @xmath239 and @xmath240 $ ] , the svm using the rkhs @xmath122 and the als loss function @xmath25 satisfies @xmath241 with probability @xmath242 not less than @xmath243 . here",
    "@xmath244 is some constant independent of @xmath245 and @xmath246 .",
    "it is well known that there exists a relationship between sobolev spaces and the scale of besov spaces , that is , @xmath247 , whenever @xmath248 and @xmath249 , see for instance ( * ? ? ?",
    "* and p.44 ) .",
    "in particular , for @xmath250 , we have @xmath251 with equivalent norms .",
    "in addition , by @xcite we have @xmath252 .",
    "thus , theorem [ theorem - final_orale_inequlity - newentropy ] also holds for decision functions @xmath217 with @xmath253 and @xmath254 .    by assuming some suitable values for @xmath232 and @xmath120 that depends on data size @xmath255 , the smoothness parameter @xmath48 , and the dimension @xmath61",
    ", we obtain learning rates for learning problem ( [ svm - expectile ] ) in the following corollary .",
    "[ corollary - lr_for_entropy_2 ] under the assumptions of theorem [ theorem - final_orale_inequlity - newentropy ] and with @xmath256 where @xmath257 and @xmath258 are user specified constants , we have , for all @xmath259 and @xmath260 , @xmath261 with probability @xmath242 not less than @xmath262 .    note that learning rates in corollary [ corollary - lr_for_entropy_2 ] depend on the choice of @xmath263 and @xmath264 , where the kernel width @xmath264 requires knowing @xmath233 which , in practice , is not available .",
    "however , ( * ? ? ? * chapter 7.4 ) , @xcite , @xcite and @xcite showed that one can achieve the same learning rates adaptively , i.e.  without knowing @xmath233 .",
    "let us recall ( * ? ? ?",
    "* definition 6.28 ) that describes a method to select @xmath232 and @xmath120 , which in some sense is a simplification of the cross - validation method .",
    "let @xmath122 be a rkhs over @xmath3 and @xmath265 and @xmath266 be the sequences of finite subsets @xmath267 $ ] . given a data set @xmath268",
    ", we define @xmath269 where @xmath270 and @xmath271 .",
    "then use @xmath272 as a training set to compute the svm decision function @xmath273 and use @xmath274 to determine @xmath275 by choosing @xmath276 such that @xmath277 every learning method that produce the resulting decision functions @xmath278 is called a training validation svm with respect to @xmath279 .    in the next theorem",
    ", we use this training - validation svm ( tv - svm ) approach for suitable candidate sets @xmath280 and @xmath281 with @xmath282 , and establish learning rates similar to ( [ lr_entropy_2 ] ) .    [ theorem - tv - svm learning rates - newentropy ] with the assumptions of theorem [ theorem - final_orale_inequlity - newentropy ] , let @xmath283 and @xmath266 be the sequences of finite subsets @xmath284 $ ] such that @xmath285 is an @xmath286-net of @xmath287 $ ] and @xmath288 is an @xmath289-net of @xmath287 $ ] with polynomially growing cardinalities @xmath290 and @xmath291 in @xmath255 .",
    "then for all @xmath260 , the tv - svm produce @xmath292 that satisfies @xmath293 where @xmath294 is a constant independent of @xmath255 and @xmath246 .",
    "so far we have only considered the case of bounded noise with known bounds , that is , @xmath295 $ ] where @xmath71 is known . in practice",
    ", @xmath10 is usually unknown and in this situation , one can still achieve the same learning rates by simply increasing @xmath10 slowly . however , more interesting is the case of unbounded noise . in the following",
    "we treat this case for distributions for which there exist constants @xmath296 and @xmath297 such that @xmath298 for all @xmath299 . in other words , the tails of the response variable @xmath5 decay sufficiently fast .",
    "it is shown in @xcite by examples that such an assumption is realistic .",
    "for instance , if @xmath300 , the assumption is satisfied for @xmath301 , see ( * ? ? ?",
    "* example 3.7 ) , and for the case where @xmath302 has the density whose tails decay like @xmath303 , the assumption holds for @xmath304 , see ( * ? ? ?",
    "* example 3.8 ) .    with this additional assumption , we present learning rates for the case of unbounded noise in the following theorem .",
    "[ theorem - unbounded noise learning rates - newentropy ] let @xmath4 and @xmath1 be a probability distribution on @xmath222 such that @xmath305 .",
    "moreover , assume that the @xmath11-expectile @xmath20 satisfies @xmath306 $ ] for @xmath307-almost all @xmath6 , and both @xmath226 and @xmath227 for some @xmath228 .",
    "in addition , assume that ( [ unbounded noise - entropy2 ] ) holds for all @xmath260 .",
    "we define @xmath308 where @xmath309 and @xmath310 are user - specified constants .",
    "moreover , for some fixed @xmath311 and @xmath312 we define @xmath313 and @xmath314 .",
    "furthermore , we consider the svm that clips decision function @xmath315 at @xmath316 after training",
    ". then there exists a @xmath294 independent of @xmath255 , @xmath56 and @xmath317 such that @xmath318 holds with probability @xmath242 not less than @xmath319 .",
    "note that the assumption ( [ unbounded noise - entropy2 ] ) on the tail of the distribution does not influence learning rates achieved in the corollary [ corollary - lr_for_entropy_2 ] .",
    "furthermore , we can also achieve same rates adaptively using tv - svm approach considered in theorem [ theorem - tv - svm learning rates - newentropy ] provided that we have upper bound of the unknown parameter @xmath320 , which depends on the distribution @xmath16 , see @xcite where this dependency is explained with some examples .",
    "let us now compare our results with the oracle inequalities and learning rates established by @xcite for least square svms .",
    "this comparison is justifiable because a ) the least square loss is a special case of @xmath32-loss for @xmath321 , b ) the target function @xmath322 is assumed to be in the sobolev or besov space similar to @xcite , and c ) the supremum and the variance bounds for @xmath32 with @xmath323 are the same as the ones used by @xcite . furthermore , recall that @xcite used the entropy number bounds to control the capacity of the rkhs @xmath155 which contains a constant @xmath55 depending on @xmath56 in an unknown manner . as a result , they obtained a leading constant @xmath324 in their oracle inequality , see ( * ? ? ?",
    "* theorem 3.1 ) for which no upper bound can be determined explicitly .",
    "we cope this problem by establishing an improved entropy number bound which not only provides the upper bound for @xmath55 but also helps to determine the value of the constant @xmath324 in the oracle inequality explicitly . as a consequence we can improve their learning rates of the form @xmath325 , where @xmath58 , by @xmath326 in other words , the nuisance parameter @xmath63 from @xcite is replaced by the logarithmic term @xmath327 .",
    "moreover , our learning rates , up to this logarithmic term , are minimax optimal , see e.g.  the discussion in @xcite . finally note that unlike @xcite we have not only established learning rates for the least squares case @xmath321 but actually for all @xmath328 .",
    "lemma [ lemma - lipschitz - constant ] we define @xmath329 by @xmath330 clearly , @xmath331 is convex and thus ( * ? ? ?",
    "* lemma a.6.5 ) shows that @xmath331 is locally lipschitz continuous .",
    "moreover , we have @xmath332 } \\abs{\\psi'(y - t)}_{1,m }              \\leq \\max\\{\\t,1-\\t\\ } \\supremum{t\\in[m ,- m]}\\abs{2(y - t ) }              \\leq c_{\\t}\\,4m\\,,\\end{aligned}\\ ] ] where @xmath94 .",
    "a simple consideration shows that this estimate is also sharp .    in order to prove theorem [ self - calibration - inquality ]",
    "recall that the risk @xmath95 in uses regular conditional probability @xmath333 , which enable us to computed @xmath95 by treating the _ inner _ and the _ outer _ integrals separately . following ( * ? ? ?",
    "* definition 3.3 , definition 3.4 ) , we therefore use _ inner @xmath25-risks _ as a key ingredient for establishing self - calibration inequalities .",
    "[ definition - inner_l_risk ] let @xmath334 be the als loss function defined by ( [ lossals ] ) and @xmath66 be a distribution on @xmath335 $ ]",
    ". then the _ inner @xmath25-risks _ of @xmath66 are defined by @xmath336 and the _ minimal inner @xmath25-risk _ is @xmath337    in the latter definition , the _ inner risks _",
    "@xmath338 for a suitable classes of distributions @xmath66 on @xmath5 are considered as a template for @xmath339 . from this",
    ", we immediately can obtain the risk of function @xmath30 , i.e.   @xmath340 .",
    "moreover , by ( * ? ?",
    "* lemma 3.4 ) , the optimal risk @xmath97 can be obtained by minimizing the _",
    "inner @xmath25-risks _ , i.e.  @xmath341 .",
    "consequently , the _ excess @xmath25-risk _ , when @xmath342 , is obtained by @xmath343 besides some technical advantages , this approach makes the analysis rather independent of the specific distribution @xmath1 . in the following theorem",
    ", we use this approach and establish the lower and the upper bound of excess inner @xmath25-risks .",
    "[ theorem - lower - upper - bound - excess - inner - risk ] let @xmath25 be the als loss function defined by ( [ lossals ] ) and @xmath66 be a distribution on @xmath101 with @xmath344 . for a fixed @xmath9 and for all @xmath8 , we have @xmath345 where @xmath346 and @xmath347 is defined in lemma [ lemma - lipschitz - constant ] .",
    "theorem [ theorem - lower - upper - bound - excess - inner - risk ] let us fix @xmath9 .",
    "then for a distribution @xmath66 on @xmath348 satisfies @xmath349 , the @xmath11-expectile @xmath350 , according to @xcite , is the only solution of @xmath351 let us now compute the excess inner risks of @xmath25 with respect to @xmath66 . to this end , we fix a @xmath352",
    ". then we have @xmath353 and @xmath354 by definition [ definition - inner_l_risk ] and using ( [ newey - expectile - expression ] ) , we obtain @xmath355 and this leads to the following excess inner @xmath25-risk @xmath356 let us define @xmath346 , then ( [ proof - eir when t > = t^ * ] ) leads to the following lower bound of excess inner @xmath25-risk when @xmath352 : @xmath357 likewise , the excess inner @xmath25-risk when @xmath358 is @xmath359 that also leads to the lower bound ( [ proof - lb_eir when t > = t^ * ] ) .",
    "now , for the proof of upper bound of the excess inner @xmath25-risks , we define @xmath360 .",
    "then ( [ proof - eir when t > = t^ * ] ) leads to the following upper bound of excess inner @xmath25-risks when @xmath352 : @xmath361 analogously , for the case of @xmath358 , ( [ proof - eir when t < t^ * ] ) also leads to the upper bound ( [ proof - ub_eir when t > = t^ * ] ) for excess inner @xmath25-risks .",
    "theorem [ self - calibration - inquality ] for a fixed @xmath24 , we write @xmath362 and @xmath363 . by theorem",
    "[ theorem - lower - upper - bound - excess - inner - risk ] , for @xmath364 , we then immediately obtain @xmath365 integrating with respect to @xmath19 leads to the assertion .",
    "lemma [ lemma - supremum and variance bound ] i ) since @xmath25 can be clipped at @xmath10 and the conditional @xmath11-expectile satisfies @xmath366 $ ] almost surely .",
    "then @xmath367}(y - t)^2\\\\   & = c_\\t\\,4m^2\\ , ,   \\end{aligned}\\ ] ] for all @xmath368 $ ] and all @xmath369 .",
    "+ ii ) using the locally lipschitz continuity of the loss @xmath25 and theorem [ self - calibration - inquality ] , we obtain @xmath370      theorem [ sec3-theorem - newentropynumber ] by ( * ? ? ?",
    "* lemma 4.5 ) , the @xmath85-log covering numbers of unit ball @xmath371 of the gaussian rkhs @xmath49 for all @xmath372 and @xmath373 satisfy @xmath374 where @xmath375 is a constant depending only on @xmath61 . from this",
    ", we conclude that @xmath376 let @xmath377 . in order to obtain the optimal value of @xmath378",
    ", we differentiate it with respect to @xmath379 @xmath380 and set @xmath381 which gives @xmath382 by plugging @xmath383 into @xmath378 , we obtain @xmath384 and consequently , @xmath85-log covering numbers ( [ appnd - proof - coveringnumber ] ) are @xmath385 where @xmath386 .",
    "now , by inverse implication of ( * ? ? ?",
    "* lemma 6.21 ) , see also ( * ? ? ?",
    "* exercise 6.8 ) , the bound on entropy number of the gaussian rbf kernel is @xmath387 for all @xmath135 , @xmath388 .",
    "theorem [ theorem - approximation_error_function - newentropy ] the assumption @xmath389 and ( * ? ? ?",
    "* theorem 2.3 ) immediately yield that @xmath390 , i.e. @xmath220 is contained in rkhs @xmath122 .",
    "furthermore , ( * ? ? ?",
    "* theorem 2.3 ) leads to the following upper bound of the regularization term @xmath391 in the next step , we bound the excess risk . by (",
    "* theorem 2.2 ) , the upper bound for @xmath108-distance between @xmath220 and @xmath20 is @xmath392 where @xmath393 , see @xcite , is constant only depending on @xmath186 and @xmath394 is the lebesgue density . now using theorem [ theorem - lower - upper - bound - excess - inner - risk ] together with ( [ excess - risk - upper - bound - newentropy ] )",
    ", we obtain @xmath395 where @xmath396 . with these results ,",
    "we finally obtain @xmath397 where @xmath398 .        by considering the linear transformation",
    "@xmath402 , it is suffices to show that the function @xmath403\\to \\mathbb{r}$ ] defined by @xmath404 is convex . to solve the latter ,",
    "we first compute the first and second derivative of @xmath405 with respect to @xmath69 , that is : @xmath406 and @xmath407 since @xmath408 $ ] , it is not hard to see that all terms in @xmath409 are strictly positive .",
    "thus @xmath410 and hence @xmath405 is convex .",
    "furthermore , by convexity of @xmath405 , it is easy to find that",
    "@xmath411}g(t)=\\max\\{\\lim_{t \\to 0}g(t ) , g(1)\\}=1.\\end{aligned}\\ ] ]    theorem [ theorem - final_orale_inequlity - newentropy ] the assumption @xmath412 and ( * ? ? ?",
    "* theorem 2.3 ) yield that @xmath413 holds for all @xmath24 .",
    "this implies that , for all @xmath414 , we have @xmath415 and hence we conclude that @xmath416 .",
    "now , by plugging the result of theorem [ theorem - approximation_error_function - newentropy ] together with @xmath417 from theorem [ sec3-theorem - newentropynumber ] and @xmath418 from lemma [ lemma - supremum and variance bound ] , into ( * ? ? ?",
    "* theorem 7.23 ) , we obtain @xmath419 where @xmath420 and @xmath421 are from theorem [ theorem - approximation_error_function - newentropy ] , @xmath422 is a constant from ( * ? ? ?",
    "* theorem 7.23 ) that depends on @xmath56 , @xmath423 , and @xmath424 is a constant only depending on @xmath61 .",
    "let us assume that @xmath425 . since @xmath426 and @xmath427,thus ( [ proof - oracle inequality - first resutl ] ) becomes @xmath428 we now consider the constant @xmath422 in more detail . to this end , by using the lipschitz constant @xmath429 from lemma [ lemma - lipschitz - constant ] and the supremum bound @xmath430 from lemma [ lemma - supremum and variance bound ] , the value of @xmath422 is , see ( * ? ? ?",
    "* theorem 7.23 ) : @xmath431 where the constants @xmath432 and @xmath433 are derived in the proof of ( * ? ? ?",
    "* theorem 7.16 ) , that is @xmath434 and by ( * ? ? ?",
    "* lemma 7.15 ) , we have @xmath435 here we are interested to bound @xmath422 for @xmath436 $ ] . for this",
    ", we first need to bound the constants @xmath432 and @xmath433 .",
    "we start with @xmath437 and obtain the following bound for @xmath438 $ ] .",
    "@xmath439 } \\big(\\frac{\\sqrt{2}-1}{\\sqrt{2}-2^{\\frac{2p-1}{2p}}}\\big)^p = e\\,,\\end{aligned}\\ ] ] where we used @xmath440 for all @xmath438 $ ] , and lemma [ apndx - lem - convexderivative ] .",
    "now the bound for @xmath432 is the following : @xmath441}\\frac{2 \\sqrt{\\ln 256}\\,c_p^p}{(\\sqrt{2}-1)(1-p)2^{p/2 } }       \\leq   \\frac{4\\ , e\\,\\sqrt{\\ln 256}}{\\sqrt{2}-1}\\,\\max{p\\in ( 0,\\frac{1}{2 } ] } \\,\\frac{1}{2^{p/2 } }       \\leq 46 \\,e\\ , .",
    "\\end{aligned}\\ ] ] analogously , the bound for the constant @xmath433 is : @xmath442 } \\left(\\frac{8\\sqrt{\\ln16}\\,c_p^p}{(\\sqrt{2}-1)(1-p)4^p}\\right)^2            \\leq \\frac{256\\ , e^2 \\ln ( 16)}{(\\sqrt{2}-1)^2 } \\max{p\\in ( 0 , \\frac{1}{2}]}\\frac{1}{4^{2p } }            \\leq 1035\\ , e^2\\ , .",
    "\\end{aligned}\\ ] ] by plugging @xmath432 and @xmath433 into ( [ proof - constant k ] ) , we thus obtain @xmath443 and by plugging this result into ( [ proof - oracle - inequality - second - result ] ) , we obtain @xmath444 where @xmath324 is a constant independent of @xmath245 and @xmath246 .",
    "corollary [ corollary - lr_for_entropy_2 ] for all @xmath259 , theorem [ theorem - final_orale_inequlity - newentropy ] yields @xmath445 with probability @xmath242 not less than @xmath262 and a constant @xmath446 . using the sequences @xmath447 and @xmath448",
    ", we obtain @xmath449 where the positive constant @xmath450 is independent of @xmath56 .",
    "[ lemma - tv - svm - rhs ] let @xmath451 , @xmath452 be a constant , @xmath453 $ ] be a finite set such that there exists a @xmath454 with @xmath455 .",
    "moreover assume that @xmath456 and @xmath457 $ ] is a finite @xmath458-net of @xmath287 $ ] .",
    "then for @xmath459 and @xmath194 we have @xmath460 where @xmath461 is a constant independent of @xmath462 .",
    "let us assume that @xmath463 and @xmath464 , and @xmath465 for all @xmath466 and @xmath467 for all @xmath468 .",
    "we thus obtain @xmath469 where @xmath470 .",
    "it is not hard to see that the function @xmath471 is optimal at @xmath472 , where @xmath257 is a constant only depends on @xmath48 and @xmath61 .",
    "furthermore , with @xmath473 , we see that @xmath474 for all @xmath475 .",
    "in addition , there exits an index @xmath476 such that @xmath477 .",
    "consequently , we have @xmath478 .",
    "using this result in ( [ proof - lemma - data - dependent - lr ] ) , we obtain @xmath479 where @xmath480 is a constant .",
    "theorem [ theorem - tv - svm learning rates - newentropy ] the proof of this theorem is the literal repetition of the proof of ( * ? ? ?",
    "* theorem 3.6 ) , however , we present here for the sake of completeness .",
    "let us define @xmath481 , then for all @xmath482 , theorem [ theorem - final_orale_inequlity - newentropy ] yields @xmath483 with probability @xmath484 not less than @xmath485 .",
    "now define @xmath486 and @xmath487 , then by using ( * ? ? ?",
    "* theorem 7.2 ) and lemma [ lemma - tv - svm - rhs ] , we obtain @xmath488 with probability @xmath242 not less than @xmath489 .",
    "theorem [ theorem - unbounded noise learning rates - newentropy ] by ( [ unbounded noise - entropy2 ] ) , we obtain @xmath490 this implies that @xmath491 this leads us to conclude with probability @xmath242 not less than @xmath492 that the svm for als loss with belatedly clipped decision function at @xmath316 is actually a clipped regularized empirical risk minimization ( cr - erm ) in the sense of ( * ? ? ?",
    "* definition 7.18 ) .",
    "consequently , ( * ? ? ?",
    "* theorem 7.20 ) holds for @xmath493 modulo a set of probability @xmath242 not less than @xmath492 . from theorem [ theorem - final_orale_inequlity - newentropy ]",
    ", we then obtain @xmath494 with probability @xmath242 not less than @xmath495 . as in the proof of corollary ( [ corollary - lr_for_entropy_2 ] ) and by using the inequality @xmath496 , for @xmath497 and @xmath446",
    ", we finally obtain @xmath498 for all @xmath312 with probability @xmath242 not less than @xmath495 .",
    "choosing @xmath499 leads to the assertion ."
  ],
  "abstract_text": [
    "<S> [ sec - head - abstract ] conditional expectiles are becoming an increasingly important tool in finance as well as in other areas of applications . </S>",
    "<S> we analyse a support vector machine type approach for estimating conditional expectiles and establish learning rates that are minimax optimal modulo a logarithmic factor if gaussian rbf kernels are used and the desired expectile is smooth in a besov sense . as a special case , our learning rates improve the best known rates for kernel - based least squares regression in this scenario . </S>",
    "<S> key ingredients of our statistical analysis are a general calibration inequality for the asymmetric least squares loss , a corresponding variance bound as well as an improved entropy number bound for gaussian rbf kernels . </S>"
  ]
}