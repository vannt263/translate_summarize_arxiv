{
  "article_text": [
    "stochastic variational inference is framed as maximizing a global variational parameter @xmath0 , which is the natural parameter of a conjugate exponential distribution @xcite . in this framework ,",
    "stochastic gradient steps are taken along the natural gradient @xcite to optimize for @xmath0 .",
    "a pleasing property of stochastic variational inference on a conjugate exponential distribution and approximation @xmath1 is that the gradient is automatically rescaled so that a unit - length step size will minimize it . for a general bayesian network , where the global variational parameters are subdivided to parameterize different factors",
    "@xmath2 in the network s variational approximation , the picture is less clear .",
    "s appendix suggests a stochastic updating scheme like that of the global version @xcite .",
    "we show here that the problem is more subtle in the general case , as component - wise noisy natural gradients can tightly couple variational parameters , and following the default recipe can sometimes lead to a scheme that `` diverges '' beyond recovery !    these remarks are of particular value to the xbox recommender system , which uses stochastic variational inference in a bayesian network on `` worldwide '' scale @xcite .",
    "some of the results presented in sec .",
    "[ sec : bmf ] are preliminary investigations that were done when designing the system in 2012 .",
    "a bayesian network between the variables @xmath3 defines the conditional dependency structure between them through their joint probability @xmath4 . following fig .",
    "[ fig : bayesnet ] , let @xmath5 be the set of indexes of parents of random variable(s ) @xmath6 ; for notational convenience we let @xmath7 denote the parent variables .",
    "the variables in the network can be hidden or observed , @xmath8 .",
    "variational bayes ( vb ) approximates the posterior @xmath9 with @xmath10 , by maximizing the evidence lower bound @xmath11 \\defined \\int   q(\\x^\\hidden ) \\log \\frac{p(\\x)}{q(\\x^\\hidden ) } \\ , \\drm \\x^\\hidden   \\le \\log p(\\x^\\observed ) \\ .\\ ] ] if @xmath12 indexes the hidden variables , we factorize the approximation with @xmath13 let @xmath14 indicate the expectation taken over @xmath15 . the bound can be maximized in a component - wise fashion by iteratively setting each @xmath16 to the maximum @xmath17 + \\sum_{k \\in \\ch(i ) } \\ebb_{j \\neq i } \\big [ \\log p(\\x_k",
    "| \\pa_k ) \\big ] + \\const \\ .\\ ] ] in many practical networks there are some @xmath18 for which number of children @xmath19 is large . in @xcite",
    ", @xmath18 is a topic - vocabulary distribution from which millions of documents are generated . in sec .",
    "[ sec : bmf ] and @xcite the interaction is bilinear , where user @xmath18 and item @xmath6 variables are combined to represent a user s affinity to an item .",
    "rather than summing over all @xmath20 for each update in ( [ eq : vbupdate ] ) , we aim to stochastically approximate the expectations .",
    "it alleviates two problems : firstly , the sum contains many terms ; secondly , the update depends on some @xmath21 which will be re - estimated , and the expense of fully estimating @xmath22 is lost as it too will be re - estimated .",
    "= [ circle , thick , draw = black!100 , fill = blue!20 , minimum size = 6 mm ] = [ circle , thick , draw = black!100 , fill = red!0 , minimum size = 6 mm ] = [ circle , draw , fill = black!100 , minimum width = 3pt , inner sep = 0pt ] = [ rectangle , thick , draw = green!50 , fill = green!0 , minimum size = 6 mm ] = [ rectangle , thick , draw = red!40 , fill = red!0 , minimum size = 6 mm ] = [ rectangle , thick , draw = blue!40 , fill = blue!0 , minimum size = 6 mm ] = [ black!100 ]    \\(xi ) @xmath18 ; ( xpar1 ) [ above of = xi ] edge [ post ] ( xi ) ; ( xpar2 ) [ right of = xpar1 ] edge [ post ] ( xi ) ; ( xpar3 ) [ left of = xpar1 ] @xmath23 ; ( xch1 ) [ below of = xi ] edge [ pre ] ( xi ) ; ( xch2 ) [ right of = xch1 ] @xmath24 edge [ pre ] ( xi ) ; ( xch3 ) [ left of = xch1 ] @xmath25 ; ( xcp1 ) [ right of = xi ] edge [ post ] ( xch2 ) ; ( xcp2 ) [ right of = xcp1 ] edge [ post ] ( xch2 ) ; ( copar ) [ right of = xcp2 ] @xmath26 ;    background ( @xmath27 ) rectangle ( @xmath28 ) ; ( @xmath29 ) rectangle ( @xmath30 ) ; ( @xmath31 ) rectangle ( @xmath32 ) ;      the updates in ( [ eq : vbupdate ] ) are straightforward when the bayesian network is conditionally conjugate ; that is , when the distribution of @xmath18 , conditioned on @xmath33 , is ( a ) drawn from an exponential family , and ( b ) is conjugate with respect to the distribution of @xmath33 .",
    "we define the exponential family as @xmath34 where @xmath35 is the natural parameter vector , @xmath36 forms the sufficient statistics , and @xmath37 defines the normalizing constant through @xmath38",
    ". we can view ( [ eq : exponential ] ) as a `` prior '' over @xmath18 .",
    "now consider a node @xmath39 in fig .",
    "[ fig : bayesnet ] .",
    "we subdivide @xmath40 , the parents of @xmath24 , into @xmath18 and its co - parents @xmath41 : @xmath42 we can view this as a contribution to the `` likelihood '' of @xmath18 .",
    "we include the co - parents as they are part of @xmath18 s markov blanket .",
    "through conjugacy , @xmath43 and @xmath44 have the same functional form with respect to @xmath18 , so that we can rewrite @xmath44 in terms of the sufficient statistics @xmath36 by defining some function @xmath45 with @xmath46 we furthermore parameterize the @xmath22 distributions in terms of their natural parameters . to distinguish them",
    ", we denote their natural parameters by @xmath47 , and define @xmath48 $ ] : @xmath49      returning to ( [ eq : vbupdate ] ) , we can write @xmath50^t \\phi_i(\\x_i ) + f_i(\\x_i ) + \\const \\ , \\label{eq : vbconjugate}\\ ] ] from which we can directly read off the updated natural parameter @xmath51 through ( [ eq : q ] ) .",
    "notice now that @xmath52 is a multi - linear function of the random variables @xmath33 , i.e.  it is linear in _ each _ parent random variable . in the same way",
    "@xmath45 is a multi - linear function of the random variables @xmath24 and @xmath41 .",
    "furthermore , @xmath53 factorizes over these variables ( except where they are observed , of course )",
    ". we can therefore reparameterize ( [ eq : vbconjugate ] ) in terms of expectations over @xmath54 , @xmath55 with @xmath56 & = \\widetilde{\\bmeta}_i \\left ( \\big\\ { \\ebb_{j } \\big [ \\phi_j(\\x_j ) \\big ] \\big\\}_{j \\in \\pa(i ) } \\right ) \\defined \\widetilde{\\bmeta}_i \\\\",
    "\\neq i } \\big [   \\bmeta_{ki}(\\x_k , \\cp_i ) \\big ] & =   \\widetilde{\\bmeta}_{ki } \\left ( \\ebb_{k } \\big [ \\phi_k(\\x_k ) \\big ] , \\ ,   \\big\\ { \\ebb_{j } \\big [ \\phi_j(\\x_j ) \\big ] \\big\\}_{j \\in \\cp(i ) } \\right ) \\defined \\widetilde{\\bmeta}_{ki } \\ .\\end{aligned}\\ ] ]    l[0pt]0pt    @xmath57 @xmath58 random nodes from @xmath59 @xmath60 _ option ( a ) : _ @xmath61 _ option ( b ) : _",
    "@xmath62    this is a key ingredient of algorithms like variational message passing @xcite .",
    "( when @xmath24 is observed , @xmath63 is kept as is , as it is averaged over a delta function . )",
    "the variational update in ( [ eq : vbconjugate ] ) becomes @xmath64 the update is a step along the natural gradient @xcite , equivalent to setting the gradient to zero by solving for the zero of the derivative of @xmath65 with respect to @xmath47 .",
    "in particular , ( [ eq : vb - update ] ) updates @xmath47 from its old value to @xmath66 using a _ step of unit length _ along the natural gradient .",
    "[ sec : component - grad ] derives the gradient @xmath67 , and sec .",
    "[ sec : component - natural - grad ] states its natural form @xmath68 .    when @xmath19 is large , not all the child nodes might be accessed in reasonable time .",
    "furthermore , when @xmath22 is re - estimated , the ( previous ) large computation is discarded and recomputed .",
    "we may alternatively consider a subsample of nodes from @xmath20 to determine the sufficient statistics . by placing a uniform distribution @xmath69 on the atoms @xmath70 ,",
    "the update from ( [ eq : vb - update ] ) is equivalent to @xmath71 $ ] .",
    "this expectation can be estimated in many ways .",
    "let set @xmath72 be a sample of @xmath73 children from @xmath59 without replacement and let @xmath74 taking expectations gives @xmath75 = \\widetilde{\\bmeta}_i + n_i   \\ebb_{\\widetilde{p}_i } [ \\widetilde{\\bmeta } ] $ ] . with @xmath76 , @xmath77 and @xmath78 ,",
    "these stochastic natural gradients are used in alg .",
    "[ alg : stochastic ] , which is a stochastic version of variational message passing . in alg .",
    "[ alg : stochastic ] , scalar @xmath79 $ ] is a forgetting rate , while delay @xmath80 discounts early iterations more .",
    "there are two options in alg .",
    "[ alg : stochastic ] . for option _ ( a ) _ , the mean value of the parameters of @xmath81 is periodic in @xmath82 , the number of factors in @xmath53 , and convergence to a local optimum can also be guaranteed for @xmath82-dependent mean values @xcite . option _ ( b ) _ is the update scheme given in @xcite .",
    "to illustrate a general bayesian network , we factorize a sparse matrix of a subsample of a million entries in the netflix data set ( @xmath83 users and @xmath84 items ) .",
    "each entry @xmath85 is user @xmath86 s rating of movie @xmath87 on a five - star rating scale . for illustrative purposes ,",
    "consider a gaussian bilinear ratings model @xmath88 for user parameter vector @xmath89 and item trait vector @xmath90 .",
    "we place a factorized prior @xmath91 on each of the entries of @xmath92 and @xmath93 .",
    "we choose a fully factorized gaussian approximation @xmath94 , with a similar approximation for @xmath95 .",
    "the vb update for @xmath96 therefore incorporates @xmath97 co - parents due to the inner product . with the gaussian s natural parameters being its precision and",
    "mean - times - precision , it is @xmath98 + \\ebb_q[v_{nk}]^2 \\\\ \\ebb_q[v_{nk } ] ( r_{mn } - \\sum_{k ' \\neq k } \\ebb_q[u_{mk ' } ] \\ebb_q[v_{nk ' } ] ) \\end{pmatrix } \\ .\\ ] ]    $ ] with @xmath99 .",
    "[ alg : stochastic ] s option _",
    "( a ) _ is shown in the left column ; option _ ( b ) _ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    "the _ global _",
    "stochastic gradient is not in its natural form , and the effect of a large variance in the gradient estimate and overshooting with too large step sizes of @xmath100 $ ] is clearly visible for small @xmath73 .",
    "note that @xmath85 s can be revisited over multiple loops in alg .",
    "[ alg : stochastic ] .",
    "different magnifications of the same two convergence plots for options _ ( a ) _ and _ ( b ) _ are shown in the three rows of graphs .",
    ", title=\"fig:\",scaledwidth=49.0% ] $ ] with @xmath99 .",
    "[ alg : stochastic ] s option _",
    "( a ) _ is shown in the left column ; option _ ( b ) _ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    "the _ global _",
    "stochastic gradient is not in its natural form , and the effect of a large variance in the gradient estimate and overshooting with too large step sizes of @xmath100 $ ] is clearly visible for small @xmath73 .",
    "note that @xmath85 s can be revisited over multiple loops in alg .",
    "[ alg : stochastic ] .",
    "different magnifications of the same two convergence plots for options _ ( a ) _ and _ ( b ) _ are shown in the three rows of graphs .",
    ", title=\"fig:\",scaledwidth=49.0% ] + $ ] with @xmath99 .",
    "[ alg : stochastic ] s option _",
    "( a ) _ is shown in the left column ; option _ ( b ) _ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    "the _ global _",
    "stochastic gradient is not in its natural form , and the effect of a large variance in the gradient estimate and overshooting with too large step sizes of @xmath100 $ ] is clearly visible for small @xmath73 .",
    "note that @xmath85 s can be revisited over multiple loops in alg .",
    "[ alg : stochastic ] .",
    "different magnifications of the same two convergence plots for options _ ( a ) _ and _ ( b ) _ are shown in the three rows of graphs .",
    ", title=\"fig:\",scaledwidth=49.0% ] $ ] with @xmath99 .",
    "[ alg : stochastic ] s option _",
    "( a ) _ is shown in the left column ; option _ ( b ) _ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    "the _ global _",
    "stochastic gradient is not in its natural form , and the effect of a large variance in the gradient estimate and overshooting with too large step sizes of @xmath100 $ ] is clearly visible for small @xmath73 .",
    "note that @xmath85 s can be revisited over multiple loops in alg .",
    "[ alg : stochastic ] .",
    "different magnifications of the same two convergence plots for options _",
    "( a ) _ and _ ( b ) _ are shown in the three rows of graphs . ,",
    "title=\"fig:\",scaledwidth=49.0% ] + $ ] with @xmath99 .",
    "[ alg : stochastic ] s option _",
    "( a ) _ is shown in the left column ; option _ ( b ) _ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    "the _ global _",
    "stochastic gradient is not in its natural form , and the effect of a large variance in the gradient estimate and overshooting with too large step sizes of @xmath100 $ ] is clearly visible for small @xmath73 .",
    "note that @xmath85 s can be revisited over multiple loops in alg .",
    "[ alg : stochastic ] .",
    "different magnifications of the same two convergence plots for options _ ( a ) _ and _ ( b ) _ are shown in the three rows of graphs .",
    ", title=\"fig:\",scaledwidth=49.0% ] $ ] with @xmath99 .",
    "[ alg : stochastic ] s option _",
    "( a ) _ is shown in the left column ; option _ ( b )",
    "_ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    "stochastic gradient is not in its natural form , and the effect of a large variance in the gradient estimate and overshooting with too large step sizes of @xmath100 $ ] is clearly visible for small @xmath73 .",
    "note that @xmath85 s can be revisited over multiple loops in alg .",
    "[ alg : stochastic ] .",
    "different magnifications of the same two convergence plots for options _ ( a ) _ and _ ( b ) _ are shown in the three rows of graphs .",
    ", title=\"fig:\",scaledwidth=49.0% ]    fig .",
    "[ fig : convergence ] shows @xmath101 $ ] as a function of the number of times that individual ratings ( observed nodes ) @xmath85 are accessed or queried ( using @xmath102 ) .",
    "the value of the bound is shown for the use of at most @xmath103 children when estimating the gradient of each random variable with ( [ eq : stochasticupdate ] ) and ( [ eq : bmf ] ) . both options _ ( a ) _ and _ ( b ) _ in alg .  [ alg : stochastic ] `` diverge '' in a numerically unrecoverable way when @xmath73 is small .",
    "this is due to the global gradient not being in its natural form , and using a step size of @xmath100 $ ] that is too big , overshooting with too large gradient steps .",
    "full vb , shown in black in fig .",
    "[ fig : convergence ] , implicitly uses @xmath104 in ( [ eq : vb - update ] ) .",
    "as the stochastic natural gradient depends on other @xmath105 , much smaller initial step sizes are required to not `` overshoot '' .",
    "the variance of the gradient is simply too large compared to @xmath106 .",
    "[ fig : convergence ] illustrates this problem for _ general bayesian networks _ ; see especially the _ top left _ figure .    in practice",
    ", we can overcome this problem overcome by starting with sufficiently small initial step sizes @xmath107 .",
    "for @xmath108 in option _ ( a ) _",
    "this was starting from @xmath109 , and @xmath110 for option _",
    "( b)_. in @xcite the value of @xmath73 varied depending on a user or item s usage , and there @xmath104 was fixed for the first ten iterations before slowly decreasing it .    have we thrown the baby ( well - scaled steps ) out with the bath water ( exact gradients ) ? maybe some . as shown by this short note , it is still an open question .",
    "to the anonymous reviewer who pointed out that the fisher information matrix of @xmath111 block - diagonal , having the fisher information matrices of @xmath112 along its diagonal : thank you",
    "!    1    s.  amari .",
    "natural gradient works efficiently in learning .",
    ", 10(2):251276 , 1998 .",
    "m.  d. hoffman , d.  m. blei , c.  wang , and j.  paisley .",
    "stochastic variational inference .",
    ", 14:13031347 , 2013 .",
    "h.  j. kushner and g.  g. yin . .",
    "springer , 2003 .",
    "u.  paquet and n.  koenigstein .",
    "one - class collaborative filtering with random graphs . in _ proceedings of the 22nd international conference on world wide web _ , www 13 , pages 9991008 , 2013 .",
    "u.  paquet and n.  koenigstein .",
    "one - class collaborative filtering with random graphs : annotated version .",
    ", 2013 .",
    "j.  winn and c.m .",
    "variational message passing .",
    ", 6:661694 , 2005 .",
    "in this appendix , we derive the component - wise gradients and their natural version , and present basic intuition for why steps down the stochastic gradient can be taken .",
    "the function that s minimized to find ( [ eq : vb - update ] ) is @xmath113 below .",
    "it is a function of @xmath47 , whilst keeping all other @xmath114 for @xmath115 fixed : @xmath116 \\\\ & = \\ebb_q \\bigg [ \\bmeta_i(\\pa_i)^t \\phi_i(\\x_i ) + f_i(\\x_i ) + g_i(\\pa_i ) \\\\ & \\qquad\\qquad + \\sum_{k \\in \\ch(i ) } \\big ( \\bmeta_{k}(\\x_i , \\cp_i)^t \\phi_k(\\x_k ) + f(\\x_k ) + g(\\x_i , \\cp_i ) \\big ) \\\\ &",
    "\\qquad\\qquad - \\blambda_i^t \\phi_i(\\x_i ) - f_i(\\x_i ) - \\tilde{g}_i(\\blambda_i ) \\bigg]\\end{aligned}\\ ] ] because of local conjugacy , @xmath44 can be rewritten in terms of the sufficient statistics @xmath36 through a multi - linear function @xmath45 of the random variables @xmath24 and @xmath41 to yield @xmath117 \\",
    ".\\end{aligned}\\ ] ] taking expectations over @xmath53 gives , as function of @xmath47 , @xmath118 - \\blambda_i^t \\ebb_i \\left [ \\phi_i(\\x_i ) \\right ] - \\tilde{g}_i(\\blambda_i ) + \\mathrm{const } \\ , \\ ] ] with @xmath119 .",
    "the derivatives of the log partition function @xmath120 with respect to @xmath47 give the expected sufficient statistics @xmath121 \\\\ - \\nabla^2 \\tilde{g}_i(\\blambda_i )   & = \\nabla \\ebb_i \\left [ \\phi_i(\\x_i ) \\right ] \\\\ & = \\ebb_i \\left [ \\big ( \\phi_i(\\x_i ) -",
    "\\ebb_i \\left [ \\phi_i(\\x_i ) \\right ]   \\big ) \\big ( \\phi_i(\\x_i ) -",
    "\\ebb_i \\left [ \\phi_i(\\x_i ) \\right ]   \\big)^t \\right ] \\\\ & = { \\sf cov}_i \\left [ \\phi_i(\\x_i ) \\right ] \\ , \\end{aligned}\\ ] ] and by using properties of the exponential family , the gradient of @xmath65 with respect to @xmath47 is therefore @xmath122 \\bigg ( \\widetilde{\\bmeta}_i + \\sum_{k \\in \\ch(i ) } \\widetilde{\\bmeta}_{ki } - \\blambda_i \\bigg ) \\ .\\ ] ] solving for @xmath123 yields the component - wise vb update @xmath124 that we find in ( [ eq : vb - update ] ) .",
    "gradient @xmath67 depends on @xmath47 through @xmath125 $ ] and @xmath47 , and in the next section we will show that the natural gradient @xmath126 removes the dependency on @xmath125 $ ] , so that it is a linear function of @xmath47 , with the minimum being attained by taking a step of length one along it .",
    "the fisher information matrix of @xmath2 is @xmath127 \\\\ & = \\ebb_i \\left [ \\big ( \\phi_i(\\x_i ) - \\ebb_i[\\phi_i(\\x_i ) ] \\big ) \\big ( \\phi_i(\\x_i ) - \\ebb_i[\\phi_i(\\x_i ) ] \\big)^t \\right ] \\\\ & = { \\sf cov}_i \\left [ \\phi_i(\\x_i ) \\right ] \\ , \\end{aligned}\\ ] ] and the component - wise natural gradient is obtained by multiplying it with @xmath67 , yielding @xmath128 a gradient descent along the natural gradient is taken with step length @xmath129 .",
    "starting at point @xmath130 , gradient descent updates it to @xmath131 with @xmath132 when the above update is compared to ( [ eq : vb - update ] ) , we see that the minimum @xmath133 is obtained by applying a step size of @xmath134 along the natural gradient .      in this section",
    "an intuitive motivation will be provided for doing stochastic gradient descent using the natural gradient , as it was defined above in sec .",
    "[ sec : component - natural - grad ] .",
    "the explanation favours an intuitive understanding above mathematical rigour .",
    "imagine that instead of @xmath66 , we have access to a sequence of samples @xmath135 , so that @xmath136 = \\blambda_i^{*}$ ]",
    ". we can write the update @xmath133 recursively using the sample average @xmath137 define @xmath138 . in the running average , @xmath139 and @xmath140 , and therefore @xmath141 and @xmath142 . in the running average with @xmath138",
    ", each gradient sample is treated equally . however , instead of incorporating fraction @xmath143 of @xmath144 into the running average",
    ", we may erase a bit more from the `` past memory '' @xmath145 to include a bit _ more _ of the recent gradient @xmath144 .",
    "how much more is permissible ?    now define @xmath146 . for @xmath147 ,",
    "the previous samples will be forgotten at a faster rate , and more of @xmath144 will be included through @xmath148 . however , at this rate past samples are forgotten too quickly , as both @xmath149 and @xmath150 . for any @xmath151 ,",
    "both infinite sums will be finite , e.g.  @xmath152 and @xmath153 , and the running average will cling on to old memories , and has too little capacity to incorporate recent gradient samples @xmath144 . between forgetting too quickly or not at all , a setting of @xmath79 $ ] in @xmath146 is therefore permissible .      the running average in sec .",
    "[ sec : stochastic - grad ] can boldly start at @xmath154 for @xmath155 , and from this unit length step along the natural gradient , accumulate gradient samples until convergence .",
    "however , it rests on the premise that neighbours @xmath105 for @xmath115 from the markov blanket of @xmath18 remain unchanged .    if this premise does not hold , much smaller steps @xmath156 with delay @xmath80 are required .",
    "this is indeed the case . as sec .",
    "[ sec : bmf ] shows , a delay @xmath80 that is sufficiently large for the stochastic gradient scheme to converge in practice is not known _ a priori_. by explicitly stating the shorthand definitions of @xmath157 and @xmath70 in ( [ eq : l - of - lambda - i ] ) , it is clear that the other @xmath105 appear through multi - linear functions in @xmath158 \\big\\}_{j \\in \\pa(i ) } \\big )   + \\sum_{k \\in \\ch(i ) } \\widetilde{\\bmeta}_{ki } \\big ( \\ebb_{k } [ \\phi_k(\\x_k ) ] , \\ ,   \\big\\ { \\ebb_{j } [ \\phi_j(\\x_j ) ] \\big\\}_{j \\in",
    "\\cp(i ) } \\big )   \\right)^t \\ebb_i \\left [ \\phi_i(\\x_i ) \\right ] \\\\ & \\quad\\quad - \\blambda_i^t \\ebb_i \\left [ \\phi_i(\\x_i ) \\right ] - \\tilde{g}_i(\\blambda_i ) + \\mathrm{const } \\ .\\end{aligned}\\ ] ] if we now consider the global gradient @xmath159 $ ] , it is clear from the above form ( multi - linear in all variables ) that we ca nt set the gradient to zero and solve for all @xmath0 explicitly , as was done in ( [ eq : vb - update ] ) .",
    "it is usually not even convex problem .",
    "the gradient steps are along the global natural gradient .",
    "it is defined as @xmath160 with @xmath161 being the fisher information matrix of @xmath53 , @xmath162 \\ .\\ ] ] @xmath161 is block - diagonal , as the covariance between @xmath36 and @xmath163 is zero for @xmath55 , due to the factorization of @xmath53 .",
    "its inverse is therefore also block - diagonal , and the natural gradient has the form @xmath164 $ ] .",
    "an alternative to alg .  [ alg : stochastic ] is to take a batch data sample @xmath165 of @xmath166 observed variables at the start of each iteration , and follow either each @xmath167 or @xmath168 .",
    "this is outlined in alg .",
    "[ alg : stochastic - global ] .",
    "$ ] with @xmath99 .",
    "[ alg : stochastic - global ] s option _",
    "( a ) _ is shown in the left column ; option _ ( b ) _ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    ", title=\"fig:\",scaledwidth=49.0% ] $ ] with @xmath99 .",
    "[ alg : stochastic - global ] s option _",
    "( a ) _ is shown in the left column ; option _ ( b ) _ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    ", title=\"fig:\",scaledwidth=49.0% ] + $ ] with @xmath99 .",
    "[ alg : stochastic - global ] s option _",
    "( a ) _ is shown in the left column ; option _ ( b ) _ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    ", title=\"fig:\",scaledwidth=49.0% ] $ ] with @xmath99 .",
    "[ alg : stochastic - global ] s option _",
    "( a ) _ is shown in the left column ; option _",
    "( b ) _ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    ", title=\"fig:\",scaledwidth=49.0% ] + $ ] with @xmath99 .",
    "[ alg : stochastic - global ] s option _",
    "( a ) _ is shown in the left column ; option _ ( b ) _ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    ", title=\"fig:\",scaledwidth=49.0% ] $ ] with @xmath99 .",
    "[ alg : stochastic - global ] s option _",
    "( a ) _ is shown in the left column ; option _ ( b ) _ is shown in the right column .",
    "the x - axes are on a logarithmic scale .",
    ", title=\"fig:\",scaledwidth=49.0% ]    fig .",
    "[ fig : convergenceglobal ] considers batch sizes of @xmath173 to 1000 , in intervals of 100 . the convergence in fig .",
    "[ fig : convergenceglobal ] is much slower than that of fig .",
    "[ fig : convergence ] . for the global update in option _",
    "( b ) _ in alg .",
    "[ alg : stochastic - global ] , the algorithm only converged on finite machine precision when @xmath174 was chosen ( smaller for some @xmath166 settings ) , whereas for option _ ( a ) _ at least the algorithm converged from @xmath175 for all settings .    in alg .",
    "[ alg : stochastic - global ] , @xmath176 is the set of hidden variables that have a child node in @xmath165 .",
    "line 9 makes provision for updating variables ( like hyper - parameters ) that do nt have observed children ; this was not required for our example ."
  ],
  "abstract_text": [
    "<S> we highlight a pitfall when applying stochastic variational inference to general bayesian networks . for global random variables approximated by an exponential family distribution , natural gradient steps , commonly starting from a unit length step size , are averaged to convergence . </S>",
    "<S> this useful insight into the scaling of initial step sizes is lost when the approximation factorizes across a general bayesian network , and care must be taken to ensure practical convergence . </S>",
    "<S> we experimentally investigate how much of the baby ( well - scaled steps ) is thrown out with the bath water ( exact gradients ) .    </S>",
    "<S> 150mm(.0,-2 cm ) nips 2014 workshop on _ advances in variational inference_. montreal , canada </S>"
  ]
}