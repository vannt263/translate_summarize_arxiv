{
  "article_text": [
    "the problem of time series analysis and prediction has a long and rich history , probably dating back to the pioneering work of yule in 1927 @xcite .",
    "the application scope is vast , as time series modeling is routinely employed across the entire and diverse range of applied statistics , including problems in genetics , medical diagnoses , air pollution forecasting , machine condition monitoring , financial investments , marketing and econometrics .",
    "most of the research activity until the 1970s was concerned with parametric approaches to the problem whereby a simple , usually linear model is fitted to the data ( for a comprehensive account we refer the reader to the monograph of brockwell and davies @xcite ) .",
    "while many appealing mathematical properties of the parametric paradigm have been established , it has become clear over the years that the limitations of the approach may be rather severe , essentially due to overly rigid constraints which are imposed on the processes .",
    "one of the more promising solutions to overcome this problem has been the extension of classic nonparametric methods to the time series framework ( see for example gyrfi , hrdle , sarda and vieu @xcite and bosq @xcite for a review and references ) .",
    "interestingly , related schemes have been proposed in the context of sequential investment strategies for financial markets .",
    "sequential investment strategies are allowed to use information about the market collected from the past and determine at the beginning of a training period a portfolio , that is , a way to distribute the current capital among the available assets . here , the goal of the investor is to maximize their wealth in the long run , without knowing the underlying distribution generating the stock prices . for more information on this subject ,",
    "we refer the reader to algoet @xcite , gyrfi and schfer @xcite , gyrfi , lugosi and udina @xcite , and gyrfi , udina and walk @xcite .",
    "the present paper is devoted to the nonparametric problem of sequential prediction of real valued sequences which we do not require to necessarily satisfy the classical statistical assumptions for bounded , autoregressive or markovian processes .",
    "indeed , our goal is to show powerful consistency results under a strict minimum of conditions . to fix the context",
    ", we suppose that at each time instant @xmath4 , the statistician ( also called the _ predictor _ hereafter ) _ is asked to guess the next outcome @xmath5 _ of a sequence of real numbers @xmath6 with knowledge of the past @xmath7 ( where @xmath8 denotes the empty string ) and the side information vectors @xmath9 , where @xmath10 . in other words , adopting the perspective of on - line learning , the elements @xmath11 and @xmath12 are revealed one at a time , in order , beginning with @xmath13 , and the predictor s estimate of @xmath5 at time @xmath14 is based on the strings @xmath15 and @xmath16 .",
    "formally , the strategy of the predictor is a sequence @xmath17 of forecasting functions @xmath18 and the prediction formed at time @xmath14 is just @xmath19 .    throughout the paper",
    "we will suppose that @xmath20 are realizations of random variables @xmath21 such that the process @xmath22 is jointly stationary and ergodic .",
    "after @xmath14 time instants , the ( _ normalized _ ) _ cumulative squared prediction error _ on the strings @xmath23 and @xmath24 is @xmath25 ideally , the goal is to make @xmath26 small",
    ". there is , however , a fundamental limit for the predictability of the sequence , which is determined by a result of algoet @xcite : for any prediction strategy @xmath27 and jointly stationary ergodic process @xmath22 , @xmath28 where @xmath29 is the minimal mean squared error of any prediction for the value of @xmath30 based on the infinite past observation sequences @xmath31 and @xmath32 .",
    "generally , we can not hope to design a strategy whose prediction error exactly achieves the lower bound @xmath33 .",
    "rather , we require that @xmath26 gets arbitrarily close to @xmath33 as @xmath14 grows .",
    "this gives sense to the following definition :    a prediction strategy @xmath27 is called universally consistent with respect to a class @xmath34 of stationary and ergodic processes @xmath22 if for each process in the class , @xmath35    thus , universally consistent strategies asymptotically achieve the best possible loss for all processes in the class .",
    "algoet @xcite and morvai , yakowitz and gyrfi @xcite proved that there exist universally consistent strategies with respect to the class @xmath36 of all bounded , stationary and ergodic processes .",
    "however , the prediction algorithms discussed in these papers are either very complex or have an unreasonably slow rate of convergence , even for well - behaved processes . building on the methodology developed in recent years for prediction of individual sequences ( see cesa - bianchi and lugosi @xcite for a survey and references ) , gyrfi and lugosi introduced in @xcite a histogram - based prediction strategy which is `` simple '' and yet universally consistent with respect to the class @xmath36 .",
    "a similar result was also derived independently by nobel @xcite .",
    "roughly speaking , both methods consider several partitioning estimates ( called _ experts _ in this context ) and combine them at time @xmath14 according to their past performance . for this , a probability distribution on the set of experts is generated , where a `` good '' expert has relatively large weight , and the average of all experts predictions is taken with respect to this distribution .    the purpose of this paper is to further investigate nonparametric expert - oriented strategies for unbounded time series prediction . with this aim in mind , in section 2.1",
    "we briefly recall the histogram - based prediction strategy initiated in @xcite , which was recently extended to unbounded processes by gyrfi and ottucsk @xcite . in section 2.2 and 2.3",
    "we offer two `` more flexible '' strategies , called respectively _ kernel _ and _ nearest neighbor - based _ prediction strategies , and state their universal consistency with respect to the class of all ( non - necessarily bounded ) stationary and ergodic processes with finite fourth moment . in section 2.4 we consider as an alternative a prediction strategy based on combining generalized linear estimates . in section 2.5",
    "we use the techniques of the previous section to give a simpler prediction strategy for stationary gaussian ergodic processes .",
    "extensive experimental results based on real - life data sets are discussed in section 3 , and proofs of the main results are given in section 4 .",
    "in this section , we briefly describe the histogram - based prediction scheme due to gyrfi and ottucsk @xcite for _ unbounded _ stationary and ergodic sequences .",
    "the strategy is defined at each time instant as a convex combination of _ elementary predictors _ ( the so - called _ experts _ ) , where the weighting coefficients depend on the past performance of each elementary predictor . to be more precise",
    ", we first define an infinite array of experts @xmath37 , @xmath38 as follows .",
    "let @xmath39 be a sequence of finite partitions of @xmath40 , and let @xmath41 be a sequence of finite partitions of @xmath42 . introduce the corresponding quantizers : @xmath43 and @xmath44 to lighten notation a bit , for any @xmath14 and @xmath45 , we write @xmath46 for the sequence @xmath47 and similarly , for @xmath48 we write @xmath49 for the sequence @xmath50 .    the sequence of experts @xmath37 , @xmath51 is defined as follows .",
    "let @xmath52 be the locations of the matches of the last seen strings @xmath53 of length @xmath54 and @xmath55 of length @xmath56 in the past according to the quantizer with parameters @xmath56 and @xmath57 : @xmath58 and introduce the truncation function @xmath59 now define the elementary predictor @xmath60 by @xmath61 where @xmath62 is defined to be @xmath63 and @xmath64 here and throughout , for any finite set @xmath65 , the notation @xmath66 stands for the size of @xmath65 .",
    "we note that the expert @xmath60 can be interpreted as a ( truncated ) histogram regression function estimate drawn in @xmath67 ( gyrfi , kohler , krzyak and walk @xcite ) .",
    "the proposed prediction algorithm proceeds with an exponential weighting average method .",
    "formally , let @xmath68 be a probability distribution on the set of all pairs @xmath69 of positive integers",
    "such that for all @xmath56 and @xmath57 , @xmath70 .",
    "fix a learning parameter @xmath71 , and define the weights @xmath72 and their normalized values @xmath73 the prediction strategy @xmath27 at time @xmath14 is defined by @xmath74    it is proved in @xcite that this scheme is universally consistent with respect to the class of all ( non - necessarily bounded ) stationary and ergodic processes with finite fourth moment , as stated in the following theorem . here and throughout the document , @xmath75 denotes the euclidean norm .",
    "[ cons ] assume that    * the sequence of partitions @xmath76 is nested , that is , any cell of @xmath77 is a subset of a cell of @xmath78 , @xmath79 ; * the sequence of partitions @xmath80 is nested ; * the sequence of partitions @xmath76 is asymptotically fine , i.e. , if @xmath81 denotes the diameter of a set , then for each sphere @xmath82 centered at the origin @xmath83 * the sequence of partitions @xmath80 is asymptotically fine .",
    "then , if we choose the learning parameter @xmath84 of the algorithm as @xmath85 the histogram - based prediction scheme @xmath27 defined above is universally consistent with respect to the class of all jointly stationary and ergodic processes such that @xmath86    the idea of combining a collection of concurrent estimates was originally developed in a non - stochastic context for on - line sequential prediction from deterministic sequences ( see cesa - bianchi and lugosi @xcite for a comprehensive introduction ) .",
    "following the terminology of the prediction literature , the combination of different procedures is sometimes termed _",
    "aggregation _ in the stochastic context .",
    "the overall goal is always the same : use aggregation to improve prediction . for",
    "a recent review and an updated list of references , see bunea and nobel @xcite and bunea , tsybakov and wegkamp @xcite .",
    "we introduce in this section a class of _ kernel - based _",
    "prediction strategies for ( non - necessarily bounded ) stationary and ergodic sequences .",
    "the main advantage of this approach in contrast to the histogram - based strategy is that it replaces the rigid discretization of the past appearances by more flexible rules .",
    "this also often leads to faster algorithms in practical applications .    to simplify the notation , we start with the simple `` moving - window '' scheme , corresponding to a uniform kernel function , and treat the general case briefly later . just like before",
    ", we define an array of experts @xmath37 , where @xmath56 and @xmath57 are positive integers .",
    "we associate to each pair @xmath69 two radii @xmath87 and @xmath88 such that , for any fixed @xmath56 @xmath89 and @xmath90 finally , let the location of the matches be @xmath91 then the elementary expert @xmath60 at time @xmath14 is defined by @xmath92 where @xmath62 is defined to be @xmath63 and @xmath93 the pool of experts is mixed the same way as in the case of the histogram - based strategy .",
    "that is , letting @xmath68 be a probability distribution over the set of all pairs @xmath69 of positive integers such that @xmath94 for all @xmath56 and @xmath57 , for @xmath71 , we define the weights @xmath72 together with their normalized values @xmath95 the general prediction scheme @xmath96 at time @xmath14 is then defined by weighting the experts according to their past performance and the initial distribution @xmath68 : @xmath97    [ th : kernel ] denote by @xmath36 the class of all jointly stationary and ergodic processes @xmath22 such that @xmath98 .",
    "choose the learning parameter @xmath84 of the algorithm as @xmath99 and suppose that ( [ eq : radius1 ] ) and ( [ eq : radius2 ] ) are verified .",
    "then the moving - window - based prediction strategy defined above is universally consistent with respect to the class @xmath36 .",
    "the proof of theorem [ th : kernel ] is in section [ proofs ] .",
    "this theorem may be extended to a more general class of kernel - based strategies , as introduced in the next remark .",
    "define a _ kernel function _ as any map @xmath100 .",
    "the kernel - based strategy parallels the moving - window scheme defined above , with the only difference that in definition ( [ troisun ] ) of the elementary strategy , the regression function estimate is replaced by @xmath101 observe that if @xmath102 is the naive kernel @xmath103 @xmath104where @xmath105 denotes the indicator function and @xmath106 , we recover the moving - window strategy discussed above .",
    "typical nonuniform kernels assign a smaller weight to the observations @xmath107 and @xmath108 whose distance from @xmath109 and @xmath110 is larger .",
    "such kernels promise a better prediction of the local structure of the conditional distribution .",
    "this strategy is yet more robust with respect to the kernel strategy and thus also with respect to the histogram strategy .",
    "this is because it does not suffer from the scaling problems of histogram and kernel - based strategies where the quantizer and the radius have to be carefully chosen to obtain `` good '' performance . to introduce the strategy , we start again by defining an infinite array of experts @xmath37 , where @xmath56 and @xmath57 are positive integers . just like before ,",
    "@xmath56 is the length of the past observation vectors being scanned by the elementary expert and , for each @xmath57 , choose @xmath111 such that @xmath112 and set @xmath113 ( where @xmath114 is the floor function ) . at time @xmath14 ,",
    "for fixed @xmath56 and @xmath57 ( @xmath115 , the expert searches for the @xmath116 nearest neighbors ( nn ) of the last seen observation @xmath109 and @xmath110 in the past and predicts accordingly .",
    "more precisely , let @xmath117 and introduce the elementary predictor @xmath118 if the sum is non void , and @xmath63 otherwise .",
    "next , set @xmath119 finally , the experts are mixed as before : starting from an initial probability distribution @xmath68 , the aggregation scheme is @xmath120 where the probabilities @xmath121 are the same as in ( [ eq : pkln ] ) .",
    "[ th : nn ] denote by @xmath36 the class of all jointly stationary and ergodic processes @xmath22 such that @xmath98 .",
    "choose the parameter @xmath84 of the algorithm as @xmath99 and suppose that ( [ eq : pl ] ) is verified .",
    "suppose also that for each vector @xmath122 the random variable @xmath123 has a continuous distribution function .",
    "then the nearest neighbor prediction strategy defined above is universally consistent with respect to the class @xmath36 .",
    "the proof is a combination of the proof of theorem [ th : kernel ] and the technique used in @xcite .",
    "this section is devoted to an alternative way of defining a universal predictor for stationary and ergodic processes .",
    "it is in effect an extension of the approach presented in gyrfi and lugosi @xcite to non - necessarily bounded processes .",
    "once again , we apply the method described in the previous sections to combine elementary predictors , but now we use elementary predictors which are generalized linear predictors .",
    "more precisely , we define an infinite array of elementary experts @xmath37 , @xmath124 as follows .",
    "let @xmath125 be real - valued functions defined on @xmath126 .",
    "the elementary predictor @xmath60 generates a prediction of form @xmath127 where the coefficients @xmath128 are calculated according to the past observations @xmath129 , @xmath130 , and @xmath119 formally , the coefficients @xmath128 are defined as the real numbers which minimize the criterion @xmath131 if @xmath132 , and the all - zero vector otherwise .",
    "it can be shown using a recursive technique ( see e.g. , tsypkin @xcite , gyrfi @xcite , singer and feder @xcite , and gyrfi and lugosi @xcite ) that the @xmath128 can be calculated with small computational complexity .",
    "the experts are mixed via an exponential weighting , which is defined the same way as earlier .",
    "thus , the aggregated prediction scheme is @xmath120 where the @xmath121 are calculated according to ( [ eq : pkln ] ) .",
    "combining the proof of theorem [ th : kernel ] and the proof of theorem 2 in @xcite leads to the following result :    [ th : gl ] suppose that @xmath133 and , for any fixed @xmath56 , suppose that the set @xmath134 is dense in the set of continuous functions of @xmath135 variables .",
    "then the generalized linear prediction strategy defined above is universally consistent with respect to the class of all jointly stationary and ergodic processes such that @xmath86    we give a sketch of the proof of theorem [ th : gl ] in section [ proofs ] .",
    "we consider in this section the classical problem of gaussian time series prediction ( cf .",
    "brockwell and davis @xcite ) . in this context ,",
    "parametric models based on distribution assumptions and structural conditions such as ar(@xmath136 ) , ma(@xmath137 ) , arma(@xmath136,@xmath137 ) and arima(@xmath136,@xmath138,@xmath137 ) are usually fitted to the data ( cf .",
    "gerencsr and rissanen @xcite , gerencsr @xcite , goldenshluger and zeevi @xcite ) .",
    "however , in the spirit of modern nonparametric inference , we try to avoid such restrictions on the process structure .",
    "thus , we only assume that we observe a string realization @xmath15 of a zero mean , stationary and ergodic , gaussian process @xmath139 , and try to predict @xmath5 , the value of the process at time @xmath14 .",
    "note that there is no side information vectors @xmath129 in this purely time series prediction framework .",
    "it is well known for gaussian time series that the best predictor is a linear function of the past : @xmath140 where the @xmath141 minimize the criterion @xmath142    following gyrfi and lugosi @xcite , we extend the principle of generalized linear estimates to the prediction of gaussian time series by considering the special case @xmath143 i.e. , @xmath144 once again , the coefficients @xmath128 are calculated according to the past observations @xmath145 by minimizing the criterion : @xmath146 if @xmath147 , and the all - zero vector otherwise .    with respect to the combination of elementary experts @xmath148 , gyrfi and lugosi applied in @xcite the so - called `` doubling - trick '' , which means that the time axis is segmented into exponentially increasing epochs and at the beginning of each epoch the forecaster is reset .",
    "in this section we propose a much simpler procedure which avoids in particular the doubling - trick .",
    "to begin , we set @xmath149 where @xmath150 and combine these experts as before",
    ". precisely , let @xmath151 be an arbitrarily probability distribution over the positive integers such that for all @xmath56 , @xmath152 , and for @xmath71 , define the weights @xmath153 and their normalized values @xmath154 the prediction strategy @xmath27 at time @xmath14 is defined by @xmath155 by combining the proof of theorem [ th : kernel ] and theorem 3 in @xcite , we obtain the following result :    [ gauss ] the linear prediction strategy @xmath27 defined above is universally consistent with respect to the class of all jointly stationary and ergodic zero - mean gaussian processes .",
    "the following corollary shows that the strategy @xmath27 provides asymptotically a good estimate of the regression function in the following sense :    [ cor1 ] under the conditions of theorem [ gauss],@xmath156    corollary [ cor1 ] is expressed in terms of an almost sure cesro consistency .",
    "it is an open problem to know whether there exists a prediction rule @xmath27 such that @xmath157 for all stationary and ergodic gaussian processes .",
    "schfer @xcite proved that , under some conditions on the time series , the consistency ( [ eq : oq ] ) holds .",
    "we evaluated the performance of the histogram , moving - window kernel , nn and gaussian process strategies on two real world data sets .",
    "furthermore , we compared these performances to those of the standard arma family of methods on the same data sets .",
    "we show in particular that the four methods presented in this paper usually perform better than the best arma results , with respect to three different criteria .",
    "the two real - world time series we investigated were the monthly usa unemployment rate for january 1948 until march 2007 ( 710 points ) and daily usa federal funds interest rate for 12 january 2003 until 21 march 2007 ( 1200 points ) respectively , extracted from the website _ economagic.com_. in order to remove first - order trends , we transformed these time series into time series of _ percentage change _ compared to the previous month or day , respectively . the resulting time series",
    "are shown in figs .",
    "[ unem ] and [ time ] .        before testing the four methods of the present paper alongside the arma methods , we tested whether the resulting time series were trend / level stationary using two standard tests , the kpss test @xcite and the pp test @xcite .",
    "for both series using the kpss test , we did not reject the null hypothesis of level stationarity at @xmath158 and @xmath159 respectively , and for both series using the pp test ( which has for null hypothesis the existence of a unit root and for alternative hypothesis , level stationarity ) , the null hypothesis was rejected at @xmath160 and @xmath159 .",
    "we remark that this means the arima@xmath161 family of models , richer than arma@xmath162 is unnecessary , or equivalently , we need only to consider the arima family arima@xmath163 . as well as this , the gaussian process method requires the normality of the data . since the original data in both data sets is discretized ( and not very finely ) , this meant that the data , when transformed into percentage changes only took a small number of fixed values .",
    "this had the consequence that directly applying standard normality tests gave curious results even when histograms of the data appeared to have near - perfect gaussian forms ; however adding small amounts of random noise to the data allowed us to not systematically reject the hypothesis of normality .",
    "given each method and each time series @xmath164 ( here , @xmath165 or @xmath166 ) , for each @xmath167 we used the data @xmath168 to predict the value of @xmath169 .",
    "we used three criteria to measure the quality of the overall set of predictions .",
    "first , as described in the present paper , we calculated the normalized cumulative prediction squared error @xmath170 ( since we start with @xmath171 for practical reasons , this is almost but not exactly what has been called @xmath172 until now ) . secondly , we calculated @xmath173 , the normalized cumulative prediction error over only the last 50 predictions of the time series in order to see how the method was working after having learned nearly the whole time series .",
    "thirdly , since in practical situations we may want to predict only the _ direction _ of change , we compared the direction ( positive or negative ) of the last @xmath174 predicted points with respect to each previous , known point , to the @xmath174 real directions .",
    "this gave us the criteria @xmath175 : the _ percentage of the direction of the last 50 points correctly predicted_.    as in @xcite and @xcite , for practical reasons we chose a finite grid of experts : @xmath176 and @xmath177 for the histogram , kernel and nn strategies , fixing @xmath178 and @xmath179 .",
    "for the histogram strategy we partitioned the space into each of @xmath180 equally sized intervals , for the kernel strategy we let the radius @xmath181 take the values @xmath182 and for the nn strategy we set @xmath183 furthermore , we fixed the probability distribution @xmath68 as the uniform distribution over the @xmath184 experts .",
    "for the gaussian process method , we simply let @xmath185 and fixed the probability distribution @xmath151 as the uniform distribution over the @xmath102 experts .    used to compare standard methods with the present nonparametric strategies , the arma@xmath162 algorithm was run for all pairs @xmath186 .",
    "the arma family of methods is a combination of an autoregressive part ar@xmath187 and a moving - average part ma@xmath188 .",
    "tables [ unemp ] and [ timetest ] show the histogram , kernel , nn , gaussian process and arma results for the unemployment and interest rate time series respectively .",
    "the three arma results shown in each table are those which had the best @xmath170 , @xmath173 and @xmath175 respectively ( sometimes two or more had the same @xmath175 , in which case we chose one of these randomly ) .",
    "the best results with respect to each of the three criteria are shown in bold .",
    ".results for histogram , kernel , nn , gaussian process and arma prediction methods on the monthly percentage change in usa unemployment rate from january 1948 until march 2007 .",
    "the three arma results are those which performed the best in terms of the @xmath170 , @xmath173 and @xmath175 criteria respectively .",
    "[ unemp ] [ cols=\"^,^,^,^\",options=\"header \" , ]     we see via tables [ unemp ] and [ timetest ] that the histogram , kernel and nn strategies presented here outperform all 36 possible arma@xmath162 models ( @xmath189 ) in terms of normalized cumulative prediction error @xmath170 , and that the gaussian process method performs similarly to the best arma method . in terms of the @xmath173 and",
    "@xmath175 criteria , all of the present methods and the best arma method provide broadly similar results . from a practical point of view , we note also that the histogram , kernel and nn methods also run much faster than a single arma@xmath162 trial on a standard desktop computer . for example , the nn method is of the order of 10 to 100 times faster than an arma@xmath162 for a time series with about 1000 points , depending on the values of @xmath136 and @xmath137 .",
    "[ breiman ] let @xmath190 be a stationary and ergodic process . for each positive integer @xmath191 ,",
    "let @xmath192 denote the left shift operator , shifting any sequence @xmath193 by @xmath191 digits to the left .",
    "let @xmath194 be a sequence of real - valued functions such that @xmath195 almost surely for some function @xmath196 .",
    "suppose that @xmath197",
    ". then @xmath198    [ expert ] let @xmath199 be a sequence of prediction strategies ( experts ) .",
    "let @xmath151 be a probability distribution on the set of positive integers .",
    "denote the normalized loss of any expert @xmath200 by @xmath201 where the loss function @xmath202 is convex in its first argument @xmath203 .",
    "define @xmath204 where @xmath205 is monotonically decreasing , and set @xmath206 if the prediction strategy @xmath17 is defined by @xmath207 then , for every @xmath208 , @xmath209    * proof of theorem [ th : kernel ] . * because of ( [ algoet ] ) it is enough to show that @xmath210 with this in mind , we introduce the following notation : @xmath211 for all @xmath132 , where @xmath62 is defined to be @xmath63 , @xmath212 and @xmath213 .",
    "thus , for any @xmath37 , we can write @xmath214    by a double application of the ergodic theorem , as @xmath215 , almost surely , for a fixed @xmath216 and @xmath217 , we may write @xmath218 therefore , for all @xmath219 and @xmath122 , @xmath220 thus , by lemma [ breiman ] , as @xmath221 , almost surely , @xmath222    denote , for borel sets @xmath223 and",
    "@xmath224 , @xmath225 and set @xmath226 next , let @xmath227 denote the closed ball with center @xmath122 and radius @xmath228 .",
    "let @xmath229 then for any @xmath219 and @xmath122 which are in the support of @xmath230 , we have @xmath231 as @xmath232 and for @xmath230-almost all @xmath122 and @xmath219 by the lebesgue density theorem ( see gyrfi , kohler , krzyak and walk @xcite , lemma 24.5 ) .",
    "therefore , @xmath233 observe that @xmath234 ^ 2\\\\ & \\leq \\left(\\mathbb e\\{y_0\\,|\\,\\|x^{0}_{-k}-\\bz\\| \\le r_{k,\\ell},\\ \\|y^{-1}_{-k}-\\bs\\|\\le r'_{k,\\ell}\\}\\right)^2\\\\ & \\qquad \\mbox{(since $ |t_{\\ell}(z)| \\leq |z|$)}\\\\ & \\leq \\mathbb e",
    "\\{y_0 ^ 2\\,|\\ , \\|x^{0}_{-k}-\\bz\\| \\le r_{k,\\ell},\\ \\|y^{-1}_{-k}-\\bs\\|\\le r'_{k,\\ell}\\}\\\\ & \\qquad \\mbox{(by jensen 's inequality).}\\\\\\end{aligned}\\ ] ] consequently , @xmath235 due to the assumptions of the theorem .",
    "therefore , for fixed @xmath56 the sequence of random variables @xmath236 is uniformly integrable and by using the dominated convergence theorem we obtain @xmath237 invoking the martingale convergence theorem ( see , e.g. , stout @xcite ) , we then have @xmath238 and consequently , @xmath239    we next apply lemma [ expert ] with the choice @xmath240 and the squared loss @xmath241 we obtain @xmath242 on one hand , almost surely , @xmath243 on the other hand , @xmath244 therefore , almost surely , @xmath245 summarizing these bounds , we get that , almost surely , @xmath246 and the theorem is proved",
    ".      for fixed @xmath56 and @xmath57 , let @xmath247 then , following the proof of theorem 2 in @xcite one can show that for all @xmath248 , @xmath249 where the @xmath128 are defined in ( [ eq : cnj ] ) . using equality ( [ cc ] ) and lemma [ breiman ] ,",
    "for any fixed @xmath56 and @xmath57 we obtain that , almost surely , @xmath250 then , with similar arguments to theorem 2 in @xcite , it can be shown that @xmath251 finally , by using lemma [ expert ] , the assumptions @xmath252 and @xmath253 , and repeating the arguments of the proof of theorem [ th : kernel ] , we obtain @xmath254 as desired .",
    "99 algoet , p. universal schemes for prediction , gambling and portfolio selection , _ ann .",
    "_ , vol .  20 , pp .",
    "901941 , 1992 .",
    "algoet , p. the strong law of large numbers for sequential decisions under uncertainty , _ ieee trans .",
    "inform . theory _",
    "40 , pp .  609633 , 1994 .",
    "bosq , d. _ nonparametric statistics for stochastic processes . estimation and prediction _ , lecture notes in statistics , 110 , springer - verlag , new york , 1996 .",
    "breiman , l. the individual ergodic theorem of information theory , _ ann . math .",
    "_ , vol .",
    "28 , pp .  809811 , 1957 .",
    "correction . _",
    "_ , vol .  31 , pp .",
    "809810 , 1960 .",
    "brockwell , p. and davis , r. a. _ time series : theory and methods _ , second edition , springer - verlag , new york , 1991 .",
    "bunea , f. and nobel , a. sequential procedures for aggregating arbitrary estimators of a conditional mean , ( _ submitted _ ) , florida state university , 2005 . `",
    "http://stat.fsu.edu/  flori / ps / bnapril2005ieee.pdf ` bunea , f. , tsybakov , a. b. and wegkamp , m. h. aggregation for gaussian regression , _ ann .",
    "( accepted ) _ , 2007 .",
    "cesa - bianchi , n. and lugosi , g. _ prediction , learning , and games _ , cambridge university press , new york , 2006 .",
    "devroye , l. , gyrfi , l. and lugosi , g. _ a probabilistic theory of pattern recognition _ , springer - verlag , new york , 1996 .",
    "gerencsr , l. @xmath255 estimation and nonparametric stochastic complexity , _ ieee trans .",
    "inform . theory _",
    "38 , pp .  17681779 , 1992 .",
    "gerencsr , l. on rissanen s predictive stochastic complexity for stationary arma processes , _",
    "j. statist .",
    "plann . inference _ ,",
    "41 , pp .",
    "303325 , 1994 .",
    "gerencsr , l. and rissanen , j. a prediction bound for gaussian arma processes , _ proc . of the 25th conference on decision and control _ , 14871490 , 1986 .",
    "goldenshluger , a. and zeevi , a. nonasymptotic bounds for autoregressive time series modeling , _ ann .",
    "_ , vol .  29 , pp .",
    "417444 , 2001 .",
    "gyrfi , l. adaptive linear procedures under general conditions , _ ieee trans .",
    "inform . theory _ , vol .",
    "30 , pp .  262267 , 1984 .",
    "gyrfi , l. , hrdle , w. , sarda , p. and vieu , p.",
    "_ nonparametric curve estimation from time series _ , lecture notes in statistics , 60 , springer - verlag , berlin , 1989 .",
    "gyrfi , l. , kohler , m. , krzyak , a. and walk , h. _ a distribution - free theory of nonparametric regression _ , springer - verlag , new york , 2002 .",
    "gyrfi , l. and lugosi , g. strategies for sequential prediction of stationary time series , in _ modeling uncertainty _ , internat .",
    "management sci .",
    ", vol .  46 , pp .  225248 , kluweer acad .",
    "boston , 2001 .",
    "gyrfi , l. , lugosi , g. and udina , f. nonparametric kernel - based sequential investment strategies , _ math . finance _ ,",
    "16 , pp .  337357 , 2006 .",
    "gyrfi , l. and ottucsk , g. sequential prediction of unbounded time series , _ ieee trans . inform . theory _",
    "53 , pp .  18661872 , 2007 .",
    "gyrfi , l. and schfer , d. nonparametric prediction , in j. a. k. suykens , g. horvth , s. basu , c. micchelli and j. vandevalle , editors , _ advances in learning theory : methods , models and applications _ , pp .  339354 , ios press , nato science series , 2003 .",
    "` http://www.szit.bme.hu/",
    "oti / portfolio / histog.pdf `",
    "gyrfi , l. , udina , f. and walk , h. nonparametric nearest neighbor based empirical portfolio selection strategies , _ technical report _",
    "oti / portfolio / nn.pdf `      morvai , g. , yakowitz , s. and gyrfi , l. nonparametric inference for ergodic , stationary time series , _ ann .",
    "_ , vol .  24 , pp .",
    "370379 , 1996 .",
    "nobel , a. on optimal sequential prediction for general processes , _ ieee trans .",
    "inform . theory _",
    "49 , pp .  8398 , 2003 .",
    "schfer , d. strongly consistent online forecasting of centered gaussian processes , _ ieee trans .",
    "inform . theory _",
    "48 , pp .  791799 , 2002 .",
    "singer , a. c. and feder , m. universal linear least - squares prediction , _ international symposium of information theory _ , 2000 .",
    "stout , w. f. _ almost sure convergence _",
    ", academic press , new york , 1974 .",
    "tsypkin , ya .",
    "z. _ adaptation and learning in automatic systems _ , academic press , new york , 1971 .",
    "yule , u. on a method of investigating periodicities in disturbed series , with special reference to wlfer s sunspot numbers , _ philos .",
    "_ , vol .  a 226 , pp .",
    "267298 , 1927 ."
  ],
  "abstract_text": [
    "<S> time series prediction covers a vast field of every - day statistical applications in medical , environmental and economic domains . in this paper </S>",
    "<S> we develop nonparametric prediction strategies based on the combination of a set of `` experts '' and show the universal consistency of these strategies under a minimum of conditions . </S>",
    "<S> we perform an in - depth analysis of real - world data sets and show that these nonparametric strategies are more flexible , faster and generally outperform arma methods in terms of normalized cumulative prediction error . </S>",
    "<S> + _ index terms _  time series , sequential prediction , universal consistency , kernel estimation , nearest neighbor estimation , generalized linear estimates .    </S>",
    "<S> nonparametric sequential prediction of time series    grard biau @xmath0 , kevin bleakley @xmath1 , + lszl gyrfi @xmath2 and gyrgy ottucsk @xmath2    @xmath3 lsta & lpma + universit pierre et marie curie  </S>",
    "<S> paris vi + bote 158 , 175 rue du chevaleret + 75013 paris , france +   + @xmath1 institut de mathmatiques et de modlisation de montpellier + umr cnrs 5149 , equipe de probabilits et statistique + universit montpellier ii , cc 051 + place eugne bataillon , 34095 montpellier cedex 5 , france +   + @xmath2 department of computer science and information theory + budapest university of technology and economics + h-1117 magyar tudsok krt . </S>",
    "<S> 2 , budapest , hungary +   + </S>"
  ]
}