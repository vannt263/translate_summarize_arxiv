{
  "article_text": [
    "consider the following regression model @xmath2 where @xmath3 is the response variable for the @xmath4th subject , @xmath5 is the @xmath6th covariate for the @xmath4th subject , @xmath7 is the corresponding regression coefficient , and @xmath8 is the error term following some specified distribution .",
    "variable selection in regression problems has long been considered as one of the most important issues in modern statistics .",
    "it involves choosing an appropriate subset @xmath9 of indices @xmath10 so that for @xmath11 , the covariates @xmath5 s and estimated coefficients @xmath12 s are scientifically meaningful in interpretation , and estimates @xmath13 have relative good properties in prediction .",
    "a general approach to tackling variable selection problems is via utilizing a criterion in which the objective function is subject to a constraint on the number of covariates @xmath14 .",
    "the well - known information - based criteria such as aic and bic are of this kind . for small @xmath15 ,",
    "these criteria can be calculated by fitting models with all possible @xmath16 combinations of the covariates . however , when the number of covariates increases , the number of candidate models increases exponentially . for large @xmath15 , it is virtually impossible to calculate these criteria for all @xmath16 candidate models .",
    "+   + several approaches have been developed by modern bayesian statistics to tackle this problem .",
    "one approach is to use stochastic search methods .",
    "the methods aim to relax the combinatorial difficulty by stochastically exploring the high posterior probability regions in the model space .",
    "a well - known example is the stochastic search variable selection ( ssvs ) proposed by george and mcculloch @xcite .",
    "the ssvs assigns a mixture prior weighted by bernoulli variables to each regression coefficient , and then applying a gibbs sampling procedure to sample the posterior probability on the bernoulli variable .",
    "the resulting monte carlo average over the bernoulli samples can be seen as an estimate for the posterior inclusion probability of the covariate , and variable selection can be done by either evaluating the posterior probability with a given threshold or evaluating point estimates of regression coefficients via hypothesis testing or creditable interval construction .",
    "a general review of ssvs can be found in @xcite .",
    "recent researches in bayesian variable selection using mcmc - based inference procedures focus on providing full bayesian solutions to variable selection problems formulated by frequentists .",
    "these include the bayesian lasso @xcite,@xcite and the bayesian elastic net @xcite .",
    "other relevant approaches include @xcite .",
    "a major advantage of mcmc - based inference procedures is that they provide a practical way to assessing posterior probabilities , and inference tasks such as variable selection or point estimation can be straightforwardly carried out based on posterior probability calculation . however , as the number of covariates @xmath15 becomes quite large , mcmc - based inference procedures may become time - consuming .",
    "in addition , convergence of mcmc - based sampling algorithms is not often guaranteed .",
    "other approaches developed in modern bayesian statistics to tackle variable selection problems include those based on shrinkage - thresholding procedures .",
    "empirical bayes methods @xcite,@xcite,@xcite aim to estimate regression coefficients by first applying numerical procedures to obtain a crude , non - zero estimate for each coefficient , and then using post - processing procedures to discard those with small values or those with large variances . on the other hand ,",
    "the maximum a posteriori ( map ) approach aims to estimate regression coefficients by directly maximizing the joint posterior density function .",
    "the bayesian logistic regression model with laplace priors developed by genkin et al .",
    "@xcite is of this kind .",
    "when the number of covariates @xmath15 is large , the map approach relies on the use of efficient search algorithms for parameter estimation . in a bayesian variable selection setting , these algorithms often involve iteratively applying shrinkage - thresholding steps to obtain estimates that have sparse features , i.e. some of them have exact zero values . in this sense ,",
    "parameter estimation and variable selection can be achieved simultaneously under the map approach .",
    "in addition , with suitable algorithms , variable selection under the map approach can also be fast and efficient .",
    "it is particularly explicit in the situation in which the number of covariates @xmath15 is large but the number of covariates with non - zero coefficients is small .",
    "+   + recent frequentists approaches to variable selection focus on applying the idea of regularization estimation in the situation in which the number of variables is much larger than the number of samples .",
    "these include the scad @xcite,@xcite , the elastic net @xcite,@xcite,@xcite , the adaptive lasso @xcite , the group lasso @xcite,@xcite , the dantzig selector @xcite , the relaxed lasso @xcite , and mc@xmath17 penalty @xcite .",
    "all these approaches can either been seen as alternatives or as extensions of the lasso estimation @xcite .",
    "in addition , these approaches appear to have corresponding bayesian interpretations .",
    "for example , the adaptive lasso can be interpreted as the one which assigns laplace priors on regression coefficients , with the scale parameter estimated by using prior knowledge of the ols estimates or the ordinary lasso estimates .",
    "theoretical results provided by knight and fu @xcite show that with regular conditions on the order magnitude of the tuning parameter , the lasso is consistent in parameter estimation .",
    "however , as shown by meinshausen and bhlmann @xcite , and zou @xcite , for the lasso estimation , consistency in parameter estimation does not imply consistency in variable selection .",
    "further conditions on the design matrix and tuning parameter should be imposed to ensure consistency in variable selection for lasso type estimations .",
    "zhao and yu @xcite established the irrepresentable condition and showed that the lasso can be asymptotically consistent in both variable selection and parameter estimation if the irrepresentable condition holds and some regular conditions on the tuning parameter are satisfied .",
    "the same condition was also established by zou @xcite and yuan and lin @xcite .",
    "+   + in this paper we develop a method to carrying out map estimation for a class of bayesian models in tackling variable selection problems .",
    "the use of map estimation in variable selection problems had previously been studied by genkin et al .",
    "@xcite in a logistic regression setting .",
    "the key difference between our approach and genkin et al.s is that our model assigns a mitchell - beauchamp prior @xcite , i.e. the gaussian - based spike and slab prior weighted by bernoulli variables , on each regression coefficient .",
    "conventionally , parameter estimation for this model relies on mcmc or other simulation - based methods .",
    "recent studies by ishwaran and rao @xcite,@xcite used a rather different approach in that regression coefficients are estimated via ols - based shrinkage methods . in our map estimation , an augmented version of the posterior joint density at logarithm scale is derived . from frequentists point of view , the map estimation is equivalent to the regularization estimation with a mixture penalty of squared @xmath0 and @xmath1 norms on regression coefficients . in practice , we apply a majorization - minimization technique to modify the penalty function so that convexity for the objective function can be achieved .",
    "we further construct a coordinate - descent algorithm based on a specified iteration scheme to obtain the map estimates .",
    "simulation studies show that using the map estimates can lead to better performances in variable selection than those based on other benchmark methods in various circumstances .",
    "moreover , theoretical results show that the map estimator is asymptotically consistent in variable selection even when frequentists irrepresentable condition is violated .",
    "+   + the paper is organized as follows .",
    "section 3 focuses methodological aspects of the proposed method .",
    "section 4 provides two simulation studies on performances of the proposed method .",
    "section 5 develops relevant asymptotic analysis for the method .",
    "section 6 extends the method to parameter estimation in the generalized linear models .",
    "real data examples are provided in section 7 .",
    "discussions and concluding remarks are given in section 8 .",
    "let @xmath18 be an @xmath19 design matrix . the @xmath20th entry of @xmath18 , @xmath21 , is denoted by @xmath5 , and the @xmath4th row of @xmath18 is denoted by @xmath22 .",
    "the transpose of @xmath18 is denoted by @xmath23 .",
    "let @xmath24 , and @xmath25 . here",
    "@xmath26 is a realization of random variable @xmath27 .",
    "denote @xmath28 the @xmath29 identity matrix . for a @xmath15-dimensional vector @xmath30 ,",
    "define the @xmath31 norm by @xmath32 , the @xmath0 norm by @xmath33 , the @xmath34 norm by @xmath35 , and the @xmath1 norm by @xmath36 , where @xmath37 is an index variable such that @xmath38 if @xmath39 , and @xmath40 otherwise .",
    "the probability density of the variable @xmath41 conditional on @xmath42 is denoted by @xmath43 . for non -",
    "zero valued coefficients in @xmath41 , denote @xmath44 the corresponding index set .",
    "finally , we define the sign function for variable @xmath41 by @xmath45",
    "we start by formulating the regression model ( [ 1 ] ) under a bayesian framework .",
    "note that in a regression model a covariate can only be selected if its coefficient is estimated with a non - zero value . based on this result",
    ", we assign an index variable @xmath46 to each covariate so that if @xmath47 then , @xmath48 and @xmath49 , and if @xmath50 , then @xmath51 and @xmath52 . with @xmath53 , the regression model ( [ 1 ] )",
    "has an equivalent representation given by @xmath54 from a variable selection point of view , the index vector @xmath55 is an indicator for candidate models .",
    "different candidate models will have different values in @xmath56 .      under a bayesian framework , we assume @xmath57 the prior on @xmath7 implies that conditional on @xmath51",
    ", @xmath7 is equal to 0 with probability one , and conditional on @xmath48 , @xmath7 follows a normal distribution with mean @xmath58 and variance @xmath59 .",
    "the prior on @xmath41 is the same as the spike - slab prior proposed by mitchell and beauchamp @xcite .",
    "in addition , given fixed @xmath60 , the hyperparameter @xmath61 will play a crucial role in controlling the concentration of @xmath7 .",
    "the prior will gradually concentrate its mass on @xmath50 as @xmath62 , and will gradually disperse its mass if @xmath63 .",
    "it implies that information the prior can provide is dependent on @xmath61 , and as @xmath61 decreases , the prior will become less informative .",
    "moreover , since @xmath64 , the mixture form of the prior allows us to express it as normal@xmath65 . this representation will be used in deriving the joint posterior density of the parameters .",
    "the variance @xmath60 has prior mean @xmath66 and variance @xmath67 $ ] given that @xmath68 .",
    "the hyperparameter @xmath69 is the prior probability of including a covariate in the regression model .",
    "the bernoulli prior on @xmath53 says that if only prior information is available , @xmath53 will have probability @xmath69 to be 1 and @xmath70 to be 0 .",
    "+   + under bayesian model ( [ 2 ] ) , the joint posterior density of @xmath41 , @xmath56 and @xmath60 can be expressed as @xmath71 with ( [ 3 ] ) , we can estimate @xmath72 via various inference methods . in this paper the map ( maximum a posteriori ) method is adopted .",
    "formally the map estimator for @xmath72 is defined by @xmath73 i.e. the minimizer of the minus 2 logarithm of the joint posterior density .",
    "the minus 2 logarithm of the joint posterior density can be explicitly expressed by @xmath74 here we have used an equivalent representation normal@xmath75 for @xmath76 given that @xmath64 . note that in ( [ method.1 ] ) the term @xmath77 vanishes since for every @xmath6 , @xmath48 implies @xmath78 . in turn , @xmath79 . on the other hand",
    ", @xmath51 implies @xmath80 , and in turn @xmath81 .",
    "+   + given fixed @xmath60 , the function ( [ method.1 ] ) has some meaningful interpretations in terms of regularization estimation on @xmath41 . for practical purposes ,",
    "we fix @xmath60 and multiply ( [ method.1 ] ) with @xmath60 in the following discussion . note that since @xmath82 , the second term in ( [ method.1 ] ) can be seen as a squared @xmath0 norm on @xmath41 weighted by @xmath61 .",
    "we write it by @xmath83 .",
    "the squared @xmath0 norm is smooth over @xmath84 , the domain of @xmath41 .",
    "since it is not singular at the origin , it can not shrink @xmath41 to zero in parameter estimation .",
    "in addition , as @xmath7 increases , the penalty value under @xmath83 will increase quadratically .",
    "it means that regression coefficients with larger absolute values will be penalized heavier than those with smaller absolute values .",
    "the undesired property leads to biased estimation in regression coefficients .",
    "the squared @xmath0 norm , however , has a good property in least squares - based estimation when the number of variables @xmath15 is larger than the number of samples @xmath85 .",
    "it provides an extra source for basis expansion in least squares estimation .",
    "it is important in the @xmath86 situation in which the gram matrix @xmath87 is not of full rank , and the ordinary least squares technique is impossible to perform . with the squared @xmath0 norm on @xmath41 , a set of pseudo basis",
    "can be added to the row of the design matrix , and the full rank condition can then be satisfied .",
    "a well - known application of this technique is ridge regression , for which the least squares estimate can be stably calculated even in the situation in which @xmath15 is much larger than @xmath85 .",
    "+   + for the fourth term in ( [ method.1 ] ) , note that by definition @xmath88 , and the term @xmath89 can be seen as an @xmath31 norm on the vector @xmath56 .",
    "we can write the fourth term in ( [ method.1 ] ) by @xmath90 , where @xmath91 $ ] . note that as @xmath69 increases , @xmath92 will decrease .",
    "it implies that a strong belief in the presence of a variable will decrease the penalty value for the variable .",
    "in addition , by definition @xmath46 , and the term @xmath93 can be seen as an @xmath1 norm on @xmath41 . to see how it can be ,",
    "note that @xmath94 , which is the @xmath1 norm by definition .",
    "here we have used the assumption that @xmath95 .",
    "we can express the fourth term in ( [ method.1 ] ) as @xmath96 .",
    "now given all other parameters fixed , the map estimator of @xmath60 can be derived by first making a derivative of ( [ method.1 ] ) with respect to @xmath60 , then setting the derivative to zero , and solving the equation for @xmath60 .",
    "the estimation of @xmath41 is then carried out given @xmath60 is fixed . with fixed @xmath60 and the regularization interpretations given above , ( [ method.1 ] ) has an equivalent representation given by @xmath97 note that here we have multiplied ( [ method.1 ] ) with @xmath60 . now with ( [ 6 ] ) , we can construct an iteration scheme to obtain ( [ 5 ] ) . at the @xmath98th iteration ,",
    "the iteration scheme is defined by @xmath99 note that the objective function ( [ 6 ] ) involves an @xmath1 norm , which by definition , is not convex .",
    "therefore related optimization tasks in the second term of ( [ 7 ] ) require some refinements . here",
    "we adopt a relaxation approach to tackling the optimization problem .",
    "we begins the approach by noting that , mathematically the @xmath1 norm on a @xmath15-dimensional vector @xmath41 can be expressed as @xmath100 ) as a function of @xmath101 and using lhopital s rule .",
    "a more detailed discussion on the properties of the log - sum function in the right hand side of ( [ 9 ] ) will be given later .",
    "now we only focus on using it in solving optimization problems involving @xmath1 constraints . with representation ( [ 9 ] ) , the objective function ( [ 6 ] ) can be re - expressed as @xmath102 if @xmath101 is small enough , the log - sum function in the right hand side of ( [ 10 ] ) will give an approximate representation of @xmath103 .",
    "graphical representations for the log - sum function with different @xmath101 and their mixtures with the squared @xmath0 norm can be found in the top panel of figure [ logsum.figure1 ] . in addition , since the log - sum function in ( [ 10 ] ) is continuous on @xmath41 , the combinatorial nature of @xmath103 is relaxed .",
    "however , the term @xmath104 is neither convex nor concave on @xmath105 , and replacing @xmath103 with ( [ 9 ] ) in ( [ 6 ] ) will still make the whole objective function ( [ 10 ] ) remain non - convex . to tackle this problem ,",
    "a majorization - minimization algorithm is adopted .",
    "majorization - minimization ( mm ) algorithms @xcite,@xcite are a set of analytic procedures aiming to tackle difficult optimization problems by modifying their objective functions so that solution spaces of the modified ones are easier to explore . for an objective function @xmath106 ,",
    "the modification procedure relies on finding a function @xmath107 satisfying the following properties : @xmath108 in ( [ 10.1 ] ) , the objective function @xmath106 is said to be majorized by @xmath107 . in this sense",
    ", @xmath107 is called the majorization function .",
    "in addition , ( [ 10.1 ] ) implies that @xmath109 is tangent to @xmath106 at @xmath110 .",
    "moreover if @xmath111 is a minimizer of @xmath109 , then ( [ 10.1 ] ) further implies that @xmath112 which means that the iteration procedure @xmath110 pushes @xmath106 toward its minimum .",
    "+   + now we turn back to the function in the right hand side of ( [ 10 ] ) .",
    "note that , since @xmath113 is a concave function of @xmath42 for @xmath114 , therefore the inequality @xmath115 holds for all @xmath114 and @xmath116 .",
    "note that the right hand side of ( [ 10.2 ] ) is convex in @xmath42 .",
    "in addition , if we let @xmath117 , then ( [ 10.2 ] ) becomes an equality , which implies that the right hand side of ( [ 10.2 ] ) satisfies the properties stated in ( [ 10.1 ] ) , therefore is a valid function for majorizing @xmath113 .",
    "[ proposition1 ] define @xmath118 and let @xmath119 be the same as ( [ 6 ] ) but without the constant term",
    ". then @xmath119 can be majorized by the following function : @xmath120 where @xmath121    _ proof of proposition [ proposition1 ] . _",
    "assume @xmath122 minimizes @xmath123 given @xmath124 .",
    "then with ( [ 9 ] ) and the inequality ( [ 10.2 ] ) , the quantity @xmath125 can be bounded in a way such that @xmath126 which verifies the first condition stated in ( [ 10.1 ] ) . for @xmath127 , @xmath128 is equal to the log - sum function in ( [ 9 ] ) , which verifies the second condition stated in ( [ 10.1 ] ) and completes the proof . +   + a graphical representation of using mm algorithms in approximating the log - sum function in ( [ 9 ] )",
    "can be found in the bottom - left panel of figure [ logsum.figure1 ] . from the argument given above",
    ", we can construct an iteration scheme to minimize @xmath129 , with the @xmath1 norm , or equivalently the log - sum function , replaced by @xmath128 defined in proposition [ proposition1 ] .",
    "for example , in ( [ 7 ] ) , @xmath130 can be obtained by carrying out the following iteration scheme : @xmath131 over index @xmath132 , where @xmath133^{-1}$ ] .",
    "the procedure of using iteration scheme ( [ 12 ] ) in solving the problem of minimizing ( [ 6 ] ) , or equivalent ( [ 10 ] ) , is called the bava - mio ( bayesian variable selection using a majorization - minimization approach ) , and the corresponding minimizer is called the bava - mio estimator .",
    "+   + note that the last term in the right hand side of ( [ 12 ] ) is a linear combination of @xmath134 , a convex function of @xmath7 , therefore given @xmath135 is convex , the whole objective function in ( [ 12 ] ) will be convex , which guarantees that the iteration scheme will converge .",
    "in addition , the minimizer ( [ 12 ] ) can be solved by using the coordinate descent algorithm proposed by friedman et al .",
    "@xcite . in practice",
    ", the coordinate descent algorithm is based on iteratively cycling a one - dimensional soft - thresholding scheme . at the @xmath136th iteration ,",
    "the soft - thresholding scheme for the @xmath6th coordinate is given by @xmath137 where @xmath138 , with @xmath139 for @xmath140 , and @xmath141 for @xmath142 . here",
    "@xmath143 is a soft - thresholding operator defined by @xmath144 . a detailed derivation of ( [ 13.1 ] )",
    "is given in appendix a.      choosing appropriate hyperparameters for prior construction is an important issue in many bayesian inference problems . for hyperparameters present in the model ( [ 2 ] ) , we consider the triple @xmath145 first .",
    "one principle we adopt in parameterizing the hyperparameters is that as the number of samples @xmath85 increases , the impact of the hyperparameters in posterior inference will become less significant .",
    "for example , in the marginalized likelihood , we wish to see @xmath146 as @xmath147 . in this sense",
    ", we may let @xmath148 .",
    "in addition , we let @xmath149 , so that the prior expectation of @xmath60 is equal to 1 . given these conditions , one of the possible choices is @xmath150 .",
    "we will will this setting in the simulation study in the later section .",
    "+   + now we consider the prior inclusion probability @xmath69 . in some circumstances",
    "data - driven empirical bayes approaches are proposed @xcite , while in other circumstances full bayesian methods that assign priors on @xmath69 are proposed @xcite . unlike previously proposed approaches , in which single point estimates were obtained for @xmath69 , we adopt an approach by specifying a feasible region for the function @xmath151\\widehat{\\phi}^{(0)}.\\end{aligned}\\ ] ] here @xmath152 for @xmath153 since the term @xmath154 uses initial values @xmath155 for @xmath153 .",
    "the function @xmath156 is the threshold used in the soft - thresholding operator ( [ 13.1 ] ) .",
    "we carry out the inference procedure with values in the feasible region and look for which values of @xmath156 lead to the best performance measured by criteria such as the one used in ten fold cross validation or the bayes factor . under this",
    "approach estimated parameters can be seen as functions of @xmath69 on the feasible region .",
    "given different values of @xmath69 , curve - like paths for estimated parameters can be obtained .",
    "the main reason we adopt this `` whole - path '' fitting strategy is that the optimization procedure may get stuck at some stationary points . to explain this , note that we need an initial value @xmath157 to run the iteration scheme ( [ 12 ] ) . by definition ,",
    "@xmath157 is a function of @xmath158 , which by definition , is a function of @xmath156 . as pointed out by cands et al .",
    "@xcite and mazumder et al .",
    "@xcite , different @xmath157 may lead to different solutions for the minimizer .",
    "under this situation a global minimum may not be guaranteed . by using the strategy given above",
    ", we can run the iteration scheme ( [ 12 ] ) with a large number of possible values of @xmath157 , therefore eliminating the possibility that the solution is stuck at some local minima . +   + in addition , the reason we do not directly specify a feasible region for @xmath69 but rather the function @xmath156 is because the threshold @xmath156 is non - linear in @xmath69 .",
    "if the function had been derived from a specified region for @xmath69 , there would have been wide and non - equal gaps between discretized values of @xmath156 . as implied in the soft - thresholding operator representation ( [ 13.1 ] ) , a large positive or negative increment in @xmath156 might lead to a sudden decrease or increase in the number of selected covariates .",
    "one way to stabilize the estimation procedure is to steadily decrease ( or increase ) the value of @xmath156 .",
    "+   + moreover , although we only specify the feasible region for @xmath156 , we can still obtain @xmath69 by using the following calibration strategy . to begin the strategy ,",
    "we fix @xmath60 and @xmath61 first . note that the minimum requirement to include a covariate in the model is that @xmath159 should be greater than @xmath156 .",
    "assume @xmath159 is upper bounded by constant @xmath160 .",
    "a feasible region for @xmath156 is defined as @xmath161 $ ] . for an arbitrary @xmath162 $ ] ,",
    "the corresponding @xmath163 for @xmath164 is given by @xmath165 where @xmath166 .",
    "in practice , we discretize the feasible region @xmath161 $ ] into equal spaces and then use the discretized values to calculate @xmath163 according to ( [ 20 ] ) .",
    "our approach is the same as the one using a fixed grid on the tuning parameter for parameter estimation .",
    "this fixed grid approach to tuning parameter selection has been adopted in @xcite , @xcite , and @xcite , and is advocated by @xcite and @xcite for fast and accurate parameter estimation .      here",
    "we provide a toy example to illustrate the bava - mio estimation .",
    "we let the number of samples @xmath167 and the number of covariates @xmath168 . for regression coefficients @xmath169 ,",
    "we let @xmath170 , @xmath171 , @xmath172 , @xmath173 , and @xmath50 for @xmath174 .",
    "we simulate each row of @xmath18 independently identically from mvn@xmath175 , and then calculate @xmath176 with @xmath177 mvn@xmath178 . for the hyperparameters , we let @xmath179 , @xmath180 and @xmath181 .",
    "further let @xmath182 .",
    "we use 100 equal spaced points to form a grid for @xmath183 .",
    "we perform two bava - mio estimations : one uses the bayes factor and the other uses ten fold cross validation for tuning parameter selection .",
    "we define the bayes factor between models @xmath184 and @xmath185 by @xmath186 where the term @xmath187 refers to the marginalized likelihood with @xmath41 and @xmath60 being integrated out with respect to their prior probability measures . for the bayesian model stated in ( [ 2 ] )",
    ", the marginalized likelihood has a closed form representation given by @xmath188}.\\nonumber\\end{aligned}\\ ] ] in subsequent sections we will use the measure ( [ bayes.factor ] ) for variable selection . in addition , for all variable selection tasks using ( [ bayes.factor ] ) , the baseline model @xmath185 will always refer to the null model .",
    "+   + the results are shown in figure [ toy.figure1 ] .",
    "the path plot in the top left panel of figure [ toy.figure1 ] shows that non - zero coefficients entered into the model earlier under the bava - mio estimation .",
    "in addition , the paths of estimated coefficients behave similar to those under the hard - thresholding estimation , that is , once a coefficient is estimated to be non - zero , the corresponding estimation path makes a sharp jump to the non - thresholded value .",
    "moreover , due to the presence of the squared @xmath0 norm in the objective function , the number of selected covariates can be larger than the number of samples . throughout the estimation procedure ,",
    "the maximum number of selected covariates is 831 , which is much larger than the number of samples @xmath167 .",
    "here we also provide the lasso estimation for regression fitting with the same data .",
    "the results are shown in the right panel of figure [ toy.figure1 ] . as compared with the lasso estimation , in which 33 covariates are selected using ten fold cross validation",
    ", the bava - mio estimations using the bayes factor and ten fold cross validation correctly select covariates with non - zero coefficients .",
    "in addition , as shown in the bottom panel of figure [ toy.figure1 ] , values of the non - zero coefficients are also estimated more accurately under the bava - mio estimations .",
    "we briefly discuss properties of the log - sum function stated in ( [ 9 ] ) .",
    "first , note that @xmath189 . by multiplying @xmath190 to the sum @xmath191 and",
    "let @xmath192 , one obtains the logarithm of the product of @xmath193 over @xmath153 . as pointed out by tipping @xcite , the term @xmath193 is an improper version of student s @xmath194 density .",
    "a rather different way is to see the log - sum function @xmath195 as a product of logarithm of the generalized pareto density , which has a parametric form given by @xmath196 for @xmath197 , @xmath198 , @xmath199 , and @xmath200 . by multiplying @xmath201 and adding a constant term @xmath202 to @xmath195 , it becomes @xmath203 , which is a logarithm of the product of generalized pareto densities with location parameter @xmath204 , scale parameter @xmath205 , and shape parameter @xmath206 .",
    "+   + the following two propositions discuss relationships between the log - sum function and the @xmath1 and @xmath31 norms .",
    "the first one states that the error rate between the log - sum function and the @xmath1 norm measured by an @xmath31 distance is of order @xmath207 as @xmath208 .",
    "proofs of the two propositions will be given in appendix b.    [ proposition2 ] define @xmath209 and @xmath210 .",
    "then for @xmath211 , there exists a positive constant @xmath212 such that @xmath213    a graphical representation of proposition [ proposition2 ] can be found in the bottom - right panel of figure [ logsum.figure1 ] .",
    "the next proposition states that the log - sum function can do better in approximating the @xmath1 norm than the @xmath31 norm as @xmath192 . on the other hand , results in this proposition also implies that the log - sum function approaches to @xmath31 norm as @xmath214 .",
    "sriperumbudur et al .",
    "@xcite gave another heuristic argument for this property .",
    "[ proposition3 ] with the same notation used in proposition 2 , for @xmath47 and @xmath215 $ ] , we have @xmath216 and @xmath217 therefore , @xmath218 and @xmath219 .",
    "first we make some remarks on the use of the @xmath1 norm in regularization estimation .",
    "fuchs @xcite , donoho et al .",
    "@xcite , and tropp @xcite independently showed that under some regular conditions , regression coefficients estimated with the @xmath1 constraint can be approximated by those estimated with the @xmath31 constraint .",
    "the advantage of using the @xmath31 norm instead of the @xmath1 norm as a constraint on regression coefficients is that minimization with an @xmath31 norm constraint is a convex optimization problem while the minimization problem with an @xmath1 norm constraint is combinatorial in nature .",
    "+   + however , as shown by fan and li @xcite , the @xmath31 norm tends to provide larger penalty values to large coefficients and smaller penalty values to small coefficients .",
    "therefore large coefficients tend to be biased estimated while zero - valued coefficients tend to be estimated with non - zero values . on the other hand ,",
    "the @xmath1 norm provides equal penalty values to all coefficients , therefore is more likely to shrink small coefficients to zero and keep large coefficients unchanged .",
    "cands et al .",
    "@xcite proposed a reweighted @xmath31 approach for sparse recovery .",
    "they showed that the @xmath1 norm can be better approximated by the sum of some log functions than the conventional @xmath31 norm .",
    "sriperumbudur et al .",
    "@xcite further explored the idea and used a modified log - sum function to approximate the @xmath1 norm in solving sparse generalized eigenvalue problems related to principal component analysis , canonical correlation analysis , and fisher s discriminant analysis . +   +",
    "we have noticed that the use of binary indicators for variable selection has been studied by yuan and lin under the name of non - negative garrotte estimator @xcite . however , the estimation procedure associated to the non - negative garrotte estimator is quite different from the bava - mio estimation proposed in this paper . in yuan and lin s proposal ,",
    "the non - negative garrotte estimation is carried out via a two stage procedure . in the first stage",
    ", least squares estimation is proposed to obtain an initial estimate for each regression coefficient . in the second stage",
    ", a soft - thresholding estimation is proposed to obtain an estimate for the binary indicator associated to each regression coefficient . under the soft - thresholding estimation ,",
    "the estimate for the binary indicator is continuous on the interval between 0 and 1 . in this sense ,",
    "the non - negative garrotte estimation can be seen as a shrinkage estimation on the least squares estimate . +   +",
    "we have also noticed that the objective function stated in ( [ 12 ] ) is similar to the one used to obtain the adaptive elastic net recently developed by zou and zhang @xcite .",
    "however , the weights used in the adaptive elastic net objective are fixed while in ( [ 12 ] ) the weights are iteratively changed throughout the optimization procedure . in addition , zou and zhang did not see the @xmath31 norm with adaptive weights as an approximation to the @xmath1 norm .",
    "in this section we conduct two simulation studies . the first one is a general test on the performance of the bava - mio estimation .",
    "the second one focuses the performance of the bava - mio estimation under various situations in which the irrepresentable condition may or may not hold .      in the first simulation study",
    "we compare the proposed method with other estimation approaches by fitting regression model @xmath176 with data generated from different simulation schemes . here",
    "@xmath220 and @xmath221 are @xmath85-dimensional vectors , @xmath18 is an @xmath19 matrix of @xmath19 matrix , and @xmath41 is a @xmath15-dimensional vector .",
    "we assume each entry in @xmath221 is i.i.d . from normal@xmath222 , and each row in the design matrix @xmath18 is i.i.d . from mvn@xmath223 . throughout the whole simulation study , we let @xmath224 , and",
    "generate @xmath7 from a standard normal distribution for @xmath225 and let @xmath50 for @xmath226 .",
    "that is , we have 10 non - zero and 110 zero coefficients in the `` true '' model .",
    "in addition , we use different values of @xmath227 in generating the design matrix @xmath18 and the error term @xmath221 .",
    "we apply three different @xmath228 to generate the design matrix .",
    "the first one has an independent structure with diagonal terms equal to 1 and off - diagonal terms equal to 0 .",
    "the second one has a covariance structure such that @xmath229 for @xmath230 and @xmath231 for @xmath232 .",
    "the third one has a toepolitz structure such that @xmath233 .",
    "now define the signal - to - noise ratio by snr @xmath234 .",
    "we consider @xmath235 and @xmath236 in generating the error term @xmath221 .",
    "these values correspond to snr @xmath237 and @xmath238 , respectively . by using @xmath18 , @xmath41 and @xmath221 ,",
    "the response vector @xmath220 is calculated by @xmath176 . for the number of samples , we consider five values @xmath239 and @xmath240 . with three different structures for @xmath228 , three different values for @xmath241 , and five different values for @xmath85",
    ", we have total @xmath242 simulation experiments . +   + we let hyperparameters @xmath150 for the cases of snr @xmath243 and @xmath238 . for the case of snr @xmath244 ,",
    "we use @xmath245 where @xmath246 is an average over the top 10 percent absolute values of the sample correlations between response @xmath220 and covariates @xmath18 .",
    "note that the prior guesses on @xmath247 and @xmath248 under the bayesian model ( [ 2 ] ) and parametrization ( [ simu.1 ] ) are given by @xmath249&=&\\frac{\\tau_{2}}{\\tau_{1}-1}=\\bigg(\\frac{1-\\widehat{\\text{corr}}}{\\widehat{\\text{corr}}}\\bigg)\\frac{p\\log p}{n}\\nonumber\\\\ \\frac{\\mathbb{e}[\\sigma^{2}|\\ \\tau_{1},\\tau_{2}]}{\\lambda}&=&\\bigg(\\frac{\\widehat{\\text{corr}}}{1-\\widehat{\\text{corr}}}\\bigg)\\sqrt{\\frac{p}{n}}.\\nonumber\\end{aligned}\\ ] ] as @xmath250 , the prior expectation of @xmath241 approaches to zero and the prior expectation of @xmath251 approaches to @xmath252 . for tuning parameter selection , we use two procedures : the bayes factor , which is defined in ( [ bayes.factor ] ) , and ten fold cross validation .",
    "the corresponding estimators are called bmio - bf and bmio - cv , respectively .",
    "+   + we also carry out three other estimation approaches for comparisons .",
    "the first one is the lasso @xcite , which is defined by @xmath253 we use r package `` glmnet '' to obtain the lasso estimates .",
    "the tuning parameter @xmath61 is selected using ten fold cross validation .",
    "the second approach is the relaxed lasso @xcite , which is defined by @xmath254 we use r package `` relaxo '' , which is the companion software to @xcite , to obtain the relaxed lasso estimates .",
    "the tuning parameter @xmath61 is selected with ten fold cross validation with 100 @xmath255 values equally spaced in @xmath256 $ ] .",
    "the third approach is the adaptive lasso @xcite , which is defined by @xmath257 we use r package `` parcor '' to obtain the adaptive lasso estimates with the default setting of @xmath258 as the initial value for @xmath259 and tuning parameter @xmath61 selected via ten fold cross validation .",
    "+   + we collect several performance measures at each simulation run .",
    "the first one is the standardized @xmath0 distance between @xmath260 and @xmath261 , which is defined by @xmath262 the second one is the predictive mean squared error of @xmath260 for a test data set , which is defined by @xmath263 the test data set contains @xmath264 data points generated using a simulation scheme the same as the training data set .",
    "the third one is the number of coefficients with non - zero estimated values @xmath265 , where @xmath266 .",
    "the final one is the sign function - based false positive rates , which is defined by @xmath267 where the sign function sign@xmath268 is defined in ( [ notation.1 ] ) .",
    "+   + for each of the 45 simulation experiments , we generate 100 runs to collect the four performance measures .",
    "we then plot the average of each peformance measure against the ratio @xmath269 , i.e. the ratio between the number of samples and the number of true coefficients with non - zero values .",
    "these plots are shown in figures [ simui.fig1 ] , [ simui.fig2 ] , and [ simui.fig3 ] for snr @xmath270 and @xmath238 , respectively . from the three figures we can see none of the estimation approaches can dominate the others in all aspects of the performance measures . in most cases ,",
    "bava - mio based estimations have smaller sign function - based false positive rates , as shown in the second column of each figure .",
    "it implies that more accurate variable selection may be done using bava - mio estimations .",
    "in general , bava - mio estimations have fewer numbers of non - zero estimates , as shown in the fourth column of each figure .",
    "these findings become more significant as the number of samples increases . in addition , since the bava - mio estimation using the bayes factor has the fewest numbers of non - zero estimates than other estimation approaches , it is surprising that the pmse and @xmath0-dis measures under the bmio - bf estimation are comparable to those under other estimation approaches , for example , in cases with snr @xmath271 and in some cases with snr @xmath244 .",
    "however , we also noticed that the bmio - bf estimation has higher values in the pmse and @xmath0-dis in the cases with snr @xmath272 , particularly in those with small numbers of samples .",
    "figure [ simu.figure1.1 ] show heatmaps for the s - fpr and rankings of the s - fpr for the five estimation approaches under the 45 simulation scenarios .",
    "the heatmaps are generated by using the graphical software gap ( generalized associated plots ) , which was developed by wu , tien and chen @xcite as companion software to @xcite ( ` http://gap.stat.sinica.edu.tw/software/gap/index.htm ` ) .",
    "the gap - based heatmaps further suggest that using bava - mio estimation can lead to more accurate variable selection .      in the second simulation study",
    "we investigate the impact of the irrepresentable condition on the performance of bava - mio estimation in variable selection . before stating the irrepresentable condition",
    ", we give some notation definitions first .",
    "we assume the true model is parametrized by @xmath273 .",
    "define @xmath274 and @xmath275 .",
    "denote @xmath276 the coefficients with indices in @xmath277 , and @xmath278 the coefficients with indices in @xmath279 .",
    "similar definitions are also applied to @xmath280 and @xmath281 , respectively .",
    "an estimator @xmath282 is said to be sign consistent in estimating @xmath41 if the probability of the event @xmath283 approaches to @xmath284 as @xmath147 .",
    "given the sign consistency holds , the estimated index set @xmath285 will be the same as the true index set @xmath277 , therefore the sign consistency implies variable selection consistency , that is , asymptotically with probability one , non - zero valued coefficients will have non - zero estimated values , and zero - valued coefficients will be estimated with zero values .",
    "+   + zhao and yu @xcite showed that if the lasso estimation wants to achieve the sign consistency , then the design matrices @xmath286 and @xmath287 must satisfy the following condition : latexmath:[\\ ] ] which completes the proof .                            .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ]    .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ]    .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ] .",
    "top : model 1 ( covariance matrix with off - diagonal terms equal to @xmath58 ) ; middle : model 2 ( covariance matrix with off - diagonal terms equal to @xmath416 ) ; bottom : model 3 ( covariance matrix with off - diagonal terms following a toepolitz structure ) .",
    "first column : standardized @xmath0-distance btween estimated and true values ; second column : sign function - adjusted false positive rate ; thrid column : prediction mean squared error ; fourth column : number of non - zero estimates.,title=\"fig:\",scaledwidth=24.0% ]         against the irrepresentable statistic under different signal - to - noise ratios.,title=\"fig:\",scaledwidth=45.0% ]   against the irrepresentable statistic under different signal - to - noise ratios.,title=\"fig:\",scaledwidth=45.0% ]   against the irrepresentable statistic under different signal - to - noise ratios.,title=\"fig:\",scaledwidth=45.0% ]   against the irrepresentable statistic under different signal - to - noise ratios.,title=\"fig:\",scaledwidth=45.0% ]    ; center @xmath587 ; right @xmath588 .",
    "top : the cv error and test error along the fitting path ; middle : the number of selected genes along the fitting path ; bottom : scatter plot for estimated label probabilities . the vertical dash line in each plot in the top two panels indicates where the tuning parameter is selected.,title=\"fig:\",scaledwidth=32.0% ] ; center @xmath587 ; right @xmath588 .",
    "top : the cv error and test error along the fitting path ; middle : the number of selected genes along the fitting path ; bottom : scatter plot for estimated label probabilities .",
    "the vertical dash line in each plot in the top two panels indicates where the tuning parameter is selected.,title=\"fig:\",scaledwidth=32.0% ] ; center @xmath587 ; right @xmath588 .",
    "top : the cv error and test error along the fitting path ; middle : the number of selected genes along the fitting path ; bottom : scatter plot for estimated label probabilities . the vertical dash line in each plot in the top two panels indicates where the tuning parameter is selected.,title=\"fig:\",scaledwidth=32.0% ] ; center @xmath587 ; right @xmath588 .",
    "top : the cv error and test error along the fitting path ; middle : the number of selected genes along the fitting path ; bottom : scatter plot for estimated label probabilities .",
    "the vertical dash line in each plot in the top two panels indicates where the tuning parameter is selected.,title=\"fig:\",scaledwidth=32.0% ] ; center @xmath587 ; right @xmath588 .",
    "top : the cv error and test error along the fitting path ; middle : the number of selected genes along the fitting path ; bottom : scatter plot for estimated label probabilities . the vertical dash line in each plot in the top two panels indicates where the tuning parameter is selected.,title=\"fig:\",scaledwidth=32.0% ] ; center @xmath587 ; right @xmath588 .",
    "top : the cv error and test error along the fitting path ; middle : the number of selected genes along the fitting path ; bottom : scatter plot for estimated label probabilities . the vertical dash line in each plot in the top two panels indicates where the tuning parameter is selected.,title=\"fig:\",scaledwidth=32.0% ] ; center @xmath587 ; right @xmath588 .",
    "top : the cv error and test error along the fitting path ; middle : the number of selected genes along the fitting path ; bottom : scatter plot for estimated label probabilities . the vertical dash line in each plot in the top two panels indicates where the tuning parameter is selected.,title=\"fig:\",scaledwidth=32.0% ] ; center @xmath587 ; right @xmath588 .",
    "top : the cv error and test error along the fitting path ; middle : the number of selected genes along the fitting path ; bottom : scatter plot for estimated label probabilities . the vertical dash line in each plot in the top two panels indicates where the tuning parameter is selected.,title=\"fig:\",scaledwidth=32.0% ] ; center @xmath587 ; right @xmath588 .",
    "top : the cv error and test error along the fitting path ; middle : the number of selected genes along the fitting path ; bottom : scatter plot for estimated label probabilities . the vertical dash line in each plot in the top two panels indicates where the tuning parameter is selected.,title=\"fig:\",scaledwidth=32.0% ]    .the sign probability @xmath589 under different signal - to - noise ratios .",
    "each value is calculated by averaging over 100 simulation runs , and the corresponding standard error is given in the bracket . the term corr . in the second line of each panel",
    "is the squared correlation between the sign probability and the irrepresentable statistic .",
    "we use kendall s @xmath590 in the correlation calculation . [ cols=\"<,^,^,^,^,^\",options=\"header \" , ]"
  ],
  "abstract_text": [
    "<S> we develop a method to carry out map estimation for a class of bayesian regression models in which coefficients are assigned with gaussian - based spike and slab priors weighted by bernoulli variables . unlike simulation - based inference methods , the proposed method directly optimizes the logarithm of the joint posterior density for parameter estimation . </S>",
    "<S> the corresponding optimization problem has an objective function in lagrangian form in that regression coefficients are regularized by a mixture of squared @xmath0 and @xmath1 norms . a tight approximation to the @xmath1 norm using majorization - minimization techniques </S>",
    "<S> is derived , and a coordinate descent algorithm in conjunction with a specified soft - thresholding scheme is used in searching for the optimizer of the approximated objective . </S>",
    "<S> simulation studies show that the proposed method can lead to more accurate variable selection than other benchmark methods . </S>",
    "<S> it also shows that the irrepresentable condition ( zhao and yu , 2006 ) appears to have less impacts on the performance of the proposed method . </S>",
    "<S> theoretical results further show that under some regular conditions , sign consistency can always be established , even when the irrepresentable condition is violated . results on posterior model consistency and estimation consistency , and an extension to parameter estimation in the generalized linear models are provided . </S>",
    "<S> +   + * keywords : * map estimation ; @xmath1 norm ; majorization - minimization algorithms ; irrepresentable condition . </S>"
  ]
}