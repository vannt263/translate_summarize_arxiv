{
  "article_text": [
    "_ compressed sensing ( cs ) _  @xcite has become an important and popular topic in several fields , including computer science , engineering , applied mathematics , and statistics .",
    "the goal of compressed sensing is to recover a sparse signal @xmath7 from a small number of non - adaptive linear measurements @xmath8 , where @xmath9 is the `` design '' matrix ( or `` sensing '' matrix ) .",
    "typically , the signal @xmath10 is assumed to be @xmath0-sparse ( i.e. , @xmath0 nonzero entries ) and neither the magnitudes nor locations of the nonzero coordinates are known .",
    "many streaming / database applications can be naturally formulated as compressed sensing problems  @xcite ( even before the name `` compressed sensing '' was proposed ) .",
    "the idea of compressed sensing may be traced back to many prior papers , for example  @xcite .    in the literature of compressed sensing ,",
    "entries of the design matrix @xmath11 are often sampled i.i.d . from a gaussian distribution ( or gaussian - like distribution ,",
    "e.g. , a distribution with a finite second moment ) .",
    "well - known recovery algorithms are often based on linear programming ( lp ) ( e.g. , _ basis pursuit _",
    "@xcite or l1 decoding ) or greedy methods such as orthogonal matching pursuit ( omp )  @xcite . in general ,",
    "l1 decoding is computationally expensive .",
    "omp is often more efficient than l1 decoding but it can still be expensive especially when @xmath0 is large .",
    "the process of collecting measurements , i.e. , @xmath8 , is often called `` random projections '' .",
    "@xcite studied the idea of `` very sparse random projections '' by randomly sparsifying the sensing matrix @xmath11 so that only a very small fraction of the entries can be nonzero . in this paper",
    ", we will continue to investigate on the idea of very sparse random projections in the context of compressed sensing .",
    "our work is related to `` sparse recovery with sparse matrices ''  @xcite , for example , the smp ( _ sparse matching pursuit _ ) algorithm  @xcite .",
    "there is a nice well - known wiki page  @xcite , which summarizes the comparisons of l1 decoding with count - min sketch  @xcite and smp .",
    "their results have shown that , in order to achieve similar recovery accuracies , count - min sketch needs about @xmath12 to 15 times more measurements than l1 decoding and smp needs about half of the measurements of count - min sketch .    in comparison",
    ", our experimental section ( e.g. , figure  [ fig_g1s1 ] ) demonstrates that the proposed method can be as accurate as ( or even more accurate than ) l1 decoding , at the same number of measurements .",
    "the major cost of our method is one linear scan of the coordinates , like count - min sketch .      in this paper ,",
    "our procedure for compressed sensing first collects @xmath13 non - adaptive linear measurements @xmath14,\\hspace{0.5 in } j = 1 , 2 , ... , m\\end{aligned}\\ ] ] here , @xmath15 is the @xmath16-th entry of the design matrix with @xmath17 i.i.d . instead of using a dense design matrix",
    ", we randomly sparsify @xmath18-fraction of the entries of the design matrix to be zero , i.e. , @xmath19 and any @xmath15 and @xmath20 are also independent . +",
    "our proposed decoding scheme utilizes two simple estimators : ( i ) the _ tie estimator _ and ( ii ) the _ absolute minimum estimator_. for convenience , we will theoretically analyze them separately . in practice",
    ", these two estimators should be combined to form a powerful decoding framework .",
    "the tie estimator is developed based on the following interesting observation on the _ ratio statistics _ @xmath21 .",
    "conditional on @xmath22 , we can write @xmath23 where @xmath24 , i.i.d . ,",
    "and @xmath25 note that @xmath26 has certain probability of being zero . if @xmath27 , then @xmath28 .",
    "thus , given @xmath13 measurements , if @xmath29 happens ( at least ) * twice * ( i.e. , a * tie * occurs ) , we can exactly identify the value @xmath30 .",
    "this is the key observation which motivates our proposal of the tie estimator .",
    "+ another key observation is that , if @xmath31 , then we will not see a nonzero tie ( i.e. , the probability of nonzero tie is 0 ) .",
    "this is due to the fact that we use a gaussian design matrix , which excludes unwanted ties .",
    "it is also clear that the gaussian assumption is not needed , as long as @xmath15 follows from a continuous distribution . in this paper",
    "we focus on gaussian design because it makes some detailed analysis easier .",
    "it turns out that , if we just need to detect whether @xmath31 , the task is easier than estimating the value of @xmath30 , for a particular coordinate @xmath32 .",
    "given @xmath13 measurements , if @xmath27 happens ( at least ) * once * , we will be able to determine whether @xmath31 .",
    "note that unlike the tie estimator , this estimator will generate `` false positives '' .",
    "in other words , if we can not be certain that @xmath31 , then it is still possible that @xmath31 indeed .    from the practical perspective , at a particular coordinate @xmath32 , it is preferable to first detect whether @xmath31 because that would require fewer measurements than using the tie estimator . later in the paper , we can see that the performance can be potentially further improved by a more general estimator , i.e. , the so - called _ absolute minimum estimator _ : @xmath33 we will also introduce a threshold @xmath34 and provide a theoretical analysis of the event @xmath35 .",
    "when @xmath36 , it becomes the `` zero - detection '' algorithm .",
    "our analysis will show that by using @xmath37 we can better exploit the prior knowledge we have about the signal and hence improve the accuracy .",
    "we will separately analyze the tie estimator and the absolute minimum estimator , for the convenience of theoretical analysis .",
    "however , we recommend a mixed procedure .",
    "that is , we first run the absolute minimum estimator in one scan of the coordinates , @xmath38 to @xmath1 .",
    "then we run the tie estimator only on those coordinates which are possibly not zero .",
    "recall that the absolute minimum estimator may generate false positives .    as an option",
    ", we can iterate this process for several rounds .",
    "after one iteration ( i.e. , the absolute minimum estimator followed by the tie estimator ) , there might be a set of coordinates for which we can not decide their values .",
    "we can compute the residuals and use them as the measurements for the next iteration .",
    "typically , a few ( e.g. , 3 or 4 ) iterations are sufficient and the major computational cost is computing the absolute minimum estimator in the very first iteration .",
    "the important task is to analyze the false positive probability : @xmath39 for some chosen threshold @xmath40 .",
    "later we will see that @xmath34 is irrelevant if we only care about the worst case .",
    "recall that , conditional on @xmath22 , we can express @xmath41 , where @xmath42 i.i.d . and @xmath26 is defined in ( [ eqn_eta ] ) .",
    "it is known that @xmath43 follows the standard cauchy distribution .",
    "therefore , @xmath44    we are ready to present the lemma about the false positive probability , including a practically useful data - dependent bound , as well as a data - independent bound ( which is convenient for worst - case analysis ) .",
    "[ lem_pr_fp ] * data - dependent bound : * @xmath45^m\\\\\\label{eqn_fp_bound } \\leq&\\left[1- \\gamma \\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left\\{\\frac{{\\epsilon}}{\\sqrt{\\gamma\\sum_{t } x_{t}^2}}\\right\\}\\right\\}\\right]^m\\end{aligned}\\ ] ] * data - independent ( worst case ) bound : * @xmath46^m\\end{aligned}\\ ] ]    * remark : * the data - dependent bound ( [ eqn_fp_exp ] ) and ( [ eqn_fp_bound ] ) can be numerically evaluated if we have information about the data .",
    "the bound will help us understand why empirically the performance of our proposed algorithm is substantially better than the worst - case bound . on the other hand , the worst case bound ( [ eqn_fp_worst ] ) is convenient for theoretical analysis .",
    "in fact , it directly leads to the @xmath47 complexity bound .",
    "+ * proof of lemma  [ lem_pr_fp ] * : for convenience , we define the set @xmath48 .",
    "@xmath49 = e\\prod_{j\\in t_i}\\left[1-\\frac{2}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon}{\\eta_{ij}^{1/2}}\\right)\\right]\\\\\\notag = & e\\left\\{\\left[1-e\\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon}{\\eta_{ij}^{1/2}}\\right ) \\right\\}\\right]^{|t_i|}\\right\\}\\\\\\notag = & \\left[1-\\gamma + \\gamma\\left\\{1-e\\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon}{\\eta_{ij}^{1/2}}\\right)\\right\\}\\right\\}\\right]^m\\\\\\notag = & \\left[1- \\gamma e\\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon}{\\eta_{ij}^{1/2}}\\right)\\right\\}\\right]^m\\end{aligned}\\ ] ]    by noticing that @xmath50 , ( where @xmath51 ) , is a convex function of @xmath52 , we can obtain an upper bound by using jensen s inequality .",
    "@xmath53^m\\\\\\notag \\leq&\\left[1- \\gamma \\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left(\\frac{{\\epsilon}}{\\left(e\\eta_{ij}\\right)^{1/2}}\\right)\\right\\}\\right]^m \\hspace{0.3 in } ( \\text{jensen 's inequality})\\\\\\notag = & \\left[1- \\gamma \\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left\\{\\frac{{\\epsilon}}{\\left(\\gamma\\sum_{t\\neq i } x_{t}^2\\right)^{1/2}}\\right\\}\\right\\}\\right]^m\\\\\\notag = & \\left[1- \\gamma \\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left\\{\\frac{{\\epsilon}}{\\sqrt{\\gamma\\sum_{t } x_{t}^2}}\\right\\}\\right\\}\\right]^m\\end{aligned}\\ ] ]    we can further obtain a worst case bound as follows .",
    "note that @xmath26 has some mass at 0 .",
    "@xmath53^m\\\\\\notag \\leq&\\left[1- \\gamma \\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon}{\\textbf{0}}\\right)\\right\\}\\mathbf{pr}\\left(\\eta_{ij}=0\\right)\\right]^m\\\\\\notag = & \\left[1-\\gamma\\left(1-\\gamma\\right)^k\\right]^m\\end{aligned}\\ ] ] @xmath54      it is also necessary to control the false negative probability : @xmath55 .",
    "[ lem_pr_fn ] @xmath56^m\\\\\\label{eqn_fn_bound } \\leq&1-\\left[1-\\frac{2}{\\pi}\\gamma\\tan^{-1}\\epsilon\\right]^m\\end{aligned}\\ ] ]    * remark : * again , if we know information about the data , we might be able to numerically evaluate the exact false negative probability ( [ eqn_fn_exp ] ) .",
    "the ( loose ) upper bound ( [ eqn_fn_bound ] ) is also insightful because it means this probability @xmath57 if @xmath58 .",
    "note that in lemma  [ lem_pr_fp ] , the worst case bound is actually independent of @xmath34 .",
    "this implies that , if we only care about the worst case performance , we do not have to worry about the false positive probability since we can always choose @xmath58 .",
    "+ * proof of lemma  [ lem_pr_fn ] : * @xmath59\\\\\\notag = & 1-e\\prod_{j\\in t_i}\\left[1-\\frac{1}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon - x_i}{\\eta_{ij}^{1/2}}\\right ) -\\frac{1}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon+x_i}{\\eta_{ij}^{1/2}}\\right)\\right]\\\\\\notag = & 1-e\\left\\{\\left[1-e\\left\\{\\frac{1}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon - x_i}{\\eta_{ij}^{1/2}}\\right ) + \\frac{1}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon+x_i}{\\eta_{ij}^{1/2}}\\right)\\right\\}\\right]^{|t_i|}\\right\\}\\\\\\notag = & 1-\\left[1-\\gamma + \\gamma\\left\\{1-e\\left\\{\\frac{1}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon - x_i}{\\eta_{ij}^{1/2}}\\right)+\\frac{1}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon+x_i}{\\eta_{ij}^{1/2}}\\right)\\right\\}\\right\\}\\right]^m\\\\\\notag = & 1-\\left[1-\\gamma e\\left\\{\\frac{1}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon+x_i}{\\eta_{ij}^{1/2}}\\right)-\\frac{1}{\\pi}\\tan^{-1}\\left(\\frac{x_i-\\epsilon}{\\eta_{ij}^{1/2}}\\right)\\right\\}\\right]^m\\end{aligned}\\ ] ]    note that @xmath60 , for @xmath61 .",
    "therefore , @xmath62^m\\\\\\notag \\leq&1-\\left[1-\\frac{2}{\\pi}\\gamma\\tan^{-1}\\epsilon\\right]^m\\end{aligned}\\ ] ] which approaches zero as @xmath58 .",
    "@xmath54      from the worst - case false positive probability bound : @xmath63^m$ ] , by choosing @xmath64 ( and @xmath58 ) , we can easily obtain the following theorem regarding the sample complexity of only using the absolute minimum estimator .",
    "[ thm_worst_complexity ] using the absolute minimum estimator and @xmath64 , for perfect support recovery ( with probability @xmath65 ) , it suffices to use @xmath66 measurements .    *",
    "remark : * the term @xmath67 approaches @xmath68 very quickly . for example , the difference is only 0.1 when @xmath69 .",
    "although the complexity result in theorem  [ thm_worst_complexity ] can be theoretically exciting , we would like to better understand why empirically we only need substantially fewer measurements . in this section , for convenience",
    ", we consider the special case of `` ternary '' signals , i.e. , @xmath70 .",
    "the exact expectation ( [ eqn_fp_exp ] ) , i.e. , @xmath71^m\\end{aligned}\\ ] ] which , in the case of ternary data , becomes @xmath72 for convenience , we write @xmath73^m   = \\left[1-\\frac{1}{k}h(\\epsilon , k,\\gamma)\\right]^m\\end{aligned}\\ ] ] where @xmath74 which can be easily computed numerically for given @xmath75 , @xmath0 , and @xmath13 . in order for @xmath76 for all @xmath32 , we should have @xmath77    it would be much more convenient if we do not have to worry about all combinations of @xmath75 and @xmath0 .",
    "in fact , we can resort to the well - studied _ poisson approximation _ by considering @xmath78 and defining @xmath79    figure  [ fig_hh ] plots @xmath80 and @xmath81 to confirm that the poisson approximation is very accurate ( as one would expect ) . at @xmath64 ( i.e. , @xmath82 ) , the two terms @xmath80 and @xmath81 are upper bounded by @xmath5 .",
    "however , when @xmath34 is not too small , the constant @xmath5 can be conservative . basically , the choice of @xmath34 reflects the level of prior information about the signal .",
    "if the signals are significantly away from 0 , then we can choose a larger @xmath34 and hence the algorithm would require less measurements .",
    "for example , if we know the signals are ternary , we can perhaps choose @xmath83 or larger .",
    "also , we can notice that @xmath64 is not necessarily the optimum choice for a given @xmath34 .",
    "in general , the performance is not too sensitive to the choice @xmath84 as long as @xmath34 is not too small and the @xmath85 is reasonably large .",
    "this might be good news for practitioners .",
    "we can also analyze the absolute minimum estimator when measurement noise is present , i.e. , @xmath86 + n_j , \\hspace{0.2in}\\text{where } n_j \\sim n(0,\\sigma^2 ) , \\hspace{0.25 in } j = 1 , 2 , ... , m\\end{aligned}\\ ] ] again , we compute the ratio statistic @xmath87 where @xmath24 , i.i.d . , and @xmath88    [ lem_pr_fpn ] * data - dependent bound : * @xmath89^m\\\\ \\leq&\\left[1- \\gamma \\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left\\{\\frac{{\\epsilon}}{\\left(\\sigma^2+\\gamma\\sum_{t } x_{t}^2\\right)^{1/2}}\\right\\}\\right\\}\\right]^m\\end{aligned}\\ ] ] * data - independent bound : * @xmath90^m\\end{aligned}\\ ] ] * data - independent complexity bound : *   with @xmath64 , in order to achieve @xmath76 for all @xmath32 , it suffices to use @xmath91 measurements .",
    "* proof of lemma  [ lem_pr_fpn ] : *  @xmath92^m\\\\\\notag \\leq&\\left[1- \\gamma \\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left(\\frac{{\\epsilon}}{\\left(e\\tilde{\\eta}_{ij}\\right)^{1/2}}\\right)\\right\\}\\right]^m \\hspace{0.3 in } ( \\text{jensen 's inequality})\\\\\\notag = & \\left[1- \\gamma \\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left\\{\\frac{{\\epsilon}}{\\left(\\sigma^2+\\gamma\\sum_{t } x_{t}^2\\right)^{1/2}}\\right\\}\\right\\}\\right]^m\\end{aligned}\\ ] ] which is still expressed in terms of the summary of the signal . to obtain a data - independent bound",
    ", we have @xmath93^m \\leq\\left[1- \\gamma \\left\\{\\frac{2}{\\pi}\\tan^{-1}\\left(\\frac{\\epsilon}{\\sigma}\\right)\\right\\}(1-\\gamma)^k\\right]^m\\end{aligned}\\ ] ] @xmath54",
    "to construct the _ tie estimator _ , we first compute @xmath94 which is anyway needed for the absolute minimum estimator . at each @xmath32 of interest ,",
    "we sort those @xmath13 @xmath95 values and examine the order statistics , @xmath96 , and their consecutive differences , @xmath97 for @xmath98 . then @xmath99    the analysis of the tie estimator is actually not difficult .",
    "recall @xmath100 where @xmath24 , i.i.d .",
    ", and @xmath101 , which has a certain probability of being zero .",
    "if @xmath27 , then @xmath28 . to reliably estimate the magnitude of @xmath30 , we need @xmath102 to happen more than once , i.e. , there should be a tie .",
    "note that @xmath103    for a given nonzero coordinate @xmath32 , we would like to have @xmath27 more than once among @xmath13 measurements . this is a binomial problem , and the error probability is simply @xmath104^m + m\\left(\\gamma \\left(1-\\gamma\\right)^{k-1}\\right)\\left[1-\\gamma \\left(1-\\gamma\\right)^{k-1}\\right]^{m-1}\\end{aligned}\\ ] ]    suppose we use @xmath105 . to ensure this error is smaller than @xmath106 for all @xmath0 nonzero coordinates , it suffices to choose @xmath13 so that @xmath107^m + m\\left(\\gamma \\left(1-\\gamma\\right)^{k-1}\\right)\\left[1-\\gamma \\left(1-\\gamma\\right)^{k-1}\\right]^{m-1}\\right\\ } \\leq \\delta\\end{aligned}\\ ] ]    it is easy to see that this choice of @xmath13 suffices for recovering the entire signal , not just the nonzero entries . this is due to the nice property of the tie estimator , which has no false positives .",
    "that is , if there is a tie , we know for sure that it reveals the true value of the coordinate .",
    "for any zero coordinate , either there is no tie or is the tie zero .",
    "therefore , it suffices to choose @xmath13 to ensure all the nonzero coordinates are recovered .",
    "[ thm_tie_complexity ] using the tie estimator and @xmath108 , for perfect signal recovery ( with probability @xmath65 ) , it suffices to choose the number of measurements to be @xmath109    * proof of theorem  [ thm_tie_complexity ] * : the recovery task is trivial when @xmath110 .",
    "consider @xmath111 and @xmath112 , i.e. , @xmath113 .",
    "we need to choose @xmath13 such that @xmath114 .",
    "let @xmath115 be such that @xmath116 , i.e. , @xmath117 .",
    "suppose we choose @xmath118 . then .",
    "@xmath119 therefore , we need to find the @xmath120 so that @xmath121 since @xmath113 , we have @xmath122 . because @xmath123 is decreasing in @xmath0 , we know that @xmath124 is decreasing in @xmath0 . also , note that @xmath125 = \\left(\\delta / k\\right)^\\alpha / k \\left(1- \\alpha\\log k/\\delta\\right)\\\\\\notag & \\frac{\\partial}{\\partial \\delta } \\left[\\log ( k/\\delta ) \\left(\\delta / k\\right)^\\alpha\\right ] = \\left(\\delta / k\\right)^\\alpha/\\delta \\left(-1 + \\alpha\\log k/\\delta\\right)\\end{aligned}\\ ] ] as we consider @xmath111 and @xmath126 , we know that , as long as @xmath127 , the term @xmath128 is increasing in @xmath106 and decreasing in @xmath0 . combining the calculations , we know that @xmath129 is decreasing in @xmath0 and increasing in @xmath106 , for @xmath130 .",
    "it is thus suffices to consider @xmath131 and @xmath132 .",
    "because @xmath133 is decreasing in @xmath120 , we only need to numerically find the @xmath120 so that @xmath134 , which happens to be @xmath135 .",
    "therefore , it suffices to choose @xmath136 measurements .",
    "it remains to show that @xmath137 .",
    "due to @xmath138 , @xmath139 , we have @xmath140 @xmath54",
    "compressed sensing is an important problem of broad interest , and it is crucial to experimentally verify that the proposed method performs well as predicted by our theoretical analysis . in this study , we closely follow the experimental setting as in the well - known wiki page ( see  @xcite ) , which compared count - min sketch , smp , and l1 decoding , on ternary ( i.e. , @xmath141 ) signals .",
    "in particular , the results for @xmath142 are available for all three algorithms .",
    "their results have shown that , in order to achieve similar recovery accuracies , count - min sketch needs about @xmath12 to 15 times more measurements than l1 decoding and smp only needs about half of the measurements of count - min sketch . + as shown in the success probability contour plot in figure  [ fig_g1s1 ] ( for @xmath64 ) , the accuracy of our proposed method is ( at least ) similar to the accuracy of l1 decoding ( based on  @xcite ) . this should be exciting because , at the same number of measurements , the decoding cost of our proposed algorithm is roughly the same as count - min sketch .",
    "compressed sensing has become a popular and important research topic . using a sparse design matrix",
    "has a significant advantage over dense design .",
    "for example , in sensing networks , we can replace a dense constellation of sensors by a randomly sparsified one , which may result in substantially saving of sensing hardware and labor costs . in this paper",
    ", we show another advantage from the computational perspective of the decoding step .",
    "it turns out that using a very sparse design matrix can lead to a computationally very efficient recovery algorithm without losing accuracies ( compared to l1 decoding ) .",
    "r.  berinde , a.c .",
    "gilbert , p.  indyk , h.  karloff , and m.j .",
    "strauss . combining geometry and combinatorics : a unified approach to sparse signal recovery . in _ communication , control , and computing , 2008 46th annual allerton conference on _ , pages 798805 , sept 2008 .                              y.c .",
    "pati , r.  rezaiifar , and p.  s. krishnaprasad .",
    "orthogonal matching pursuit : recursive function approximation with applications to wavelet decomposition . in _ signals , systems and computers , 1993 .",
    "1993 conference record of the twenty - seventh asilomar conference on _ , pages 4044 vol.1 , nov 1993 ."
  ],
  "abstract_text": [
    "<S> we study the use of _ very sparse random projections _ for compressed sensing ( sparse signal recovery ) when the signal entries can be either positive or negative . in our setting , the entries of a gaussian design matrix are randomly sparsified so that only a very small fraction of the entries are nonzero . </S>",
    "<S> our proposed decoding algorithm is simple and efficient in that the major cost is one linear scan of the coordinates . </S>",
    "<S> we have developed two estimators : ( i ) the _ tie estimator _ , and ( ii ) the _ absolute minimum estimator_. using only the tie estimator , we are able to recover a @xmath0-sparse signal of length @xmath1 using @xmath2 measurements ( where @xmath3 is the confidence ) . using only the absolute minimum estimator , we can detect the support of the signal using @xmath4 measurements . for a particular coordinate </S>",
    "<S> , the absolute minimum estimator requires fewer measurements ( i.e. , with a constant @xmath5 instead of @xmath6 ) . </S>",
    "<S> thus , the two estimators can be combined to form an even more practical decoding framework . </S>",
    "<S> + prior studies have shown that existing one - scan ( or roughly one - scan ) recovery algorithms using sparse matrices would require substantially more ( e.g. , one order of magnitude ) measurements than l1 decoding by linear programming , when the nonzero entries of signals can be either negative or positive . in this paper , following a known experimental setup  @xcite , we show that , at the same number of measurements , the recovery accuracies of our proposed method are ( at least ) similar to the standard l1 decoding . </S>"
  ]
}