{
  "article_text": [
    "contrastive divergence ( cd ) algorithm @xcite has been widely used for parameter inference of markov random fields .",
    "this first example of application is given by hinton @xcite to train restricted boltzmann machines , the essential building blocks for deep belief networks @xcite .",
    "the key idea behind cd is to approximate the computationally intractable term in the likelihood gradient by running a small number ( @xmath6 ) of steps of a markov chain monte carlo ( mcmc ) run .",
    "thus it is much faster than the conventional mcmc methods that run a large number to reach equilibrium distributions . despite of cd s empirical success , theoretical understanding of its behavior is far less satisfactory .",
    "both computer simulation and theoretical analysis show that cd may fail to converge to the correct solution @xcite . studies on theoretical convergence properties have thus been motivated .",
    "yuille relates the algorithm to the stochastic approximation literature and gives very restrictive convergence conditions @xcite .",
    "others show that for restricted boltzmann machines the cd update is not the gradient of any function @xcite , but that for full - visible boltzmann machines the cd update can be viewed as the gradient of pseudo - likelihood function if adopting a simple scheme of gibbs sampling @xcite . in any case , the fundamental question of why cd with finite @xmath6 can work asymptotically in the limit of @xmath7 has not been answered .",
    "this paper studies the convergence properties of cd algorithm in exponential families and gives the convergence conditions involving the number of steps of markov kernel transitions @xmath6 , spectral gap of markov kernels , concavity of the log - likelihood function and learning rate @xmath8 of cd updates ( assumed fixed in our analyses ) .",
    "this enables us to establish the convergence of cd with a fixed @xmath6 to the true parameter as the sample size @xmath9 increases .",
    "section 2 describes the cd algorithm for exponential family with parameter @xmath10 and data @xmath11 .",
    "section 3 states our main result : denoting @xmath1 the parameter sequence generated by cd algorithm from an i.i.d .",
    "data sample @xmath0 , a sufficiently large @xmath6 can guarantee @xmath12 under mild conditions .",
    "section 4 shows that @xmath1 is a markov chain under @xmath13 , the conditional probability measure given any realization of data sample @xmath14 , and impose three constraints on @xmath15 , which hold asymptotically with probability @xmath16 .",
    "thereafter sections 5 - 8 studies @xmath1 under @xmath13 in the framework of markov chain theory , and show that the chain is _ positive harris recurrent _ and thus processes a unique invariant distribution @xmath2 .",
    "the invariant distribution @xmath2 concentrates around the mle @xmath17 at a speed arbitrarily slower than @xmath3 , and @xmath6 only affects the coefficient factor of the concentration rate .",
    "section 9 completes the proof of the main result .    for convenience of the reader ,",
    "we assume throughout sections 3 - 9 that the exponential family under study is a set of continuous probability distributions and show in section 10 how to get a similar conclusion for the case of discrete probability distribution .",
    "we also provide two numerical experiments to illustrate the theories in section 11 .",
    "consider an exponential family over @xmath18 with parameter @xmath19 @xmath20 where @xmath21 is the carrier measure , @xmath22 is the sufficient statistic and @xmath23 is the cumulant generating function @xmath24    we assume @xmath25 is bounded , then the natural parameter domain @xmath26 ( if it is not empty ) .",
    "@xmath23 is convex and differentiable at any interior point of the natural parameter domain , and both the gradient and hessian of cumulant generating function @xmath23 exist @xmath27\\\\ \\sigma(\\theta ) & \\triangleq \\nabla^2 \\lambda(\\theta ) = \\mathbb{c}\\text{ov}_\\theta [ \\phi(x)]\\end{aligned}\\ ] ]    given an i.i.d .",
    "sample @xmath28 generated from a certain underlying distribution @xmath29 , the log - likelihood function is @xmath30 and the gradient @xmath31 assuming the positive definiteness of @xmath32 , the maximum likelihood estimate ( mle ) @xmath17 uniquely exists and satisfies @xmath33 or equivalently @xmath34 maximum likelihood learning can be done by gradient ascent @xmath35\\ ] ] where learning rate @xmath36 .    when computing the gradient @xmath37 , the first term @xmath38 is easy to compute .",
    "but it is usually difficult to compute the second term @xmath39 , which involves a complicated integral over @xmath40 .",
    "markov chain monte carlo ( mcmc ) methods may generate a random sample from a markov chain with the equilibrium distribution @xmath41 and approximate @xmath39 by the sample average . however , markov chains take a large number of steps to reach the equilibrium distributions .    to address this problem ,",
    "hinton proposed the contrastive divergence ( cd ) method @xcite .",
    "the idea of cd is to replace @xmath39 and @xmath37 with @xmath42 respectively , where @xmath43 is obtained by a small number ( @xmath6 ) of steps of an mcmc run starting from the observed sample @xmath44 .",
    "formally , denote by @xmath45 the markov transition kernel with @xmath41 as equilibrium distribution .",
    "cd first run markov chains for @xmath6 steps @xmath46 and makes update @xmath47.\\ ] ]    denote by @xmath48 the markov operator associated with @xmath45 , i.e. @xmath49 and by @xmath50 the second largest absolute eigenvalue of @xmath48 .",
    "markov kernel @xmath48 is said to have @xmath51-spectral gap @xmath52 if @xmath53 . the convergence rate of mcmc depends on @xmath51-spectral gap @xcite .    throughout the paper , @xmath54 denotes the @xmath6-step transition kernel of @xmath55 , @xmath56 denotes the distribution of markov chain after @xmath6-step transition starting from initial distribution @xmath57 , and @xmath58 denotes the @xmath6-step markov operator of @xmath48 .",
    "we also let @xmath59 denote the @xmath60-norm @xmath61 .",
    "we base the convergence properties of cd algorithms for for exponential family of continuous distributions on the assumptions [ assumption : x ] , [ assumption : theta ] , [ assumption : concavity ] , [ assumption : operator continuity ] , [ assumption : spectral gap ] , [ assumption : kernel positiveness ] .",
    "theorem [ theorem : main ] states our main result , whose proof is presented in sections 4 - 9 .",
    "we later show in section 10 a similar conclusion for the case of discrete distribution .    1 .",
    "[ assumption : x ] @xmath62 is bounded , i.e. there exists some constant @xmath63 such that @xmath64^d$ ] for any @xmath65 .",
    "[ assumption : theta ] @xmath66 is convex and compact , and the true parameter @xmath67 is an interior point of @xmath68 .",
    "[ assumption : concavity ] for any @xmath69 , @xmath70 are linearly independent under @xmath71 , and thus @xmath32 is positive definite .",
    "this assumption together with continuity of @xmath32 and [ assumption : theta ] immediately implies the existence of the bounds for smallest and largest eigenvalues of @xmath32 @xmath72 4 .",
    "[ assumption : operator continuity ] define a metric @xmath73 on the set of markov operators @xmath74 as @xmath75 and assume the lipchitz continuity of @xmath48 in sense that @xmath76 5 .",
    "[ assumption : spectral gap ] markov operators @xmath48 have @xmath51-spectral gap @xmath52 and @xmath77 6 .",
    "[ assumption : kernel positiveness ] @xmath25 is a convex set . using mcmc transition kernel @xmath45 , @xmath78 has a conditional probability density function @xmath79 conditioning on @xmath10 and @xmath80 .",
    "for any @xmath65 , @xmath81 .",
    "[ theorem : main ] assume [ assumption : x ] , [ assumption : theta ] , [ assumption : concavity ] , [ assumption : operator continuity ] , [ assumption : spectral gap ] , [ assumption : kernel positiveness ] , and the data sample @xmath0 i.i.d .. there exists some constant @xmath82 . for any @xmath6 and learning rate @xmath8 satisfying @xmath83 cd algorithm",
    "generates a sequence @xmath1 such that for any @xmath84 @xmath85",
    "it is not hard to see that cd generates a markov chain @xmath86 in the parameter space @xmath68 given any realization of the data sample @xmath87 . indeed , denote by @xmath88 the @xmath6-step mcmc random sample to estimate the cd gradient from @xmath89 to @xmath90 .",
    "the filtration @xmath91 contains all historical information until @xmath92 step of cd . the cd update @xmath93\\ ] ]",
    "is merely function of data @xmath87 , current parameter @xmath89 and @xmath6-step mcmc sample @xmath88 .",
    "conditioning on data sample @xmath94 and current parameter @xmath95 , @xmath96 is independent to the history of cd updates .",
    "thus @xmath1 is indeed a homogeneous @xmath97-adapted markov chain under @xmath98 , the conditional probability measure given @xmath99 .",
    "thereafter the remaining of the paper studies cd path @xmath86 in the framework of markov chain theory . from now on @xmath100 denotes the conditional probability measure given data sample @xmath94 and parameter @xmath10 . and",
    "@xmath101 and @xmath102 denote the expectation and covariance under @xmath100 .",
    "next we impose three constraints ( [ eqn : sample mean ] ) , ( [ eqn : mle ] ) , ( [ eqn : empirical process ] ) on data sample @xmath15 , which are shown in lemma [ lemma : finite sample constraints ] to hold asymptotically with probability @xmath16 as @xmath4 .",
    "we later show that the markov chain @xmath1 converges to an invariant distribution under @xmath98 if the data sample @xmath87 satisfies these constraints . @xmath103 where @xmath67 is the true parameter and @xmath104 is any number between @xmath105 and @xmath106 .",
    "[ lemma : finite sample constraints ] assume [ assumption : x ] , [ assumption : theta ] , [ assumption : concavity ] , [ assumption : operator continuity ] , and @xmath107 .",
    "@xmath108 for any @xmath109 .",
    "the result that ( [ eqn : sample mean ] ) and ( [ eqn : mle ] ) hold asymptotically with probability @xmath16 follows from standard theorems in large sample theory @xcite .",
    "therefore it suffices to show ( [ eqn : empirical process ] ) holds asymptotically with probability @xmath16 . to this end ,",
    "we define @xmath110 and bound the tail of empirical process @xmath111\\\\ & = n^{-1/2}\\sum_{i=1}^n \\left [ \\intx \\phi(y)k_\\theta^m(x_i , y)dy - \\intx \\phi(y)k_\\theta^m p_{{\\theta^*}}(y ) dy\\right]\\end{aligned}\\ ] ] by theorem 2.14.9 in @xcite , which relates the tail of empirical process to _ covering number _ of function class . details of proof are provided in appendix .",
    "our study on the markov chain @xmath1 starts from appropriately bounding bias and variance of cd gradient @xmath112 under @xmath13 .",
    "lemma [ lemma : gradient approx error ] shows that the bias of @xmath112 is @xmath113 depending on the mixing rate of chains in mcmc @xmath114 , the mcmc step number @xmath6 , sample size @xmath9 and the distance between @xmath10 and the mle @xmath17 , and that the covariance of @xmath112 is @xmath115 depending on the sample size @xmath9 .",
    "write the gradient approximation error @xmath116    [ lemma : gradient approx error ] assume [ assumption : x ] , [ assumption : theta ] and [ assumption : spectral gap ] and data sample @xmath94 satisfies ( [ eqn : mle ] ) and ( [ eqn : empirical process ] )",
    ". then @xmath117 for some constant @xmath82 , where @xmath118 is the @xmath51-spectral gap of markov operators @xmath48 in mcmc and @xmath104 is introduced by inequalities ( [ eqn : mle ] ) and ( [ eqn : empirical process ] ) . and @xmath119 \\le \\frac{dc^2}{n}.\\ ] ]    for simplicity of notations , we abbreviate @xmath120 as @xmath121 .",
    "@xmath122 implying @xmath123 the second term is bounded by @xmath124 in inequality ( [ eqn : empirical process ] ) .",
    "for the first term , consider each @xmath125 , @xmath126 where @xmath127 is the lipchitz constant of the continuously differentiable function @xmath128 , and the last step follows from @xmath129 putting ( [ eqn : mle ] ) , ( [ eqn : empirical process ] ) and ( [ eqn : bias ] ) together yields @xmath130 also , noting that @xmath131 are conditional independent ( but not identically distributed ) since we run @xmath9 markov chains independently starting from different @xmath132 , write @xmath133\\\\ & =   \\frac{1}{n^2 } \\sum_{i=1}^n \\sum_{j=1}^d   \\int_\\mathcal{x } \\left(\\phi_j(y ) - \\int_\\mathcal{x } \\phi_j(z ) k_\\theta^m(x_i , z ) dz\\right)^2 k_\\theta^m(x_i , y)dy\\\\ & \\le \\frac{dc^2}{n}.\\end{aligned}\\ ] ]",
    "when studying the convergence of cd method , we hypothesize that starting at some @xmath134 far away from @xmath17 , the exact gradient @xmath135 is large enough to dominate the approximation error of @xmath6-step mcmc sampling , and bring a positive _ drift _ in log - likelihood , until @xmath95 is close to @xmath17 and @xmath135 fails to suppress the mcmc error . to precisely characterize this phenomenon",
    ", we give the definitions of _ drift _ and establish the _ drift condition _ with _ lyapunov function _",
    "@xmath136 being the log - likelihood gap at @xmath10 compared to the mle @xmath17 .",
    "[ definition : drift ] ( drift ) let @xmath137 be some function on the state space of a markov chain @xmath138 .",
    "the one - step drift of @xmath139 is defined as @xmath140    [ definition : drift condition ] ( drift condition ) @xmath139 satisfies the drift condition if @xmath141 with some @xmath142 and some subset of the state space @xmath143 .",
    "@xmath139 is called a lyapunov function for the markov chain @xmath138 .",
    "lemma [ lemma : drift condition ] shows that function @xmath144 satisfies _ drift condition _ outside of closed balls @xmath145 with @xmath146 and @xmath147 .",
    "[ lemma : drift condition ] assume [ assumption : x ] , [ assumption : theta ] , [ assumption : concavity ] , [ assumption : spectral gap ] , and data sample @xmath94 satisfies ( [ eqn : mle ] ) and ( [ eqn : empirical process ] ) . for any @xmath6 and learning rate @xmath8 satisfying @xmath148 the chain @xmath1 has lyapunov function @xmath149 which satisfies drift condition outside closed ball @xmath150 for any @xmath146 with @xmath151 where @xmath152 n^{-1 + 2\\gamma_1}\\end{aligned}\\ ] ]    for simplicity of notations , we abbreviate @xmath37 as @xmath153 , @xmath112 as @xmath154 , and @xmath120 as @xmath121 .",
    "the difference @xmath155 when moving from @xmath10 to @xmath156 is bounded from above as @xmath157 where @xmath158 is a convex combination of @xmath10 and @xmath159 .",
    "the first term @xmath160 is constant .",
    "expectation of the second term is @xmath161 and expectation of the third term is @xmath162 + \\left\\vert \\earg{g_\\text{cd } } \\right\\vert^2 \\nonumber\\\\ & \\le \\text{trace}\\left[\\mathbb{c}\\text{ov}^\\mathbf{x}_\\theta \\delta g \\right ] + \\left(\\vert g \\vert + \\vert \\earg{\\delta g}\\vert\\right)^2.\\end{aligned}\\ ] ] since @xmath163 , @xmath164 putting ( [ eqn : second term in drift ] ) , ( [ eqn : third term in drift ] ) and ( [ eqn : gradient bounds ] ) with lemma [ lemma : gradient approx error ] together yields @xmath165 \\le -\\eta ( a \\vert \\theta -\\hat{\\theta}_n\\vert^2 - b_n \\vert \\theta -\\hat{\\theta}_n\\vert - c_n).\\ ] ] the rhs of ( [ eqn : quadratic drift ] ) has a quadratic form of @xmath166 , and it is clear that @xmath167 for sufficiently large @xmath6 and sufficiently small @xmath8 . then @xmath168 \\le 0\\ ] ]",
    "can guarantee @xmath169 \\le 0 $ ] .",
    "in particular , the drift condition holds outside any closed ball @xmath170 centering at mle of radius @xmath171 with @xmath146 , i.e. @xmath172 \\le -\\delta < 0\\ ] ] with @xmath173\\\\ & \\ge \\eta ( \\beta^2 - 1 ) ( a   r_n^2 -   b r_n)\\\\ & = \\eta ( \\beta^2 - 1)c_n\\end{aligned}\\ ] ]    * remark . * a function @xmath174",
    "is called _ supharmonic _ for a transition probability @xmath175 at @xmath80 if @xmath176 . and it is called _ strong supharmonic _ if @xmath177 for some positive @xmath178 .",
    "we actually prove in lemma [ lemma : drift condition ] that @xmath179 is strong supharmonic at @xmath180 .",
    "we later see in theorem [ theorem : tweedie ] a nice connection between strong supharmonic functions , positive recurrence of markov chains , and supmartingales .",
    "tweedie @xcite connected the _ drift condition _ in definition [ definition : drift condition ] to positive recurrence of sets in the state space by a markov chain .",
    "we restate this result in theorem [ theorem : tweedie ] and provide a proof based on sup - martingales and sup - harmonic functions in appendix .",
    "next , corollary [ corollary : positive recurrent balls ] combines lemma [ lemma : drift condition ] with theorem [ theorem : tweedie ] and concludes that the closed balls @xmath170 centering at the mle @xmath17 of radius @xmath171 are positive recurrent by the chain @xmath1 .",
    "[ theorem : tweedie ] ( theorem 6.1 in @xcite ) suppose a markov chain @xmath138 has a non - negative function @xmath139 on the state space satisfying the drift condition in definition [ definition : drift condition ] with some @xmath142 and set @xmath143 .",
    "let @xmath181 be the first hitting time of @xmath143 if starting from @xmath182 or the first returning time otherwise , then @xmath183 where @xmath184 is the transition probability of the chain .",
    "thus , if @xmath185 also holds , then the set @xmath143 is positive recurrent .",
    "_ lyapunov function _ is widely used in stochastic stability or optimal control study @xcite . as we have seen in theorem [ theorem : tweedie ] , a suitably designed _ lyapunov function _ can determine the positive recurrence of sets of a markov chain . we proceed to apply theorem [ theorem :",
    "tweedie ] to the markov chain @xmath1 , for which @xmath179 satisfies the _ drift condition _ outside of any closed ball @xmath170 in lemma [ lemma : drift condition ] , and conclude in corollary [ corollary : positive recurrent balls ] that closed balls @xmath170 centering at mle are positive recurrent by the chain @xmath1 .    [ corollary : positive recurrent balls ] following theorem [ theorem : tweedie ] and lemma [ lemma : drift condition ] , @xmath170 for each @xmath186 are positive recurrent by the chain @xmath1 .",
    "let @xmath187 be the first hitting or returning time of @xmath170 by the chain @xmath1 .",
    "lemma [ lemma : drift condition ] establishes the drift condition for the likelihood gap function @xmath179 outside of @xmath170 , i.e. @xmath188 the compactness of @xmath68 and continuity of @xmath179 follow the boundedness of @xmath179 , implying @xmath189 both conditions of theorem [ theorem : tweedie ] are satisfied , thus @xmath170 is positive recurrent .",
    "next we prove the positive harris recurrence of the chain @xmath1 , which further implies the distribution convergence of markov chain in total variation .",
    "[ definition : small set ] an accessible set @xmath143 is called a small set of a markov chain @xmath138 if @xmath190 for some positive @xmath84 and probability measure @xmath191 over the state space .    [ definition : positive harris recurrence ] a markov chain @xmath138 is called harris recurrent if there exists a set @xmath143 s.t .    1 .",
    "@xmath143 is recurrent .",
    "@xmath143 is a small set .",
    "if @xmath143 is positive recurrent in addition , then the chain is called positive harris recurrent .",
    "[ lemma : positive harris recurrence ] assume [ assumption : x ] , [ assumption : theta ] , [ assumption : concavity ] , [ assumption : operator continuity ] , [ assumption : spectral gap ] , [ assumption : kernel positiveness ] . for sufficiently large @xmath9 and for any @xmath6 and learning rate @xmath8 satisfying @xmath148 data sample @xmath94 satisfying ( [ eqn : sample mean ] ) , ( [ eqn : mle ] ) and ( [ eqn : empirical process ] ) , the chain @xmath192 generated by cd updates is positive harris recurrent .    since corollary [ corollary : positive recurrent balls ] ensures the positive recurrence of @xmath170 , it suffices to show @xmath170 is a small set by checking definition [ definition : small set ] . since @xmath67 is an interior point of @xmath68 and @xmath193 a continuous mapping",
    ", @xmath194 is an interior point of @xmath25 . denoting by @xmath195 the boundary of @xmath25 , @xmath196",
    "if ( [ eqn : sample mean ] ) @xmath197 holds , @xmath198 for sufficiently large @xmath9 .",
    "then for any @xmath199 @xmath200.\\ ] ] assumption [ assumption : kernel positiveness ] implies @xmath201 has positive density over @xmath25 , which is strictly bounded away from @xmath105 for any @xmath10 , so does @xmath202\\ ] ] over @xmath203 $ ] .",
    "denote by @xmath204 the transition kernel of the chain @xmath192 , then @xmath205\\right\\ } > 0.\\ ] ] let @xmath191 be the uniform measure on @xmath170 .",
    "there exists some constant @xmath84 such that @xmath206 for any borel set @xmath207 , completing the proof .",
    "as stated in theorems 6.8.5 , 6.8.7 , 6.8.8 in @xcite , any aperiodic , positive harris recurrent chain @xmath138 processes an unique invariant distribution @xmath208 , and the chain @xmath138 converges to the invariant distribution @xmath208 in total variation for @xmath208-a.e . starting point @xmath209 .",
    "we strengthen these results for chain @xmath1 in corollary [ corollary : chain convergence ] .",
    "[ corollary : chain convergence ] following lemma [ lemma : positive harris recurrence ] ,    1 .   the positive harris recurrent chain @xmath1 has an unique invariant probability measure @xmath2 2 .",
    "let @xmath210 be the set of @xmath69 s.t .",
    "@xmath211 3 .",
    "@xmath2 and the lesbegue measure are absolutely continuous w.r.t .",
    "each other .",
    "4 .   @xmath2 has a positive density function over @xmath68 .",
    "( [ eqn : total variation ] ) holds for almost all @xmath69 ( in sense of lesbegue measure ) . 6 .   for any @xmath212 such that @xmath213 , @xmath214    clearly the chain @xmath1 is aperiodic",
    ". then parts ( a)(b ) are immediate consequences of lemma [ lemma : positive harris recurrence ] and theorems 6.8.5 , 6.8.7 , 6.8.8 in @xcite .",
    "proceed to prove part ( c ) . on the first hand , part ( b ) and absolutely continuity of @xmath215 w.r.t .",
    "the lesbegue measure imply absolutely continuity of @xmath2 w.r.t . the lesbegue measure . on the other hand ,",
    "the invariant measure @xmath2 is a maximal irreducibility measure ( see definition [ definition : maximal irreducibility measure ] ) by theorems 10.0.1 and 10.1.2 in @xcite .",
    "hence the lesbegue measure is absolutely continuous w.r.t .",
    "@xmath2 , completing the proof of part ( c ) .",
    "further , parts ( b)(c ) imply ( d)(e ) .",
    "part ( f ) is another consequence of part ( a ) ( see details in section 17.1 in @xcite ) .",
    "[ definition : maximal irreducibility measure ] let @xmath208 be a positive measure on the state space of chain @xmath216 . if for any @xmath209 and set @xmath217 in the state space @xmath218 implies the accessibility of set @xmath217 by the chain from any start point @xmath209 , we say @xmath208 is an irreducible measure for the chain ( or the chain is @xmath208-irreducible ) .",
    "an irreducible measure @xmath219 specifying the minimal family of null sets , i.e. @xmath220 for any irreducible measure , is called a maximal irreducibility measure .",
    "lemma [ lemma : invariant distribution concentration ] shows that the invariant distribution @xmath2 concentrates on positive recurrent ball @xmath170 .",
    "[ lemma : invariant distribution concentration ] following corollary [ corollary : chain convergence ] , the invariant probability measure @xmath208 concentrates on @xmath170 as @xmath221 for any @xmath222 and @xmath223 increasing with @xmath9 , while the ball @xmath170 shrinks with radius @xmath224    by ( [ eqn : quadratic drift ] ) in lemma [ lemma : drift condition ] , @xmath225 rearranging terms yields @xmath226 at @xmath227 , @xmath228 at @xmath199 , @xmath229 thus @xmath230 noting that @xmath231 , letting @xmath223 increase with @xmath9 yields @xmath232 while the ball @xmath170 has shrinking radius @xmath233",
    "now we can complete the proof of the main result in theorem [ theorem : main ] : the estimator @xmath234 converges to the true parameter @xmath67 in probability as the sample size @xmath4 in sense that for any @xmath235 @xmath85    with lemma [ lemma : finite sample constraints ] , it suffices to show @xmath236 for any realization of data sample @xmath94 satisfying ( [ eqn : sample mean ] ) , ( [ eqn : mle ] ) , ( [ eqn : empirical process ] ) .",
    "write @xmath237 the first term , by part ( f ) of corollary [ corollary : chain convergence ] , converges to 0 , i.e. @xmath238 the second term , by jensen s inequality and lemma [ lemma : invariant distribution concentration ] , vanishes as @xmath9 increases .",
    "@xmath239 the third term @xmath240 as in ( [ eqn : mle ] ) .",
    "therefore as @xmath4 @xmath241",
    "if the sufficient statistic @xmath242 in the exponential family is discrete , we have a similar conclusion as stated in theorem [ theorem : discrete case ] .",
    "in contrast to positive harris recurrence of @xmath1 in theorem [ theorem : main ] for the continuous case , @xmath1 for the discrete case is a markov chain with countable state space , and may admit multiple invariant distributions .    [",
    "theorem : discrete case ] consider an exponential family of discrete probability distributions .",
    "assume [ assumption : x ] , [ assumption : theta ] , [ assumption : concavity ] , [ assumption : operator continuity ] , [ assumption : spectral gap ] and    1 .",
    "@xmath25 is finite , and for each @xmath125 , elements in @xmath243 have rational ratios . [",
    "assumption : rational ratio ]    the conclusion in theorem [ theorem : main ] is also true .",
    "if the sufficient statistic @xmath242 in the exponential family is discrete and has finite possible values , the cd gradient @xmath244 has finite possible values @xmath245 . starting from any initial parameter",
    "guess @xmath246 , the chain @xmath247 g_k\\ ] ] lies in a countable state space @xmath248 . and",
    "@xmath249 is a finite set due to [ assumption : rational ratio ] . by decomposition theorem ,",
    "@xmath250 can be partitioned uniquely as @xmath251 where @xmath252 is the set of transient states and the @xmath253 are disjoint , irreducible closed set of recurrent states .",
    "the chain either remains forever in @xmath252 , or it lies eventually in some @xmath253 .",
    "we argue by contradiction that the first of these possibility does not occur .",
    "suppose for the sake of contradiction that the chain @xmath192 forever lies in @xmath252 . by corollary [ corollary : positive recurrent balls ] , @xmath170 is positively recurrent .",
    "thus the chain visits @xmath254 infinitely many times , and thus visits at least one state in @xmath254 infinitely many times for reason that @xmath255 is finite .",
    "such a state is recurrent , contradicting the fact that it belonging to the set of transient states @xmath252 .",
    "therefore the chain will eventually lies in the first irreducible set of recurrent states it enters , and converges to the corresponding invariant distribution @xmath2 .",
    "every invariant distribution @xmath2 concentrates in @xmath170 with @xmath232 the same convergence rate with that in theorem [ theorem : main ] can be obtained .",
    "theorem [ theorem : discrete case ] establishes the convergence property of cd algorithm for relative simple exponential family of discrete distributions , which satisfy the assumption [ assumption : rational ratio ] .",
    "it suffices to guaranteed the success of cd algorithm for restricted boltzmann machines ( rbm ) which has @xmath256 .    also , it is noteworthy that cd algorithm converges to the mle even for more complicated cases in which @xmath25 is infinite and/or elements in @xmath25 have irrational ratios , if one takes the finite precision of computation into account . due to the finite precision of any computer ,",
    "numbers are always rounded or truncated . in real world , the update rule of cd algorithm is @xmath257 + \\varepsilon\\ ] ] where @xmath258 is the numerical error incurred such that @xmath259 is substituted by its nearby grid point @xmath260 . as @xmath261 + \\left[u(\\theta_1 ) - u(\\theta)\\right ] \\le \\mathcal{o}(\\vert \\varepsilon \\vert ) + \\left[u(\\theta_1 ) - u(\\theta)\\right],\\ ] ] we have the following drift condition akin to ( [ eqn : quadratic drift ] ) @xmath262 \\le -\\eta ( a \\vert \\theta -\\hat{\\theta}_n\\vert^2 - b_n \\vert \\theta -\\hat{\\theta}_n\\vert - ( c_n + \\mathcal{o}(\\vert \\varepsilon \\vert))).\\ ] ] if the precision of computation copes well with the sample size @xmath9 , i.e. @xmath263 , the chain @xmath264 is positive recurrent to the ball @xmath170 centering at the mle @xmath17 . noting the ball @xmath170 contains finitely many grid points , a similar argument to theorem [ theorem : discrete case ] can prove that the chain @xmath264 admits invariant distributions concentrating around the mle",
    "we conduct numerical experiments on the bivariate normal model @xmath265 with unknown mean @xmath266 ( parameter @xmath10 ) and known covariance matrix @xmath267 .",
    "figure  [ fig::gn50]-[fig::gn500 ] shows cd path @xmath268 given a sample @xmath15 of size @xmath269 with true parameter @xmath270 , respectively . for each data sample , cd-3 ( i.e. cd with @xmath6=3 )",
    "is applied to learn the parameter @xmath266 .    in each of figure",
    "[ fig::gn50]-[fig::gn500 ] , subplot ( a ) shows the cd paths @xmath268 in the parameter space with different start points @xmath271 .",
    "they illustrate that the estimated parameter initially directly moves towards to true parameter but eventually randomly walks around the true parameter .",
    "furthermore , comparison of figures  [ fig::gn50](a ) , [ fig::gn100](a ) , [ fig::gn500](a ) shows that the region of the random walk decreases in size as the sample size @xmath9 increases .",
    "subplot ( b ) shows the true gradient of the likelihood function in each case .",
    "subplot ( c ) presents the approximate gradient used by cd . for each grid point in the subplot ( c ) , we run cd-3 5 times and draw 5 estimated gradients to reveal the randomness in cd approach .",
    "subplot ( d ) reveals the directions of these gradients by normalizing the magnitude of these gradients . according to the plots ( b ) and ( c )",
    ", it can be observed that the magnitude and direction of the gradient become smaller and more stochastic when the point moves closer to the true parameter . comparing the three figures we can see that the range of randomness decreases as the sample size increases .",
    "this is exactly what our theory predicts .    0.2 in    [ cols=\"^,^ \" , ]     -0.2 in [ fig::gn500 ]      in our next experiment ,",
    "we simulate data from the restricted boltzmann machines ( rbm ) .",
    "the cd method is a standard way for inferring rbm during the training of deep belief neural network @xcite .",
    "there are @xmath6 visible units @xmath272 to represent observable data and @xmath9 hidden units @xmath273 to capture dependencies between observed variables . in the simulation ,",
    "we focus on the binary rbm which the random variables @xmath274 take values @xmath275 and the joint distribution of @xmath274 is given by @xmath276 with the energy function    @xmath277    in the simulation , the data sets is generated from a rbm with the weight matrix @xmath278 and @xmath279 for @xmath280.figure  [ fig::rbm100]-[fig::rbm1000000 ] shows the approximate gradients for @xmath281 and @xmath282 .",
    "in each figure , subplots in the lower triangular part show the approximate gradients .",
    "let @xmath283 .",
    "subplot @xmath284 in the lower triangular part gives the projections of the gradient onto the plane @xmath285 , at those points @xmath286 satisfying @xmath287 equal to 0.5 approximately , for @xmath288 not equal to @xmath289 or @xmath290 .",
    "the corresponding directions of these gradients are given in the upper triangular part of each figure .",
    "again , the behaviour of the cd approximate gradients is in agreement with our theory .    0.2 in    .,width=604 ]",
    "-0.2 in    0.2 in    .,width=604 ]    -0.2 in    0.2 in    .,width=604 ]    -0.2 in",
    "in lemma [ lemma : finite sample constraints ] , the result that ( [ eqn : sample mean ] ) and ( [ eqn : mle ] ) hold asymptotically with probability @xmath16 follows from standard theorems in large sample theory @xcite .",
    "therefore it suffices to show inequality ( [ eqn : empirical process ] ) holds asymptotically .",
    "therefore it suffices to show inequality ( [ eqn : empirical process ] ) holds with asymptotically probability @xmath16 . to this end , we applied limit theorem for empirical processes @xcite , which relates the tail of empirical process to _ covering number _ of function class .",
    "( covering number ) let @xmath291 be an arbitrary semi - metric space . then the covering number @xmath292 is the minimal number of balls of radius @xmath84 needed to cover @xmath293 .",
    "formally , @xmath294    [ theorem : vaart ] ( theorem 2.14.9 in @xcite ) let @xmath295 i.i.d . and",
    "@xmath293 be a class of functions @xmath296 $ ] .",
    "if @xmath297 where @xmath298 are constants , @xmath299 is a probability measure on @xmath40 , and @xmath300 then for every @xmath301 @xmath302 \\right| > t \\right ) \\le \\left(\\frac{c_2 t}{\\sqrt{s}}\\right)^s e^{-2t^2},\\ ] ] where constant @xmath303 only depends on @xmath304 .",
    "we proceed to bound the tail @xmath305 by using theorem [ theorem : vaart ] .",
    "assume [ assumption : x ] , [ assumption : theta ] and [ assumption : operator continuity ] and @xmath107 .",
    "then @xmath306 as @xmath4 , for any @xmath109 .",
    "let @xmath307 , then @xmath308 for @xmath125 , let @xmath309\\\\ & = n^{-1/2}\\sum_{i=1}^n \\left [ \\int \\phi_j(y)k_\\theta^m(x_i , y)dy - \\int \\phi_j(y)k_\\theta^m p_{{\\theta^*}}(y ) dy\\right].\\end{aligned}\\ ] ] and view @xmath310 as a stochastic process indexed by @xmath69 . @xmath311\\\\ & = \\sum_{i=0}^{m-1 } \\int_\\mathcal{x } \\left [ k_\\theta k_\\theta^{m - i-1}\\phi_j - k_{\\theta ' } k_\\theta^{m - i-1}\\phi_j\\right](y ) k_{\\theta'}^i(x , y ) dy\\end{aligned}\\ ] ] implying @xmath312 where @xmath73 is the metric and @xmath313 is the lipchitz constant introduced by assumption [ assumption : operator continuity ] .",
    "it concludes that @xmath314 denoting by @xmath315 the function class of @xmath316 , it follows that @xmath317 applying theorem [ theorem : vaart ] to function class @xmath315 yields @xmath318 as @xmath4 .",
    "further , @xmath319 as @xmath4 , completing the proof .",
    "we first show that , if @xmath182 , @xmath320 is a super - martingale adapted to @xmath321 s canonical filtration @xmath97 .",
    "the adaptedness of @xmath322 to @xmath97 follows @xmath323 being a @xmath97-stopping time .",
    "it suffices to show @xmath324 \\le 0 $ ] , then we also have integrability of non - negative @xmath322 by induction @xmath325 . indeed ,",
    "@xmath326 \\mathbb{i}(t \\le t)\\\\ & = 0\\\\ ( m_{t+1 } - m_t ) \\mathbb{i}(t \\ge t+1 ) & = \\left[\\left(v(z_{t+1 } ) + ( t+1)\\delta_1\\right ) - \\left(v(z_t ) + t\\delta_1\\right)\\right]\\mathbb{i}(t \\ge t+1)\\\\ & = \\left [ v(z_{t+1 } ) - v(z_t ) + \\delta \\right ]",
    "\\mathbb{i}(t \\ge t+1)\\end{aligned}\\ ] ] implying for @xmath327 @xmath328 & = \\mathbb{e}_z \\left [ ( m_{t+1 } - m_t ) \\mathbb{i}(t \\le t ) |",
    "\\mathcal{g}_t \\right ] + \\mathbb{e}_z \\left[(m_{t+1}-m_t ) \\mathbb{i}(t \\ge t+1 ) |",
    "\\mathcal{g}_t \\right]\\\\ & = \\mathbb{e}_z \\left [ \\left(v(z_{t+1 } ) - v(z_t ) + \\delta\\right ) \\mathbb{i}(t",
    "\\ge t+1)| \\mathcal{g}_t \\right]\\\\ & \\overset{(i)}{= } \\mathbb{e}_z \\left [ \\left(v(z_{t+1 } ) - v(z_t ) + \\delta\\right ) | \\mathcal{g}_t \\right ] \\mathbb{i}(t \\ge t+1)\\\\ & \\overset{(ii)}{= } \\mathbb{e}_z \\left [ \\left(v(z_{t+1 } ) - v(z_t ) + \\delta\\right ) | z_t \\right ] \\mathbb{i}(t \\ge t+1)\\\\ &",
    "\\overset{(iii)}{\\le } \\left[-\\delta + \\delta\\right ] \\mathbb{i}(t \\ge t+1)\\\\ & = 0\\end{aligned}\\ ] ] where ( i ) follows @xmath323 is a @xmath97-stopping time , and thus @xmath329 , ( ii ) is due to the markov property of @xmath216 and ( iii ) follows @xmath330 ( given @xmath331 and @xmath327 ) and the drift condition in definition [ drift condition ] .",
    "consequently , @xmath332 for @xmath327 .",
    "that is @xmath333 implying with non - negativeness of @xmath139 @xmath334 taking @xmath335 , the monotone convergence theorem yields @xmath336 furthermore , one step analysis gives @xmath337 for @xmath338 , completing the proof .",
    "this work is supported by nsf of us under grant dms-1407557 .",
    "the authors would like to thank prof .",
    "lester mackey , dr .",
    "rachel wang and weijie su for valuable advice .",
    "hinton , g. e. ( 2002 ) .",
    "training products of experts by minimizing contrastive divergence .",
    "_ neural computation _ * 14(8 ) * 17711800 .",
    "hinton , g. , s. osindero and y. teh ( 2006 ) . a fast learning algorithm for deep belief nets . _ neural computation _",
    "* 18(7 ) * 15271554 .",
    "mackay , d. ( 2001 ) .",
    "failures of the one - step learning algorithm . in available",
    "electronically at http://www.inference .",
    "phy.cam.ac.uk/mackay/abstracts/gbm.html ] .",
    "sutskever , i. and tieleman , t. ( 2010 ) . on the convergence properties of contrastive divergence . in _ international conference on artificial intelligence and statistics _ 789 - 795 .",
    "hyvrinen , a. ( 2006 ) .",
    "consistency of pseudolikelihood estimation of fully visible boltzmann machines .",
    "_ neural computation _ * 18(10 ) * 2283 - 2292 ."
  ],
  "abstract_text": [
    "<S> this paper studies the convergence properties of contrastive divergence algorithm for parameter inference in exponential family , by relating it to markov chain theory and stochastic stability literature . </S>",
    "<S> we prove that , under mild conditions and given a finite data sample @xmath0 i.i.d . in an event with probability approaching to 1 , </S>",
    "<S> the sequence @xmath1 generated by cd algorithm is a positive harris recurrent chain , and thus processes an unique invariant distribution @xmath2 . </S>",
    "<S> the invariant distribution concentrates around the maximum likelihood estimate at a speed arbitrarily slower than @xmath3 , and the number of steps in markov chain monte carlo only affects the coefficient factor of the concentration rate . finally we conclude that as @xmath4 , @xmath5    ,    , </S>"
  ]
}