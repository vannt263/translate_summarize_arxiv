{
  "article_text": [
    "in the m / g/1 queueing model , customers arrive at a single server with independent interarrival times , @xmath2 , distributed according to the @xmath3 distribution . here , @xmath4 is the arrival rate , hence @xmath2 has density function @xmath5 for @xmath6 and @xmath7 otherwise .",
    "they are served with independent service times @xmath8 , which have a @xmath9 distribution .",
    "( our mcmc approach can be generalized to other service time distributions . )",
    "we do not observe the interarrival times , only the interdeparture times , @xmath10 .",
    "the goal is to infer the unknown parameters of the queueing model , @xmath11 , using observed interdeparture times , @xmath12 .",
    "we assume that the queue is empty before the first arrival .",
    "the process of interdeparture times can be written as @xmath13 where @xmath14 is the time from the arrival of customer @xmath15 ( or from 0 when @xmath16 ) to the arrival of customer @xmath17 , @xmath18 is the arrival time of the @xmath19-th customer , and @xmath20 is the departure time of the @xmath19-th customer , with @xmath21 defined to be @xmath7 .",
    "we take the arrival times , @xmath22 , to be latent variables .",
    "the @xmath22 evolve in time as a markov process .",
    "viewed this way , the queueing model can be summarized as follows : @xmath23 for convenience , set @xmath24 .",
    "the joint density of the @xmath22 and the @xmath10 can be factorized as follows @xmath25 let @xmath26 be a prior for @xmath27 .",
    "the posterior distribution @xmath28 of @xmath27 is then @xmath29 the integral ( [ eq : vxpdf ] ) can not be computed analytically .",
    "hence to sample from @xmath28 we need to include the @xmath22 in the mcmc state and sample from the joint posterior distribution of the @xmath22 and @xmath27 given the data , which is @xmath30 taking the values of @xmath27 from each draw from ( [ eq : vxpost ] ) and ignoring the @xmath22 will yield a sample from the posterior distribution of @xmath27 .",
    "our mcmc procedure will be based on combining five different updates .",
    "the first is a gibbs update that draws new values for each @xmath22 , given values of the other @xmath22 , @xmath27 , and the data .",
    "the second is a simple metropolis update for @xmath27 , given values of the @xmath22 and the data .",
    "the last three updates are novel  one metropolis `` shift '' update and two metropolis - hastings - green `` scale '' updates that propose to simultaneously change components of @xmath27 and all of the @xmath22 .",
    "we first work out how to apply standard gibbs sampling updates for the @xmath22 , for which we need to derive the full conditional density for each @xmath22 given the other @xmath22 and @xmath27 .",
    "we denote all of the @xmath31 except @xmath22 as @xmath32 .",
    "we consider three cases , when @xmath33 , when @xmath34 and when @xmath35 .",
    "for the case @xmath33 , we have @xmath36)}{\\theta_{2 } - \\theta_{1}}\\nonumber \\\\   & & \\times \\ \\theta_{3}e^{-\\theta_{3 } ( v_{2 } - v_{1 } ) } i(v_{1 } \\leq v_{2 } ) \\nonumber \\\\ & \\propto & i(v_{1 } \\in [ \\max(0 , x_{1 } - \\theta_{2 } ) , \\min(v_{2 } , x_{1 } - \\theta_{1})])\\end{aligned}\\ ] ] so , conditional on the parameters and the observed data , the distribution of @xmath37 is @xmath38 .",
    "when @xmath34 , we have @xmath39)}{\\theta_{2 } - \\theta_{1}}\\nonumber \\\\ & & \\times \\ \\theta_{3}e^{-\\theta_{3 } ( v_{i + 1 } - v_{i})}i(v_{i } \\leq v_{i+1 } ) \\notag \\\\ & \\propto & i(v_{i } \\in [ v_{i-1 } , v_{i+1}])i(y_{i } \\in [ \\theta_{1 } + \\max(0 , v_{i } - x_{i - 1 } ) , \\theta_{2 } + \\max(0 , v_{i } - x_{i - 1 } ) ] ) \\ \\ \\ \\ \\ \\ \\label{eq : innercase}\\end{aligned}\\ ] ] to simplify this expression , note that @xmath40 , and first consider the case @xmath41 . when this is so",
    ", we must have @xmath42 , hence , in this case @xmath22 will have a uniform distribution on @xmath43 $ ] . now consider the case @xmath44 .",
    "we rewrite expression ( [ eq : innercase ] ) as @xmath45)i(y_{i } \\in [ \\theta_{1 } + \\max(0 , v_{i } - x_{i - 1 } ) , \\theta_{2 } + \\max(0 , v_{i } - x_{i - 1})])i(v_{i } \\leq x_{i-1 } ) } \\nonumber \\\\ & + & i(v_{i } \\in [ v_{i-1 } , v_{i+1}])i(y_{i } \\in [ \\theta_{1 } + \\max(0 , v_{i } - x_{i - 1 } ) , \\theta_{2 } + \\max(0 , v_{i } - x_{i - 1})])i(v_{i } > x_{i-1 } ) \\nonumber \\\\ & & = \\ i(v_{i } \\in [ v_{i-1 } , x_{i+1 } ] ) + i(v_{i }   \\in ( x_{i-1 } , \\min(v_{i+1 } , x_{i } - \\theta_{1 } ) ] ) \\nonumber \\\\ & & = \\ i(v_{i } \\in [ v_{i-1 } ,   \\min(v_{i+1 } , x_{i } - \\theta_{1})])\\end{aligned}\\ ] ] we see that for @xmath44 , @xmath22 will have a uniform distribution on @xmath46 $ ] .",
    "finally , for the case @xmath35 , we have @xmath47)\\end{aligned}\\ ] ] from this it follows that @xmath48 will have an exponential distribution , truncated to @xmath49 $ ] when @xmath50 and to @xmath51 $ ] when @xmath52 .",
    "all of the above distributions can be easily sampled from by using the inverse cdf method .",
    "the case @xmath35 deserves a small note .",
    "if we suppose the truncation interval of @xmath48 is @xmath53 $ ] , then the cdf and inverse cdf will be @xmath54 so we can sample @xmath48 as @xmath55 where @xmath56 is uniform@xmath57 .",
    "we use simple metropolis updates ( metropolis , et al ( 1953 ) ) to sample from the conditional posterior distribution of @xmath27 given values of the @xmath22 and the data , @xmath58 .",
    "when doing simple metropolis updates of @xmath27 given the arrival times and the data , we used the 1-to-1 reparametrization @xmath59 .    a simple metropolis update for @xmath60 that leaves @xmath61 invariant proceeds as follows .",
    "we choose a symmetric proposal density @xmath62 , for which @xmath63 .",
    "given the current value @xmath60 , we generate a proposal @xmath64 .",
    "we compute the acceptance probability @xmath65 and let the new state , @xmath66 , be @xmath67 with probability @xmath68 and otherwise reject the proposal , letting @xmath69 .",
    "we use a normal proposal with independent coordinates centered at the current value of @xmath60 , updating all components of @xmath60 at once .",
    "if the proposed value for @xmath70 or @xmath71 is outside its range , the proposal can be immediately rejected .    to prevent overflow or underflow , all mcmc computations use the logarithm of the posterior ( [ eq : vxpost ] ) , which ( up to an additive constant ) simplifies to @xmath72 whenever the following constraints are satisfied @xmath73 otherwise , @xmath74 .",
    "the dominant operation when computing the log likelihood for a data set of length @xmath75 is checking the constraints ( [ eq : c1 ] ) to ( [ eq : c5 ] ) , which requires time proportional to @xmath75 . storing the constraints on @xmath76 and @xmath77 given by ( [ eq : c2 ] ) to ( [ eq : c5 ] ) during evaluation of the log likelihood at @xmath78 allows us to evaluate the log likelihood for another @xmath79 and the same @xmath80 in constant time .",
    "this allows us to do @xmath81 additional metropolis updates for less than @xmath81 times the computational cost it takes to do a single update , which is likely to make sampling more efficient . a similar improvement in sampling should be possible for models with other service time distributions that have low - dimensional sufficient statistics . as well , doing additional simple metropolis updates makes an imperfect choice of proposal distribution have less of an effect on sampling efficiency .",
    "the gibbs and simple metropolis updates are sufficient to give an ergodic mcmc scheme . however , as we will see , these updates are sometimes very inefficient when used on their own . in this section , we introduce our novel shift and scale updates , which can make mcmc sampling much more efficient .",
    "shift updates and scale updates are used to sample from the joint posterior distribution of the parameters and the latent variables , @xmath82 .",
    "the shift update takes the form of a standard metropolis update , while the scale updates are metropolis - hastings - green ( mhg ) updates .",
    "in general , an mhg update proceeds as follows .",
    "we introduce an extra variable @xmath83 .",
    "given a current value @xmath84 and a density @xmath85 we generate @xmath86 .",
    "we then propose @xmath87 .",
    "the function @xmath88 is the inverse of itself , with a jacobian @xmath89 which is nowhere zero or infinite .",
    "we compute the acceptance probability @xmath90 and let the new state , @xmath91 be @xmath92 with probability @xmath68 and let @xmath93 otherwise . for more on the mhg algorithm ,",
    "see geyer ( 2003 ) .",
    "the motivation for the shift updates and scale updates is that conditioning on given values of the arrival times constrains the range of parameter values for which the posterior density is non - zero , preventing us from changing the parameters by a significant amount .",
    "this can lead to inefficient sampling when @xmath27 is updated given the @xmath22 .",
    "in contrast , our new shift and scale updates change the latent variables and the parameters simultaneously , in accordance with their dependence structure , thus allowing for much greater changes to the parameters .",
    "hard constraints on @xmath27 given @xmath94 are nt necessary for these updates to be beneficial .",
    "if the service time density were non - zero for all positive values , the distribution of @xmath27 given @xmath94 might still be much more concentrated than the marginal posterior for @xmath27 .",
    "we first consider updates that shift both the minimum service time , @xmath76 , and all the arrival times , @xmath22 , keeping the range of service times , @xmath95 , fixed .",
    "note that for @xmath33 and for all @xmath19 when @xmath96 , we have @xmath97 .",
    "hence the values of one or more of the @xmath22 will constrain how much we can propose to change @xmath76  any proposal to change @xmath76 that violates these constraints must be rejected .    a shift update",
    "addresses the presence of these constraints as follows .",
    "we first draw a shift @xmath98 from any distribution symmetric around @xmath7 . then",
    ", we propose new values @xmath99 for all @xmath19 and @xmath100 .",
    "so , a proposal to increase the minimum service time is coupled with a simultaneous proposal to decrease all of the arrival times in a way that keeps the constraints between the arrival times and the minimum service time satisfied .",
    "a shift proposal will be rejected if it proposes a value for @xmath37 or @xmath76 that is less than @xmath7 . otherwise ,",
    "the acceptance probability for a shift update is @xmath101      next , we consider range scale updates that propose to simultaneously change the latent variables and the range of service times @xmath95 , which is also constrained by the latent variables  specifically , for @xmath37 and for all @xmath102 where @xmath103 , we have @xmath104 .",
    "these updates propose to scale the `` gaps '' @xmath105 ( gaps between the current arrival time and the latest possible arrival time ) , while at the same time proposing to scale @xmath95 by the same amount , keeping @xmath76 fixed .    in particular , we first fix ( or draw from some distribution ) a scale factor @xmath106 .",
    "we then draw @xmath107 .",
    "the function @xmath108 proposes to change @xmath22 to @xmath109 and @xmath95 to @xmath110 .",
    "we set @xmath111 .",
    "the jacobian @xmath112 .    a range scale proposal will be rejected if it proposes a set of @xmath22 for which some @xmath113 or for which @xmath114 . otherwise , the mhg acceptance probability for a range scale update is @xmath115      finally , we consider rate scale updates that propose to simultaneously change both the interarrival times @xmath116 , and the arrival rate @xmath4 .",
    "we motivate these updates as follows .",
    "we expect the distribution of @xmath4 given the interarrival times @xmath2 to be concentrated around @xmath117 .",
    "consequently , we would expect the joint posterior of @xmath4 and the @xmath2 to have a high density along a ridge with values @xmath118 and @xmath119 for some @xmath120 , as long as @xmath118 and @xmath119 do not lie in region with low probability .",
    "so , proposing to change @xmath2 to @xmath118 and @xmath4 to @xmath119 for some @xmath120 potentially keeps us in a region of high density .",
    "we perform these updates in terms of @xmath121 . in detail",
    ", this update proceeds as follows .",
    "we first fix ( or draw from some distribution ) a scale @xmath122 .",
    "we then draw @xmath107 .",
    "the function @xmath123 proposes to change all @xmath2 to @xmath124 and @xmath125 to @xmath126 .",
    "we set @xmath111 .",
    "the jacobian @xmath127 .",
    "we reject the proposal immediately if it violates the following constraints : for @xmath19 where @xmath41 the proposed arrival time @xmath128 must be in @xmath129 $ ] , and for @xmath19 where @xmath44 , we must have @xmath130 . otherwise , the mhg acceptance probability for a rate scale update is @xmath131      although a scheme consisting of the shift , range scale , and rate scale updates changes all of the latent variables and parameters , it is not ergodic . to see this ,",
    "consider updating the arrival times @xmath22 , or equivalently , interarrival times @xmath116 .",
    "shift updates do not change the interarrival times .",
    "range scale updates with scale factor @xmath132 change each interarrival time as @xmath133 , and rate scale updates with scale factor @xmath134 change each interarrival time as @xmath135 . if we are in a situation where @xmath136 and @xmath137 for one or more @xmath138 , then all subsequent updates will keep @xmath136 .",
    "note that this is true even if @xmath132 and @xmath134 are randomly selected at each update .",
    "hence , our novel updates must still be combined with simple gibbs sampling updates of the @xmath22 s to get an ergodic sampling scheme .",
    "we also still do simple metropolis updates for @xmath27 .",
    "although this is not essential for ergodicity , it makes sampling a lot more efficient .",
    "the goal of our empirical study is to determine when using the novel updates improves sampling efficiency .",
    "we generate three simulated data sets that are representative of a range of possible scenarios , sample from the posterior distributions of @xmath27 for each of these data sets using various sampling schemes , and compute autocorrelation times to compare sampling efficiency .",
    "the sort of data arising from the observation of a queueing system can be roughly classified into one of three scenarios .",
    "the first scenario is when the interarrival times are , on average , smaller than the service times , that is , arrivals are relatively frequent . in this case",
    "the queue is generally full , and empty only early on .",
    "hence , most observed interdeparture times will tend to come from the service time distribution , and only a small number of interdeparture times will be relevant for inferring the arrival rate .",
    "consequently , we expect there to be large uncertainty in @xmath4 , but less for @xmath76 and @xmath77 .",
    "the second scenario is when interarrival times are on average slightly larger than the service times .",
    "the queue is then sometimes empty , and sometimes contains a number of people . in this case",
    ", we expect the data to be informative about all parameters .",
    "this is also the case of most practical interest , as it corresponds to how we may expect a queueing system to behave in the real world .",
    "the third scenario is when arrivals happen rarely , while the service time is relatively small . in this case",
    ", the queue will usually be empty .",
    "interarrival times are then informative for inferring @xmath4 , as most of them will come from the @xmath3 distribution with a small amount of added @xmath139 noise .",
    "however , inference of the service time bounds @xmath76 and @xmath77 will now be based on how significantly the distribution of the interarrival times ( for a given @xmath4 ) deviates from the exponential distribution",
    ". this difference may be rather small , and so we expect that the posterior for the service time bounds will be rather diffuse .",
    "the three data sets we generated each consisted of @xmath140 interdeparture times , corresponding to each of the three scenarios above . for the first scenario we take @xmath141 , for the second , @xmath142 , and for the third , @xmath143 .",
    "the plots in figure [ fig : datasets ] of the number of people in the queue up until the last arrival against time for each of the three data sets demonstrate that the data sets have the desired qualitative properties .",
    "0.3        0.3        0.3     numerical values of the simulated interdeparture times are presented in table [ table : datasets ] .",
    ".simulated interdeparture times @xmath144 [ cols=\">,>,>\",options=\"header \" , ]     from table [ table : tauest ] , we see that using just the shift , or just a scale update ( either a range or a rate scale update ) improves performance for sampling parameters that are changed by one of these updates . for a scheme which uses all updates ,",
    "the greatest improvement in performance for sampling @xmath125 is in the case of frequent arrivals ( efficiency gain of 179 times ) , while perfomance improvement when sampling @xmath70 and @xmath71 is greatest in the case of rare arrivals ( efficiency gains of 58 and 61 times ) . in other cases , performance neither increases nor decreases significantly .",
    "these results are in approximate agreement with the estimates of the standard errors of the posterior mean estimates in table [ table : meanest ] .    in an additional run (",
    "not used to compute the autocorrelation times shown here ) of the basic plus rate scheme , for the rare arrivals scenario , we found that the sampler got stuck for a while in a region of the parameter space .",
    "the basic and basic + shift methods ( when initialized to a state from this `` stuck '' region ) also stayed in this `` stuck '' region for a while before visting other regions .",
    "the basic plus range and basic plus all methods , when initialized with the stuck state , quickly returned to sampling other regions .",
    "so , autocorrelation time estimates for the rare arrivals scenario , for methods other than basic plus all and basic plus rate , probably underestimate actual autocorrelation times .",
    "we illustrate the performance of the samplers with trace plots in figures [ fig : tracefreq ] , [ fig : traceinter ] , and [ fig : tracerare ] . to produce these figures , we ran the samplers for an equal amount of computation time and thinned each run to @xmath145 points .",
    "the black line on each plot is the true parameter value .",
    "0.45        0.45     0.45        0.45     0.45        0.45     0.45        0.45     0.45        0.45     0.45        0.45     0.45        0.45     0.45        0.45     0.45        0.45     in the case of frequent arrivals , the marginal posterior of @xmath125 is diffuse but concentrated given all @xmath80 and the data , so simultaneously updating all @xmath80 and @xmath125 makes sampling more efficient .",
    "in the case of rare arrivals , the marginal posteriors of both @xmath70 and @xmath71 are diffuse , while they are concentrated given all @xmath80 and the data . simultaneously updating all @xmath80 and",
    "either @xmath70 or @xmath71 then leads to a noticeable gain in efficiency . in the other cases ,",
    "the data is more informative about the parameters , so additional knowledge of the latent variables does not change the concentration of the posterior by as much . as a result , sampling efficiency is not affected as significantly by changing the latent variables and the parameters simultaneously .",
    "in this paper , we have shown how bayesian inference with mcmc can be performed for the m / g/1 queueing model .",
    "as mentioned earlier , fearnhead and prangle ( 2010 ) used abc for inference in the m / g/1 queueing model .",
    "fearnhead and prangle ( 2010 ) also used abc for inference in the ricker model of population dynamics , in which we assume that a population process is observed with poisson noise .",
    "the basis of all abc methods is the ability to simulate observations from a given stochastic model .",
    "the simulation process for a set of observations is always driven by a latent process of sampling and transforming certain random variables .",
    "the distributions of these random variables then determine the distribution of the final observed quantities .",
    "if we consider the latent variables which drive the simulation process jointly with the observations , then we can think of the observations as coming from a model with some latent structrure .",
    "these latent variables and the observed data will sometimes have a tractable joint density , which makes doing bayesian inference with mcmc possible , at least in principle .",
    "in an earlier work , shestopaloff and neal ( 2013 ) , we used the same approach as in this paper to do bayesian inference for the ricker model , i.e. including additional latent variables in the mcmc state and sampling for them as well as model parameters .",
    "we compared a basic mcmc scheme with several `` ensemble mcmc '' schemes for bayesian inference in the ricker model , and showed that using the ensemble schemes leads to a significant improvement in efficiency when compared to the basic mcmc scheme . like for the m / g/1 queueing model",
    ", we have shown that bayesian inference with mcmc is possible for the ricker model , but requires more sophisticated mcmc methods for sampling to be efficient .",
    "it would be interesting to apply alternative inference methods for models with a time series structure to the m / g/1 queue , for example particle filters and particle markov chain monte carlo ( pmcmc ) ( andrieu , doucet , and holenstein ( 2010 ) ) .",
    "as mentioned earlier , it should be possible to extend the mcmc method in this paper to service time distributions other than the uniform one used in this paper .",
    "the most direct extension would be to consider location - scale service time distributions .",
    "besides the simple metropolis updates , we can then do additional updates for the location parameter ( or some 1-to-1 function of it ) using a shift update and additional updates for the scale parameter ( or some 1-to-1 function of it ) using a range scale update .",
    "computation would not be impacted so long as ( [ eq : logpost ] ) can be written in terms of low - dimensional sufficient statistics .",
    "this research was supported by the natural sciences and engineering research council of canada .",
    "a.  s.  is in part funded by an nserc postgraduate scholarship .",
    "r.  n.  holds a canada research chair in statistics and machine learning .",
    "0.2 in          fearnhead , p. , prangle , d. ( 2012 ) `` constructing summary statistics for approximate bayesian computation : semi - automatic approximate bayesian computation '' , _ journal of the royal statistical society b _ , vol .",
    "74 , pp .  1 - 28",
    ".      metropolis , n. , rosenbluth , a.w . ,",
    "rosenbluth , m.n . ,",
    "teller , a.h . , and teller , e. ( 1953 ) .",
    "`` equation of state calculations by fast computing machines '' . _ journal of chemical physics _ , vol .  21 , pp .  1087 - 1092 .",
    "neal , r. m. , beal , m. j. , and roweis , s. t. ( 2004 ) `` inferring state sequences for non - linear systems with embedded hidden markov models '' , in s. thrun , et al ( editors ) , _ advances in neural information processing systems 16 _ , mit press .",
    "neal , r. m. ( 2003 ) `` markov chain sampling for non - linear state space models using embedded hidden markov models '' , technical report no .",
    "0304 , department of statistics , university of toronto , http://arxiv.org/abs/math/0305039 ."
  ],
  "abstract_text": [
    "<S> we introduce an efficient mcmc sampling scheme to perform bayesian inference in the m / g/1 queueing model given only observations of interdeparture times . </S>",
    "<S> our mcmc scheme uses a combination of gibbs sampling and simple metropolis updates together with three novel `` shift '' and `` scale '' updates . we show that our novel updates improve the speed of sampling considerably , by factors of about @xmath0 to about @xmath1 on a variety of simulated data sets .    </S>",
    "<S> this paper proposes a new approach to computation for bayesian inference for the m / g/1 queue ( markovian arrival process / general service time distribution/1 server ) . </S>",
    "<S> inference for this model using abc ( approximate bayesian computation ) was previously considered by bonassi ( 2013 ) , fearnhead and prangle ( 2012 ) , and blum and francois ( 2010 ) . </S>",
    "<S> abc , in general , does not yield samples from the exact posterior distribution . </S>",
    "<S> we use the strategy of considering certain unobserved quantities as latent variables , allowing us to use markov chain monte carlo ( mcmc ) , which converges to the exact posterior distribution . </S>"
  ]
}