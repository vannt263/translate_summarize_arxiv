{
  "article_text": [
    "some physics simulations require the solution of poisson s equation with an isolated source distribution and a vanishing boundary condition at infinity .",
    "a common example is the calculation of the newtonian gravitational potential of a self - gravitating astrophysical system .",
    "poisson s equation is @xmath4 where @xmath5 describes the known distribution of the source that generates the potential @xmath6 .",
    "for instance , @xmath7 is proportional to the mass density in the context of newtonian gravity , and to the charge density in electrostatics .",
    "several methods exist to solve the discretized poisson s equation on a uniform grid .",
    "these include , for example , multigrid methods , iterative / relaxation methods , several matrix methods , and methods that employ fourier transforms ( for discussion of some of these , see for instance @xcite ) . here",
    "we implement , and extend to three dimensions , a particular method @xcite ( see also @xcite and @xcite ) of the latter class .",
    "( another well - known approach for isolated systems based on fourier transforms @xcite , also discussed in @xcite , would not be as straightforward to parallelize and is not discussed here . )",
    "an advantage of this approach is that discrete fourier transform algorithms have been well - studied , with the fast fourier transform ( fft ) being the most commonly employed ; it requires @xmath8 operations , where @xmath9 is the number of elements to transform .",
    "several fft implementations , some freely available , also exist as libraries suitable for end - users .",
    "the key issue addressed by the implementation described here is the parallelization of an fft - based algorithm for solving poisson s equation for an isolated system .",
    "obtaining such solutions in three dimensions requires resources that at present are most commonly available in distributed - memory parallel computers .",
    "machines of this type allow large problems to be decomposed  for example , into multiple spatial subdomains  and distributed across different ` processes ' to be solved in parallel .",
    "( each ` process ' contains its own copy of the program ; can only access memory locations allocated either statically or dynamically by the program ; and can communicate with other processes only through a specific protocol , the message passing interface ( mpi ) @xcite presently being the most widely used . )",
    "while poisson s equation must be solved globally on the computational domain and across multiple processes , most fft implementations require that all data be accessible on a single process ; therefore data movement and coordination across multiple processes are key ingredients of our fft - based approach .    our poisson solver for isolated systems in three dimensions , named `",
    "pspfft ` ( `` p`oisson ` s`olver `",
    "p`arallel ` fft ` ' ) , is available as a library that can readily be used by and linked to other programs , subject to a few constraints on numbers of processes and mesh points described in subsec .",
    "[ subsubsec : decomposition ] .",
    "we use the fftw library @xcite , but our use of it is abstracted in such a way that other fft libraries could be used without having to make widespread changes throughout the code .",
    "we use mpi to manage data movement and communication across processes , but our calls to message passing routines are abstracted as well .",
    "this paper is organized as follows .",
    "section [ sec : solutionmethod ] outlines the algorithm as well as implementation details specific to our code .",
    "this is followed in section [ sec : installationusage ] by instructions on installation and use of the program as a library .",
    "test problems illustrating the convergence properties and performance of our implementation are presented in section [ sec : numericalresults ] .",
    "section [ sec : conclusion ] contains concluding remarks .",
    "our problem is to solve eq .",
    "( [ eq : poisson ] ) with the boundary condition @xmath10 as @xmath11 ( vanishing boundary condition ) . use of the green s function @xmath12 which satisfies @xmath13 and obeys the vanishing boundary condition , yields the formal solution @xmath14 this integral may be evaluated with the help of the convolution theorem .",
    "given the fourier transforms @xmath15 and @xmath16 of @xmath17 and @xmath5 , the fourier transform of the potential is @xmath18 the potential @xmath19 is then obtained by an inverse fourier transform .    when the fourier transforms are to be done with ffts , use of a regular mesh with the same spacings in each dimension is most natural ; but in principle it should be possible to use any mesh for which a coordinate transformation can bring the mesh point positions to triplets of integers .",
    "for instance , to allow for a regular mesh with numbers of mesh points @xmath20 and unequal ( but constant ) mesh point spacings @xmath21 in the three dimensions , eqs .",
    "( [ eq : greens])-([eq : convolution ] ) become @xmath22 where the values of the transformed coordinates @xmath23 corresponding to the mesh points are triplets of integers ranging from @xmath24 to @xmath25 in the three dimensions respectively .",
    "( note that the jacobian of the coordinate transformation has been absorbed into the numerator of eq .",
    "( [ eq : greens2 ] ) , with the denominator still being @xmath26 times the distance from the origin . )    the implementation of boundary conditions at infinity on a necessarily finite computational domain can be handled ` exactly ' , that is to say , with only discretization error , via a mesh doubling procedure and use of the standard periodic form of the discrete fourier transform @xcite ( see also @xcite and @xcite ) .",
    "figure [ fig : mesh_doubling ] illustrates this in two dimensions .",
    "the ` active ' portion of the mesh corresponds to the original computational domain , while the ` inactive ' portions are those arising from doubling the extent of the mesh in each dimension .",
    "the source distribution is set to its known physical values in the active region , and to zero in the inactive regions .",
    "indexing cells by integer triplets @xmath27 ( the position of the mesh points in transformed coordinates @xmath23 ) , the green s function in the active and inactive regions is @xmath28^{-1}.   & & \\label{eq : greens_function}\\end{aligned}\\ ] ] that is , the green s function in the active region follows eq .",
    "( [ eq : greens2 ] ) , and is set up in the inactive regions in such a way that a periodic replication of the doubled mesh in all dimensions yields eq .",
    "( [ eq : greens2 ] ) in the entire region @xmath29 surrounding the origin .",
    "the value for @xmath30 regularizes the singularity at the origin by assigning it to be equal to the largest off - origin value on the mesh ; for @xmath31 , this reduces ( up to conventions for sign and the @xmath26 factor ) to the prescription given in @xcite .     cells . ]      ` pspfft ` is written in fortran 95 , using an object - oriented programming style to a degree convenient and possible within that language .",
    "the library fftw @xcite provides our basic fft functionality , and we use mpi @xcite to implement parallelization across multiple processes .      in many physical simulations the problem size is large enough that the computational domain is spatially decomposed into multiple subdomains , each assigned to a different process . in the general case",
    "the extent of the domain  and / or the number of mesh points  need not be the same in all dimensions . for the purpose of minimizing communications , decompositions yielding subdomains with low surface - to - volume ratio",
    "are favorable in situations ( such as hydrodynamics ) in which nearest - neighbor information is required .",
    "our code assumes that the source @xmath5 of eq .",
    "( [ eq : poisson ] ) is available , and that the potential @xmath19 is desired , in a simple ` brick ' decomposition : in three dimensions , the computational domain is divided in each dimension by @xmath32{n_p}$ ] , the cube root of the number of processes @xmath33 . for simplicity",
    "we require @xmath34 to be a perfect cube ( in three dimensions ) , and the number of mesh points in each dimension to be evenly divisible by @xmath35 .",
    "figure [ fig : domain_decomposition ] illustrates the brick decomposition .",
    "the brick decomposition is not convenient for ffts , however , because a single transform is most naturally and efficiently performed on data accessible to a single process ; therefore our solver has its working storage arranged in ` pillars ' rather than bricks .",
    "( a distributed parallel fft that leaves data in place in the brick decomposition  and therefore requires parallelized one - dimensional ffts  has also been mentioned in the literature @xcite .",
    "in contrast , our approach allows in principle the use of any of several widely available and highly optimized serial fft libraries . )",
    "the ` length ' of what we term ` @xmath36 pillars ' spans the full extent of the computational domain in the @xmath36 direction .",
    "the ` width ' of the @xmath36 pillars is their extent in the @xmath37 direction , which is @xmath38 times the @xmath37 extent of the bricks .",
    "this implies another constraint imposed by our solver : the number of mesh points @xmath39 spanned by the @xmath37 extent of a brick must itself be evenly divisible by @xmath35 .",
    "the ` height ' of the @xmath36 pillars , which is their extent in the @xmath40 direction , is the same as the extent of the bricks in the @xmath40 direction . by similar construction",
    "( and with similar constraints on @xmath41 and @xmath42 ) , we have @xmath37 pillars and and @xmath40 pillars whose ( width , height ) are taken to be their extents in @xmath43 and @xmath44 respectively .",
    "these ` pillar decompositions ' cover the same total volume and contain the same total number of mesh points as the brick decomposition , as illustrated in fig .",
    "[ fig : bricks_to_pillars ] .",
    "finally , we note that the lengths of the pillars are doubled as necessary to accommodate the mesh doubling procedure , so that the pillars span both the active and inactive portions of the mesh .",
    "pillars from a three - dimensional mesh assigned to twenty - seven processes . here",
    "only the first ( lowest in the @xmath40 direction ) @xmath45 slab of bricks is shown ; other slabs independently follow the same transformation .",
    "the left panel shows the whole computational domain decomposed into bricks , demarcated by solid lines and assigned to processes labeled by the numbers in solid circles",
    ". dashed lines in the left panel mark the chunks of data that need to be sent to the processes labeled by the numbers in dashed squares in order to build the pillars . as indicated by the dotted boundaries , processes @xmath46 $ ] ,",
    "@xmath47 $ ] , and @xmath48 $ ] form three separate groups , each with its own subcommunicator within which chunks of data are exchanged during the construction of the @xmath36 pillars . in the right panel",
    ", we see that each process ( again , labeled by numbers in solid circles ) also owns a pillar .",
    "the boundaries between pillars are now marked by solid lines , and the dashed lines indicate the chunks of data that the process received from other processes labeled by numbers in dashed squares . ]    because of the row - major nature of fortran array storage , a pillar s length , width , and height correspond in our code to the first , second , and third dimensions of a rank - three array .",
    "this allows a @xmath49-number of one - dimensional ffts to be performed efficiently on contiguous data , specifically on lines containing a number of data points equal to the pillar length .",
    "the construction of pillars from bricks and vice - versa requires data movement across different processes . using mpi ,",
    "this is accomplished by creating a subcommunicator for each group of processes that need to communicate data among themselves , as illustrated in fig .",
    "[ fig : bricks_to_pillars ] .",
    "for each group , a call to ` mpi_alltoall ` can then be made with the group s subcommunicator in order to achieve the construction of pillars .",
    "these subcommunicators are saved to be used in the reverse process of deconstructing pillars back into bricks .",
    "a multidimensional fft can be accomplished as a sequence of sets of one - dimensional transforms .",
    "the number of required operations is still of @xmath8 , where @xmath50 is the total number of mesh points .",
    "one possibility for achieving computational efficiency is to transpose data between transforms in subsequent dimensions in order to achieve contiguity of memory access in each dimension . in any case ,",
    "such transpose operations become a necessity in a distributed memory environment if parallelization of individual one - dimensional transforms is to be avoided .",
    "the sequence of transforms and transposes is as follows .",
    "data are initially loaded into the solver s @xmath36 pillars : during initialization the green s function is set up directly in the @xmath36 pillars according to eq .",
    "( [ eq : greens_function ] ) , while the source is transferred from the brick decomposition to the @xmath36 pillars at the beginning of every solve . with data loaded in @xmath36 pillars , multiple one - dimensional transforms in the @xmath36 dimension",
    "are simultaneously performed by all processes .",
    "the @xmath37 pillars are then populated , independently in separate @xmath45 ` slabs ' , as illustrated in fig .",
    "[ fig : x - pillars_to_y - pillars ] . for each slab a separate mpi group with its own subcommunicator is created ; thus are are @xmath32{n_p}$ ] subcommunicators , each of which has @xmath51 processes . within each subcommunicator a call to ` mpi_alltoall ` transposes the data from @xmath36 pillars to @xmath37 pillars so that ffts can be performed in the @xmath37 direction .",
    "similar transposes in @xmath52 slabs allow ffts to be performed in @xmath40 pillars . here",
    "the multiplication of the transforms of the green s function and the source takes place as well , with the resulting fourier - space solution of the poisson equation overwriting the transformed source distribution . a reverse sequence of backward transforms and transposes gets the solution ( modulo a normalization factor due to the multiple transforms ) back into real space , stored in the @xmath36 pillars .",
    "finally the solution is redistributed from the active portion of the @xmath36 pillars to the brick decomposition , overwriting the storage in which the source was delivered .",
    "pillars to @xmath37 pillars on a three - dimensional mesh assigned to twenty - seven processes . as",
    "before , only the first ( lowest in the @xmath40 direction ) @xmath45 slab is shown .",
    "the solid rectangles demarcate data owned by different processes , labeled by numbers in solid circles .",
    "dashed lines mark chunks of data that need to be sent to ( left panel ) and received from ( right panel ) processes labeled by numbers in dashed boxes . in this example , a slab with 9 processes forms a single mpi group with its own subcommunicator , and the transpose is accomplished with a call to ` mpi_alltoall ` . ]",
    "this sequence of transforms and transposes makes use of permanent storage for the source distribution in @xmath36 , @xmath37 , and @xmath40 pillars , which at the end of the solve is reused to store the potential .",
    "this same storage is then updated with a new source distribution on the next call to the poisson solver .",
    "the transform of the green s function is computed only once , and stored permanently in @xmath40 pillars , when the solver is initialized .",
    "computation of the transformed green s function requires @xmath36 pillars and @xmath37 pillars , but these are deallocated at the end of the initialization .",
    "` pspfft ` is distributed as a gzip - compressed ` .tar ` file . upon uncompression and extraction , the top - level directory",
    "` pspfft ` has a ` readme ` file and four subdirectories : ` build ` , ` config ` , ` source ` , and ` test ` .",
    "complete installation instructions are given in the ` readme ` file . during the build process",
    ", object files ( with a ` .o ` extension ) and fortran module files ( usually with a ` * .mod ` extension ) are placed in the ` build ` directory .",
    "the ` config ` directory includes a configuration file in which system - specific values for the build and installation process are specified ; these include the name of the mpi compiler wrapper , the location of the fftw library , and the location of the installation .",
    "the directories ` source ` and ` test ` contain the ` pspfft ` source code and a test program respectively .",
    "the test program solves the problem described in sec .",
    "[ subsec : homogeneous_spheroid ] .",
    "brief installation instructions are as follows . after replacing the system - specific values in the file ` config / makefile_config ` , the user should change to the ` build ` directory to build and install the library by typing    .... > make > make install   ....    this creates and copies the library file ` libpspfft.a ` and the fortran module files to the installation directory .",
    "the library file can then be linked to programs that need the solver .",
    "this code snippet and the explanation that follows illustrate how to use the ` pspfft ` library in a fortran program .",
    "[ source , numberlines ] ---- program main    use pspfft     implicit none    include ' mpif.h '       integer : : &      error    integer , dimension(3 ) : : &      ncellsperprocess , &      ntotalcells     real(kr ) , dimension(3 ) : : &      cellwidth     type(arrayreal_3d_base ) , dimension(1 ) : : &      sourcesolution    type(pspfft_form ) , pointer : : &      poissonsolver      call mpi_init(error )    ! -- add lines to set ncellsperprocess , ntotalcells , and cellwidth    call initialize(sourcesolution , ncellsperprocess )    ! -- add lines to fill in sourcesolution with the source distribution    call create(poissonsolver , cellwidth ,   ntotalcells , mpi_comm_world , &                verbosityoption = console_info_2 )    call solve(ps , sourcesolution )    ! -- add lines that use the potential returned in sourcesolution    call destroy(ps )    call finalize(sourcesolution )    call mpi_finalize(error ) end program main ----    all the derived data types , parameters , and subroutines used in the above example ( except mpi - related variables and subroutines , such as ` mpi_comm_world ` and ` mpi_init ` ) are defined by the fortran module ` pspfft ` , which is used by the main program on line @xmath53 .",
    "after initializing mpi , the user should specify the number of cells per process , the total number of cells across all processes , and the cell width  all of which are arrays specifying these quantities in each of the three dimensions  as indicated on line @xmath54 .",
    "the variable ` sourcesolution ` is an array of derived data type ` arrayreal_3d_base ` .",
    "this is essentially a facility to make an array of rank - three arrays . `",
    "arrayreal_3d_base ` has the following definition :    .... type , public : : arrayreal_3d_base    real(kr ) , dimension ( : , : , : ) , allocatable : : &      data end type arrayreal_3d_base ....    the first , second , and third dimensions in ` sourcesolution%data ` correspond to the @xmath36 , @xmath37 , and @xmath40 dimensions on the mesh subdomain owned by a particular process .",
    "its value specifies the source in that particular cell .",
    "therefore this variable initially specifies the source distribution on the mesh which should be filled in by user .",
    "line @xmath55 simply initializes ` sourcesolution ` by allocating the necessary storage .",
    "the call to the ` create ( ) ` subroutine on line @xmath56 allocates storage for the ` poissonsolver ` variable and does all the necessary initializations . an optional argument ` verbosityoption ` controls the verbosity of the messages the solver prints to ` stdout ` .",
    "acceptable values ( defined as parameters ) , in decreasing order of verbosity , are ` console_info_2 ` , ` console_info_1 ` , ` console_warning ` , and ` console_error ` . if the argument is omitted , the verbosity level is set to ` console_error ` ( the least verbose ) by default .",
    "it is possible to replace the fourth argument ( i.e. ` mpi_comm_world ` in this example ) with a different mpi communicator which specifies a subgroup of processes that should be involved in solving the poisson equation , subject to the constraints on numbers of processes and mesh points described in section [ subsubsec : decomposition ] .",
    "line @xmath57 solves the poisson equation with the source distribution passed as an argument . upon return ,",
    "the variable for the source is overwritten with the values of the potential .",
    "the allocatable storage is deallocated by the calls to ` destroy ( ) ` and ` finalize ` in lines @xmath58 .",
    "all public subroutines exposed by ` pspfft ` are defined as generic interfaces using the function and subroutine overloading feature of fortran 95 .",
    "therefore , other subroutines with the same names may be defined and will not conflict with the library as long as they are defined as generic interfaces .    assuming the above code example is in the file ` main.f90 ` , compilation and linking to ` pspfft ` are accomplished by a command like the following :    .... > mpif90 -l $ ( install)/lib -l pspfft -i $ ( install)/include main.f90 ....    where ` $ ( install ) ` is the installation location of the library , and ` mpif90 ` may also need to be replaced by a system - specific mpi compiler wrapper .",
    "in this section we present a few illustrative test problems , investigate the numerical convergence of our code with respect to mesh resolution , and explore its scaling behavior on a distributed - memory parallel computer .",
    "the chosen test problems are broadly similar to systems encountered in astrophysical simulations , except that they have analytic solutions ; this allows us to verify the correctness of our implementation .      consider the gravitational potential of a sphere of radius @xmath59 and uniform mass density @xmath60 .",
    "the source is @xmath61 , and the potential as a function of radius @xmath62 from the center of the sphere has a simple analytic solution : @xmath63    we compute the potential for a sphere of radius @xmath64 and mass density @xmath65 in a cartesian computational box with inner and outer boundaries at @xmath66 $ ] respectively in all dimensions .",
    "the sphere is centered on the origin of the coordinate system . for each mesh resolution",
    ", we calculate the potential in two ways : first , by using the analytic solution above with @xmath67 , and second , by using our implementation of the poisson solver . by varying the mesh resolution we can check the convergence properties of our solver with respect to spatial resolution .",
    "the potential for this test problem is shown in figure [ fig : uniform_mass_potential ] .    .",
    "the figure is a slice through the three - dimensional mesh crossing the origin to show the @xmath68-plane .",
    "the solid black line indicates the surface of the sphere at radius @xmath64 .",
    "the mesh resolution is 256 cells in each dimension .",
    "[ fig : uniform_mass_potential ] ]    we use the usual definition of the @xmath69 norm to measure the error of the potential @xmath70 computed by our solver relative to the analytic solution @xmath71 : @xmath72 the summation is over all cells , indexed by @xmath73 , @xmath74 , @xmath75 , .",
    "the @xmath69 norm gives a single number as a quantitative measure of the error for a given mesh resolution ; in contrast , figure [ fig : uniform_mass_error ] illustrates the distribution of the relative error on the grid for a particular resolution , which is representative ( by different constant factor ) of the error distribution for other resolutions .",
    "figure [ fig : sphere_convergence ] shows the convergence of our solver ( relative error as a function of resolution ) for this problem .",
    "the convergence of the error trend is better than first order .     but without the summation over all cells , of the potential of a spherical uniform - density mass .",
    "the figure is a slice through the mesh showing the @xmath68-plane .",
    "the mesh resolution is 256 cells in each dimension .",
    "the largest errors are on the surface of the sphere due to the nature of the cartesian grid .",
    "[ fig : uniform_mass_error ] ]    -norm relative error of the computed potential as compared to the analytical solution is plotted as function of the following mesh resolutions : @xmath76 $ ] .",
    "the dot - dashed and dashed lines are references for first- and second - order error convergence respectively . ]      a more general case of the previous test problem is the potential of a spheroid with uniform density @xmath60 .",
    "the spheroid is formed by an ellipse centered at the origin and rotated about the @xmath40 axis , and is described by the equation @xmath77 where @xmath78 and @xmath79 are the semi - diameters of the spheroid .",
    "the spheroid is oblate when @xmath80 , with eccentricity @xmath81 defined as @xmath82 the gravitational potential of a homogeneous spheroid @xcite is a simpler case of the potential of a homogeneous ellipsoid given in @xcite . inside the spheroid , @xmath83 , \\label{eq : spheroid_potential_inside}\\ ] ] where @xmath84 @xmath85 outside the spheroid the potential is given by @xmath86 , \\label{eq : spheroid_potential_outside}\\ ] ] with @xmath87 in which @xmath88 is the positive root of the equation @xmath89    we compute the potential for a spheroid with eccentricity @xmath90 and semi - major axis @xmath91 on a cartesian computational box of size two in each dimension .",
    "as before , we set @xmath65 .",
    "figure [ fig : spheroid_potential ] shows the computed potential for a particular mesh resolution .     and semi - major axis @xmath91 on a mesh with spatial resolution of @xmath92 cells in each dimension .",
    "the figures are slices of the mesh through the origin showing both the @xmath68- and @xmath93-plane .",
    "the solid black line indicates the surface of the spheroid .",
    "[ fig : spheroid_potential ] ]    as in the previous test problem , we consider the error of the numerical solution relative to the analytic solution .",
    "figure [ fig : spheroid_error ] illustrates the spatial distribution of the error for a particular resolution .",
    "the convergence of the error ( specifically , the @xmath69-norm ) with higher resolution is shown in fig .",
    "[ fig : spheroid_convergence ] ; we see that on this problem our solver has higher than first order convergence , but less than second order .",
    "cells in each dimension .",
    "slices showing the @xmath68- and @xmath93-planes are shown .",
    "as before , the solid black line indicates the surface of the spheroid .",
    "[ fig : spheroid_error ] ]    $ ] . ]      in this problem we place two separate homogeneous spheroids in the computational domain .",
    "the extent of the domain in the @xmath36 direction is twice that of the previous test problem , so that the @xmath36 dimension has inner and outer boundaries at coordinates @xmath94 .",
    "the spheroids are centered on coordinates @xmath95 $ ] . in order to maintain the same effective resolution as our previous test problem , we double the number of cells in @xmath36 dimension only , resulting in a rectangular computational box .    , eccentricity @xmath96 , and semi - major axis @xmath91 .",
    "the solid black lines indicate the spheroids surfaces . the mesh resolution is @xmath97 cells .",
    "[ fig : binary_spheroid_potential ] ]    figure [ fig : binary_spheroid_potential ] shows the potential for this test problem , which is the sum of the potentials of the individual spheroids .",
    "thus the analytic solution for this test problem is obtained by modifying the analytic solution found in section [ subsec : homogeneous_spheroid ] to account for the shift of the spheroids centers from the origin .",
    "this is done by substituting @xmath98 for @xmath36 in eqs .",
    "[ eq : spheroid_potential_inside ] , [ eq : spheroid_potential_outside ] , and [ eq : spheroid_potential_auxiliary ] , where @xmath99 is the @xmath36 coordinate of the center of spheroid .    as before , we vary the mesh resolution for this test problem to do a convergence test of our solver .",
    "this is shown in fig .",
    "[ fig : binary_spheroid_convergence ] .",
    "the convergence trend is similar to those of the previous test problems , namely , our solver converges better than first order , but does not reach second order convergence .",
    "we test the weak scaling behavior of our solver by increasing the number of parallel processes while increasing the mesh resolution to maintain a constant amount of work per process .",
    "the total cpu time per solve can be expressed @xmath100 where @xmath33 is the number of processes and @xmath101 for perfect weak scaling . in fig .",
    "[ fig : weak_scaling ] we plot the wall time per solve @xmath102 for the homogeneous spheroid and binary spheroid problems , averaged over 2500 solves ( solid lines ) .",
    "the scaling tests were carried out on a cray xt-4 with quad - core 2.1 ghz amd opteron 1354 ( budapest ) processors and 8 gb of ddr2 - 800 memory per node . for compiling the code , we used the vendor - provided pgi compiler version 10 and fftw version 3.2 .",
    "also shown are idealized theoretical trends in the absence of communication costs ( dashed lines ) . for the fft alone",
    "we expect @xmath103 where @xmath78 is a constant and @xmath104 is the total number of cells . rewriting in terms of @xmath105 , where @xmath106 is the number of cells per process , we have @xmath107 where @xmath79 is a new constant .",
    "thus the theoretical expectation we plot is @xmath108 with the normalization @xmath79 set by equating eq .",
    "[ eq : twall2 ] to eq .",
    "[ eq : twall ] for @xmath109 .",
    "the number of cells per process is @xmath110 for the homogeneous spheroid and @xmath111 for the binary spheroid .",
    "we attribute the difference between the measured and theoretically ideal trends to communication costs that rise with the number of processes .",
    "we do not consider this extra cost severe , as the time per solve is still about 1 second with @xmath112 processes .",
    "plotted against number of processes @xmath33 , demonstrating the weak scaling behavior of ` pspfft ` .",
    "the homogeneous sphere and spheroid test problems assign @xmath113 cells per process ( black solid curve connected with squares ) , while the binary spheroid test problem assigns @xmath114 cells per process due to the doubling of computational domain in one dimension ( red solid curve connected with circles ) .",
    "the theoretically expected trend for the fft alone  without communication costs  is shown by the dashed lines , whose vertical offsets are set to match the measured values for @xmath109 .",
    "we have described our implementation of a parallel solver for poisson s equation for an isolated system .",
    "our solution method uses fourier transforms on a distributed unigrid mesh ; in particular we use the fftw library .",
    "we employ a common protocol , message passing interface ( mpi ) , for communication between processes during a global solve on a distributed - memory system .",
    "we have shown test problems , numerical convergence , and the weak scaling behavior of our solver .",
    "we distribute the solver as a library , ` pspfft ` , which is suitable for use as part of a parallel simulation system .",
    "this research used resources of the oak ridge leadership facility at the oak ridge national laboratory , which is supported by the office of science of the u.s .",
    "department of energy ( doe ) .",
    "c. y. c. acknowledges support from the office of nuclear physics and the office of advanced scientific computing research of doe .",
    "oak ridge national laboratory is managed by ut - battelle , llc , for the doe .",
    "r. d. b. acknowledges support from nsf - oci-0749204 ."
  ],
  "abstract_text": [
    "<S> we describe an implementation to solve poisson s equation for an isolated system on a unigrid mesh using ffts . </S>",
    "<S> the method solves the equation globally on mesh blocks distributed across multiple processes on a distributed - memory parallel computer . </S>",
    "<S> test results to demonstrate the convergence and scaling properties of the implementation are presented . the solver is offered to interested users as the library ` pspfft ` .    </S>",
    "<S> * program summary *    _ program title : _ pspfft + _ program obtainable from : _ </S>",
    "<S> http://astro.phys.utk.edu/_media/codes:pspfft-1.0.tar.gz , cpc program library + _ cpc catalogue identifier : _ ` </S>",
    "<S> aejk_v1_0 ` + _ licensing provisions : _ standard cpc license , http://cpc.cs.qub.ac.uk/licence/licence.html + _ </S>",
    "<S> programming language : _ fortran 95 + _ computer : _ any architecture with a fortran 95 compiler , distributed memory clusters + _ operating system : _ linux , unix + _ ram : _ depends on the problem size , approximately @xmath0 mbytes for @xmath1 cells per process + _ has the code been vectorized or parallelized ? </S>",
    "<S> : _ yes , using mpi + _ number of processors used : _ arbitrary number ( subject to some constraints ) . </S>",
    "<S> it has been tested from @xmath2 up to @xmath3 processors . </S>",
    "<S> + _ keywords : _ poisson s equation , poisson solver + _ classification : _ 4.3 differential equations + _ external routines / libraries : _ mpi ( http://www.mcs.anl.gov/mpi/ ) , fftw ( http://www.fftw.org ) , + silo ( https://wci.llnl.gov/codes/silo/ ) ( only necessary for running test problem ) + _ nature of problem : _ solving poisson s equation globally on unigrid mesh distributed across multiple processes on distributed memory system . </S>",
    "<S> + _ solution method : _ numerical solution using multidimensional discrete fourier transform in a parallel fortran 95 code . </S>",
    "<S> + _ unusual features : _ this code can be compiled as library to be readily linked and used as a black - box poisson solver with other codes . </S>",
    "<S> + _ running time : _ depends on the size of the problem , but typically less than 1 second per solve + </S>"
  ]
}