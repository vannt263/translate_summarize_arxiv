{
  "article_text": [
    "in the era of big data , the rapid expansion of computing capacities in automatic data generation and acquisition brings data of unprecedented size and complexity , and raises a series of scientific challenges such as storage bottleneck and algorithmic scalability @xcite . to overcome the difficulty , some approaches for generating scalable approximate algorithms",
    "have been introduced in the literature such as low - rank approximations of kernel matrices for kernel principal component analysis @xcite , incomplete cholesky decomposition @xcite , early - stopping of iterative optimization algorithms for gradient descent methods @xcite , and greedy - type algorithms .",
    "another method proposed recently is distributed learning based on a divide - and - conquer approach and a particular learning algorithm implemented in individual machines @xcite .",
    "this method produces distributed learning algorithms consisting of three steps : partitioning the data into disjoint subsets , applying a particular learning algorithm implemented in an individual machine to each data subset to produce an individual output ( function ) , and synthesizing a global output by utilizing some average of the individual outputs .",
    "this method can successfully reduce the time and memory costs , and its learning performance has been observed in many practical applications to be as good as that of a big machine which could process the whole data .",
    "theoretical attempts have been recently made in @xcite to derive learning rates for distributed learning with least squares regularization under certain assumptions .",
    "this paper aims at error analysis of the distributed learning with regularized least squares and its approximation to the algorithm processing the whole data in one single machine .",
    "recall @xcite that in a reproduced kernel hilbert space ( rkhs ) @xmath1 induced by a mercer kernel @xmath2 on an input metric space @xmath3 , with a sample @xmath4 where @xmath5 is the output space , the least squares regularization scheme can be stated as @xmath6 here @xmath7 is a regularization parameter and @xmath8 is the cardinality of @xmath9 .",
    "this learning algorithm is also called kernel ridge regression in statistics and has been well studied in learning theory .",
    "see e.g. @xcite .",
    "the regularization scheme ( [ all estimate ] ) can be explicitly solved by using a standard matrix inversion technique , which requires costs of @xmath10 in time and @xmath11 in memory . however",
    ", this matrix inversion technique may not conquer challenges on storages or computations arising from big data .",
    "the distributed learning algorithm studied in this paper starts with partitioning the data set @xmath9 into @xmath12 disjoint subsets @xmath13 .",
    "then it assigns each data subset @xmath14 to one machine or processor to produce a local estimator @xmath15 by the least squares regularization scheme ( [ all estimate ] ) .",
    "finally , these local estimators are communicated to a central processor , and a global estimator @xmath16 is synthesized by taking a weighted average @xmath17 of the local estimators @xmath18 .",
    "this algorithm has been studied with a matrix analysis approach in @xcite where some error analysis has been conducted under some eigenfunction assumptions for the integral operator associated with the kernel , presenting error bounds in expectation .    in this paper",
    "we shall use a novel integral operator approach to prove that @xmath16 is a good approximation of @xmath19 .",
    "we present a representation of the difference @xmath20 in terms of empirical integral operators , and analyze the error @xmath20 in expectation without any eigenfunction assumptions . as a by - product , we present the best learning rates for the least squares regularization scheme ( [ all estimate ] ) in a general setting , which surprisingly has not been done for a general kernel in the literature ( see detailed comparisons below ) .",
    "our analysis is carried out in the standard least squares regression framework with a borel probability measure @xmath21 on @xmath22 .",
    "the sample @xmath9 is independently drawn according to @xmath21 .",
    "a primary assumption in our integral operator approach is the compactness of the input metric space @xmath3 . under this assumption",
    ", the mercer kernel @xmath23 defines an integral operator @xmath24 on @xmath25 as @xmath26 where @xmath27 is the function @xmath28 in @xmath25 and @xmath29 is the marginal distribution of @xmath21 on @xmath3 .",
    "our error bounds in expectation for the distributed learning algorithm ( [ distrilearn ] ) require the uniform boundedness condition for the output @xmath30 , that is , for some constant @xmath31 , there holds @xmath32 almost surely .",
    "our bounds are stated in terms of the approximation error @xmath33 where @xmath34 is the data - free limit of ( [ all estimate ] ) defined by @xmath35 @xmath36 denotes the norm of @xmath37 , the hilbert space of square integrable functions with respect to @xmath29 , and @xmath38 is the regression function ( conditional mean ) of @xmath21 defined by @xmath39 with @xmath40 being the conditional distribution of @xmath21 induced at @xmath41 .",
    "since @xmath2 is continuous , symmetric and positive semidefinite , @xmath24 is a compact positive operator of trace class and @xmath42 is invertible .",
    "define a quantity measuring the complexity of @xmath25 with respect to @xmath29 , the _ effective dimension _ , to be the trace of the operator @xmath43 as @xmath44 in section [ proofs ] we shall prove the following first main result of this paper concerning error bounds in expectation of @xmath20 in @xmath25 and in @xmath37 .",
    "denote @xmath45 .",
    "[ mainexpected ] assume @xmath32 almost surely . if @xmath46 for @xmath47 , then we have @xmath48 and @xmath49 where @xmath50 is a constant depending only on @xmath51 .",
    "for minimax rates of sobolev spaces with @xmath52 , the parameters satisfy @xmath53 .",
    "a more general case with an arbitrary @xmath12 is covered as follows .",
    "[ mainequalsize ] assume @xmath32 almost surely . if @xmath46 for @xmath47 , and @xmath54 satisfies @xmath55 for some constant @xmath56 , then we have @xmath57 and @xmath58 where @xmath59 is a constant depending only on @xmath51 , @xmath60 , and the largest eigenvalue of @xmath24 .    in the special case that @xmath61 , the approximation error can be bounded as @xmath62 . a more general condition can be imposed for the regression function as @xmath63 where the integral operator @xmath64 is regarded as a compact positive operator on @xmath65 and its @xmath66th power",
    "is well defined for any @xmath67 .",
    "the condition ( [ approxr ] ) means @xmath38 lies in the range of @xmath68 , and the special case @xmath61 corresponds to the choice @xmath69 .",
    "we shall see in the proof given in section [ proofsconf ] that ( [ approxr ] ) with @xmath70 implies @xmath71 .",
    "the effective dimension of a sobolev space on a closed domain of a euclidean space with the uniform distribution @xmath29 satisfies @xmath72 for some @xmath73 .",
    "these facts together with corollary [ mainequalsize ] yield the following nice convergence rates for the distributed learning algorithm .",
    "[ specialequalsize ] assume ( [ approxr ] ) for some @xmath74 , @xmath32 almost surely and @xmath72 for some @xmath75 . if @xmath46 for @xmath47 with @xmath76 and @xmath77 , then we have @xmath78 and @xmath79 in particular , when @xmath61 and @xmath80 , the choice @xmath81 for the parameter yields @xmath82 and @xmath83      the second main result of this paper is a sharp error bound for the least squares regularization scheme ( [ all estimate ] ) .",
    "we can even relax the uniform boundedness to a moment condition that for some constant @xmath84 , @xmath85 where @xmath86 is the conditional variance defined by @xmath87 .    the following learning rates for the least squares regularization scheme ( [ all estimate ] )",
    "will be proved in section [ proofsconf ] .",
    "the existence of @xmath88 is ensured by @xmath89<\\infty$ ] .",
    "[ mainconfid ] assume @xmath89<\\infty$ ] and ( [ uniformbound ] ) for some @xmath90 .",
    "then we have @xmath91 \\leq \\left(1 + 58\\kappa^4   + 58\\kappa^2\\right ) ( 1 + \\kappa ) \\left(1 + \\frac{1}{(n\\lambda)^2 } + \\frac{{\\mathcal n}(\\lambda)}{n\\lambda}\\right )   \\nonumber \\\\ & & \\quad \\biggl\\{\\biggl(1 +    \\frac{1}{\\sqrt{n\\lambda}}\\biggr ) \\|f_\\lambda - f_\\rho\\|_\\rho + \\sqrt{\\left\\|\\sigma^2_\\rho\\right\\|_p }   \\left(\\frac{{\\mathcal n}(\\lambda)}{n}\\right)^{\\frac{1}{2 } ( 1-\\frac{1}{p } ) } \\left(\\frac{1}{n\\lambda}\\right)^{\\frac{1}{2p}}\\biggr\\}. \\label{confboundgen}\\end{aligned}\\ ] ]    if the parameters satisfy @xmath53 , we have the following explicit bound .    [ specialpara ]",
    "assume @xmath89<\\infty$ ] and ( [ uniformbound ] ) for some @xmath90 .",
    "if @xmath54 satisfies ( [ lambdacond ] ) with @xmath52 , then we have @xmath92 = o\\left(\\|f_\\lambda - f_\\rho\\|_\\rho + \\left(\\frac{{\\mathcal n}(\\lambda)}{n}\\right)^{\\frac{1}{2 } ( 1-\\frac{1}{p } ) } \\left(\\frac{1}{n\\lambda}\\right)^{\\frac{1}{2p}}\\right).\\ ] ] in particular , if @xmath93 , that is , the conditional variances are uniformly bounded , then @xmath92 = o\\left(\\|f_\\lambda - f_\\rho\\|_\\rho + \\sqrt{\\frac{{\\mathcal n}(\\lambda)}{n}}\\right).\\ ] ]    in particular , when ( [ approxr ] ) is satisfied , we have the following learning rates .",
    "[ mainminmax ] assume @xmath89<\\infty$ ] , ( [ uniformbound ] ) for some @xmath90 , and ( [ approxr ] ) for some @xmath70 . if @xmath72 for some @xmath75 , then by taking @xmath94 we have @xmath92 = o\\left(n^{- \\frac{2r \\alpha}{2 \\alpha \\max\\{2r , 1\\ } + 1 } + \\frac{1}{2p }   \\frac{2 \\alpha -1}{2 \\alpha \\max\\{2r , 1\\ } + 1}}\\right).\\ ] ] in particular , when @xmath93 ( the conditional variances are uniformly bounded ) , we have @xmath92 = o\\left(n^{-\\frac{2 r \\alpha}{2 \\alpha \\max\\{2r , 1\\ } + 1}}\\right).\\ ] ]    combing bounds for @xmath95 and @xmath96 , we can derive learning rates for the distributed learning algorithm ( [ distrilearn ] ) for regression .",
    "[ finalrate ] assume @xmath32 almost surely and ( [ approxr ] ) for some @xmath97 . if @xmath72 for some @xmath75 , @xmath46 for @xmath47 , and @xmath12 satisfies the restriction @xmath98 then by taking @xmath99 , we have @xmath100 = o\\left(n^{-\\frac{2 \\alpha r}{4 \\alpha r + 1}}\\right).\\ ] ]",
    "the least squares regularization scheme ( [ all estimate ] ) is a classical algorithm for regression and has been extensively investigated in statistics and learning theory .",
    "there is a vast literature on this topic . here for a general kernel beyond the sobolev kernels , we compare our results with the best learning rates in the existing literature .",
    "denote the set of positive eigenvalues of @xmath24 as @xmath101 arranged in a decreasing order , and a set of normalized ( in @xmath25 ) eigenfunctions @xmath102 of @xmath24 corresponding to the eigenvalues @xmath101 .    under the assumption that the orthogonal projection @xmath103 of @xmath38 in @xmath65 onto the closure of @xmath25 satisfies ( [ approxr ] ) for some @xmath104 , and that the eigenvalues @xmath105 satisfy @xmath106 with some @xmath107 , it was proved in @xcite that @xmath108 = 0,\\ ] ] where @xmath109 and @xmath110 denotes a set of probability measures @xmath21 satisfying some moment decay condition ( which is satisfied when @xmath32 ) .",
    "this learning rate is suboptimal due to the limitation taken for @xmath111 and the logarithmic factor in the case @xmath112 . in particular , to have @xmath113 with confidence @xmath114 , one needs to restrict @xmath115 to be large enough and has the constant @xmath116 depending on @xmath117 to be large enough .",
    "our learning rates in corollary [ mainminmax ] have the advantages of holding in expectation and having no logarithmic factor in the case @xmath112 .    under the assumption that @xmath32 almost surely , the eigenvalues @xmath105 satisfy @xmath118 with some @xmath107 and @xmath119 , and for some constant @xmath120 , the pair @xmath121 satisfies @xmath122 for every @xmath123 , it was proved in @xcite that for some constant @xmath124 depending only on @xmath125 and @xmath126 , with confidence @xmath114 , for any @xmath127 , @xmath128 here @xmath129 is the projection onto the interval @xmath130 $ ] defined @xcite by @xmath131 and @xmath132 is the approximation error defined by @xmath133 when @xmath61 , @xmath134 and the choice @xmath135 gives a learning rate of order @xmath136 .",
    "but one needs to impose the condition ( [ interpolationc ] ) for the functions spaces @xmath65 and @xmath25 , and to take the projection onto @xmath130 $ ] .",
    "our learning rates in corollary [ mainminmax ] do not require such a condition for the function spaces , nor do we take the projection . learning rates for the least squares regularization scheme ( [ all estimate ] ) in the @xmath25-metric",
    "have been investigated in the literature @xcite .    for the distributed learning algorithm ( [ distrilearn ] ) with subsets @xmath13 of equal size , under the assumption that for some constants @xmath137 and @xmath138 , the eigenfunctions @xmath102",
    "satisfy @xmath139 \\leq a^{2 k } , \\qquad i=1 , 2 , \\ldots,\\ ] ] that @xmath61 and @xmath118 for some @xmath107 and @xmath119 , it was proved in @xcite that for @xmath140 and @xmath12 satisfies the restriction @xmath141 with a constant @xmath142 depending only on @xmath125 , there holds @xmath143 = o\\left(n^{-\\frac{2\\alpha}{2 \\alpha + 1}}\\right)$ ] .",
    "this interesting result was achieved by a matrix analysis approach for which the eigenfunction assumption ( [ eigenfcnc ] ) played an essential role .",
    "this eigenfunction assumption generalizes the classical case that the eigenfunctions are uniformly bounded : @xmath144 .",
    "an example of a @xmath145 mercer kernel was presented in @xcite to show that smoothness of the mercer kernel does not guarantee the uniform boundedness of the eigenfunctions .",
    "our learning rates stated in corollary [ specialequalsize ] do not require such an eigenfunction assumption . also , our restriction ( [ mrestrict ] ) for the number @xmath12 of local processors is more general when @xmath125 is close to @xmath146 .",
    "notice that the learning rates stated in corollary [ specialequalsize ] are for the difference @xmath20 between the output function of the distributed learning algorithm ( [ distrilearn ] ) and that of the algorithm ( [ all estimate ] ) using the whole data . in the special case of @xmath112",
    ", we can see that @xmath147 , achieved by choosing @xmath81 , is smaller as @xmath12 becomes larger .",
    "this is natural because the error @xmath148 reflects more the sample error and should become smaller when we use more local processors . on the other hand , as one expects , increasing the number @xmath12 of local processors would increase the approximation error for the regression problem , which can also be seen from the bound with @xmath81 stated in theorem [ mainconfid ] .",
    "the result in corollary [ finalrate ] with @xmath149 compensates and gives the best learning rate @xmath150 = o\\left(n^{-\\frac{2 r \\alpha}{4 \\alpha r + 1}}\\right)$ ] by restricting @xmath12 as in ( [ restrictmfinal ] ) .    in this paper",
    "we only consider distributed learning with the regularized least squares ( [ all estimate ] ) .",
    "it would be nice to extend our analysis to distributed learning with other learning algorithms @xcite .",
    "our error estimates are achieved by a novel second order decomposition of operator differences in our integral operator approach .",
    "we approximate the integral operator @xmath24 by the empirical integral operator @xmath151 on @xmath25 defined with the input data set @xmath152 as @xmath153 where the reproducing property @xmath154 for @xmath123 is used .",
    "since @xmath2 is a mercer kernel , @xmath155 is a finite - rank positive operator and @xmath156 is invertible .",
    "the operator difference in our study is @xmath157 with @xmath158 and @xmath159 .",
    "our second order decomposition for the difference @xmath157 is stated as follows .",
    "[ operatordifference ] let @xmath160 and @xmath161 be invertible operators on a banach space .",
    "then we have @xmath162 in particular , we have @xmath163    we can decompose the operator @xmath157 as @xmath164 this is the first order decomposition .",
    "write the last term @xmath165 as @xmath166 and apply another first order decomposition similar to ( [ operatordiffii ] ) as @xmath167 it follows from ( [ operatordiffii ] ) that @xmath168 then the desired identity ( [ operatordiffform ] ) is verified .",
    "the lemma is proved .    to estimate norms of various operators involving the approximation of @xmath24 by @xmath151 , we need the following probability inequality for vector - valued random variables in @xcite .    [ pinelislemma ] for a random variable @xmath169 on @xmath170 with values in a hilbert space @xmath171 satisfying @xmath172 almost surely , and a random sample @xmath173 independent drawn according to @xmath21 , there holds with confidence @xmath174 , @xmath175\\biggr\\| \\leq { 2 \\widetilde{m } \\log \\bigl(2/\\widetilde{\\delta}\\bigr ) \\over s }   + \\sqrt{{2 e ( \\|\\xi\\|^2 ) \\log \\bigl(2/\\widetilde{\\delta}\\bigr ) \\over s}}.\\ ] ]",
    "note that @xmath176 and the @xmath66th power of the compact positive operator @xmath42 or @xmath156 is well defined for any @xmath177 .",
    "the following lemma provides estimates for the operator @xmath178 in the second order decomposition ( [ secondorder ] ) . as in @xcite",
    ", we use effective dimensions defined by ( [ effectdim ] ) to estimate operator norms .",
    "[ keynormsoperator ] let @xmath9 be a sample drawn independently according to @xmath21 .",
    "then the following estimates for the operator norm @xmath179 hold .",
    "\\(a ) @xmath180 \\leq \\frac{\\kappa^2 { \\mathcal n}(\\lambda)}{|d|}.$ ]    \\(b ) for any @xmath181 , with confidence at least @xmath182 , there holds @xmath183 where we denote the constant @xmath184    \\(c ) for any @xmath185 , there holds @xmath186\\right\\}^{\\frac{1}{d } } \\leq   ( 2 d \\gamma ( d))^{\\frac{1}{d } } { \\mathcal b}_{|d| , \\lambda},\\ ] ] where @xmath187 is the gamma function defined for @xmath188 by @xmath189 .",
    "we apply lemma ( [ pinelislemma ] ) to the random variable @xmath190 defined by @xmath191 it takes values in @xmath192 , the hilbert space of hilbert - schmidt operators on @xmath25 , with inner product @xmath193 here tr denotes the trace of a ( trace - class ) linear operator .",
    "the norm is given by @xmath194 where @xmath195 is an orthonormal basis of @xmath25 .",
    "the space @xmath196 is a subspace of the space of bounded linear operators on @xmath197 , denoted as @xmath198 , with the norm relations @xmath199    now we use effective dimensions to estimate norms involving @xmath190 .",
    "the random variable @xmath190 defined by ( [ eta1 ] ) has mean @xmath200 and sample mean @xmath201 .",
    "recall the set of normalized ( in @xmath25 ) eigenfunctions @xmath102 of @xmath24 .",
    "it is an orthonormal basis of @xmath25 .",
    "if we regard @xmath24 as an operator on @xmath65 , the normalized eigenfunctions in @xmath65 are @xmath202 and they form an orthonormal basis of the orthogonal complement of the eigenspace associated with eigenvalue @xmath203 . by the mercer theorem , we have the following uniform convergent mercer expansion @xmath204 take the orthonormal basis @xmath102 of @xmath25 . by the definition of the hs norm",
    ", we have @xmath205 for a fixed @xmath206 , @xmath207 and @xmath208 can be expended by the orthonormal basis @xmath209 as @xmath210 hence @xmath211 combining this with ( [ mercer ] ) , we see that @xmath212 and @xmath213   \\leq   \\kappa^2 e\\left[\\sum_\\ell \\frac{\\left(\\varphi_\\ell ( x)\\right)^2}{\\lambda_\\ell + \\lambda}\\right ] = \\kappa^2 \\sum_\\ell \\frac{\\int_{\\mathcal x } \\left(\\varphi_\\ell(x)\\right)^2 d \\rho_x}{\\lambda_\\ell + \\lambda}.\\end{aligned}\\ ] ] but @xmath214",
    "so we have @xmath215   \\leq \\kappa^2 \\sum_\\ell \\frac{\\lambda_\\ell}{\\lambda_\\ell + \\lambda } = \\kappa^2 \\hbox{tr}\\left(\\left(l_k+\\lambda i\\right)^{-1 } l_k\\right ) = \\kappa^2 { \\mathcal n}(\\lambda)\\ ] ] and @xmath216\\right\\|^2_{hs } = e\\left[\\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\left\\{l_k- l_{k , d(x)}\\right\\}\\right\\|_{hs}^2\\right ] \\leq \\frac{\\kappa^2 { \\mathcal n}(\\lambda)}{|d|}.\\ ] ] then our desired inequality in part ( a ) follows from the first inequality of ( [ normrelation ] ) .    from ( [ expandkx ] ) and ( [ eta1bound ] ) , we find a bound for @xmath190 as @xmath217 applying lemma [ pinelislemma ] to the random variable @xmath190 with @xmath218 , we know by ( [ normrelation ] ) that with confidence at least @xmath182 , @xmath219 - \\frac{1}{|d| } \\sum_{x\\in d(x ) } \\eta_1 ( x)\\right\\| & \\leq & \\left\\|e[\\eta_1 ] - \\frac{1}{|d| } \\sum_{x\\in d(x ) } \\eta_1 ( x)\\right\\|_{hs } \\\\   & \\leq & { 2 \\kappa^2 \\log \\bigl(2/\\delta\\bigr ) \\over |d|\\sqrt{\\lambda } }   + \\sqrt{\\frac{2 \\kappa^2 { \\mathcal n}(\\lambda)\\log \\bigl(2/\\delta\\bigr)}{|d|}}.\\end{aligned}\\ ] ] writing the above bound by taking a factor @xmath220 , we get the desired bound ( [ normprepareeta2 ] ) .    recall @xmath221 defined by ( [ bdef ] ) .",
    "apply the formula @xmath222",
    "= \\int_0^\\infty \\hbox{prob}\\left[\\xi > t\\right ] d t\\ ] ] for nonnegative random variables to @xmath223 and use the bound @xmath224 = \\hbox{prob}\\left[\\xi^{\\frac{1}{d } } > t^{\\frac{1}{d}}\\right ] \\leq 2 \\exp\\left\\{-\\frac{t^{\\frac{1}{d}}}{{\\mathcal b}_{|d| , \\lambda}}\\right\\}\\ ] ] derived from ( [ normprepareeta2 ] ) .",
    "we find @xmath225 \\leq \\int_0^\\infty 2 \\exp\\left\\{-\\frac{t^{\\frac{1}{d}}}{{\\mathcal b}_{|d| , \\lambda}}\\right\\ } d t\\ ] ] which equals @xmath226 .",
    "then the desired bound in part ( c ) follows from @xmath227 and the lemma is proved .    to analyze the error @xmath20",
    ", we need the following representation in terms of the difference of inverse operators denoted by @xmath228 and @xmath229 for the data subset @xmath14 . the empirical integral operator @xmath155 is defined with @xmath9 replaced by the data subset @xmath14 .    define two random variables @xmath230 and @xmath231 with values in the hilbert space @xmath25 by @xmath232    [ difference ] assume @xmath89 < \\infty$ ] . for @xmath7 , we have @xmath233 \\delta_j \\nonumber \\\\        & = & \\sum_{j=1}^m \\frac{|d_j|}{|d| } \\left[q_{d_j(x)}\\right ] \\delta'_j + \\sum_{j=1}^m \\frac{|d_j|}{|d| } \\left[q_{d_j(x)}\\right ] \\delta''_j   - \\left[q_{d(x)}\\right ] \\delta_d , \\label{expressdifflk}\\end{aligned}\\ ] ] where @xmath234 , \\quad \\delta_d : = \\frac{1}{|d| } \\sum_{z \\in d } \\xi_\\lambda ( z ) - e[\\xi_\\lambda],\\ ] ] and @xmath235.\\ ] ]    a well known formula ( see e.g. @xcite ) asserts that @xmath236 so we know that @xmath237 also , with the whole data @xmath9 , we have @xmath238 but @xmath239 = \\sum_{j=1}^m \\frac{|d_j|}{|d| } \\left\\{\\frac{1}{|d_j| } \\sum_{z \\in d_j } \\xi_\\lambda ( z ) - e[\\xi_\\lambda]\\right\\ } = \\sum_{j=1}^m \\frac{|d_j|}{|d| } \\delta_j.\\ ] ] hence @xmath240 then the first desired expression for @xmath20 follows .    by adding and subtracting the operator @xmath241 , writing @xmath242 , and noting @xmath243=0 $ ] , we know that the first expression implies ( [ expressdifflk ] ) .",
    "this proves lemma [ difference ] .",
    "to apply ( [ expressdifflk ] ) for our error analysis , we also need to bound norms involving @xmath244 and @xmath245 .",
    "we are able to give the following estimates even after multiplying with @xmath246 taken from the operator @xmath247 or @xmath248 .",
    "[ keynorms ] let @xmath9 be a sample drawn independently according to @xmath21 and @xmath249 be a measurable bounded function on @xmath250 and @xmath251 be the random variable with values on @xmath25 given by @xmath252 for @xmath253 .",
    "then the following statements hold .",
    "\\(a ) @xmath254 = { \\mathcal n}(\\lambda).$ ]    \\(b ) for any @xmath181 , with confidence at least @xmath182 , there holds @xmath255\\right)\\right\\|_k   \\leq \\frac{2",
    "\\|g\\|_\\infty \\log \\bigl(2/\\delta\\bigr ) } { \\sqrt{|d| } } \\left\\{\\frac{\\kappa } { \\sqrt{|d|\\lambda } } + \\sqrt{{\\mathcal n}(\\lambda)}\\right\\}.\\end{aligned}\\ ] ]    consider the random variable @xmath256 defined by @xmath257 it takes values in @xmath25 . by",
    "( [ expandkx ] ) , it satisfies @xmath258 so @xmath259 = e\\left[\\sum_{\\ell } \\frac{(\\varphi_\\ell ( x))^2}{\\lambda_\\ell + \\lambda}\\right ] = { \\mathcal n}(\\lambda).\\ ] ] this is the statement of part ( a ) .    for part ( b )",
    ", we consider another random variable @xmath260 defined by @xmath261 it takes values in @xmath25 and satisfies @xmath262 so @xmath263 and @xmath264   \\leq \\|g\\|_\\infty^2 e\\left[\\sum_\\ell \\frac{\\left(\\varphi_\\ell ( x)\\right)^2}{\\lambda_\\ell + \\lambda}\\right ] = \\|g\\|_\\infty^2 { \\mathcal n}(\\lambda).\\ ] ] applying lemma [ pinelislemma ] proves the statement in part ( b ) .",
    "to illustrate how to apply the second order decomposition ( [ secondorder ] ) for operator differences in our integral operator approach , we prove in this section our main result on error bounds for the least squares regularization scheme ( [ all estimate ] ) .",
    "[ mainpropls ] assume @xmath89 < \\infty$ ] and ( [ uniformbound ] ) for some @xmath90",
    ". then we have @xmath265 & \\leq & \\left(1 + 58\\kappa^4   + 58\\kappa^2\\right ) \\left(1 + \\frac{1}{(|d|\\lambda)^2 } + \\frac{{\\mathcal n}(\\lambda)}{|d|\\lambda}\\right ) \\\\ & & \\left\\{\\kappa^{\\frac{1}{p}}\\sqrt{\\left\\|\\sigma^2_\\rho\\right\\|_p }   \\left(\\frac{{\\mathcal n}(\\lambda)}{|d|}\\right)^{\\frac{1}{2 } ( 1-\\frac{1}{p } ) } \\left(\\frac{1}{|d|\\lambda}\\right)^{\\frac{1}{2p } } + \\kappa   \\frac{\\|f_\\lambda - f_\\rho\\|_\\rho}{\\sqrt{|d|\\lambda}}\\right\\}.\\end{aligned}\\ ] ]    we recall the expression ( [ fdlambda ] ) for @xmath266 and the notation @xmath267 defined by ( [ qdnotation ] ) for the operator difference @xmath268 .",
    "then we see @xmath269 \\delta_d + \\left(l_{k}+\\lambda        i\\right)^{-1 } \\delta_d.\\ ] ] to estimate the @xmath65 norm , we use the identity @xmath270 and get @xmath271 \\delta_d\\right\\|_k + \\left\\|l_k^{1/2}\\left(l_{k}+\\lambda        i\\right)^{-1 } \\delta_d\\right\\|_k.\\ ] ]    we apply the second order decomposition ( [ secondorder ] ) , use the bounds @xmath272 , @xmath273 , @xmath274 , and know that @xmath275 \\delta_d\\right\\|_k \\leq \\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\left\\{l_k- l_{k , d(x)}\\right\\ } \\left(l_{k } + \\lambda i\\right)^{-1 } \\delta_d\\right\\|_k + \\\\ & & \\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\left\\{l_k- l_{k , d(x)}\\right\\ }   \\left(l_{k , d(x)}+\\lambda        i\\right)^{-1 } \\left\\{l_k- l_{k , d(x)}\\right\\ } \\left(l_{k } + \\lambda i\\right)^{-1 } \\delta_d\\right\\|_k \\\\ & & \\leq \\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\left\\{l_k- l_{k , d(x)}\\right\\}\\right\\| \\frac{1}{\\sqrt{\\lambda } } \\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\delta_d\\right\\|_k + \\\\ & & \\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\left\\{l_k- l_{k , d(x)}\\right\\}\\right\\| \\frac{1}{\\lambda } \\left\\|\\left\\{l_k- l_{k , d(x)}\\right\\}\\left(l_{k } + \\lambda i\\right)^{-1/2}\\right\\| \\biggl\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\delta_d\\biggr\\|_k.\\end{aligned}\\ ] ] for convenience , we introduce the notation @xmath276 then the above bound can be restated as @xmath277 \\delta_d\\right\\|_k \\leq \\left(\\frac{\\xi_d}{\\sqrt{\\lambda } } + \\frac{\\xi_d^2}{\\lambda}\\right)\\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\delta_d\\right\\|_k.\\ ] ] hence @xmath278 and by the schwarz inequality we have @xmath279 \\leq \\left\\{e\\left[\\left(1 + \\frac{\\xi_d}{\\sqrt{\\lambda } } + \\frac{\\xi_d^2}{\\lambda}\\right)^2\\right]\\right\\}^{1/2 } \\left\\{e\\left[\\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\delta_d\\right\\|_k^2\\right]\\right\\}^{1/2}.\\ ] ]    to deal with the expected value in ( [ firstdecom ] ) , as in lemma [ difference ] , we separate @xmath245 as @xmath280 where @xmath281.\\ ] ] then @xmath282\\right\\}^{1/2 } \\nonumber \\\\ & & \\leq \\left\\{e\\left[\\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\delta'_d\\right\\|_k^2\\right]\\right\\}^{1/2 } + \\left\\{e\\left[\\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\delta''_d\\right\\|_k^2\\right]\\right\\}^{1/2}. \\label{separate}\\end{aligned}\\ ] ] observe that @xmath283 each term in this expression is unbiased because @xmath284 .",
    "this unbiasedness and the independence tell us that @xmath285\\right\\}^{1/2 } & = & \\left\\{\\frac{1}{|d| } e\\left[\\left\\|(y - f_\\rho ( x))\\left[\\left(l_{k } + \\lambda i\\right)^{-1/2}\\right ] ( k_x)\\right\\|_k^2\\right]\\right\\}^{1/2 } \\nonumber \\\\ & = & \\left\\{\\frac{1}{|d| } e\\left[\\sigma^2_\\rho ( x ) \\left\\|\\left[\\left(l_{k } + \\lambda i\\right)^{-1/2}\\right ] ( k_x)\\right\\|_k^2\\right]\\right\\}^{1/2}. \\label{deltad'}\\end{aligned}\\ ] ]    if @xmath286 , then @xmath287 and by lemma [ keynorms ] we have @xmath288\\right\\}^{1/2 }   \\leq \\sqrt{\\left\\|\\sigma^2_\\rho\\right\\|_\\infty } \\sqrt{{\\mathcal n}(\\lambda)/|d|}.\\ ] ]    if @xmath289 with @xmath290 , we take @xmath291 ( @xmath292 for @xmath293 ) satisfying @xmath294 and apply the hlder inequality @xmath295 \\leq \\left(e[|\\xi|^p]\\right)^{1/p } \\left(e[|\\eta|^{q}]\\right)^{1/q}$ ] to @xmath296 , @xmath297 to find @xmath298 ( k_x)\\right\\|_k^2\\right ] \\leq \\left\\|\\sigma^2_\\rho\\right\\|_{p } \\left\\{e\\left[\\left\\|\\left[\\left(l_{k } + \\lambda i\\right)^{-1/2}\\right ] ( k_x)\\right\\|_k^{2q}\\right]\\right\\}^{1/q}.\\ ] ] but @xmath299 ( k_x)\\right\\|_k^{2q-2 } \\leq \\left(\\kappa/\\sqrt{\\lambda}\\right)^{2 q -2}\\ ] ] and @xmath300= { \\mathcal n}(\\lambda)$ ] by lemma [ keynorms ] .",
    "so we have @xmath285\\right\\}^{1/2 }   & \\leq & \\left\\{\\frac{1}{|d| } \\left\\|\\sigma^2_\\rho\\right\\|_{p } \\left\\{\\frac{\\kappa^{2q -2}}{\\lambda^{q-1 } } { \\mathcal n}(\\lambda)\\right\\}^{1/q}\\right\\}^{1/2 } \\\\ & = & \\sqrt{\\left\\|\\sigma^2_\\rho\\right\\|_p }",
    "\\kappa^{\\frac{1}{p } } \\left(\\frac{{\\mathcal n}(\\lambda)}{|d|}\\right)^{\\frac{1}{2 } ( 1-\\frac{1}{p } ) } \\left(\\frac{1}{|d|\\lambda}\\right)^{\\frac{1}{2p}}.\\end{aligned}\\ ] ] combining the above two cases , we know that for either @xmath93 or @xmath301 , @xmath288\\right\\}^{1/2 } \\leq \\sqrt{\\left\\|\\sigma^2_\\rho\\right\\|_p } \\kappa^{\\frac{1}{p } } \\left(\\frac{{\\mathcal n}(\\lambda)}{|d|}\\right)^{\\frac{1}{2 } ( 1-\\frac{1}{p } ) } \\left(\\frac{1}{|d|\\lambda}\\right)^{\\frac{1}{2p}}.\\ ] ]    the second term of ( [ separate ] ) can be bounded easily as @xmath302\\right\\}^{1/2 } & \\leq & \\frac{1}{\\sqrt{|d| } } \\left\\{e\\left[\\left(f_\\rho ( x ) - f_\\lambda ( x)\\right)^2 \\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } ( k_x ) \\right\\|_k^2\\right]\\right\\}^{1/2 } \\\\ & \\leq & \\frac{1}{\\sqrt{|d| } } \\left\\{e\\left[\\left(f_\\rho ( x ) - f_\\lambda ( x)\\right)^2 \\frac{\\kappa^2}{\\lambda}\\right]\\right\\}^{1/2 } = \\frac{\\kappa \\|f_\\rho - f_\\lambda\\|_\\rho}{\\sqrt{|d|\\lambda}}.\\end{aligned}\\ ] ] putting the above estimates for the two terms of ( [ separate ] ) into ( [ firstdecom ] ) and applying lemma [ keynormsoperator ] to get @xmath303\\right\\}^{1/2 } & \\leq & 1 + \\left\\{e\\left[\\frac{\\xi_d^2}{\\lambda}\\right]\\right\\}^{1/2 } +   \\left\\{e\\left[\\frac{\\xi_d^4}{\\lambda^2}\\right]\\right\\}^{1/2 } \\\\   & \\leq & 1 + \\left\\{\\frac{\\kappa^2 { \\mathcal n}(\\lambda)}{|d|\\lambda}\\right\\}^{1/2 } +   \\left\\{\\frac{48 { \\mathcal b}_{|d| , \\lambda}^4}{\\lambda^2}\\right\\}^{1/2 } \\\\   & \\leq & 1 + \\frac{58\\kappa^4}{(|d|\\lambda)^2 } + \\frac{58\\kappa^2 { \\mathcal n}(\\lambda)}{|d|\\lambda},\\end{aligned}\\ ] ] we know that @xmath304 $ ] is bounded by @xmath305 then our desired error bound follows .",
    "the proof of the proposition is complete .",
    "* proof of theorem [ mainconfid ]  *    combining proposition [ mainpropls ] with the triangle inequality @xmath306 , we know that @xmath307 & \\leq & \\|f_\\lambda - f_\\rho\\|_\\rho + \\left(1 + 58\\kappa^4   + 58\\kappa^2\\right ) \\left(1 + \\frac{1}{(|d|\\lambda)^2 } + \\frac{{\\mathcal n}(\\lambda)}{|d|\\lambda}\\right ) \\\\ & & \\left\\{\\kappa^{\\frac{1}{p}}\\sqrt{\\left\\|\\sigma^2_\\rho\\right\\|_p } \\left(\\frac{{\\mathcal n}(\\lambda)}{|d|}\\right)^{\\frac{1}{2 } ( 1-\\frac{1}{p } ) } \\left(\\frac{1}{|d|\\lambda}\\right)^{\\frac{1}{2p } } + \\frac{\\kappa}{\\sqrt{|d|\\lambda } }   \\|f_\\lambda - f_\\rho\\|_\\rho\\right\\}.\\end{aligned}\\ ] ] then the desired error bound holds true , and the proof of theorem [ mainconfid ] is complete .",
    "* proof of corollary [ specialpara ]  *    by the definition of effective dimension , @xmath308 combining this with the restriction ( [ lambdacond ] ) with @xmath52 , we find @xmath309 and @xmath310 . putting these and the restriction ( [ lambdacond ] ) with @xmath52 into the error bound ( [ confboundgen ] ) ,",
    "we know that @xmath91 \\leq \\left(1 + 58\\kappa^4   + 58\\kappa^2\\right ) ( 1 + \\kappa ) \\left(1 + \\frac{(\\lambda_1 + c_0)^2c_0 ^ 2}{\\lambda_1 ^ 2 } + c_0\\right ) \\nonumber \\\\ & & \\biggl\\{\\biggl(1 +    \\sqrt{(\\lambda_1 + c_0 ) c_0/\\lambda_1}\\biggr ) \\|f_\\lambda - f_\\rho\\|_\\rho + \\sqrt{\\left\\|\\sigma^2_\\rho\\right\\|_p } \\left(\\frac{{\\mathcal n}(\\lambda)}{n}\\right)^{\\frac{1}{2 } ( 1-\\frac{1}{p } ) } \\left(\\frac{1}{n\\lambda}\\right)^{\\frac{1}{2p}}\\biggr\\}.\\end{aligned}\\ ] ] then the desired bound follows .",
    "the proof of corollary [ specialpara ] is complete .    *",
    "proof of corollary [ mainminmax ]  *    the representation @xmath311 for @xmath88 , with @xmath312 , tells us the well known fact that the condition ( [ approxr ] ) with @xmath70 yields a bound for @xmath313 as @xmath314    if @xmath315 for some constant @xmath316 , then the choice @xmath94 yields @xmath317 so ( [ lambdacond ] ) with @xmath52 is satisfied . with this choice",
    "we also have @xmath318 putting these estimates into corollary [ specialpara ] , we know that @xmath307 & = & o\\left(n^{-\\frac{2\\alpha r}{2 \\alpha \\max\\{2r , 1\\}+1 } } + n^{- \\frac{\\alpha \\max\\{2r , 1\\}}{2 \\alpha \\max\\{2r , 1\\ } + 1 } + \\frac{1}{2p }   \\frac{2 \\alpha -1}{2 \\alpha \\max\\{2r , 1\\ } + 1}}\\right ) \\\\ & = & o\\left(n^{- \\frac{\\alpha \\min\\left\\{2 r , \\max\\{2r , 1\\}\\right\\}}{2 \\alpha \\max\\{2r , 1\\ } + 1 } + \\frac{1}{2p }   \\frac{2 \\alpha -1}{2 \\alpha \\max\\{2r , 1\\ } + 1}}\\right).\\end{aligned}\\ ] ] but we find @xmath319 by discussing the two different cases @xmath320 and @xmath321 .",
    "then our conclusion follows immediately .",
    "the proof of corollary [ mainminmax ] is complete .",
    "in this section , we prove our first main result on the error @xmath20 in the @xmath25 metric and @xmath322 metric .",
    "the following result is more general , allowing different sizes for data subsets @xmath323 .",
    "[ mainexpectedgeneral ] assume that for some constant @xmath324 , @xmath325 almost surely .",
    "then we have @xmath326 \\leq c'_\\kappa \\left(\\frac{1}{(n \\lambda)^2 } + \\frac{{\\mathcal n}(\\lambda)}{n\\lambda}\\right ) \\biggl\\{\\frac{\\|f_\\lambda - f_\\rho\\|_\\rho}{\\sqrt{n\\lambda } } \\sum_{j=1}^m \\left(\\frac{|d|}{|d_j|}\\right)^{\\frac{3}{2 } } + m \\sqrt{\\lambda } \\sqrt{\\frac{{\\mathcal n}(\\lambda)}{n \\lambda}}\\biggr\\ } \\\\ & & +   c'_\\kappa m \\sqrt{\\lambda } \\left\\{\\sum_{j=1}^m \\left(\\frac{|d_j|}{|d|}\\right)^2 \\left(\\frac{1}{|d_j|^2 \\lambda^2 } + \\frac{{\\mathcal n}(\\lambda)}{|d_j|\\lambda}\\right)^2 \\left\\{1 + \\left(\\frac{1}{|d_j|^2 \\lambda^2 } + \\frac{{\\mathcal n}(\\lambda)}{|d_j|\\lambda}\\right)^2 \\right\\}\\right\\}^{1/2}\\end{aligned}\\ ] ] and @xmath327 \\leq",
    "c'_\\kappa \\left(\\frac{1}{(n \\lambda)^2 } + \\frac{{\\mathcal n}(\\lambda)}{n\\lambda}\\right ) \\biggl\\{\\frac{\\|f_\\lambda - f_\\rho\\|_\\rho}{\\sqrt{n}\\lambda } \\sum_{j=1}^m \\left(\\frac{|d|}{|d_j|}\\right)^{\\frac{3}{2 } } + m \\sqrt{\\frac{{\\mathcal n}(\\lambda)}{n \\lambda}}\\biggr\\ } \\\\ & & +   c'_\\kappa m",
    "\\left\\{\\sum_{j=1}^m \\left(\\frac{|d_j|}{|d|}\\right)^2 \\left(\\frac{1}{|d_j|^2 \\lambda^2 } + \\frac{{\\mathcal n}(\\lambda)}{|d_j|\\lambda}\\right)^2 \\left\\{1 + \\left(\\frac{1}{|d_j|^2 \\lambda^2 } + \\frac{{\\mathcal n}(\\lambda)}{|d_j|\\lambda}\\right)^2 \\right\\}\\right\\}^{1/2},\\end{aligned}\\ ] ] where @xmath328 is a constant depending only on @xmath51 .    recall ( [ expressdifflk ] ) in lemma [ difference ] .",
    "it enables us to express @xmath329 where the terms @xmath330 are given by @xmath331 \\delta'_j , \\",
    "j_2 = \\sum_{j=1}^m \\frac{|d_j|}{|d| } \\left[l_k^{1/2}q_{d_j(x)}\\right ] \\delta''_j , \\ j_3 = -\\left[l_k^{1/2}q_{d(x)}\\right ] \\delta_d.\\ ] ] these three terms will be dealt with separately in the following .",
    "for the first term @xmath332 of ( [ fdrhodecom ] ) , each summand with @xmath333 can be expressed as @xmath334 ( k_x)$ ] , and is unbiased because @xmath284 .",
    "the unbiasedness and the independence tell us that @xmath335 \\leq \\left\\{e\\left[\\left\\|j_1\\right\\|_k^2\\right]\\right\\}^{1/2 } \\leq \\left\\{\\sum_{j=1}^m \\left(\\frac{|d_j|}{|d|}\\right)^2 e\\left[\\left\\|\\left[l_k^{1/2}q_{d_j(x)}\\right ] \\delta'_j\\right\\|_k^2\\right]\\right\\}^{1/2}.\\end{aligned}\\ ] ] let @xmath333 .",
    "the relation ( [ fdlambdafirst ] ) derived from the second order decomposition ( [ secondorder ] ) in the proof of proposition [ mainpropls ] yields @xmath336 \\delta'_j\\right\\|_k^2 \\leq \\left(\\frac{\\xi_{d_j}}{\\sqrt{\\lambda } } + \\frac{\\xi_{d_j}^2}{\\lambda}\\right)^2 \\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\delta'_j\\right\\|_k^2.\\ ] ] now we apply the formula ( [ expectedform ] ) to estimate the expected value of ( [ primeone ] ) . by part ( b ) of lemma [ keynormsoperator ] , for @xmath181",
    ", there exists a subset @xmath337 of @xmath338 of measure at least @xmath182 such that @xmath339 applying part ( b ) of lemma [ keynorms ] to @xmath340 with @xmath341 and the data subset @xmath14 , we know that there exists another subset @xmath342 of @xmath338 of measure at least @xmath182 such that @xmath343 combining this with ( [ xidjest ] ) and ( [ primeone ] ) , we know that for @xmath344 , @xmath345 \\delta'_j\\right\\|_k^2 \\leq   \\left(\\frac{{\\mathcal b}_{|d_j| , \\lambda}^2}{\\lambda } + \\frac{{\\mathcal b}_{|d_j| , \\lambda}^4}{\\lambda^2}\\right ) \\left(\\frac{m}{\\kappa}\\right)^2 { \\mathcal b}_{|d_j| , \\lambda}^2",
    "\\left(2 \\log \\bigl(2/\\delta\\bigr)\\right)^6.\\ ] ] since the measure of the set @xmath346 is at least @xmath347 , by denoting @xmath348 we see that @xmath349 \\delta'_j\\right\\|_k^2 > { \\mathcal c}_{|d_j| , \\lambda } \\left(\\log \\bigl(2/\\delta\\bigr)\\right)^6\\right ] \\leq 2 \\delta.\\ ] ] for @xmath350 , the equation @xmath351 has the solution @xmath352 when @xmath353 , we have @xmath349 \\delta'_j\\right\\|_k^2 > t\\right ] \\leq 2 \\delta_t = 4 \\exp\\left\\{-\\left(t/{\\mathcal c}_{|d_j| , \\lambda}\\right)^{1/6}\\right\\}.\\ ] ] this inequality holds trivially when @xmath354 since the probability is at most @xmath355 .",
    "thus we can apply the formula ( [ expectedform ] ) to the nonnegative random variable @xmath356 \\delta'_j\\right\\|_k^2 $ ] and obtain @xmath357 \\delta'_j\\right\\|_k^2\\right ] = \\int_0^\\infty \\hbox{prob}\\left[\\xi > t\\right ] d t \\leq \\int_0^\\infty 4 \\exp\\left\\{-\\left(t/{\\mathcal c}_{|d_j| , \\lambda}\\right)^{1/6}\\right\\ } d t\\ ] ] which equals @xmath358 .",
    "therefore , @xmath335",
    "\\leq \\left\\{\\sum_{j=1}^m \\left(\\frac{|d_j|}{|d|}\\right)^2 24 \\gamma ( 6 ) { \\mathcal c}_{|d_j| , \\lambda } \\right\\}^{1/2 } \\\\ & & \\leq 1536\\sqrt{5 } \\kappa m \\left\\{\\sum_{j=1}^m \\left(\\frac{|d_j|}{|d|}\\right)^2 \\lambda \\left(\\frac{\\kappa^2}{|d_j|^2 \\lambda^2 } + \\frac{{\\mathcal n}(\\lambda)}{|d_j|\\lambda}\\right)^2 \\left\\{1 + 8 \\kappa^2 \\left(\\frac{\\kappa^2}{|d_j|^2 \\lambda^2 } + \\frac{{\\mathcal n}(\\lambda)}{|d_j|\\lambda}\\right)^2 \\right\\}\\right\\}^{1/2}.\\end{aligned}\\ ] ]    for the second term @xmath359 of ( [ fdrhodecom ] ) , we use the second order decomposition ( [ secondorder ] ) again and obtain @xmath345 \\delta''_j\\right\\|_k \\leq \\left(\\frac{\\xi_{d_j}}{\\sqrt{\\lambda } } + \\frac{\\xi_{d_j}^2}{\\lambda}\\right ) \\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } \\delta''_j\\right\\|_k.\\ ] ] applying the schwarz inequality and lemmas [ keynormsoperator ] and [ keynorms ] , we get @xmath360 \\delta''_j\\right\\|_k\\right ]   & \\leq & \\left\\{e\\left[\\left(\\frac{\\xi_{d_j}}{\\sqrt{\\lambda } } + \\frac{\\xi_{d_j}^2}{\\lambda}\\right)^2\\right]\\right\\}^{1/2 } \\\\   & & \\frac{1}{\\sqrt{|d_j| } } \\left\\{e\\left[\\left(f_\\rho ( x ) - f_\\lambda ( x)\\right)^2 \\left\\|\\left(l_{k } + \\lambda i\\right)^{-1/2 } ( k_x ) \\right\\|_k^2\\right]\\right\\}^{1/2 } \\\\ & \\leq & \\left(\\left\\{\\frac{\\kappa^2 { \\mathcal n}(\\lambda)}{|d_j|\\lambda}\\right\\}^{1/2 } +   \\left\\{\\frac{48 { \\mathcal b}_{|d_j| , \\lambda}^4}{\\lambda^2}\\right\\}^{1/2}\\right ) \\frac{\\kappa \\|f_\\rho - f_\\lambda\\|_\\rho}{\\sqrt{|d_j|\\lambda } } \\\\ & \\leq &   \\left(\\frac{58\\kappa^4}{(|d_j|\\lambda)^2 } + \\frac{58\\kappa^2 { \\mathcal n}(\\lambda)}{|d_j|\\lambda}\\right)\\frac{\\kappa \\|f_\\rho - f_\\lambda\\|_\\rho}{\\sqrt{|d_j|\\lambda}}.\\end{aligned}\\ ] ] it follows that @xmath361 \\leq   \\left(\\frac{58\\kappa^4}{(|d| \\lambda)^2 } + \\frac{58\\kappa^2 { \\mathcal n}(\\lambda)}{|d|\\lambda}\\right ) \\sum_{j=1}^m 2\\frac{|d|}{|d_j| } \\frac{\\kappa \\|f_\\rho - f_\\lambda\\|_\\rho}{\\sqrt{|d_j|\\lambda}}.\\ ] ]    the last term @xmath362 of ( [ fdrhodecom ] ) has been handled in the proof of proposition [ mainpropls ] by ignoring the summand @xmath363 in the expression for @xmath266 , and we find from the trivial bound @xmath364 with @xmath93 that @xmath365 \\leq   \\left(\\frac{58\\kappa^4}{(|d|\\lambda)^2 } + \\frac{58\\kappa^2 { \\mathcal n}(\\lambda)}{|d|\\lambda}\\right ) \\left(2 m \\left(\\frac{{\\mathcal n}(\\lambda)}{|d|}\\right)^{\\frac{1}{2 } }   + \\frac{\\kappa \\|f_\\rho - f_\\lambda\\|_\\rho}{\\sqrt{|d|\\lambda}}\\right).\\ ] ] combining the above estimates for the three terms of ( [ fdrhodecom ] ) , we see that the desired error bound in the @xmath65 metric holds true .    the estimate in the @xmath25 metric follows from the steps in deriving the error bound in the @xmath65 metric except that in the representation ( [ fdrhodecom ] ) the operator @xmath176 in the front disappears .",
    "this change gives an additional factor @xmath366 , the bound for the operator @xmath246 , and proves the desired error bound in the @xmath25 metric .",
    "since @xmath46 for @xmath47 , the bound in theorem [ mainexpectedgeneral ] in the @xmath65 metric can be simplified as @xmath367 & \\leq & c'_\\kappa \\left(\\frac{1}{(n \\lambda)^2 } + \\frac{{\\mathcal n}(\\lambda)}{n\\lambda}\\right ) \\biggl\\{\\frac{\\|f_\\lambda - f_\\rho\\|_\\rho}{\\sqrt{n\\lambda } } m^{\\frac{5}{2 } } + m \\sqrt{\\lambda } \\sqrt{\\frac{{\\mathcal n}(\\lambda)}{n \\lambda}}\\biggr\\ } \\\\ & & +   c'_\\kappa m \\frac{\\sqrt{\\lambda}}{\\sqrt{m } } \\left(\\frac{m^2}{n^2 \\lambda^2 } + \\frac{m{\\mathcal n}(\\lambda)}{n\\lambda}\\right ) \\left\\{1 + \\left(\\frac{m^2}{n^2 \\lambda^2 } + \\frac{m{\\mathcal n}(\\lambda)}{n\\lambda}\\right)\\right\\ } \\\\ & \\leq & c'_\\kappa \\left(\\frac{m}{(n \\lambda)^2 } + \\frac{{\\mathcal n}(\\lambda)}{n\\lambda}\\right ) \\biggl\\{\\frac{\\|f_\\lambda - f_\\rho\\|_\\rho}{\\sqrt{n\\lambda } } m^{\\frac{5}{2 } } + m \\sqrt{\\lambda } \\sqrt{\\frac{{\\mathcal n}(\\lambda)}{n \\lambda } }   \\\\ & & +   m \\sqrt{\\lambda } \\sqrt{m } \\left\\{1 + \\left(\\frac{m^2}{n^2",
    "\\lambda^2 } + \\frac{m{\\mathcal n}(\\lambda)}{n\\lambda}\\right)\\right\\}\\biggr\\}.\\end{aligned}\\ ] ] notice that the term @xmath368 can be bounded by @xmath369 .",
    "then the desired error bound in the @xmath65 metric with @xmath370 follows .",
    "the proof for the error bound in the @xmath25 metric is similar .",
    "the proof of theorem [ mainexpected ] is complete .",
    "as in the proof of corollary [ specialpara ] , the restriction ( [ lambdacond ] ) implies @xmath309 and @xmath371 .",
    "it follows that @xmath372 putting these bounds into theorem [ mainexpected ] , we know that the expected value @xmath148 is bounded by @xmath373 and @xmath58 where @xmath374 this proves corollary [ mainequalsize ] .      if @xmath315 for some constant @xmath316 , then the choice @xmath77 satisfies ( [ lambdacond ] ) . with this choice",
    "we also have @xmath375 since the condition ( [ approxr ] ) yields @xmath376 , we have by corollary [ mainequalsize ] , @xmath377 the inequality @xmath378 is equivalent to @xmath379 and it can be expressed as ( [ mrestrict ] ) . since",
    "( [ mrestrict ] ) is valid , we have @xmath380 this proves the first desired convergence rate .",
    "the second rate follows easily .",
    "this proves corollary [ specialequalsize ] .",
    "the assumption @xmath72 tells us that for some constant @xmath316 , @xmath382 so the choice @xmath383 yields @xmath384 if @xmath12 satisfies @xmath385 then ( [ lambdacond ] ) is valid , and by corollary [ mainequalsize ] , @xmath386 thus , when @xmath12 satisfies @xmath387 we have @xmath388 and thereby @xmath100 = o\\left(n^{-\\frac{2 \\alpha r}{4 \\alpha r + 1}}\\right).\\ ] ] finally , we notice that ( [ restrictmfinal ] ) is equivalent to the combination of ( [ restrictmcor ] ) and ( [ restrictmiicor ] ) .",
    "so our conclusion follows .",
    "this proves corollary [ finalrate ] .",
    "i. steinwart , d. hush , and c. scovel .",
    "optimal rates for regularized least squares regression . in proceedings of the 22nd annual conference on learning theory ( s. dasgupta and a. klivans , eds . ) , pp .",
    "79 - 93 , 2009 ."
  ],
  "abstract_text": [
    "<S> we study distributed learning with the least squares regularization scheme in a reproducing kernel hilbert space ( rkhs ) . by a divide - and - conquer approach , the algorithm partitions a data set into disjoint data subsets , applies the least squares regularization scheme to each data subset to produce an output function , and then takes an average of the individual output functions as a final global estimator or predictor . </S>",
    "<S> we show with error bounds in expectation in both the @xmath0-metric and rkhs - metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine . </S>",
    "<S> our error bounds are sharp and stated in a general setting without any eigenfunction assumption . </S>",
    "<S> the analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach . even for the classical least squares regularization scheme in the rkhs associated with a general kernel , we give the best learning rate in the literature .    </S>",
    "<S> distributed learning , divide - and - conquer , error analysis , integral operator , second order decomposition . </S>"
  ]
}