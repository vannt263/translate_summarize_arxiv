{
  "article_text": [
    "nowadays it is very popular to represent and analyze statistical data using random graph or network models .",
    "the vertices in such a graph correspond to data points , whereas edges in the graph indicate that the adjacent vertices are `` similar '' or `` related '' to each other . in this paper",
    "we consider the problem of data clustering in a random geometric graph setting .",
    "we are given a sample of points drawn from some underlying probability distribution on a metric space .",
    "the goal is to cluster the sample points into `` meaningful groups '' .",
    "a standard procedure is to first transform the data to a neighborhood graph , for example a @xmath0-nearest neighbor graph . in a second step , the cluster structure",
    "is then extracted from the graph : clusters correspond to regions in the graph that are tightly connected within themselves and only sparsely connected to other clusters .",
    "there already exist a couple of papers that study statistical properties of this procedure in a particular setting : when the true underlying clusters are defined to be the connected components of a density level set in the underlying space . in his setting , a test for detecting cluster structure and outliers is proposed in @xcite . in @xcite",
    "the authors build a neighborhood graph in such a way that its connected components converge to the underlying true clusters in the data .",
    "@xcite compare the properties of different random graph models for identifying clusters of the density level sets .",
    "while the definition of clusters as connected components of level sets is appealing from a theoretical point of view , the corresponding algorithms are often too simplistic and only moderately successful in practice . from a practical point of view , clustering methods based on graph partitioning algorithms are more robust .",
    "clusters do not have to be perfectly disconnected in the graph , but are allowed to have a small number of connecting edges between them .",
    "graph partitioning methods are widely used in practice .",
    "the most prominent algorithm in this class is spectral clustering , which optimizes the normalized cut ( @xmath1 ) objective function ( see below for exact definitions , and @xcite for a tutorial on spectral clustering ) .",
    "it is already known under what circumstances spectral clustering is statistically consistent @xcite .",
    "however , there is one important open question . when applying graph - based methods to given sets of data points , one obviously has to build a graph first , and there are several important choices to be made : the type of the graph ( for example , @xmath0-nearest neighbor graph , the @xmath2-neighborhood graph or a gaussian similarity graph ) , the connectivity parameter ( @xmath0 or @xmath2 or @xmath3 , respectively ) and the weights of the graph .",
    "making such choices is not so difficult in the domain of supervised learning , where parameters can be set using cross - validation .",
    "however , it poses a serious problem in unsupervised learning .",
    "while different researchers use different heuristics and their `` gut feeling '' to set these parameters , neither systematic empirical studies have been conducted ( for example , how sensitive the results are to the choice of graph parameters ) , nor do theoretical results exist which lead to well - justified heuristics .",
    "in this paper we study the question if and how the results of graph - based clustering algorithms are affected by the graph type and the parameters that are chosen for the construction of the neighborhood graph .",
    "we focus on the case where the best clustering is defined as the partition that minimizes the normalized cut ( ncut ) or the cheeger cut .",
    "our theoretical setup is as follows . in a first step",
    "we ignore the problem of actually _ finding _ the optimal partition .",
    "instead we fix some partition of the underlying space and consider it as the `` true '' partition . for any finite set of points drawn from the underlying space we consider the clustering of the points that is induced by this underlying partition",
    ". then we study the convergence of the @xmath1 value of this clustering as the sample size tends to infinity .",
    "we investigate this question on different kinds of neighborhood graphs .",
    "our first main result is that depending on the type of graph , the clustering quality measure converges to different limit values .",
    "for example , depending on whether we use the @xmath4 graph or the @xmath2-graph , the limit functional integrates over different powers of the density . from a statistical point of view , this is very surprising because in many other respects , the @xmath4 graph and the @xmath2-graph behave very similar to each other .",
    "just consider the related problem of density estimation . here",
    ", both the @xmath0-nearest neighbor density estimate and the estimate based on the degrees in the @xmath2-graph converge to the same limit , namely the true underlying density .",
    "so it is far from obvious that the @xmath1 values would converge to different limits .    in a second step",
    "we then relate these results to the setting where we optimize over all partitions to find the one that minimizes the @xmath1 .",
    "we can show that the results from the first part can lead to the effect that the minimizer of @xmath1 on the @xmath4 graph is different from the minimizer of @xmath1 on the @xmath2-graph or on the complete graph with gaussian weights .",
    "this effect can also be studied in practical examples .",
    "first , we give examples of well - clustered distributions ( mixtures of gaussians ) where the optimal limit cut on the @xmath4 graph is different from the one on the @xmath2-neighborhood graph .",
    "the optimal limit cuts in these examples can be computed analytically . next we can demonstrate that this effect can already been observed on finite samples from these distributions .",
    "given a finite sample , running normalized spectral clustering to optimize ncut leads to systematically different results on the @xmath4 graph than on the @xmath2-graph .",
    "this shows that our results are not only of theoretical interest , but that they are highly relevant in practice .    in the following section we formally define the graph clustering quality measures and the neighborhood graph types we consider in this paper .",
    "furthermore , we introduce the notation and technical assumptions for the rest of the paper . in section  [ sec : limits_quality_measures ] we present our main results on the convergence of @xmath1 and the @xmath5 on different graphs . in section  [ sec",
    ": experiments ] we show that our findings are not only of theoretical interest , but that they also influence concrete algorithms such as spectral clustering in practice .",
    "all proofs are deferred to section  [ sec : proofs ] .",
    "note that a small part of the results of this paper has already been published in  @xcite .",
    "given a directed graph @xmath6 with weights @xmath7 and a partition of the nodes @xmath8 into @xmath9 we define @xmath10 and @xmath11 .",
    "if @xmath12 is an undirected graph we replace the ordered pair @xmath13 in the sums by the unordered pair @xmath14 .",
    "note that by doing so we count each edge twice in the undirected graph .",
    "this introduces a constant of two in the limits but it has the advantage that there is no need to distinguish in the formulation of our results between directed and undirected graphs .    intuitively , the @xmath15 measures how strong the connection between the different clusters in the clustering is , whereas the volume of a subset of the nodes measures the `` weight '' of the subset in terms of the edges that originate in it .",
    "an ideal clustering would have a low @xmath15 and balanced clusters , that is clusters with similar volume .",
    "the graph clustering quality measures that we use in this paper , the normalized cut and the cheeger cut , formalize this trade - off in slightly different ways : the normalized cut is defined by @xmath16    these definitions are useful for general weighted graphs and general partitions .",
    "as was said in the beginning we want to study the values of @xmath1 and @xmath5 on neighborhood graphs on sample points in euclidean space and for partitions of the nodes that are induced by a hyperplane @xmath17 in @xmath18 .",
    "the two halfspaces belonging to @xmath17 are denoted by @xmath19 and @xmath20 .",
    "having a neighborhood graph on the sample points @xmath21 , the partition of the nodes induced by @xmath17 is @xmath22 . in the rest of this paper for a given neighborhood graph @xmath23 we set @xmath24 .",
    "similarly , for @xmath25 or @xmath26 we set @xmath27 .",
    "accordingly we define @xmath28 and @xmath29 .    in the following we introduce the different types of neighborhood graphs and weighting schemes that are considered in this paper .",
    "the graph types are :    * _ the @xmath0-nearest neighbor ( @xmath4 ) graphs _ , where the idea is to connect each point to its @xmath0 nearest neighbors .",
    "however , this yields a directed graph , since the @xmath0-nearest neighbor relationship is not symmetric .",
    "if we want to construct an undirected @xmath4 graph we can choose between the mutual @xmath4 graph , where there is an edge between two points if both points are among the @xmath0 nearest neighbors of the other one , and the symmetric @xmath4 graph , where there is an edge between two points if only one point is among the @xmath0 nearest neighbors of the other one . in our proofs for the limit expressions it will become clear that these do not differ between the different types of @xmath4 graphs .",
    "therefore , we do not distinguish between them in the statement of the theorems , but rather speak of `` the @xmath4 graph '' . * _ the @xmath2-neighborhood graph _ , where a radius @xmath2 is fixed and two points are connected if their distance does not exceed the threshold radius @xmath2 . note that due to the symmetry of the distance we do not have to distinguish between directed and undirected graphs . * _ the complete weighted graph _",
    ", where there is an edge between each pair of distinct nodes ( but no loops ) . of course , in general we would not consider this graph a neighborhood graph .",
    "however , if the weight function is chosen in such a way that the weights of edges between nearby nodes are high and the weights between points far away from each other are almost negligible , then the behavior of this graph should be similar to that of a neighborhood graph .",
    "one such weight function is the gaussian weight function , which we introduce below .",
    "the weights that are used on neighborhood graphs usually depend on the distance of the end nodes of the edge and are non - increasing .",
    "that is , the weight @xmath30 of an edge @xmath31 is given by @xmath32 with a non - increasing weight function @xmath33 .",
    "the weight functions we consider here are the _ unit weight function _",
    "@xmath34 , which results in the unweighted graph , and the _ gaussian weight function _",
    "@xmath35 with a parameter @xmath36 defining the bandwidth .    of course , not every weighting scheme is suitable for every graph type .",
    "for example , as mentioned above , we would hardly consider the complete graph with unit weights a neighborhood graph .",
    "therefore , we only consider the gaussian weight function for this graph . on the other hand , for the @xmath4 graph and the @xmath2-neighborhood graph with gaussian weights",
    "there are two `` mechanisms '' that reduce the influence of far - away nodes : first the fact that far - away nodes are not connected to each other by an edge and second the decay of the weight function .",
    "in fact , it turns out that the limit expressions we study depend on the interplay between these two mechanisms .",
    "clearly , the decay of the weight function is governed by the parameter @xmath3 . for the @xmath2-neighborhood graph the radius @xmath2 limits the length of the edges .",
    "asymptotically , given sequences @xmath37 and @xmath38 of bandwidths and radii we distinguish between the following two cases :    * _ the bandwidth @xmath39 is dominated by the radius @xmath40 _ , that is @xmath41 for @xmath42 , * _ the radius @xmath40 is dominated by the bandwidth @xmath39 _ , that is @xmath43 for @xmath42 .    for the @xmath4 graph we can not give a radius up to",
    "which points are connected by an edge , since this radius for each point is a random variable that depends on the positions of all the sample points .",
    "however , it is possible to show that for a point in a region of constant density @xmath44 the @xmath45-nearest neighbor radius is concentrated around @xmath46{k_n/((n-1 ) \\eta_d p)}$ ] , where @xmath47 denotes the volume of the unit ball in euclidean space @xmath18 .",
    "that is , the @xmath4 radius decays to zero with the rate @xmath46{k_n / n}$ ] . in the following it",
    "is convenient to set for the @xmath4 graph @xmath48{k_n / n}$ ] , noting that this is not the @xmath0-nearest neighbor radius of any point but only its decay rate . using this `` radius '' we distinguish between the same two cases of the ratio of @xmath40 and @xmath39 as for the @xmath2-neighborhood graph .    for the sequences @xmath38 and @xmath37 we always assume @xmath49 , @xmath50 and @xmath51 , @xmath52 for @xmath42 . furthermore , for the parameter sequence @xmath53 of the @xmath4 graph we always assume @xmath54 , which corresponds to @xmath49 , and @xmath55 .    in the rest of this paper we denote by @xmath56 the lebesgue measure in @xmath18 . furthermore , let @xmath57 denote the closed ball of radius @xmath2 around @xmath58 and @xmath59 , where we set @xmath60 .",
    "we make the following * general assumptions in the whole paper : *    _ _    * the data points @xmath61 are drawn independently from some density @xmath44 on @xmath18 .",
    "the measure on @xmath18 that is induced by @xmath44 is denoted by @xmath62 ; that means , for a measurable set @xmath63 we set @xmath64 . * the density @xmath44 is bounded from below and above",
    ", that is @xmath65 .",
    "in particular , it has compact support @xmath66 . * in the interior of @xmath66 , the density @xmath44 is twice differentiable and @xmath67 for a @xmath68 and all @xmath58 in the interior of @xmath66 . *",
    "the cut hyperplane @xmath17 splits the space @xmath18 into two halfspaces @xmath69 and @xmath70 ( both including the hyperplane @xmath17 ) with positive probability masses , that is @xmath71 , @xmath72 .",
    "the normal of @xmath17 pointing towards @xmath19 is denoted by @xmath73 . *",
    "if @xmath74 the boundary @xmath75 is a compact , smooth @xmath76-dimensional surface with minimal curvature radius @xmath77 , that is the absolute values of the principal curvatures are bounded by @xmath78 .",
    "we denote by @xmath79 the normal to the surface @xmath75 at the point @xmath80 .",
    "furthermore , we can find constants @xmath81 and @xmath82 such that for all @xmath83 we have @xmath84 for all @xmath85 . *",
    "if @xmath74 we can find an angle @xmath86 such that @xmath87 for all @xmath88 .",
    "if @xmath89 we assume that ( the point ) @xmath17 is in the interior of @xmath66 .",
    "the assumptions on the boundary @xmath75 are necessary in order to bound the influence of points that are close to the boundary .",
    "the problem with these points is that the density is not approximately uniform inside small balls around them .",
    "therefore , we can not find a good estimate of their @xmath4 radius and on their contribution to the @xmath15 and the volume . under the assumptions above we can neglect these points .",
    "as we can see in equations   and   the definitions of @xmath1 and @xmath5 rely on the @xmath15 and the volume .",
    "therefore , in order to study the convergence of @xmath1 and @xmath5 it seems reasonable to study the convergence of the @xmath15 and the volume first . in section  [ sec : proofs ] the corollaries  [ cor : cut_knn_unweighted]-[cor : cut_knn_sigma_to_r_to_zero ] and the corollaries  [ cor : vol_knn_unweighted]-[cor : vol_knn_r_to_sigma_to_infinity ] state the convergence of the @xmath15 and the volume on the @xmath4 graphs . the corollaries  [ cor : cut_unweighted]-[cor : cut_r_graph_gaussian_weights_r_to_sigma_to_zero ] state the convergence of the @xmath15 on the @xmath2-graph and the complete weighted graph , whereas the corollaries  [ cor : volume_unweighted]-[cor : volume_r_graph_r_to_sigma_to_zero ] state the convergence of the volume on the same graphs .",
    "these corollaries show that there are scaling sequences @xmath90 and @xmath91 that depend on @xmath92 , @xmath40 and the graph type such that , under certain conditions , almost surely @xmath93 for @xmath42 , where @xmath94 and @xmath95 are constants depending only on the density @xmath44 and the hyperplane @xmath17 .",
    "weighting & @xmath96 & @xmath97 @xmath4-graph & @xmath97 @xmath2-graph + unweighted & @xmath98 & @xmath99 & @xmath100 + weighted @xmath101 & @xmath102 & @xmath103 & @xmath103 + weighted @xmath104 & @xmath105 & @xmath106 & @xmath107 +   +   + weighting & @xmath96 & + weighted & @xmath102 & +   +   + weighting & @xmath108 & @xmath109 @xmath4-graph & @xmath109 @xmath2-graph + unweighted & @xmath110 & @xmath111 & @xmath112 + weighted , @xmath101 & @xmath113 & @xmath114 & @xmath114 + weighted , @xmath104 & @xmath115 & @xmath116 & @xmath117 +   +   + weighting & @xmath108 & + weighted & @xmath113 & +    having defined these limits we define , analogously to the definitions in equations   and  , the limits of @xmath1 and @xmath5 as @xmath118 in our following main theorems we show the conditions under which we have for @xmath42 almost sure convergence of @xmath119 furthermore , for the unweighted @xmath2-graph and @xmath4-graph and for the complete weighted graph with gaussian weights we state the optimal convergence rates , where `` optimal '' means the best trade - off between our bounds for different quantities derived in section  [ sec : proofs ] . note that we will not prove the following theorems here .",
    "rather the proof of theorem  [ thm : limit_ncut_cheegercut_knn_graph ] can be found in section  [ sec : proof_main_theorem_knn ] , whereas the proofs of theorems  [ thm : limit_ncut_cheegercut_r_graph ] and  [ thm : limit_ncut_cheegercut_complete_graph ] can be found in section  [ sec : proofs_main_theorems_r_graph ] .",
    "[ thm : limit_ncut_cheegercut_knn_graph ] for a sequence @xmath53 with @xmath54 for @xmath42 let @xmath23 be the @xmath45-nearest neighbor graph on the sample @xmath120 .",
    "set @xmath121 or @xmath122 and let @xmath123 denote the corresponding limit as defined in equations   and  .",
    "set @xmath124    * let @xmath23 be the unweighted @xmath4 graph . if @xmath125 in the case @xmath89 and @xmath55 in the case @xmath74 we have @xmath126 for @xmath42 almost surely .",
    "the optimal convergence rate is achieved for @xmath127{n^3 \\log n}$ ] in the case @xmath89 and @xmath128 in the case @xmath74 . for this choice of @xmath45",
    "we have @xmath129{\\log n / n})$ ] in the case @xmath89 and @xmath130{\\log n / n})$ ] for @xmath74 .",
    "* let @xmath23 be the @xmath4-graph with gaussian weights and suppose @xmath131 for an @xmath132 .",
    "then we have almost sure convergence of @xmath126 for @xmath42 if @xmath55 and @xmath133 .",
    "* let @xmath23 be the @xmath4-graph with gaussian weights and @xmath104 .",
    "then we have almost sure convergence of @xmath126 for @xmath42 if @xmath125 in the case @xmath89 and @xmath55 in the case @xmath74 .",
    "[ thm : limit_ncut_cheegercut_r_graph ] for a sequence @xmath134 with @xmath49 for @xmath42 let @xmath23 be the @xmath40-neighborhood graph on the sample @xmath120 . set @xmath121 or @xmath122 and let @xmath123 denote the corresponding limit as defined in equations   and  . set @xmath124    * let @xmath23 be unweighted . then @xmath126 almost surely for @xmath42 if @xmath135 .",
    "the optimal convergence rate is achieved for @xmath136{\\log n / n}$ ] for a suitable constant @xmath137 . for this choice of @xmath40",
    "we have @xmath138{\\log n /",
    "n } ) $ ] .",
    "* let @xmath23 be weighted with gaussian weights with bandwidth @xmath50 and @xmath139 for @xmath42 .",
    "then @xmath126 almost surely for @xmath42 if @xmath140 .",
    "* let @xmath23 be weighted with gaussian weights with bandwidth @xmath50 and @xmath104 for @xmath42",
    ". then @xmath126 almost surely for @xmath42 if @xmath141 .",
    "the following theorem presents the limit results for @xmath1 and @xmath5 on the complete weighted graph .",
    "one result that we need in the proof of this theorem is corollary  [ cor : cut_complete_graph ] on the convergence of the @xmath15 .",
    "note that in  @xcite a similar @xmath15 convergence problem is studied for the case of the complete weighted graph , and the scaling sequence and the limit differ from ours .",
    "however , the reason is that in that paper the _ weighted _",
    "@xmath15 is considered , which can be written as @xmath142 , where @xmath143 denotes the normalized graph laplacian matrix and @xmath33 is an @xmath92-dimensional vector with @xmath144 if @xmath145 is in one cluster and @xmath146 if @xmath145 is in the other cluster . on the other hand , the standard @xmath15 , which we consider in this paper ,",
    "can be written ( up to a constant ) as @xmath147 , where @xmath148 denotes the unnormalized graph laplacian matrix .",
    "( for the definitions of the graph laplacian matrices and their relationship to the @xmath15 we refer the reader to  @xcite . )",
    "therefore , the two results do not contradict each other .",
    "[ thm : limit_ncut_cheegercut_complete_graph ] let @xmath23 be the complete weighted graph with gaussian weights and bandwidth @xmath39 on the sample points @xmath120 . set @xmath121 or @xmath122 and",
    "let @xmath123 denote the corresponding limit as defined in equations   and  .",
    "set @xmath124 under the conditions @xmath50 and @xmath133 we have almost surely @xmath126 for @xmath42 .",
    "the optimal convergence rate is achieved setting @xmath149{\\log n / n}$ ] with a suitable @xmath150 . for this choice of @xmath39",
    "the convergence rate is in @xmath151 for any @xmath132 .",
    "let us decrypt these results and for simplicity focus on the @xmath15 value .",
    "when we compare the limits of the @xmath15 ( cf .",
    "table  [ table : cut_limits ] ) it is striking that , depending on the graph type and the weighting scheme , there are two substantially different limits : the limit @xmath152 for the unweighted @xmath2-neighborhood graph , and the limit @xmath153 for the unweighted @xmath0-nearest neighbor graph .",
    "the limit of the @xmath15 for the complete weighted graph with gaussian weights is the same as the limit for the unweighted @xmath2-neighborhood graph .",
    "there is a simple reason for that : on both graph types the weight of an edge only depends on the distance between its end points , no matter where the points are .",
    "this is in contrast to the @xmath4-graph , where the radius up to which a point is connected strongly depends on its location : if a point is in a region of high density there will be many other points close by , which means that the radius is small . on the other hand , this radius is large for points in low - density regions .",
    "furthermore , the gaussian weights decline very rapidly with the distance , depending on the parameter @xmath3 .",
    "that is , @xmath3 plays a similar role as the radius @xmath2 for the @xmath2-neighborhood graph .",
    "the two types of @xmath2-neighborhood graphs with gaussian weights have the same limit as the unweighted @xmath2-neighborhood graph and the complete weighted graph with gaussian weights .",
    "when we compare the scaling sequences @xmath96 it turns out that in the case @xmath139 this sequence is the same as for the complete weighted graph , whereas in the case @xmath104 we have @xmath154 , which is the same sequence as for the unweighted @xmath2-graph corrected by a factor of @xmath155 .",
    "in fact , these effects are easy to explain : if @xmath139 then the edges which we have to remove from the complete weighted graph in order to obtain the @xmath40-neighborhood graph have a very small weight and their contribution to the value of the @xmath15 can be neglected .",
    "therefore this graph behaves like the complete weighted graph with gaussian weights . on the other hand , if @xmath104 then all the edges that remain in the @xmath40-neighborhood graph have approximately the same weight , namely the maximum of the gaussian weight function , which is linear in @xmath155 .    similar effects can be observed for the @xmath0-nearest neighbor graphs .",
    "the limits of the unweighted graph and the graph with gaussian weight and @xmath104 are identical ( up to constants ) and the scaling sequence has to correct for the maximum of the gaussian weight function .",
    "however , the limit for the @xmath4-graph with gaussian weights and @xmath139 is different : in fact , we have the same limit expression as for the complete weighted graph with gaussian weights .",
    "the reason for this is the following : since @xmath40 is large compared to @xmath39 at some point all the @xmath0-nearest neighbor radii of the sample points are very large .",
    "therefore , all the edges that are in the complete weighted graph but not in the @xmath4 graph have very low weights and thus the limit of this graph behaves like the limit of the complete weighted graph with gaussian weights .",
    "finally , we would like to discuss the difference between the two limit expressions , where as examples for the graphs we use only the unweighted @xmath2-neighborhood graph and the unweighted @xmath4-graph .",
    "of course , the results can be carried over to the other graph types .",
    "for the @xmath15 we have the limits @xmath153 and @xmath152 . in dimension 1",
    "the difference between these expressions is most pronounced : the limit for the @xmath4 graph does not depend on the density @xmath44 at all , whereas in the limit for the @xmath2-graph the exponent of @xmath44 is @xmath156 , independent of the dimension .",
    "generally , the limit for the @xmath2-graph seems to be more sensitive to the absolute value of the density .",
    "this can also be seen for the volume : the limit expression for the @xmath4 graph is @xmath157 , which does not depend on the absolute value of the density at all , but only on the probability mass in the halfspace @xmath158 .",
    "this is different for the unweighted @xmath2-neighborhood graph with the limit expression @xmath159 .",
    "[ cols=\"^,^ \" , ]",
    "in this paper we have investigated the influence of the graph construction on the graph - based clustering measures normalized cut and cheeger cut .",
    "we have seen that depending on the type of graph and the weights , the clustering quality measures converge to different limit results .",
    "this means that ultimately , the question about the `` best @xmath1 '' or `` best cheeger cut '' clustering , given infinite amount of data , has different answers , depending on which underlying graph we use .",
    "this observation opens pandora s box on clustering criteria : the `` meaning '' of a clustering criterion does not only depend on the exact definition of the criterion itself , but also on how the graph on the finite sample is constructed .",
    "this means that one graph clustering quality measure is not just `` one well - defined criterion '' on the underlying space , but it corresponds to a whole bunch of criteria , which differ depending on the underlying graph .",
    "more sloppy : a clustering quality measure applied to one neighborhood graph does something different in terms of partitions of the underlying space than the same quality measure applied to a different neighborhood graph .",
    "this shows that these criteria can not be studied isolated from the graph they are applied to .    from a theoretical side",
    ", there are several directions in which our work can be improved . in this paper",
    "we only consider partitions of euclidean space that are defined by hyperplanes .",
    "this restriction is made in order to keep the proofs reasonably simple .",
    "however , we are confident that similar results could be proven for arbitrary smooth surfaces",
    ".    another extension would be to obtain uniform convergence results .",
    "here one has to take care that one uses a suitably restricted class of candidate surfaces @xmath17 ( note that uniform convergence results over the set of all partitions of @xmath18 are impossible , cf .",
    ". this result would be especially useful , if there existed a practically applicable algorithm to compute the optimal surface out of the set of all candidate surfaces .",
    "for practice , it will be important to study how the different limit results influence clustering results .",
    "so far , we do not have much intuition about when the different limit expressions lead to different optimal solutions , and when these solutions will show up in practice .",
    "the examples we provided above already show that different graphs indeed can lead to systematically different clusterings in practice .",
    "gaining more understanding of this effect will be an important direction of research if one wants to understand the nature of different graph clustering quality measures .",
    "in many of the proofs that are to follow in this section a lot of technique is involved in order to come to terms with problems that arise due to effects at the boundary of our support @xmath66 and to the non - uniformity of the density @xmath44 . however , if these technicalities are ignored , the basic ideas of the proofs are simple to explain and they are similar for the different types of neighborhood graphs . in section  [ sec : proofs_intuitively ] we discuss these ideas without the technical overhead and define some quantities that are necessary for the formulation of our results .",
    "in section  [ sec : proofs_knn_graph ] we present the results for the @xmath0-nearest neighbor graph and in section  [ sec : proofs_r_graph ] we present those for the @xmath2-graph and the complete weighted graph .",
    "each of these sections consists of three parts : the first is devoted to the @xmath15 , the second is devoted to the volume , and in the third we proof the main theorem for the considered graphs using the results for the @xmath15 and the volume .    the sections on the convergence of the @xmath15 and the volume always follow the same scheme : first , a proposition concerning the convergence of the @xmath15 or the volume for general monotonically decreasing weight functions is given .",
    "using this general proposition the results for the specific weight functions we consider in this paper follow as corollaries .    since the basic ideas of our proofs are the same for all the different graphs , it is not worth repeating the same steps for all the graphs . therefore , we decided to give detailed proofs for the @xmath0-nearest neighbor graph , which is the most difficult case .",
    "the @xmath2-neighborhood graph and the complete weighted graph can be treated together and we mainly discuss the differences to the proof for the @xmath4 graph .",
    "the limits of the @xmath15 and the volume for general weight function are expressed in terms of certain integrals of the weight function over `` caps '' and `` balls '' , which are explained later . for a specific weight function",
    "these integrals have to be evaluated .",
    "this is done in the lemmas in section  [ sec : gaussian_weight_function ] .",
    "furthermore , this section contains a technical lemma that helps us to control boundary effects .        _",
    "first step : decompose @xmath28 into @xmath160 and @xmath161 _ + under our general assumptions there exist constants @xmath162 , which may depend on the limit values of the cut and the volume , such that for sufficiently large @xmath92 @xmath163    _ second step : bias / variance decomposition of @xmath15 and volume terms _ + in order to show the convergence of the @xmath15-term we do a bias / variance decomposition @xmath164 and show the convergence to zero of these terms separately .",
    "clearly , the same decomposition can be done for the volume terms . in the following we call these terms the `` bias term of the @xmath15 '' and the `` variance term of the @xmath15 '' and similarly for the volume .    for both , the @xmath15 and the volume",
    ", there is one result in this section dealing with the convergence properties of the bias term and the variance term on each particular graph type and weighting scheme .    _",
    "third step : use concentration of measure inequalities for the variance term _",
    "+ bounding the deviation of a random variable from its expectation is a well - studied problem in statistics and there are a couple of so - called concentration of measure inequalities that bound the probability of a large deviation from the mean . in this paper",
    "we use mcdiarmid s inequality for the @xmath4 graphs and a concentration of measure result for @xmath165-statistics by hoeffding for the @xmath2-neighborhood graph and the complete weighted graph .",
    "the reason for this is that each of the graph types has its particular advantages and disadvantages when it comes to the prerequisites for the concentration inequalities : the advantage of the @xmath4 graph is that we can bound the degree of a node linearly in the parameter @xmath0 , whereas for the @xmath2-neighborhood graph we can bound the degree only by the trivial bound @xmath166 and for the complete graph this bound is even attained .",
    "therefore , using the same proof as for the @xmath4-graph is suboptimal for the latter two graphs . on the other hand , in these graphs the connectivity between points",
    "is not random given their position and it is always symmetric .",
    "this allows us to use a @xmath165-statistics argument , which can not be applied to the @xmath4-graph , since the connectivity there may be unsymmetric ( at least for the directed one ) and the connectivity between each two points depends on all the sample points .",
    "note that these results are of a probabilistic nature , that is we obtain results of the form @xmath167 for a sequence @xmath168 of non - negative real numbers .",
    "if for all @xmath169 the sum @xmath170 is finite , then we have almost sure convergence of the variance term to zero by the borel - cantelli lemma .",
    "_ fourth step : bias of the @xmath15 term _ + while all steps so far were pretty much standard , this part is the technically most challenging part of our convergence proof .",
    "we have to prove the convergence of @xmath171 to @xmath97 ( and similarly for the volume ) . omitting all technical difficulties like boundary effects and the variability of the density",
    ", the basic ideas can be described in a rather simple manner .",
    "the first idea is to break the @xmath15 down into the contributions of each single edge .",
    "we define a random variable @xmath172 that attains the weight of the edge between @xmath145 and @xmath173 , if these points are connected in the graph and on different sides of the hyperplane @xmath17 , and zero otherwise . by the linearity of the expectation and the fact that the points are sampled i.i.d .",
    "@xmath174    now we fix the positions of the points @xmath175 and @xmath176 . in this case",
    "@xmath172 can attain only two values : @xmath177 if the points are connected and on different sides of @xmath17 , and zero otherwise .",
    "we first consider the @xmath2-neighborhood graph with parameter @xmath40 , since here the existence of an edge between two points is determined by their distance , and is not random as in the @xmath4 graph .",
    "two points are connected if their distance is not greater than @xmath40 and thus @xmath178 if @xmath179 .",
    "furthermore , @xmath178 if @xmath58 and @xmath180 are on the same side of @xmath17 .",
    "that is , for a point @xmath181 we have @xmath182 by integrating over @xmath18 we obtain @xmath183 and denote the integral on the right hand side in the following by @xmath184 .",
    "integrating the conditional expectation over all possible positions of the point @xmath58 in @xmath18 gives @xmath185 we only consider the integral over the halfspace @xmath19 here , since the other integral can be treated analogously .",
    "the important idea in the evaluation of this integral is the following : instead of integrating over @xmath19 , we initially integrate over the hyperplane @xmath17 and then , at each point @xmath186 , along the normal line through @xmath187 , that is the line @xmath188 for all @xmath189 .",
    "this leads to @xmath190     integration along the normal line through @xmath187 . obviously , for @xmath191 the intersection @xmath192 is empty and therefore @xmath193 .",
    "for @xmath194 the points in the cap are close to @xmath187 and therefore the density in the cap is approximately @xmath195 . , title=\"fig : \" ] +    this integration is illustrated in figure  [ fig : integration_r_graph ] .",
    "it has two advantages : first , if @xmath58 is far enough from @xmath17 ( that is , @xmath196 for all @xmath186 ) , then @xmath197 and the corresponding terms in the integral vanish .",
    "second , if @xmath58 is close to @xmath186 and the radius @xmath40 is small , then the density on the ball @xmath198 can be considered approximately uniform , that is we assume @xmath199 for all @xmath200 .",
    "thus , @xmath201 where the last step follows with lemma  [ lem : cap_integral ] .",
    "since this integral of the weight function @xmath202 over the `` caps '' plays such an important role in the derivation of our results we introduce a special notation for it : for a radius @xmath203 and @xmath204 we define @xmath205 although these integrals also depend on @xmath92 we do not make this dependence explicit . in fact ,",
    "the parameter  @xmath2 is replaced by the radius @xmath40 in the case of the @xmath2-neighborhood graph or by a different graph parameter depending on @xmath92 for the other neighborhood graphs .",
    "therefore the dependence of @xmath206 on @xmath92 will be understood .",
    "note that we allow the notation @xmath207 , if the indefinite integral exists .",
    "the integral @xmath208 for @xmath209 is needed for the following reason : for the @xmath165-statistics bound on the variance term we do not only have to compute the expectation of @xmath172 , but also their variance .",
    "but the variance can in turn be bounded by the expectation of @xmath210 , which is expressed in terms of @xmath211 .    in the @xmath2-neighborhood graph points",
    "are only connected within a certain radius  @xmath40 , which means that to compute @xmath212 we only have to integrate over the ball @xmath213 , since all other points can not be connected to @xmath175 .",
    "this is clearly different for the complete graph , where every point is connected to every other point .",
    "the idea is to fix a radius @xmath40 in such a way as to make sure that the contribution of edges to points outside @xmath213 can be neglected , because their weight is small .",
    "since @xmath214 if the points are on different sides of @xmath17 we have for @xmath181 @xmath215 for the gaussian weight function the integral converges to zero very quickly , if @xmath139 for @xmath42 .",
    "thus we can treat the complete graph almost as the @xmath2-neighborhood graph .    for the @xmath0-nearest neighbor graph the connectedness of points depends on their @xmath0-nearest neighbor radii that is , the distance of the point to its @xmath0-th nearest neighbor , which is itself a random variable .",
    "however , one can show that with very high probability the @xmath0-nearest neighbor radius of a point in a region with uniform density @xmath44 is concentrated around @xmath216 .",
    "since we assume that @xmath54 for @xmath42 the expected @xmath4 radius converges to zero .",
    "thus the density in balls with this radius is close to uniform and the estimate becomes more accurate .",
    "upper and lower bounds on the @xmath0-nearest neighbor radius that hold with high probability are given in lemma  [ lem : prob_knn_radii ] .",
    "the idea is to perform the integration above for both , the lower bound on the @xmath4 radius and the upper bound on the @xmath4 radius .",
    "then it is shown that these integrals converge to the same limit .",
    "_ fifth step : bias of the volume terms _ + the bias of the volume term can be treated similarly to the cut term .",
    "we define @xmath217 if @xmath145 and @xmath173 are connected in the graph and @xmath178 otherwise .",
    "note that we do not need the condition that the points have to be on different sides of the hyperplane  @xmath17 as for the @xmath15 .",
    "then , for a point @xmath85 if we assume that the density is uniform within distance @xmath40 around @xmath58 @xmath218 where the last integral transform follows with lemma  [ lem : ball_integral ]",
    ". integrating over @xmath18 we obtain @xmath219 since the integral over the balls is so important in the formulation of our general results we often call it the `` ball integral '' and introduce the notation @xmath220 for a radius @xmath221 and @xmath204 .",
    "the remarks that were made on the `` cap integral '' @xmath222 above also apply to the `` ball integral '' @xmath223 .",
    "_ sixth step : plugging in the weight functions _",
    "+ having derived results on the bias term of the @xmath15 and volume for general weight functions , we can now plug in the specific weight functions in which we are interested in this paper .",
    "this boils down to the evaluation of the `` cap '' and `` ball '' integrals @xmath224 and @xmath225 for these weight functions . for the unit weight function",
    "the integrals can be computed exactly , whereas for the gaussian weight function we study the asymptotic behavior of the `` cap '' and `` ball '' integral in the cases @xmath104 and @xmath139 for @xmath42 .",
    "as we have already mentioned we will give the proofs of our general propositions in detail here and then discuss in section  [ sec : proofs_r_graph ] how they have to be adapted to the @xmath2-neighborhood graph and the complete weighted graph .",
    "this means , that lemmas  [ lem : cap_integral ] and  [ lem : ball_integral ] that are necessary for the proof of the general propositions can be found in this section , although they are also needed for the @xmath2-graph and the @xmath0-nearest neighbor graph .",
    "this section consists of four subsections : in section  [ sec : knn_radii ] we define some quantities that help us to deal with the fact that the connectivity between two points is random even if we know their distance .",
    "these quantities will play an important role in the succeeding sections .",
    "section  [ sec : cut_term_knn ] presents the results for the @xmath15 term , whereas section  [ sec : vol_term_knn ] presents the results for the volume term .",
    "finally , these results are used to proof theorem  [ thm : limit_ncut_cheegercut_knn_graph ] , the main theorem for the @xmath0-nearest neighbor graph in section  [ sec : proof_main_theorem_knn ] .    in the subsections on the @xmath15-term and the volume term we always present the proposition for general weight functions first .",
    "then the lemmas follow that are used in the proof of the proposition . finally , we show corollaries that apply these general results to the specific weight functions we consider in this paper .",
    "an overview of the proof structure is given in figure  [ fig : proof_structure ] .     and",
    "[ pro : volume_knn_general ] state bounds for general weight functions on the bias and the variance term of the @xmath15 and the volume , respectively .",
    "lemma  [ lem : prob_knn_radii ] shows the concentration of the @xmath4 radii , lemma  [ lem : boundary_strip_to_zero ] is needed to bound the influence of points close to the boundary .",
    "lemma  [ lem : cap_integral ] and  [ lem : ball_integral ] perform the integration of the weight function over `` caps '' and `` balls '' . in lemmas  [",
    "pro : integration_unit_weight_function]-[pro : gaussian_weight_function ] the general `` ball '' and `` cap '' integrals are evaluated for the specific weight functions we use .",
    "using these results , corollaries  [ cor : cut_knn_unweighted]-[cor : cut_knn_sigma_to_r_to_zero ] dealing with the @xmath15 and corollaries  [ cor : vol_knn_unweighted]-[cor : vol_knn_r_to_sigma_to_infinity ] dealing with the volume are proved . finally , in theorem",
    "[ thm : limit_ncut_cheegercut_knn_graph ] the convergence of @xmath1 and @xmath5 are analyzed using the result of these corollaries .",
    ", scaledwidth=80.0% ]      as we have explained in section  [ sec : proofs_intuitively ] the basic ideas of our convergence proofs are similar for all the graphs .",
    "however , there is one major technical difficulty for the @xmath0-nearest neighbor graph : the existence of an edge between two points depends on all the other sample points and it is random , even if we know the distance between the points .",
    "however , each sample point @xmath145 is connected to its @xmath0 nearest neighbors , that means to all points with a distance not greater than that of the @xmath0-th nearest neighbor .",
    "this distance is called the @xmath0-nearest neighbor radius of point @xmath145 .",
    "unfortunately , given a sample point we do not know this radius without looking at all the other points . the idea to overcome",
    "this difficulty is the following : given the position of a sample point we give lower and upper bounds on the @xmath4 radius that depend on the density around the point and show that with high probability the true radius is between these bounds .",
    "then we can replace the integration over balls of a fixed radius with the integration over balls with the lower and upper bound on the @xmath4 radius in the proof for the bias term and then show that these integrals converge towards each other . furthermore , under our assumptions the radius of all the points can be bounded from above , which helps to bound the influence of far - away points .    in this section",
    "we define formally the bounds on the @xmath0-nearest neighbor radii , since these will be used in the statement of the general proposition . in lemma",
    "[ lem : prob_knn_radii ] we state the bounds on the probabilities that the true @xmath4 radius is between our bounds for the cases we need in the proofs .",
    "we first introduce the upper bound @xmath226 on the maximum @xmath0-nearest neighbor radius of a point not depending on its position .",
    "second , we use that given a point @xmath58 ( far enough ) in the interior of @xmath66 the conditional @xmath4 radius of a sample point at @xmath58 is highly concentrated around a radius @xmath227 .",
    "formally , we define @xmath228{\\frac{4}{\\gamma p_{\\min } \\eta_d } \\frac{k_n}{n-1 } } , \\qquad       \\text { and } \\qquad r_n ( x ) = \\sqrt[d]{\\frac{k_n}{(n-1 ) p(x ) \\eta_d } }       \\qquad \\text { for all $ x \\in c$. }    \\end{aligned}\\ ] ] as to the concentration we state sequences of lower and upper bounds , @xmath229 and @xmath230 that converge to @xmath227 such that for all @xmath85 that are not in a small boundary strip the probability that a point in @xmath58 is connected to a point in @xmath180 becomes small if the distance between @xmath58 and @xmath180 exceeds @xmath230 and becomes large if the distance is smaller than @xmath229 .    clearly , the accuracy of the bounds depends on how much the density can vary around @xmath58 .",
    "setting @xmath231 the density in the ball of radius @xmath232 around @xmath58 can vary between @xmath233 and @xmath234 .",
    "furthermore , we have to `` blow up '' or shrink the radii a bit in order to be sure that the true @xmath4 radius is between them . to this end",
    "we introduce a sequence @xmath235 with @xmath236 and @xmath237 for @xmath42",
    ". then we can define @xmath238{(1 - 2\\xi_n ) ( 1-\\delta_n ) } r_n ( x )       \\quad \\text{and } \\quad       r_n^+ ( x ) = \\sqrt[d]{(1 + 2\\xi_n ) ( 1+\\delta_n ) } r_n ( x ) .",
    "\\end{aligned}\\ ] ] note that @xmath239 converges to zero , since @xmath226 converges to zero as @xmath46{k_n / n}$ ] .",
    "the sequence @xmath240 is chosen such that it converges to zero reasonably fast , but that with high probability @xmath230 and @xmath229 are bounds on the @xmath4 radius of a point at @xmath58 .    in order to quantify the probability of connections , which we seek to bound",
    ", we define the function @xmath241 $ ] by @xmath242 where @xmath243 denotes the event that there is an edge between the sample points @xmath244 and @xmath245 in the ( directed or undirected ) @xmath0-nearest neighbor graph .      [",
    "pro : cut_knn_general ] let @xmath23 be the directed , symmetric or mutual @xmath0-nearest neighbor graph with a monotonically decreasing weight function @xmath202 . set @xmath246 for some @xmath247 in the definition of @xmath248",
    ". then we have for the bias term @xmath249{\\frac{k_n}{n } } \\right ) \\\\        &",
    "\\qquad + o \\left ( \\min \\left\\ { n^{-\\delta_0 } f_n \\left ( \\inf_{x \\in c } r_n ( x ) \\right ) ,        { f_b}^{(1 ) } ( \\infty ) - { f_b}^{(1 ) } \\left ( \\inf_{x \\in c } r_n ( x ) \\right ) \\right\\ } \\right ) \\\\        & \\qquad + o \\left ( \\min \\left\\{\\left ( \\sqrt[d]{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right ) f_n \\left ( \\inf_{x \\in c } r_n^- ( x ) \\right )         \\left ( \\frac{k_n}{n } \\right)^{1 + 1/d } ,         { f_c}^{(1 ) } ( \\infty ) - { f_c}^{(1 ) } ( \\inf_{x \\in c } r_n^- ( x ) )         \\right\\ } \\right )    .",
    "\\end{aligned}\\ ] ]      we define for @xmath252 , @xmath253 the random variable @xmath172 as @xmath254 for both , a directed and an undirected graph we have @xmath255 and by the linearity of expectation and the fact that the points are independent and identically distributed , we have @xmath256 in the convergence proof for the variance term of the @xmath15 for the @xmath2-neighborhood graph in proposition  [ pro : lim_cut_general_weight_r_graph ] we need a bound on @xmath257 . since this can be derived similarly to @xmath258 we state the following for @xmath259 for @xmath204 .",
    "we define @xmath243 to be the event that the sample points @xmath244 and @xmath245 are connected in the graph .",
    "conditioning on the location of the points @xmath260 and @xmath261 we obtain @xmath262 if @xmath244 and @xmath245 on the same side of the hyperplane @xmath17 , otherwise @xmath263 therefore ,",
    "if @xmath260 and @xmath261 are on different sides of @xmath17 @xmath264 with @xmath265 as above we have @xmath266      we only deal with the first integral here , the second can be computed analogously . by a simple transformation of the coordinate system we can write this integral as an integral along the hyperplane @xmath17 , and for each points @xmath187 in @xmath17 we integrate over the normal line through @xmath187 . in the following we find lower and upper bounds on the integral @xmath269 where we have set @xmath270",
    "we first give a bound on the right hand side of equation  .",
    "setting @xmath273 and @xmath274 , we have ( considering that the integrand is positive and @xmath275 ) @xmath276 that is , we have to derive upper bounds on the two integrals on the right hand side .",
    "first let @xmath277 , that is @xmath278 and @xmath279 .",
    "consequently @xmath280 for @xmath281 .",
    "on the other hand , if @xmath282 we have @xmath283 for all @xmath284 . setting @xmath285",
    "we have with lemma  [ lem : prob_knn_radii ] @xmath286 for all @xmath284 .",
    "hence @xmath287 since @xmath288 for @xmath289 and @xmath202 is monotonically decreasing .",
    "therefore , for all @xmath277 @xmath290 and thus @xmath291        we have according to lemma  [ lem : boundary_strip_to_zero ] @xmath298 .",
    "consequently , using @xmath299{k_n / n})$ ] and plugging in @xmath300 @xmath301{\\frac{k_n}{n } }        + \\min \\left\\ { \\exp \\left ( - k_n/8 \\right ) f_n^q \\left ( \\inf_{x \\in c } r_n ( x ) \\right ) , \\left ( { f_b}^{(q ) } ( \\infty ) - { f_b}^{(q ) } ( r_n^{\\max } ) \\right ) \\right\\ } \\right ) .",
    "\\end{aligned}\\ ] ]    now we consider the term in equation  . in the following , note that with @xmath231 we have for all @xmath85 with @xmath302 and @xmath303 @xmath304",
    "we assume that @xmath92 is sufficiently large such that @xmath305 .    for any",
    "@xmath306 and any @xmath307 we have @xmath308 if @xmath309 we use the trivial bound @xmath310 .",
    "otherwise we have with lemma  [ lem : prob_knn_radii ] for all @xmath311 that @xmath312 with @xmath313 . using , furthermore , the bound @xmath314 we obtain @xmath315 that is , we obtain for @xmath316 @xmath317 where in the last inequality we have applied lemma  [ lem : cap_integral ] .",
    "now , we want to find an upper bound on @xmath320 for @xmath306 , that is @xmath321 .",
    "we use the following decomposition @xmath322 we use in the first term the trivial bound @xmath323 and in the second term the monotonicity of @xmath202 and the bound @xmath324 on the probability of connectedness when the distance is greater than @xmath325 from lemma  [ lem : prob_knn_radii ] to obtain @xmath326 using a bound on the density in the balls @xmath327 we obtain @xmath328 and observe that @xmath329 if @xmath330 since in this case @xmath331 .                with our choice of @xmath240",
    "we have , considering that @xmath247 , @xmath343 that is , for @xmath92 sufficiently large such that @xmath344 , considering that @xmath345{k_n / n}$ ] and plugging in @xmath346 we have @xmath347{\\frac{k_n}{n } } + \\delta_n \\right ) f_n^q \\left ( \\inf_{x \\in c } r_n^- ( x ) \\right )         \\left ( r_n^{\\max } \\right)^{d+1 } , { f_c}^{(q ) } \\left ( \\infty \\right ) - { f_c}^{(q ) } \\left ( \\inf_{x \\in c } r_n^- ( x ) \\right ) \\right\\ } \\right ) \\\\         & \\qquad \\quad + o \\left ( \\sqrt[d]{\\frac{k_n}{n } } { f_c}^{(q ) } \\left ( r_n^{\\max } \\right )        + \\min \\left\\ { \\exp \\left ( - \\delta_n^2 \\frac{k_n}{4 } \\right ) f_n^q \\left ( \\inf_{x \\in c } r_n ( s ) \\right ) ,       { f_b}^{(q ) } ( \\infty ) - { f_b}^{(q ) } ( \\inf_{x \\in c } r_n(x ) ) \\right\\ } \\right )           \\end{aligned}\\ ] ]        finally , we discuss the choice of @xmath240 . with this choice of @xmath240",
    "we have @xmath355 .",
    "note that this is the fastest convergence rate of @xmath240 for which the exponential term converges polynomially in @xmath356 , which we will need in the proof of the following corollaries .",
    "in all the other terms above @xmath240 has to be chosen as small as possible , so this is the best convergence rate for @xmath240 . note",
    "further that for this choice of @xmath240 we require @xmath357 , since @xmath240 has to converge to zero .",
    "now we proof the bound for the variance term . according to corollary  3.2.3 from  @xcite the maximum degree of the symmetric @xmath45-nearest neighbor graph",
    "is bounded by @xmath358 , where @xmath359 denotes the kissing number in dimension @xmath360 , that is , the maximum number of unit hypershpheres that touch another unit hypersphere without any intersections .",
    "thus , removing a point from the graph and inserting it in a different place the number of ( undirected ) edges in the @xmath15 can change by at most @xmath361 .",
    "since we count undirected edges twice we obtain for all types of @xmath0-nearest neighbor graphs @xmath362 where @xmath363 denotes the value of the @xmath15 in a graph where exactly one point has been moved to a different place . thus by mcdiarmid s inequality for a suitable constant @xmath364 @xmath365    the following lemma states bounds on @xmath366 , that is the probability of edges between points at @xmath58 and @xmath180 , in the cases that we need in the convergence proofs for the @xmath15 and the volume .",
    "[ lem : prob_knn_radii ] let @xmath23 be the directed , mutual or symmetric @xmath45-nearest neighbor graph .",
    "let @xmath367 be sufficiently small such that @xmath368 . then , if @xmath369 and @xmath370 we have @xmath371 .",
    "set @xmath231 and define @xmath372 .",
    "let @xmath92 be sufficiently large such that @xmath305 and let @xmath373 with @xmath236 for @xmath42 and @xmath374 for sufficiently large @xmath92 .",
    "we first show bounds on the probability of connectedness for the directed @xmath0-nearest neighbor graph .",
    "these are used in the second part of this proof in order to show bounds for the undirected graph as well .",
    "let @xmath390 denote the event that there exists an edge between @xmath145 and @xmath173 in the directed @xmath0-nearest neighbor graph .",
    "first we show the statement concerning the maximal @xmath0-nearest neighbor radius . for any @xmath85 we have @xmath391{\\frac{4}{\\gamma p_{\\min } \\eta_d } \\frac{k_n}{n-1 } } \\right ) \\right )         \\geq p_{\\min } { \\mathcal{l}}_d \\left ( b \\left ( x , \\sqrt[d]{\\frac{4}{\\gamma p_{\\min } \\eta_d } \\frac{k_n}{n-1 } } \\right ) \\cap c \\right ) \\\\        & \\geq p_{\\min } \\gamma",
    "{ \\mathcal{l}}_d \\left ( b \\left ( x , \\sqrt[d]{\\frac{4}{\\gamma p_{\\min } \\eta_d } \\frac{k_n}{n-1 } } \\right ) \\right )         = p_{\\min } \\gamma \\frac{4}{\\gamma p_{\\min } \\eta_d } \\frac{k_n}{n-1 } \\eta_d         = \\frac{4 k_n}{n-1 } .",
    "\\end{aligned}\\ ] ]    now suppose we fix @xmath244 and @xmath245 with @xmath392 .",
    "if @xmath165 denotes the random variable that counts the number of points @xmath393 in @xmath394 we have @xmath395 . setting @xmath396 , we certainly have @xmath397 for @xmath398 and thus we obtain with a tail bound for the binomial distribution from @xcite , which was first proved in  @xcite , @xmath399    in the following we show the statements concerning the upper bound @xmath325 on the @xmath0-nearest neighbor radii of points in regions of relatively homogeneous density .",
    "the proof for the lower bound @xmath400 is similar and is therefore omitted .",
    "note , however , that the technical condition @xmath401 is needed for this case .",
    "first we show how we can bound the density in the balls @xmath402 : for any @xmath403 we have by taylor s theorem @xmath404 and thus , with @xmath231 , @xmath405 these bounds are used below to bound the probability mass of balls within @xmath402 .",
    "now , we bound the probability mass in @xmath406 and @xmath407 from below , when @xmath380 .",
    "we first observe that @xmath408{\\frac{(1 + 2\\xi_n ) ( 1+\\delta_n ) k_n}{(n-1 ) p(s ) \\eta_d } }         \\leq \\sqrt[d]{\\frac{4 k_n}{(n-1 ) \\gamma p_{\\min } \\eta_d } } = r_n^{\\max } .",
    "\\end{aligned}\\ ] ]    suppose @xmath409 .",
    "then @xmath410 with @xmath411 .",
    "if @xmath412 we know that @xmath413 , since @xmath58 and @xmath180 are on different sides of the hyperplane @xmath17 .",
    "we set @xmath414 , that is the point on the line connecting @xmath187 and @xmath58 with distance @xmath325 from @xmath187 .",
    "then , by construction , @xmath415 and @xmath416",
    ". thus @xmath417      if @xmath421 we set @xmath422 , that is the point on the line connecting @xmath187 and @xmath180 with distance @xmath325 from @xmath187 .",
    "then , by construction , @xmath423 and @xmath424 . since @xmath58 and @xmath180 are on different sides of @xmath17 we have @xmath425",
    ". therefore @xmath426",
    "we show how to bound @xmath427 .",
    "the same bound can be shown for the probability mass in @xmath428 , @xmath429 and @xmath430 , since all of these balls lie in @xmath431 .",
    "we have , since @xmath305 , @xmath432    let @xmath433 and @xmath434 .",
    "then , we have for @xmath435 @xmath436 and thus , by the tail bound from  @xcite , @xmath437 we have @xmath438 and @xmath439 and thus , using @xmath440 , @xmath441 this analysis can be carried over to the case @xmath442 and the same bound holds .      in the final step of the proof we use the results derived so far to show the results for the undirected @xmath0-nearest neighbor graphs .",
    "for the mutual @xmath4 graph we have by definition @xmath445 .",
    "thus , clearly , @xmath446 and @xmath447 this implies @xmath448            by a translation and rotation of our coordinate system in @xmath18 such that @xmath460 is the origin and @xmath461 the first coordinate axis we obtain for @xmath307 @xmath462 where we have set @xmath463 thus , @xmath464      therefore , both the integrals we want to compute are equal to @xmath468 which we will treat in the following .",
    "first we are going to compute the @xmath76-dimensional integral @xmath469 .",
    "setting @xmath470 we can write @xmath469 as the following integral in @xmath471 : @xmath472 plugging in this expression for @xmath469 we obtain @xmath473    substituting with polar coordinates @xmath474 with @xmath475 $ ] and @xmath476 $ ] , we have @xmath477_{\\theta=0}^{\\pi/2 }        \\ \\mathrm{d } u         = \\frac{1}{d-1 } \\int_{u=0}^r u^{d } f(u ) \\",
    "\\mathrm{d } u       \\end{aligned}\\ ] ]        ( unweighted @xmath4-graph ) [ cor : cut_knn_unweighted ]",
    "let @xmath23 be the unweighted @xmath0-nearest neighbor graph and let @xmath202 be the unit weight function",
    ". then @xmath480{\\frac{n}{k_n } } \\operatorname{cut}_n        - \\frac{2 \\eta_{d-1}}{(d+1 ) \\eta_d^{1 + 1/d } } \\int_{s } p^{1 - 1/d } ( s ) \\ \\mathrm{d } s \\right|         = o \\left ( \\sqrt[d]{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right )       \\end{aligned}\\ ] ] and , for a suitable constant @xmath364 @xmath481{\\frac{n}{k_n } } \\operatorname{cut}_n         - { \\mathbb{e}}\\left ( \\frac{1}{n k_n } \\sqrt[d]{\\frac{n}{k_n } } \\operatorname{cut}_n\\right ) \\right| > { \\ensuremath{\\varepsilon}}\\right )        \\leq 2 \\exp \\left ( - \\tilde{c } { \\ensuremath{\\varepsilon}}^2 n^{1 - 2/d } k_n^{2/d } \\right ) .",
    "\\end{aligned}\\ ] ]      therefore , @xmath485 multiplying this term with the factor @xmath486 we obtain a constant limit .",
    "we now multiply the inequality for the bias term in proposition  [ pro : cut_knn_general ] with this factor and deal with the error terms .        for the last error term we have @xmath492{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right )         f_n \\left ( \\inf_{x \\in c } r_n^- ( x ) \\right ) \\left ( \\frac{k_n}{n } \\right)^{1 + 1/d }         = o \\left ( \\sqrt[d]{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right ) .",
    "\\end{aligned}\\ ] ]    thus , considering that @xmath493{k_n / n}$ ] , we obtain @xmath480{\\frac{n-1}{k_n } } \\operatorname{cut}_n        - \\frac{2 \\eta_{d-1}}{(d+1 ) \\eta_d^{1 + 1/d } } \\int_{s } p^{1 - 1/d } ( s ) \\",
    "\\mathrm{d } s \\right| \\\\        & \\qquad = \\left ( \\frac{n-1}{k_n } \\right)^{1 + 1/d } \\left| \\frac{\\operatorname{cut}_n}{n ( n-1 ) }         - 2 \\int_{s } p^2 ( s ) { f_c}^{(1 ) } \\left ( r_n ( s ) \\right ) \\",
    "\\mathrm{d}s \\right|        = o \\left ( \\sqrt[d]{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right ) .",
    "\\end{aligned}\\ ] ]    for the _ variance term _ we have with proposition  [ pro : cut_knn_general ] and @xmath494 @xmath481{\\frac{n-1}{k_n } } \\operatorname{cut}_n         - { \\mathbb{e}}\\left ( \\frac{1}{n k_n } \\sqrt[d]{\\frac{n-1}{k_n } } \\operatorname{cut}_n\\right ) \\right| > { \\ensuremath{\\varepsilon}}\\right )         = \\pr \\left ( \\left| \\operatorname{cut}_n - { \\mathbb{e}}\\left ( \\operatorname{cut}_n \\right ) \\right| >         n k_n \\sqrt[d]{\\frac{k_n}{n-1 } } { \\ensuremath{\\varepsilon}}\\right ) \\\\        & \\qquad \\leq 2 \\exp \\left ( - \\tilde{c } \\frac{{\\ensuremath{\\varepsilon}}^2 n^2 k_n^2 ( k_n/(n-1))^{2/d}}{n k_n^2 f_n^2 ( 0 ) } \\right )         \\leq 2 \\exp \\left ( - \\tilde{c } { \\ensuremath{\\varepsilon}}^2 n^{1 - 2/d } k_n^{2/d } \\right ) .",
    "\\end{aligned}\\ ] ]      [ cor : cut_knn_r_to_sigma_to_zero ] let @xmath23 be the @xmath0-nearest neighbor graph with gaussian weight function and let @xmath496",
    ". then @xmath497{\\frac{n}{k_n } } \\operatorname{cut}_n \\right )         - \\frac{2 \\eta_{d-1 } \\eta_d^{-1 - 1/d}}{(d+1 ) ( 2 \\pi)^{d/2 } }         \\int_s p^{1 - 1/d } ( s ) \\ \\mathrm{d } s    \\right|        = o \\left ( \\left ( \\frac{1}{\\sigma_n } \\sqrt[d]{\\frac{k_n}{n } } \\right)^{2 } + \\sqrt[d]{\\frac{k_n}{n } }         + \\sqrt{\\frac{\\log n}{k_n } } \\right )       \\end{aligned}\\ ] ] and , for a suitable constant @xmath364 @xmath481{\\frac{n}{k_n } } \\operatorname{cut}_n         - { \\mathbb{e}}\\left ( \\frac{1}{n k_n } \\sqrt[d]{\\frac{n}{k_n } } \\operatorname{cut}_n\\right ) \\right| > { \\ensuremath{\\varepsilon}}\\right )        \\leq 2 \\exp \\left ( - \\tilde{c } { \\ensuremath{\\varepsilon}}^2 n^{1 - 2/d } k_n^{2/d } \\right ) .",
    "\\end{aligned}\\ ] ]    according to lemma  [ pro : gaussian_weights_r_to_sigma_to_zero ] we have for all @xmath482 @xmath498 plugging in @xmath499{k_n/((n-1 ) \\eta_d p(s))}$ ] we obtain @xmath500{\\frac{k_n}{(n-1 ) \\eta_d p(s ) } }         \\right)^2       \\end{aligned}\\ ] ] and therefore @xmath501{\\frac{k_n}{(n-1 ) \\eta_d p(s ) } }         \\right)^2         \\leq \\tilde{c}_1 \\left ( \\frac{k_n}{\\sigma_n^d n } \\right)^{2/d }       \\end{gathered}\\ ] ] for a suitable constant @xmath502 .",
    "therefore @xmath503    now , we consider the error terms of proposition  [ pro : cut_knn_general ] .",
    "for the first one we have , using that @xmath504 and , furthermore , @xmath505{k_n/(n-1)})$ ] @xmath506{\\frac{k_n}{n } }         = o \\left ( \\sigma_n^{d }   \\left ( \\frac{n-1}{k_n } \\right)^{1 + 1/d }        \\sigma_n^{-d } \\left ( \\frac{k_n}{n-1 } \\right)^{1 + 1/d }         \\sqrt[d]{\\frac{k_n}{n } } \\right )        = o \\left ( \\sqrt[d]{\\frac{k_n}{n } } \\right ) .",
    "\\end{aligned}\\ ] ]      for the third error term we have with @xmath509 and the monotonicity of @xmath202 @xmath510{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right ) f_n \\left ( \\inf_{x \\in c } r_n^- ( x ) \\right )        \\left ( \\frac{k_n}{n } \\right)^{1 + 1/d }         = o \\left ( \\sqrt[d]{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right ) .",
    "\\end{aligned}\\ ] ]    for the _ variance term _ we have with proposition  [ pro : cut_knn_general ] and @xmath511 for a suitable constant @xmath512 @xmath513{\\frac{n-1}{k_n } } \\operatorname{cut}_n         - { \\mathbb{e}}\\left ( \\frac{\\sigma_n^d}{n k_n } \\sqrt[d]{\\frac{n-1}{k_n } } \\operatorname{cut}_n\\right ) \\right| > { \\ensuremath{\\varepsilon}}\\right ) = \\pr \\left ( \\left| \\operatorname{cut}_n - { \\mathbb{e}}\\left ( \\operatorname{cut}_n \\right ) \\right| >",
    "\\frac{n k_n}{\\sigma_n^d } \\sqrt[d]{\\frac{k_n}{n-1 } } { \\ensuremath{\\varepsilon}}\\right ) \\\\        & \\qquad \\leq 2 \\exp \\left ( - \\tilde{c } ' \\frac{{\\ensuremath{\\varepsilon}}^2 n^2 k_n^2 \\sigma_n^{-2d } ( k_n/(n-1))^{2/d}}{n k_n^2 f_n^2 ( 0 ) }",
    "\\right )        \\leq 2 \\exp \\left ( - \\tilde{c } { \\ensuremath{\\varepsilon}}^2 n^{1 - 2/d } k_n^{2/d } \\right ) ,      \\end{aligned}\\ ] ] where we have set @xmath514 .",
    "[ cor : cut_knn_sigma_to_r_to_zero ] we consider the @xmath4 graph with gaussian weight function .",
    "let @xmath516 and @xmath517 for @xmath42 .",
    "then there exists a constant @xmath364 such that @xmath518{\\frac{k_n}{n } }         + \\frac{1}{\\sigma_n } \\exp \\left ( - \\tilde{c } \\left ( \\frac{1}{\\sigma_n }         \\sqrt[d]{\\frac{k_n}{n } } \\right)^2 \\right ) \\right ) .",
    "\\end{aligned}\\ ] ] furthermore , suppose @xmath46{k_n / n } \\geq \\sigma_n^\\alpha$ ] for an @xmath132 and @xmath92 sufficiently large . then there exist non - negative random variables @xmath519 such that @xmath520 with @xmath521 for a constant @xmath522 , and @xmath523 .",
    "with lemma  [ pro : gaussian_weight_function ] we have for for @xmath46{k_n / n } / \\sigma_n$ ] sufficiently large @xmath524{\\frac{k_n}{n } } \\right)^2 \\right ) \\right ) ,      \\end{aligned}\\ ] ] where we use that @xmath44 and @xmath525 are bounded .          for the third error term",
    "we observe that if @xmath92 is sufficiently large such that @xmath530 and @xmath531 then for all @xmath85 , @xmath532{\\frac{(1 - 2\\xi_n)(1-\\delta_n ) k_n}{(n-1 ) p(x ) \\eta_d } }        \\geq \\sqrt[d]{\\frac{k_n}{4 p_{\\max } \\eta_d n } } .",
    "\\end{aligned}\\ ] ] then we have with lemma  [ pro : gaussian_weight_function ] @xmath533{\\frac{k_n}{n } } \\right)^2 \\right )   \\right ) .",
    "\\end{aligned}\\ ] ]    now we proof the bound for the variance term . unfortunately , the bound in proposition  [ pro : cut_knn_general ] based on mcdiarmid s inequality does not give good results",
    ". therefore we proof a bound on the variance term directly .",
    "we set @xmath534 to be the @xmath160 in the complete graph with gaussian weights on the sample and we set @xmath535 to be sum of the weights of the edges that are in the @xmath15 but not in the @xmath4 graph",
    ". then @xmath536 and we have @xmath537 the first deviation term is dealt with in corollary  [ cor : cut_complete_graph ] .",
    "we denote with @xmath538 the event that the @xmath0-nearest neighbor radius of all the points is greater than @xmath539{k_n/(2 p_{\\max } \\eta_d ( n-1))}$ ] .",
    "one can show similarly to the proof of lemma  [ lem : prob_knn_radii ] that @xmath540 and thus @xmath541 for sufficiently large @xmath92 , since @xmath55 . if @xmath538 holds , all the edges in @xmath535 must have weight lower than @xmath542 , whereas if @xmath543 holds the maximum edge weight is @xmath544 .",
    "there are @xmath545 possible edges and thus @xmath546 since @xmath517 for @xmath42 .    under the condition @xmath46{k_n",
    "/ n } \\geq \\sigma_n^\\alpha$ ] with @xmath132 we have for sufficiently large @xmath92 and a suitable constant @xmath547 @xmath548 where we use that the exponential term converges to zero faster than any power of @xmath39 .",
    "[ pro : volume_knn_general ] let @xmath23 be the @xmath0-nearest neighbor graph with a monotonically decreasing weight function @xmath202 and let @xmath25 or @xmath26",
    ". then @xmath550{\\frac{k_n}{n } } { f_b}^{(1 ) } \\left ( r_n^{\\max } \\right ) \\right)+ o \\left ( \\min \\left\\ { f_n^q \\left ( \\inf_{x \\in c } r_n ( x ) \\right )         n^{-\\delta_0 } , { f_b}^{(1 ) } ( \\infty ) - { f_b}^{(1 ) } ( \\inf_{x \\in c } r_n ( x ) )    \\right\\ }    \\right ) \\\\         & \\qquad \\quad + o \\left ( \\min \\left\\ { f_n^q \\left ( \\inf_{x \\in c } r_n^- ( x ) \\right ) \\left(\\sqrt[d]{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right )         \\frac{k_n}{n } , { f_b}^{(1 ) } \\left ( \\infty \\right ) - { f_b}^{(1 ) } \\left ( \\inf_{x \\in c } r_n^- ( x ) \\right )   \\right\\ } \\right ) .",
    "\\end{aligned}\\ ] ] where we set @xmath551 for a @xmath247 in the definition of @xmath229 .      similarly to the proof of for the @xmath15",
    "we define for @xmath252 , @xmath253 the random variable @xmath172 as @xmath553 and then have @xmath554 . with a function @xmath265 that indicates the probability of connectedness we obtain @xmath555 setting @xmath556 and @xmath557 we can decompose the outer integral into integrals over @xmath558 and @xmath559 .",
    "therefore , using that according to lemma  [ lem : boundary_strip_to_zero ] the volume of @xmath558 is in @xmath565 , @xmath566{\\frac{k_n}{n } } { f_b}^{(q ) } \\left ( r_n^{\\max } \\right ) \\right)\\\\        & \\qquad + o \\left ( \\min \\left\\ { \\sqrt[d]{\\frac{k_n}{n } } \\left ( { f_b}^{(q ) } ( \\infty ) - { f_b}^{(q ) } ( r_n^{\\max } ) \\right ) ,         \\sqrt[d]{\\frac{k_n}{n } } f_n^q \\left ( r_n^{\\max } \\right ) \\exp \\left ( - k_n/8 \\right )        \\right\\ } \\right ) .",
    "\\end{aligned}\\ ] ]    for @xmath382 we introduce as in the proof for the @xmath15 radii @xmath567 and @xmath568 that depend on @xmath240 and @xmath239 defined there .",
    "these radii approximate the true @xmath4 radius . for a lower bound",
    "we obtain @xmath569 for some weight functions , especially the gaussian , we can use @xmath570 whereas for other ones it is better to use @xmath571    similarly we obtain an upper bound , with an additional term @xmath572 or @xmath573 bounding the influence of points that are further away than @xmath230 . combining the bounds we obtain @xmath574 setting @xmath575 we obtain @xmath576 and the same for @xmath577 .",
    "clearly , for @xmath247 we have @xmath578 and @xmath579 .",
    "thus , with @xmath580{k_n / n})$ ] , @xmath581{\\frac{k_n}{n } } { f_b}^{(q ) } \\left ( r_n^{\\max } \\right ) \\right ) \\\\         & \\qquad \\quad + o \\left ( \\min \\left\\ { f_n^q \\left ( \\inf_{x \\in c } r_n^- ( x ) \\right ) \\left(\\sqrt[d]{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right )         \\frac{k_n}{n } , { f_b}^{(q ) } \\left ( \\infty \\right ) - { f_b}^{(q ) } \\left ( \\inf_{x \\in c } r_n^- ( x ) \\right )   \\right\\ } \\right)\\\\        & \\qquad \\quad + o \\left ( \\min \\left\\ { f_n^q \\left ( \\inf_{x \\in c } r_n ( x ) \\right )         n^{-\\delta_0 } , { f_b}^{(q ) } ( \\infty ) - { f_b}^{(q ) } ( \\inf_{x \\in c } r_n ( x ) )    \\right\\ }    \\right ) .",
    "\\end{aligned}\\ ] ]      combining all the bounds above we obtain the result for the bias term .",
    "the bound for the variance term can be obtained with mcdiarmid s inequality similarly to the proof for the @xmath15 in proposition  [ pro : cut_knn_general ] .",
    "[ cor : vol_knn_unweighted ] let @xmath23 be the unweighted @xmath4 graph with weight function @xmath490 and let @xmath25 or @xmath26",
    ". then we have for the bias term @xmath586{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right ) .",
    "\\end{aligned}\\ ] ] and for the variance term for a suitable constant @xmath250 @xmath587      therefore by multiplying the expression in proposition  [ pro : volume_knn_general ] with @xmath589 we obtain for any @xmath247 @xmath590{\\frac{k_n}{n } } { f_b}^{(1 ) } \\left ( r_n^{\\max } \\right ) \\right ) \\\\        & \\quad + o \\left ( \\frac{n-1}{k_n } f_n \\left ( \\inf_{x\\in c } r_n^{- } ( x ) \\right ) n^{-\\delta_0 } \\right ) \\\\        & \\quad + o \\left ( \\frac{n-1}{k_n } \\frac{k_n}{n } \\left ( \\sqrt[d]{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right )         f_n \\left ( \\inf_{x \\in c } r_n^- ( x ) \\right )   \\right ) .",
    "\\end{aligned}\\ ] ] using @xmath591 and @xmath490 we obtain @xmath586{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right ) .",
    "\\end{aligned}\\ ] ]      [ cor : vol_knn_r_to_sigma_to_zero ] consider the @xmath4 graph with gaussian weights and @xmath592 .",
    "let @xmath25 or @xmath26 .",
    "then we have for the bias term @xmath593{\\frac{k_n}{n } } \\right)^2      + \\sqrt[d]{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right )      \\end{aligned}\\ ] ] and for the variance term , for a suitable constant @xmath364 , @xmath594    according to lemma  [ pro : gaussian_weights_r_to_sigma_to_zero ] we have for all @xmath85 @xmath595 plugging in @xmath596{k_n/((n-1 ) \\eta_d p(x))}$ ] and dividing by @xmath597 we obtain for points in the support of @xmath44 @xmath598 therefore , using the boundedness of @xmath44 @xmath599    now , we consider the error terms from proposition  [ pro : volume_knn_general ] of the other difference @xmath600 as we have seen above @xmath601 can be bounded by a constant .",
    "thus we have for the first term @xmath602{\\frac{k_n}{n } } { f_b}^{(1 ) } \\left ( r_n^{\\max } \\right )        = o \\left ( \\sqrt[d]{\\frac{k_n}{n } } \\right ) .",
    "\\end{aligned}\\ ] ]      for the third term we have @xmath605{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right )        f_n \\left ( \\inf_{x \\in c } r_n^- ( x ) \\right )        & \\leq \\left ( \\sqrt[d]{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right )        \\sigma_n^{d } f_n \\left ( 0 \\right ) \\\\        & =   \\left ( \\sqrt[d]{\\frac{k_n}{n } } + \\sqrt{\\frac{\\log n}{k_n } } \\right )        \\frac{1}{(2 \\pi)^{d/2 } } .",
    "\\end{aligned}\\ ] ]      [ cor : vol_knn_r_to_sigma_to_infinity ] let @xmath23 be the @xmath4 graph with gaussian weights .",
    "then for the bias term for a constant @xmath502 @xmath608{\\frac{k_n}{n } }         + \\exp \\left ( - \\tilde{c}_1   \\left ( \\frac{1}{\\sigma_n } \\sqrt[d]{\\frac{k_n}{n } } \\right)^{2 }   \\right )   \\right ) .",
    "\\end{aligned}\\ ] ] let , furthermore , @xmath46{k_n / n } \\geq \\sigma_n^\\alpha$ ] for an @xmath132 and @xmath92 sufficiently large . then there exist non - negative random variables @xmath519 such that @xmath609 with @xmath521 for a constant @xmath522 , and @xmath523 .",
    "for the first error term we use that according to lemma  [ pro : gaussian_weight_function ] @xmath613 is bounded by one for @xmath92 sufficiently large .",
    "therefore @xmath46{k_n / n } { f_b}^{(1 ) } ( r_n^{\\max } )      = o ( \\sqrt[d]{k_n / n } ) $ ] .",
    "for the second and third error term we observe that if @xmath92 is sufficiently large such that @xmath530 and @xmath531 then @xmath614{\\frac{(1 - 2\\xi_n)(1-\\delta_n ) k_n}{(n-1 ) p(x ) \\eta_d } }        \\geq \\sqrt[d]{\\frac{k_n}{4 p_{\\max } \\eta_d n } } ,      \\end{aligned}\\ ] ] and therefore , for both , the second and the third error term , @xmath615{\\frac{k_n}{n } } \\right)^2 \\right ) \\right ) .",
    "\\end{aligned}\\ ] ]           of theorem  [ thm : limit_ncut_cheegercut_knn_graph ] as discussed in section  [ sec : proofs_intuitively ] we can study the convergence of the bias and variance terms of the @xmath15 and the volume separately .    for the _ unweighted graph _ we have with corollary  [ cor : cut_knn_unweighted ] that under the condition @xmath357 the bias term for the @xmath15 is in @xmath616{k_n / n } + \\sqrt{\\log n / k_n } ) $ ] . for some @xmath169 the probability that the variance term exceeds @xmath617 is bounded by @xmath618 for a suitable constant @xmath250 .",
    "clearly , the bias term converges to zero under the condition @xmath357 . for the almost sure convergence of the variance term",
    "we need the stricter condition in dimension @xmath89 .",
    "the convergence of the volume - term follows with corollary  [ cor : vol_knn_unweighted ] , since the requirements for this convergence are weaker . in the case",
    "@xmath74 we obtain the optimal rates by equating the two bounds of the bias term and checking that the variance term converges as well at this rate . in the case @xmath89 the optimal rate is determined by the variance term .    for the _ @xmath4-graph with gaussian weights and @xmath139 _ we need the stronger condition @xmath131 for an @xmath132 in order to show convergence of both , the bias term and the variance term .",
    "under this condition we have according to corollaries  [ cor : cut_knn_sigma_to_r_to_zero ] and [ cor : vol_knn_r_to_sigma_to_infinity ] that the bias term of both , the @xmath15 and the volume , is in @xmath619 , since the exponential term converges as @xmath39 .      for the _ @xmath4-graph with gaussian weights and @xmath104 _ according to corollary  [ cor : cut_knn_r_to_sigma_to_zero ] the bias term of the @xmath15 is in @xmath620 .",
    "the probability that the variance term of the @xmath15 exceeds an @xmath169 is bounded by @xmath621 for a suitable constant @xmath250 , which is the same expression as in the unweighted case .",
    "therefore , we have almost sure convergence of the @xmath15-term to zero under the same conditions as for the unweighted @xmath4 graph .",
    "this section consists of three parts : in the first one the convergence of the bias and variance term of the @xmath15 is studied , whereas in the second part that convergence is studied for the volume . combining these results we can proof the main theorems on the convergence of @xmath1 and @xmath5 for the @xmath2-graph and the complete weighted graph .",
    "section  [ sec : cut_r_graph_complete_graph ] and section  [ sec : volume_r_graph_complete_graph ] are built up similarly : first , a proposition for a general weight function is given .",
    "the results are stated in terms of the `` cap '' and `` ball '' integrals and some properties of the weight function . then",
    "four corollaries follow , where the general result is applied to the complete weighted graph with gaussian weight function and to the @xmath2-graph with the specific weight functions we consider in this paper .",
    "some words on the proofs : the results on the bias terms for general weight functions can be shown analogously to the corresponding results for the @xmath4 graph . since the connectivity in these graphs given",
    "the position of two points is not random they are even simpler .",
    "furthermore , all the error terms in the result for the @xmath4 graph that are due to the uncertainty in the connectivity radius can be dropped for the @xmath2-graph and the complete weighted graph .",
    "therefore , in the proof of the bias term of the @xmath15 we only discuss the adaptations that are made to the proof of the @xmath4 graph .    as explained in section  [ sec : proofs_intuitively ] the situation is different for the variance term , where the convergence proof for the @xmath4-graph would lead to suboptimal results when carried over to the other two graphs . for this reason we give a different proof for the convergence of the variance term in the proof of the general result for the @xmath15 . it can be easily carried over to the volume and thus we omit it there .    as to the corollaries we only proof two of them : that for the complete weighted graph and that for the @xmath2-graph with gaussian weights and @xmath104 for @xmath42 . the proof of the corollary for the unweighted graph is very simple , that of the corollary for the @xmath2-graph with gaussian weights and @xmath622 is identical to the proof for the complete weighted graph where we can ignore one term .    the proofs in section  [ sec : volume_r_graph_complete_graph ]",
    "are completely omitted : the general result on the bias term can be proved analogously to that for the @xmath4 graph , if the adaptations that are discussed in the proof for the bias term of the @xmath15 are made .",
    "the general result on the variance term of the volume is proved analogously to that on the variance term of the @xmath15 .",
    "the proofs of the corollaries also work analogously to the corresponding proofs for the @xmath15 .",
    "the proofs of the main theorems in section  [ sec : proofs_main_theorems_r_graph ] collect the bounds of the corollaries and identify the conditions that have to hold for the convergence of @xmath1 and @xmath5 .",
    "[ [ sec : cut_r_graph_complete_graph ] ] the @xmath15 term in the @xmath2-graph and the complete weighted graph ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    [ pro : lim_cut_general_weight_r_graph ] let @xmath38 be a sequence that fulfills the conditions on parameter sequences of the @xmath2-neighborhood graph .",
    "let @xmath23 denote the @xmath2-neighborhood graph with parameter @xmath40 or the complete weighted graph on @xmath120 with a monotonically decreasing weight function @xmath584 .",
    "we set @xmath623      as was said in the introduction we do not give the detailed proof of this proposition here , since it is similar to the proof of the corresponding proposition for the @xmath4-graph but simpler : the radius @xmath40 is the same everywhere , that is we can set @xmath627 for all @xmath186 .",
    "furthermore , the connectivity is not random , that is we can set @xmath628 for the @xmath2-neighborhood graph , whereas we set @xmath629,@xmath630 and @xmath631 for the complete weighted graph .",
    "we obtain @xmath632 and thus the result for the bias term immediately .",
    "the following corollary can be proved by plugging in the results of lemma  [ pro : integration_unit_weight_function ] into the bounds of proposition  [ pro : lim_cut_general_weight_r_graph ] .",
    "we do not give the details here .",
    "[ cor : cut_complete_graph ] consider the complete weighted graph @xmath23 with gaussian weight function .",
    "then we have for the bias term for any @xmath132 @xmath644 for the variance term we can find a constant @xmath364 such that for @xmath92 sufficiently large @xmath645    let @xmath40 be a sequence with @xmath49 and @xmath139 for @xmath42 .",
    "we use the bound from proposition  [ pro : lim_cut_general_weight_r_graph ] and the fact that @xmath646 can be bounded by a constant due to lemma  [ pro : gaussian_weight_function ] to obtain @xmath647                  we do not state the proof of the following corollary , since it is similar to the proof of the last one .",
    "the difference is , that we do not have to consider the @xmath664-terms , which are zero in the case of the @xmath2-graph .",
    "[ cor : cut_r_graph_sigma_to_r_to_zero ] let @xmath23 be the @xmath2-graph with gaussian weight function and let @xmath41 for @xmath42",
    ". then we have for the bias term @xmath665 for the variance term we can find a constant @xmath522 such that @xmath666    [ cor : cut_r_graph_gaussian_weights_r_to_sigma_to_zero ] consider the @xmath2-neighborhood graph with gaussian weight function and let @xmath43 for @xmath42 . then we can find a constant @xmath364 such that @xmath667 and @xmath668    multiplying the bound in proposition  [ pro : lim_cut_general_weight_r_graph ] with @xmath669 , which can be bounded by a constant according to lemma  [ pro : gaussian_weights_r_to_sigma_to_zero ] , and using @xmath670 we obtain @xmath671 on the other hand , by the boundedness of @xmath44 and @xmath525 , and with lemma  [ pro : gaussian_weights_r_to_sigma_to_zero ] @xmath672 combining these two bounds we obtain the result for the bias term .      with lemma  [ pro : gaussian_weights_r_to_sigma_to_zero ]",
    "we obtain @xmath675 for sufficiently large @xmath92 . with the same proposition and plugging in @xmath544",
    "we obtain @xmath676 . plugging in these results above",
    "we obtain the bound for the variance term .",
    "the following results are stated without proof : proposition  [ pro : volume_r_graph_general ] can be proved analogously to proposition  [ pro : volume_knn_general ] if the remarks on the difference between the @xmath4-graph and @xmath2-neighborhood graph in the proof of proposition  [ pro : lim_cut_general_weight_r_graph ] are considered .",
    "the corollaries can be shown similarly to the corresponding corollaries in the previous section .",
    "[ pro : volume_r_graph_general ] let @xmath23 be the @xmath40-neighborhood graph or the complete weighted graph with a weight function @xmath202 and set @xmath677 as in proposition  [ pro : lim_cut_general_weight_r_graph ] .",
    "then @xmath678 for the variance term we have @xmath679      [ cor : volume_complete_graph ] consider the complete weighted graph with the gaussian weight function and a parameter sequence @xmath50 .",
    "then we have for any @xmath132 @xmath682 furthermore there is a constant @xmath512 such that @xmath683    [ cor : volume_r_graph_sigma_to_r_to_zero ] let @xmath23 be the @xmath2-neighborhood graph with gaussian weights and let @xmath622 for @xmath42",
    ". then we have for the bias term for sufficiently large @xmath92 @xmath684 and for the variance term for a suitable constant @xmath512 @xmath685    [ cor : volume_r_graph_r_to_sigma_to_zero ] let @xmath23 be the @xmath2-neighborhood graph with gaussian weights and let @xmath104 for @xmath42 .",
    "then we have for the bias term for sufficiently large @xmath92 @xmath686 and for the variance term for a suitable constant @xmath364 @xmath687        for the _ unweighted @xmath2-graph _ we have with corollary  [ cor : cut_unweighted ] that the bias term of the @xmath15 is in @xmath688 and that for @xmath169 we can find a constant @xmath250 such that the probability that the variance term of the @xmath15 exceeds @xmath617 is bounded by @xmath689 .",
    "thus the @xmath15-term converges almost surely to zero for @xmath49 and @xmath141 .",
    "it follows from corollary  [ cor : volume_unweighted ] that under these conditions the @xmath690-term also converges to zero .",
    "the best convergence rate for the @xmath15-term is @xmath691{\\log n / n}$ ] , which is achieved setting @xmath692{\\log n / n}$ ] .",
    "setting @xmath40 in this way the convergence rate of the @xmath690-term is also @xmath691{\\log n / n}$ ] .    for the @xmath2-graph with _ gaussian weights and @xmath139",
    "_ we have with corollaries  [ cor : cut_r_graph_sigma_to_r_to_zero ] and  [ cor : volume_r_graph_sigma_to_r_to_zero ] that the bias term of both , the @xmath15 and the volume , is in @xmath693 . furthermore , we can find a constant @xmath364 such that the probability that the variance term of the @xmath15 exceeds an @xmath169 is bounded by @xmath689 .",
    "similarly , the variance term of the volume would converge almost surely for @xmath694 .",
    "this implies almost sure convergence of @xmath695 to zero under the condition @xmath140 for @xmath42 .    for the @xmath2-graph with _ gaussian weights and @xmath104",
    "_ we have with corollary  [ cor : cut_r_graph_gaussian_weights_r_to_sigma_to_zero ] a rate of @xmath696 for the bias term of the @xmath15 .",
    "furthermore , the probability that the variance term exceeds an @xmath169 is bounded by @xmath697 with a constant @xmath250 .",
    "therefore , the @xmath15-term almost surely converges to zero under the conditions @xmath49 and @xmath141 . under these conditions with corollary  [ cor : volume_r_graph_r_to_sigma_to_zero ] the volume - term also converges to zero .     of theorem  [ thm : limit_ncut_cheegercut_complete_graph ] as discussed in section  [ sec : proofs_intuitively ] we can study the convergence of the bias and variance terms of the @xmath15 and the volume separately .    with corollaries  [ cor : cut_complete_graph ] and",
    "[ cor : volume_complete_graph ] we have that the bias term of both , the @xmath15 and the volume is in @xmath698 for any @xmath132 . furthermore , the probability that the variance term of the @xmath15 exceeds an @xmath169 is bounded by @xmath699 with a suitable constant @xmath250 . for the variance term of the volume the exponent in this bound",
    "is only @xmath360 .",
    "consequently , we have almost sure convergence to zero under the condition @xmath140 .",
    "for any fixed @xmath132 the optimal convergence rate is achieved setting @xmath700 . since the variance term has to converge for any @xmath132 we choose @xmath701 and achieve a convergence rate of @xmath702 for any @xmath132 .",
    "[ pro : gaussian_weights_r_to_sigma_to_zero ] let @xmath202 denote the gaussian weight function with parameter @xmath39 and let @xmath705",
    ". then we have for @xmath204 for the cap integral @xmath706 for the ball integral @xmath707 we have @xmath708        the following lemma is necessary to bound the influence of points close to the boundary on the @xmath15 and the volume .",
    "the first statement is used for the @xmath15 , whereas the second statement is used for the volume .",
    "[ lem : boundary_strip_to_zero ] let the general assumptions hold and let @xmath38 be a sequence with @xmath49 for @xmath42 .",
    ". then @xmath719 . for @xmath25 or @xmath26",
    ". then @xmath721 .",
    "maier , m. , von  luxburg , u. , and hein , m. influence of graph construction on graph - based clustering measures .",
    "in koller , d. , schuurmans , d. , bengio , y. , and bottou , l. , editors , _ advances in neural information processing systems 21 _ , pages 10251032 . mit press , 2009 .",
    "narayanan , h. , belkin , m. , and niyogi , p. on the relation between low density separation , spectral clustering and graph cuts . in schlkopf , b. , platt , j. , and hoffman , t. , editors , _ advances in neural information processing systems 19 _ , pages 10251032 . mit press , 2007 ."
  ],
  "abstract_text": [
    "<S> we study the scenario of graph - based clustering algorithms such as spectral clustering . </S>",
    "<S> given a set of data points , one first has to construct a graph on the data points and then apply a graph clustering algorithm to find a suitable partition of the graph . </S>",
    "<S> our main question is if and how the construction of the graph ( choice of the graph , choice of parameters , choice of weights ) influences the outcome of the final clustering result . to this end </S>",
    "<S> we study the convergence of cluster quality measures such as the normalized cut or the cheeger cut on various kinds of random geometric graphs as the sample size tends to infinity . </S>",
    "<S> it turns out that the limit values of the same objective function are systematically different on different types of graphs . </S>",
    "<S> this implies that clustering results systematically depend on the graph and can be very different for different types of graph . </S>",
    "<S> we provide examples to illustrate the implications on spectral clustering . </S>"
  ]
}