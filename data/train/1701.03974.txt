{
  "article_text": [
    "online convex optimization ( oco ) is an emerging methodology for sequential inference with well documented merits especially when the sequence of convex costs varies in an unknown and possibly adversarial manner @xcite .",
    "starting from the seminal papers @xcite and @xcite , most of the early works evaluate oco algorithms with a _",
    "static regret _ , which measures the difference of costs ( a.k.a .",
    "losses ) between the online solution and the overall best static solution in hindsight .",
    "if an algorithm incurs static regret that increases sub - linearly with time , then its performance loss averaged over an infinite time horizon goes to zero ; see also @xcite , and references therein .    however , static regret is not a comprehensive performance metric @xcite .",
    "take online parameter estimation as an example .",
    "when the true parameter varies over time , a static benchmark ( time - invariant estimator ) itself often performs poorly so that achieving sub - linear static regret is no longer attractive .",
    "recent works @xcite extend the analysis of static regret to that of _ dynamic regret _ , where the performance of an oco algorithm is benchmarked by the best dynamic solution with a - priori information on the one - slot - ahead cost function .",
    "sub - linear dynamic regret is proved to be possible , if the dynamic environment changes slow enough for the accumulated variation of either costs or per - slot minimizers to be sub - linearly increasing with respect to the time horizon .",
    "when the per - slot costs depend on previous decisions , the so - termed competitive difference can be employed as an alternative of the static regret @xcite .",
    "the aforementioned works @xcite deal with dynamic costs focusing on problems with time - invariant constraints that must be strictly satisfied , but do not allow for instantaneous violations of the constraints .",
    "the _ long - term _ effect of such instantaneous violations was studied in @xcite , where an online algorithm with sub - linear static regret and sub - linear accumulated constraint violation was also developed .",
    "the regret bounds in @xcite have been improved in @xcite and @xcite .",
    "decentralized optimization with consensus constraints , as a special case of having long - term but time - invariant constraints , has been studied in @xcite .",
    "nevertheless , @xcite do not deal with oco under time - varying adversarial constraints .",
    "c * 3|c  reference   &  type of benchmark   & long - term constraint & adversarial constraint + @xcite & static and dynamic & no & no + @xcite & static & no & no + @xcite & dynamic & no & no + @xcite & dynamic & no & no + @xcite & dynamic & no & no + @xcite & static & yes & no + @xcite & dynamic & yes & no + this work & dynamic & yes & yes +    in this context , the present paper considers oco with time - varying constraints that must be satisfied in the long term . under this setting",
    ", the learner first takes an action without knowing a - priori either the adversarial cost or the time - varying constraint , which are revealed by the nature subsequently .",
    "its performance is evaluated by : i ) _ dynamic regret _ that is the optimality loss relative to a sequence of instantaneous minimizers with known costs and constraints ; and , ii ) _ dynamic fit _ that accumulates constraint violations incurred by the online learner due to the lack of knowledge about future constraints .",
    "we compare the oco setting here with those of existing works in table [ tab : comp ] .",
    "we further introduce a modified online saddle - point ( mosp ) method in this novel oco framework , where the learner deals with time - varying costs as well as time - varying but long - term constraints .",
    "we analytically establish that mosp simultaneously achieves sub - linear dynamic regret and fit , provided that the accumulated variations of both minimizers and constraints grow sub - linearly .",
    "this result provides valuable insights for oco with long - term constraints : _ when the dynamic environment comprising both costs and constraints does not change on average , the online decisions provided by mosp are as good as the best dynamic solution over a long time horizon . _    to demonstrate the impact of these results , we further apply the proposed mosp approach to a dynamic network resource allocation task , where online management of resources is sought without knowing future network states .",
    "existing algorithms include first- and second - order methods in the dual domain @xcite , which are tailored for time - invariant deterministic formulations .",
    "to capture the temporal variations of network resources , stochastic formulation of network resource allocation has been extensively pursued since the seminal work of @xcite ; see also the celebrated stochastic dual gradient method in @xcite .",
    "these stochastic approximation - based approaches assume that the time - varying costs are i.i.d . or generally samples from a stationary ergodic stochastic process @xcite .",
    "however , performance of most stochastic schemes is established in an asymptotic sense , considering the ensemble of per slot averages or infinite samples across time .",
    "clearly , stationarity may not hold in practice , especially when the stochastic process involves human participation .",
    "inheriting merits of the oco framework , the proposed mosp approach operates in a fully online mode without requiring non - causal information , and further admits finite - sample performance analysis under a sequence of non - stochastic , or even adversarial costs and constraints .",
    "relative to existing works , the main contributions of the present paper are summarized as follows .    1 .",
    "we generalize the standard oco framework with only adversarial costs in @xcite to account for both adversarial costs and constraints .",
    "different from the regret analysis in @xcite , performance here is established relative to the best dynamic benchmark , via metrics that we term dynamic regret and fit .",
    "we develop an mosp algorithm to tackle this novel oco problem , and analytically establish that mosp yields simultaneously sub - linear dynamic regret and fit , provided that the accumulated variations of per - slot minimizers and constraints are sub - linearly growing with time .",
    "our novel oco approach is tailored for dynamic resource allocation tasks , where mosp is compared with the popular stochastic dual gradient approach .",
    "relative to the latter , mosp remains operational in a broader practical setting without probabilistic assumptions .",
    "numerical tests demonstrate the gain of mosp over state - of - the - art alternatives .",
    "_ outline_. the rest of the paper is organized as follows . the oco problem with long - term constraints",
    "is formulated , and the relevant performance metrics are introduced in section ii . the mosp algorithm and its performance analysis",
    "are presented in section iii .",
    "application of the novel oco framework and the mosp algorithm in network resource allocation , as well as corresponding numerical tests , are provided in section iv .",
    "section v concludes the paper .    _",
    "notation_. @xmath0 denotes expectation , @xmath1 stands for probability , @xmath2 stands for vector and matrix transposition , and @xmath3 denotes the @xmath4-norm of a vector @xmath5 .",
    "inequalities for vectors , e.g. , @xmath6 , are defined entry - wise .",
    "the positive projection operator is defined as @xmath7^+:=\\max\\{\\mathbf{a},\\mathbf{0}\\}$ ] , also entry - wise .",
    "in this section , we introduce the generic oco formulation with long - term time - varying constraints , along with pertinent metrics to evaluate an oco algorithm .",
    "we begin with the classical oco setting , where constraints are time - invariant and must be strictly satisfied .",
    "oco can be viewed as a repeated game between a learner and nature @xcite .",
    "consider that time is discrete and indexed by @xmath8 . per slot @xmath8",
    ", a learner selects an action @xmath9 from a convex set @xmath10 , and subsequently nature chooses a ( possibly adversarial ) loss function @xmath11 through which the learner incurs a loss @xmath12 .",
    "the convex set @xmath13 is a - priori known and fixed over the entire time horizon .",
    "although this standard oco setting is appealing to various applications such as online regression and classification @xcite , it does not account for potential variations of ( possibly unknown ) constraints , and does not deal with constraints that can possibly be satisfied in the long term rather than a slot - by - slot basis .",
    "online optimization with time - varying and long - term constraints is well motivated for applications such as navigation , tracking , and dynamic resource allocation @xcite .",
    "taking resource allocation as an example , time - varying long - term constraints are usually imposed to tolerate instantaneous violations when available resources can not satisfy user requests , and hence allow flexible adaptation of online decisions to temporal variations of resource availability .    to broaden the applicability of oco to these scenarios",
    ", we consider that per slot @xmath8 , a learner selects an action @xmath9 from a known and fixed convex set @xmath14 , and then nature reveals not only a loss function @xmath15 but also a time - varying ( possibly adversarial ) penalty function @xmath16 .",
    "this function leads to a time - varying constraint @xmath17 , which is driven by the unknown dynamics in various applications , e.g. , on - demand data request arrivals in resource allocation .",
    "different from the known and fixed set @xmath13 , the time - varying constraint @xmath17 can vary arbitrarily or even adversarially from slot to slot .",
    "it is revealed only after the learner makes her / his decision , and hence it is hard to be satisfied in every time slot .",
    "therefore , the goal in this context is to find a sequence of online solutions @xmath18 that minimize the aggregate loss , and ensures that the constraints @xmath19 are satisfied in the long - term on average .",
    "specifically , we aim to solve the following online optimization problem    [ eq.prob ] @xmath20    where @xmath21 is the time horizon , @xmath22 is the decision variable , @xmath23 is the cost function , @xmath24^{\\top}$ ] denotes the constraint function with @xmath25th entry @xmath26 , and @xmath27 is a convex set .",
    "the formulation extends the standard oco framework to accommodate adversarial time - varying constraints that must be satisfied in the long term . complemented by algorithm development and performance analysis to be carried in the following sections , the main contribution of the present paper is incorporation of long - term and time - varying constraints to markedly broaden the scope of oco .      regarding performance of online decisions @xmath28 ,",
    "static regret is adopted as a metric by standard oco schemes , under time - invariant and strictly satisfied constraints .",
    "the static regret measures the difference between the online loss of an oco algorithm and that of the best fixed solution in hindsight @xcite . extending the definition of static regret over @xmath21 slots to accommodate time - varying constraints",
    ", it can be written as ( see also @xcite ) @xmath29 where the best static solution @xmath30 is obtained as @xmath31 a desirable oco algorithm in this case is the one yielding a sub - linear regret @xcite , meaning @xmath32 .",
    "consequently , @xmath33 implies that the algorithm is `` on average '' no - regret , or in other words , not worse asymptotically than the best fixed solution @xmath30 . though widely used in various oco applications , the aforementioned _ static regret _",
    "metric relies on a rather coarse benchmark , which may be less useful especially in dynamic settings .",
    "for instance , ( * ? ? ?",
    "* example 2 ) shows that the gap between the best static and the best dynamic benchmark can be as large as @xmath34 .",
    "furthermore , since the time - varying constraint @xmath35 is not observed before making a decision @xmath9 , its feasibility can not be checked instantaneously .    in response to the quest for improved benchmarks in this dynamic setup , two metrics are considered here : _",
    "dynamic regret _ and _ dynamic fit_. the notion of dynamic regret ( also termed tracking regret or adaptive regret ) has been recently introduced in @xcite to offer a competitive performance measure of oco algorithms under time - invariant constraints .",
    "we adopt it in the setting of by incorporating time - varying constraints @xmath36 where the benchmark is now formed via a sequence of best dynamic solutions @xmath37 for the instantaneous cost minimization problem subject to the instantaneous constraint , namely @xmath38 clearly , the dynamic regret is always larger than the static regret in , i.e. , @xmath39 , because @xmath40 is always no smaller than @xmath41 according to the definitions of @xmath30 and @xmath42 .",
    "hence , a sub - linear dynamic regret implies a sub - linear static regret , but not vice versa . to ensure feasibility of online decisions , the notion of _",
    "dynamic fit _ is introduced to measure the accumulated violation of constraints ; under time - invariant long - term constraints @xcite or under time - varying constraints @xcite .",
    "it is defined as @xmath43^+\\right\\|.\\end{aligned}\\ ] ] observe that the dynamic fit is zero if the accumulated violation @xmath44 is entry - wise less than zero . however , enforcing @xmath45 is different from restricting @xmath9 to meet @xmath46 in each and every slot .",
    "while the latter readily implies the former , the long - term ( aggregate ) constraint allows adaptation of online decisions to the environment dynamics ; as a result , it is tolerable to have @xmath47 and @xmath48 .",
    "an ideal algorithm in this broader oco framework is the one that achieves both sub - linear dynamic regret and sub - linear dynamic fit .",
    "a sub - linear dynamic regret implies `` no - regret '' relative to the clairvoyant dynamic solution on the long - term average ; i.e. , @xmath49 ; and a sub - linear dynamic fit indicates that the online strategy is also feasible on average ; i.e. , @xmath50 .",
    "unfortunately , the sub - linear dynamic regret is not achievable in general , even under the special case of where the time - varying constraint is absent @xcite . for this reason ,",
    "we aim at designing and analyzing an online strategy that generates a sequence @xmath28 ensuring sub - linear dynamic regret and fit , under mild conditions that must be satisfied by the cost and constraint variations .",
    "in this section , a modified online saddle - point method is developed to solve , and its performance and feasibility are analyzed using the dynamic regret and fit metrics .",
    "consider now the per - slot problem , which contains the current objective @xmath51 , the current constraint @xmath17 , and a time - invariant constraint set @xmath13 . with @xmath52 denoting the lagrange multiplier associated with the time - varying constraint , the online ( partial ) lagrangian of can be expressed as @xmath53 where @xmath54 remains implicit .",
    "for the online lagrangian , we introduce a modified online saddle point ( mosp ) approach , which takes a modified descent step in the primal domain , and a dual ascent step at each time slot @xmath8 .",
    "specifically , given the previous primal iterate @xmath55 and the current dual iterate @xmath56 at each slot @xmath8 , the current decision @xmath9 is the minimizer of the following optimization problem @xmath57 where @xmath58 is a positive stepsize , and @xmath59 is the gradient is non - differentiable .",
    "the performance analysis still holds true for this case .",
    "] of primal objective @xmath60 at @xmath61 . after the current decision @xmath9 is made , @xmath51 and @xmath62 are observed , and the dual update takes the form @xmath63^{+}=\\big[\\bm{\\lambda}_t+\\mu \\mathbf{g}_t(\\mathbf{x}_t)\\big]^{+}\\ ] ] where @xmath64 is also a positive stepsize , and @xmath65 is the gradient of online lagrangian with respect to ( w.r.t . ) @xmath66 at @xmath67 .    the primal gradient step of the classical saddle - point approach in @xcite is tantamount to minimizing a first - order approximation of @xmath68 at @xmath61 plus a proximal term @xmath69 .",
    "we call the primal - dual recursion and as a modified online saddle - point approach , since the primal update is not an exact gradient step when the constraint @xmath62 is nonlinear w.r.t .",
    "however , when @xmath62 is linear , and reduce to the approach in @xcite .",
    "similar to the primal update of oco with long - term but time - invariant constraints in @xcite , the minimization in penalizes the exact constraint violation @xmath62 instead of its first - order approximation , which improves control of constraint violations and facilitates performance analysis of mosp .",
    "* initialize : * primal iterate @xmath70 , dual iterate @xmath71 , and proper stepsizes @xmath58 and @xmath64 .",
    "update primal variable @xmath9 by solving .",
    "observe the current cost @xmath51 and constraint @xmath62 .",
    "update the dual variable @xmath72 via .",
    "we proceed to show that for mosp , the dynamic regret in and the dynamic fit in are both sub - linearly increasing if the variations of the per - slot minimizers and the constraints are small enough . before formally stating this result",
    ", we assume that the following conditions are satisfied .",
    "[ ass.0 ] for every @xmath8 , the cost function @xmath51 and the time - varying constraint @xmath62 in are convex .",
    "[ ass.1 ] for every @xmath8 , @xmath51 has bounded gradient on @xmath13 ; i.e. , @xmath73 ; and @xmath62 is bounded on @xmath13 ; i.e. , @xmath74",
    ".    [ ass.2 ] the radius of the convex feasible set @xmath13 is bounded ; i.e. , @xmath75 .",
    "[ ass.3 ] there exists a constant @xmath76 , and an interior point @xmath77 such that @xmath78 .",
    "assumption [ ass.0 ] is necessary for regret analysis in the oco setting .",
    "assumption [ ass.1 ] bounds primal and dual gradients per slot , which is also typical in oco @xcite .",
    "assumption [ ass.2 ] restricts the action set to be bounded .",
    "assumption [ ass.3 ] is slater s condition , which guarantees the existence of a bounded lagrange multiplier @xcite .    under these assumptions ,",
    "we are on track to first provide an upper bound for the dynamic fit .",
    "[ them1 ] define the maximum variation of consecutive constraints as @xmath79^+\\right\\|\\ ] ] and assume the slack constant @xmath80 in assumption [ ass.3 ] to be larger than the maximum variation^+ > \\max_{\\mathbf{x}\\in { \\cal x}}$ ] @xmath81^+\\big\\|$ ] , which is valid when the region defined by @xmath17 is large enough , or , the trajectory of @xmath62 is smooth enough across time . ] ; i.e. , @xmath82",
    ". then under assumptions [ ass.0]-[ass.3 ] and the dual variable initialization @xmath83 , the dual iterate for the mosp recursion - is bounded by @xmath84 and the dynamic fit in is upper - bounded by @xmath85 where @xmath86 , @xmath87 , @xmath88 , and @xmath80 are as in assumptions [ ass.1]-[ass.3 ] .",
    "see appendix [ app.thm1 ] .",
    "theorem [ them1 ] asserts that under a mild condition on the time - varying constraints , @xmath89 is uniformly upper - bounded , and more importantly , its scaled version @xmath90 upper bounds the dynamic fit .",
    "observe that with a fixed primal stepsize @xmath58 , @xmath91 is in the order of @xmath92 , thus a larger dual stepsize essentially enables a better satisfaction of long - term constraints .",
    "in addition , a smaller @xmath93 leads to a smaller dynamic fit , which also makes sense intuitively .    in the next theorem",
    ", we further bound the dynamic regret .",
    "[ them2 ] under assumptions [ ass.0]-[ass.3 ] and the dual variable initialization @xmath94 , the mosp recursion - yields a dynamic regret @xmath95 where @xmath96 is the accumulated variation of the per - slot minimizers @xmath42 defined as @xmath97 and @xmath98 is the accumulated variation of consecutive constraints @xmath99^+\\right\\|\\!.\\!\\end{aligned}\\ ] ]    see appendix [ app.thm2 ] .",
    "theorem [ them2 ] asserts that mosp s dynamic regret is upper - bounded by a constant depending on the accumulated variations of per - slot minimizers and time - varying constraints as well as the primal and dual stepsizes .",
    "while the dynamic regret in the current form is hard to grasp , the next corollary shall demonstrate that @xmath100 can be very small .",
    "based on theorems [ them1]-[them2 ] , we are ready to establish that under the mild conditions for the accumulated variation of constraints and minimizers , the dynamic regret and fit are sub - linearly increasing with @xmath21 .    [ ref.coro1 ] under assumptions [ ass.0]-[ass.3 ] and the dual variable initialization @xmath94 ,",
    "if the primal and dual stepsizes are chosen such that @xmath101 , then the dynamic fit is upper - bounded by @xmath102 in addition , if the temporal variations of optimal arguments and constraints satisfy @xmath103 and @xmath104 , then the dynamic regret is sub - linearly increasing , i.e. , @xmath105    plugging @xmath101 into , the bound in readily follows .",
    "likewise , we have from that @xmath106 considering the upper bound on the dual iterates in , it follows that @xmath107 , which implies that @xmath108 therefore , we deduce that @xmath109 , if @xmath103 and @xmath104 .",
    "observe that the sub - linear regret and fit in corollary [ ref.coro1 ] are achieved under a slightly `` strict '' condition that @xmath103 and @xmath104 .",
    "the next corollary shows that this condition can be further relaxed if a - priori knowledge of the time - varying environment is available .",
    "[ ref.coro2 ] consider assumptions [ ass.0]-[ass.3 ] are satisfied , and the dual variable is initialized as @xmath94 . if there exists a constant @xmath110 such that the temporal variations satisfy @xmath111 and @xmath112 , then choosing the primal and dual stepsizes as @xmath113 leads to the dynamic fit @xmath114 and the corresponding dynamic regret @xmath115    corollary [ ref.coro2 ] provides valuable insights for choosing optimal stepsizes in non - stationary settings . specifically , adjusting stepsizes to match",
    "the variability of the dynamic environment is the key to achieving the optimal performance in terms of dynamic regret and fit .",
    "intuitively , when the variation of the environment is fast ( a larger @xmath116 ) , slowly decaying stepsizes ( thus larger stepsizes ) can better track the potential changes .",
    "theorems [ them1 ] and [ them2 ] are in the spirit of the recent work @xcite , where the regret bounds are established with respect to a dynamic benchmark in either deterministic or stochastic settings",
    ". however , @xcite do not account for long - term and time - varying constraints , while the dynamic regret analysis is generalized here to the setting with long - term constraints .",
    "interesting though , sub - linear dynamic regret and fit can be achieved when the dynamic environment consisting of the per - slot minimizer and the time - varying constraint _ does not vary on average _ , that is , @xmath96 and @xmath98 are sub - linearly increasing over @xmath21 .",
    "although the dynamic benchmark in is more competitive than the static one in , it is worth noting that the sequence of the per - slot minimizer @xmath117 in is not the optimal solution to problem . defining the sequence of optimal solutions to as @xmath118 , it is instructive to see that computing each minimizer",
    "@xmath117 in only requires one - slot - ahead information ( namely , @xmath51 and @xmath119 ) , while computing each @xmath120 within @xmath121 requires information over the entire time horizon ( that is , @xmath122 and @xmath123 ) .",
    "for this reason , we use the subscript `` off '' in @xmath121 to emphasize that this solution comes from offline computation with information over @xmath21 slots .",
    "note that for the cases without long - term constraints @xcite , the sequence of offline solutions @xmath121 coincides with the sequence of per - slot minimizers @xmath124 .",
    "regarding feasibility , @xmath121 exactly satisfies the long - term constraint , while the solution of mosp satisfies on average under mild conditions ( cf .",
    "corollary 1 ) . for optimality , the cost of the online decisions @xmath28 attained by mosp is further benchmarked by the offline solutions @xmath118 . to this end , define mosp s _ optimality gap _ as    [ subeq.opt-gap ] @xmath125 intuitively , if @xmath118 are close to @xmath124 , the dynamic regret @xmath126 is able to provide an accurate performance measure in the sense of @xmath127 .",
    "specifically , one can decompose the optimality gap as @xmath128    where @xmath129 corresponds to the dynamic regret @xmath100 in capturing the regret relative to the sequence of per - slot minimizers with one - slot - ahead information , and @xmath130 is the difference between the performance of per - slot minimizers and the offline optimal solutions .",
    "although the second term appears difficult to quantify , we will show next that @xmath130 is driven by the accumulated variation of the dual functions associated with the instantaneous problems .    to this end , consider the dual function of the instantaneous primal problem , which can be expressed by minimizing the online lagrangian in at time @xmath8 , namely @xcite @xmath131 likewise , the dual function of over the entire horizon is @xmath132 where equality ( a ) holds since the minimization is separable across the summand at time @xmath8 , and equality ( b ) is due to the definition of the per - slot dual function in . as the primal problems and are both convex , slater s condition in assumption [ ass.3 ] implies that strong duality holds .",
    "accordingly , @xmath130 in can be written as @xmath133 which is the difference between the dual objective of the static best solution , i.e. , @xmath134 , and that of the per - slot best solution for , i.e. , @xmath135 . leveraging this special property of the dual problem",
    ", we next establish that @xmath130 can be bounded by the variation of the dual function , thus providing an estimate of the optimality gap .",
    "[ prop1 ] define the variation of the dual function from time @xmath8 to @xmath136 as @xmath137 and the total variation over the time horizon @xmath21 as @xmath138 .",
    "then the cost difference between the best offline solution and the best dynamic solution satisfies @xmath139 where @xmath117 is the minimizer of the instantaneous problem , and @xmath120 solves with all future information available . combined with",
    ", it readily follows that @xmath140 where @xmath100 is defined in , and @xmath127 in .",
    "instead of going to the primal domain , we upper bound @xmath130 via the dual representation in . letting @xmath141 denote any slot in @xmath142",
    ", we have @xmath143 the first inequality comes from the definition @xmath135 .",
    "note that if @xmath144 , the proposition readily follows from .",
    "we will prove this inequality by contradiction .",
    "assume there exists a slot @xmath145 such that @xmath146 , which implies that @xmath147 where inequalities ( a ) and ( c ) come from the fact that @xmath148 is the accumulated variation over @xmath21 slots , and hence @xmath149 , while ( b ) is due to the hypothesis above .",
    "note that @xmath150 in contradicts the fact that @xmath151 is the maximizer of @xmath152 .",
    "therefore , we have @xmath153 , which completes the proof .",
    "the following remark provides an approach to improving the bound in proposition 1 .",
    "although the optimality gap in appears to be at least linear w.r.t .",
    "@xmath21 , one can use the `` restarting '' trick for dual variables , similar to that for primal variables in the unconstrained case ; see e.g. , @xcite . specifically ,",
    "if the total variation @xmath148 is known a - priori , one can divide the entire time horizon @xmath154 into @xmath155 sub - horizons ( each with @xmath156 slots ) , and restart the dual iterate @xmath66 at the beginning of each sub - horizon . by assuming that @xmath157 is sub - linear w.r.t .",
    "@xmath21 , one can guarantee that @xmath158 always exists . in this case , the bound in can be improved by @xmath159 where the two summands are sub - linear w.r.t .",
    "@xmath21 provided that @xmath160 over each sub - horizon is sub - linear ; i.e. , @xmath161 .",
    "interested readers are referred to @xcite for details of this restarting trick , which are omitted here due to space limitation .",
    "in this section , we solve the network resource allocation problem within the oco framework , and present numerical experiments to demonstrate the merits of our mosp solver .",
    "consider the resource allocation problem over a cloud network @xcite , which is represented by a directed graph @xmath162 with node set @xmath163 and edge set @xmath164 , where @xmath165 and @xmath166 .",
    "nodes considered here include mapping nodes collected in the set @xmath167 , and data centers collected in the set @xmath168 ; i.e. , we have @xmath169 .",
    ", mapping node @xmath170 has an exogenous workload @xmath171 plus that stored in the queue @xmath172 , and schedules workload @xmath173 to data center @xmath174 .",
    "data center @xmath174 serves an amount of workload @xmath175 out of the assigned @xmath176 as well as that stored in its queue @xmath177 .",
    "the thickness of each edge is proportional to its capacity.,scaledwidth=50.0% ]    [ fig : system ]    per time @xmath8 , each mapping node @xmath170 receives an exogenous data request @xmath171 , and forwards the amount @xmath173 to each data center @xmath174 in accordance with bandwidth availability . each data center @xmath174 schedules workload @xmath175 according to its resource availability . regarding @xmath175 as the weight of a virtual outgoing edge @xmath178 from data center @xmath174 , edge set",
    "@xmath179 contains all the links connecting mapping nodes with data centers , and all the `` virtual '' edges coming out of the data centers .",
    "the @xmath180 node - incidence matrix is formed with the @xmath181-th entry @xmath182 for compactness , collect the data workloads across edges @xmath183 in a resource allocation vector @xmath184^{\\top}\\in \\mathbb{r}^{e}_+$ ] , and the exogenous load arrival rates of all nodes in a vector @xmath185^{\\top}\\in \\mathbb{r}_+^{i}$ ] .",
    "then , the aggregate ( endogenous plus exogenous ) workloads of all nodes are given by @xmath186 . when the @xmath25-th entry of @xmath186 is positive , there is service residual at node @xmath25 ; otherwise , node @xmath25 over - serves the current workload arrival .",
    "assume that each data center and mapping node has a local data queue to buffer unserved workloads @xcite . with @xmath187^{\\top}$",
    "] collecting the queue lengths at each mapping node and data center , the queue update is @xmath188^{+}$ ] , where @xmath189^+$ ] ensures that the queue length is always non - negative .",
    "the bandwidth limit of link @xmath190 is @xmath191 , and the resource capability of data center @xmath174 is @xmath192 , which can be compactly expressed by @xmath193 with @xmath194 and @xmath195^{\\top}$ ] .",
    "the overall system diagram is depicted in fig .",
    "[ fig : system ] .    for each data center ,",
    "the power cost @xmath196 depends on a time - varying parameter @xmath197 , which captures the energy price and the renewable generation at data center @xmath174 during slot @xmath8 .",
    "the bandwidth cost @xmath198 characterizes the transmission delay and is parameterized by a time - varying scalar @xmath199 .",
    "scalars @xmath197 and @xmath199 can be readily extended to vector forms . to keep the exposition simple",
    ", we use scalars to represent time - varying factors at nodes and edges .    per slot @xmath8",
    ", the instantaneous cost @xmath12 aggregates the costs of power consumed at all data centers plus the bandwidth costs at all links , namely @xmath200 where the objective can be also written as @xmath201 with @xmath202^{\\top}$ ] concatenating all time - varying parameters .",
    "aiming to minimize the accumulated cost while serving all workloads , the optimal workload routing and allocation strategy in this cloud network is the solution of the following optimization problem @xmath203^{+},\\,\\forall t\\nonumber\\\\ & \\mathbf{q}_1\\geq\\mathbf{0},~\\mathbf{q}_{t+1}=\\mathbf{0}\\end{aligned}\\ ] ] where @xmath204 is the given initial queue length , and @xmath205 guarantees that all workloads arrived have been served at the end of the scheduling horizon .",
    "note that is time - coupled , and generally challenging to solve without information of future workload arrivals and time - varying cost functions .",
    "therefore , we reformulate to fit our oco formulation by relaxing the queue recursion in , namely @xmath206 which readily leads to @xmath207 , since @xmath208 and @xmath205 . therefore ,",
    "instead of solving , we aim to tackle a relaxed problem that is in the form of oco with long - term constraints , given by @xmath209 where the workload flow conservation constraint @xmath210 must be satisfied in the long term rather than slot - by - slot .",
    "clearly , is in the form of .",
    "therefore , the mosp algorithm of section [ sec.osp ] can be leveraged to solve in an _ online _ fashion , with provable performance and feasibility guarantees .",
    "specifically , with @xmath211 , the primal update boils down to a simple gradient update @xmath212 , where @xmath213 defines projection onto the convex set @xmath13 .",
    "the dual update is @xmath214^{+}\\!$ ] , which can be nicely regarded as a scaled version of the relaxed queue dynamics in , with @xmath215 .",
    "in addition to simple closed - form updates , mosp can also afford a fully decentralized implementation by exploiting the problem structure of network resource allocation , where each mapping node or data center decides the amounts on all its _ outgoing links _ , and",
    "only exchanges information with its _ one - hop neighbors_. per time slot @xmath8 , the primal update at mapping node @xmath170 includes variables on all its outgoing links , given by    [ eq.dist-net ] @xmath216_{0}^{\\bar{x}^{jk}}\\!\\!\\!,\\;\\forall k\\in{\\cal k}\\ ] ] and the dual update reduces to @xmath217^{+}.\\ ] ] likewise , for data center @xmath174 , the primal update becomes @xmath218_0^{\\bar{y}^k}\\ ] ] where @xmath219_0^{\\bar{y}^k}\\!\\!:=\\min\\{\\bar{y}^k,\\max\\{\\cdot\\,,0\\}\\}$ ] , and the dual recursion is @xmath220^{+}.\\ ] ]    distributed mosp for online network resource allocation is summarized in algorithm [ algo2 ] .",
    "* initialize : * primal iterate @xmath70 , dual iterate @xmath71 , and proper stepsizes @xmath58 and @xmath64 .",
    "each mapping node @xmath170 performs and each data ccccenter @xmath174 runs .",
    "mapping nodes and data centers observe local costs cccand workload arrivals .",
    "each mapping node @xmath170 performs and each data ccccenter @xmath174 performs .",
    "mapping nodes ( data centers ) send multipliers to all cccneighboring data centers ( mapping nodes ) .",
    "the dynamic network resource allocation problem in section [ subsec.onrl ] has so far been studied in the stochastic setting @xcite .",
    "classical approaches include lyapunov optimization @xcite and the stochastic dual ( sub)gradient method @xcite , both of which rely on stochastic approximation ( sa ) @xcite . in the context of stochastic optimization",
    ", the time - varying vectors @xmath221 with @xmath222^{\\top}$ ] appearing in the cost and constraint are assumed to be independent realizations of a random variable @xmath223 .",
    "constitute a sample path from an ergodic stochastic process @xmath224 , which converges to a stationary distribution ; see e.g. , @xcite .",
    "] in an sa - based stochastic optimization algorithm , per time @xmath8 , a policy first observes a realization @xmath225 of the random variable @xmath223 , and then ( stochastically ) selects an action @xmath226 .",
    "however , in contrast to minimizing the _ observed cost _ in the oco setting , the goal of the stochastic resource allocation is usually to minimize the limiting average of the _ expected cost _ subject to the so - termed stability constraint , namely    [ eq.stoc-prob ] @xmath227\\label{eq.stoc - proba}\\\\ \\text{s .",
    "t.}~~&\\mathbf{q}_{t+1}=\\left[\\mathbf{q}_t+\\mathbf{a}\\mathbf{x}_t+\\mathbf{b}_t\\right]^{+}\\!,~\\forall t\\label{eq.stoc - probb}\\\\ & \\lim_{t\\rightarrow \\infty}\\frac{1}{t}\\sum_{t=1}^t \\mathbb{e}\\left[\\mathbf{q}_t\\right ] \\leq \\mathbf{0}\\label{eq.stoc - probc}\\end{aligned}\\ ] ]    where he expectation in is taken over @xmath223 and the randomness of @xmath9 and @xmath228 induced by all possible sample paths @xmath229 via ; and the stability constraint implies a finite bound on the accumulated constraint violation .",
    "in contrast to the observed costs in , each decision @xmath9 is evaluated by all possible realizations in @xmath223 here . however , as @xmath228 in couples the optimization variables over an infinite time horizon , is intractable in general .",
    "prior works @xcite have demonstrated that can be tackled via a tractable stationary relaxation , given by    [ eq.stoc-relax ] @xmath230\\\\ \\text{s .",
    "t.}~&\\lim_{t\\rightarrow \\infty}\\frac{1}{t}\\sum_{t=1}^t \\mathbb{e}\\left[\\mathbf{a}\\mathbf{x}_t+\\mathbf{b}_t\\right ] \\leq \\mathbf{0}\\label{eq.stoc - relaxb}\\end{aligned}\\ ] ]    where the time - coupling constraints and are relaxed to the limiting average constraint .",
    "such a relaxation can be verified similar to the queue relaxation in ; see also @xcite .",
    "note that is still challenging since it involves expectations in both costs and constraints , and the distribution of @xmath223 is usually unknown .",
    "even if the joint probability distribution function were available , finding the expectations would not scale with the dimensionality of @xmath223 .",
    "a common remedy is to use the stochastic dual gradient ( sdg ) iteration ( a.k.a .",
    "lyapunov optimization ) @xcite . specifically , with @xmath231 denoting the multipliers associated with the expectation constraint",
    ", the sdg method first observes one realization @xmath225 at each slot @xmath8 , and then performs the dual update as @xmath232^{+},\\;\\forall t\\end{aligned}\\ ] ] where @xmath56 is the dual iterate at time @xmath8 , @xmath186 is the stochastic dual gradient , and @xmath64 is a positive ( and typically constant ) stepsize .",
    "the actual allocation or the primal variable @xmath9 appearing in needs be found by solving the following sub - problems , one per slot @xmath8 @xmath233    for the considered network resource allocation problem , sdg in - entails a well - known cost - delay tradeoff @xcite .",
    "specifically , with @xmath234 denoting the optimal objective , sdg can achieve an @xmath235-optimal solution such that @xmath236 \\!\\leq\\ !",
    "f^*\\!+{\\cal o}(\\mu)$ ] , and guarantee queue lengths satisfying @xmath237\\!=\\!{\\cal o}({1}/{\\mu})$ ] .",
    "therefore , reducing the optimality gap @xmath235 will essentially increase the average network delay @xmath92 .",
    "the optimality of sdg is established relative to the offline optimal solution of , which can be thought as the time - average _ optimality gap _ in under the oco setting .",
    "interestingly though , the optimality gap under the stochastic setting is equivalent to the ( expected ) dynamic regret , since their ( expected ) difference @xmath238\\}_{t=1}^t)$ ] in reduces to zero . to see this , note that @xmath239 $ ] and @xmath240 $ ] are time - invariant , hence the dual problem of each per - slot subproblem in is time - invariant .",
    "this reduction means that the sdg solver of the dynamic problem in leverages its inherent stationarity ( through the stationary dual problem ) , in contrast to the non - stationary nature of the oco framework .    below",
    "we highlight several differences of the novel mosp in algorithm [ algo2 ] with the sdg recursion in - for the dynamic network resource allocation task .",
    "( d1 ) from an operational perspective , sdg observes the current state @xmath225 first , and then performs the resource allocation decision @xmath9 accordingly .",
    "therefore , at the beginning of slot @xmath8 , sdg needs to precisely know the non - causal information @xmath225 . inheriting the merits of oco ,",
    "on the other hand , mosp operates in a fully _ predictive _ mode , which decides @xmath9 without knowing the cost @xmath51 and the constraint @xmath62 ( or @xmath225 ) at time @xmath8 .",
    "this feature of mosp is of major practical importance when costs and availability of resources are not available at the point of making decisions ; e.g. , online demand response in smart grids @xcite and resource allocation in wireless networking @xcite .",
    "( d2 ) from a computational point of view , mosp reduces to a simple saddle - point recursion with primal ( projected ) gradient descent and dual gradient ascent for the network resource allocation problem , both of which incur affordable complexity .",
    "however , the primal update of sdg in generally requires solving a convex program per time slot @xmath8 , which leads to much higher computational complexity in general .",
    "( d3 ) with regards to the theoretical claims , the time - varying vector @xmath225 in sdg typically requires a rather restrictive probabilistic assumption , to establish sdg optimality in either the ensemble average @xcite or the limiting ergodic average sense @xcite .",
    "in contrast , leveraging the oco framework , mosp admits finite - sample performance analysis with non - stochastic observed costs and constraints , which can even be adversarial .      in this section ,",
    "we provide numerical tests to demonstrate the merits of the proposed mosp algorithm in the application of dynamic network resource allocation .",
    "consider the geographical workload routing and allocation task in with @xmath241 mapping nodes and @xmath242 data centers .",
    "the instantaneous network cost in is @xmath243 where @xmath244 is the energy price at data center @xmath174 at time @xmath8 , and @xmath245 is the per - unit bandwidth cost for transmitting from mapping node @xmath170 to data center @xmath174 . with the bandwidth limit @xmath191 uniformly randomly generated within @xmath246 $ ]",
    ", we set the bandwidth cost of each link @xmath190 as @xmath247 .",
    "the resource capacities @xmath248 at all data centers are uniformly randomly generated from @xmath249 $ ] .",
    "we consider the following two cases for the time - varying parameters @xmath250 and @xmath251 :    * case 1 ) * parameters @xmath250 and @xmath251 are independently drawn from time - invariant distributions .",
    "specifically , @xmath244 is uniformly distributed over @xmath252 $ ] , and the delay - tolerant workload @xmath171 arrives at each mapping node @xmath170 according to a uniform distribution over @xmath253 $ ] .    *",
    "case 2 ) * parameters @xmath250 and @xmath251 are generated according to non - stationary stochastic processes .",
    "specifically , @xmath254 with i.i.d .",
    "noise @xmath255 uniformly distributed over @xmath252 $ ] , while @xmath256 with i.i.d .",
    "noise @xmath257 uniformly distributed over @xmath258 $ ] .    finally , with time horizon @xmath259 , the stepsize in and is set to @xmath260 , and for and to @xmath261 .",
    "mosp is benchmarked by three strategies : sdg in section [ subsec.sdg ] , the sequence of per - slot best minimizers in , and the offline optimal solution that solves at once with all future costs and constraints available .",
    "note that at the beginning of each slot @xmath8 , the exact prices @xmath262 and demands @xmath263 for the coming slot are generally not available in practice @xcite .",
    "since the original sdg updates and require non - causal knowledge of @xmath262 and @xmath263 to decide @xmath9 , we modify them for fairness in this online setting by using the prices and demands at slot @xmath264 to obtain @xmath9 . in this case that we we term online dual gradient ( odg ) , the performance guarantee of sdg may not hold .",
    "nevertheless , as shown in the next , different constant stepsizes for odg s dual update in still lead to quite different performance and feasibility behaviors .",
    "for this reason , odg is studied under stepsizes @xmath265 and @xmath266 .                    figs .",
    "[ fig.cost1]-[fig.fit1 ] show the test results for case 1 under i.i.d .",
    "costs and constraints . clearly , mosp in fig .",
    "[ fig.cost1 ] converges to a smaller time - average cost than odg with the two stepsizes .",
    "the time - average cost of mosp is slightly higher than the per - slot optimal solution , as well as the offline optimal solution with all information of the costs and constraints available over horizon @xmath21 .",
    "[ fig.reg1 ] confirms the conclusion made from fig .",
    "[ fig.cost1 ] , where the dynamic regret ( cf . ) of mosp grows much slower than that of odg . regarding the dynamic fit ( cf .",
    "[ fig.fit1 ] demonstrates that odg with @xmath267 has a smaller fit than that of @xmath265 , and similar to the dynamic fit of mosp .",
    "according to the well - known trade - off between cost ( optimality ) and delay ( constraint violations ) in @xcite , increasing @xmath268 will improve the dynamic fit of odg but degrade its dynamic regret .",
    "therefore , mosp is favorable in case 1 since it has much smaller regret when its dynamic fit is similar to that of odg with @xmath267 .",
    "it is worth mentioning that theoretically speaking , the dynamic regret of mosp may not be sub - linear in this i.i.d .",
    "case , since the accumulated cost and constraint variation is not necessarily small enough ( cf . theorem [ them2 ] ) . however , mosp is robust in this aspect at least for the numerical tests we carried .",
    "simulation tests using non - stationary costs and constraints are shown in figs .",
    "[ fig.cost2]-[fig.fit2 ] .",
    "different from case 1 , the time - average cost of mosp is not only smaller than odg , but also smaller than the per - slot optimum obtained via ; see fig .",
    "[ fig.cost2 ] .",
    "a similar conclusion can be also drawn through the growths of dynamic regret in fig .",
    "[ fig.reg2 ] . from a high level , this is because the difference between the cost of the per - slot minimizers and that of the offline solutions is no longer small in the non - stationary case . regarding fig .",
    "[ fig.fit2 ] , both odg and mosp have finite dynamic fits in the sense that the accumulated constraint violations do not increase with time .",
    "the dynamic fit of mosp is much smaller than that of odg with @xmath265 , and comparable to that of odg with @xmath269 .",
    "therefore , in this non - stationary case , mosp also significantly outperforms odg in both dynamic regret and fit .",
    "oco with both adversarial costs and constraints has been studied in this paper .",
    "different from existing works , the focus is on a setting where some of the constraints are revealed after taking actions , they are tolerable to instantaneous violations , but must be satisfied on average .",
    "performance of the novel oco algorithm is measured by : i ) the difference of its objective relative to the best dynamic solution with one - slot - ahead information of the cost and the constraint ( dynamic regret ) ; and , ii ) its accumulated amount of constraint violations ( dynamic fit ) .",
    "it has been shown that the proposed mosp algorithm adapts to the considered oco setting with adversarial constraints . under standard assumptions ,",
    "mosp simultaneously yields sub - linear dynamic regret and fit , if the accumulated variations of the per - slot minimizers and adversarial constraints are sub - linearly increasing with time .",
    "algorithm design and performance analysis in this novel oco setting , under adversarial constraints and with a dynamic benchmark , broaden the applicability of oco to a wider application regime , which includes dynamic network resource allocation and online demand response in smart grids .",
    "numerical tests demonstrated that the proposed algorithm outperforms state - of - the - art alternatives under different scenarios .",
    "the authors would like to thank prof .",
    "xin wang for helpful discussions and comments .        squaring the dual variable update",
    ", we have @xmath272^{+}\\right\\|^2   \\leq \\left\\|\\bm{\\lambda}_t+\\mu \\mathbf{g}_t(\\mathbf{x}_t)\\right\\|^2 \\nonumber \\\\                                   & = \\|\\bm{\\lambda}_t\\|^2 + 2\\mu\\bm{\\lambda}_t^{\\top}\\mathbf{g}_t(\\mathbf{x}_t)+\\mu^2\\|\\mathbf{g}_t(\\mathbf{x}_t)\\|^2 .",
    "\\end{aligned}\\ ] ] and the proof is complete after rearranging terms and dividing both sides by 2 .",
    "the proof follows the steps in ( * ? ? ?",
    "* theorem 7 ) , but generalizes the result from static regret with time - invariant constraints to dynamic regret with time - varying and long - term constraints .",
    "recall that the primal iterate @xmath273 is the optimal solution to the following optimization problem ( cf . )",
    "@xmath274 then for any interior point @xmath275 in assumption [ ass.3 ] , it follows that @xmath276 where ( a ) follows by choosing @xmath277 such that @xmath278 and recalling the non - negativity of @xmath72 ; inequality ( b ) is because @xmath279 holds for any non - negative vector @xmath72 .    rearranging terms in",
    ", it follows that @xmath280 where ( c ) holds since @xmath13 confines @xmath281 and @xmath282 ; ( d ) uses the cauchy - schwartz inequality twice ; ( e ) leverages the bounds in assumption [ ass.2 ] , namely , @xmath283 , @xmath284 , and @xmath285 .    plugging into in lemma [ lemma1 ]",
    ", we have @xmath286^+\\!\\!\\!-\\epsilon\\mu\\|\\bm{\\lambda}_{t+1}\\|\\nonumber\\\\       & \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad~~+2\\mu gr+\\frac{\\mu r^2}{2\\alpha}+\\frac{\\mu^2 m^2}{2}\\nonumber\\\\       & \\stackrel{(h)}{\\leq}\\mu\\bar{v}(\\mathbf{g})\\|\\bm{\\lambda}_{t+1}\\|\\!-\\!\\epsilon\\mu\\|\\bm{\\lambda}_{t+1}\\|\\!+\\!2\\mu gr\\!+\\!\\frac{\\mu r^2}{2\\alpha}\\!+\\!\\frac{\\mu^2 m^2}{2}\\!\\!\\!\\end{aligned}\\ ] ] where ( f ) uses the upper bound in assumption [ ass.1 ] such that @xmath287 , ( g ) holds since @xmath288 , and ( h ) follows from the cauchy - schwartz inequality and the definition of the maximum variation @xmath93 in .          in this case , it follows that @xmath292^+ - \\bm{\\lambda}_{t+1}\\| \\nonumber \\\\                             & \\stackrel{(i)}{\\geq } \\|\\bm{\\lambda}_{t+2}\\| - \\|\\mu \\mathbf{g}_{t+1}(\\mathbf{x}_{t+1})\\| \\nonumber \\\\                             & \\stackrel{(j ) } { > } \\frac{2 gr+{r^2}/(2\\alpha)+(\\mu m^2)/{2}}{\\epsilon-\\bar{v}(\\mathbf{g})}\\end{aligned}\\ ] ] where ( i ) is due to the non - expansive property of the projection operator , and inequality ( j ) uses and @xmath293 in assumption [ ass.1 ] .",
    "however , since @xmath82",
    ", implies that we have @xmath294 if holds . by definition of the dual drift",
    ", @xmath294 implies that @xmath295 , which contradicts and .",
    "in addition , observe that the dual variable is initialized by @xmath83 , and consequently @xmath296 .",
    "therefore , for every @xmath8 , we have that @xmath297 holds .    using the dual recursion in , it follows that @xmath298 . rearranging terms",
    ", we have @xmath299 with @xmath300 , implies that @xmath301^+\\!\\!\\leq { \\bm{\\lambda}_{t+1}}/{\\mu}$ ] , which completes the proof by taking norms on both sides and using the dual upper bound .      per slot @xmath8 , the primal update @xmath273 is the minimizer of the optimization problem in ; hence , @xmath302 where ( a ) uses the strong convexity of the objective in ; see also ( * ? ? ?",
    "* corollary 1 ) . adding @xmath12 in yields @xmath303 where ( b ) is due to",
    "the convexity of @xmath51 , and ( c ) comes from the fact that @xmath288 and the per - slot optimal solution @xmath42 is feasible ( i.e. , @xmath304 ) such that @xmath305 .",
    "next , we bound the term @xmath306 by @xmath307 where @xmath308 is an arbitrary positive constant , and ( d ) is from the bound of gradients in assumption [ ass.1 ] .",
    "plugging into , we have @xmath309 where equality ( e ) follows by choosing @xmath310 to obtain @xmath311 .    using the dual drift bound in lemma [ lemma1 ] again , we have @xmath312^++\\!\\frac{\\mu m^2}{2}\\!+\\frac{\\alpha g^2}{2}\\nonumber\\\\      \\stackrel{(h)}{\\leq}&f_t(\\mathbf{x}^*_t)\\!+\\!\\frac{1}{2\\alpha}\\big(\\|\\mathbf{x}^*_t\\!-\\!\\mathbf{x}_t\\|^2\\!-\\!\\|\\mathbf{x}^*_t\\!-\\!\\mathbf{x}_{t+1}\\|^2\\big)\\!+\\!\\|\\bm{\\lambda}_{t+1}\\|v(\\mathbf{g}_t)\\nonumber\\\\      & \\qquad\\qquad+\\frac{\\mu m^2}{2}\\!+\\frac{\\alpha g^2}{2 }      \\end{aligned}\\ ] ] where",
    "( f ) follows from ; ( g ) uses non - negativity of @xmath72 and the gradient upper bound @xmath313 ; and ( h ) follows from the cauchy - schwartz inequality and the definition of the constraint variation @xmath314 in .    by interpolating intermediate terms in @xmath315 , we have that @xmath316 where ( i ) follows from the radius of @xmath13 in assumption [ ass.2 ] such that @xmath317 . plugging into",
    ", it readily leads to @xmath318    summing up over @xmath319 , we find @xmath320 where ( j ) uses the upper bound of @xmath89 in that we define as @xmath321 , and ( k ) follows from the definition of accumulated variations @xmath98 in .",
    "the definition of dynamic regret in finally implies that @xmath322 where ( @xmath323 ) follows since : i ) @xmath324 due to the compactness of @xmath325 ; ii ) @xmath326 ; and , iii ) @xmath327 if @xmath94 .",
    "this completes the proof .",
    "a.  jadbabaie , a.  rakhlin , s.  shahrampour , and k.  sridharan , `` online optimization : competing with dynamic comparators , '' in _ intl .",
    "conf . on artificial intelligence and statistics _ ,",
    "san diego , ca , may 2015 .",
    "a.  mokhtari , s.  shahrampour , a.  jadbabaie , and a.  ribeiro , `` online optimization in dynamic environments : improved regret rates for strongly convex problems , '' in _ proc .",
    "ieee conf . on decision and control _ , las vegas , nv , dec . 2016 .",
    "n.  chen , j.  comden , z.  liu , a.  gandhi , and a.  wierman , `` using predictions in online optimization : looking forward with an eye on the past , '' in _ proc .",
    "acm sigmetrics _ , antibes juan - les - pins , france , jun .",
    "2016 , pp . 193206",
    ".    l.  l. andrew , s.  barman , k.  ligett , m.  lin , a.  meyerson , a.  roytman , and a.  wierman , `` a tale of two metrics : simultaneous bounds on competitiveness and regret , '' in _ proc . annual conf .",
    "on learning theory _ , princeton , nj , jun .",
    "2013 .",
    "s.  paternain and a.  ribeiro , `` online learning of feasible strategies in unknown environments , '' _ ieee trans .",
    "_ , to appear , 2016 .",
    "[ online ] .",
    "available : https://arxiv.org/pdf/1604.02137v1.pdf                  a.  beck , a.  nedic , a.  ozdaglar , and m.  teboulle , `` an @xmath329 gradient method for network resource allocation problems , '' _ ieee trans .",
    "control of network systems _ , vol .  1 , no .  1 ,",
    "6473 , mar .",
    "2014 .",
    "l.  tassiulas and a.  ephremides , `` stability properties of constrained queueing systems and scheduling policies for maximum throughput in multihop radio networks , '' _ ieee trans .",
    "_ , vol .",
    "37 , no .  12 , pp .",
    "19361948 , dec . 1992 .",
    "a.  g. marques , l.  m. lopez - ramos , g.  b. giannakis , j.  ramos , and a.  j. caamao , `` optimal cross - layer resource allocation in cellular networks using channel - and queue - state information , '' _ ieee trans . veh .",
    "_ , vol .",
    "61 , no .  6 , pp",
    ". 27892807 , jul .",
    "2012 .",
    "t.  chen , x.  wang , and g.  b. giannakis , `` cooling - aware energy and workload management in data centers via stochastic optimization , '' _",
    "ieee j. sel .",
    "topics signal process . _ ,",
    "10 , no .  2 ,",
    "402415 , mar ."
  ],
  "abstract_text": [
    "<S> existing approaches to online convex optimization ( oco ) make sequential one - slot - ahead decisions , which lead to ( possibly adversarial ) losses that drive subsequent decision iterates . </S>",
    "<S> their performance is evaluated by the so - called _ regret _ that measures the difference of losses between the online solution and the _ best yet fixed _ overall solution in _ </S>",
    "<S> hindsight_. the present paper deals with online convex optimization involving adversarial loss functions and adversarial constraints , where the constraints are revealed after making decisions , and can be tolerable to instantaneous violations but must be satisfied in the long term . </S>",
    "<S> performance of an online algorithm in this setting is assessed by : i ) the difference of its losses relative to the _ best dynamic _ solution with one - slot - ahead information of the loss function and the constraint ( that is here termed _ </S>",
    "<S> dynamic regret _ ) ; and , ii ) the accumulated amount of constraint violations ( that is here termed _ dynamic fit _ ) . in this context , a modified online saddle - point ( mosp ) scheme is developed , and proved to simultaneously yield sub - linear dynamic regret and fit , provided that the accumulated variations of per - slot minimizers and constraints are sub - linearly growing with time . </S>",
    "<S> mosp is also applied to the dynamic network resource allocation task , and it is compared with the well - known stochastic dual gradient method . under various scenarios , numerical experiments demonstrate the performance gain of mosp relative to the state - of - the - art .    </S>",
    "<S> constrained optimization , primal - dual method , online convex optimization , network resource allocation . </S>"
  ]
}