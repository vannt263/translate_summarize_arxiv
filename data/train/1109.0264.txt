{
  "article_text": [
    "distributed storage systems have reached such a massive scale that recovery from failures is now part of regular operation rather than a rare exception  @xcite .",
    "large scale deployments typically need to tolerate multiple failures , both for high availability and to prevent data loss .",
    "erasure coded storage achieves high failure tolerance without requiring a large number of replicas that increase the storage cost @xcite .",
    "three application contexts where erasure coding techniques are being currently deployed or under investigation are cloud storage systems , archival storage , and peer - to - peer storage systems like cleversafe and wuala ( see e.g.  @xcite )    one central problem in erasure coded distributed storage systems is that of maintaining an encoded representation when failures occur . to maintain the same redundancy when a storage node leaves the system , a _ newcomer _",
    "node has to join the array , access some existing nodes , and exactly reproduce the contents of the departed node . repairing a node failure in an erasure coded system requires in - network combinations of coded packets , a concept called network coding .",
    "network coding has been investigated for numerous applications including p2p systems , wireless ad hoc networks and various storage problems ( see e.g. @xcite ) .    in this paper",
    "we focus on network coding techniques for exact repair of a node failure in an erasure coded storage system  @xcite , @xcite .",
    "there are several metrics that can be optimized during repair : the total information read from existing disks during repair  @xcite , the total information communicated in the network @xcite ( called repair bandwidth  @xcite ) , or the total number of disks required for each repair  @xcite .",
    "currently , the most well - understood metric is that of repair bandwidth . for designing @xmath1 erasure codes that have @xmath2 storage nodes and can tolerate any @xmath3 failures , an information theoretic tradeoff between the repair bandwidth @xmath4 and the storage per node @xmath5 was established in  @xcite , using cut - set bounds on an information flow graph .",
    "explicit code constructions exist for the the two extreme points on this bandwidth - storage tradeoff , see e.g.  @xcite . despite this substantial amount of prior work",
    ", there are no practical code constructions of efficiently repairable codes with data rates above @xmath0 .",
    "further , different performance metrics might be of interest in different applications .",
    "it seems that for cloud storage applications the main performance bottleneck is the disk i / o overhead for repair , which is proportional to the number of nodes @xmath6 involved in rebuilding a failed node .        * our contribution",
    "* : in this paper we introduce the first family of distributed storage codes that have simple look - up repair and can achieve arbitrarily high rates .",
    "our constructions are very simple to implement and perform exact repair by simple packet combinations .",
    "specifically , we design simple regenerating codes ( src ) that have high - rate , very small disk - i / o @xmath6 , and minimal repair computation .",
    "an @xmath7-src is a code for @xmath2 storage nodes that can tolerate @xmath3 erasures , where each node stores a fraction @xmath8 of the file size in coded chunks . to repair a single coded chunk we need to access @xmath9 disks and read @xmath10 chunk from each disk .",
    "the regeneration of an entire lost node costs a fraction @xmath11 in repair bandwidth and @xmath12 disk accesses .",
    "our codes have rate @xmath13 , which can be made arbitrarily close to @xmath14 , for constant in @xmath15 erasure resiliency .",
    "we experimentally evaluate the proposed codes in a realistic cloud storage simulator that models node rebuilds in hadoop .",
    "our simulator was initially validated on a real hadoop system of @xmath16 machines connected by a @xmath10gb / s network .",
    "our subsequent experiment involves @xmath17 machines and compares the performance of src to replication and standard reed - solomon codes .",
    "we find that srcs add a new attractive point in the design space of redundancy mechanisms for cloud storage .",
    "the first requirement from our storage code is the @xmath1 property : a code will be storing information in @xmath2 storage nodes and should be able _ to tolerate any combination of @xmath3 failures _ without data loss .",
    "we refer to codes that have this reliability as `` _ @xmath1 erasure codes _ , '' or codes that have `` _ the @xmath1 property_. ''    one well - known class of erasure codes that have this property is the family of maximum distance separable ( mds ) codes  @xcite . in short ,",
    "an mds code is a way to take a data object of size @xmath18 , split it into chunks of size @xmath19 and create @xmath2 chunks _ of the same size _ that have the @xmath1 property .",
    "it can be seen that mds codes achieve the @xmath1 property with the minimum storage overhead possible : any @xmath15 storage nodes jointly store @xmath18 bits of useful information , which is the minimum possible to guarantee recovery .",
    "our second requirement is efficient exact repair  @xcite .",
    "when one node fails or becomes unavailable , the stored information should be easily reconstructable using other surviving nodes .",
    "simple regenerating codes achieve the @xmath1 property and simple repair simultaneously by separating the two problems .",
    "large mds codes are used to provide reliability against any @xmath3 failures while very simple xors applied _ over the mds coded packets _ provide efficient exact repair when single node failures happen .",
    "-src . ]",
    "we give a first overview of our construction through a simple example in fig .",
    "[ code_42 ] , which shows an @xmath20-src .",
    "the original data object is split in @xmath21 chunks @xmath22 .",
    "we first encode @xmath23 $ ] in @xmath24 $ ] and @xmath25 $ ] in @xmath26 $ ] using any standard @xmath27 mds code .",
    "this can be easily done by multiplication of the data with the @xmath28 generator matrix @xmath29 of the mds code to form @xmath24=[f_1\\;f_2]{\\bf g}$ ] and @xmath26=[f_3\\;f_4]{\\bf g}$ ] .",
    "then we generate a parity out of each `` level '' of coded chunks , i.e. , @xmath30 , which results in an aggregate of @xmath31 chunks .",
    "we circularly place these chunks in @xmath21 nodes , each storing @xmath32 , as shown in fig . 1 .",
    "it is easy to check that this code has the @xmath1 property and in fig .",
    "[ rec_42 ] we show an example by failing nodes @xmath10 and @xmath21 .",
    "any two nodes contain two @xmath33 and two @xmath34 chunks which through the outer mds codes can be used to recover the original data object .",
    "we note that the parity chunks are not used in this process , which shows the sub - optimality of our construction .     in a @xmath35-src ]    in fig .",
    "[ rep_42 ] , we give an example of a single node repair of the @xmath35-src .",
    "we assume that node @xmath10 is lost and a newcomer joins the system . to reconstruct @xmath36",
    ", the newcomer has to download @xmath37 and @xmath38 from nodes @xmath32 and @xmath21 .",
    "this simple repair scheme is possible due to the way that we placed the chunks in the @xmath21 storage nodes : each node stores @xmath32 chunks with different index .",
    "the newcomer reconstructs each lost chunk by downloading , accessing , and xoring @xmath39 other chunks . in this process",
    "the outer mds codes are not used .    in short",
    "our codes combine outer mds codes and simple parities to provide fault tolerance and efficient repair respectively . due to this separation of duties ,",
    "our codes are suboptimal . however , as we show subsequently this optimality loss corresponds to asymptotically negligible loss in storage efficiency and only a logarithmic factor overhead compared to the optimal information theoretic storage bounds .",
    "we now present our general src construction for the @xmath40 case .",
    "let a file @xmath41 , of size @xmath42 , that we cut into @xmath39 parts , say @xmath43,\\ ] ] where @xmath44 , for @xmath45 $ ] , where @xmath46 = \\{1,\\ldots , n\\}$ ] and @xmath47 is the finite field over which all operations are performed .",
    "our coding process , is a two - step one : first we independently encode each of the file parts using an outer mds code and generate simple parity sum out of them .",
    "then we store the coded chunks and the parity sum chunks in a specific way in @xmath2 storage components .",
    "this encode and place scheme enables easy repair of lost chunks and arbitrary erasure tolerance .",
    "we start with an @xmath1 mds code that we use to encode _ independently _ each of the @xmath39 file parts of size @xmath15 , @xmath48 and @xmath49 , into two coded vectors , @xmath50 and @xmath51 , of length @xmath2 .",
    "this encoding process is given by @xmath52 where @xmath53 is the outer mds code generator matrix .",
    "we pose no requirements on that mds code , in the sense that any @xmath1 mds design will work for our purposes .",
    "the maximum distance of the code ensures that any @xmath15 encoded chunks of @xmath50 can reconstruct @xmath48 ; the same goes for any @xmath15 chunks from @xmath51 , i.e. , we can use them to reconstruct @xmath48 .",
    "we continue by generating a parity sum vector by adding the two coded vectors @xmath50 and @xmath51 @xmath54 where @xmath55 ; we note that the index @xmath56 of the parity sum @xmath57 is the same as the subscript of the @xmath39 coded chunks that generate it .",
    "this process yields @xmath58 chunks : @xmath59 coded chunks in the vectors @xmath50 and @xmath51 , and @xmath2 parity sum chunks , i.e. , the vector @xmath60 .",
    "we proceed by placing these @xmath58 chunks in @xmath2 storage nodes in the following way : each storage node will be storing @xmath32 chunks , one from @xmath50 , one from @xmath51 , and one from the parity vector @xmath61 .",
    "we require that these @xmath32 chunks do not share a subscript .",
    "this subscript requirement can be guaranteed by the following circular placement of chunks in the @xmath62-th node @xmath63,\\ ] ] where @xmath64 $ ] and @xmath65 denotes modulus addition on the ring @xmath66 ( for example @xmath67 ) .",
    "the above circular chunk placement results in the following coded array of @xmath2 storage nodes + @xmath68   + we can observe that for @xmath69 , indeed the @xmath32 chunks of each node do not share a subscript .",
    "in this section , we present the erasure resiliency and coding rate of the @xmath70-src and prove the following theorem . due to lack of space we do not present some proofs in full length and we give sketches instead . the extended version of the paper with full proofs can be found online at @xcite .",
    "the @xmath70-src can tolerate any possible combination @xmath3 erasures and has effective coding rate @xmath71 .    :",
    "the @xmath1 property of the src is inherited by the underlying mds outer codes : we can always retrieve the file by connecting to any subset of @xmath15 nodes of the storage array .",
    "any subset of @xmath15 nodes contain @xmath15 chunks of each of the two file parts @xmath48 and @xmath49 , which can be retrieved by inverting the corresponding @xmath72 submatrices of the mds generator matrix @xmath29 hence , the @xmath1 property of the two identical outer mds pre - codes renders gives the @xmath70-src its @xmath1 property .",
    "we proceed by calculating the coding rate ( space efficiency ) @xmath73 of the @xmath70-src , by considering the ratio of the total amount of useful stored information , to the total amount of data that is stored .",
    "that is , the ratio of the initial file size to the expedited storage @xmath74 @xmath75 + hence , the @xmath70-src is an erasure code with rate upper bounded by @xmath76 : for fixed erasure tollerance , @xmath77 , the src can have rate arbitrarily close to @xmath76 , that is , @xmath78    the @xmath70 src construction that is presented in this section can be generalize to constructions where the rate can be made arbitrarily high .",
    "this is done by increasing the amount of chunks stored per node and the degree of the parity sums from @xmath39 to @xmath9 .",
    "these constructions are presented in section iii .      for the general @xmath70-src , when a single node is lost , or a single chunk of that lost node is requested to be accessed , the repair process is initiated .",
    "to sustain high data availability in the presence of chunk and node erasures , the repair process has to be fast and simple : it should be low cost with respect to information read , communicated , and with respect to the number of total disk accesses .",
    "the circular placement of chunks in the src enables easy repair of single lost chunks , or single node failures , with respect to the aforementioned metrics .",
    "this is due to the fact that each chunk that is lost shares an index with @xmath39 more chunks stored in @xmath39 distinct nodes . by contacting these @xmath39 remaining nodes",
    ", we can repair the lost chunk by a simple xor operation . for the repair of a single chunk or a single node",
    ", we have the following theorem .",
    "the repair of a single chunk of the @xmath70-src costs @xmath39 in repair bandwidth and chunk reads , that is a fraction @xmath79 of the file size , and @xmath39 disk accesses .",
    "moreover , the repair of a single node failure costs @xmath80 in repair bandwidth and chunk reads , that is a fraction @xmath81 of the file size , and @xmath21 in disk accesses .",
    "let for example node @xmath64 $ ] fail , that is , chunks @xmath33 , @xmath82 , and @xmath83 are lost .",
    "then , a newcomer joins the storage array and wishes to regenerate the lost information . to reconstruct @xmath84 , the newcomer connects to the two chunks available in the storage system that share the same subscript @xmath62 , i.e. , it connects to the node that contains the parity @xmath85 and to the node that contains the chunk @xmath86 .",
    "the newcomer can then restore the lost chunk @xmath33 simply by subtracting @xmath34 from the parity @xmath87 .",
    "this repair process is summarized in the following @xmath32 steps .    [ cols=\"^,<\",options=\"header \" , ]     hence , repairing a single coded chunk requires @xmath88 chunk downloads , reads , @xmath9 and disk accesses . to reconstruct the parity sum chunk @xmath89",
    ", we need to connect to the @xmath9 nodes that contain the chunks @xmath90 , @xmath91 $ ] which generate it .    to repair a single node failure we need to communicate and read @xmath92 symbols .",
    "the total number of disk accesses for a single node repair is given by the number of distinct indices in the set @xmath93 to enumerate the distinct indices in @xmath94 , we first count the number of distinct indices between sets @xmath95 and @xmath96 for all @xmath91 $ ] .",
    "we observe that @xmath97 and @xmath98 that is , for any two `` consecutive '' chunk repairs , we need to access @xmath99 storage nodes . starting with @xmath9 disk accesses for the first chunk repair",
    ", each additional chunk repair requires an additional disk access , with respect to what has already been accessed .",
    "the total number of disks accessed is @xmath100 therefore , to repair a single node failure an aggregate of @xmath101 disk accesses is required , when @xmath102 . if @xmath103 then the number of total disk accesses is @xmath104 .",
    "@xmath75 + in fig .",
    "[ comparison ] , we give a comparison table between mds , msr , mbr , and simple regenerating codes , with respect to 1 ) storage capacity per node @xmath5 , 2 ) repair bandwidth per single node repair @xmath4 , 3 ) number of disk accesses per single node repair @xmath6 , and 4 ) effective coding rate @xmath73 .",
    "we consider msr and mbr codes that connect to @xmath105 remaining nodes for a single node failure .",
    "observe that the number of disk acceses in the src is a design parameter that can be set to a constant by appropriately choosing @xmath9 , which can be orders less than @xmath15 .",
    "regenerating codes @xcite have the property that a single node failure can be repaired by any subset of @xmath6 remaining nodes , and @xmath106 is fixed by the specific code design . in sharp contrast , srcs are look - up repair codes : for a single node failure , only a specific @xmath107 subset of the remaining nodes can reconstruct the file and @xmath107 can be a constant , or a function of @xmath15 that potentially grows much slower than @xmath108 .      in this subsection , we consider the asymptotics of the src .",
    "what happens if we fix @xmath109 and let the degree of parities @xmath9 grow as a function of @xmath15 ?",
    "let for example @xmath110 then , the repair of a single node costs @xmath111 , with @xmath112 . in comparison ,",
    "a single node failure of an @xmath1 msr code costs @xmath113 .",
    "if we let @xmath15 and @xmath2 grow and fix @xmath114 we obtain @xmath115 the effective coding rate of the src is given by @xmath116 therfore , compared to repair optimal mds codes , i.e. msr codes , srcs with @xmath117 sacrifice asymptotically negligible coding rate and have a logarithmic overhead compared to minimum bandwidth node repair , when at the same time they attain very easy repair based on simple xors , with logarithmic in @xmath15 number of disk accesses .",
    "in addition to our theoretical analysis , we evaluate srcs in a realistic cloud storage simulator .",
    "we only tested srcs with @xmath40 in this paper .",
    "this case allows the most efficient repair but at somewhat high storage overhead .",
    "we leave the exploration of other choices of @xmath9 and the involved tradeoffs as future work .",
    "we first present the architecture of the cloud storage system that our simulator is modeling .",
    "the architecture contains one master server and a great number of data storage servers , similar to that of gfs  @xcite and hadoop  @xcite .",
    "as a cloud storage system may store up to tens of petabytes of data , we expect numerous failures and hence fault tolerance and high availability are critical . to offer high data reliability , the master server needs to monitor the health status of each storage server and detect failures promptly .    in the systems of interest",
    ", data is partitioned and stored as a number of fixed - size chunks , which in hadoop can be 64 mb or 128 mb .",
    "chunks form the smallest accessible data units and in our system are set to be 64 mb . to tolerate storage server failures ,",
    "replication or erasure codes are employed to generate redundant chunks .",
    "then , several chunks are grouped and form a redundancy set  @xcite .",
    "if one chunk is lost , it can be reconstructed from other surviving chunks .",
    "to repair the chunks due to a failure event , the master server will initiate the repair process and schedule repair jobs .",
    "we implemented a discrete - event simulator of a cloud storage system using a similar architecture and data repair mechanism as hadoop . to provide accurate simulation results , our simulator models most entities of the involved components such as machines and chunks .",
    "when performing repair jobs , the simulator keeps track of the details of each repair process which gives us a detailed performance analysis .",
    "we first calibrated our simulator to accurately model the data repair behavior of hadoop . during the validation",
    ", we ran one experiment on a real hadoop system .",
    "this system contains 16 machines , which are connected by a 1gb / s network .",
    "each machine has about 410 gb data , namely approximately 6400 chunks .",
    "then , we manually failed one machine , and let hadoop repair the lost data . after the repair was completed , we analyzed the log file of hadoop and derived repair time of each chunk .",
    "next , we ran a similar experiment in our simulator .",
    "we also collected the repair time of each chunk from the simulation .",
    "we present the cdf of the repair time of both experiments in fig .",
    "[ sim : fig : cdf ] .",
    "[ sim : fig : cdf ] shows that the repair result of the simulation matches the results of the real hadoop system very well , particularly when the percentile is below 95 .",
    "therefore , we conclude that the simulator can precisely simulate the data repair process of hadoop .",
    "now we observe how storage overhead varies when we grow @xmath118 .",
    "we compare three codes : 3-way replication , reed - solomon ( rs ) codes , and src . to make the storage overhead easily understood , we define the cost of storing one byte as the metric of how many bytes",
    "are stored for each useful byte .",
    "obviously , high cost results in high storage overhead . as 3-way",
    "replication is a popularly used approach , we use it as the base line for comparison .",
    "the result is presented in fig .  [",
    "sim : fig : cost ] .",
    "[ sim : fig : cost ] shows that when @xmath3 is fixed , the normalized cost of both the rs - code and the src decreases as @xmath2 grows .",
    "when @xmath118 grows to @xmath119 , the normalized cost of src is 0.54 , and that of rs - code is 0.36 . in other words ,",
    "@xmath120 srcs need approximately half the storage of 3-way replication .",
    "it is worth noting that the cost of srcs will further reduce if we use larger values of @xmath9 , but at the cost of slower repair .",
    "[ sim : sec : repair ] in this experiment , we measure the throughput of repairing one failed data server .",
    "the experiment involves a total of 100 machines , each storing 410 gb of data .",
    "we fail at random one machine and start the data repair process .",
    "after the repair is finished , we measure the elapsed time and calculate the repair throughput .",
    "the results are shown in fig .",
    "[ sim : fig : repair ] .",
    "note that the throughput of using 3-way replication is constant across different @xmath118 since there is no such dependency on these parameters .        from fig .",
    "[ sim : fig : repair ] we can make two observations .",
    "first , 3-way replication has the best repair performance followed by src , while the rs - code offers the worst performance .",
    "this is not surprising due to the amount of data that has to be accessed for the repair .",
    "second , the repair performance of src remains constant on various @xmath1 , but the performance of rs - code becomes much worse as @xmath2 grows .",
    "this is one of the major benefits of src , i.e. , the repair performance can be independent from @xmath118 .",
    "furthermore , the repair throughput of src is about 500mb / s , approximately 64% of the 3-way replication s performance .      in a real system",
    ", repair can take place in two situations .",
    "one situation is when we need to repair a failed data storage server .",
    "another situation is when we wish to read a piece of data , but it is stored in a storage server that is currently unavailable .",
    "the two situations differ in whether the repaired data is stored or not .",
    "the first situation is a regular repair operation , which writes the repaired data back to the system .",
    "the second situation repairs the data in the main memory and then simply drops it after serving the read request .",
    "we call the latter degraded read .",
    "the degraded read performance is important , since clients can notice performance degradations when servers have temporary or permanent failures .",
    "we use a similar experimental environment to what we presented in section  [ sim : sec : repair ] .",
    "the only difference is after a chunk is repaired , we do not write it back . the performance results are presented in fig .",
    "[ sim : fig : read ] .",
    "we can also make two observations from fig .  [",
    "sim : fig : read ] .",
    "first , for all three codes , the performance trend of degraded read performance is similar to that of repair performance , shown in fig .",
    "[ sim : fig : repair ] .",
    "second , for a code with the same @xmath1 , the degraded read performance is higher than that of repair performance , due to less accessed data .",
    "again , src achieves approximately 60% degraded read performance of 3-way replication .",
    "now we analyze the data reliability of an src cloud storage system .",
    "we use a simple markov model  @xcite to estimate the reliability . for simplicity",
    ", failures happen only to disks and we assume no failure correlations .",
    "we note that we expect correlated failures to further benefit srcs over replication since they spread the data to more nodes and hence achieve better diversity protection under correlated failure scenarios .",
    "this , however , remains to be verified in a more thorough experimental study of coded cloud storage systems .",
    "we assume that the mean time to failure ( mttf ) of a disk is 5 years and the system stores 1pb data . to be conservative ,",
    "the repair time is 15 minutes when using 3-way replication and 30 minutes for src , which is in accordance to fig .",
    "[ sim : fig : repair ] . in the case of rs - code , the repair time depends on @xmath15 of @xmath118 . with these parameters ,",
    "we first measure the reliability of one redundancy set , and then use it to derive the reliability of the entire system .",
    "the estimated mttf of the entire storage system is presented in fig .",
    "[ sim : fig : reliability ] .",
    "[ sim : fig : reliability ] shows that the data reliability of the @xmath32-way replication is in the order of @xmath121 .",
    "this is consistent with the results in  @xcite .",
    "we can observe that the reliability of srcs is much higher than 3-way replication .",
    "even for the high rate ( low storage overhead ) @xmath119 case , srcs are several orders of magnitude more reliable than 3-way replication .",
    "this is benefited from the high repair speed of srcs .",
    "rs codes show a significantly different trend .",
    "although the reliability of @xmath122 and @xmath123 are higher than 3-way replication , the reliability of the rs - code reduces greatly when @xmath118 grows .",
    "this happens because their repair performance rapidly decreases as @xmath15 grows .",
    "we introduced a novel family of distributed storage codes that are formed by combining mds codes and simple locally decodable parities for efficient repair and high fault tolerance .",
    "we theoretically show that our codes have the @xmath1 reliability , have asymptotically optimal storage and are within a logarithmic factor from optimality in repair bandwidth .",
    "one very significant benefit is that the number of nodes that need to be contacted for repair can be made a small constant , independent of @xmath124 .",
    "further , srcs can be easily implemented by combining any prior mds code implementation with xoring of coded chunks and the appropriate chunk placement into nodes .",
    "we presented a comparison of the proposed codes with replication and reed - solomon codes using a cloud storage simulator .",
    "we have interest on relatively large values of @xmath118 because when we keep @xmath3 constant , larger values of @xmath15 impose lower storage overhead ( higher code rates ) .",
    "standard reed - solomon codes can not operate in this regime since their repair cost increases linearly in @xmath15 . on the contrary ,",
    "srcs require only a constant number of nodes involved in each repair and can therefore achieve very good storage overhead with good performance . as an example , if we compare a @xmath120 src with 3-way replication we find that the src requires approximately half the storage but has approximately 60% worse degraded read performance .",
    "the main strength of the src in this comparison , however , is that it provides approximately four more zeros of data reliability compared to replication .",
    "the comparison with reed - solomon leads almost certainly to a win of srcs when slightly more storage is allowed .    in conclusion",
    "we think that srcs add new feasible points in the tradeoff space of distributed storage codes .",
    "they deliver comparable performance to 3-way replication and significantly higher data reliability at a lower storage cost .",
    "our preliminary investigation therefore suggests that srcs should be attractive for real cloud storage systems .",
    "a. asterjadhi , e. fasolo , m. rossi , j. widmer , and m. zorzi .",
    "`` towards network coding - based protocols for data broadcasting in wireless ad hoc networks . '' in _ ieee transactions on wireless commun .",
    "_ , vol . 9 , pp . 662  673 , feb",
    "2010      o. khan , r. burns , j. plank , and c. huang , `` in search of i / o - optimal recovery from disk failures , '' to appear in _ hot storage 2011 , 3rd workshop on hot topics in storage and file systems _ , portland , or , jun .",
    ", 2011 .",
    "z. wang , a. g. dimakis , and j. bruck , `` rebuilding for array codes in distributed storage systems , '' in _ proc .",
    "workshop on the application of communication theory to emerging memory technologies ( actemt ) _ , 2010 .",
    "l. xiang , y. xu , j.c.s .",
    "lui , and q. chang , `` optimal recovery of single disk failure in rdp code storage systems '' in _ proc .",
    "acm sigmetrics ( 2010 ) international conference on measurement and modeling of computer systems _",
    "rashmi , n. b. shah , p. v. kumar , and k. ramchandran  explicit construction of optimal exact regenerating codes for distributed storage , \" in _ allerton conf . on control , comp . , and comm .",
    "_ , urbana - champaign , il , september 2009 .",
    "k.  rashmi , n.  b. shah , and p.  v. kumar , `` optimal exact - regenerating codes for distributed storage at the msr and mbr points via a product - matrix construction , '' submitted to ieee transactions on information theory .",
    "preprint available at at http://arxiv.org/pdf/1005.4178 .",
    "s. el rouayheb and k. ramchandran , `` fractional repetition codes for repair in distributed storage systems , '' in proc . of",
    "_ 48th allerton conf . on commun . ,",
    "control and comp .",
    "_ , monticello , il , september 2010 . , ",
    "v. r. cadambe , c. huang , s. a. jafar , and j. li , `` optimal repair of mds codes in distributed storage via subspace interference alignment , '' _ arxiv pre - print 2011_. preprint available at http://arxiv.org/abs/1106.1250 .",
    "k. w. shum and y. hu , `` exact minimum - repair - bandwidth cooperative regenerating codes for distributed storage systems , '' to appear in _ 2011 ieee symposium on information theory ( isit)_. preprint available at http://arxiv.org/abs/1102.1609 .",
    "q.  xin , e.  l. miller , d.  d.  e. long , s.  a. brandt , t.  schwarz , and w.  litwin , `` reliability mechanisms for very large storage systems , '' in _",
    "msst 03 : proc .",
    "of the 20th ieee symposium on massive storage systems and technologies _ , 2003 .",
    "d.  ford , f.  labelle , f.  i. popovici , m.  stokely , v .- a .",
    "t.  l. barroso , c.  grimes , and s.  quinlan , `` availability in globally distributed storage systems , '' in _",
    "osdi 10 : proc . of the 9th usenix symposium on operating systems design and implementation _ , 2010 ."
  ],
  "abstract_text": [
    "<S> network codes designed specifically for distributed storage systems have the potential to provide dramatically higher storage efficiency for the same availability . </S>",
    "<S> one main challenge in the design of such codes is the exact repair problem : if a node storing encoded information fails , in order to maintain the same level of reliability we need to create encoded information at a new node . </S>",
    "<S> one of the main open problems in this emerging area has been the design of simple coding schemes that allow exact and low cost repair of failed nodes and have high data rates . </S>",
    "<S> in particular , all prior known explicit constructions have data rates bounded by @xmath0 .    in this paper </S>",
    "<S> we introduce the first family of distributed storage codes that have simple look - up repair and can achieve arbitrarily high rates . </S>",
    "<S> our constructions are very simple to implement and perform exact repair by simple xoring of packets . </S>",
    "<S> we experimentally evaluate the proposed codes in a realistic cloud storage simulator and show significant benefits in both performance and reliability compared to replication and standard reed - solomon codes . </S>"
  ]
}