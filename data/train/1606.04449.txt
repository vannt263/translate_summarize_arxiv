{
  "article_text": [
    "in its simplest form , a standard one dimensional recurrent neural network ( rnn ) can be defined as @xmath0 \\right ) , \\quad \\pmb y(t ) = \\pmb w_2 \\left [ \\begin{array}{c }      \\pmb x(t ) \\\\      1      \\end{array } \\right],\\ ] ] where @xmath1 is a discrete time index , @xmath2 an element - wise sigmoid function , @xmath3 the input sequence , @xmath4 the hidden state sequence , @xmath5 the output sequence , and @xmath6 and @xmath7 are two weight matrices with proper dimensions .",
    "rnn is a powerful tool for sequence modeling , and its gradient can be conveniently calculated , e.g. , via backpropagation through time ( bptt ) @xcite .",
    "unfortunately , learning rnn turns out to be extremely difficult when it is used to solve problems requiring long term memories @xcite . exploding and vanishing gradients , especially the latter one , are suspected to be the causes . hence long and short term memory ( lstm ) and its variants @xcite",
    "are invented to overcome the vanishing gradient issue mainly by the use of forgetting gates . however , as a specially modified model , lstm may still fail to solve certain problems that are suitable for its architecture , e.g. , finding xor relationship between two binary symbols with a long lag .",
    "furthermore , as a more complicated model , it does not necessarily always outperform the standard rnn model on certain natural problems as reported in @xcite .",
    "another way to address vanishing gradient is to directly penalize rnn connections encouraging vanishing gradients @xcite .",
    "however , as an ad hoc method , its impact on the convergence and performance of rnn training is unclear .",
    "one important discovery made in @xcite is that rnn requiring long term memories can be trained using hessian free optimization , a conjugate gradient ( cg ) method tailored for neural network training with the use of a backpropagation like procedure for curvature matrix - vector product evaluation @xcite .",
    "however , due to its use of line search , hessian free optimization requires a large mini - batch size for gradient and cost function evaluations , making it computationally demanding for problems with large training sample sizes .",
    "recently , a preconditioned stochastic gradient descent ( psgd ) algorithm is proposed in @xcite .",
    "it is a simple and general procedure to upgrade a stochastic gradient descent ( sgd ) algorithm to a second - order algorithm by exploiting the curvature information extracted exclusively from noisy stochastic gradients .",
    "it is virtually tuning free , and applicable equally well to both convex and non - convex problems , a striking difference from many optimization algorithms , including the hessian free one , which assume positive definite hessian matrices at least for their derivations .",
    "naturally , we are curious about its performance on rnn training , especially on those challenging pathological synthetic problems since they are effectively impossible for sgd @xcite .",
    "our results suggest that although the issue of exploding and vanishing gradients arises naturally in rnn , efficient learning is still possible when the gradients are properly preconditioned .",
    "experimental results on the mnist handwritten digit recognition task suggest that preconditioning helps to improve convergence as well even when no long term memory is required .",
    "we briefly summarize the psgd theory in @xcite . let us consider the minimization of cost function , @xmath8,\\ ] ] where @xmath9 is a parameter vector to be optimized",
    ", @xmath10 is a random vector , @xmath11 is a loss function , and @xmath12 takes expectation over @xmath10 . at the @xmath13th iteration of psgd , we evaluate two stochastic gradients over the same randomly drawn samples : the original gradient @xmath14 at point @xmath15 , and a perturbed gradient @xmath16 at point @xmath17 , where @xmath18 is a tiny random vector . by introducing gradient perturbation as @xmath19 , a positive definite preconditioner , @xmath20 , can be pursued by minimizing criterion @xmath21,\\ ] ] where @xmath12 takes expectation over random vector @xmath18 . under mild conditions ,",
    "such a @xmath20 exists and is unique @xcite . as a result ,",
    "the psgd learning rule is written as , @xmath22 where @xmath23 is a normalized step size .",
    "the preconditioner can be conveniently estimated using stochastic relative ( natural ) gradient descent with mini - batch size @xmath24 .",
    "the rationality of psgd is that minimizing criterion ( [ criterion ] ) leads to a preconditioner scaling the gradient such that the amplitude of @xmath25 approximately matches that of @xmath18 as @xmath26 \\pmb p_k = e[\\delta \\pmb \\theta_k\\delta \\pmb \\theta_k^t]$ ] . when gradient noise vanishes , we have @xmath27 , a relationship comparable to @xmath28 , where @xmath29 is the hessian at @xmath15",
    ". hence psgd can be regarded as a stochastic version of the deterministic newton method when @xmath20 converges to @xmath30 and @xmath31 .",
    "but unlike the newton method , psgd applies equally well to non - convex optimizations since @xmath20 can be chosen to be positive definite even when @xmath29 is indefinite .    in the context of rnn training",
    ", @xmath20 damps exploding gradients and amplifies vanishing gradients by trying to match the scales of vectors @xmath25 and @xmath18 . in this way , a single preconditioner solves both the exploding and vanishing gradient issues in learning rnn , while conventionally , several different strategies are developed and combined to fix these two issues , e.g. , gradient clipping , using penalty term to discourage vanishing gradient , forgetting gate , etc ..        it is straightforward to apply psgd to rnn training by stacking all the elements in @xmath6 and @xmath7 to form a single coefficient vector @xmath32 .",
    "the resultant preconditioner has no sparsity .",
    "hence , such a brutal force solution is practical only for small scale problems with up to thousands of parameters to learn .      for large scale problems ,",
    "it is necessary to enforce certain sparse structures on the preconditioner so that it can be stored and manipulated on computers .",
    "supposing the dimensions of @xmath3 , @xmath4 and @xmath5 are @xmath33 , @xmath34 and @xmath35 respectively , one example is to enforce @xmath36 to have form @xmath37 where the dimensions of positive definite matrices @xmath38 , @xmath39 , @xmath40 , and @xmath41 are @xmath34 , @xmath42 , @xmath35 and @xmath43 respectively , and @xmath44 and @xmath45 denote kronecker product and direct sum respectively .",
    "algorithms for learning these @xmath46 , @xmath47 , are detailed in @xcite as well .",
    "we mainly study the performance of psgd with sparse preconditioner due to its better scalability with respect to problem sizes .",
    "we consider a real world handwritten digit recognition problem @xcite , and a set of pathological synthetic problems originally proposed in @xcite and restudied in @xcite .",
    "details of these pathological synthetic problems can be found in @xcite and the supplement of @xcite . for continuous problems ( outputs are continuous ) , mean squared error ( mse ) loss is used , and for discrete problems ( outputs are discrete ) , cross entropy loss is used .",
    "the same parameter settings as in @xcite are used for psgd , and no problem - specific hand tweaking is made .",
    "specifically , the preconditioner is initialized to identity matrix , and then updated using stochastic relative gradient descent with mini - batch size @xmath24 , step size @xmath48 and sampling @xmath49 from gaussian distribution @xmath50 element - wisely , where @xmath51 is the accuracy in double precision .",
    "the recurrent matrix of rnn is initialized to a random orthogonal matrix such that neither exploding nor vanishing gradient issue is severe at the beginning , loosely comparable to setting large initial biases in the forgetting gates of lstm @xcite .",
    "other non - recurrent weights are element - wisely initialized to small random numbers drawn from normal distribution .",
    "mini - batch size @xmath52 and step size @xmath48 are used for rnn training .",
    "program code written in matlab and supplemental materials revealing more detailed experimental results can be found at https://sites.google.com/site/lixilinx/home/psgd .",
    "we consider the addition problem from @xcite where a rnn is trained to predict the sum of a pair of marked , but randomly located , continuous random numbers in a sequence .",
    "for sgd , clipped stochastic gradient with clipping threshold @xmath24 is used to address the exploding gradient issue .",
    "sgd seldom succeeds on this problem when the sequence length is no less than @xmath52 . to make the problem easier , sequences with length uniformly distributed in range",
    "@xmath53 $ ] are used for training , hoping that sgd can learn the desired patterns from shorter sequences and then generalize them to longer ones .",
    "1 shows three learning curves for three algorithms using the same initial guess and step size : sgd , psgd with a sparse preconditioner , and psgd with a dense preconditioner .",
    "clearly , psgd with a dense preconditioner converges the fastest .",
    "the sparse preconditioner helps a lot as well , despite its simplicity .",
    "sgd converges the slowest",
    ".     neurons . , title=\"fig:\",scaledwidth=50.0% ]",
    "+      we consider the four groups of pathological synthetic problems in @xcite .",
    "the first group includes the addition , multiplication , and xor problems ; the second group includes the @xmath54-bit and @xmath55-bit temporal order problems ; the third group only has the random permutation problem ; and the fourth group are the @xmath56-bit and @xmath57-bit noiseless memorization problems .",
    "totally we have eight problems . in the addition and multiplication problems",
    ", rnn needs to memorize continuous random numbers with certain precision for many steps . in the @xmath54-bit and @xmath55-bit temporal order problems , rnn needs to memorize two and three widely separated binary bits and their order , respectively .",
    "the xor problem challenges both rnn and lstm training since this problem can not be decomposed into smaller ones . in the random permutation problem ,",
    "rnn is taught to predict random unpredictable symbols , except the one at the end of sequence , leading to extremely noisy gradients .",
    "on the contrary , all symbols in the @xmath56-bit and @xmath57-bit memorization problems , except those information carrying bits , can be trivially predicted , but are not task related , thus diluting the importance of task related gradient components .    we follow the experimental configurations in @xcite so that the results can be compared .",
    "the results reported in @xcite could be biased because according to the descriptions in @xcite , for most problems , rnn is trained on sequences with length uniformly distributed in range @xmath58 $ ] .",
    "this considerably facilitates the training since rnn has chances to learn the desired patterns from short sequences and then to generalize them to long ones , as shown in experiment 1 .",
    "we follow the configurations in @xcite to ensure that there is no short time lag training exemplar to facilitate learning .    among these eight problems ,",
    "the @xmath56-bit memorization problem is special in the way that it only has @xmath59 distinct input sequences .",
    "hence we set its mini - batch size to @xmath59 .",
    "then the gradient is exact , no longer stochastic .",
    "psgd applies to deterministic optimization as well , but extra cares need to be taken to prevent the arising of an ill - conditioned hessian since psgd is essentially a second - order optimization algorithm .",
    "note that the cross entropy loss is invariant to the sum of elements in @xmath5 .",
    "thus @xmath7 only needs to have @xmath60 degrees of freedom .",
    "its extra @xmath43 degrees of freedom cause singular hessian all over the parameter space .",
    "we remove those extra @xmath43 degrees of freedom in @xmath7 by constraining all its columns having zero sum .",
    "we would like to point out that gradient noise in stochastic gradient naturally regularizes the preconditioner estimation as shown in @xcite .",
    "hence we have no need to remove those extra @xmath43 degrees of freedom in @xmath7 for the other five discrete problems .    only the psgd with sparse preconditioner",
    "is tested . for each problem , four sequence lengths , @xmath61 , @xmath62 , @xmath52 and @xmath63 , are considered . for each problem with each sequence length , five independent runs starting from different random initial guesses are carried out . a run is said to be failed when it fails to converge within the maximum allowed number of iterations , which is set to @xmath64 for psgd .",
    "table i summarizes the failure rate results .",
    "note that rnn training may take a long time .",
    "hence , we have not finished all five runs for a few test cases due to limited resources .    .",
    "psgd s failure rate shown as ( number of failed runs)/(number of total runs ) on eight pathological problems [ cols=\"^,^,^,^,^\",options=\"header \" , ]      not every practical rnn learning problem is as pathological as the above studied synthetic problems .",
    "still , psgd could take nontrivial advantages over sgd such as faster and better convergence even when no long term memory is required . here",
    ", the classic mnist handwritten digit recognition task is considered @xcite .",
    "the original @xmath65 images are zero padded to @xmath66 ones .",
    "2 shows the architecture of a small but deep two dimensional rnn used to recognize the zero padded images .",
    "no long term memory is required as either dimension only requires eight steps of back propagation .",
    "pixels without overlapping .",
    "feature dimensions in the five rnn layers are @xmath67 , @xmath59 , @xmath68 , @xmath69 , and @xmath70 , respectively .",
    "boldface arrow points the propagation direction of state variables in each recurrent layer .",
    "the last layer is a softmax function with the last state of the last recurrent layer as its input .",
    ", title=\"fig:\",scaledwidth=50.0% ] +    both sgd and psgd start from the same random initial guess , and use the same step size and mini - batch size .",
    "psgd uses layer - wise kronecker product preconditioner .",
    "no preprocessing , pretraining or artificially distorted version of the original training samples is used .",
    "3 plots the test error rate convergence curves . here , the test error rate is the ratio of the number of misclassified testing samples to the total number of testing samples . from fig .  3",
    ", one observes that psgd always converges faster and better than sgd .",
    "it is interesting to compare the test error rates here with that listed on @xcite achieved by convolutional neural networks without using distorted version of the original training samples . here ,",
    "sgd and psgd converge to test error rates @xmath71 and @xmath72 , respectively .",
    "they are comparable to the ones listed on @xcite achieved using convolutional neural networks without and with pretraining , respectively .",
    "preconditioned stochastic gradient descent ( psgd ) is a general and simple learning algorithm , and requires little tuning effort .",
    "we have tested psgd on eight pathological synthetic recurrent neural network ( rnn ) training problems .",
    "although these problems may fail stochastic gradient descent ( sgd ) miserably , psgd works quite well on them , and even could outperform hessian - free optimization , a significantly more complicated algorithm than both sgd and psgd . while sgd is workable for many practical problems without requiring long term memory , psgd still provides nontrivial advantages over it such as faster and better convergence as demonstrated in the mnist handwritten digit recognition example .    unlike many traditional second - order optimization algorithms which assume positive definite hessian",
    ", psgd is designed for both convex and non - convex optimizations .",
    "this might explains its superior performance even its implementation is just slightly more complicated than sgd .",
    "psgd works well with small mini - batch sizes to reduce computational complexity due to its inherent ability to damp gradient noise naturally , while many off - the - shelf algorithms require a large mini - batch size for accurate gradient and cost function evaluations to facilitate line search .",
    "furthermore , psgd is easier to use since its step size is normalized , saving the trouble of step size selection by either hand tweaking or using step size searching algorithms .",
    "its preconditioner can have flexible forms , providing trade off room between performance and complexity .",
    "these properties make psgd an attractive alternative to sgd and many other stochastic optimization algorithms ."
  ],
  "abstract_text": [
    "<S> this paper studies the performance of a recently proposed preconditioned stochastic gradient descent ( psgd ) algorithm on recurrent neural network ( rnn ) training . </S>",
    "<S> psgd adaptively estimates a preconditioner to accelerate gradient descent , and is designed to be simple , general and easy to use , as stochastic gradient descent ( sgd ) . </S>",
    "<S> rnns , especially the ones requiring extremely long term memories , are difficult to train . </S>",
    "<S> we have tested psgd on a set of synthetic pathological rnn learning problems and the real world mnist handwritten digit recognition task . </S>",
    "<S> experimental results suggest that psgd is able to achieve highly competitive performance without using any trick like preprocessing , pretraining or parameter tweaking .    preconditioned stochastic gradient descent , stochastic gradient descent , recurrent neural network , optimization . </S>"
  ]
}