{
  "article_text": [
    "efficient solvers for partial differential equations ( pdes ) are required in all areas of science and engineering .",
    "the design and implementation of these solvers requires a wide set of skills , including , but not limited to : knowledge of the system being simulated ; creation and implementation of appropriate numerical schemes ; analysis of the resulting linear and nonlinear operators ; and efficient low - level implementation of the chosen schemes .",
    "it is therefore rare that a single individual possesses the full range of skills to successfully deliver both an algorithmically and computationally efficient solver on their own .    to address this multi - disciplinary problem requires software frameworks that enable scientists with complementary skills and specialisations to collaborate _ without _ each one of them requiring full knowledge of the system",
    "this can be achieved by carefully designing composable interfaces that capture the natural abstraction at each level of the simulation stack .",
    "an important application is the simulation of fluid systems which are described by the navier stokes equations .",
    "in contrast to other mixed finite element methods such as the taylor - hood element , discretisations based on mimetic finite element methods ( see e.g.  @xcite ) exactly preserve certain physical properties of the system under study .",
    "in particular , the divergence theorems @xmath0 and @xmath1 hold exactly for the _ discretised _ operators ; this is not the case for some other mixed methods such as the taylor - hood element .",
    "mimetic methods reproduce the favourable properties of the c - grid staggering , but allow the use of higher order discretisations and non - orthogonal grids .",
    "this is particularly important in global atmospheric modelling applications , where the `` pole problem '' of standard longitude - latitude grids introduces artificial time step limitations near the pole and restricts the parallel scalability of the code  @xcite .",
    "while mimetic finite element schemes are very popular in applications such as numerical climate- and weather- prediction , their efficient implementation poses significant challenges . to deliver forecasts under tight operational timescales",
    ", solvers have to be algorithmically robust , make optimal use of modern manycore chip architectures and scale to large node counts on distributed memory clusters .",
    "elliptic pdes arise in implicit time stepping methods of the atmospheric equations of motion and their solution often forms the bottleneck of the full model code .",
    "since the earth s atmosphere is represented by a thin spherical shell , the resulting discrete system is highly anisotropic .",
    "this requires the use of bespoke preconditioners , in particular the tensor - product multigrid approach in  @xcite has turned out to algorithmically efficient  @xcite .",
    "the implementation of multigrid algorithms for low - order finite difference discretisations on structured grids is straightforward but this is not the case for sophisticated finite element discretisations on more complex geometries for several reasons : since the hdiv ( velocity ) mass matrix is not diagonal , the iterative solver algorithm is significantly more involved than the corresponding finite difference version . in the finite element case",
    "the innermost compute kernels can not be expressed as simple stencil applications and - to achieve optimal performance on a particular architecture - the loop nest has to be optimised , bearing in mind hardware specific properties such as the cache layout . in a distributed memory setting halo exchanges and",
    "overlapping of computation and communication require a careful partitioning of the grid .",
    "the firedrake / pyop2  @xcite framework allows the automatic assembly of finite element operators from their weak formulation and the expression of the algorithm at a high abstraction level .",
    "architecture dependent optimised low - level c - code is automatically generated and executed with just - in - time compilation techniques .    in this paper",
    ", we discuss the extension of the firedrake framework to support the development of geometric multigrid solvers for finite element problems .",
    "we illustrate its use , and investigate the performance of the resulting method , in the development of a geometric multigrid solver for a mimetic finite element discretisation of the atmospheric equations of motion .",
    "this is a challenging problem and an excellent test case for the developed abstractions since both the implementation of the underlying finite element scheme and the design of an efficient multigrid method are non - trivial .",
    "we show how our approach simplifies the implementation of the solver , by cleanly separating the different aspects of the model .",
    "in particular we demonstrate how the chosen abstractions allow the easy implementation of a column - local matrix representation which is crucial for the block - jacobi smoother .",
    "this particular smoother is algorithmically optimal for strongly anisotropic problems and a key ingredient of the tensor - product multigrid algorithm in  @xcite .",
    "a careful performance analysis confirms that our implementation is efficient in the sense that it uses a significant fraction of the system s peak performance for bandwidth - bound applications .",
    "an alternative and very successful approach to the implementation of finite element solvers is the use of templated c++ code  @xcite .",
    "this allows the user full control over all components of the algorithm and multigrid solvers for finite element discretisations have been implemented for example in dune  @xcite and deal.ii  @xcite .",
    "however , the key computational kernels ( such as the local application of the operator in weak form ) have to be written by hand and any optimisation is limited by the capabilities of the available compiler .",
    "in contrast , frameworks like fenics  @xcite and firedrake use domain - specific compilers to carry out optimisations that are infeasible for general purpose compilers to perform on a low - level representation of the same algorithm  @xcite . compared to fenics , where expressing non - finite element operations requires the programmer to explicitly manage all parallelism and mesh iteration , one of the advantages of our approach is the straightforward implementation of any local operations as computational kernels in pyop2  @xcite ; this is crucial for the preconditioners considered in this work .",
    "the paper is structured as follows . in section  [ sec : firedrakepyop2 ] , we give a brief overview of the firedrake software framework .",
    "section  [ sec : multigrid - operators ] lays out the mathematical abstractions of the multigrid method , and how we organise the software abstractions around them .",
    "the multigrid method for our model problem , an atmospheric gravity wave , is discussed in section  [ sec : multigrid3d ] .",
    "we characterise the performance of the resulting scheme in section  [ sec : results ] and conclude in section  [ sec : conclusion ] .    [",
    "[ main - achievements ] ] main achievements + + + + + + + + + + + + + + + + +    we demonstrate the performance and parallel scalability of the solver on the archer supercomputer and compare our geometric multigrid implementation to a matrix - explicit implementation based on the petsc library  @xcite . in the latter case",
    "we use the boomeramg  @xcite preconditioner from the hypre suite  @xcite to solve the dg - pressure system .",
    "we show that the performance of the solver for the low order discretisations treated in the paper is memory bound , and quantify the absolute performance of the most computationally intensive kernels in our solver algorithm by a detailed analysis of memory traffic .",
    "we find that the computationally most expensive kernels utilise a significant fraction of the peak memory bandwidth .",
    "firedrake  @xcite is a python system for the solution of partial differential equations by the finite element method .",
    "it builds on the abstractions introduced in the fenics project  @xcite to present a high - level , automated , problem solving environment .",
    "firedrake enforces a strong separation of concerns between _ employing _ the finite element method , the _ implementation _ of the local discretisation of the mathematical operators , and their parallel execution over a mesh .",
    "the execution of kernels over the mesh is carried out using an iteration abstraction layer , pyop2  @xcite .",
    "this layer is explicitly exposed to the model developer , and allows them to write and execute _ custom _ kernels over the mesh .",
    "the critical observation is that most operations that fall only slightly outside the finite element abstraction may still be formulated as the execution of a _ local _ operation over some set of mesh entities , these can be expressed as a pyop2 _ parallel loop_. this separate abstraction layer allows the user to worry about the local operations : parallelisation is carried out automatically by pyop2 exactly as it is for finite element kernels in firedrake itself .",
    "more details of the interaction between firedrake and pyop2 can be found in  ( * ? ? ?",
    "* section 4 ) .",
    "multigrid methods  @xcite are an algorithmically optimal approach to solving many pdes , especially those involving elliptic operators .",
    "they rely on a hierarchy of scales to cheaply and efficiently compute properties of the system _ at the appropriate scale _ : modes that vary slowly in space can be accurately represented using only a few degrees of freedom and are thus best solved for on coarse grids , whereas fast modes need high spatial accuracy . here",
    "we briefly provide an overview of the mathematical operations in terms of a two - grid setup , and then describe our implementation in firedrake .",
    "let @xmath2 be the approximation space on the `` coarse '' grid and @xmath3 the space on the fine grid , these need not necessarily be nested , although the implementation is simplified if they are .",
    "a complete multigrid cycle is built from a few basic operators . in algorithm  [ alg : two - grid ]",
    "we show the form of _ correction scheme _",
    "multigrid ( so called because on the coarse grid we solve for the correction to the fine grid equation ) on two levels , given a linear system @xmath4 .",
    "@xmath5 )    the extension to multiple levels recursively applies this two - level cycle to compute @xmath6 .",
    "this setup suggests that we need to provide facilities for computing restrictions and prolongations .",
    "we also need the ability to compute coarse grid operators , and we need to be able to apply smoothers ( effectively some form of linear solver ) . in listing  [ lst : two - grid ] ,",
    "we show how this abstract framework translates into our implementation in firedrake for a simple two - dimensional example .",
    "we are able to exploit the existing facilities for the vast majority of the implementation , we merely need a few extensions to deal with hierarchies of meshes and transferring between them , which we discuss below .    .... from firedrake import * # construct mesh hierarchy ... coarse = uniticosahedralspheremesh(2 ) mh = meshhierarchy(coarse , 1 ) # ... and corresponding function space hierarchy based on piecewise linear elements vh = functionspacehierarchy(mh , \" cg \" , 1 ) v = vh[-1 ]    u = trialfunction(v ) v = testfunction(v ) # define forms for a and b a = ( dot(grad(u ) , grad(v ) ) + u*v)*dx x = spatialcoordinate(mh[-1 ] ) l = exp(-0.5*abs(x)**2/(0.5 * 0.5))*v*dx    # solution on fine grid uf = function(v ) # residual r = l - action(a , uf ) af = assemble(a ) bf = assemble(l ) # pre - smooth ( m1 ) with two point - jacobi iterations solve(af , uf , bf , solver_parameters={'ksp_type ' : ' richardson ' ,                                       ' ksp_max_it ' : 2 ,                                       ' ksp_convergence_test ' : ' skip ' ,                                       ' ksp_initial_guess_nonzero ' : true ,                                       ' pc_type ' : ' jacobi ' } )    duf = function(v ) vc = vh[0 ] uc = function(vc ) bc = function(vc ) # restrict residual ( m2 ) restrict(assemble(r ) , bc ) ac = assemble(coarsen_form(a ) ) # exact coarse solve ( m3 ) solve(ac , uc , bc , solver_parameters={'pc_type ' : ' lu ' ,                                       ' ksp_type ' : ' preonly ' } ) # prolongate correction ( m4 ) prolong(uc , duf ) uf + = duf # post - smooth ( m5 ) with three point - jacobi iterations solve(af , uf , bf , solver_parameters={'ksp_type ' : ' richardson ' ,                                       ' ksp_max_it ' : 3 ,                                       ' pc_type ' : ' jacobi ' ,                                       ' ksp_convergence_test ' : ' skip ' ,                                       ' ksp_initial_guess_nonzero ' : true } ) ....      it is clear that the first object we will need is a hierarchy of grids ( or meshes , in firedrake s parlance ) .",
    "this will encapsulate the relationship between the refined grids ( providing information on the fine grid cells corresponding to coarse grid cells ) .",
    "these are provided by the firedrake ` meshhierarchy ` object .",
    "similarly , we shall need to represent discrete solution spaces .",
    "firedrake uses ` functionspace ` objects for this , and we extend these with a ` functionspacehierarchy ` . as with",
    "the ` meshhierarchy ` this encapsulates the relationship between function spaces on related grids ( allowing us to determine the degrees of freedom in the fine grid that are related to a coarse grid cell ) .",
    "we note that in this work , we only treat hierarchically refined grids .",
    "this does not affect the abstractions we discuss , although it does simplify some of the implementation .",
    "nested finite element spaces admit particularly simple implementation of restriction and prolongation . to compute the restriction operator we use fiat  @xcite to evaluate the coarse basis at the node points on the fine grid .",
    "this allows us to express the coarse cell basis functions in terms of linear combinations of fine cell basis functions .",
    "since the mesh is regularly refined , we need only do this once and can use the same weighting for all cells . the restriction operator",
    "can then be expressed simply by applying this combination kernel to a given residual using a pyop2 parallel loop ( exposed as ` restrict ` in the firedrake interface ) . in the same way we compute the interpolation from @xmath2 into @xmath3 , which is just the natural embedding , using fiat , and",
    "apply the kernel over the mesh with pyop2 .",
    "forming the coarse grid operators is straightforward using the existing facilities of firedrake and ufl  @xcite .",
    "rediscretised operators are readily available simply by taking the ufl expression for the fine grid operator and assembling it on the coarse grid ( achieved using the ` coarsen_form ` operation in listing  [ lst : two - grid ] ) .",
    "these rediscretised operators have minimal stencil .",
    "in addition , it is also straightforward to provide simpler operators ( perhaps throwing away couplings that do not contribute on coarse grids ) by explicitly defining the operator symbolically on the appropriate coarse level and using it in the smoother",
    ".      firedrake uses petsc to provide solvers for linear systems . as such , for assembled matrices , we can use as a smoother any linear solver that petsc makes available . in listing  [ lst : two - grid ] , for example",
    ", we use two jacobi - preconditioned richardson iterations as a pre - smoother , solve the coarse problem exactly with lu and then use three preconditioned richardson iterations as a post - smoother .",
    "naturally , we are free to implement our own smoothers instead , perhaps we do not have an assembled matrix and therefore can not use `` black - box '' smoothers . indeed , the key ingredient to achieve optimal performance is the use of the correct smoother .",
    "this is of particular importance in the tensor - product multigrid scheme we discuss in the rest of the paper , since an assembled operator is not available and there is structure in the problem ( the strong vertical anisotropy ) we wish to exploit in the smoothers .",
    "to illustrate the power of these abstractions we consider an important model system for meteorological applications : the equations for linear gravity wave propagation in the global atmosphere .",
    "the corresponding system of pdes is discretised with a mixed finite element discretisation .",
    "the problem is solved in a thin spherical shell which represents the earth s atmosphere .",
    "the thickness of the atmosphere is several orders smaller than the radius of the earth .",
    "this flatness of the domain is typical for applications in atmospheric modelling and introduces a strong grid - aligned anisotropy .",
    "as we shall see , for a mixed finite element discretisation this problem is significantly more complex than the simple example shown in listing  [ lst : two - grid ] .",
    "we will therefore use it to both characterise the performance of our multigrid implementation , and validate that we have exposed the correct abstractions . for the linear solver to converge rapidly , the anisotropy has to be treated correctly in the preconditioner and the main challenge is the implementation of an optimal smoother .",
    "the pyop2 abstraction level allows the expression of this smoother in terms of tailored data structures and low - level kernels which implement the line - relaxation method that is key to exploit the vertical anisotropy .",
    "since the pde we are solving is elliptic , one option is to employ an algebraic multigrid ( amg ) preconditioner . on highly anisotropic domains",
    ", one must take special care in constructing the coarse grid operators and smoothers to achieve mesh independence .",
    "see for example  @xcite , where the authors use smoothed - aggregation amg to precondition the velocity block when solving the nonlinear stokes equations in the context of ice - sheet dynamics . to account for the strong anisotropy ,",
    "the aggregation strategy is adapted to maintain the column structure of the degrees of freedom , and a smoother based on incomplete factorisations is used .",
    "there are a number of reasons why an amg approach might not always be desirable .",
    "the amg preconditioner can only use galerkin coarse grid operators , and so the coarse grid stencil will be large . more importantly , unless special care is taken , the coarsening operation will not necessarily obey the anisotropy inherent in the problem , perhaps leading to suboptimal algorithmic performance . finally , we found in  @xcite that a bespoke preconditioner , based on the tensor - product multigrid of  @xcite , is superior to amg based methods if it is applied to a simplified model equation discretised with the finite volume method ; indeed the geometric multigrid approach turned out to be about @xmath7 faster than black - box amg implementations from the dune  @xcite and hypre  @xcite libraries .",
    "moreover the tensor - product preconditioner can be shown to be optimal for grid - aligned anisotropies . in a recent paper  @xcite we have also demonstrated numerically that the method works well for atmospheric applications under slightly more general circumstances .",
    "these encouraging results motivate us to study the geometric multigrid implementation reported in this work .",
    "the following linear gravity wave problem for pressure @xmath8 , velocity @xmath9 and buoyancy @xmath10 can be obtained by linearising the full navier - stokes equations for large scale atmospheric flow    3 & = p + b , & & = -c^2 @xmath11 , & & = -n^2@xmath11 .",
    "[ eqn : continuousequations ]    for simplicity we assume that both the speed of sound @xmath12 and the buoyancy frequency @xmath13 are constant and enforce the ( strong ) boundary condition @xmath14 at the upper and lower boundary of the atmosphere . the domain @xmath15 can be expressed as a tensor - product @xmath16 $ ] where @xmath17 is the two - dimensional surface of a sphere with radius @xmath18 .",
    "as described above , the domain is very flat , i.e. @xmath19 . to construct function spaces for mimetic finite element discretisations ,",
    "consider the following de rham complexes in one- , two- and three dimensions :    3 _ 0 _ 1 , & & _ 0 _ 1 _ 2 , & & _ 0 _ 1 _ 2 _ 3 .",
    "[ eqn : all_spaces ]    we seek a solution to eq .  [ eqn : continuousequations ] with    3 @xmath11 & _ 2 ^ 0 = _",
    "2^h _ 2 ^ 0,z & b & _ b , & p & _ 3 , [ eqn : functionspaces ]    where @xmath20 is the subspace of @xmath21 whose normal component vanishes on the boundary of the domain . @xmath22 and @xmath23 are respectively the `` horizontal '' and `` vertical '' parts of @xmath20 .",
    "the remaining spaces are @xmath24 and @xmath25 .",
    "this choice of spaces for @xmath9 and @xmath10 is analogous to the charney - phillips staggering .",
    "we refer the reader to  @xcite for the implementation and further description of tensor - product spaces in firedrake .",
    "it is worth highlighting here that the decomposition of the velocity space into a horizontal ( @xmath26 ) and vertical ( @xmath27 ) component is very important for the construction of the tensor - product multigrid preconditioner described below .",
    "we discretise in time using an implicit scheme which is a special case of the crank - nicholson method  @xcite , resulting in the following weak system for the increments @xmath28 , @xmath29 and @xmath30 @xmath31    \\langle\\phi,\\delta p\\rangle    + \\frac{\\delta t}{2}c^2\\langle\\phi,\\nabla\\cdot\\delta{\\ensuremath{\\boldsymbol{u}}}\\rangle    & = -\\delta tc^2\\langle\\phi,\\nabla\\cdot{\\ensuremath{\\boldsymbol{u}}}_0\\rangle    \\equiv \\langle\\phi , r_\\phi\\rangle & \\forall \\phi\\in{\\mathbb{w}}_3    \\\\[0ex ]    \\langle\\gamma,\\delta b\\rangle    + \\frac{\\delta t } { 2}n^2\\langle\\gamma,\\delta{\\ensuremath{\\boldsymbol{u}}}\\cdot{\\hat{{\\ensuremath{\\boldsymbol{z}}}}}\\rangle    & = -\\delta t n^2\\langle\\gamma,{\\ensuremath{\\boldsymbol{u}}}_0\\cdot{\\hat{{\\ensuremath{\\boldsymbol{z}}}}}\\rangle    \\equiv \\langle \\gamma , r_b\\rangle & \\forall \\gamma\\in{\\mathbb{w}}_b \\label{eqn : increments }   \\end{aligned}\\ ] ] where @xmath32 is the normal vector in the vertical direction and @xmath33 , @xmath34 and @xmath35 are the known fields are the previous time step . in this work",
    "we consider two choices for the three - dimensional complex in eq .",
    "[ eqn : all_spaces ] introduced in  @xcite and summarised in tab .",
    "[ tab : finiteelements ] . in the following",
    "they are referred to as `` lowest order '' ( lo ) and `` next - to - lowest order '' ( nlo ) .",
    ".finite element discretisations used in this work .",
    "@xmath36 is the continuous polynomial element of degree @xmath37 , @xmath38 the corresponding discontinuous elements of the same degree .",
    "@xmath39 is the lowest order raviart - thomas element @xcite and @xmath40 is the element described in @xcite . [",
    "cols=\"<,^,^,^,^\",options=\"header \" , ]     consequently in any tests that we carried out the matrix - explicit implementation of the geometric multigrid solver was much faster than a matrix - free version .",
    "as a result the performance of both the amg and multigrid solver is limited by the speed with which the matrix and field vector can be loaded from memory , and with which the matrix can be assembled in the setup phase ( explicit matrix assembly of @xmath41 was also necessary for the lu decomposition that is required at next - to - lowest order ) . since the matrix size is comparable in both cases , it is not surprising that the time per iteration and absolute solution time is comparable for both methods .",
    "this picture is likely to change at higher discretisation orders where sum factorisation techniques can improve performance dramatically : as shown in  @xcite on modern chip architectures with a large flop - to - bandwidth ratio , sum - factorised matrix free implementations can be faster than memory bound implementations which assemble the operator and apply it in a sparse matrix representation . at high order , storing the assembled matrix",
    "can also require significantly more memory and limit the simulated problem size .",
    "the results of a weak scaling study of the multigrid solvers on archer is shown in fig .",
    "[ fig : weakscaling ] . as in the previous section we fix the courant number at @xmath42 , decreasing the timestep linearly with higher grid resolution .",
    "the largest system solved at lowest order had @xmath43 unknowns on 1536 cores ( 64 full nodes ) ; at next - to - lowest order we solved problems with up to @xmath44 degrees of freedom on 6144 cores ( 256 full nodes ) .    .",
    "single - node data is shown with a gray background . ]    .",
    "single - node data is shown with a gray background . ]    all solvers show excellent weak scaling once the nodes are fully populated .",
    "any increases in runtime on partially occupied nodes can probably be attributed to the increasing saturation of the memory bandwidth as more cores are used .",
    "the implementation of bespoke preconditioners for complex finite element problems requires suitable abstractions which allow a separation of concerns between the algorithm developer and the computational scientist .",
    "we discussed the implementation of appropriate abstractions in the firedrake finite element framework .",
    "we then used the extended system to build a bespoke preconditioner for the mixed finite element discretisation of a linear gravity wave system in a thin spherical shell  an important model system in atmospheric flow applications .",
    "motivated by earlier work in  @xcite , we constructed a bespoke tensor - product geometric multigrid preconditioner , which is tailored to the strong grid aligned anisotropy in the vertical direction .",
    "this preconditioner results in a solver which is around @xmath45 faster than a purely algebraic approach using petsc fieldsplit preconditioning and hypre s algebraic multigrid to solve the resulting elliptic problem . for operationally relevant cfl numbers ,",
    "the multigrid preconditioners are about twice as fast as the single level methods popular in current operational models .",
    "the different abstractions of the firedrake / pyop2 framework simplify the implementation significantly : the user can express the algorithm at the correct abstraction level , while still obtaining very good performance .",
    "this is demonstrated by our careful analysis of key components of the algorithm , which we show are running at a significant fraction of the theoretical peak memory bandwidth .",
    "for simpler finite volume or finite difference discretisations on structured grids we previously demonstrated previously that it is possible to achieve further speedups by using a matrix - free implementation  @xcite .",
    "however , here we show that this is not possible for the low - order finite element discretisations . in fact",
    "a matrix - based implementation based on a boomeramg preconditioner achieves almost the same performance as our bespoke geometric multigrid solvers .",
    "the balance between matrix - free and assembled operators may change with changes in future hardware , and if we are able to exploit some of the structure in the tensor - product basis ( reducing the flop counts of the element kernels ) ; we intend to study this in the future .",
    "there are several ways of extending the current work . in realistic atmospheric models orography",
    "will lead to a distortion of the grid and the decomposition of the velocity function space into a purely horizontal and purely vertical component is no longer valid , which can have an impact on the performance of the solver .",
    "nevertheless the tensor - product multigrid algorithm , which assumes a perfect factorisation , can still be used as a good preconditioner , as has been confirmed by preliminary experiments ( not reported here ) .",
    "performance gains are also expected from a decomposition of the operators into tensor products . if , for example , an operator @xmath46 can be written as a tensor product @xmath47 , assembling ( and applying ) the operators @xmath48 and @xmath49 separately requires @xmath50 operations ( and storage ) instead of @xmath51 , and , since @xmath52 this will lead to significant speedups .",
    "this approach has been explored for a simplified problem in  @xcite . in the finite element setting used in this work",
    "it would require a shallow - atmosphere approximation in the preconditioner .    instead of using a schur - complement factorisation and applying a multigrid preconditioner to the positive - definite system in ( [ eqn : helmholtzoperator ] ) , one could also the multigrid algorithm for the full system in ( [ eqn : pressurevelocitysystem ] ) or ( [ eqn : equations3x3 ] ) .",
    "this requires the construction of suitable smoothers , for example based on `` distributive iterations ''  @xcite .",
    "ultimately the linear solver will be used inside a newton iteration to solve a non - linear problem in the full atmospheric model , and more careful numerical studies will have to be carried out in this context . while a non - linear multigrid ( fas ) scheme  @xcite could also be explored , the non - linearities in the problem considered here might not be large enough to justify this approach which requires computationally expensive non - linear smoothers .",
    "guided by the work reported here , we are currently working on implementing both the petsc fieldsplit preconditioner and the geometric multigrid preconditioner in the fortran 2003 code base that is used to develop the met office s next generation dynamical core ( codenamed `` gungho '' ) .",
    "this work was funded as part of the nerc project on next generation weather and climate prediction ( ngwcp ) , grant numbers ne / k006789/1 and ne / k006754/1 .",
    "lm additionally acknowledges funding from epsrc grant ep / m011054/1 .",
    "we gratefully acknowledge input from discussions with our collaborators in the met office dynamics research group and the gungho ! project .      all numerical experiments in this paper",
    "were performed with the following versions of software , which we have archived on zenodo : firedrake  @xcite ; pyop2  @xcite ; fiat  @xcite ; ffc  @xcite ; coffee  @xcite ; petsc  @xcite ; petsc4py  @xcite ; ufl  @xcite .",
    "additionally , the simulation code itself is archived as  @xcite and the performance data and plotting scripts as  @xcite .",
    "f.  brezzi , j.  douglas  jr , m.  fortin , l.  d. marini , efficient rectangular mixed finite elements in two and three space variables , esaim : mathematical modelling and numerical analysis - modlisation mathmatique et analyse numrique 21  ( 4 ) ( 1987 ) 581604 .      a.  t.  t. mcrae , c.  j. cotter , energy- and enstrophy - conserving schemes for the shallow - water equations , based on mimetic finite elements , quarterly journal of the royal meteorological society 140  ( 684 ) ( 2014 ) 22232234 . http://arxiv.org/abs/1305.4477 [ ] .",
    "e.  h. mller , r.  scheichl , massively parallel solvers for elliptic partial differential equations in numerical weather and climate prediction , quarterly journal of the royal meteorological society 140  ( 685 ) ( 2014 ) 26082624 .",
    "http://arxiv.org/abs/1307.2036 [ ] .",
    "a.  dedner , e.  mller , r.  scheichl , efficient multigrid preconditioners for atmospheric flow simulations at high aspect ratio , international journal for numerical methods in fluids 80  ( 1 ) ( 2016 ) 76102 . http://arxiv.org/abs/1408.2981 [ ] .",
    "f.  rathgeber , g.  r. markall , l.  mitchell , n.  loriant , d.  a. ham , c.  bertolli , p.  h.  j. kelly , pyop2 : a high - level framework for performance - portable simulations on unstructured meshes , in : high performance computing , networking storage and analysis , sc companion : , ieee computer society , 2012 , pp .",
    "11161123 .",
    "f.  rathgeber , d.  a. ham , l.  mitchell , m.  lange , f.  luporini , a.  t.  t. mcrae , g .-",
    "bercea , g.  r. markall , p.  h.  j. kelly , firedrake : automating the finite element method by composing abstractions , submitted ( 2015 ) . http://arxiv.org/abs/1501.01809 [ ] .",
    "g.  kanschat , y.  mao , multigrid methods for @xmath53-conforming discontinuous galerkin methods for the stokes equations , journal of numerical mathematics 23  ( 1 ) ( 2015 ) 5166 . http://arxiv.org/abs/1501.06021 [ ] .",
    "s.  balay , w.  d. gropp , l.  c. mcinnes , b.  f. smith , efficient management of parallelism in object oriented numerical software libraries , in : e.  arge , a.  m. bruaset , h.  p. langtangen ( eds . ) , modern software tools in scientific computing , birkhuser press , 1997 , pp .",
    "163202 .",
    "s.  balay , s.  abhyankar , m.  f. adams , j.  brown , p.  brune , k.  buschelman , l.  dalcin , v.  eijkhout , w.  d. gropp , d.  kaushik , m.  g. knepley , l.  c. mcinnes , k.  rupp , b.  f. smith , s.  zampini , h.  zhang , petsc users manual , tech .",
    "anl-95/11 - revision 3.6 , argonne national laboratory ( 2015 ) .",
    "r.  d. falgout , u.  meier - yang , hypre : a library of high performance preconditioners , in : p.  m.  a. sloot , c.  j.  k. tan , j.  j. dongarra , a.  g. hoekstra ( eds . ) , lecture notes in computer science , vol .",
    "2331 , springer , 2002 , pp .",
    "632641 .",
    "m.  s. alns , a.  logg , k.  b. lgaard , m.  e. rognes , g.  n. wells , unified form language : a domain - specific language for weak formulations of partial differential equations , acm transactions on mathematical software 40  ( 2 ) ( 2014 ) 9:19:37 .",
    "http://arxiv.org/abs/1211.4047 [ ] .",
    "t.  isaac , g.  stadler , o.  ghattas , http://dx.doi.org/10.1137/140974407[solution of nonlinear stokes equations discretized by high - order finite elements on nonconforming and anisotropic meshes , with application to ice sheet dynamics ] , siam journal on scientific computing 37  ( 6 ) ( 2015 ) b804b833 .",
    "http://arxiv.org/abs/http://dx.doi.org/10.1137/140974407 [ ] , http://dx.doi.org/10.1137/140974407 [ ] . http://dx.doi.org/10.1137/140974407    m.  blatt , p.  bastian , the iterative solver template library , in : b.  kgstrm , e.  elmroth , j.  dongarra , j.  wasniewski ( eds . ) , applied parallel computing .",
    "state of the art in scientific computing , vol .",
    "4699 of lecture notes in computer science , springer berlin heidelberg , 2007 , pp .",
    "666675 .",
    "a.  t.  t. mcrae , g .-",
    "bercea , l.  mitchell , d.  a. ham , c.  j. cotter , automated generation and symbolic manipulation of tensor product finite elements , siam journal on scientific computingto appear .",
    "http://arxiv.org/abs/1411.2940 [ ] .",
    "raviart , j.  m. thomas , a mixed finite element method for 2nd order elliptic problems , in : mathematical aspects of finite element methods , vol .",
    "606 of lecture notes in mathematics , springer berlin heidelberg , 1977 , pp . 292315 .",
    "t.  davies , m.  j.  p. cullen , a.  j. malcolm , m.  h. mawson , a.  staniforth , a.  a. white , n.  wood , a new dynamical core for the met office s global and regional modelling of the atmosphere , quarterly journal of the royal meteorological society 131  ( 608 ) ( 2005 ) 17591782 .",
    "n.  wood , a.  staniforth , a.  white , t.  allen , m.  diamantakis , m.  gross , t.  melvin , c.  smith , s.  vosper , m.  zerroukat , j.  thuburn , an inherently mass - conserving semi - implicit semi - lagrangian discretization of the deep - atmosphere global non - hydrostatic equations , quarterly journal of the royal meteorological society 140  ( 682 ) ( 2014 ) 15051520 .",
    "y.  saad , m.  h. schultz , http://dx.doi.org/10.1137/0907058[gmres : a generalized minimal residual algorithm for solving nonsymmetric linear systems ] , siam journal on scientific and statistical computing 7  ( 3 ) ( 1986 ) 856869 .",
    "http://arxiv.org/abs/http://dx.doi.org/10.1137/0907058 [ ] , http://dx.doi.org/10.1137/0907058 [ ] .",
    "http://dx.doi.org/10.1137/0907058    c.  c. paige , m.  a. saunders , http://dx.doi.org/10.1137/0712047[solution of sparse indefinite systems of linear equations ] , siam journal on numerical analysis 12  ( 4 ) ( 1975 ) 617629 .",
    "[ ] , http://dx.doi.org/10.1137/0712047 [ ] . http://dx.doi.org/10.1137/0712047",
    "s.  j. thomas , j.  p. hacker , p.  k. smolarkiewicz , r.  b. stull , http://dx.doi.org/10.1175/1520-0493(2003)131<2464:spfnam>2.0.co;2[spectral preconditioners for nonhydrostatic atmospheric models ] , monthly weather review 131  ( 10 ) ( 2003 ) 24642478 .",
    "http://arxiv.org/abs/http://dx.doi.org/10.1175/1520-0493(2003)131<2464:spfnam>2.0.co;2 [ ] , http://dx.doi.org/10.1175/1520-0493(2003)131<2464:spfnam>2.0.co;2 [ ] . http://dx.doi.org/10.1175/1520-0493(2003)131<2464:spfnam>2.0.co;2[http://dx.doi.org/10.1175/1520-0493(2003)131<2464:spfnam>2.0.co;2 ]",
    "g.  markall , f.  rathgeber , l.  mitchell , n.  loriant , c.  bertolli , d.  a. ham , p.  h.  j. kelly , performance - portable finite element assembly using pyop2 and fenics , supercomputing 7905 ( 2013 ) 279289 . http://dx.doi.org/10.1007/978-3-642-38750-0_21 [ ] .",
    "i.  z. reguly , g.  r. mudalige , c.  bertolli , m.  b. giles , a.  betts , p.  h.  j. kelly , d.  radford , acceleration of a full - scale industrial cfd application with op2 , ieee transactions on parallel and distributed systems 27  ( 5 ) ( 2016 ) 12651278 . http://dx.doi.org/10.1109/tpds.2015.2453972 [ ] .",
    "f.  luporini , a.  l. varbanescu , f.  rathgeber , g .-",
    "bercea , j.  ramanujam , d.  a. ham , p.  h.  j. kelly , cross - loop optimization of arithmetic intensity for finite element local assembly , acm transactions on architecture and code optimization 11  ( 4 ) ( 2015 ) 57:157:25 .",
    "w.  d. gropp , d.  k. kaushik , d.  e. keyes , b.  f. smith , towards realistic performance bounds for cfd codes , in : d.  keyes , a.  ecer , j.  periaux , n.  satofuka ( eds . ) , parallel cfd 1999 , north - holland , 2000 , pp .",
    "241248 .",
    "d.  a. may , j.  brown , l.  le  pourhiet , ptatin3d : high - performance methods for long - term lithospheric dynamics , in : proceedings of the international conference for high performance computing , networking , storage and analysis , sc 14 , ieee press , 2014 , pp . 274284 .",
    "vos , s.  sherwin , r.  kirby , from h to p efficiently : implementing finite and spectral / hp element methods to achieve optimal performance for low - and high - order discretisations , jcp 229  ( 13 ) ( 2010 ) 51615181 ."
  ],
  "abstract_text": [
    "<S> the implementation of efficient multigrid preconditioners for elliptic partial differential equations ( pdes ) is a challenge due to the complexity of the resulting algorithms and corresponding computer code . for sophisticated ( mixed ) </S>",
    "<S> finite element discretisations on unstructured grids an efficient implementation can be very time consuming and requires the programmer to have in - depth knowledge of the mathematical theory , parallel computing and optimisation techniques on manycore cpus . in this paper </S>",
    "<S> we show how the development of bespoke multigrid preconditioners can be simplified significantly by using a framework which allows the expression of the each component of the algorithm at the correct abstraction level . </S>",
    "<S> our approach ( 1 ) allows the expression of the finite element problem in a language which is close to the mathematical formulation of the problem , ( 2 ) guarantees the automatic generation and efficient execution of parallel optimised low - level computer code and ( 3 ) is flexible enough to support different abstraction levels and give the programmer control over details of the preconditioner . </S>",
    "<S> we use the composable abstractions of the firedrake / pyop2 package to demonstrate the efficiency of this approach for the solution of strongly anisotropic pdes in atmospheric modelling . </S>",
    "<S> the weak formulation of the pde is expressed in unified form language ( ufl ) and the lower pyop2 abstraction layer allows the manual design of computational kernels for a bespoke geometric multigrid preconditioner . </S>",
    "<S> we compare the performance of this preconditioner to a single - level method and hypre s boomeramg algorithm . </S>",
    "<S> the firedrake / pyop2 code is inherently parallel and we present a detailed performance analysis for a single node ( 24 cores ) on the archer supercomputer . </S>",
    "<S> our implementation utilises a significant fraction of the available memory bandwidth and shows very good weak scaling on up to 6,144 compute cores .    </S>",
    "<S> geometric multigrid ,  atmospheric modelling ,  preconditioner ,  ( mixed ) finite elements ,  domain - specific compilers </S>"
  ]
}