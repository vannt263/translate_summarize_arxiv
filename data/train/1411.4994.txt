{
  "article_text": [
    "in the dispersive regime of circuit - qed the resonator frequency depends on the qubit state so that driving the cavity and observing the output of the cavity corresponds to a quantum non - demolition measurement . for outcome  0 \" ( 1 ) the mean voltage leaving the cavity represents a time - dependent coherent state , denoted @xmath69 , that is typically small in magnitude .",
    "the phase - space evolution of @xmath69 is determined by the deterministic differential equation @xmath70 where @xmath71 is the measurement drive and @xmath72 is the cavity decay rate .    for our experiment ,",
    "the qubit transition frequency is @xmath73 ghz and the readout resonator frequency is @xmath74 ghz .",
    "the qubit anharmonicity is @xmath75 mhz with @xmath0 ( energy relaxation ) time of @xmath76s and coherence time @xmath77s .",
    "the dispersive shift and line width of the readout resonator are measured to be @xmath78 mhz and @xmath79 khz , respectively so that @xmath80 .",
    "the output mode @xmath81 from the resonator can be related to the field inside the resonator by @xmath82 where @xmath83 is the separation between the pointer states , @xmath84 is the mean value of the coherent states and @xmath85 represents the qubit information with @xmath86 and @xmath87 being the probability of the qubit being in state 0 and 1 respectively  @xcite .",
    "typically this field is amplified by a linear phase - preserving amplifier to the output mode @xmath88 , @xmath89 where @xmath90 is the power gain and @xmath91 is the extra noise added by the amplifier .",
    "here we have made an assumption that the bandwidth of the amplifier is constant and larger than the bandwidth of the signal being measured . since the commutators must satisfy @xmath92 & = \\left[b(t),b^{\\dagger}(t^\\prime)\\right ] = \\openone \\delta(t - t^\\prime),\\end{aligned}\\ ] ] the noise must satisfy @xmath93 = \\openone(1-g)\\delta(t - t'),\\end{aligned}\\ ] ] which , from the generalized uncertainty principle of an arbitrary operator @xmath94 , @xmath95 . using eq .",
    "[ eq : outputmode ] the noise in the output mode @xmath88 is @xmath96is the added noise normalized by the gain  @xcite . in our view",
    "this is the best number to quantify an amplifier and for the quantum limit it takes the value 1/2 for a phase preserving amplifier .",
    "other useful quantities are the instantaneous input and output signal - to - noise ratios of the amplifier defined by @xmath97 where @xmath98 is the efficiency of the amplifier and represents how well the input field is mapped to the output field .",
    "another useful quantity is the noise - figure @xmath99 of the amplifier , which is the ratio @xmath100 .",
    "we see that @xmath101 which is 2 for a quantum limited amplifier .    in circuit",
    "qed the information about the qubit state is contained in a single quadrature . from eq .",
    "[ eq : field ] this is the quadrature set by @xmath102 and as a result when we subtract the mean value we obtain @xmath103 this overestimates the noise as the information is only in one quadrature . defining the measurement quadrature @xmath104 $ ] it is simple to show that @xmath105 @xmath106 where @xmath107 is the efficiency of measuring information in a single quadrature for a linear phase preserving amplifier . note this is a factor of two less than the efficiency of the amplifier .",
    "a general linear amplifier can be described by the output mode @xmath88 , @xmath108 and from preservation of the commutation relations @xmath109",
    "= \\openone(1-m^2+|l|^2)\\delta(t - t'),\\end{aligned}\\ ] ] we have @xmath110 . setting @xmath111 results in @xmath112 . to amplify a single quadrature",
    "@xmath113 $ ] ( phase sensitive amplifier ) we set @xmath114 giving @xmath115 + \\mathrm{re } [ h(t)e^{-i\\theta}],\\\\ q(t)&=\\frac{1}{\\sqrt{g } }   \\mathrm{im } [ b(t)e^{-i\\theta } ] + \\mathrm{im } [ h(t)e^{-i\\theta}],\\end{aligned}\\ ] ] where @xmath116 is the power gain . from the generalized uncertainty principle @xmath117 where @xmath118 is the gain normalized added noise .",
    "the instantaneous snr@xmath119 for the quadrature @xmath120 is @xmath121 where @xmath122 .",
    "that is , by using a phase sensitive amplifier tuned to the correct phase the effective snr can be a factor of two better then a phase preserving amplifier .      in a typical measurement protocol",
    "the measurement outcome is the integration of the signal @xmath88 from the amplifier with a weighting kernel ( filter ) , @xmath123dt = \\sum_j |w_j|\\mathrm{re } [ e^{-i\\phi_j } c_j ] \\delta t,\\end{aligned}\\ ] ] where @xmath124 is the measurement time and @xmath125 is the kernel . under the assumption that the noise is symmetric a useful measure to quantify the measurement is the separation @xmath126 where 0 and 1 label the two states of the qubit . in ref .  @xcite it was shown that the optimal kernel under the additional assumptions of gaussian and diagonal covariance matrices is found by maximizing @xmath94 and is given by @xmath127 where @xmath128 is the separation of the pointer states and @xmath129 $ ] .",
    "the achievable fidelity @xmath28 is the value of the assignment fidelity @xmath31 assuming that the noise in our system is ideal ( symmetric and diagonal gaussian covariance matrices ) . in reality",
    ", the noise does not satisfy these properties so we must fit it to an ideal noise model . to do this",
    ", we implemented the above filtering method on our data and plotted the resulting distributions for the different classes ( @xmath6 and @xmath5 ) of @xmath130 in fig .",
    "[ fig : gaussian_fits ] .",
    "if the noise was ideal we should obtain two gaussian histograms with identical variance .",
    "in reality , we see there are significant non - gaussian statistics and so we fit the histograms to double gaussian distributions of equal variance to obtain the means and standard deviation we would expect in the ideal noise case .",
    "we obtain @xmath131 and @xmath132 , and a standard deviation of @xmath133 , which gives a value for @xmath94 ( defined in eq .",
    "[ eq : sep ] ) of @xmath134 .",
    "we can now compute @xmath28 by combining the ideal noise assumption with the definition of the assignment fidelity , @xmath2 + \\mathbb{p}\\left[1|0\\right]\\right)/2.\\ ] ] here @xmath3 $ ] ( @xmath4 $ ] ) is the probability of obtaining outcome  1 \" (  0 \" ) given the system was prepared in @xmath6 ( @xmath5 ) .",
    "since the noise is ideal @xmath28 takes the form @xmath135 where @xmath94 is defined in eq .",
    "[ eq : sep ] . from this expression",
    "we obtain @xmath136 for our system .",
    "we note that the separation @xmath94 in the time independent limit can be related to the signal - to - noise defined above by snr@xmath137 .",
    "this is straightforward to show by direct substitution of the quantities in eq .",
    "[ eq : sep ] .",
    "@xmath94 is used here as it is a standard measure of separation in ml  @xcite .",
    "the quadratic program for the svm  @xcite is given by the equation @xmath138 where @xmath139 is the expected outcome ( taken to be -1 or 1 ) .",
    "this has a quadratic objective function with linear inequality constraints .",
    "the soft - margin formalism adds slack variables representing the degree of misclassification to the constraints in eq .",
    "( [ eq : svmprimal ] ) and modifies the objective function to include a mislabelling cost term . the modified quadratic program that includes a soft - margin",
    "is given by the equation @xmath140 in the dual form of this problem , the @xmath141 variables vanish and @xmath142 is a  box constraint \" that bounds the lagrange multipliers .",
    "we implemented 10-fold cross - validation on the training set and found a misclassification error of 0.0163 which agrees well with @xmath143 .",
    "this implies large @xmath142 value is likely not needed and varying @xmath142 between 0 and 100 gave an optimal value of @xmath31 close to 1 .",
    "k - means clustering  @xcite is formulated as the following optimization problem @xmath144 here there are @xmath49 sets @xmath145 with means @xmath146 and so the goal is to partition the data into @xmath49 sets that minimize the within - set distance to the mean @xmath146 .",
    "the k - means algorithm only has guaranteed convergence to a local minimum and finding the global optimum is an np - hard problem  @xcite .",
    "therefore initialization of the subclass means in eq .",
    "[ eq : kmeans ] can be important to find meaningful solutions .",
    "typically initialization is a random process that can lead to small variation in the solutions .",
    "this can be circumvented by explicitly defining the initial means to be the average of the output subclass means over many realizations .",
    "this helps ensure the algorithm is reproducible and more stable .",
    "the plot of the heating subclass of @xmath9 found from the k - means algorithm with @xmath51 is given in fig .",
    "[ fig : heating_subclass ] and the plot of the four subclasses of @xmath10 found from the k - means algorithm with @xmath55 is given in fig .",
    "[ fig : four_subclasses ] ."
  ],
  "abstract_text": [
    "<S> high - fidelity measurements are important for the implementation of quantum information protocols . </S>",
    "<S> current methods for classifying measurement trajectories in superconducting qubit systems produce fidelities systematically lower than those predicted by experimental parameters . here , we place current classification methods within the framework of machine learning ( ml ) algorithms and improve on them by investigating more sophisticated ml approaches . </S>",
    "<S> we find that non - linear algorithms and clustering methods produce significantly higher assignment fidelities that help close the gap to the fidelity achievable under ideal noise conditions . clustering methods group trajectories into natural subsets within the data , which allows for the diagnosis of systematic errors . </S>",
    "<S> we find large clusters in the data associated with @xmath0 processes and show these are the main source of discrepancy between our experimental and achievable fidelities . these error diagnosis techniques help provide a path forward to improve qubit measurements .    </S>",
    "<S> the ability to perform accurate measurements is important for maximizing the information one can extract from a physical system . </S>",
    "<S> this is especially true in experimental quantum information processing since quantum systems are highly susceptible to noise effects and error rates of quantum operations and measurements must be small for fault - tolerant quantum computation to be a reality  @xcite . </S>",
    "<S> our goal is to provide methods for diagnosing measurement errors and increasing fidelities by using classification and clustering algorithms in machine learning ( ml ) . </S>",
    "<S> while we apply our methods in a superconducting qubit measurement system , we anticipate that the generality of these techniques can be useful in a broader class of systems .    </S>",
    "<S> superconducting quantum bits ( qubits ) have become a promising candidate for building a fault - tolerant quantum computer due to their long coherence times  @xcite , high - fidelity multi - qubit gate operations  @xcite , and the abilitiy to perform single - shot measurements  @xcite , in a circuit qed architecture  @xcite . </S>",
    "<S> remarkable progress has been made in reducing error - rates of these operations however considerable work is still needed to implement fault - tolerant quantum computation in large networks of qubits . in circuit </S>",
    "<S> quantum electrodynamics ( cqed ) a superconducting anharmonic oscillator , such as a transmon  @xcite , is coupled to a resonator , producing a state - dependent shift of the resonator frequency . </S>",
    "<S> this allows for qubit measurements by driving the resonator and recording the output trajectory  @xcite in phase ( i - q ) space . in practice </S>",
    "<S> there are significant sources of random noise and systematic effects , such as @xmath0 processes where the system spontaneously decays to its ground state , that can make single - shot trajectories appear complex and difficult to distinguish .    </S>",
    "<S> our experimental system is a single qubit ( q4 ) in a planar lattice of four superconducting qubits  @xcite . </S>",
    "<S> we show current methods for assigning outcomes to measurement trajectories in this system produce assignment fidelities ( defined below in eq .  </S>",
    "<S> ( [ eq : assfid ] ) ) that are much lower than the predicted achievable value derived under ideal noise conditions . </S>",
    "<S> we utilize various ml algorithms to obtain deeper insight into the behavior of the trajectories and bring fidelities up closer to our expected values . </S>",
    "<S> we find a total increase in assignment fidelity from 0.9586 using current methods to 0.9821 ( @xmath1 increase ) using non - linear ml classifiers . </S>",
    "<S> the strong performance of non - linear classifiers indicates systematic effects , such as heating and @xmath0 events , could be a significant source of error in our measurements .    to verify this , we use ml clustering methods to group the data into naturally occurring subsets . </S>",
    "<S> we find a large cluster consisting of @xmath0 events whose size is consistent with the experimentally measured @xmath0 time . </S>",
    "<S> replacing this cluster with random non-@xmath0 events gives assignment fidelities approaching 0.995 which is much closer to the achievable value of 0.9999 . </S>",
    "<S> going to higher orders we find a much small cluster corresponding to heating of the ground state into the excited state . before moving on , </S>",
    "<S> let us make a few points about using ml methods for trajectory discrimination and improving measurements . </S>",
    "<S> first , the methods we present here can be useful in a much broader context . </S>",
    "<S> any measurement scheme that produces patterns in a geometric space can potentially benefit from more advanced ml methods . investigating </S>",
    "<S> the applicability to different systems will depend on the details of each situation . </S>",
    "<S> second , these methods are applicable even if we are trying to improve higher fidelity measurements than that of this paper . </S>",
    "<S> the key is that these methods can be tailored according to the types of noise present . </S>",
    "<S> third , ml methods have also been applied to other problems in quantum information such as phase estimation  @xcite and asymptotic state estimation  @xcite .    </S>",
    "<S> the standard metric for characterizing how well a single - qubit measurement assigns outcomes is the assignment fidelity @xmath2 + \\mathbb{p}\\left[1|0\\right]\\right)/2.\\ ] ] here @xmath3 $ ] ( @xmath4 $ ] ) is the probability of obtaining outcome  0 \" (  1 \" ) given the system was prepared in @xmath5 ( @xmath6 ) . </S>",
    "<S> hence @xmath7 $ ] and ideally @xmath8 . </S>",
    "<S> our measurement framework is the dispersive limit of cqed , where observing the resonator output voltage provides a quantum non - demolition qubit measurement . for outcome </S>",
    "<S>  0 \" ( 1 ) the voltage leaving the cavity represents a coherent state i - q trajectory , and single - shot trajectories are obtained by amplifying the cavity signal . </S>",
    "<S> the main parameters of our system are given in  @xcite and complete experimental information is given in ref .  </S>",
    "<S> @xcite .    our data </S>",
    "<S> consists of 51200 single - shot trajectories ( shots ) , half initially prepared in @xmath6 and the other half in @xmath5 ( denote these classes by @xmath9 and @xmath10 ) . </S>",
    "<S> the trajectories are time - ordered and the first half is used as a training set to predict the second half for classification . </S>",
    "<S> the mean trajectories for each class , denoted @xmath11 and @xmath12 , as well as examples of single - shot trajectories , are given in fig .  </S>",
    "<S> [ fig : mean_trajs ] . </S>",
    "<S> we see that each shot can be complicated but there are enough shots to ensure smooth , well - separated means . </S>",
    "<S> the total measurement time @xmath13 is @xmath14s and @xmath15 $ ] is discretized into 163 time - points so trajectories are represented by vectors @xmath16 ( let @xmath17 ) where the first ( last ) 163 entries correspond to the real ( imaginary ) parts of the trajectory .    </S>",
    "<S> the current method of classifying trajectories  @xcite is to integrate each trajectory with a filter ( kernel , weight ) function @xmath18 . </S>",
    "<S> formally , if @xmath19 is a single trajectory , under the assumption that the covariance matrices @xmath20 and @xmath21 of each class are equal , gaussian , and diagonal , the optimal @xmath18 is equal to the ratio of the difference between the mean trajectories @xmath22 and @xmath23 and the variance @xmath24 . </S>",
    "<S> if @xmath25 is the diagonal covariance matrix then @xmath26.\\end{aligned}\\ ] ] the assignment fidelity using this method is @xmath27 .    </S>",
    "<S> the  achievable \" assignment fidelity for our experiment , @xmath28 , is the fidelity we would obtain under ideal noise conditions . by </S>",
    "<S>  ideal noise \" we mean the noise satisfies the assumptions above for the method of  @xcite to hold . </S>",
    "<S> the details of this calculation , along with a brief introduction to measurements and amplifiers , is contained in  @xcite . </S>",
    "<S> we obtain @xmath29 and so there is a large discrepancy between @xmath30 and @xmath31 that is due to a wide variety of factors such as state - preparation errors and non - gaussian / non - linear effects . </S>",
    "<S> this discrepancy provides the motivation for investigating better methods for classifying trajectories .    the idea behind machine learning ( ml ) classification is to obtain a classifying ( discriminating ) surface in @xmath32 under constraints such as the form of the noise . for gaussian noise </S>",
    "<S> the optimal discriminator is a quadratic surface  @xcite ( quadratic discriminant analysis - qda ) given by @xmath33x + x^t \\left[\\sigma_0^{-1}\\mu_0-\\sigma_1^{-1}\\mu_1\\right],\\end{aligned}\\ ] ] and the threshold is @xmath34 where @xmath35 represents the determinant . </S>",
    "<S> if @xmath36 the quadratic term in eq .  </S>",
    "<S> ( [ eq : qdaeq ] ) disappears and the surface reduces to a hyperplane  @xcite ( linear discriminant analysis - lda ) , @xmath37.\\end{aligned}\\ ] ] comparing with eq .  </S>",
    "<S> ( [ eq : ldadiag ] ) we see the current method of  @xcite is equivalent to lda with the added assumption of diagonal covariance matrices .    </S>",
    "<S> qda can achieve a more accurate value of @xmath31 as it allows for @xmath38 and thus a quadratic ( non - linear ) discriminating surface . </S>",
    "<S> we computed @xmath31 using the  fitcdiscr \" function in matlab for four different methods : ldad , lda , qdad , and qda (  d \" represents diagonal covariance matrix and ldad is the method of  @xcite ) . </S>",
    "<S> the results are in the second column of table  [ table : assfid_discriminant_methods ] . </S>",
    "<S> not surprisingly , we find qdad improves upon ldad and allowing non - diagonal covariance matrices produces higher @xmath31 . </S>",
    "<S> the values in table  [ table : assfid_discriminant_methods ] are the sample means from 100 repetitions . </S>",
    "<S> the sample variances @xmath39 are typically on the order of @xmath40 indicating stable and reproducible results .    </S>",
    "<S> a value of @xmath31 for qda was not attainable due to singular covariance matrices , which is a result of overfitting the data ( having more variables than required from the correlation time in the trajectories ) . to remedy this problem , </S>",
    "<S> we first perform dimensionality reduction using principal component analysis ( pca )  @xcite and find @xmath41 of the variance in the data can be accounted for in a subspace of dimension @xmath42 . loosely speaking , </S>",
    "<S> this implies correlation times of @xmath43 ns . the results with a pca pre - processing step ( using  princomp \" in matlab ) are in the third column of table  [ table : assfid_discriminant_methods ] . </S>",
    "<S> a value of qda can now be computed and as expected it provides the highest @xmath31 out of all cases considered .    </S>",
    "<S> c c c c c method & all time - points & pca + [ 0.5ex ]    ldad & 0.9586 & 0.9557 + lda & 0.9701 & 0.9586 + qdad & 0.9627 & 0.9648 + qda &  & 0.9712 + [ 1ex ]    [ table : assfid_discriminant_methods ]    these classification methods have assumed gaussian noise . </S>",
    "<S> more robust methods are needed as we expect non - gaussian behavior . </S>",
    "<S> we approach this in two ways . </S>",
    "<S> the first is via the support vector machine ( svm )  @xcite , which makes no assumption on the form of the noise and can be extended to extremely general non - linear discriminating surfaces . </S>",
    "<S> the second is to utilize  clustering \" methods in ml to naturally goup the data into clusters from which we perform _ multi - class _ classification . </S>",
    "<S> we first describe the svm method .    </S>",
    "<S> the linear svm is a quadratic program based on maximizing the minimum margins of the data , where the margin of a data point is its distance to a separating hyperplane  @xcite . </S>",
    "<S> the non - linear svm is derived from the dual form of the linear svm by defining a kernel that maps the data to a higher - dimensional space . </S>",
    "<S> the key is that the linear svm in the higher - dimensional space allows for non - linear discrimination in @xmath32 . due to its generality and simplicity , we chose a radial basis ( gaussian ) function kernel . </S>",
    "<S> we implemented the svm using the  fitcsvm \" function in matlab . </S>",
    "<S> the classification was repeated 100 times and the mean values with the optimal soft - margin parameter are contained in table  [ table : assfid_svm_methods ] ( see  @xcite for details ) . </S>",
    "<S> the sample variances @xmath39 in @xmath31 are approximately @xmath44 indicating stable results . </S>",
    "<S> the non - linear svm produces the highest assignment fidelity out of all methods considered thus far , indicating non - linear effects are present .    </S>",
    "<S> c c c c c method & all time - points & pca + [ 0.5ex ]    linear svm & 0.9753 & 0.9571 + non - linear svm & 0.9821 & 0.9739 + [ 1ex ]    [ table : assfid_svm_methods ]    our second method for implementing a non - linear classifier combines classification and _ clustering _ algorithms . </S>",
    "<S> clustering naturally groups the data into subsets and is  unsupervised \" since it requires no training data . </S>",
    "<S> we utilize k - means clustering  @xcite since it has features that suit our purposes well , however we anticipate similar results can be obtained with other standard clustering algorithms such as heirarchical methods . for an explicit formulation of the k - means clustering problem see  @xcite .    </S>",
    "<S> we used the matlab  </S>",
    "<S> kmeans \" function to find @xmath45 clusters in each of @xmath9 and @xmath10 . </S>",
    "<S> we chose @xmath45 to take into account both variance and systematic effects . </S>",
    "<S> the mean trajectories and size of the six subclasses are given in fig .  </S>",
    "<S> [ fig : subclass_m4 ] . </S>",
    "<S> we see @xmath9 is split relatively evenly into the subclasses @xmath46 , @xmath47 , @xmath48 that capture variance in the trajectories . </S>",
    "<S> we do not see a subclass of @xmath9 corresponding to heating of the ground state , however we implemented k - means for larger @xmath49 and found a heating subclass of size @xmath50 for @xmath51 ( see fig .  </S>",
    "<S> [ fig : heating_subclass ] ) .    </S>",
    "<S> @xmath10 has strikingly different properties as subclass @xmath52 is comprised of @xmath0 processes . @xmath53 and @xmath54 are similar in size and capture variance in the trajectories . </S>",
    "<S> the key point is we have found _ explicit shot indices _ for @xmath0 events . </S>",
    "<S> we verified that @xmath52 is comprised of @xmath0 trajectories by performing k - means with @xmath55 . </S>",
    "<S> we found that the @xmath53 and @xmath54 subclasses remain virtually the same while the @xmath0 subclass @xmath52 is now split into two according to variance in these trajectories ( see fig .  </S>",
    "<S> [ fig : four_subclasses ] ) . from fig .  </S>",
    "<S> [ fig : subclass_m4 ] , @xmath56 of the @xmath5 preparations result in a @xmath0 event , which is consistent with the percentage calculated from system parameters  @xcite , @xmath57 .    to perform classification </S>",
    "<S> , we lift the @xmath0 subclass @xmath52 to a class @xmath58 of its own , redefine @xmath59 , keep @xmath9 as before , and perform _ multi - class _ classification on @xmath9 , @xmath10 , and @xmath58 . </S>",
    "<S> we implemented four multi - class algorithms in matlab ; multi - class lda , multi - class svm ,  totalboost \" , and </S>",
    "<S>  rusboost \" . </S>",
    "<S> the latter two are examples of boosting algorithms which assemble an ensemble of weak learners ( classifiers ) in a network to create a final strong learner by iteratively re - weighting data points according to previous results  @xcite . </S>",
    "<S> the rusboost method  @xcite is particularly useful since it is tailored to the case of one class ( here @xmath58 ) being significantly smaller than the rest .    </S>",
    "<S> the results are in table  [ table : assfid_multiclass_methods ] . </S>",
    "<S> we again see an increase in assignment fidelities over the discriminant analysis methods of table  [ table : assfid_discriminant_methods ] . </S>",
    "<S> not surprisingly , rusboost provides the most significant increase . </S>",
    "<S> we repeated the k - means algorithm 50 times with random initializations and found it to be relatively stable ( sample variance @xmath39 of @xmath60 ) . </S>",
    "<S> we repeated this using fixed initialization of the means and obtained a variance of 0 .    out of all methods considered , </S>",
    "<S> non - linear svm s produce the greatest increase in @xmath31 ( 0.9586 to 0.9821 ) . </S>",
    "<S> we also note all methods are relatively stable with reproducible assignment fidelities ( each method was repeated @xmath61 times ; the sample means of @xmath31 are the table values and the sample variances are @xmath62 ) .    </S>",
    "<S> c c c method & all time - points & pca + [ 0.5ex ]    multi - lda & 0.9768 & 0.9689 + multi - svm & 0.9784 & 0.9717 + totalboost & 0.9527 & 0.9413 + rusboost & 0.9788 & 0.9723 +    [ table : assfid_multiclass_methods ]    while we have improved @xmath31 to 0.9821 , we are still far from @xmath63 . </S>",
    "<S> it is possible much of the remaining discrepancy comes from @xmath0 events . to investigate this </S>",
    "<S> we propose the simple error diagnosis test of replacing each @xmath0 event found from the k - means algorithm with a random element from @xmath64 . </S>",
    "<S> this provides a measure of @xmath31 when @xmath0 is negligible . </S>",
    "<S> the means of 100 samples for each method are contained in table  [ table : assfid_replace ] ( variances are @xmath65 ) . </S>",
    "<S> non - linear svm produces the highest value of @xmath31 however _ for all methods _ </S>",
    "<S> @xmath66 , which is more consistent with @xmath63 . </S>",
    "<S> this confirms @xmath0 events are the significant reason for lower @xmath31 values than expected .    </S>",
    "<S> c c c method & all time - points & pca + [ 0.5ex ]    ldad & 0.9920 & 0.9909 + lda & 0.9921 & 0.9928 + qdad & 0.9918 & 0.9908 + qda &  & 0.9927 + linear svm & 0.9936 & 0.9943 + non - linear svm & 0.9945 & 0.9949 +    [ table : assfid_replace ]    one attempt to reduce the significance of @xmath0 is to reduce @xmath13 , however this implies the trajectories will spend less time near their steady states and assignment errors due to variance will increase.to observe this , we truncated the trajectories to different @xmath13 and calculated @xmath31 using the non - linear svm . from fig .  </S>",
    "<S> [ fig : assfid_vary_time ] we see @xmath67s appears close to optimal . </S>",
    "<S> moreover , a much shorter measurement time of @xmath68s ( not shown in fig .  </S>",
    "<S> [ fig : assfid_vary_time ] ) is needed to achieve @xmath31 from lda . </S>",
    "<S> this is a strong message that better classifiers can allow for shorter measurement times . </S>",
    "<S> longer measurement times than the current @xmath14s decrease @xmath31 due to an increase in @xmath0 events .    </S>",
    "<S> to conclude , we have utilized ml to understand and improve the readout in a superconducting system . </S>",
    "<S> we find more sophisticated classification algorithms can potentially allow for shorter measurement times and increase assignment fidelities . </S>",
    "<S> non - linear svm s provided the largest increase in assignment fidelity from 0.9586 to 0.9821 ( @xmath1 ) . </S>",
    "<S> clustering helped diagnose the prevalence of systematic effects by finding clusters in the data corresponding to single - shot identification of heating and @xmath0 effects . </S>",
    "<S> we verified @xmath0 events are a significant source of error as the assignment fidelity increases from 0.9821 to 0.9945 when the @xmath0 cluster is replaced with typical trajectories . </S>",
    "<S> this is more consistent with our achievable fidelity and the remaining discrepancy can be due to effects such as heating and state - preparation errors . moving forward </S>",
    "<S> , we expect these methods will help provide insight for improving readout , especially when non - linear and non - gaussian effects are present .    </S>",
    "<S> we acknowledge support from aro under contract w911nf-14 - 1 - 0124 and iarpa under contract w911nf-10 - 1 - 0324 . </S>",
    "<S> we acknowledge helpful discussions with oliver dial , stefan filipp , blake johnson , jim rozen , colm ryan , marcus silva , and matthias steffen .    </S>",
    "<S> 10    p.  shor , in _ proceedings of the 37th annual symposium on foundations of computer science ( focs ) _ </S>",
    "<S> ( ieee press , burlington , vt , 1996 ) .    </S>",
    "<S> h.  paik et  al . </S>",
    "<S> , phys . </S>",
    "<S> rev . </S>",
    "<S> lett . * 107 * , 240501 ( 2011 ) .    </S>",
    "<S> j.  b. chang et  al . , applied physics letters * 103 * , 012602 ( 2013 ) .    </S>",
    "<S> r.  barends et  al . , physical review letters * 111 * , ( 2013 ) .    </S>",
    "<S> r.  barends et  al . , </S>",
    "<S> nature * 508 * , 500 ( 2014 ) .    </S>",
    "<S> f.  mallet et  al . , nat phys * 5 * , 791 ( 2009 ) .    </S>",
    "<S> n.  bergeal et  al . </S>",
    "<S> , nature * 465 * , 64 ( 2010 ) .    </S>",
    "<S> j.  e. johnson et  al . , phys . </S>",
    "<S> rev . </S>",
    "<S> lett . * 109 * , 050506 ( 2012 ) .    d.  rist et  al . </S>",
    "<S> , phys . </S>",
    "<S> rev . </S>",
    "<S> lett . </S>",
    "<S> * 109 * , 050507 ( 2012 ) .    </S>",
    "<S> a.  blais et  al . </S>",
    "<S> , phys . </S>",
    "<S> rev . </S>",
    "<S> a * 69 * , 062320 ( 2004 ) .    </S>",
    "<S> j.  koch et  al . </S>",
    "<S> , phys . </S>",
    "<S> rev . </S>",
    "<S> a * 76 * , 042319 ( 2007 ) .    j.  gambetta et  al . </S>",
    "<S> , phys . </S>",
    "<S> rev . </S>",
    "<S> a * 77 * , 012112 ( 2008 ) .    </S>",
    "<S> a.  d. crcoles et  al . , arxiv:1410.6419 ( 2014 ) .    </S>",
    "<S> a.  hentschel and b.  c. sanders , phys . </S>",
    "<S> rev . </S>",
    "<S> lett . * 104 * , 063603 ( 2010 ) .    </S>",
    "<S> m.  gu and w.  kotowski , new journal of physics * 12 * , 123032 ( 2010 ) .    </S>",
    "<S> see supplemental material for further details .    </S>",
    "<S> c.  a. ryan et  al . , arxiv:1310.6448 ( 2013 ) .    </S>",
    "<S> t.  m. cover , electronic computers , ieee transactions on * ec-14 * , 326 ( 1965 ) .    </S>",
    "<S> r.  a. fisher , annals of eugenics * 7 * , 179 ( 1936 ) .    </S>",
    "<S> k.  pearson , philosophical magazine series 6 * 2 * , 559 ( 1901 ) .    c.  cortes and v.  vapnik , machine learning * 20 * , 273 ( 1995 ) .    b.  e. boser , i.  m. guyon , and v.  n. vapnik , in _ proceedings of the fifth annual workshop on computational learning theory _ </S>",
    "<S> ( acm , new york , ny , usa , 1992 ) , colt 92 , pp . 144152 .    </S>",
    "<S> s.  lloyd , information theory , ieee transactions on * 28 * , 129 ( 1982 ) .    </S>",
    "<S> c.  bishop , _ pattern recognition and machine learning _ ( springer , 2007 ) .    c.  seiffert et  al . , systems , man and cybernetics , part a : systems and humans , ieee transactions on * 40 * , 185 ( 2010 ) . </S>"
  ]
}