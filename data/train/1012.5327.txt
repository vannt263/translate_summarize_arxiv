{
  "article_text": [
    "modulation level classification ( mlc ) is a process which detects the transmitter s digital modulation level from a received signal , using a priori knowledge of the modulation class and signal characteristics needed for downconversion and sampling . among many modulation classification methods  @xcite ,",
    "a cumulant ( cm ) based classification  @xcite is one of the most widespread for its ability to identify both the modulation class and level .",
    "however , differentiating among cumulants of the same modulation class , but with different levels , i.e. 16qam vs. 64qam , requires a large number of samples .",
    "a recently proposed method  @xcite based on a goodness - of - fit ( gof ) test using kolmogorov - smirnov ( ks ) statistic has been suggested as an alternative to the cm - based level classification which require lower number of samples to achieve accurate mlc .    in this letter",
    ", we propose a novel mlc method based on distribution distance functions , namely kuiper ( k )  @xcite  ( * ? ? ?",
    "3.1 ) and ks distances , which is a significant simplification of methods based on gof .",
    "we show that using a classifier based only on k - distance achieves better classification than the ks - based gof classifier . at the same time",
    ", our method requires only @xmath0 additions in contrast to @xmath1 additions for the ks - based gof test , where @xmath2 is the number of distinct modulation levels , @xmath3 is the sample size and @xmath4 is the number of test points used by our method .",
    "following  @xcite , we assume a sequence of @xmath3 discrete , complex , i.i.d . and sampled baseband symbols , @xmath5 $ ] , drawn from a modulation order @xmath6 , transmitted over awgn channel , perturbed by uniformly distributed phase jitter and attenuated by an unknown factor @xmath7 . therefore , the received signal is given as @xmath8 $ ] , where @xmath9 , @xmath10 and @xmath11 .",
    "the task of the modulation classifier is to find @xmath12 , from which @xmath13 was drawn , given @xmath14 .",
    "without loss of generality , we consider unit power constellations and define snr as @xmath15 .",
    "the proposed method modifies mlc technique based on gof testing using the ks statistic  @xcite .",
    "since the ks statistic , which computes the minimum distance between theoretical and empirical cumulative distribution function ( ecdf ) , requires _ all _ cdf points , we postulate that similarly accurate classification can be obtained by evaluating this distance using a smaller set of points in the cdf .",
    "let @xmath16=f(\\mathbf{r})$ ] where @xmath17 is the chosen feature map and @xmath18 is the number extracted features .",
    "possible feature maps include @xmath19 ( magnitude , @xmath20 ) or the concatenation of @xmath21 and @xmath22 ( quadrature , @xmath23 ) .",
    "the theoretical cdf of @xmath24 given @xmath12 and @xmath25 , @xmath26 , is assumed to be known a priori ( methods of obtaining these distributions , both empirically and theoretically , are presented in  ( * ? ? ?",
    "iii - a ) ) . the @xmath2 cdfs , one for each modulation level , define a set of test points @xmath27 with the distribution distances given by @xmath28 for @xmath29 , @xmath30 , and @xmath31 , corresponding to the maximum positive and negative deviations , respectively .",
    "note the symmetry in the test points such that @xmath32 .",
    "thus , there are @xmath33 test points for a @xmath2 order classification .",
    "the ecdf , given as @xmath34 is evaluated at the test points to form @xmath35 , @xmath36 . here",
    ", @xmath37 equals to one if the input is true , and zero otherwise . by evaluating @xmath38 only at the test points in ( [ eq : testpoints ] ) , we get @xmath39 which are then used to find an estimate of the maximum positive and negative deviations @xmath40 of the ecdf to the true cdfs .",
    "the operation of finding the ecdf at the given testpoints ( [ eq : dij ] ) can be implemented using a simple thresholding and counting operation and does not require samples to be sorted as in  @xcite . the metrics in ( [ eq : dj ] ) are used to find the final distribution distance metrics @xmath41 which are the reduced complexity versions of the ks distance ( rcks ) and the k distance ( rck ) , respectively .",
    "finally , we use the metrics in ( [ eq : metrics ] ) as substitutes to the true distance - based classifiers with the following rule : choose @xmath42 such that @xmath43 in the remainder of the letter , we define @xmath44 and @xmath45 , where @xmath46 .",
    "let @xmath47 $ ] denote the set of test points , @xmath48 , sorted in ascending order . for notational consistency",
    ", we also define the following points , @xmath49 and @xmath50 . given that these points are distinct , they partition @xmath51 into @xmath52 regions",
    ". an individual sample , @xmath53 , can be in region @xmath54 , such that @xmath55 , with a given probability , determined by @xmath26 .    assuming @xmath53 are independent of each other",
    ", we can conclude that given @xmath51 , the number of samples that fall into each of the @xmath52 regions , @xmath56 $ ] , is jointly distributed according to a multinomial pmf given as @xmath57 where @xmath58 $ ] , and @xmath59 is the probability of an individual sample being in region @xmath54 . given that @xmath51 is drawn from @xmath12 , @xmath60 , for @xmath61 .",
    "now , with particular @xmath62 , the ecdf at all the test points is @xmath63,\\quad f_n(t_l ) = { \\frac{1}{n } } \\sum\\limits_{i=1}^l n_i.\\ ] ] therefore , we can analytically find the probability of classification to each of the @xmath2 classes as @xmath64 for the rck classifier .",
    "a similar expression can be applied to rcks , replacing @xmath65 with @xmath66 in ( [ eq : probabilityofclassification ] ) .",
    "given that the theoretical cdfs change with snr , we store distinct cdfs for @xmath67 snr values for each modulation level ( impact of the selection of @xmath67 on the accuracy is discussed further in section [ sec : detection_samples ] . )",
    "further , we store @xmath68 theoretical cdfs of length @xmath69 each . for the non - reduced complexity classifiers that require sorting samples ,",
    "we use a sorting algorithm whose complexity is @xmath70 . from table  [",
    "table : complexity ] , we see that for @xmath71 rck / rcks tests use less addition operations than k / ks - based methods  @xcite and cm - based classification  @xcite . for @xmath72 ,",
    "the rck method is more computationally efficient when implemented in asic / fpga , and is comparable to cm in complexity when implemented on a cpu .",
    "in addition , the processing time would be shorter for an asic / fpga implementation , which is an important requirement for cognitive radio applications .",
    "furthermore , their memory requirements are also smaller since @xmath69 has to be large for a smooth cdf .",
    "it is worth mentioning that the authors in  @xcite used the theoretical cdf , but used @xmath69 as the number of samples to generate the cdf in their complexity figures .",
    "the same observation favoring the proposed rck / rcks methods holds for the magnitude - based ( mag ) classifiers  ( * ? ? ?",
    "* sec iii - a ) .",
    ".number of operations and memory usage [ cols=\"<,<,<,<\",options=\"header \" , ]     [ table : complexity ]",
    "as an example , we assume that the classification task is to distinguish between m - qam , where @xmath73 . for comparison we also present classification result based on maximum likelihood estimation ( ml ) .",
    "in the first set of experiments we evaluate the performance of the proposed classification method for different values of snr .",
    "the results are presented in fig .",
    "[ fig : vary_snr ] .",
    "we assume fixed sample size of @xmath74 , in contrast to  ( * ? ? ?",
    "1 ) to evaluate classification accuracy for a smaller sample size . we confirm that even for small sample size , as shown in  ( * ? ? ?",
    "1 ) , cm has unsatisfying classification accuracy at high snr . in",
    "( 10,17)db region rck clearly outperforms all detection techniques , while as snr exceeds @xmath7517db all classification methods ( except cm ) converge to one . in low snr region , ( 0,10)db , ks , rcks , rck perform equally well , with cm having comparable performance .",
    "the same observation holds for larger sample sizes , not shown here due to space constraints .",
    "note that the analytical performance metric developed in section  [ sec : analysis ] for rck and rcks matches perfectly with the simulations .",
    "for the remaining results , we set @xmath76db , unless otherwise stated .",
    "= 50 ; ( an . )",
    " analytical result using ( [ eq : probabilityofclassification ] ) . ]      in the second set of experiments , we evaluate the performance of the proposed classification method as a function of sample size @xmath3 .",
    "the result is presented in fig .",
    "[ fig : vary_n ] . as observed in fig .",
    "[ fig : vary_snr ] , also here cm has the worst classification accuracy , e.g. 5% below upper bound at @xmath77 .",
    "the rck method performs best at small sample sizes , @xmath78 . with @xmath79 ,",
    "the accuracy of rck and ks is equal .",
    "classification based on rcks method consistently falls slightly below rck and ks methods .",
    "in general , rcks , rck and ks converge to one at the same rate .",
    "db . ]      in the third set of experiments we evaluate the performance of the proposed classification method as a function of snr mismatch and phase jitter .",
    "the result is presented in fig .",
    "[ fig : mismatch ] . in case of snr mismatch ,",
    "[ fig : vary_gamma ] , our results show the same trends as in  ( * ? ? ? * fig .",
    "4 ) ; that is , all classification methods are relatively immune to snr mismatch , i.e. the difference between actual and maximum snr mismatch is less than 10% in the considered range of snr values .",
    "this justifies the selection of the limited set of snr values @xmath67 for complexity evaluation used in section  [ sec : complexity ] .",
    "as expected , ml shows very high sensitivity to snr mismatch . note",
    "again the perfect match of analytical result presented in section  [ sec : analysis ] with the simulations .    in the case of phase jitter caused by imperfect downconversion , we present results in fig .",
    "[ fig : vary_phase ] for @xmath80db as in  @xcite , in contrast to @xmath76db used earlier , for comparison purposes .",
    "we observe that our method using the magnitude feature , rck / rcks ( mag ) , as well as the cm method , are invariant to phase jitter .",
    "rck and rcks perform almost equally well , while cm is worse than the other three methods by @xmath7510% .",
    "as expected , the ml performs better than all other methods .",
    "quadrature - based classifiers , as expected , are highly sensitive to phase jitter .",
    "note that in the small phase jitter , @xmath81 , quadrature - based classifiers perform better than others , since the sample size is twice as large as in the former case .",
    "in this letter we presented a novel , computationally efficient method for modulation level classification based on distribution distance functions .",
    "specifically , we proposed to use a metric based on kolmogorov - smirnov and kuiper distances which exploits the distance properties between cdfs corresponding to different modulation levels .",
    "the proposed method results in faster mlc than the cumulant - based method , by reducing the number of samples needed .",
    "it also results in lower computational complexity than the ks - gof method , by eliminating the need for a sorting operation and using only a limited set of test points , instead of the entire cdf .",
    "o.  a. dobre , a.  abdi , y.  bar - ness , and w.  su , `` survey of automatic modulation classification techniques : classical approaches and new trends , '' _ iet communications _ , vol .  1 , no .  2 , pp .",
    "137156 , apr .",
    "2007 .",
    "g.  a.  p. cirrone , s.  donadio , s.  guatelli , a.  mantero , b.  masciliano , s.  parlati , m.  g. pia , a.  pfeiffer , a.  ribon , and p.  viarengo , `` a goodness of fit statistical toolkit , '' _ ieee trans .",
    "_ , vol .",
    "51 , no .  5 ,",
    "pp . 20562063 , oct ."
  ],
  "abstract_text": [
    "<S> we present a novel modulation level classification ( mlc ) method based on probability distribution distance functions . </S>",
    "<S> the proposed method uses modified kuiper and kolmogorov - smirnov distances to achieve low computational complexity and outperforms the state of the art methods based on cumulants and goodness - of - fit tests . </S>",
    "<S> we derive the theoretical performance of the proposed mlc method and verify it via simulations . </S>",
    "<S> the best classification accuracy , under awgn with snr mismatch and phase jitter , is achieved with the proposed mlc method using kuiper distances . </S>"
  ]
}