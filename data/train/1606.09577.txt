{
  "article_text": [
    "learning using privileged information ( first proposed by vapnik et .",
    "@xcite ) seeks to bring in privileged information to assist the learner .",
    "this information is privileged because the learner may use it choose a hypothesis , but the privileged information will be unavailable making decisions based on the hypothesis .",
    "this paper proposes a lupi method that directly minimizes the variance diameters of the hypothesis spaces under consideration , an essential quantity in fast - convergence literature @xcite .",
    "this approach applies to discriminant - based hypotheses spaces where predictions are derived from thresholding the the discriminant value , which should be totally orderable .",
    "we show that the discriminant ordering defines equivalence classes for the elements of the space , and these classes are directly related the the variance diameters we seek to control .",
    "if we could restrict the hypothesis space to just the good equivalence classes , we would reduce the variance diameters and improve the speed of convergence .",
    "selecting a good equivalence class requires some external definition of desirable order .",
    "this is privileged information .",
    "this raises a natural question:_what is a desirable order ? _ if ordering information were provided by an oracle according to some true distribution , then _ any _ ordering would provide desirable variance diameters .",
    "however , since an empirical ordering is the best we can hope for , a good ordering is one which has favorable convergence properties in ordinal regression .",
    "low ordinal loss is sufficient , while more general characterizations may be possible .",
    "from another perspective , orderings according to conditional probability @xmath0 have great appeal , as models that provide good estimates of conditional probability allow broader application than ones that do not .",
    "moreover , confidence seems to be a sliver of common ground between human and machine learning .",
    "thus , we consider orderings which seem to bear some relationship to conditional probability , though quite loose . while a total ordering is best for controlling variance diameters , two independent orderings for each class can be shown to be very nearly as good .",
    "this arrangement allows us to expand the scope of tasks where useful privileged information is available .",
    "we ca nt hope to provide any kind of overview of the field of order statistics .",
    "web search made this a lucrative and popular field .",
    "nevertheless , we believe our starting point is novel :    * order * is any property of a set of real numbers that is invariant under any invertible increasing transformation .",
    "[ def : order ]    ordering naturally defines equivalence classes on hypotheses spaces .",
    "suppose some distribution @xmath1 generates feature vectors @xmath2 , and suppose @xmath3 and @xmath4 are hypotheses in a space @xmath5 .",
    "if an increasing function @xmath6 exists so that for all @xmath7 , @xmath8 , then @xmath3 and @xmath4 are in the same equivalence class .",
    "the thread which connects order to the pattern recognition task is the growth function , which measures the number of possible labelings of a set of points of size @xmath9 by a 0/1-hypothesis set @xmath10 .",
    "we assume that @xmath10 is defined by characteristic functions of real - valued functions , as in @xmath11 .",
    "we observe that if @xmath12 contains a restricted number of order equivalence classes , then the growth function of @xmath13 is also restricted .",
    "we propose that the relationship can be made precise by the machinery of variance - based risk bounds .    as a thought experiment ,",
    "consider the variance diameter of @xmath3 and @xmath4 when combined with an appropriate 0/1 loss function , and restricted to one class . more formally",
    ", suppose now that @xmath1 generates feature vectors and labels @xmath14 jointly .",
    "zero - one loss is : @xmath15 $ ] is depicted as @xmath16 , and the empirical risk on a finite set of size @xmath9 is depicted as @xmath17 .",
    "there are numerous more measurability and existence assumptions that we will not cover here . ]",
    "then for any @xmath3 and @xmath4 in the same order equivalence class for for any @xmath18 and @xmath19 , it is easy to show that @xmath20 except for the restriction to a single class , this is just the relationship for variance diameters the minimum of the true risk over @xmath13 .",
    "the following must hold uniformly for all @xmath21 : @xmath22 \\\\ \\leq &   \\operatorname*{\\mathrm{e}}[l^{01}(h(x),y ) - l^{01}(h'(x),y ) ]   \\end{split } \\label{withinclassvariance}\\end{aligned}\\ ] ] our presentation is slightly different because the variance can be upper bounded by the expectation of the absolute value and we have not specialized to @xmath23 . note that many presentations of fast convergence can lead an in - attentive reader to believe that @xmath23 must be the bayes rule .",
    "this is true if one is using the mammen - tsybakov noise conditions to establish the desired variance relationship , but not necessary if the relationship can be established another way , as we do here .",
    "] that is needed for fast rates .",
    "there are two tasks required to extend the thought experiment to a real learning formulation .",
    "first , the relationship needs to apply in both classes simultaneously .",
    "the complicating factor to just adding the two per - class relationships is that the absolute - value arguments in the two respective rhs s could have opposite signs and cancel out .",
    "this can happen when the decision threshold falls in very different places relative to the ordering .",
    "we fix this by requiring that the loss in the two classes be balanced as a constraint on valid solutions . in effect , this is a constraint on the location of the decision boundary in the equivalence class definition , preventing the situations where there is cancellation .",
    "more significantly , there is no access to the equivalence classes based only on empirical information .",
    "this is the majority of the analysis in the remainder of the paper . in short ,",
    "we relax the notion of the equivalence class to metric balls , and then we bound bound the deviation of empirical balls from their true diameter .",
    "we define a metric on orderings so that two hypotheses are in the same equivalence class if their metric distance is 0 .",
    "the metric , when shown to have favorable properties , allows us to create balls of restricted variance diameter based on a finite sample using ordinary empirical risk minimization .",
    "let @xmath24 be the set of all increasing continuous functions .",
    "let @xmath1 generate vectors @xmath2 , and consider any two functions @xmath25 . then the order distance between @xmath3 and @xmath4 is @xmath26\\ ] ]    the metric axioms ( up to equivalence class elements ) are not hard to check ; the invertability of @xmath6 is indispensable here .    while many definitions would satisfy the metric axioms , we chose this definition for two reasons . first , it is a direct extension of the result we saw for equivalence classes . if @xmath27 , then ( [ equivsingleclassvariance ] ) holds with @xmath28 added to the right - hand side .",
    "second , the fact that 0/1 loss is an underlying component allows us to borrow a great deal from the standard results in machine learning .    since we have no access to true probabilities in a statistical learning setting , to proceed we have to derive a way to get access to @xmath29 .",
    "we accomplish this by extending @xmath29 to measure the order distance of a function @xmath30 to some _ ground truth _ ordering instead of another function .",
    "the key observation is that functions within @xmath31 of the ground truth ordering are within @xmath32 of each other by the triangle inequality ",
    "an empirical version of the equivalence classes we set out to find .",
    "we will redefine this extension of @xmath29 as @xmath33 for clarity :    suppose @xmath1 jointly generates feature vectors @xmath34 and real - valued labels @xmath35 .",
    "let @xmath5 be some hypothesis space . @xmath36\\ ] ]    the task at hand is to analyze risk bounds for @xmath33 that allows us , with high probability , to identify functions where @xmath37 . based on a finite sample .",
    "the key observation is that the infimum ( @xmath38 ) and supremum ( @xmath39 ) in the definition of @xmath33 can be handled by a uniform bound on the deviations of a loss function over the joint space @xmath40 .",
    "we define that loss function to be @xmath41 :    @xmath42    we will show that if @xmath13 has bounded level vc - dimension , . ]",
    "then the loss class for @xmath41 over @xmath40 also has bounded vc dimension , so we can derive uniform risk bounds for empirical risk minimization .",
    "we observe that level vc - dimension already satisfies an order invariance .",
    "that is , the inclusion of @xmath43 does nt have any affect on the estimated growth function .",
    "let @xmath44 .",
    "then the level vc dimension of @xmath45 is the same as for @xmath13 .",
    "the proof is a simple shattering argument .",
    "obviously , @xmath46 , so the vc - dimension is not less .",
    "it is not more because any rule @xmath47 can be replicated by @xmath48 for an appropriately chosen @xmath49 .",
    "these two observations give the following theorem :    [ thm : ordinalregressionbound ] suppose @xmath13 has level vc - dimension @xmath50 .",
    "there exists a universal constant @xmath51 such that uniformly for all @xmath21 the following holds with probability at least @xmath52 : @xmath53    the bound is a straightforward application of uniform risk bounds , provided that the vc dimension of the loss class can be found .",
    "this is a straightforward shattering argument .",
    "suppose @xmath13 has level vc dimension @xmath50 .",
    "suppose there exists a set @xmath54 and some @xmath55 such that @xmath56 can attain any labeling .",
    "considering the two sets @xmath57 and @xmath58 , one of these has size at least @xmath59 by the pigeonhole principle .",
    "call the set @xmath60",
    ". then @xmath61 is shattered , which contradicts our assumption about the level vc dimension of @xmath13 .",
    "hence , the vc - dimension is at most @xmath62 .",
    "extending the argument to allow separate orderings in each class is similar ; there are now four pigeonholes because we have to consider a separate threshold for each ordering , so @xmath63 points will ensure a contradiction .",
    "the ordinal regression bound is the substantial piece of the puzzle , but to make use of it have to enforce class balance .",
    "suppose that the user provides some parameter @xmath64 as a weight to favor or discourage loss in one class over another :    [ lossbalance ] loss balance , given a specified parameter @xmath65 , is measured by @xmath66    the vc dimension of this loss class can be analyzed in terms of the vc dimension for the underlying hypothesis space just like in the ordinal regression application , giving a similar bound . with this in hand , we are ready for the main theorem :    [ thm : riskbound ] suppose some predefined ordering is provided .",
    "consider the subset of @xmath13 that satisfies the predefined ordering up to loss @xmath28 and assume that the user provides a loss balance parameter that is empirically satisfied : @xmath67 assume that this set is not empty .",
    "let @xmath68 be the empirical minimizer ( of @xmath69 ) and @xmath70 be the true minimizer ( of @xmath71 ) over @xmath12 .",
    "then there exists a constant @xmath51 such that the following holds uniformly for all @xmath72 and for all @xmath73 with probability at least @xmath74 :    @xmath75    the key to interpreting this bound is to note that one tunes @xmath76 to optimize the value in @xmath9 .",
    "while the first term on the rhs dominates , let @xmath77 , but when this drops below subsequent terms , @xmath76 can be set like @xmath78 , giving an overall convergence like @xmath79 down to a constant times @xmath28 , and then @xmath76 would be set like @xmath80 thereafter . obviously , a small @xmath28 is important ; that is to say , the privileged orderings should fit well to some @xmath21 under @xmath33 .",
    "however , @xmath28 could be much smaller under favorable circumstances .",
    "we believe it would be possible to characterize these by extending the mammen - tsybakov noise conditions to @xmath33 . a proof is given in the appendix .",
    "we now study methods to find @xmath82 , assuming a linear or rkhs hypothesis space with defined level vc - dimension .",
    "we note that for a fixed @xmath30 , the optimal @xmath6 can be found by means of a dynamic program . a method to construct real - valued functions with defined level vc - dimension",
    "was given by vapnik @xcite .",
    "the technique requires the model to separate each successive example by a minimum margin . as common with zero - one loss , we relax it to hinge loss to make a convex model .",
    "assume that examples are sorted increasing in @xmath83 and there are no duplicates ( which require extra attention ) .",
    "define @xmath84 in the following formulation , @xmath51 is a user - defined capacity control parameter and @xmath85 can be assumed : @xmath86 this is a convex quadratic programming problem , though regrettably requiring @xmath87 ( where @xmath9 is the number of examples ) dummy variables to compute the max in line [ eqn : badline ] .",
    "this makes the program intractable ( by machine learning standards ) , at least without a special solver .",
    "the regression problem can be made more tractable by relaxing it to alternative ordinal regression formulations , such as the one proposed by sashua & levin @xcite .",
    "their formulation penalizes uses optimization variables to define ordered slots according to the sort order of the targets @xmath83 , and a training example is penalized if it does not project into its slot .",
    "it can be shown that the relaxation loss is an upper bound on the unrelaxed loss .",
    "the relaxed formulation is a quadratic program with just @xmath9 constraints .",
    "the global - order svm ( go - svm ) is the name for the formulation we propose .",
    "it is simply is the usual svm hypothesis space ( thick hyperplanes ) and loss ( hinge ) , but it is simultaneously optimized with the sashua & levin @xcite ordinal regression relaxation , with the svm discriminant @xmath64 constrained to be the same as the ordering hypothesis @xmath64 .",
    "this constraint implements the restriction from @xmath13 ( the svm hypothesis space ) to @xmath12 ( hypotheses that satisfy an ordinal condition ) , as defined in theorem [ thm : riskbound ] .",
    "loss and capacity control are traded off between the bi - objectives by means of user - selectable weights .",
    "capacity control in both svm and the ordinal regression formulations is attained by the relationship between the squared norm of the predictor @xmath64 and the size of the margin . in either formulation by itself",
    ", one can fix the margin size and place all capacity control in the squared norm of @xmath64 , trading it off with loss .",
    "however , @xmath64 serves a two - fold role in this formulation ; therefore implementing different capacities for the two learning objectives requires setting the margins . noting that the @xmath88-svm formulations @xcite have the margin as an optimization variable , we extended the approach so that the usual tradeoff between loss and capacity is preserved .",
    "the formulation is @xmath89 here , @xmath90 is an index function that returns an in - order , unique index for each distinct oracle value for in each class , and @xmath91 is a vector of interval boundaries .",
    "ordering is enforced because there are no empty intervals .",
    "the within - class ordering variant in principle requires two ordinal regressions , but in practice it can be done with a trick using the index function @xmath90 by creating an empty interval .",
    "variable @xmath64 is the linear predictor , @xmath92 is a bias term , @xmath93 is the hinge loss for the classification problem , and @xmath94 is hinge loss for the ordinal problem .",
    "constant @xmath95 is defined to control the feasible range of @xmath96 .",
    "it is @xmath97 if there are not ties in the ordering .",
    "parameter @xmath98 controls the vc - dimension of the 0/1 loss class , @xmath96 controls the maximum vc - dimension of each subproblem in @xmath99 .",
    "finally , parameter @xmath100 is related to choosing the size of @xmath28 in theorem [ thm : riskbound ] , expressed in terms of the permissiveness of the ordinal loss compared to the 0/1 loss .",
    "we do not attempt to enforce loss balance , as theory tells we should ; from any computed solution , we can still apply the bound for as if we had constrained it to the value achieved by the optimum ; moreover , we have never known the uncontrained optimum to have unreasonable loss balance , and we have no reason to prefer otherwise .    like @xmath88-svm @xcite , the optimization problem can be characterized in terms of @xmath98 and @xmath96 and training data .",
    "the it can be proved that the problem is primal and dual feasible for @xmath101 $ ] , @xmath102 $ ] , and @xmath103 ; and primal unbounded / dual infeasible otherwise .",
    "the representer theorem @xcite holds for go - svm , so the solution can be expressed in terms of the dual variables and kernels can be used .",
    "we used matlab s interior - point convex quadratic programming solver .",
    "the baseline @xmath88-svm formulation was also implemented using the same solver , so that differences in numerical accuracy could not arise .",
    "the goal of evaluation is to prove that the order oracle hypothesis space allows faster convergence than a learning formulation which considers only the labels .",
    "since go - svm is an extension of standard svm , it is a logical baseline , and we compare only to that .",
    "however , these experiments are similar to other common experiments in lupi literature , and we will point these to the reader where appropriate . moreover , because of the construction of the go - svm hypothesis spaces , it can not outperform svm by virtue of a richer hypothesis space .",
    "faster convergence is the only alternative explanation .",
    "the evaluation is not intended to be a statement about the fitness of the hypothesis spaces for the learning task , but only about the ability of the learner to select the best element .",
    "the experimental setup is to hold out a testing set and sample remaining examples for 12 random realizations of training and validation sets .",
    "the validation set is used for a set of model selection experiments , and results are reported on the test set , which is used for all experiments . testing sets contained at least 1800 examples .",
    "the formulations have a fixed , auto - scaling parameter @xmath88 , and we use structural risk minimization to choose from a fixed set of parameters @xmath104 $ ] .    the rbf kernel width ( where used ) is chosen from the @xmath105$]-quantiles of the pairwise distance of training points .",
    "the kernel parameter was chosen by a hold - out validation on the svm experiment and re - used in the go - svm formulation to cut down the size of the model search .",
    "the @xmath100 parameter in the go - svm method was chosen from @xmath106 $ ] .",
    "the first evaluation is up / down prediction of the mackey - glass synthetic timeseries @xcite .",
    "it was used in the lupi setting ( svm+ ) in @xcite , where the authors used a 4-dimensional embedding @xmath107 in order to predict @xmath108 .",
    "privileged information was a 4-dimensional embedding around the target : @xmath109 .",
    "the authors compared svm+ to svm .",
    "we were not able to replicate their results for either svm or svm+ , which we suspect arises from the parameters used to generate the timeseries .",
    "( we used an integration step size of @xmath110 , with points created every 10 , delay constant @xmath111 , and initial value @xmath112 . )",
    "we use @xmath113 as the order oracle .",
    "we use an rbf kernel for all experiments with this dataset .",
    "the second evaluation is predicting binary survival at a fixed time from onset .",
    "we create synthetic datasets using the same procedure as shiao & cherkassky ( * ? ? ?",
    "* personal communication ) , with noise level @xmath110 and no censoring , which is given by an exponential distribution parameter @xmath114 .",
    "while censored data are an inherent aspect of survival studies , we avoid it in this case because the ordinal model can be modified to accommodate the partial information that censored examples contain ; thus , it is an experiment for another day .",
    "they compare svm , svm+ , and the cox proportional hazards model .",
    "privileged information for svm+ was related to the patient s overall survival time and whether the event time is right censored ( only known to be greater than some value ) .",
    "we use the ( absolute ) difference in the fixed prediction horizon and the event time for the order oracle , and we ignore whether an example is censored .",
    "we consider only linear models .",
    "the last evaluation is handwritten digit recognition , which was used by vapnik and vahist @xcite for svm+ and slightly adapted by lapin _",
    "_ for their proposed lupi method @xcite .",
    "the task is to classify downsampled ( @xmath115 ) mnist images based on pixel values .",
    "lapin added human - annotated confidence scores to training examples ( available for download ) .",
    "we repeat the experiment using their data preparation and using their annotators confidence scores as the order oracle .",
    "these experiments use an rbf kernel .",
    "the go - svm formulation considers a model space of 3 dimensions : a parameter to control the complexity of the classification problem , a parameter to control the complexity of the ordinal regression problem , and a parameter that balances the loss between these two problems .",
    "all of the parameters are chosen from fixed lists , which were detailed supra .",
    "the most basic form of model selection requires choosing the best node of the grid .",
    "we found that traditional hold - out model selection strategies are more difficult with go - svm .",
    "the trouble appears to be because the assumptions of structural risk minimization @xcite no longer hold . in traditional srm",
    ", the nested hypothesis spaces ensures that the loss expectation of the empirical risk minimizer ( as a function of complexity ) is coercive , making the selection of a minimum considerably more reliable than if it occurred at random . in our framework",
    ", there is no total ordering of hypothesis complexity .",
    "some hypothesis spaces ( defined by parameters ) have good convergence , while others do not .",
    "the task is to differentiate them .",
    "we analyzed loss surfaces with respect to various dimensions of the parameter selection grid .",
    "we used synthetic datasets where we could to generate a great number of examples .",
    "we observed that , although the surfaces were not coercive , they tended to be smooth .",
    "since the parameters of the models have a direct interpretation , parameters that are similar should have similar performance in expectation when trained on the same set .",
    "thus , we decided to try a gaussian filter to smooth the validation results , and then select the minimum in a grid search .",
    "the filter was constructed apriori , and was used for all the datasets in the evaluation .    in each experiment",
    ", we computed the loss on the test set that would have been found by each of three methods :    1 .   a standard holdout , with the held - out validation set the same size as the training set .",
    "one might use cross - validation in practice .",
    "this is called ` unsmooth . ' 2 .   the gaussian smoothing technique using the same holdout .",
    "this is called ` smoothed . '",
    "3 .   a very large ` oracle ' validation set which reveals how good the best hypothesis space is",
    "this is called ` extended . '",
    "the gaussian filter was of size 5x5x3 , with the smallest dimension corresponding to the @xmath100 parameter .",
    "( in experiments using a kernel parameter , we used one found via the svm model search , so this was not a grid search parameter . )",
    "it has the property that any projection along coordinate directions of the filter is gaussian .",
    "this was convolved with the tensor of validation scores using zero - degree smooth extrapolation ; that is , the tensor is padded out with constants which are the same as the nearest true element of the tensor .",
    "the convolution gives a new tensor that is the same dimension as the un - smoothed tensor .",
    "we also considered two alternative validation scenarios : first , selecting a model based on the un - smoothed tensor , and second , investigating the effect of having a much larger validation set available .",
    "the large validation set is intended to point out the gap between the best hypothesis spaces that can be created using the ordinal constraint technique , and the one which can in practice be selected .",
    "we point out that many lupi research papers require validation sets that would not ordinarily be a reasonable split between training and testing data ( for example @xcite .",
    "each row of the table gives the size of the training and test sets .",
    "the columns give the model selection procedure .",
    "[ results ]    .results ( error rate ) for all experiments .",
    "winners are reported excluding the extended validation experiments . [ cols=\"<,^,^,^,^,^ \" , ]      a table of results is given at table ( [ results ] ) .",
    "as a reminder , sizes for training and testing are given in parenthesis with the experiment name .",
    "the std , non - smooth , and smoothed methods used a validation set the same size as the training set .",
    "we note first that the gap between the extended validation model selection and the performance of the typical technique is larger for go - svm than for standard svm .",
    "this is a blessing in that we have the opportunity to find a better model , but also a curse in that the variance is higher .",
    "it appears that this strain of lupi methods is bound by model selection issues .",
    "the gaussian smoothing approach seems to have been effective on the digits dataset , and certainly did not hinder performance significantly where the un - smoothed model selection turned out to be superior .    the mackey - glass dataset is the only one which has strongly significant results . although the gains in other datasets are small , the fact that they are supported by theory implies they should not be overlooked . moreover , they are consistent with results reported by other authors",
    ". there comparisons with other works for the mackey - glass and digits experiments .",
    "the mackey - glass experiment appeared in the original svm+ paper @xcite .",
    "they report , based on a training set of 100 examples , that svm had an error rate of .052 , whereas the best svm+ formulation was at .048 .",
    "furthermore , this was based on a validation set of size 500 .",
    "we have reached that level of performance with considerably less data .",
    "the digits experiment is intended to replicate one in @xcite .",
    "we specifically replicated the experiment in which conditional probability weights were created by humans with an intent to help a machine .",
    "this task is well - suited to the order invariance that go - svm is built on , as humans have a fundamentally ordinal notion of confidence . in that study at a sample size of 80 , the difference between the best and worst methods under study was about .01.073 compared to .083 ( approximately ) .",
    "the size of the validation set in use , would be comparable to our extended experiment .",
    "their best method , however , did not use human weights . in their experiment , the human weights information improved over svm by about .003 , whereas our gain is .006 .    in conclusion , the fact that the formulation can find faster - converging models than formulations which do nt consider order information supports the underlying theory .",
    "it appears likely the order information is helpful in scenarios when the prediction task discretizes some continuous attribute , such as in the timeseries and survival prediction tasks .",
    "the original svm+ paper @xcite touched off a fair amount of research in the area .",
    "most research , with limited exceptions , has focused on developing and evaluating formulations @xcite rather than attempting to develop theory to understand when and why such a technique might be useful .",
    "pechyony et .",
    "@xcite analyze the svm+ algorithm in terms of variance bounds .",
    "while it shares with this work a major emphasis on variance bounds , that work considers the svm+ loss function as given and derives bounds for it , whereas this paper works the other way in attempting to derive a formulation based on the bound .",
    "lapin et .",
    "@xcite propose weighting examples based on class conditional probability , and is most directly similar to the ideas proposed here .",
    "intuitively , the method encourages a learner to prioritize performance on the easy examples over the hard examples .",
    "unfortunately , the theoretical motivation for departing from empirical risk minimization takes a tenuous path through svm+ @xcite , namely that svm+ is reducible to weighted learning .",
    "the heart of their method is based on a loss function based on weights interpreted as conditional probability ; however , a theoretical analysis is not provided .",
    "our is somewhat more general in allowing order in variances .",
    "the first result is a bridge between the variance conditions and the metric balls of hypotheses we can actually define .",
    "this result shows that the uniform variance conditions can be relaxed by a small constant , depicted here as @xmath116 , at the expense of the rate of convergence when the bound is small .",
    "suppose there is a loss class @xmath117 with vc - dimension @xmath50 , and let @xmath118 attain @xmath119 $ ] .",
    "there is a uniform constant @xmath51 such that if the following holds uniformly for all @xmath120 , @xmath121   \\leq \\frac{1}{h } \\operatorname*{\\mathrm{e}}[f - f ' ] + d(n)\\end{aligned}\\ ] ] then for any @xmath122 , the following holds uniformly with probability @xmath52 : @xmath123 , \\operatorname*{\\mathrm{e}}_n[f ' - f_n]\\right ) \\\\",
    "\\leq & \\frac{8}{n\\phi } \\left ( 4 c^2 v \\log{n } + \\left ( 1 + 2\\phi \\right ) \\log { \\frac{1}{\\delta } } \\right ) + 32\\phi d(n).\\end{aligned}\\ ] ]    i follow the definitions and notation in bucheron _",
    "* theorem 5.5 ) . in their notation",
    "it is straightforward to show that @xmath124 . for vc - classes",
    ", it can be proved that @xmath125 @xcite .",
    "the risk bound depends on the solution of a fixed - point equation .",
    "let @xmath126 be the solution of @xmath127 .",
    "let @xmath128 .",
    "the following analysis shows that @xmath129 , which implies @xmath130 .",
    "@xmath131 at step [ taylor ] i used that the first - order approximation of the square root is an upper bound .",
    "the bound @xmath132 can be substituted wholesale into the bound given by bucheron in the statement of the theorem , which gives the theorem .",
    "the next step is to show that the combination of the ordinal constraint and the balance constraint are sufficient to bound the variance diameter of the subset of a hypothesis space that satisfies those conditions .",
    "[ pairwisebound ] suppose that an ordering and loss balance parameter @xmath64 are provided and and that @xmath133 .",
    "that is , @xmath134 , @xmath135 , and @xmath136 and @xmath137 .",
    "finally , suppose @xmath138 .",
    "then @xmath139 + 4(b+d ) $ ] .    for the purposes of the proof",
    ", we decompose the expectation by class .",
    "let @xmath140 and @xmath141 .",
    "let @xmath142 and @xmath143 .",
    "similarly , let @xmath144 and @xmath145 be loss functions defined by conditional expectations .",
    "the outline of the argument is shown below .",
    "we will expand each line subsequently .",
    "@xmath146 p_p + \\operatorname*{\\mathrm{e}}_{p_n } [ l^{01}(f(x),-1 ) - l^{01}(g(x),-1 ) ] p_n + 4(d_0+b_0 ) \\label{lossbalanceline}\\\\ = \\ ; & l^{01}(f ) - l^{01}(g ) + 4(d_0+b_0)\\end{aligned}\\ ] ]    we begin by proving the inequality at line [ variancebound ]",
    ". consider only the positive class for a moment .",
    "let @xmath147 be the decision ( or margin , as a simple extension ) boundary for @xmath148 and @xmath149 the boundary for @xmath91 .",
    "then @xmath150 is @xmath151 for @xmath152 and @xmath153 otherwise .",
    "we have noted the triangle inequality relationship between @xmath29 and @xmath33 .",
    "suppose @xmath134 and @xmath135 , then @xmath154 .",
    "let @xmath155 and @xmath156 be monotone functions as defined ( implicitly ) in @xmath33 .",
    "then @xmath157 is a continuous monotone function which makes the metric relationship true .",
    "we will first show @xmath158 | + 2d_0 \\\\ \\leftrightarrow & \\operatorname*{\\mathrm{e}}_{p_p } [ { \\mathbbm{1}}_{f(x ) \\leq \\delta_f \\ ; \\wedge \\ ; g(x ) > \\delta_g } ] +   \\operatorname*{\\mathrm{e}}_{p_p } [ { \\mathbbm{1}}_{f(x ) > \\delta_f \\ ; \\wedge \\ ; g(x ) \\leq \\delta_g } ] \\\\ & \\leq | \\operatorname*{\\mathrm{e}}_{p_p } [ { \\mathbbm{1}}_{f(x ) \\leq \\delta_f \\ ; \\wedge \\ ; g(x ) > \\delta_g } ] - \\operatorname*{\\mathrm{e}}_{p_p } [ { \\mathbbm{1}}_{f(x ) > \\delta_f \\ ; \\wedge \\ ; g(x ) \\leq \\delta_g } ] | + 2d_0\\end{aligned}\\ ] ] rewriting concisely , we wish to show @xmath159 .",
    "in fact , this holds because either @xmath160 or @xmath161 , which can be proved in the following way : expanding @xmath162 we have : @xmath163 = & \\operatorname*{\\mathrm{e}}[{\\mathbbm{1}}_{f \\leq \\delta_f \\ ; \\wedge \\ ; g > \\delta_g \\ ; \\wedge \\ ; g > m_g^f(\\delta_f ) } ] + \\operatorname*{\\mathrm{e } } [ { \\mathbbm{1}}_{f \\leq \\delta_f \\ ; \\wedge \\ ; g > \\delta_g \\ ; \\wedge \\ ; g \\leq m_g^f(\\delta_f)}]\\\\ \\leq & d_0 + \\operatorname*{\\mathrm{e } } [ { \\mathbbm{1}}_{g > \\delta_g \\ ; \\wedge \\ ; g \\leq m_g^f(\\delta_f ) } ] \\label{zeroexpectationterm}\\end{aligned}\\ ] ]    the expectation term in line [ zeroexpectationterm ] is @xmath153 if @xmath164 . repeating the procedure for @xmath92 shows that the corresponding term is @xmath153 if @xmath165 .",
    "since one of those conditions must be true , at least one of @xmath162 or @xmath92 is bounded by @xmath28 , which proves line [ positivevariancebound ] .",
    "a bound for the negative class is identical .",
    "this proves inequality [ variancebound ] .    to prove inequality [ lossbalanceline ] , the assumptions on class balance are needed . since we assumed that @xmath166 , then either ( or",
    "both ) @xmath167 or @xmath168 . if both are true , then the desired inequality ( [ lossbalanceline ] ) is trivial .",
    "suppose that @xmath167 and @xmath169 .",
    "@xmath170 where we used that @xmath171 , @xmath172 , @xmath173 , and @xmath65 .",
    "the proof if @xmath168 and @xmath174 uses that @xmath175 and @xmath176 ."
  ],
  "abstract_text": [
    "<S> we propose to accelerate the rate of convergence of the pattern recognition task by directly minimizing the variance diameters of certain hypothesis spaces , which are critical quantities in fast - convergence results . </S>",
    "<S> we show that the variance diameters can be controlled by dividing hypothesis spaces into metric balls based on a new order metric . </S>",
    "<S> this order metric can be minimized as an ordinal regression problem , leading to a lupi application where we take the privileged information as some desired ordering , and construct a faster - converging hypothesis space by empirically restricting some larger hypothesis space according to that ordering . </S>",
    "<S> we give a risk analysis of the approach . </S>",
    "<S> we discuss the difficulties with model selection and give an innovative technique for selecting multiple model parameters . </S>",
    "<S> finally , we provide some data experiments . </S>"
  ]
}