{
  "article_text": [
    "wireless sensor networks consist of small devices ( sensors ) with limited resources ( e.g. , low cpu power , small bandwidth , limited battery and memory ) .",
    "they can be deployed to monitor objects , measure temperature , detect fires , and other disaster phenomena .",
    "they are often used in isolated , hard to reach areas , where human involvement is limited .",
    "consequently , data acquired by sensors may have short lifetime , and any processing on it within the network should have low complexity and power consumption  @xcite .",
    "we consider a large - scale wireless sensor networks with @xmath0 sensors . among them , @xmath4 sensors have collected ( sensed ) some information . since sensors are often short - lived because of limited energy or hostile environment , it is desirable to disseminate the acquired information throughout the network so that each of the @xmath0 nodes stores one ( possibly coded ) packet and the original @xmath1 source packets can be recovered in a computationally simple way from any @xmath5 of nodes for some small @xmath6 . here , the sensors do not know locations of each other , and they do not maintain any routing tables .",
    "various solutions to the centralized version of this problem have been proposed , and are based on well known coding schemes such as fountain codes  @xcite or mds codes  @xcite . to distribute the information from multiple sources throughout the network",
    "so that each node stores a coded packet as if obtained by centralized lt ( luby transform ) coding  @xcite , lin  _ et al . _",
    "@xcite proposed a solution that uses random walks with traps . to achieve the desired code degree distribution , they employed the metropolis algorithm to specify transition probabilities of the random walks . in this way ,",
    "the original @xmath1 source packets are encoded by lt codes and the decoding process can be done by querying any @xmath5 arbitrary sensors . because of properties of lt codes , the encoding and decoding complexity are linear and therefore have low energy consumption .    in the methods of @xcite , the knowledge of the total number of sensors @xmath0 and sources @xmath1",
    "is required for calculating the number of random walks that each source needs to initiate and for calculating the probability of trapping at each sensor .",
    "another type of global information , namely , the maximum node degree ( i.e. , the maximum number of neighbors ) in the network , is also required to perform the metropolis algorithm .",
    "however , for a large - scale sensor network , such global information may not be easy to obtain by each individual sensor , especially when there is possibility of change in topology .",
    "moreover , the algorithms proposed in  @xcite assume that each sensor encodes only after receiving enough source packets .",
    "this requires each sensor to maintain a large enough temporary memory buffer , which may not be practical in real sensor networks .    in this paper",
    ", we propose two new algorithms to solve the distributed storage problem in large - scale sensor networks .",
    "we refer to these algorithms as lt - codes based distributed storage - i ( ltcds - i ) and lt - codes based distributed storage - ii ( ltcds - ii ) .",
    "both algorithms use simple random walks without trapping to disseminate source packets .",
    "in contrast to the methods in  @xcite , both algorithms demand little global information and memory at each sensor . in ltcds - i , only the values of @xmath0 and @xmath1 are needed , whereas the maximum node degree , which is more difficult to obtain , is not required . in ltcds - ii , no sensor needs to know any global information ( that is , knowing @xmath0 and @xmath1 is no longer required ) .",
    "instead , sensors can obtain good estimates for those parameters by using some properties of random walks",
    ". moreover , in both algorithms , instead of waiting until all the necessary source packets are collected to do encoding , each sensor makes decisions and performs encoding online upon each reception of resource packets .",
    "this mechanism reduces the memory demand significantly .",
    "the main contributions of this paper are as follows :    we propose two new algorithms ( ltcds - i and ltcds - ii ) for distributed storage in large - scale sensor networks , using simple random walks and lt codes .",
    "these algorithms are simpler , more robust , and less constrained in comparison to previous solutions .",
    "we present complexity analysis of both algorithms , including transmission , encoding , and decoding complexity .",
    "we evaluate and illustrate the performance of both algorithms by extensive simulation .",
    "this paper is organized as follows .",
    "we start with a short survey of the related work in section  [ sec : relatedwork ] . in section",
    "[ sec : model_ltcodes ] , we introduce the network model and present luby transform ( lt ) codes . in section  [ sec : ltcdsalgs ] , we propose two lt codes based distributed storage algorithms called ltcds - i and ltcds - ii",
    ". we then present simulation studies and provide performance analysis of the proposed algorithms in section  [ sec : simulation ] , and concluded in section  [ sec : conclusion ] .",
    "the most related work to one presented here is  @xcite .",
    "lin  _ el al .",
    "_ studied the question `` how to retrieve historical data that the sensors have gathered even if some sensors are destroyed or disappeared from the network ? ''",
    "they analyzed techniques to increase _ persistence _ of sensed data in a random wireless sensor network , and proposed two decentralized algorithms using fountain codes to guarantee the persistence and reliability of cached data on unreliable sensors .",
    "they used random walks to disseminate data from multiple sensors ( sources ) to the whole network .",
    "based on the knowledge of the total number of sensors @xmath0 and sources @xmath1 , each source calculates the number of random walks it needs to initiate , and each sensor calculates the number of source packets it needs to trap . in order to achieve some desired packet distribution , the transition probabilities of random walks",
    "are specified by the well known metropolis algorithm  @xcite .",
    "dimakis  _ el al . _ in  @xcite proposed a decentralized implementation of fountain codes that uses geographic routing , where every node has to know its location .",
    "the motivation for using fountain codes is their low decoding complexity . also",
    ", one does not know in advance the degrees of the output nodes in this type of codes .",
    "the authors proposed a randomized algorithm that constructs fountain codes over a grid network using only geographical knowledge of nodes and local randomized decisions .",
    "fast random walks are used to disseminate source data to the storage nodes in the network .",
    "kamara _ el al .",
    "_ in  @xcite proposed a novel technique called _ growth codes _ to increase data _ persistence _ in wireless sensor networks , namely , increase the amount of information that can be recovered at the sink .",
    "growth coding is a linear technique in which information is encoded in an online distributed way with increasing degree of a storage node .",
    "kamara  _ el al .",
    "_ showed that _ growth codes _ can increase the amount of information that can be recovered at any storage node at any time period whenever there is a failure in some other nodes .",
    "they did not use robust or soliton distributions , but proposed a new distribution depending on the network condition to determine degrees of the storage nodes .",
    "the motivation for their work was that    positions and topology of the nodes are not known .",
    "they assume a round time of node updates , meaning with increasing the time @xmath7 , degree of a symbol is increased .",
    "this is the idea behind growth degrees .",
    "they provide practical implementations of growth codes and compare its performance with other codes .",
    "the decoding part is done by querying an arbitrary sink , if the original sensed data has been collected correctly then finish , otherwise query another sink node .",
    "lun  _ el . al .",
    "_ in  @xcite proposed two decentralized algorithms to compute the minimum - cost subgraphs for establishing multicast connections using network coding .",
    "also , they extended their work to the problem of minimum - energy multicast in wireless networks as well as they studied directed point - to - point multicast and evaluated the case of elastic rate demand .",
    "in this section , we introduce our network model and provide background of fountain codes and , in particular , one important class of fountain codes  lt ( luby transform ) codes  @xcite .",
    "our wireless sensor network consists of @xmath0 nodes that are uniformly distributed at random in a region @xmath8 ^ 2 $ ] for @xmath9 .",
    "the _ density _ of the network is given by @xmath10 where @xmath11 is the two - dimensional lebesgue measure ( or area ) of @xmath12 .",
    "each sensor node has an identical communication radius @xmath13 ; thus any two nodes can communicate with each other if and only if their distance is less than or equal to 1 . this model is known as random geometric graphs  @xcite . among these @xmath0 nodes ,",
    "there are @xmath1 source nodes that have information to be disseminated throughout the network for storage .",
    "these @xmath1 nodes are uniformly and independently distributed at random among the @xmath0 nodes .",
    "usually , the fraction of source nodes , i.e. , @xmath14 , is not very large ( e.g. , @xmath15 , or @xmath16 ) .",
    "note that , although we assume the nodes are uniformly distributed at random in a region , our algorithms and results do not rely on this assumption .",
    "in fact , they can be applied for any network topology , for example , regular grids .",
    "we assume that no node has knowledge about the locations of other nodes and no routing table is maintained ; consequently , the algorithm proposed in  @xcite can not be applied .",
    "moreover , we assume that each node has limited or no knowledge of global information , but know its neighbors .",
    "the limited global information refers to the total numbers of nodes @xmath0 and sources @xmath1 .",
    "any further global information , for example the maximal number of neighbors in the network , is not available .",
    "hence , the algorithms proposed in  @xcite are not applicable .",
    "( node degree ) consider a graph @xmath17 , where @xmath18 and @xmath19 denote the set of nodes and links , respectively . given @xmath20 , we say @xmath21 and @xmath22 are _ adjacent _ ( or @xmath21 is adjacent to @xmath22 , and vice versa ) if there exists a link between @xmath21 and @xmath22 , i.e. , @xmath23 . in this case , we also say that @xmath21 and @xmath22 are _",
    "neighbors_. denote by @xmath24 the set of neighbors of a node @xmath21 .",
    "the number of neighbors of a node @xmath21 is called the _ node degree _ of @xmath21 , and denoted by @xmath25 , i.e. , @xmath26 .",
    "the _ mean degree _ of a graph @xmath27 is then given by @xmath28 where @xmath29 is the total number of nodes in @xmath27 .",
    "[ fig : fountaincodes ]   source blocks chosen uniformly and independently at random from @xmath1 source inputs , where @xmath30 is drawn according to a probability distribution @xmath31.,title=\"fig : \" ]    for @xmath1 source blocks @xmath32 and a probability distribution @xmath31 with @xmath33 , a fountain code with parameters @xmath34 is a potentially limitless stream of output blocks @xmath35 .",
    "each output block is obtained by xoring @xmath30 randomly and independently chosen source blocks , where @xmath30 is drawn from a specially designed distribution @xmath31 .",
    "this is illustrated in figure  [ fig : fountaincodes ] .",
    "fountain codes are rateless , and one of their main advantage is that the encoding operations can be performed online .",
    "the encoding cost is the expected number of operation sufficient for generating an output symbol , and the decoding cost is the expected number of operations sufficient to recover the @xmath1 input blocks .",
    "another advantage of fountain codes , as opposed to purely random codes is that their decoding complexity can be made low by appropriate choice of @xmath31 , with little sacrifice in performance .",
    "the decoding of fountain codes can be done by message passing .",
    "( code degree ) for fountain codes , the number of source blocks used to generate an encoded output @xmath36 is called the code degree of @xmath36 , and denoted by @xmath37 . by constraction",
    ", the code degree distribution @xmath31 is the probability distribution of @xmath37 .",
    "lt ( luby transform ) codes are a special class of fountain codes which uses _ ideal soliton _ or _ robust soliton _ distributions  @xcite .",
    "the ideal soliton distribution @xmath38 for @xmath1 source blocks is given by @xmath39 let @xmath40 , where @xmath41 is a suitable constant and @xmath42 .",
    "the robust soliton distribution for @xmath1 source blocks is defined as follows .",
    "define @xmath43 and let @xmath44 the robust soliton distribution is given by @xmath45    the following result provides the performance of the lt codes with robust soliton distribution  ( * ? ? ?",
    "* theorems 12 and 13 ) .",
    "[ lemma : decoding - lt - codes ] for lt codes with robust soliton distribution , @xmath1 original source blocks can be recovered from any @xmath46 encoded output blocks with probability @xmath47 .",
    "both encoding and decoding complexity is @xmath48 .",
    "in this section , we present two lt - codes based distributed storage ( ltcds ) algorithms . in both algorithms ,",
    "the source packets are disseminated throughout the network by a simple random walk . in the first one , called ltcds - i algorithm",
    ", we assume that each node in the network has limited the global information , that is , knows the total number of sources @xmath1 and the total number of nodes @xmath0 .",
    "unlike the scheme proposed in in  @xcite , our algorithm does not require the nodes to know the maximum degree of the graph , which is much harder to obtain than @xmath1 and @xmath0 .",
    "the second algorithm , called ltcds - ii , is a fully distributed algorithm which does not require nodes to know any global information .",
    "the price we pay for this benefit is extra transmissions of the source packets to obtain estimates for @xmath0 and @xmath1 .      in ltcds - i",
    ", we assume that each node in the network knows the values of @xmath1 and @xmath0 .",
    "we use simple random walks  @xcite for each source to disseminate its information to the whole network . at each round , each node @xmath21 that has packets to transmit chooses one node @xmath22 among its neighbors uniformly independently at random , and sends the packet to the node @xmath22 . in order to avoid local - cluster effect  each source packet is trapped most likely by its neighbor nodes  we let each node accept a source packet equiprobably . to achieve this , we also need each source packet to visit each node in the network at least once .    for a random walk on a graph ,",
    "the  _ cover time _ is defined as follows  @xcite :    ( cover time ) given a graph @xmath27 , let @xmath49 be the expected length of a random walk that starts at node @xmath21 and visits every node in @xmath27 at least once .",
    "the _ cover time _ of @xmath27 is defined by @xmath50    for a simple random walk on a random geometric graph , the following result bounds the cover time  @xcite .",
    "[ lemma : cover - time ] if a random geometric graph with @xmath0 nodes is a connected graph with high probability , then @xmath51    as a result of lemma [ lemma : cover - time ] , we can set a counter for each source packet and increase the counter by one after each forward transmission until the counter reaches some threshold @xmath52 to guarantee that the source packet visits each node in the network at least once .",
    "the detailed descriptions of the initialization , encoding and storage phases ( steps ) of ltcds - i algorithm are given below :    * initialization phase : *    each node @xmath21 in the network draws a random number @xmath53 according to the distribution @xmath38 given by   ( or @xmath54 given by  ) .",
    "each source node @xmath55 generates a header for its source packet @xmath56 and puts its i d and a counter @xmath57 with initial value zero into the packet header .",
    "we set up tokens for initial and update packets .",
    "we assume that a token is set to zero for an initial packet and @xmath13 for an update packet .",
    "@xmath58    each source node @xmath59 sends out its own source packet @xmath56 to another node @xmath21 which is chosen uniformly at random among all its neighbors @xmath60 .",
    "the chosen node @xmath21 accepts this source @xmath61 with probability @xmath62 and updates its storage as @xmath63 where @xmath64 and @xmath65 denote the packet that the node @xmath21 stores before and after the updating , respectively , and @xmath66 represents xor operation .",
    "no matter whether the source packet is accepted or not , the node @xmath21 puts it into its forward queue and set the counter of @xmath56 as @xmath67    * encoding phase : *    in each round , when a node @xmath21 receives at least one source packet before the current round , @xmath21 forwards the head - of - line ( hol ) packet @xmath68 in its forward queue to one of its neighbor @xmath22 , chosen uniformly at random among all its neighbors @xmath24 .",
    "depending on how many times @xmath68 has visited @xmath22 , the node @xmath22 makes its decisions :    if it is the first time that @xmath68 visits @xmath22 , then the node @xmath22 accepts this source packet with probability @xmath69 and updates its storage as @xmath70    if @xmath68 has visited @xmath22 before and @xmath71 where @xmath72 is a system parameter , then the node @xmath22 accepts this source packet with probability 0 .",
    "no matter @xmath68 is accepted or not , the node @xmath22 puts it into its forward queue and increases the counter of @xmath68 by one : @xmath73    if @xmath68 has visited @xmath22 before and @xmath74 then the node @xmath22 discards the packet @xmath68 forever .",
    "* storage phase : *    when a node @xmath21 makes its decisions for all the source packets @xmath75 , i.e. , all these packets have visited the node @xmath21 at least once , the node @xmath21 finishes its encoding process by declaring the current @xmath76 to be its storage packet .",
    "the pseudo - code of these steps is given in ltcds - i algorithm  [ alg : ltcds - i ] .",
    "the following theorem establishes the code degree distribution of each storage node induced by the ltcds - i algorithm .",
    "generate @xmath53 according to @xmath38 ( or @xmath54 ) generate header of @xmath56 and @xmath77 @xmath78 choose @xmath79 uniformly at random , send @xmath56 to @xmath21 coin = rand(1 ) @xmath76 = @xmath80 put @xmath56 into @xmath21 s forward queue @xmath81 choose @xmath82 uniformly at random send hol packet @xmath56 in @xmath21 s forward queue to @xmath22 coin = rand(1 ) @xmath83 = @xmath84 put @xmath56 into @xmath22 s forward queue @xmath81 put @xmath56 into @xmath22 s forward queue @xmath81",
    "+    [ theorem : ltcds - i ] when a sensor network with @xmath0 nodes and @xmath1 sources finishes the storage phase of the ltcds - i algorithm , the code degree distribution of each storage node @xmath21 is given by @xmath85 where @xmath53 is given in the initialization phase of the ltcds - i algorithm from distribution @xmath86 ( i.e. , @xmath38 or @xmath54 ) , and @xmath87 is the code degree of the node @xmath21 resulting from the algorithm .    for each node @xmath21 , @xmath53 is drawn from a distribution @xmath86 ( i.e. , @xmath38 or @xmath54 ) .",
    "given @xmath53 , the node @xmath21 accepts each source packet with probability @xmath62 independently of each other and @xmath53 .",
    "thus , the number of source packets that the node @xmath21 accepts follows a binomial distribution with parameter @xmath62 .",
    "hence , @xmath88 and thereafter holds .",
    "theorem  [ theorem : ltcds - i ] indicates that the code degree @xmath87 is not the same as @xmath53 .",
    "in fact , one may achieve the exact desired code degree distribution by letting all the sensors hold the received source packets in their temporary buffer until they collect all @xmath1 source packets .",
    "then they can randomly choose @xmath53 packets .",
    "in this way , the resulting degree distribution is exactly the same as @xmath89 or @xmath90 .",
    "however , this requires that each sensor has enough buffer or memory , which is usually not practical , especially when @xmath1 is large .",
    "therefore , in ltcds - i , we assume each sensor has very limited memory and let them make their decision upon each reception .",
    "fortunately , from figure  [ fig : rsdistributino ] , we can see that at the high degree end , the resulting code degree distribution obtained by the ltcds - i algorithm perfectly matches the desired code degree distribution , i.e. , either the ideal soliton distribution @xmath89 or the robust soliton distribution @xmath90 . for the resulting degree distribution and the desired degree distributions",
    ", the difference only lies at the low degree end , especially at degree 1 and degree 2 .",
    "in particular , the resulting degree distribution has higher probability at degree 1 and lower probability at degree 2 than the desired degree distributions .",
    "the fact that higher probability at degree 1 turns out to compensate the lower probability at degree 2 so that the resulting degree distribution has very similar encoding and decoding behavior as lt codes using either the ideal soliton distribution or the robust soliton distribution . in our future study , we will provide theoretical analysis and prove that the degree distribution in  [ eq : ltcds - i - code - degree ] is equivalent , but not the same , as the degree distributed used in lt encoding  @xcite .",
    "therefore , we have the following theorem , which can be proved by the same method for lemma  [ lemma : decoding - lt - codes ] , see  @xcite .    [",
    "theorem : decoding - ltcds - i ] suppose sensor networks have @xmath0 nodes and @xmath1 sources and the ltcds - i algorithm uses the robust soliton distribution @xmath90 .",
    "then , when @xmath0 and @xmath1 are sufficient large , the @xmath1 original source packets can be recovered from any @xmath46 storage nodes with probability @xmath47 .",
    "the decoding complexity is @xmath48 .",
    "theorem  [ theorem : decoding - ltcds - i ] asserts that when @xmath0 and @xmath1 are sufficiently large , the performance of the ltcds - i is similar to lt coding .",
    "another main performance metric is the transmission cost of the algorithm , which is characterized by the total number of transmissions ( the total number of steps of @xmath1 random walks ) .",
    "[ theorem : transmission - ltcds - i ] denote by @xmath91 the total number of transmissions of the ltcds - i algorithm , then we have @xmath92 where @xmath1 is the total number of sources , and @xmath0 is the total number of nodes in the network .    we know that each one of @xmath1 source packets is stooped and discarded if and only if it has been forwarded for @xmath93 times , for some constant @xmath72",
    "then the total number of transmissions of the ltcds - i algorithm for all @xmath1 packets is a direct consequence and it is given by  .",
    "in many scenarios , especially when a change in network topology occurs because of , for example , node mobility or node failures , the exact values of @xmath0 and @xmath1 may not be available to all nodes .",
    "therefore , to design a fully distributed storage algorithm which does not require any global information is very important and useful . in this subsection",
    ", we present such an algorithm based on lt codes , called ltcds - ii .",
    "the idea behind this algorithm is to utilize some features of simple random walks to do inference to obtain individual estimates of @xmath0 and @xmath1 for each node .",
    "we introduce of _ inter - visit time _ and _ inter - packet time _  @xcite as follows :    ( inter - visit time ) for a random walk on a graph , the _ inter - visit time _ of node @xmath21 , @xmath94 , is the amount of time between any two consecutive visits of the random walk to node @xmath21 .",
    "this inter - visit time is also called _ return time_.    for a simple random walk on random geometric graphs , the following lemma provides results on the expected inter - visit time of any node .",
    "the proof is straightforward by following the standard result of stationary distribution of a simple random walk on graphs and the mean return time for a markov chain  @xcite . for completeness",
    ", we provide the proof in appendix  6.1 .",
    "[ lemma : inter - visit - time ] for a node @xmath21 with node degree @xmath25 in a random geometric graph , the mean inter - visit time is given by @xmath95=\\frac{\\mu n}{d_n(u)},\\ ] ] where @xmath96 is the mean degree of the graph given by equation  .    from lemma  [ lemma : inter - visit - time ]",
    ", we can see that if each node @xmath21 can measure the expected inter - visit time @xmath97 $ ] , then the total number of nodes @xmath0 can be estimated by @xmath98}{\\mu}.\\ ] ] however , the mean degree @xmath96 is a global information and may be hard to obtain .",
    "thus , we make a further approximation and let the estimate of @xmath0 by the node @xmath21 be @xmath99.\\ ] ] hence , every node @xmath21 computes its own estimate of @xmath0 . in our distributed storage algorithms ,",
    "each source packet follows a simple random walk .",
    "since there are @xmath1 sources , we have @xmath1 individual simple random walks in the network . for a particular random walk ,",
    "the behavior of the return time is characterized by lemma  [ lemma : inter - visit - time ] . on the other hand , lemma  [ lemma : inter - packet - time ] below provides results on the inter - visit time among all @xmath1 random walks , which is called inter - packet time for our algorithm , defined as follows :    ( inter - packet time ) for @xmath1 random walks on a graph , the _ inter - packet time _ of node @xmath21 , @xmath100 , is the amount of time between any two consecutive visits of those @xmath1 random walks to node @xmath21 .    for the mean value of inter - packet time , we have the following lemma , for which the proof is given in appendix  6.2 .",
    "[ lemma : inter - packet - time ] for a node @xmath21 with node degree @xmath25 in a random geometric graph with @xmath1 simple random walks , the mean inter - packet time is given by @xmath101=\\frac{e[t_{visit}(u)]}{k}=\\frac{\\mu n}{kd_n(u)},\\ ] ] where @xmath96 is the mean degree of the graph given by  .    from lemma  [ lemma : inter - visit - time ] and lemma  [ lemma : inter - packet - time ] ,",
    "it is easy to see that for any node @xmath21 , an estimation of @xmath1 can be obtained by @xmath102}{e[t_{packet}(u)]}.\\ ] ]    after obtaining estimates for both @xmath0 and @xmath1 , we can employ similar techniques used in ltcds - i to do lt coding and storage .",
    "the detailed descriptions of the initialization , inference , encoding , and storage phases of ltcds - ii algorithm are given below :    * initialization phase : *    each source node @xmath103 generates a header for its source packet @xmath56 and puts its i d and a counter @xmath57 with initial value zero into the packet header .",
    "each source node @xmath59 sends out its own source packet @xmath56 to one of its neighbors @xmath21 , chosen uniformly at random among all its neighbors @xmath60 .",
    "the node @xmath21 puts @xmath56 into its forward queue and sets the counter of @xmath56 as @xmath104    * inference phase : *    for each node @xmath21 , suppose @xmath105 is the first source packet that visits @xmath21 , and denote by @xmath106 the time when @xmath105 has its @xmath107-th visit to the node @xmath21 .",
    "meanwhile , each node @xmath21 also maintains a record of visiting time for each other source packet @xmath108 that visited it .",
    "let @xmath109 be the time when source packet @xmath108 has its @xmath107-th visit to the node @xmath21 .",
    "after @xmath105 visiting the node @xmath21 @xmath110 times , where @xmath110 is system parameter which is a positive constant , the node @xmath21 stops this monitoring and recoding procedure .",
    "denote by @xmath111 the number of source packets that have visited at least once upon that time .    for each node @xmath21 ,",
    "let @xmath112 be the number of visits of source packet @xmath108 to the node @xmath21 and let @xmath113 then , the average inter - visit time for node @xmath21 is given by @xmath114    let @xmath115 and @xmath116 , then the inter - packet time is given by @xmath117    then the node @xmath21 can estimate the total number of nodes in the network and the total number of sources as @xmath118 and @xmath119    in this phase , the counter @xmath57 of each source packet @xmath57 is incremented by one after each transmission .",
    "* encoding phase : *    when a node @xmath21 obtains estimates @xmath120 and @xmath121 , it begins encoding phase which is the same as the one in ltcds - i algorithm except that the code degree @xmath53 is drawn from distribution @xmath38 ( or @xmath54 ) with replacement of @xmath1 by @xmath121 , and a source packet @xmath56 is discarded if @xmath122 , where @xmath123 is a system parameter which is a positive constant .",
    "* storage phase : *    when a node @xmath21 has made its decisions for @xmath124 source packets , it finishes its encoding process and @xmath76 becomes the storage packet of @xmath21 .",
    "the total number of transmissions ( the total number of steps of @xmath1 random walks ) in the ltcds - ii algorithm has the same order as ltcds - i .",
    "[ theorem : transmission - ltcds - ii ] denote by @xmath125 the total number of transmissions of the ltcds - ii algorithm , then we have @xmath126 where @xmath1 is the total number of sources , and @xmath0 is the total number of nodes in the network .",
    "in the interference phase of the ltcds - ii algorithm , the total number of transmissions is upper bounded @xmath127 for some constants @xmath128 .",
    "that is because each node needs to receive the first visit source packet for @xmath110 times , and by lemma  [ lemma : inter - visit - time ] , the mean inter - visit time is @xmath129 .    in the decoding phase , the same as in the ltcds - i algorithm , in order to guarantee that each source packet visits all the nodes at least once , the number of steps of the simple random walk is @xmath130 . in other words ,",
    "each source packet is stopped and discarded if and only if the counter reaches the threshold @xmath131 for some system parameter @xmath123 .",
    "therefore , we have  .",
    "now , we turn our attention to data updating after all storage nodes saved their values @xmath132 , but a sensor node , say @xmath59 , wants to update its value to the appropriate set of storage nodes in the network",
    ". the following updating algorithm applies for both ltcds - i and ltcds - ii . for simplicity",
    ", we illustrate the idea with ltcds - i .",
    "assume the sensor node prepared a packet with its i d , old data @xmath56 , new data @xmath133 along with a time - to - live parameter @xmath134 initialized to zero .",
    "we will use also a simple random walk for data update .",
    "@xmath135    if we assume that the storage nodes keep i d s of the accepted packets , then the problem becomes simple .",
    "we just run a random walk and check for the coming packet s @xmath136 . assume the node @xmath21 keeps track of all @xmath136 s of its accepted packets .",
    "then @xmath21 accepts the updated message if @xmath136 of the coming packet is already included in the @xmath21 s @xmath136 list .",
    "otherwise @xmath21 forwards the packet incrementing the time - to - live counter .",
    "if this counter reaches the threshold value , then the packet will be discarded .",
    "the following steps describe the update scenario :    * preparation phase : *    the node @xmath59 prepares its new packet with the new and old data along with its i d and counter . also , @xmath59 add an update counter @xmath137 initialized at @xmath13 for the first updated packet .",
    "so , we assume that the following steps happen when @xmath137 is set to @xmath13 .",
    "@xmath135 @xmath59 chooses at random a neighbor node @xmath21 , and sends its @xmath61 .    *",
    "encoding phase : *    the node @xmath21 checks if the @xmath61 is an update or first - time packet .",
    "if it is first - time packet it will accept , forward , or discard it as shown in ltcds - i algorithm  [ alg : ltcds - i ] . if @xmath61 is an updated packet , then the node @xmath21 will check if @xmath138 is already included in its accepted list .",
    "if yes , then it will update its value @xmath76 as follows .",
    "@xmath139    if no , it will add this updated packet into its forward queue with incrementing the counter @xmath140    the @xmath61 will be discarded if @xmath141 where @xmath72 is a system parameter . in this case , we need @xmath72 to be large enough , so all old data @xmath56 will be updated to the new data @xmath142 .",
    "* storage phase : *    if all nodes are done with updating their values @xmath143 .",
    "one can run the decoding phase to retrieve the original and update information .",
    "now , since we run only one simple random walk for each update , if @xmath144 is the number of nodes updating their values , then we have the following result .",
    "the total number of transmissions needed for the update process is bounded by @xmath145 .",
    "in this section , we study performance of the proposed ltcds - i and ltcds - ii algorithms for distributed storage in wireless sensor networks through simulation .",
    "the main performance metric we investigate is the _ successful decoding probability _ versus the _ decoding ratio_.    ( decoding ratio ) _ decoding ratio _",
    "@xmath146 is the ratio between the number of queried nodes @xmath144 and the number of sources @xmath1 , i.e. , @xmath147    ( successful decoding probability ) _ successful decoding probability _",
    "@xmath148 is the probability that the @xmath1 source packets are all recovered from the @xmath144 querying nodes .    in our simulation ,",
    "@xmath148 is evaluated as follows .",
    "suppose the network has @xmath0 nodes and @xmath1 sources , and we query @xmath144 nodes .",
    "there are @xmath149 ways to choose such @xmath144 nodes , and we pick one tenth of these choices uniformly at random : @xmath150 let @xmath151 be the size of the subset these @xmath152 choices of @xmath144 query nodes from which the @xmath1 source packets can be recovered .",
    "then , we evaluate the successful decoding probability as @xmath153                ]    figure  [ fig : ltcds - i - ps-1 ] shows the decoding performance of ltcds - i algorithm with ideal soliton distribution with small number of nodes and sources .",
    "the network is deployed in @xmath154 ^ 2 $ ] , and the system parameter @xmath72 is set as @xmath155 . from the simulation results we can see that when the decoding ratio is above 2 , the successful decoding probability is about @xmath156 .",
    "another observation is that when the total number of nodes increases but the ratio between @xmath1 and @xmath0 and the decoding ratio @xmath146 are kept as constants , the successful decoding probability @xmath148 increases when @xmath157 and decreases when @xmath158 .",
    "this is also confirmed by the results shown in figure  [ fig : ltcds - i - ps-2 ] . in figure",
    "[ fig : ltcds - i - ps-2 ] , the network has constant density as @xmath159 and the system parameter @xmath160 .    in figure",
    "[ fig : ltcds - i - ps-4 ] , we fix the decoding ratio @xmath146 as 1.4 and 1.7 , respectively , and fix the ratio between the number of sources and the number of nodes as @xmath15 , i.e. , @xmath161 , and change the number of nodes @xmath0 from 500 to 5000 . from the results , it can be seen that as @xmath0 grows , the successful decoding probability increases until it reaches some platform which is the successful decoding probability of real lt codes .",
    "this confirms that ltcds - i algorithm has the same asymptotical performance as lt codes .    to investigate how the system parameter @xmath72 affects the decoding performance of the ltcds - i algorithm",
    ", we fix the decoding ratio @xmath146 and change @xmath72 .",
    "the simulation results are shown in figure  [ fig : ltcds - i - ps-3 ] .",
    "for the scenario of 1000 nodes and 100 sources , @xmath146 is set as 1.6 , and for the scenario of 500 nodes and 50 sources , @xmath146 is set as 1.8 .",
    "the code degree distribution is also the ideal soliton distribution , and the network is deployed in @xmath162 ^ 2 $ ] .",
    "it can be seen that when @xmath163 , @xmath148 keeps almost like a constant , which indicates that after @xmath164 steps , almost all source packets visit each node at least once .",
    "figure  [ fig : ltcds - ii - ps-1 ] compares the decoding performance of ltcds - ii and ltcds - i with ideal soliton distribution with small number of nodes and sources . as in figure  3",
    ", the network is deployed in @xmath154 ^ 2 $ ] , and the system parameter is set as @xmath165 . to guarantee each node",
    "obtain accurate estimations of @xmath0 and @xmath1 , we set @xmath166 .",
    "it can be seen that the decoding performance of the ltcds - ii algorithm is a little bit worse than the ltcds - i algorithm when decoding ratio @xmath146 is small , and almost the same when @xmath146 is large .",
    "figure  8 compares the decoding performance of ltcds - ii and ltcds - i with ideal soliton distribution with medium number of nodes and sources , where the network has constant density as @xmath159 and the system parameter @xmath167 .",
    "we observe different phenomena .",
    "the decoding performance of the ltcds - ii algorithm is a little bit better than the ltcds - i algorithm when decoding ratio @xmath146 is small , and almost the same when @xmath146 is large .",
    "that is because for the simulation in figure  [ fig : ltcds - ii - ps-2 ] , we set @xmath167 which is larger than @xmath165 set for the simulation in figure  6 .",
    "the larger value of @xmath123 guarantees that each node has the chance to accept each source packet , which results in a more uniformly distribution .    figure  [ fig : ltcds - ii - est-2]figure  [ fig : ltcds - ii - est-3 ] shows the histogram of the estimation results of @xmath0 and @xmath1 of each node for three scenarios : figure  [ fig : ltcds - ii - est-2 ] shows the results for 200 nodes and 20 sources ; and figure  10 shows the results for 1000 nodes and 100 sources . in the first two scenarios , we set @xmath166 . from the results we can see that , the estimations of @xmath1 are more accurate and concentrated than the estimations of @xmath0 .",
    "this is because the estimation of @xmath1 only depends on the ratio between the expected inter - visit time and the expected inter - packet time , which is independent of the mean degree @xmath96 and the node degree @xmath25 . on the other hand ,",
    "the estimation of @xmath0 is actually depends on @xmath96 and @xmath25 .",
    "however , in the ltcds - ii algorithm , each node approximates @xmath96 as its own node degree @xmath25 , which causes the deviation of the estimations of @xmath0 .    to investigate how the system parameter @xmath110 affects the decoding performance of the ltcds - ii algorithm",
    ", we fix the decoding ratio @xmath146 and @xmath123 , and change @xmath110 .",
    "the simulation results are shown in figure  [ fig : ltcds - ii - ps-3 ] . from the simulation results",
    ", we can see that when @xmath110 is chosen to be small , the performance of the ltcds - ii algorithm is very poor .",
    "this is due to the inaccurate estimations of @xmath1 and @xmath0 of each node .",
    "when @xmath110 is large , for example , when @xmath168 , the performance is almost the same .    ,",
    "in this paper , we studied a model for large - scale wireless sensor networks , where the network nodes have low cpu power and limited storage .",
    "we proposed two new decentralized algorithms that utilize fountain codes and random walks to distribute information sensed by @xmath1 sensing source nodes to @xmath0 storage nodes .",
    "these algorithms are simpler , more robust , and less constrained in comparison to previous solutions that require knowledge of network topology , maximum degree of a node , or knowing values of @xmath0 and @xmath1  @xcite .",
    "we computed the computational encoding and decoding complexity of these algorithms and simulated their performance with small and large numbers of @xmath1 and @xmath0 nodes .",
    "we showed that a node can successfully estimate the number of sources and total number of nodes if it can only compute the _",
    "inter - visit time _ and _ inter - packet time_.    our future work will include raptor codes based distributed networked storage algorithms for sensor networks .",
    "we also plan to provide theoretical results and proofs for the results shown in this paper , where the limited space is not an issue .",
    "our algorithm for estimating values of @xmath0 and @xmath1 is promising , we plan to investigate other network models where this algorithm is beneficial and can be utilized .",
    "the authors would like to thank the reviewers for their comments .",
    "they would like to express their gratitude to all bell labs & alcatel - lucent staff members for their hospitality and kindness .",
    "on the other hand , for a reversible markov chain , the expected return time for a state @xmath170 is given by  @xcite @xmath171=\\frac{1}{\\pi(i)},\\ ] ] where @xmath172 is the stationary distribution of state @xmath170 .    from   and  , we have for a simple random on a graph , the expected inter - visit time of node @xmath21 is @xmath173=\\frac{2|e|}{d_n(u)}=\\frac{\\mu n}{d_n(u)},\\ ] ] where @xmath96 is the mean degree of the graph .      for a given node @xmath21 and @xmath1 simple random walks ,",
    "each simple random walk has expected inter - visit time @xmath174 .",
    "we now view this process from another perspective : we assume there are @xmath1 nodes @xmath175 uniformly distributed in the network and an agent from node @xmath21 follows a simple random walk .",
    "then the expected inter - visit time for this agent to visit any particular @xmath176 is the same as @xmath174 .",
    "however , the expected inter - visit time for any two nodes @xmath176 and @xmath177 is @xmath178 which gives the expected inter - packet time ."
  ],
  "abstract_text": [
    "<S> we consider large - scale networks with @xmath0 nodes , out of which @xmath1 are in possession , ( _ e.g. , _ have sensed or collected in some other way ) @xmath1 information packets . in the scenarios in which network nodes are vulnerable because of , for example , limited energy or a hostile environment </S>",
    "<S> , it is desirable to disseminate the acquired information throughout the network so that each of the @xmath0 nodes stores one ( possibly coded ) packet and the original @xmath1 source packets can be recovered later in a computationally simple way from any @xmath2 nodes for some small @xmath3 .    </S>",
    "<S> we developed two distributed algorithms for solving this problem based on simple random walks and fountain codes . </S>",
    "<S> unlike all previously developed schemes , our solution is truly distributed , that is , nodes do not know @xmath0 , @xmath1 or connectivity in the network , except in their own neighborhoods , and they do not maintain any routing tables . in the first algorithm , </S>",
    "<S> all the sensors have the knowledge of @xmath0 and @xmath1 . in the second algorithm , each sensor estimates these parameters through the random walk dissemination . </S>",
    "<S> we present analysis of the communication / transmission and encoding / decoding complexity of these two algorithms , and provide extensive simulation results as well . </S>"
  ]
}