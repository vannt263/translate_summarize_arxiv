{
  "article_text": [
    "we begin with the standard normal linear regression model setup @xmath3 where @xmath4 is an @xmath5 vector of observations , @xmath6 is an @xmath7 matrix of @xmath8 potential predictors where @xmath9 and rank @xmath10 , and @xmath11 is a @xmath12 vector of unknown regression coefficients , and @xmath13 is unknown variance . based on observing @xmath4 , we consider the problem of giving the predictive density @xmath14 of a future @xmath15 vector @xmath16 where @xmath17 here @xmath18 is a fixed @xmath19 design matrix of the same @xmath8 potential predictors in @xmath6 , and the rank of @xmath20 is assumed to be @xmath21 .",
    "we also assume that @xmath4 and @xmath16 are conditionally independent given @xmath11 and @xmath13 .",
    "note that in most earlier papers on such prediction problems , @xmath13 is assumed known , partly because this typically makes the problem less difficult .",
    "however , the assumption of unknown variance is more realistic , and we treat this more difficult case in this paper . in the following",
    "we denote by @xmath22 all the unknown parameters @xmath23 .    for each value of @xmath4 , a predictive estimate @xmath24 of @xmath25 is often evaluated by the kullback - leibler ( kl ) divergence @xmath26 which is called the kl divergence loss from @xmath25 to @xmath24 .",
    "the overall quality of the procedure @xmath24 for each @xmath22 is then conveniently summarized by the kl risk @xmath27 where @xmath28 is the density of @xmath4 in .",
    "@xcite showed that the bayesian solution with respect to the prior @xmath29 under the loss @xmath30 given by is what is called the bayesian predictive density @xmath31 where @xmath32 for the prediction problems in general , many studies suggest the use of the bayesian predictive density rather than plug - in densities of the form @xmath33 , where @xmath34 is an estimated value of @xmath22 . in our setup of the problem",
    ", @xcite showed that the bayesian predictive density with respect to the right invariant prior is the best equivariant and minimax .",
    "although the bayesian predictive density with respect to the right invariant prior is a good default procedure , it has been shown to be inadmissible in some cases .",
    "specifically , when @xmath13 is assumed to be known and the following are assumed @xmath35 where @xmath36 is an positive integer , @xmath37 is an @xmath38 vector each component of which is one , and @xmath39 is the kronecker product , @xcite showed that the shrinkage bayesian predictive density with respect to the harmonic prior @xmath40 dominates the best invariant bayesian predictive density with respect to @xmath41 @xcite extended @xcite s result to general shrinkage priors including @xcite s prior . as pointed out in the above , we will assume that the variance @xmath13 is unknown in this paper .",
    "the first decision - theoretic result in the unknown variance case was derived by @xcite .",
    "he showed that , under the same assumption of @xcite given by , the bayesian predictive density with respect to the shrinkage prior @xmath42 dominates the best invariant predictive density which is the bayesian predictive density with respect to the right invariant prior @xmath43    from a more general viewpoint , the kl - loss given by is in the class of @xmath0-divergence introduced by @xcite and defined by @xmath44 where @xmath45 clearly the kl - loss given by corresponds to @xmath46 .",
    "@xcite showed that a generalized bayesian predictive density under @xmath47 is @xmath48^{2/(1-\\alpha ) } & \\alpha \\neq 1 \\\\",
    "\\exp\\ { \\int \\log p(\\tilde{y}|\\psi)\\pi(\\psi|y)d\\psi\\ } & \\alpha=1 .",
    "\\end{cases}\\ ] ] hence the bayesian predictive density of the form may not be good under @xmath0-divergence with @xmath49 . but as @xcite pointed out in the estimation problem , decision - theoretic properties often seem to depend on the general structure of the problem ( the general type of problem ( location , scale ) , and the dimension of the parameter space ) and on the prior in a bayesian - setup , but not the loss function .",
    "in fact , we will show , under the assumption and the @xmath50 loss , the predictive density with respect to the same shrinkage prior given by improves on the best invariant predictive density with respect to ( see section [ sec : minimax - d_1 ] ) . from this viewpoint , we are generally interested in how robust the stein effect already founded under @xmath47 loss for a specific @xmath0 is under @xmath47 loss for general @xmath0 . for example",
    ", we can find some concrete problems as follows .",
    "problem 1 : :    under the assumption and the @xmath51 loss for    @xmath52 , does the predictive density with respect to    the same shrinkage prior given by improve on the best invariant    predictive density with respect to ?",
    "problem 2 - 1 : :    under @xmath53 loss , even if @xmath54 , the best    invariant predictive density remains inadmissible because an improved    non - bayesian predictive density is easily found .",
    "( see section    [ sec : minimax - d_1 ] . )",
    "can we find improved bayesian predictive densities    for this case ( @xmath54 ) ?",
    "problem 2 - 2 : :    [ prob2.2 ] under @xmath55 and the @xmath47    loss with @xmath56 , does the best invariant    predictive density keep inadmissibility ?",
    "if so , which bayesian    predictive density improve it ?    in this paper ,",
    "a main focus is on problem 2 - 1 and 2 - 2 . for problem 2 - 1",
    ", we will give an exact solution .",
    "we could not solve problem 2 - 2 in this paper , but by a natural extension of the shrinkage prior considered for problem 2 - 1 ( @xmath57 loss ) , we will provide a class of predictive densities which we hope lead the solution in the future work .",
    "in addition , problem 1 is open .",
    "the organization of this paper is as follows .",
    "we treat not only simple design matrices like but also general ones noted at the beginning of this section . in order to make the structure of our problem clearer , section [ sec : canonical ] gives the canonical form of the problem . in section [ sec : density ] , we consider a natural extension of a hierarchical prior which was originally proposed in @xcite and @xcite for the problem of estimating @xmath11 . using it , we will construct a bayesian predictive density under @xmath47 loss for @xmath58 and @xmath2 . in section [ sec : minimax - d_1 ] , we show that a subclass of the bayesian predictive densities proposed in section [ sec : density ] is minimax under @xmath53 loss even if @xmath8 is small .",
    "section [ sec : cr ] gives concluding remarks .",
    "in the section , we reduce the problem to a canonical form . to simplify expressions and to make matters a bit clearer",
    "it is helpful to rotate the problem via the following transformation .",
    "first we note that for the observation @xmath4 , sufficient statistics are @xmath59 where @xmath60 and @xmath61 are independent .",
    "* case i : * when @xmath62 , let @xmath63 be a nonsingular @xmath64 matrix which simultaneously diagonalizes matrices @xmath65 and @xmath66 , where @xmath63 satisfies @xmath67 where @xmath68 .",
    "let @xmath69 and @xmath70 .",
    "* case ii : * when @xmath71 , there exists an @xmath72 matrix @xmath73 such that @xmath74 is a @xmath75 non - singular matrix and also @xmath76 is an @xmath77 zero matrix .",
    "further there exists an @xmath78 orthogonal matrix @xmath79 which diagonalizes @xmath80 , the covariance matrix of @xmath81 , i.e. , @xmath82 where @xmath83 .",
    "there also exists a @xmath84 matrix @xmath85 such that @xmath86 then @xmath87 and @xmath88 where @xmath89 are independent and have multivariate normal distributions @xmath90 and @xmath91 respectively .",
    "let @xmath92 and @xmath93",
    ".    in summary , a canonical form of the prediction problem is as follows .",
    "we observe @xmath94 where @xmath95 , @xmath96 , @xmath97 and @xmath98 .",
    "when @xmath62 , @xmath88 is empty .",
    "then the problem is to give a predictive density of an @xmath99-dimensional future observation @xmath100 where @xmath101 is an @xmath102 matrix , which is given by @xmath103 and hence satisfies @xmath104 .",
    "notice that , under the assumption given by , @xmath105 becomes @xmath106 , @xmath88 is empty , and @xmath101 becomes @xmath107 .",
    "the distribution of @xmath16 in is the same as in , so it is just the @xmath16 s that have been transformed . in the remainder of the paper , we will consider the problem in its canonical form , and . we will use the notation @xmath108 in the following",
    "although it may be more appropriate to use @xmath109 or @xmath110 .",
    "in this section , we consider the following class of hierarchical prior densities , @xmath111 , for the canonical model given by and .",
    "@xmath112 where @xmath113 with @xmath114 for @xmath115 , @xmath116 and @xmath117 .",
    "the integral which appears in the bayesian predictive density below will be well - defined when @xmath118 .",
    "an essentially equivalent class was considered for the problem of estimating @xmath119 and @xmath13 in @xcite respectively .",
    "when @xmath62 , the prior on @xmath120 is empty and we have only to eliminate @xmath121 from the representation of the bayesian solution in the following theorems [ thm : case-1 ] and [ thm : case-2 ] , in order to have the corresponding result .      [ thm : case-1 ] the generalized bayes predictive density under @xmath47 divergence with respect to the prior is given by @xmath123 where @xmath124 and where @xmath125    see appendix .",
    "the first term @xmath126 is the best invariant predictive density , and is bayes with respect to the right invariant prior @xmath127 . upon normalizing ,",
    "@xmath126 is multivariate-@xmath128 with the mean @xmath129 .",
    "we omit the straightforward calculation .",
    "@xcite show that @xmath126 has a constant minimax risk .",
    "the second term , @xmath130 , is a pseudo multivariate-@xmath128 density with the mean vector @xmath131 .",
    "since @xmath132 is clearly satisfied , @xmath130 induces a shrinkage effect toward the origin .",
    "the complexity in the second term is reduced considerably with the choice @xmath133 , in which case @xmath134 , @xmath135 and @xmath136 .",
    "however , since the covariance matrix of @xmath137 , @xmath138 , is diagonal but not necessarily a multiple of @xmath139 , the introduction of @xmath140 seems reasonable . indeed in the context of ridge regression ,",
    "@xcite and @xcite have argued that shrinking unstable components more than stable components is reasonable .",
    "an ascending sequence of @xmath141 s leads to this end .",
    "hence the complexity , while perhaps not pleasing , is nevertheless potentially useful .",
    "[ thm : case-2 ] the generalized bayes predictive distribution under @xmath53 divergence with respect to the prior is normal distribution @xmath143 where @xmath144 and where @xmath145 and @xmath146 .",
    "see appendix .",
    "it is quite interesting to note that the bayesian predictive density @xmath147 for @xmath148 given in section [ sec : case-1 ] converges to @xmath149 as @xmath150 where @xmath151 denotes the @xmath99-variate normal density with the mean vector @xmath152 and the covariance matrix @xmath153 .    since the bayes solution is the plug - in predictive density as shown in theorem [ thm : case-2 ] , we pay attention only to the properties of plug - in predictive density under the loss @xmath50 .",
    "the @xmath0-divergence with @xmath2 , from @xmath154 , the predictive normal density with plug - in estimators @xmath155 and @xmath156 , to @xmath157 , the true normal density , is given by @xmath158 in , @xmath159 denotes the scale invariant quadratic loss @xmath160 for @xmath119 and @xmath161 denotes the stein s or entropy loss @xmath162 for @xmath13 .",
    "hence when the prediction problem under @xmath0-divergence with @xmath163 is considered from the bayesian point of view , the bayesian solution is the normal distribution with plug - in bayes estimators and the prediction problem reduces to the simultaneous estimation problem of @xmath119 and @xmath13 under the sum of losses as in .",
    "in this section , we give analytical results on minimaxity under @xmath53 loss . as pointed out in the previous section , the prediction problem under @xmath53 loss , reduces to the simultaneous estimation problem of @xmath119 and @xmath13 under the sum of losses as in .",
    "clearly the umvu estimators of @xmath119 and @xmath13 are @xmath164 and @xmath165 .",
    "these are also generalized bayes estimators with respect to the the right invariant prior @xmath166 and are hence minimax .",
    "the constant minimax risk is given by @xmath167 where @xmath168 and @xmath169 .",
    "recall that from observation @xmath4 , there exist independent sufficient statistics given by : @xmath170 where @xmath95 , @xmath96 , @xmath97 and @xmath171 .",
    "when @xmath62 , @xmath88 is empty .    in the variance estimation problem of @xmath13 under @xmath161 , @xcite showed that @xmath172 is dominated by @xmath173 for any combination of @xmath174 including even @xmath175 .",
    "hence , in the simultaneous estimation problem of @xmath119 and @xmath13 , we easily see that @xmath176 is dominated by @xmath177 and hence have the following result .",
    "the estimator @xmath178 is inadmissible for any combination of @xmath174 .",
    "the improved solution , @xmath177 , is unfortunately not bayes .",
    "when @xmath179 and @xmath180 we can construct a bayesian solution using our earlier studies as follows . in the estimation problem of @xmath119 under @xmath159 , @xcite showed that the generalized bayes estimator of @xmath119 with respect to the harmonic - type prior @xmath181 improves on the umvu estimator @xmath182 when @xmath179 and is satisfied . in the variance estimation problem of @xmath13 under @xmath161 , although @xcite did not state so explicitly , they showed that the generalized bayes estimator of @xmath13 with respect to the same prior dominates the umvu estimator @xmath183 when @xmath179 .",
    "hence the prior gives an improved bayesian solution in the simultaneous estimation problem of @xmath119 and @xmath13 when @xmath179 and is satisfied .",
    "( note that under the special assumption introduced in section [ sec : intro ] , @xmath105 becomes the multiple of identity matrix and hence is automatically satisfied . )    however , in the above construction of the bayesian solution , two assumptions , @xmath179 and , are needed .",
    "further even if @xmath184 and @xmath88 exists , the bayes procedure does not depend on @xmath88 .",
    "this is not desirable because the statistic @xmath88 has some information about @xmath185 or @xmath13 .",
    "in fact , the stein - type estimator of variance @xmath186 as well as @xmath187 dominates @xmath183 and hence @xmath188 also dominates @xmath176 in the simultaneous estimation problem .",
    "now we show that a subclass of the generalized bayes procedure under @xmath53 given in section [ sec : case-2 ] improves on the generalized bayes procedure with respect to the right invariant prior .",
    "we assume neither @xmath179 nor . additionally the proposed procedure does depend on @xmath88 if it exists .",
    "[ thm : minimaxity ] the generalized bayes estimators of theorem [ thm : case-2 ] , @xmath189 where @xmath145 , dominate the umvu estimators ( @xmath87 and @xmath172 ) under the loss if @xmath117 and @xmath190 where @xmath191    see appendix .",
    "clearly @xmath192 and @xmath193 are always positive",
    ". now consider @xmath194 .",
    "assume @xmath194 is negative for fixed @xmath195 .",
    "but there exits @xmath196 such that @xmath197 makes @xmath194 positive .",
    "hence we can freely choose an ascending sequence of @xmath141 s which guarantees the minimaxity of @xmath198 and increased shrinkage of unstable components .",
    "we make some comments about domination results under the @xmath53 loss for the case of a known variance , say @xmath199 . by and",
    ", the prediction problem under the @xmath53 loss reduces to the problem of estimating an @xmath200-dimensional mean vector @xmath119 under the quadratic loss @xmath201 in the case where there exists a sufficient statistic @xmath202 .",
    "it is well known that the umvu estimator @xmath87 is admissible when @xmath203 and inadmissible when @xmath179 .",
    "minimax admissible estimators for @xmath179 have been proposed by many researchers including @xcite , @xcite , @xcite , and @xcite . on the other hand , for kl ( i.e.  @xmath46 ) loss , @xcite used some techniques including the heat equation and stein s identity , and eventually found a new identity which links kl risk reduction to stein s unbiased estimate of risk reduction .",
    "based on the link , they obtained sufficient conditions on the bayesian predictive density for minimaxity .",
    "hence we expect that there should exist an analogous relationship between the prediction problem under the @xmath47 loss for @xmath204 and the problem of estimating the mean vector .",
    "as far as we know , this is still an open problem .",
    "in this paper we have studied the construction and behavior of generalized bayes predictive densities for normal linear models with unknown variance under @xmath0-divergence loss .",
    "in particular we have shown that the best equivariant , ( bayes under the right invariant prior ) and minimax predictive density under @xmath53 is inadmissible in all dimensions and for all residual degrees of freedom .",
    "we have found a class of improved hierarchical generalized bayes procedures , which gives a solution to problem 2 - 1 of section [ sec : intro ] .",
    "the domination results in this paper are closely related to those in @xcite for the respective problems of estimating the mean vector under quadratic loss and the variance under stein s loss . in fact a key observation that aids in the current development is that the bayes estimator under @xmath53 loss is a plug - in estimator normal density with mean vector and variance closely related to those of the above papers , and that the @xmath53 loss is the sum of a quadratic loss in the mean and stein s loss for the variance .",
    "we expect that an extension of a hierarchical prior given in section [ sec : case-1 ] , for the prediction problem under the @xmath51 loss for @xmath56 , can form a basis to solve problem 2 - 2 of section [ sec : intro ] . to date ,",
    "unfortunately , we have been less successful in extending the domination results to the full class of @xmath0-divergence losses .",
    "the bayesian predictive density @xmath147 under the divergence @xmath47 for general @xmath148 is proportional to @xmath205^{\\frac{2}{1-\\alpha}},\\ ] ] and hence the the integral in brackets is concretely written as @xmath206 to aid in the simplification of the integration with respect to @xmath119 , we first re - express those terms involving @xmath119 by completing the square , and neglecting , for now , the factor @xmath207 .",
    "let @xmath208 .",
    "then @xmath209 the `` residual term '' , @xmath210 may be expressed as @xmath211 where @xmath212 where @xmath213 is given by and @xmath214 where @xmath215 and @xmath216 are given by .",
    "the third equalities in and will be proved in lemma [ lemma - appendix ] below .",
    "similarly we may re - express the terms involving @xmath120 as @xmath217 after integration with respect to @xmath119 and @xmath120 , the integral given by is proportional to @xmath218 note that in an identity given by @xcite , ( see page 1758 ) @xmath219 the integral of the right - hand side reduces the beta function @xmath220 when @xmath221 .",
    "hence the integral is exactly proportional to @xmath222 since the bayesian predictive density @xmath147 with respect to the prior @xmath111 is proportional to the integral to the @xmath223 power , the theorem follows .",
    "the function @xmath229 is re - expressed as @xmath230 since @xmath231 we obtain @xmath232 and @xmath233 hence we have @xmath234 since the matrix for quadratic form of @xmath137 in the `` residual term '' may be written as @xmath235 the lemma follows .",
    "the bayes predictive density @xmath147 under the divergence @xmath47 for @xmath2 is proportional to @xmath236}{e[\\eta|v , v_*,s ] } \\right\\|^2 \\right ) .",
    "\\end{split}\\ ] ] hence the bayes solution with respect to the prior density @xmath237 under @xmath53 is the plug - in normal density @xmath238 where @xmath239 denotes the @xmath99-variate normal density with the mean vector @xmath240 and the covariance matrix @xmath241 and where @xmath242 and @xmath243 are given by @xmath244 } { e[\\eta|y]}= v - \\frac{d \\nabla_v m(v , v_*,s ) } { 2\\{\\partial/\\partial s\\}m(v , v_*,s ) } , \\\\",
    "\\hat{\\sigma}^2_\\pi & = \\frac{1}{e[\\eta|y ] } = -\\frac{m(v , v_*,s ) } { 2\\{\\partial/\\partial s\\}m(v , v_*,s ) } ,   \\end{split}\\ ] ] and where @xmath245 is the marginal density given by @xmath246 now we consider the marginal density of @xmath247 with respect to the prior @xmath111 , with @xmath2 .",
    "using essentially the same calculations as in section [ sec : case-1 ] , we obtain the marginal density in the relatively simple form @xmath248 from the expression in , a straightforward calculation gives the the estimators of @xmath119 and @xmath13 in the simple form @xmath249 where @xmath145 , respectively .",
    "this completes the proof .",
    "@xcite showed that , under the @xmath250 loss , the risk function of a general shrinkage estimator @xmath251 with suitable @xmath252 is given by @xmath253 = e\\left [ \\frac{\\| \\hat{\\theta}_\\phi - \\theta\\|^2}{\\sigma^2}\\right ] \\\\ & =   e\\left [ \\frac{\\| v - \\theta\\|^2}{\\sigma^2}\\right]+ e \\left [ \\frac{\\phi(w)}{w } \\left\\ {   \\psi(v , v_*,c , d,\\nu ) \\bigg ( ( n - k+2)\\phi(w )   \\right .",
    "\\\\ & \\quad    \\left .",
    "+ 4 \\left\\ { 1- \\frac{w\\phi'(w)}{\\phi(w)}(1+\\phi(w ) \\right\\ }    \\bigg ) -2\\sum_{i=1}^l \\frac{d_i}{c_i } \\right\\ } \\right ] , \\end{split}\\ ] ] where @xmath254 is given by @xmath255 for @xmath256 , we have @xmath257 which is always positive when @xmath258 . since @xmath259 is bounded from above by @xmath260 , the risk function of @xmath261 satisfies @xmath262\\\\ & \\leq \\mbox{mr}_{\\theta } + e \\left [ \\frac{\\nu}{1+\\nu+w } \\left\\ { -2\\sum \\frac{d_i}{c_i } +   \\max \\frac{d_i}{c_i }   \\frac{\\{(n - k+2)\\nu+4\\}w^2}{(1+\\nu+w)^2 }   \\right .",
    "\\\\ & \\quad \\left",
    "+ \\max \\frac{d_i}{c_i } \\frac{(\\nu+1)\\{\\nu(n - k-2)+4\\}w}{(1+\\nu+w)^2 } \\right\\ } \\right ] , \\end{split}\\ ] ] where @xmath263 .",
    "next we consider the risk function of @xmath264 where @xmath265 , which is given by @xmath266   & = e\\left [ \\left(1-\\frac{\\phi(w)}{w}\\right)\\frac{s}{(n - k)\\sigma^2 }       -\\log \\frac{s}{(n - k)\\sigma^2 } \\right.\\\\ & \\quad \\left .",
    "-\\log\\left(1-\\frac{\\phi(w)}{w}\\right )   -1      \\right ] \\\\ & = \\mbox{mr}_{\\sigma^2 } + e\\left [ -\\frac{\\phi(w)}{w\\sigma^2}\\frac{s}{n - k}-\\log \\left(1-\\frac{\\phi(w)}{w}\\right)\\right ] , \\end{split}\\ ] ] where @xmath267 and @xmath169 . by the chi - square identity",
    "( see e.g.  @xcite ) , we have @xmath268= e\\left [ ( n - k+2)\\frac{\\phi(w)}{w}-2\\phi'(w)\\right].\\ ] ] also using the relation @xmath269 for @xmath270 , we have @xmath271   \\\\ & \\leq \\mbox{mr}_{\\sigma^2 } + e\\left [ \\frac{\\phi(w)}{w } \\left\\ { \\frac{2}{n - k}\\left ( \\frac{w\\phi'(w)}{\\phi(w ) } -1 \\right )   + \\frac{1}{2}\\max\\frac{\\phi(w)/w}{1-\\phi(w)/w } \\right\\ } \\right ] . \\end{split}\\ ] ] for @xmath272 , we have @xmath273 \\\\ & \\leq \\mbox{mr}_{\\sigma^2 } + e\\left [ \\frac{\\nu}{1+\\nu+w } \\left\\ { -\\frac{2}{n - k}\\frac{w}{1+\\nu+w }   + \\frac{\\nu}{2 } \\right\\ } \\right ] .",
    "\\end{split}\\ ] ] hence @xmath274 + \\frac{m}{2 } e[l_2(\\hat{\\sigma}^2_{\\nu , c } , \\sigma^2 ) ]   \\leq \\mbox{mr}_{\\theta,\\sigma^2 } - \\frac{\\nu}{2 } e\\left [ \\frac{\\psi(w)}{(1+\\nu+w)^3}\\right]\\ ] ] where @xmath167 is the minimax risk given by and @xmath275 hence the theorem follows ."
  ],
  "abstract_text": [
    "<S> this paper considers estimation of the predictive density for a normal linear model with unknown variance under @xmath0-divergence loss for @xmath1 . we first give a general canonical form for the problem , and then give general expressions for the generalized bayes solution under the above loss for each @xmath0 . for a particular class of hierarchical generalized priors studied in maruyama and strawderman ( 2005 , 2006 ) for the problems of estimating the mean vector and the variance respectively , we give the generalized bayes predictive density . </S>",
    "<S> additionally , we show that , for a subclass of these priors , the resulting estimator dominates the generalized bayes estimator with respect to the right invariant prior when @xmath2 , i.e. , the best ( fully ) equivariant minimax estimator . </S>"
  ]
}