{
  "article_text": [
    "given a statistical population of discrete events @xmath1 generated by a stationary dynamic process , one of the most interesting statistical properties of the population and hence of the process is its entropy . if the sample space , i.e. the number of different elements which are allowed to occur in the population , is small compared with the size of a drawn sample one can approximate the probabilities @xmath0 of the elements @xmath1 by their relative frequencies @xmath2 and one finds for the observed entropy",
    "@xmath3 @xmath4 if the number of the allowed different events is not small compared with the size of the sample the approximation @xmath5 yields dramatically wrong results . in this case the knowledge of the frequencies is not sufficient to determine the entropy .",
    "the aim of this paper is to provide a method to calculate the entropy and other statistical characteristics for the case that the approximation ( [ approximation ] ) does not hold .",
    "an interesting example of such systems are subsequences ( words ) of length @xmath6 of symbolic sequences of length @xmath7 written using an alphabet of @xmath8 letters .",
    "examples are biosequences like dna ( @xmath9 , @xmath10 ) , literary texts ( @xmath11 letters and punctuation marks , @xmath12 ) and computer files ( @xmath13 , @xmath7 arbitrary ) . for the case of biosequences",
    "there is a variety of @xmath14 different words of length @xmath15 . to measure the probability distribution of the words directly by counting their frequencies we need at least a sequence of length @xmath16 to have reliable statistics .",
    "therefore the ensemble of subsequences of length @xmath6 is a typical example where the precondition does not hold . to illustrate the problem we calculate the observed entropy @xmath17 for @xmath18 words of length @xmath6 in a bernoulli sequence with @xmath13 where both symbols occur with the same probability .",
    "the exact result is @xmath19 . in figure",
    "[ hbern ] we have drawn the values of @xmath20 and @xmath17 over @xmath6 . the observed entropy values are correct for small word length @xmath6 when we can approximate the probabilities by the relative frequencies . for larger word length , however , the observed entropies",
    "are significantly below the exact values , even for very large samples ( circles : @xmath21 , diamonds : @xmath22 ) .    under several strong preconditions the probabilities of words in sequences can be estimated from the frequencies using various correction methods  @xcite .",
    "the advanced algorithm proposed in  @xcite is based on a theorem by mcmillan and khinchin @xcite saying that for word length @xmath23 the frequencies of the admitted substrings of a sequence are equally distributed .",
    "if one is interested in the entropies for _ finite _ words , however , the theoretical basis to apply this theorem is weak and there is no evidence about the reliability of the results .",
    "moreover this theorem is proven for markov sequences only . in sequences gathered from natural languages , biosequences and other natural or artificial sources",
    "it is very unlikely that the probabilities of the words of interesting length , e.g.  words or sentences for languages , amino acids or elements of the hidden `` dna language '' for biosequences , are equally distributed .",
    "otherwise we had to assume that all english five ",
    "words are equally frequent .",
    "certainly this is not the case .",
    "to calculate the entropy of a distribution it is not necessary to determine for each event @xmath1 the probability @xmath0 .",
    "it is sufficient to determine the values of the probabilities without knowing which probability belongs to which event .",
    "generally spoken if we assume to have @xmath24 events there are @xmath25 different relations @xmath26 .",
    "we need not to determine one particular ( the correct relation ) but only one arbitrary of them .",
    "hence the calculation of the entropy is @xmath25 times easier than to determine the probability @xmath0 for each event @xmath1 .",
    "we assume a special order where the first element has the largest probability , the second one the second largest etc .",
    "we call this distribution zipf  ordered .",
    "zipf ordering means that the probabilities of the elements are ordered according to their rank and therefore the distribution @xmath0 is a monotonically decaying function .",
    "the following procedure describes a method how to reconstruct the zipf  ordered probability distribution @xmath0 from a finite sample .",
    "provided we have some reason to expect ( to guess ) the parametric form of the probability distribution . as an example we use a simple distribution @xmath27 with @xmath28 consisting of a linearly decreasing and a constant part @xmath29 then the algorithm runs as follows :    1 .",
    "find the frequencies @xmath30 for the @xmath18 events @xmath1 and order them according to their value ( zipf  order ) .",
    "the index @xmath1 runs over all _ different _ events occurring in the sample ( @xmath31 } ) .",
    "note : there are @xmath18 events but only @xmath32 different ones .",
    "normalize this distribution @xmath33 .",
    "there are various sophisticated algorithms to find the frequencies of large samples and to order them ( e.g.  @xcite ) .",
    "as in earlier papers @xcite we applied for finding the elements a `` hashing''method and for sorting a mixed algorithm consisting of `` quicksort '' for the frequent elements and `` distribution counting '' for the long tail of elements with low frequencies .",
    "guess initial conditions for the parameters ( in our case @xmath34 , @xmath35 and @xmath36 ) .",
    "3 .   generate @xmath37 samples of @xmath18 random integers ( @xmath38 , @xmath39 , @xmath40 ) according to the parametric probability distribution @xmath27 . in the following examples we used @xmath41 .",
    "order each of the samples according to the ranks @xmath42 ( @xmath43 ) .",
    "average over the @xmath37 ordered samples @xmath44 with @xmath45 and @xmath46 . since we want to determine the averaged or typical zipf  ordered distribution , it is important to order the elements first and then to average . normalize the averaged distribution of the frequencies @xmath47 4 .",
    "measure the deviation @xmath48 between the normalized averaged simulated frequency distribution @xmath49 and the frequency distribution @xmath50 of the given sample according to a certain rule , e.g. @xmath51 5 .",
    "change the parameters of the guessed probability distribution @xmath0 ( in our case the parameters @xmath34 , @xmath35 and @xmath36 ) due to an optimization rule ( e.g.  @xcite ) which minimizes @xmath48 and proceed with the third step until the deviation @xmath48 is sufficiently small .",
    "extract the interesting statistical properties out of the probability distribution @xmath0 using the parameters @xmath52 , @xmath53 and @xmath54 which have been gathered during the optimization process .",
    "we generated a statistical ensemble @xmath22 according to the probability distribution eq .",
    "( [ probability ] ) with @xmath55 , @xmath56 and @xmath57 . fig .",
    "[ test ] ( solid lines ) shows the probability distribution @xmath0 and the zipf  ordered frequencies @xmath2 .",
    "optimizing the parametric guessed probability distribution using the proposed method we find for the optimized parameters @xmath58 , @xmath59 and @xmath60 , i.e. the guessed and the actual distributions fall almost together .    since we know the original probability distribution ( eq .  [ probability ] ) we can compare its exact entropy with the entropy of the guessed probability distribution @xmath61 and with the observed entropy @xmath3 due to eqs .",
    "( [ hdefinition1],[hdefinition ] ) .",
    "@xmath62 we found @xmath63 and @xmath64 , the exact value according to @xmath27 ( eq .  [ probability ] ) is @xmath65 .",
    "now we try to guess a probability of a more complicated form @xmath66 ( as we will show below this function approximates the probability distribution of the words in an english text . ) the variables @xmath34 and @xmath67 can be eliminated due to normalization and continuity condition .",
    "the test sample of size @xmath22 was generated using @xmath68 , @xmath69 , @xmath70 and @xmath71 .",
    "after the optimization we guess the parameters @xmath72 , @xmath73 , @xmath74 and @xmath75 .",
    "[ test1 ] shows the original and the guessed probability distributions and the zipf  ordered frequencies for both cases .",
    "the guessed entropy @xmath76 approximates the exact value @xmath77 very well while the observed entropy @xmath78 shows a clear deviation from the correct value .      with the ansatz ( [ pade ] ) we tried to guess the probability distribution of the words of different length @xmath6 in the text `` moby dick '' by h.  melville  @xcite .",
    "the text was mapped to an alphabet of @xmath79 letters as described in  @xcite . depending on overlapping or non  overlapping counting of the words we expect different results .",
    "we note that overlapping counting is statistically not correct since the elements of the sample are not statistically independent , however , only overlapping counting yields enough words to get somehow reasonable results for the observed entropy .",
    "we will show that our method works in both cases , overlapping and non  overlapping .",
    "[ beispiel ] shows the ordered frequencies of @xmath80 words of the length @xmath81 .",
    "the optimized distribution eq .",
    "( [ pade ] ) reproduces the original frequency distribution ( moby dick ) with satisfying accuracy .    using the ansatz  ( [ pade ] ) we found @xmath82 , @xmath83 , @xmath84 and @xmath85 .",
    "this calculation was carried out for various word lengths @xmath6 .",
    "[ entropies ] shows the entropies @xmath86 and @xmath87 according to eqs .",
    "( [ hdefinition1],[hdefinition ] ) as a function of @xmath6 .",
    "all results obtained have been derived from a set of @xmath88 non - overlapping words taken from the text of length @xmath89 .",
    "when we count overlapping words , we find surprisingly that the entropy is quite insensitive ( see curves using filled and empty diamonds in fig .  [ entropies ] ) .",
    "the rather difficult problem of overlapping or non  overlapping counting will be addressed in detail in  @xcite .",
    "since the exact probability distribution for the words in `` moby dick '' is unknown we compare the guessed entropy ( crosses ) with the observed entropy ( empty diamonds : overlapping counting , full diamonds : non - overlapping counting ) and an estimation of the entropy using an extrapolation method ( see  @xcite ) , all based on the same set of data ( @xmath90 ) , and with the observed entropy based on a twenty times larger set of data ( boxes : overlapping counting ) .",
    "as expected , for longer word length @xmath6 the observed values @xmath86 underestimate the entropy . for small @xmath6 they are reliable due to the reliable statistics . the guessed entropy @xmath91 agrees for small @xmath6 with the observed entropy and for large @xmath6 with the extrapolated values .",
    "the form of the guessed theoretical distribution @xmath92 is arbitrary as long as it is a normalized monotonically decreasing function ( zipf - order ) .",
    "suppose that one has no information about the mechanism which generated a given sample .",
    "then one has to find the functional form of the guessed distribution which is most appropriate to a given problem , i.e. in the ideal case the guessed distribution contains the real probability distribution as special case without being too complicated .",
    "an ansatz @xmath92 is suited if the optimized guessed probability distribution reproduces the frequency distribution of the original sample with satisfactory accuracy .",
    "the ansatz  ( [ pade ] ) looks rather artificial : in fact we tried several forms of the guessed probability distribution and the one proposed in eq .  ( [ pade ] ) turned out to be the best of them .",
    "none of the others reproduces the frequencies sufficiently correct . for demonstration",
    "we assume the function @xmath93 with the normalization @xmath94 .",
    "the optimized function is drawn in figure  [ beispiel ] ( _ guessed ( b ) _ ) .",
    "we find that the frequency distribution reproduced from this function differs much more from the original frequency distribution ( moby dick ) than that of the guess according to eq .",
    "( [ pade ] ) .",
    "admittedly any similar ansatz showing a well pronounced peak for low ranks _ ( frequent words ) _ , a long plateau with slow decrease _ ( standard words ) _ and a long tail _ ( seldom words ) _ , could give reliable results as well .",
    "anyhow there is no wide choice for the parametric form of the probability distribution .",
    "( [ pade ] ) belongs to the class of distributions fulfilling this _ three ",
    "region criterion_. for a more detailed discussion of the statistics of words see e.g.  @xcite and many references therein .",
    "the problem addressed in this paper was to find the rank ordered probability distribution from the given frequency distribution of a finite sample . for finite samples ( bernoulli sequence and english text )",
    "we have shown that the calculation of the entropy using the relative frequencies instead of the ( unknown ) probabilities yields wrong results .",
    "we could show that the proposed algorithm is able to find the correct parameters of a guessed probability distribution which reproduces the statistical characteristics of a given symbolic sequence .",
    "the method has been tested for samples generated by well defined sources , i.e. by known probability distributions , and for an unknown source , i.e. the word distribution of an english text . for the sample sequences we have",
    "evidence that the algorithm yields reliable results .",
    "the deviations of the entropy values from the correct values are rather small and in all cases far better than the observed entropies . for the unknown source `` moby dick '' we have no direct possibility to check the quality of the method , however , the calculated entropy values agree for small word lengths @xmath6 with the observed entropy and for larger @xmath6 with the results of an extrapolation method  @xcite . in this sense",
    "both approaches support each other .",
    "the proposed algorithm can be applied to the trajectories of dynamic systems .",
    "formally the trajectory of a discrete dynamics is a text written in a certain language using @xmath8 different letters .",
    "the rank ordered distribution of sub - trajectories of length @xmath6 belongs to the most important characteristics of a discrete dynamic system . in this way we consider the analysis of english text as an example for the analysis of a very complex dynamic system .    in many cases",
    "there is a principal limitation of the available data , i.e. the available samples are small with respect to the needs of a reliable statistics , and hence there is a principal limitation for the calculation of the statistical properties using frequencies instead of probabilities . for such cases using the proposed method one can calculate values which could not be found so far .",
    "given a finite set of data the proposed method yields the most reliable values for the zipf  ordered distributions and the entropies which are presently available .",
    "the method is not restricted to the calculation of the entropy but all statistical properties which depend on the zipf  ordered probability distribution can be estimated using the proposed algorithm ."
  ],
  "abstract_text": [
    "<S> we propose a new method for the calculation of the statistical properties , as e.g. the entropy , of unknown generators of symbolic sequences . </S>",
    "<S> the probability distribution @xmath0 of the elements @xmath1 of a population can be approximated by the frequencies @xmath2 of a sample provided the sample is long enough so that each element @xmath1 occurs many times . </S>",
    "<S> our method yields an approximation if this precondition does not hold . for a given @xmath2 </S>",
    "<S> we recalculate the zipf  ordered probability distribution by optimization of the parameters of a guessed distribution . </S>",
    "<S> we demonstrate that our method yields reliable results .    </S>",
    "<S> entropy estimation , information science </S>"
  ]
}