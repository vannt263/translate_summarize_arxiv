{
  "article_text": [
    "given a set of empirical observations obeying a certain ( unknown ) mathematical model , a parameter estimator is a function which maps the available data to a certain parameter space of interest .",
    "a very desirable property for an estimator is that of robustness which characterizes a relative insensitivity of the estimator to deviations of the observed data from the assumed model .",
    "more specifically , this property is central in situations where the data are prone to non gaussian noise or disturbances of possibly arbitrarily large amplitude ( called outliers ) .",
    "the quest for robust estimators has led to the development of many estimators such as the least absolute deviation ( lad ) @xcite , the least median of squares @xcite , the least trimmed squares @xcite , the class of m - estimators @xcite .",
    "evaluating formally to what extent a given estimator is robust requires setting a quantitative measure of robustness .",
    "incidentally such a measure can serve as comparison criterion between different robust estimators . generally , the robustness is assessed in term of the maximum proportion of outliers in the total data set that the estimator can handle while remaining stable ( see for example the concept of breakdown point @xcite ) .",
    "more recently the maximum correntropy @xcite has emerged as an information - theoretic estimation framework which induces some robustness properties with respect to outliers .",
    "although maximum correntropy estimation is closely related to m - estimation , its discovery has broadened the horizon of possibilities for designing robust identification schemes . as a matter of fact",
    ", it has been successfully applied to a variety of estimation problems such as linear / nonlinear regression , filtering , face recognition in computer vision @xcite .",
    "[ [ contribution ] ] contribution + + + + + + + + + + + +    although the maximum correntropy based estimators have been gaining an increasing success , the formal analysis of its robustness properties is still a largely open research question . in this paper",
    "we propose such an analysis for a class of maximum correntropy based estimators applying to linear regression problems .",
    "more precisely , the contribution of the current paper is articulated around the following two questions :    * to what extent the maximum correntropy estimation framework is robust to outliers ?",
    "+ by robustness , it is meant here a certain insensitivity of the estimator to large errors of possibly arbitrarily large magnitude . to address this question ,",
    "we derive parametric estimation error bounds induced by the estimator in function of both the degree of richness of the regression data and on the fraction of outliers . in summary",
    ", we show that if the regression data enjoy some richness properties and if the number of outliers is reasonably small , then the parametric estimation error remains stable .",
    "indeed the proportion of outliers that the estimator is capable to correct depends on how rich the regressor matrix is .",
    "moreover , the estimation error appears to be a decreasing function of the richness measure . *",
    "how does richness of the training data set influence the robustness of the estimator and how to characterize it ?",
    "+ we provide an appropriate characterization of the richness in terms of the cardinality of the regressor vectors which are strongly correlated to any vector of the ambient space .",
    "as such however , this quantitative measure of richness is not computable at an affordable price . to alleviate this difficulty the paper proposes some estimates of this measure thus allowing for the approximation of the parametric estimation error bounds .",
    "[ [ outline ] ] outline + + + + + + +    the rest of this paper is organized as follows .",
    "section [ sec : pbm ] presents the robust regression problem and define the class of maximum correntropy estimators whose properties are to be studied in the paper .",
    "it also introduces the general setting of the paper .",
    "the main analysis results are developed in section [ sec : analysis ] . in section",
    "[ sec : simulations ] we run numerical experiments to illustrate the richness measure and the evolution of the derived error bounds with respect to the amount of noise .",
    "finally , section [ sec : conclusion ] contains concluding remarks concerning this work .",
    "[ [ notations ] ] notations + + + + + + + + +    @xmath0 is the set of real numbers ; @xmath1 is the set of real nonnegative numbers ; @xmath2 is the set of natural integers .",
    "@xmath3 will denote the number of data points and @xmath4 the associated index set .",
    "for any finite set @xmath5 , @xmath6 refers to the cardinality of @xmath5 .",
    "however , whenever @xmath7 is a real number @xmath8 is the absolute value of @xmath7 . for @xmath9}^\\top\\in { \\mathbb{r}}^n$ ] , @xmath10 will denote the @xmath11-norm of @xmath7 defined by @xmath12 , for @xmath13 , @xmath14 .",
    "the exponential of a real number @xmath15 will be denoted @xmath16 or @xmath17 according to visual convenience .",
    "let @xmath18 and @xmath19 be some stochastic processes taking values respectively in @xmath20 and @xmath0 .",
    "they are assumed to be related by an equation of the form @xmath21 where @xmath22 represent an unobserved error sequence ; @xmath23 is an unknown parameter vector .",
    "eq . may describe a static system or a dynamic one . in the latter case",
    ", we will conveniently assume that the so - called regressor ( or explanatory vector ) @xmath24 has the following structure @xmath25}^\\top$ ] , i.e. , is an fir - type ( finite impulse response ) system , with @xmath26 then denoting its input signal at time @xmath27 .",
    "[ assum : ergodicity ] the joint stochastic process @xmath28 is independently and and identically distributed .    the noise sequence @xmath29 satisfies the following : there is @xmath30 such that if we define the index sets @xmath31 and @xmath32 , then the cardinality of @xmath33 is `` much larger '' than that of @xmath34",
    ".    we will formalize latter in the paper what `` much larger '' can mean .",
    "similarly as in @xcite , we can assume that @xmath35 is of the form @xmath36 where @xmath37 is a bounded dense noise sequence and @xmath38 is a sparse noise sequence in the sense that only a few elements of it are different from zero .",
    "however its nonzero elements are allowed to take on arbitrarily large values ( called in this case , outliers ) .",
    "[ [ problem ] ] problem + + + + + + +    given a finite collection @xmath39 of measurements obeying the system equation , the robust regression problem of interest here is the one of finding a reliable estimate of the parameter vector @xmath40 despite the effect of arbitrarily large errors .",
    "let @xmath41 denote a candidate parameter vector ( pv ) which we would like , ideally , to coincide with the true pv @xmath40 .",
    "given @xmath24 and @xmath41 , the prediction we can made of @xmath42 is @xmath43 .",
    "it is then the goal of the estimation method to select @xmath41 such that @xmath42 and @xmath44 are close in some sense for any @xmath27 .",
    "closeness will be be measured in term of the so - called maximum correntropy between the measured output @xmath42 and the predicted value @xmath44 .",
    "the correntropy is an information - theoretic measure of similarity between two arbitrary random variables @xcite .",
    "more specifically , let @xmath45 and @xmath46 be two random variables defined on the same probability space , and taking values in @xmath0 . let @xmath47 be a positive - definite kernel function ( see e.g. , @xcite for a definition ) .",
    "the correntropy @xmath48 between @xmath45 and @xmath46 with respect to a kernel function @xmath49 , is defined by @xmath50,\\ ] ] where @xmath51 $ ] refers to the expected value with respect to the joint distribution of @xmath52 . in a more explicit form",
    ", we have @xmath53 with @xmath54 being the joint probability density function of @xmath52 .",
    "although the original definition of correntropy in @xcite fixes @xmath49 to be the gaussian kernel , it is possible to extend it to any positive definite kernel function .",
    "we consider in this paper a kernel function of the form @xmath55 where @xmath56 is a user - specified parameter and @xmath57 is a function which satisfies the following properties .    1 .",
    "@xmath58 is positive - definite : @xmath59 @xmath60 and @xmath61 if and only if @xmath62 2 .",
    "@xmath58 is symmetric : @xmath63 3 .",
    "@xmath58 is nondecreasing : @xmath64 whenever @xmath65 4",
    ".   there exists @xmath66 such that @xmath67 @xmath68 .",
    "the correntropy maximization is an estimation framework where one tries to maximize the correntropy . in the regression problem",
    "stated above , we aim to find the parameter vector @xmath41 that maximizes , @xmath69 is indeed constant i.e. , independent of @xmath27",
    ". hence @xmath27 refers here to an arbitrary time index . ]",
    "@xmath69 , the correntropy between @xmath42 and @xmath70 with respect to @xmath49 .",
    "in practice however the distribution but this follows from @xmath71 . ] @xmath71 is generally unknown so that one can not evaluate the exact correntropy . as a consequence of this difficulty one would be content in practice with maximizing a sample estimate of the correntropy .",
    "assume that we are given a set @xmath39 of data points sampled independently from the joint distribution @xmath71 .",
    "then in virtue of assumption [ assum : ergodicity ] , an estimate of the correntropy is given by @xmath72 \\end{aligned}\\ ] ] for all @xmath73 .",
    "hence the maximum correntropy estimator studied in this paper is the possibly set - valued map @xmath74 which maps the data to parameter space , @xmath75 although the focus of this paper is the analysis of the properties of the estimator , let us mention in passing that the underlying optimization problem in is non convex .",
    "this implies that solving numerically can be challenging .",
    "however it can be interpreted iteratively as a weighted least squares problem in the case for example where @xmath49 is taken to be the gaussian kernel .",
    "we will get back to this in section [ sec : simulations ] .",
    "as discussed in the introduction , an estimator of the form is intuitively thought ( and empirically shown ) to be endowed with some robustness properties . by this , we mean that it is able to keep behaving reasonably well when a certain fraction of the available data points are affected by noise components @xmath35 of possibly arbitrarily large magnitude .",
    "the question of main interest in this paper is to characterize quantitatively up to what extent the estimator defined in can be insensitive to outliers .",
    "as will be seen , the robustness property is inherited from both the structure of the estimator and the richness of the regression data .",
    "we are therefore interested in formalizing as well that richness and how it contributes to the stability properties of the estimator .    to proceed with the analysis , let us introduce some notations .",
    "for convenience make the following assumption .",
    "[ eq : x - neq-0 ] the regressor sequence @xmath76 satisfies : @xmath77 for all @xmath78 .",
    "note that assumption [ eq : x - neq-0 ] is without loss of generality . under this assumption ,",
    "let us pose @xmath79 with @xmath80 denoting euclidean norm . upon dividing the system equation by @xmath81",
    ", we can even assume that @xmath82 .",
    "let @xmath83{0}{1}$ ] be a real number . for any @xmath84 ,",
    "define the index set @xmath85 with @xmath86}\\in { \\mathbb{r}}^{n\\times n}$ ] a matrix formed with all the regressors .",
    "finally , let @xmath87 be the ratio between the minimum cardinality @xmath88 can attain for all possible values of @xmath89 , and the number @xmath3 of columns in @xmath90 , i.e. , @xmath91 the number @xmath92 measures somehow the richness of the regression data .",
    "intuitively , @xmath92 reflects a dense spanning of all directions of the vector space @xmath20 by the vectors @xmath76 . for a given @xmath93",
    ", it is desired that @xmath92 be as large as possible .",
    "we will refer to it as the correlation measure of the matrix @xmath90 at the level @xmath94 .",
    "it appears intuitively that @xmath92 is a decreasing function of @xmath94 .",
    "clearly , we get @xmath95 for @xmath96 for finite @xmath3 while @xmath97 for @xmath98 . for a given matrix",
    ", it would be interesting to be able to evaluate numerically the quantitative measure @xmath92 of richness .",
    "indeed , this value will be required for numerical assessment of the error bound to be derived in section [ subsec : main - result ] .",
    "however computing exactly the value of @xmath92 is a hard combinatorial problem .",
    "we therefore discuss how to reach estimates of @xmath92 at an affordable cost . to this end , let @xmath99}\\in { \\mathbb{r}}^{n\\times n}$ ] be the matrix obtained from @xmath90 by normalizing its columns to unit @xmath100-norm , i.e. , @xmath101 for all @xmath27 .",
    "then introduce the number @xmath102 which is solely a function of the matrix @xmath90 , hence the notation .",
    "note that the so - defined @xmath103 lies necessarily in the real interval @xmath104{0}{1}$ ] .",
    "moreover , it can be usefully observed that @xmath105 , with @xmath106 referring to the square root of the minimum eigenvalue .",
    "now for any @xmath78 consider the following index set @xmath107 where @xmath108 .",
    "it is assumed in the definition that @xmath109 so that @xmath110 .",
    "finally , let @xmath111 be the ratio between the minimum cardinality of the finite set @xmath112 over all @xmath27 living in @xmath113 and the number @xmath3 of columns in @xmath90",
    ". then we can estimate @xmath92 as follows .",
    "[ prop : estimates - rho ] let @xmath114 be a real matrix .",
    "then , for all @xmath115{0}{1}$ ] with @xmath116 , @xmath117 with @xmath118 denoting the minimum eigenvalue of @xmath119 and @xmath120 is defined as in .",
    "the proof of this proposition uses the following lemma .",
    "* thm 5.14)[lem : zhang ] let @xmath121 such that @xmath122 with @xmath123 denoting the conjugate transpose of @xmath7",
    ". then @xmath124 equality holds if and only if there exists @xmath125 such that either @xmath126 or @xmath127 .",
    "the upper bound is immediate . to see this ,",
    "let @xmath128 be the eigenvector associated with the smallest eigenvalue of @xmath119",
    ". then @xmath129 it follows that @xmath130 . the upper inequality in follows by additionally taking into consideration the obvious fact that @xmath131 .",
    "we now prove the inequality @xmath132 .",
    "to begin with , note from that for any @xmath84 satisfying @xmath133 , there exists @xmath134 such that @xmath135 .",
    "consider an index @xmath136 , such that @xmath137 for some @xmath138{0}{1}$ ] . then observe that @xmath139 is equivalent to @xmath140 on the other hand , by applying lemma [ lem : zhang ] , we can write @xmath141 it follows that for @xmath139 to hold , it is sufficient that @xmath142 which in turn is equivalent to @xmath143 with @xmath144{0}{1}$ ] by the assumption that @xmath109 .",
    "hence , for @xmath145 to be greater than or equal to @xmath94 , it is enough that @xmath146 .",
    "this means that for a given @xmath27 , @xmath147 being in the index set @xmath112 defined in is a sufficient condition for @xmath139 for all @xmath84 such that @xmath133 .",
    "therefore @xmath148 hence implying that @xmath149 .",
    "the key benefit of proposition [ prop : estimates - rho ] is that it provides a method for estimating the measure @xmath92 defined in at an affordable cost .",
    "note however that while the upper bound in can be computed easily , obtaining the lower bound @xmath150 is still challenging .",
    "the reason is that this bound involves the number @xmath103 in whose numerical evaluation requires solving a nonconvex optimization problem .",
    "nevertheless , it can be approximated through some heuristics , e.g. by a sequence of linear programs .",
    "equipped with the measure of informativity introduced above , we can now state the main result of this paper , which stands as follows .",
    "[ thm : main - theorem ] let @xmath151 and @xmath152 with @xmath29 denoting the noise sequence in .",
    "let @xmath58 be a function obeying p1-p4 .",
    "assume that the following condition is satisfied for some @xmath153{0}{1}$ ] , @xmath154 then for any @xmath155 with @xmath156 being generated by system , it holds that @xmath157 where @xmath158 \\end{aligned}\\ ] ] if in addition , @xmath58 is strictly increasing on @xmath1 , then @xmath159    let @xmath160 then for any @xmath161 , it holds that @xmath162\\leq \\sum_{t=1}^n\\exp\\big[-\\gamma\\ell(y_t - x_t^\\top\\theta^\\star)\\big]\\ ] ] taking in particular @xmath163 and invoking the system equation , it follows that @xmath164+\\sum_{t\\in i_\\varepsilon^c}\\exp\\big[-\\gamma\\ell(v_t)\\big ]      & \\leq \\sum_{t\\in i_\\varepsilon^0 } \\exp\\big[-\\gamma\\ell(v_t - x_t^\\top\\eta^\\star)\\big ] \\\\       & \\quad \\quad + \\sum_{t\\in i_\\varepsilon^c } \\exp\\big[-\\gamma\\ell(v_t - x_t^\\top\\eta^\\star)\\big ]      \\end{aligned}\\ ] ] where we have posed @xmath165 .",
    "this implies that @xmath166\\leq \\sum_{t\\in i_\\varepsilon^0 } \\exp\\big[-\\gamma\\ell(v_t - x_t^\\top\\eta^\\star)\\big ] + \\left|i_\\varepsilon^c\\right|\\ ] ] with @xmath167 for any @xmath168 , we have @xmath169 by the symmetry and nondecreasing properties of @xmath58 . as a consequence , @xmath170\\geq \\sum_{t\\in i_\\varepsilon^0}\\exp\\big[-\\gamma\\ell(\\varepsilon)\\big]$ ] . on the other hand , by the fourth property of the function @xmath58",
    ", @xmath171 hence implying that @xmath172 . combining these observations allows us to write @xmath173-\\left|i_\\varepsilon^c\\right|          & \\leq   \\exp\\big[\\gamma \\ell(\\varepsilon)\\big]\\sum_{t\\in i_\\varepsilon^0 } \\exp\\big[-\\gamma \\alpha_\\ell\\ell(x_t^\\top\\eta^\\star)\\big]\\\\          & \\ : \\quad = \\exp\\big[\\gamma \\ell(\\varepsilon)\\big]\\sum_{t\\in i_\\varepsilon^0\\cap \\mathscr{i}_\\alpha(x,\\eta^\\star ) } \\exp\\big[-\\gamma \\alpha_\\ell\\ell(x_t^\\top\\eta^\\star)\\big]\\\\          & \\ : \\qquad \\ : \\ :   + \\exp\\big[\\gamma \\ell(\\varepsilon)\\big]\\sum_{t\\in i_\\varepsilon^0\\cap \\mathscr{i}^c_\\alpha(x,\\eta^\\star ) } \\exp\\big[-\\gamma \\alpha_\\ell\\ell(x_t^\\top\\eta^\\star)\\big ]      \\end{aligned}\\ ] ] in the last equality we have partitioned the set @xmath174 into @xmath175 and @xmath176 with @xmath177 being the complement of @xmath178 in @xmath113 .",
    "note from that for all @xmath179 , @xmath180 so that @xmath181 . plugging these observations into the above inequality yields @xmath182\\big\\{\\left|i_\\varepsilon^0\\right|\\exp\\big[-\\gamma\\ell(\\varepsilon)\\big]-\\left|i_\\varepsilon^c\\right|\\big\\}\\\\      & \\qquad \\qquad \\leq   \\left|i_\\varepsilon^0\\cap \\mathscr{i}_\\alpha(x,\\eta^\\star)\\right|\\exp\\big[-\\gamma\\alpha_\\ell \\ell(\\alpha r_x \\left\\|\\eta^\\star\\right\\|_2)\\big ]   + \\left|i_\\varepsilon^0\\cap \\mathscr{i}^c_\\alpha(x,\\eta^\\star)\\right| .",
    "\\end{aligned}\\ ] ] by observing that @xmath183 , we can rearrange the above inequality in the form @xmath184 - 1\\big\\ }      \\end{aligned}\\ ] ] now by exploiting the definition of @xmath92 , we can observe that @xmath185 moreover since @xmath186 the assumption guarantees that @xmath187 .",
    "therefore since the term on the right hand side of is negative , it holds that @xmath188 - 1\\big\\ } \\end{aligned}\\ ] ] then direct algebraic calculations lead to @xmath189\\leq 1\\ ] ] where @xmath190 is defined as in .",
    "indeed , in virtue of the assumption , @xmath190 is positive .",
    "hence we have @xmath191 of course , if @xmath58 is monotonically increasing on @xmath1 then it is invertible and the error bound in follows .",
    "a few comments follow from this result .",
    "a key assumption of the theorem is condition .",
    "what it requires is that on the one hand , the proportion of outliers be somehow small and on the other hand , that the regression data @xmath90 be rich in the sense that @xmath92 be large enough for a given nonzero @xmath153{0}{1}$ ]",
    ". an important teaching of this condition is that the richer the data matrix @xmath90 , the larger the number of outliers that can be corrected by the estimator .",
    "we can interpret as a sufficient condition for the stability of the estimator since it guarantees a bounded estimation error .",
    "a second comment concerns the amplitude of the error bound given in .",
    "for the purpose of making this bound small , we need the constant @xmath190 to be close to one .",
    "again we see that this is favored by a small number of outliers and a rich data set .",
    "an interesting special case is when @xmath192 , which occurs when the data are only affected by some outliers ( @xmath193 ) . in this case",
    "the number @xmath190 defined in reduces to @xmath194 which tend to suggest , since @xmath195 , that no exact recovery might be achieved once the data are affected by a single outlier . in contrast , a robust estimator such as the lad estimator ( see , e.g. , @xcite ) is able to achieve exact recovery under a relatively significant proportion of nonzero errors .",
    "we now discuss some special instances of theorem [ thm : main - theorem ] corresponding to two kernels which are frequently used for estimation . for convenience of the discussion ,",
    "let us introduce the following notation .",
    "let @xmath196 @xmath197\\ ] ] whenever @xmath198 and @xmath199 otherwise .",
    "the maximum laplacian correntropy estimator ( mce - l ) corresponds to the case where the function @xmath58 in is taken to be such that @xmath200 . as a result",
    ", the function @xmath49 takes the form @xmath201 it is straightforward to see that the properties p1-p4 are satisfied by @xmath58 with @xmath202 . theorem [ thm : main - theorem ] can be specialized to this case as follows .",
    "[ cor : bound - exp - l1 ] @xmath203 + let @xmath174 be defined as in theorem [ thm : main - theorem ] .",
    "assume that the following condition is satisfied @xmath204 for some @xmath153{0}{1}$ ] .",
    "+ then for any @xmath205 with @xmath206 defined as in , it holds that @xmath207      the most used form of correntropy is the one based on the gaussian kernel which , by omitting the normalizing factor , we can write in the form @xmath208 with @xmath209",
    ". we will refer to the associated estimator as the maximum gaussian correntropy estimator ( mce - g ) . here",
    ", the function @xmath58 is defined by @xmath210 and it can be quickly verified that it satisfies the properties p1-p4 . in particular , p4 is satisfied with @xmath211 .",
    "moreover @xmath58 is clearly monotonic on @xmath1 . as a consequence",
    ", we get a corollary of theorem [ thm : main - theorem ] as follows .",
    "let @xmath174 be defined as in theorem [ thm : main - theorem ] .",
    "assume that the following condition is satisfied @xmath212 for some @xmath153{0}{1}$ ] .",
    "+ then for any @xmath213 , it holds that @xmath214^{1/2}.\\ ] ]      we now consider the situation where only a noisy observation @xmath215 of the regressor vector @xmath24 in is available for prediction .",
    "this scenario is referred to as the robust error - in - variable ( eiv ) regression problem .",
    "then the predictor output is given by @xmath216 indeed theorem [ thm : main - theorem ] remains valid for this case . to see this note that the system equation can be rewritten as @xmath217 where @xmath218 .",
    "then clearly the theorem applies to the eiv scenario with @xmath76 and @xmath29 , replaced respectively by @xmath219 and @xmath220 .",
    "one limitation however in this case is that for a given @xmath30 , the cardinality of the set @xmath221 is likely to be much smaller than in the situation where the regressors are noise - free .",
    "the purpose of this section is to provide a numerical illustration of the richness measure and of the estimation error bound .",
    "the system example considered for the experiment is of an fir - type and is given by @xmath222 which can be written in the form with @xmath223}^\\top$ ] and @xmath224}^\\top$ ] . for the data - generation experiment , assume that @xmath225 i.e. , @xmath226 is sampled independently and identically from a zero - mean gaussian distribution of unit variance . as for the noise signal @xmath29 , it is defined as @xmath227 with @xmath228 where @xmath229 refers to the uniform distribution and @xmath38",
    "is a sequence of sparse noise with only a few nonzero elements ( which are otherwise not constrained in magnitude ) ; the nonzero elements of @xmath38 are here sampled from @xmath230 .",
    "we generate @xmath231 data pairs @xmath232 and carry out a comparison between three estimators : on the one hand , the maximum laplacian correntropy estimator ( mce - l ) and the maximum gaussian correntropy estimator ( mce - g ) and on the other hand , the least absolute deviation ( lad ) estimator ( which is also called @xmath233 estimator ) .",
    "recall that mce - g and mce - l involve non convex optimization . here",
    "they are heuristically implemented as a reweighted iterative least squares estimator and as a reweighted @xmath233 estimator respectively .",
    "the results are represented in figure [ fig : comparison_mce_estimates ] .",
    "what this suggests is that for fixed values of the design parameters @xmath234 and @xmath235 ( see eqs and for the roles of these parameters ) , lad and mce - l enjoy a similar performance for small amount of noise .",
    "but as the noise level increases , lad shows better stability capabilities than the mce - l .",
    "note that overall mce - g tends to perform best in the setting of this experiment .",
    "[ ] [ ] mce - l [ ] [ ] mce - g [ ] [ ] lad [ ] [ ] relative error [ ] [ ] @xmath236    in function of the level @xmath236 of noise . the range @xmath237 of @xmath236 corresponds indeed to a range of about @xmath238 for the signal - to - noise ratio in this experiment .",
    "the results are obtained from a monte - carlo simulation of size @xmath239 . for each experiment , the proportion of outliers is maintained fixed and equal to @xmath240 .",
    "design parameters : @xmath241 and @xmath242 .",
    ", title=\"fig : \" ]      we provide a graphical representation of how the informativity measure @xmath92 may , for a given data matrix @xmath114 , evolve with respect to the dimensions @xmath243 of @xmath90 and the degree @xmath94 of richness demanded ( see figure [ fig : estimates - rho ] ) .",
    "the estimated range for @xmath92 is based on eq . .",
    "here @xmath90 is formed from an fir - type of regressors with an input sampled from a zero - mean and unit variance gaussian distribution .",
    "our experiments in this specific study tend to suggest that @xmath92 is a non decreasing function of the ratio @xmath243 and a decreasing function of @xmath94 . moreover , the estimated range ( gray region in fig .",
    "[ fig : estimates - rho ] ) gets wider when @xmath243 is small .    [ ] [ ] @xmath94 [ ] [ ] @xmath92 +      the goal here is to illustrate the variation of the estimation error bounds as functions of the design parameters and the properties of the data in the special cases and .",
    "if for each level @xmath236 of noise , we select the parameter @xmath244 such that @xmath245 , then the error bounds have a linear rate of change with respect to @xmath236 as depicted in figure [ fig : bounds ] . note that here those bounds apply to the absolute parametric error so that they are not directly comparable to the ( empirical ) relative parametric errors reported in figure [ fig : comparison_mce_estimates ] . in any case , it is fair to observe that the theoretical bounds are conservative in the sense that they are generally higher than the true empirical errors .",
    "conservativeness is indeed a common feature for these types of results due to the various inequalities employed for the derivation .",
    "nevertheless , the main interest of theorem [ thm : main - theorem ] is that it provides a sufficient condition for the stability of the maximum correntropy estimator , a condition that depends explicitly on the degree of informativity of the regression data and on the proportion of outliers .",
    "moreover , by expressing error bounds which involve explicitly the design parameters , the theorem gives insights into how to tune those parameters with the aim to improve estimation performance .",
    "+ a further remark one can make is that the general formula for the error bound in has a kind of universal feature in the following sense : since the bound does not involve the magnitude of the true @xmath40 ( for an fir - type system for example ) , it is in principle valid regardless of @xmath40 .",
    "hence the relative error will be as smaller as the amplitude of the to - be - estimated vector is larger .",
    "[ ] [ ] bounds [ ] [ ] @xmath236 , @xmath246 , @xmath247 , @xmath248 in both cases . ,",
    "title=\"fig : \" ]",
    "in this paper we have proposed an analysis of the robustness properties of a correntropy maximization framework for regression problems .",
    "the class of estimators considered is quite general and include the gaussian and laplacian kernels as special cases .",
    "the contribution of the work consists in ( i ) deriving an appropriate notion of richness for the regression data ; ( ii ) proving stability of the considered class of estimators under the derived richness condition when the data are subject to dense and sparse noise ( outliers ) .",
    "our main result states that if the regression data are rich enough and if the number of outliers is small in some sense , then the parametric estimation error is bounded .",
    "the results come with explicit bounds which , in default of being exactly computable , can be estimated with computable estimates ."
  ],
  "abstract_text": [
    "<S> in this paper we formulate a solution of the robust linear regression problem in a general framework of correntropy maximization . </S>",
    "<S> our formulation yields a unified class of estimators which includes the gaussian and laplacian kernel - based correntropy estimators as special cases . </S>",
    "<S> an analysis of the robustness properties is then provided . </S>",
    "<S> the analysis includes a quantitative characterization of the informativity degree of the regression which is appropriate for studying the stability of the estimator . using this tool </S>",
    "<S> , a sufficient condition is expressed under which the parametric estimation error is shown to be bounded . </S>",
    "<S> explicit expression of the bound is given and discussion on its numerical computation is supplied . for illustration purpose , </S>",
    "<S> two special cases are numerically studied .    _ </S>",
    "<S> keywords : _ robust estimation , system identification , maximum correntropy , outliers . </S>"
  ]
}