{
  "article_text": [
    "over the past couple of decades the amount of raw data available has started to grow at an exponential rate , doubling approximately every 12 months @xcite , while the amount of data being consumed by users remains linear @xcite .",
    "the so - called ` big data ' phenomenon imposes an urgent need to develop , possibly with the aid of high - speed computing and cheap data storage , efficient pattern detection methods and data mining techniques aimed at identifying a few but highly relevant pieces of information in an ever - increasing noisy or irrelevant background .",
    "one of the most important and widespread examples of the big data phenomenon is time series data , as witnessed by the impressive growth of databases of electronic and mobile - device communication patterns in large social systems , financial returns in stock markets , physiological signals such as heartbeat and brain dynamics , gene expression profiles , and finally climate , weather and earthquake activity . in all these examples , high - dimensional ( multiple ) time series originate from the dynamical activity of the constituent units ( such as stocks , people , neurons , genes , etc . ) of large systems with complicated internal interactions .",
    "for this reason , ` big time series data ' offer an unprecedented empirical resource for the science of complex systems .",
    "multiple time series are in fact the key ingredient required in order to face one of the main challenges for our modern understanding of real - world complex systems : the identification of an emergent , mesoscopic level of dynamical organization which is intermediate between the microscopic dynamics of indivual units ( e.g. neurons ) and the macroscopic dynamics of the system as a whole ( e.g. the brain ) .",
    "many complex systems are indeed organized in a modular way , with functionally related units being correlated with each other , while at the same time being relatively less ( or even negatively ) correlated with functionally dissimilar ones . while the existence of such a modular organization is intuitively plausible , its empirical identification is still an open problem , complicated by the fact that modules are typically emergent , in the sense that they are not evident _ a priori _ from a local inspection of static , or even dynamic , similarities or connections among individual units . in neuroscience , for instance ,",
    "` functional brain networks ' are precisely defined by the correlated dynamical activity of neurons , as opposed to ` structural brain networks ' which are instead defined by static neuronal connections @xcite .",
    "remarkably , it has been proposed that the observed divergence between functional and structural brain networks represents a signature of the brain s many - to - one ( degenerate ) function - structure relationships which allow diverse functions to arise from a static neuronal anatomy @xcite .",
    "similarly , in the analysis of financial markets it has been observed that groups of correlated stocks evolve in time and only partially overlap with industrial sectors , implying that the ( static ) industrial classification fails to capture the dynamical modularity of real markets  @xcite .",
    "the approaches proposed so far to infer some form of modular or hierarchical organization from multiple time series are based on ( necessarily arbitrary ) criteria used to filter information @xcite .",
    "as we discuss in more detail below , these filtering criteria are either the introduction of thresholds or a geometric embedding in some metric space with pre - defined properties .",
    "our aim in the present paper is that of going beyond the limitations imposed by these arbitrary criteria .",
    "we propose that , both conceptually and algorithmically , the identification of mesocopic modules whose dynamical activity is more correlated internally than with that of other modules , requires iterated recursions into many attempted partitions of the system , an inherently non - local operation . by their nature ,",
    "threshold - based or geometric methods are unfortunately not suited to deal with this sort of iterative partitioning problem .",
    "our strategy towards a solution is the adaptation of a different class of rapidly developing techniques , specifically those aimed at identifying the static mesoscopic organization in complex networks , a problem known as _ community detection _ @xcite .",
    "communities within networks are groups of nodes that are more densely connected to each other than would be expected under a suitable null hypothesis . additionally , the nodes within a community are less connected to the nodes within other communities of the same network .",
    "several methods have been proposed over the last decade in order to empirically detect communities within networks .",
    "different techniques have explored different ways to optimize the search over all possible partitions of the system .",
    "conceptually , these methods contain precisely the ingredients that we need in order to solve our problem of identifying the hidden mesoscopic organization encoded within multiple time series .",
    "adapting the existing community detection techniques to deal with time series data is the main goal of this paper .",
    "while the idea of using community detection algorithms in order to analyse time series data has been already exploited a few times in the past @xcite , the attempts made so far have basically replaced network data with cross - correlation matrices . here",
    "we show that this procedure suffers from the limitation that the underlying null hypotheses used in network - based community detection algorithms are inconsistent with the properties of correlation matrices .",
    "we illustrate that one of the undesired consequences is a systematic bias in the search over partitions , that becomes stronger as the heterogeneity of the size of the ` true ' communities increases .    here",
    "we propose a solution to this problem by introducing appropriate redefinitions of the so - called _ modularity _ @xcite , the core quantity that most methods aim at maximizing when searching the space of possible partitions . while in ordinary community detection methods the modularity is defined in terms of a _",
    "null model _ that is ( approximately ) correct for networks , in the methods we propose the modularity is defined in terms of different null models that are appropriate for time series data and therefore dictated by random matrix theory ( rmt ) @xcite .",
    "we also adapt three popular algorithms that have been proposed to find the optimal partition ( in networks ) , i.e. the one that maximizes the modularity . as a result ,",
    "we end up with three community detection algorithms that are consistent with time series data and represent the counterparts of the most popular techniques used in network analysis .",
    "we also provide extensions to resolve hierarchically nested subcommunities ( multiresolution community detection ) and ` hard ' cores versus ` soft ' peripheries inside communities ( multifrequency and time dependent community detection ) .    after introducing our theoretical framework",
    ", we put special emphasis on financial applications , where the units of the system are assets and the corresponding time series are sequences of logarithmic price increments @xcite .",
    "even though advanced techniques to analyse correlations have been developed in other fields as well , financial time series analysis is one of the most active domains in this respect ( another important example is that of functional brain networks , as we have already mentioned ) .",
    "we show that our methods allow us to efficiently probe the mesoscopic structure of different financial markets and ascertain communities of corporations , based on the time series of their daily stock returns .",
    "we uncover a variety of correlations between stocks of different industry sectors , not intuitively obvious from the sectorial taxonomy alone , thus confirming in a more rigorous manner the aforementioned result that market correlations only partially overlap with industry classifications .",
    "more importantly , the communities we detect after removing noisy and market - wide dependencies turn out to be internally correlated and mutually anti - correlated , a feature of particular relevance for portfolio optimization and risk management .",
    "we also analyse the stability of communities over different frequency resolutions and time horizons , thereby identifying groups of ` hard stocks ' that reside stably in the core of communities and groups of ` soft stocks ' that alternate between communities .",
    "the rest of the paper is organized as follows : in section [ sec : existing ] we briefly describe the most important approaches that have been proposed in order to filter correlation matrices and highlight their issues with characterizing the modular properties of systems described by multiple time series . in section",
    "[ sec : inconsistencies ] we show that the existing community detection algorithms are based on a null hypothesis that is inconsistent for time series data , making these methods inadequate as well . in sec .",
    "[ sec : methods ] we then introduce alternative and appropriate null models based on rmt , and exploit them in order to redefine three of the most popular community detection algorithms , in a way that makes them consistent with time series data . in sec .",
    "[ sec : results ] we apply our methods to several time series of daily stock returns , from various financial markets around the globe . in sec .",
    "[ sec : resol ] we analyse the dependence of community structure on the temporal resolution ( i.e. the frequency ) of the original time series . in sec .",
    "[ sec : dyn ] we investigate the evolution of community structure over time . finally , in sec .",
    "[ sec : conclusions ] we summarize our results and provide some conclusions .",
    "we start by introducing some useful notation .",
    "let us consider a system with @xmath0 units .",
    "the single time series @xmath1 represents the temporally ordered activity of the @xmath2-th unit of the system over @xmath3 timesteps . in the case of financial markets , @xmath2 is typically one particular stock and @xmath4 is the ` log - return ' of stock @xmath2 , i.e. the difference between the logarithms of the price of @xmath2 at times @xmath5 and @xmath6 ( more details will be given later ) .",
    "the whole set of @xmath0 time series , denoted by @xmath7 , describes the synchronous activity of all the units of the system .",
    "the vast majority of the available techniques aimed at quantifying the level of mutual dependency within such a set of multiple time series exploit the information encoded in the @xmath8 _ cross - correlation matrix_. the cross - correlation matrix @xmath9 measures the mutual dependencies among @xmath0 time series on a scale between @xmath10 and @xmath11 .",
    "the @xmath12 entry of @xmath9 is defined as the pearson correlation coefficient @xmath13 \\equiv\\frac{\\textrm{cov}[x_i , x_j]}{\\sqrt{\\textrm{var}[x_i]\\cdot \\textrm{var}[x_j ] } } , \\label{eq : corrg}\\ ] ] where @xmath14\\equiv \\overline{x_i x_j}-\\overline{x_i}\\cdot\\overline{x_j } \\label{eq : cov}\\ ] ] is the covariance of @xmath15 and @xmath16 and @xmath17\\equiv\\sigma^2_i\\equiv\\overline{x_i^2}-\\overline{x_i}^2=\\textrm{cov}[x_i , x_i]\\ ] ] is the variance of @xmath15 . in the above equations , the bar denotes a temporal average , i.e. @xmath18 clearly , the diagonal entries of the correlation matrix are @xmath19 .    we will assume , as routinely done in order to filter out the intrinsic heterogeneity of time series , that each series @xmath15 has been _ standardized _ by subtracting out the temporal average @xmath20 and dividing the result by the standard deviation @xmath21 , i.e. that @xmath15 has been redefined to @xmath22 .",
    "then the following expressions hold : @xmath23= \\overline{x^2_i}=1,&\\\\ & { c}_{ij}=\\textrm{cov}[x_i , x_j ] = \\overline{x_i x_j}. & \\label{eq : corrs}\\end{aligned}\\ ] ]    note that , despite in statistics the notation @xmath24 $ ] , @xmath25 $ ] or @xmath26 $ ] usually denotes a _ population _ value , i.e. a theoretical value calculated using the knowledge of the ( joint ) probability distributions for @xmath15 and @xmath16 , all quantities we have defined so far are instead _ sample _ quantities , i.e. measured on the specific realized values of a set of time series .",
    "our choice of a somewhat unconventional notation is merely due to the fact that it allow us to describe various operations more compactly .",
    "we will need to denote the population value of a quantity only in a few cases , and when this happens such population value will coincide with the expected value @xmath27 ( over the joint probability distribution of the random variables @xmath28 involved ) of the corresponding sample quantity @xmath29 .",
    "we will therefore directly express population quantities in terms of expected values when necessary .",
    "we stress that empirical cross - correlation matrices are intrinsically limited by the fact that they assume _ temporally stationary _ and _ linearly interdependent _ time series .",
    "clearly , both assumptions are in general violated in real financial markets and many other complex systems .",
    "nonetheless , cross - correlations are still the most widely used quantity .",
    "improving the definition of correlations is a very important open problem , but is beyond the scope of this paper . here , we want to overcome the limitations encountered when the methods introduced so far to process or filter correlation matrices are used in order to identify a mesoscopic modular structure .",
    "these current limitations are in place even when correlations are an appropriate measure , i.e. for stationary and linearly interdependent time series .",
    "therefore , our goal is that of introducing a consistent methodology that makes optimal use of correlation matrices in order to resolve the mesoscopic organization of complex systems .",
    "if improved measures of interdependency are introduced , our approach will still represent a valuable guideline in order to implement a consistent community detection framework in that case as well .",
    "in what follows , we review the most important correlation - based approaches and their limitations .",
    "we will put special emphasis on financial time series , even if our discussion is more general .      among the proposed approaches to filter cross - correlation matrices ,",
    "the simplest one is perhaps that of focusing on the strongest ( off - diagonal ) correlations by introducing a threshold value and discarding all the correlations below the threshold .",
    "the result can be represented as a network , also known as an _ asset graph _ ( ag ) in the econophysics literature @xcite , connecting the nodes whose time series are more strongly correlated . since the method entirely depends on the choice of the threshold , one usually investigates how the properties of the ag change as the threshold is varied .",
    "the method is quite robust to noise , precisely because it discards the weakest correlations that are more subject to random fluctuations .",
    "however , for the same reason it fails in detecting a mesoscopic organization ( if present ) of the system .",
    "in fact , the use of a global threshold prevents the identification of modules whose internal correlations , even if below the threshold because they are weak with respect to the strongest ones , are still significantly stronger than the external correlations with different modules .",
    "therefore , while valuable as a filtering technique , the ag discards a significant amount of information and is not best suited to detect emergent groups of correlated time series .",
    "we provide additional information about ags , along with an explicit example , when we analyze real financial data in sec.[sec : standard ] .",
    "another filtering approach looks for the _ minimal spanning tree _ ( mst ) obtained again from the strongest correlations , but now retaining only the @xmath30 correlations that are required for each node to be reachable from any other node via a connected path , while discarding those that produce loops @xcite .",
    "this procedure automatically produces an agglomerative hierarchical clustering ( a dendrogram ) of the original time series and requires that the correlation matrix is ` renormalized ' at each iteration of the clustering according to some protocol ( the one having some distinct theoretical advantage is the so - called single - linkage clustering algorithm @xcite ) , until a final filtered matrix is obtained .",
    "the mst method does not require the introduction of an arbitrary threshold , but it assumes that the original correlations are well approximated by the filtered ones .",
    "at a geometrical level , this corresponds to the assumption that the metric space in which the original time series are embedded ( via the definition of a proper correlation - based distance ) effectively reduces to a so - called _ ultrametric _ space where well - separated clusters of points are hierarchically nested within larger well - separated clusters @xcite .",
    "even if the method exploits the correlations required for the mst to span the entire set of time series , it discards all the weaker correlations .",
    "moreover , the approximating ( renormalized ) correlations are progressively more distant from the original ones as higher and higher levels of the taxonomic tree are resolved .",
    "this means that the method is more reliable when using the strongest correlations to determine the low - level structure of the taxonomic tree ( small clusters of time series ) , while it is progressively less reliable when using the weaker correlations to determine the high - level taxonomy ( medium - sized and large clusters ) .    with the above warning in mind ,",
    "the method allows one to identify correlated groups of stocks lying on separate ` branches ' of the mst or that become disconnected when the associated dendrogram is cut at some level .",
    "however , this comes at the price of introducing an arbitrary threshold on the value of the correlation again .",
    "moreover , just like the ag technique , the mst one does not compare internal and cross - group correlations ( possibly with the aid of a null model ) in order to identify emergent mesoscopic modules .      an alternative approach , which is similar in spirit to the mst but discards less information , is the so - called _ planar maximally filtered graph _ ( pmfg ) @xcite .",
    "this method allows one to retain not just the correlations required to form the mst , but also a number of additional ones , provided that the resulting structure is a _ planar graph _ ( a network that can be drawn on a plane without creating intersecting links ) .",
    "a nice feature of the pmfg is that it always contains the entire mst , so that the former provides additional , and not just different , information with respect to the latter .",
    "however , also this method is affected by some degree of arbitrariness , which lies again in the properties of the postulated , approximating structure .",
    "there is no obvious reason why stocks ( or other time series ) should find a natural embedding in a bidimensional plane .",
    "in fact , the pmfg has also been described as the simplest case of a more general procedure based on the embedding of high - dimensional data in lower - dimensional manifolds with a controllable _ genus _",
    "( number of ` handles ' or ` holes ' ) @xcite .",
    "the pmfg corresponds to the case when the genus is zero .",
    "so the arbitrariness of the method can be rephrased as its dependence on some value of the genus that must be fixed _ a priori_.",
    "the method has been extended in a variety of ways in order to produce a nested hierarchy of time series by exploiting the properties of the embedding space @xcite . however , as with the mst , the target of these methods is that of finding the postulated approximating structure , rather than optimizing the search of groups of time series that are more correlated internally than with each other .",
    "we finally mention an important technique , based on random matrix theory ( rmt )  @xcite , which is widely used in order to identify the non - random properties of empirical correlation matrices .",
    "we will use this technique extensively in the paper .",
    "a correlation matrix constructed from @xmath0 completely random time series of duration @xmath3 has ( in the limits @xmath31 and @xmath32 with @xmath33 ) a very specific distribution of its eigenvalues , known as the marcenko - pastur or sengupta - mitra distribution  @xcite .",
    "this distribution reads @xmath34 and @xmath35 otherwise , where the maximum ( @xmath36 ) and minimum ( @xmath37 ) eigenvalues are given by @xmath38 ^ 2 .",
    "\\label{eq : lambda+-}\\ ] ] the bulk of the eigenvalues of an empirical correlation matrix that fall within the range @xmath39 $ ] can be considered to be mostly due to random noise .",
    "thus , any eigenvalues larger than the maximum eigenvalue @xmath36 predicted by the marcenko - pastur distribution are deemed to represent meaningful structure in the data  @xcite .",
    "that being the case , any empirical correlation matrix @xmath9 can be decomposed as the sum of two matrices : @xmath40 where ( using @xmath41 and @xmath42 notation ) @xmath43 is the ` random ' component constituted from the eigenvalues @xmath44 less than or equal to @xmath36 ( usually , the eigenvalues smaller than @xmath45 are included as well ) and their corresponding eigenvectors @xmath46 , and @xmath47 is the ` structured ' component constituted from the remaining eigenvalues corresponding to eigenvalues larger than @xmath48 .",
    "the deviation of the spectra of real correlation matrices from the rmt prediction provides an effective way to filter out noise from empirical data , and also illustrates some robust property of financial markets .",
    "for instance , in fig .",
    "[ fig : eigendensity ] we superimpose the eigenvalue density of the empirical correlation matrix obtained from @xmath49 log - returns of daily closing prices of @xmath50 stocks of the s&p 500 index ( from 2001 to 2011 ) and the corresponding expectation given by the marcenko - pastur distribution with the same values of @xmath0 and @xmath3 . as also observed in a multitude of previous studies @xcite , a typical feature of the spectrum of empirical correlation matrices",
    "is that the largest observed eigenvalue @xmath51 is much larger than all other eigenvalues ( see inset of fig .",
    "[ fig : eigendensity ] ) .",
    "the corresponding eigenvector @xmath52 has all positive signs and one can therefore identify this eigencomponent of the correlations as the so - called _ market mode _",
    "@xcite , i.e. a common factor influencing all stocks within a given market . interpreting this ,",
    "the bulk of the correlation between pairs of stocks is attributed to a single common factor , much as all boats in a harbor will rise and fall with the tide .    in order to clearly see which ` boats ' are rising and falling relative to one another",
    ", one must subtract out the common ` tide ' , which in terms of the correlation matrix leads to the further decomposition @xmath53 where we have rewritten the structured component as @xmath54 , with @xmath55 ( representing the ` market ' mode ) and @xmath56 ( representing the remaining correlations ) .",
    "the correlations embodied by @xmath57 act neither at the level of individual stocks ( uncorrelated noise ) , nor at that of the entire market .",
    "such correlations act at the level of sub - groups of stocks within a market , and they are often referred to as the ` group ' mode @xcite .",
    "the eigenvectors contributing to @xmath57 have alternating signs , and this allows the identification of groups of stocks that are influenced in a similar manner by one or more common factors  @xcite . broadly speaking , these groups are expected to reflect some sectorial or sub - sectorial classification of stocks according to their industrial category , however the overlap between nominal asset classes and groups of empirically correlated stocks is only partial  @xcite .",
    "we should at this point stress that the above discussion makes some strong assumptions , which have been recently criticized",
    ". in particular , the interpretation of the largest eigenvalue in terms of a ` market ' mode and the assumption that the elimination of the market and ` noise ' modes does not alter the information present in the remaining subspace are not correct in general , and sometimes only approximate @xcite .",
    "moreover , the eigencomponents of the correlation matrix , and consequently the filtered correlation matrix itself , can end up being not proper correlation matrices , and alternative constructions enforcing the required properties have been proposed @xcite .",
    "finally , the way to filter out the ` market ' and ` noise ' modes is not unique @xcite .",
    "bearing these limitations is mind , rmt is still to be considered a valuable tool to filter empirical correlation matrices and clean them from both stock - level ( random ) and market - wide fluctuations .",
    "however , after this pre - processing , filtered correlation data still needs to be analyzed according to the particular research question .",
    "for instance , the matrix @xmath57 is often processed further and used as an alternative , filtered input in all the algorithms ( ag , mst and pmgf ) described above .",
    "so rmt alone is not enough in order to resolve the mesoscopic organization of markets , in the sense defined above .",
    "in the previous section , we clarified that many of the available techniques used to identify the most relevant correlations are not designed to isolate groups of time series whose dynamical activity is more correlated internally than with that of other groups . at an abstract level , achieving this task would require iterated recursions into many attempted partitions of the system , an inherently non - local and computationally demanding operation .",
    "notably , an entire branch of network science is devoted to an analogous problem , known as _ community detection _ @xcite . in this section ,",
    "we briefly illustrate the principles of community detection in networks and show how that knowledge can be in principle transferred to our initial problem , namely the identification of a mesoscopic organization across multiple time series .",
    "we also show that despite the many progressive inroads made in this direction so far , they often rely on an inherently biased approach .      in network analysis",
    ", community detection is the process of identifying relatively dense clusters of nodes .",
    "there has been a flurry of research in the area of community detection over the last decade @xcite . in this paper",
    "we focus on the method of _ modularity optimization _",
    "@xcite , which is one of the most popular methods identifying _ non - overlapping _ communities .",
    "it should be noted that various alternative methods other than modularity optimization exist , including techniques that resolve overlapping communities @xcite .",
    "however , this method has the advantage of being based on a null model , acting as a community - free benchmark to which the real network is compared .",
    "it is the appropriate modification of such benchmarks that will lead us , in sec.[sec : methods ] , to a redefinition of modularity optimization methods valid for correlation matrices .",
    "we restrict ourselves to undirected networks , since they exhibit the same symmetry property as correlation matrices .",
    "given a network with @xmath0 nodes , one can introduce a number of partitions of the @xmath0 nodes into non - overlapping sets .",
    "each such partition can be mathematically represented by an @xmath0-dimensional vector @xmath58 where the @xmath2-th component @xmath21 denotes the set in which node @xmath2 is placed by that particular partition .",
    "then , one can introduce the so - called _ modularity _ @xmath59 as a measure of the effectiveness of a particular partition @xmath58 in identifying densely connected groups of nodes .",
    "the process of modularity optimization seeks to find the optimal partition that maximizes the value of @xmath59 , by varying the communities to which the different nodes of the network belong .",
    "the modularity @xmath59 is expressed in the form @xmath60 \\delta(\\sigma_i,\\sigma_j),\\ ] ] where , here and throughout the paper , the sum is intended to run over _ all _ pairs of nodes even if we are considering undirected networks , and we are also including the diagonal elements corresponding to @xmath61 since many expressions become simpler with this choice .",
    "the meaning of the different terms of the above expression is as follows .",
    "the delta function is @xmath62 if @xmath63 and @xmath64 if @xmath65 , ensuring that only nodes within the same community contribute to the sum . for binary networks",
    ", @xmath66 is the entry of the adjacency matrix representing the presence ( @xmath67 ) or absence ( @xmath68 ) of a link between nodes @xmath2 and @xmath69 in the observed network .",
    "the initial pre - factor works to normalize the value of @xmath59 between @xmath10 and @xmath11 , where @xmath70 is twice the total number @xmath71 of links .",
    "the term @xmath72 is a key element determining the outcome of the entire community detection process .",
    "it mathematically represents a null model for the network , i.e. an expectation for @xmath66 under some suitable null hypothesis . the most popular null model for a binary network , known as the _ configuration model _",
    ", is one where the expected value @xmath73 of the degree @xmath74 ( number of links ) of each node @xmath2 is equal to the value @xmath75 observed in the real network and where the topology is otherwise completely random .",
    "this null hypothesis ensures that the local heterogeneity of nodes , e.g. the fact that more popular people naturally have more friends in social networks , is appropriately controlled for .",
    "mathematically , this model is approximately ( i.e. only when the heterogeneity of the degrees is weak @xcite ) represented by the expression @xmath76 which gives a rough estimate of the probability that nodes @xmath2 and @xmath69 are connected , _ under the null hypothesis that the observed network s structure is completely explained on the basis of the different degrees of vertices_.    for weighted networks , @xmath66 denotes the weight of the link between nodes @xmath2 and @xmath69 , @xmath74 is called the _ strength _ of node @xmath2 and @xmath77 is twice the total weight ( of all links in the network ) .",
    "still , eq.([eq : null ] ) is used without modifications @xcite to determine the ( again approximate @xcite ) expected edge weight _ under the null hypothesis that the network s structure is completely explained on the basis of the observed strengths of all vertices_.    the accuracy and usefulness of the results obtained from the process of modularity optimization depend heavily on the choice and suitability of the null model .",
    "when the null hypothesis is true , no higher - order patterns ( including communities ) are present .",
    "consistently , one expects the modularity in eq.([qnewman ] ) to be close to zero for every partition . in maximizing the modularity for a network which does have community structure ,",
    "the nodes that are more tightly connected than one would expect on the basis of their individual characteristics will be clustered together in the same community , while the nodes for which the opposite occurs will be placed in different communities .",
    "it should be noted that , in the context of network analysis , the modularity function defined in eq.([qnewman ] ) suffers from a main drawback : it can not resolve communities below a typical scale @xcite .",
    "resolution limit _ was proven to be rooted in the specific mathematical form of eq.([eq : null ] ) used to represent the null model .",
    "however , it was not proven to be due to the concept of the null model itself , i.e. to the choice of comparing the real network with an ensemble of graphs with given degrees ( or strenghts ) .",
    "in particular , we stress again that eq.([eq : null ] ) only approximately represents such an ensemble , the exact formula being a more complicated nonlinear equation @xcite .",
    "whether the resolution limit disappears if the exact expression is used in place of eq.([eq : null ] ) has never been investigated .",
    "rather , it has been proposed @xcite that a way to change the resolution of the community detection is the introduction of an extra resolution parameter @xmath78 in the null model , i.e. replacing eq.([eq : null ] ) with @xmath79 many studies have indeed shown that , as @xmath80 is varied , different hierarchical levels of the community structure can be revealed , so that a so - called _ multiresolution method _ can be obtained @xcite . in general",
    ", multiresolution methods can resolve smaller subcommunities , which are nested inside larger communities .",
    "one should however bear in mind that the resolution parameter was originally introduced in an _ ad hoc _ fashion and without a theoretical foundation , its main justification being an agreement _ a posteriori _ with the hierarchical community structure expected in some real - world networks . only later",
    ", it was shown to have some physical interpretation in terms of an inverse time required to explore the network under certain assumptions @xcite .",
    "when extending modularity - based algorithms to the analysis of multiple time series , we will address the problem of multiresolution community detection in a fundamentally different way , which avoids _ ad hoc _ parameters and is theoretically consistent with the properties of correlation matrices ( see sec .",
    "[ sec : multi ] ) .",
    "the appealing properties of community detection in networks clearly have the potential to solve our initial problem of finding groups of time series that are more correlated than we would expect .",
    "however , one should be very careful in identifying the correlation - based problem with the network - based one .",
    "a nave approach would be that of treating the empirical correlation matrix @xmath9 as a weighted network , and looking for communities using the modularity as defined in eq.([qnewman ] ) , i.e. setting @xmath81 .",
    "this would result in a modularity of the form @xmath82 \\delta(\\sigma_i , \\sigma_j),\\ ] ] where @xmath83 , @xmath84 and @xmath85 .",
    "this idea has been recently exploited , sometimes with modifications , to study communities of interest rates @xcite and stocks @xcite in financial markets .",
    "unfortunately , although the above approach has made a lot of headway , it suffers from some fundamental flaws and can lead to biased results , as we now show .",
    "the problem arises because the null model defined in eq.([eq : null ] ) , while ( approximately @xcite ) valid when the matrix @xmath86 describes a network , is inconsistent if @xmath86 is replaced by a correlation matrix @xmath9 . to see this ,",
    "note that if @xmath81 and if @xmath15 denotes a standardized time series @xmath2 ( see sec .",
    "[ sec : existing ] ) , then eq.([eq : corrs ] ) implies @xmath87=\\textrm{cov}[x_i , x_{tot } ] , \\label{eq : ki}\\ ] ] where @xmath88 is the time series of the total increment @xmath89 .",
    "note that , even if all @xmath15 s are standardized , @xmath90 has zero mean but non - unit variance , and is therefore _ not _ standardized .",
    "similarly , @xmath91=\\textrm{var}[x_{tot } ] . \\label{eq:2m}\\ ] ] it then follows that @xmath92\\cdot \\textrm{cov}[x_i , x_{tot}]}{\\textrm{var}[x_{tot}]}\\nonumber\\\\ & = & \\textrm{corr}[x_i , x_{tot}]\\cdot \\textrm{corr}[x_j , x_{tot } ] .",
    "\\label{eq : badnull1}\\end{aligned}\\ ] ]    we therefore arrive at an important conclusion : for correlation matrices , the ` nave ' modularity as ordinarily defined in eq.([qnewman ] ) with the ordinary specification given in eq.([eq : null ] ) corresponds to the following null hypothesis : @xmath93\\cdot \\textrm{corr}[x_j , x_{tot } ] .",
    "\\label{eq : badnull2}\\ ] ] when used within the modularity function , the above null model will not necessarily give more importance to pairs of strongly correlated time series , but rather to pairs of time series whose ` direct ' correlation @xmath94 is larger than the product of the correlations of each time series with the ` common signal ' @xmath90 . on the other hand , if we want to detect communities of time series that are empirically more correlated than expected under the hypothesis that all time series are independent of each other , we know that the correct null model ( at least for infinitely long time series , a hypothesis that we will relax later ) is @xmath95 i.e. the expected correlation matrix @xmath96 should be the @xmath8 identity matrix @xmath97 .",
    "other acceptable forms of @xmath98 based on realistic properties of correlation matrices will be discussed later .",
    "the origin of the problematic discrepancy between eq.([eq : badnull2 ] ) and eq.([eq : goodnull ] ) is the fact that the null model defined in eq.([eq : null ] ) is meant to represent networks with given degrees , i.e. matrices with given column and row sums .",
    "any matrix that matches this constraint is admissible , in the sense that it represents a possible network consistent with the hypothesis that degrees are an important structural constraint .",
    "by contrast , sums over rows or columns of correlation matrices do not represent any meaningful constraint , as evident from eq.([eq : ki ] ) .",
    "moreover , not every symmetric real matrix with given row and column sums is a possible correlation matrix .",
    "correlation matrices must also be _ positive - semidefinite _ , i.e. have non - negative eigenvalues .",
    "a little algebra shows that eq .",
    "fulfills this property , but in a very extreme way : the eigenvalues of the matrix having elements @xmath99 are @xmath100 ( with multiplicity @xmath30 ) and @xmath101\\big)^2 \\label{eq : eigenvalue}\\ ] ] ( with multiplicity @xmath11 ) .",
    "this result holds irrespective of the original data , e.g. also for correlated and finite - length time series .",
    "our discussion of the spectrum of realistic correlation matrices in sec .",
    "[ sec : rmt ] strongly indicates that a sensible null model for correlation matrices should feature an eigenvalue distribution that is not easily reducible to the extremely simple one found above .",
    "similar conceptual limitations are encountered also in more sophisticated null models , which while allowing for both positive and negative link weights @xcite , still consider all possible matrices ( many of which are inconsistent with correlation matrices ) with given sums over rows and columns .",
    "more importantly , the above problems can not be solved by the introduction of resolution parameters . if , in analogy with eq.([eq : multinull ] ) , we consider the generalized null model @xmath102\\cdot \\textrm{corr}[x_j , x_{tot } ] \\label{eq : badmultinull}\\ ] ] ( with @xmath78 ) , we are still left with an expression that can not be reduced to eq.([eq : goodnull ] ) or some other meaningful alternatives , which we will introduce later in sec .",
    "[ sec : ourmod ] .",
    "for instance , the eigenvalues become @xmath103 , where @xmath104 still takes only the two values shown above . further aspects of this limitation are explicitly illustrated in a benchmark case below , and imply that appropriate multiresolution community detection methods for correlation matrices should be implemented in a completely different way ( see sec . [",
    "sec : multi ] ) .",
    "to have an idea of the consequence of using the nave approach , i.e. the application of a network - based modularity directly to a cross - correlation matrix , we consider an ideal benchmark case where @xmath0 _ infinitely long _ time series are divided into @xmath105 ` true ' communities , specified by a ` true ' partition @xmath106 .",
    "we assume that each community @xmath107 is made of @xmath108 standardized time series ( with @xmath109 ) that are perfectly correlated with each other and completely uncorrelated to the time series in other communities , i.e. @xmath110=\\textrm{cov}[x_i , x_j]=\\delta(\\sigma_i^*,\\sigma_j^*).\\ ] ] in such a case , @xmath111=\\sum_{j=1}^n \\textrm{cov}[x_i , x_j]=n_{\\sigma_i^*}\\ ] ] ( where @xmath112 is the number of time series in the community of the time series @xmath2 ) and @xmath113=\\sum_{i , j}\\textrm{cov}[x_i , x_j]=\\sum_{a=1}^c n^2_{a}.\\ ] ]    from the last two equations it follows that eq.([eq : badnull2 ] ) , or more generally eq .",
    ", can be rewritten as @xmath114 ( with @xmath78 ) , which is the fundamental result showing the inconsistency of the nave approach , and the nature of the resulting bias .",
    "equation ( [ eq : badnull3 ] ) can never lead to the correct expectation ( [ eq : goodnull ] ) because it can not produce off - diagonal zeros .",
    "if there are @xmath105 equally sized communities of @xmath115 time series each , then @xmath116 for all @xmath117 , i.e. the distribution of @xmath99 has a single peak and zero standard deviation . in this case , apart from the minor in the summation defining @xmath118 .",
    "however , for heterogeneously sized community the bias of the nave approach can not be eliminated . ]",
    "problem of non - unit diagonal entries , the use of @xmath99 in eq.([qnewman ] ) can still be justified on the basis of the fact that @xmath119 is a constant term having no effect on the modularity maximization . however , for heterogeneously sized communities , eq.([eq : badnull3 ] ) does not lead to a mere overall shift in the modularity . as the size heterogeneity increases",
    ", the distribution of the off - diagonal entries of @xmath99 will become broader .",
    "in general , @xmath99 is larger for pairs of time series belonging to larger communities .",
    "this effect is shown in fig .",
    "[ fig : offdiag ] for three choices of benchmark communities .",
    "the above consideration implies that the standard deviation ( irrespective of the average ) of the off - diagonal ( @xmath120 ) entries of @xmath99 can be taken as a quantitative measure of the _ bias _ induced by eq .. this definition depends linearly on the multiresolution parameter @xmath80 .",
    "alternatively , the _ coefficient of variation _ ( standard deviation divided by average value ) of the off - diagonal entries of @xmath99 is a measure of the _ relative bias _ of the nave approach , and is independent of @xmath80 .",
    "one should bear in mind that when the value of the coefficient of variation is much lower than one , the heterogeneity is moderate while when it approaches or exceeds one then the heterogeneity is such that the average value is no longer representative of the distribution .    in fig .",
    "[ fig : bias ] we show both the bias ( for @xmath121 ) and the relative bias as a function of size heterogeneity , the latter being in turn defined as the coefficient of variation of community size .",
    "we see that the ( relative ) bias first steadily increases as the size heterogeneity increases from zero to approximately two , and then decreases when the heterogeneity further increases .",
    "this decrease corresponds to entering an extremely heterogeneous regime where there is a giant community of @xmath122 nodes , and other very small communities of @xmath123 nodes . in this regime ,",
    "the effective number of communities is practically one and the distribution of @xmath99 becomes sharp again , as most entries have the same value .",
    "so , for a very broad range of heterogeneity ( say , when the coefficient of variation of community sizes is between 0.5 and 2.5 ) , the ( relative ) bias is very strong . in this regime , eq.([eq : badnull3 ] ) gives a prediction @xmath124 ( close to the correct expectation ) only for pairs of time series belonging to the smallest community . for such time",
    "series the difference @xmath125 is still close to one , and one therefore expects that the smallest community will be detected correctly .",
    "however , for time series belonging to larger communities @xmath99 increases , progressively biasing the community detection . for the largest community",
    ", the expected internal correlation is always larger than the correlation among any pair of communities , so @xmath125 is very low and this community is paradoxically difficult to detect .    it should be noted that the use of the multiresolution parameter @xmath80 does not help reduce the relative bias , as the latter is independent of @xmath80 . in order to reduce the absolute bias ( which for @xmath121 has values around @xmath126 in the relevant regime , see fig .",
    "[ fig : bias ] ) to small values ( say of the order of @xmath127 ) , @xmath80 should be set to very small values ( around @xmath128 ) , which is another way of saying that the null model in eq . should effectively be replaced by that in eq .",
    "( we recall that we are referring only to the off - diagonal entries here ) , thus confirming our previous discussion .",
    "the above results lead us to conclude that the ordinary definition of modularity , even with the introduction of a multiresolution parameter , can not properly detect communities .",
    "this limitation would systematically bias any modularity - based community detection algorithm .",
    "it is therefore clear that ordinary network - based clustering methods , when used with correlation matrices , lead to incorrect results . in the rest of the paper",
    ", we try to overcome this limitation .",
    "time series and @xmath129 communities .",
    "the bias is defined as the standard deviation ( coefficient of variation ) of the off - diagonal entries @xmath99 of the null model defined in eq . with @xmath121 , while the relative bias is defined as the coefficient of variation of the same entries ( and is independent of @xmath80 ) . ]",
    "we now come to the most pertinent of our results , i.e. the introduction of improved and consistent methods to cluster multiple time series using appropriate null models . in sec.[sec : ourmod ] we give three redefinitions of the modularity @xmath59 that make use of the results of rmt , which we summarized in sec.[sec : rmt ] . in sec.[sec : redef ] we introduce the correlation - based counterparts of three of the most popular community detection algorithms used in network analysis . in sec.[sec : multi ] we discuss how these algorithms can be further extended in order to obtain appropriate , multiresolution community detection methods . finally , in sec.[sec :",
    "bench ] we benchmark our methods on various test cases .      from our previous discussion",
    "it should be clear that simply replacing network data with correlation matrices in eq.([qnewman ] ) leads to eq.([eq : qcorr ] ) where @xmath130 is @xmath131 and the null model @xmath98 is incorrectly given by eq.([eq : badnull2 ] ) .",
    "we now introduce three redefinitions of modularity based on appropriate null models .",
    "the end result of this redefinition will be a set of modularity functions that correctly identify communities of correlated time series . for compactness",
    ", we postpone the possible ( re)definition of @xmath130 to the end of this discussion , in sec . [",
    "sec : unif ] .      we have already noted that , for infinitely long time series , the correct expression corresponding to the null hypothesis of independency is given by eq.([eq : goodnull ] ) .",
    "this leads us to a first redefinition of modularity with expectation @xmath132 , i.e. @xmath133 \\delta(\\sigma_i,\\sigma_j)\\nonumber\\\\ & = & \\frac { 1}{c_{norm}}\\sum_{i , j } c_{ij}^{(\\delta)}\\delta(\\sigma_i,\\sigma_j ) , \\label{qcor}\\end{aligned}\\ ] ] where @xmath134 ( @xmath97 being the @xmath8 identity matrix ) , so that @xmath135 .      for finite - length independent time series",
    ", we should further modify our null model to one which anticipates a certain amount of noise , as determined by rmt ( see sec.[sec : rmt ] ) .",
    "in such a case , we know that the correct null hypothesis is @xmath136 where @xmath137 is given by eq.([eq : cr ] ) .",
    "this gives us a second redefinition of modularity for dealing with noisy correlation matrices : @xmath138 \\delta(\\sigma_i,\\sigma_j)\\nonumber\\\\ & = & \\frac { 1}{c_{norm}}\\sum_{i , j } c_{ij}^{(s ) } \\delta(\\sigma_i,\\sigma_j ) , \\label{qnoise}\\end{aligned}\\ ] ] note that now in general @xmath139 as a result of the eigendecomposition defined in eq.([eq : rmt+ ] ) .",
    "however , the diagonal terms with @xmath61 give an irrelevant constant contribution to the modularity , due to the fact that @xmath140 for all @xmath2 , independently of the particular partition @xmath58 .",
    "this makes the above definition well defined even in the presence of non - zero diagonal entries .",
    "lastly , we consider the case where we expect an overall level of positive correlation among all time series , or ` global mode ' .",
    "for instance , we have already mentioned that in financial markets the presence of the ` market mode ' ( see sec .",
    "[ sec : rmt ] ) generally results in a positive correlation affecting all pairs of stocks altogether .",
    "the corresponding dominant positive component @xmath141 of @xmath9 would make @xmath59 be maximized by the ( trivial ) partition where all time series are in the same community . in order to detect non - trivial communities , we can choose a null model that includes both the random component of the correlation matrix and the global or market mode , i.e. @xmath142 where @xmath137 and @xmath141 are given by eqs.([eq : cr ] ) and ( [ eq : cm ] ) respectively .",
    "this yields our third and final formulation for the modularity : @xmath143 \\delta(\\sigma_i , \\sigma_j)\\nonumber\\\\ & = & \\frac { 1}{c_{norm}}\\sum_{i , j } c_{ij}^{(g ) } \\delta(\\sigma_i,\\sigma_j ) , \\label{qfinance}\\end{aligned}\\ ] ] in this case as well , @xmath144 as a result of the eigendecomposition defined in eq.([eq : rmt++ ] ) , but this does not affect the outcome of the community detection .",
    "the above definition is now explicitly aimed at detecting mesoscopic communities , which are in between the ` microscopic ' level of unit - specific noise and the ` macroscopic ' level of system - wide fluctuations .",
    "while the existence of the market mode is well established in finance , for other types of time series it might be inappropriate to postulate the existence of a global mode .",
    "however , we also expect that , whenever the use of @xmath145 or @xmath146 yields only a single community , the most plausible reason is the existence of a global mode .",
    "accordingly , we expect that the use of @xmath147 might be the most appropriate way to filter out global dependencies for a variety of systems , not only for financial markets .",
    "moreover , as we discuss at length in sec.[sec : multi ] , iteratively filtering out the global mode from the correlation matrices restricted to individual communities can result in the definition of a useful multiresolution method to resolve multiple hierarchical levels of community structure , if present .",
    "for simplicity in what follows , it is useful to express the three definitions of modularity we gave in eqs .",
    "( [ qcor ] ) , ( [ qnoise ] ) and ( [ qfinance ] ) in unified form : @xmath148 where @xmath149 in what follows , given a choice of @xmath150 we will refer to @xmath151 as the ` filtered ' correlation matrix .",
    "the overall constant @xmath130 has no role in determining the final partition , but it does have a role when different systems , or different snapshots of the same system ( including dynamical analyses of community structure ) , are compared . for simplicity",
    "we keep the same definition as in eq .",
    ", i.e. @xmath152 .",
    "\\label{eq : norm}\\ ] ] this definition implies that the modularity is the sum of intra - community ( filtered ) correlations , divided by the variance of the total increment @xmath90 .",
    "this variance is a natural measure of the _ volatility _ of the system over the considered time window , which in the case of financial time series is an important property of the market . in other words , eq",
    ". automatically controls for the volatility of the system , a feature that is typically desirable when analysing the evolution of ( the community structure of ) wildly fluctuating systems .",
    "however , in some cases it might be interesting to compare the above modularity with one calculated using a different definition of @xmath130 , e.g. one that does not control for the volatility .",
    "it should be noted that the above definition is such that the typical ( for real - world systems like financial markets ) values of the modularity defined in eq . will tend to be much lower than the typical ( for real - world networks ) values of the modularity defined in eq .",
    ", even for systems with well - defined communities .",
    "one should bear this consideration in mind when interpreting the ( maximized ) modularity value as a measure of the strength of community structure in the system . unlike its network counterpart , our definition of the modularity does not quantify the strength of community structure in an absolute scale between @xmath10 and @xmath153 .",
    "it only has a meaning in relative terms , and the more information is contained in the null model , the lower the value of the resulting modularity .",
    "we remind the reader of the fact that , since the results of rmt used in the above definition hold only in the regime where @xmath0 and @xmath3 are both large ( with @xmath154 ) , we require the original time series to respect these conditions . the requirement @xmath154 is sometimes referred to as the ` curse of dimensionality ' in the literature , since it implies that , in order to study the cross - correlations of a large set of time series , one needs to extend the time interval so much that the assumption of stationarity ( implicit , as we mentioned , in the definition of cross - correlations themselves ) is violated . on the other hand , choosing sufficiently short time intervals to make the time series approximately stationary implies that the number @xmath0 of time series be severely reduced .",
    "one should therefore choose the data in such a way that a reasonable compromise is achieved .",
    "this is an ordinary trade - off to be made in the analysis of any empirical ( financial ) cross - correlation matrix .",
    "we finally stress that the three rmt - based null models we have adopted do not represent the only possible choices .",
    "one might for instance exploit more sophisticated results @xcite and introduce refined null models that overcome some of the limitations of rmt that we mentioned in sec . [",
    "sec : rmt ] .",
    "these alternative choices can then be incorporated into our approach by redefining @xmath155 and consequently @xmath151 .",
    "exploring the entire space of possibilities is beyond the scope of this paper .",
    "the key point we are stressing here is that , whatever the choice of the null model , it must respect some realistic properties of correlation matrices .",
    "the network - based definition of modularity , which has been used so far , does not do so and as such is not the best choice .",
    "our approach can therefore be considered as a guideline , in order to introduce improved techniques in the future .",
    "the discussion so far completes our first task of introducing modularity functions which are consistent with the properties of correlation matrices .",
    "our second task is that of incorporating the above definition(s ) into community detection algorithms that seek to maximize the modularity .",
    "below , we start by briefly mentioning the algorithms we adapted in order to search for the optimal partition ( more extended descriptions are in the appendix ) and then prove an important property of the optimal partition itself , namely the fact that its communities are internally positively correlated and mutually negatively correlated .",
    "given our new definition of modularity in eq.([eq : qunified ] ) , we can not directly apply the traditional optimization algorithms devised for graphs , since the majority of these algorithms rely in some way or another on the properties of the original network - based definition of modularity , where the degrees of nodes are used to construct the null model . for this reason",
    ", we selected three of the most popular network - based community detection algorithms and reformulated them to be compatible with time series data and our new definition of modularity .",
    "the three algorithms we selected are known as the potts ( or spin glass ) method @xcite , the louvain method  @xcite and the spectral method  @xcite .",
    "note that even if these techniques are customarily referred to as ` methods ' , they can actually be considered as three different algorithms implementing the same method of modularity maximization .",
    "since the appropriate redefinition of these algorithms can require quite technical discussions , it is described in the appendix .",
    "we note that there exist many modularity maximization algorithms , some of which may already be much better suited to our definition of modularity .",
    "however , we wanted to choose popular algorithms whose original specifications required varying levels of rework , ranging from verification of its suitability to accommodate time series based modularity through to modifications of the underlying tenets of the algorithm itself .",
    "doing so , allows us the possibility to illustrate further differences between network - based and correlation - based community detection problems .",
    "the reader is again referred to the appendix for a detailed discussion of these differences .",
    "we now prove the result that the partition maximizing the modularity ( whichever method is used to search for it ) is characterized by positive intra - community ( filtered ) correlations and negative inter - community ( filtered ) correlations .",
    "let us first define the ` renormalized ' inter - community correlations ( also see the appendix ) @xmath156 where the notation @xmath157 indicates that the node @xmath2 belongs to the community @xmath107 , and the sum is over all such nodes .",
    "now , assume that we have identified the optimal partition maximizing the modularity , and consider the modularity change @xmath158 that would be obtained by further merging two different communities of the optimal partition , say @xmath107 and @xmath159 . from eq . ,",
    "we can write this change as @xmath160 -\\big[\\tilde{c}^{(l)}_{aa}+\\tilde{c}^{(l)}_{bb}\\big]\\nonumber\\\\ & = & 2\\tilde{c}^{(l)}_{ab}.\\end{aligned}\\ ] ] the above change can not be positive , otherwise merging @xmath107 and @xmath159 would further increase the modularity , which is impossible since @xmath107 and @xmath159 are communities of the optimal partition . therefore @xmath161 which also implies @xmath162 . on the other hand , for every community @xmath107 of the optimal partition we must have @xmath163 , otherwise @xmath107 would give a negative contribution to the modularity , which is impossible as the partition where all nodes of @xmath107 are isolated communities would have higher modularity than the optimal partition . taken together , these considerations imply that @xmath164 the above result follows simply from the maximization of eq.([eq : qunified ] ) and will be confirmed empirically in sec.[sec : anticorrelation ] .",
    "so our algorithms effectively partition the network into mutually anti - correlated communities of positively correlated time series , where it is intended that the term ` ( anti-)correlated ' refers to the residual correlations remaining after applying the filtering procedure defined by eq.([eq : l ] ) . for this reason",
    ", we will sometimes use the term ` residually ( anti-)correlated ' when referring to the sign of filtered correlations . as we will discuss in more detail in sec.[sec : anticorrelation ] , this property has important consequences for portfolio optimization and risk management .",
    "we now come to the problem of introducing an appropriate multiresolution method .",
    "as we mentioned , one way to resolve a hierarchical community structure in ordinary networks using a modularity - based community detection algorithm is that of introducing a resolution parameter @xmath80 as in eq.([eq : multinull ] ) .",
    "we have already noted , in our discussion of eq.([eq : badmultinull ] ) , that the same operation would not cluster correlation matrices appropriately if applied to the nave null model appearing in eq.([eq : badnull2 ] ) .",
    "the same kind of limitation persists if we introduce a resolution parameter multiplying any of the three improved null models @xmath165 defining eq.([eq : qunified ] ) through eq.([eq : l ] ) .",
    "while the range of any observed correlation coefficient @xmath94 is @xmath166}$ ] by construction , a resolution paramater would unreasonably map the range of the expected correlation @xmath98 to @xmath167}$ ] .",
    "similarly , since the null correlation matrices @xmath165 we introduced are obtained from the eigencomponents of the observed correlation matrix @xmath9 , rescaling them by @xmath80 is equivalent to an overall rescaling of the corresponding eigenvalues of @xmath9 , which is again an unjustified operation .",
    "given the above limitations , which indicate a lack of theoretical foundation for resolution parameters in the case of correlation matrices , we introduce a completely different multiresolution approach that is specifically designed for multiple time series , and has no counterpart in network analysis .",
    "after running one of our newly introduced community detection algorithms on the original empirical correlation matrix @xmath9 , for each community of size @xmath168 in the optimal partition we consider the corresponding @xmath169 sub - matrix @xmath170 of @xmath9 . for this sub - matrix",
    ", we define the three null models @xmath171 as discussed in sec.[sec : ourmod ] for the original matrix @xmath9 . by running our community detection algorithms recursively inside each of the communities",
    ", we can thus resolve subcommunities within communities .",
    "iterating this procedure identifies a hierarchical community structure , if present . within each community",
    ", the procedure stops automatically when it resolves no further subcommunities .    at each iteration ,",
    "the ` noise ' component @xmath172 will have the same interpretation as when it is identified on the entire correlation matrix , since @xmath170 is the sub - matrix of the original matrix @xmath9 and _ not _ of the filtered matrix @xmath151 defined in eq.([eq : l ] ) , so it still contains the node - specific noise component ( the reason why we do not consider the sub - matrix of @xmath151 is because , as we mentioned , the latter may not be a proper correlation matrix @xcite and can not thus be filtered further using rmt ) .",
    "the ` global ' mode @xmath173 is now interpreted as the ` community ' mode , i.e. a common factor influencing all the time series within that particular community .",
    "this will now include both the system - wide mode @xmath141 , restricted to the subspace relative to @xmath170 , that would be identified on the entire matrix @xmath9 ( e.g. , in the case of financial time series , the market mode ) _ and _ a genuinely community - specific mode not shared with the time series in other communities .",
    "different communities are therefore possibly characterized by different community modes , and the fact that both this mode and the restriction of the global mode are filtered out is precisely what allows the algorithm to resolve deeper hierarchical modules .",
    "finally , the ` group ' component @xmath174 represents the effect of subgroups nested within the specific community , if present .",
    "it should be noted that the original correlation matrix @xmath9 typically has large dimensionality ( large @xmath0 ) , a property ensuring that the results of rmt , in particular the expected eigenvalue distribution appearing in eq .",
    ", hold to a satisfactory level . however , when considering smaller subcommunities , rmt becomes less reliable because eq",
    ". no longer holds for small sets of nodes . for this reason , for small submatrices ( low - dimensional @xmath170 ) it is preferable to determine the eigenvalues @xmath175 not via eq . , but by randomly shuffling the temporal increments of the original time series and constructing the corresponding spectrum as shown in fig .",
    "[ fig : eigendensity ] .",
    "we conclude by noting that , for the particular case of multiple time series , there is another ` multiresolution ' character , which can be attached to the problem of community detection , namely the fact that different communities can in principle be obtained for different choices of the initial temporal resolution , i.e. for different choices of the frequency of the original time series ( e.g. second , minute , or daily returns ) .",
    "note that this notion of temporal resolution is specific to correlation matrices and has no analogue in the ordinary problem of community detection in networks .",
    "it is also not necessarily attached to an idea of hierarchy , in the sense that we do not expect e.g. communities obtained at higher frequency to be necessarily nested within communities obtained at lower frequency ( even if this can reasonably happen in some cases ) . to distinguish this specific notion from the usual one of multiresolution community detection",
    ", we will refer to it as the ` multifrequency ' problem and address it separately in sec.[sec : resol ] .          before applying our methods to the analysis of real correlation matrices , we ran a series of tests confirming that we can correctly detect correlated sets of time series in controlled benchmark cases .",
    "our benchmarks consist of heterogeneously sized communities of time series that are internally correlated and additionally display varying levels of noise and global signal ( market mode ) .",
    "the reason why we consider heterogeneous community sizes is because this is the more challenging case where we showed the nave method to display a higher bias ( see sec .",
    "[ sec : bias ] ) .",
    "we constructed these benchmarks by first choosing the number @xmath0 of time series , the number @xmath105 of communities and the desired number @xmath108 of time series in each community @xmath107 , such that @xmath109 ( as in sec .",
    "[ sec : bias ] ) .",
    "then we generated @xmath105 random and uncorrelated time series ( with @xmath154 ) with values @xmath176 ( where @xmath177 ) drawn independently from a normal distribution with zero mean and unit variance .",
    "then , we created @xmath108 identical copies of the @xmath107-th time series , for all @xmath107 .",
    "to each of the resulting @xmath0 time series , each labeled by an index @xmath2 , we added a local noise @xmath178 ( a new normally distributed random variable with zero mean and unit variance , independent of all the other ones ) multiplied by a ` noise parameter ' @xmath179 and a global signal @xmath180 ( again , an independent normally distributed random variable with zero mean and unit variance ) multiplied by a ` market - mode parameter ' @xmath181 .",
    "this resulted in a set @xmath182 of @xmath0 time series with values @xmath183 note that this procedure is similar to the so - called ` factor models ' used in financial analysis @xcite .",
    "the time series @xmath182 were further standardized to obtain a final set @xmath184 of @xmath0 time series , each with zero mean and unit variance , in compliance with the general prescription mentioned in sec . [",
    "sec : existing ] .",
    "we generated several benchmarks according to the recipe described above , for various choices of @xmath0 , @xmath105 , @xmath185 , @xmath186 and @xmath187 .",
    "in general , when @xmath188 the benchmark is similar to the ideal one described in sec.[sec : bias ] : the communities are completely correlated internally ( all the time series in the same communities are identical ) and uncorrelated with the time series in other communities .",
    "this results in a benchmark partition @xmath106 such that , for infinite time series , @xmath189 .",
    "however , for finite ( but still such that @xmath154 as prescribed by random matrix theory , see sec.[sec : rmt ] ) time series , @xmath94 will be affected by noise . as @xmath186 and @xmath187 increase , additional noise will be generated and the community structure will be more difficult to detect . if @xmath190 ( @xmath191 ) then the amplitude of the global mode ( local noise ) is the same as that of the community signal .",
    "therefore when @xmath186 and/or @xmath187 approach or exceed one , the community detection problem becomes more challenging .",
    "still , the ambition of our method is that of correctly identifying the benchmark partition @xmath106 even in this ` hard ' regime .    in fig .",
    "[ cghm ] we show nine benchmarks , organized in a @xmath192 table with different combinations of values for @xmath186 and @xmath187 . in all these cases ,",
    "the communities to detect are the same set of @xmath129 heterogeneously sized communities shown previously in fig.[fig : offdiag]c .",
    "the color maps show the values of the entries of the filtered correlation matrix @xmath57 defined in eq . ,",
    "i.e. the residual correlations obtained after removing the noise and market - mode components .",
    "it can be seen that , even for values of @xmath186 and @xmath187 exceeding one , the filtered matrices always display a clear block - diagonal structure with a visible contrast across diagonal and off - diagonal blocks .    in all these benchmarks we confirmed that , using the corresponding modularity @xmath147 defined in eq . , our method succeeded in detecting the correct partition @xmath106 .",
    "we quantitatively measured the performance of our method in terms of a metric known as variation of information ( @xmath193 )  @xcite , which measures the entropy difference between two partitions of the same network , providing a rigorous way for us to quantify the similarity between the ` true ' partition and the one identified by our method . more precisely , @xmath193",
    "involves the use of shannon s entropy to measure the amount of uncertainty that exists across the set of communities of two different partitions of the same network .",
    "it provides a quantitative measure of the difference between two partitions , a normalized value where zero implies the two partitions are completely identical and one implies that they are completely unrelated . as can be seen from fig .",
    "[ cghm ] , the values of @xmath193 ( averaged over multiple runs of the community detection algorithm ) are zero or extremely small , indicating a perfect or almost perfect performance of the method",
    ".    the average ( over multiple runs ) maximum modularity value @xmath194 obtained in the above benchmarks is also illustrated in fig .",
    "lower values of the modularity imply that the network as a whole is more homogeneous in its construction , to the extent that the detected communities exhibit only a relatively weak increase in their collective correlation , above the ambient level . as expected , we see that the modularity decreases for increasing levels of market mode .",
    "increasing levels of noise however do not have such a strong effect , since noisy time series tend to diminish the strength of the intra - community correlations , which enter in both the numerator and denominator of the modularity .",
    "in contrast , the market mode has significant impact on the inter - community correlations , which primarily end up only in the denominator of the modularity .",
    "hence the observed decrease in modularity with an increase in market mode .",
    "the corresponding low values of the modularity confirm what we had anticipated about the effects of eq .. we should bear these effects in mind when interpreting the ( low ) values of the modularity arising from the partition of real financial time series , where the market mode is very strong . the fact that our method correctly identifies the benchmark partitions even for strong market mode ( and low resulting modularity ) makes us confident that it will also properly detect the community structure of real markets .",
    "having redefined the modularity consistently with the properties of correlation matrices and appropriately reconfigured three different techniques for optimizing it , we are now in a position to apply our methodology to a variety of real - world data sets and evaluate the quality of the results",
    ". in particular we will apply our three algorithms and the null model expressed in eq . to time series representing stock prices from a variety of stock indexes that span multiple industries and multiple countries .",
    "we first obtained static results , including the multiresolution community structure as introduced in sec .",
    "[ sec : multi ] , using time series of log - returns of daily closing prices for all the three indexes .",
    "these results are shown in this section .",
    "then , we considered different temporal ( frequency ) resolutions and studied the time dynamics of community structure .",
    "these additional results are described in secs.[sec : resol ] and [ sec : dyn ] respectively .",
    "the indexes we used are the s&p 500 ( us large cap .",
    "stocks ) , the ftse 100 ( british large cap . ) and the nikkei 225 ( japanese large cap . ) . for each of these indexes",
    ", we considered a period of 2500 trading days , corresponding to approximately 10 years of market activity , from 2001q4 to 2011q3 .",
    "we selected all stocks for which complete data are available during this period .",
    "this resulted in the selection of 445 s&p stocks , 78 ftse stocks and 193 nikkei stocks .",
    "all these stocks are classified within the global industry classification standard ( gics ) .",
    "the complete taxonomy can be found online , however we briefly mention that there are ten top - level ` sectors ' ( see table [ tbl : gicscolors ] ) split into 24 sub - categories called ` industry groups ' , which are in turn divided into 68 ` industries ' .",
    ".the 10 industry sectors in the global industry classification standard ( gics ) , with the color representation used to highlight the sectors in the following figures . [ cols=\"<,^,<,^ \" , ]     in figs .",
    "[ fig : indexmetricsa ] and [ fig : indexmetricsb ] we show the value of the maximized modularity @xmath194 and number of detected communities as the result of running all our three algorithms on the filtered correlation matrices for the s&p 500 , the nikkei 225 and the ftse 100 .",
    "we recall from the discussion following eq . and from the benchmarks studied in sec .",
    "[ sec : bench ] that , unlike the corresponding problem in network analysis , our choice of @xmath130 implies very small values of the maximized modularity , even in the presence of well - defined communities , when the market mode is strong .",
    "so the small values of @xmath194 shown in fig .",
    "[ fig : indexmetricsa ] do not imply a poor or weak community structure .",
    "it can be seen from fig .",
    "[ fig : indexmetricsa ] that all three algorithms perform very closely in terms of the maximized modularity value they achieve .",
    "similarly , if we compare the number of communities found by the three methods ( see fig.[fig : indexmetricsb ] ) we find that the number of communities is quite stable as well .    in table",
    "[ vi ] we quantify more rigorously the differences in the composition of the communities detected by the three algorithms , by showing the @xmath193 ( see sec.[sec : bench ] ) among all pairs of algorithms , for all the three indexes .",
    "the values are quite low , indicating that the partitions found by different algorithms are very similar .",
    "we now come to the application of the multiresolution community detection approach we introduced in sec.[sec : multi ] . in the case of financial markets ,",
    "the community - specific correlation responsible for the modular structure shown so far can be regarded , from the perspective of all stocks within one community , as a ` micro market mode ' . just as the market mode discussed",
    "previously is responsible for the collective tide of an entire market , a similar force can be extrapolated at the community level .",
    "as discussed in sec.[sec : multi ] , accounting for this ` community mode ' in the leading eigenvalue and corresponding eigenvector of the correlation submatrix restricted to an individual community allows us to incorporate its effects , together with those of the overall market mode into the null model and again , detect any underlying structure , which surfaces upon the removal of its influence .",
    "figure [ fig : subcommunitiesa ] shows the result of a single layer of recursion into the five communities of the s&p 500 ( depicted previously in fig .",
    "[ fig : spcommunities ] ) .",
    "again , we note that the sub - communities are all residually anti - correlated with each other ( within each parent community ) but maintain an internal positive correlation . although not obvious from the graph , the sub - communities tend to fall along gics industry sector lines , with some interesting exceptions , as before .    to call out a few examples , in community @xmath195 ( fig.[fig : subcommunitiesa ] ) , which contains all of the it stocks , we see the sub - communities separating along industry group and industry lines .",
    "sub - community @xmath196 is comprised of only _ software _ stocks , @xmath197 contains all of the _ semiconductor & semiconductor equipment _ stocks , and @xmath198 contains all of the _ internet software & services _ stocks .",
    "interestingly enough though , @xmath198 also contains amazon inc . and priceline inc . from the consumer discretionary sector , which one could argue are quite aligned with the internet . continuing the analysis further",
    ", we see that in community @xmath199 the finance community sub - community @xmath200 contains all of the _ commercial bank _ stocks , while @xmath201 contains all but one of the _ insurance _ companies .",
    "@xmath202 is exclusively _ real estate investment trusts ( reits ) _ and accounts for all of them .",
    "similar partitions can be seen in the other sub communities and further recursion into these communities produce still further separation , close to but not exactly in line with the gics classification .",
    "figure [ fig : comhierarchy ] depicts the hierarchical nature of the s&p 500 to three layers deep .",
    "the process can be continued until no single community can be partitioned further into any combination of two or more sets which are anti correlated with each other .",
    "for instance , community @xmath203 ( fig.[fig : subcommunitiesa ] ) which contains a variety of stocks from various gics sectors separates out such that the bulk of the stocks in the different sectors find themselves in their own sub community . if we further probe into community @xmath204 , which contains all of the health care stocks we see that the sub - communities ( not shown ) fall very closely along industry lines , with five communities each comprised predominantly of _ pharmaceutical _ , _ biotechnology _ and _ life science tools _ , _ health care providers & services _ , _ health care equipment & supplies _ and everything else , respectively . albeit interesting , these results invite inspection of the stocks that end up in the `` everything else '' community .",
    "these stocks were deemed correlated with the other health care stocks , when they were all placed in community @xmath204 , and include mcgraw hill inc .",
    ", h&r block and waste management inc .",
    "none of these stocks immediately stand out as being fundamentally related to health care .",
    "similar outliers exist in the other communities as well .",
    "it may well be the case that there is good reason for their association , for example a shared parent company , sizable investment , common board members or some other significant relationship , or it may be purely coincidental .",
    "gaining a better understand of this takes us to our next lines of experimentation .",
    "having examined the mesoscopic structure of a set of financial markets , one might be curious as to whether that structure is specific to the chosen frequency of the original time series .",
    "that is , one would like to check whether the same communities would be retrieved if the returns which comprised the original time series were calculated every minute , every half hour or every two days . to answer this",
    "` multifrequency ' community detection problem , in this section we evaluate the robustness of partitions at a variety of temporal resolutions .      in order to maintain consistency with the results previously described in this paper",
    ", we use the same time frame but , instead of working with daily log - returns , we created new data using minute log - returns for the s&p 500 stocks .",
    "this has the initial effect of greatly increasing the amount of data being using : from 2500 data points per stock for the daily returns to approximately 900,000 for the minute returns . in order to accommodate some missing data from the minute returns",
    ", we had to reduce the set of 445 to 413 stocks , noting that the removed stocks were relatively evenly distributed across the top level sectors of the gics , so as not to deplete any one particular sector . with the minute return time data of these 413 stocks we created nine new sets of time",
    "series , corresponding to a variety of different resolutions @xmath205 spanning the same ten - year period : @xmath206 for example , the 5-min data was created by taking the price of every stock every 5th minute throughout the day . from these nine sets of time",
    "series we then proceeded in the same fashion as was previously described for daily return data , creating correlation matrices and leveraging rmt filtering to produce the respective null models .      to measure the effects of resolution we applied all three of the community detection algorithms discussed above to all nine data sets , yielding various values for the modularity @xmath194 of the partition ( see fig .",
    "[ fig : resqplot ] ) .",
    "since there can be multiple peaks within a modularity landscape  @xcite , all yielding the same value of @xmath194 but exhibiting different community structures , we use @xmath193 ( see sec .",
    "[ sec : bench ] ) as a measure of the difference between the partitions .",
    "since @xmath193 is a comparative measure , we ( arbitrarily ) use the community structure previously ascertained from the daily returns as the point of reference .",
    "thus , as can be seen in fig .",
    "[ fig : resviplot ] , the @xmath193 for the 1-day returns is 0 by construction , indicating perfect similarity , whereas the community structure for every other resolution shows some level of deviation .",
    "overall , it can be seen from the combination of figs .",
    "[ fig : resqplot ] and [ fig : resviplot ] that there exists a considerable amount of consistency between the communities detected at differing resolutions , with the @xmath194 values remaining almost constant and @xmath193 deviating slightly with each resolution interval but indicating in most cases no more than a 10% difference between the communities of a particular resolution and those of the 1-day resolution .",
    "this means that the correlations between large groups of stocks are not strongly dependent on the resolution of the chosen time step .",
    "one might expect to see fluctuations in the variance of stocks at smaller time resolutions , where the more volatile periods of trading ( such as market open and market close ) are captured .",
    "however since we are dealing with correlation matrices , this variance is normalized away .",
    "moreover , we recall that our definition of @xmath130 in eq .",
    "controls for the varying volatitily ( variance of the total log - return over all stocks ) , allowing us to focus solely on the relationships between the stocks themselves .",
    "although the values of @xmath194 and @xmath193 do provide reasonable insight into the robustness of community structure at the different resolutions , we take the analysis one step further and examine the communities from the perspective of the individual stocks .",
    "that is , we can further examine the community affiliation of individual stocks at the various resolutions to ascertain the frequency of times any two stocks find themselves in the same community as each other .",
    "we show the results of this analysis in fig .",
    "[ fig : resolutionhm ] , which is a heat map of the different stocks , such that the color of every pair indicates the frequency of co - occurrence in the same community , across all resolutions . for example",
    ", if two stocks were always in the same community ( unit frequency ) then their entry in the heat map is white , while if they were never in the same community regardless of the time step chosen ( zero frequency ) then their entry is black .",
    "stocks which share a community for some time steps are shades of red ( lower frequency ) or yellow ( higher frequency ) .",
    "as we can see , the results are in line with the graph of @xmath193 in fig .",
    "[ fig : resviplot ] .",
    "that is , the communities tend to consist of a large core of ` hard ' stocks that are unwavering over the different resolutions , plus a small amount of ` soft ' stocks that fluctuate between communities , presumably giving rise to the 10% fluctuation in community structure observed with the @xmath193 analysis . a significant finding is the existence of a group of soft stocks that alternate across the utilities , health care and consumer staples communities , and of another group of soft stocks alternating across the consumer discretionary and financials communities .",
    "it should be noted that our identification of the ` hard ' stocks that are most of the time part of the core of a community and the ` soft ' ones that are instead alternating across communities is a way to take the potentially overlapping nature of communities into account , even if using a non - overlapping method like modularity maximization .",
    "this possibility has no counterpart in the standard network - based community detection problem , and is offered by the intrinsic dependence of correlation matrices on the frequency of the original time series . in what follows , we will use the dynamical evolution of correlations to explore another dimension of variability leading to an alternative way to resolve overlapping communities of multiple time series .",
    "when optimizing a portfolio , there is a constant need to choose an adequate period of history from which to try to predict future behavior of the assets in the portfolio .",
    "choosing too short a period will inaccurately bias one s results , because extreme events are weighted too heavily .",
    "similarly , choosing too long a history can imply stability where none exists . in general , analyzing the stability of communities over time provides us with reassurance that our models are in fact producing statistically significant results as well as providing insightful information about the data itself .",
    "for example , it is well known in finance that markets become much more globally correlated during periods of economic decline . stated in the terminology",
    "we have been using throughout this paper , they fall more under the influence of the market mode and relinquish the structure provided by the group mode .",
    "that being the case , we would expect to see communities lose coherence during periods in the dataset that we know to have been economically troublesome , for example the tech bubble bursting in 2000 - 2001 or the sub - prime lending crisis , 2007 - 2008 .    since we have shown that the 15-minute data set yield very similar communities to the daily data set for the s&p , we can feel reasonably assured that we can use 15-minute data instead of the daily data , which will allow us to examine the s&p data set using a sliding time window of 2 years , corresponding to a ratio @xmath207 , and even look at more fine grained windows , e.g. 6 months .",
    "we now seek to examine the community structure over sequential periods of two years to unearth any anomalies which might exist .",
    "we again apply the different methods of community detection using the two - year window time series sets of the s&p 500 and subsequent null models created using rmt filtering . as we did for our analysis of resolution",
    ", here we evaluate the modularity function @xmath194 for each period ( see fig .",
    "[ fig : qtimespx2year ] ) along with the @xmath193 ( see fig . [",
    "fig : vitimespx2year ] ) , where for @xmath193 we are comparing each window with the initial two - year window .    as before",
    ", we see that all three algorithms perform in a reasonably similar manner .",
    "however , unlike our analysis of robustness over different resolutions ( which showed little change in community structure or in the modularity ) , here we see that @xmath194 fluctuates over the different windows .",
    "we recall again that , as we mentioned in our discussion following eq . , our choice of @xmath130 is already discounting ( the evolution of ) the volatility of the market .",
    "still , we see that @xmath194 rises slowly from the period ending in 2003 to the period ending in 2007 , implying an increase in the strength of communities , and then falls by more than 50% by the end of the period ending in 2009 .",
    "this drop implies a de - coherence of the communities throughout that period , quite possibly attributed to the financial crash of 2007 - 2008 .",
    "this seems in line with the observation that during periods of financial crisis , markets tend to become more globally correlated , overwhelming the effect of group - level correlations .",
    "however , it is interesting that the values of @xmath193 have remained quite small and stationary ( fig .",
    "[ fig : vitimespx2year ] ) .",
    "this indicates that , despite the fluctuating value of the modularity ( i.e. of the relative intra - community correlations ) , the composition of the communities has remained very stable over time .     for the different methods over a 2-year sliding window spanning the time frame from oct 2001 to oct 2011 .",
    "( potts method in green squares , louvain method in orange circles and spectral method in blue triangles).,scaledwidth=45.0% ]      we can continue to probe this system at a finer grained resolution of time periods , to see if the observations made with the two year window hold up .",
    "again , we plot both @xmath194 and @xmath193 for the same ten - year period of the s&p and present the results in figs . [",
    "fig : qtimespx6month ] and [ fig : vitimespx6month ] respectively .",
    "we can immediately see that the homogeneity of community structure that we see when probing the data using two - year time windows still exists for the most part , but there exists some fluctuations in modularity and community composition over the various six - month periods . the graph of @xmath194 in fig .",
    "[ fig : qtimespx6month ] reinforces the observation from fig .",
    "[ fig : qtimespx2year ] of a significant drop in modularity around the time of the most recent financial crisis , and more accurately pinpoints it to the last half of 2007 .",
    "the @xmath193 plot in figure [ fig : vitimespx6month ] indicates again that although the strength of community structure , as measured by @xmath194 , may have been decreasing , the overall composition of the communities remained relatively constant .    to further examine the coherence and fluctuations in communities across all of the six - month windows , in fig .",
    "[ fig : vispx6monthhm ] we provide a heat map showing the mutual @xmath193 between every two pairs of 6-month windows .",
    "each square in the matrix is a colored representation of the value of @xmath193 between the @xmath208 and @xmath209 6-month period .",
    "of particular interest , we can see in the lower right corner of the image ( which displays the @xmath193 between the most recent time windows ) that the communities are slightly more similar than communities generated from the other windows .",
    "this indicates that there was less movement of stocks between communities during the most recent couple of years of the past decade . additionally , these periods are closer to the community structure observed when we measured the entire ten - year period .    as far as explaining this behavior in financial and economical terms , we are again left to hypothesize .",
    "perhaps the observed effect is due to a solidification of communities of stocks caused by the financial collapse , or perhaps it is merely the result of increased accessibility to the markets . with the advent of smartphones , tablets , ease of streaming and subscribing to news feeds and social networks in conjunction with faster trading systems , quantitative and high frequency trading ,",
    "it is conceivable that our increased access to information and the ability to act on it in near real - time has caused a solidifying behavior of the stocks within communities .",
    "we display here one final take on the results obtained from our sliding time window , but this time with a stock - centric view , similar to that which we performed for the multifrequency analysis in sec.[sec : resol ] . in the previous sections we have alluded to how communities change with time .",
    "one question that should be addressed in conjunction with the previous discussion of time scales is then how the composition of a community changes over time .",
    "we have already seen from a variety of @xmath193 plots that across each of the six - month periods , the sets of communities look slightly different from each other , but what changes are actually taking place ?",
    "are there groups of stocks that form tight knit , unwavering cores of communities ? or do they morph fluidly from one to the other , maintaining no coherence over the entire span of ten years ?    to address this , we examined the sets of stocks that comprised the communities of each six - month time frame of the s&p over the course of ten years and created a co - occurrence matrix like the one previously shown in fig.[fig : resolutionhm ] , where we calculated the frequency of periods during which any pair of stocks resided in the same community . the resulting heat map is presented in figure [ fig : comhm6months ] .",
    "again , pairs of stocks which were in the same community all the time are white , and those which were never in the same community are black .",
    "the list of stocks is too long to place on the figure as axis labels , but from observing the raw results we can make some very interesting observations , which we have tried to summarize by labeling again the graph with gics industry sectors .",
    "we can see that over the course of ten years , the communities do exhibit strong cores which are unwavering in their construction and constantly anti - correlated with each other .",
    "for example there exists a set of core energy , it and financial stocks which always reside in their own community , but never share a community with each other .",
    "groups of energy , materials and utilities stocks almost always share the same community , but there have been instances when they did not .",
    "finance is broken into a couple of segments of stocks , such as banks , real estate investment trusts ( reits ) , etc . where the smaller groups always trade with each other but not necessarily aggregated together in a larger community .",
    "similarly , health care stocks are fractured in subsets of highly correlated groups of pharmaceuticals , services and biotech , whose allegiance to the larger industry sectors , such as it and consumer staples is more fluid .",
    "these trends display interesting overlap with the hierarchical community structure of the s&p discussed earlier in sec.[sec : hiera ] .",
    "we also see individual stocks from one top - level industry sector spending most of their time in communities comprised predominantly of a different top - level industry sector , for example amazon ( consumer discretionary ) spends 90% of the time in the it group , as does motorola ( telecommunications ) .     between every pair of 6-month time windows , as well as the @xmath193 between each window and the total 10-year period ( leftmost column and top row ) .",
    "most notably , there is a slight increase in the similarity of the communities of the last 5 periods 2009 - 2011 .",
    "( produced using the louvain algorithm).,scaledwidth=49.0% ]",
    "in this paper we have addressed the challenging problem of the detection of communities of strongly correlated time series , whose importance resides in the possibility of identifying a mesoscopic level of organization in the dynamics of complex systems .",
    "while the available techniques to analyze correlation matrices failed to detect such modules , we have shown how the concepts of null models , modularity and community detection developed in network theory can be appropriately modified in order to successfully cluster matrices of multiple time series .",
    "our redefinitions of the standard methods solve a number of problems encountered when correlation matrices are navely regarded as weighted networks and when ordinary community detection methods are used improperly .    through the use of various financial markets as examples ,",
    "we have demonstrated how community detection can be used as a tool to extract specific structural information from time series data . by surfacing group correlations and trends of the stocks in the s&p 500 , the ftse 100 and the nikkei 225",
    ", we were able to isolate well - defined communities of stocks such that each community exhibited an internal positive correlation between its constituent stocks , where those same stocks exhibited an aggregate residual anti - correlation with the stocks of each of the other communities .",
    "while some of these communities showed an association with the more qualitative classification expressed in the gics industry sector taxonomy , our approach was able to uncover a host of interesting correlations between stocks of different sectors and industry groups , as well as unsuspected residual anti - correlations between stocks of the same sector .",
    "as such , our methods and results show that the observed patterns are irreducible to a standard taxonomy , and therefore highlight nontrivial patterns .",
    "moreover , they could prove particularly useful in a number of different fields of finance , such as portfolio optimization and risk management .",
    "it is worth pointing out that our modifications to the potts , louvain and spectral optimization algorithms for community detection , although beneficial in and of themselves , act as a proof of concept opening the door to the adaptation of other techniques existing in the field of community detection , allowing e.g. for overlapping , multiresolution , or hierarchical communities @xcite .",
    "similarly , alternative null models controlling for additional or more sophisticated features of the data can also be developed and incorporated in our approach .",
    "the key point is that these models , unlike the nave approach , which has been used so far , should always be consistent with correlation matrices .",
    "we hope that our approach will stimulate further research in this direction .",
    "moreover , although we have focused on financial time series as our primary example of real - world data , our general methodology can of course be applied or adapted to any type of time series data , hopefully yielding equally promising results .",
    "we conclude by noting that , abstractly , the ordinary ( network - based ) community detection techniques and the ( correlation - based ) clustering that we have introduced can be thought of as lying at two opposite extremes of a more general problem , in the following sense .",
    "the network - based clustering is in the vast majority of cases aimed at identifying groups of statically linked objects ( as captured by a single temporal snapshot of the network ) while disregarding their possibly correlated evolution .",
    "by contrast , the correlation - based clustering that we have introduced assumes that the community - defining features are precisely those determining synchronized trends of dynamical activity among nodes , and that the presence ( if any ) of static dependencies among the latter can be disregarded .",
    "one could of course imagine a more general framework where both static linkages and temporal correlations contribute to the definition of communities , possibly overcoming the ` functional versus structural ' dichotomy such as the one existing in brain network analysis that we mentioned in the introduction .",
    "the present work thus represents one step towards the introduction of a fundamentally more general interpolating formalism .",
    "dg acknowledges support from the dutch econophysics foundation ( stichting econophysics , leiden , the netherlands ) with funds from beneficiaries of duyfken trading knowledge bv , amsterdam , the netherlands .",
    "this work was also supported by the eu project multiplex ( contract 317532 ) and the netherlands organization for scientific research ( nwo / ocw ) .",
    "in this appendix we show that we can successfully reformulate three of the most popular network - based community detection algorithms in order to properly detect communities of correlated time series using the modified modularity function defined in eq.([eq : qunified ] ) .",
    "we stress again that even if the techniques , which we are going to describe , can be considered as three different algorithms implementing the same method of modularity maximization , they are often referred to as different ` methods ' in the literature . in what follows ,",
    "we will sometimes make use of this somewhat improper terminology .",
    "we will also necessarily use a vocabulary that applies more properly to networks than to time series : for instance , a time series will be often denoted as a ` node ' ( or ` vertex ' ) of the ` network ' , and the correlation between two time series will be denoted as the weight of the ` link ' ( or ` edge ' ) between the corresponding nodes .",
    "the first of the three methods we have selected is based on the so - called @xmath210-state potts model @xcite .",
    "it represents the system as a @xmath210-state spin glass , where each node maintains a spin state @xmath21 ( as given by some attempted partition @xmath58 ) and the weights of the edges between nodes map to coupling strengths .",
    "so any partition of the network is regarded as a spin configuration @xmath58 . in this paradigm ,",
    "the modularity @xmath59 is proportional to the negative energy @xmath211 of the system .",
    "the goal of optimization is then to find the ground state of a spin glass , which corresponds to the maximum value for the modularity .",
    "the use of a multi - state super - paramagnetic model for graph clustering was first introduced by blatt , wiseman and domany  @xcite and later revised by reichardt and bornholdt  @xcite ; it is upon the latter that we base our extension to incorporate multiple time series .    within the @xmath210-state potts spin glass model , reichardt and bornholdt construct a hamiltonian by rationalizing a number of energy contributions from the edges between nodes within the same community and nodes in different communities : @xmath212}_\\text{external links}\\nonumber\\\\ & & -   \\sum_{i , j}d_{ij}\\underbrace{(1-a_{ij})[1-\\delta(\\sigma_i , \\sigma_j)]}_\\text{external non - links } , \\label{hamiltonian}\\end{aligned}\\ ] ] where the contributions from the various types of links can be tuned through the set of coefficients , @xmath213 , @xmath214 , @xmath215 , @xmath216 . instead of directly maximizing the modularity @xmath59 defined in eq .",
    ", reichardt and bornholdt minimize the hamiltonian @xmath217 .",
    "the latter ( under certain conditions and some simplifying assumptions ) can be condensed to @xmath218\\delta(\\sigma_i , \\sigma_j),\\ ] ] where @xmath66 is the observed value and @xmath72 is the corresponding null model for that edge . the actual search over spin configurations",
    "is done using simulated annealing @xcite , which is an approximate technique that in general returns a different solution each time it is used .    in the same vein ,",
    "introducing a hamiltonian corresponding to our correlation - based modularity is equally straightforward , however we need to ensure that the logic and derivation that was used to develop the original network - based hamiltonian holds true for a network created from time series data . for a correlation - based network",
    "we have the situation where every node is connected to every other node , in principle eliminating the energy contributed by non - links in eq.([hamiltonian ] ) above .",
    "however , as is ordinarily done when applying the potts model to weighted networks , we can replace the energy contribution of non - links with the energy contribution of links whose weight is less than expected , allowing us to immediately introduce our null model . maintaining the balance between internal and external edges ( @xmath213 = @xmath215 and @xmath214 = @xmath216 ) as was done by reichardt and bornholdt in their original derivation of the hamiltonian , we end up with a variant of eq . directly derived from a complete weighted network where @xmath66 and @xmath72 are replaced by the observed correlation @xmath94 and one of our three null models @xmath219 defined in sec .",
    "[ sec : ourmod ] , giving @xmath220 apart from the absence of @xmath130 , the r.h.s .",
    "of the above expression is the opposite of the r.h.s . of eq .. therefore our optimization method using the potts model will attempt to find the lowest value of the hamiltonian , which will correspond to the highest modularity . for the rest ,",
    "our algorithm is identical to the procedure described by reichardt and bornholdt @xcite .",
    "therefore the potts model is a simple algorithm to adapt to correlation matrices , the reason being that although the modularity of the system is explained using a spin - glass model , the actual optimization process is performed using simulated annealing , which keeps working even if we use our redefinition of modularity as the cost function @xcite .",
    "we now consider a second approach to the problem of modularity optimization .",
    "possibly one of the most successful approaches , the louvain method  @xcite ( named after the university from which it emerged ) is a simple , greedy , agglomerative algorithm whose strength lies in the fact that it is computationally fast . unlike the spin - glass model , the louvain method does not set up a framework for its optimization problem .",
    "it simply starts from the definition of modularity specified in eq . and derives a new , more computationally efficient equation for testing the relative gain in modularity by moving a node from one community to another .",
    "it is this equation that allows the louvain method to perform so well .",
    "the method initially considers all nodes as placed in individual communities , and then calculates ( sequentially for each node @xmath2 ) the gain of modularity associated with moving node @xmath2 to the same community where each of its neighbours @xmath69 belong .",
    "the algorithm explores all possible such moves and implements those that give the maximum gain in modularity , and the first iteration stops when no further improvement is possible .",
    "then , a new ` renormalized ' network is built by merging all nodes within the previously found communities into a single ` hypernode ' , and the algorithm is iterated again until there are no more possible changes and a maximum of modularity is attained .",
    "to do so , the renormalized weight of the link between two hypernodes is defined as the sum of the weight of the links between nodes in the corresponding two communities .",
    "links between nodes of the same community lead to self - loops for the corresponding hypernode .",
    "the key requirement of the louvain method is that the system can be properly renormalized , i.e. that successive coarse - grainings of the system remain consistent with the meaning of the modularity at the corresponding level of aggregation . in an ordinary network",
    "this is relatively straightforward to show , i.e. a hypernode obtained merging two or more nodes can be legitimately interpreted ( from the point of view of the modularity function ) as a coarse - grained node with a self - loop to itself and renormalized interactions to all other ( hyper)nodes .",
    "it is however not intuitively obvious whether our modularity defined in eq.([eq : qunified ] ) admits an equivalently consistent definition of ` renormalized time series ' obtained by ` merging ' two or more time series . and",
    "even if such a definition exists , one should understand how to correctly define also the renormalized interactions and self - loops .",
    "to this end , we recall from eq.([eq : corrs ] ) that if @xmath15 and @xmath16 are two standardized time series then @xmath221 $ ] .",
    "we can therefore exploit the fact that the covariance is a bilinear function of its arguments to calculate the following renormalized interactions between two hypernodes ( communities ) @xmath107 and @xmath159 : @xmath222\\nonumber\\\\ & = & \\textrm{cov}\\big[\\sum_{i\\in a}x_i,\\sum_{j\\in b}x_{j}\\big].\\end{aligned}\\ ] ] the above formula shows that , if we define the ` renormalized time series ' of community @xmath107 as @xmath223 then we can consistently define the renormalized interactions as @xmath224 \\label{eq : l1}\\ ] ] and the renormalized self - loops as @xmath225=\\textrm{var}\\big[\\tilde{x}_a\\big].\\ ] ] we therefore find that , for a graph composed of financial time series , renormalized interactions have a correct interpretation in terms of covariances , rather than correlations .",
    "they also show that the summation of a group of time series yields something that resembles an index fund of the set of stocks , so the concept of aggregating nodes maintains a strong grounding in reality .",
    "we now have to check whether the modularity function remains consistent with the null model when defined at the level of renormalized nodes .",
    "note that the linearity of the definition of @xmath226 ensures that , given any of our null models defined in sec.[sec : ourmod ] , we can write @xmath227 this means that the filtered quantity @xmath228 can be similarly renormalized as @xmath229 for each of the three cases in eq.([eq : l ] ) .",
    "now , imagine that in subsequent iterations of the model the hypernodes are further merged into ` communities of communities ' .",
    "the resulting ` metapartition ' can be specified by a vector @xmath230 of dimension smaller than ( or equal to , if the metapartition is trivial ) any of the original vectors @xmath231 .",
    "each element @xmath232 denotes the community to which the hypernode @xmath107 is placed by the metapartition .",
    "if @xmath58 denotes the underlying ( node - level ) partition identified by the metapartition @xmath230 ( i.e. @xmath233 for all @xmath157 ) , we can define the renormalized modularity @xmath234 where , in analogy with eq .",
    ", we have defined @xmath235 equation coincides with the original modularity defined at the level of individual nodes .",
    "this means that the modularity is manifestly invariant under renormalization , implying that we can indeed consistently redefine a coarse - grained modularity at each iteration of the louvain method .",
    "the second requirement of the louvain method is the fact that the change in the modularity obtained by adding a previously isolated node to a given pre - existing community can be easily calculated .",
    "this ensures the computational efficiency of the algorithm . in adapting the model to correlation - based networks",
    ", we must start from eq . and check whether this is still the case , and if so arrive at a new corresponding expression for the modularity change .",
    "we will do so using directly the invariant modularity defined in eq.([eq : qinvariant ] ) , so that we are sure that the result will hold at any aggregation level .",
    "given the modularity @xmath236 , we denote the modularity change obtained by adding the ( hyper)node @xmath237 to the community @xmath238 by @xmath239 and calculate it as the difference between @xmath240 for a ( meta)partition @xmath241 where @xmath237 is part of the community @xmath238 ( i.e. @xmath242 ) and @xmath243 for a ( meta)partition @xmath244 where @xmath237 is isolated in its own community ( i.e. @xmath245 ) .",
    "since @xmath246 for all @xmath247 and @xmath248 for all @xmath247 , we can write this difference as @xmath249\\nonumber\\\\ & = & \\frac{1}{c_{norm}}\\sum_{a}\\tilde{c}^{(l)}_{ia}\\big[\\delta(\\tilde{\\sigma}'_i,\\tilde{\\sigma}'_a)-\\delta(\\tilde{\\sigma}''_i,\\tilde{\\sigma}''_a)\\big]\\nonumber\\\\ & = & \\frac{1}{c_{norm } } \\sum_{a \\in j } \\tilde{c}^{(l)}_{ia}\\nonumber\\\\ & = & \\frac{\\tilde{c}^{(l)}_{ij}}{c_{norm}}. \\label{eq : k}\\end{aligned}\\ ] ] that is , the change in modularity obtained from adding a ( hyper)node @xmath237 to a pre - existing community @xmath238 is simply proportional to the renormalized interaction between @xmath237 and @xmath238 , i.e. the sum of the ( filtered ) correlations of all time series within @xmath237 with all those within @xmath238 .",
    "note that in the above formula the notation @xmath250 implies @xmath247 , since @xmath237 does not ( yet ) belong to @xmath238 .",
    "similarly , it is possible to calculate the change in modularity @xmath251 obtained when a ( hyper)node @xmath237 belonging to a community @xmath252 is disconnected from the latter and placed in its own isolated community . combining these two contributions",
    ", we can easily calculate the change in modularity @xmath253 obtained by moving a ( hyper)node @xmath237 from a community @xmath252 to a different community @xmath238 .",
    "so our reformulation above satisfies also the second requirement of the louvain method at all aggregation levels , and allows us to define a computationally efficient method to detect communities of time series .",
    "we now come to the third and final method of optimizing the modularity cost function .",
    "spectral optimization is the process of using matrix eigendecomposition to recursively bisect a network into communities of nodes according to the principle of maximizing the modularity function  @xcite .",
    "the matrix which is the subject of the eigendecomposition is the so - called _ modularity matrix _ appearing in eq . and having entries @xmath254 .",
    "in other words , the modularity matrix @xmath255 is the difference between the observed network , represented by the adjacency matrix @xmath86 , and the null model @xmath256 . in the spectral method ,",
    "the modularity matrix is eigendecomposed into its constituent eigenvalues and eigenvectors , the intent being to isolate the eigenvector corresponding to the largest eigenvalue and use the signs of the elements of this vector to infer an optimal partition .",
    "specifically , the network is split into two communities , each comprising the nodes corresponding to eigenvector components with the same sign .",
    "the process is implemented recursively in each partition ( deriving a new modularity matrix for every community ) , until no further increase in modularity is obtained .",
    "we need to extend this algorithm to accommodate correlation - based networks .",
    "in our case , as clear from eqs . and , the modularity matrix is @xmath151 , i.e. the filtered matrix defined using one of our three null models .",
    "we fill therefore adapt the procedure outlined by newman in the original paper , and implement the spectral optimization method by iteratively bisecting the network into two sub - communities ( say @xmath107 and @xmath159 ) .",
    "each such bisection can be denoted either by an appropriate partition vector @xmath58 or equivalently by a vector @xmath257 having elements @xmath258 if node @xmath2 belongs to ( say ) community @xmath107 and @xmath259 if @xmath2 belongs to community @xmath159 .",
    "the correspondence between these vectors is given by @xmath260 given a bisection , we can therefore rewrite our unified correlation - based modularity @xmath261 defined in eq.([eq : qunified ] ) as @xmath262 in newman s original formulation the last term sums to zero , because the network - based modularity matrix @xmath255 has the property that all of its rows sum to zero .",
    "however , this is not the case with our correlation - based modularity matrix @xmath263 defined in eq .. so we retain the second term and , defining @xmath264 , rewrite our modularity in matrix form as @xmath265 the vector @xmath257 maximizing @xmath266 is easily found as the vector matching the signs of the components of the eigenvector of @xmath267 corresponding to the largest eigenvalue .",
    "clearly , both @xmath130 and @xmath268 have no effect on the result , making the original procedure of the spectral algorithm consistent with our reformulation .",
    "after the initial bisection , we need to calculate the potential modularity change @xmath158 obtained by further subdividing the communities yielded in the previous step .",
    "let us consider the case where one community ( say @xmath107 ) among the ones obtained thus far in the algorithm is further subdivided into two new communities ( say @xmath269 and @xmath270 ) .",
    "if @xmath257 is a vector ( restricted to the vertices in @xmath107 only ) denoting the bisection of @xmath107 into @xmath269 and @xmath270 , then the modularity change associated with such bisection reads @xmath271\\nonumber\\\\ & = & \\frac{1}{c_{norm}}\\big [ \\sum_{i , j \\in a } c^{(l)}_{ij } \\frac{s_i s_j + 1}{2 } - \\sum_{i , j \\in a } c^{(l)}_{ij}\\big]\\nonumber\\\\ & = & \\frac{1}{2c_{norm}}\\big [ \\sum_{i , j \\in a } c^{(l)}_{ij } s_i s_j -\\sum_{i , j \\in a } c^{(l)}_{ij}\\big ]   \\nonumber\\\\ & = & \\frac{\\langle s| \\textbf{c}_a^{(l ) } | s\\rangle } { 2c_{norm } } + \\frac{\\tilde{c}^{(l)}_{aa}}{2c_{norm } } , \\label{eq : deltaqnewman}\\end{aligned}\\ ] ] where @xmath272 represents the sub - matrix of @xmath267 restricted to the subset of nodes within community @xmath107 , and the notation @xmath273 is borrowed from eq .. as with the initial bisection , @xmath257 is chosen to maximize @xmath274 by selecting its elements to match the sign of the eigenvector corresponding to the largest eigenvalue of the matrix @xmath272 .",
    "r. neuman , y. j. park and e. panek , tracking the flow of information into the home : an empirical assessment of the digital revolution in the united states , 19602005 , international journal of communication 6 , 1022 ( 2012 ) .",
    "p. ronhovde , s. chakrabarty , d. hu , m. sahu , k.k .",
    "sahu , k.f .",
    "kelton , n.a .",
    "mauro and z. nussinov , detection of hidden structures for arbitrary scales in complex physical systems , sci .",
    "rep . 2 , 329 ( 2012 ) ."
  ],
  "abstract_text": [
    "<S> a challenging problem in the study of complex systems is that of resolving , without prior information , the emergent , mesoscopic organization determined by groups of units whose dynamical activity is more strongly correlated internally than with the rest of the system . </S>",
    "<S> the existing techniques to filter correlations are not explicitly oriented towards identifying such modules and can suffer from an unavoidable information loss . </S>",
    "<S> a promising alternative is that of employing community detection techniques developed in network theory . </S>",
    "<S> unfortunately , this approach has focused predominantly on replacing network data with correlation matrices , a procedure that tends to be intrinsically biased due to its inconsistency with the null hypotheses underlying the existing algorithms . here </S>",
    "<S> we introduce , via a consistent redefinition of null models based on random matrix theory , the appropriate correlation - based counterparts of the most popular community detection techniques . </S>",
    "<S> our methods can filter out both unit - specific noise and system - wide dependencies , and the resulting communities are internally correlated and mutually anti - correlated . </S>",
    "<S> we also implement multiresolution and multifrequency approaches revealing hierarchically nested sub - communities with ` hard ' cores and ` soft ' peripheries . </S>",
    "<S> we apply our techniques to several financial time series and identify mesoscopic groups of stocks which are irreducible to a standard , sectorial taxonomy , detect ` soft stocks ' that alternate between communities , and discuss implications for portfolio optimization and risk management . </S>"
  ]
}