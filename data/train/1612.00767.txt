{
  "article_text": [
    "over the last years the ever increasing complexity of machine learning ( ml ) models together with the increasing amount of data that is available to train them has resulted in a great demand for parallel training and inference algorithms that can be deployed over many machines . to meet this demand ,",
    "ml practitioners have increasingly relied on ( asynchronous ) parallel stochastic gradient descent ( sgd ) methods for optimizing model parameters @xcite .",
    "in contrast to this , the literature on efficient parallel methods for sampling from the posterior over model parameters given some data is much more scarce .",
    "examples of algorithms for this setting typically are constrained to specific model classes @xcite or `` only '' consider data parallelism @xcite ; in summary  with the exception of recent work by @xcite  a general sampling pendant of asynchronous sgd methods is missing .",
    "in this paper we consider the general problem of sampling from an arbitrary posterior distribution over parameters @xmath0 given a set of observed datapoints @xmath1 where we have @xmath2 machines available to speed up the sampling process . without loss of generality we can write the mentioned posterior as @xmath3 where we define @xmath4 to be _ the potential energy _ @xmath5 .",
    "many algorithms for solving this problem such as hamiltonian monte carlo @xcite further augment this potential energy with terms depending on auxiliary variables @xmath6 to speed up sampling . in this more general case",
    "we write the posterior as @xmath7 , where @xmath8 denotes the collection of all variables and @xmath9 denotes the _ hamiltonian _ @xmath10 .",
    "samples from @xmath11 can then be obtained by sampling from @xmath12 and discarding @xmath13 if we additionally assume that marginalizing out @xmath13 from @xmath12 results only in a constant offset @xmath14 ; that is we require @xmath15 .",
    "stochastic gradient mcmc ( sgmcmc ) algorithms solve the above described sampling problem by assuming access to a  possibly noisy ",
    "gradient of the potential @xmath4 with respect to parameters @xmath16 . in this case ",
    "assuming properly chosen auxiliary variables  sampling can be performed via an analogy to physics by simulating a system based on the hamiltonian @xmath9 .",
    "more precisely , following the general formulation from @xcite , one can simulate a stochastic differential equation ( sde ) of the form @xmath17 where @xmath18 denotes the deterministic drift incurred by @xmath9 , @xmath19 is a diffusion matrix , the square root is applied element - wise , and @xmath20 denotes brownian motion . under fairly general assumptions on the functions @xmath21 and @xmath22 the unique stationary distribution of this system is equivalent to the posterior distribution @xmath11 .",
    "specifically , @xcite showed that if @xmath23 is of the following specialized form ( in which we are free to choose @xmath24 and @xmath25 ) : @xmath26 then the stationary distribution is equivalent to the posterior if @xmath24 is positive semi - definite and @xmath25 is skew - symmetric .",
    "importantly , this holds also if only noisy estimates @xmath27 of the gradient @xmath28 computed on a randomly sampled subset of the data are available ( as in our case ) .      in practice , for any choice of @xmath9 , @xmath24 and @xmath25 , simulating the differential equation on a digital computer involves two approximations .",
    "first , the sde is simulated in discretized steps resulting in the update rule @xmath29 + { \\mathcal{n}}\\big ( 0 , 2\\epsilon_t    d({\\mathbf{z}}_t ) \\big ) , \\label{eq : sgmcmc_updates}\\ ] ] where we slightly abuse notation and take @xmath30 to denote the addition of _ a sample from _ an m - dimensional multivariate gaussian distribution .",
    "second , when dealing with large datasets , exact computation of the gradient @xmath28 becomes computationally prohibitive and one thus relies on a stochastic approximation computed on a randomly sampled subset of the data : @xmath27 with @xmath31 where @xmath32 .",
    "the stochastic gradient @xmath27 is then gaussian distributed with some variance @xmath33 ; leading to the noise term in the above described sde .",
    "using these two approximations one can derive the following discretized system of equations for a stochastic gradient variant of hamiltonian monte carlo @xmath34 which , following @xcite , can be seen as an instance of equation with @xmath35 , @xmath36 and where @xmath37 and @xmath38 .",
    "we now show how one can utilize the computational power of @xmath2 machines to speed up a given sampling procedure relying on the dynamics described in equations .",
    "as mentioned before , the update equations derived in section [ sect : sghmc ] involve alternating updates to both the parameters and the uauxiliary variables , leading to an inherently sequential algorithm .",
    "if we now want to utilize @xmath2 machines to speed up sampling we thus face the non - trivial challenge of parallelizing these updates . in general",
    "we can identify two solutions is based on a large number of data - points ( or if we want to reduce the variance of our estimate by increasing @xmath39 ) we could potentially spread out the data - set @xmath40 over the @xmath2 machines allowing us to parallelize computation without the need for asynchronous updating . while this is an interesting problem in its own right and there",
    "already exists a considerable amount of literature on running parallel mcmc over sub - sets of data @xcite we here focus on parallelization schemes that do not make this assumption because of their broad applicability . ] :    * i ) * for a naive parallelization strategy we can send the variables @xmath16 to @xmath2 different machines every @xmath41 steps from a parameter server .",
    "each machine then computes a gradient estimate for the current step @xmath42 ( note that we only approximately have @xmath43 due to the communication period @xmath41 , i.e. @xmath44 at each machine might be a stale parameter ) .",
    "the server then waits for @xmath45 gradient estimates @xmath42 to be sent back and simulates the system from eq .",
    "using @xmath46 ;    * ii ) * we can set - up @xmath2 mcmc chains ( one per machine ) which independently update a parameter vector @xmath47 ( where @xmath48 $ ] denotes the machine ) , following the dynamics from eq . .",
    "while the second approach clearly results in markov chains that asymptotically sample from the correct distribution  and might result in a more diverse set of samples than a simulation using a single chain ",
    "it is also clear that it can not speed up convergence of the individual chains ( our desired goal ) as there is no interaction between them .",
    "the first approach , on the other hand , is harder to analyze .",
    "we can observe that if @xmath49 and we wait for @xmath50 gradient estimates in each step we obtain an sg - mcmc algorithm with parallel gradient estimates but synchronous updates of the dynamic equations .",
    "consequently , such a setup preserves the guarantees of standard sg - mcmc but _ requires synchronization between all machines in each step_. in a real - world experiment ( where we might have heterogeneous machines and communication delays ) this will result in a large communication overhead . for choices of @xmath51 and @xmath52 ",
    "the regime we are interested in  we can not rely on the standard convergence analysis for sg - mcmc anymore . nonetheless , if we concentrate on the analysis of @xmath51 and @xmath53 ( i.e. completely asynchronous updates ) we can interpret the stale parameters to simply result in more noisy estimates of @xmath54 that can be used within the dynamic equations from eq .",
    "[ eq : sghmc ] .",
    "the efficacy of such a parallelization scheme then intuitively depends on the amount of additional noise introduced by the stale parameters and requires a new convergence analysis . during",
    "the preparation of this manuscript , concurrent work on sgmcmc with stale gradients derived a theoretical foundation for this intuition @xcite .",
    "interestingly , we will in the following empirically show that the additional noise is unproblematic for small @xmath41 in the range @xmath55 ( for which a convergence speed - up with naive parallelization can therefore be achieved , but which result in a large communication overhead in distributed systems ) but becomes problematic with growing @xmath41 .",
    "we believe these results are in accordance with the mentioned recent work by @xcite , yet a unification of their theory with our proposed new algorithm remains as important future work .",
    "given the negative analysis from section [ sect : parallel_schemes ] one might wonder whether approach * ii ) * ( the idea of running @xmath2 parallel mcmc chains ) can be altered such that the @xmath2 chains are only loosely coupled ; allowing for faster convergence while avoiding excessive communication . to this end",
    "we propose to consider the following alternative parallelization scheme :    * iia ) * to speed up @xmath2 sgmcmc chains we couple the @xmath2 parameter vectors through an additional center variable @xmath56 to which they are elastically attached .",
    "we collect updates to this center variable at a central server and broadcast an updated version of it every @xmath41 steps across all machines .",
    "we note that an approach based on this idea was recently utilized to derive an asynchronous sgd optimizer in @xcite , serving as our main inspiration .",
    "a discussion of the connection between their deterministic and our stochastic dynamics is presented in section [ sect : easgd ] .    to derive an asynchronous sgmcmc variant with @xmath2 samplers and elastic coupling ",
    "as described above  we consider an augmented hamiltonian with @xmath57 : @xmath58 where we can interpret @xmath56 as a centering mass ( with momentum @xmath59 ) through which the asynchronous samplers are elastically coupled and @xmath60 specifies the coupling strength .",
    "it is easy to see that for @xmath61 we can decompose the sum from eq . into @xmath2 independent terms which each constitute a hamiltonian corresponding to the standard sghmc case ( and",
    "we thus recover the setup of @xmath2 independent mcmc chains ) .",
    "further , for @xmath62 we obtain a joint , altered , hamiltonian in which  as desired ",
    "the @xmath2 parameter vectors are elastically coupled .",
    "simulating a system according to this hamiltonian exactly would again result in an algorithm requiring synchronization in each simulation step ( since the change in momentum for all parameters depends on the position of the center variable and thus , implicitly , on all other parameters ) .",
    "if we , however , assume only a noisy measurement of the center variable and its momentum is available in each step we can derive a mostly asynchronous algorithm for updating each @xmath63 . to achieve this",
    "let us assume we store our current estimate for the center variable and its momentum at a central server .",
    "this server receives updates for @xmath56 and @xmath59 from each of the @xmath2 samplers every @xmath41 steps and replies with their current values . assuming a gaussian normal distribution on the error of this current value each sampler then keeps track of a noisy estimate @xmath64 of the center variable which is used for simulating updates derived from the hamiltonian in equation . from these assumptions",
    "we derive the following discretized dynamical equations : @xmath65 where @xmath66 specifies the noise due to stochastic gradient estimates and @xmath67 is the variance of the aforementioned noisy center variable . as before",
    ", we use the notation @xmath68 to refer to a sample from a gaussian distribution with mean @xmath69 and covariance @xmath70 .",
    "we note that , while the presented dynamical equations were derived from sghmc , the elastic coupling idea does not depend on the basic hamiltonian from equation .",
    "we can thus derive similar asynchronous samplers for any sgmcmc variant including first order stochastic langevin dynamics @xcite or any of the more advanced techniques reviewed in @xcite .",
    "when inspecting the equations we can observe that , similar to approach * i * , they also contain an additional noise source : noise is injected into the system due to potential staleness of the center variable .",
    "however , since this noise only indirectly affects the parameters @xmath71 one might hope that the center variable acts as a buffer , damping the injected noise .",
    "if this were the case , we would expect the resulting algorithm to be more robust to communication delays than the naive parallelization approach .",
    "in addition to this consideration , the proposed dynamical equations have a convenient form which makes it easy to verify that they fulfill the conditions for a valid sgmcmc procedure .    the dynamics of the system from eq .",
    "has the posterior distribution @xmath72 as the stationary distribution for all @xmath2 samplers .    to show this",
    ", we first establish that equations correspond to a discretized dynamics following the general form given by equations and where @xmath73)$ ] and @xmath74 , with @xmath75 and @xmath76 , thus fulfilling the requirements outlined in section [ sect : sgmcmc ] .",
    "furthermore , to see that marginalization of the auxiliary variables results in a constent offset , we can first identify @xmath77 , with @xmath78 $ ] . solving",
    "the integral @xmath79 then amounts to evaluating gaussian integrals and is therefore easily checked to be constant , as required .",
    "thus , simulating the the dynamical equations results in samples from @xmath80 and discarding the auxilliary variables @xmath13 gives the desired result .",
    "we designed an initial set of three experiments to validate our sampler .",
    "first , to get an intuition for the behavior of the elastic coupling term we tested the sampler on a low dimensional toy example . in figure",
    "[ fig : toy_example ] we show the first 100 steps taken by both standard sghmc ( left ) and our elastically coupled sampler ( ec - sghmc ) with four parallel threads ( right ) when sampling from a two - dimensional gaussian distribution ( starting from the same initial guess ) .",
    "the hyperparameters were set to @xmath81 , @xmath82 , @xmath83 .",
    "we observe that two independent runs of sghmc take fairly different initial paths and , depending on the noise , it can happen that sghmc only explores low - density regions of the distribution in its first steps ( cf .",
    "purple curve ) .",
    "in contrast the four samplers with elastic coupling quickly sample from high density regions and show coherent behaviour .    as a second experiment , we compare ec - sghmc to standard sghmc and the naive parallelization described in section [ sect : parallel_schemes ] ( async sghmc in the following ) .",
    "we use each method for sampling the weights of a two layer fully connected neural network ( 800 units per layer , relu activations , gaussian prior on the weights , batch size @xmath84 ) that is applied to classifying the mnist dataset . for our purposes",
    ", we can interpret the neural network as a function that parameterizes a probability distribution over classes @xmath85 where @xmath86 is the label of the given image and @xmath87 is the output vector of the neural network with parameters @xmath88 .",
    "we place a gaussian prior on the network weights @xmath89 ( where we chose @xmath90 ) and sample from the posterior @xmath91 for a given dataset @xmath92 .",
    "the results of this experiment are depicted in figure [ fig : nets ] ( left ) .",
    "we plot the negative log likelihood over time and observe that both parallel samplers ( using @xmath93 parallel threads ) perform significantly better than standard sghmc .",
    "however , when we increase the communication delay and only synchronize threads every @xmath94 steps the additional noise injected into the sampler via this procedure becomes problematic for async sghmc whereas ec - sghmc copes with it much more gracefully .         finally , to test the scalability of our approach , we sampled the weights of a 32-layer residual net applied to the cifar-10 dataset .",
    "we again aim to sample the posterior given by equation only now the neural network @xmath87 is the 32-layer residual network described in @xcite ( with batch - normalization removed ) .",
    "the results of this experiment are depicted in figure [ fig : nets ] ( right ) showing that , again , ec - sghmc leads to a significant speed - up over standard sghmc sampling .",
    "as described in section [ sect : method ] an elastic coupling technique similar to the one used in this manuscript was recently used to accelerate asynchronous stochastic gradient descent @xcite .",
    "although the purpose of our paper is not to derive a new sgd method  we instead aim to arrive at a scalable mcmc sampler  it is instructive to take a closer look at the connection between the two methods .    to establish this connection we aim to re - derive the elastic averaging sgd ( easgd ) method from @xcite as the deterministic limit of the dynamical equations from . removing the added noise from these equations , setting @xmath95 to the identity matrix , and performing the variable substitutions @xmath96 , @xmath97 , @xmath98 yields the dynamical equations @xmath99 in comparison , re - writing the updates for the easgd variant with momentum ( eamsgd ) from @xcite in our notation ( and replacing the nesterov momentum with a standard momentum ) we obtain @xmath100 where , additionally , @xcite propose to only update @xmath101 every @xmath41 steps and drop the terms including @xmath101 in all update equations in intermittent steps .",
    "as expected , at first glance the two sets of update equations look very similar .",
    "interestingly , they do however differ with respect to integration of the elastic coupling term and the updates to the center variable : in the eamsgd equations the center variables are not augmented with a momentum term and the elastic coupling force influences the parameter values @xmath88 directly rather than , indirectly , through their momentum @xmath102 .    from the physics perspective that we adopt in this paper these updates are thus `` wrong '' in the sense that they break the interpretation of the variables @xmath88 , @xmath56 and @xmath102 as generalized coordinates and generalized momenta .",
    "it should be noted that there also is no straight - forward way to recover a valid sgmcmc sampler corresponding to a stochastic variant of equations from the hamiltonian given in equation .",
    "our derivation thus suggests alternative update equations for eamsgd .",
    "an interesting avenue for future experiments thus is to thoroughly compare the deterministic updates from equations with the eamsgd updates both in terms of empirical performance and with respect to their convergence properties .",
    "an initial test we performed suggests that the former perform at least as good as eamsgd .",
    "we also note that easgd without momentum can exactly be recovered as the deterministic limit of our approach ( without the above described discrepancies ) if we were to randomly re - sample the auxilliary momentum variables in each step  and would thus simulate stochastic gradient langevin dynamics @xcite .",
    "in this manuscript we have considered the problem of parallel asynchronous mcmc sampling with stochastic gradients .",
    "we have introduced a new algorithm for this problem based on the idea of elastically coupling multiple sgmcmc chains .",
    "first experiments suggest that the proposed method compares favorably to a naive parallelization strategy but additional experiments are required to paint a conclusive picture .",
    "we have further discussed the connection between our method and the recently proposed stochastic averaging sgd ( easgd ) optimizer from @xcite , revealing an alternative variant of easgd with momentum ."
  ],
  "abstract_text": [
    "<S> we consider parallel asynchronous markov chain monte carlo ( mcmc ) sampling for problems where we can leverage ( stochastic ) gradients to define continuous dynamics which explore the target distribution . </S>",
    "<S> we outline a solution strategy for this setting based on stochastic gradient hamiltonian monte carlo sampling ( sghmc ) which we alter to include an elastic coupling term that ties together multiple mcmc instances . </S>",
    "<S> the proposed strategy turns inherently sequential hmc algorithms into asynchronous parallel versions . </S>",
    "<S> first experiments empirically show that the resulting parallel sampler significantly speeds up exploration of the target distribution , when compared to standard sghmc , and is less prone to the harmful effects of stale gradients than a naive parallelization approach . </S>"
  ]
}