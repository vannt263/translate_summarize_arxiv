{
  "article_text": [
    "this paper develops tools for performing formal statistical inference for predictions generated by a broad class of methods developed under the algorithmic framework of data analysis . in particular , we focus on ensemble methods  combinations of many individual , frequently tree - based , prediction functions  which have played an important role .",
    "we present a variant of bagging and random forests , both initially introduced by @xcite , in which base learners are built on randomly chosen subsamples of the training data and the final prediction is taken as the average over the individual outputs .",
    "we demonstrate that this fits into the statistical framework of u - statistics , which were shown to have minimum variance by @xcite and later demonstrated to be asymptotically normal by @xcite .",
    "this allows us to demonstrate that under weak regularity conditions , predictions generated by these subsample ensemble methods are asymptotically normal .",
    "we also provide a method to consistently estimate the variance in the limiting distribution without increasing the computational cost so that we may produce confidence intervals and formally test feature significance in practice . though not the focus of this paper",
    ", it is worth noting that this subbagging procedure  suggested by @xcite for use in model selection  was shown by @xcite to outperform traditional bagging in many situations .",
    "we consider a general supervised learning framework in which an outcome @xmath0 is predicted as a function of @xmath1 features @xmath2 by the function @xmath3 = f(\\bm{x})$ ] .",
    "we also allow binary classification so long as the model predicts the probability of success , as opposed to a majority vote , so that the prediction remains real valued .",
    "additionally , we assume a training set @xmath4 consisting of @xmath5 independent examples from the process that is used to produce the prediction function @xmath6 . throughout the remainder of this paper",
    ", we implicitly assume that the dimension of the feature space @xmath1 remains fixed , though nothing in the theory provided prohibits a growing number of features so long as our other explicit conditions on the statistical behavior of trees are met .",
    "statistical inference proceeds by asking the counterfactual question ,  what would our results look like if we regenerated these data . \"",
    "that is , if a new training set was generated and we reproduced @xmath6 , how different might we expect the predictions to be ? to illustrate , consider the hypothesis that the feature @xmath7 does not contribute to the outcome at any point in the feature space :    @xmath8    a formal statistical test begins by calculating a test statistic @xmath9 and asks , `` if our data was generated according to @xmath10 and we generated a new training set and recalculated @xmath11 , what is the probability that this new statistic would be larger than @xmath12 ? ''",
    "that is , we are interested in estimating @xmath13 . in most fields , a probability of less than 0.05",
    "is considered sufficient evidence to reject the assumption that the data was generated according to @xmath10 .",
    "of course , a 0.05 chance can be obtained by many methods ( tossing a biassed coin , for example ) so we also seek a statistic @xmath11 such that when @xmath10 is false , we are likely to reject .",
    "this probability of correctly rejecting @xmath10 is known as the power of the test , with more powerful tests clearly being more useful .    here",
    "we propose to conduct the above test by comparing predictions generated by @xmath6 and @xmath14 . before doing so however",
    ", we consider the simpler hypothesis involving the value of a prediction :    @xmath15    though often of less scientific importance , hypotheses of this form allow us to generate confidence intervals for predictions .",
    "these intervals are defined to be those values of @xmath16 for which we do not have enough evidence to reject @xmath17 . in practice , we choose @xmath16 to be the prediction @xmath18 generated by the ensemble method in order to provide a formalized notion of plausible values of the prediction , which is , of course , of genuine interest .",
    "our results begin here because the statistical machinery we develop will provide a distribution for the values of the prediction .",
    "this allows us to address @xmath17 , after which we can combine these tests to address hypotheses like @xmath10 .    although this form of statistical analysis is ubiquitous in scientific literature , it is worthwhile contrasting this form of analysis with an alternative based on probably approximately correct ( pac ) theory , as developed by @xcite and @xcite",
    ". pac theory provides a uniform bound on the difference between the true error and observed training error of a particular estimator , also referred to as a hypothesis . in this framework",
    ", @xmath19 is some error of the function @xmath20 which is estimated by @xmath21 based on the data .",
    "a bound is then found for @xmath22 where @xmath23 is some class of functions that includes @xmath6 . since this bound is uniform over @xmath23",
    ", it applies to @xmath6 and we might think of comparing @xmath24 with @xmath25 using such bounds .",
    "while appealing , these bounds provide the accuracy of our estimate of @xmath26 but do not account for how the true @xmath26 might change when @xmath6 is reproduced with new training data .",
    "the uniformity of these bounds could be used to account for the uncertainty in @xmath6 if it is chosen to minimize @xmath21 over @xmath23 , but this is not always the case , for example , when using tree - based methods .",
    "we also expect the same uniformity to make pac bounds conservative , thereby resulting in tests with lower power than those we develop .",
    "our analysis relies on the structure of subsample - based ensemble methods , specifically making use of classic @xmath27-statistic theory .",
    "these estimators have a long history ( see , for example , original work by @xcite or @xcite , or @xcite which has a modern overview ) , frequently focussed on rank - based non - parametric tests , and have been shown to have an asymptotically normal sampling distribution by @xcite .",
    "our application to subsample ensembles requires the extension of these results to some new cases as well as methods to estimate the asymptotic variance , both of which we provide .",
    "u - statistics have traditionally been employed in the context of statistical parameter estimation . from this classical statistical perspective",
    ", we treat ensemble - tree methods like bagging and random forests as estimators and thus the limiting distributions and inference procedures we develop are with respect to the expected prediction generated by the ensemble .",
    "that is , given a particular prediction point @xmath28 , our limiting normal distributions are centered at the expected ensemble - based prediction at @xmath28 and not necessarily @xmath29 .",
    "such forms of distributional analysis are common in other nonparametric regression settings  see @xcite section 4.8 , for example .",
    "more details on appropriate interpretations of the results are provided throughout the paper , in particular in section 4.1 .    in order to claim that the inference procedures proposed here are asymptotically valid for @xmath29",
    ", the ensemble must consistently predict @xmath29 at a rate of @xmath30 or faster . though this is the case for many classical estimators , establishing fast uniform rates of convergence for tree - based ensembles",
    "has proven extremely difficult .",
    "@xcite discuss consistency of general partition - type models in the final chapter of their seminal book in the context of both classification and regression .",
    "@xcite restrict their attention to classification , but prove consistency of certain idealized bagging and random forest estimators , provided the individual trees are consistent .",
    "this paper also discusses a more general version of bagging , where the samples used to construct individual base learners may be proper subsamples of the training set taken with replacement as opposed to full bootstrap samples , so as to include the subbagging approach .",
    "@xcite further examines the consistency of random forests and investigates their behavior in the presence of a sparse feature space .",
    "recently , @xcite proved consistency for a mathematically tractable variant of random forests and in some cases , achieved empirical performance on par with the original random forest procedure suggested by breiman .",
    "@xcite prove consistency for their reinforcement learning trees , where embedded random forests are used to decide splitting variables , and achieve significant improvements in empirical mse for some datasets .",
    "however , no rates of convergence have been developed that could be applied to analyze the ensemble methods we consider here .    beyond these consistency efforts , mathematical analyses of ensemble learners",
    "has been somewhat limited . @xcite",
    "propose estimating the standard error of bagged trees and random forests using jackknife and bootstrap estimators .",
    "recently , @xcite proposed applying the jackknife and infinitesimal jackknife procedures introduced by @xcite for estimating standard errors in random forest predictions . @xcite",
    "have received significant attention for developing _ bart _ , a bayesian  sum - of - trees \" statistical model for the underlying regression function that allows for pointwise posterior inference throughout the feature space as well as estimates for individual feature effects .",
    "recently , @xcite extended the bart approach by suggesting a permutation - based approach for determining feature relevance and by introducing a procedure to allow variable importance information to be reflected in the prior .",
    "the layout of this paper is as follows : we demonstrate in section  [ sec : treesasustatistics ] that ensemble methods based on subsampling can be viewed as u - statistics . in section  [ sec : estimatingthevariance ]",
    "we provide consistent estimators of the limiting variance parameters so that inference may be carried out in practice .",
    "inference procedures , including a test of significance for features , are discussed in section  [ sec : inferenceprocedures ] .",
    "simulations illustrating the limiting distributions and inference procedures are provided in section  [ sec : simulations ] and the inference procedures are applied to a real dataset provided by cornell university s lab of ornithology in section  [ sec : realdata ] .",
    "we begin by introducing the subbagging and subsampled random forest procedures that result in estimators in the form of u - statistics . in both cases ,",
    "we provide an algorithm to make the procedure explicit .",
    "we begin with a brief introduction to u - statistics ; see @xcite for a more thorough treatment .",
    "let @xmath31 where @xmath32 is the parameter of interest and suppose that there exists an unbiased estimator @xmath33 of @xmath32 that is a function of @xmath34 arguments .",
    "then we can write    @xmath35    and without loss of generality , we may further assume that @xmath33 is permutation symmetric in its arguments since any given @xmath33 may be replaced by an equivalent permutation symmetric version .",
    "the minimum variance unbiased estimator for @xmath32 is given by    @xmath36    where the sum is taken over all @xmath37 subsamples of size @xmath38 and is referred to as a u - statistic with kernel @xmath33 of rank @xmath38 .",
    "when both the kernel and rank remain fixed , @xcite showed that these statistics are asymptotically normal with limiting variance @xmath39 where    @xmath40    and @xmath41 .",
    "the 1 in the subscript comes from the fact that there is 1 example in common between the two subsamples . in general",
    ", @xmath42 denotes a covariance in the form of ( [ zetadefn ] ) with @xmath43 examples in common .    given infinite computing power and a consistent estimate of @xmath44 , hoeffding s original result is enough to produce a subbagging procedure with asymptotically normal predictions .",
    "suppose that as our training set , we observe @xmath45 where @xmath2 is a vector of features and @xmath0 is the response .",
    "fix @xmath34 and let @xmath46 be a subsample of the training set . given a feature vector @xmath47 where we are interested in making a prediction , we can write the prediction at @xmath28 generated by a tree that was built using the subsample @xmath46 as a function @xmath48 from @xmath49 to @xmath50 . taking all @xmath37 subsamples , building a tree and predicting at @xmath28 with each , we can write our final subbagged prediction at @xmath28 as    @xmath51    by averaging the @xmath37 tree - based predictions . treating each ordered pair as one of @xmath38 inputs into the function @xmath48 ,",
    "the estimator in ( [ treeest ] ) is in the form of a u - statistic since tree - based estimators produce the same predictions independent of the order of the training data .",
    "thus , provided the distribution of predictions at @xmath28 has a finite second moment and @xmath52 , the distribution of subbagged predictions at @xmath28 is asymptotically normal . note that in this context , @xmath44 is the covariance between predictions at @xmath28 generated by trees trained on datasets with 1 sample in common .    of course , building @xmath37 trees is compuationally infeasible for even moderately sized training sets and an obvious substantial improvement in computationally efficiency can be achieved by building and averaging over only @xmath53 trees . in this case , the estimator in ( [ treeest ] ) , appropriately scaled , is called an _ incomplete _ u - statistic .",
    "when the @xmath54 subsamples are selected uniformly at random with replacement from the @xmath37 possibilities , the resulting incomplete u - statistic remains asymptotically normal ; see @xcite or @xcite page 200 for details .",
    "though more computationally efficient , there remains a major shortcomming with this approach : the number of samples used to build each tree , @xmath38 , remains fixed as @xmath55 .",
    "we would instead like @xmath38 to grow with @xmath5 so that trees can be grown to a greater depth , thereby presumably producing more accurate predictions . incorporating this ,",
    "our estimator becomes    @xmath56    statistics of this form were discussed by @xcite and called _ infinite order _ u - statistics ( ious ) in the complete case , when @xmath57 , and _ resampled _ statistics in the incomplete case .",
    "specifically , frees considers the situation where , given an @xmath58 sample @xmath59 and kernel @xmath60 , @xmath61 and @xmath62 and goes on to develop sufficient conditions for consistency and asymptotic normality whenever @xmath54 grows faster than @xmath5 .",
    "in contrast , the theorem below introduces a central limit theorem for estimators of the same form as in ( [ treeestgrowingrank ] ) but with respect to their individual means @xmath63 and covers all possible growth rates of @xmath54 with respect to @xmath5 . in this context ,",
    "only minimal regularity conditions are required for asymptotic normality .",
    "we begin with an assumption on the distribution of estimates for the general u - statistic case .",
    "* condition 1 : * _ let @xmath64 with @xmath65 and define @xmath66 .",
    "then for all @xmath67 , _",
    "@xmath68    this condition serves to control the tail behavior of the predictions and allows us to satisfy the lindeberg condition needed to obtain part _",
    "( i ) _ of theorem 1 below .",
    "[ subbaggingthm ] let @xmath69 and let @xmath70 be an incomplete , infinite order u - statistic with kernel @xmath60 that satisfies condition 1 .",
    "let @xmath71 such that @xmath72 for all @xmath5 and some constant @xmath73 , and let @xmath74 .",
    "then as long as @xmath75 and @xmath76 ,    a.   if @xmath77 , then @xmath78 . b.   if @xmath79 , then @xmath80 . c.   if @xmath81 , then @xmath82 .",
    "[ subbaggingthm ]    condition 1 , though necessary for the general u - statistic setting , is a bit obscure .",
    "however , in our regression context , when the regression function is bounded and the errors have exponential tails , a more intuitive lipschitz - type condition given in proposition 1 is sufficient .",
    "though stronger than necessary , this alternative condition allows us to satisfy the lindeberg condition and is reasonable to expect of any supervised learning method .    *",
    "proposition 1 : * _ for a bounded regression function @xmath20 , if there exists a constant @xmath43 such that for all @xmath83 , _",
    "where @xmath85 , @xmath86 , and where @xmath87 and @xmath88 are i.i.d . with exponential tails ,",
    "then condition 1 is satisfied .",
    "_    a number of important aspects of these results are worth pointing out .",
    "first , note from theorem [ subbaggingthm ] that the trees are built with subsamples that are approximately square root of the size of the full training set .",
    "this condition is not necessary for the proof , but ensures that the variance of the u - statistic in part @xmath89 converges to 0 as is typically the case in central limit theorems . by maintaining this relatively small subsample size",
    ", we can build many more trees and maintain a procedure that is computationally equivalent to traditional bagging based on full bootstrap samples . also note that no particular assumptions are placed on the dimension @xmath1 of the feature space ; the number of features may grow with @xmath5 so long as the stated conditions remain satisfied .",
    "the final condition of theorem [ subbaggingthm ] , that @xmath76 , though not explicitly controllable , should be easily satisfied in many cases . as an example , suppose that the terminal node size is bounded by @xmath90 so that trees built with larger training sets are grown to greater depths . then if the form of the response is @xmath91 where @xmath92 has variance @xmath93 , @xmath94 will be bounded below by @xmath95 .",
    "finally , note that the assumption of exponential tails on the distribution of regression errors in proposition 1 is stronger than necessary .",
    "indeed , so long as @xmath96 , we need only insist that @xmath97 .",
    "the proofs of theorem and proposition [ subbaggingthm ] are provided in appendix a. the subbagging algorithm that produces asymptotically normal predictions at each point in the feature space is provided in algorithm [ algo : subbagging ] .",
    "load training set select size of subsamples @xmath98 and number of subsamples @xmath54    take subsample of size @xmath98 from training set build tree using subsample use tree to predict at @xmath28    average the @xmath54 predictions to get final estimate @xmath99    note that this procedure is precisely the original bagging algorithm suggested by breiman , but with proper subsamples used to build trees instead of full bootstrap samples . in section [ sec : estimatingthevariance ] , we provide consistent estimators for the limiting variance parameters in theorem [ subbaggingthm ] so that we may carry out inference in practice .",
    "we would also like to acknowledge similar work currently in progress by @xcite .",
    "wager builds upon the potential nearest neighbor framework introduced by @xcite and seeks to provide a limiting distribution for the case where many trees are used in the ensemble , roughly corresponding to our result @xmath89 in theorems 1 and 2 .",
    "the author considers only an idealized class of trees based on the assumptions in @xcite as well as additional _ honesty _ and _ regularity _ conditions that allow @xmath98 to grow at a faster rate , and demonstrates that when many monte carlo samples are employed , the infinitesimal jackknife estimator of variance is consistent and predictions are asymptotically normal .",
    "this estimator has roughly the same computational complexity as those we propose in section 3 and should scale well subject to some additional bookkeeping .",
    "in contrast , the theory we provide here takes into account all possible rates of monte carlo sampling via the three cases discussed in theorems 1 and 2 and we provide a consistent means for estimating each corresponding variance .",
    "the distributional results described above for subbagging do not insist on a particular tree building method .",
    "so long as the trees generate predictions that satisfy minimal regularity conditions , the experimenter is free to use whichever building method is preferred .",
    "the subbagging procedure does , however , require that each tree in the ensemble is built according to the same method .",
    "this insistence on a uniform , non - randomized building method is in contrast with random forests .",
    "the original random forests procedure suggested by @xcite dictates that at each node in each tree , the split may occur on only a randomly selected subset of features .",
    "thus , we may think of each tree in a random forest as having an additional randomization parameter @xmath100 that determines the eligible features that may potentially be split at each node . in a general u - statistic context ,",
    "we can write this _ random kernel _",
    "u - statistic as    @xmath101    so that we can write a random forest estimator as    @xmath102    due to this additional randomness , random forests and random kernel u - statistics in general do not fit within the framework developed in the previous section so we develop new theory for this expanded class .",
    "suppose @xmath103 and that these randomization parameters are selected independently of the original sample @xmath104 .",
    "consider the statistic    @xmath105    so that @xmath106 .",
    "taking the expectation with respect to @xmath107 , the kernel becomes fixed and hence @xmath108 conforms to the non - random kernel u - statistic theory .",
    "thus , @xmath108 is asymptotically normal in both the complete and incomplete cases , as well as in the complete and incomplete infinite order cases , by theorem [ subbaggingthm ] . given this asymptotic normality of @xmath108 , in order to retain asymptotic normality of the corresponding random kernel version , we need only show that    @xmath109    we make use of this idea in the proof of the following theorem , which is provided in appendix a.    [ rfthm ] let @xmath110 be a random kernel u - statistic of the form defined in equation ( [ rfestimator ] ) such that @xmath108 satisfies condition 1 and suppose that @xmath111 for all @xmath5 , @xmath75 , and @xmath74 .",
    "then , letting @xmath112 index the subsamples , so long as @xmath76 and @xmath113 @xmath110 is asymptotically normal and the limiting distributions are the same as those provided in theorem  [ subbaggingthm ] .",
    "note that the variance parameters @xmath94 and @xmath114 in the context of random kernel u - statistics are still defined as the covariance between estimates generated by the ( now random ) kernels . thus , in the specific context of random forests , these variance parameters correspond to the covariance between predictions generated by trees , but each tree is built according to its own randomization parameter @xmath107 and this covariance is taken over @xmath107 as well .",
    "the final condition of theorem [ rfthm ] that    @xmath113    simply ensures that the randomization parameter @xmath107 does not continually pull predictions from the same subsample further apart as @xmath55 .",
    "this condition is satisfied , for example , if the response @xmath115 is bounded and should also be easily satisfied for any reasonable implementation of random forests .    the subsampled random forest algorithm that produces asymptotically normal predictions is provided in algorithm [ algo : rf ] . as with subbagging ,",
    "this subsampled random forest algorithm is exactly a random forest with subsamples used to build trees instead of full bootstrap samples .",
    "load training set select size of subsamples @xmath98 and number of subsamples @xmath54    select subsample of size @xmath98 from training set build tree based on randomization parameter @xmath116 use this tree to predict at @xmath28    average the @xmath54 predictions to obtain final estimate @xmath117    another random - forest - type estimator based on a crossed design that results in an infinite order _ generalized _ u - statistic is provided in appendix b. however , the above formulation most resembles breiman s original procedure and is more computationally feasible than the method mentioned in appendix b , so we consider only this random kernel version of random forests in the simulations and other work that follows .",
    "the limiting distributions provided in theorem [ subbaggingthm ] depend on the unknown mean parameter @xmath118 as well as the unknown variance parameters @xmath94 and @xmath114 . in order for us to be able to use these distributions for statistical inference in practice",
    ", we must establish consistent estimators of these parameters .",
    "it is obvious that we can use the sample mean  i.e. the prediction from our ensemble  as a consistent estimate of @xmath119 , but determining an appropriate variance estimate is less straightforward .    in equation ( [ zetadefn ] ) of the previous section , we defined @xmath120 as the covariance between two instances of the kernel with @xmath43 shared arguments , so the sample covariance between predictions may serve as a consistent estimator for both @xmath94 and @xmath114 .",
    "however , in practice we find that this often results in estimates close to 0 , which may then lead to an overall negative variance estimate .",
    "it is not difficult to show - see @xcite page 11 for details - that an equivalent expression for @xmath120 is given by    @xmath121    to estimate @xmath120 for our tree - based ensembles , we begin by selecting @xmath43 observations @xmath122 , which we refer to as initial fixed points , from the training set .",
    "we then select several subsamples of size @xmath98 from the training set , each of which must include @xmath122 , build a tree with each subsample , and record the mean of the predictions at @xmath28 .",
    "let @xmath123 ( mc for  monte carlo \" ) denote the number of subsamples drawn so that this average is taken over @xmath123 predictions .",
    "we then repeat the process for @xmath124 initial sets of fixed points and take our final estimate of @xmath120 as the variance over the @xmath124 final averages , yielding the estimator    @xmath125    where @xmath126 denotes the @xmath127 set of initial fixed points and @xmath128 denotes the @xmath129 subsample that includes @xmath126 ( which is used here as shorthand for the argument to the tree function @xmath130 ) .",
    "now , since we assume that the orginal data in the training set is i.i.d .",
    ", the random variables @xmath131 are also i.i.d .  and",
    "since the sample variance is a u - statistic , @xmath132 is a consistent estimator .",
    "the algorithm for calculating @xmath133 is provided in algorithm [ algo : zeta1 ] .",
    "note that when @xmath134 , each of the subsamples is identical so we need only use @xmath135 which simplifies the estimation procedure for @xmath114 .",
    "the procedure for calculating @xmath136 is provided in algorithm [ algo : zetak ] .",
    "select initial fixed point @xmath137    select subsample @xmath138 of size @xmath98 from training set that includes @xmath137 build tree using subsample @xmath138 use tree to predict at @xmath28    record average of the @xmath123 predictions    compute the variance of the @xmath124 averages    select subsample of size @xmath98 from training set build tree using subsample this subsample use tree to predict at @xmath28    compute the variance of the @xmath124 predictions    choosing the values of @xmath124 and @xmath123 will depend on the situation .",
    "the number of iterations required to accurately estimate the variance depends on a number of factors , including the tree building method and true underlying regression function .",
    "of course , ideally these estimation parameters should be chosen as large as is computationally feasible . in our simulations",
    ", we find that in most cases , only a relatively small number of initial fixed point sets are needed , but many more monte carlo samples are often necessary for accurate estimation . in most cases , we used an @xmath123 of at least 500 . recall",
    "that because our trees are built with small subsamples , we can build correspondingly more trees at the same computational cost .",
    "the algorithms for producing the subbagged or subsampled random forest predictions as well as the above algorithms for estimating the variance parameters are all that is needed to perform statistical inference . we can begin with algorithm [ algo : subbagging ] or [ algo : rf ] to generate the predictions , followed by algorithms [ algo : zeta1 ] and [ algo : zetak ] to estimate the variance parameters @xmath94 and @xmath114 .",
    "this procedure of running these 3 algorithms",
    "seperately is what we will refer to as the _ external _ variance estimation method , since the the variance parameters are estimated outside of the orginal ensemble .",
    "by contrast , we could instead generate the predictions and estimate the variance parameters in one procedure by taking the mean and variance of the predictions generated by the trees used to estimate @xmath94 .",
    "algorithm [ algo : internal ] outlines the steps in this _ internal _ variance estimation method .",
    "select initial fixed point @xmath137    select subsample @xmath138 of size @xmath98 from training set that includes @xmath137 build tree using subsample @xmath138 use tree to predict at @xmath28 and record prediction    record average of the @xmath123 predictions    compute the variance of the @xmath124 averages to estimate @xmath94 compute the variance of all predictions to estimate @xmath114 compute the mean of all predictions to estimate @xmath119    this internal variance estimation method is more computationally efficient and has the added benefit of producing variance estimates by simply changing the way in which the subsamples are selected .",
    "this means that we are able to obtain all parameter estimates we need to conduct inference at no greater computational cost than building the original ensemble .",
    "although theorems [ subbaggingthm ] and [ rfthm ] dictate that the subsamples used in the ensemble be selected uniformly at random , we find that the additional correlation introduced by selecting the subsamples in this way and using the same subsamples to estimate all parameters does not affect the limiting distribution .",
    "in this section , we describe the inference procedures that may be carried out after performing the estimation procedures .      in section  [ sec : treesasustatistics ]",
    ", we showed that predictions from subbagging and subsampled random forests are asymptotically normal and in section  [ sec : estimatingthevariance ] we provided consistent estimators for the parameters in the limiting normal distributions .",
    "thus , given a training set , we can estimate the approximate distribution of predictions at any given feature vector of interest @xmath28 . to produce a confidence interval for predictions at @xmath28",
    ", we need only estimate the variance parameters and take quantiles from the appropriate limiting distribution .",
    "formally , our confidence interval is @xmath139 $ ] where the lower and upper bounds , @xmath140 and @xmath141 , are the @xmath142 and @xmath143 quantiles respectively of the normal distribution with mean @xmath144 and variance @xmath145 where @xmath133 and @xmath136 are the variance estimates and @xmath146 .",
    "this limiting distribution is that given in result @xmath147 of theorem [ subbaggingthm ] which is the distribution we recommend using in practice .",
    "as mentioned in the introduction , these confidence intervals can also be used to address hypotheses of the form    @xmath148    formally , we can define the test statistic    @xmath149    and reject @xmath10 if @xmath150 is greater than the the @xmath151 quantile of the standard normal .",
    "this corresponds to a test with type 1 error rate @xmath152 so that @xmath153 = p[\\ ; |t| > 1 - \\frac{\\alpha}{2 } \\;|\\ ; \\theta_{k_n } = c ] = \\alpha$ ] .",
    "however , this testing procedure is equivalent to simply checking whether @xmath43 is within the calculated confidence interval : if @xmath43 is in the confidence interval , then we fail to reject this hypothesis that the true mean prediction is equal to @xmath43 , otherwise we reject .    finally , recall that these confidence intervals are for the expected prediction @xmath119 and not necessarily for the true value of the underlying regression function @xmath154 .",
    "if the tree building method employed is consistent so that @xmath155 , then as the sample size increases , the tree should be ( on average ) producing more accurate predictions , but in order to claim that our confidence intervals are asymptotically valid for @xmath32 , we need for this convergence to occur at rate of @xmath30 or faster .",
    "however , in general , the rate of convergence will depend on not only the tree - building method and true underlying regression function , but also on the location of the prediction point within the feature space .",
    "note that theorems 1 and 2 apply not only to tree - based ensembles , but to any estimator that can be written in the form of an infinite order u - statistic , as in equation ( [ treeestgrowingrank ] )",
    ". some of these ensembles may be straightforward to analyze , but for others , such as random forest predictions near the edge of the feature space , it may be difficult to establish a universal rate of consistency . however , even when @xmath30-consistency can not be guaranteed , these intervals still provide valuable information not currently available with existing tools . in these cases ,",
    "the confidence interval provides a reasonable range of values for where the prediction might fall if the ensemble was recomputed using a new training set ; areas of the feature space where confidence intervals are relatively large indicate regions where the ensemble is particularly unstable .",
    "compare this , for example , to the standard approach of withholding some ( usually small ) portion of the training set and comparing predictions made at these hold - out points to the corresponding known responses .",
    "such an approach provides some information as to the _ accuracy _ of the learner at specific locations throughout the feature space , but says nothing about the _ stability _ of these predictions .",
    "thus , instead of relying only on measures of overall goodness - of - fit such as mse or sse , these intervals allow users to investigate prediction variability at particular points or regions and in this sense , can be seen as a measure of how much the accuracy of predictions at that point is due to chance .",
    "the limiting distributions developed in theorems [ subbaggingthm ] and [ rfthm ] also allow us a way to test the significance of features .",
    "in many situations , data are recorded for a large number of features but a sparse true regression structure is suspected .",
    "suppose that the training set consists of @xmath1 features , @xmath156 and consider a reduced set @xmath157 .",
    "let @xmath158 be a set of feature vectors where we are interested in making predictions .",
    "also , let @xmath159 denote the function that maps feature vectors to their corresponding true mean prediction and let @xmath160 denote the same type of function that maps from the reduced feature space .",
    "that is , for a particular prediction point of interest @xmath28 , @xmath161 is the true mean prediction @xmath119 generated by trees built using the full feature space , and @xmath162 is the true mean prediction @xmath163 generated by trees that are only permitted to utilize features in the reduced set .",
    "then , for each test point @xmath164 , we would like to know whether @xmath165 so that we can determine the predictive influence of features not in @xmath166 .",
    "more formally , we would like to test the hypothesis    @xmath167    rejecting this null hypothesis means that a feature not in the reduced feature space @xmath166 makes a significant contribution to the prediction at at least one of the test points .    to perform this test with a training set of size @xmath5 , we take @xmath54 subsamples , each of size @xmath98 , and build a tree with each subsample .",
    "denote these subsamples @xmath168 and for a given feature vector @xmath169 , let @xmath170 denote the average over the predictions at @xmath169 generated from the @xmath54 trees .",
    "then , using the same subsamples @xmath168 , again build a tree with each , but using only those features in @xmath166 , and let @xmath171 be the average prediction at @xmath169 generated by these trees .",
    "finally , define the difference function    @xmath172    as the difference between the two ensemble predictions .",
    "note that we can write    @xmath173    so that this difference function is a u - statistic .",
    "thus , if we have only a single test point of interest , @xmath174 is asymptotically normal , so @xmath175 is asymptotically @xmath176 and we can use @xmath175 as a test statistic .",
    "however , it is more often the case that we have several test points of interest . in this case , define @xmath177 to be the vector of observed differences in the predictions    @xmath178    so that , provided a joint distribution exists with respect to lebesgue measure , @xmath177 has a multivariate normal distribution with mean vector    @xmath179    which we estimate with    @xmath180    as well as a covariance matrix @xmath181 .",
    "this covariance matrix has parameters @xmath182 and @xmath183 , the multivariate analogues of @xmath94 and @xmath114 .",
    "consistent estimators for these multivariate parameters can be obtained by simply replacing the variance calculation in algorithms [ algo : zeta1 ] and [ algo : zetak ] with a covariance . for clarity ,",
    "the procedure for obtaining @xmath184 is provided in algorithm [ algo : sigma1est ] in appendix c.    finally , combining these predictions to form a consistent estimator @xmath185 we have that    @xmath186    under @xmath10 .",
    "thus , in order to test the hypothesis in ( [ htlab1 ] ) , we compare the test statistic @xmath187 to the @xmath188 quantile of the @xmath189 distribution to produce a test with type 1 error rate @xmath152 . if our test statistic is larger than this critical value , we reject the null hypothesis .",
    "this setup , though straightforward , may not always definitively decide the significance of features . in some cases ,",
    "even randomly generated features that are unrelated to the response can be reported significant .",
    "depending on the building method , tree - based algorithms may take advantage of additional randomness in features even when the particular values of those features do not directly contribute to the response .",
    "for this reason , we also recommend repeating the testing procedure by comparing predictions generated using the full dataset to predictions generated by a dataset with randomly generated values  commonly obtained by permuting the values in the training set  for the features not in the reduced feature set to test hypostheses of the form    @xmath190",
    "the testing procedure remains exactly the same except that to calculate the second set of trees , we simply substitute the reduced training set for a training set with the same number of features , but with randomized values taking the place of the original values for the additional features . rejecting this null hypothesis allows us to conclude that not only do the additional features not in the reduced training set make a significant contribution to the predictions , but that the contribution is significantly more than could be obtained simply by adding additional randomness .",
    "there are also two additional tests that may be performed .",
    "first , we can test whether predictions generated by a training set with randomized values for the additional features are significantly different from predictions generated by the reduced feature set .",
    "if a significant difference is found , then the trees in the ensemble are making use of the additional randomness or possibly an accidental structure in the randomized features .",
    "as a final check , we can compare predictions generated by two training sets , each with randomized values for the features not in the reduced set .",
    "in the unlikely event that a significant difference is found between these predictions , it is again likely due to an accidental structure in the randomized values .",
    "both of these tests can be performed in exactly the same fashion by substituting the appropriate training sets .",
    "we present here a small simulation study in order to illustrate the limiting distributions derived in section [ sec : treesasustatistics ] and also to demonstrate the inference procedures proposed in the previous section .",
    "we consider two different underlying regression functions :    1 .",
    "@xmath191 ; @xmath192 $ ] 2 .",
    "@xmath193 ; @xmath194 ^ 5 $ ]    the first function corresponds to simple linear regression ( slr ) and was chosen for simplicity and ease of visualization .",
    "the second was initially considered by @xcite in development of the multivariate adaptive regression spline ( mars ) procedure and was recently investigated by @xcite . in each case , features were selected uniformly at random from the feature spaces and responses were sampled from @xmath195 , where @xmath196 , to form the training sets .",
    "we begin by illustrating the distributions of subbagged predictions . in the slr case",
    ", predictions were made at @xmath197 and in the mars case , predictions were made at @xmath198 .",
    "the histograms of subbagged predictions are shown in figure [ fig : slrandmarshist ] .",
    "each histogram is comprised of 250 simulations .",
    "histograms of subbagged predictions at @xmath197 in the slr case ( top row ) and at @xmath198 in the mars case ( bottom row ) .",
    "the total sample size , number of subsamples , and size of each subsample are denoted by @xmath5,@xmath199 , and @xmath38 , respectively in the plot titles . ]    for each histogram , the size of the training set @xmath5 , number of subsamples @xmath199 , and size of each subsample @xmath38 , is provided in the title .",
    "each tree in the ensembles was built using the ` rpart ` function in ` r ` , with the additional restriction that at least 3 observations per node were needed in order for the algorithm to consider splitting on that node .",
    "overlaying each histogram is the density obtained by estimating the parameters in the limiting distribution . in each case , we take the limiting distribution to be that given in result _",
    "( ii ) _ of theorem [ subbaggingthm ] ; namely that the predictions are normally distributed with mean @xmath200 and variance @xmath201 .",
    "the mean @xmath202 was estimated as the empirical mean across the 250 subbagged predictions . to estimate @xmath203 , 5000 new subsamples of size @xmath38",
    "were selected and with each subsample , a tree was built and used to predict at @xmath204 and @xmath205 was taken as the empirical variance between these predictions . to estimate @xmath44 , we follow the procedure in algorithm [ algo : zeta1 ] with @xmath206 and @xmath207 in the slr cases and with @xmath208 and @xmath207 in the mars cases .",
    "note that since we are only interested in verifying the distributions of predictions , the variance parameters are estimated only once for each case and not for each ensemble .",
    "it is worth noting that the same variance estimation procedure with @xmath208 and @xmath209 lead to an overestimate of the variance , so we reiterate that using a large @xmath210 seems to provide better results , even when @xmath211 is relatively small . in each case",
    ", we use @xmath212 as a plug - in estimate for @xmath213 .",
    "we also repeated this procedure and generated the distribution of predictions according to the internal variance estimation method described in algorithm [ algo : internal ] .",
    "details and histograms are provided in appendix d. these distributions appear to be the same as when the subsamples are selected uniformly at random , as in the external variance estimation method .",
    "histograms of subbagged predictions with larger subsample size @xmath38 and full bootstrap samples .",
    "predictions are made at @xmath198 . ]",
    "note that the distributional results in theorem [ subbaggingthm ] require @xmath214 , so in practice , the subsample size @xmath38 should be small relative to @xmath5 . in the above simulations , we choose @xmath38 slightly larger than @xmath30 and the distributions",
    "appear normally distributed with the correct limiting distribution . however , though this restriction on the growth rate of the subsample size is sufficient for asymptotic normality , it is perhaps not necessary . in our simulations",
    ", we found that ensembles built with larger @xmath38 are still approximately normal , but begin to look increasingly further from normal as @xmath38 increases .",
    "the histograms in figure [ fig : bigkhist ] show the distribution of subbagged predictions in the mars case with @xmath215 and @xmath216 and also with @xmath217 so that we are using full bootstrap samples to build the ensembles in the latter case .",
    "the parameters in the limiting distribution are estimated in exactly the same manner as with the smaller @xmath38 for the case where @xmath218 . in the bootstrap case",
    ", we can not follow our subbagging procedure exactly since the bootstrap samples used to build each tree in the ensemble must be taken with replacement , so we do not attempt to estimate the variance .",
    "these distributions look less normal and we begin to overestimate the variance in the case where @xmath218 .",
    "we move now to building confidence intervals for predictions and examine their coverage probabilities .",
    "we begin with the slr case , with @xmath219 , @xmath220 , and @xmath221 and as above , predict at @xmath204 . to build the confidence intervals , we generate 250 datasets and with each dataset",
    ", we produce a subbagged ensemble , estimate the parameters in the limiting distribution , and take the 0.025 and 0.975 quantiles from the estimated limiting normal distribution to form an approximate 95% confidence interval .",
    "the mean of this limiting normal @xmath222 was estimated as the mean of the predictions generated by the ensemble .",
    "the variance parameter @xmath203 was estimated by drawing 500 new subsamples , not necessarily used in the ensemble , and calculating the variance between predictions generated by the resulting 500 trees and @xmath44 was estimated externally using @xmath223 and @xmath224 .        in order to assess the coverage probability of our confidence intervals ,",
    "we first need to estimate the true mean prediction @xmath222 at @xmath204 that would be generated by this subbagging ensemble . to estimate this true mean",
    ", we built 1000 subbagged ensembles and took the mean prediction generated by these ensembles , which we found to be 20.02 - very close to the true underlying value of 20 .",
    "in this case , we found a coverage probability of 0.912 , which means that 228 of our 250 confidence intervals contained our estimate of the true mean prediction .",
    "these confidence intervals are shown in figure [ fig : slrci ] .",
    "the horizontal line in the plot is at 20.02 and represents our estimate of the true expected prediction .",
    "this same procedure was repeated for the slr case with @xmath215 and @xmath225 , the mars case with @xmath226 and @xmath227 , and the mars case with @xmath215 and @xmath228 . in each case",
    ", we produced 250 confidence intervals and the coverage probabilities are shown in table [ tablecovprob ] .",
    "the parameters in the limiting distributions were estimated externally in exactly the same fashion , using @xmath223 and @xmath224 to estimate @xmath44 .",
    "these slightly higher coverage probabilities mean that we are overestimating the variance , which is likely due to smaller values of the estimation parameters @xmath211 and @xmath210 being used to estimate @xmath44 .",
    "& & & & external variance estimate & internal variance estimate & + slr & 200 & 30 & 19.94 & 0.912 & 0.912 & + slr & 1000 & 60 & 19.99 & 0.956 & 0.936 & + mars & 500 & 50 & 17.43 & 0.980 & 0.984 & + mars & 1000 & 75 & 17.56 & 0.996 & 0.996 & +    we also repeated this procedure for generating confidence intervals using the internal variance estimation method .",
    "the resulting coverage probabilities are remarkably similar to the external variance estimation method and are shown in table [ tablecovprob ] .",
    "these ensembles were built using @xmath223 and @xmath224 .",
    "we now explore the hypothesis testing procedure for assessing feature significance .",
    "we focus on the mars case , where our training set now consists of 6 features @xmath229 , but the response @xmath115 depends only on the first 5 .",
    "the values of the additional feature @xmath230 are sampled uniformly at random from the interval @xmath231 $ ] and independently of the first 5 features .",
    "the top row of histograms involve test points equally spaced between 0 and 1 and the bottom row corresponds to points randomly selected from the interior of the feature space . ]",
    "we begin by looking at the distribution of test statistics when the test set consists 41 equally spaced points between 0 and 1 .",
    "that is , the first test point is @xmath232 , the second is @xmath233 , and so on so that the last test point is @xmath234 . for this test ,",
    "we are interested in looking at the difference between trees built using all features and those built using only the first 5 so that in the notation in section [ testsofsignificance ] , @xmath235 .",
    "we ran 250 simulations with @xmath236 , @xmath237 , and @xmath238 using a test set consisting of all 41 test points , the 20 central - most points , and the 5 central - most points .",
    "the parameter @xmath239 was estimated externally using @xmath240 and @xmath241 and @xmath242 was estimated by taking the covariance of the difference in predictions generated by 5000 trees .",
    "these covariance parameters are estimated only once instead of within each ensemble since we are only interested in the distribution of test statistics .",
    "histograms of the resulting test statistics along with an overlay of the estimated @xmath243 densities are shown in the top row of figure [ hyptesthist ] .",
    "we repeated this procedure , this time with a test set consisting of points in the center of the hypercube so we are not predicting close to any edge of the feature space .",
    "the value of each feature in the test set was selected uniformly at random from @xmath244 $ ] .",
    "a total of 41 such test points were generated , and histograms of the 250 resulting test statistics were produced in the cases where we use 5 of these test points , 20 test points , and all 41 test points .",
    "these histograms and estimated @xmath243 densities are shown in the bottom row of figure [ hyptesthist ] .",
    "note that the bottom row appears to be a better fit and thus there appears to be some bias occuring when test points are selected near the edges of the feature space .    to check the alpha level of the test  the probability of incorrectly rejecting the null hypothesis ",
    "we simulated 250 new training sets and used the test set consisting of 41 randomly selected central points .",
    "for each training set , we built full and reduced subbagged estimates , allowed and not allowed to utilize @xmath230 respectively , estimated the parameters , and performed the hypothesis test .",
    "for each ensemble , the variance parameter @xmath239 was estimated externally using @xmath223 and @xmath245 and @xmath242 was estimated externally on an independently generated set of trees .    in this setup ,",
    "none of the 250 simulations resulted in rejecting the null hypothesis , so our empirical alpha level was 0 .",
    "a histogram of the resulting test statistics is shown on the left of figure [ alphalevelhist ] ; the critical value , the 0.95 quantile of the @xmath246 , is 56.942 .",
    "thus , as with the confidence intervals , we are being conservative . recall that our confidence interval simulations with @xmath236 , @xmath237 , and @xmath238 predicting at @xmath247 captured our true value 99.6% of the time , so this estimate of 0 , though conservative , is not necessarily unexpected .",
    "we also repeated this procedure using an internal variance estimate with @xmath223 and @xmath245 and found an alpha level of 0.14 .",
    "the histogram of test statistics resulting from the internal variance estimation method is shown on the right in figure [ alphalevelhist ] . here",
    "we see that the correlation introduced by not taking an i.i.d .",
    "selection of subsamples to build the ensemble may be slightly inflating the test statistics .          thus far , our simulations have dealt only with subbagged ensembles , but recall that theorem [ rfthm ] established the asymptotic normality for predictions generated by random forests as well .",
    "the histograms in figure  [ rfhist ] show the distribution of predictions generated by subsampled random forests at @xmath248 when the true underlying function is the mars function .",
    "these trees were grown using the ` randomforest ` function in ` r ` with the ` ntree ` argument set to 1 . at each node in each tree , 3 of the 5 features @xmath249 were selected at random as candidates for splits and we insisted on at least 2 observations in each terminal node .",
    "the histograms show the empirical distribution of 250 subsampled random forest predictions and the overlaid density is the limiting normal @xmath250 with the variance parameters estimated externally .",
    "our estimate of the mean of this distribution was taken as the empirical mean of the 250 predictions .",
    "our estimate of @xmath203 was taken as the empirical variance of predictions across 5000 new trees and the estimate for @xmath44 was calculated with @xmath251 and @xmath252 .",
    "histograms of random forest predictions at @xmath248 with estimated normal density overlaid . ]",
    "we now apply our inference methods to a real dataset provided by the lab of ornithology at cornell university .",
    "the data is part of the ongoing _ ebird _ citizen science project described in @xcite .",
    "this project is hosted by cornell s lab of ornithology and relies on citizens , referred to as _",
    "birders _ , to submit reports of bird observations . location ,",
    "bird species observed and not observed , effort level , and number of birds of each species observed are just a few of the variables participants are asked to provide .",
    "in addition to the data contained in these reports , landcover characteristics as reported in the 2006 united states national land cover database are also available so that information about the local terrain may be used to help predict species abundance .    for our analysis ,",
    "we restrict our attention to observations ( and non - observations ) of the indigo bunting species .",
    "for the first part of our analysis , we further restrict our attention to observations made during the year 2010 .",
    "a little more than 400,000 reports of either presence or absence of indigo buntings were recorded during 2010 and the dataset consists of 23 features . like many species ,",
    "the abundance of indigo buntings is known to fluctuate throughout the year , so we have two primary goals : ( 1 ) to produce confidence intervals for monthly abundance and ( 2 ) to show that the feature ` month ' is significant for predicting abundance .",
    "[ fig : ibplot ]   monthly counts of indigo bunting observations.,title=\"fig : \" ]    a presence / absence plot of indigo buntings by month is shown in figure [ fig : ibplot ] .",
    "a few features of this plot are worth pointing out .",
    "most obviously , there are many more absence observations each month than presence observations .",
    "this makes sense because each time a birder submits a report , they note when indigo buntings are not present .",
    "next , we see that this species is only observed during the warmer months , so month seems highly significant for predicting abundance .",
    "finally , we see that all months have a large number of reports , so we need not worry about underreporting issues throughout the year .",
    "first we produce the confidence intervals .",
    "the goal is to get an idea of the monthly abundance , which we can think of as ` probability of observation ' , so our test points will be 12 vectors , one for each month . for the values of the other features , we will use the average of the values recorded for that feature , or , in the case of categorical features , use the most popular category .",
    "some features , such as elevation , have missing values which are not included in calculating the averages .",
    "we also removed the _ day _ feature from the training set , since day of the year is able to capture any effect of month . since the training set consists of approximately 400,000 observations , we use a subsample size @xmath253 , slightly more than the square root of the training set size .",
    "we build a total of @xmath254 trees and take the variance of these predictions at each point to be our estimates of @xmath203 .",
    "we use an external estimate with @xmath251 and @xmath241 to estimate @xmath44 . for each tree",
    "built , we require a minimum of 20 observations to consider splitting an internal node .",
    "we also repeat this procedure using an internal estimate of variance with @xmath251 and @xmath241 .    [",
    "fig : ebirdci ]   monthly confidence intervals for indigo bunting abundance.,title=\"fig : \" ]    the confidence intervals are shown in figure [ fig : ebirdci ] . note that the pattern seen in figure [ fig : ibplot ] is mirrored in the confidence interval plot : the confidence intervals are higher during months where more positive observations are recorded .",
    "it is also interesting to note that the width of the confidence intervals is larger during months of higher abundance .",
    "observe in figure [ fig : ibplot ] that even for months with many positive observations reported , many more negative observations are also reported .",
    "thus , for these months there are likely a number of trees in the ensemble with nearly all positive or negative observations so we expect a higher variance in predictions . for months when very few positive observations are made , nearly all observations in the terminal node",
    "will be abscence observations , thus resulting in a very small variance and much narrower confidence intervals .",
    "based on a visual inspection of the confidence intervals , there appears to be a clear significant difference in abundance between certain months , but we need to account for correlations in our predictions so we also conduct hypothesis tests .",
    "we conduct these formal tests for the significance of month , following the procedure in section [ testsofsignificance ] . to perform this test , we randomly selected 20 points from the training set as the test set and calculated the test statistic based on an internal variance estimate with @xmath251 and @xmath241 .",
    "we calculated a test statistic of 4233.10 and a critical value , the 0.95 quantile of the @xmath255 , of only 31.41 which yields a p - value of approximately 0 , so it seems that month is highly significant for predicting abundance .",
    "however , when we generated random values for month and repeated the testing procedure , we calculated a test statistic of 58.02 which , though significantly smaller than the test statistic calculated on the original training set , is still significant . to ensure that the randomized months we selected did not add any accidental structure , we compared predictions generated using this training set to those generated by another training set , also with randomized months . here",
    "we find a test statistic of only 2.36 and thus there is no significant difference between these predictions , so the trees are simply taking advantage of additional randomness .",
    "finally , we test for a difference in predictions generated by the original training set and those generated by the training set with random values of month . in this case , we calculated a test statistic of 2336.14 which is highly significant so we can conclude that month is significant for predicting abundance and the contribution to the prediction is significantly more than would be expected by simply adding a random feature to the model .",
    "this significant effect of month comes as no surprise as indigo buntings are known to be a migratory species .",
    "however , many scientists also believe that migrations may change from year to year and thus year may also be significant for predicting abundance . for this test",
    ", we used the full training dataset consisting of approximately 1 million observations from 2004 to 2010 and as a test set , randomly selected 20 observations from the training set . given this larger training set",
    ", we increased our subsample size to @xmath256 and our monte carlo sample size to @xmath257 and again performed the tests using the internel variance estimation method . in the first setup where we test predictions from the full training set against predictions generated from the training set without year , we find a test statistic of 94.43 which means that year is significant for making predictions as it is larger than the critical value of 31.41 . however , as was the case in testing the significance of month , we find that a randomly generated year feature is also significant with a test statistic of 52.51 . following in the same manner as above ,",
    "we compare predictions from two training sets , each with a randomized year feature , and we find no significant difference in these predictions with a test statistic of only 4.70 .",
    "finally , we test for a difference in predictions between the full training set and a dataset with random year and find that there is a significant difference in these predictions with a test statistic of 109.72 . thus , as was the case with month , we can conclude that year is significant for predicting abundance and the contribution to the prediction is significantly more than would be expected by simply adding a random feature to the model .",
    "this work demonstrates that formal statistical inference procedures are possible within the context of supervised ensemble learning , even when individual base learners are difficult to analyze mathematically .",
    "demonstrating that ensembles built with subsamples can be viewed as u - statistics allows us to calculate limiting distributions for predictions and consistent estimation procedures for the parameters allow us to compute confidence intervals for predictions and formally test the significance of features .",
    "moreover , using the internal variance estimation procedure , we are able to do so at no additional computational cost . in his controversial paper , @xcite contrasted traditional statistical data analysis models that emphasize interpretation with the modern algorithmic approach of machine learning where the primary goal is prediction and among the differences invoked was the statistician s concern for formalized inference .",
    "our hope is that this work be seen as something of a bridge between breiman s two cultures .",
    "the distributions and procedures we discuss apply to a very general class of supervised learning algorithms .",
    "we focus on bagging and random forests with tree - based base learners due to their popularity , but any supervised ensemble method that satisfies the conditions in theorems [ subbaggingthm ] and [ rfthm ] will generate predictions that have these limiting distributions and the inference procedures can be carried out in the same way . by the same reasoning",
    ", our procedures also make no restrictions on the tree - building method .",
    "there are also some small modifications that can be made to our procedure which would still allow for an asymptotically normal limiting distribution .",
    "first , we assumed that our subsamples were selected with replacement so that in theory , the same subsample could be selected multiple times .",
    "this choice was based primarily on the fact that ensuring the exact subsample was not taken twice would typically require extra computational work .",
    "however , even for relatively small datasets , the probability of selecting the same subsample more than once is very small , and not surprisingly , the limiting distributions remain identical when the subsamples are taken without replacement .",
    "furthermore , we also did not allow repeat observations within subsamples , which made the resulting estimator a u - statistic .",
    "had we selected the subsamples themselves with replacement , our estimator would be a v - statistic  a closely related class of estimators introduced by @xcite  and similar theory could be developed .",
    "it is also worth pointing out that in practice , we always selected the same number of subsamples @xmath54 and subsample size @xmath98 for each prediction point @xmath28 but in theory , these could be chosen differently for each point of interest and the same can be said of the estimation parameters @xmath211 and @xmath210 .",
    "however , choosing different values of these parameters for different prediction points involves significantly more bookkeeping and we advise against it in practice .",
    "our approach also raises some issues . perhaps most obviously , the parameter in our inference procedures is the expected prediction generated by trees built in the prescribed fashion , as opposed to the true value of the underlying regression function , @xmath29 .",
    "though some tree - building methods have been shown to be consistent , the bias introduced may not be negligible so we have to be careful about interpreting our results",
    ". it may be possible to employ a residual bootstrap to try and correct for bias and we plan to explore this in future work .",
    "another open question that we hope to address in future work is how to select the test points when testing feature significance . in the ebird application",
    ", we randomly selected points from the training set , but it would be beneficial to investigate optimal strategies for selecting both the number and location of test points . finally , the distributional results we provide could potentially allow us to test more complex hypotheses about the structure of the underlying regression function , for example , the interaction between covariates .",
    "this work was supported by the following grants : nsf deb-125619 , nsf cdi type ii 1125098 , and nsf dms-1053252 .",
    "the authors would also like to thank the lab of ornithology at cornell university for providing interesting data .",
    "33 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    savina andonova , andre elisseeff , theodoros evgeniou , and massimiliano pontil .",
    "a simple algorithm for learning stable machines . in _ ecai _ , pages 513517 , 2002 .",
    "grard biau .",
    "nalysis of a random forests model . _ the journal of machine learning research _ , 98888:0 10631095 , 2012 .",
    "grard biau , luc devroye , and gbor lugosi .",
    "onsistency of random forests and other averaging classifiers . _ the journal of machine learning research _ , 9:0 20152033 , 2008 .",
    "justin bleich , adam kapelner , edward  i george , shane  t jensen , et  al .",
    "variable selection for bart : an application to gene regulation . _ the annals of applied statistics _ , 80 ( 3):0 17501781 , 2014 .",
    "gunnar blom .",
    "ome properties of incomplete u - statistics .",
    "_ biometrika _ , 630 ( 3):0 573580 , 1976 .",
    "leo breiman . agging predictors .",
    "_ machine learning _ ,",
    "24:0 123140 , 1996 .    leo breiman .",
    "statistical modeling : the two cultures ( with comments and a rejoinder by the author ) . _ statistical science _ , 160 ( 3):0 199231 , 2001 .",
    "leo breiman .",
    "andom forests .",
    "_ machine learning _ , 45:0 532 , 2001 .",
    "leo breiman , jerome friedman , charles  j. stone , and r.a .",
    "olshen . _ classification and regression trees_. wadsworth , belmont , ca , 1st edition , 1984 .",
    "hugh  a chipman , edward  i george , and robert  e mcculloch .",
    ": bayesian additive regression trees . _",
    "the annals of applied statistics _ , 40 ( 1):0 266298 , 2010 .",
    "misha denil , david matheson , and nando de  freitas .",
    "narrowing the gap : random forests in theory and in practice .",
    "arxiv:1310.1415v1 [ stat.ml ] , october 2013 .",
    "bradley efron .",
    "estimation and accuracy after model selection .",
    "_ journal of the american statistical association _ ,",
    "1090 ( 507):0 9911007 , 2014 .",
    "randall  l eubank .",
    "_ nonparametric regression and spline smoothing_. crc press , 1999 .",
    "edward  w frees .",
    "nfinite order u - statistics .",
    "_ scandinavian journal of statistics _ , pages 2945 , 1989 .",
    "jerome  h friedman .",
    "ultivariate adaptive regression splines . _",
    "the annals of statistics _ , pages 167 , 1991 .",
    "paul  r halmos .",
    "the theory of unbiased estimation . _ the annals of mathematical statistics _ , 170 ( 1):0 3443 , 1946 .    wassily hoeffding . a class of statistics with asymptotically normal distribution .",
    "_ the annals of mathematical statistics _ , 190 ( 3):0 293325 , 1948 .    svante janson .",
    "the asymptotic distributions of incomplete u - statistics . _ zeitschrift fr wahrscheinlichkeitstheorie und verwandte gebiete _ , 660 ( 4):0 495505 , 1984",
    ".    maurice  g kendall . a new measure of rank correlation .",
    "_ biometrika _ , 300 ( 1/2):0 8193 , 1938 .    alan  j lee . _",
    "u - statistics : theory and practice_. crc press , new york , 1990 .",
    "yi  lin and yongho jeon .",
    "random forests and adaptive nearest neighbors .",
    "_ journal of the american statistical association _ , 1010 ( 474):0 578590 , 2006 .    nicolai meinshausen .",
    "quantile regression forests .",
    "_ the journal of machine learning research _ , 7:0 983999 , 2006 .    oseph sexton and petter laake .",
    "standard errors for bagged and random forest estimators .",
    "_ computational statistics & data analysis _ , 530 ( 3):0 801811 , 2009 .",
    "brian  l sullivan , christopher  l wood , marshall  j iliff , rick  e bonney , daniel fink , and steve kelling .",
    "ebird : a citizen - based bird observation network in the biological sciences .",
    "_ biological conservation _ ,",
    "1420 ( 10):0 22822292 , 2009 .",
    "leslie  g valiant .",
    "a theory of the learnable . _ communications of the acm _ , 270 ( 11):0 11341142 , 1984 .",
    "aad  w van  der vaart .",
    "_ asymptotic statistics _ , volume  3",
    ". cambridge university press , 2000 .",
    "vladimir  n vapnik and a  ya chervonenkis . on the uniform convergence of relative frequencies of events to their probabilities .",
    "_ theory of probability & its applications _ , 160 ( 2):0 264280 , 1971 .",
    "r  von mises . on the asymptotic distribution of differentiable statistical functions .",
    "_ the annals of mathematical statistics _ , 180 (",
    "3):0 309348 , 1947 .    stefan wager .",
    "symptotic theory for random forests .",
    "arxiv:1405.0352 [ math.st ] , may 2014 .",
    "stefan wager , trevor hastie , and bradley efron .",
    "confidence intervals for random forests : the jackknife and the infinitesimal jackknife .",
    "_ journal of machine learning research _ , 15:0 16251651 , 2014 .",
    "frank wilcoxon .",
    "individual comparisons by ranking methods . _ biometrics bulletin _ , 10 ( 6):0 8083 , 1945 .    faisal zaman and hideo hirose .",
    "effect of subsampling rate on subbagging and related ensembles of stable classifiers . in _",
    "pattern recognition and machine intelligence _ , pages 4449 .",
    "springer , 2009 .    ruoqing zhu , donglin zeng , and michael  r kosorok .",
    "reinforcement learning trees . _ journal of the american statistical association _ , 0 ( just - accepted ) , 2015 .",
    "we present here the proofs of the theorems provided in section  [ sec : treesasustatistics ] .",
    "we begin with a lemma from @xcite .",
    "[ leelemma ] ( lee 1990 , lemma a , page 201 ) let @xmath258 be a sequence of constants such that @xmath259 and @xmath260 and let the random variables @xmath261 have a multinomial distribution , @xmath262 .",
    "then as @xmath263 , the limiting distribution of    @xmath264    is @xmath265 .",
    "additionally , it will be useful to have the limiting distribution of complete infinite order u - statistics , which we provide in the lemma below .",
    "[ iousdistn ] let @xmath69 and let @xmath266 be a complete , infinite order u - statistic with kernel @xmath60 satisfying condition 1 and @xmath71 such that @xmath72 for all @xmath5 and some constant @xmath73 and @xmath75 .",
    "then    @xmath267    the proof of lemma [ leelemma ] is provided in @xcite page 201 .",
    "the proof of lemma [ iousdistn ] follows in exactly the same fashion as the proof of result _",
    "( i ) _ in theorem [ subbaggingthm ] below .",
    "we take advantage of these lemmas in the following proofs .",
    "* theorem 1 *    _ let @xmath69 and let @xmath70 be an incomplete , infinite order u - statistic with kernel @xmath60 that satisfies condition 1 .",
    "let @xmath71 such that @xmath72 for all @xmath5 and some constant @xmath73 , and let @xmath74 . then as long as @xmath75 and @xmath76 , _",
    "a.   if @xmath77 , then @xmath78 . b.   if @xmath79 , then @xmath80 . c.   if @xmath81 , then @xmath82 .",
    "* proof : * + _ ( i ) _ suppose first that @xmath77 . in the interest of clarity , we follow the hjek projection method discussed in @xcite chapters 11 and 12 .",
    "define the hjek projection of @xmath268 as    @xmath269    so that for each term in the sum , we have    @xmath270    where , in keeping with the notation in @xcite , we let @xmath112 index the subsamples . define @xmath271 and let @xmath272 be the number of subsamples that contain @xmath273 .",
    "since we assume that the subsamples are selected uniformly at random with replacement ,    @xmath274    so we can rewrite ( [ onesum ] ) as    @xmath275    so that taking the sum yields    @xmath276    now we establish the asymptotic normality of @xmath277 .",
    "define the triangular array @xmath278    so that for each variable in the array , we have    @xmath279    and    @xmath280    and thus the row - wise sum of the variances is    @xmath281    for @xmath67 , the lindeberg condition is given by @xmath282    by condition 1 , and thus the lindeberg condition is satisfied .",
    "thus , by the lindeberg - feller central limit theorem ,    @xmath283    or , rewriting ,    @xmath284    now , we need to compare the limiting variance ratio of @xmath285 and its hjek projection @xmath277 . for incomplete u - statistics of fixed rank",
    ", @xcite showed that the variance of the incomplete u - statistic @xmath286 consisting of @xmath199 subsamples selected uniformly at random with replacement is given by    @xmath287    where @xmath27 is the complete u - statistic analogue .",
    "extending this result to our situation where @xmath38 and @xmath199 may both depend on @xmath5 , we have    @xmath288    where the variance of the complete u - statistic @xmath289 is given by    @xmath290    the details of this calculation are described in @xcite page 163 .",
    "thus , looking at the limit of the variance ratio , we have    @xmath291    note that in the second line , @xmath292 since @xmath293 , @xmath294 by assumption , and @xmath295 since @xmath296 is bounded .",
    "finally , by slutsky s theorem and theorem 11.2 in @xcite , we have    @xmath297    where @xmath298 and    @xmath299    so that    @xmath300    as desired .",
    "_ ( ii ) _ @xmath302 _ ( iii ) _ now suppose @xmath303 .",
    "we follow the proof technique in @xcite page 200 which is based on the work of @xcite .",
    "let @xmath304 denote the set of all possible subsamples of size @xmath98 .",
    "in the following work , we use the notation @xmath305 in place of @xmath306 in subscripts and summation notation . consider the random vector @xmath307 , where the @xmath129 element denotes the number of times the @xmath129 subsample appears in @xmath285 . since the subsamples are selected uniformly at random with replacement , @xmath308 .",
    "let @xmath309 be the characteristic function of @xmath310 and let @xmath311 denote the limiting characteristic function of @xmath312 where @xmath266 is the corresponding complete u - statistic .",
    "additionally , let @xmath313 be the characteristic function of the random variable    @xmath314    then we have    @xmath315 \\right ) \\\\ & = \\mathbb{e } \\left ( exp \\left [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k_n ) } m_{s_i}(h_{k_n}(s_i ) - \\theta_{k_n } ) \\right ) \\right ] \\right ) \\\\ & = \\mathbb{e } \\left ( \\mathbb{e } \\left ( exp \\left [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k_n ) } m_{s_i}(h_{k_n}(s_i ) - \\theta_{k_n } ) \\right ) \\right ] \\middle \\vert z_1 , ... , z_n \\right ) \\right)\\\\ & = \\mathbb{e } \\bigg ( \\mathbb{e } \\bigg ( exp \\bigg [ it m_{n}^{-1/2 } \\bigg ( \\sum_{i=1}^{(n , k_n ) } \\bigg ( m_{s_i } + \\frac{m_n}{\\binom{n}{k_n } } -   \\frac{m_n}{\\binom{n}{k_n } } \\bigg ) \\\\ & \\hspace{53 mm } \\times \\big(h_{k_n}(s_i ) - \\theta_{k_n}\\big ) \\bigg ) \\bigg ] \\ ; \\bigg| \\ ; z_1 , ... , z_n \\bigg ) \\bigg)\\\\ & = \\mathbb{e } \\bigg ( \\mathbb{e } \\bigg ( exp \\bigg [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k_n ) } \\frac{m_n}{\\binom{n}{k_n } } ( h_{k_n}(s_i ) - \\theta_{k_n } ) \\right ) \\bigg ] \\\\ &",
    "\\hspace{8 mm } \\times exp \\bigg [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k ) } \\left ( m_{s_i } -   \\frac{m_n}{\\binom{n}{k_n } } \\right)(h_{k_n}(s_i ) - \\theta_{k_n } ) \\right ) \\bigg ] \\bigg| z_1 , ... , z_n \\bigg ) \\bigg)\\\\ & = \\mathbb{e } \\bigg ( exp \\bigg [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k_n ) } \\frac{m_n}{\\binom{n}{k_n } } ( h_{k_n}(s_i ) - \\theta_{k_n } ) \\right ) \\bigg ] \\\\ & \\hspace{8 mm } \\times \\mathbb{e } \\bigg ( exp \\bigg [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k_n ) } \\left ( m_{s_i } -   \\frac{m_n}{\\binom{n}{k_n } } \\right)(h_{k_n}(s_i ) - \\theta_{k_n } ) \\right ) \\bigg ] \\bigg| z_1 , ... , z_n \\bigg ) \\bigg)\\\\ & = \\mathbb{e } \\bigg ( exp \\bigg [ it \\sqrt{m_n } { u_{n , k_n}}\\bigg ] \\hspace{2 mm } \\phi_{n , k_n , m_n}^{(m)}(t ) \\bigg ) \\\\\\end{aligned}\\ ] ]    and now , taking limits , we have    @xmath316 \\hspace{2 mm } \\phi_{n , k_n , m_n}^{(m)}(t ) \\bigg ) \\\\ & = \\mathbb{e } \\bigg ( \\lim_{n \\rightarrow \\infty } exp \\bigg [ it \\sqrt{m_n } { u_{n , k_n}}\\bigg ] \\hspace{2 mm } \\lim_{n \\rightarrow \\infty } \\phi_{n , k_n , m_n}^{(m)}(t ) \\bigg ) \\\\\\end{aligned}\\ ] ]    so that by the preceeding lemmas ,    @xmath316 \\bigg ) exp \\bigg [ - t^2 \\zeta_{k_n , k_n } / 2 \\bigg ] \\\\ & = \\lim_{n \\rightarrow \\infty } \\mathbb{e } \\bigg ( exp \\bigg [ it \\big ( \\frac{\\sqrt{m_n}}{\\sqrt{n } } \\big ) \\sqrt{n } { u_{n , k_n}}\\bigg ] \\bigg ) exp \\bigg [ - t^2 \\zeta_{k_n , k_n } / 2 \\bigg ] \\\\ & = \\phi(\\alpha^{-1/2 } t ) exp \\bigg [ - t^2 \\zeta_{k_n , k_n } / 2 \\bigg ] \\\\ & = exp \\bigg [ - ( t \\alpha^{-1/2})^2 k_{n}^{2 } \\zeta_{1,k_n } / 2 \\bigg ] exp \\bigg [ - t^2 \\zeta_{k_n , k_n } / 2 \\bigg ] \\\\ & = exp \\bigg [ -t^2 \\big ( \\frac{k_{n}^{2}}{\\alpha } \\zeta_{1,k_n }   + \\zeta_{k_n , k_n } \\big ) / 2 \\bigg ] \\\\\\end{aligned}\\ ] ]    which is the characteristic function of a normal distribution with mean 0 and variance @xmath317 . note that when @xmath81 , the first term in the variance is 0 , so the limiting variance reduces to @xmath114 , as desired .",
    "@xmath318    * proposition 1 : * _ for a bounded regression function @xmath20 , if there exists a constant @xmath43 such that for all @xmath83 , _",
    "where @xmath85 , @xmath86 , and where @xmath87 and @xmath88 are i.i.d . with exponential tails ,",
    "then condition 1 is satisfied . _    * proof*. first consider the particular @xmath319 .",
    "since @xmath20 is bounded , we can define    @xmath320    and since tree - based predictions can not fall outside the range of responses in the training set , @xmath321 for all possible @xmath322 .",
    "furthermore , by the lipschitz condition ,    @xmath323    and thus , applying jensen s inequality , we have    @xmath324    now , define the set of interest in the lindeberg condition as @xmath325 so that we may write    @xmath326    finally , continuing from equation ( [ lct1 ] ) of theorem 1 ,    @xmath327 \\\\ & \\hspace{50 mm } \\times \\left ( ck_n \\mathbb{e } \\big| \\epsilon_1 \\big| + m - \\theta_{k_n } \\right)^2 \\\\ & = 0 \\\\\\end{aligned}\\ ] ]    as desired , so long as @xmath328  which is the case with exponential tails  and @xmath329 .",
    "@xmath318    * theorem 2 *    _ let @xmath110 be a random kernel u - statistic of the form defined in equation ( [ rfestimator ] ) such that @xmath108 satisfies condition 1 and suppose that @xmath111 for all @xmath5 , @xmath75 , and @xmath74",
    ". then , letting @xmath112 index the subsamples , so long as @xmath76 and _    @xmath113    @xmath110 is asymptotically normal and the limiting distributions are the same as those provided in theorem  [ subbaggingthm ] .",
    "* proof*. we begin with the case where @xmath77 and we make use of this result in the proof of the case where @xmath330 . as in section [ subsec : rf ] , define @xmath106 .",
    "we have    @xmath331 \\notag \\\\ & = \\mathbb{e } \\left [ \\frac{1}{m_n^2}\\left ( \\sum_{\\beta } h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) - \\mathbb{e}_{\\omega } \\left ( \\sum_{\\beta } h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) \\right ) \\right)^2 \\right ] \\notag \\\\ & = \\mathbb{e } \\frac{1}{m_n^2}\\left ( \\sum_{\\beta } h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) - \\left ( \\sum_{\\beta } \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) \\right ) \\right)^2 \\notag \\\\ & = \\mathbb{e } \\left [ \\frac{1}{m_n^2 } \\sum_{\\beta } \\left ( h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) - \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) \\right)^2 \\right ] \\notag \\\\ & \\hspace{3 mm } + \\mathbb{e } \\bigg [ \\frac{1}{m_n^2 } \\sum_{\\beta_{i } \\neq \\beta_{j } } \\left ( h_{k_n}^{(\\omega)}(z_{\\beta_{i_{1 } } } , ... , z_{\\beta_{i_{k_n } } } ) - \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{i_{1 } } } , ... , z_{\\beta_{i_{k_n } } } ) \\right ) \\notag \\\\ & \\hspace{50 mm } \\times \\ ; \\left ( h_{k_n}^{(\\omega)}(z_{\\beta_{j_{1 } } } , ... , z_{\\beta_{j_{k_n } } } ) - \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{j_{1 } } } , ... , z_{\\beta_{j_{k_n } } } ) \\right ) \\bigg ] \\notag \\\\\\end{aligned}\\ ] ]    we focus now on the second term , involving the cross terms with different subsamples and randomization parameters .",
    "splitting apart the expectation and moving the expectation with respect to @xmath107 inside , we can write the second term as    @xmath332 \\notag \\\\ & = \\mathbb{e}_{\\bm{x } } \\bigg [ \\frac{1}{m_n^2 } \\sum_{\\beta_{i } \\neq \\beta_{j } } \\mathbb{e}_{\\omega } \\left ( h_{k_n}^{(\\omega)}(z_{\\beta_{i_{1 } } } , ... , z_{\\beta_{i_{k_n } } } ) - \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{i_{1 } } } , ... , z_{\\beta_{i_{k_n } } } ) \\right ) \\\\ & \\hspace{15 mm } \\times \\mathbb{e}_{\\omega } \\left ( h_{k_n}^{(\\omega)}(z_{\\beta_{j_{1 } } } , ... , z_{\\beta_{j_{k_n } } } ) - \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{j_{1 } } } , ... , z_{\\beta_{j_{k_n } } } ) \\right ) \\bigg ] \\notag \\\\   & = \\mathbb{e}_{\\bm{x } } \\bigg [ \\frac{1}{m_n^2 } \\sum_{\\beta_{i } \\neq \\beta_{j } } \\left ( \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{i_{1 } } } , ... , z_{\\beta_{i_{k_n } } } ) - \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{i_{1 } } } , ... , z_{\\beta_{i_{k_n } } } ) \\right ) \\notag \\\\ & \\hspace{15 mm } \\times \\left ( \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{j_{1 } } } , ... , z_{\\beta_{j_{k_n } } } ) - \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{j_{1 } } } , ... , z_{\\beta_{j_{k_n } } } ) \\right ) \\bigg ] \\notag \\\\   & = \\mathbb{e}_{\\bm{x } } \\bigg [ \\frac{1}{m_n^2 } \\sum_{\\beta_{i } \\neq \\beta_{j } } 0 \\times 0 \\bigg ] \\notag \\\\ & = 0 \\notag \\end{aligned}\\ ] ]    and thus we need only investigate the first term .",
    "we have    @xmath333 \\\\ & = \\frac{1}{m_n^2 } \\sum_{\\beta } \\mathbb{e } \\left ( h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) - \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) \\right)^2 \\\\ & = \\frac{1}{m_n^2 } m_n \\mathbb{e } \\left ( h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) - \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) \\right)^2 \\\\ & = \\frac{1}{m_n } \\mathbb{e } \\left ( h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) - \\mathbb{e}_{\\omega } h_{k_n}^{(\\omega)}(z_{\\beta_{1 } } , ... , z_{\\beta_{k_n } } ) \\right)^2 . \\\\\\end{aligned}\\ ] ]    putting all of this together , we have    @xmath334    so long as @xmath335 and @xmath336 , as desired .",
    "@xmath301    now we handle the case where @xmath330 . first note that when @xmath337 for all @xmath5 , this reduces to simply averaging over an @xmath338 sample and thus asymptotic normality can be obtained via the classic central limit theorem so assume that eventually @xmath339 . the remaining steps in this proof",
    "are nearly identical to the proof of results _",
    "( ii ) _ and _ ( iii ) _ of theorem [ subbaggingthm ] . again , let @xmath304 denote the set of all possible subsamples of size @xmath98 and let @xmath307 denote the random vector that counts the number of times each subsample appears so that @xmath308 since we assume the subsamples are selected uniformly at random with replacement .",
    "define @xmath340 , let @xmath309 be the characteristic function of @xmath341 , and let @xmath311 denote the limiting characteristic function of @xmath342 .",
    "finally , let @xmath313 be the characteristic function of the random variable    @xmath343    then we have    @xmath344 \\right ) \\\\ & = \\mathbb{e } \\left ( exp \\left [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k_n ) } m_{s_i}({h_{k_n}^{(\\omega_i)}}(s_i ) - \\theta_{k_n}^ { * } ) \\right ) \\right ] \\right ) \\\\ & = \\mathbb{e } \\left ( \\mathbb{e } \\left ( exp \\left [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k_n ) } m_{s_i}({h_{k_n}^{(\\omega_i)}}(s_i ) - \\theta_{k_n}^ { * } ) \\right ) \\right ] \\middle \\vert z_1 , ... , z_n,\\omega \\right ) \\right)\\\\ & = \\mathbb{e } \\bigg ( \\mathbb{e } \\bigg ( exp \\bigg [ it m_{n}^{-1/2 } \\bigg ( \\sum_{i=1}^{(n , k_n ) } \\left ( m_{s_i } + \\frac{m_n}{\\binom{n}{k_n } } -   \\frac{m_n}{\\binom{n}{k_n } } \\right ) \\\\ & \\hspace{67 mm } \\times \\big({h_{k_n}^{(\\omega_i)}}(s_i ) - \\theta_{k_n}^{*}\\big ) \\bigg ) \\bigg ] \\ ; \\bigg| \\ ; z_1 , ... , z_n,\\omega \\bigg ) \\bigg)\\\\ & = \\mathbb{e } \\bigg ( \\mathbb{e } \\bigg ( exp \\bigg [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k_n ) } \\frac{m_n}{\\binom{n}{k_n } } ( { h_{k_n}^{(\\omega_i)}}(s_i ) - \\theta_{k_n}^ { * } ) \\right ) \\bigg ] \\\\ &",
    "\\hspace{10.5 mm } \\times exp \\bigg [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k ) } \\left ( m_{s_i } -   \\frac{m_n}{\\binom{n}{k_n } } \\right)({h_{k_n}^{(\\omega_i)}}(s_i ) - \\theta_{k_n}^ { * } ) \\right ) \\bigg ] \\bigg| z_1 , ... , z_n,\\omega \\bigg ) \\bigg)\\\\ & = \\mathbb{e } \\bigg ( exp \\bigg [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k_n ) } \\frac{m_n}{\\binom{n}{k_n } } ( { h_{k_n}^{(\\omega_i)}}(s_i ) - \\theta_{k_n}^ { * } ) \\right ) \\bigg ] \\\\ & \\hspace{3 mm } \\times \\mathbb{e } \\bigg ( exp \\bigg [ it m_{n}^{-1/2 } \\left ( \\sum_{i=1}^{(n , k_n ) } \\left ( m_{s_i } -   \\frac{m_n}{\\binom{n}{k_n } } \\right)({h_{k_n}^{(\\omega_i)}}(s_i ) - \\theta_{k_n}^ { * } ) \\right ) \\bigg ] \\bigg| z_1 , ... , z_n,\\omega \\bigg ) \\bigg)\\\\ & = \\mathbb{e } \\bigg ( exp \\bigg [ it m_{n}^{-1/2 } \\bigg ( \\sum_{i=1}^{(n , k_n ) } \\frac{m_n}{\\binom{n}{k_n } } \\bigg ( { h_{k_n}^{(\\omega_i)}}(s_i ) - \\mathbb{e}_{\\omega } { h_{k_n}^{(\\omega_i)}}(s_i ) \\\\ & \\hspace{69 mm } + \\mathbb{e}_{\\omega } { h_{k_n}^{(\\omega_i)}}(s_i ) - \\theta_{k_n}^ { * } \\bigg ) \\bigg ) \\bigg ] \\hspace{2 mm } \\phi_{n , k_n , m_n}^{(m)}(t ) \\bigg ) \\\\ & = \\mathbb{e } \\bigg ( exp \\bigg [ it \\sqrt{m_n } \\bigg ( \\frac{1}{\\binom{n}{k_n } } \\sum_{i=1}^{(n , k_n ) } \\bigg ( { h_{k_n}^{(\\omega_i)}}(s_i ) - \\mathbb{e}_{\\omega } { h_{k_n}^{(\\omega_i)}}(s_i ) \\bigg ) \\\\ & \\hspace{50 mm } + \\frac{1}{\\binom{n}{k_n } } \\sum_{i=1}^{(n , k_n ) } \\bigg ( \\mathbb{e}_{\\omega } { h_{k_n}^{(\\omega_i)}}(s_i ) - \\theta_{k_n}^ { * } \\bigg ) \\bigg ) \\bigg ] \\phi_{n , k_n , m_n}^{(m)}(t ) \\bigg ) .",
    "\\\\\\end{aligned}\\ ] ]    now , note that since we are in the case where @xmath303 , @xmath345 and thus , by the previous result in the case where @xmath77 , the first term converges to 0 and we have    @xmath346 \\hspace{2 mm } \\lim_{n \\rightarrow \\infty } \\phi_{n , k_n , m_n}^{(m)}(t ) \\bigg ) \\\\ & = \\mathbb{e } \\bigg ( \\lim_{n \\rightarrow \\infty } exp \\bigg [ it \\sqrt{m_n } \\ ; u_{\\omega ; n , k_n , m_n}^ { * } \\bigg ] \\hspace{2 mm } \\lim_{n \\rightarrow \\infty } \\phi_{n , k_n , m_n}^{(m)}(t ) \\bigg ) \\\\\\end{aligned}\\ ] ]    so that by exactly the same arguments as in the proof of theorem [ subbaggingthm ]    @xmath347 \\\\\\ ] ]    which is the characteristic function of a normal distribution with mean 0 and variance @xmath348 .",
    "further , when @xmath81 , the variance reduces to @xmath349 , as desired .",
    "typically , each tree in a random forest is built according to an independently selected randomization parameter @xmath100 .",
    "alternatively , for each @xmath100 , we could choose to build an entire set of trees , one for each of the @xmath54 subsamples , so that if @xmath350 randomization parameters are used , a total of @xmath351 trees are built .",
    "we could then write the prediction at @xmath28 generated by this alternative random forest estimator as    @xmath352    where here , we explicitly treat @xmath100 as an input to the function .",
    "statistics of the form in ( [ altrf ] ) are referred to as _ two - sample _ or _",
    "generalized _ u - statistics and similar results regarding asymptotic normality have been established for fixed - rank kernels ; see @xcite or @xcite for details .",
    "we mention this as a possible alternative only because the resulting estimator takes the well established form of a generalized u - statistic .",
    "readers familiar with u - statistics may be more comfortable with this approach than with the random kernel approach that more closely resembles the type of random forests used in practice .",
    "however , since this formulation strays from breiman s original procedure and is far more computationally intensive , we consider only the random kernel version described in section [ subsec : rf ] .",
    "the algorithm for estimating @xmath182 as needed for the hypothesis testing procedure is given below .",
    "select initial fixed point @xmath137    select subsample @xmath138 of size @xmath98 from training set that includes @xmath137 build _ full _ tree using subsample @xmath138 build _ reduced _ tree using subsample @xmath138 utilizing only reduced feature space use both full tree and reduced tree to predict at each test point record difference in predictions    record average of the @xmath123 differences in predictions    compute the covariance of the @xmath124 averages",
    "here we examine the distribution of predictions when the ensemble is built according to the internal variance estimation method described in algorithm [ algo : internal ] .",
    "since we are only interested in the distribution of predictions , we omit the steps in algorithm [ algo : internal ] for estimating the variance parameters and use the same estimates as in the external case above . for both the slr case with @xmath353 and the mars case with @xmath354 , we use @xmath206 and @xmath209 to produce a total of @xmath355 predictions in each ensemble .",
    "a total of 250 ensembles were built and the resulting histograms with estimated normal densities overlaid are shown in figure [ fig : internal ] below .",
    "we see that these distributions appear to be the same as when the subsamples are selected uniformly at random , as in the external variance estimation method ."
  ],
  "abstract_text": [
    "<S> this work develops formal statistical inference procedures for predictions generated by supervised learning ensembles . </S>",
    "<S> ensemble methods based on bootstrapping , such as bagging and random forests , have improved the predictive accuracy of individual trees , but fail to provide a framework in which distributional results can be easily determined . instead of aggregating full bootstrap samples , we consider predicting by averaging over trees built on subsamples of the training set and demonstrate that the resulting estimator takes the form of a u - statistic . as such , </S>",
    "<S> predictions for individual feature vectors are asymptotically normal , allowing for confidence intervals to accompany predictions . in practice </S>",
    "<S> , a subset of subsamples is used for computational speed ; here our estimators take the form of incomplete u - statistics and equivalent results are derived . </S>",
    "<S> we further demonstrate that this setup provides a framework for testing the significance of features . </S>",
    "<S> moreover , the internal estimation method we develop allows us to estimate the variance parameters and perform these inference procedures at no additional computational cost . </S>",
    "<S> simulations and illustrations on a real dataset are provided . </S>"
  ]
}