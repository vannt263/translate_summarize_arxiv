{
  "article_text": [
    "the pattern classification problem is a problem of assigning a discrete class label to a given data sample represented by its feature vector @xcite .",
    "it has many applications in various fields , including bioinforamtics @xcite , biometrics verification @xcite , computer networks @xcite , and computer vision @xcite .",
    "for example , in the face recognition problem , given a face image , the target of pattern classification is to assign it to a person who has been registered in a database @xcite .",
    "this problem is usually composed of two different components  feature extraction @xcite and classification @xcite .",
    "feature extraction refers to the procedure of extracting an effective and discriminant feature vector from a data sample , so that different samples of different classes could be separated easily .",
    "this procedure is usually highly domain - specific .",
    "for example , for the face recognition problem , the visual feature should be extracted using some image processing technologies , whereas for the problem of predicting zinc - binding sites from protein sequences , the biological features should be extracted using some biological knowledge @xcite . in terms of feature extraction of this paper",
    ", it is highly inspired by a hierarchical bayesian inference algorithm proposed in [ 24 ] .",
    "this new method created in @xcite has advanced the ground - truth feature extraction field and has provided a more optimal method for this procedure . on the other hand , different from feature extraction ,",
    "classification is a much more general problem .",
    "we usually design a class label prediction function as a classifier for this purpose .",
    "to learn the parameter of a classifier function , we usually try to minimize the classification error of the training samples in a training set and simultaneously reduce the complexity of the classifier . for example",
    ", the most popular classifier is support vector machine ( svm ) , which minimizes the hinge losses to reduce the classification error , and at the same time minimizes the @xmath0 norm of the classifier parameters to reduce the complexity . in this paper",
    ", we focus on the classification aspect .",
    "mutual information @xcite is defined as the information shared between two sets of variables .",
    "it has been used as a criterion of feature extraction for pattern classification problems @xcite .",
    "however , surprisingly , it has never been directly explored in the problem of classifier learning .",
    "actually , mutual information has a strong relation to kullback - leibler divergence , and there are many works using kl - divergence for classifiers @xcite .",
    "moreno et al .",
    "@xcite proposed a novel kernel function for support vector classification based on kullback - leibler divergence , while liu and shum @xcite proposed to learn the most discriminating feature that maximizes the kullback - leibler divergence for the adaboost classifier .",
    "however , both these methods do not use the kl - divergence based criterion to learn parameters of linear classifiers . to bridge this gap , in this paper , for the first time , we try to investigate using mutual information as a criterion of classifier learning .",
    "we propose to learn a classifier by maximizing the mutual information @xmath1 between the classification response variable @xmath2 and the true class label variable @xmath3",
    ". the classification response variable @xmath2 is a function of classifier parameters and data samples .",
    "the insight is that mutual information is defined as the information shared between @xmath2 and @xmath3 . from the viewpoint of information theory ,",
    "if the two variables are not mutually independent , and one variable is known , it usually reduces the uncertainty about the other one .",
    "then mutual information is used to measure how much uncertainty is reduced in this case . to illuminate how the mutual information can be used to measure the classification accuracy , we consider the two extreme cases :    * on one hand , if the classification response variable @xmath2 of a data sample is randomly given , and it is independent of its true class label @xmath3 , then knowing @xmath2 does not give any information about @xmath3 and vice versa , and the mutual information between them could be zero , i.e. , @xmath4 . * on the other hand ,",
    "if @xmath2 is given so that @xmath3 and @xmath2 are identical , knowing @xmath2 can help determine the value of @xmath3 exactly as well as reduce all the uncertainty about @xmath3 .",
    "this is the ideal case of classification , and knowing @xmath2 can reduce all the uncertainty about @xmath3 . in this case , the mutual information is defined as the uncertainty contained in @xmath2 ( or @xmath3 ) alone , which is measured by the entropy of @xmath2 or @xmath3 , denoted by @xmath5 or @xmath6 respectively , where @xmath7 is the entropy of a variable .",
    "since f and y are identical , we can have @xmath8 .    naturally , we hope that the classification response @xmath2 can predict the true class label @xmath3 as accurately as possible , and knowing @xmath2 can reduce the uncertainty about @xmath3 as much as possible .",
    "thus , we propose to maximize the mutual information between @xmath2 and @xmath3 with regard to the parameters of a classifier . to this end",
    ", we proposed a mutual information regularization term for the learning of classifier parameters .",
    "an objective function is constructed by combining the mutual information regularization term , a classification error term and a classifier complexity term .",
    "the classifier parameter is learned by optimizing the objective function with a gradient descend method in an iterative algorithm .",
    "the rest parts of this paper are organized as follows : in section [ sec : met ] , we introduce the proposed classifier learning method . the experiment results are presented in section [ sec : exp ] . in section [ sec : con ] the paper is concluded .",
    "in this section , we introduce the proposed classifier learning algorithm to maximize the mutual information between the classification response and the true class label .",
    "we suppose that we have a training set denoted as @xmath9 , where @xmath10 is the @xmath11-dimensional feature vector for the @xmath12-th training sample , and @xmath13 is the number of training samples . the class label set for the training samples is denoted as @xmath14 , where @xmath15 is the class label of the @xmath12-th sample .",
    "to learn a classifier to predict the class label of a given sample with its feature vector @xmath16 , we design a linear function as a classifier ,    @xmath17    where @xmath18 is the classifier parameter vector , @xmath19 is the classification response of @xmath16 given the classifier parameter @xmath18 , and @xmath20 is the signum function which transfers the classification response to the final binary classification result .",
    "we also denote the classification response set of the training samples as @xmath21 where @xmath22 is the classification response of the @xmath12-th training sample . to learn the optimal classification parameter @xmath18 for the classification problem , we consider the following three problems :      to learn the optimal classification parameter @xmath18",
    ", we hope the classification response @xmath2 of a data sample @xmath16 obtained with the learned @xmath18 can predict its true class label @xmath3 as accurately as possible .",
    "to measure the prediction error , we use a loss function to compare a classification response against its corresponding true class label . given the classifier parameter @xmath18 , the loss function of the @xmath12-th training sample @xmath23 with its classification response @xmath24 and true class label @xmath25 is denoted as @xmath26 .",
    "there are a few different loss functions which could be considered .",
    "hinge loss : :    is used by the svm classifier @xcite , and it is defined    as    +    @xmath27    +    where @xmath28 is defined as    +    @xmath29 squared loss : :    is usually used by regression problems @xcite , and it    is defined as    +    @xmath30 logistic loss : :    is defined as follows , and it is also popular in regression problems    @xcite ,    +    @xmath31    =    \\log \\left [ 1 +   \\exp(-y_i { { \\textbf{w}}}^\\top { { \\textbf{x}}}_i)\\right ] .",
    "\\end{aligned}\\ ] ] exponential loss : :    is anther popular loss function which could be used by both    classification and regression problems @xcite , which is defined as    +    @xmath32    obviously , to learn an optimal classifier , the average loss of all the training samples should be minimized with regard to @xmath18 . thus the following optimization problem is obtained by applying a loss functions to all training samples ,    @xmath33      to reduce the complexity of the classifier to prevent the over - fitting problem , we also regularize the classifier parameter by a @xmath0 norm term as    @xmath34      we also propose to learn the classifier by maximizing the mutual information @xmath1 between the classification response variables @xmath35 and the true class label variables @xmath36 .",
    "the mutual information between two variables @xmath35 and @xmath36 is defined as    @xmath37    where @xmath5 is the marginal entropy of @xmath2 , which is used to measure the uncertainty about @xmath2 , and @xmath38 is the entropy of @xmath2 conditional on @xmath3 , which is used as the measure of uncertainty of @xmath2 when @xmath3 is given . to use the mutual information as a criterion to learn the classifier parameters , we first need to estimate @xmath5 and @xmath39 .",
    "estimation of @xmath5 : :    we use the training samples to estimate @xmath5 , and    according to the definition of entropy , we have    +    @xmath40    +    where @xmath41 is the probability density of    @xmath2 .",
    "it could be seen that the entropy of @xmath2    is the expectation of @xmath42 @xcite .",
    "the    non - parametric kernel density estimation ( kde ) @xcite is used to    estimate the probability density function @xmath41 ,    +    @xmath43    +    where @xmath44 is the gaussian kernel function @xcite and @xmath45 is    the bandwidth parameter @xcite .",
    "estimation of @xmath39 : :    we also use the training samples to estimate @xmath39 , and    according to its definition , we have    +    @xmath46    +    where @xmath47 is the probability density of class label @xmath48 ,    @xmath49 is the number    of samples with the class label equal to @xmath48 , and    +    @xmath50    +    is the conditional entropy of @xmath2 given the class label    @xmath51 @xcite .",
    "we also use the kde to    estimate the conditional probability density function    +    @xmath52    +    substituting it to ( [ equ : h_fy ] ) , we have the estimated    @xmath39 ,    +    @xmath53    with the estimated entropy @xmath5 and the conditional entropy @xmath39 , the mutual information between the variable @xmath2 and @xmath3 could be rewritten as the function of parameter @xmath18 by substituting @xmath24 ,    @xmath54    to learn the classifier parameter @xmath18 , we maximize the mutual information with regard to @xmath18 ,    @xmath55    * remark * : it should be noted that similar to our method , the algorithm proposed in @xcite maximizes kl - divergence between the class pdf , @xmath56 , and the total pdf , @xmath41 , therefore , @xcite has relation to method in kullback - leibler boosting . however , different from our method , it uses kl - divergence as a criterion to select the most discriminating features , whereas our method uses mutual information as a criterion to learn the classifier parameter .      by combining the optimization problems proposed in ( [ equ : ob2 ] ) , ( [ equ : ob3 ] ) and ( [ equ : ob1 ] ) , the optimization problem for the proposed classifier parameter learning method is obtained as    @xmath57    where @xmath58 and @xmath59 are tradeoff parameters . in the objective function , there are three terms . the first one is optimized",
    "so that the prediction error is minimized , the second term is used to contral the complexity of the classifier , and the last term is introduced so that the mutual information between the classification response and the true class label can be maximized .",
    "direct optimization to ( [ equ : og4 ] ) is difficult . instead of seeking a closed - form solution",
    ", we try to optimize it using gradient descent method in an iterative algorithm @xcite . in each iteration , we employ the gradient descent method to update @xmath18 . according to the optimization theory , if @xmath60 is defined and differentiable in a neighborhood of a point @xmath61 , then @xmath60 decreases faster if @xmath18 goes from @xmath61 in the direction of the negative gradient of @xmath60 at @xmath61 , @xmath62 .",
    "thus the new @xmath18 is obtained by    @xmath63    where @xmath64 is the descent step .",
    "the key step is to compute the gradient of @xmath60 , which is calculated as    @xmath65    where @xmath66 and @xmath67 are the gradient of @xmath26 and @xmath68 respectively .",
    "they are given analytically as follows .",
    "we give the analytical gradients of different definitions of @xmath26 as follows :    hinge loss : :    is not a smooth function , but we can first update @xmath28    using previous @xmath18 as in ( [ equ : tau ] ) , and    then fix it when we derivate    @xmath66 ,    +    @xmath69 squared loss : :    is a smooth function , and its gradient is    +    @xmath70 logistic loss : :    is also smooth with its gradient as    +    @xmath71 exponential loss : :    is also smooth , and its gradient can be obtained as    +    @xmath72      the gradient of @xmath68 is computed as    @xmath73\\\\ = & - \\sum_{i=1}^n \\left(\\log p({{\\textbf{w}}}^\\top { { \\textbf{x}}}_i)+ 1 \\right ) \\nabla p({{\\textbf{w}}}^\\top { { \\textbf{x}}}_i ) \\\\ & + \\sum_{c\\in \\{+1,-1\\ } } \\frac{n_c}{n } \\left ( \\sum_{i : y_i = c } \\left ( \\log p({{\\textbf{w}}}^\\top { { \\textbf{x}}}_i|y = c ) +   1 \\right ) \\nabla p({{\\textbf{w}}}^\\top { { \\textbf{x}}}_i|y = c ) \\right ) , \\end{aligned}\\ ] ]    where the gradients of @xmath74 and @xmath75 are computed as    @xmath76",
    "in this section , we evaluate the proposed classification method on two real world pattern classification problems .      zinc is an important element for many biological processes of an organism , and it is closely related to many different diseases .",
    "moreover , it is also critical for proteins to play their functional roles @xcite .",
    "thus functional annotation of zinc - binding proteins is necessary to biological process control and disease treatment . to this end",
    ", predicting zinc - binding sites of proteins shows its importance in bioinformatics problems . in the first experiment",
    ", we evaluate the proposed classification method on the problem of predicting zinc - binding sites .      for the purpose of experiment , we collected a set of amino acids of four types , which are cys , his , glu and asp ( ched ) .",
    "these four types are the most common zinc - binding site types , which take up roughly 96% of the known zinc - binding sites . in the collected data set , there are 1,937 zinc - binding cheds and 11,049 non - zinc - binding cheds , resulting a data set of 13,986 data samples . given a candidate ched , the problem of zinc - binding site prediction is to predict if it is a zinc - binding site or a non - zinc - binding site . in this experiment",
    ", we treated a zinc - binding ched as a positive sample , and a non - zinc - binding ched as a negative sample . to extract features from a ched , we computed the position specific substitution matrices ( pssm ) @xcite , the relative weight of gapless real matches to pseudocounts ( rw - grmtp ) @xcite , shannon entropy @xcite , and composition of @xmath77-spaced amino acid pairs ( cksaap ) @xcite , and concatenated them to form a feature vector for each data sample",
    ". please note that the value of each feature was scaled to the range between -1 and 1 , so that the performance does not depend on the selection of scaling .    to conduct the experiment",
    ", we used the 10-fold cross validation protocol @xcite .",
    "the entire data set was split into ten non - overlapping folds , and each fold was used as a test set in turn , while the remaining nine folds were combined and used as a training set .",
    "the proposed algorithm was performed to the training set to learn a classifier from the feature vectors of the training samples , and then the learned classifier was used to predict the class labels of the test samples .",
    "please note that the tradeoff parameters of the proposed algorithm was tuned within the training set .",
    "the averaged value of the hyper - parameters @xmath58 and @xmath59 are 5.8 and 44.8 .",
    "the parameter @xmath45 was computed as @xmath78 , where @xmath79 was the median value of distances between pairs of training samples , and the averaged value of @xmath80 was 0.451 .",
    "the classification performance was measured by comparing the predicted labels against the true labels .",
    "the receiver operating characteristic ( roc ) and recall - precision curves were used as performance metrics .",
    "the roc curve was obtained by plotting true positive rates ( tprs ) against false positive rates ( fprs ) , while recall - precision curve was obtained by plotting precision against recall values .",
    "tpr , fpr , recall , and precision are defined as    @xmath81    where @xmath82 , @xmath83 , @xmath84 and @xmath85 represent the number of true positives , false positives , false negatives and true negatives , respectively .",
    "moreover , area under roc curve ( auc ) @xcite was used as a single performance measure .",
    "a good classifier should achieve a roc curve close to the top left corner of the figure , a recall - precision curve close to the top right corner , and also a high auc value .      in this experiment",
    ", we compared the proposed mutual information regularized classifier against the original loss functions based classifier without mutual information regularization , so that the improvement achieved by maximum mutual information regularization could be verified .",
    "the four different loss functions listed in section [ sec : met ] were considered , and the corresponding classifiers were evaluated here .",
    "the roc and recall - precision curves of four loss functions based classification methods are given in fig .",
    "[ fig : zincroc ] .",
    "the proposed maximum mutual information regularized method is denoted as  maxmutinf \" after a loss function title in the figure .",
    "it turns out that maximum mutual information regularization improves all the four loss functions based classification methods significantly .",
    "although various loss functions achieved different performances , all of them could be boosted by reducing the uncertainty about true class labels , which could be measured by the mutual information between class labels and classification responses .",
    "therefore , the results show that maximizing mutual information is highly effective in reducing uncertainty of true class labels , and hence it can significantly improve the quality of classification .",
    "-regularization , and `` loss - maxmutinf '' stands for combination of classification loss , @xmath0 and maximum mutual information - regularization.,title=\"fig:\",scaledwidth=70.0% ] +    moreover , we also plotted aucs of different methods in fig . [",
    "fig : zincauc ] .",
    "again , we observe that maximum mutual information regularization improves different loss functions based classifiers",
    ". we also can see that among these four loss functions , hinge loss achieves the highest auc values , while squared loss achieves the lowest .",
    "the auc value of classifiers regularized by both hinge loss and mutual information is 0.9635 , while that of squared loss and mutual information is even lower than 0.95 .",
    "the performances of logistic and exponential loss functions are similar , and they are between the performances of hinge loss and squared loss .",
    "+    since the mutual information is used as a new regularization technique , we are also interested in how the proposed regularization alone works . we therefore compared the following three cases .    1 .   * conventional case * which only uses the classification loss regularization .",
    "this case is corresponding to setting @xmath86 in ( [ equ : og4 ] ) . in this case",
    ", we only used the hinge loss since it has been shown that this loss function obtains better accuracy than other loss functions .",
    "* mutual information regularization case * which is corresponding to the problem in ( [ equ : og4 ] ) when the first term is ignored .",
    "* hybrid regularization case * which is the proposed framework which combines the classification loss minimization and mutual information regularization .",
    "the comparison results are given in fig .",
    "[ fig : figzinccompare ] .",
    "it can be seen that the conventional case which only uses the hinge loss function achieved better results than the method with only mutual information regularization , and the hybrid regularization achieved the best results .",
    "this means mutual information regularization can not obtain good performance by itself and should be used with traditional loss functions .",
    "antinuclear autoantibodies ( ana ) test is a technology used to determine whether a human immune system is creating antibodies to fight against infections .",
    "ana is usually done by a specific fluorescence pattern of hep-2 cell images @xcite .",
    "recently , there is a great need for computer based hep-3 cell image classification , since manual classification is time - consuming and not accurate enough . in the second experiment",
    ", we will evaluate the performance of the proposed classifier on the problem of classifying hep-2 cell images .      in this experiment",
    ", we used the database of hep-2 cell images of the icip 2014 competition of cell classification by fluorescent image analysis @xcite . in this data set , there are 13,596 cell images , and they belong to six cell classes , which are namely centromere , golgi , homogeneous , nucleolar , nuclearmembrane , and speckled . each cell image is segmented by a mask image showing the boundary of the cell .",
    "moreover , the entire data set is composed of two groups of different tensity types , which are intermediate and positive .",
    "overall , the intermediate group outnumbers the positive group , with an exception that , for the cases of centromere and speckled , the latter marginally outnumbers the former . the number of images in different classes of two groups are given in fig .",
    "[ fig : hep2data ] . to present each image for the classification problem , we extracted shape and texture features and concatenated them to form a visual feature vector @xcite .",
    "+    experiments were conducted in two groups respectively .",
    "we also adopted the 10-fold cross validation for the experiment . to handle the problem of multiple class problem",
    ", we used the one - against - all strategy .",
    "each class was treated as a positive class in turn , while all remaining five classes were combined to form a negative class .",
    "a classifier was learned for each class to discriminate it from other classes .",
    "a test sample was assigned to a class with the largest classification response .",
    "the classification accuracy was used as a classification performance metric .",
    "the boxplots of accuracies of the 10-fold cross validation on the two groups of hep-2 cell image data set are given in fig .",
    "[ fig : hep2result ] . from this figure",
    ", it could be observed that for both two groups of data sets , the proposed regularization method can improve the classification performances significantly , despite of the variety of loss functions .",
    "it can also be seen that the performances on the second group ( positive ) is inferior to that of the first group ( intermediate ) .",
    "this indicates that it is more difficult to classify cell images when their contrast is low .",
    "however , the improvement achieved by mutual information regularization is consistent over these two groups",
    ".     +   +   +   +",
    "can knowing the classification response of a data sample reduce uncertainty about its true class label ? in this paper , we proposed this question and tried to answer it by learning an optimal classifier to reduce such uncertainty .",
    "insighted by the fact that the reduced uncertainty can be measured by the mutual information between classification responses and true class labels , we proposed a new classifier learning algorithm , by maximizing the mutual information when learning the classifier . particularly , our algorithm adds a maximum mutual information regularization term .",
    "we investigated the classification performances when maximum mutual information was used to regularize the classifier learning based on four different loss functions .",
    "the the experimental results show that the proposed regularization can improve the classification performances of all these four loss function based classifiers . in the future , we will study how to apply the proposed algorithm on large scale dataset based on some distributed big data platforms @xcite and use it to signal and power integrity applications @xcite .",
    "this work was supported by grants from king abdullah university of science and technology ( kaust ) , saudi arabia .",
    "t.  ojala , m.  pietikinen , t.  menp , multiresolution gray - scale and rotation invariant texture classification with local binary patterns , ieee transactions on pattern analysis and machine intelligence 24  ( 7 ) ( 2002 ) 971987 .",
    "q.  sun , f.  hu , q.  hao , mobile target scenario recognition via low - cost pyroelectric sensing system : toward a context - enhanced accurate identification , ieee transactions on systems , man , and cybernetics .",
    "systems 44  ( 3 ) ( 2014 ) 375384 .",
    "j.  wang , x.  gao , q.  wang , y.  li , prodis - contshc : learning protein dissimilarity measures and hierarchical context coherently for protein - protein comparison in protein database retrieval , bmc bioinformatics 13  ( suppl 7 ) ( 2012 ) s2 .",
    "p.  wang , intelligent pattern recognition and applications to biometrics in an interactive environment , in : grapp 2009 - proceedings of the 4th international conference on computer graphics theory and applications , 2009 , pp .",
    "is21is22 .",
    "k.  roy , p.  bhattacharya , c.  y. suen , towards nonideal iris recognition based on level set method , genetic algorithms and adaptive asymmetrical svms , engineering applications of artificial intelligence 24  ( 3 ) ( 2011 ) 458475 .        l.  xu , z.  zhan , s.  xu , k.  ye , an evasion and counter - evasion study in malicious websites detection , in : 2014 ieee conference on communications and network security ( cns ) ( ieee cns 2014 ) , san francisco , usa , 2014 .",
    "q.  cai , y.  yin , h.  man , dspm : dynamic structure preserving map for action recognition , in : multimedia and expo ( icme ) , 2013 ieee international conference on , 2013 , pp . 16 . http://dx.doi.org/10.1109/icme.2013.6607606 [ ] .",
    "y.  zhou , l.  li , t.  zhao , h.  zhang , region - based high - level semantics extraction with cedd , in : network infrastructure and digital content , 2010 2nd ieee international conference on , ieee , 2010 , pp .",
    "404408 .    p.",
    "jonathon  phillips , h.  moon , s.  rizvi , p.  rauss , the feret evaluation methodology for face - recognition algorithms , ieee transactions on pattern analysis and machine intelligence 22  ( 10 ) ( 2000 ) 10901104 .",
    "wang , i.  almasri , x.  gao , adaptive graph regularized nonnegative matrix factorization via feature selection , in : pattern recognition ( icpr ) , 2012 21st international conference on , ieee , 2012 , pp . 963966 .",
    "t.  subbulakshmi , a.  afroze , multiple learning based classifiers using layered approach and feature selection for attack detection , in : 2013 ieee international conference on emerging trends in computing , communication and nanotechnology , ice - ccn 2013 , 2013 , pp .",
    "308314 .",
    "z.  chen , y.  wang , y .- f .",
    "zhai , j.  song , z.  zhang , zincexplorer : an accurate hybrid method to improve the prediction of zinc - binding sites from protein sequences , molecular biosystems 9  ( 9 ) ( 2013 ) 22132222 .",
    "p.  j. moreno , p.  p. ho , n.  vasconcelos , a kullback - leibler divergence based kernel for svm classification in multimedia applications , in : advances in neural information processing systems 16 , mit press , 2004 , pp . 13851392 .",
    "o.  yildiz , e.  alpaydin , statistical tests using hinge/-sensitive loss , in : computer and information sciences iii - 27th international symposium on computer and information sciences , iscis 2012 , 2013 , pp .",
    "153160 .",
    "s.  bach , b.  huang , b.  london , l.  getoor , hinge - loss markov random fields : convex inference for structured prediction , in : uncertainty in artificial intelligence - proceedings of the 29th conference , uai 2013 , 2013 , pp .",
    "3241 .",
    "a.  elgammal , r.  duraiswami , d.  harwood , l.  davis , background and foreground modeling using nonparametric kernel density estimation for visual surveillance , proceedings of the ieee 90  ( 7 ) ( 2002 ) 11511162 .",
    "s.  zhong , d.  chen , q.  xu , t.  chen , optimizing the gaussian kernel function with the formulated kernel target alignment criterion for two - class pattern classification , pattern recognition 46  ( 7 ) ( 2013 ) 20452054 .",
    "a.  carvalho , p.  ado , p.  mateus , efficient approximation of the conditional relative entropy with applications to discriminative learning of bayesian network classifiers , entropy 15  ( 7 ) ( 2013 ) 27162735 .",
    "a.  porta , g.  baselli , d.  liberati , n.  montano , c.  cogliati , t.  gnecchi - ruscone , a.  malliani , s.  cerutti , measuring regularity by means of a corrected conditional entropy in sympathetic outflow , biological cybernetics 78  ( 1 ) ( 1998 ) 7178 .",
    "a.  kumar , d.  hati , t.  thaker , l.  miah , p.  cunningham , c.  domene , t.  bui , a.  drake , l.  mcdermott , strong and weak zinc binding sites in human zinc-2-glycoprotein , febs letters 587  ( 24 ) ( 2013 ) 39493954 .",
    "z.  liu , y.  wang , c.  zhou , y.  xue , w.  zhao , h.  liu , computationally characterizing and comprehensive analysis of zinc - binding sites in proteins , biochimica et biophysica acta - proteins and proteomics 1844  ( 1 part b ) ( 2014 ) 171180 .",
    "s.  menchetti , a.  passerini , p.  frasconi , c.  andreini , a.  rosato , improving prediction of zinc binding sites by modeling the linkage between residues close in sequence , in : research in computational molecular biology , springer , 2006 , pp .",
    "309320 .",
    "p.  agrawal , m.  vatsa , r.  singh , hep-2 cell image classification : a comparative analysis , in : lecture notes in computer science ( including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics ) , vol .",
    "8184 lncs , 2013 , pp .",
    "195  202 .",
    "y.  wang , y.  su , g.  agrawal , supporting a light - weight data management layer over hdf5 , in : cluster , cloud and grid computing ( ccgrid ) , 2013 13th ieee / acm international symposium on , ieee , 2013 , pp . 335342 .",
    "y.  wang , w.  jiang , g.  agrawal , scimate : a novel mapreduce - like framework for multiple scientific data formats , in : cluster , cloud and grid computing ( ccgrid ) , 2012 12th ieee / acm international symposium on , ieee , 2012 , pp .",
    "443450 .",
    "q.  sun , f.  hu , h.  qi , context awareness emergence for distributed binary pyroelectric sensors , in : multisensor fusion and integration for intelligent systems ( mfi ) , 2010 ieee conference on , ieee , 2010 , pp .",
    "162167 .",
    "h.  liu , f.  shi , y.  wang , n.  wong , frequency - domain transient analysis of multitime partial differential equation systems , in : vlsi and system - on - chip ( vlsi - soc ) , 2011 ieee / ifip 19th international conference on , ieee , 2011 , pp .",
    "160163 .",
    "y.  wang , z.  zhang , c .- k .",
    "koh , g.  shi , g.  k. pang , n.  wong , passivity enforcement for descriptor systems via matrix pencil perturbation , computer - aided design of integrated circuits and systems , ieee transactions on 31  ( 4 ) ( 2012 ) 532545 .",
    "lei , y.  wang , q.  chen , n.  wong , on vector fitting methods in signal / power integrity applications , in : proceedings of the international multiconference of engineers and computer scientists 2010 , imecs 2010 , newswood limited .",
    ", 2010 , pp .",
    "14071412 .",
    "y.  wang , z.  zhang , c .- k .",
    "koh , g.  k. pang , n.  wong , peds : passivity enforcement for descriptor systems via hamiltonian - symplectic matrix pencil perturbation , in : proceedings of the international conference on computer - aided design , ieee press , 2010 , pp ."
  ],
  "abstract_text": [
    "<S> in this paper , a novel pattern classification approach is proposed by regularizing the classifier learning to maximize mutual information between the classification response and the true class label . </S>",
    "<S> we argue that , with the learned classifier , the uncertainty of the true class label of a data sample should be reduced by knowing its classification response as much as possible . </S>",
    "<S> the reduced uncertainty is measured by the mutual information between the classification response and the true class label . to this end , </S>",
    "<S> when learning a linear classifier , we propose to maximize the mutual information between classification responses and true class labels of training samples , besides minimizing the classification error and reducing the classifier complexity . </S>",
    "<S> an objective function is constructed by modeling mutual information with entropy estimation , and it is optimized by a gradient descend method in an iterative algorithm . </S>",
    "<S> experiments on two real world pattern classification problems show the significant improvements achieved by maximum mutual information regularization .    pattern classification , maximum mutual information , entropy , gradient descend </S>"
  ]
}