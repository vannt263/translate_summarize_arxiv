{
  "article_text": [
    "clustering is the process of grouping similar objects together while separating dissimilar ones apart .",
    "this simple idea has a wide range of applications in different areas of scientific research . in _",
    ", clustering can identify co - expressed genes that work together for the same metabolic pathway  . in _ neuroscience",
    "_ , clustering can also identify regions of neurons in the brain that are physically or functionally connected  @xcite .",
    "both the _ human genome _ and the _ human connectome _ are highly complex systems , with about @xmath0 protein - coding genes in the human genome  @xcite and @xmath1 neurons composing the cerebral cortex of the human brain  @xcite .",
    "therefore , the ability to group similar genes or neurons together based on their interactions is very helpful , as it reduces the complex systems into smaller , and so , more manageable subsystems for further studies",
    ".    there are different techniques for capturing the detailed physical structure and functional interaction in a biological system",
    ". for the human genome , the expression levels of different genes in different individuals ( or tissues ) can be measured by the microarray analysis  @xcite or rna - sequencing  @xcite . for the physical connections of the neurons , called the physical connectome ,",
    "electron microscopy ( em ) has been used to map out the entire structural interconnections of the neurons in a small living creature called the nematode _ caenorhabditis elegans _",
    ".  elegans _ )  @xcite . for the human brain of living subjects",
    ", em does not apply , but a magnetic resonance imaging ( mri ) technique called diffusion spectral imaging ( dsi ) can be used instead  @xcite .",
    "the functional connectome of the neurons can also be studied by capturing the stimulation patterns of the neurons directly using electroencephalography ( eeg ) or indirectly using another mri technique called functional mri ( fmri ) . however , with the huge volume and variety of data available  @xcite , _ the main challenge is to automate the clustering process using a mathematical criterion that leads to meaningful , yet arithmetically simple to compute , clusters . _",
    "we believe that the key to this challenge lies in _ a better understanding of what information is , and how we can measure mutual information quantitatively_. in this work , we propose a novel information - theoretic approach to clustering , called _ info - clustering _ , and show that it applies to the study of the complex biological systems of the genes and neurons .",
    "the idea is to _ regard each object as a piece of information , and then group subsets of the objects together if their mutual information exceeds a certain threshold .",
    "_ by varying the threshold value , a hierarchy of clusters can be obtained .",
    "an application of info - clustering is in the study of the human genome .",
    "we know that the biological information of a human being is encoded entirely in its dna sequences .",
    "a dna sequence is further divided into segments called the genes .",
    "some genes are protein - coding in the sense that they express themselves in the form of gene products such as enzymes , hormones and receptors .",
    "these proteins carry out important functions that sustain different metabolic pathways . however , it is not entirely clear    1 .",
    "how the genes work together to sustain the metabolic pathways , and 2 .",
    "how do mutations of the genes cause a certain disease such as cancer .",
    "clustering is a helpful first step in studying the metabolic pathways and disease pathology .",
    "this is because it helps identify smaller subsets of related genes that work closely together .",
    "more precisely , although different genes express differently in different people , or even in different tissues of the same person , genes that are co - regulated tend to have similar expression patterns  @xcite",
    ". such co - expression of the genes means that there is mutual information among the genes .",
    "if we have a way to measure such information , then we can cluster the genes according to their mutual information .",
    "another application of info - clustering is in the study of the human brain .",
    "we know that the brain carries out important tasks such as perception , emotion , thought and memory .",
    "the way it works is that , the human brain consists of many cells called neurons .",
    "these neurons are physically wired together by fiber - like projections called axons .",
    "the neurons stimulate each other in some pathway circuitries to carry out the important brain functions .",
    "more precisely , when a neuron is excited by an external stimulation , it sends an electrical signal down the axon , which stimulates one or more target neurons through the synapses .",
    "the stimulation mechanism of the neurons has inspired a family of learning models in artificial intelligence called artificial neural networks . in machine learning , such artificial neural networks can be used by deep learning methods to perform complicated tasks such as image recognition  @xcite . the performance of such methods often superior to alternative approaches .",
    "however , exactly why the technique works so well is not entirely clear .",
    "it remains a mystery as to    1 .",
    "how the stimulations of neurons lead to the complicated brain functions , and 2 .",
    "how the damages or anomalies in the brain lead to mental disorders such as schizophrenia , bipolar disorder , autism and attention deficit hyperactivity disorder .",
    "we believe information theory  @xcite lies close to the heart of these problems because the stimulation mechanism by electrical and chemical signals are simply transmission and processing of information .",
    "it was recently discovered  @xcite that the brain segregates into tightly connected regions , and there are important network hubs , called the rich - clubs , that connect between the different regions .",
    "most neural signals pass through those network hubs , and therefore , damages to such network hubs can be detrimental . on the other hand , such information super highways were found to improve the performance of artificial neural networks  @xcite , because they allow many layers of neurons to communicate effectively with each other . indeed , the formation of communities and the small - world topology  @xcite are observed in social networks where people interact by communicating information .",
    "since neurons also interact by transmitting information , we believe info - clustering can be applied to these information systems to discover or explain the communities with a large amount of intra - cluster communications as well as network hubs that support important inter - cluster communications .      in this section",
    ", we give a summary of the contributions of this work and a brief survey of previous works pointing out , whenever possible , similarities and differences between info - clustering and existing approaches .",
    "this brief survey is neither complete nor intended to present info - clustering as a replacement of existing approaches , but rather to motivate info - clustering and help properly position it relative to existing works .",
    "many clustering algorithms have been proposed , even for gene clustering  @xcite . however , the conventional approach has been typically of a heuristic nature with a primary focus on algorithmic simplicity  @xcite .",
    "such an _ algorithmic approach _ suffers several shortcomings , as it was already indicated by some researchers  @xcite .",
    "for example , the well - known @xmath2-means clustering algorithm and self - organizing map require prior knowledge of the number of clusters , which is a well - known difficult task .",
    "for the @xmath2-means clustering algorithm , the similarity between objects is measured by the distance between the data points associated with the objects .",
    "this raises the concern that there are several different choices for defining the distance between two points or two clusters .",
    "various mathematical criteria have been proposed . however , such criteria appear to be  easy to fool \" in the sense that there are examples for which the resulting clustering solution is obviously not the desired one  @xcite . _",
    "the problem is that distance is fundamentally a pairwise measure , and there is no clear unique extension to the case involving more than two data points .",
    "_    there are clustering techniques that do not require any prior knowledge of the clusters , but their objective functions are often too difficult to compute . as a concrete example ,",
    "correlation clustering  @xcite specifies the similarity structure by a simple graph , with positive edges between similar nodes and negative edges between dissimilar nodes . the objective is to cluster the nodes in a way that minimizes the total number of pairs of similar nodes in different clusters and dissimilar nodes in the same cluster . despite the conceptual simplicity in its formulation ,",
    "the problem was shown to be np - hard  @xcite .",
    "this motivated the search for an approximation solution , such as the randomized @xmath3-approximation algorithm in  @xcite , which was also recently extended to a parallel version for clustering big data  @xcite .",
    "however , the obtained clusters are not reproducible since the randomization can result in very different - looking clusters . while there are indexes that evaluate the quality of the clusters , and algorithms that combine different clustering solutions together , a coherent theoretical ground is desired .",
    "the problem of clustering is quite unique in the sense that it attempts to discover unknown patterns in the data .",
    "indeed , @xcite raised the question of whether clustering is more of an art than a science , because the existing methods of evaluating a clustering solution are not entirely justified .",
    "however , rather than declaring no satisfying solution to the problem , or jumping too quickly to a specific algorithm or dataset , we believe it is more important to _ lay a rigorous theoretical ground , upon which many meaningful and practical implementations can be developed_. such a paradigm should be general enough to capture complicated similarity structures , and be able to reduce to computationally feasible algorithms under verifiable simplifying model assumptions .",
    "indeed , information theory has already been considered in some previous works on data clustering  @xcite . in particular , for gene clustering , the well - known shannon s mutual information  @xcite was used as a measure of similarity between two genes in the clustering algorithm by mutual information relevance networks ( mirn )  @xcite .",
    "the measure was reported to be less sensitive to outliers , among other benefits .    unfortunately , _",
    "shannon s mutual information only measures the amount of information mutual to two random variables _ and so , its use for the multivariate case involving multiple random variables in @xcite was not properly justified . as an illustration of this",
    ", we give a concrete example where the clustering by mirn fails to return the desired cluster .",
    "many other information - theoretic frameworks make use of a proposed multivariate extension of shannon s mutual information , called the total correlation  @xcite .",
    "even with this choice of similarity measure , there have been very different approaches .",
    "for example , the hierarchical clustering by mutual information in @xcite made use of the grouping property of the total correlation for three random variables .",
    "the correlation explanation algorithm in @xcite used the conditional total correlation directly in the objective function to partition the random variables according to a latent tree model . in @xcite ,",
    "the total correlation was further broken down into a sum of the so - called interaction multi - information .",
    "while these works consider information theory to be a promising framework for machine learning , a rigorous common theoretical ground is still missing .",
    "for instance , the clustering solutions in @xcite and @xcite have algorithmic characterizations which do not lead to a unique clustering solution .",
    "the approaches are mainly supported by experimental rather than theoretical results .    instead of shannon s mutual information or the total correlation , _ info - clustering makes use of a multivariate information measure called the multivariate mutual information ( mmi ) that can capture the higher - order correlation among multiple random variables .",
    "the mmi originates from the divergence upper bound in @xcite for the capacity of the secret key agreement problem .",
    "although the bound was shown by @xcite to be slack ( in the case with helpers ) , @xcite also identified the rather general ( no - helper ) case when the bound is tight , and interpreted the corresponding expression as a measure of mutual dependency among multiple random variables .",
    "this established an alternative characterization of the secret key capacity that was formally studied as a measure of mutual information in @xcite , where many interpretations and properties of the measure were discovered to naturally extend those of shannon s mutual information .",
    "the expression was therefore named and regarded as the same notion of mutual information as shannon has defined in his seminal work  @xcite , but extended to the multivariate case .",
    "we pause here to make some important remarks on the mmi :    the mmi has various concrete operational meanings .",
    "indeed , it was shown in  @xcite to be precisely the capacity of the secret key agreement problem in  @xcite and the max - flow min - cut characterization of network coding throughput  @xcite .",
    "it is also related to the source coding problem of communication for omniscience  @xcite and the problem s extension to successive omniscience  @xcite .    among other information - theoretic properties ,",
    "the mmi satisfies the well - known data processing inequality , which has been used in @xcite to derive new results or resolve some conjectures in other multiterminal information theory problems  @xcite .",
    "the term mmi has also been used ( though not very widely ) to refer to mcgill s multiple information .",
    "as we will explain in  [ sec : mmi ] , there is an issue with such an extension of shannon s mutual information , causing it to be negative for the example shown in fig .",
    "[ fig : idiagram ] .",
    "a correction of this extension will lead to the non - negative mmi we consider .",
    "the mmi was also called the minimum partition information in @xcite , but the name was based on the characterization of the mmi by partitions  @xcite , which is only one of the many possible characterizations .",
    "e.g. , an axiomatic formulation of the mmi is given in @xcite using the so - called mutually correlative property .",
    "a more abstract mathematical form of the mmi for a submodular set function instead of multiple random variables appeared in the work  @xcite of fujishige and the work  @xcite of narayanan on the principal lattice of partitions of a submodular function .",
    "the mmi enriches the abstract mathematical structure with precise information - theoretic meaning by specializing the submodular function to the entropy function .",
    "the mmi has also been applied to clustering by @xcite and was shown to be superior to shannon s mutual information under the proposed framework in @xcite . however , unlike info - clustering , the work did not go deep into the information - theoretic interpretations of the mmi , and therefore , did not identify the clustering solution we found . instead , it considered clustering as a universal communication problem , with a decoder that recovers patterns of the transmitted message as clusters .",
    "this idea is interesting although it is unclear whether this model assumption is fruitful or limiting , and whether the universal communication problem can lead to an efficient clustering solution .",
    "the theoretical underpinning of the mmi is a mathematical structure called the principal sequence of partitions ( psp )  @xcite . on the one hand ,",
    "this structure enables the mmi and the clusters to be computed in strongly polynomial time  @xcite ( see also @xcite ) , and adds a new dimension to multi - terminal information theory  @xcite . on the other hand , the mmi enriches the abstract mathematical structure with information - theoretic meanings .",
    "there is also an existing clustering algorithm , called the minimum average cost ( mac ) clustering  @xcite , which builds implicitly upon the principal sequence of partitions to construct the clusters .",
    "however , the exact formulation is based on an abstract mathematical criterion that minimizes certain average of a submodular cost function , which differs from that of the principal sequence of partitions .",
    "we show by concrete examples that the mac clustering is different from info - clustering in general . instead of building our clustering solution on the abstract mathematical structure of the psp , we",
    "_ start with a seemingly different but more meaningful formulation and eventually connect it to the psp using the properties of the mmi_. we also prove the hierarchical structure of info - clustering separately based on a general property of the mmi , so that potentially other information measures satisfying such property can be applied . building upon this abstract mathematical framework , a duality result was recently proved in @xcite relating the info - clustering problem with the feature selection problem .",
    "the info - clustering formulation was also extended slightly there to map to the more elaborate structure of the principal lattice of partitions ( plp ) instead of just the psp .",
    "the info - clustering paradigm is general . under some simplifying assumptions on the correlation structure ,",
    "we show that the solution reduces to the clustering solution by mirn  @xcite for gene clustering .",
    "another common model reduction is by assuming a jointly gaussian distribution , as in the gene clustering method called the clustering identification by connectivity kernel ( click )  @xcite .",
    "we show that , under the jointly gaussian assumption , info - clustering reduces to a clustering solution that depends only on the covariance matrix through the spectra of its submatrices .",
    "this appears to be a new spectral clustering technique different from the spectral clustering algorithm in  @xcite , which was only used as an approximate solution to the np - hard problem of finding the minimum normalized cut for image segmentation  @xcite .    under the pairwise independent network ( pin )",
    "model  @xcite , where the random variables have a graphical correlation structure , the mmi reduces to the partition connectivity for tree packing  @xcite , a well - known notion in combinatorial optimizations  @xcite .",
    "the mmi was also shown to be equal to the maximum multicast throughput of an undirected network , giving it the usual connectivity notion of max - flow min - cut for graphs , which can be further extended to information flows over hypergraphs and , more generally , matroids  @xcite .",
    "we show that under the pin model , the clustering solution corresponds to the psp of graphs , and the idea is extended further to hypergraphs and more general channel models , following the usual extension of commodity flow to information flows in network coding .",
    "because the physical interconnections among the neurons can be specified by a graph or a hypergraph , with edges being channels that transmit information , the graphical reduction of the info - clustering algorithm can potentially be applied to identify regions of tightly connected neurons in the brain with high intra - cluster communication rates .",
    "the infomap  @xcite is another clustering algorithm applied to cluster the human connectome  @xcite .",
    "the idea , like one of the interpretations of info - clustering , is to decompose the network by information flows .",
    "however , different from info - clustering , it uses a random walk over a graph as an analogy to information flows over a network .",
    "the clusters are obtained by optimizing a special two - stage source coding of the random walk .",
    "unfortunately , the optimization is difficult , and can only be solved approximately .",
    "the two - stage source code is also far from the optimal source coding scheme that achieves the entropy rate  @xcite .",
    "another information - theoretic approach , called the integrated information theory ( iit ) , has also been proposed in neuroscience  @xcite to study consciousness based on the structure and dynamics of the brain .",
    "a measure called the integrated information was defined to measure how integrated the subsystems are within a large system .",
    "another information - theoretic measure is defined in @xcite to measure the segregation of a large system into separate subsystems .",
    "the motivation of such measures is the construction of a whole - brain computational model that can help explain some important features of the brain .",
    "however , despite similarity to the info - clustering paradigm , the proposed measures do not have clear operational meanings because some distribution , normalization factors and parameters are chosen in a rather ad - hoc manner .",
    "we will show that the info - clustering paradigm leads to a more meaningful measure of segregation and integration .",
    "indeed , the info - clustering paradigm is not limited to biological systems .",
    "it can also apply to other information systems or social networks , like the measure of segregation proposed based on social interactions in  @xcite .    in summary , info - clustering has the following advantages :    1 .",
    "the clustering procedure is driven by a new multivariate information measure called the mmi , which extends shannon s mutual information between two random variables to the mutual information among multiple random variables . like shannon s mutual information  @xcite ,",
    "the mmi has concrete operational meanings in various information - theoretic problems , including source coding , network communication and security .",
    "2 .   the clusters can be computed in strongly polynomial time due to the underlying mathematical structure called the psp . however , unlike the related mac clustering algorithm , info - clustering has a meaningful formulation not based directly upon the abstract mathematical structure .",
    "the clustering solution is unique and well - defined , unlike many other algorithmic formulations that may require an initial solution or an assumption on the number of clusters .",
    "4 .   under the markov tree model ,",
    "info - clustering reduces to an existing gene clustering algorithm called the clustering by mirn .",
    "this shows that info - clustering can apply to gene clustering and help justify existing clustering algorithms with the concrete operational meanings of info - clustering .",
    "5 .   for some non - markov tree models ,",
    "the clustering by mirn fails to capture the higher - order statistical dependency among multiple random variables , while info - clustering succeeds to identify the correct clusters .",
    "6 .   under a hypergraphical source model ,",
    "info - clustering reduces to the psp of hypergraphs .",
    "it gives the psp a concrete operational meaning as clustering by network information flow .",
    "such a model can be applied to cluster the neurons by their physical connections .",
    "7 .   under the jointly gaussian assumption ,",
    "info - clustering reduces to a method of clustering by the covariance matrix . compared to the existing spectral clustering method , it is a different algorithm that has concrete information - theoretic meaning .",
    "a meaningful measure of integration and segregation can be derived in a more rigorous way than the integrated information theory ( iit ) , with applications beyond biological systems such as social networks .    while there are many practical approximations and implementations possible for info - clustering , the focus of this paper is on the theoretical development and its potential biological applications in the study of human genome and connectome .",
    "* organization : * the paper will be organized as follows .",
    "the info - clustering paradigm will be formulated in  [ sec : hc ] and characterized in  [ sec : cpsp ] , with the detailed clustering procedures implemented in algorithm  [ algo : iteration ] , [ algo : fp ] and [ algo : psp ] .",
    "its biological applications are through the model reductions in  [ sec : reduction ] .",
    "* notations : * throughout this paper , unless otherwise specified , we use sans - serif upper - case letters ( e.g. , @xmath4 , @xmath5 , etc . ) to denote random variables and calligraphic font upper - case letters ( e.g. , @xmath6 , @xmath7 , etc . ) to denote collections of sets . for any collection of sets @xmath7",
    "whose elements are subsets of some finite set , we use @xmath8 to denote the inclusion - wise maximal elements of @xmath7 , i.e. , @xmath9",
    "the info - clustering formulation in this paper separates into two main components .",
    "the first component is a formulation using a threshold constraint on the mmi .",
    "we show that the solution is hierarchical , and so , an iterative algorithm can be used to compute the clusters .",
    "we will make the proof general using only a simple property of the mmi rather than its detailed definition , i.e. , the results herein hold for any multivariate information measure that satisfies such a property .",
    "we use the two terms  multivariate information \" and  multivariate mutual information ( mmi ) \" for two distinct meanings , where the former refers to a _",
    "general _ information measure for multiple random variables as detailed in this section while the latter refers to a _",
    "specific _ information measure defined in the next section as .",
    "the second component of the formulation is a refinement of the clustering solution based on further properties of the mmi as detailed in the next section .",
    "the reason for the two - step characterization is not only for theoretical elegance but also for practical implementations of info - clustering in subsequent work .",
    "the more general hierarchical solution developed in this section may allow the mmi to be approximated and estimated from data more efficiently with a tunable level of computational and sample complexity .      to cluster objects using information theory",
    ", we first associate each object we want to cluster , say @xmath10 , with all the information that describes it .",
    "the information is represented by a random variable , say @xmath11 , which can be viewed as a file containing some measurements of the object  @xmath10 .",
    "then , we cluster all the objects based on the mutual information among the random variables @xmath11 s .",
    "= [ opacity=0.2,fill = gray ]    .15    1em    \\(z ) [ matrix of math nodes , every cell/.style = anchor = base , minimum width=2*,minimum height=1.2*u , column 1/.style = anchor = base east , row sep=0.2*,column sep=0.5*u ] & & + _ 1:= & ( _ a , & _",
    "d ) + _ 2:= & ( _ a , & _",
    "d ) + _ 3:= & _ a & + _ 4:= & _ b & + _ 5:= & _ b & + _ 6:= & _ c & + ; ; ; ;    .75    1em = [ circle , minimum size=.3em , fill ]    \\(c ) [ matrix of math nodes , nodes in empty cells , row sep=1*,column sep=0.1 * , column 6/.style = column sep=1*u , column 7/.style = anchor = base west , column sep=1*u , left delimiter=\\ { ] \\{\\{1 , & 2 , & 3 , & 4 , & 5 , & 6 } } &  g<0 & + \\{\\{1 , & 2 , & 3 } , & \\{4 , & 5 } } & & ",
    "g[0,1 ) + \\{\\{1 , & 2 } } & & & & &  g [ 1 , 2 ) +  0 & & & & & &  g2 + ; ( c.west ) node [ left ] @xmath12 ; ( c-1 - 1-|c-1 - 8 ) to node ( 12 ) ( c-2 - 1-|c-1 - 8 ) to node ( 23 ) ( c-3 - 1-|c-1 - 8 ) to node ( 34 ) ( c-4 - 1-|c-1 - 8 ) node[label=[label distance = u]below:@xmath13 ; /in 12/@xmath14,23/@xmath15,34/@xmath16 ( c-1 - 1|- ) ",
    "( ) node [ point , label=[label distance = u]right : ] ; ; ; ;    as a motivating example , consider clustering the random variables @xmath11 s defined in fig .",
    "[ fig : eg : z ] .",
    "the random variables are correlated in the sense that they share some uniformly random independent bits @xmath17 and @xmath18 . , and the random bits are mutually independent . ]",
    "it is desirable to group @xmath19 and @xmath20 in a cluster because they share a common bit @xmath21 , but it is desirable to group @xmath22 and @xmath23 in a different cluster because they share a different independent bit @xmath24 .",
    "it is also desirable to group @xmath25 and @xmath26 as a smaller cluster ( compared to @xmath27 ) because they share the additional bit @xmath18 ( in addition to the bit @xmath21 ) .",
    "more generally , let @xmath28 be a finite set of objects we want to cluster and @xmath29 be the vector of random variables associated with the objects .",
    "for every subset @xmath30 of at least two random variables , we measure their shared information by some finite real number @xmath31 , i.e. , @xmath32 this multivariate information quantity should depend on the joint distribution @xmath33 of the random vector @xmath34 , but its precise definition will be postponed to  [ sec : mmi ] because _ the combinatorial structure of the following hierarchical clustering formulation of info - clustering does not depend on the particular choice of this measure , allowing potentially other measures to be used : _    [ def : cluster ]    for a _ threshold _ @xmath35 , the set of _ clusters _ is defined as @xmath36 the maximality   requirement ensures consistency among the clusters , i.e. , a cluster @xmath37 with @xmath38 does not separate apart any other highly correlated random variables in a larger set @xmath39 that also satisfies @xmath40 . for notational convenience , @xmath41 denotes the collection of all clusters at different thresholds .    in words , given a threshold @xmath42 , we consider subsets @xmath37 of two or more elements from @xmath28 , such that the random variables @xmath34 indexed by the elements of @xmath37 have the multivariate information quantity @xmath31 _ strictly larger _ than the threshold @xmath42 .",
    "out of all such non - singleton subsets satisfying the _ threshold constraint _ , we pick the inclusion - wise maximal subsets to be the clusters . to put it simply , the idea of clustering is to group together random variables , as many as possible , such that the group has increasingly larger amount of multivariate information .",
    "the desired level of multivariate information is specified by the threshold @xmath42 .",
    "the following is an illustration of the threshold - constraint formulation when applied to the motivating example above .",
    "let @xmath43 and define @xmath44 in terms of the independent uniformly random bits @xmath17 and @xmath18 as shown in fig .",
    "[ fig : eg : z ] . for this example , the dependency structure is simple , and so , let us define the information measure as the number of shared bits : @xmath45 for instance , @xmath46 because @xmath25 , @xmath26 , and @xmath20 share the common bit @xmath21 while @xmath22 and @xmath23 share the common bit @xmath24 .",
    "similarly , @xmath47 because @xmath25 and @xmath26 share the common bit @xmath18 in addition to the bit @xmath21 . finally , @xmath48 because @xmath49 is independent of all other random variables @xmath11 for @xmath50 .    if @xmath51 , then asks for the maximal subsets with shared information strictly larger than zero . by , the sets whose shared information is larger than zero are @xmath52 , and any subset of two or more elements from @xmath27 .",
    "the maximal among such subsets are the clusters at threshold zero , i.e. , @xmath53 . if @xmath54 instead , then asks for the maximal subsets of random variables with more than @xmath55  bit of shared information .",
    "the only choice is the subset @xmath56 , and so it is the only cluster at threshold @xmath55 , i.e. , @xmath57 . for other values of @xmath42 :    for @xmath58 , the entire set @xmath28 is the only cluster because @xmath59 and @xmath28 is the maximal set trivially .    for @xmath60 , there are two clusters @xmath27 and @xmath52 because @xmath61 , and each of the sets is maximal .    for @xmath62 ,",
    "the set @xmath63 is the only cluster because it is the only set with the shared information @xmath64 .",
    "there are no clusters for @xmath65 because no set of random variables has more than @xmath66  bits of shared information .",
    "the complete clustering solution is illustrated in fig .  [",
    "fig : eg : cluster ] .",
    "there are various important properties we can observe from the clustering solution in fig .",
    "[ fig : eg : cluster ] :    * the set of clusters changes at a finite set of threshold values , namely the set of thresholds @xmath67 .",
    "the threshold values are the shared information of the clusters and each cluster occupies a contiguous interval between two of the threshold values .",
    "( e.g. , the threshold value @xmath66 is the shared information of the cluster @xmath63 that appears over the interval @xmath62 . ) in particular , the smallest threshold value @xmath68 is the shared information of the entire set of random variables , and the largest threshold value @xmath66 is the maximum shared information over all subsets that contain at least two random variables . * for each threshold @xmath42 , the clusters are disjoint . for two different thresholds ,",
    "two clusters are either disjoint or the larger - threshold cluster is a proper subset of the smaller - threshold cluster .",
    "( e.g. , the cluster @xmath27 at @xmath69 does not intersect with the other cluster @xmath52 at the same threshold , but it contains the cluster @xmath63 that arises at the larger threshold @xmath70 . ) * there is an iterative relationship among the clusters : the cluster of a cluster of @xmath44 is also a cluster of @xmath44 . for example , if we consider clustering the random variables from the cluster @xmath27 , then @xmath63 is a cluster of @xmath27 .",
    "note that @xmath63 is also a cluster of @xmath44 .    in the next subsection",
    ", we will show that the above hierarchical structure holds more generally .      for convenience",
    ", we will use @xmath71 ( @xmath72 ) to represent the value that is arbitrarily close to but strictly smaller ( larger ) than @xmath42 .",
    "more precisely , we write @xmath73 ( @xmath74 ) for the limit of @xmath75 as @xmath76 increases ( decreases ) to @xmath42 from below ( above ) .",
    "the limit exists because there is only a finite number of clusters , i.e. , @xmath77 even though the set @xmath78 of all clusters   is a union over all real threshold values @xmath42 . define the set of _ critical values _ for clustering @xmath44 as @xmath79 this is the set of threshold values of interest , because the set of clusters changes at those values .",
    "the following theorem asserts that the set of critical values is also finite .",
    "[ thm : cluster ]    the set of critical values can be written as @xmath80 for some positive integer @xmath81 with @xmath82 for @xmath83 .",
    "furthermore , assuming @xmath84 is the collection of the sets of clusters for the critical values , then the complete clustering   of @xmath44 is given as @xmath85 finally , the first and last critical values are @xmath86 and @xmath87 is the set of all maximal @xmath37 achieving the maximum amount of multivariate information .",
    "see appendix  [ sec : a ] .",
    "[ def : critical ] we will use @xmath88 and @xmath89 to denote the @xmath10-th critical value and the number of critical values for @xmath44 respectively . for simplicity and when there is no ambiguity , we may drop the dependency on @xmath44 and write , e.g. , @xmath90 as @xmath91 .    applying the definition of clusters   to an arbitrary subset @xmath92 with size at least two",
    ", the definition above extends to any such subset , where in this case the simplified notation @xmath93 will mean @xmath94 .",
    "next , we show that every cluster @xmath95 of @xmath44 can be obtained by computing the set of clusters of @xmath96 for some larger ( previous ) cluster @xmath97 of @xmath44 :    [ thm : iterate ] for each @xmath98 and @xmath99 , @xmath100 that is , we can obtain @xmath95 by computing clusters that correspond to the first critical value of an earlier cluster @xmath97",
    ".    see appendix  [ sec : b ] .    according to theorem  [ thm : iterate ] , we can compute the complete solution to the clustering problem if we can compute the first set @xmath101 of clusters for all subsets @xmath102 .",
    "however , without any additional properties of the multivariate information measure , it is unclear whether the iterative algorithm can be computed efficiently due to the following issues :    1 .",
    "[ item : issue1 ] while theorem  [ thm : iterate ] states that every cluster @xmath95 is in the first set @xmath103 of clusters of a larger cluster @xmath97 , the converse may not be true .",
    "that is , a cluster of @xmath96 in @xmath103 may not be a cluster of @xmath44 , because the maximality in needs to be verified in addition .",
    "[ item : issue2 ] the total number of clusters under a general multivariate information measure can be large . by",
    ", @xmath91 is an _ antichain _ in the sense that a cluster in @xmath91 can not be a subset of another cluster in @xmath91 . without any other restriction ,",
    "the size of an antichain can be exponential in @xmath104 by _ sperner s theorem _",
    "@xcite .    to illustrate the issues above more clearly :    [ eg : imm ] consider @xmath105 and",
    "let the multivariate information quantity be defined as follows : for @xmath106 , @xmath107 from this , it follows that the set of clusters is given by @xmath108 consider now the cluster @xmath27 of @xmath44 .",
    "it is not hard to verify that @xmath109 is a cluster ( in the first set of clusters ) of @xmath110 .",
    "however , the set @xmath109 is not a cluster of @xmath44 because the proper superset @xmath111 has a larger multivariate information of @xmath3 .",
    "next , we show that both issues can be resolved if the following simple , but in our opinion fundamental , property is satisfied by the multivariate information measure : @xmath112 for all @xmath113 , @xmath114 and @xmath115 .",
    "this property holds for the mmi we will consider in ( ( * ? ? ?",
    "* corollary  5.1 ) ) , but it also holds for some other multivariate information quantities .",
    "[ thm : laminarity ] for any multivariate information measure that satisfies the property , the collection of all clusters @xmath78 forms a _ laminar family _",
    "@xcite , i.e. , @xmath116 for all clusters @xmath117 .",
    "in particular , for every @xmath35 , the set @xmath118 consists of disjoint clusters .",
    "consider two clusters @xmath117 with @xmath115 . without loss of generality ,",
    "assume @xmath119 . by , @xmath120 and so @xmath121 or it would contradict the maximality of @xmath122 .",
    "that is , @xmath123 and so we have .    under the context of data clustering , the aforementioned laminarity is usually known as _ hierarchical clustering _ or _",
    "dendrogram_. by theorem  [ thm : laminarity ] , any multivariate information measure that satisfies the property will necessarily lead to a clustering solution that is guaranteed to be hierarchical . if we define a similarity relation @xmath124 to mean that there exists @xmath125 containing both @xmath10 and @xmath126 such that @xmath127 , then it can be shown that @xmath128 is an equivalence relation for any threshold @xmath42 . in particular",
    ", implies that the relation is transitive .",
    "the set @xmath118 of clusters can be shown to be precisely the set of non - singleton equivalence classes , and so the clusters are disjoint .",
    "our next result shows that under the condition , the complete solution to the clustering problem can indeed be computed iteratively from the first set of clusters of a previous cluster .",
    "[ thm : iterate2 ] for any multivariate information measure that satisfies the property , @xmath129 where for any @xmath92 , the set @xmath130 is the collection of all clusters of @xmath131 ( similar to ) .",
    "see appendix  [ sec : c ] .    for any multivariate information measure that satisfies the property , the total number of clusters @xmath132 which is linear in the number of random variables to be clustered .",
    "consider proving by an induction on the size of @xmath28 .",
    "in particular , consider the non - trivial case when @xmath91 is non - empty .",
    "the base case @xmath133 holds trivially . by ,",
    "@xmath134 where the first inequality is by the inductive hypothesis and the last is because @xmath91 consists of disjoint proper subsets of @xmath28 by theorem  [ thm : laminarity ] .",
    "hence , given an algorithm that can compute the first critical value @xmath135 and the first set @xmath136 of clusters of any given set of random variables , we can compute the entire clustering solution by applying the algorithm at most @xmath104 times .",
    "the pseudocode is given in algorithm  [ algo : iteration ] .",
    "@xmath137 empty queues ; enqueue @xmath28 to @xmath138 ;    the algorithm computes the list @xmath139 of all clusters @xmath140 and their associated values @xmath31 .",
    "it calls the function iteratively to obtain the first critical value @xmath42 and the first set @xmath141 of clusters of every previously discovered cluster stored temporarily in @xmath138 . the newly discovered clusters in @xmath141 are further added to @xmath138 .",
    "in this section , we focus on the clustering solution under the mmi measure in  @xcite .",
    "although a general property , namely , property  , suffices for a laminar hierarchical clustering solution in  [ sec : hc ] , the resulting clusters may be trivial or meaningless if the multivariate information measure is not chosen properly .",
    "( for instance , any choice of @xmath31 that is non - decreasing in @xmath37 will satisfy but will only produce the trivial cluster @xmath142 . )",
    "we will explain the meaning of the mmi measure precisely and show that the corresponding hierarchical clustering formulation is related to a non - trivial mathematical structure called the principal sequence of partitions ( psp )  @xcite of the entropy function .",
    "consequently , the solution can be computed by some well - studied submodular function optimization techniques  @xcite that can run in strongly polynomial time .",
    "recall that for the simple example considered earlier in fig .",
    "[ fig : eg : z ] , the mutual information of a given set was measured by the number of bits shared by the random variables in the set . for a general source model , _",
    "shannon s mutual information _  @xcite provides a well - accepted measure in the bivariate case involving only two random variables : @xmath143 where @xmath144 denotes the kullback  liebler divergence  @xcite between the distributions @xmath145 and @xmath146 .",
    "the divergence on the r.h.s .",
    "of can be interpreted as a statistical distance to independence , because it equals zero if and only if the two random variables are independent .",
    "a straightforward extension of shannon s mutual information to the multivariate case is watanabe s total correlation  @xcite : @xmath147 which is equal to zero if and only if the random variables @xmath11 for @xmath148 are mutually independent .",
    "while the total correlation captures the mutual independence among the random variables , it fails to capture many other forms of independence relation .",
    "the work in @xcite aims at a more precise understanding of this and the formulation of the mmi , as follows , capable of capturing any form of independence that might exist among three or more random variables .",
    "let @xmath149 be the collection of all possible partitions of @xmath28 that splits @xmath28 into at least two nonempty disjoint subsets .",
    "( in other words , @xmath149 is the collection of all set partitions of @xmath28 except the trivial partition @xmath142 . ) for any partition @xmath150 of @xmath28 , the product distribution @xmath151 specifies an independence relation , namely , that the agglomerated random variables @xmath152 s are mutually independent .",
    "a well - constructed measure needs to ensure that the mutual information is measured at zero as long as an independence relation exists among the random variables from @xmath28 , not just when all the random variables from @xmath28 are mutually independent .",
    "we now introduce the mmi measure from @xcite :    [ eq : mi ] @xmath153    clearly , by the above definition , we have @xmath48 if and only if there exists an independence relation among the random variables from @xmath28 .",
    "[ eg : i ] for the example considered earlier in fig .",
    "[ fig : eg : z ] , we have @xmath48 because of the independence relation @xmath154 and so @xmath155 with @xmath156 .",
    "the divergence expression   of the mmi derives from a divergence upper bound  ( * ? ? ? * ( 26 ) in example  4 ) on the secrecy capacity for the multiterminal secret key agreement problem .",
    "the bound was derived in the general case with helpers , and was considered as a heuristically meaning upper bound to the lp characterization of the capacity in @xcite .",
    "the bound was shown to be tight for the case involving @xmath66 or @xmath3  users even involving helpers , but it was left open in @xcite whether the bound is tight beyond @xmath3 users . @xcite extended the brute - force search of @xcite and showed with the help of a computer program that the bound is tight for @xmath157 or @xmath158 users . however",
    ", a counter - example involving @xmath159 users with @xmath3 helpers was also discovered , showing that the bound is loose with the presence of helpers and therefore does not have the same meaning as the secrecy capacity .",
    "nevertheless , it was identified and proved in ( * ? ? ?",
    "* theorem  1 ) ( * ? ? ?",
    "* theorem  1.1 ) ( * ? ? ? * theorem  2.1 ) that the bound is tight in the no - helper case even under a general private source distribution , using only the well - known submodularity of entropy .",
    "this establishes the concrete operational meaning of the mmi as the secrecy capacity in the general no - helper case , much like the way shannon s mutual information was shown to characterize the channel capacity in the seminal work  @xcite of shannon . a first attempt to interpret the mmi as a measure of mutual information among multiple random variables ( and to explain the normalization factor of @xmath160 in ) appeared in (",
    "* section  iv ) .",
    "it is useful to compute @xmath161 by rewriting the divergence in terms of shannon s entropy or mutual information as follows :    @xmath162    where we used @xmath163 to denote the blocks of the partition @xmath164 .",
    "the measure @xmath161 is also written more explicitly in @xcite as @xmath165 through , we can verify that the mmi measure defined as above is consistent with the measure of shared bits used in example  [ eg : imm ] for the special source model in fig .",
    "[ fig : eg : z ] .",
    ".45     .45     .6 2em 2em    .35 2em   2em    [ eg : ii ] consider the example in fig .",
    "[ fig : eg : z ] .",
    "the values of @xmath166 for different partitions @xmath164 are @xmath167 where the last term is obtained by applying with @xmath168 .",
    "hence , @xmath169 with @xmath170 being the unique optimal partition .    at the first sight , the normalization factor @xmath160 on the r.h.s .",
    "of may appear arbitrary .",
    "this factor is not included in other proposed information measures involving the divergence , such as the total correlation   and the integrated information  @xcite .",
    "however , it turns out that such a factor has an important information - theoretic meaning , which relates it to the non - trivial , but polynomial - time solvable , mathematical structure of the psp .",
    "indeed , the normalization factor is often overlooked in other proposed multivariate information measures based on the independence relations , such as the total correlation in .",
    "this is because the factor only affects the measure when the independence relations do not hold , i.e. , when the measure is non - zero .    to help understand the reasoning behind such a normalization",
    ", we will introduce the residual independence relation  @xcite by extending the well - known venn diagram interpretation of shannon s mutual information shown in fig .",
    "[ fig : venn ] according to the identity @xmath171    from the venn diagram , the mutual information has the meaningful interpretation as the amount of overlap in the randomness of the individual random variables .",
    "this interpretation has been extended by @xcite to the information diagram , and the amount of overlap can be measured by the mcgill s multiple information  @xcite using the inclusion - exclusion principle .",
    "( see fig .  [",
    "fig : idiagram ] . )",
    "unfortunately , the mcgill s multiple information can be negative even for a very simple example involving three random variables  @xcite , contradicting the basic intuition that mutual information should be non - negative .    to  fix \" this problem ,",
    "one may rewrite equivalently as @xmath172 } + { \\color{black}`1[h(\\rz_2)-i(\\rz_1\\wedge \\rz_2)`2]}.\\label{eq : rir2}\\end{gathered}\\ ] ] note that the l.h.s .",
    "is the total _",
    "residual _ randomness after removing the mutual information , and the equality states that the total residual randomness is equal to the sum of the individual residual randomness in each random variable , as illustrated in fig .",
    "[ fig : rir2 ] . the important interpretation of the equality is that :    _ no double counting in the sum means precisely that there is no mutual information left in the residual randomness .",
    "_    the above idea can be extended to the multivariate case as follows .",
    "consider a partition @xmath150 and define the residual independence relation ( rir ) as @xmath173,\\label{eq : rir}\\end{aligned}\\ ] ] i.e. , the total residual randomness after removing some real value @xmath35 is equal to the sum of the individual residual randomness of the agglomerated random variables @xmath152 s .",
    "this is illustrated in fig .",
    "[ fig : rir ] .    we can now interpret the mmi   as the _ smallest _ @xmath35 such that the rir holds for some partition @xmath150 . to see this , a simple re - arrangement of the terms in gives",
    "@xmath174              = i_{\\mcp}(\\rz_v)\\end{aligned}\\ ] ] where the last equality follows from . minimizing over all possible",
    "@xmath150 gives rise to the mmi , and the normalization factor @xmath160 appears naturally under the context of rir due to the fact that any information mutual to the set of random variables should appear in every agglomerated random variable , and is therefore over - counted in the divergence .",
    "note also that the residual randomness can be shown to be always non - negative as expected when the partition is the minimizing partition , and when the random variables are discrete .",
    "back to the clustering problem , how do we compute the clusters efficiently using the mmi ?",
    "surprisingly , the optimal partition achieving the mmi   gives us the desired clustering solution .",
    "the optimal partition achieving the mmi may not be unique , but it can be shown that the set of optimal partitions form a semi - lattice , which together with the partition @xmath175 is referred to as the _ dilworth truncation lattice _",
    "the lattice structure means that there exists a unique finest partition , which is called the _ fundamental partition _",
    "it turns out that the non - singleton elements of the fundamental partitions give us the desired clusters .    .",
    "the non - singleton elements of the fundamental partition give some of the clusters shown in fig .",
    "[ fig : eg : cluster ] , namely , the first set of clusters at the threshold @xmath135 .",
    "( see theorem  [ thm : cp].),width=288 ]    the proof is rather involved , but the idea can be illustrated using the previous example in fig .",
    "[ fig : eg : z ] .",
    "as explained in example  [ eg : i ] , @xmath48 because of the independence relation @xmath176 .",
    "there are also other independence relations , which correspond to further merging of the agglomerated random variables as shown in fig .",
    "[ fig : dtl ] .",
    "hence , the fundamental ( finest optimal ) partition is @xmath177 note that , the non - singleton subsets in the fundamental partition , namely @xmath27 and @xmath52 , are the clusters in @xmath178 in fig .  [",
    "fig : eg : cluster ] .",
    "if we take the cluster @xmath27 and further compute its mmi , we have ( from example  [ eg : ii ] ) @xmath169 with the unique optimal partition @xmath179 note that the non - singleton subset @xmath56 in the fundamental partition of the cluster @xmath180 is a cluster of the entire set of random variables at the larger threshold of @xmath181 . in other words , clusters at larger thresholds",
    "can be obtained from the fundamental partition of some clusters at smaller thresholds .",
    "below , we will show more generally that computing the fundamental partitions iteratively gives the complete info - clustering solution .",
    "recall from theorem  [ thm : cluster ] that the first critical value @xmath182 is always given by the mmi @xmath183 of the entire set . to determine the corresponding first set @xmath91 of clusters , denote the set of optimal partitions to by @xmath184 in general ,",
    "@xmath185 may contain more than one optimal partition , but the optimal partitions are related by the following partial order defined for any partitions @xmath164 and @xmath186 as @xmath187 we say that @xmath164 is finer / smaller than @xmath186 whenever @xmath188 , and we use @xmath189 to denote the strict inequality .",
    "the following result is known from @xcite .",
    "[ prop : fundamental ] there is a unique finest optimal partition , which we denote by @xmath190 and refer to as the fundamental partition for @xmath44 .",
    "( namely , @xmath191 , where the minimum is with respect to @xmath192 in  . )",
    "furthermore , the fundamental partition @xmath190 with the singletons removed , i.e. , @xmath193 , is the set of maximal subsets @xmath30 with @xmath194 .",
    "since we have @xmath195 by , and the non - singleton elements from @xmath190 are , by proposition  [ prop : fundamental ] , the maximal subsets of random variables with mutual information strictly larger than @xmath182 , we have by   that :    [ thm : cp ] @xmath196 with the first critical value @xmath195 .    by proposition  [ prop : fundamental ] , the mmi measure defined in is guaranteed to satisfy the key property by ( * ? ? ?",
    "* corollary  5.1 ) . this can be argued as follows .",
    "suppose to the contrary of that both @xmath197 and @xmath198 are strictly larger than @xmath199 .",
    "then , the non - singleton elements in the fundamental partition @xmath200 must consist of a superset of @xmath122 as well as a superset of @xmath201 . however , the two supersets can not be disjoint as @xmath115 , and they can not be @xmath202 either , which is a contradiction .",
    "the following result thus follows immediately from theorems  [ thm : laminarity ] and [ thm : iterate2 ] .    under the mmi measure ,",
    "the clustering solution is guaranteed to be hierarchical .",
    "furthermore , all clusters can be obtained by computing the fundamental partition iteratively for every previously obtained cluster .",
    "given an algorithm that computes the fundamental partition exactly or approximately , we can compute the entire info - clustering solution following the iterative procedure in algorithm  [ algo : iteration ] .",
    "this is stated more precisely in algorithm  [ algo : fp ] .",
    "the complexity is again @xmath104 times the complexity in calculating the fundamental partition by .",
    "can be saved by using the psp rather than the iterative algorithm to compute the clusters . nevertheless , the iterative algorithm is useful as it potentially allows us to compute the entire clustering solution approximately based on an approximate algorithm of computing the fundamental partition . ]",
    "@xmath137 empty queues ; enqueue @xmath28 to @xmath138 ;    similar to algorithm  [ algo : iteration ] , the algorithm computes the list @xmath139 of all clusters @xmath140 and their associated values @xmath31 .",
    "it calls the function iteratively to obtain the first critical value @xmath42 and the fundamental partition @xmath164 of every previously discovered cluster stored temporarily in @xmath138 .",
    "the non - singleton elements of @xmath164 are the desired clusters further added to @xmath138 .",
    "as pointed out in @xcite , the mmi and the fundamental partition can both be computed in polynomial time assuming the entropies of arbitrary subsets of the random variables in hand are also computable in polynomial time .",
    "this result is based on the property that the ( conditional ) mutual information is non - negative , or equivalently , the entropy is submodular  @xcite .",
    "hence , the iterative algorithm in the previous section can discover the info - clustering solution in polynomial time .",
    "quite surprisingly , based on the rir   interpretation of the mmi in ",
    "[ sec : mmi ] , we find that the info - clustering solution can be mapped to the polynomial - time solvable mathematical structure of the principal sequence of partitions ( psp ) .",
    "the implication is that one can compute the general info - clustering solution more efficiently than the iterative algorithm , using techniques such as @xcite .",
    "this understanding will also allow us to compare the info - clustering solution to the closely related approach of mac clustering  @xcite .",
    "the study of psp from an information - theoretic perspective appears to be new , and we are beginning to discover more information - theoretic interpretations in other problems  @xcite .",
    "define for @xmath35 the residual entropy function  @xcite:@xmath203 where @xmath204 is the usual entropy function  @xcite .",
    "@xmath205 measures the residual randomness of @xmath34 introduced in  [ sec : mmi ] . for notational simplicity , the dependency on @xmath34 is implicit here .",
    "the entropy function is well - known  @xcite . ] to be submodular  @xcite , i.e. , for all @xmath206 , @xmath207 and so , it is clear that the residual entropy function @xmath208   is also submodular .",
    "the dilworth truncation of the submodular residual entropy function is defined as    [ eq : dt ] @xmath209",
    "\\kern1em \\text{for $ b\\subseteq v$ , where}\\label{eq : dt:1}\\\\          h_{`g}[\\mcp ] & : = \\sum_{c\\in \\mcp } h_{`g}(c)\\label{eq : dt:2 }      \\end{aligned}\\ ] ] and @xmath210 is the collection of all partitions of @xmath37 into non - empty subsets .",
    "note that the difference between @xmath210 and @xmath211 is that @xmath210 includes the trivial partition @xmath212 as well , i.e. , @xmath213 .",
    "the dilworth truncation is itself a submodular set function , and can be calculated efficiently in strongly polynomial time  @xcite for any given set using edmonds greedy algorithm and the _ submodular function minimization ( sfm)_. the running time is @xmath214 , where @xmath215 is the running time of the submodular function minimization over the ground set @xmath28 .",
    "( see @xcite and @xcite . )",
    "@xmath215 can be strongly polynomial assuming that the entropy function can be evaluated efficiently for every given subset of random variables .    to characterize the info - clustering solution",
    ", we will focus on the dilworth truncation evaluated at @xmath28 : @xmath216\\label{eq : g2}\\end{aligned}\\ ] ] and think of it as a function of @xmath42 . more precisely ,",
    "it is a minimization of the function @xmath217=\\sum_{c\\in \\mcp } h_{`g}(c ) = \\sum_{c\\in \\mcp } h(\\rz_c ) - ` g\\abs{\\mcp},\\end{aligned}\\ ] ] which is linear in @xmath42 with @xmath218 since @xmath219 is a minimization over a finite collection of linear curves , it must be piecewise linear .",
    "more explicitly , for a given @xmath220 , let @xmath221 be the set of partitions attaining the minimization in ( [ eq : g2 ] ) , then at @xmath222 the dilworth truncation is given as the curve with the minimum slope among @xmath221 .",
    "thus , @xmath219 is piecewise linear in @xmath42 with slopes decreasing from @xmath223 to @xmath224 and taking only integer values , as shown in fig .",
    "[ fig : p ] .",
    "since there is a finite number of partitions of @xmath28 , the curve can be characterized by the set of _ turning points _",
    "s where the slope changes .",
    "denote the turning points as @xmath226 for some positive integer @xmath81 , and call @xmath227    the _ critical values for the dilworth truncation _",
    "@xmath219 .",
    "[ eg : dt ]    consider the example in fig .",
    "[ fig : eg : z ] . to compute @xmath228 for @xmath229 , note that the values of @xmath230 $ ] for different partitions",
    "@xmath164 are @xmath231 the minimum , @xmath228 , of the above lines is plotted in fig .",
    "[ fig : p:1 ] . for @xmath232 , the minimum",
    "is achieved uniquely by @xmath233 . for @xmath234 , the minimum",
    "is achieved uniquely by @xmath235 . for @xmath236 , the minimum",
    "is achieved uniquely by the partition into singletons .",
    "similarly , @xmath219 can be plotted as the minimum of a set of lines in fig .",
    "[ fig : p:2 ] . for @xmath237 , the minimum",
    "is achieved uniquely by @xmath238 . for @xmath239 , the minimum",
    "is achieved uniquely by @xmath240 . for @xmath234 , the minimum",
    "is achieved uniquely by @xmath241 . for @xmath236 , the minimum",
    "is achieved uniquely by the partition into singletons .",
    "the connection of the dilworth truncation to the mmi is through the rir , as shown in ( * ? ? ? * theorem  5.1 ) .",
    "when @xmath42 is sufficiently small , @xmath242 because @xmath230 $ ] has the largest slope of @xmath223 when @xmath243 .",
    "more precisely , @xmath244-h_{`g}[\\set{v } ] =   d`1(\\extendvert{p_{\\rz_v}\\|\\prod\\nolimits_{c\\in \\mcp } p_{\\rz_c}}`2 ) + ( 1-\\abs{\\mcp})`g,\\ ] ] which will be positive , i.e. , @xmath230>h_{`g}[\\set{v}]=h_{`g}(v)$ ] , for @xmath245 ( or @xmath246 ) and @xmath42 is sufficiently small .",
    "therefore , it follows from that @xmath247 is the intersection between @xmath248 and @xmath249 $ ] , and so @xmath250 satisfies the equation @xmath251 , \\label{eq : g }      \\end{aligned}\\ ] ] which translates directly to the rir in .",
    "hence , we have @xmath252 .",
    "furthermore , @xmath247 lies on @xmath230 $ ] if and only if @xmath253 , since @xmath185 defined in is the set of solutions to the minimization in , as can be seen from the rir interpretation of the mmi .",
    "since the fundamental partition @xmath190 is the unique finest partition in @xmath254 , @xmath255 $ ] has the smallest slope and therefore uniquely defines the line segment following @xmath247 .",
    "note that , it is not clear a priori that the critical values   defined for the dilworth truncation @xmath219 are precisely the critical values in @xmath256   defined for info - clustering  , even though the above result from @xcite shows that it is the case for the first critical value",
    ". we will show the stronger result that _ not only the two sets of critical values match , but that the line segments of the dilworth truncation give the desired info - clustering solution .",
    "_ e.g. , from fig .",
    "[ fig : p:2 ] , the critical values of the dilworth truncation can be verified to be precisely the critical values for the info - clustering solution in fig .",
    "[ fig : eg : cluster ] .",
    "furthermore , the sequence of partitions defining the line segments in fig .",
    "[ fig : p:2 ] contains all the clusters in fig .  [",
    "fig : eg : cluster ] as its non - singleton elements .",
    "this sequence of partitions is the psp , which will be defined more precisely below .",
    "let @xmath257 be the set of solutions to the minimization in @xmath258 .",
    "the elements of @xmath259 form a lattice :    [ pro : dtl ] the set of optimal solutions to the dilworth truncation @xmath260 of a submodular function @xmath261 forms a lattice ( with respect to the partial order in ) called the _ dilworth truncation lattice_.    for instance , when we specialize the submodular function to the entropy function @xmath262 for @xmath44 , the first critical value is @xmath263 and the associated lattice of partitions is @xmath264 .",
    "let @xmath265 and @xmath266 be , respectively , the ( unique ) minimum and maximum partitions in the lattice @xmath259 .",
    "the following proposition asserts that , for all @xmath10 , the extreme partitions @xmath265 and @xmath267 are equal .",
    "( in particular , the fundamental partition is @xmath268 . )",
    "furthermore , the extreme partitions for different values of @xmath10 form a sequence of successively finer partitions , referred to as the psp .    [ pro : psp ] there is a unique sequence of partitions with respect to the partial order     [ eq : psp ] @xmath269 called the _ principal sequence of partitions ( psp ) _ , which satisfies @xmath270    for @xmath271 .",
    "more explicitly , @xmath272 , @xmath273 and @xmath274 .",
    "same the argument in @xcite , the psp is computable in strongly polynomial time in @xmath275 . for completeness",
    ", we include a simple proof below .    for @xmath276 , the line segment of @xmath219 for @xmath277 is defined by @xmath278 $ ] for some partition @xmath279 because it passes through both turning points @xmath280 and @xmath281 .",
    "since it has the smallest and largest slopes among all other lines through @xmath225 and @xmath281 respectively , @xmath282 is the minimum in @xmath259 and maximum in @xmath283 as desired .",
    "@xmath284 and @xmath285 because @xmath230 $ ] has the largest and smallest slopes when @xmath164 is @xmath286 and @xmath287 respectively .    the desired connection between info - clustering and the psp of the entropy function follows from the main result below , which gives an interpretation to every critical value of the dilworth truncation using the psp .    [ thm : xi ]",
    "the @xmath10-th critical value of @xmath219   is    [ eq : xi ] @xmath288-h[\\pzp_{i-1}]}{\\abs{\\mcp}-\\abs{\\pzp_{i-1}}}\\label{eq : xi1}\\\\              & = \\min_{c\\in \\pzp_{i-1}:\\abs{c}>1 } i(\\rz_{c } ) .",
    "\\label{eq : xi2 }          \\end{aligned}\\ ] ]    the set of optimal solutions to is @xmath289 .",
    "the set of optimal solutions to , denoted as @xmath290 , is equal to @xmath291 , or equivalently , @xmath292 furthermore , with the product of set families @xmath293 and @xmath294 defined as @xmath295 , we have @xmath296 \\times \\prod_{c\\in              \\pzp_{i-1}\\cap \\pzp_i } \\kern-1.1em \\set{\\set{c } } \\label{eq : pii }      \\end{aligned}\\ ] ] which consists of refinements of @xmath297 by successively partitioning one or more blocks @xmath298 according to @xmath299 .",
    "see appendix  [ sec : psp_proof ]    when @xmath300 , reduces to @xmath263 because @xmath301 and @xmath302-h[\\pzp_{i-1}]=d(p_{\\rz_v}\\|\\prod_{c\\in \\mcp } p_{\\rz_c})$ ] .",
    "reduces to @xmath303 with @xmath304 and @xmath305 .    for @xmath306",
    ", means that the other critical values can be obtained simply by iteratively computing the mmi for the non - singleton blocks of the fundamental partitions .",
    "is essentially the iteration in to obtain the clusters iteratively .",
    "therefore , the critical values for the dilworth truncation coincide with the critical values for the set of clusters , and the clusters are the non - singleton elements of the partitions in the psp .",
    "this is summarized in the following corollary .",
    "( see definition  [ def : critical ] ) for some of the notations . )",
    "[ cor : main ] for @xmath307 , we have @xmath308 with the critical value @xmath88 being the @xmath10-th critical value for @xmath219 .    since the info - clustering solution maps to the entire psp of the entropy function , we can compute the clustering solution in strongly polynomial time as well .",
    "the algorithm is given in algorithm  [ algo : psp ] , which is based on the algorithm of @xcite .",
    ", @xmath309 empty arrays of size @xmath310 ; @xmath311 , @xmath312 ; @xmath313\\leftarrow\\mcq$ ] ; ;    algorithm  [ algo : psp ] computes the sequence of critical values and the psp , and stores them in the arrays and respectively .",
    "the desired clusters can then be obtained from the non - singleton subsets in the psp . as in @xcite ,",
    "the procedure starts with two partitions @xmath314 in the psp , and then check if there is any other partition @xmath186 in the psp with @xmath315 . to do so , it first computes the intersection point @xmath316 of the two lines @xmath317 $ ] and @xmath230 $ ] , and then check whether @xmath318 is equal to the dilworth truncation @xmath319 ( which is computed by lines  [ ln : psp : dt:1][ln : psp : dt:2 ] and stored in @xmath320 $ ] ) .",
    "if they are equal ( line  [ ln : psp : check ] ) , then @xmath321 and @xmath164 are two consecutive partitions with no other partition between them in the psp , and so @xmath322 is a critical value .",
    "otherwise , the optimal partition @xmath186 achieving the dilworth truncation must be a partition in the psp satisfying @xmath323 . in this case",
    ", the procedure can be invoked in a recursive manner to further identify other partitions in the psp that may lie between @xmath321 and @xmath186 , and between @xmath186 and @xmath164 .",
    "the complexity of the algorithm is mainly due to the computations of the dilworth truncation ( lines  [ ln : psp : dt:1][ln : psp : dt:2 ] ) by the submodular function minimization  @xcite .",
    "the number of such computations is at most @xmath324 , and each has a complexity of @xmath325 .",
    "therefore , the overall complexity is @xmath326 . can be used to approximate the entire clustering solution . ]    indeed , @xcite also proposed the mac clustering algorithm that builds upon the algorithm for finding the psp for a submodular cost function . although we have shown that info - clustering is also intimately connected to the psp of the entropy function , the two clustering approaches are different in two ways :    1 .   unlike info - clustering where the mmi is specified as a measure of mutual information under a meaningful hierarchical clustering formulation ,",
    "the formulation of the mac clustering does not specify how one should choose the submodular cost function for clustering .",
    "hence , the mathematical criterion of mac does not have a concrete operational meaning , that is , it is unclear in what sense are the elements in the same cluster are similar .",
    "2 .   unlike info - clustering where the solution maps precisely to the _ entire _ psp of the entropy function ,",
    "the solution of the mac clustering is sensitive to shifts of the cost function by a constant , and is therefore not identical to the psp of the submodular cost function .    in appendix",
    "[ sec : mac ] , we give detailed explanations with concrete examples differentiating the two algorithms .",
    "in this section , we show that info - clustering reduces to simpler clustering solutions under some special models .",
    "model reduction is important for practical implementations because learning the entropy function from data , and even evaluating the entropy of an arbitrary distribution can take exponential time with respect to the number of random variables .    in the following ,",
    "we show that the clustering algorithm by mutual information relevance networks ( mirn )  @xcite is a special case when @xmath44 forms a markov tree .",
    "we also show that , if @xmath44 is jointly gaussian , the clustering solution will depend only on the covariance matrix , which may be estimated more easily from data .",
    "finally , if @xmath44 has a hypergraphical correlation , then info - clustering reduces to the procedure of computing the psp for hypergraphs , which is useful in clustering the human connectome .",
    "= 1.5 mm    we first introduce the clustering by mirn in  @xcite for gene clustering .",
    "this clustering algorithm first constructs a weighted complete graph , where the nodes represent the genes to be clustered .",
    "the weight of the edge between the nodes  @xmath10 and @xmath126 is equal to the shannon s mutual information @xmath327 , which may be estimated from measurements of the expression level @xmath11 and @xmath328 of the corresponding genes  @xmath10 and @xmath126 respectively .",
    "an example of such a graph is shown in fig .",
    "[ fig : cl1 ] for the simple source model in fig .",
    "[ fig : eg : z ] . in fig .",
    "[ fig : cl1 ] , each blue edge has weight zero .",
    "each red edge has weight one , except for the red edge between node  @xmath55 and @xmath66 , which has weight two .",
    "given a threshold @xmath42 , the algorithm filters the edges by removing all edges with weights no larger than @xmath42 . the clusters at threshold @xmath42 are then defined as the non - singleton components of the resulting graphs .",
    "such non - singleton components are called the mirn .",
    "for instance , in the case of fig .",
    "[ fig : cl ] , the edge removal ( or clustering ) for different @xmath42 s is as follows :    when @xmath58 , we have the complete graph since the mutual information is non - negative .",
    "consequently , we have the trivial cluster @xmath28 .    for @xmath60 , all the blue edges are removed since they have weight equal to @xmath68 .",
    "hence , we have the two clusters @xmath27 and @xmath52 .    for @xmath62 ,",
    "only the edge between nodes  @xmath55 and @xmath66 remains , and so , we have the cluster @xmath63 .    for @xmath65 ,",
    "all edges are removed and so we have no clusters .",
    "note that the clusters we obtained from the above edge - filtering procedure are precisely the clusters we obtained by info - clustering in fig .",
    "[ fig : eg : cluster ] .",
    "we can show more generally that , info - clustering reduces to the clustering by mirn when the random variables @xmath11 s form a markov tree . in this example",
    ", we indeed have a markov chain structure , namely , @xmath329 which is a special case of the markov tree .",
    "if the random variables do not form a markov tree , then the mirn solution turns out to correspond to applying info - clustering after approximating the correlation structure by a markov tree .",
    "more precisely , for a set of random variables whose distribution does not necessarily factor according to a markov tree , the clustering solution by mirn corresponds to the solution resulting from applying info - clustering to any markov tree obtained via the chow ",
    "liu tree approximation  @xcite of the distribution .",
    "to explain the reduction above between info - clustering and clustering by mirn , we first define the clustering by mirn more formally using some graph - theoretic notations . for a simple graph @xmath330 with the vertex set @xmath28 , we denote its edge set by @xmath331 ( with the calligraphic font used for set families ) . for @xmath332",
    ", we write @xmath333 to indicate that @xmath126 is reachable from @xmath10 via a path in @xmath330 .",
    "note that @xmath334 is an equivalence relation , and we denote the set of equivalence classes as : @xmath335 where @xmath336 denotes the collection of all partitions of @xmath28 into non - empty disjoint sets .",
    "each element in @xmath337 is the vertex set of a connected component of @xmath330 , which will be considered as a cluster by mirn as we describe below .    for any threshold @xmath35 ,",
    "let @xmath338 , or simply @xmath339 , be a graph with vertex set @xmath28 and edge set @xmath340 in words , we think of @xmath341 as a complete graph and associate each edge @xmath342 with weight @xmath327 .",
    "then , the graph @xmath339 can be obtained from @xmath341 by removing the  light \" edges , i.e. , edges with weight no larger than @xmath42 .",
    "[ def : rn ] the non - singleton connected components of @xmath338 are called the mutual information relevance networks ( mirn ) .",
    "the corresponding clusters are given by : @xmath343 where @xmath344 is the partition of the vertices of @xmath339 according to the connected components of @xmath339 .",
    "next , we introduce the chow",
    " liu tree approximation under which can be obtained from info - clustering .",
    "consider a tree @xmath345 with vertex set @xmath28 .",
    "a dependency - tree approximation  @xcite to @xmath44 , denoted as @xmath346 , can be written in terms of the marginal distributions @xmath34 for @xmath347 as : @xmath348 for @xmath349 .",
    "such a distribution forms a markov tree or a bayesian network ( in which the in - degree of every vertex is at most one ) with respect to @xmath345 , i.e. , we can relabel the indices in @xmath28 to @xmath350 such that@xmath351    the set of chow  liu trees is defined as @xmath352 here , ( a ) follows from . for any @xmath353 , @xmath346 is called a chow  liu tree approximation to @xmath44 .    the celebrated chow ",
    "liu algorithm  @xcite computes a chow  liu tree as a maximum weight spanning tree since the minimization in corresponds to maximizing the second term on the right - hand side of , which is the total weight of the tree .    the main result of this subsection is the following theorem on the equivalence between the clustering by mirn and the clustering by mmi under the chow  liu tree approximation .    [ thm : tt ] the clustering of @xmath44 by mmi   under the chow ",
    "liu tree approximation   is @xmath354 for any @xmath35 and any @xmath353 .",
    "such a solution is identical to the clustering by mirn   and independent of the choice of @xmath353 .",
    "see appendix  [ sec : d ] .",
    "the proof of the equivalence makes use of the following result which evaluates the mmi for any dependency - tree distribution .",
    "[ thm : pp ] for any tree @xmath345 on the vertex set @xmath28 , @xmath355 where , as in , @xmath356 denotes the tree @xmath345 with edges @xmath357 of weight @xmath358 removed .",
    "was discovered in @xcite to be the secrecy capacity for markov trees but is new .",
    "see appendix  [ sec : e ] .",
    "in other words , the connected components of @xmath356 for @xmath359 characterize the fundamental partition for any dependency - tree distribution @xmath346 .",
    "the following theorem shows that @xmath356 in fact characterizes the entire hierarchical clustering   of @xmath346 for different values of @xmath42 :    [ thm : c ] for any tree @xmath345 on @xmath28 and any @xmath35 , @xmath360 furthermore , the critical value @xmath361 in is the @xmath10-th smallest value in @xmath362 and the partition @xmath282 in is @xmath363 , for @xmath364 and @xmath365 in place of @xmath152 .",
    "see appendix  [ sec : f ] .    however , the chow ",
    "liu tree approximation incurs a loss .",
    "indeed , the clustering by mirn fails to capture higher - order statistics beyond pairwise mutual information , because the algorithm only requires the knowledge of the pairwise mutual information .",
    "the following is a concrete example where the clustering by mirn fails , while the general info - clustering without the chow  liu tree approximation succeeds .",
    "[ eg : mirn - fails ] let @xmath366 , @xmath367 where @xmath368 s are independent uniformly random bits .",
    "it can be shown that @xmath11 s are pairwise independent , and so their pairwise mutual information are all zero .",
    "the clustering by mirn will construct a complete graph with zero weight on the edges .",
    "hence , it will not return any cluster for threshold @xmath369 , because all the edges get removed in @xmath370  .",
    "however , we know that @xmath25 , @xmath26 and @xmath20 share some mutual information , because @xmath20 can be completely determined by @xmath25 and @xmath26 . indeed , it can be shown that @xmath371 and so the random variables share non - negative mutual information .",
    "the general info - clustering algorithm will correctly find the cluster @xmath27 at threshold @xmath372 .",
    "the gaussian distribution is often used as a simplifying assumption because the distribution is completely characterized by its mean and covariance , both of which can be estimated quite efficiently from data .",
    "the measure of segregation in @xcite , for instance , is simply the differential entropy of a set of random variables assuming a jointly gaussian distribution .",
    "the click algorithm  @xcite for gene clustering also makes certain assumption about the distribution being gaussian , e.g. , in the computation of the parameters and the threshold test . however , these assumptions are often mixed with other simplifications that make it rather difficult to tract the validity or the impact of the gaussian assumption .",
    "for instance , the measure of segregation has a noise variance that is chosen in an ad - hoc manner to make the differential entropy in the desired range .",
    "for the click algorithm , the clustering solution is defined as the end result of an algorithmic procedure , but only some of the steps are justified by the gaussian assumption .",
    "in other words , the clustering solution does not appear to be uniquely defined from the mixture of algorithmic procedure and gaussian assumption .",
    "in contrast , we will derive a unique info - clustering solution assuming the random vector @xmath44 is jointly gaussian with zero mean and covariance matrix @xmath373 .",
    "it follows that any random subvector @xmath34 for @xmath30 is also jointly gaussian with zero mean and covariance matrix @xmath374 where @xmath375 is the submatrix of @xmath373 with the rows and columns indexed by elements outside @xmath37 removed . in the following , we use @xmath376 to denote the determinant of any square matrix @xmath377 .    for the jointly gaussian source @xmath44 defined above ,",
    "@xmath378    where @xmath379 is a vector of the eigenvalues of @xmath380 and @xmath381 is the matrix @xmath373 but with the entry at row @xmath10 and column @xmath126 forced to @xmath68 if @xmath10 and @xmath126 belong to different blocks in @xmath164 .",
    "is obtained by substituting the following differential entropy into , @xmath382 follows from the fact that the determinant  @xmath383 is the product @xmath384 of the eigenvalues .",
    "is because , by possibly reordering the indices in @xmath28 , @xmath381 can be written as a block diagonal matrix with @xmath380 for @xmath385 being the blocks in the main diagonal .",
    "hence , @xmath386 .",
    "[ pro : gauss : psp ] for the jointly gaussian source @xmath44 defined above , the clusters are the non - singleton subsets from the psp of the submodular function @xmath387 .",
    "from , the clustering solution can be regarded as spectral clustering in the sense that it depends on the spectrum of the submatrices of the covariance matrix .",
    "however , it is a new clustering method different from the usual spectral clustering solution such as the one for approximately minimizing the normalized cuts  @xcite .      in order to make info - clustering applicable to the clustering of neurons based on their physical connectome",
    ", we need to convert the deterministic physical connections of neurons to a random source @xmath44 .",
    "we will show that this conversion is possible by reducing the info - clustering solution under the hypergraphical source model  @xcite .",
    "to explain the idea , we start with the emulated source model .    [ def : esn ] for @xmath148 ,",
    "let @xmath388 such that , @xmath389 the vector @xmath44 is called an emulated source network .    we can think of every @xmath148 as a terminal that can send an input signal @xmath390 independently over a channel that returns the output signal @xmath391 to terminal @xmath10 , where @xmath392 is deterministic and @xmath393 s are independent channel noises that satisfy @xmath394 .",
    "( note that the observation @xmath395 of terminal  @xmath10 may depend on the input specified by other terminals . ) since @xmath11 captures all the information in the input and output signals associated with terminal  @xmath10 , the mmi among @xmath11 s reflects the mutual information among the terminals , and so we can cluster the terminals accordingly . the mmi has the following special form :    [ pro : esn ] for the emulated source network in definition  [ def : esn ] , @xmath396 which is an achievable secret key rate under a multiterminal channel model  ( * ? ? ?",
    "*  ii - b ) .    in network information theory ,",
    "the mapping to the conditional mutual information , @xmath397 in , is a cut function ( evaluated at the cut set @xmath398 ) that measures the total amount of information flow from the terminals in @xmath399 to the terminals in @xmath398 . similar to the usual graphical cut function ,",
    "this cut function is also submodular with respect to @xmath398 .",
    "therefore , the info - clustering algorithm will return the non - singleton subsets in the psp of the cut function as the clusters .",
    "a special case of interest is when the channel @xmath400 consists of a set of broadcast links among the subsets of the terminals .",
    "more precisely , consider a hypergraph with vertex set @xmath28 , edge set @xmath401 , and edge function @xmath402 .",
    "each hyperedge @xmath403 is regarded as a broadcast link with sender specified by @xmath404 and receivers being the terminals in @xmath405 .",
    "@xmath406 is called the orientation of the edge @xmath357 .",
    "the capacity of the broadcast link is specified by the non - negative weight @xmath407 .",
    "more precisely , the emulated source @xmath44 is defined using    @xmath408    and the input - output relationship of each broadcast link @xmath357 : @xmath409    where the first equation says that the outputs of the broadcast link are equal to its input , and the second equation means that the capacity @xmath407 is the log cardinality of the input alphabet set , which is the maximum amount of information that can be sent across the broadcast link . for instance , such a broadcast link can be used as a simple model for the physical connection between neurons because a neuron broadcasts signals to one or more neurons through the gap junctions and chemical synapses .",
    "the weight @xmath407 can be obtained from the number of synapses .",
    "more elaborate models , such as the interference link in @xcite , the adt network in @xcite , and the matroidal network link model  @xcite , can also be considered .",
    "it is easy to argue that the mmi is maximized by the uniform input distribution , and the emulated source can be equivalently defined as follows without depending on the orientation @xmath406 : . ]",
    "[ def : bc ] a broadcast network with respect to the hypergraph @xmath410 is defined as @xmath411 with @xmath412 uniformly distributed and @xmath413 .",
    "the fact that the source model does not depend on the orientation @xmath406 means that one needs not distinguish between directed and undirected links for info - clustering .",
    "for instance , even though the gap junction in neurons is undirected and the chemical synapses are directed , the direction does not affect the clustering .",
    "this is because each link , directed or not , leads to a piece of information shared _ symmetrically _ among both the sender and the receivers .",
    "the choice of the uniform input distribution can also be justified more rigorously . in the secret key agreement problem under the channel model  (",
    "*  vi - b ) , the uniform distribution on the input was shown to achieve the secrecy capacity , which is precisely the mmi @xmath183 .",
    "furthermore , the mmi can also be written in the form of a max - flow min - cut expression that characterizes the maximum multicast rate of network coding  @xcite.the mmi can be written in terms of the directed cut function for the hypergraph :    the mmi of the broadcast network   is    [ eq : zi ] @xmath414    where @xmath415 is a hypergraph of @xmath416 with an arbitrary choice of the orientation @xmath406 for each edge , @xmath417 are the set of in - coming edges into @xmath398 and the partition of @xmath357 respectively .    even though the mmi does not depend on the orientation @xmath406 , as shown in , it is informative to consider the alternative form in that is stated with an arbitrary choice of the orientation @xmath406 . in particular , from , we can deduce that :    [ pro : psp : hyp ] for the hypergraphical source @xmath44 defined above , the clusters are the non - singleton subsets from the psp of the submodular in - cut function @xmath418 .",
    "indeed , the physical connectome may be simplified as a graph instead of a hypergraph because the polyadic synapses that connect one neuron to multiple neurons are rare  @xcite . in the special case when the hypergraph is a graph @xmath419 with @xmath420 , the broadcast network in reduces to the graphical network called the pairwise independent network ( pin )  @xcite .",
    "it is straightforward to show that the mmi in can be further written as the strength of the graph :    @xmath421    where @xmath422 is the submodular undirected cut function with the edge cut @xmath423    the factor of @xmath66 in the denominator of comes from the fact that an edge that crosses @xmath164 overlap with two disjoint subsets in @xmath164 , so it is doubly counted in the numerator . since the factor does not affect the psp , we have the following result :    [ pro : psp : g ] for the graphical source @xmath44 defined above , the clusters are the non - singleton subsets from the psp of the undirected cut function @xmath424 .    by the tutte  nash - williams tree packing theorem , the strength of a graph has the meaningful interpretation as the maximum amount of fractional tree packings of the graph  @xcite , which can also be extended to more general notion of partition connectivity for hypergraphs  @xcite .",
    "it can be shown that the principal sequence for graphs correspond to successive packing of forests , with the first critical value being the strength of the graph and the last critical value being the fractional arboricity , defined as the maximum amount of forests one can fractionally pack in the graph  @xcite .",
    "in this section , we provide some discussions on how info - clustering can be used for the clustering of genes and neurons . for concreteness",
    ", we will describe some available datasets , and explain what one may potentially learn from them .      as described in section  [ sec : cl ] , the clustering by mirn  @xcite is a special case of info - clustering under the chow  liu tree approximation .",
    "therefore , the experimental results in @xcite can be regarded as preliminary results of info - clustering , which may potentially be improved by considering higher - order correlation beyond pairwise mutual information as shown in example  [ eg : mirn - fails ] .",
    "the work in @xcite considered the dataset from @xcite , which involves 2467 genes of a species of yeast called saccharomyces cerevisiae .",
    "the expression level of each gene was measured under 79 different conditions , including different stages of the cell cycle , temperatures , and time points . with @xmath28 denoting the set of all genes , the different expression levels of gene @xmath148 were regarded as i.i.d .",
    "realizations of a random variable @xmath11 that can be used for info - clustering . towards this end ,",
    "the pairwise mutual information @xmath327 between genes @xmath10 and @xmath126 was estimated using the empirical joint distribution of @xmath11 and @xmath328 after uniform quantization ( since the expression levels are real - valued ) .",
    "we note that the empirical entropy after quantization can also be approximated without computing the empirical distribution  @xcite .",
    "similarly , the mmi beyond the pairwise mutual information can be estimated from the empirical distribution of the quantized expression levels .",
    "the idea is to compute the empirical entropies of subsets of random variables @xmath34 after quantization , and use them in to estimate the mmi .",
    "the mmi of the quantized random variables is shown to approach the mmi of the continuous random variables in ( * ? ? ?",
    "* appendix  b ) , and the details of the quantization can be found therein . however , computing the empirical joint distribution of a subset of random variables or estimating the joint entropy from the data samples takes exponential time with respect to the size of the subset  @xcite .",
    "this seems to suggest that some heuristics might be needed to tackle the problem of estimating the mmi .",
    "for example , an approach considered in @xcite was to use a file compression algorithm to return the file size after compressing the data associated with the subset of random variables .",
    "alternatively , one may consider other model reduction techniques so that the simplifying assumption made is clear .",
    "as described in section  [ sec : cif ] , info - clustering can be specialized to cluster graphical networks . when applied to the physical connectome of neurons , it can identify clusters of tightly connected neurons , as well as the important inter - cluster connections , the damage of which may cause detrimental effects . while the physical connectome may not represent the functional connectome , i.e. , the task - specific stimulation patterns of the neurons , the resulting clusters may be studied in conjunction with the functional connectome to understand how neurons work together to manifest consciousness and to carry out brain functions  @xcite .    instead of looking at the human connectome data , as an illustration",
    ", we will consider a small and nearly complete physical connectome dataset in @xcite for a small creature called nematode _ c.  elegans_. a set of 279 neurons in the somatic nervous system was considered , and the dataset is in the form of an adjacency matrix , recording the total number of synaptic contacts ( gap junction and chemical synapses ) between every pairs of neurons .",
    "the adjacency matrix defines the weighted graph for info - clustering .",
    "we can compute the cut function of the graph from the adjacency matrix , and then obtain the desired clusters from the psp of the cut function as described in section  [ sec : cif ] .",
    "it is worth pointing out that , owing to the similarity between mac clustering algorithm of @xcite and info - clustering as pointed out in appendix  [ sec : mac ] , we expect the performance of info - clustering to be close to that of the mac clustering for cut functions , which was shown in @xcite to be competitive with the existing leading algorithms for clustering graphical networks .",
    "based on the info - clustering paradigm , we can derive some meaningful measures to describe the clustering solutions .",
    "for example , the mmi @xmath183 naturally measures how integrated the objects in @xmath28 are .",
    "this is because the more interaction among the objects in a system , the larger the mutual information they share .",
    "such an argument is supported by the concrete operational meanings of @xmath183 as the secrecy capacity for the multiterminal secret key agreement problem , the multicast throughput for the network coding problem , and the partition connectivity for hypergraphical or graphical models .    a measure called the integrated information",
    "was proposed in ( * ? ? ?",
    "* ( 2b ) ) to measure how integrated a system is .",
    "this measure may appear similar to the mmi in the sense that it is defined as the divergence from the joint distribution of the overall system to the product of the marginal distributions of some subsystems .",
    "( the subsystems are obtained by partitioning the system according to what is called the minimum information partition . ) however , there are two fundamental differences between the two definitions :    1 .",
    "in contrast to the fundamental partition @xmath190 , the minimum information partition is obtained by an additional normalization factor that forces the partition to be more balanced . however",
    ", this additional factor makes the problem intractable .",
    "this is similar to the normalized - cut minimization problem , which is np - hard to solve .",
    "in general , the cluster size has nothing to do with the amount of information mutual to the elements in the cluster .",
    "thus , this additional factor can steer the clustering procedure away from finding a small cluster that has high mutual information .",
    "2 .   unlike the mmi , the divergence expression in the integrated information with respect to the minimum information partition , say @xmath164 , is not normalized by the factor @xmath425 . as we have described using the concept of residual independence relation , the factor is needed to account for the double counting in the mutual information in each subsystem .",
    "therefore , the integrated information does not have the desired information - theoretic meaning .",
    "in addition , the integrated information is computed from the a posteriori probability that is marginalized using a uniform input distribution .",
    "however , the a posteriori distribution can be viewed as a channel , which can be handled by info - clustering as in the emulated source model in   [ sec : cif ] .",
    "moreover , instead choosing a uniform distribution by assumption , we can justify such a choice as one that maximizes the mmi in the case of the hypergraphical model .",
    "a measure of segregation was also proposed in @xcite . however , there are two issues of the formulation :    1 .",
    "the measure assumes a jointly gaussian distribution rather than a general source distribution , so it is unclear how the measure can capture a more general correlation structure .",
    "the measure is normalized using a noise variance , which is chosen in an ad - hoc manner without a concrete interpretation .",
    "the normalization is also done in a way different from the usual signal - to - noise ratio for the mimo gaussian channel  @xcite .",
    "we believe that the measure of segregation is simply a dual to the measure of integration , i.e. , the mmi can be used to measure segregation and there is no need to define another fundamental quantity .",
    "more precisely , we can measure the segregation of a cluster @xmath398 of @xmath28 as @xmath426.\\label{eq : segregation}\\end{aligned}\\ ] ] the index is non - negative because @xmath427 by the formulation   of clusters , and it is upper bounded by @xmath55 because of the non - negativity of the mmi .",
    "the index is large ( the ratio @xmath428 is small ) if the cluster @xmath398 is more integrated than the entire set @xmath28 , that is to say , @xmath398 is more segregated from the rest of the nodes in @xmath399 .",
    "depending on the application , one may further compute the average , minimum , or maximum segregation among a set of clusters to show how segregated the clusters are from each other .",
    "in this work , we proposed a new information - theoretic approach to clustering biological systems .",
    "in particular , we formulated the info - clustering paradigm and showed how it can be applied to study the human genome and connectome .",
    "compared to the conventional algorithmic approaches , info - clustering follows a bottom - up theoretical approach for clustering . rather than justifying the algorithm purely by data , which was shown to have many issues",
    ", we believe that it is more important to lay a rigorous mathematical theory before algorithmic simplifications .",
    "in particular , the info - clustering is formulated in a meaningful way without requiring any prior knowledge of the number of clusters nor an initial solution to start the clustering algorithm .",
    "the solution is shown to be unique , with meaningful information - theoretic interpretations as well as an elegant mathematical structure for efficient computation .",
    "more precisely , we formulated the clustering problem   using a threshold test on the mmi , and showed that the solution is hierarchical under a simple , but general , property   of the mmi , which also holds for some other choices of multivariate mutual information measures .",
    "the clustering solution is characterized by a finite set of critical values and their corresponding finite sets of clusters  .",
    "the formulation is different from the classical one in the sense that the set of clusters is not required a priori to form a partition .",
    "instead , the set of all clusters is shown to be laminar   using the general property   of the mmi .",
    "consequently , the complete clustering solution can be computed iteratively in algorithm  [ algo : iteration ] .    using the precise definition   of the mmi",
    ", we further showed that the clustering solution maps to the psp of the entropy function .",
    "more precisely , the set   of critical values for info - clustering is precisely the set of critical values   for the dilworth truncation   of the residual entropy function  .",
    "the corresponding set of clusters are the non - singleton subsets from the psp   of the entropy function .",
    "this connection is non - trivial .",
    "it is based on the iterative relation   among the clusters and the iterative relation   among the psp .",
    "this connection not only enriches the abstract mathematical structure of the psp with the concrete operational meanings from information theory , but also provides a concrete clustering solution that can be computed from the psp in strongly polynomial time .",
    "indeed , we showed that info - clustering reduces to simpler and more practical algorithms under some special source models .",
    "unlike the approximation algorithms of many clustering formulations , which focuses mainly on algorithmic simplicity , the model reduction for info - clustering specifies precisely what kind of correlation structure is assumed in return for the algorithmic simplicity .",
    "consequently , we can verify whether the simplifying model applies to the case of interest , and identify the weaknesses of the simplified algorithm .",
    "in particular , we showed that under the markov tree model info - clustering reduces to the gene clustering algorithm by mirn  .",
    "if the correlation structure is not a markov tree , the clustering by mirn corresponds to the info - clustering algorithm under the chow ",
    "liu tree approximation .",
    "this shows that not only can info - clustering apply in practice to gene clustering , but it can also be used to justify existing techniques such as clustering by mirn properly , with a concrete example showing how the chow  liu tree approximation may fail to capture the more complex multivariate correlation beyond the pairwise mutual information .",
    "we also considered the usual gaussian assumption , which simplifies the info - clustering solution to a clustering algorithm by the covariance matrix , or more specifically , the eigenvalues of the submatrices of the covariance matrix ( proposition  [ pro : gauss : psp ] . ) .",
    "this is a new spectral clustering technique that follows precisely from the info - clustering paradigm without any approximation . for the study of the human connectome",
    ", we also examined the specification of info - clustering to the hypergraphical model , which can capture the possibility of polyadic physical connections among neurons . in this case",
    ", the solution reduces to the psp of hypergraphs and graphs ( proposition  [ pro : psp : hyp ] and [ pro : psp : g ] ) , which can be computed more efficiently than the psp of the entropy function of a general source model .",
    "in addition to the algorithmic simplicity , the solution also has a meaningful interpretation as the network information flow : clusters are simply subnetworks that support large information flows .",
    "finally , using the info - clustering paradigm , we also demonstrated how the mmi can be used as a measure of the integration of a cluster , which can further be used to measure how segregated a cluster is from the other objects or clusters  .",
    "the measures do not assume any particular source model or choice of parameters .",
    "their values can be computed and justified from the info - clustering solution .    addtoresetequationsection    addtoresettheoremsection    addtoresetlemmasection    addtoresetcorollarysection    addtoresetexamplesection    addtoresetremarksection    addtoresetpropositionsection    addtoresetdefinitionsection    addtoresetsubclaimtheorem",
    "the following is a necessary and sufficient condition for a set to be a cluster :    [ pro : maxb ] a non - empty non - singleton subset of @xmath28 is a cluster of @xmath44 if and only if it can not be enlarged without reducing multivariate information quantity , i.e. , @xmath429 @xmath430    for @xmath106 .",
    "suppose the r.h.s .  of holds .",
    "then , we have @xmath431 ( and therefore the l.h.s . of ) because @xmath37 is a maximal subset with multivariate information at least the threshold @xmath432 .",
    "suppose the r.h.s .  of does not hold ,",
    "i.e. , there exists a proper superset @xmath39 with @xmath433 .",
    "it follows that @xmath40 whenever @xmath38 and so @xmath37 can not be maximal in for any threshold @xmath35 .",
    "therefore , the l.h.s .  of",
    "does not hold either .",
    "[ pro : upgamma ] @xmath434 , consisting of the multivariate information quantities of the clusters .    if @xmath435 for some @xmath35 , then @xmath436 and so @xmath437 by definition  .",
    "consider any @xmath438 .",
    "then , by , we have one of the following two cases :    there exists @xmath439 , i.e. , a cluster that disappears at @xmath42 .",
    "we must have @xmath440 by , and so @xmath441 as desired .",
    "there exists @xmath442 , i.e. , a cluster that appears at @xmath42 . by proposition",
    "[ pro : maxb ] , this happens only if there is a larger cluster @xmath39 that disappears at @xmath42 , which reduces to the previous case .",
    "we are now ready to prove theorem  [ thm : cluster ] . for @xmath443 ,",
    "the set @xmath118 contains @xmath28 by definition  .",
    "indeed , @xmath28 is the unique cluster because it is the largest subset of @xmath28 .",
    "it follows that @xmath263 and @xmath444 for @xmath445 .    by the definition of critical values  ,",
    "the cluster @xmath118 must remain unchanged for @xmath42 between consecutive critical values .",
    "therefore , @xmath446 for @xmath447 and @xmath448 .",
    "when @xmath449 , we have @xmath450 because no solution @xmath37 to can have @xmath31 larger than the maximum  , which must therefore be the last critical value .",
    "since the clusters in @xmath87 remain to be clusters for @xmath451 , they must achieve the maximum value of the multivariate information quantity .",
    "= [ fill = gray , opacity=0.3,draw = black ] = [ draw , thick , blue,<- ] = [ draw , thick , red , decorate , decoration = crosses , pre = moveto , pre length=1*,post = moveto , post length=1*,shape size=0.5*u ]    .5    .45    to help understand the proof , the readers may refer to fig .",
    "[ fig : iterate:1 ] for a summary of the relationship among the clusters in the proof .",
    "let @xmath97 be a cluster of @xmath44 that is a proper superset of @xmath95 , i.e. , we have @xmath452 such a choice of @xmath97 exists because @xmath28 is a feasible choice , but there can be multiple feasible choices .",
    "we choose any one that maximizes @xmath453 .",
    "we will show that @xmath97 and @xmath454 satisfy .",
    "note that @xmath455 and @xmath456 because @xmath457 and @xmath98 respectively .",
    "we also have @xmath458 because , otherwise , @xmath459 contradicts the maximality of @xmath98 .",
    "altogether , we have @xmath460 where the equality  ( b ) is by .",
    "( a ) , ( b ) and ( c ) implies @xmath461 as desired by .",
    "furthermore , @xmath462 ( from ( c ) and ( d ) ) and the fact that @xmath95 is a cluster of @xmath44 implies that @xmath95 is also a cluster of @xmath96 . however , to establish , we need to show the stronger statement that @xmath463 .",
    "now , @xmath464 because we at least have @xmath465 with @xmath466 ( from ( b ) , ( c ) and ( d ) ) .",
    "therefore , we have @xmath463 as desired by unless there exists @xmath467 suppose to the contrary that such a subset @xmath37 exists . then , @xmath468 because @xmath469 .",
    "we will show that , regardless of whether @xmath37 is a cluster of @xmath44 or not , there is a contradiction to the maximality of @xmath453 among all feasible subsets @xmath97 satisfying .",
    "suppose @xmath140 .",
    "then , holds with @xmath97 replaced by @xmath37 and so @xmath470 ( from ( b ) and ( f ) ) contradicts the choice of @xmath97 .",
    "suppose @xmath471 .",
    "then , there exists @xmath472 this is the complete scenario shown in fig .",
    "[ fig : iterate:1 ] .",
    "it follows that @xmath473 ( from ( e ) and ( g ) ) and @xmath474 ( from ( a ) , ( b ) , ( f ) and ( h ) ) .",
    "therefore , holds with @xmath97 replaced by @xmath398 , to be the previous value of @xmath453 , in which case ( a ) need not be used to argue the contradiction . ] but @xmath475 ( from ( b ) , ( f ) and ( h ) ) contradicts the choice of @xmath97 .",
    "it suffices to show that for any cluster @xmath95 of @xmath44 , we have @xmath476 i.e. , a cluster of @xmath477 must be a cluster of @xmath44 .",
    "then , from , a simple induction on @xmath42 over the finite set @xmath256 will immediately lead to .    to prove , consider any @xmath37 that is in the r.h.s .  of",
    "then , by definition  , @xmath37 is a maximal subset of @xmath28 with @xmath38 for some @xmath35 . since @xmath478",
    ", we also have that @xmath37 is a maximal subset of @xmath95 with @xmath38 , i.e. , @xmath37 is a cluster of @xmath477 , belonging to the set in the l.h.s .  of",
    "therefore , @xmath479 holds for .",
    "it remains to show the reverse inclusion @xmath480 for . to help understand the proof",
    ", the readers may refer to fig .",
    "[ fig : iterate:2 ] for a summary of the relationship among the clusters .",
    "suppose to the contrary that a cluster @xmath97 of @xmath477 is not a cluster of @xmath44 .",
    "note that @xmath481 because @xmath95 is a cluster of @xmath44 but @xmath97 is not . therefore , we have the strict inequality @xmath482 . by",
    ", there exists @xmath483 which implies that @xmath484 we will show that @xmath485 , contradicting laminarity  .",
    "@xmath486 because both @xmath398 and @xmath95 contains the non - empty set @xmath97 .",
    "@xmath487 or simply @xmath488 because , if to the contrary that @xmath489 , then @xmath490 and @xmath491 assumed above contradicts the fact that @xmath97 is a cluster of @xmath477 .",
    "@xmath492 or simply @xmath493 because , if to the contrary that @xmath494 , then @xmath495 and @xmath496 derived above contradict the fact that @xmath95 is a cluster of @xmath44 .",
    "the line segment preceding the @xmath497 is @xmath498 $ ] since @xmath499 by .",
    "the curve after @xmath225 has a strictly smaller slope than @xmath500 by the definition of a turning point , and so @xmath501 is a solution to @xmath502 = \\min_{\\substack{\\mcp\\in \\pi(v ) : \\abs{\\mcp}>\\abs{\\pzp_{i-1 } } } } h_{`g}[\\mcp ]      \\end{aligned}\\ ] ] where the r.h.s",
    ".  corresponds to @xmath219 for @xmath503 , with the set of optimal partitions at @xmath501 being @xmath504 . rearranging the terms",
    ", it follows that @xmath505 - h_{`g_i } [ \\pzp_{i-1 } ] \\\\          & = \\min_{\\substack{\\mcp\\in \\pi(v ) : \\abs{\\mcp}>\\abs{\\pzp_{i-1 } } } } h[\\mcp ] - h [ \\pzp_{i-1 } ] - ` g_i(\\abs{\\mcp}-\\abs{\\pzp_{i-1 } } ) \\\\          & = \\min_{\\substack{\\mcp\\in \\pi(v ) : \\abs{\\mcp}>\\abs{\\pzp_{i-1 } } } } \\frac{h[\\mcp ] - h [ \\pzp_{i-1}]}{\\abs{\\mcp}-\\abs{\\pzp_{i-1 } } } - ` g_i ,      \\end{aligned}\\ ] ] which implies .",
    "the last expression is obtained from the previous by multiplying @xmath506 , which preserves both the minimum value of @xmath68 and the set of minimum solutions , namely @xmath504 . since @xmath499 ,",
    "every optimal solution is finer than @xmath297 , and so it does not lose optimality to impose @xmath507 in or equivalently , @xmath508 using the above , can be rewritten as @xmath509 - h(c)`2]}{\\sum_{c\\in \\pzp_{i-1 } } ( \\abs{\\mcp^c}-1 ) } \\\\          & \\leq \\min_{c\\in \\pzp_{i-1}:\\abs{c}>1 } \\min_{\\mcp^c\\in \\pi'(c ) } \\frac{d(p_{\\rz_c}\\|\\prod_{c'\\in \\mcp^c } p_{\\rz_{c ' } } ) } { \\abs{\\mcp^c}-1 }       \\end{aligned}\\ ] ] the last expression is obtained by imposing @xmath510 for all but one @xmath511 with @xmath512 , and substituting @xmath513 -      h(c)=d(p_{\\rz_c}\\|\\prod_{c'\\in \\mcp^c } p_{\\rz_{c'}})$ ] .",
    "it is equal to the r.h.s .  of by the definition of @xmath514  .",
    "( the existence of a block @xmath515 is guaranteed by proposition  [ pro : psp ] since @xmath297 is coarser than the partition into singletons , i.e. , @xmath516 . ) to show the reverse inequality , let @xmath42 be the r.h.s .  of and @xmath517 be the set of optimal solutions .",
    "then , for all @xmath511 and @xmath518 , we have @xmath513 -      h(c)\\geq`g ( \\abs{\\mcp^c}-1)$ ] and so @xmath519 - h(c)`2]}{\\sum_{c\\in \\pzp_{i-1 } } ( \\abs{\\mcp^c}-1 ) } \\\\                  & \\geq \\frac{\\sum_{c\\in \\pzp_{i-1 } } ` g ( \\abs{\\mcp^c}-1)}{\\sum_{c\\in                  \\pzp_{i-1 } } ( \\abs{\\mcp^c}-1)}= ` g       \\end{aligned}\\ ] ] equality happens if and only if , for all @xmath511 , either we have @xmath520 or we have @xmath521 and @xmath522 .",
    "this implies @xmath523 and therefore .",
    "the proof relies on theorem  [ thm : c ] proved in appendix  [ sec : e ] , which relies on theorem  [ thm : pp ] proved in appendix  [ sec : f ] .",
    "first note that to prove theorem  [ thm : tt ] , it suffices to prove @xmath524 , because this implies by . in other words , we want to show that the vertex sets of the connected components of @xmath356 are the same as those of @xmath339 , i.e. , for any @xmath332 , we have @xmath525 if and only if @xmath526 . the direct part ( only if ) is obvious , because @xmath356 is a subgraph of @xmath339 .",
    "to prove the converse ( if ) part , we will use the following exchange property for spanning trees .",
    "[ lem : exchange ] consider two spanning trees @xmath345 and @xmath527 on the vertex set @xmath28 .",
    "for any @xmath528 , there exists @xmath529 such that the graph @xmath345 with @xmath357 replaced by @xmath530 , denoted as @xmath531 , is a spanning tree .    now , suppose to the contrary that @xmath532 but @xmath526 . let @xmath533 be the set of edges in a path from @xmath10 to @xmath126 in @xmath339 .",
    "let @xmath357 be an edge in the path from @xmath10 to @xmath126 in @xmath345 but with @xmath358 , and therefore not in @xmath534 nor @xmath533 .",
    "such an edge exists by the assumption @xmath532 .",
    "let @xmath330 be the graph with edge set @xmath535 .",
    "there exists a spanning tree @xmath527 of @xmath330 since @xmath330 is connected , which follows from the facts that @xmath345 is spanning and @xmath533 connects the incident vertices of the removed edge @xmath357 .",
    "since @xmath528 , we have by lemma  [ lem : exchange ] that there exists @xmath529 such that @xmath531 is a spanning tree .",
    "the tree @xmath531 has a larger weight than @xmath345 because @xmath536 , as @xmath537 .",
    "this contradicts the maximality of @xmath538 .",
    "we have thus completed the proof of theorem  [ thm : tt ] .",
    "we remark that while the above arguments are purely of a graph - theoretical nature , in proving theorems  [ thm : pp ] , and subsequently theorem  [ thm : c ] , we rely on some information - theoretic properties of the mmi  . the following is a lower bound on @xmath161 specific to the dependency - tree distributions :    [ lem : ip ] consider the notation in . for @xmath150 , @xmath539 where @xmath540 .",
    "moreover , equality holds if we have @xmath541 note that simply means that the subgraph of @xmath345 induced on each @xmath385 is a subtree .    by",
    ", we can express @xmath542   in terms of the entropies as @xmath543 .",
    "it follows from the definition of @xmath544 that @xmath545 , and so @xmath546          & { \\mathop{=}\\limits^{\\text{(a ) } } } \\sum_{c\\in \\mcp } \\overbrace{\\sum_{i\\in c } \\underbrace{h(\\rz_i^t|\\rz^t_{\\set{j\\in c\\mid j <",
    "i}})}_{\\text{(iii)}}}^{\\text{(i ) } ' } - \\overbrace{\\sum_{i\\in v } \\underbrace{h(\\rz_i^t|\\rz^t_{\\set{j\\in v\\mid j < i}})}_{\\text{(iv)}}}^{\\text{(ii)}'}\\\\          & { \\mathop{\\geq}\\limits^{\\text{(b ) } } } \\sum_{c\\in \\mcp } ` 1[h(\\rz_{\\min c})+\\sum_{i\\in c`/\\set{\\min c } } h(\\rz_i^t|\\rz^t_{{\\mathsf{p}}_i})`2 ] \\\\          & \\kern1em - \\sum_{i\\in v } h(\\rz_i^t|\\rz^t_{{\\mathsf{p}}_i})\\\\          & = \\sum_{c\\in \\mcp } ` 1[h(\\rz_{\\min c } ) - h(\\rz_{\\min c } | \\rz_{{\\mathsf{p}}_{\\min c}})`2 ]      \\end{aligned}\\ ] ] which is equal to @xmath547 , completing the proof of . to obtain",
    "( a ) , we applied the chain rule @xmath548 and @xmath549 . to obtain ( b ) , we used @xmath550 by the markov relation  .",
    "@xmath551 when @xmath552 , and @xmath553 for @xmath554 by the markov relation and the fact that conditioning reduces entropy .",
    "equality holds if and only if holds , again due to the markov relation  .",
    "we are now ready to prove theorem  [ thm : pp ] . by in lemma",
    "[ lem : ip ] , @xmath555 where ( a ) is because the minimum edge weight on the right is no larger than the average on the left ; ( b ) is because @xmath556 .",
    "the above implies @xmath557 for   by the definition   of mmi . to prove the reverse inequality ,",
    "let @xmath558 and @xmath559 .",
    "we shall argue that : @xmath560    * \\(c ) is because the equality condition   holds .",
    "more precisely , every @xmath385 is the vertex set of a connected component , and so , for all @xmath561 , we have @xmath562 unless @xmath552 . * to argue ( d ) , it suffices to show that @xmath563 , because @xmath564 for all @xmath565 . for any @xmath385",
    ", we have @xmath566 because @xmath567 by .",
    "therefore , any edge @xmath568 is also in @xmath569 .",
    "conversely , consider any edge @xmath570 and @xmath571 for some @xmath572 in @xmath28 .",
    "suppose to the contrary that @xmath573 . by the equality condition   proved earlier , we have @xmath552 , contradicting @xmath573 .",
    "now that is proved , we have @xmath359 and @xmath574 . to prove ,",
    "suppose to the contrary that @xmath575 , i.e. , there exists @xmath576 with @xmath577 .",
    "we have @xmath578 where ( e ) is by the definition of @xmath42 ; ( f ) is by ; and ( g ) is because @xmath576 . by , every edge in @xmath579 has weight @xmath42 , and so @xmath580 , implying @xmath581 .",
    "however , @xmath582 which contradicts @xmath580 .",
    "note here that the last equality follows from the proof of ( d ) above .",
    "this completes the proof of theorem  [ thm : pp ]      we shall prove by induction that , for @xmath583 , @xmath361 is the @xmath10-th smallest value of @xmath584 for @xmath585 , and @xmath586 with @xmath282 defined in for @xmath365 in place of @xmath152 .",
    "this will imply by corollary  [ cor : main ] . by and with @xmath300",
    ", we have @xmath587 and @xmath588 .",
    "this implies the base case under and , namely that , @xmath589 is the smallest @xmath584 and @xmath590 .",
    "let @xmath591 be the subgraph of @xmath345 induced on the subset @xmath125 of vertices . by , for @xmath592",
    ", @xmath593 here , ( a ) is by the inductive hypothesis @xmath594 as well as that @xmath595 .",
    "( b ) is because @xmath596 is the union of @xmath597 over @xmath598 ( for @xmath599 ) .",
    "the above equalities implies that @xmath361 is the @xmath10-th smallest value of @xmath584 for @xmath585 because the r.h.s .  of ( b )",
    "is , by the inductive hypothesis that @xmath600 is the @xmath601-st smallest value , and the fact that @xmath602 contains all edges in @xmath603 with weights strictly larger than @xmath600 .",
    "it remains to show @xmath604 . from , we have @xmath605 here , ( d )",
    "is by applying to .",
    "( e ) is by rewriting @xmath606 as @xmath607 and then applying @xmath608 because @xmath609 means that every edge of @xmath610 has weight strictly larger than @xmath361 by .",
    "finally , ( f ) follows from @xmath611 , which can be argued as follows .",
    "@xmath479 is obvious because @xmath612 . to prove the reverse inclusion , note that @xmath613 and so the edge in @xmath614 must be in a connected component of @xmath602 , namely , a subtree @xmath615 induced on some @xmath616 .",
    "this completes the proof of theorem  [ thm : c ] .",
    "the objective of _ minimum average cost ( mac ) clustering _ is to obtain a partition @xmath164 of size @xmath617 for some threshold @xmath2 as the set of clusters , and the singleton elements in the partition are also regarded as clusters in satisfying the constraint @xmath617 . to solve this problem using our clustering solution by multivariate mutual information",
    ", it is natural to use @xmath618 where @xmath282 s form the psp of the entropy function in .",
    "@xmath282 is the coarsest partition from the psp with more than @xmath2 parts .",
    "the clustering solution proposed by @xcite obtains the partition by solving the following minimum average cost constraint instead :    [ eq : dc:2 ] @xmath619\\label{eq : dc:2:b }      \\end{aligned}\\ ] ]    where @xmath620 is a submodular function that needs to be chosen appropriately .",
    "the question of interest is , whether there is an obvious choice of @xmath620 in terms of the entropy function @xmath262 for which the two clustering solutions in and are the same .",
    "the similarity is more apparent by thinking of @xmath164 in as the solution to , namely , @xmath621-h[\\pzp_{i-1}]}{\\abs{\\mcp}-\\abs{\\pzp_{i-1}}},\\ ] ] which is similar to except for the numerator and @xmath2 in place of @xmath622 .",
    "note that the choice of @xmath297 depends on @xmath2 according to .",
    "in particular , @xmath623 but _ equality is not needed _",
    "so long as the solution @xmath282 to the above minimization satisfies @xmath624 as required by .    for graphical networks",
    ", @xcite chooses @xmath620 to be the cut function of the graph , which is also the case for info - clustering by information flow in section  [ sec : cif ] .",
    "the following is a concrete example that distinguishes info - clustering from mac clustering .    [ eg : mac : cut ] consider a weighted graph @xmath330 with vertex set @xmath366 , edge set @xmath625 , and @xmath626 where @xmath627 and @xmath628 are the edge and weight functions as in section  [ sec : cif ] .",
    "the psp of the cut function @xmath422 ( see ) can be shown to be @xmath629 for @xmath630 , info - clustering will return @xmath631 according , since @xmath631 is the coarsest partition with more than @xmath2 parts .",
    "however , choosing @xmath620 to be the cut function , mac clustering does not return the same solution because the average cost   of @xmath632 is strictly smaller than that of @xmath631 : @xmath633 indeed , it can be shown that @xmath632 achieves the minimum average cost among all other partitions of @xmath28 , and so mac clustering will return the less intuitive clustering by @xmath632 instead of @xmath631 .    actually , @xmath620 was assumed to be non - negative in  @xcite , because then , the constraint @xmath617 can be dropped from without changing the solution .",
    "doing so reduces to computing the dilworth truncation , which can be done efficiently and guaranteed to return a partition in the psp , despite the possibility of returning one that is finer than required , as shown in the previous example ( since cut function is non - negative ) . in the general case when @xmath620 can be negative , removing",
    "the constraint @xmath617 from can potentially change the solution to something outside the psp , and so it is unclear whether the clusters can be computed efficiently . in the following ,",
    "we will further compare mac clustering to info - clustering without assuming @xmath620 to non - negative .",
    "it can be shown that constant scaling of @xmath620 does not change the solution to , but constant shift does . to ensure submodularity , a reasonable choice of @xmath620 is @xmath634 for some appropriate constant shift @xmath635 .",
    "we will show that there is a choice of @xmath635 such that the clustering solutions for @xmath636 are the same for and . however , there is no choice of @xmath635 for which the complete clustering solutions for different @xmath2 are the same .",
    "more precisely , the fundamental partition @xmath637 in our clustering solution   can be obtained from with @xmath638 to see this , rewrite with the above choice and @xmath636 : @xmath639 the first term on the r.h.s .",
    "is @xmath183 by and , and so the finest optimal partition is @xmath190 as desired .",
    "note that we have allowed @xmath620 to be negative above @xmath640 .",
    "however , it turns out that , even if we allow @xmath634 to be negative , there is no choice of @xmath635 for which the complete clustering solutions in and are the same :    [ eg : mac ]     +   +    let @xmath366 and @xmath641 where @xmath390 are independent uniformly random bits .",
    "the dilworth truncation @xmath219 is plotted in fig .  [",
    "fig : mac:1 ] .",
    "our clustering solution consists of the psp : @xmath642 the solution respects the symmetry of the correlation in @xmath110 and the independence between @xmath22 and @xmath110 .    to compute the mac clustering in , rewrite with @xmath634 @xmath643 .",
    "\\end{aligned}\\ ] ] for @xmath636 and @xmath644 , the l.h.s .  and r.h.s .",
    "are plotted in fig .",
    "[ fig : mac:2 ] . since the curve for the l.h.s .",
    "intersect the curve on the r.h.s .  along the line segment @xmath645 $ ]",
    ", the partition @xmath646 is an optimal solution to the r.h.s .. if @xmath647 , then @xmath648 will intersect the line segment corresponding to @xmath649 $ ] instead of @xmath645 $ ] . therefore , in order to have @xmath646 to be the solution , we must have @xmath650 . for @xmath630 and @xmath644 , the plot in fig .",
    "[ fig : mac:3 ] shows that the optimal partition to the r.h.s .",
    "is not a partition in the psp .",
    "this is the case even for @xmath650 because increasing @xmath42 will only move the intersection point to the left further away from @xmath649 $ ] .",
    "the optimal partition , such as @xmath651 , does not appear to respect the symmetry in the correlation among @xmath110 .",
    "the authors would like to thank prof .",
    "raymond w.  yeung , the co - director of the institute of network coding ( inc ) at the chinese university of hong kong , for his generous support of our research on information theory ; prof .",
    "lav r.  varshney , prof .",
    "rosanna y - y .",
    "chan , and prof .",
    "chen change loy for their suggestions of relevant works in machine learning and neuroscience ; dr .  javad b. ebrahimi , dr .",
    "ravi k.  raman , and dr .",
    "ni ding for their helpful discussions ; prof .",
    "frank kschischang , prof .",
    "devavrat shah , and the colleagues at inc , whose comments have helped significantly improve the presentation of the paper .",
    "we would also like to thank the associate editor prof .",
    "peter thomas and the reviewers for their detailed reading and insightful comments .",
    "the first author would like to thank prof .",
    "imre csiszr for the discussion on the divergence upper bound for secret key agreement and the issue of tightness , and prof .",
    "prakash narayan for his recognitions of the contribution of this work .",
    "he would also like to thank his ph.d .",
    "advisor , prof .",
    "lizhong zheng , for leading him to the field of information theory .      c.  chan , a.  al - bashabsheh , j.  ebrahimi , t.  kaced , and t.  liu , `` multivariate mutual information inspired by secret - key agreement , '' _ proceedings of the ieee _ ,",
    "103 , no .",
    "10 , pp . 18831913 , oct 2015 .",
    "c.  chan and t.  liu , `` clustering of random variables by multivariate mutual information under chow - liu tree approximations , '' in _ fifty - third annual allerton conference on communication , control , and computing _ , allerton retreat center , monticello , illinois , pp .",
    "993999 , sep .",
    "2015 .",
    "g.  deco , g.  tononi , m.  boly , and m.  l. kringelbach , `` rethinking segregation and integration : contributions of whole - brain modelling , '' _ nature reviews neuroscience _",
    "16 , no .  7 , pp . 430439 , 2015 .",
    "g.  tononi , a.  r. mcintosh , d.  p. russell , and g.  m. edelman , `` functional clustering : identifying strongly interactive brain regions in neuroimaging data , '' _ neuroimage _ , vol .  7 , no .  2 , pp .",
    "133149 , 1998 .",
    "f.  a. azevedo , l.  r. carvalho , l.  t. grinberg , j.  m. farfel , r.  e. ferretti , r.  e. leite , r.  lent , s.  herculano - houzel _",
    "et  al . _",
    ", `` equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled - up primate brain , '' _ journal of comparative neurology _",
    "513 , no .  5 , pp . 532541 , 2009 .",
    "r.  d. morin , m.  bainbridge , a.  fejes , m.  hirst , m.  krzywinski , t.  j. pugh , h.  mcdonald , r.  varhol , s.  j. jones , and m.  a. marra , `` profiling the hela s3 transcriptome using randomly primed cdna and massively parallel short - read sequencing , '' _ biotechniques _ ,",
    "45 , no .  1 , p.  81",
    ", 2008 .      j.  white , e.  southgate , j.  thomson , and s.  brenner , `` the structure of the nervous system of the nematode caenorhabditis elegans : the mind of a worm , '' _ philos trans r soc lond b biol sci _ , vol .",
    "314 , pp . 1340 , 1986 .",
    "s.  w. emmons , `` the beginning of connectomics : a commentary on white et al .",
    "( 1986 )  ` the structure of the nervous system of the nematode caenorhabditis elegans ' , '' _ phil .",
    ". b. _ , vol .",
    "370 , no . 1666 , p. 20140309",
    ", 2015 .",
    "t.  a. jarrell , y.  wang , a.  e. bloniarz , c.  a. brittin , m.  xu , j.  n. thomson , d.  g. albertson , d.  h. hall , and s.  w. emmons , `` the connectome of a decision - making neural network , '' _ science _ , vol .",
    "6093 , pp . 437444 , 2012 .",
    "v.  j. wedeen , p.  hagmann , w .- y .",
    "i. tseng , t.  g. reese , and r.  m. weisskoff , `` mapping complex tissue architecture with diffusion spectrum magnetic resonance imaging , '' _ magnetic resonance in medicine _ , vol .",
    "54 , no .  6 , pp .",
    "13771386 , 2005 .",
    "v.  j. wedeen , r.  wang , j.  d. schmahmann , t.  benner , w.  tseng , g.  dai , d.  pandya , p.  hagmann , h.  darceuil , and a.  j. de  crespigny , `` diffusion spectrum magnetic resonance imaging ( dsi ) tractography of crossing fibers , '' _ neuroimage _ , vol .",
    "41 , no .  4 , pp .",
    "12671277 , 2008 .",
    "l.  of  neuro  imaging and m.  c. for biomedical imaging  at massachusetts general  hospital , `` human connectome project , '' 2009 , [ online ; accessed apr-2016 ] .",
    "[ online ] .",
    "available : http://www.humanconnectomeproject.org        t.  srlie , r.  tibshirani , j.  parker , t.  hastie , j.  marron , a.  nobel , s.  deng , h.  johnsen , r.  pesich , s.  geisler _ et  al .",
    "_ , `` repeated observation of breast tumor subtypes in independent gene expression data sets , '' _ proceedings of the national academy of sciences _ , vol .",
    "100 , no .",
    "14 , pp . 84188423 , 2003 .",
    "s.  nigam , m.  shimono , s.  ito , f .-",
    "yeh , n.  timme , m.  myroshnychenko , c.  c. lapish , z.  tosi , p.  hottowy , w.  c. smith _",
    "et  al . _ , `` rich - club organization in effective connectivity among cortical neurons , '' _ the journal of neuroscience _ , vol .",
    "36 , no .  3 , pp .",
    "670684 , 2016 .",
    "k.  he , x.  zhang , s.  ren , and j.  sun , `` deep residual learning for image recognition , '' _ arxiv preprint arxiv:1512.03385 _ , 2015 , presented at 2016 ieee conference on computer vision and pattern recognition .",
    "x.  pan , d.  papailiopoulos , s.  oymak , b.  recht , k.  ramchandran , and m.  i. jordan , `` parallel correlation clustering on big graphs , '' in _ advances in neural information processing systems _ , 2015 , pp .",
    "8290 .",
    "m.  aghagolzadeh , h.  soltanian - zadeh , b.  araabi , and a.  aghagolzadeh , `` a hierarchical clustering based on mutual information maximization , '' in _ 2007 ieee international conference on image processing _",
    ", vol .  1 ,",
    "sept 2007 , pp .",
    "277i  280 .",
    "k.  nagano , y.  kawahara , and s.  iwata , `` minimum average cost clustering . '' in _ nips _ , j.  d. lafferty , c.  k.  i. williams , j.  shawe - taylor , r.  s. zemel , and a.  culotta , eds.1em plus 0.5em minus 0.4emcurran associates , inc . , 2010 , pp . 17591767",
    ".    m.  mukherjee , c.  chan , n.  kashyap , and q.  zhou , `` bounds on the communication rate needed to achieve sk capacity in the hypergraphical source model , '' in _ proc . of ieee int .",
    "symp . on inf .",
    ", july 2016 , pp . 25042508 .",
    "m.  mukherjee , n.  kashyap , and y.  sankarasubramaniam , `` on the public communication needed to achieve sk capacity in the multiterminal source model , '' _ ieee trans .",
    "inf . theory _",
    "62 , no .  7 , pp .",
    "38113830 , july 2016 .",
    "c.  chan , a.  al - bashabsheh , q.  zhou , and t.  liu , `` duality between feature selection and data clustering , '' _ arxiv preprint arxiv:1609.08312 _ , 2016 , preliminary results published at the fifty - fourth annual allerton conference on communication , control , and computing , allerton retreat center , monticello , illinois .",
    "w.  liu , g.  xu , and b.  chen , `` the common information of @xmath81 dependent random variables , '' in _ forty - eighth annual allerton conference on communication , control , and computing _ , sept 2010 , pp .",
    "836843 .",
    "n.  milosavljevic , s.  pawar , s.  el  rouayheb , m.  gastpar , and k.  ramchandran , `` deterministic algorithm for the cooperative data exchange problem , '' in _ proc .",
    "of ieee int .",
    "symp . on inf .",
    "theory _ , jul .",
    "2011 .",
    "m.  b. eisen , p.  t. spellman , p.  o. brown , and d.  botstein , `` cluster analysis and display of genome - wide expression patterns , '' _ proceedings of the national academy of sciences _ , vol .",
    "95 , no .",
    "25 , pp . 1486314868 , 1998 .",
    "chung chan received the b.sc .",
    ", m.eng . and ph.d . from the eecs department at mit in 2004 , 2005 and 2010 respectively .",
    "he is currently a research assistant professor at the institute of network coding , the chinese university of hong kong .",
    "his research is in the area of information theory , with applications to network coding , multiple - terminal source coding and security problems that involve high - dimensional statistics .",
    "he is currently working on machine learning applications such as data clustering and feature selection .",
    "ali al - bashabsheh received a b.sc .",
    "( 2001 ) and an m.sc ( 2005 ) in electrical engineering from jordan university of science and technology , an m.sc .",
    "( 2012 ) in mathematics from carleton university , and a ph.d .",
    "( 2014 ) in electrical engineering from the university of ottawa . since april 2014",
    ", he has been a postdoctoral fellow at the institute of network coding at the chinese university of hong kong .",
    "his research interests include graphical models , coding theory , and information theory .",
    "qiaoqiao zhou received his b.b.a . in business administration and m.s . in electrical engineering from beijing university of post and telecommunication , china , in 2011 and 2014 , respectively . from 2014 to 2015 , he was a research assistant at the institute of network coding , the chinese university of hong kong .",
    "he is currently a ph.d .",
    "candidate at the department of information engineering , the chinese university of hong kong .",
    "his research interests include information - theoretic security and machine learning .",
    "tarik kaced was born in france , he received his b.sc . in fundamental computer science from cole normale suprieure",
    "de lyon in 2007 , and his m.sc . from universit",
    "de nice sophia - antipolis in 2009 .",
    "he completed his ph.d .",
    "degree in computer science in 2012 at universit de montpellier 2 in the escape team from lirmm .",
    "he has been a post - doctoral fellow at the institute of network coding at the chinese university of hong kong for two years .",
    "he was a post - doctoral fellow at in universit paris - est crteil at the algorithmic , complexity and logic laboratory .",
    "tie liu was born in jilin , china in 1976 .",
    "he received his b.s .",
    "( 1998 ) and m.s .",
    "( 2000 ) degrees , both in electrical engineering , from tsinghua university , beijing , china and a second m.s .",
    "degree in mathematics ( 2004 ) and ph.d .",
    "degree in electrical and computer engineering ( 2006 ) from the university of illinois at urbana - champaign .",
    "since august 2006 he has been with texas a&m university , where he is currently an associate professor with the department of electrical and computer engineering .",
    "his primary research interest is in the area of information theory and statistical information processing .",
    "liu received an m. e. van valkenburg graduate research award ( 2006 ) from the university of illinois at urbana - champaign and a faculty early career development ( career ) award ( 2009 ) from the national science foundation .",
    "he was a technical program committee co - chair for the 2008 ieee global communications conference ( globecom ) and a general co - chair for the 2011 ieee north american school of information theory .",
    "he currently serves as an associate editor for shannon theory for the ieee transactions on information theory ."
  ],
  "abstract_text": [
    "<S> we formulate an info - clustering paradigm based on a multivariate information measure , called multivariate mutual information , that naturally extends shannon s mutual information between two random variables to the multivariate case involving more than two random variables . with proper model reductions , </S>",
    "<S> we show that the paradigm can be applied to study the human genome and connectome in a more meaningful way than the conventional algorithmic approach . </S>",
    "<S> not only can info - clustering provide justifications and refinements to some existing techniques , but it also inspires new computationally feasible solutions .    </S>",
    "<S> genome , connectome , data clustering , multivariate mutual information , principal sequence of partitions </S>"
  ]
}