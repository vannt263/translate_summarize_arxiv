{
  "article_text": [
    "are widely applied in many areas of signal processing  @xcite , where their popularity owes largely to efficient algorithms on the one hand and advantages of sparse wavelet representations on the other .",
    "the sparseness property means that while the distribution of the original signal values may be very diffuse , the distribution of the corresponding wavelet coefficients is often highly concentrated , having a small number of very large values and a large majority of very small values  @xcite .",
    "it is easy to appreciate the importance of sparseness in signal compression ,  @xcite .",
    "the task of removing noise from signals , or _ denoising _ , has an intimate link to data compression , and many denoising methods are explicitly designed to take advantage of sparseness and compressibility in the wavelet domain , see e.g. ,  @xcite@xcite .    among the various wavelet - based denoising methods those suggested by donoho and johnstone  @xcite are",
    "the best known .",
    "they follow the frequentist minimax approach , where the objective is to asymptotically minimize the worst - case @xmath0 risk simultaneously for signals , for instance , in the entire scale of hlder , sobolev , or besov classes , characterized by certain smoothness conditions .",
    "by contrast , bayesian denoising methods minimize the _ expected _ ( bayes ) risk , where the expectation is taken over a given prior distribution supposed to govern the unknown true signal  @xcite .",
    "appropriate prior models with very good performance in typical benchmark tests , especially for images , include the class of generalized gaussian densities  @xcite , and scale - mixtures of gaussians  @xcite ( both of which include the gaussian and double exponential densities as special cases ) .",
    "a third approach to denoising is based on the minimum description length ( mdl ) principle  @xcite@xcite .",
    "several different mdl denoising methods have been suggested  @xcite@xcite .",
    "we focus on what we consider as the most pure mdl approach , namely that of rissanen  @xcite .",
    "our motivation is two - fold : first , as an immediate result of refining and extending the earlier mdl denoising method , we obtain a new practical method with greatly improved performance and robustness .",
    "secondly , the denoising problem turns out to illustrate theoretical issues related to the mdl principle , involving the problem of unbounded parametric complexity and the necessity of encoding the model class .",
    "the study of denoising gives new insight to these issues .",
    "formally , the denoising problem is the following .",
    "let @xmath1 be a signal represented by a real - valued column vector of length @xmath2 .",
    "the signal can be , for instance , a time - series or an image with its pixels read in a row - by - row order .",
    "let @xmath3 be an @xmath4 regressor matrix whose columns are basis vectors .",
    "we model the signal @xmath5 as a linear combination of the basis vectors , weighted by coefficient vector @xmath6 , plus gaussian i.i.d .",
    "noise : @xmath7 where @xmath8 is the noise variance . given an observed signal @xmath5",
    ", the ideal is to obtain a coefficient vector @xmath9 such that the signal given by the transform @xmath10 contains the informative part of the observed signal , and the difference @xmath11 is noise .    for technical convenience , we adopt the common restriction on @xmath3 that the basis vectors span a _ complete orthonormal _ basis .",
    "this implies that the number of basis vectors is equal to the length of the signal , @xmath12 , and that all the basis vectors are orthogonal unit vectors .",
    "there are a number of wavelet transforms that conform to this restriction , for instance , the haar transform and the family of daubechies transforms  @xcite .",
    "formally , the matrix @xmath3 is of size @xmath13 and orthogonal with its inverse equal to its transpose .",
    "also the mapping @xmath14 preserves the euclidean norm , and we have parseval s equality : @xmath15 geometrically this means that the mapping @xmath14 is a rotation and/or a reflection . from a statistical point of view",
    ", this implies that any spherically symmetric density , such as gaussian , is invariant under this mapping .",
    "all these properties are shared by the mapping @xmath16 .",
    "we call @xmath17 the inverse wavelet transform , and @xmath18 the forward wavelet transform",
    ". note that in practice the transforms are not implemented as matrix multiplications but by a fast wavelet transform similar to the fast fourier transform ( see  @xcite ) , and in fact not even the matrices need be written down .    for complete bases , the conventional maximum likelihood ( least squares )",
    "method obviously fails to provide denoising unless the coefficients are somehow restricted since the solution @xmath19 gives the reconstruction @xmath20 equal to the original signal , including noise . the solution proposed by rissanen  @xcite is to consider each subset of the basis vectors separately and to choose the subset that allows the shortest description of the data at hand .",
    "the length of the description is determined by the normalized maximum likelihood ( nml ) code length .",
    "the nml model involves an integral , which is undefined unless the range of integration ( the support ) is restricted .",
    "this , in turn , implies hyper parameters , which have received increasing attention in various contexts involving , e.g. , gaussian , poisson and geometric models  @xcite@xcite .",
    "rissanen used renormalization to remove them and to obtain a second - level nml model .",
    "although the range of integration has to be restricted also in the second - level nml model , the range for ordinary regression problems does not affect the resulting criterion and can be ignored .",
    "roos et al .",
    "@xcite give an interpretation of the method which avoids the renormalization procedure and at the same time gives a simplified view of the denoising process in terms of two gaussian distributions fitted to informative and non - informative coefficients , respectively . in this paper",
    "we carry this interpretation further and show that viewing the denoising problem as a clustering problem suggests several refinements and extensions to the original method .",
    "the rest of this paper is organized as follows . in sec .",
    "[ sec : mdl ] we reformulate the denoising problem as a task of clustering the wavelet coefficients in two or more sets with different distributions . in sec .",
    "[ sec : refined ] we propose three different modifications of rissanen s method , suggested by the clustering interpretation . in sec .",
    "[ sec : results ] the modifications are shown to significantly improve the performance of the method in denoising both artificial and natural signals .",
    "the conclusions are summarized in sec .",
    "[ sec : conclusions ] .",
    "we rederive the basic model  ( [ eq : model ] ) in such a way that there is no need for renormalization .",
    "this is achieved by inclusion of the coefficient vector @xmath21 in the model as a variable and by selection of a ( prior ) density for @xmath21 .",
    "while the resulting nml model will be equivalent to rissanen s renormalized solution , the new formulation is easier to interpret and directly suggests several refinements and extensions .",
    "consider a fixed subset @xmath22 of the coefficient indices .",
    "we model the coefficients @xmath23 for @xmath24 as independent outcomes from a gaussian distribution with variance @xmath25 . in the basic hard threshold version",
    "all @xmath23 for @xmath26 are forced to equal zero .",
    "thus the extended model is given by @xmath27 this way of modeling the coefficients is akin to the so called _ spike and slab _ model often used in bayesian variable selection  @xcite and applications to wavelet - based denoising  @xcite ( and references therein ) . in relation to the sparseness property mentioned in the introduction , the ` spike ' consists of coefficients with @xmath26 that are equal to zero , while the ` slab ' consists of coefficients with @xmath28 described by a gaussian density with mean zero",
    "this is a simple form of a scale - mixture of gaussians with two components . in sec .",
    "[ sec : subband ] we will consider a model with more than two components .",
    "let @xmath29 , where @xmath30 gives the representation of the noise in the wavelet domain .",
    "the vector @xmath31 is the wavelet representation of the signal @xmath5 , and we have @xmath32 it is easy to see that the maximum likelihood parameters are obtained directly from @xmath33 the i.i.d .",
    "gaussian distribution for @xmath34 in ( [ eq : extendedmodel ] ) implies that the distribution of @xmath30 is also i.i.d . and",
    "gaussian with the same variance , @xmath8 . as a sum of two independent random variates",
    ", each @xmath35 has a distribution given by the convolution of the densities of the summands , @xmath23 and the @xmath36th component of @xmath30 . in the case",
    "@xmath37 this is simply @xmath38 . in the case @xmath24 the density of the sum is also gaussian , with variance given by the sum of the variances , @xmath39 .",
    "all told , we have the following simplified representation of the extended model where the parameters @xmath40 are implicit : @xmath41 where @xmath42 denotes the variance of the informative coefficients , and we have the important restriction @xmath43 which we will discuss more below .",
    "the task of choosing a subset @xmath44 can now be seen as a clustering problem : each wavelet coefficient belongs either to the set of the informative coefficients with variance @xmath45 , or the set of non - informative coefficients with variance @xmath8 .",
    "the mdl principle gives a natural clustering criterion by minimization of the code - length achieved for the observed signal ( see  @xcite ) .",
    "once the optimal subset is identified , the denoised signal is obtained by setting the wavelet coefficients to their maximum likelihood values  ; i.e. , retaining the coefficients in @xmath44 and discarding the rest , and doing the inverse transformation .",
    "it is well known that this amounts to an orthogonal projection of the signal to the subspace spanned by the wavelet basis vectors in @xmath44 .",
    "the code length under the model   depends on the values of the two parameters , @xmath45 and @xmath8 .",
    "the standard solution in such a case is to construct a single representative model for the whole model class such that the representative model is universal ( can mimic any of the densities in the represented model class ) .",
    "the minimax optimal universal model ( see  @xcite ) is given by the so called normalized maximum likelihood ( nml ) model , originally proposed by shtarkov  @xcite for data compression .",
    "we now consider the nml model corresponding to the extended model   with the index set @xmath44 fixed",
    ".    denote by @xmath46 the number of coefficients for which @xmath47 .",
    "the nml density under the extended model   for a given coefficient subset @xmath44 is defined as @xmath48 where @xmath49 and @xmath50 are the maximum likelihood parameters for the data @xmath5 , and @xmath51 is the important normalizing constant .",
    "the constant @xmath51 is also known as the _ parametric complexity _ of the model class defined by @xmath44 .    restricting the data such that the maximum likelihood parameters satisfy @xmath52 and ignoring the constraint @xmath53 , the code length under the extended model   is approximated by bits . ]",
    "@xmath54 plus a constant independent of @xmath44 , with @xmath55 and @xmath56 denoting the sum of the squares of all the wavelet coefficients and the coefficients for which @xmath28 , respectively ( see the appendix for a proof ) .",
    "the code length formula is very accurate even for small @xmath2 since it involves only the stirling approximation of the gamma function .",
    "the set of sequences satisfying the restriction @xmath57 depends on @xmath44 .",
    "for instance , consider the case @xmath58 . in a model with @xmath59",
    ", the restriction corresponds to a union of four squares , whereas in a model with either @xmath60 or @xmath61 , the relevant area is an annulus ( two - dimensional spherical shell ) .",
    "however , the restriction can be understood as a definition of the support of the corresponding nml model , not a rigid restriction on the data , and hence models with varying @xmath44 are still comparable as long as the maximum likelihood parameters for the observed sequence satisfy the restriction .",
    "the code length obtained is identical to that derived by rissanen with renormalization  @xcite ( note the correction to the third term of   in  @xcite ) .",
    "the formula has a concise and suggestive form that originally lead to the interpretation in terms of two gaussian densities  @xcite .",
    "it is also the form that has been used in subsequent experimental work with somewhat mixed conclusions  @xcite : while for gaussian low variance noise it gives better results than a universal threshold of donoho and johnstone  @xcite ( visushrink ) , over - fitting occurs in noisy cases  @xcite ( see also sec .",
    "[ sec : results ] below ) , which is explained by the fact that omission of the third term is justified only in regression problems with few parameters .",
    "it was proved in  @xcite that the criterion   is minimized by a subset @xmath44 which consists of some number @xmath62 of the largest or smallest wavelet coefficients in absolute value .",
    "it was also felt that in denoising applications the data are such that the largest coefficients will minimize the criterion .",
    "the above alternative formulation gives a natural solution to this question : by the inequality @xmath63 , the set of coefficients with larger variance , i.e. , the one with larger absolute values should be retained , rather than _ vice versa_.    in reality the nml model corresponding to the extended model  ( [ eq : simplermodel ] ) is identical to rissanen s renormalized model only if the inequality @xmath64 is ignored in the calculations ( see the appendix ) .",
    "however , the following proposition ( proved in the appendix ) shows that the effect of doing so is independent of @xmath62 , and hence irrelevant .",
    "[ prop : ignore ] the effect of ignoring the constraint @xmath53 is exactly one bit .",
    "we can safely ignore the constraint and use the model without the constraint as a starting point for further developments for the sake of mathematical convenience .",
    "it is customary to ignore encoding of the index of the model class in mdl model selection ; i.e. , encoding the number of parameters when the class is in one - to - one correspondence with the number of parameters .",
    "one simply picks the class that enables the shortest description of the data without considering the number of bits needed to encode the class itself .",
    "note that here we do not refer to encoding the parameter values as in two - part codes , which are done implicitly in the so - called ` one - part codes ' such as the nml and mixture codes . in most cases",
    "there are not too many classes and hence omitting the code length of the model index has no practical consequence .",
    "when the number of model classes is large , however , this issue does become of importance . in the case of denoising ,",
    "the number of different model classes is as large as @xmath65 ( with @xmath2 as large as @xmath66 ) and , as we show , encoding of the class index is crucial .",
    "the encoding method we adopt for the class index is simple .",
    "we first encode @xmath62 , the number of retained coefficients with a uniform code , which is possible since the maximal number @xmath2 is fixed .",
    "this part of the code can be ignored since it only adds a constant to all code lengths .",
    "secondly , for each @xmath62 there are a number of different model classes depending on which @xmath62 coefficients are retained .",
    "note that while the retained coefficients are always the _ largest _ @xmath62 coefficients , this information is not available to the decoder at this point and the index set to be retained has to be encoded .",
    "there are @xmath67 sets of size @xmath62 , and we use a uniform code yielding a code length @xmath68 nats , corresponding to a prior probability @xmath69    applying stirling s approximation to the factorials and ignoring all constants wrt .",
    "@xmath44 gives the final code length formula @xmath70 the proof can be found in the appendix .",
    "this way of encoding the class index is by no means the only possibility but it will be seen to work sufficiently well , except for one curious limitation : as a consequence of modeling both the informative coefficients and the noise by densities from the same gaussian model , the code length formula approaches the same value as @xmath62 approaches either zero or @xmath2 , which actually are disallowed .",
    "hence , it may be that in cases where there is little information to recover , the random fluctuations in the data may yield a minimizing solution near @xmath71 instead of a correct solution near @xmath60 .",
    "a similar phenomenon has been demonstrated for `` saturated '' bernoulli models with one parameter for each observation  @xcite , and resembles the inconsistency problem of bic in markov chain order selection  @xcite : in all these cases pure random noise is incorrectly identified as maximally regular data . in order to prevent this we simply restrict @xmath72 , which seems to avoid such problems .",
    "a general explanation and solution for these phenomena would be of interest terms in algorithmic information theory . ] .",
    "it is an empirical fact that for most natural signals the coefficients on different subbands corresponding to different frequencies ( and orientations in 2d data ) have different characteristics . basically , the finer the level , the more sparse the distribution of the coefficients , see fig .  [",
    "fig : histo ] .",
    "( this is not the case for pure gaussian noise or , more interestingly , signals with fractal structure  @xcite . ) within the levels the histograms of the subbands for different orientations of 2d transforms typically differ somewhat , but the differences between orientations are not as significant as between levels .",
    "finer levels have narrower ( more sparse ) distributions than coarser levels ; the finest level ( 9 ) is drawn with solid line . ]    in order to take the subband structure of wavelet transforms into account , we let each subband @xmath73 have its own variance , @xmath74 . we choose the set of the retained coefficients separately on each subband , and let @xmath75 denote the set of the retained coefficients on subband @xmath76 , with @xmath77 . for convenience ,",
    "let @xmath78 be the set of all the coefficients that are not retained . note that this way we have @xmath79 . in order to encode the retained and the discarded coefficients on each subband",
    ", we use a similar code as in the ` flat ' case ( sec .",
    "[ sec : encodemodel ] ) . for each subband @xmath80 , the number of nats needed is @xmath81 . ignoring again the constraint @xmath82 ,",
    "the levels can be treated as separate sets of coefficients with their own gaussian densities just as in the previous subsection , where we had two such sets .",
    "the code length function , including the code length for @xmath44 , becomes after stirling s approximation to the gamma function and ignoring constants as follows : @xmath83 the proof is omitted since it is entirely analogous to the proof of eq .",
    "( see the appendix ) , the only difference being that now we have @xmath84 gaussian densities instead of only two .",
    "notwithstanding the added code - length for the retained indices , for the case @xmath85 this coincides with the original setting , where the subband structure is ignored , eq .  , since we then have @xmath86 .",
    "this code can be extended to allow @xmath87 for some subbands simply by ignoring such subbands , which formally corresponds to reducing @xmath88 in such cases the constants ignored also get reduced .",
    "this effect is very small compared to terms in  , and can be safely ignored since codes with positive constants added to the code lengths are always decodable . ] .    finding the index sets @xmath75 that minimize the nml code length simultaneously for all subbands @xmath76 is computationally demanding .",
    "while on each subband the best choice always includes some @xmath89 largest coefficients , the optimal choice on subband @xmath76 depends on the choices made on the @xmath90 other subbands .",
    "a reasonable approximate solution to the search problem is obtained by iteration through the subbands and , on each iteration , finding the locally optimal coefficient set on each subband , given the current solution on the other subbands .",
    "since the total code length achieved by the current solution never increases , the algorithm eventually converges , typically after not more than five iterations .",
    "algorithm  1 in fig .",
    "[ fig : al1 ] implements the above described method . following established practice  @xcite ,",
    "all coefficients are retained on the smallest ( coarsest ) subbands .",
    "ll +   + 0 . &    ' '' ''    set @xmath91 + 1 .",
    "& initialize @xmath92 for all @xmath93 + 2 . & do until convergence + 3 .",
    "& for each @xmath94 + 4 . & optimize @xmath89 wrt .",
    "criterion   + 5 . &",
    "end + 6 . & end + 7 .",
    "& for each @xmath95 + 8 .",
    "& if @xmath26 then set @xmath96 + 9 . &",
    "end + 10 . & output @xmath97      the methods described above can be used to determine the mdl model , defined by a subset @xmath44 of the wavelet coefficients , that gives the shortest description to the observed data . however , in many cases there are several models that achieve nearly as good a compression as the best one .",
    "intuitively , it seems then too strict to choose the single best model and discard all the others .",
    "a modification of the procedure is to consider a _ mixture _ , where all models indexed by @xmath44 are weighted by eq .  :",
    "@xmath98 such a mixture model is universal ( see e.g.  @xcite ) in the sense that with increasing sample size the per sample average of the code length @xmath99 approaches that of the best @xmath44 for all @xmath5 .",
    "consequently , predictions obtained by conditioning on past observations converge to the optimal ones achievable with the chosen model class .",
    "a similar approach with mixtures of trees has been applied in the context of compression  @xcite .    for denoising purposes",
    "we need a slightly different setting since we can not let @xmath2 grow . instead , given an observed signal @xmath5 , consider another image @xmath100 from the same source .",
    "denoising is now equivalent to predicting the mean value of @xmath100 . obtaining predictions for @xmath100 given @xmath5 from the mixture",
    "is in principle easy : one only needs to evaluate a conditional mixture @xmath101 with new updated ` posterior ' weights for the models , obtained by multiplying the nml density by the prior weights and normalizing wrt .",
    "@xmath44 : @xmath102 since in the denoising problem we only need the mean value instead of a full predictive distribution for the coefficients , we can obtain the predicted mean as a weighted average of the predicted means corresponding to each @xmath44 by replacing the density @xmath103 by the coefficient value @xmath104 obtained from @xmath5 for @xmath28 and zero otherwise , which gives the denoised coefficients @xmath105 where the indicator function @xmath106 takes value one if @xmath28 and zero otherwise .",
    "thus the mixture prediction of the coefficient value is simply @xmath35 times the sum of the weights of the models where @xmath24 with the weights given by eq .  .",
    "the practical problem that arises in such a mixture model is that summing over all the @xmath65 models is intractable .",
    "since this sum appears as the denominator of  , we can not evaluate the required weights .",
    "we now derive a tractable approximation . to this end , let @xmath107 denote a model determined by @xmath28 iff @xmath108 , and let @xmath109 denote a particular one with @xmath108 .",
    "also , let @xmath110 be the model with maximal nml posterior weight  .",
    "the weight with which each individual coefficient contributes to the mixture prediction can be obtained from @xmath111 note that the ratio @xmath112 is equal to @xmath113 this can be approximated by @xmath114 which means that the exponential sums in the numerator and the denominator are replaced by their largest terms assuming that forcing @xmath115 to be one or zero has no effect on the other components of @xmath116 .",
    "the ratio of two weights can be evaluated without knowing their common denominator , and hence this gives an efficient recipe for approximating the weights needed in eq .  . intuitively , if fixing @xmath117 decreases the posterior weight significantly compared to @xmath108 , the approximated value of @xmath112 becomes large and the @xmath118th coefficient is retained near its maximum likelihood value @xmath35 .",
    "conversely , coefficients that increase the code length when included in the model are shrunk towards zero .",
    "thus , the mixing procedure implements a general form of ` soft ' thresholding , of which a restricted piece - wise linear form has been found in many cases superior to hard thresholding in earlier work  @xcite .",
    "such soft thresholding rules have been justified in earlier works by their improved theoretical and empirical properties , while here they arise naturally from a universal mixture code .",
    "the whole procedure for mixing different coefficient subsets can be implemented by replacing step 8 of algorithm  1 in fig .",
    "[ fig : al1 ] by the instruction @xmath119 where @xmath120 denotes the approximated value of @xmath112 .",
    "the behavior of the resulting soft threshold is illustrated in fig .",
    "[ fig : thres ] .    ): the original wavelet coefficient value @xmath35 on the x - axis , and the thresholded value @xmath121 on the y - axis . for coefficients with large absolute value ,",
    "the curve approaches the diagonal ( dotted line ) .",
    "the general shape of the curve is always the same but the scale depends on the data : the more noise , the wider the non - linear part . ]",
    "the effect of the three refinements of the mdl denoising method was assessed separately and together on a set of artificial 1d signals  @xcite and natural images commonly used for benchmarking .",
    "the signals were contaminated with gaussian pseudo - random noise of known variance @xmath122 , and the denoised signal was compared with the original signal .",
    "the daubechies d6 wavelet basis was used in all experiments , both in the 1d and 2d cases .",
    "the error was measured by the peak - signal - to - noise ratio ( psnr ) , defined as @xmath123 where @xmath124 is the difference between the maximum and minimum values of the signal ( for images @xmath125 ) ; and @xmath126 is the mean squared error .",
    "the experiment was repeated 15 times for each value of @xmath122 , and the mean value and standard deviation was recorded .",
    "the compared denoising methods were the original mdl method  @xcite without modifications ; mdl with the modification of sec .",
    "[ sec : encodemodel ] ; mdl with the modifications of secs .",
    "[ sec : encodemodel ] and  [ sec : subband ] ; and mdl with the modifications of secs .",
    "[ sec : encodemodel ] , [ sec : subband ] and [ sec : mixture ] . for comparison",
    ", we also give results for three general denoising methods applicable to both 1d and 2d signals , namely visushrink  @xcite , sureshrink  @xcite , and bayesshrink  @xcite can be reproduced using the package . ] .",
    "figure  [ fig : blockdemo ] illustrates the denoising results for the _ blocks _ signal  @xcite with signal length @xmath127 .",
    "the original signal , shown in the top - left display , is piece - wise constant .",
    "the standard deviation of the noise is @xmath128 .",
    "the best method , having the highest @xmath129 ( and equivalently , the smallest @xmath126 ) is the mdl method with all the modifications proposed in the present work , labeled mdl ( a - b - c ) in the figure .",
    "another case , the _ peppers _ image with noise standard deviation @xmath130 , is shown in fig .",
    "[ fig : pepperdemo ] , where the best method is bayesshrink .",
    "visually , sureshrink and bayesshrink give a similar result with some remainder noise left , while mdl ( a - b - c ) has removed almost all noise but suffers from some blurring .",
    "the relative performance of the methods depends strongly on the noise level .",
    "figure  [ fig : curves ] illustrates this dependency in terms of the relative psnr compared to the mdl ( a - b - c ) method .",
    "it can be seen that the mdl ( a - b - c ) is uniformly the best among the four mdl methods except for a range of small noise levels in the _ peppers _ case , where the original method  @xcite is slightly better .",
    "moreover , it can be seen that the modifications of secs .",
    "[ sec : subband ] and  [ sec : mixture ] improve the performance on all noise levels for both signals .",
    "the right panels of fig .",
    "[ fig : curves ] show that the overall best method is bayesshrink , except for small noise levels in _ blocks _ , where the mdl ( a - b - c ) method is the best .",
    "this is explained by the fact that the generalized gaussian model used in bayesshrink is especially apt for natural images but less so for 1d signals of the kind used in the experiments .",
    "the above observations generalize to other 1d signals and images as well , as shown by tables  [ tab:1d ] and  [ tab:2d ] .",
    "for some 1d signals ( _ heavisine _ , _ doppler _ ) the sureshrink method is best for some noise levels . in images , bayesshrink is consistently superior for low noise cases , although it can be debated whether the test setting where the denoised image is compared to the original image , which in itself already contains some noise , gives meaningful results in the low noise regime . for moderate to",
    "high noise levels , bayesshrink , mdl ( a - b - c ) and sureshrink typically give similar psnr output .",
    "+      +         [ cols=\"^,^,^ \" , ]     rccccccccc & ( rissanen , 2000 ) & mdl ( a ) & mdl ( a - b ) & mdl ( a - b - c ) & visushrink & sureshrink & bayesshrink & & sd +   + @xmath131 & 39.1 & 36.6 & 38.5 & 39.3 & 37.3 & 43.2 & * 46.9 & @xmath132 &  + 10 & 31.6 & 30.8 & 31.8 & 32.4 & 30.1 & 32.8 & * 33.1 & @xmath132 & 0.02 + 20 & 25.0 & 27.8 & 28.8 & 29.4 & 27.1 & 29.5 & * 29.9 & @xmath132 & 0.03 + 30 & 19.8 & 26.0 & 27.1 & 27.6 & 25.4 & 27.8 & * 28.2 & @xmath132 & 0.03 + 40 & 16.7 & 24.9 & 26.0 & 26.5 & 24.3 & 26.4 & * 27.0 & @xmath132 & 0.04 +   +   + @xmath131 & 36.2 & 33.2 & 35.1 & 35.9 & 32.9 & 39.2 & * 40.3 & @xmath132 &  + 10 & 30.2 & 28.6 & 29.8 & 30.5 & 28.0 & 31.3 & * 31.7 & @xmath132 & 0.02 + 20 & 24.2 & 25.8 & 26.8 & 27.5 & 25.2 & 27.9 & * 28.3 & @xmath132 & 0.03 + 30 & 19.6 & 24.3 & 25.2 & 25.8 & 23.7 & 26.1 & * 26.5 & @xmath132 & 0.02 + 40 & 16.6 & 23.2 & 24.2 & 24.7 & 22.8 & 24.9 & * 25.3 & @xmath132 & 0.03 +   +   + @xmath131 & 41.4 & 36.7 & 42.5 & 43.5 & 41.0 & 47.4 & * 54.2 & @xmath132 &  + 10 & 31.4 & 30.7 & 31.5 & 32.1 & 30.2 & 32.5 & * 32.8 & @xmath132 & 0.06 + 20 & 24.7 & 27.3 & 28.1 & 28.7 & 26.8 & 28.7 & * 29.2 & @xmath132 & 0.05 + 30 & 19.7 & 25.4 & 26.4 & 27.0 & 24.9 & 26.9 & * 27.4 & @xmath132 & 0.06 + 40 & 16.7 & 24.2 & 25.2 & 25.7 & 23.7 & 25.4 & * 26.2 & @xmath132 & 0.07 +   +   + @xmath131 & 38.9 & 36.1 & 37.9 & 38.7 & 36.9 & 42.7 & * 51.2 & @xmath132 &  + 10 & 30.7 & 29.3 & 30.3 & 31.0 & 28.6 & * 31.5 & * 31.5 & @xmath132 & 0.04 + 20 & 24.7 & 25.9 & 26.9 & 27.6 & 25.1 & 27.1 & * 27.9 & @xmath132 & 0.05 + 30 & 19.9 & 23.9 & 24.9 & 25.5 & 23.1 & 24.6 & * 25.9 & @xmath132 & 0.05 + 40 & 16.8 & 22.4 & 23.3 & 23.9 & 21.6 & 22.8 & * 24.4 & @xmath132 & 0.08 + * * * * * * * * * * * * * * * * * * * * *",
    "we have revisited an earlier mdl method for wavelet - based denoising for signals with additive gaussian white noise . in doing",
    "so we gave an alternative interpretation of rissanen s renormalization technique for avoiding the problem of unbounded parametric complexity in normalized maximum likelihood ( nml ) codes .",
    "this new interpretation suggested three refinements to the basic mdl method which were shown to significantly improve empirical performance .",
    "the most significant contributions are : i ) an approach involving what we called the _ extended model _ , to the problem of unbounded parametric complexity which may be useful not only in the gaussian model but , for instance , in the poisson and geometric families of distributions with suitable prior densities for the parameters ; ii ) a demonstration of the importance of encoding the model index when the number of potential models is large ; iii ) a combination of universal models of the mixture and nml types , and a related predictive technique which should also be useful in mdl denoising methods ( e.g.  @xcite ) that are based on finding a single best model , and other predictive tasks .",
    "_ proof of eq .  : _ the proof of eq .",
    "is technically similar to the derivation of the _ renormalized _ nml model in  @xcite , which goes back to  @xcite .",
    "first note that due to orthonormality , the density of @xmath5 under the extended model is always equal to the density of @xmath31 evaluated at @xmath133 .",
    "thus , for instance , the maximum likelihood parameters for data @xmath5 are easily obtained by maximizing the density of @xmath31 at @xmath134 .",
    "the density of @xmath31 is given by @xmath135 where @xmath136 denotes a gaussian density function with mean @xmath137 and variance @xmath122 .",
    "let @xmath56 be the sum of squares of the wavelet coefficients with @xmath28 : @xmath138 and let @xmath55 denote the sum of all wavelet coefficients . with slight abuse of notation , we also denote these two by @xmath139 and @xmath140 , respectively .",
    "let @xmath62 be the size of the set @xmath44 .",
    "the likelihood is maximized by parameters given by @xmath141 with the maximum likelihood parameters ( [ eq : mlpar ] ) the likelihood ( [ eq : lik ] ) becomes @xmath142 the normalization constant @xmath143 is also easier to evaluate by integrating the likelihood in terms of @xmath31 : @xmath144 where @xmath145 is given by @xmath146 and the range of integration @xmath147 is defined by requiring that the maximum likelihood estimators   are both within the interval @xmath148 $ ] .",
    "it will be seen that the integral diverges without these bounds . the integral factors in two parts involving only the coefficients with @xmath47 and @xmath26 respectively .",
    "furthermore , the resulting two integrals depend on the coefficients only through the values @xmath139 and @xmath149 , and thus , they can be expressed in terms of these two quantities as the integration variables  we denote them respectively by @xmath150 and @xmath151 .",
    "the associated riemannian volume elements are infinitesimally thin spherical shells ( surfaces of balls ) ; the first one with dimension @xmath62 and radius @xmath152 , the second one with dimension @xmath153 and radius @xmath154 , given by @xmath155 + thus the integral in   is equivalent to @xmath156 both integrands become simply of the form @xmath157 and hence , the value of the integral is given by @xmath158 plugging ( [ eq : integral ] ) into ( [ eq : rawnorm ] ) gives the value of the normalization constant @xmath159 normalizing the numerator ( [ eq : numerator ] ) by @xmath143 , and canceling like terms finally gives the nml density : @xmath160 and the corresponding code length becomes @xmath161 applying stirling s approximation @xmath162 to the gamma functions yields now @xmath163 rearranging the terms gives the formula @xmath164 where @xmath165 is a constant wrt .",
    "@xmath44 , given by @xmath166    _ proof of proposition  [ prop : ignore ] : _ the maximum likelihood parameters   may violate the restriction @xmath43 that arises from the definition @xmath42 .",
    "the restriction affects range of integration in eq .",
    "giving the non - constant terms as follows @xmath167 using the integral @xmath168 gives then @xmath169 where the first two terms can be written as @xmath170 combining with the third term of   changes the plus into a minus and gives finally @xmath171 which is exactly half of the integral in eq .",
    ", the constant terms being the same .",
    "thus , the effect of the restriction on the code length where the _ logarithm _ of the integral is taken , is one bit , i.e. , @xmath172 nats .",
    "_ proof of eq .",
    "[ eq : withchoose ] : _ the relevant terms in the code length @xmath68 , i.e.  those depending on @xmath62 , for the index of the model class are @xmath173 \\\\     & = -\\ln ( k(n - k ) ) - \\ln \\gamma(k ) -",
    "\\ln \\gamma(n - k ) , \\end{aligned}\\ ] ] which gives after stirling s approximation ( ignoring constant terms ) @xmath174 adding this to eq .",
    "[ eq : onelevel ] ( without the constant @xmath2 ) gives eq .  .",
    "the authors thank peter grnwald , steven de rooij , jukka heikkonen , vibhor kumar , and hannes wettig for valuable comments .",
    "moulin and j.  liu , `` analysis of multiresolution image denoising schemes using generalized gaussian and complexity priors , '' _ ieee trans .",
    "information theory _ , vol .",
    "45 , no .  3 , pp .",
    "909919 , apr .",
    "m.  wainwright and e.  simoncelli , `` scale mixtures of gaussians and the statistics of natural images , '' in _ advances in neural information processing systems _ , s.  solla , t.  leen , and k",
    "muller , eds .",
    "12.1em plus 0.5em minus 0.4emmit press , may 2000 , pp .",
    "855861 .",
    "j.  portilla , v.  strela , m.  wainwright , and e.  simoncelli , `` image denoising using scale mixtures of gaussians in the wavelet domain , '' _ ieee trans .",
    "image processing _ , vol .",
    "12 , no .  11 , pp . 13381351 , nov .",
    "2003 .",
    "p.  grnwald , `` a tutorial introduction to the minimum description length principle , '' in _ advances in mdl : theory and applications _ , p.  grnwald , i.  myung , and m.  pitt , eds.1em plus 0.5em minus 0.4emmit press , 2005 .",
    "n.  saito , `` simultaneous noise suppression and signal compression using a library of orthonormal bases and the minimum description length criterion , '' in _ wavelets in geophysics_.1em plus 0.5em minus 0.4em academic press , 1994 , pp .",
    "299324 .",
    "f.  liang and a.  barron , `` exact minimax strategies for predictive density estimation , data compression , and model selection , '' _ ieee trans .",
    "information theory _ ,",
    "50 , no .  11 , pp . 27082726 , nov .",
    "2004 .",
    "t.  roos , p.  myllymki , and h.  tirri , `` on the behavior of mdl denoising , '' in _ proc .",
    "tenth int .",
    "workshop on ai and stat .",
    "_ , r.  g. cowell and z.  ghahramani , eds.1em plus 0.5em minus 0.4emsociety for ai and statistics , 2005 , pp . 309316 .",
    "p.  kontkanen , p.  myllymki , w.  buntine , j.  rissanen , and h.  tirri , `` an mdl framework for data clustering , '' in _ advances in mdl : theory and applications _ , p.  grnwald , i.  myung , and m.  pitt , eds.1em plus 0.5em minus 0.4emmit press , 2005 .",
    "j.  ojanen , t.  miettinen , j.  heikkonen , and j.  rissanen , `` robust denoising of electrophoresis and mass spectrometry signals with minimum description length principle , '' _ febs letters _ , vol .",
    "13 , pp . 107113 , 2004 ."
  ],
  "abstract_text": [
    "<S> we refine and extend an earlier mdl denoising criterion for wavelet - based denoising . </S>",
    "<S> we start by showing that the denoising problem can be reformulated as a clustering problem , where the goal is to obtain separate clusters for informative and non - informative wavelet coefficients , respectively . </S>",
    "<S> this suggests two refinements , adding a code - length for the model index , and extending the model in order to account for subband - dependent coefficient distributions . </S>",
    "<S> a third refinement is derivation of soft thresholding inspired by predictive universal coding with weighted mixtures . we propose a practical method incorporating all three refinements , which is shown to achieve good performance and robustness in denoising both artificial and natural signals .    </S>",
    "<S> minimum description length ( mdl ) principle , wavelets , denoising . </S>"
  ]
}