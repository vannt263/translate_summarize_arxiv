{
  "article_text": [
    "a key approach to obtaining lower bounds on probabilities of rare events is based on the idea of a change of measure . in this approach ,",
    "the underlying probability measure is replaced by a reference probability measure under which the probability of the event in question does not decay exponentially , and the exponent of the bound is given by the kullback ",
    "leibler ( kl ) divergence between the two probability measures .",
    "one then optimizes the estimate over all reference measures having the property alluded to above .",
    "this idea is standard for deriving lower bounds in large deviations theory ( see , e.g. , @xcite ) , where it is sometimes referred to as tilting . in the context of information",
    "theory it has been used in applications including ( i ) the derivation of the sphere  packing bound for discrete memoryless channels ( dmc s ) , using csiszr and krner s method ( * ? ? ?",
    "* theorem 5.3 ) ; ( ii ) marton s converse theorem on the source coding exponent @xcite . in the former ,",
    "the resulting exponential error bound is tight at least in some range of high coding rates . in the latter , it is virtually always tight ( for finite  alphabet memoryless sources ) , as there exists a matching upper bound .    in @xcite , atar , chowdhary and dupuis presented what may be viewed as an extension of this approach to situations where the probability of the event of interest may also decay exponentially under the reference measure .",
    "the estimate is then given in terms of the corresponding rnyi divergence . at the heart of the approach lies the _ logarithmic probability comparison bounds _ ( lpcb ) that compare the probability of an event under two measures at a logarithmic scale in terms of the respective rnyi divergence .",
    "specifically , if @xmath0 and @xmath1 are probability measures on a measurable space and @xmath2 is an event on it then @xmath3 for @xmath4 , where @xmath5 denotes rnyi divergence of order @xmath6 ( see definition and details in section [ sec2 ] ) .",
    "this bound is tight in the sense that , given @xmath0 and @xmath2 , one can find @xmath1 for which it holds as equality . thus if @xmath7 and @xmath8 are sequences of probability measures and we denote by @xmath9 the exponential decay rate of the probability under @xmath10 and by @xmath11 that under @xmath12 , then with @xmath13 , one obtains @xmath14 this gives a lower bound on the decay rate @xmath15 under a sequence of measures of interest in terms of that under reference measures , @xmath11 . by switching the roles of @xmath7 and @xmath8 one obtains an analogous upper bound .",
    "one natural use of is when @xmath1 is a model for which we have information on the decay rate ( exactly or as a bound ) , whereas @xmath0 is harder to analyze . in this case",
    ", a key step is to provide a useful estimate of the divergence term @xmath16 .",
    "another way to view is as what is often called a _ robust bound _",
    ", where one attempts to obtain performance bounds on a whole family of true models @xmath0 , and @xmath1 serves in defining this family . for example , the family of true models might consist of all @xmath0 for which the divergence from @xmath1 does not exceed a certain bound , in the sense that @xmath17 , some @xmath18 .",
    "then it is immediate from that for all @xmath0 in the family , @xmath19 while the latter has been the main motivation in @xcite , both viewpoints will be addressed in this paper .",
    "some benefits of the approach include : ( i ) the ability to compare , not only probabilities of a given event , but also expectations of a given function under the two ( sequences of ) measures ( this relies on a more general inequality than ; see section [ sec2 ] ) , ( ii ) the presence of the free parameter @xmath6 , that can be optimized over in order to tighten the bound , and ( iii ) the possibility to derive both upper and lower bounds by the same method .    the objective of this paper is to present the lpcb and the aforementioned method to the information theory audience and to demonstrate its power and usefulness as a tool for deriving upper and lower bounds in a variety of applications , including both source coding and channel coding scenarios . because it compares two probability measures , the bound is especially natural to apply in situations of mismatch between the true underlying model and the one to which the coding ",
    "decoding schemes are tailored .",
    "also , as will be seen in the sequel , in most of these applications , the setting is sufficiently general that no alternative bounds are available to the best knowledge of the authors , such as , for example , coding for channels with additive interference of unlimited memory and mismatch . in some of these scenarios ,",
    "the exponential bounds obtained are tight in the sense that they are attained at least for some instance of the problem .",
    "our main contributions are summarized as follows .    * highlighting the relevance of the approach to information theory ; * developing general upper and lower bounds on channel coding error exponents for the matched , mismatched and robust settings based on the lpcb ; * using the approach to derive new bounds on error exponents for a host of particular channel models including gaussian channels with long memory interference , the inter - symbol interference channel , the fading channel , and the binary erasure channel ; * obtaining new bounds for source coding and the problem of guessing .",
    "the outline of the remaining part of this paper is as follows . in section 2",
    ", we present the lpcb . in section 3 , we explain its use in estimating probabilities of rare events",
    ". we also present a corollary regarding small perturbations between reference and true models .",
    "section 4 is devoted to the channel coding framework .",
    "finally , section 5 provides further application examples .",
    "_ notation .",
    "_ a vector ( deterministic or random ) of the form @xmath20 will be written as @xmath21 .",
    "when the dimension @xmath22 is understood from the context , the vector will sometimes be written as the corresponding bold font letter , @xmath23 .",
    "the probability law of a random variable @xmath24 under a probability measure @xmath0 is denoted by @xmath25 , and the conditional law of @xmath26 given @xmath24 under @xmath0 by @xmath27 . when there is no room for ambiguity , these subscripts will be omitted .",
    "expectation with respect to a probability measure @xmath0 will be denoted by @xmath28 .",
    "again , the subscript will be omitted if the underlying probability distribution is clear from the context .",
    "the entropy of a distribution @xmath1 will be denoted by @xmath29 .",
    "let a measurable space @xmath30 be given , and denote by @xmath31 the set of all probability measures on it .",
    "for @xmath4 and @xmath32 , the rnyi divergence of degree @xmath6 of @xmath1 from @xmath0 is defined by rather than @xmath33 . by choosing the latter we follow the notation used in @xcite . ]",
    "@xmath34 where @xmath35 denotes absolute continuity of @xmath1 with respect to @xmath0 , and @xmath36 denotes the radon - nikodym derivative . for @xmath37",
    "one extends this definition by letting @xmath38 be the kl divergence , namely @xmath39 for @xmath1 and @xmath0 fixed , @xmath40 is nondecreasing as a map from @xmath41 to @xmath42 $ ] .",
    "moreover , if @xmath43 and @xmath44 then @xmath45 is finite and continuous on @xmath46 . for extension to @xmath47 and many other useful properties of the divergence , see @xcite , @xcite , @xcite and @xcite .",
    "the well - known convex duality between exponential integrals and kl divergence @xcite states that for any bounded measurable function @xmath48 , and every @xmath49 , @xmath50.\\ ] ]    it has recently been shown ( in @xcite ; earlier related calculations appeared in @xcite ) that @xmath51 , \\qquad { \\alpha}>1.\\ ] ] formally , one can recover from by taking the limit @xmath52 and using @xmath53 in place of the limit of @xmath5 as @xmath52 .",
    "now , as a consequence of one obtains for @xmath4 and @xmath32 the bound @xmath54 given an event @xmath55 , one can take @xmath56 to assume the values @xmath57 and @xmath58 on @xmath2 and its complement , respectively , and on taking the limit @xmath59 , deduce from the above that @xmath60 ( see @xcite for the details ) .",
    "inequalities and are referred to in @xcite as the _ risk - sensitive functionals comparison bound _ and _ logarithmic probability comparison bound _ , respectively .",
    "it is important to mention that both inequalities are tight in the sense that given @xmath1 and @xmath56 , there exists a ( unique ) measure , namely @xmath61 , @xmath62 , for which holds with equality . and",
    "given @xmath1 [ resp .",
    ", @xmath0 ] and @xmath2 for which @xmath64 [ resp . ,",
    "@xmath65 , there exists a ( unique ) measure , namely @xmath66 [ resp .",
    ", @xmath67 for which holds with equality .",
    "another useful fact is that both also give lower bound in addition to an upper bound , by interchanging the roles of the measures .",
    "it is well known that can be used to obtain estimates on probabilities of rare events ( see @xcite ) . by an approach developed in @xcite , the representation also leads to such estimates , by appealing to and .",
    "we now present this approach .",
    "consider first the simple case where a sequence of real valued random variables @xmath69 defined on the given measurable space is i.i.d .   under both probability measures",
    "@xmath0 and @xmath1 .",
    "denote by @xmath10 and @xmath12 the respective probability laws of the vector @xmath70 .",
    "it is a simple fact that the rnyi divergence scales as @xmath71 .",
    "thus for @xmath72 and any event @xmath73 measurable on the sigma - field generated by @xmath74 , that is , for some borel subset @xmath75 of @xmath76 , @xmath77 , one has @xmath78 this gives a comparison of the exponential rates involving only the rnyi divergence between the two marginals . in greater generality , when @xmath79 are not necessarily i.i.d .   under the measures @xmath0 and @xmath1 , with @xmath10 and @xmath12 still denoting the respective probability laws of @xmath80 , for @xmath72 ,",
    "let @xmath81 and @xmath73 be a bounded , measurable function and an event , that are both measurable on the sigma - field generated by that vector .",
    "then again , from and , @xmath82    \\le\\frac{1}{{\\alpha}n } \\ln { \\mbox{\\boldmath $ e$}}_q[e^{{\\alpha}g_n(x^n ) } ] + \\frac{1}{n } d_{\\alpha}(p_n\\|q_n),\\ ] ] @xmath83 denote @xmath84 and @xmath85 then @xmath86 combining this with the bound obtained by interchanging @xmath0 and @xmath1 , one obtains the two - sided bound on the exponential decay rate under @xmath0 in terms of that under @xmath1 : @xmath87 note that upper and a lower bounds analogous to can be deduced from for limits of the left - hand side of . in the sequel ,",
    "when the limits exist , we write @xmath88 and @xmath89 as @xmath90 . we will usually take @xmath0 to be the model of interest , or the ` true ' model , and @xmath1 will be the reference model .",
    "it is instructive to note that inequalities and , that are valid for each @xmath22 , provide some information that is lost when passing to the limit , as for example in the i.i.d .",
    "case alluded to above , where the divergence term @xmath91 is given explicitly .",
    "this viewpoint of the approach has been further developed in @xcite .",
    "however , in this paper , we will use the bounds exclusively in their limit forms , given by .    to relate to the standard change of measure technique , consider the upper bound on @xmath92 ( which corresponds to a lower bound on probabilities ) in the case where the probabilities of the event of interest are order 1 at the logarithmic scale , namely @xmath93 .",
    "then one can take @xmath52 . since the divergence term converges ( formally ) to that given in terms of the kl divergence , the standard change of measure method recovers .",
    "the bounds are useful when for a given model of interest @xmath0 , one can find a reference model @xmath1 for which the exponents are known or can be bounded , and at the same time , one can efficiently estimate the divergence term .",
    "this is demonstrated in this article several times . whereas the case alluded to above , in which both @xmath0 and @xmath1 have i.i.d .",
    "structure , is most instructive , we will apply the bounds in scenarios that go far beyond that .",
    "in fact , the bound we develop are more effective in situations where the model of interest @xmath0 has long memory properties ( such as , in the setting of channel coding , models that have interference , fading or erasure with long range correlations ) .",
    "a useful framework is when the true model consists of a small perturbation of the reference model .",
    "here we analyze a simple case where the alphabet is finite , and obtain a bound involving the second moment of the perturbation size . while the proof of the result is simple ,",
    "it is an archetype of the argument used several times in the sequel for more complicated models in which the noise is dominant .",
    "these include the very noisy channel ( see p.  155 , eq .",
    "( 3.4.23 ) of @xcite ) @xmath94 $ ] and , in the same spirit , the weak interference channel @xmath95 $ ] .",
    "let a vector @xmath96 take values in @xmath97 where @xmath98 is a finite set , and assume that the vector is i.i.d .   under both the measures @xmath0 and @xmath1",
    ". denote by @xmath10 and @xmath12 the respective probability laws of the vector . denoting @xmath99 and @xmath100 ,",
    "assume that @xmath101 $ ] for all @xmath102 , where @xmath103 .",
    "assuming @xmath104 charges all of @xmath98 , so does @xmath0 , provided that @xmath105 is small .",
    "let @xmath73 be any sequence of events of the form @xmath77 , where @xmath75 is a borel subset of @xmath76 and use the notation for @xmath92 and @xmath106 .",
    "[ prop31 ] denote @xmath107 .",
    "then @xmath108    * proof : *  one has @xmath109\\\\ & = & \\frac{1}{{\\alpha}({\\alpha}-1)}\\ln\\left[\\sum_{y } q(y)[1+\\epsilon(y)]^{1-{\\alpha}}\\right]\\\\ & \\le&\\frac{1}{{\\alpha}({\\alpha}-1)}\\ln\\left[\\sum_{y } q(y)[1+(1-{\\alpha})\\epsilon(y)+\\frac{1}{2}{\\alpha}({\\alpha}-1)\\epsilon^2(y)]+c({\\alpha})\\|{\\epsilon}\\|^3\\right]\\\\ & = & \\frac{1}{2}\\sum_y q(y)\\epsilon^2(y)+\\tilde c({\\alpha})\\|{\\epsilon}\\|^3\\\\ & = & \\frac{1}{2}\\overline{\\epsilon^2}+\\tilde c({\\alpha})\\|{\\epsilon}\\|^3,\\end{aligned}\\ ] ] for suitable @xmath110 and @xmath111 .",
    "now , by the assumed i.i.d.structure , @xmath112 .",
    "thus by , for every @xmath4 , @xmath113.\\ ] ] the function @xmath114 , @xmath115 , @xmath116 attains minimum at @xmath117 and the minimum is given by @xmath118 . therefore @xmath119",
    "@xmath120    a more general setting is discussed in a recent paper @xcite ( specifically eq .",
    "( 50 ) therein ) where for a parametric family @xmath121 , one has @xmath122 where @xmath123 is the fisher information . in this case , the bound is valid with @xmath124 replaced by @xmath125 .",
    "this section addresses the use of the lower and upper bounds in the context of channel coding .",
    "we begin by considering , in subsection [ sec41 ] , a general framework where we describe the relevance of the bounds in three contexts : ( 1 ) bounds on performance for a given channel in terms of a reference channel ; ( 2 ) bounds for mismatched decoding ; ( 3 ) robust bounds . in subsections",
    "[ sec42][sec45 ] , we consider several specific channel models of interest , where our methods yield new bounds . these include interference with long range dependence , discrete and continuous time gaussian ( and non - gaussian ) channels with fading , and the binary channel with erasure .        in channel coding ,",
    "messages are encoded , transmitted over a noisy channel and decoded .",
    "the precise setting that we shall use is as follows .",
    "a message @xmath126 from a set of @xmath127 messages , @xmath128 , is encoded into a codeword @xmath129 of length @xmath22 , whose coordinates all take on values in a space @xmath130 , that for the purposes of this paper may be either finite or a euclidean space @xmath131 ( some @xmath132 ) . here , @xmath133 is the coding rate in nats per channel use .",
    "we let @xmath134 denote the codebook .",
    "our analysis allows for the codebook to be either deterministic or random , settings which we refer to as deterministic and random coding , respectively . when a codeword @xmath135 is transmitted over a channel , a channel output @xmath136 is produced , where again @xmath98 is either a given finite set or @xmath137 ( some @xmath138 ) .",
    "the decoder observes the vector @xmath139 and produces an estimate @xmath140 using a metric decoder , i.e. , @xmath141 where ties are broken by an arbitrary deterministic rule , and @xmath142 is an additive _ decoding metric _ function , that is , it takes the form @xmath143 where @xmath144 is a given borel measurable function . to describe the model probabilistically ,",
    "we consider now the input and output of the channel as random variables , and write them as @xmath145 and @xmath146 . the message and estimated message are also random now but still denoted @xmath126 and @xmath147 , respectively .",
    "the collection of these random variables ( for all values of @xmath22 ) , as well as the codebooks ( in the case of random coding ) are defined on a probability space @xmath148 .",
    "the probabilistic elements and assumptions of the model are as follows : + ( i ) @xmath126 is uniformly distributed over @xmath149 .",
    "consequently ( assuming throughout that all codewords are distinct ) , @xmath150 for deterministic coding , and @xmath151 in the case of random coding .",
    "+ ( ii ) the model for the channel is described by the conditional distribution of @xmath146 given @xmath145 .",
    "this conditional distribution is denoted by @xmath152 if we denote by @xmath153 the error event then the error probability is given by @xmath154 . in the case of random coding ,",
    "this can be written as @xmath155 $ ] , which is interpreted as the mean probability of error when averaged over codes .",
    "the decoding metric @xmath156 is not assumed to be matched to the channel ( as is the case , for example , when @xmath157 takes the product form @xmath158 and @xmath159 is proportional to @xmath160 ) .",
    "we will sometimes assume ( without essential loss of generality ) that the given code @xmath134 is a constant composition code ( ccc ) , that is , all codewords have the same empirical distribution , which converges to a given probability distribution @xmath161 as @xmath162 .",
    "a _ reference channel _ is another probability measure , @xmath1 , on @xmath163 , which models a ( possibly ) different channel . in this work",
    ", we will always assume that , under @xmath1 , the distribution of the codes ( in the case of random coding ) as well as the probability of each codeword , is the same as under @xmath0 ; specifically , and are valid with @xmath0 replaced by @xmath1 .    for deterministic coding ,",
    "let @xmath10 and @xmath12 denote the joint distribution of the two @xmath22-vectors @xmath164 under @xmath0 and @xmath1 , respectively .",
    "it will be assumed that , given @xmath22 , the correspondence between @xmath126 and @xmath145 is one - to - one . as a result ,",
    "the error event is measurable with respect to the @xmath165-field generated by @xmath164 . in the case of random coding ,",
    "it is not natural to assume that the correspondence alluded to above is always one - to - one . in this case",
    "we use the same notation , @xmath10 and @xmath12 , to denote the respective distributions of the quadruple @xmath166 .",
    "the error event is then measurable with respect to the @xmath165-field of this quadruple .",
    "thus by , we have for every @xmath22 and every @xmath4 , the lower bound @xmath167 and the upper bound @xmath168",
    "adapting the notation to the present setting , we write @xmath169 where @xmath170 is any channel model , and we emphasize the dependence on the rate @xmath171 and on the metric @xmath172 ( however , in the sequel , we sometimes suppress the dependence on @xmath171 and @xmath172 when there is no room for confusion ) .",
    "the notation @xmath16 from will be used here with @xmath173 and @xmath174 again denoting the respective joint distribution of the @xmath22-vectors @xmath164 .",
    "we thus obtain from , for every @xmath4 , the bounds @xmath175      we identify three ways in which the above bounds can be used . in all cases , we think of @xmath0 as the true channel model and @xmath1 as a reference .",
    "\\(i ) _ bounds on performance of the true channel in terms of a reference channel .",
    "_    one can obtain lower [ upper ] bounds on error exponents for true channel models by means of a lower [ resp .",
    ", upper ] bound for a reference model .",
    "suppose that @xmath172 and a reference channel @xmath1 are given , where @xmath172 is matched to @xmath1 . more generally , suppose that a parametric family @xmath176 is given such that a given , fixed metric @xmath172 is matched to each member of the family .",
    "assume further that one knows a lower bound , @xmath177 on the error exponent @xmath178 .",
    "then for a metric @xmath179 that is matched to @xmath0 , we obtain @xmath180.\\ ] ]    similarly , an upper bound is possible for given @xmath0 and @xmath172 , when for reference channels @xmath1 one knows an upper bound @xmath181 on @xmath182 ( when @xmath172 is not necessarily matched to @xmath1 ) and then @xmath183.\\ ] ]    \\(ii ) _ bounds on performance of mismatched decoding . _    when @xmath172 is matched to a reference channel @xmath1 , or a parametric family thereof , the second inequality in serves as an upper bound on the mismatched error exponent ( of using @xmath172 with the true channel @xmath0 ) in terms of matched error exponent bounds ( of using @xmath172 with the reference channels @xmath184 to which it is matched ) .",
    "a similar statement is valid for the upper bound .",
    "to recapitulate , the above inequalities give bounds on the error exponents under the true channel , operating with a decoder that is matched to another channel in terms of error exponents of the latter .",
    "\\(iii ) _ robust bounds . _",
    "consider a family @xmath185 of true channels . as a performance criterion for the decoder , it is of interest to study the minimum error exponent within the family , namely @xmath186 optimizing over decoders gives @xmath187",
    "thus @xmath188 is the best possible guarantee on the performance of all channels within the family when the communication system operates with a single decoder @xmath172 ( where ` best ' refers to the selection of @xmath172 ) .",
    "we can take advantage of the fact that the aforementioned bounds for a fixed channel model , @xmath0 , are independent of @xmath0 for @xmath189 , in order to obtain information on @xmath188 . to this end ,",
    "fix a reference channel @xmath1 , and assume that it is a member of the family @xmath185 . then automatically , @xmath190 , where @xmath191 is matched to @xmath1 .",
    "as far as a lower bound is concerned , recall that for @xmath189 , and fixed @xmath6 , @xmath192 let @xmath193 .",
    "then , for @xmath4 , @xmath194 whereas the max - min problem posed by is typically notoriously hard , the optimization problem that now appears in the bound is easy to handle , since the optimal decoder for @xmath1 is the one matched to it .",
    "thus we have @xmath195\\le { { \\cal e}}(r , f)\\le e(r , q , d_q).\\ ] ]    the points of view ( i)(iii ) presented above will be further explored and demonstrated for the specific models to be considered .      here we consider the mismatched channel problem where both @xmath0 and @xmath1 are memoryless . for simplicity , we assume that @xmath130 and @xmath98 are discrete . given a metric @xmath172 , it is natural to consider as a parametric family of reference channels @xmath196 given by @xmath197 where @xmath198 and @xmath199 and @xmath200 , @xmath102 , are the parameters of the channel . then the decoding metric @xmath172 is matched to each of these channels , namely @xmath172 is the maximum likelihood ( ml ) decoding metric for @xmath201 for each @xmath202 and @xmath203 .",
    "it is instructive to note that , as @xmath204 , the channel becomes `` noisier '' , i.e. , the output becomes proportional to @xmath205 , independently of the input .",
    "assume a constant composition code .",
    "then , for @xmath196 , we can calculate the divergence term as @xmath206^{\\alpha}[\\pi({\\mbox{\\boldmath $ x$}})p({\\mbox{\\boldmath $ y$}}|{\\mbox{\\boldmath $ x$}})]^{1-{\\alpha}}\\right)\\\\ & = \\frac{1}{n({\\alpha}-1)}\\ln\\left(\\sum_{{\\mbox{\\boldmath $ x$}}\\in{{\\cal",
    "c}}_n}\\pi({\\mbox{\\boldmath $ x$}})\\sum_{{\\mbox{\\boldmath $ y$}}\\in{{\\cal y}}^n } \\prod_{i=1}^n[q^{\\alpha}(y_i|x_i)p^{1-{\\alpha}}(y_i|x_i)]\\right)\\\\ & = \\frac{1}{n({\\alpha}-1)}\\ln\\left(\\sum_{{\\mbox{\\boldmath $ x$}}\\in{{\\cal c}}_n}\\pi({\\mbox{\\boldmath $ x$ } } ) \\prod_{i=1}^n\\left[\\sum_{y\\in{{\\cal y}}}q^{\\alpha}(y|x_i)p^{1-{\\alpha}}(y|x_i)\\right]\\right)\\\\ & = \\frac{1}{n({\\alpha}-1)}\\ln\\left(\\sum_{{\\mbox{\\boldmath $ x$}}\\in{{\\cal c}}_n}\\pi({\\mbox{\\boldmath $ x$ } } ) \\prod_{\\bar x\\in{{\\cal x } } } \\left[\\sum_{y\\in{{\\cal y}}}q^{\\alpha}(y|\\bar x)p^{1-{\\alpha}}(y|\\bar x)\\right ] ^{n\\mu(\\bar x)}\\right)\\\\ & = \\frac{1}{({\\alpha}-1)}\\sum_{x\\in{{\\cal x}}}\\mu(x)\\ln\\left ( \\sum_{y\\in{{\\cal y}}}q^{\\alpha}(y|x)p^{1-{\\alpha}}(y|x)\\right)\\\\ & = { \\alpha}\\sum_{x\\in{{\\cal x}}}\\mu(x)d_{\\alpha}(q(\\cdot|x)\\|p(\\cdot|x)).\\end{aligned}\\ ] ] thus the term reduces to one that involves rnyi divergence at the single - letter conditional marginals .",
    "substituting in , we obtain @xmath207 now , @xmath182 is an error exponent for _ matched decoding _ for the channel @xmath1 , and is therefore upper bounded by any upper bound on the reliability function , such as the well - known straight ",
    "line bound @xmath208 ( cf .",
    "sections 3.63.8 of @xcite ) .",
    "thus , we have @xmath209.\\ ] ]    to put in the context of known results , let @xmath210 denote the single ",
    "letter mutual information between @xmath24 and @xmath26 , induced by the joint distribution @xmath211 , that is , @xmath212.\\ ] ] in is known @xcite that @xmath213 let us show that if fact reduces to . given @xmath171 , and a random coding distribution @xmath214 , consider @xmath1 for which @xmath215 .",
    "then @xmath216 , and so eq .  ( [ bound ] ) is further upper bounded by @xmath217 now , @xmath218 is a monotonically non  decreasing as a function of @xmath219 , and taking the limit @xmath220 results in @xmath221 , which recovers by minimizing over @xmath1 and maximizing over @xmath214 .",
    "recall the general upper bound @xmath222 which holds for any pair of channel models @xmath0 and @xmath1 and every @xmath4 .",
    "we can iterate this estimate so as to compare another model , @xmath223 , to @xmath1 by relating it first to @xmath0 and @xmath0 to @xmath1 .",
    "this may be useful in situations when estimating the divergence of @xmath0 from @xmath1 and that of @xmath223 from @xmath0 is easier than estimating the divergence of @xmath223 from @xmath1 .",
    "indeed , expressing relation for @xmath224 and @xmath0 gives , for any @xmath225 , @xmath226 consequently , for any @xmath4 and @xmath225 , @xmath227 we use this approach in one of the results of subsection [ sec42 ] .      in this section",
    "we are interested in a channel of the form @xmath228 for a generic sequence of functions @xmath229 . here",
    "@xmath230 is an i.i.d .",
    "@xmath231 noise ( although we will also address a more general i.i.d .",
    "noise in the sequel ) .",
    "the main assumption is that the interference functions @xmath232 are bounded .",
    "however , no assumption is made about @xmath232 that limits the range of correlations of the interference signal .",
    "we let @xmath0 be a probability measure under which @xmath230 is as described above , and @xmath233 and @xmath230 are mutually independent .",
    "this model will be studied via the reference channel @xmath1 , under which @xmath234 where @xmath235 are i.i.d .",
    "@xmath236 ( @xmath237 being a parameter ) , independent of @xmath233 . thus under the true channel ,",
    "@xmath238 ^ 2\\right\\},\\ ] ] while under @xmath239 , @xmath240    [ th31 ] denote by @xmath241 the straight  line ( upper ) bound on @xmath242 .",
    "assume that for every @xmath22 , and @xmath243 , @xmath244 and @xmath245 for constants @xmath246 and @xmath247 .",
    "then , for any sequence of codes and any decoder , @xmath248}{2({\\alpha}-1)}+\\frac{{\\alpha}s{\\mathnormal{\\gamma}}^2}{2\\sigma^2[1+{\\alpha}(s-1)]}\\big\\}.\\end{aligned}\\ ] ]    the proof of this result , that appears in the appendix , uses the following identity in estimating the rnyi divergence . for any real @xmath249 , @xmath250 , @xmath251 and @xmath252 such that @xmath253 , @xmath254 this identity is used , in addition , in several other proofs in the sequel .",
    "we emphasize that for the model under consideration , the authors are not aware of any other alternative bound on the error exponents .",
    "consider now the choice @xmath255 in . in this case , the expression simplifies to @xmath256 the optimal @xmath219 is easily found to be @xmath257 which yields @xmath258    the structure of the above bound is reminiscent of the bound from proposition [ prop31 ] .",
    "however , the bound above is valid not only for weak interference .",
    "results of similar structure appear several times in the sequel .    note that the above bound has a clear weakness of having a floor of @xmath259 independent of the rate @xmath171 .",
    "this is an inherent limitation stemming from the way the we apply the bound .",
    "however , one may apply additional considerations to address this difficulty .",
    "specifically , one can use the idea of the straight  line bound ( c.f .",
    "theorem 3.8.1 in @xcite and list of size @xmath260 , as can easily be shown by a simple extension of fano s inequality for list decoding .",
    "this is done by using the fact that @xmath261 ( unlike the case of ordinary decoding where @xmath262 ) . ] ) , to improve the bound using the smallest straight  line function that touches the curve @xmath263 , passing through the point @xmath264 , where @xmath265 is the capacity of the true channel .",
    "the latter is upper bounded by @xmath266 $ ] , where @xmath267 is an upper bound on the average power of @xmath145 . in what follows",
    "we will denote this improved bound by @xmath268 .",
    "we now focus on the case of a very noisy channel , where bounds can be computed explicitly and insight can be obtained .",
    "we thus study the implication of theorem [ th31 ] , specifically of , to the case where @xmath269 , where @xmath233 satisfies @xmath270 a.s .",
    ", for a given power limitation @xmath271 . in this case",
    ", the capacity of the reference channel ( with @xmath255 ) is about @xmath272 and the capacity of the true channel is ( upper bounded by ) @xmath273 .",
    "the error exponent is given by ( see p.  157",
    "( 3.4.33 ) of @xcite ) @xmath274 now , accordingly , @xmath275 ^ 2 & r < c_q/4\\\\ \\left[\\sqrt{c_q}+{\\mathnormal{\\gamma}}/(\\sqrt{2}\\sigma)-\\sqrt{r}\\right]^2 & c_q/4 \\le r < c_q\\\\ { \\mathnormal{\\gamma}}^2/(2\\sigma^2 ) & r >",
    "c_q\\end{array}\\right.\\nonumber\\\\ & = & \\left\\{\\begin{array}{ll } \\left[\\sqrt{c_q/2 - r}+{\\mathnormal{\\gamma}}/(\\sqrt{2}\\sigma)\\right]^2 & r < c_q/4\\\\ \\left(\\sqrt{c}-\\sqrt{r}\\right)^2 & c_q/4 \\le r < c_q\\\\ { \\mathnormal{\\gamma}}^2/(2\\sigma^2 ) & r > c_q.\\end{array}\\right.\\end{aligned}\\ ] ]    note that at least in the intermediate range , between @xmath276 and @xmath277 , the bound is tight in the sense that there exists an interference signal that achieves it .",
    "it corresponds to the coherent sum of the desired signal and the interference , which is the case when @xmath278 is proportional to @xmath279 .",
    "the improvement at high rates is provided by the straight  line that passes through the points @xmath280 and @xmath264 .",
    "the result we thus obtain for the very noisy channel is @xmath281 ^ 2 & r < c_q/4\\\\ \\left(\\sqrt{c}-\\sqrt{r}\\right)^2 & c_q/4 \\le r < c_q\\\\ \\frac{{\\mathnormal{\\gamma}}^2(c - r)}{2\\sigma^2(c - c_q ) } & c_q\\le r < c.\\end{array}\\right.\\ ] ]    at rate zero ( and general snr ) , the bound one obtains from the discussion above , by selecting @xmath255 , is @xmath282 it turns out that for @xmath283 one can solve the full optimization problem , including the minimization over the parameter @xmath284 .",
    "in fact , one can even solve an extended problem , in which the reference model has one additional free parameter , namely a gain factor @xmath285 : instead of , one considers @xmath286 of the form @xmath287 , where @xmath288 however , the bound one obtains is exactly .",
    "we can also derive a lower bound by appealing to . in this context",
    ", it is more natural to consider the setting of random coding because existing bounds for reference models are of this type . denoting the sequence of random codes by @xmath289 ,",
    "the relevant divergence term for using is @xmath290.\\ ] ] recalling our assumption that under the true model @xmath0 and under the reference model @xmath1 the distribution of the codes is equal , we have @xmath291.\\ ] ] now , the estimate on the divergence term appearing in the proof of theorem [ th31 ] can be carried out for the above in a similar manner , and one obtains the same bound regardless of the code @xmath292 . for simplicity , we can specialize to @xmath255 , which gives @xmath293,\\end{aligned}\\ ] ] the solution of which is @xmath294    one can use the above bound to estimate the capacity of the the channel @xmath0 .",
    "it is bounded below by the rate @xmath171 at which @xmath295 .",
    "for the example of the very noisy channel , this gives @xmath296 .",
    "this bound is attained by the interference signal that is anti  coherent with the desired signal , i.e. , @xmath297 .",
    "all three interpretations mentioned in subsection [ sec41 ] are relevant for the results of this section .",
    "specifically , the bounds of theorem [ th31 ] and are valid whether @xmath172 is matched to @xmath0 or not .",
    "next , to demonstrate the robust bounds interpretation in the context of these results , let @xmath298 denote the reference channel ( with @xmath255 ) and for a fixed @xmath247 , denote by @xmath185 the family of true channels @xmath0 for which @xmath232 are all bounded by @xmath247 .",
    "then by and the bound @xmath299 that follows from the previous paragraph , we have @xmath300 specifically , the performance of a single decoder , namely the one matched to @xmath301 , is bounded by the above two bounds whenever the interference signal is bounded by the constant @xmath247 .      here",
    "we use the idea of iterating the bound , as presented in subsection [ sec41 ] , in order address non - gaussian i.i.d .   noise .",
    "going back to the general setting of theorem [ th31 ] , recall from and that under @xmath0 and @xmath1 , respectively , we have the models @xmath302 @xmath303 where @xmath230 and @xmath235 are i.i.d .",
    "@xmath304 , independent of @xmath233 .",
    "consider now an additional model @xmath223 described by @xmath305 where @xmath306 are i.i.d .   but need not be gaussian .",
    "the main point is that estimating the divergence of @xmath223 from @xmath0 is simple , whereas the estimates on the divergence of @xmath0 from @xmath1 have already been established , thus by appealing to , one can relate @xmath223 to @xmath1 by combining the two estimates .",
    "denote by @xmath307 the single ",
    "letter rnyi divergence , where for a measure @xmath214 and r.v .",
    "@xmath308 , @xmath309 denotes the probability law of @xmath308 under @xmath214 . since both @xmath306 and @xmath230 are i.i.d .",
    ", we can make use of the simple fact that @xmath310 moreover , since under both @xmath223 and @xmath0 , the noise sequence is independent of the signal @xmath233 and the latter has the same law , it follows that @xmath311 now , denote by @xmath173 and @xmath10 the respective laws of @xmath312 under @xmath223 and @xmath0 .",
    "note by and that @xmath313 and @xmath314 for suitable deterministic functions @xmath315 and @xmath81 . as a result",
    ", the data processing inequality ( see theorem 9 of @xcite ) gives @xmath316 . hence @xmath317 using in gives @xmath318 we use our previous results that estimate @xmath319 and optimize over @xmath6 .",
    "with @xmath320 given by , we have @xmath321 an analogous estimate can be established for an upper bound on the exponent , as well as for all other channel models that we treat in the sequel .",
    "consider truncated gaussian noise distribution for @xmath322 , namely , for a given constant @xmath249 , assume @xmath323}(w)$ ] , where @xmath324 is the standard normal density , and @xmath325 .",
    "assume @xmath326 is standard normal .",
    "it is easy to see that @xmath327 thus using and taking the limit @xmath328 , @xmath329      we next study the gaussian intersymbol interference ( isi ) channel model , denoted by @xmath0 , given by @xmath330 where @xmath230 is i.i.d .",
    "@xmath304 , independent of @xmath233 , and @xmath331 is given .",
    "while the proposed method yields new results for interference with unlimited correlation length ( theorem [ th31 ] ) , an analogous treatment of the model turns out not to be useful , as it leads to bounds that are inferior to existing bounds , for both matched and mismatched decoding .",
    "however , as we now demonstrate , the robust bound interpretation discussed in subsection [ sec41 ] gives rise to new results for this model .",
    "note that the model is a special case of the main model studied in this section .",
    "because of the special structure of the interference and some further assumptions we make regarding the correlation structure , the bounds that we are able to provide are much more explicit than those given by theorem [ th31 ]",
    ".    the following will be assumed .",
    "the rate is @xmath283 , and the channel is very noisy , that is , @xmath332 . the decoder uses the mismatched decoding metric @xmath333 .",
    "all codewords have energy @xmath334 and a fixed empirical autocorrelation structure @xmath335 denote @xmath336 and @xmath337_{i , j=1}^k$ ] and let @xmath338 and @xmath339 .",
    "then @xmath340 and @xmath341 are related to the empirical correlation between signal and interference @xmath342 and interference power , respectively .",
    "specifically , @xmath343 note that always @xmath344 .",
    "[ th45 ] consider a sequence of codes satisfying and for a specific vector @xmath345 .",
    "denote by @xmath185 the family of true models @xmath0 of the form , where @xmath346 varies over all vectors having fixed @xmath340 and @xmath341 .",
    "denote @xmath347 and @xmath348 .",
    "if @xmath349 then @xmath350    the proof appears in the appendix .",
    "we consider the channel @xmath351 where @xmath230 is an additive noise process and @xmath352 is a fading process .",
    "we let @xmath0 be a probability measure under which the processes @xmath352 , @xmath230 and @xmath233 are mutually independent , and @xmath230 is i.i.d .",
    "also , @xmath233 is assumed to satisfy the constraint @xmath353 for all @xmath243 . as a reference ,",
    "consider a channel with no fading .",
    "that is , consider a probability measure @xmath1 under which @xmath354 where the law of triplet @xmath355 under @xmath1 is the same as that of @xmath356 under @xmath0 .",
    "in particular , under @xmath1 , @xmath235 are i.i.d .",
    "@xmath304 , and the three processes @xmath233 , @xmath352 and @xmath235 are mutually independent .",
    "we assume that @xmath352 is a stationary , zero - mean gaussian process and that @xmath357 $ ] are absolutely summable .",
    "let @xmath358 denote the spectral density of @xmath202 , namely @xmath359    [ th42 ] let @xmath0 and @xmath1 stand for the discrete - time gaussian noise channel with and , respectively , without fading , described above .",
    "denote @xmath360 .",
    "then for any @xmath4 such that @xmath361 , @xmath362{{\\rm d}}{\\omega},\\end{aligned}\\ ] ] @xmath363{{\\rm d}}{\\omega}.\\ ] ]    see the appendix for a proof .",
    "note that for fixed @xmath6 , the gap between the upper and lower bound increases with @xmath358 .",
    "this occurs due to the fact that the distance between the model @xmath0 and the reference model @xmath1 , as measured in terms of the divergence , increases by strengthening the fading .",
    "when @xmath364 , the models @xmath0 and @xmath1 agree , and then so do the upper and lower bounds ( upon optimizing over @xmath6 ) .",
    "while it is difficult to optimize over @xmath6 in general , in the next paragraph we consider special cases where the results are more explicit .",
    "consider the case of @xmath352 given by the autoregressive ( ar ) model @xmath365 where @xmath306 are i.i.d .",
    "@xmath366 , @xmath367 and @xmath352 is stationary .",
    "we have @xmath368 , @xmath369 , and @xmath370 thus @xmath371 gives @xmath372 and so , whenever @xmath373 is bounded away from zero , which holds iff @xmath374 one has @xmath375    we next further develop based on the residue theorem , by which one has @xmath376 whenever @xmath377 for the complex logarithmic function @xmath378 .",
    "specifically , if we express @xmath373 as @xmath379 for a real @xmath380 with @xmath377 , and @xmath381 , then @xmath382 . to calculate @xmath380 and @xmath383 , write for @xmath384 , @xmath385 the solution to this is @xmath386 , @xmath387 where @xmath388 under , the discriminant is positive , and therefore @xmath389 and @xmath383 are real numbers .",
    "also , one checks that under , @xmath390 hence not to be considered . as for @xmath341 , we have @xmath391 under that condition .",
    "thus @xmath392 , and we have @xmath393 the limit case @xmath394 : this is when the fading amplitude goes to zero , we have the bound converging to @xmath395 , and optimizing over @xmath6 gives @xmath396 , that is the best possible bound under the circumstances .    using the lower bound gives the following bound , complementing , namely @xmath397{{\\rm d}}{\\omega}\\\\    & = \\frac{{\\alpha}-1}{{\\alpha}}e(q)-\\frac{1}{2{\\alpha } }    \\ln\\frac{\\xi({\\alpha})-\\sqrt{\\xi^2({\\alpha})-4a^2}}{2a^2 } ,    \\label{25}\\end{aligned}\\ ] ] for all @xmath4 satisfying .",
    "figure 1 depicts the above bound as a function of @xmath219 for various values of @xmath396 .",
    "note that the range of the parameter @xmath219 is of the form @xmath398 , where @xmath399 is the smallest @xmath6 which violates condition .",
    "the right end of the graphs in figures 1(a ) and 1(b ) correspond to @xmath399 .",
    "as function of @xmath219 .",
    "the five graphs correspond to @xmath400 and @xmath401 , with @xmath402 and @xmath403 , where in plot ( a ) , @xmath404 , and in plot ( b ) , @xmath405 .",
    "_ , title=\"fig : \" ] + ( a ) +   as function of @xmath219 .",
    "the five graphs correspond to @xmath400 and @xmath401 , with @xmath402 and @xmath403 , where in plot ( a ) , @xmath404 , and in plot ( b ) , @xmath405 .",
    "_ , title=\"fig : \" ] + ( b )    we comment that the bounds are tight in the small fading limit .",
    "namely , as the amplitude of the fading perturbation goes to zero , the optimal bounds ( obtained by choosing @xmath6 suitably ) converge to @xmath396 .",
    "indeed , as @xmath394 , the argument of the logarithmic function converges to @xmath406 , by which that follows .",
    "note that one can treat the small fading limit in greater generality ( beyond the ar process ) .",
    "denote @xmath407 . fix @xmath408 and assume @xmath409 .",
    "denote @xmath410 . using the bound @xmath411 for all @xmath412 s.t .",
    "@xmath413 in theorem [ th42 ] gives , for every fixed @xmath4 , @xmath414{{\\rm d}}{\\omega}\\\\ & = \\frac{{\\alpha}}{{\\alpha}-1}e(q)+\\frac{{\\alpha}{{a}}^2 r_0}{2{\\sigma}^2 } + \\frac{\\kappa{\\delta}^2}{2({\\alpha}-1)}. \\label{84}\\end{aligned}\\ ] ] optimizing over the parameter @xmath6 in the range @xmath415 can now be carried out easily ( in a manner similar to that in proposition [ prop31 ] ) .      a standard model for a white gaussian channel in continuous time",
    "is given by @xmath416 where @xmath230 is a brownian motion .",
    "let @xmath1 be a probability measure under which @xmath230 is a standard brownian motion , and let @xmath233 and @xmath352 be real - valued processes such that the three processes @xmath230 , @xmath233 and @xmath352 are mutually independent .",
    "assume that @xmath233 satisfies the amplitude constraint @xmath417 for all @xmath243 , @xmath1-a.s .",
    ", where @xmath418 is a constant .",
    "one can obtain from @xmath1 a model for a channel with fading , in which @xmath202 is the fading process , by means of a change of measure .",
    "to this end , consider the filtration @xmath419\\},\\ ] ] and let @xmath420 , \\qquad t\\ge0.\\ ] ] it is assumed throughout that , for every @xmath421 , @xmath422 we later provide a sufficient condition for this to hold .",
    "note that , as a result , one has @xmath423 , and so novikov s condition for @xmath424 to be an @xmath425-martingale under @xmath1 is satisfied ( see corollary 3.5.13 of @xcite ) . for @xmath421 ,",
    "let @xmath426 and @xmath427 be probability measures on @xmath428 , defined by @xmath429,\\qquad { { \\cal a}}\\in{{\\cal f}}_t,\\ ] ] where @xmath430 denotes the indicator function of @xmath2 .",
    "then @xmath431 , and by girsanov s theorem ( theorem 3.5.1 of @xcite ) one has @xmath432 where , under @xmath427 , the triplet @xmath433)$ ] has the same law as that of @xmath434)$ ] under @xmath426 ( thus under @xmath1 ) . in particular , under the measure @xmath427 , @xmath235 is a standard brownian motion , and the three processes @xmath230 , @xmath233 and @xmath352 are mutually independent . as a result , @xmath427 is a model for an additive white gaussian noise channel with a fading process @xmath352 .",
    "it is assumed that @xmath352 is a separable , zero - mean stationary gaussian process ( under @xmath426 ; equivalently under @xmath427 ) . the spectral density of @xmath352 , that is , the function @xmath358 for which @xmath435=\\int_{-{\\infty}}^{\\infty}e^{it{\\omega}}{\\mathnormal{\\sigma}}_\\theta({\\omega}){{\\rm d}}{\\omega}$ ] , is assumed to satisfy @xmath436 .",
    "the following , that can be seen as a continuous - time analogue of theorem [ th42 ] , is the main result of this subsection .",
    "[ th43 ] let @xmath0 and @xmath1 stand for the continuous time white noise channel models with and without fading , described above .",
    "assume @xmath437 . then holds .",
    "moreover , with @xmath438 , for any @xmath4 such that @xmath439 , @xmath440{{\\rm d}}{\\omega}\\ ] ] @xmath441{{\\rm d}}{\\omega}.\\ ] ]    see the appendix for a proof .    for an encoder / decoder optimized for @xmath1 , an expression for @xmath442 is well known ( see section 8.2 of @xcite ) , namely , with @xmath443 , @xmath444 as a result , and give bounds on the mismatched error exponents for the model with fading , when the encoder and decoder are matched to @xmath1 .",
    "the lower bound appears to be new even for the matched channel exponent , that is , when the right - hand side of serves as a lower bound on the error exponent for an encoder / decoder that are matched to @xmath0 .",
    "the expression in is simple when @xmath358 is constant on its support .",
    "specifically , consider the case @xmath445 on the interval @xmath446 $ ] . then @xmath447 where @xmath448,\\ ] ] provided @xmath449 .",
    "next consider a model where the fading process takes the form of a stationary ornstein - uhlenbeck process , namely @xmath450 where @xmath451 is a standard brownian motion and @xmath452 and @xmath453 are constants .",
    "then the spectral density is given by @xmath454 , and by a calculation from p.  130 of @xcite , one has @xmath455{{\\rm d}}{\\omega}=\\frac{1}{2}a-\\frac{1}{2}\\sqrt{a^2 - 4b^2c}\\ ] ] provided @xmath456 .",
    "thus @xmath457 where @xmath458 provided @xmath459 and @xmath460 .    while it is hard to optimize over @xmath6 , it is possible to do so if we bound @xmath380 from above by @xmath461 and assume @xmath462 .",
    "that is , @xmath463 . in particular",
    ", we must assume @xmath464 .",
    "we therefore have from @xmath465 the minimum of this upper bound over all @xmath6 in that range can be computed . indeed ,",
    "note that , as @xmath52 from the right , @xmath466 .",
    "moreover , the derivative of @xmath467 , that is given by @xmath468+\\frac1{{\\alpha}-1}\\frac{2b^2p{\\alpha}}{\\sqrt{a^2 - 4b^2p{\\alpha}^2}},\\ ] ] tends to @xmath469 as @xmath470 from the left . as a result , and since the equation @xmath471 turns out to have a unique root @xmath399 in that range , the minimizing @xmath6 must be equal to @xmath399 .",
    "this unique root is given by @xmath472 where @xmath473 with this notation , the optimal upper bound of the form is given by @xmath474 as @xmath394 , we have @xmath475 and as a consequence @xmath476 .",
    "that is , we recover the exponent @xmath396 as the fading intensity tends to zero .    as for a corresponding lower bound",
    ", we have @xmath477 a calculation shows that the maximizing @xmath6 is @xmath478 and so @xmath479 as @xmath394 we have @xmath480 and so @xmath481 .",
    "we next consider the channel @xmath482 where @xmath483 is i.i.d .",
    "noise whereas @xmath484 is an erasure process . here ,",
    "@xmath484 , @xmath485 and @xmath483 take values in @xmath486 and @xmath487 denotes addition modulo @xmath488 .",
    "it is assumed that @xmath489 and @xmath490 are mutually independent .",
    "we let @xmath491 and assume @xmath492 .",
    "the first model we examine for @xmath489 is a hidden markov model ( an additional model appears afterwards ) . specifically , we let @xmath493 be a stationary markov process on the state space @xmath494 ( independent of @xmath495 ) with a given transition probability matrix @xmath496 , assumed to be irreducible . for a given function @xmath497 ,",
    "@xmath251 is given by @xmath498 , @xmath499 .",
    "denote by @xmath0 the probability measure induced by the above processes .",
    "let @xmath1 denote a reference probability measure , under which @xmath500 where , for each @xmath22 , the law of the triplet @xmath501 is the same as that of @xmath502 under @xmath0 ( in particular , the three are mutually independent under @xmath1 ) .    to calculate the rnyi divergence , note that @xmath503p({\\mbox{\\boldmath $ x$}})p({\\mbox{\\boldmath $ a$}}),\\ ] ] where for @xmath504 , @xmath505 if @xmath506 and @xmath507 if @xmath508",
    ". also , @xmath509p({\\mbox{\\boldmath $ x$}})p({\\mbox{\\boldmath $ a$}}).\\ ] ] denoting by @xmath10 and @xmath12 the respective laws of @xmath510 , we have @xmath511    \\\\&=\\frac{1}{n({\\alpha}-1 ) }    \\ln { \\mbox{\\boldmath $ e$}}_{p}\\big[\\prod_{t : a_t=0,x_t=1}\\big(\\frac{p(y_t|x_t)}{p(y_t|a_tx_t)}\\big)^{\\alpha}\\big]\\\\    & = \\frac{1}{n({\\alpha}-1 ) }    \\ln { \\mbox{\\boldmath $ e$}}_{p}\\prod_{t : a_t=0,x_t=1 }    \\big[p \\big(\\frac{1-p}{p}\\big)^{\\alpha}+(1-p)\\big(\\frac{p}{1-p}\\big)^{\\alpha}\\big]\\\\    & \\le\\frac{1}{n({\\alpha}-1 ) }    \\ln { \\mbox{\\boldmath $ e$}}_{p}\\prod_{t : a_t=0}{\\delta}({\\alpha})\\\\    & = \\frac{1}{n({\\alpha}-1 ) }    \\ln { \\mbox{\\boldmath $ e$}}_{p}\\big[{\\delta}({\\alpha})^{n-\\sum_{t=1}^n a_t}\\big],\\end{aligned}\\ ] ] where we use the fact that @xmath512 .",
    "let @xmath513 , @xmath514 , and denote by @xmath515 the frequency of times @xmath243 when @xmath516 .",
    "then we can write the above as @xmath517.\\end{aligned}\\ ] ] by similar considerations one obtains @xmath518.\\ ] ]    for @xmath519 , let @xmath520 , where @xmath521 then @xmath522 is an irreducible matrix for every @xmath523 and , by the perron - frobenius theorem , has a real positive eigenvalue , denoted by @xmath524 , that dominates all eigenvalues in absolute value .",
    "it is known that the random variables @xmath525 satisfy the large deviation principle with the good rate function @xmath526 $ ] , defined as @xmath527 ( for the terminology see @xcite ; for the above result see theorem 3.1.2 therein ) .",
    "thus by varadhan s lemma ( theorem 4.3.1 of @xcite ) , it follows that @xmath528    = \\sup_{x\\in{\\mathbb{r}}}[x\\ln{\\delta}({\\alpha})-i(x)].\\ ] ] we thus have    [ th44 ] for @xmath1 the binary channel and @xmath0 the binary channel with erasure described above , for every @xmath4 , @xmath529,\\ ] ] and @xmath530.\\ ] ]      we now examine another model for the erasure process @xmath489 . in this model , the erasure process satisfies a single hard constraint , namely that the relative number of erasures @xmath531 is a.s .- bounded .",
    "specifically , for some constant @xmath532 $ ] , it is assumed that @xmath533 a.s .",
    ", for every @xmath22 . to relate this to the previous model , note that this may occur when the ( stationary , markov ) process @xmath534 taking values in @xmath494 is cyclic , and where the subset @xmath535 of states corresponding to erasure has cardinality @xmath383 with @xmath536",
    "of course , the class of processes @xmath251 satisfying the current assumption is much broader .",
    "note that and are valid . as a result",
    ", we obtain in this case , for @xmath4 , @xmath537 clearly , this model has a property analogous to that established for the channel with fading , namely that as @xmath538 , both bounds converge ( upon optimization with respect to @xmath6 ) to @xmath396 .",
    "consider the problem of rate  distortion coding of a source sequence @xmath539 given by @xmath540 where , under the probability measure @xmath0 , @xmath233 is an i.i.d .",
    ", @xmath304 process and @xmath424 is a process that is independent of @xmath233 . for simplicity ,",
    "assume the random vector @xmath541 has density , denoted @xmath542 .",
    "each source sequence @xmath543 is compressed to a string of @xmath544 nats , from which the decoder reconstructs an approximated sequence @xmath545 .",
    "we are interested in a lower bound on @xmath546 where @xmath172 is large enough so that this probability decays exponentially .",
    "the joint density of @xmath547 under @xmath0 is thus given by @xmath548 , where @xmath56 is the i.i.d .",
    "@xmath304 ) density .",
    "we consider a reference measure @xmath1 , under which the joint density of @xmath547 is @xmath549 .",
    "since the event @xmath550 is measurable on the sigma - field of @xmath146 , and under @xmath1 , @xmath146 and @xmath551 are mutually , independent , the law of @xmath551 is irrelevant for the estimation of @xmath552 , in the sense that @xmath553 , where we denote by @xmath554 the law of @xmath146 under @xmath1 ( equivalently , that of @xmath145 under @xmath0 ) . in the appendix ,",
    "we show that @xmath555,\\ ] ] where @xmath556 is the rate",
    " distortion function of the gaussian source @xmath233 and @xmath557    we now calculate the divergence term . with @xmath10 and @xmath12 denoting the respective laws of @xmath547 , @xmath558\\nonumber\\\\ & = & \\frac{1}{n({\\alpha}-1)}\\ln\\left[\\int_{{{\\rm i\\!r}}^n}\\mbox{d}{\\mbox{\\boldmath $ z$}}f_z({\\mbox{\\boldmath $ z$}})\\exp\\left\\{\\frac{{\\alpha}({\\alpha}-1)\\|{\\mbox{\\boldmath $ z$}}\\|^2}{2\\sigma^2}\\right\\}\\right],\\end{aligned}\\ ] ] where the second step follows by appealing to identity .",
    "the usefulness of the bound will now depend on estimating the last expression .",
    "obviously , for this expression to be finite , the tails of @xmath542 must decay faster than those of a gaussian .    consider the , for example , the case where @xmath559 almost surely . in this case , the right - hand side of is bounded by @xmath560 .",
    "using this bound together with in gives @xmath561}{{\\alpha}-1}+\\frac{{\\alpha}a^2}{2\\sigma^2}\\right]=\\left(\\sqrt{{\\mathnormal{\\phi}}[r - r_g(d)]}+\\frac{a}{\\sqrt{2}\\sigma}\\right)^2.\\end{aligned}\\ ] ] in a similar way , one obtains @xmath562}-\\frac{a}{\\sqrt{2}\\sigma}\\right)^2.\\ ] ]    an analogous derivation can be made for the case where @xmath233 is a binary memoryless source with parameter @xmath563 , @xmath424 is a binary interference with normalized hamming weight limited by @xmath564 , and @xmath565 .",
    "we then end up with @xmath566}{{\\alpha}-1}\\right],\\ ] ] where @xmath567 is the source coding error exponent @xcite associated with @xmath233 .",
    "a possible extension of this example is associated with the problem of separate encodings and joint decoding of correlated sources .",
    "let @xmath568 be @xmath22 independent copies of a random pair @xmath569 distributed according to @xmath570 , @xmath571 , @xmath102 .",
    "the sequences @xmath572 and @xmath573 are compressed separately by two encoders ( that do not cooperate ) at rates @xmath574 and @xmath575 , respectively .",
    "the respective compressed bit  streams are both fed into a joint decoder that produces reconstructions @xmath576 and @xmath577 , whose components take on values in alphabets @xmath578 and @xmath579 , respectively",
    ". let @xmath580 and @xmath581 be given distortion functions .",
    "we are interested in a lower bound on @xmath582 for some prescribed distortion levels @xmath583 and @xmath584 .",
    "we wish to pass to a reference source for which @xmath572 and @xmath573 are statistically independent , that is , @xmath585 . under @xmath1 ,",
    "the probability of the above event decays exponentially at rate @xmath586 , where @xmath587 and @xmath588 are the source coding exponents of the separate reference sources , @xmath589 and @xmath590 , respectively .",
    "thus , our upper bound on the exponent is given by @xmath591+{\\alpha}d_{{\\alpha}}(q_x\\times q_y\\|p_{xy})\\right\\}.\\end{aligned}\\ ] ] in this setting , to the best of our knowledge , there does not exist any competing bound in the literature .",
    "let @xmath592 be a random vector with a given distribution .",
    "let @xmath593 be a sequence of ` guesses ' of the random vector @xmath146 that is generated without observing @xmath146 . within distortion @xmath172 from @xmath139 , denoting by @xmath594 the hamming distance and fixing a distortion level @xmath595 ,",
    "let @xmath596 denote the number of trials it takes to correctly guess @xmath146 within distortion level @xmath172 , i.e. , @xmath597 in @xcite , it was shown that for a given discrete memoryless source @xmath1 and a given parameter @xmath598 , @xmath599,\\ ] ] where @xmath301 denotes the marginal of @xmath1 , @xmath600 denotes the rate  distortion function of the source @xmath601 , and the supremum is over @xmath224 in the set of probability measures over the alphabet of @xmath602 .    using the comparison bounds , we can estimate this quantity for a more general model .",
    "specifically , consider the model discussed at the end of subsection [ sec51 ] .",
    "namely , @xmath603 is binary and takes the form @xmath565 , where , under a probability measure @xmath0 , @xmath233 and @xmath424 are mutually independent , and @xmath485 are i.i.d .   with parameter @xmath563 .",
    "assuming that the normalized number of times @xmath243 when @xmath604 is bounded by a constant @xmath564 , the rnyi divergence term is bounded by @xmath605,\\ ] ] where as before , @xmath10 and @xmath12 are the respective laws of @xmath547 .",
    "we can now appeal to . using this inequality ( with the roles of @xmath0 and @xmath1 interchanged )",
    ", we have for arbitrary @xmath4 and denoting @xmath606 , @xmath607 using and in gives @xmath608\\\\ & \\qquad\\qquad -\\frac{a}{{\\alpha}-1}\\ln[p^{{\\alpha}}(1-p)^{1-{\\alpha}}+(1-p)^{{\\alpha}}p^{1-{\\alpha}}]\\big\\}.\\end{aligned}\\ ] ]",
    "here we prove the lpcb is the case where the support is a finite set ( see @xcite for the general setting ) .",
    "let @xmath130 be a finite set , let @xmath609 and @xmath610 be two probability distributions defined on it and let @xmath611 be a given function .",
    "* proof : *  when one does not have @xmath615 , the divergence term above equals @xmath616 by definition , and there is nothing to prove .",
    "hence assume @xmath615 .",
    "denote by @xmath617 , @xmath618 and @xmath619 the support of @xmath0 , @xmath1 and @xmath554 , respectively .",
    "let @xmath620 .",
    "using hlder s inequality with the exponents @xmath6 and @xmath621 and measure @xmath622 , @xmath623 thus @xmath624 for @xmath625 not in @xmath267 , @xmath626 , and because @xmath615 , also @xmath627 .",
    "thus , on the left - hand side , the summation can be performed over all of @xmath130 . as a result",
    ", taking logarithms and dividing by @xmath628 , using the definition of the divergence gives the inequality . to show the final assertion set @xmath629 for @xmath630 and @xmath57 off of that set .",
    "here , @xmath631 by assumption .",
    "substituting in gives equality by a direct calculation .",
    "@xmath120      a bound on the divergence between any two univariate gaussians is deduced from identity as follows . given @xmath632 , @xmath633 such that @xmath634 , and any @xmath4 and @xmath635 , @xmath636}\\right\\}\\right]\\nonumber\\\\ & \\quad=\\frac{1}{{\\alpha}({\\alpha}-1)}\\left\\{\\frac{{\\alpha}\\ln s}{2}-\\frac{\\ln[1+{\\alpha}(s-1)]}{2}+\\frac{{\\alpha}({\\alpha}-1)s\\xi^2}{2\\sigma^2[1+{\\alpha}(s-1)]}\\right\\}\\nonumber\\\\ & \\quad=\\frac{\\ln s}{2({\\alpha}-1)}- \\frac{\\ln[1+{\\alpha}(s-1)]}{2{\\alpha}({\\alpha}-1)}+\\frac{s\\xi^2}{2\\sigma^2[1+{\\alpha}(s-1)]}\\nonumber\\\\ & \\quad\\le\\frac{\\ln s}{2({\\alpha}-1)}- \\frac{\\ln[1+{\\alpha}(s-1)]}{2{\\alpha}({\\alpha}-1)}+\\frac{s{\\mathnormal{\\gamma}}^2}{2\\sigma^2[1+{\\alpha}(s-1)]}.\\end{aligned}\\ ] ]    let @xmath10 and @xmath12 denote the respective probability laws of @xmath164 . then @xmath637 \\\\ & = & \\frac{1}{{\\alpha}({\\alpha}-1)}\\ln\\sum_{{\\mbox{\\boldmath $ x$}}}\\int { { \\rm d}}{\\mbox{\\boldmath $ y$}}\\big(\\frac{q({\\mbox{\\boldmath $ y$}}|{\\mbox{\\boldmath $ x$}})}{p({\\mbox{\\boldmath $ y$}}|{\\mbox{\\boldmath $ x$}})}\\big)^{\\alpha}p({\\mbox{\\boldmath $ y$}}|{\\mbox{\\boldmath $ x$}})\\pi({\\mbox{\\boldmath $ x$ } } ) \\\\ & = & \\frac{1}{{\\alpha}({\\alpha}-1)}\\ln\\sum_{{\\mbox{\\boldmath $ x$}}}\\pi({\\mbox{\\boldmath $ x$}})\\left[\\int_{{{\\rm i\\!r}}^n}\\mbox{d}{\\mbox{\\boldmath $ y$}}(2\\pi\\sigma^2/s)^{-{\\alpha}n/2}(2\\pi\\sigma^2)^{-(1-{\\alpha})n/2}\\times\\right.\\nonumber\\\\ & & \\left.\\exp\\left\\{-\\frac{s{\\alpha}}{2\\sigma^2}\\sum_{t=1}^n(y_t - x_t)^2\\right\\}\\cdot \\exp\\left\\{-\\frac{1-{\\alpha}}{2\\sigma^2}\\sum_{t=1}^n[y_t - x_t - g_t(x^n , y^{t-1})]^2\\right\\}\\right]\\nonumber\\\\ & = & \\frac{n\\ln s}{2({\\alpha}-1)}-\\frac{n\\ln(2\\pi\\sigma^2)}{2{\\alpha}({\\alpha}-1)}+ \\frac{1}{{\\alpha}({\\alpha}-1)}\\ln\\sum_{{\\mbox{\\boldmath $ x$}}}\\pi({\\mbox{\\boldmath $ x$}})\\left[\\int_{{{\\rm i\\!r}}^n}\\mbox{d}{\\mbox{\\boldmath $ y$}}\\times\\right.\\nonumber\\\\ & & \\left.\\exp\\left\\{-\\sum_{t=1}^n\\left(\\frac{s{\\alpha}}{2\\sigma^2}[y_t - x_t]^2 + \\frac{1-{\\alpha}}{2\\sigma^2}[y_t - x_t - g_t(x^n , y^{t-1})]^2\\right)\\right\\}\\right]\\\\ & { \\stackrel{\\triangle}{=}}&\\frac{n\\ln s}{2({\\alpha}-1)}-\\frac{n\\ln(2\\pi\\sigma^2)}{2{\\alpha}({\\alpha}-1)}+ \\frac{1}{{\\alpha}({\\alpha}-1)}\\cdot z_n\\end{aligned}\\ ] ] let us focus on the expression of @xmath525 .",
    "for @xmath638 let @xmath639 . then @xmath640 ^ 2 + \\frac{1-{\\alpha}}{2\\sigma^2}[y_t - x_t - g_t(x^n , y^{t-1})]^2\\right)\\right\\}\\times\\right.\\nonumber\\\\ & & \\left.\\int_{{\\rm i\\!r}}\\mbox{d}y_n \\exp\\left\\{-\\left(\\frac{s{\\alpha}}{2\\sigma^2}(y_n - x_n)^2 + \\frac{1-{\\alpha}}{2\\sigma^2}[y_n - x_n - g_n(x^n , y^{n-1})]^2\\right)\\right\\}\\right]\\\\ & = & \\ln\\sum_{{\\mbox{\\boldmath $ x$}}}\\pi({\\mbox{\\boldmath $ x$}})\\left[\\int_{{{\\rm i\\!r}}^{n-1}}\\mbox{d}y^{n-1}\\times\\right.\\nonumber\\\\ & & \\left.\\exp\\left\\{-\\sum_{t=1}^{n-1}\\left(\\frac{s{\\alpha}}{2\\sigma^2}[y_t - x_t]^2 + \\frac{1-{\\alpha}}{2\\sigma^2}[y_t - x_t - g_t(x^n , y^{t-1})]^2\\right)\\right\\}\\times\\right.\\nonumber\\\\ & & \\left.\\sqrt{\\frac{2\\pi\\sigma^2}{1+{\\alpha}(s-1 ) } } \\exp\\left\\{\\frac{s{\\alpha}({\\alpha}-1)g_n^2(x^n , y^{n-1})}{2\\sigma^2[1+{\\alpha}(s-1)]}\\right\\}\\right]\\\\ & \\le&\\ln\\sum_{x^{n-1}}\\pi(x^{n-1})\\left[\\int_{{{\\rm i\\!r}}^{n-1}}\\mbox{d}y^{n-1}\\times\\right.\\nonumber\\\\ & & \\left.\\exp\\left\\{-\\sum_{t=1}^{n-1}\\left(\\frac{s{\\alpha}}{2\\sigma^2}[y_t - x_t]^2 + \\frac{1-{\\alpha}}{2\\sigma^2}[y_t - x_t - g_t(x^n , y^{t-1})]^2\\right)\\right\\}\\right]+\\nonumber\\\\ & & \\frac{1}{2}\\ln\\left[\\frac{2\\pi\\sigma^2}{1+{\\alpha}(s-1)}\\right]+ \\frac{s{\\alpha}({\\alpha}-1)\\max_{x^n , y^{n-1}}g_n^2(x^n , y^{n-1})}{2\\sigma^2[1+{\\alpha}(s-1)]}\\\\ & \\le&z_{n-1}+\\frac{1}{2}\\ln\\left[\\frac{2\\pi\\sigma^2}{1+{\\alpha}(s-1)}\\right ] + \\frac{s{\\alpha}({\\alpha}-1){\\mathnormal{\\gamma}}_n^2}{2\\sigma^2[1+{\\alpha}(s-1)]}.\\end{aligned}\\ ] ] from this recursion on @xmath525 , we have @xmath641 + \\frac{s{\\alpha}({\\alpha}-1)\\sum_{t=1}^n{\\mathnormal{\\gamma}}_t^2}{2\\sigma^2[1+{\\alpha}(s-1)]}\\\\ & \\le&\\frac{n}{2}\\ln\\left[\\frac{2\\pi\\sigma^2}{1+{\\alpha}(s-1)}\\right ] + \\frac{ns{\\alpha}({\\alpha}-1){\\mathnormal{\\gamma}}^2}{2\\sigma^2[1+{\\alpha}(s-1)]}.\\end{aligned}\\ ] ]",
    "therefore @xmath642 + \\frac{ns{\\alpha}({\\alpha}-1){\\mathnormal{\\gamma}}^2}{2\\sigma^2[1+{\\alpha}(s-1)]}\\right\\}\\nonumber\\\\ & = \\frac{n\\ln s}{2({\\alpha}-1)}- \\frac{n\\ln[1+{\\alpha}(s-1)]}{2{\\alpha}({\\alpha}-1 ) } + \\frac{ns{\\mathnormal{\\gamma}}^2}{2\\sigma^2[1+{\\alpha}(s-1)]}. \\label{57}\\end{aligned}\\ ] ] substituting in , using the bound @xmath643 for every @xmath172 , and finally optimizing over @xmath284 and @xmath6 , yields .",
    "@xmath120      as a reference , we will use the models @xmath644 , under which @xmath645 where @xmath235 are i.i.d .",
    "@xmath646 , independent of @xmath233 . here ,",
    "@xmath647 and @xmath648 are parameters .",
    "note that , for each of the models @xmath1 , @xmath172 is the optimal decoding metric .",
    "one has @xmath649 and @xmath650 , where @xmath651 is given by @xmath652 in order to calculate the rnyi divergence , we use the identity with the assignments : @xmath653 , @xmath654 , @xmath655 and @xmath656 , to get , under the assumption @xmath657 @xmath658^{n/2 } \\cdot\\exp\\left\\{-\\frac{{\\alpha}(1-{\\alpha})\\theta \\sum_t\\left[(1-\\phi)x_t+\\sum_{i=1}^kh_ix_{t - i}\\right]^2 } { { \\alpha}+2(1-{\\alpha})\\theta\\sigma^2}\\right\\}\\\\ & = & \\left[\\frac{2\\pi\\sigma^2}{{\\alpha}+2(1-{\\alpha})\\theta\\sigma^2}\\right]^{n/2 } \\cdot\\exp\\left\\{-\\frac{n{\\alpha}(1-{\\alpha})\\theta s [ ( 1-\\phi)^2 + 2(1-\\phi)r_1+r_2]}{{\\alpha}+2(1-{\\alpha})\\theta\\sigma^2}\\right\\}.\\end{aligned}\\ ] ] therefore @xmath659}{{\\alpha}+2(1-{\\alpha})\\theta\\sigma^2}\\right\\}\\right]\\nonumber\\\\ & = \\frac{1}{n{\\alpha}({\\alpha}-1)}\\ln\\left [ \\frac{(2\\theta\\sigma^2)^{n(1-\\alpha)/2}}{({\\alpha}+2(1-{\\alpha})\\theta\\sigma^2)^{n/2 } } \\cdot\\exp\\left\\{-\\frac{n{\\alpha}(1-{\\alpha})\\theta s [ ( 1-\\phi)^2 + 2(1-\\phi)r_1+r_2]}{{\\alpha}+2(1-{\\alpha})\\theta\\sigma^2}\\right\\}\\right]\\nonumber\\\\ & = -\\frac{\\ln(2\\theta\\sigma^2)}{2{\\alpha}}-\\frac{\\ln({\\alpha}+2(1-{\\alpha})\\theta\\sigma^2)}{2{\\alpha}({\\alpha}-1 ) } + \\frac{\\theta s[(1-\\phi)^2 + 2(1-\\phi)r_1+r_2]}{{\\alpha}+2(1-{\\alpha})\\theta\\sigma^2}.\\end{aligned}\\ ] ] for a code of rate zero operating over the reference channel @xmath1 , the best achievable exponent is known to be @xmath660 where we have used an extension of the zero  rate lower bound of @xcite , @xcite that applies to codes with a given composition @xmath214 ( see sections 2 and 4 of @xcite )",
    ". then we have a lower bound from , for @xmath4 , @xmath661 thus @xmath662}{{\\alpha}+2(1-{\\alpha})\\theta\\sigma^2}\\right],\\end{aligned}\\ ] ] where @xmath663 the maximization over @xmath285 is simple since the objective is quadratic in @xmath285 .",
    "in particular , the part that depends on @xmath285 is of the form @xmath664 , where @xmath665 and @xmath666 the maximum of @xmath664 is @xmath667 and our lower bound becomes , @xmath668}{{\\alpha}+2(1-{\\alpha})\\theta\\sigma^2 } \\\\ & \\qquad -\\frac{2\\theta { \\alpha}({\\alpha}-1)s(1+r_1)^2 } { 4({\\alpha}-1)^2\\theta^2\\sigma^4-{\\alpha}^2}.\\end{aligned}\\ ] ] it would be more convenient to define @xmath669 $ ] , @xmath670 , and to transform the parameter set from @xmath671 to @xmath672 . denoting @xmath673+\\ln[{\\alpha}(1-\\tau)]}{2{\\alpha}},\\ ] ] the expression is then @xmath674\\\\ & = { \\mathnormal{\\phi}}({\\alpha},\\tau)+\\frac{s}{2\\sigma^2}\\big[-\\frac{\\tau}{1-\\tau}(r_2-r_1 ^ 2 ) + \\frac{\\tau}{1+\\tau}(1+r_1)^2\\big ] , \\label{72}\\end{aligned}\\ ] ] to be maximized over @xmath675 . now",
    "the function @xmath676 is always non  positive ( the maximum over @xmath670 for a given @xmath219 is zero ) and it vanishes for @xmath677 ( hence this is the optimum choice of @xmath678 .",
    "thus , we are left with maximizing the second term of over @xmath679 .",
    "recall that @xmath344 , and that @xmath680 and @xmath348 .",
    "the maximum is given by @xmath681 ^ 2 & a\\le b ,      \\\\",
    "0 & \\text{otherwise . }",
    "\\end{cases}\\ ] ] this establishes the first inequality in .    in the case",
    "@xmath349 , the maximizing @xmath679 is given by @xmath682 .",
    "if we use this in the expression for the optimal @xmath6 and @xmath202 , we obtain that the optimal @xmath202 is @xmath683 and the optimal @xmath684 is given by @xmath685 . thus under the selected reference model ,",
    "@xmath686 by virtue of , this gives namely @xmath687 @xmath120      to work with the upper bound , we compute the rnyi divergence term , @xmath688.\\ ] ] we have @xmath689p({\\mbox{\\boldmath $ x$}})p({\\mbox{\\boldmath $ \\theta$}}),\\ ] ] where @xmath690 , and @xmath691p({\\mbox{\\boldmath $ x$}})p({\\mbox{\\boldmath $ \\theta$}}).\\ ] ] thus @xmath692\\\\ & = \\frac{1}{n({\\alpha}-1 ) } \\ln { \\mbox{\\boldmath $ e$}}_{p}\\big[\\exp\\big\\{\\frac{{\\alpha}}{2\\sigma^2}\\sum_{t=1}^n[\\theta_t^2x_t^2 - 2\\theta_tx_t(y_t - x_t ) ] \\big\\}\\big ] \\\\ & = \\frac{1}{n({\\alpha}-1 ) } \\ln { \\mbox{\\boldmath $ e$}}_{p}\\big[\\exp\\big\\{\\frac{{\\alpha}}{2\\sigma^2}\\sum_{t=1}^n[-\\theta_t^2x_t^2 -2\\theta_tx_tw_t]\\big\\}\\big ] \\\\ & = \\frac{1}{n({\\alpha}-1 ) } \\ln { \\mbox{\\boldmath $ e$}}_{p}\\big[\\exp\\big\\{\\frac{{\\alpha}({\\alpha}-1)}{2\\sigma^2 } \\sum_{t=1}^n\\theta_t^2x_t^2\\big\\}\\big ] \\\\ & \\le \\frac{1}{n({\\alpha}-1 ) } \\ln { \\mbox{\\boldmath $ e$}}_{p}\\big[\\exp\\big\\{\\frac{{\\alpha}({\\alpha}-1){{a}}^2}{2\\sigma^2 } \\sum_{t=1}^n\\theta_t^2\\big\\}\\big ] , \\label{24+}\\end{aligned}\\ ] ] where in the last line we have used the assumption @xmath417 .",
    "we have assumed that @xmath202 is a stationary , zero - mean gaussian process .",
    "thus the limit @xmath693 can be computed using szego s theorem ( see @xcite ) . to this end ,",
    "note first that the exponential moment is given by @xmath694 where @xmath695 is the covariance matrix of @xmath696 , @xmath499 .",
    "next , if @xmath697 is a sequence of hermitian toeplitz matrices of the form @xmath698 $ ] , where @xmath699 are absolutely summable , and their spectral density @xmath700 , @xmath701 , satisfies @xmath702 , @xmath701 , one has by theorem 13 of @xcite , that @xmath703 recall that we assume that @xmath704 are absolutely summable . then , with @xmath360 , we obtain the bound @xmath705 assuming @xmath361 . as for",
    "the lower bound , a calculation similar to that of  gives @xmath706\\\\    & \\le \\frac{1}{n{\\alpha}}\\ln e_q    \\big[\\exp\\frac{{\\alpha}({\\alpha}-1){{a}}^2}{2\\sigma^2}\\sum_{t=1}^n\\theta_t^2\\big].\\end{aligned}\\ ] ] using the same considerations as before gives @xmath707 where again we assume that @xmath708 satisfies @xmath361 .",
    "our estimates of the rnyi divergence are based on large deviation results from @xcite .",
    "we first note that the divergence is given by @xmath709\\\\    & = \\frac{1}{{\\alpha}({\\alpha}-1)}\\ln { \\mbox{\\boldmath $ e$}}_{p_t }    \\exp\\big[-\\frac{{\\alpha}}{\\sigma}\\int_0^t\\theta_sx_s{{\\rm d}}w_s+\\frac{{\\alpha}}{2\\sigma^2 }    \\int_0^t(\\theta_sx_s)^2{{\\rm d}}s\\big].\\end{aligned}\\ ] ] note by and that @xmath710 and so @xmath711 thus @xmath712.\\end{aligned}\\ ] ] under @xmath427 , conditioned on @xmath713)$ ] , the integral @xmath714 is a gaussian random variable with mean zero and variance @xmath715 .",
    "thus @xmath716.\\end{aligned}\\ ] ] a similar calculation for @xmath717\\\\    & = \\frac{1}{{\\alpha}({\\alpha}-1)}\\ln { \\mbox{\\boldmath $ e$}}_{q_t }    \\exp\\big[\\frac{{\\alpha}}{\\sigma}\\int_0^t\\theta_sx_s{{\\rm d}}w_s-\\frac{{\\alpha}}{2\\sigma^2 }    \\int_0^t(\\theta_sx_s)^2{{\\rm d}}s\\big]\\end{aligned}\\ ] ] gives @xmath718.\\end{aligned}\\ ] ] as a result , the two divergences are equal , and using @xmath417 , we can bound them as follows : @xmath719.\\end{aligned}\\ ] ] it is shown in lemma 3 of @xcite that @xmath720    = -\\frac{1}{4\\pi}\\int_{-{\\infty}}^{{\\infty}}\\ln[1 - 4\\pi c{\\mathnormal{\\sigma}}_\\theta({\\omega})]{{\\rm d}}{\\omega},\\ ] ] provided that @xmath721 .",
    "specifically , holds since we have assumed that @xmath722 .",
    "moreover , with @xmath438 , for any @xmath4 such that @xmath723 , we obtain from and ( by a derivation analogous to that of ) @xmath724{{\\rm d}}{\\omega}\\ ] ] @xmath725{{\\rm d}}{\\omega}.\\ ] ] @xmath120      to prove ( [ app ] ) , we follow the main steps of @xcite , with a little twist since in our case the distortion measure ( which is quadratic ) is unbounded .",
    "consider an arbitrary rate ",
    "distortion code @xmath726 , @xmath127 , @xmath171 being the coding rate .",
    "let us denote the event under discussion by @xmath727 where @xmath728 is the @xmath243th component of the reproduction word @xmath729 .",
    "let @xmath730 denote the rate ",
    "distortion function of the gaussian memoryless source @xmath731 with variance @xmath732 .",
    "we first show that under the assumption that @xmath733 , there exists a constant @xmath734 such that @xmath735 for all sufficiently large @xmath22 .",
    "let @xmath736 where @xmath737 denotes expectation under @xmath731 .",
    "let @xmath738 denote the optimum distortion of @xmath731 at rate @xmath171 .",
    "then , obviously , @xmath739 where the first inequality is by our assumption .",
    "the equality is by definition of @xmath740 and the second inequality is due to the fact that @xmath741 may not be optimal for @xmath731 .",
    "since @xmath742 is monotonically decreasing , then @xmath743 now , let us denote @xmath744 and let @xmath745 be an arbitrary large distortion level . then",
    ", assuming , without loss of generality , that the zero  vector belongs to @xmath741 , and so , @xmath746 , we have : @xmath747\\cdot d+\\int_{{\\mbox{\\boldmath $ y$}}:~\\delta({\\mbox{\\boldmath $ y$}})\\ge d}\\tilde{g}({\\mbox{\\boldmath $ y$}})\\delta({\\mbox{\\boldmath $ y$}})\\mbox{d}{\\mbox{\\boldmath $ y$}}\\nonumber\\\\ & = & [ 1-\\tilde{g}({{\\cal e}})]\\cdot d+\\int_{{\\mbox{\\boldmath $ y$}}:~d\\le \\delta({\\mbox{\\boldmath $ y$}})\\le d_0}\\tilde{g}({\\mbox{\\boldmath $ y$}})\\delta({\\mbox{\\boldmath $ y$}})\\mbox{d}{\\mbox{\\boldmath $ y$}}+ + \\int_{{\\mbox{\\boldmath $ y$}}:~\\delta({\\mbox{\\boldmath $ y$}})\\ge d_0}\\tilde{g}({\\mbox{\\boldmath $ y$}})\\delta({\\mbox{\\boldmath $ y$}})\\mbox{d}{\\mbox{\\boldmath $ y$}}\\nonumber\\\\ & \\le&[1-\\tilde{g}({{\\cal e}})]\\cdot d+\\tilde{g}({{\\cal e}})\\cdot d_0+\\frac{1}{n}\\int_{{\\mbox{\\boldmath $ y$}}:~\\|{\\mbox{\\boldmath $ y$}}\\|^2\\ge nd_0 } \\tilde{g}({\\mbox{\\boldmath $ y$}})\\cdot\\|{\\mbox{\\boldmath $ y$}}\\|^2\\mbox{d}{\\mbox{\\boldmath $ y$}}\\end{aligned}\\ ] ] now , the last term , which is @xmath748 is easily shown is the negative derivative of @xmath749 w.r.t .",
    "@xmath284 . ] to decrease exponentially provided that @xmath750 .",
    "thus , we have @xmath751 which is positive for @xmath22 large enough . for example , beyond a certain @xmath752 , it exceeds @xmath753 , which we take to be @xmath754 .",
    "now , for a given @xmath755 , let @xmath756 .",
    "then , by the weak law of large numbers , @xmath757 for all large @xmath22 .",
    "thus , @xmath758}\\mbox{d}{\\mbox{\\boldmath $ y$}}\\\\ & \\ge&\\tilde{g}({{\\cal e}}\\cap{{\\cal t}}_\\epsilon)\\cdot\\exp\\{-n[d(\\tilde{g}\\|g)+\\epsilon]\\}\\\\ & \\ge&[\\tilde{g}({{\\cal e}})-\\tilde{g}({{\\cal t}}_\\epsilon^c)]\\cdot\\exp\\{-n[d(\\tilde{g}\\|g)+\\epsilon]\\}\\\\ & \\ge&[\\alpha(\\tilde{\\sigma}^2,d , r)-\\frac{1}{2}\\alpha(\\tilde{\\sigma}^2,d , r ) ] \\cdot\\exp\\{-n[d(\\tilde{g}\\|g)+\\epsilon]\\}\\\\ & = & \\frac{1}{2}\\alpha(\\tilde{\\sigma}^2,d , r ) \\cdot\\exp\\{-n[d(\\tilde{g}\\|g)+\\epsilon]\\}.\\end{aligned}\\ ] ] since this is true for all @xmath732 with @xmath759 , the tightest bound is obtained by minimizing @xmath760,\\ ] ] in the range @xmath761 , which is attained at @xmath762 , yielding the following upper bound on the exponent : @xmath763={\\mathnormal{\\phi}}[r - r_g(d)].\\ ] ] @xmath120"
  ],
  "abstract_text": [
    "<S> a well - known technique in estimating probabilities of rare events in general and in information theory in particular ( used , e.g. , in the sphere  </S>",
    "<S> packing bound ) , is that of finding a reference probability measure under which the event of interest has probability of order one and estimating the probability in question by means of the kullback - leibler divergence . a method has recently been proposed in @xcite , that can be viewed as an extension of this idea in which the probability under the reference measure may itself be decaying exponentially , and the rnyi divergence is used instead . the purpose of this paper is to demonstrate the usefulness of this approach in various information  theoretic settings . for the problem of channel coding </S>",
    "<S> , we provide a general methodology for obtaining matched , mismatched and robust error exponent bounds , as well as new results in a variety of particular channel models . </S>",
    "<S> other applications we address include rate - distortion coding and the problem of guessing .    * index terms : * change - of - measure , error exponent , mismatch , rnyi divergence . </S>"
  ]
}