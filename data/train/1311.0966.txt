{
  "article_text": [
    "machine learning algorithms based on stochastic neural network models such as and deep networks are currently the state - of - the - art in several practical tasks @xcite .",
    "the training of these models requires significant computational resources , and is often carried out using power - hungry hardware such as large clusters @xcite or graphics processing units @xcite .",
    "their implementation in dedicated hardware platforms can therefore be very appealing from the perspectives of power dissipation and of scalability .",
    "neuromorphic systems exploit the physics of the device to emulate very densely the performance of biological neurons in a real - time fashion , while dissipating very low power  @xcite .",
    "the distributed structure of suggests that neuromorphic circuits and systems can become ideal candidates for such a platform .",
    "furthermore , the communication between neuromorphic components is often mediated using asynchronous address - events @xcite enabling them to be interfaced with event - based sensors @xcite for embedded applications , and to be implemented in a very scalable fashion @xcite .",
    "currently , and the algorithms used to train them are designed to operate efficiently on digital processors , using batch , discrete - time , iterative updates based on exact arithmetic calculations . however , unlike digital processors , neuromorphic systems compute through the continuous - time dynamics of their components , which are typically neurons @xcite , rendering the transfer of such algorithms on such platforms a non - trivial task .",
    "we propose here a method to construct using neuron models and to train them using an online , event - driven adaptation of the algorithm .",
    "we take inspiration from computational neuroscience to identify an efficient neural mechanism for sampling from the underlying probability distribution of the .",
    "neuroscientists argue that brains deal with uncertainty in their environments by encoding and combining probabilities optimally @xcite , and that such computations are at the core of cognitive function @xcite . while many mechanistic theories of how the brain might achieve this exist , a recent _ neural sampling _ theory postulates that the spiking activity of the neurons encodes samples of an underlying probability distribution @xcite .",
    "the advantage for a neural substrate in using such a strategy over the alternative one , in which neurons encode probabilities , is that it requires exponentially fewer neurons .",
    "furthermore , abstract model neurons consistent with the behavior of biological neurons can implement sampling @xcite , and sampled in this way can be efficiently trained using , with almost no loss in performance @xcite .",
    "we identify the conditions under which a dynamical system consisting of neurons performs neural sampling .",
    "these conditions are compatible with neuromorphic implementations of neurons , suggesting that they can achieve similar performance .",
    "the calibration procedure necessary for configuring the parameters of the spiking neural network is based on firing rate measurements , and so is easy to realize in software and in hardware platforms .    in standard ,",
    "weight updates are computed on the basis of alternating , feed - forward propagation of activities @xcite . in a neuromorphic implementation",
    ", this translates to reprogramming the network connections and resetting its state variables at every step of the training . as a consequence , it requires two distinct dynamical systems : one for normal operation ( _ i.e. _ testing ) , the other for training , which is highly impractical . to overcome this problem , we train the neural using an online adaptation of . we exploit the recurrent structure of the network to mimic the discrete `` construction '' and `` reconstruction '' steps of in a spike - driven fashion , and to carry out the weight updates . each sample ( spike ) of each random variable ( neuron ) causes synaptic weights to be updated .",
    "we show that , over longer periods of time , these microscopic updates behave like a macroscopic weight update .",
    "compared to standard , no additional programming overhead is required during the training steps , and both testing and training take place in the same dynamical system .    because are generative models , they can act simultaneously as classifiers , content - addressable memories , and carry out probabilistic inference .",
    "we demonstrate these features in a mnist hand - written digit task @xcite , using an network consisting of @xmath0  visible  neurons and @xmath1 `` hidden '' neurons .",
    "the spiking neural network was able to learn a generative model capable of recognition performances with accuracies up to @xmath2 , which is close to the performance obtained using standard and gibbs sampling , @xmath3 .",
    "we describe here the conditions under which a dynamical system composed of neurons can perform neural sampling . in",
    "has been proven that abstract neuron models consistent with the behavior of biological spiking neurons can perform sampling of a boltzmann distribution @xcite .",
    "two conditions are sufficient for this .",
    "first , the instantaneous firing rate of the neuron verifies : @xmath4 with @xmath5 proportional to @xmath6 , where @xmath7 is the membrane potential and @xmath8 is an absolute refractory period during which the neuron can not fire .",
    "@xmath9 describes the neuron s instantaneous firing rate as a function of @xmath7 at time @xmath10 , given that the last spike occurred at @xmath11 .",
    "the average firing rate of this neuron model for stationary @xmath7 is the sigmoid function : @xmath12 second , the membrane potential of neuron @xmath13 is equal to the linear sum of its inputs : @xmath14 where @xmath15 is a constant bias , and @xmath16 represents the pre - synaptic spike train produced by neuron @xmath17 and is set to 1 for a duration @xmath18 after the neuron has spiked .",
    "the terms @xmath19 are identified with the time course of the , _",
    "i.e. _ the response of the membrane potential to a pre - synaptic spike .",
    "the two conditions above define a neuron model , to which we refer as the `` abstract neuron model '' .",
    "the network then samples from a boltzmann distribution : @xmath20 where @xmath21 is the partition function , and @xmath22 can be interpreted as an energy function @xcite .",
    "an important fact of the abstract neuron model is that , according to the dynamics of @xmath16 , the are `` rectangular '' and non - additive .",
    "the implementation of a large number of synapses producing such is very difficult to realize in hardware , when compared to first - order linear filters that result in `` alpha''-shaped @xcite .",
    "this is because , in the latter model , the synaptic dynamics are linear , such that a single hardware synapse can be used to generate the same current that would be generated by an arbitrary number of synapses ( see also next section ) . as a consequence , we will use alpha - shaped instead of rectangular in our models .",
    "the use of the alpha over the rectangular is the major source of degradation in sampling performance , as we will discuss in sec .",
    "[ sec : kldivergence ] .",
    "[ [ stochastic - neurons . ] ] stochastic neurons .",
    "+ + + + + + + + + + + + + + + + + + +    a neuron whose instantaneous firing rate is consistent with eq .",
    "( [ eq : refr_exp_hazard ] ) can perform neural sampling .",
    "( [ eq : refr_exp_hazard ] ) is a generalization of the poisson process to the case when the firing probability depends on the time of the last spike ( _ i.e. _ it is a renewal process ) , and so can be verified only if the neuron fires stochastically @xcite .",
    "stochasticity in neurons can be obtained through several mechanisms , such as a noisy reset potential , noisy firing threshold , or noise injection @xcite .",
    "the first two mechanisms necessitate stochasticity in the neuron s parameters , and therefore may require specialized circuitry . but noise injection in the form of background poisson spike trains requires only synapse circuits , which are present in many neuromorphic implementation of spiking neurons @xcite .",
    "furthermore , poisson spike trains can be generated self - consistently in balanced excitatory - inhibitory networks @xcite , or using finite - size effects and neural mismatch @xcite .",
    "we show that the abstract neuron model in eq .",
    "( [ eq : refr_exp_hazard ] ) can be realized in a simple dynamical system consisting of leaky neurons with noisy currents .",
    "the neuron s membrane potential below firing threshold @xmath23 is governed by the following differential equation : @xmath24 where @xmath25 is a membrane capacitance , @xmath26 is the membrane potential of neuron @xmath13 , @xmath27 is a leak conductance , @xmath28 is a white noise term of amplitude @xmath29 ( which can for example be generated by background activity ) , @xmath30 its synaptic current and @xmath23 is the neuron s firing threshold . when the membrane potential reaches @xmath23 , an action potential is elicited . after a spike",
    "is generated , the membrane potential is clamped to the reset potential @xmath31 for a refractory period @xmath8 .    in the case of the neural , the currents @xmath30 depend on the layer the neuron is situated in . for a neuron @xmath13 in layer @xmath32 @xmath33 where @xmath34 is a current representing the data ( _ i.e. _ the external input ) , @xmath35 is the feedback from the hidden layer activity and the bias , and the @xmath36 s are the respective synaptic weights .    for a neuron @xmath17 in layer @xmath37 , @xmath38 where @xmath39 is the feedback from the visible layer , and @xmath40 and @xmath41 are poisson spike trains implementing the bias .",
    "the dynamics of @xmath39 and @xmath35 correspond to a first - order linear filter , so each incoming spike results in that rise and decay exponentially ( _ i.e. _ alpha- ) @xcite .",
    "can this neuron neuron verify the conditions required for neural sampling ?",
    "the membrane potential is already assumed to be equal to the sum of the as required by neural sampling .",
    "so to answer the above question we only need to verify whether eq .  ( [ eq : refr_exp_hazard ] ) holds . eq .",
    "( [ eq : clif ] ) is a langevin equation which can be analyzed using the fokker - planck equation @xcite .",
    "the solution to this equation provides the neuron s input / output response , _",
    "i.e. _ its transfer curve ( for a review , see @xcite ) : @xmath42 where @xmath43 is the error function ( the integral of the normal distribution ) , @xmath44 is the stationary value of the membrane potential when injected by a constant current @xmath45 , @xmath31 is the reset voltage , and @xmath46 .    according to eq .",
    "( [ eq : refr_exp_hazard_tau ] ) , the condition for neural sampling requires that the average firing rate of the neuron to be the sigmoid function .",
    "although the transfer curve of the noisy neuron eq .",
    "( [ eq : tf ] ) is not equal to the sigmoid function , it was previously shown that with an appropriate choice of parameters , the shape of this curve can be very similar to it @xcite . we observe that , for a given refractory period @xmath8 , the smaller ratio @xmath47 in eq .",
    "( [ eq : clif ] ) , the better the transfer curve resembles a sigmoid function ( fig .",
    "[ fig : tf ] ) .",
    "transfer curve of a leaky neuron for three different parameter sets where @xmath48 , and @xmath49 $ ] ( dashed grey ) . in this plot",
    ", @xmath50 is varied to produce different ratios @xmath47 .",
    "the three plots above shows that the fit with the sigmoid function ( solid black ) improves as the ratio decreases.,title=\"fig : \" ]   transfer curve of a leaky neuron for three different parameter sets where @xmath48 , and @xmath49 $ ] ( dashed grey ) . in this plot ,",
    "@xmath50 is varied to produce different ratios @xmath47 .",
    "the three plots above shows that the fit with the sigmoid function ( solid black ) improves as the ratio decreases.,title=\"fig : \" ] +   transfer curve of a leaky neuron for three different parameter sets where @xmath48 , and @xmath49 $ ] ( dashed grey ) . in this plot ,",
    "@xmath50 is varied to produce different ratios @xmath47 .",
    "the three plots above shows that the fit with the sigmoid function ( solid black ) improves as the ratio decreases.,title=\"fig : \" ]    with a small @xmath47 , the transfer function of a neuron can be fitted to @xmath51 where @xmath52 and @xmath53 are the parameters to be fitted .",
    "the choice of the neuron model described in eq .",
    "( [ eq : clif ] ) is not critical for neural sampling : a relationship that is qualitatively similar to eq .",
    "( [ eq : tf ] ) holds for neurons with a rigid ( reflective ) lower boundary @xcite which is common in neurons , and for neurons with conductance - based synapses @xcite .",
    "this result also shows that synaptic weights @xmath54 , @xmath55 , which have the units of charge are related to the weights @xmath56 by a factor @xmath57 . to relate the neural activity to the boltzmann distribution , eq .",
    "( [ eq : boltzmann_distr ] ) , each neuron is associated to a binary random variable which is assumed to take the value @xmath58 for a duration @xmath18 after the neuron has spiked , and zero otherwise , similarly to @xcite .",
    "the relation between the random vector and the neurons spiking activity is illustrated in fig .",
    "[ fig : example_rbm10 ] .",
    "[ [ calibration - protocol . ] ] calibration protocol .",
    "+ + + + + + + + + + + + + + + + + + + + +    in order to transfer the parameters from the probability distribution eq .",
    "( [ eq : boltzmann_distr ] ) to those of the neurons , the parameters @xmath53 , @xmath52 in eq .",
    "( [ eq : tf_fit ] ) need to be fitted .",
    "an estimate of a neuron s transfer function can be obtained by computing its spike rate when injected with different values of constant inputs @xmath45 .",
    "the refractory period @xmath18 is the inverse of the maximum firing rate of the neuron , so it can be easily measured by measuring the spike rate for very high input current @xmath45 .",
    "once @xmath18 is known , the parameter estimation can be cast into a simple linear regression problem by fitting @xmath59 with @xmath60 .",
    "[ fig : calibration ] shows the transfer curve when @xmath61{ms}$ ] , which is approximately exponential in agreement with eq .",
    "( [ eq : refr_exp_hazard ] ) .",
    "the shape of the transfer curse is strongly dependent on the noise amplitude . in the absence of noise , the transfer curve is a sharp threshold function , which softens as the amplitude of the noise is increased ( fig .",
    "[ fig : tf ] ) . as a result ,",
    "both parameters @xmath53 and @xmath52 are dependent on the variance of the input currents from other neurons @xmath62 . since @xmath63 ,",
    "the effect of the fluctuations on the network is similar to scaling the synaptic weights and the biases which can be problematic .",
    "however , by selecting a large enough noise amplitude @xmath29 and a slow enough input synapse time constant , the fluctuations due to the background input are much larger than the fluctuations due to the inputs . in this case ,",
    "@xmath52 and @xmath53 remain approximately constant during the sampling .",
    "neural mismatch can cause @xmath52 and @xmath53 to differ from neuron to neuron . from eq .",
    "( [ eq : tf_fit ] ) and the linearity of the postsynaptic currents @xmath62 in the weights , it is clear that this type of mismatch can be compensated by scaling the synaptic weights and biases accordingly .",
    "the calibration of the parameters @xmath53 and @xmath52 quantitatively relate the spiking neural network s parameters to the .",
    "in practice , this calibration step is only necessary for mapping pre - trained parameters of the onto the spiking neural network .",
    "although we estimated the parameters of software simulated neurons , parameter estimation based on firing rate measurements were shown to be an accurate and reliable method for neurons as well @xcite",
    ".    transfer function of neurons driven by background white noise ( eq .  ( [ eq : clif ] ) ) .",
    "we measure the firing rate of the neuron as a function of a constant current injection to estimate @xmath64 , where for constant @xmath65 , @xmath66 .",
    "( top ) the transfer function of noisy neurons in the absence of refractory period ( @xmath67 , circles ) .",
    "we observe that @xmath68 is approximately exponential over a wide range of inputs , and therefore compatible with neural sampling .",
    "crosses show the transfer curve of neurons implementing the abstract neuron eq .",
    "( [ eq : refr_exp_hazard ] ) , exactly . ( bottom ) with an absolute refractory period the transfer function approximates the sigmoid function .",
    "the firing rate saturates at @xmath69{hz}$ ] due to the refractory period chosen for the neuron . .",
    "]    [ cols=\"<,<,^,^ \" , ]      we train the to learn a generative model of the mnist handwritten digits using event - driven ( see sec .  [",
    "sec : nnarch ] for details ) .",
    "for training , @xmath70 digits selected randomly from a training set consisting of 10000 digits were presented in sequence , with an equal number of samples for each digit .",
    "the raster plots in fig .",
    "[ fig : trained_rbm ] show the spiking activity of each layer before and after learning for epochs of duration @xmath71{ms}$ ] .",
    "the top panel shows the population - averaged weight . after training ,",
    "the sum of the upwards and downward excursions of the average weight is much smaller than before training , because the learning is near convergence .",
    "the second panel shows the value of the modulatory signal @xmath72 .",
    "the third panel shows the input current ( @xmath73 ) and the current caused by the recurrent couplings ( @xmath74 ) .    two methods to estimate the overall classification performance of the neural can be used .",
    "the first is by neural sampling : the visible layer is clamped to the digit only , and the network is run for @xmath75 .",
    "the known label is then compared with the positions of the group of class neurons that had the highest population rate .",
    "the second method is by minimizing free - energy : the neural parameters are extracted , and for each data sample , the class neurons with the lowest free - energy ( see materials and methods ) is compared to the known label . in both cases ,",
    "recognition was tested for @xmath76 data samples that were not used during the training .",
    "the results are summarized in fig .",
    "[ tab : performance ] .    as a reference we provide the best performance achieved using the standard and one unit per class label ( @xmath77 ) ( fig .",
    "[ tab : performance ] , table row 1 ) , @xmath3 . by mapping the learned parameters to the neural the recognition accuracy reached @xmath78 .    when training a neural of neurons using event - driven , the recognition result was @xmath2 ( fig .",
    "[ tab : performance ] , table row 2 ) .",
    "the performance of this obtained by minimizing its free - energy was @xmath79 .",
    "the learned parameters performed well for classification using the free - energy calculation which suggests that the network learned a model that is consistent with the mathematical description of the .    in an energy - based model like",
    "the the free - energy minimization should give the upper bound on the discrimination performance @xcite .",
    "for this reason , the fact that the recognition accuracy is higher when sampling as opposed to using the free - energy method may appear puzzling . however , this is possible because the neural does not exactly sample from the boltzmann distribution , as explained in sec .",
    "[ sec : kldivergence ] .",
    "this suggests that event - driven compensates for the discrepancy between the distribution sampled by the neural and the boltzmann distribution , by learning a model that is tailored to the spiking neural network .",
    "excessively long training durations can be impractical for real - time neuromorphic systems .",
    "fortunately , the learning using event - driven is fast : compared to the off - line training ( @xmath80 presentations , in mini - batches of @xmath81 samples ) the event - driven training succeeded with a smaller number of data presentations ( @xmath70 ) , which corresponded to @xmath82{s}$ ] of simulated time .",
    "this suggests that the training durations are achievable for real - time neuromorphic systems .",
    "[ [ the - choice - of - the - number - of - class - neurons - n_c . ] ] the choice of the number of class neurons @xmath83 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    event - driven underperformed in the case of @xmath58 neuron per class label ( @xmath77 ) , which is the common choice for standard and gibbs sampling .",
    "this is because a single neuron firing at its maximum rate of @xmath69{hz}$ ] can not efficiently drive the rest of the network without tending to induce spike - to - spike correlations ( _ e.g. _ synchrony ) , which is incompatible with the assumptions made for sampling with neurons and event - driven . as a consequence , the generative properties of the neural degrade .",
    "this problem is avoided by using several neurons per class label ( in our case four neurons per class label ) because the synaptic weight can be much lower to achieve the same effect , resulting in smaller spike - to - spike correlations .",
    "we test the neural as a generative model of the mnist dataset of handwritten digits , using parameters obtained by running the event - driven .    in the context of the handwritten digit task ,",
    "the s generative property enables it to classify digits , generate them , and infer a digit by combining partial evidence .",
    "these features are clearly illustrated in the following experiment ( fig .",
    "[ fig : classification_reconstruction_inference ] ) .",
    "first the digit @xmath84 is presented ( _ i.e. _ layer @xmath85 is driven by layer @xmath86 ) and the correct class label in @xmath87 activated .",
    "second , the neurons associated to class label @xmath88 are clamped , and the network generated its learned version of the digit .",
    "third , the right - half part of a digit @xmath89 is presented , and the class neurons are stimulated such that only @xmath84 or @xmath90 are able to activate ( the other class neurons are inhibited , indicated by the gray shading ) . because the stimulus is inconsistent with @xmath90 , the network settled to @xmath84 and reconstructed the left part of the digit .    the latter part of the experiment illustrates the integration of information between several partially specified cues , which is of interest for solving sensorimotor transformation or multi - modal sensory cue integration problems @xcite .",
    "this feature has been used for auditory - visual sensory fusion in a spiking model @xcite . there , the authors trained a with visual and auditory data , which learned to associate the two sensory modalities , very similarly to how class labels and visual data are associated in our architecture .",
    "their network was able to resolve a similar ambiguity as in our experiment in fig .",
    "[ fig : classification_reconstruction_inference ] , but using auditory inputs instead of a class label .    in the digit generation mode ,",
    "the trained network had a tendency to be globally bistable , whereby the layer @xmath85 completely deactivated layer @xmath37 .",
    "since all the interactions between @xmath85 and @xmath87 take place through the hidden layer , @xmath87 could not reconstruct the digit . to avoid this , we added two populations of neurons that were wired to layers @xmath32 and @xmath37 , respectively .",
    "the parameters of these neurons and their couplings were tuned such that each layer was strongly excited when it s average firing rate fell below @xmath91{hz}$ ] .",
    "[ [ neural - parameters - with - finite - precision . ] ] neural parameters with finite precision .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in hardware systems , the parameters related to the weights and biases can not be set with floating - point precision , as can be done in a digital computer . in current neuromorphic implementations",
    "the synaptic weights can be configured at precisions of about @xmath89 bits @xcite .",
    "we characterize the impact of finite - precision synaptic weights on performance by discretizing the weight and bias parameters to @xmath89 bits and @xmath88 bits .",
    "the set of possible weights were spaced uniformly in the interval @xmath92 , where @xmath93 are the mean and the standard deviation of the parameters across the network , respectively .",
    "the classification performance of mnist digits degraded gracefully . in the @xmath89 bit case , it degrades only slightly to @xmath94 , but in the case of @xmath88 bits , it degrades more substantially to @xmath95 . in both cases ,",
    "the still retains its discriminative power , which is encouraging for implementation in hardware neuromorphic systems .",
    "neuromorphic systems are promising alternatives for large - scale implementations of and deep networks , but the common procedure used to train such networks , , involves iterative , discrete - time updates that do not straightforwardly map on a neural substrate .",
    "we solve this problem in the context of the with a spiking neural network model that uses the recurrent network dynamics to compute these updates in a continuous - time fashion .",
    "we argue that the recurrent activity coupled with dynamics implements an event - driven variant of . using event - driven",
    ", the network connectivity remains unchanged during training and testing , enabling the system to learn in an on - line fashion , while being able to carry out functionally relevant tasks such as recognition , data generation and cue integration .",
    "the algorithm can be used to learn the parameters of probability distributions other than the boltzmann distribution ( even those without any symmetry assumptions ) .",
    "our choice for the , whose underlying probability distribution is a special case of the boltzmann distribution , is motivated by the following facts : they are universal approximators of discrete distributions @xcite ; the conditions under which a spiking neural circuit can naturally perform mcmc sampling of a boltzmann distribution were previously studied @xcite ; and form the building blocks of many deep learning models such as , which achieve state - of - the - art performance in many machine learning tasks @xcite .",
    "the ability to implement with spiking neurons and train then using event - based paves the way towards on - line training of of spiking neurons @xcite .",
    "we chose the mnist handwritten digit task as a benchmark for testing our model .",
    "when the was trained with standard , it could recognize up to 926 out of 1000 of out - of - training samples .",
    "the mnist handwritten digits recognition task was previously shown in a digital neuromorphic chip @xcite , which performed at @xmath96 accuracy , and in a software simulated visual cortex model @xcite .",
    "however , both implementations were configured using weights trained off - line .",
    "a recent article showed the mapping of off - line trained onto spiking neural network @xcite .",
    "their results demonstrated hand - written digit recognition using neuromorphic event - based sensors as a source of input spikes .",
    "their performance reached up to @xmath97 using leaky neurons .",
    "the use of off - line combined with an additional layer explains to a large extent their better performance compared to ours .",
    "our work extends @xcite by demonstrating an on - line training using synaptic plasticity , testing its robustness to finite weight precision , and providing an interpretation of spiking activity in terms of neural sampling .    to achieve the computations necessary for sampling from the",
    ", we have used the neural sampling framework @xcite , where each spike is interpreted as a sample of an underlying probability distribution .",
    "@xcite proved that abstract neuron models consistent with the behavior of biological spiking neurons can perform , and have applied it to a basic learning task in a fully visible boltzmann machine .",
    "we extended the neural sampling framework in three ways : first , we identified the conditions under which a dynamical system consisting of neurons can perform neural sampling ; second , we verified that the sampling of was robust to finite - precision parameters ; third , we demonstrated learning in a boltzmann machine with hidden units using synapses .    in the neural sampling framework , neurons behave stochastically .",
    "this behavior can be achieved in neurons using noisy input currents , created by a poisson spike train .",
    "spike trains with poisson - like statistics can be generated with no additional source of noise , for example by the following mechanisms : balanced excitatory and inhibitory connections @xcite , finite - size effects in a large network , and neural mismatch @xcite .",
    "the latter mechanism is particularly appealing , because it benefits from fabrication mismatch and operating noise inherent to neuromorphic implementations  @xcite .",
    "other groups have also proposed to use neuron models for computing the boltzmann distribution . @xcite",
    "have shown that noisy neurons activation function is approximately sigmoidal as required by the boltzmann machine , and have devised a scheme whereby a global inhibitory rhythm drives the network to generate samples of the boltzmann distribution . @xcite",
    "have demonstrated a deep belief network of neurons that was trained off - line , using standard and tested it using the mnist database . independently and simultaneously to this work , @xcite demonstrated that conductance - based neurons in a noisy environment are compatible with neural sampling as described in @xcite .",
    "similarly , @xcite find that the choice of non - rectangular and the approximations made by the neurons are not critical to the performance of the neural sampler .",
    "our work extends all of those above by providing an online , -based learning rule to train sampled using neurons .",
    "[ [ applicability - to - neuromorphic - hardware . ] ] applicability to neuromorphic hardware .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    neuromorphic systems are sensible to fabrication mismatch and operating noise .",
    "fortunately , the mismatch in the synaptic weights and the activation function parameters @xmath53 and @xmath52 are not an issue if the biases and the weights are learned , and the functionality of the is robust to small variations in the weights caused by discretization .",
    "these two findings are encouraging for neuromorphic implementations of .",
    "however , at least two conceptual problems of the presented architecture must be solved in order to implement such systems on a large - scale .",
    "first , the symmetry condition required by the does not necessarily hold . in a neuromorphic device",
    ", the symmetry condition is impossible to guarantee if the synapse weights are stored locally at each neuron . sharing one synapse circuit per pair of neurons can solve this problem",
    ". this may be impractical due to the very large number of synapse circuits in the network , but may be less problematic when using ( also called _ memristors _ ) crossbar arrays to emulate synapses @xcite .",
    "are a new class of nanoscale devices whose current - voltage relationship depends on the history of other electrical quantities @xcite , and so act like programmable resistors . because they can conduct currents in both directions , one circuit can be shared between a pair of neurons .",
    "a second problem is the number of recurrent connections .",
    "even our of modest dimensions involved almost 2 million synapses , which is impractical in terms of bandwidth and weight storage .",
    "even if a very high number of weights are zero , the connections between each pair of neurons must exist in order for a synapse to learn such weights .",
    "one possible solution is to impose sparse connectivity between the layers @xcite .",
    "this remains to be tested in our model .",
    "[ [ outlook - a - custom - learning - rule . ] ] outlook : a custom learning rule .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our method combines neurons that perform neural sampling and the rule .",
    "although we showed that this leads to a functional model , we do not know whether event - driven is optimal in any sense .",
    "this is partly due to the fact that @xmath98 is an approximate rule @xcite , and it is still not entirely understood why it performs so well , despite extensive work in studying its convergence properties @xcite .",
    "furthermore , the distribution sampled by the neuron does not exactly correspond to the boltzmann distribution , and the average weight updates in event - driven differ from those of standard , because in the latter they are carried out at the end of the reconstruction step .",
    "a very attractive alternative is to derive a custom synaptic plasticity rule that minimizes some functionally relevant quantity ( such as kullback - leibler divergence or contrastive divergence ) , _ given _ the encoding of the information in the neuron @xcite .",
    "a similar idea was recently pursued in @xcite , where the authors derived a triplet - based synaptic learning rule that minimizes an upper bound of the kullback - leibler divergence between the model and the data distributions .",
    "interestingly , their rule had a similar global signal that modulates the learning rule , as in event - driven , although the nature of this resemblance remains to be explored .",
    "such custom learning rules can be very beneficial in guiding the design of on - chip plasticity in neuromorphic and nanotechnologies , and will be the focus of future research .",
    "this work was partially funded by the national science foundation ( nsf efri-1137279 ) , the office of naval research ( onr muri 14 - 13 - 1 - 0205 ) , and the swiss national science foundation ( pa00p2_142058 ) .",
    "we thank all the anonymous reviewers of a previous version of this article for their constructive comments .",
    "j.  arthur , p.  merolla , f.  akopyan , r.  alvarez , a.  cassidy , s.  chandra , s.  esser , n.  imam , w.  risk , dbd rubin , et  al . building block of a programmable neuromorphic substrate : a digital neurosynaptic core . in _ neural networks ( ijcnn ) , the 2012 international joint conference on _ , pages 18 .",
    "ieee , 2012 .",
    "j.  bergstra , o.  breuleux , f.  bastien , p.  lamblin , r.  pascanu , g.  desjardins , j.  turian , d.  warde - farley , and y.  bengio .",
    "theano : a cpu and gpu math expression compiler . in _ proceedings of the python for scientific computing conference ( scipy ) _ , volume  4 , 2010 .",
    "e.  chicca and s.  fusi .",
    "stochastic synaptic plasticity in deterministic avlsi networks of spiking neurons . in frank rattay ,",
    "editor , _ proceedings of the world congress on neuroinformatics _ , argesim reports , pages 468477 , vienna , 2001 .",
    "argesim / asim verlag .",
    "d.  corneil , d.  sonnleithner , e.  neftci , e.  chicca , m.  cook , g.  indiveri , and r.  douglas .",
    "function approximation with uncertainty propagation in a vlsi spiking neural network . in _ international joint conference on neural networks ,",
    "ijcnn 2012 _ , pages 29902996 .",
    "ieee , 2012 .",
    "deiss , r.j .",
    "douglas , and a.m. whatley . a pulse - coded communications infrastructure for neuromorphic systems . in w.  maass and c.m .",
    "bishop , editors , _ pulsed neural networks _ , chapter  6 , pages 15778 . mit press , 1998 .",
    "j.  fiser , p.  berkes , g.  orbn , and m.  lengyel .",
    "statistically optimal perception and learning : from behavior to neural representations : perceptual learning , motor learning , and automaticity .",
    ", 14(3):119 , 2010 .",
    "g.  indiveri , b.  linares - barranco , t.j .",
    "hamilton , a.  van schaik , r.  etienne - cummings , t.  delbruck , s .- c .",
    "liu , p.  dudek , p.  hfliger , s.  renaud , j.  schemmel , g.  cauwenberghs , j.  arthur , k.  hynna , f.  folowosele , s.  saighi , t.  serrano - gotarredona , j.  wijekoon , y.  wang , and k.  boahen .",
    "neuromorphic silicon neuron circuits . , 5:123 , 2011 .",
    "s.  joshi , s.  deiss , m.  arnold , j.  park , t.  yu , and g.  cauwenberghs .",
    "scalable event routing in hierarchical neural array architecture with global synaptic connectivity . in _ cellular nanoscale networks and their applications ( cnna ) , 2010 12th international workshop on _ , pages 16 .",
    "ieee , 2010 .",
    "b.  pedroni , s.  das , e.  neftci , k.  kreutz - delgado , and g.  cauwenberghs .",
    "neuromorphic adaptations of restricted boltzmann machines and deep belief networks . in _ international joint conference on neural networks , ijcnn 2013 _ , 2013 .",
    "( accepted ) .",
    "j.  schemmel , d.  brderle , a.  grbl , m.  hock , k.  meier , and s.  millner .",
    "a wafer - scale neuromorphic hardware system for large - scale neural modeling . in _ international symposium on circuits and systems , iscas 2010 _ , pages 19471950 .",
    "ieee , 2010 .",
    "teresa serrano - gotarredona , timothe masquelier , themistoklis prodromakis , giacomo indiveri , and bernabe linares - barranco .",
    "stdp and stdp variations with memristors for spiking neuromorphic learning systems . ,",
    "7 , 2013 .",
    "t.  yu and g.  cauwenberghs .",
    "log - domain time - multiplexed realization of dynamical conductance - based synapses . in _ international symposium on circuits and systems , ( iscas ) , 2010 _ , pages 2558 2561 .",
    "ieee , june 2010 ."
  ],
  "abstract_text": [
    "<S> and deep belief networks have been demonstrated to perform efficiently in a variety of applications , such as dimensionality reduction , feature learning , and classification . </S>",
    "<S> their implementation on neuromorphic hardware platforms emulating large - scale networks of spiking neurons can have significant advantages from the perspectives of scalability , power dissipation and real - time interfacing with the environment . however the traditional architecture and the commonly used training algorithm known as are based on discrete updates and exact arithmetics which do not directly map onto a dynamical neural substrate . here , we present an event - driven variation of to train a constructed with neurons , that is constrained by the limitations of existing and near future neuromorphic hardware platforms . </S>",
    "<S> our strategy is based on neural sampling , which allows us to synthesize a spiking neural network that samples from a target boltzmann distribution . </S>",
    "<S> the recurrent activity of the network replaces the discrete steps of the algorithm , while carries out the weight updates in an online , asynchronous fashion . + we demonstrate our approach by training an composed of leaky neurons with synapses to learn a generative model of the mnist hand - written digit dataset , and by testing it in recognition , generation and cue integration tasks . </S>",
    "<S> + our results contribute to a machine learning - driven approach for synthesizing networks of spiking neurons capable of carrying out practical , high - level functionality . </S>"
  ]
}