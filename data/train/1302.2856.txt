{
  "article_text": [
    "sequential bitwise processing plays a key role in several general - purpose lossless data compression algorithms , including @xcite , @xcite and the recently emerging @xcite family of compression algorithms .",
    "all of these algorithms belong to the class of statistical data compression algorithms , which split the compression phase into modelling and coding .",
    "a statistical model assigns probabilities to upcoming symbols and these are translated into corresponding codes . assigning a high probability to",
    "the actually upcoming symbol leads to a short encoding , thus producing compression .",
    "the ideal code length corresponding to a prediction can closely be approximated via @xcite . hence improving prediction accuracy is crucial for compression .",
    "recently , -based compression algorithms have been of high public interest , due to the enormous compression achieved .",
    "unfortunately , there is little up - to - date literature on the internals of the involved algorithms @xcite .",
    "compression algorithms combine multiple binary predictors and are characterized by low processing speed and the best compression rates in multiple benchmarks up to date .",
    "ensemble prediction has previously been applied successfully in other areas of research , e.g. , time series forecast and classification @xcite , and form a promising direction of research . in the field of compression",
    "an ensemble approach is often called .",
    "the most elementary task of the prediction model is sequential probability assignment , i.e. , predicting the probability distribution @xmath0 of the upcoming symbol @xmath1 based on the already encountered sequence @xmath2 over a finite alphabet @xmath3 .",
    "such a task typically arises when working with context models .",
    "a finite number of symbols preceding @xmath1 can be used to condition the probability , which leads to finite context modelling @xcite .",
    "the finite context , e.g. , the character immediately preceding the current one , splits the source sequence into sub - sequences .",
    "ths ar oftn calld contxt historis .",
    "for instance the context history of the context `` e '' ( underlined ) regarding the last sentence is `` s__ndxs '' , an underscore represents a space symbol .",
    "this work focuses on binary alphabets , @xmath4 and uses the convention @xmath5 .",
    "experiments have shown that a local adaption of the computed statistics during modelling typically improves compression .",
    "thus more recent observations are of higher importance for probability assignment @xcite .",
    "this observation was made more or less accidentally due to limited calculation precision , which lead to a periodic rescaling of character counts @xcite .",
    "previous work investigated the effect of scaling @xcite and pointed out an approximate probability estimation model for binary sequences based on exponential smoothing @xcite .",
    "another aspect is the presence of noise within observations .",
    "an imperfect choice of conditioning contexts will lead to observations within context histories , which deviate from the governing probability distribution .",
    "we consider such events as outliers or simply noise .",
    "a recent work @xcite studied the effect of a limited probability interval , i.e. , @xmath6 \\subset [ 0 , 1]$ ] along with the estimation of the parameter @xmath7 regarding a series of binary random variables .",
    "a limited probability interval can be explained by viewing an observed sequence as the outcome of the transmission of the `` true '' sequence through a noisy channel ( i.e. , an extension to the original source model ) .",
    "results indicate that having knowledge about the parameters @xmath8 and @xmath9 can lead to significant improvements in compression for short to medium sized sequences . thus using the restriction @xmath10",
    "$ ] can represent a countermeasure for noisy observations .",
    "the previous section explained the aspects of observation recency and observation uncertainty .",
    "based on these ideas we enhance a standard approach for sequential binary prediction and introduce a new prediction model .",
    "we further employ our prediction model to construct a new ensemble compression algorithm .",
    "this compression algorithm is intended to be used as a second step algorithm in based compression .",
    "both , the sequential prediction model and the ensemble compression algorithm , contain constants ( fixed during compression or decompression ) , which influence the probability estimation and the compression .",
    "we denote such constants as parameters of the algorithm or parameters of the prediction model ( which should not be confused with parameters of a distribution ) . in the general setting",
    "there are no simple rules for choosing the ( unknown ) parameters . among the set of feasible parameters , we want to chose the parameters according to a certain objective . in data compression",
    "this objective is the minimization of the size of the compressed output .",
    "most of the parameter optimization in the area of data compression was carried out using ad - hoc hand - tuning , e.g. , @xcite , @xcite and @xcite . in this work",
    "we want to introduce systematic approaches to automated parameter optimization , since these will improve the compression performance compared to ad - hoc hand - tuning .",
    "we distinguish two versions of automatic parameter optimization in compression , which we call offline and online optimization . given a training data set the models parameters",
    "can be fitted once and remain static during future usage ( offline optimization ) .",
    "this approach requires a carefully chosen set of training data . since the optimization takes place only once and not prior to every compression pass there are no significant restrictions on the amount of data and the associated processing time . on the other hand , adding an initial optimization pass prior to compression and saving the parameters along with the compressed data refers to an online approach . however , there are more severe restrictions on the utilized resources .",
    "we consider a situation in which the optimization pass requires orders of magnitude more time than the actual ( de-)compression process impractical for online optimization . in this work",
    "we focus on online optimization and incorporate an automated optimization pass into the ensemble model mentioned above . coupling online",
    "optimization and statistical compression leads to asymmetric statistical compression , a new family of statistical compression algorithms . without optimization",
    "such algorithms are typically symmetric , since modelling and coding is required during compression and decompression .",
    "similar approaches to asymmetric algorithms exist in the field of audio compression @xcite .",
    "there is another non - obvious benefit in using optimization .",
    "assume an algorithm @xmath11 achieves a certain compression rate using an ad - hoc parametrization . a computationally cheaper algorithm @xmath12 produces compression comparable to @xmath11 along with optimized parameters .",
    "thus the compression time is reduced when @xmath11 is replaced by @xmath12 .",
    "this argument holds especially for offline optimization : the time required for optimization does not need to be included in the compression time , since optimization is only carried out once .",
    "the remaining part of this work is divided into four further sections .",
    "first we present a new elementary , binary prediction model , its application to non - binary alphabets and an approach to ensemble prediction .",
    "section [ sec : optimization ] briefly summarizes iterative numeric optimization and its application to the presented modelling algorithms . afterwards section [ sec : experiments ] evaluates the model components performance and the impact of optimization .",
    "as previously mentioned in section [ sec : intro ] the most essential task is to estimate the probability distribution given the series of binary random variables @xmath13 and an instance @xmath14 , where @xmath15 out of @xmath16 bits are one .",
    "assuming random variables @xmath17 , i.e. , @xmath18 for all @xmath19 and some fixed @xmath20 $ ] , one can calculate the probability of a given outcome @xmath21 via @xmath22 when @xmath21 is fixed an estimation @xmath23 of @xmath24 can be obtained via maximizing @xmath25 , or via minimizing the entropy @xmath26 note that logarithms are to the base two .",
    "the result of minimizing is the well - known maximum likelihood estimator @xmath27 .",
    "equation is rewritten to yield @xmath28 since we assume that the coding cost of more recent events is of higher importance , we modify to become a weighted entropy ( cf .",
    "@xcite ) @xmath29 where @xmath30 is some weight sequence . in this way",
    ", the value of @xmath24 is strongly linked to more recent observations ( steps @xmath31 ) .",
    "next we address the aspect of observation uncertainty , similar to @xcite .",
    "the observations @xmath32 are viewed to be the outcome of a binary symmetric channel . on the transmitter side",
    "the outcome @xmath32 of a binary random variable @xmath17 is sent through the channel .",
    "the receiver observes a corrupted bit @xmath33 with a probability @xmath34 , i.e. , the outcome of a binary random variable @xmath35 .",
    "summarizing @xmath36 holds for some @xmath37 .",
    "thus is modified to become the expected , weighted entropy @xmath38 since we can only observe the receiver side .",
    "we assume that the statistical properties of the bit sequence do not change rapidly ( i.e. , @xmath39 ) and approximate @xmath40 using the solution of the minimum - entropy problem @xmath41 which results in @xmath42 thus modelling uncertainty via restricts the probability interval to be @xmath43 $ ] . as a side effect the problem of assigning a probability to the opposite bit @xmath44 when processing a deterministic sequence @xmath45",
    "is solved .",
    "the source model discussed above contains several ( generally unknown ) parameters - the weight sequence and @xmath34 . in order to use the source model for prediction",
    "these parameters have to be chosen .",
    "a bad choice leads to redundancy during coding , e.g. , in some step @xmath19 the actual value of @xmath46 could be located outside of the restricted probability interval , but the model is only able to assign values in @xmath47 $ ] depending on the estimated parameter @xmath48",
    ".      equation can already be utilized to obtain a probability estimation given a weight sequence and @xmath34 . however , from a practical point of view and as a matter of convenience an estimation should be calculated incrementally , hence we select an exponentially decaying weight sequence @xmath49 with @xmath50 $ ] .",
    "equation becomes @xmath51 where @xmath52 which can be reformulated to yield an adjustment proportional to the prediction error @xmath53 initially we have @xmath54 and @xmath55 .",
    "note that the sequence @xmath56 is a geometric series and therefore @xmath57 { } \\frac{1}{1-\\lambda}. \\label{eq : tn_series}\\ ] ] for a very long sequence exponential smoothing can be used as an approximation of , i.e , @xmath58 depending on the computational resources different approximations seem acceptable :    * * exact model @xmath59 . *",
    "an estimator state is @xmath60 , computed according to . * * exponential smoothing @xmath61 .",
    "* the state is given by @xmath62 and is updated following .",
    "selecting @xmath63 , results in a very efficient calculation using bit shifts and additions / subtractions only .",
    "note that @xmath64 can be approximated more closely by imposing an upper limit on @xmath56 , or @xmath16 , respectively .",
    "this yields a state @xmath65 , with @xmath66 for a threshold @xmath67 .",
    "the values of @xmath68 are found using a lookup table and @xmath69 is set according to .",
    "all approximations described above share the same parameters @xmath70 and @xmath71 .      ' '' ''    ' '' ''    the previous section dealt with the modelling of a binary alphabet . in general the compression algorithms work on @xmath16-ary alphabets @xmath3 , typically @xmath72 .",
    "hence bitwise processing requires an alphabet decomposition , i.e. , a mapping @xmath73 and to indicate the code length . without loss of generality",
    "we may assume that @xmath74 .",
    "within this work we use a fixed decomposition , which we call `` flat decomposition '' , i.e. , @xmath75 ( e.g. , @xmath76 ) and @xmath77 for every symbol @xmath78 . modelling the probability distribution of @xmath79",
    "is split into @xmath80 consecutive steps @xmath81 working with conditional probabilities increases the prediction accuracy .",
    "a natural choice are order-@xmath82 contexts , which have successfully been applied to text compression @xcite .",
    "an order-@xmath82 context consists of the last @xmath82 characters immediately preceding the current one .",
    "figure [ fig : deco ] illustrates the bitwise modelling process using an order-1 context . depending on the underlying data other choices can be reasonable as well , e.g. , the neighbouring pixels in image compression @xcite , @xcite .",
    "section [ sec : intro ] mentioned the successful application of ensemble models in other areas . in the area of compression such techniques are known @xcite , but there has been less interest in directly applying them .",
    "such techniques allow multiple models to contribute with their advantages without cumulating their disadvantages @xcite . during modelling a probability",
    "must be calculated for each alphabet symbol @xmath78 , hence combining @xmath83 models roughly requires @xmath84 operations . on the other hand bitwise processing",
    "just requires @xmath85 operations on average , where @xmath86 is the average code length . without making further assumptions about symbol frequencies , i.e. , applying the decomposition described in section [ sec : deco ] , we get @xmath87 .",
    "an advantage of compared to is that it does not need to handle symbols , which did not appear in the current context , in a special way @xcite .",
    "indicates the presence of such a situation using an artificial escape symbol , whose probability needs to be modelled in every context . however , such situations may add redundancy , since there is code space allocated for possibly never appearing symbols .",
    "this issue can be crucial for @xcite .",
    "a disadvantage of is the requirement of multiple models simultaneously , which has heavy impact on processing speed and memory requirements .",
    "we now describe the outline of our approach to ensemble prediction , or respectively .",
    "it is based on a source switching model @xcite .",
    "consider a set of @xmath83 sources and a probabilistic switching mechanism , which selects source @xmath88 with a probability of @xmath89 ( in step @xmath19 ) where @xmath90 .",
    "note that the switching model should not be confused with volf s switching method @xcite , which is based on switching between _ source coding algorithms _ rather than constructing an ensemble source model . in its current state @xmath91",
    "the selected source emits a one - bit with the probability .",
    "afterwards a state transition takes place for each source resulting in the next state @xmath92 . in an analogous fashion",
    "the switching probabilities may vary , i.e. , these may depend on a state , too .",
    "summarizing the probability of a one - bit in step @xmath19 is @xmath93 thus the assumption ( or approximation ) of a switching source results in a linear ensemble prediction ( linear mixing ) .",
    "unfortunately , normally no information about the internals of the source ( e.g. , involved states and transitions ) or the characteristics of",
    "the probability assignment is available .",
    "the assignment is up to the designer .      ' '' ''    ` eiehdnkleeeeeeeeeeeiiiiiiiiiiiiyyeeeeei iieeeeiieeeeiieeeeeeeeeeeeeeyiyyyyiiiii iiyyyiyyiyyiiyyyyiyyyyyiyyyeeeeeeeeeeee eeeeeeeeeeeeeeeeeyyeeeeeeceeeeeeeeeieee eeeeehhohhhhheeeeeeeeeeeeeeeeeeieeeeeee eeeeeeeeeeeeeeeeeeyeeeeeeeeeeeeeeeeeeee iiiieeeeeeeeeeeeeeeeeeeeeeeeeeehheeeeee eeeeeeeeeeeeeeeeeieeeeeeeeeeeeeeeeeeyee eeeeeeeeeeeyeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee `    ' '' ''      to examine the prediction model described in sections [ sec : el_model ] and [ sec : el_model_approx ] we will compare its performance to the well - known - and -estimators @xcite with scaling @xcite .      for testing the ensemble approach we introduce a simple ensemble compression algorithm intended as a second step algorithm in based compression",
    ". sorts the characters in its input by context , hence it groups similar contexts together @xcite . since these contexts are often succeeded by the same characters output",
    "mostly consists of long interleaved runs of characters , see fig .",
    "[ fig : bwt_output ] .",
    "such sequences can be modelled as non - stationary @xcite .",
    "we model the output as the outcome of a switching source , which consists of two individual non - stationary sources .",
    "one source randomly emits characters independent of the previous sequence ( order-0 ) , this is intended to model interruptions in a single characters run .",
    "a second source emits characters based on the character immediately preceding the current position ( order-1 ) .",
    "in contrast to the first source it is intended to model the long runs of identical characters .",
    "the individual models are implemented using the binary predictors described in section [ sec : modelling ] .",
    "we assume the switching probabilities to be constant , i.e. , @xmath94 $ ] .    each individual model presented in section [ sec : el_model_approx ]",
    "has two parameters @xmath70 and @xmath34 .",
    "the previously described postprocessor has five parameters , @xmath95 , @xmath96 , @xmath97 , @xmath98 and @xmath99 , respectively .",
    "following these observations the next section will provide a way of optimizing the parameters .",
    "' '' ''    @xmath100 @xmath101 compute a search direction @xmath102 along which @xmath103 decreases perform a line search @xmath104 update the solution @xmath105 next step @xmath106    ' '' ''    we decompose a model into its structure and parameters . improving the model structure is a task which is typically carried out by humans .",
    "model parameters can be fitted automatically to a typical training data set .",
    "there are different approaches , depending on the optimization target .",
    "a differentiable optimization target allows the usage of local search procedures , for instance newton s method , see standard literature on these well - known techniques , e.g. , @xcite . when no derivative information is available ( i.e. , a non - differentiable optimization target ) or the search space is highly multimodal other stochastic search techniques should be preferred , see e.g. , @xcite . in our setting we want to minimize the average code length @xmath103 , depending on the of the prediction model @xmath107 where @xmath108 is given by a modification of @xmath109 here boldface symbols indicate matrices or vectors . the parameter search should take place within the hypercube formed by the inequality constraints @xmath110 \\subset \\mathbb{r}^n$ ]",
    ". in this work",
    "we want to focus on derivative - based optimization techniques based on quadratic programming , since @xmath103 is differentiable .",
    "figure [ fig : num_opt ] shows the typical outline of such an optimization procedure .",
    "the models described in the previous section span a low - dimensional search space , e.g. , @xmath111 . opposed to the small number of parameters a function evaluation is , depending on the amount of training data , time consuming .",
    "it requires to run the corresponding model along with the calculation of derivatives .",
    "since we want to use an online - optimization approach , the `` training data '' is the data to be actually compressed , i.e. , we know it prior to optimization .      consider a quadratic approximation @xmath112 of the target function @xmath103 as a result of the taylor - expansion at @xmath113 @xmath114 differentiating in @xmath102 and solving for its roots yields a search direction @xmath115 the matrix @xmath116 can either be estimated iteratively or computed directly . given a valid point @xmath117 $ ] a step towards @xmath102 might lead to a violation of the constraints .",
    "hence the constraints influence the computation of @xmath102 . in order to calculate a feasible direction",
    "we adopt a slight modification of the method in @xcite , which we will now summarize briefly .",
    "first the index set of binding constraints @xmath118 is identified depending on @xmath119 an element @xmath120 of @xmath121 is given by @xmath122 depending on the elements @xmath123 of @xmath116 .",
    "with @xmath124 we denote the @xmath88-th component of @xmath125 , the same holds for @xmath126 and @xmath127 , respectively .",
    "the set @xmath128 contains the indices of blocked directions , i.e. , @xmath129 is located on a constraint boundary and @xmath124 points towards the constraint .",
    "constraints contained in @xmath130 block movements along the directions fulfilling the conditions and @xmath131 blocks movements , which would leave the feasible region due to the linear transform described by @xmath116 .",
    "finally given @xmath132 the search direction is obtained via @xmath133 and @xmath134    during the optimization of the parameters of a single model , we compute the gradient @xmath135 and @xmath136 directly . when carrying out the experiments for the ensemble model this turned out to be too expensive computationally to be practical for our purposes . instead of computing @xmath116",
    "we used the approximation in conjunction with the sherman - morrison formula @xcite resulting in a quasi - newton step .      according to fig .",
    "[ fig : num_opt ] an estimation of the step length is the next step in the optimization procedure .",
    "a step along @xmath102 can still leave the feasible region , when stepping too far .",
    "there is an upper limit @xmath137 of @xmath8 imposed by the non - binding constraints @xmath138 in the case of an approximation of @xmath116 the line search was carried out using quadratic interpolation .",
    "the derivative information of @xmath139 is already available at @xmath140 . due to the calculation of @xmath141 and @xmath102",
    "we get @xmath142 now a value @xmath143 $ ] fulfilling @xmath144 is located .",
    "the minimum of the interpolation polynomial is given by @xmath145 if @xmath146 is decreased sufficiently , i.e. , @xmath147 where @xmath148 , we set @xmath149 and the line search is finished . otherwise @xmath9 is replaced with @xmath150 and the process is repeated .",
    "the optimization algorithm stops , when all components @xmath151 are in the range @xmath152 $ ] .",
    "it turned out that the precision requirements are rather relaxed , @xmath153 $ ] gives satisfying results .",
    "a higher request in precision translates into compression gains typically below @xmath154 bpc , which can be considered insignificant .",
    "the number of iterations has been limited to @xmath155 .      to perform the optimization process it is necessary to calculate the partial derivatives , since these form the gradient and the hessian . for reasons of convenience",
    "we introduce @xmath156 note that here @xmath157 denotes the natural logarithm .",
    "using this convention becomes @xmath158 and a partial derivative w.r.t .",
    "@xmath129 , a component of @xmath159 , is @xmath160 since @xmath161 we may write @xmath162^n , \\ ] ] the first derivative @xmath163 and the second derivative @xmath164 ^ 2 + \\frac{\\partial h(y , p)}{\\partial p}\\frac{\\partial^2   p}{\\partial x_i^2 } \\\\ %    \\frac{\\partial^2 h(y , p)}{\\partial x_i^2 } & = & \\left [ \\frac{\\partial",
    "h(y , p)}{\\partial x_i } \\right]^2 + \\frac{\\partial h(y , p)}{\\partial p}\\frac{\\partial^2   p}{\\partial x_i^2 }      \\end{aligned}\\ ] ] can easily be obtained .",
    "first the optimization of a single model is examined , i.e. , @xmath165 .",
    "we can restate as @xmath166 where @xmath167 in the case of @xmath64 or @xmath168 for @xmath169 , cf . and .",
    "thus the required partial derivatives of @xmath46 can be expressed as @xmath170 depending on the choice of the model , see section [ sec : el_model_approx ] , the term @xmath171 remains a function of @xmath70 , . utilizing the iterative nature of the expressions for the exact model ( @xmath64 )",
    "are given by @xmath172\\end{aligned}\\ ] ] with the abbreviations @xmath173 exponential smoothing ( @xmath169 ) , , yields the following expressions : @xmath174 for the initial step , @xmath175 , all derivatives have been initialized to be zero .      as stated in section [ sec : modelling ] an ensemble model consists of an order-0 and an order-1 non - stationary model ( predicting @xmath176 and @xmath177 ) and a switching probability , or weight @xmath99 .",
    "thus a point in parameter space is @xmath178 .",
    "the expressions for calculating the gradient worked out above just need to be modified slightly .",
    "higher order partial derivatives are estimated using .",
    "the partial derivatives of are given by @xmath179 where @xmath180 , @xmath46 is the mixed prediction in step @xmath19 , see , and @xmath181 finally the remaining derivative for the ensemble model is @xmath182",
    ".compression rates ( calgary corpus ) in bpc and average context history length @xmath183 of different order context histories for the , and the developed @xmath64 and @xmath169 estimators ( section [ sec : el_model_approx ] ) . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     [ tab : bwt_coders ]",
    "in this paper a new approach to modelling non - stationary binary sequences was studied and possible low - complexity implementations have been shown . using an iterative parameter - optimization method the parameters of the model",
    "can be fitted to training data automatically . in all test cases",
    "the new model shows a good performance compared to the - and -estimators .",
    "both classic estimators are surpassed except in one case , where our models show slightly worse results .",
    "thus in the case of compressing non - stationary data the presented models typically improves compression . beside the usage as a binary predictor on its own an ensemble model based on two non - stationary submodels for compressing output has been designed .",
    "an alphabet decomposition is required to map the @xmath16-ary alphabet to a binary sequence , so the binary predictor can be used .",
    "the ensemble model contains an optimization pass prior to the actual compression .",
    "such a simple ensemble model , together with online - optimization , shows good compression performance .",
    "note that the ensemble model is very simple and does not apply any parsing strategies or post transforms  it directly models symbol probabilities of plain output . in order to make such an approach more practical further steps need to be taken to speed up the optimization process .",
    "combining multiple models in data compression is highly successful in practice , but more research in this area is needed .",
    "the author would like to thank martin aumller , michael rink and martin dietzfelbinger for helpful suggestion and corrections , which improved the readability and made this paper easier to understand .",
    "x.  wang and n.  j. davidson , `` the upper and lower bounds of the prediction accuracies of ensemble methods for binary classification , '' in _ proc . international conference on machine learning and applications _ , vol .  9 , 2010 , pp . 373378 .",
    "m.  filho , t.  ohishi , and r.  ballini , `` ensembles of selected and evolved predictors using genetic algorithms for time series prediction , '' in _ proc .",
    "ieee congress on evolutionary computation _ ,",
    "vol .  8 , 2006 , pp . 28722879 .",
    "g.  shamir , t.  tjalkens , and f.  willems , `` low - complexity sequential probability estimation and universal compression for binary sequences with constrained distributions , '' in _ proc .",
    "ieee international symposium on information theory _ ,",
    "vol .  21 , 2008 , pp . 995999",
    ".          f.  ghido and l.  tabus , `` optimization - quantization for least squares estimates and its application for lossless audio compression , '' in _ proc .",
    "acoustics , speech and signal processing _ , vol .  33 , 2008 , pp . 193196 ."
  ],
  "abstract_text": [
    "<S> in this paper an approach to modelling non - stationary binary sequences , i.e. , predicting the probability of upcoming symbols , is presented . after studying the prediction model </S>",
    "<S> we evaluate its performance in two non - artificial test cases . </S>",
    "<S> first the model is compared to the and estimators . </S>",
    "<S> secondly a statistical ensemble model for compressing output is worked out and evaluated . a systematic approach to the parameter optimization of an individual model and the ensemble model </S>",
    "<S> is stated .    </S>",
    "<S> * this paper is a preprint ( ieee `` accepted '' status ) . *    </S>",
    "<S> * ieee copyright notice . </S>",
    "<S> *   2011 ieee . personal use of this material is permitted . </S>",
    "<S> permission from ieee must be obtained for all other uses , in any current or future media , including reprinting / republishing this material for advertising or promotional purposes , creating new collective works , for resale or redistribution to servers or lists , or reuse of any copyrighted component of this work in other works .    * </S>",
    "<S> doi . </S>",
    "<S> * 10.1109/ccp.2011.22    http://doi.ieeecomputersociety.org/10.1109/ccp.2011.22    data compression ; sequential prediction ; parameter optimization ; numerical optimization ; combining models ; mixing ; ensemble prediction </S>"
  ]
}