{
  "article_text": [
    "the radial component of the position of a distant object is inferred from its cosmological redshift , induced by the expansion of the universe ; the light observed from a distant galaxy appears to us at longer wavelengths than in the rest frame of that galaxy .",
    "the most accurate determination of the exact redshift , @xmath4 , comes from directly observing the spectrum of an extragalactic source and measuring a consistent multiplicative shift , relative to the rest frame , of various emission ( or absorption ) features .",
    "the rest - frame wavelengths of these emission lines are known to a high degree of accuracy which can be conferred onto the measured spectroscopic redshifts , @xmath5 .",
    "however , the demand on telescope time to obtain spectra for every source in deep , wide surveys is prohibitively high , and only relatively small area spectroscopic campaigns can reach faint magnitudes ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , or at the other extreme , relatively bright magnitudes over larger areas ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "this forces us towards the use of photometric observations to infer the redshift by other means . rather than individual spectra",
    ", the emission from a distant galaxy is observed in several broad filters , facilitating the characterization of the spectral energy distribution ( sed ) of fainter sources , at the expense of fine spectral resolution .",
    "photometric redshift methods largely fall into two categories , based on either sed template fitting or machine learning .",
    "template fitting software such as hyperz ; @xcite , zebra ; @xcite , eazy ; @xcite and le phare @xcite rely on a library of sed templates for a variety of different galaxy types , which ( given the transmission curves for the photometric filters being used ) can be redshifted to fit the photometry .",
    "this method can be refined in various ways , often with the use of simulated seds rather than only those observed at low redshift , composite seds , and through calibration using any available spectroscopic redshifts .",
    "machine learning methods such as artificial neural networks ( e.g. annz ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , nearest - neighbour ( nn ) @xcite , genetic algorithms ( e.g. * ? ? ?",
    "* ) , self - organized maps @xcite and random forest @xcite , to name but a few , rely on a significant fraction of sources in a photometric catalogue having spectroscopic redshifts .",
    "these ` true ' redshifts are used to train the algorithm .",
    "in addition to providing a point estimate , machine learning methods can provide the degree of uncertainty in their prediction @xcite .",
    "both methods have their strengths and weaknesses , with the best performance often depending on the available data and the intended science goals .",
    "as such , future surveys may well depend on contributions from both in tandem , but there has been extensive work on comparing the current state of the art in public software using a variety of techniques @xcite .",
    "artificial neural networks motivate the most commonly used machine learning software @xcite , however gaussian processes ( e.g. * ? ? ?",
    "* ) have not yet become well established in this area , despite comparison by @xcite suggesting that they may outperform the popular annz code , using the rms error as a metric .    in this paper",
    ", we introduce a novel sparse kernel regression model that greatly reduces the number of basis ( kernel ) functions required to model the data considered in this paper .",
    "this is achieved by allowing each kernel to have its own hyper - parameters , governing its shape .",
    "this is in contrast to the standard kernel - based models in which a set of global hyper - parameters are optimized ( such as is typical in gaussian process ( gp ) methods ) .",
    "the complexity cost of such a kernel - based regression model is @xmath6 , where @xmath7 is the number of basis functions .",
    "this cubic time complexity arise from the cost of inverting an @xmath7 by @xmath7 covariance matrix . in a standard gaussian process model @xcite ,",
    "seen as a kernel regression algorithm , we may regard the basis functions , as located at the @xmath7 points in the training sample .",
    "this renders such an approach unusable for many large training data applications where scalability is a major concern .",
    "much of the work done to make gps more scalable is either to ( a ) make the inverse computation faster or ( b ) use a smaller representative set from the training sample to reduce the rank and ease the computation of the covariance matrix .",
    "examples of ( a ) include methods such as structuring the covariance matrix such that it is much easier to invert , using toeplitz @xcite or kronecker decomposition @xcite , or inverse approximation as an optimization problem @xcite .",
    "to reduce the number of representative points ( b ) , an @xmath8 subset of the training sample can be selected which maximizes the accuracy or the numerical stability of the inversion @xcite .",
    "alternatively , one may search for `` inducing '' points not necessarily present in the training sample , and not necessarily even lying within the data range , to use as the basis set such that the probability of the data being generated from the model is maximized @xcite .",
    "approaches such as relevance vector machines ( rvm ; * ? ? ?",
    "* ) and support vector machines ( svm ; * ? ? ?",
    "* ) are basis - function models .",
    "however , unlike sparse gps , they do not learn the basis functions locations but rather apply shrinkage to a set of kernels in the form of weight - decay on the linear weights that couple the kernels , located at training data points , to the regression .    in this paper",
    ", we propose a non - stationary sparse gaussian model to target photometric redshift estimation .",
    "the key difference between the proposed approach and other basis function models , is that our model does not use shrinkage ( automatic relevance determination ) external to the kernel , but instead has a length - scale parameter in each kernel .",
    "this allows for parts of the input - output regression mapping to have different characteristic length - scales .",
    "we can see this as allowing for shrinkage and reducing the need for more basis functions , as well as allowing for non - stationary mappings .",
    "a regular gp , sparse gp or rvm does not do this , and we demonstrate that this is advantageous to photometric redshift estimation . furthermore , the model is presented within a framework with components that address other challenges in photometric redshift estimation such as incorporating a weighting scheme as an integral part of the process to remove , or introduce , any systematic bias , and a prior mean function to enhance the extrapolation performance of the model .",
    "the results are demonstrated on photometric redshift estimation for a simulated _ euclid_-like survey @xcite and on observational data from the 12th data release of the sloan digital sky survey ( sdss ) @xcite . in particular",
    ", we use the weighting scheme to remove any distribution bias and introduce a linear bias to directly target the mission s requirement .",
    "the paper is organised as follows , a brief introduction to gaussian processes for regression is presented in section [ sec - gaussian - process ] followed by an introduction to sparse gps in section [ sec - sparse - gaussian - processes ] .",
    "the proposed approach is described in section [ sec - proposed - approach ] followed by an application to photometric redshift estimation in section [ sec - application ] , where the details of the mock dataset are described .",
    "the experiments and results are discussed in section [ sec - experiments ] on the simulated survey , and in section [ sec - experiments - sdss ] we demonstrate the performance of the proposed model and compare it to annz on the sdss 12th data release .",
    "finally , we summarize and conclude in section [ sec - summary ] .",
    "in many modelling problems , we have little prior knowledge of the explicit functional form of the function that maps our observables onto the variable of interest .",
    "imposing , albeit sensible , parametric models , such as polynomials , makes a tacit bias . for this reason ,",
    "much of modern function modelling is performed using _ non - parametric _ techniques . for regression ,",
    "the most widely used approach is that of _ gaussian processes _ @xcite .",
    "a gaussian process is a supervised non - linear regression algorithm that makes few explicit _ parametric _ assumptions about the nature of the function fit . for this reason ,",
    "gaussian processes are seen as lying within the class of bayesian non - parametric models .",
    "the underlying assumption in a gp is that , given a set of input @xmath9 and a set of target outputs @xmath10 , where @xmath7 is the number of samples in the dataset and @xmath11 is the dimensionality of the input , the observed target @xmath12 is generated by a function @xmath13 of the input @xmath14 plus additive noise @xmath15 : @xmath16 the noise @xmath17 is taken to be normally distributed with a mean of zero and variance @xmath18 , or @xmath19 . to simplify the notation",
    ", it is assumed that @xmath20 ( this can readily be achieved without loss of generality , via a linear whitening process ) and univariate , although the derivation can be readily extended to multivariable problems .",
    "the conditional probability of the observed variable given the function is hence distributed as follows : @xmath21 a gp then proceeds by applying a _",
    "bayesian _ treatment to the problem to infer a probability distribution over the space of possible functions @xmath13 given the data : @xmath22    this requires us to define a prior , @xmath23 , over the function space . the function is normally distributed with a mean of zero , to match the mean of the normalized variable @xmath24 , with a covariance _ function _ @xmath25 , i.e. @xmath26 . the covariance function captures prior knowledge about the relationships between the observables .",
    "most widely used covariance functions assume that there is local similarity in the data , such that nearby inputs are mapped to similar outputs .",
    "the covariance @xmath25 can therefore be modelled as a function of the input @xmath27 , @xmath28 , where each element @xmath29 and @xmath30 is the covariance function . for @xmath25 to be a valid covariance it has to be symmetric and positive semi - definite matrix ; arbitrary functions for @xmath30 can not guarantee these constraints . a class of functions that guarantees these structural constraints are referred to as _ mercer kernels",
    "a commonly used kernel function , which is the focus of this work , is the squared exponential kernel , defined as follows : @xmath31 where @xmath32 and @xmath33 are referred to as the height ( output , or variance ) and characteristic length ( input ) scale respectively , which correspond to tunable _ hyper - parameters _ of the model .",
    "the similarity between two input vectors , under the squared exponential kernel , is a non - linear function of the euclidean distance between them .",
    "we note that this choice of kernel function guarantees continuity and smoothness in the function and all its derivatives . for a more extensive discussion of covariances ,",
    "the reader is referred to @xcite or @xcite . with the likelihood @xmath34 and prior @xmath23 ,",
    "the marginal likelihood @xmath35 can be computed as follows : @xmath36 by multiplying the likelihood and the prior and completing the square over @xmath13 , we can express the integration as a normal distribution independent of @xmath13 multiplied by a another normal distribution over @xmath13 .",
    "the distribution independent of @xmath13 can then be taken out of the integral , and the integration of the second normal distribution with respect to @xmath13 will be equal to one .",
    "the resulting distribution of the marginal likelihood is distributed as follows @xcite : @xmath37 the marginal likelihood of the full data set can hence be computed as follows : @xmath38    the aim of a gp , is to maximize the probability of observing the target @xmath24 given the input @xmath27 , eq . .",
    "note that the only free parameters to optimize in the marginal likelihood are the parameters of the kernel and the noise variance , collectively referred to as the _ hyper - parameters _ of the model .",
    "it is more convenient however to maximize the log of the marginal likelihood , eq .",
    ", since the log function is a monotonically increasing function , maximizing the log of a function is equivalent to maximizing the original function .",
    "the log likelihood is given as : @xmath39    we search for the optimal set of hyper - parameters using a gradient - based optimization , hence we require the derivatives of the log marginal likelihood with respect to each hyper - parameter . in this paper ,",
    "the l - bfgs algorithm was used to optimize the objective which uses a quasi - newton method to compute the search direction in each step by approximating the inverse of the hessian matrix from the history of gradients in previous steps @xcite .",
    "it is worth mentioning that non - parametric models require the optimization of few hyper - parameters that do not grow with the size of the data and are less prone to overfitting .",
    "the distinction between parameters and hyper - parameters of a model is that the former directly influence the input - output mapping , for example the linear coupling weights in a basis function model , whereas the latter affect properties of distributions in the probabilistic model , for example the widths of kernels .",
    "although this distinction is somewhat semantic , we keep to this nomenclature as it is standard in the statistical machine learning literature .",
    "once the hyper - parameters have been inferred , the conditional distribution of future predictions @xmath40 for test cases @xmath41 given the training sample can be inferred from the joint distribution of @xmath40 and the observed targets @xmath24 .",
    "if we assume that the joint distribution is a multivariate gaussian , then the joint probability is distributed as follows : @xmath42 where we introduce the shorthand notations @xmath43 , @xmath44 , @xmath45 and @xmath46 .",
    "the conditional distribution @xmath47 is therefore distributed as follows : @xmath48 if we assume a non - zero prior mean @xmath49 over the function , @xmath50 , and an un - normalized @xmath24 with mean @xmath51 , the mean of the posterior distribution will be equal to : @xmath52    the main drawback of gps is the @xmath6 computational cost required to invert the @xmath53 matrix @xmath54 .",
    "the _ sparse gaussian process _ allows us to reduce this computational cost and is detailed in the following section .",
    "gaussian processes are often described as non - parametric regression models due to the lack of an explicit parametric form .",
    "indeed gp regression can also be viewed as a functional mapping @xmath55 parameterized by the data and the kernel function , followed by linear regression via optimization of the following objective : @xmath56 where @xmath57 are the set of coefficients for the linear regression model that maps the transformed features @xmath25 to the desired output @xmath24 .",
    "the feature transformation @xmath25 evaluate how `` similar '' a datum is to every point in the training sample , where the similarity measure is defined by the kernel function . if two points have a high kernel response via eq .",
    ", this will result in very correlated features , adding extra computational cost for very little or no added information . selecting a subset of the training sample that maximizes",
    "the preserved information is a research question addressed in @xcite , whereas in @xcite the basis functions are treated as a search problem rather than a selection problem and their locations are treated as hyper - parameters which are optimized .",
    "these approaches result in a transformation @xmath58 , in which @xmath59 is the number of basis functions used .",
    "the transformation matrix @xmath25 will therefore be a rectangular @xmath7 by @xmath60 matrix and the solution for @xmath57 in eq .",
    "is calculated via standard linear algebra as : @xmath61    even though these models improve upon the computational cost of a standard gp , very little is done to compensate for the reduction in modelling power caused by the `` loss '' of basis functions .",
    "the selection method is always bounded by the full gp s accuracy , on the _ training _ sample , since the basis set is a subset of the full gp basis set . on the other hand ,",
    "the sparse gp s ability to place the basis set freely across the input space does go some way to compensate for this reduction , as the kernels can be optimized to describe the distribution of the data . in other words , instead of training a gp model with all data points as basis functions , or restricting it to a subset of the training sample which require some cost to select them , a set of inducing points is used in which their locations are treated as hyper - parameters of the model to be optimized . in both a full and a low rank approximation gp , a global set of hyper - parameters is used for all basis functions , therefore limiting the algorithm s local modelling capability . moreover",
    ", the objective in eq . minimizes the sum of squared errors , therefore for any non - uniformly distributed output , the optimization routine will bias the model towards the mean of the output distribution and will seek to fit preferentially the region of space where there are more data .",
    "hence , the model might allow for very poor predictions for few points in poorly represented regions , e.g. the high redshift range , in order to produce good predictions for well represented regions .",
    "therefore , the error distribution as a function of redshift is not uniform unless the training sample is well balanced , producing a model that is sensitive to how the target output is distributed .    in the next section ,",
    "a method is proposed which addresses the above issues by parametrizing each basis function with bespoke hyper - parameters which account for variable density and/or patterns across the input space .",
    "this is particularly pertinent to determining photometric redshifts , where complete spectroscopic information may be restricted or biased to certain redshifts or galaxy types , depending on the target selection for spectroscopy of the training sample .",
    "this allows the algorithm to learn more complex models with fewer basis functions .",
    "in addition , a weighting mechanism to remove any distribution bias from the model is directly incorporated into the objective .",
    "in this paper , we extend the sparse gp approach by modelling each basis ( kernel ) function with its own set of hyper - parameters . the kernel function in eq .",
    "is hence redefined as follows : @xmath62 where @xmath63 are the set of basis coordinates and @xmath64 is the corresponding length scale for basis @xmath65 . the multivariate input is denoted as @xmath66 . throughout the rest of the paper , @xmath67 denotes the @xmath68-th row of matrix @xmath27 , or @xmath14 for short , whereas @xmath69 denotes the @xmath65-th column and @xmath70 refers to the element at row @xmath68 and column @xmath65 in matrix @xmath27 , and similarly for other matrices .",
    "note that the hyper - parameter @xmath32 has been dropped , as it interferes with the regularization objective .",
    "this can be seen from the final prediction equation @xmath71 , the weights are always multiplied by their associated @xmath32 . therefore , the optimization process will always compensate for decreasing @xmath72 by increasing @xmath73 .",
    "dropping the height variance ensures that the kernel functions do not grow beyond control and delegates learning the linear coefficients and regularization to the weights in @xmath57 .",
    "the derivatives with respect to each length scale and position are provided in equations eq . and",
    "respectively :    @xmath74    the symbol @xmath75 denotes the hadamard product , i.e. element - wise matrix multiplication and @xmath76 denotes a column vector of length @xmath7 with all elements set to 1 . finding the set of hyper - parameters that optimizes the solution , is in effect finding the set of radial basis functions defined by their positions @xmath77 and radii @xmath78 that jointly describe the patterns across the input space . by parametrizing them differently ,",
    "the model is more capable to accommodate different regions of the space more specifically .",
    "a global variance model assumes that the relationship between the input and output is global or equal across the input space , whereas a variable variance model , or non - stationary gp , makes no assumptions and learns the variable variances for each basis function which reduces the need for more basis functions to model the data .",
    "the kernel in eq . can be further extended to , not only model each basis function with its own radius @xmath64 , but also model each one with its own covariance @xmath79 .",
    "this enables the basis function to have any arbitrary shaped ellipses giving it more flexibility .",
    "the kernel in eq . can be extended as follows : @xmath80    furthermore , to make the optimization process faster and simpler , we define the additional variables :    @xmath81    where @xmath82 is a local affine transformation matrix for basis function @xmath65 and @xmath83 is the application of the local transformation to the data . optimizing with respect to @xmath84 directly ensures that the covariance matrix is positive definite .",
    "this makes it faster from a computational perspective as the kernel functions for all the points with respect to a particular basis can be computed more efficiently as follows : @xmath85 the exponent in eq .",
    "basically computes the sum of squares in each row of @xmath83 .",
    "this allows for a more efficient computation of the kernel functions for all the points in a single matrix operation .",
    "the derivatives with respect to each @xmath84 and @xmath86 are shown in eq . and",
    "eq . respectively .",
    "@xmath87    setting up the problem in this manner allows the setting of matrix @xmath84 to be of any size @xmath11 by @xmath88 , where @xmath89 which can be considered as a low rank approximation to @xmath90 without affecting the gradient calculations .",
    "in addition , the inverse of the covariance can be set to @xmath91 in the low rank approximation case to ensure that the final covariance can model a diagonal covariance .",
    "this is referred to as _ factor analysis distance _ @xcite but previously used to model a global covariance as opposed to variable covariances as is the case here .      in the absence of observations , all bayesian models ,",
    "gaussian processes included , rely on their priors to provide function estimation .",
    "for the case of gaussian processes this requires us to consider the prior over the function , especially the prior mean .",
    "for example , the first term in the mean prediction in eq . ,",
    "@xmath49 , is our prior mean in which we learn the deviation from using a gp .",
    "similarly , we may consider a mean _ function _ that is itself a simple linear regression from the independent to dependent variable .",
    "the parameters of this function are then inferred and the gp infers non - linear deviations . in the absence of data ,",
    "e.g. in extrapolative regions , the gp will fall back to the linear regression prediction @xcite .",
    "we can incorporate this directly into the optimization objective instead of having it as a separate preprocessing step by redefining @xmath25 as a concatenation of the linear and non - linear features , or setting @xmath92 $ ] and @xmath93 $ ] , where @xmath94 is the linear regression s coefficients and @xmath95 is the bias .",
    "the prediction can then be formulated as follows :    @xmath96    furthermore , the regularization matrix , @xmath97 , in eq .",
    "can be modified so that it penalises the learning of high coefficients for the non - linear terms , @xmath57 , but small or no cost for learning high linear terms , @xmath94 and @xmath95 , by setting the corresponding elements in the diagonal of @xmath98 to 0 instead of @xmath18 , or the last @xmath99 elements .",
    "therefore , as @xmath18 goes to infinity , the model will approach a simple linear regression model instead of fallen back to zero .      thus far in the discussion , we make the tacit assumption that the objective of the inference process is to minimize the sum of squared errors between the model and target function values .",
    "although this is a suitable objective for many applications , it is intrinsically biased by uneven distributions of training data , sacrificing accuracy in less represented regions of the space . ideally we would like to train a model with a balanced data distribution to avoid such bias .",
    "this however , is a luxury that we often do not have .",
    "for example , the lack of strong emission lines that are detectable with visible - wavelength spectrographs in the `` redshift - desert '' at @xmath100 means that this redshift range is often under - represented in spectroscopic samples .",
    "a common technique is to either over - sample or under - sample the data to achieve balance @xcite . in under - sampling",
    ", samples are removed from highly represented regions to achieve balance , over - sampling on the other hand duplicates under represented samples .",
    "both approaches come with a cost ; in the former good data are wasted and in the latter more computation is introduced due to the data size increase . in this paper , we perform cost - sensitive learning , which increases the intrinsic error function in under - represented regions . in regression tasks ,",
    "such as we consider here , the output can be either discretized and treated as classes for the purpose of cost assignment , or a specific bias is used such as @xmath101 . to mimic a balanced data set in our setup",
    ", the galaxies were grouped by their spectroscopic redshift using non - overlapping bins of width 0.1 .",
    "the weights are then assigned as follows for balanced training : @xmath102 where @xmath103 is the error cost for sample @xmath68 , @xmath104 is the frequency of samples in bin number number @xmath68 , @xmath105 is the number of bins and @xmath106 is the set of samples in set number @xmath95 .",
    "eq . assigns a weight to each training point which is the maximum bin frequency over the frequency of the bin in which the source belongs .",
    "this ensures that the error cost of source @xmath68 is inversely proportional to its spectroscopic redshift frequency in the training sample .",
    "the normalized weights are assigned as follows :    @xmath107    after the weights have been assigned , they can be incorporated directly into the objective as follows : @xmath108    the difference between the objectives in eq . and",
    "is the introduction of the diagonal matrix @xmath109 , where each element @xmath110 is the corresponding cost @xmath103 for sample @xmath68 . the first term in eq .",
    "is a matrix operation form for a weighted sum of squares @xmath111 , where the solution can be found analytically as follows : @xmath112 the only modification to the gradient calculation is to set the matrix @xmath113 . in standard sum of squared errors , @xmath114 or the identity matrix .",
    "it is worth emphasising that this component of the framework does not attempt to weight the training sample in order to match the distribution of the test sample , or matching the spectroscopic distribution to the photometric distribution as proposed in @xcite , @xcite and applied to photometric redshift in @xcite , but rather gives the user of the framework the ability to control the cost per sample to serve different science goals depending on the application . in this paper ,",
    "the weighting scheme was used for two different purposes , the first was to virtually balance the data to mimic training on a uniform distribution , and the second was to directly target the weighted error of @xmath101 .",
    "in this section , we specifically target the photometric bands and depths planned for _ euclid_. _ euclid _ aims to provide imaging data in a broad @xmath115 band and the more standard near - infrared @xmath116 , @xmath117 and @xmath118 bands , while ground - based ancillary data are expected in the optical @xmath119 , @xmath120 , @xmath68 and @xmath4 bands @xcite .",
    "we use a mock dataset from @xcite , consisting of the @xmath119 , @xmath120 , @xmath68 , @xmath4 , @xmath115 , @xmath116 , @xmath117 and @xmath118 magnitudes ( to 10@xmath32 depths of 24.6 , 24.2 , 24.4 , 23.8 , 25.0 for the former , and 5@xmath32 depth of 24.0 for each of the latter three near - infrared filters ) for 185,253 simulated sources .",
    "we remove all sources with @xmath121 to simulate the target magnitudes set for _ euclid_. in addition , we remove any sources with missing measurements in any of their bands prior to training ( only 15 sources ) .",
    "no additional limits on any of the bands were used , however in section [ subsec - prior - mean ] we do explicitly impose limits on the riz band to test the extrapolation performance of the models .",
    "the distribution of the spectroscopic redshift is provided in figure [ fig - zspec - histogram ] .",
    "for all experiments on the simulated data , we ignore the uncertainties on the photometry in each band and train only on the magnitudes since in the simulated data set , unlike real datasets , the log of the associated errors are linearly correlated with the magnitudes , especially in the targeted range , therefore adding no information .",
    "however , they were fed as input to annz to satisfy the input format of the code .",
    "1        1        1     we preprocess the data using principle component analysis ( pca ; * ? ? ?",
    "* ) to de - correlate the features prior to learning , but retain all features with no dimensionality reduction .",
    "de - correlation accelerates the convergence rate of the optimization routine especially when using a logistic - type kernel machines such as neural networks @xcite . to understand this ,",
    "consider a simple linear regression example where we would like to solve for @xmath57 in @xmath122 , the solution for this is @xmath123 .",
    "note that if @xmath124 is de - correlated @xmath125 , therefore learning @xmath126 depends only on the @xmath68-th column of @xmath124 and it is independent from learning @xmath127 , where @xmath128 . in an optimization approach , the convergence rate is a function of the condition number of the @xmath129 matrix , which is minimized in the case of de - correlated data .",
    "this represents a quadratic error surface which helps accelerate the search .",
    "this is particularly important in the application addressed in this paper because the magnitudes measured in each filter are strongly correlated with each other .",
    "five algorithms are considered to model the data ; artificial neural networks ( annz ; * ? ? ?",
    "* ) , a gp with low rank approximation ( stablegp ; * ? ? ?",
    "* ) , a sparse gp with global length scale ( gp - gl ) , a gp with variable length scale ( gp - vl ) and a gp with variable covariances ( gp - vc ) . for annz , a single layer network is used , and to satisfy the input format for the code , the data were not de - correlated and the uncertainties on photometry for each band were used as part of the training input . for stablegp , we use the sr - vp method proposed in @xcite . in subsequent tests",
    ", the variable @xmath60 refers to the number of hidden units in annz , the rank in stablegp , and the number of basis functions in gp - gl , gp - vl and gp - vc .",
    "the time complexities for each algorithm are shown in table [ table - time - complexity ] .",
    "the data were split at random into 80 per cent for training , 10 per cent for validation and 10 per cent for testing .",
    "we note that we investigate the accuracy for various training sample sizes in section  [ sec - sizetraining ] .",
    "all models were trained using the entire redshift range available , but we only report the performance on the redshift range of @xmath2 to target the parameter space set out in @xcite .",
    "we train each model for 500 iterations in each run and the validation sample was used for model selection and parameter tuning , but all the results here are reported on the test sample , which is not used in any way during the training process .",
    "table [ table - metrics ] shows the metrics used to report the performance of each algorithm .",
    "+ annz ( @xmath130-layers ) & @xmath131 + stablegp & @xmath132 + gp - gl & @xmath133 + gp - vl & @xmath133 + gp - vc & @xmath134 +    [ table - time - complexity ]    [ cols=\"<,<,<\",options=\"header \" , ]     [ table - final - results ]    0.35        0.35     0.35        0.35     0.35        0.35     0.35        0.35",
    "in this paper a sparse gaussian process framework is presented and applied to photometric redshift estimation .",
    "the framework is able to out perform annz , sparse gp parametrized by a set of global hyper - parameters and low rank approximation gp .",
    "the performance increase is attributed to the handling of distribution bias via a weighting scheme integrated as part of the optimization objective , parametrizing each basis function with bespoke covariances , and integrating the learning of the prior mean function to enhance the extrapolation performance of the model .",
    "the methods were applied to a simulated dataset and sdss dr12 where the proposed approach consistently outperforms the other models on the important metrics ( @xmath135 and @xmath136 ) .",
    "we find that the model scales linearly in time with respect to the size of the data , and has a better generalization performance compared to the other methods even when presented with a limited training set .",
    "results show that with only 30 per cent of the data , the model was able to reach accuracy close to that of using the full training sample . even when data were selectively removed based on @xmath115 magnitudes , the model was able to show the best recovery performance compared to the other models .",
    "the cost - sensitive learning component of the framework regularizes the predictions to limit the effect caused by the biased distribution of the output and allows for direct optimization of the survey objective ( e.g. @xmath137 ) .",
    "again , the algorithm consistently outperforms other approaches , including annz and stablegp , in all reported experiments .",
    "we also investigate how the size of the training sample and the basis set affects the accuracy of the photometric redshift prediction .",
    "we show that for the simulated set of galaxies , based on the work of @xcite , we are able to obtain a photometric redshift accuracy of @xmath138 and @xmath139 using 1600 basis functions which is a factor of seven improvement over the standard annz implementation . we find that gp - vc out - performed annz on the real data from sdss - dr12 , with an improvement in accuracy of @xmath140per cent , even when restricted to have the same number of free parameters . in future work",
    "we will test the algorithm on a range of real data , and pursue investigations of how the algorithm performs over different redshift regimes and for different galaxy types .",
    "iaa acknowledges the support of king abdulaziz city for science and technology .",
    "mjj and snl acknowledge support from the uk space agency .",
    "the authors would like to thank the reviewers for their valuable comments ."
  ],
  "abstract_text": [
    "<S> accurate photometric redshifts are a lynchpin for many future experiments to pin down the cosmological model and for studies of galaxy evolution . in this study , </S>",
    "<S> a novel sparse regression framework for photometric redshift estimation is presented . </S>",
    "<S> synthetic dataset simulating the _ euclid _ survey and real data from sdss dr12 are used to train and test the proposed models . </S>",
    "<S> we show that approaches which include careful data preparation and model design offer a significant improvement in comparison with several competing machine learning algorithms . </S>",
    "<S> standard implementations of most regression algorithms use the minimization of the sum of squared errors as the objective function . for redshift inference , this induces a bias in the posterior mean of the output distribution , which can be problematic . in this paper </S>",
    "<S> we directly minimize the target metric @xmath0 and address the bias problem via a distribution - based weighting scheme , incorporated as part of the optimization objective . </S>",
    "<S> the results are compared with other machine learning algorithms in the field such as artificial neural networks ( ann ) , gaussian processes ( gps ) and sparse gps . </S>",
    "<S> the proposed framework reaches a mean absolute @xmath1 , over the redshift range of @xmath2 on the simulated data , and @xmath3 over the entire redshift range on the sdss dr12 survey , outperforming the standard annz used in the literature . </S>",
    "<S> we also investigate how the relative size of the training sample affects the photometric redshift accuracy . </S>",
    "<S> we find that a training sample of > 30 per cent of total sample size , provides little additional constraint on the photometric redshifts , and note that our gp formalism strongly outperforms annz in the sparse data regime for the simulated data set .    </S>",
    "<S> [ firstpage ]    methods : data analysis  galaxies : distances and redshifts </S>"
  ]
}