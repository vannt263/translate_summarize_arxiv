{
  "article_text": [
    "the system described in this paper has been designed for the cogalex - v shared task , focusing on the corpus - based identification of semantic relations . since distributional semantic models ( henceforth dsms ) were proposed as a special topic of interest for the current edition of the cogalex workshop , we decided to base our classifier on a number of distributional measures that have been used by past natural language processing ( nlp ) research to discriminate between a specific semantic relation and other relation types .",
    "the task is splitted into the following subtasks :    * for each word pair , the participating systems have to decide whether the terms are semantically related or not ( true and false are the only possible outcomes ) ; * for each word pair , the participating systems have to decide which semantic relation holds between the terms of the pair .",
    "the five possible semantic relations are synonymy ( syn ) , antonymy ( ant ) , hypernymy ( hyper ) , meronymy ( part_of ) and no semantic relation at all ( random ) .",
    "our system managed to achieve good results in discriminating between related and random pairs in the first subtask , but unfortunately it struggled in the second one , also due to the high difficulty of the task itself .",
    "in particular , the recall for some of the semantic relations of interest seems to be extremely low , suggesting that our unsupervised distributional measures do not provide sufficient information to characterize them , and that it could be probably useful to integrate such scores with other sources of evidence ( e.g. information on lexical patterns of word co - occurrence ) .",
    "the paper is organized as follows : in section 2 , we summarize related works on the task of semantic relation identification ; in section 3 , we introduce our system , by describing the classifier and the features .",
    "finally , in section 4 we present and discuss our results .",
    "distinguishing between related and unrelated words and , then , discriminating among semantic relations are very important tasks in nlp , and they have a wide range of applications , such as textual entailment , text summarization , sentiment analysis , ontology learning , and so on .",
    "for this reason , several systems over the last few years have been proposed to tackle this problem , using both unsupervised and supervised approaches ( see the works of lenci and benotto and shwartz et al .",
    "on hypernymy ; weeds et al . and santus et al .",
    "on hypernymy and co - hyponymy ; mohammad et al . and santus et al . on antonymy ) .",
    "however , many of these works focus on a single semantic relation , e.g. antonymy , and describe methods or measures to set it apart from other relations .",
    "there have not been many attempts , at the best of our knowledge , to deal with corpus - based semantic relation identification in a multiclass classification task .",
    "few exceptions include the works by turney on similarity , antonymy and analogy , and by pantel and pernacchiotti on espresso , a weakly supervised , pattern - based algorithm .",
    "both these systems are based on patterns , which are known to be more precise than dsms , even though they suffer from lower recall ( i.e. they in fact require words to co - occur in the same sentence ) .",
    "dsms , on the other hand , offer higher recall at the cost of lower precision : while they are strong in identifying distributionally similar words ( i.e. nearest neighbors ) , they do not offer any principled way to discriminate between semantic relations ( i.e. the nearest neighbors of a word are not only its synonyms , but they also include antonyms , hypernyms , and so on ) .    the attempts to provide dsms with the ability of automatically identifying semantic relations include a large number of unsupervised methods @xcite , which are unfortunately far from achieving the perfect accuracy . in order to achieve higher performance , supervised methods",
    "have been recently adopted , also thanks to their ease @xcite .",
    "many of them rely on distributional word vectors , either concatenated or combined through algebraic functions .",
    "others use as features either patterns or scores from the above - mentioned unsupervised methods .",
    "while these systems generally obtain high performance in classification tasks involving a single semantic relation , they have rarely been used on multiclass relation classification . on top of it , some scholars have questioned their ability to really learn semantic relations @xcite , claiming that they rather learn some lexical properties from the word vectors they are trained with .",
    "this was also confirmed by an experiment carried out by santus et al . , showing that up to 100% synthetic switched pairs ( i.e. _ banana - animal _ ; _ elephant - fruit _ ) are misclassified as hypernyms if the system is not provided with some of these negative examples during training .",
    "recently , count based vectors have been substituted by prediction - based ones , which seem to slightly improve the performance in some tasks , such as similarity estimation @xcite , even though levy et al . demonstrated that these improvements were most likely due to the optimization of hyperparameters that were instead left unoptimized in count - based models ( for an overview on word embeddings , see gladkova et al . ) .",
    "on top of it , when combined with supervised methods , the low interpretability of their dimensions makes it even harder to understand what the classifiers actually learn @xcite .    finally , the recent attempt of shwartz et al . of combining patterns and distributional information achieved extremely promising results in hypernymy identification .",
    "our system , root18 , is a random forest classifier @xcite and it is based on the 18 features described in the following subsections . the system in its best setting makes use of the gini impurity index as the splitting criterion and has 10 as the maximum tree depth .",
    "the half of the total number of features were considered for each split .",
    "our data come from _ ukwac _",
    "@xcite , a 2 billion tokens corpus of english built by crawling the .uk internet domain . for the extraction of our features",
    ", we generated several distributional spaces , which differ according to the window size and to the statistical association measure that was used to weight raw co - occurrences .",
    "since we obtained the best performances with window size 2 and positive pointwise mutual information @xcite , we report the results only for this setting .      [ [ frequency ] ] frequency    it is a basic property of words and it is a very discriminative information . in this type of task , it proved to be competitive in identifying the directionality of pairs of hypernyms @xcite , since we expect hypernyms to have higher frequency than hyponyms . for each pair , we computed three features : the frequency of each word ( _ freq1,2 _ ) and their difference ( _ difffreq _ ) .",
    "[ [ co - occurrence ] ] co - occurrence    we compute the co - occurrence frequency ( _ cooc _ ) between the two terms in each pair .",
    "this measure has been claimed to be particularly useful to spot antonyms @xcite , since they are expected to occur in the same sentence more often than chance ( e.g. _ are you friend or foe ? _ ) .",
    "[ [ entropy ] ] entropy    in information theory , this score is related to the informativeness of a message : the lower its entropy , the higher its informativeness @xcite . moreover",
    ", subordinate terms tend to have higher amounts of informativeness than superordinate ones .",
    "we computed the entropy of each word in the pair ( _ entr1,2 _ ) , plus the difference between entropies ( _ diffentr _ ) .",
    "[ [ cosine - similarity ] ] cosine similarity    it is a standard measure in dsms to compute similarity between words @xcite .",
    "this measure is very useful to discriminate between related and unrelated terms .",
    "@xmath0    [ [ linsimilarity ] ] linsimilarity    linsimilarity @xcite is a different similarity measure , computed as the ratio of shared context between _ u _ and _ v _ to the contexts of each word : @xmath1+\\vec{v}[c]}{\\sum_{c \\in \\vec{u } } \\vec{u}[c ] + \\sum_{c \\in \\vec{v } } \\vec{v}[c]}\\ ] ]    [ [ directional - similarity - measures ] ] directional similarity measures    we extracted several directional similarity measures that were proposed to detect hypernyms , such as _ weedsprec _ , _ cosweeds _ , _ clarkede _ and _ invcl _ ( for a review , see lenci and benotto ) .",
    "they are all based on the _ distributional inclusion hypotesis _ , according to which if a word _ u _ is semantically narrower to _ v _ , then a significant number of the salient features of _ u _ will be included also in _",
    "[ [ apsyn ] ] apsyn    this measure and the following _ apant _ do not rely on the full distribution of words , but on the top _",
    "n _ most related contexts of the words according to some statistical association measure .",
    "apsyn @xcite computes a weighted intersection of the top _",
    "n _ context of the target words : @xmath2 that is , for every feature _ f _ included in the intersection between the top n features of @xmath3 and @xmath4 ( @xmath5 , @xmath6 respectively ) , the measure adds 1 divided by the average rank of the feature in the rankings of the top n features of @xmath3 and @xmath4 .    [ [ apant ] ] apant    _ apant _ @xcite is defined as the inverse of apsyn .",
    "this unsupervised measure tries to discriminate between synonyms and antonyms by relying on the hypothesis that words with similar distribution ( i.e. high vector cosine ) that do not share their most relevant contexts ( i.e. what apsyn computes ) are likely to be antonyms . for each pair , we computed apsyn and apant for the top 1000 and for the top 100 contexts .",
    "[ [ same - pos ] ] same pos    we realized that many of the random pairs in the data included words with different parts of speech .",
    "therefore , we decided to add a boolean value to our set of features : 1 if the most frequent pos of the words in the pair were the same , 0 otherwise .",
    "the task organizers provided a training and a test set extracted from evalution 1.0 , a resource that was specifically designed for evaluating systems on the identification of semantic relations @xcite .",
    "evalution 1.0 was derived from wordnet @xcite and conceptnet @xcite and it consists of almost 7500 word pairs , instantiating several semantic relations .",
    "the training and the test set included , respectively , 3054 and 4260 word pairs and they are lexical - split , that is , the two sets do not share any pair . since words were not tagged , we performed pos - tagging with the treetagger @xcite .",
    "[ my - label1 ]    .precision , recall and f - measure scores for subtask 1 and 2 .",
    "the numbers between parentheses in the root18 rows refer to the number of estimators used by the classifier . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]",
    "results are much less convincing for subtask 2 .",
    "in particular , the recall values are extremely low , especially for some of the semantic relations : part_of , for example , is often below 0.15 . for such relation",
    "we have no dedicated features in our system , so the difficulty in identifying meronyms are not a surprise . on the other hand , root18 showed the benefits of the inclusion of several measures targeting hypernymy , since the latter is the most accurately recognized relation ( precision often @xmath7 0.4 ) , recording also the higher recall ( always @xmath7 0.3 , even in the worst performing models ) .    the performance did not show any particular improvement by increasing the number of the decision trees , so that our best overall results are obtained by the model with 500 estimators ( precision = 0.343 , recall = 0.218 and f - score = 0.261 ) .",
    "as for the contributions of the single features , apsyn1000 ( 0.19 ) and cosine ( 0.09 ) are still the top ones , followed by cosweeds ( 0.07 ) and apant1000 ( 0.06 ) .    table 4 describes the confusion matrix , which shows that randoms are properly working as distractors for the model , leading to a large number of misclassification . synonyms",
    "are often confused with hypernyms and this might be due to the fact that the difference between the two is subtle .",
    "these results suggest that measures based on the distributional inclusion hypothesis are not always efficient in discriminating between synonyms and hypernyms .",
    "antonyms are confused with hypernyms and _ vice versa _ , which might be due to the fact that neither share their most relevant features , obtaining therefore similar apant scores @xcite .",
    "meronyms , finally , are mostly confused with hypernyms , which is almost surely due to the generality spread that characterize both relations and that is captured by both frequency and entropy in our system .",
    "our results clearly highlight the difficulty of dsms in discriminating between several semantic relations at once .",
    "such models , in fact , rely on a vague definition of semantic similarity ( i.e. distributional similarity ) which does not offer any principled way to distinguish among different types of semantic relations .    nonetheless , it is still feasible for traditional dsms to achieve good performances on the recognition of taxonomical relations @xcite , for which metrics can be defined on the basis of feature inclusion , of context informativeness etc .",
    "for other relations , such as antonymy and meronymy , it is not easy to define measures based on distributional similarity ( for the latter relation , it is difficult even to find an univocal definition : see morlane - hondre ): apant works relatively well in discriminating antonyms from synonyms , but  as noticed by santus et al .",
    " this measure has also a bias towards hypernyms , which explains why these are often confused . a possible solution , in our view ,",
    "would be the integration of dsms with pattern - based information , in a way that is already being shown by some of the current state - of - the - art systems ( see , for example , shwartz et al . ) .",
    "such integration has the advantage of combining the precision of the patterns with the high recall of dsms .",
    "finally , we may assume that also the configuration of the original dataset could contribute to our results , since some pairs in the dataset have ambiguous words and the target relations hold for only one of the their meanings . disambiguating the pairs , at least by part - of - speech ,",
    "would certainly help in improving the results .",
    "a simple method might consist in computing the vector cosine for the pairs with the target words declined in all possible pos ( i.e. vv , nn , jj ) and then maintain in the dataset only the pair with the higher value .",
    "this work has been carried out thanks to the support of the a*midex grant ( n anr-11-idex-0001 - 02 ) funded by the french government `` investissements davenir '' program ."
  ],
  "abstract_text": [
    "<S> in this paper , we describe root 18 , a classifier using the scores of several _ unsupervised distributional measures _ as features to discriminate between semantically related and unrelated words , and then to classify the related pairs according to their semantic relation ( i.e. _ synonymy _ , _ antonymy _ , _ hypernymy _ , _ part - whole meronymy _ ) . </S>",
    "<S> our classifier participated in the cogalex - v shared task , showing a solid performance on the first subtask , but a poor performance on the second subtask . </S>",
    "<S> the low scores reported on the second subtask suggest that distributional measures are not sufficient to discriminate between multiple semantic relations at once . </S>"
  ]
}