{
  "article_text": [
    "researchers have recently started investigating sequence - to - sequence ( seq2seq ) models for dialogue applications .",
    "these models typically use neural networks to both represent dialogue histories and to generate or select appropriate responses .",
    "such models are able to leverage large amounts of data in order to learn meaningful natural language representations and generation strategies , while requiring a minimum amount of domain knowledge and hand - crafting .",
    "although the seq2seq framework is different from the well - established goal - oriented setting  @xcite , these models have already been applied to several real - world applications , with microsoft s system xiaoice  @xcite and google s smart reply system  @xcite as two prominent examples .",
    "researchers have mainly explored two types of seq2seq models .",
    "the first are generative models , which are usually trained with cross - entropy to generate responses word - by - word conditioned on a dialogue context  @xcite .",
    "the second are discriminative models , which are trained to select an appropriate response from a set of candidate responses @xcite . in a related strand of work ,",
    "researchers have also investigated applying neural networks to the different components of a standard dialogue system , including natural language understanding , natural language generation , dialogue state tracking and evaluation @xcite . in this paper , we focus on generative models trained with cross - entropy .",
    "one weakness of current generative models is their limited ability to incorporate rich dialogue context and to generate meaningful and diverse responses @xcite . to overcome this challenge ,",
    "we propose new generative models that are better able to incorporate long - term dialogue history , to model uncertainty and ambiguity in dialogue , and to generate responses with high - level compositional structure .",
    "our experiments demonstrate the importance of the model architecture and the related inductive biases in achieving this improved performance .",
    "represent natural language utterances .",
    "variables @xmath0 represent discrete or continuous stochastic latent variables .",
    "( a ) : classic lstm model , which uses a shallow generation process .",
    "this is problematic because it has no mechanism for incorporating uncertainty and ambiguity and because it forces the model to generate compositional and long - term structure incrementally on a word - by - word basis .",
    "( b ) : vhred expands the generation process by adding one latent variable for each utterance , which helps incorporate uncertainty and ambiguity in the representations and generate meaningful , diverse responses .",
    "( c ) : mrrnn expands the generation process by adding a sequence of discrete stochastic variables for each utterance , which helps generate responses with high - level compositional structure.,scaledwidth=80.0% ]",
    "* hred * : the hierarchical recurrent encoder - decoder model ( hred ) @xcite is a type of seq2seq model that decomposes a dialogue into a two - level hierarchy : a sequence of utterances , each of which is a sequence of words .",
    "hred consists of three recurrent neural networks ( rnns ) : an _ encoder _ rnn , a _",
    "rnn and a _ decoder _ rnn .",
    "each utterance is encoded into a real - valued vector representation by the _ encoder _ rnn .",
    "these utterance representations are given as input to the _ context _ rnn , which computes a real - valued vector representation summarizing the dialogue at every turn .",
    "this summary is given as input to the _ decoder _ rnn , which generates a response word - by - word .",
    "unlike the rnn encoders in previous seq2seq models , the _ context _ rnn is only updated once every dialogue turn and uses the same parameters for each update .",
    "this gives hred an inductive bias that helps incorporate long - term context and learn invariant representations .",
    "* vhred * : the latent variable hierarchical recurrent encoder - decoder model ( vhred ) @xcite is an hred model with an additional component : a high - dimensional stochastic latent variable at every dialogue turn . as in hred , the dialogue context is encoded into a vector representation using _ encoder _ and _ context _ rnns . conditioned on the summary vector at each dialogue turn ,",
    "vhred samples a multivariate gaussian variable , which is given along with the summary vector as input to the _ decoder _ rnn .",
    "the multivariate gaussian latent variable allows modelling ambiguity and uncertainty in the dialogue through the latent variable distribution parameters ( mean and variance parameters ) .",
    "this provides a useful inductive bias , which helps vhred encode the dialogue context into a real - valued embedding space even when the dialogue context is ambiguous or uncertain , and it helps vhred generate more diverse responses .",
    "* mrrnn * : the multiresolution rnn ( mrrnn ) @xcite models dialogue as two parallel stochastic sequences : a sequence of high - level coarse tokens ( coarse sequences ) , and a sequence of low - level natural language words ( utterances ) .",
    "the coarse sequences follow a latent stochastic process ",
    "analogous to hidden markov models  which conditions the utterances through a hierarchical generation process .",
    "the hierarchical generation process first generates the coarse sequence , and conditioned on this generates the natural language utterance . in our experiments , the coarse sequences are defined as either noun sequences or activity - entity pairs ( predicate - argument pairs ) extracted from the natural language utterances . the coarse sequences and utterances are modelled by two separate hred models .",
    "the hierarchical generation provides an important inductive bias , because it helps mrrnn model high - level , compositional structure and generate meaningful and on - topic responses .",
    "we apply our generative models to dialogue response generation on the ubuntu dialogue corpus  @xcite . for each example , given a dialogue context , the model must generate an appropriate response .",
    "we also present results on twitter in the appendix .",
    "this task has been studied extensively in the recent literature @xcite .",
    "[ 1]>p#1    l80mml50 mm * dialogue context * & * model response * + hello i tried to install ubuntu studio but i get w : failed to fetch < url >  404 not found when apt - get update @xmath1 the ppa you added is not online & * mrrnn act . -ent .",
    ": * try a different mirror * mrrnn noun : * something * vhred : * but",
    "i do nt know how to add it * hred : * how do i do that ?",
    "+ hi community .. is there any difference between ``  /.bashrc '' ``  /.bashrc .",
    "local '' @xmath1 yes , they re different files :-) the default  /.bashrc",
    "sources  /.bashrc .",
    "sorry i could undersatnd ... if i write on terminal `` gedit  /.bashrc '' it opens a file but how i open ``  /.bashrc .",
    "local '' @xmath1 `` gedit  /.bashrc .",
    "local '' ... @xmath1 but it just open blank file nothing inside it & * mrrnn act .- ent .",
    ": * open up gedit  /.bashrc , there is a file called `` something '' * mrrnn noun : * the file is empty .. * vhred : * it s in  /.bashrc or  /.bashrc * hred : * what are you trying to do ? +",
    "hey guys what do you in general use for irc something ubuntu xchat or xchat - gnome ? @xmath1 without -gnome .",
    "that is just cut down @xmath1 you mean drop xchat - gnome and go with xchat ? & * mrrnn act . -ent .",
    ": * i m using xchat right now * mrrnn noun : * what is xchat - gnome ?",
    "* vhred : * correct * hred : * yes + when setting up rules with iptables command only writes changes this file `` etciptables .",
    "i ask this so i can backup before messing anything @xmath1 sudo iptables - save something .",
    "dat to backup your rules restore with sudo iptables - restore <  something .",
    "dat & * mrrnn act . -ent .",
    ": * i do nt see any reason why you need iptables to do that * mrrnn noun : * are you using ubuntu ?",
    "* vhred : * thx * hred : * thanks +    * corpus * : the ubuntu dialogue corpus consists of about half a million dialogues extracted from the _ # ubuntu _ internet relayed chat ( irc ) channel .",
    "users entering this chat channel usually have a specific technical problem .",
    "typically , users first describe their problem , and other users try to help them resolve it .",
    "the technical problems range from software - related and hardware - related issues ( e.g. installing packages , fixing broken drivers ) to informational needs ( e.g. finding software ) .",
    "* evaluation * : we carry out an in - lab human study to evaluate the model responses .",
    "we recruit 5 human evaluators .",
    "we show each evaluator between 30 and 40 dialogue contexts with the ground truth response , and 4 candidate model responses . for each example",
    ", we ask the evaluators to compare the candidate responses to the ground truth response and dialogue context , and rate them for fluency and relevancy on a scale 04 , where 0 means incomprehensible or no relevancy and 4 means flawless english or all relevant .",
    "in addition to the human evaluation , we also evaluate dialogue responses w.r.t .",
    "the activity - entity metrics proposed by  @xcite .",
    "these metrics measure whether the model response contains the same activities ( e.g. download , install ) and entities ( e.g. ubuntu , firefox ) as the ground truth responses .",
    "models that generate responses with the same activities and entities as the ground truth responses  including expert responses , which often lead to solving the user s problem  are given higher scores .",
    "sample responses from each model are shown in table  [ table : ubuntu - examples - small ] .",
    ".ubuntu evaluation using f1 metrics w.r.t .",
    "activities and entities ( mean scores @xmath2 confidence intervals ) , and human fluency and human relevancy scores given on a scale 0 - 4 ( @xmath3 indicates scores significantly different from baseline models at @xmath4 confidence ) [ cols=\"<,^,^,^,^,^,^,^\",options=\"header \" , ]     * results * : the results are given in table  [ tabel : ubuntu_results ] . the mrrnns perform substantially better than the other models w.r.t .",
    "both the human evaluation study and the evaluation metrics based on activities and entities .",
    "mrrnn with noun representations obtains an f1 entity score at @xmath5 , while all other models obtain less than half f1 scores between @xmath6 , and human evaluators consistently rate its fluency and relevancy significantly higher than all the baseline models .",
    "mrrnn with activity representations obtains an f1 activity score at @xmath7 , while all other models obtain less than half f1 activity scores between @xmath8 , and performs substantially better than the baseline models w.r.t . the f1 entity score .",
    "this indicates that the mrrnns have learned to model high - level , goal - oriented sequential structure in the ubuntu domain .",
    "followed by these , vhred performs better than the hred and lstm models w.r.t . both activities and entities .",
    "this shows that vhred generates more appropriate responses , which suggests that the latent variables are useful for modeling uncertainty and ambiguity .",
    "finally , hred performs better than the lstm baseline w.r.t . both activities and entities , which underlines the importance of representing longer - term context .",
    "these conclusions are confirmed by additional experiments on response generation for the twitter domain ( see appendix ) .",
    "we have presented generative models for dialogue response generation .",
    "we have proposed architectural modifications with inductive biases towards 1 ) incorporating longer - term context , 2 ) handling uncertainty and ambiguity , and 3 ) generating diverse and on - topic responses with high - level compositional structure .",
    "our experiments show the advantage of the architectural modifications quantitatively through human experiments and qualitatively through manual inspections .",
    "these experiments demonstrate the need for further research into generative model architectures .",
    "although we have focused on three generative models , other model architectures such as memory - based models @xcite and attention - based models @xcite have also demonstrated promising results and therefore deserve the attention of future research .    in another line of work ,",
    "researchers have started proposing alternative training and response selection criteria  @xcite .",
    "@xcite propose ranking candidate responses according to a mutual information criterion , in order to incorporate dialogue context efficiently and retrieve on - topic responses .",
    "@xcite further propose a model trained using reinforcement learning to optimize a hand - crafted reward function .",
    "both these models are motivated by the lack of _ diversity _ observed in the generative model responses .",
    "similarly , @xcite propose a hybrid model  combining retrieval models , neural networks and hand - crafted rules  trained using reinforcement learning to optimize a hand - crafted reward function .",
    "in contrast to these approaches , without combining several models or having to modify the training or response selection criterion , vhred generates more diverse responses than previous models .",
    "similarly , by optimizing the joint log - likelihood over sequences , mrrnns generate more appropriate and on - topic responses with compositional structure .",
    "thus , improving generative model architectures has the potential to compensate  or even remove the need  for hand - crafted reward functions .",
    "at the same time , the models we propose are not necessarily better language models , which are more efficient at compressing dialogue data as measured by word perplexity .",
    "although these models produce responses that are preferred by humans , they often result in higher test set perplexity than traditional lstm language models .",
    "this suggests maximizing log - likelihood ( i.e. minimizing perplexity ) is not a sufficient training objective for these models .",
    "an important line of future work therefore lies in improving the objective functions for training and response selection , as well as learning directly from interactions with real users .",
    "* corpus * : we experiment on a twitter dialogue corpus  @xcite containing about one million dialogues .",
    "the task is to generate utterances to append to existing twitter conversations .",
    "this task is typically categorized as a non - goal - driven task , because any fluent and on - topic response may be adequate . * evaluation * : we carry out a human study on amazon mechanical turk ( amt ) .",
    "we show human evaluators a dialogue context along with two potential responses : one response generated from each model conditioned on the dialogue context . we ask evaluators to choose the response most appropriate to the dialogue context .",
    "if the evaluators are indifferent , they can choose neither response . for each pair of models we conduct two experiments : one where the example contexts contain at least @xmath9 unique tokens ( _ long context _ ) , and one where they contain at least @xmath10 ( not necessarily unique ) tokens ( _ short context _ ) .",
    "we experiment with the lstm , hred and vhred models , as well as a tf - idf retrieval - based baseline model .",
    "we do not experiment with the mrrnn models , because we do not have appropriate coarse representations for this domain .    * results * : the results given in table  [ table : human - study - twitter ] show that vhred is strongly preferred in the majority of the experiments . in particular , vhred is strongly preferred over the hred and tf - idf baseline models for both short and long context settings .",
    "vhred is also strongly preferred over the lstm baseline model for long contexts , although the lstm model is preferred over vhred for short contexts.for short contexts , the lstm model is often preferred over vhred because the lstm model tends to generate very _ generic _ responses .",
    "generic _ or _ safe _ responses are reasonable for a wide range of contexts , but are not useful when applied through - out a dialogue , because the user would loose interest in the conversation . in conclusion , vhred performs substantially better overall than competing models , which suggests that the high - dimensional latent variables help model uncertainty and ambiguity in the dialogue context and help generate meaningful responses .",
    "lccc * opponent * & * wins * & * losses * & * ties * +   + vhred vs lstm & @xmath11 @xmath12 & @xmath13 @xmath14 & @xmath15 @xmath16 + vhred vs hred & @xmath17 @xmath18 & @xmath19 @xmath20 & @xmath21 @xmath22 + vhred vs tf - idf & @xmath23 @xmath24 & @xmath25 @xmath22 & @xmath26 @xmath27 +   + vhred vs lstm & @xmath28 @xmath29 & @xmath30 @xmath31 & @xmath32 @xmath33 + vhred vs hred & @xmath34 @xmath18 & @xmath35 @xmath20 & @xmath36 @xmath20 + vhred vs tf - idf & @xmath37 @xmath38 & @xmath39 @xmath31 & @xmath40 @xmath41 +"
  ],
  "abstract_text": [
    "<S> researchers have recently started investigating deep neural networks for dialogue applications . in particular , </S>",
    "<S> generative sequence - to - sequence ( seq2seq ) models have shown promising results for unstructured tasks , such as word - level dialogue response generation . </S>",
    "<S> the hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies , while requiring a minimum amount of domain knowledge and hand - crafting . </S>",
    "<S> an important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses . in support of this goal </S>",
    "<S> , we review recently proposed models based on generative encoder - decoder neural network architectures , and show that these models have better ability to incorporate long - term dialogue history , to model uncertainty and ambiguity in dialogue , and to generate responses with high - level compositional structure . </S>"
  ]
}