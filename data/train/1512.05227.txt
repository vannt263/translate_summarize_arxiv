{
  "article_text": [
    "fine - grained visual categorization ( fgvc ) has received increased interest from the computer vision community in recent years . by definition ,",
    "fgvc , as a sub - field of object recognition , aims to distinguish subordinate categories within an entry - level category .",
    "for example , in fine - grained flower categorization @xcite , we want to identify the species of a flower in an image , such as  nelumbo nucifera ( lotus flower ) , \"  tulip \" or  cherry blossom . \"",
    "other examples include classifying different types of plants @xcite , birds @xcite , dogs @xcite , insects @xcite , galaxies @xcite ; recognizing brand , model and year of cars @xcite ; and face identification @xcite .",
    "most existing fgvc methods fall into a classical two - step scheme : feature extraction followed by classification @xcite .",
    "since these two steps are independent , the performance of the whole system is often suboptimal compared with an end - to - end system using convolutional neural networks ( cnn ) that can be globally optimized via back - propagation @xcite .",
    "therefore , in this work , we focus on developing an end - to - end cnn - based method for fgvc . however , compared with general purpose visual categorization , there are three main challenges arising when using such end - to - end cnn - based systems for fgvc .",
    "firstly , * lack of training data*. current commonly used cnn architectures such as alexnet @xcite , vggnet @xcite , googlenet - inception @xcite and resnet @xcite have large numbers of parameters that require vast amounts of training data to achieve reasonably good performance .",
    "commonly used fgvc databases @xcite , however , are relatively small , typically with less than a few tens of thousands of training images .    secondly ,",
    "compounding the above problem , fgvc can involve * large numbers of categories*. for example , arguably , it is believed that there are more than @xmath0 species of flowers in the world @xcite . as a point of reference ,",
    "modern face identification systems need to be trained on face images coming from millions of different identities ( categories ) . in such scenarios , the final fully connected layer of a cnn before the softmax layer would contain too many nodes , thereby making the training infeasible .",
    "lastly , * high intra - class vs.  low inter - class variance*. in fgvc , we confront two somewhat conflicting requirements : distinguishing visually similar images from different categories while allowing reasonably large variability ( pose , color , lighting conditions , etc . ) within a category . as an example illustrated in fig .",
    "[ fig : flower ] , images from different categories could have similar shape and color . on the other hand , sometimes images within same category",
    "can be very dissimilar due to nuisance variables .",
    "in such a scenario , since approaches that work well on generic image classification often focus on inter - class differences rather than intra - class variance , directly applying them to fgvc could make visually similar categories hard to be distinguished .        in this paper",
    ", we propose a framework that aims to address all three challenges .",
    "we are interested in the following question : given an fgvc task with its associated training and test set , are we able to improve the performance by bootstrapping more training data from the web ? in light of this , we propose a unified framework using deep metric learning with humans in the loop , illustrated in fig .",
    "[ fig : overview ] .",
    "we use an iterative approach for dataset bootstrapping and model training . in each round , the model trained from last round is used to generate fine - grained confidence scores ( probability distribution ) for all the candidate images on categories .",
    "only images with highest confidence score larger than a threshold are kept and put into the corresponding category .",
    "then , for each category , by comparing with exemplar images and category definitions , human labelers remove false positives ( hard negatives ) .",
    "images that pass the human filtering will be included into the dataset as new ( vetted ) data .",
    "finally , we re - train our classification model by incorporating newly added data and also leveraging the hard negatives marked by human labelers . the updated model will be used for the next round of dataset bootstrapping .",
    "although we focus on flower categorization in this work , the proposed framework is applicable to other fgvc tasks .    in order to capture within - class variance and",
    "utilize hard negatives as well , we propose a triplet - based deep metric learning approach for model training .",
    "a novel metric learning approach enables us to learn low - dimensional manifolds with multiple anchor points for each fine - grained category .",
    "these manifolds capture within - category variances and remain discriminative to other categories .",
    "the data can be embedded into a feature space with dimension much lower than the number of categories . during the classification",
    ", we generate the categorical confidence score by using multiple anchor points located on the manifolds .",
    "in summary , the proposed framework handles all three challenges in fgvc mentioned above . using the proposed framework , we are able to grow our training set and get a better fine - grained classifier as well .",
    "* fine - grained visual categorization ( fgvc)*. many approaches have been proposed recently for distinguishing between fine - grained categories .",
    "most of them @xcite use two independent steps : feature extraction and classification .",
    "fueled by the recent advances in convolutional neural networks ( cnn ) @xcite , researchers have gravitated to cnn features @xcite rather than traditional hand - crafted features such as llc @xcite or fisher vectors @xcite . sometimes , the information from segmentation @xcite , part annotations @xcite , or both @xcite is also used during the feature extraction .",
    "current state - of - the - art methods @xcite all adopt cnn - based end - to - end schemes that learn feature representations from data directly for classification .",
    "although our method also draws upon a cnn - based scheme , there are two major differences .",
    "1 ) rather than using softmax loss , we aim to find a low - dimensional feature embedding for classification .",
    "2 ) we incorporate humans into the training loop , with the human - provided input contributing to the training of our model .",
    "* fine - grained visual datasets*. popular fine - grained visual datasets @xcite are relatively small scale , typically consisting of around 10 thousand training images or less .",
    "there are some efforts recently in building large - scale fine - grained datasets @xcite .",
    "we differ from these efforts both in terms of our goal and our approach .",
    "instead of building a dataset from scratch , we aim to bootstrap more training data to enlarge the existing dataset we have . in addition , instead of human labeling , we also use a classifier to help during the dataset bootstrapping .",
    "the most similar work in terms of dataset bootstrapping comes from yu et al .",
    "@xcite , which builds a large - scale scene dataset with @xmath1 common categories using deep learning with humans in the loop .",
    "however , we are bootstrapping a fine - grained dataset with much more categories ( 620 ) .",
    "moreover , instead of a dataset , we can also get a model trained with combined human - machine efforts .",
    "* deep metric learning*. another line of related work is metric learning with cnns using pairwise @xcite or triplet constraints @xcite .",
    "the goal is to use a cnn with either pairwise ( contrastive ) or triplet loss to learn a feature embedding that captures the semantic similarity among images .",
    "compared with traditional metric learning methods that rely on hand - crafted features @xcite , deep metric learning directly learns from data and achieves much better performance .",
    "recently , it has been successfully applied to variety of problems including face recognition and verification @xcite , image retrieval @xcite , semantic hashing @xcite , product design @xcite , geo - localization @xcite and style matching @xcite .",
    "in contrast with previous methods , we propose a novel strategy that enables the learning of continuous manifolds .",
    "in addition , we also bring humans in the loop and leverage their inputs during metric learning .",
    "one of the main challenges in fine - grained visual recognition is the scarcity of training data .",
    "labeling of fine - grained categories is tedious because it calls for experts with specialized domain knowledge .",
    "this section presents a bootstrapping framework on how to grow a small scale , fine - grained dataset in an efficient manner .      in this first step",
    ", we wish to collect a large pool of candidate images for fine - grained subcategories under a coarse category , , flowers .",
    "the most intuitive way to crawl images could resort to image search engines like google or bing .",
    "however , those returned images are often iconic , presenting a single , centered object with a simple background , which is not representative of natural conditions .",
    "on the other hand , with the prevalence of powerful personal cameras and social networks , people capture their day - to - day photos and share them via platforms like instagram or flickr .",
    "those natural images uploaded by web users offer us a rich source of candidate images , often with tags that hint at the semantic content .",
    "so if we search  flower \" on instagram , a reasonable portion of returned images should be flower images .",
    "naturally , we will need a filtering process to exclude the non - flower images .",
    "we first downloaded two million images tagged with ",
    "flower \" via the instagram api . to remove the images that clearly contain no flowers , we pre - trained a flower classifier based on googlenet - inception @xcite with @xmath2k images . by feeding all the downloaded images to this classifier ,",
    "we retained a set of nearly one million images , denoted as @xmath3 , with confidence score larger than @xmath4 .      given an initial fine - grained dataset @xmath5 of @xmath6 categories and a candidate set @xmath3 , the goal of dataset bootstrapping is to select a subset @xmath7 of the images from @xmath3 that match with the original @xmath6 categories .",
    "we divided the candidate set into a list of @xmath8 subsets : @xmath9 and used an iterative approach for dataset bootstrapping with @xmath8 iterations in total .",
    "each iteration consists of three steps .",
    "consider the @xmath10-th iteration .",
    "first , we trained a cnn - based classifier ( see sec .",
    "[ sec : metric ] ) using the seed dataset @xmath11 , where @xmath12 contains the hard negatives from the previous step .",
    "second , using this classifier , we assigned each candidate image @xmath13 to one of the @xmath6 categories .",
    "images with confidence score larger than @xmath4 form a high quality candidate set @xmath14 for the original @xmath6 categories .",
    "third , we asked human labelers with domain expertise to identify true positives @xmath15 and false positives @xmath16 , where @xmath17 .",
    "exemplar images and category definitions were shown to the labelers .    compared to the traditional process requiring the labeler to select one of @xmath6 categories per image , we asked labelers to focus on a binary decision task which entails significantly less cognitive load . noting that these false positives @xmath16 are very similar to ground - truths , we regard them as hard negatives @xmath18 .",
    "true positives were also included to expand our dataset : @xmath19 for the next iteration .",
    "it is worth mentioning this bootstrapping framework is similar in spirit to the recent work @xcite that used semi - automatic crowdsourcing strategy to collect and annotate videos .",
    "however , the key difference is we design a deep metric learning method ( see sec .  [",
    "sec : metric ] ) that specifically makes the use of the large number of hard negatives @xmath20 in each iteration .",
    "we frame our problem as a deep metric learning task .",
    "we choose metric learning for mainly two reasons .",
    "first , compared with classic deep networks that use softmax loss in training , metric learning enables us to find a low - dimensional embedding that can well capture high intra - class variance .",
    "second , metric learning is a good way to leverage human - labeled hard negatives .",
    "it is often difficult to get categorical labels for these hard negatives .",
    "they could belong to flower species outside the dataset , or non - flower images .",
    "therefore , directly incorporating human - labeled hard negatives into a multi - way classification scheme such as softmax is infeasible , while it is quite natrual to include them into the metric learning .",
    "[ fig : comparison ] illustrates the differences between cnn with softmax and cnn for metric learning in 3-dimensional feature space . in order to minimize softmax loss",
    ", we try to map all images within the same category to a single point in feature space , which loses the intra - class variance . in this figure",
    ", we try to map category @xmath21 to @xmath22^\\top$ ] , @xmath23 to @xmath24^\\top$ ] and @xmath25 to @xmath26^\\top$ ] , respectively .",
    "we need @xmath6 nodes in final feature layer to represent @xmath6 categories . however , in metric learning , we can learn manifolds and the dimensionality of the feature layer could be much smaller than @xmath6 .",
    "in addition , the manifold can preserve useful intra - class variances such as color and pose .     denotes a group of images within the same category . ]",
    "our goal is to learn a non - linear low - dimensional feature embedding @xmath27 via cnn , such that given two images @xmath28 and @xmath29 , the euclidean distance between @xmath30 and @xmath31 can reflect their semantic dissimilarity ( whether they come from same category or not ) .",
    "typically , people use pairwise or triplet information to learn the feature embedding .    in the pairwise case",
    "@xcite , @xmath27 is learned from a set of image pairs @xmath32 with corresponding labels @xmath33 indicating whether @xmath34 and @xmath35 is similar . in the triplet case @xcite , @xmath27",
    "is learned from a set of image triplets @xmath36 , which constrains the reference image @xmath28 to be more similar with the image @xmath37 of the same category compared with any image @xmath38 of different class .",
    "we can see triplet constraints offer more fine - grained information : by making use of relative comparisons it is adaptive to differing granularity of similarity while the pairwise counterpart is not .",
    "we therefore use triplet information to develop an end - to - end cnn - based approach for fgvc .",
    "the triplet - based deep metric learning framework is illustrated in fig .",
    "[ fig : triplet_framework ] . in each iteration",
    ", the input triplet @xmath39 is sampled from the training set , where image @xmath28 is more similar to @xmath37 relative to @xmath38 .",
    "then the triplet of three images are fed into an identical cnn simultaneously to get their non - linear feature embeddings @xmath30 , @xmath40 and @xmath41 .",
    "the cnn could be any arbitrary architecture such as alexnet @xcite , vggnet @xcite or googlenet - inception @xcite .",
    "since we need to compute the distances in feature space , all the features should be normalized to eliminate the scale differences .",
    "we use @xmath42-normalization for this purpose : @xmath43 .",
    "we use the triplet loss same as wang et al .",
    "@xcite used , which can be expressed as @xmath44 where @xmath45 is a hyper - parameter that controls the distance margin after the embedding .",
    "this hinge loss function will produce a non - zero penalty of @xmath46 if the @xmath42 distance between @xmath28 and @xmath38 is smaller than the @xmath42 distance between @xmath28 and @xmath37 adding a margin @xmath45 in feature space : @xmath47 .",
    "the loss will be back propagated to each layer of the cnn and their corresponding parameters are updated through stochastic gradient descent .     is closer to @xmath37 than it is to @xmath38 .",
    "we train a cnn to preserve this relative ordering under feature embedding @xmath27 . ]",
    "the most challenging part of training a triplet - based cnn lies in the triplet sampling .",
    "since there are @xmath48 possible triplets on a dataset with @xmath49 training data , going through all of them would be impractical for large @xmath49 .",
    "a good triplet sampling strategy is needed to make training feasible .",
    "we observed that during training , if we use randomly sampled triplets , many of them satisfy the triplet constraint well and give nearly zero loss in eqn .",
    "[ eqn : triplet_loss ] .",
    "that is , those easy triplets have no effect in updating model parameters but we waste our time and resources in passing them through the network .",
    "this makes the training process extremely inefficient and unstable : only few examples make contributions to the training within a batch .",
    "therefore , we use an online hard negatives mining scheme : only train on those triplets that violate the triplet constraint and give non - zero loss will be included into the training .",
    "why not simply train from the hardest negatives , i.e. , triplets with the largest @xmath50 ? because there are noisy data in the training set and trying to satisfy them ruins the overall performance .",
    "a similar scenario was also reported in @xcite .    in our framework , instead of using images coming from categories that are different from the reference image , we also incorporate false positives marked by human labelers as hard negative candidates .",
    "those false positives are all misclassified by our model and thus provide us access to an excellent source of hard negatives .      typically , given the reference image @xmath28 , the positive image @xmath37 is sampled from all images within the same category as @xmath28 .",
    "suppose we have a training set with @xmath49 images @xmath51 with labels @xmath52 from @xmath53 categories , where @xmath54 and @xmath55 . in this",
    "setting , considering a reference image @xmath28 within a fine - grained category , suppose the maximum between - class distance for @xmath28 in feature space is bounded by @xmath56 .",
    "that is , @xmath57 , @xmath58 .",
    "in order to have @xmath59 triplet loss for the reference image @xmath28 , we need @xmath60 , @xmath61 .",
    "therefore , @xmath62 where @xmath63 , @xmath64 the squared within - class pairwise distance is bounded by @xmath65 .",
    "thus , by using triplet loss with positives sampled from all images in the same class , we are trying to map all images within that class into a hypersphere with radius @xmath66 .",
    "in fgvc , between - class distances could be very small compared with the within - class distances .",
    "in such a scenario , @xmath67 could be very close to or even less than @xmath59 , which makes the training process very difficult .",
    "however , if we only force positives to be close to the reference locally , we are able to learn an extended manifold rather than a contracted sphere . as illustrated in fig .",
    "[ fig : manifold ] , as the considered local positive region grows , the learned manifold will be increasingly contracted , eventually becoming a sphere when using all positives within the same category .",
    "the triplet sampling strategy we used is summarized in fig .",
    "[ fig : sampling ] . given a reference image @xmath28 ( in the blue bounding box ) we sample positive images @xmath68 ( in the green bounding boxes ) from the local region inside the same category .",
    "negative images @xmath69 are sampled from different categories but we only keep those hard negatives ( marked by red bounding boxes ) : negatives that violate the triplet constraint with respect to the positives we chose .",
    "after the manifold learning step , we adopt a soft voting scheme using anchor points on manifolds for classification . for each category",
    ", the anchor points are generated by k - means clustering on the training set in feature space .",
    "suppose we have @xmath6 categories and each category has @xmath53 anchor points .",
    "the @xmath70-th anchor point for category @xmath10 is represented as @xmath71 , where @xmath72 , @xmath73 .",
    "given an input query image @xmath28 , we first extract its feature embedding @xmath30 from our network , then the confidence score for category @xmath10 is generated as    @xmath74    the predicted label of @xmath28 is the category with the highest confidence score : @xmath75 .",
    "@xmath76 is a parameter controlling the  softness \" of label assignment and closer anchor points play more significant roles in soft voting . if @xmath77 , only the nearest anchor point is considered and the predicted label is  hard \" assigned to be the same as the nearest anchor point . on the other hand , if @xmath78 , all the anchor points are considered to have the same contribution regardless of their distances between @xmath30 .",
    "notice that during the prediction , the model is pre - trained offline and all the anchor points are calculated offline . therefore , given a query image , we only need a single forward pass in our model to extract the features .",
    "since we have learned a low - dimensional embedding , computing the distances between features and anchor points in low - dimensional space is very fast .",
    "as we just described , after metric learning , we use k - means to generate anchor points for representing manifolds and prediction .",
    "this could lead to suboptimal performance .",
    "in fact , we can go one step further to directly learn anchor points by including soft voting into our triplet - based metric learning model , which is illustrated in fig .",
    "[ fig : net ] . for simplicity ,",
    "the data part is not shown .",
    "in contrast to the previous model in fig .",
    "[ fig : triplet_framework ] that uses only triplet information , we also leverage the category label @xmath79 for the reference image @xmath28 and learn anchor points for classification",
    ". we can generate confidence scores @xmath80 for @xmath30 using anchor points @xmath81 by soft voting in eqn .",
    "[ eqn : soft_voting ] .",
    "the classification loss we used is logistic loss on top of confidence score : @xmath82 where @xmath83 is given in eqn .",
    "[ eqn : soft_voting ] by substituting @xmath10 with @xmath79 .",
    "if we have very high confidence score on the true category , @xmath84 , then the loss will be very small : @xmath85 .",
    "the overall loss is the weighted sum of triplet and classification loss :    @xmath86    during training , the loss will be back - propagated to both cnn and anchor points .",
    "anchor point @xmath71 will be updated based on the gradient of the loss with respect to @xmath71 : @xmath87 .",
    "since we combine both triplet and categorical information and also learn anchor points directly for classification , we can expect better performance over the triplet - based model .",
    "in this section , we present experiments to evaluate the proposed deep metric learning approach against traditional two - step metric learning using deep features and commonly used softmax loss on our flower dataset and another publicly available dataset .",
    "we also evaluate the effectiveness of dataset bootstrapping and training with humans in the loop .",
    "we compare the performance of the proposed deep metric learning approach with the following baselines : ( 1 ) softmax loss for classification ( * softmax * ) .",
    "the most commonly used scheme in general purpose image classification .",
    "the deep network is trained from data with categorical label using softmax loss .",
    "we can get label prediction directly from the network output .",
    "( 2 ) triplet loss with naive sampling ( * triplet - naive * ) .",
    "the architecture illustrated in fig .",
    "[ fig : triplet_framework ] with randomly sampled triplets : given a reference image , the triplet is formed by randomly sampling a positive from same category and a negative from different category .",
    "those triplets are directly fed into triplet network . during testing",
    ", we use the classification scheme described in sec .",
    "[ sec : classification ] .",
    "( 3 ) triplet loss with hard negative mining ( * triplet - hn * ) . as discussed in sec .",
    "[ sec : hard_negative ] , instead of feeding all the triplets into the network , we only keep those hard negatives that violate triplet constraint .",
    "( 4 ) triplet loss with manifold learning ( * triplet - m * ) . as mentioned in sec .",
    "[ sec : manifold ] , the positives are sampled locally with respect to the reference image from same category .",
    "( 5 ) triplet loss with anchor points learning ( * triplet - a * ) .",
    "we combine anchor points learning with triplet network as illustrated in fig .  [",
    "fig : net ] . during testing , the network directly output label prediction based on confidence scores .",
    "in addition , we also compared with state - of - the art fgvc approaches on publicly available dataset .",
    "since the network is trained via stochastic gradient descent , in order to do online sampling of triplets , we need to extract features on the entire training set , which is certainly inefficient if we do it for each iteration . therefore , as a trade - off",
    ", we adopt a quasi - online sampling strategy : after every @xmath88 iterations , we pause the training process and extract features on the training set , then based on their euclidean distances in feature space , we do triplet sampling ( local positives and hard negatives ) to generate a list of triplets for next @xmath88 iterations and resume the training process using the newly sampled triplets .",
    "the cnn architecture we used is googlenet - inception @xcite , which achieved state - of - the - art performance in large - scale image classification on imagenet @xcite .",
    "all the baseline models are trained with fine - tuning using pre - trained googlenet - inception on imagenet dataset .",
    "we used caffe @xcite , an open source deep learning framework , for the implementation and training of our networks .",
    "the models are trained on nvidia tesla k80 gpus .",
    "the training process typically took about @xmath89 days on a single gpu to finish @xmath90 iterations with @xmath91 triplets in a batch per each iteration .",
    "we evaluate the baselines on our flower dataset and publicly available cub-200 birds dataset @xcite .",
    "there are several parameters in our model and the best values are found through cross - validation .",
    "for all the following experiments on both dataset , we set the margin @xmath45 in triplet loss to be @xmath92 ; the feature dimension for @xmath27 to be @xmath93 ; the number of anchor points per each category @xmath53 to be @xmath94 ; the @xmath76 in soft voting to be @xmath89 .",
    "we set @xmath95 to make sure that the triplet loss term and the classification loss term in eqn .",
    "[ eqn : overall_loss ] have comparable scale . for the size of positive sampling region",
    ", we set it to be @xmath96 of nearest neighbors within same category .",
    "the effect of positive sampling region size will also be presented later in this section .",
    "* flowers-620 . *",
    "_ flowers-620 _ is the dataset we collected and used for dataset bootstrapping , which contains @xmath97 images from @xmath98 flower species , in which @xmath99 images are used for training .",
    "the performance comparison of mean accuracy is summarized in tab .",
    "[ tab : flower ] .",
    ".performance comparison on our _ flowers-620 _ dataset .",
    "[ cols=\"^,^\",options=\"header \" , ]     [ tab : flower+ins ]    compared with results in tab .  [ tab : flower ] , we got @xmath100 improvement by dataset bootstrapping .",
    "if we look at the breakdown , @xmath101 came from the newly added instagram training images and @xmath102 came from human labeled hard negatives , indicating hard negatives has similar importance as positive images .",
    "on the other hand , softmax only gained @xmath103 by using hard negatives , which verifies our intuition that the triplet network is a better choice for utilizing hard negatives .",
    "the proposed framework fully utilizes combined human - machine efforts to enlarge the dataset as well as train a better model .      for qualitative evaluation purpose , in fig .",
    "[ fig : embedding ] , we show the @xmath104-dimensional embedding of _",
    "_ training set using pca on features extracted from the trained triplet - a model . within",
    "the zoomed in regions , we can observe the effectiveness of our method in capturing high intra - class variances .",
    "for example , flowers from same category with different colors are mapped together in upper right and lower right regions .",
    "-d embedding of _ flower-620 _ training set .",
    "we can observe that intra - class variance is captured in upper right and lower right regions . ]",
    "in this work , we have presented an iterative framework for fine - grained visual categorization and dataset bootstrapping based on a novel deep metric learning approach with humans in the loop .",
    "experimental results have validated the effectiveness of our framework .",
    "we train our model mainly based on triplet information .",
    "although we adopt an effective and efficient online triplet sampling strategy , the training process could still be slow , which is a limitation of our method .",
    "some future work directions could be discovering and labeling novel categories during dataset bootstrapping with a combined human - machine framework or incorporating more information ( , hierarchical information , semantic similarity ) into the triplet sampling strategy ."
  ],
  "abstract_text": [
    "<S> existing fine - grained visual categorization methods often suffer from three challenges : lack of training data , large number of fine - grained categories , and high intra - class vs.  low inter - class variance . in this work </S>",
    "<S> we propose a generic iterative framework for fine - grained categorization and dataset bootstrapping that handles these three challenges . </S>",
    "<S> using deep metric learning with humans in the loop , we learn a low dimensional feature embedding with anchor points on manifolds for each category . </S>",
    "<S> these anchor points capture intra - class variances and remain discriminative between classes . in each round , images with high confidence scores from our model are sent to humans for labeling . by comparing with exemplar images , </S>",
    "<S> labelers mark each candidate image as either a  true positive \" or a  false positive . \" </S>",
    "<S> true positives are added into our current dataset and false positives are regarded as  hard negatives \" for our metric learning model . then the model is re - trained with an expanded dataset and hard negatives for the next round . to demonstrate the effectiveness of the proposed framework , we bootstrap a fine - grained flower dataset with 620 categories from instagram images . </S>",
    "<S> the proposed deep metric learning scheme is evaluated on both our dataset and the cub-200 - 2001 birds dataset . </S>",
    "<S> experimental evaluations show significant performance gain using dataset bootstrapping and demonstrate state - of - the - art results achieved by the proposed deep metric learning methods . </S>"
  ]
}