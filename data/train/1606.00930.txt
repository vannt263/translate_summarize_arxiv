{
  "article_text": [
    "@xcite evaluated 179 different implementations of classification algorithms ( from 17 different families of algoritms ) on 121 public datasets .",
    "we believe that such highly empirical research are very important both for researchers in machine learning and specially for practitioners . for researchers ,",
    "this form of research allows them to focus their efforts on more likely useful endeavors .",
    "for example , if a researcher is interested in developing algorithms for very large classification problems , it is probably more useful to develop a big - data random forest ( which is the family of classification algoritms with best performance according to @xcite ) than to do it for bayesian networks ( mainly naive bayes ) or even nearest neighbors methods , which perform worse than random forests .    for practitioners ,",
    "this form of research is even more important .",
    "practitioners in machine learning will have limited resources , time , and expertise to test many different classification algorithms on their problem , and this form of research will allow them to focus on the most likely useful algorithms .    despite its importance and breath , we believe that @xcite had some `` imperfections '' which we address in this research .",
    "the `` imperfections '' are discussed below as the extensions we carried in this paper :    * we used the same set of datasets , but we transform them so that the problems are all binary .",
    "many classification algorithms are defined only to binary problems , for example the svm family , logistic regression , among others . of course there are meta - extensions of such algorithms to multi - class problems , for example , one - vs - one , one - vs - all , error correcting output coding ( ecoc ) @xcite , stacked generalization @xcite , pairwise coupling @xcite , among others . also , there are alternative formulations for specific binary classifiers to deal with multiclass , for example , @xcite for svm , @xcite for logistic regression , and so on .",
    "+ for the algorithms that are intrinsically binary , the application to multiclass problems poses two problems .",
    "the first is that one has to decide on which meta - extension to use , or if one should use the specific multiclass formulation of the algorithm . in some cases ,",
    "one meta - extension is implemented by default , and the user must be aware that this decision was already made for him / her .",
    "for example , the libsvm default approach is one - vs - one . but a second , more subtle problem is the search for hyperparameters : it is very common that each combination of hyperparameters are tested only once for all classifiers in the one - vs - one solution .",
    "that is , all @xmath0 classifiers in a one - vs - one solution has the same set of hyperparameters , and that may cause a decrease in accuracy in comparison to the case in which classifier is allow to choose its one set of hyperparameter .",
    "thus , on multiclass problems , those intrinsically binary algorithms could be facing many disadvantages .",
    "+ on the issue of binary classifiers , @xcite did not include in their comparisons the gradient boosting machine ( ` gbm ` ) algorithm , considered a very competitive algorithm for classification problems because , as reported in @xcite , the implementation did not work in multiclass problems .",
    "we included ` gbm ` in our comparison .",
    "* we reduced the number of classifiers to only a few classes / algorithms and not different implementations of the same algorithm .",
    "@xcite compared an impressing 179 different classification programs , but it was unclear how many were just different implementations of the same algorithm , and how many were variations within the same `` family '' of algorithms .",
    "we believe that for practitioner and research communities , it is more useful to have an understanding of how different families of algorithms rank in relation to each other . for",
    "the practitioner , which should have more limited access to the different algorithms , this knowledge allow them to order which algorithms should be applied first to their particular problem .",
    "+ in fact , @xcite also perform an analysis of their results based on the algorithm s `` family '' , but they have difficulty of extracting useful information from this analysis , since in most cases , different `` implementations '' in the same family have widely different results . in one analysis , they rank the families by the worse performing member , which does not provide useful information .",
    "but in the end , their conclusions are mainly centered on the families of classifiers , because that is the most useful level of knowledge . from the abstract of the paper : + _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ a few models are clearly better than the remaining ones : random forest , svm with gaussian and polynomial kernels , extreme learning machine with gaussian kernel , c5.0 and avnnet ( a committee of multi - layer perceptrons implemented in r with the caret package ) .",
    "the random forest is clearly the best family of classifiers ( 3 out of 5 bests classifiers are rf ) , followed by svm ( 4 classifiers in the top-10 ) , neural networks and boosting ensembles ( 5 and 3 members in the top-20 , respectively ) . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * we performed a more careful search for hyperparameters for each classification algorithm .",
    "given @xcite daunting task of testing 179 programs , they had to rely on default values for the hyperparameters which may lead to suboptimal results .",
    "since we are working with significantly fewer algorithms , we could spend more time selecting reasonable ranges of values for the hyperparameters .",
    "in particular we tried to limit the number of training steps for the hyperparameter search in 24 so that no algorithm would have an advantage of having more degrees of freedom to adjust to the data ( but in section  [ sec : hyper ] we discuss that some algorithms may allow testing many values of the hyperparameter with just one training and testing step ) . * besides computing when two algorithms are significantly different in their error rates , in the standard null - hypothesis significance tests ( nhst ) , we also use a bayesian analysis to discover when the differences of two algorithms has no `` practical consequences . ''",
    "as far as we know , this is the first time a bayesian anova is used to compare classifiers , which we believe is an important direction in analysis of results in empirical machine learning . but more significantly is the use of `` limits of practical significance '' , that is , the definition of thresholds below which the differences are irrelevant from practical purposes , which goes beyond an more standard `` significance test '' analysis currently used in the machine learning literature .",
    "+ @xcite follow the standard null hypothesis significant test in analyzing their result , but even within this framework , their analysis is not as rigorous as it should have been .",
    "the nist standard for comparing many classifiers across different datasets was proposed by @xcite and it is discussed in section  [ sec : demsar ] .",
    "in particular , when testing the different algorithms for statistical significant differences , the demsar procedure requires one to use the nemenyi test , which is a nonparametric test that performs the appropriate multiple comparison p - value correction . but @xcite used a paired t - test ( a parametric test ) between the first ranked and the following 9 top ranked algorithms , apparently without multiple comparions corrections .",
    "very likely , given the large number of comparisons need to contrast all 179 algorithms , very few , if any of the pairwise comparisons would have been flagged as significant by the nemenyi test .",
    "+ in this paper we followed the full demsar procedure to analyse the results , but we also used the bayesian anova to verify when the differences between the algoritms is not only `` statistically significant '' , but also `` of practical significance '' . + @xcite first two ranked programs are two different implementation of the same algorithm - parallel implementation of random forest ( first ranked ) and a non - parallel implementation ( second ranked ) .",
    "the authors state that in some sense , the difference between the two results should not be `` important '' ( they use the term `` significant '' ) and indeed the statistical significance analysis shows that the difference was not statistically significant , but neither was the next 6 ranked algorithms ( in comparison to the top performing ) . *",
    "we studied the computational costs of running a `` standard '' implementation of the different algorithms . with information regarding the computational cost",
    "a practitioner may want to balance execution time and expected accuracy .",
    "furthermore , this information may encourage the practitioner choose other implementations that the ones tested , and researcher to develop faster implementations of the best ranked algorithms .",
    "in general terms the experimental procedure followed by this research is the following .",
    "each dataset @xmath1 ( the datasets are discussed in section  [ sec : datasets ] ) is divided in half into two subsets @xmath2 and @xmath3 , each with the same proportion of classes . for each subset @xmath4",
    ", we used a a 5-fold cross validation procedure to select the best set of hyperparameters for the procedure @xmath5 , ( @xmath6 ) .",
    "then we trained the whole subset @xmath4 using the procedure @xmath5 with hyperparameters @xmath7 and computed the error rate on the subset @xmath8 , ( where @xmath9 and @xmath10 ) .",
    "we call the error rate of algorithm @xmath5 when learning on the subset @xmath4 , with hyperparameters @xmath6 , when testing on the subset @xmath8 as @xmath11 .",
    "the expected for procedure @xmath5 error on ( whole ) dataset @xmath12 is @xmath13 and it is computed as the average @xmath14 we then performed the analyses described in sections  [ sec : demsar ] and [ sec : banova ] on the sets @xmath15 for all datasets @xmath12 ( described in section  [ sec : datasets ] and for all classification algorithms @xmath5 ( section  [ sec : class - algor ] ) .",
    "we started with the 121 datasets collected from the uci site , processed , and converted by the authors of @xcite into a unified format .",
    "the datasets is derived from the 165 available at uci website in march 2013 , where 56 were excluded by the authors of @xcite . for the remaining 121 , @xcite converted all categorical variables to numerical data , and",
    "each feature was standardized to zero mean and standard deviation equal to 1 .",
    "we downloaded the datasets preprocessed by the authors of @xcite in november 2014 .",
    "we performed the following transformations :    * 65 of the datasets were multiclass problems .",
    "we converted them to a binary classification problem by ordering the classes by their names , and alternatively assigning the original class to the positive and negative classes .",
    "the datasets have different proportions between the positive and negative classes , approximately normally distributed with mean 0.6 and standard deviation of 0.17 .",
    "* 19 of the datasets were separated into different training and test sets and on the november 2014 data , the test set was not standardized .",
    "we standardized the test set ( separately from the train set ) and joined both sets to create a single dataset .",
    "* we removed the 6 datasets with less than 100 datapoints , * the 9 datasets with more than 10.000 datapoints ( more data points for subset ) we searched the hyperparameters on a subset of 5000 datapoints .",
    "the final training was done with the whole subset , and the testing with the other full subset .",
    "thus , in this research we tested the algorithms in 121 - 6 ( very small datasets removed ) = 115 datasets .",
    "details of each dataset are described in [ ap : ds ] .",
    "we used 14 classification algorithms from very different families .",
    "as discussed in the introduction , we argued that one of the possible criticisms to the @xcite paper is that the authors do not distinguish different algorithms from different implementations of that algorithm .    although we do not have a clear or formal definition of what are `` families of algorithms '' we believe that we have a sufficiently diverse collection of algorithms .",
    "we chose not to use algorithms that do not require hyperparameters , such as linear discriminant analysis ( lda ) and logistic regression .",
    "hyperparameters allow the algorithm to better adjust to the data details , and so we believe lda and logistic regressions would be in disadvantages to the other algorithm .",
    "we added regularized versions of these algorithms , which contain at least one hyperparameters .    thus , ` glmnet ` ( l1 and l2 regularized logistic regression @xcite ) and ` sda ` a l1-regularized lda are two mainly linear algorithms .",
    "we would also add ` bst ` a boosting of linear models .    among the distance based classifiers , `",
    "knn ` is the usual k - nearest neighbor , and ` rknn ` is a bagging of knn , where each base learner is a ` knn ` on a random sample of the original features . `",
    "lvq ` , or learning vector quantization @xcite is a cluster plus distance , or dictionary based classification : a set of `` prototypes , '' or clusters , or `` codebook '' is discovered in the data , many for each class , and new data is classified based on the distance to these prototypes .",
    "neural network based classifiers include ` nnet ` a common 1-hidden layer logistic network , and ` elm ` or extreme learning machines @xcite .",
    "extreme learning machines are a 1-hidden layer neural network , where the weights from the input to the hidden layer are set at random , and only the second set of weights are learned ( usually via least square optimization ) .",
    "we only tested the naive bayes ` nb ` as a bayesian network based algorithm .",
    "we divided the svm family into the linear svm ( ` svmlinear ` ) , the polynomial svm ( ` svmpoly ` ) and the rbf svm ( ` svmradial ` ) .",
    "finally we included one implementation of random forests ( * rf * ) and one implementation of gradient boosting machine classifiers ( ` gbm ` ) @xcite .",
    "the implementation information of the algorithms are listed below .",
    "bst : :    boosting of linear classifiers .",
    "we used the r implementation in the    package _ bst _ @xcite elm : :    extreme learning machines implementation : package _ elmnn _",
    "@xcite ) gbm : :    gradient boosting machines .",
    "implementation : package _ gbm _",
    "@xcite glmnet : :    elastic net logistic regression classifier .",
    "implementation : package    _ glmnet _",
    "@xcite ) knn : :    k - nearest neighbors classifier .",
    "implementation : package _ class _",
    "lvq : :    learning vector quantization .",
    "implementation : package _ class _ @xcite ) nb : :    naive bayes classifier : package _ klar _",
    "@xcite nnet : :    a 1-hidden layer neural network with sigmoid transfer function .",
    "implementation : package _ nnet _",
    "@xcite rf : :    random forest .",
    "implementation : package _ randomforest _",
    "@xcite rknn : :    a bagging of knn classifiers on a random subset of the original    features .",
    "implementation : package _ rknn _",
    "@xcite sda : :    a l1 regularized linear discriminant classifier .",
    "implementation :    package _ sparselda _ @xcite svmlinear : :    a svm with linear kernel .",
    "@xcite ) svmpoly : :    a svm with polynomial kernel .",
    "@xcite ) svmradial : :    a svm with rbf kernel .",
    "@xcite )      the randomforest classifier is a particularly convenient algorithm to discuss the grid search on hyperparameters .",
    "most implementation of ` rf ` use two or three hyperparameters : the ` mtry ` , the number of trees , and possible some limit of complexity of the trees , either the minimum size of a terminal node , or the maximum profundity , or a maximum number of terminal nodes .",
    "we did not set a range of possible values for the hyperparameters that limit the complexity of the trees .",
    "mtry is the number of random features that will be used to construct a particular tree .",
    "there is a general suggestion ( we do not know the source or other papers that tested this suggestion ) that this number should be the square root of the number of features of the dataset .",
    "if nfeat is the number of features of the dataset , we set the possible values to @xmath16 . also",
    ", the range is limited to at most nfeat@xmath17 .",
    "the number of trees is what we will call a * free hyperparameter * , that is , an hyper - parameter can be tested for many values but it only need one training step .",
    "one can train a random forest with @xmath18 trees , but at testing time , some implementations can return the classification of each tree on the test data .",
    "thus one can test the accuracy of a random forest with @xmath19 trees , just by computing the more frequent classification of the first @xmath20 trees ( or any random sample of @xmath20 trees ) .",
    "thus , to select this hyperparameter , it is not necessary to create a grid with multiple training or testing .",
    "so , for the random forest , ` ntree ` is a free hyperparameter , that will be tested from 500 to 3000 by 500 .",
    "but we also put another limit on the number of trees , half of the number data points in the subset .",
    "the number of repetitions or boosts in a boosting procedure is also a free parameter .",
    "the last class of free hyperparameter refer to an implementation of classification algorithms that calculate all the possible values of a parameter that causes changes in the learned function .",
    "the only relevant algorithm for this research is the elastic - net regularized logistic regression implemented by the package glmnet @xcite ) , which computes all the values ( or the _ path _ as it is called ) of the regularization parameter @xmath21 .",
    "@xcite discuss a complete path algorithm for svm ( for the c hyperparameter ) but we did not use this implementation in this paper .",
    "we list the range of hyperparameters for each of the algorithms tested , where nfeat is the number of features in the data and ndat is the number of data points .",
    "bst : :    hyperparameters : shrinkage = @xmath22 .",
    "free    hyperparameter , number of boosts , from 100 to 3000 by 200 , at most    ndat .",
    "elm : :    hyperparameter : number of hidden units = at most 24 values equally    spaced between @xmath23 and ndat@xmath24 gbm : :    hyperparameters : interaction - depth = 1 .. 5 ,    shrinkage=@xmath25 .",
    "number of boosts is a free    hyperparameter , tested from 50 to 500 with increments of 20 to at most    ndat .",
    "glmnet : :    hyperparameters @xmath26 , 8 values equally spaced between    0 and 1 .",
    "free hyperparameter @xmath21 , 20 values    geometrically spaced between @xmath27 to    @xmath28 .",
    "knn : :    hyperparameter k:= 1 , and at most 23 random values from 3 to    ndat@xmath29 .",
    "lvq : :    hyperparameter : size of the codebook = at most 8 values equally spaced    between @xmath30 and @xmath31 nfeat nb : :    hyperparameters : usekernel = @xmath32 true , false    @xmath33 , fl = @xmath34 nnet : :    hyperparameter : number of hidden units = at most 8 values equally    spaced between @xmath35 and nfeat@xmath24 , decay =    @xmath36 .",
    "rf : :    hyperparameter mtry = @xmath37    up to a value of nfeat@xmath24 .",
    "number of trees is a free    hyperparameters , tested from 500 to 3000 by 500 up to    ndat@xmath24 .",
    "rknn : :    hyperparameters : mtry = 4 values equally spaced between 2 and    nfeat@xmath38 , k= 1 , and at most 23 random values from 3 to    ndat@xmath29 .",
    "the number of classifiers is a free    hyperparameter from 5 to 100 , in steps of 20 .",
    "sda : :    hyperparameter : @xmath21 = 8 values geometrically spaced    from @xmath39 to @xmath40 svmlinear : :    .",
    "hyperparameter : c = @xmath41 .",
    "svmpoly : :    hyperparameter c as in the linear kernel and degree from 2 to 5 .",
    "svmradial : :    hyperparameters c as in the linear svm and    @xmath42 .      the data described above , the r programs that tested the 14 algorithms , the results of running there algorithms , the r programs used to analyse these results and generate the tables and figures in this paper , and the saved interactions of the mcmc algorithm are available at https://figshare.com/s/d0b30e4ee58b3c9323fb .",
    "we will follow the procedure proposed by @xcite .",
    "the procedure suggests one should first apply a friedman test ( which can be seen as a non - parametric version of the repeated measure anova test ) to determine if there is sufficient evidence that the error rate measures for each procedure are not samples from the same distribution . if the p - value is below 0.05 ( for a 95% confidence ) than one can claim that it is unlikely that the error rates are all `` the same '' and one can proceed to compare each algorithm to the others . when comparing all algorithms among themselves , which is the case here , demsar proposed the nemenyi test , which will compute the p - value of all pairwise comparisons .",
    "again , a p - value below 0.05 indicates that that comparison is statistically significant , that is , it is unlikely that two sets fo error rates are samples from the same distribution .",
    "a standard null hypothesis significant test assumes the null hypothesis , usually that the two samples came form the same population and computes the probability ( p - value ) of two samples from the same population having as large a difference in mean ( or median ) as the one encountered in the data .",
    "if the p - value is not low , one * can not claim * that the null hypothesis is true and that the two data sets came from the same population ( and thus all observed differences are due to `` luck '' ) . failing to disprove the null hypothesis because the p - value is too high is not the same as proving the null hypothesis .",
    "equivalence tests are a form of `` proving '' a weaker version of the usual null hypothesis .",
    "equivalence tests assume as the null hypothesis that the difference between the mean ( or median ) of the two sets is above a certain threshold , and if the p - value is low enough , one can claim that this null hypothesis is false , and thus that the difference between the means is smaller than the threshold .",
    "are useful when that threshold is a limit of * practical irrelevance * , that is , a limit below which changes in the mean of two groups are of no practical consequence . of course ,",
    "this limit of irrelevance is very problem dependent .",
    "section  [ sec : irrelevance1 ] will discuss our proposal for such a limit .",
    "a different approach to prove that the differences are not important is to use bayesian methods .",
    "bayesian methods will compute the ( posterior ) distribution of probability for some set of measures , given the prior hypothesis on those measures .",
    "thus a bayesian method can compute the posterior distribution of the difference of two means .",
    "the area of the distribution of the difference that falls within the limits of irrelevance is the probability that the difference is of no practical importance . in bayesian statistics , the limit of irrelevance",
    "is called region of practical equivalence ( rope ) .",
    "the standard bayesian anova , as described in @xcite is based on normal distributions .",
    "we are interested in a 2-factor anova , where one of the factors is the classification algorithm , and the other factor is the dataset .",
    "we assume that there is no interaction component , that is , we are not interested in determining particularities of each algorithm on each dataset - we are making a claim that the datasets used in this research are a sample of real world problems , and we would like to make general statements about the algorithms .",
    "let us denote @xmath43 as the error rate for the algorithm @xmath5 on dataset @xmath44 , then the usual hierarchical model is @xcite :    @xmath45    where @xmath46 is the uniform distribution with @xmath47 and @xmath48 as the low and high limits ; @xmath49 is the normal distribution with mean @xmath50 and standard deviation @xmath51 ; and gamma(@xmath52 ) is the gamma distribution with mode @xmath20 and standard deviation @xmath51 - note that this is not the usual parametrization of the gamma distribution .",
    "@xmath53 is the standard deviation of the @xmath43 data , and @xmath54 , the mean of that data .",
    "we are interested in the joint posterior probability @xmath55 . from that one",
    "can compute the relevant pairwise differences @xmath56 and in particular , how much of the probability mass fall within the region of irrelevant differences . more specifically , if @xmath57 , then the simulation approach we will follow generates a set @xmath58 for @xmath59 where @xmath60 is the number of chains in the mcmc simulation .",
    "we compute characteristics of distribution of @xmath56 from the @xmath61 .",
    "finally , one commonly used _",
    "robust _ variation of the model above is to substitute the normal distribution in equation  [ eq1 ] for a student - t distribution , with low degree of freedom , that is : @xmath62 where exp@xmath63 is the exponential distribution with rate @xmath21 .",
    "we also run the robust version of the model , as discussed in [ app : robust ] .",
    "we propose two different forms of defining the threshold of irrelevance for differences in error rates .",
    "we will compute the two thresholds and use the lowest of the two measures as the threshold of irrelevance .",
    "the first proposal is to compare the two measures of error for each dataset and for each classification algorithm . in equation  [ eq:3 ]",
    "they were the two terms @xmath64 and @xmath65 , that is , the error of classifier @xmath5 when learning on training subset @xmath3 and tested on the subset @xmath2 , and the dual of that .",
    "the difference @xmath66 can be seen as the change on the error rate that one would expect from applying the classifier @xmath5 on two different samples ( @xmath2 and @xmath3 ) from the same population ( @xmath67 ) .",
    "we will then compute the median of @xmath68 for all datasets @xmath12 and for all classification algorithms @xmath5 that are among the three best ( as calculated by @xmath69 ) for each dataset .",
    "that is , we are considering as a threshold of irrelevance , the median of the change one should expect from using a good classifier ( among the top three for that dataset ) on two different samples of the same dataset .",
    "the second proposal compares the estimate of the error @xmath70 computed from the 5-fold cv procedure on the subset @xmath2 which selects the best hyperparameters with the measure of the error itself .",
    "we have not made explicit the steps in the 5-fold cv , but intuitively , for each combination of hyperparameters values @xmath71 , it computes the average of the error of training in 4 folds and testing on the remaining one .",
    "let us call it @xmath72 - that is the cv error computed within the @xmath2 subset itself .",
    "we select the combination of hyperparameters that have the lowest @xmath72 . but this cv error is an estimate of the future error of the classifier ( trained with that combination of hyperparameters ) . on the other hand",
    "@xmath70 is a `` future '' error of the classifier - the training set is slightly different from the 5-fold cv since it includes the whole subset @xmath2 while for each fold it included only 4/5 of @xmath2 , and the testing set @xmath3 is totally new .",
    "the difference @xmath73 can be seen as the change in error one would expect from applying the classifier @xmath5 on a slightly larger training set and testing it on completely new data .",
    "we will compute the median of @xmath74 for all datasets @xmath12 and for all classification algorithms @xmath5 that are among the three best ( as calculated by @xmath69 ) for each dataset .    in both these proposals , we are defining the threshold of irrelevance based on a `` futility '' point of view , and not based on theoretical considerations .",
    "one knows that given a different sample from the same distribution or given new data to test , all algorithms will have different error rates .",
    "we compute to measures of computational costs to run the algorithms .",
    "the first one is the * 1-train - test * , which measures the total time to train the algorithm @xmath5 on a subset @xmath4 and to test it in the subset @xmath8 .",
    "thus , that is the time to train the algorithm and to run it on two equally sized data .",
    "but all algorithms must search for the appropriate hyperparameters .",
    "thus also compute the total time to search for the correct hyperparameter ( using , as discusses above a 5-fold cv ) .",
    "but different algorithms may have a different gird size of tentative hyperparameters ( since as discussed for some algorithms some of range of hyperparameter may depend on characteristics of the dataset ) .",
    "thus we divide the total time to search for the best hyperparameter by the number of hyperparameter combinations tested .",
    "we call it the * per hyperparameter * time .",
    "since the execution time varies greatly on different datasets , we will use the mean rank of each execution time to rank the algorithms , and use the demsar procedure to determine which execution times are significantly different than the others ( from a statistical sense ) .",
    "we have no intuition on what could be considered an irrelevant change in either execution times , so we will not perform the bayesian anova analysis .",
    "the program ran on different cores of a cluster of different machines , so there is no guarantee that the machines have the same hardware specification .",
    "but we ran all the 14 algorithms for a particular subset on the same program .",
    "thus the unit of distribution is all the 14 algorithms searching for the best combination of hyperparameters on a 5-fold cv of half of a dataset , then training on the full half dataset with the best selection of the hyperparameters and finally testing the classifier on the other half of the dataset .",
    "therefore , for the timing analysis we do not average the two measures from the subsets to obtain the measure per dataset .",
    "instead we perform the demsar statistical analysis using the subsets as subject indicator .",
    "table  [ tab1 ] list the mean ranking of all algorithms , the number of times each algorithm was among the top for a dataset .",
    "the best performing algorithm , in terms of mean rank across all subsets was the random forest , followed by svm with gaussian kernels , followed by gradient boosting machines .",
    "the three worst performing algorithms in terms of mean rank were boosting of linear classifiers naive bayes and l1-regularized lda .",
    "we make no claim that these three algorithms are intrinsically `` bad '' .",
    "it is possible that our choice of hyperparameters was outside the range of more useful values , or the implementation used was particularly not robust .",
    "in particular both ` nb ` and ` sda ` did not run at all for 20 subsets , which may explain partially their poor performance .",
    "the ranking of the algorithm is dense , that is , all best performing algorithm receive rank 1 , all second best algorithms receive rank 2 , and so on .",
    "also for the ranking , we rounded the error rates to 3 significant digits , so two algorithms have the same rank if their error rates have a difference of less than 0.0005 .",
    "we do not use the rounding for the calculations of the irrelevance threshold .       we can not perform the verification of the model using posterior predictive check because the @xmath75 discrepancy needs the variance of the data . under the robust model ,",
    "the variance of the data depends also on the degrees of freedom of the student - t distribution , and in the case of the robust simulations is 1.12 , and the variance of the student - t distribution is not defined for degrees of freedom below 2 ."
  ],
  "abstract_text": [
    "<S> we tested 14 very different classification algorithms ( random forest , gradient boosting machines , svm - linear , polynomial , and rbf - 1-hidden - layer neural nets , extreme learning machines , k - nearest neighbors and a bagging of knn , naive bayes , learning vector quantization , elastic net logistic regression , sparse linear discriminant analysis , and a boosting of linear classifiers ) on 115 real life binary datasets . </S>",
    "<S> we followed the demsar analysis and found that the three best classifiers ( random forest , gbm and rbf svm ) are not significantly different from each other . </S>",
    "<S> we also discuss that a change of less then 0.0112 in the error rate should be considered as an irrelevant change , and used a bayesian anova analysis to conclude that with high probability the differences between these three classifiers is not of practical consequence . </S>",
    "<S> we also verified the execution time of `` standard implementations '' of these algorithms and concluded that rbf svm is the fastest ( significantly so ) both in training time and in training plus testing time .    </S>",
    "<S> * keywords : * classification algorithms ; comparison ; binary problems ; demsar procedure ; bayesian analysis </S>"
  ]
}