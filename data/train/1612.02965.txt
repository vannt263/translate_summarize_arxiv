{
  "article_text": [
    "batfled is a generative probabilistic model with the structure shown in figure 1 .",
    "matrices of input feature values for the training set examples are multiplied by learned projection matrices to form latent matrices .",
    "the number of columns in the latent matrices are set parameters determining the size of the latent space for that mode .",
    "the response tensor is factored using a tucker decomposition ( tucker , 1964 ) into the latent matrices and a core tensor .",
    "this three - way matrix multiplication can be framed as a sum of outer products of the columns of the latent matrices weighted by values in the core tensor . in order to allow for lower - order interactions between latent factors and to obviate the need for normalization of responses , a column of ones",
    "is added to each latent matrix .",
    "thus the core element corresponding to the outer product of these columns acts as a global constant added to all responses and products of other columns with the @xmath0s columns can encode interactions between only one or two of the modes . for comparison , we also implement a candecomp / parafac ( cp ) decomposition ( hitchcock , 1927 ) , a special case of the tucker decomposition where the latent space for each mode must be the same size and the core matrix only has values along the super - diagonal .    in order to learn the values in the projection matrices , latent matrices and core tensor , we impose a probabilistic framework placing distributions on each of the unknown values .",
    "the distributions are chosen to maintain conjugacy and to encourage sparsity in the projection matrices and the core tensor .",
    "all of the major distributions are gaussian and where sparsity is desired , means are set to zero and gamma priors are placed on the precision ( inverse variance ) . setting the shape and scale for these gamma distributions to extreme values ( shape @xmath1 , scale @xmath2 ) encourages most of the variances to be near zero and the corresponding gaussian to have a tight peak centered at zero .",
    "variances are shared across rows of the projection matrices so that each feature is either selected or not for all the latent factors .",
    "due to size of the batfled model , training would be computationally intractable with sampling methods , so instead we use a variational bayesian approach .",
    "this seeks to maximize a lower bound of the posterior probability of the unknown values given the observed data and set parameters by approximating the full joint ` p ' distribution with a factorable ` q ' distribution .",
    "the parameters of each of these ` q ' distributions rely only on the expected values of other parameters in the model and equations for the optimal parameter values can be found analytically .",
    "thus , once these update equations are derived , training proceeds by initializing all parameters randomly and updating each in sequence given the expectation of the other parameters .",
    "full model details are in the supplemental material .",
    "once trained , there are four types of prediction that this model can perform .",
    "first , if there are missing values in the response tensor , these can be filled in by multiplying the latent matrices through the core tensor . in this warm",
    "start prediction we estimate a response for a combination when we have seen responses for other combinations involving the same input feature vectors .",
    "second , given a vector of input features for a new example for one mode , cold start predictions can be made for that example when combined with any example for the other two modes . multiplying through the projection matrix and core tensor , a new matrix slice of the response tensor",
    "is formed representing responses for this new test example across all training data .",
    "similarly , cold start prediction is possible for new exmples from two modes yielding a vector of responses across the third mode and lastly , cold start prediction for all three modes produces a single predicted response value .",
    "we first test the model on data simulated from the three - mode structure shown in figure 1 and compare it to baseline approaches . for these tests ,",
    "response data is generated with 30 training examples and 100 features for each mode , 10 of which are used to produce the responses .",
    "each mode has four latent factors , so with the added columns of ones , the core tensor is 5x5x5 .",
    "zero - centered gaussian noise with a standard deviation of 1/10 of the response standard deviation is added to responses and each method is trained on 28 examples for each mode with 1% of interactions removed for warm start testing .    in order to ascertain what types of interactions each of the tested models are able to discover ,",
    "we generate data with three different sparse core tensors .",
    "the first core tensor only has non - zero values along the three primary edges , so the response tensor is formed using only 1d interactions between one latent factor from one mode and two columns of ones from the other modes .",
    "the second core tensor has non - zero values on three primary faces and so it allows 1d and 2d interactions between modes . the last core tensor can have non - zero values in any position , but is sparse so that only 1/2 of the possible interactions have non - zero weight",
    "cancer cell line drug screen panels consist of a number of patient - derived cancer cell lines which are treated with a broad range of drugs at varying doses , grown for several days and imaged to assess cellular death .",
    "the resulting response data is inherently three - dimensional with one measurement for each combination of cell line , drug and dose .",
    "current studies summarize these responses across doses by extracting parameters of fitted curves , and attempt to predict these values using genomic features .",
    "instead , we predict response at each dose in the hope is that by incorporating more information , more useful relationships between the cell line genomics and drug structures can be revealed .",
    "one such study was presented as a prediction challenge by the dialogue on reverse engineering assessment and methods ( dream ) project ( costello et al . , 2014 ) . this challenge released data publicly so that teams from around the world could compete in the task of predicting responses in a panel of 52 breast cancer cell lines treated with 26 drugs .",
    "although the original challenge was to predict a summary curve measure , data on response at each dose was also released and is utilized here .",
    "a set of 17 cell lines were held out as the final test set , but since the task of predicting responses for drugs was not part of the challenge , no drugs were separated into the test data set . in order to show performance on predicting multiple mode combinations , we present results both from 10 fold cross - validation runs on the 35 cell lines and 26 drugs in the training set ( leaving out 4 cell lines and 3 drugs in each fold ) , as well as results for models trained with all of the original dream training data on the final 17 cell lines .",
    "the 733 input features for cell lines consist of binary indicators of cancer subtype , binary indicators of gene mutations and continuous measures of gene expression for a set of genes known to be associated with cancer ( bindal et al . ,",
    "the 433 input features for drugs consist of binary indicators of known gene targets , binary indicators of chemical substructures and continuous measures of 1d and 2d structural features extracted using the padel software ( yap , 2011 ) .",
    "we examine the performance of the batfled model both for predicting the responses and in selecting the true predictors and compare to four types of baseline models . in the first baseline",
    ", we predict the mean response across the training data averaging as many responses as possible for the the prediction task .",
    "all other models are run in ten replicates with input features consisting of vectors of the features for each mode concatenated together and a single continuous response for each combination of input examples .",
    "we train elastic net models using the r package ` glmnet ' ( friedman , et al . 2010 ) with different sparsity settings ranging from ridge regression ( @xmath3 ) to lasso ( @xmath4 ) .",
    "random forest and neural net models are run in r using the ` h2o ' package ( aiello et al .",
    "random forest models with 1,000 and 5,000 trees of depth 5 , and 1,000 trees of depth 10 are tested as well as neural net models with 1 , 2 and 3 hidden layers with 1,500 , 750 and 500 nodes in each layer respectively .",
    "neural net models use rectified linear activation functions with a dropout fraction of @xmath5 on the input layer and @xmath6 on hidden layers .",
    "full results are given in the supplemental material .",
    "on simulated data , batfled models are run for @xmath7 iterations with prior parameters @xmath8 and @xmath9 .",
    "the tucker models are given @xmath10x@xmath10x@xmath10 cores and the cp models a latent dimension of @xmath11 .",
    "all experiments were run in 10 replicates and we report the mean rmse across replicates for representative models in table 1 . also shown are auroc ( area under the receiver operator characteristic ) measures for these models when selecting predictors in data generated with 1,2 and 3d interactions .",
    "while batfled performs comparably to baseline methods when predicting linear interactions it is able to learn higher - order relationships in data while other methods can not",
    ". additionally , batfled selects the correct features in models with higher - order relationships at a higher frequency than other models .    for the dream data , we compare to the same baseline methods and report pearson correlation measures in table 2 .",
    "since the dream data has more input features and less than half the total responses than the simulated data , we believe that it is somewhat underpowered for this prediction task .",
    "simulation with artificially underpowered datasets indicate that this can be partially overcome by performing multiple rounds of testing to reduce the number of features . for the the cross - validation runs we found the best performance by first training batfled models for 200 iterations with a @xmath12x@xmath12x@xmath12 core and strong sparsity priors ( @xmath1 and @xmath13 ) , keeping the union top 15% of predictors across folds , and retraining for 40 iterations without encouraging sparsity . for the final dream testing runs , we tried a similar scheme , but found that a single round of training for 120 iterations with a @xmath12x@xmath12x@xmath12 core and strong sparsity priors performed as well as the second round .",
    "our current results suggest that batfled is able to predict responses as well or better than other baseline methods in both the cross - validation setting and on held - out test data on this very difficult challenge .",
    "in addition , batfled may be able to discover interactions across modes that other methods can not .",
    "as technological advancements in biology allow for the characterization of systems across an increasing number of molecular , structural , and behavioral dimensions , methods that can combine such datasets in a principled and interpretable manor are greatly needed .",
    "tensor factorizations provide a natural framework for organizing such data and this work demonstrates the usefulness of these methods in a predictive setting .",
    "batfled is designed for general use , is freely available and can be easily applied to many different applications .",
    "a fully functional r package is available at https://www.github.com/nathanlazar/batfled3d .",
    "thanks to dr .",
    "shannon mcweeney , dr .",
    "adam margolin and dr .",
    "lucia carbone .",
    "aiello , s. , kraljevic , t. , maj , p. and with contributions from the h2o.ai team ( 2016 ) .",
    "h2o : r interface for h2o .",
    "r package version 3.10.0.8 .",
    "https://cran.r-project.org/package=h2o    bindal , n. , forbes , s.a . ,",
    "beare , d. , gunasekaran , p. , leung , k. , kok , c.y . , jia , m. , bamford , s. , cole , c. , ward , s. , et al .",
    "cosmic : the catalogue of somatic mutations in cancer . _ genome biology _ * 12 * : p3 .",
    "costello , j.c .",
    ", heiser , l.m . ,",
    "georgii , e. , gnen , m. , menden , m.p . , wang , n.j . , bansal , m. , ammad - ud - din , m. , hintsanen , p. , khan , s.a . , et al .",
    "a community effort to assess and improve drug sensitivity prediction algorithms . _ nature biotechnology _ * 32 * : 12021212 .",
    "friedman , j. , hastie , t.  & tibshirani r.  ( 2010 ) .",
    "regularization paths for generalized linear models via coordinate descent .",
    "_ journal of statistical software _ * 33*(1 ) : 1 - 22 .",
    "url http://www.jstatsoft.org/v33/i01/      tucker , l.r .",
    "the extension of factor analysis to three - dimensional matrices . in h.",
    "gulliksen , and n.  frederiksen ( eds . ) , _ contributions to mathematical psychology .",
    ".  110127 new york : holt , rinehart and winston .",
    "friedman , j. , hastie , t.  & tibshirani r.  ( 2010 ) .",
    "regularization paths for generalized linear models via coordinate descent .",
    "_ journal of statistical software _ * 33*(1 ) : 1 - 22 .",
    "the notation throughout this paper mostly follows the conventions of the excellent review ( kolda and bader , 2007 ) .",
    "i will use boldface capital letters to refer to tensors , capital letters to refer to matrices ( and index upper limits ) , boldface lower case letters to refer to vectors and lower - case letters to refer to scalars .",
    "indices will typically range from @xmath0 to their capital version so , for example , the three modes of the third - order response tensor @xmath14 have indices @xmath15,@xmath16 and @xmath17 where @xmath18 , @xmath19 and @xmath20 and a typical element will be written @xmath21 .",
    "i use @xmath22 for the input feature data , @xmath23 for the projection matrices , @xmath24 for the matrix of priors on the precision ( inverse variance ) of the projection matrices , @xmath25 for the latent matrices , and @xmath26 for the core tensor .",
    "the superscripts indicate the three modes ( @xmath27 , @xmath28 and @xmath29 to suggest cell lines , drugs and doses ) and core elements ( @xmath30 ) . for simplicity",
    "we denote all the observed values and set parameters ( except the responses ) by @xmath31 . @xmath32 and all random variables by @xmath33 @xmath34      the full joint distribution of the the prior parameters @xmath35 , projection matrices @xmath36 , latent factor matrices @xmath37 and the core",
    "tensor @xmath26 given the observed data @xmath38 and the set parameters @xmath39 @xmath40 is summarized as @xmath41 .",
    "this distribution factors as follows ( dependencies on set parameters are omitted for clarity ) .",
    "@xmath42 the distributional assumptions for each of the factors are      where @xmath44 is a normal distribution with mean @xmath45 and variance @xmath46 and @xmath47 is a gamma distribution with shape @xmath48 and scale @xmath49 .",
    "we use a shared variance @xmath46 for the response tensor and shared variances @xmath50 , @xmath51 and @xmath52 for the elements of the latent matrices @xmath53 and @xmath54 respectively . the gamma distributions on the @xmath55 parameters for the projection matrices @xmath56 and @xmath57 and the core @xmath26 allows the user to encourage sparsity in these parts of the model . by setting the prior shape and scale parameters to extreme values ( ex . @xmath1 and",
    "@xmath13 ) the majority of @xmath55 values move toward zero as the model is trained .",
    "the precision values in the projection ( @xmath23 ) matrices can optionally be shared across rows which encourages the use of the same predictor variables for all latent factors .",
    "also , columns of ones can be introduced into the input ( @xmath22 ) matrices and latent ( @xmath25 ) matrices . in the input matrices",
    "this allows for bias terms , and in the a latent matrices , these columns allow the model to learn marginal coefficients that apply to lower - dimensional subsets of the response tensor .",
    "for example if all three @xmath25 matrices have a constant , we extend the ranges of @xmath58 , @xmath59 and @xmath60 to include a zero index and the corresponding element of the core matrix @xmath61 can learn the intercept for the response tensor ; a value that is added to all responses .",
    "similarly the @xmath62 element is a coefficient for the first latent factor for the first mode and encodes the one - dimensional effects that this latent factor will have on response regardless of the other two modes .",
    "the coefficient @xmath63 encodes interactions between the first latent factors of modes one and two that occur regardless of mode three .",
    "with the model established above , our goal is to find the posterior distribution of variables in the model given the observations @xmath64 as well as the marginal likelihood or model evidence @xmath65 .",
    "exact inference of the posterior is not analytically solvable and sampling based approaches like markov chain monte carlo ( mcmc ) would be computationally prohibitive for a model of this size .",
    "instead we use a variational approximation ( or variational bayes ) approach that maximizes a lower bound on the log of the posterior .",
    "this lower bound can be written as :      the new @xmath67 distribution is a joint distribution of the same variables as the @xmath68 distribution that approximates the @xmath68 distribution under the assumption that @xmath67 is factorable in some convenient way . although the @xmath67 distribution has the same unknown variables as @xmath68 , the parameters of these distributions ( mean , variance , etc . )",
    "are different than those given above . closed forms for these parameters can be derived analytically and they depend on expectations over the @xmath67 distributions of other variables in the model .",
    "thus , in order to find the @xmath67 distribution that best approximates the @xmath68 distribution an iterative expectation maximization process is employed .",
    "thus the lower bound can be written : @xmath71 \\nonumber \\\\   & + \\mathbb{e}_{q(h^c)q(a^c)}[p(h^c|a^c , x^c ) ] \\nonumber \\\\   & +   \\mathbb{e}_{q(h^d)q(a^d)}[\\log p(h^d|a^d , x^d ) ] \\nonumber \\\\   & +   \\mathbb{e}_{q(h^s)q(a^s ) } [ \\log p(h^s|a^s , x^s ) ] \\nonumber \\\\   & + \\mathbb{e}_{q(a^c)q(\\boldsymbol{\\lambda}^c ) } [ \\log p(a^c|\\boldsymbol{\\lambda}^c ) ]   +   \\mathbb{e}_{q(a^d)q(\\boldsymbol{\\lambda}^d ) } [ \\log p(a^d|\\boldsymbol{\\lambda}^d ) ]   +   \\mathbb{e}_{q(a^s)q(\\boldsymbol{\\lambda}^s ) } [ \\log p(a^s|\\boldsymbol{\\lambda}^s ) ] \\nonumber \\\\   & + \\mathbb{e}_{q(\\boldsymbol{\\lambda}^c)q(\\boldsymbol{\\tau}^c ) } [ \\log p(\\boldsymbol{\\lambda}^c|\\boldsymbol{\\tau}^c ) ]   +   \\mathbb{e}_{q(\\boldsymbol{\\lambda}^d)q(\\boldsymbol{\\tau}^d ) } [ \\log p(\\boldsymbol{\\lambda}^d|\\boldsymbol{\\tau}^d ) ]   +   \\mathbb{e}_{q(\\boldsymbol{\\lambda}^s)q(\\boldsymbol{\\tau}^s ) } [ \\log p(\\boldsymbol{\\lambda}^s|\\boldsymbol{\\tau}^s ) ] \\nonumber \\\\   & + \\mathbb{e}_{q(\\boldsymbol{\\tau}^c ) } [ \\log p(\\boldsymbol{\\tau}^c ) ]   +   \\mathbb{e}_{q(\\boldsymbol{\\tau}^d ) } [ \\log p(\\boldsymbol{\\tau}^d ) ]   +   \\mathbb{e}_{q(\\boldsymbol{\\tau}^s ) } [ \\log p(\\boldsymbol{\\tau}^s ) ] \\nonumber \\\\   & - \\mathbb{e}_{q(h^c)}[\\log q(h^c ) ]    -   \\mathbb{e}_{q(h^d)}[\\log q(h^d ) ]    -   \\mathbb{e}_{q(h^s)}[\\log q(h^s ) ] \\nonumber \\\\    & - \\mathbb{e}_{q(a^c)}[\\log q(a^c ) ]   -   \\mathbb{e}_{q(a^d)}[\\log q(a^d ) ]   -   \\mathbb{e}_{q(a^s)}[\\log q(a^s ) ] \\nonumber \\\\   & - \\mathbb{e}_{q(\\boldsymbol{\\lambda}^c ) } [ \\log q(\\boldsymbol{\\lambda}^c ) ]    -   \\mathbb{e}_{q(\\boldsymbol{\\lambda}^d ) } [ \\log q(\\boldsymbol{\\lambda}^d ) ]    -   \\mathbb{e}_{q(\\boldsymbol{\\lambda}^s ) } [ \\log q(\\boldsymbol{\\lambda}^s ) ] \\nonumber \\\\   & - \\mathbb{e}_{q(\\boldsymbol{\\tau}^c ) } [ \\log q(\\boldsymbol{\\tau}^c ) ]    -   \\mathbb{e}_{q(\\boldsymbol{\\tau}^d ) } [ \\log q(\\boldsymbol{\\tau}^d ) ]    -   \\mathbb{e}_{q(\\boldsymbol{\\tau}^s ) } [ \\log q(\\boldsymbol{\\tau}^s)]\\end{aligned}\\ ] ]    the parameters for the @xmath67 distributions of a given variable @xmath72 that maximize this lower bound can be obtained by finding the expectation of @xmath73 with respect to the q distributions over all other variables ( denoted @xmath74 ) .",
    "this gives expressions ( update equations ) for each of the @xmath67 distributions that depend only on moments of the other variables ( and fixed parameters ) .",
    "@xmath79^{t } x^c \\right ) \\sigma ( \\textbf{a}_{r_1}^c ) \\\\",
    "\\sigma ( \\textbf{a}_{r_1}^c ) & = \\left ( \\frac{1}{\\sigma_c^2 } ( x^c)^t x^c + \\mathbb{e } [ \\boldsymbol{\\lambda}_{r_1}^c ] i \\right)^{-1}\\end{aligned}\\ ] ]          @xmath81 \\mathbb{e } [ h^d_{jr_2 } ] \\mathbb{e } [ h^s_{kr_3 } ] \\right .",
    "\\\\    & - \\sum_{r^*_1 \\neq r_1 } \\sum_{r_2}^{r_2 } \\sum_{r_3}^{r_3 } \\mathbb{e } [ c_{r_1r_2r_3 } ] \\mathbb{e}[h^c_{ir^*_1 } ]",
    "\\left [       \\sum_{r^*_2 \\neq r_2 } \\sum_{r^*_3 \\neq r_3 } \\mathbb{e}[c_{r^*_1r^*_2r^*_3 } ] \\mathbb{e}[h^d_{jr_2 } ] \\mathbb{e}[h^d_{jr^*_2 } ]      \\mathbb{e}[h^s_{kr_3 } ] \\mathbb{e}[h^s_{kr^*_3 } ]   \\right . \\\\    & \\hspace{1 in } + \\sum_{r^*_2 \\neq r_2 } \\mathbb{e}[c_{r^*_1r^*_2r_3 } ] \\mathbb{e}[h^d_{jr_2 } ] \\mathbb{e}[h^d_{jr^*_2 } ]      \\left ( \\mathbb{e}[h^s_{kr_3}]^2 + var(h^s_{kr_3 } ) \\right ) \\\\    & \\hspace{1 in } + \\sum_{r^*_3 \\neq r_3 } \\mathbb{e}[c_{r^*_1r_2r^*_3 } ] \\left ( \\mathbb{e}[h^d_{jr_2}]^2 + var(h^d_{jr_2 } ) \\right )    \\mathbb{e}[h^s_{kr_3 } ] \\mathbb{e}[h^s_{kr^*_3 } ] \\\\    & \\hspace{1 in } + \\mathbb{e}[c_{r^*_1r_2r_3 } ] \\left ( \\mathbb{e}[h^d_{jr_2}]^2 + var(h^d_{jr_2 } ) \\right )       \\left ( \\mathbb{e}[h^s_{kr_3}]^2 + var(h^s_{kr_3 } ) \\right ) \\bigg ] \\bigg ] \\\\    & + \\frac{1}{\\sigma_c^2 } \\textbf{x}^c_i \\textbf{a}^c_{r_1 } \\bigg)\\end{aligned}\\ ] ]      @xmath82 \\mathbb{e } [ c_{r_1r^*_2r^*_3 } ]       \\mathbb{e } [ h^d_{jr_2 } ] \\mathbb{e } [ h^d_{jr^*_2 } ] \\mathbb{e } [ h^s_{kr_3 } ] \\mathbb{e } [ h^s_{kr^*_3 } ] \\\\    & \\hspace{.1 in } + \\sum_{r^*_2 \\neq r_2 } \\mathbb{e } [ c_{r_1r_2r_3 } ] \\mathbb{e } [ c_{r_1r^*_2r_3 } ]       \\mathbb{e } [ h^d_{jr_2 } ] \\mathbb{e } [ h^d_{jr^*_2 } ] \\left ( \\mathbb{e } [ h^s_{kr_3}]^2 + var(h^s_{kr_3 } ) \\right ) \\\\    & \\hspace{.1 in } + \\sum_{r^*_3 \\neq r_3 } \\mathbb{e } [ c_{r_1r_2r_3 } ] \\mathbb{e } [ c_{r_1r_2r_3^ * } ]       \\left ( \\mathbb{e } [ h^d_{jr_2}]^2 + var(h^d_{jr_2 } ) \\right ) \\mathbb{e } [ h^s_{kr_3 } ] \\mathbb{e } [ h^s_{kr^*_3 } ]   \\\\    & \\hspace{.1 in } + \\left ( \\mathbb{e } [ c_{r_1r_2r_3}]^2 + var(c_{r_1r_2r_3 } ) \\right )       \\left ( \\mathbb{e } [ h^d_{jr_2}]^2 + var(h^d_{jr_2 } ) \\right ) \\left ( \\mathbb{e } [ h^s_{kr_3}]^2 + var(h^s_{kr_3 } ) \\right ) \\bigg ]    + \\frac{1}{\\sigma_c^2 } \\bigg)^{-1 } \\end{aligned}\\ ] ]          @xmath84 \\mathbb{e}[h^d_{jr_2 } ] \\mathbb{e}[h^s_{kr_3 } ]   \\\\    & + \\mathbb{e } [ h^c_{ir_1 } ] \\mathbb{e } [ h^d_{jr_2 } ] \\mathbb{e } [ h^s_{kr_3 } ]     \\sum_{r^*_1 \\neq r_1 } \\sum_{r^*_2 \\neq r_2 } \\sum_{r^*_3 \\neq r_3 } \\mathbb{e}[c_{r^*_1r^*_2r^*_3 } ]     \\mathbb{e } [ h^c_{ir^*_1 } ] \\mathbb{e } [ h^d_{jr^*_2 } ] \\mathbb{e } [ h^s_{kr^*_3 } ] \\\\    & + \\left ( \\mathbb{e } [ h^c_{ir_1}]^2 + var(h^c_{ir_1 } ) \\right ) \\mathbb{e } [ h^d_{jr_2 } ] \\mathbb{e } [ h^s_{kr_3 } ]     \\sum_{r^*_2 \\neq r_2 } \\sum_{r^*_3 \\neq r_3 } \\mathbb{e}[c_{r_1r^*_2r^*_3 } ] \\mathbb{e } [ h^d_{jr^*_2 } ] \\mathbb{e } [ h^s_{kr^*_3 } ] \\\\    & + \\mathbb{e } [ h^c_{ir_1 } ] \\left(\\mathbb{e } [ h^d_{jr_2}]^2 + var(h^d_{jr_2 } ) \\right ) \\mathbb{e } [ h^s_{kr_3 } ]     \\sum_{r^*_1 \\neq r_1 } \\sum_{r^*_3 \\neq r_3 } \\mathbb{e}[c_{r^*_1r_2r^*_3 } ] \\mathbb{e } [ h^c_{ir^*_1 } ] \\mathbb{e } [ h^s_{kr^*_3 } ] \\\\    & + \\mathbb{e } [ h^c_{ir_1 } ] \\mathbb{e } [ h^d_{jr_2 } ] \\left ( \\mathbb{e } [ h^s_{kr_3}]^2 + var(h^s_{kr_3 } ) \\right )     \\sum_{r^*_1 \\neq r_1 } \\sum_{r^*_2 \\neq r_2 } \\mathbb{e}[c_{r^*_1r^*_2r_3 } ] \\mathbb{e } [ h^c_{ir^*_1 } ] \\mathbb{e } [ h^d_{jr^*_2 } ] \\\\    & + \\left ( \\mathbb{e } [ h^c_{ir_1}]^2 + var(h^c_{ir_1 } ) \\right ) \\left(\\mathbb{e } [ h^d_{jr_2}]^2 + var(h^d_{jr_2 } ) \\right ) \\mathbb{e } [ h^s_{kr_3 } ]     \\sum_{r^*_3 \\neq r_3 } \\mathbb{e}[c_{r_1r_2r^*_3 } ] \\mathbb{e } [ h^s_{kr^*_3 } ] \\\\    & + \\left ( \\mathbb{e } [ h^c_{ir_1}]^2 + var(h^c_{ir_1 } ) \\right ) \\mathbb{e } [ h^d_{jr_2 } ] \\left ( \\mathbb{e } [ h^s_{kr_3}]^2 + var(h^s_{kr_3 } ) \\right )    \\sum_{r^*_2 \\neq r_2 } \\mathbb{e}[c_{r_1r^*_2r_3 } ] \\mathbb{e } [ h^d_{jr^*_2 } ] \\\\    & + \\mathbb{e } [ h^c_{ir_1 } ] \\left(\\mathbb{e } [ h^d_{jr_2}]^2 + var(h^d_{jr_2 } ) \\right ) \\left ( \\mathbb{e } [ h^s_{kr_3}]^2 + var(h^s_{kr_3 } ) \\right )    \\sum_{r^*_1 \\neq r_1 } \\mathbb{e}[c_{r^*_1r_2r_3 } ] \\mathbb{e } [ h^c_{ir^*_1 } ] \\biggr ] \\\\\\end{aligned}\\ ] ]    @xmath85 ^ 2 + var(h^c_{ir_1 } ) \\right )     \\left(\\mathbb{e } [ h^d_{jr_2}]^2 + var(h^d_{jr_2 } ) \\right ) \\\\    & \\hspace{1.4 in } \\left ( \\mathbb{e } [ h^s_{kr_3}]^2 + var(h^s_{kr_3 } ) \\right ) + \\mathbb{e } [ \\lambda^r_{r_1r_2r_3 } ] \\bigg)^{-1 } \\\\\\end{aligned}\\ ] ]",
    "responses are simulated from the structure assumed by the batfled model .",
    "each of the three modes has 30 samples , 4 latent factors and 100 features , 10 of which influence response .",
    "two samples for each mode are held out as validation data . for the ` 1d ' interaction responses ,",
    "the core only has values along the edges corresponding to the constant columns in the latent matrices .",
    "this means that no interactions between modes influence responses . for the `",
    "1d & 2d ' interaction responses , the core only has values in the matrix slices corresponding to the constant columns , so responses are built from interactions between at most two of the modes .",
    "for the ` 1d , 2d & 3d ' interaction responses , the core has values throughout , but is sparse so half of potential interactions have weight zero and do not contribute to the responses .",
    "batfled tucker models are trained for 100 iterations with a @xmath10x@xmath10x@xmath10 core and sparsity priors @xmath8 and @xmath86 on both the projection matrices and the core .",
    "batfled cp models are trained for 100 iterations with @xmath11 latent features and sparsity priors @xmath8 and @xmath86 on the projection matrices .",
    "the features for elastic net , neural net and random forest models are vectors consisting of the features for each mode concatenated .",
    "elastic net models are trained using the r package ` glmnet ' ( friedman et al . 2010 ) and we choose lambda values that give the most regularized model within one standard deviation of the model with the lowest cross - validated error .",
    "neural net models are trained using the ` h2o ' r package ( aiello et al .",
    "2016 ) with @xmath0 , @xmath87 and @xmath88 hidden layers with @xmath89 , @xmath90 and @xmath91 nodes in each layer respectively .",
    "activations are rectified linear functions and a dropout fraction of @xmath5 on the input layer and @xmath6 on the hidden layers are used . for each replication , convergence is determined by monitoring the mean squared error ( mse ) on ten cross - validation folds and stopping when the mse changes by less than @xmath92 across five epochs .",
    "random forest models are also trained in r using the h2o package .",
    "splitting is determined using the the automatic option ` mtries=-1 ' and models with @xmath93 trees of depth @xmath10 , @xmath94 trees of depth @xmath10 , and @xmath95 trees of depth @xmath12 are tested .",
    "when running on dream data separate predictors are trained for each dose for elastic net , random forest models and neural net models .",
    "the inputs for each method are vectors containing the @xmath96 features for cell lines and @xmath97 features for drugs concatenated together .",
    "elastic net models were run using the r package ` glmnet ' and while the ` h2o ' package was used for random forest and neural net training .",
    "the same model parameters that were used on the simulated data were tested here .    for cross - validation runs ,",
    "batfled models were run for 200 iterations with strong sparsity priors ( @xmath1 and @xmath13 ) on the first round .",
    "the second round was run using the union of the top 15% of predictors across folds ( 278 cell line features and 163 drug features for cp models , 152 cell line features and 105 drug features for tucker models ) , for 40 iterations , without encouraging sparsity ( @xmath4 and @xmath98 ) .",
    "tucker models use a @xmath12x@xmath12x@xmath12 core and cp models have a latent dimension of @xmath99 .    for the final testing on the 17 held - out cell lines , batfled models were run for 120 iterations for both rounds with sparsity encouraged only on first round . again",
    "the union of the top 15% of predictors across replicates were used for the second round ( 191 cell line features and 102 drug features for cp models and 330 cell line features and 172 drug features for tucker models ) .",
    "elastic net , random forest and neural net models were run as above .",
    "all models were run on intel xeon processors in parallel when possible .",
    "the elastic net models run on a single core take a few minutes to complete while random forest and neural net models take 10 - 20 minutes on the simulated data while using 24 cores . on the dream data ,",
    "run time was similar for the random forest models , but significantly longer for neural net models with some taking up to 14 hours .",
    "the batfled models tested here run in 10 - 20 minutes for simulated data and up to 8 hours on the dream data using 16 cores .",
    "however , the code has not been optimized very throughly yet",
    ", so significant improvements are likely possible ."
  ],
  "abstract_text": [
    "<S> the vast majority of current machine learning algorithms are designed to predict single responses or a vector of responses , yet many types of response are more naturally organized as matrices or higher - order tensor objects where characteristics are shared across modes . </S>",
    "<S> we present a new machine learning algorithm batfled ( * * ba**yesian * * t**ensor * * f**actorization * * l**inked to * * e**xternal * * d**ata ) that predicts values in a three - dimensional response tensor using input features for each of the dimensions . </S>",
    "<S> batfled uses a probabilistic bayesian framework to learn projection matrices mapping input features for each mode into latent representations that multiply to form the response tensor . by utilizing a tucker decomposition </S>",
    "<S> , the model can capture weights for interactions between latent factors for each mode in a small core tensor . </S>",
    "<S> priors that encourage sparsity in the projection matrices and core tensor allow for feature selection and model regularization . </S>",
    "<S> this method is shown to far outperform elastic net and neural net models on cold start tasks from data simulated in a three - mode structure . </S>",
    "<S> additionally , we apply the model to predict dose - response curves in a panel of breast cancer cell lines treated with drug compounds that was used as a dialogue for reverse engineering assessments and methods ( dream ) challenge . </S>"
  ]
}