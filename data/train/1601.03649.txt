{
  "article_text": [
    "it is becoming increasingly clear that the relative timings of spikes transmitted by neurons , and not just their firing rates , is used to convey information regarding the features of input stimuli @xcite .",
    "spike - timing as an encoding mechanism is advantageous over rate - based codes in the sense that it is capable of tracking rapidly changing features , for example briefly presented images projected onto the retina @xcite or tactile events signalled by the fingertip during object manipulations @xcite .",
    "it is also apparent that spikes are generated with high temporal precision , typically on the order of a few milliseconds under variable conditions @xcite .",
    "the indicated importance of precise spiking as a means to process information has motivated a number of theoretical studies on learning methods for ( reviewed in @xcite ) . despite this",
    ", there still lack supervised learning methods that can combine high technical efficiency with biological plausibility , as well as those claiming a solid theoretical foundation .",
    "for example , while the previously proposed @xcite and @xcite rules have both demonstrated success in training to form precise temporal representations of spatio - temporal spike patterns , they have lacked analytical rigour during their formulation ; like many existing supervised learning methods for , these rules have been derived from a heuristic , spike - based reinterpretation of the widrow - hoff learning rule , therefore making it difficult to predict the validity of their solutions in general .    the e - learning @xcite has emerged as a supervised learning method with stronger theoretical justification , considering that it instead works to minimise an error function based on the @xcite ; the is a metric for measuring the temporal difference between two neural spike trains , and is determined by computing the minimum cost required to transform one spike train into another via the addition , removal or temporal - shifting of individual spikes . in this study ,",
    "two supervised learning rules were formulated : the first termed e - learning , which is specifically geared towards classifying spike patterns using precisely - timed output spikes , and which provides high network capacity in terms of the number of memorised patterns .",
    "the second rule is termed i - learning , which is more biologically plausible than e - learning but comes at the cost of a reduced network memory capacity .",
    "the e - learning rule has less biological relevance than i - learning given its restriction to offline - based learning , as well as its dependence on synaptic variables that are non - local in time .",
    "further , analytical , spike - based learning methods have been proposed in @xcite , such as the rule , and have demonstrated very high network capacity , but these similarly have been restricted in their implementation to offline learning .    a probabilistic method which optimises by gradient ascent the likelihood of generating a desired output spike train has been introduced by pfister et al . in @xcite .",
    "this supervised method has strong theoretical justification , and importantly has been shown to give rise to synaptic weight modifications that mimic the results of experimental protocols measuring the change in synaptic strength , triggered by the relative timing differences of pre- and postsynaptic spikes @xcite .",
    "furthermore , the statistical framework in which this method has been devised is general , allowing for its extension to diverse learning paradigms such as reinforcement - based learning @xcite , backpropagation - based learning as applied to multilayer @xcite and recurrently connected networks @xcite . despite this",
    ", a potential drawback to this approach comes from its reliance on a stochastic neuron model for generating output spikes ; although this model is well suited to reinforcement - based learning which relies on variable spiking for stochastic exploration @xcite , it is less well suited to the supervised learning of precisely timed output spikes where variable responses become more of a hindrance .    to address these shortcomings",
    ", we present here two supervised learning rules , termed and , which are initially derived based on the statistical method of @xcite , but later adapted for compatibility with the deterministically spiking neuron model . in this way",
    ", these rules claim a stronger theoretical basis than many existing spike - based learning methods , and yet still allow for the learning of precisely timed output spikes .",
    "we then use these rules for demonstrative purposes to explore the conditions under which synaptic plasticity most effectively takes place in to allow for precise temporal encoding .",
    "these two rules differ in their formulation with respect to the treatment of output spike trains : while simply relies on the _ instantaneous _ difference between a target and actual output spike train to inform synaptic weight modifications , goes a step further , and _ exponentially filters _ output spike trains in order to provide more stable weight changes . by this filtering mechanism , we find the rule is able to match the high performance of the e - learning rule .",
    "we conclude by indicating the increased biological relevance of the rule over many existing spike - based supervised methods , based on this spike train filtering mechanism .",
    "this work is organised as follows .",
    "first , the and learning rules are formulated for consisting of deterministic neurons , and compared with existing , and structurally similar , spike - based learning rules .",
    "next , synaptic weight changes triggered by the and rules are analysed under various conditions , including their dependence on the relative timing difference between pre- and postsynaptic spikes , and more generally their dynamical behaviour over time .",
    "the proposed rules are then tested in terms of their accuracy when encoding large numbers of arbitrarily generated spike patterns using temporally - precise output spikes .",
    "for comparison purposes , results are also obtained for the technically efficient e - learning rule .",
    "finally , the rules are discussed in relation to existing supervised methods , as well as their their biological significance .",
    "this section proposes two supervised learning rules for , termed and , that are initially formulated using the statistical approach of @xcite for analytical rigour , but later adapted for use with a deterministically spiking neuron model for the purpose of precise temporal encoding .",
    "this section begins by describing the simplified , underpinning the formulation of the and synaptic plasticity rules .",
    "the neuron is a commonly used spiking neuron model , owing to its relative simplicity and analytical tractability , and represents a special case of the more general spike response model @xcite . for these reasons ,",
    "we begin our analysis by considering a single postsynaptic neuron @xmath0 with a membrane potential @xmath1 at time @xmath2 , defined by the simplified : @xmath3 where the membrane potential is measured with respect to the neuron s resting potential .",
    "this equation signifies a dependence of the neuron s membrane potential on its presynaptic input pattern @xmath4 from @xmath5 synapses , as well as its own sequence of emitted output spikes , @xmath6 , where @xmath7 is its latest spike before @xmath2 .",
    "an actual output spike occurs at a time @xmath8 when @xmath1 crosses the neuron s firing threshold @xmath9 from below .",
    "the first term on the rhs of the above equation describes a weighted summation of the presynaptic input : the parameter @xmath10 refers to the synaptic weight from a presynaptic neuron @xmath11 , the kernel @xmath12 corresponds to the shape of an evoked and @xmath13 , @xmath14 , is a list of presynaptic firing times from @xmath11 . the second term on the rhs describes the refractoriness of the neuron due to postsynaptic spiking , controlled by the reset kernel @xmath15 .    in more detail",
    ", the kernel evolves according to @xmath16 where @xmath17 is the neuron s membrane capacitance and @xmath18 describes the time course of a postsynaptic current elicited due to a presynaptic spike .",
    "the term @xmath19 is the heaviside step function , and is defined such that @xmath20 for @xmath21 and @xmath22 otherwise .",
    "here we approximate the postsynaptic current s time course using an exponential decay @xcite : @xmath23 where @xmath24 is the total charge transferred due to a single presynaptic spike and @xmath25 is a synaptic time constant .",
    "hence , using eq .",
    "( [ eq : lif0_alpha_kernel ] ) , the integral of eq .",
    "( [ eq : psp_kernel_current ] ) can be evaluated to yield the kernel : @xmath26\\ , \\theta(s ) \\;,\\ ] ] where its coefficient is given by @xmath27 . the reset kernel in eq .",
    "( [ eq : potential ] ) evolves according to @xmath28 with its coefficient given by @xmath29 , where the reset potential @xmath30 is the value the neuron s membrane potential is set to immediately after a postsynaptic spike is fired .",
    "in our analysis we set the model parameters as follows : @xmath31 , @xmath32 , @xmath33 , @xmath34 and @xmath35 ; for these choices of parameters , a single presynaptic spike evokes a with a maximum value of after a lag time close to , and the postsynaptic neuron s membrane potential is reset to its resting value of immediately after firing .",
    "shown in fig .",
    "[ fig1 ] are graphical illustrations of the postsynaptic current , and reset kernels , as well an example of a resulting postsynaptic membrane potential as defined by eq .",
    "( [ eq : potential ] ) .    .",
    "( b ) the kernel @xmath12 .",
    "( c ) the reset kernel @xmath15 .",
    "( d ) the resulting membrane potential @xmath1 as defined by eq .",
    "( [ eq : potential ] ) . in this example , a single presynaptic spike is received at @xmath36 , and a postsynaptic spike is generated at @xmath37 from selectively tuning both the synaptic weight @xmath10 and firing threshold @xmath9 values .",
    "we take @xmath38 for the neuron s membrane capacitance , such that the postsynaptic current attains a maximum value of . ]",
    "we now explore in more detail the spike generation mechanism of the postsynaptic neuron .",
    "currently , firing events are considered to take place only when the neuron s membrane potential crosses a predefined firing threshold @xmath9 .",
    "alternatively , however , we may instead consider output spikes that are generated via a stochastic process with a time - dependent , instantaneous firing rate @xmath39 , such that firing events may occur even at moments when the neuron s membrane potential is below the firing threshold .",
    "the instantaneous firing rate @xmath39 is formally referred to as the stochastic intensity of the neuron , and arbitrarily depends on the distance between the neuron s membrane potential and formal firing threshold @xmath9 according to @xmath40 where @xmath1 is defined by eq .",
    "( [ eq : potential ] ) and @xmath41 is an arbitrary function that is commonly referred to as the neuron s ` escape rate ' @xcite .",
    "various choices exist to define the functional form of the neuron s escape rate .",
    "a common choice is to assume an exponential dependence : @xmath42 where @xmath43 is the instantaneous firing rate of the neuron at threshold @xmath9 , and the parameter @xmath44 determines the ` smoothness ' of the firing rate about the threshold @xcite .",
    "it is important to note that in taking the limit @xmath45 the deterministic model can be recovered , the utility of which shall become apparent later .      implementing a stochastic model for generating postsynaptic spikes according to eq .",
    "( [ eq : stochastic_intensity ] ) is advantageous , given that it allows for the determination of the likelihood of generating some desired sequence of target output spikes @xmath46 containing @xmath47 spikes in response to an input spike pattern @xmath48 . as shown originally by @xcite ,",
    "the log - likelihood is given by @xmath49 where @xmath50 is a target postsynaptic spike train and @xmath51 is the duration over which the input pattern @xmath48 is presented . importantly ,",
    "since the neuron model is described by the linear and the escape rate is exponential , the log - likelihood is a concave function of its parameters @xcite .",
    "log - concavity is ideal since it ensures no non - global local maxima exist in the likelihood , thereby allowing for computationally efficient parameter optimisation methods .    in our analysis , we seek to maximise the log - likelihood of a postsynaptic neuron generating a desired target output spike train @xmath52 through modifying the strengths of synaptic weights in the network .",
    "this can be achieved through the technique of gradient ascent , such that a synaptic weight @xmath10 is modified according to @xmath53 hence , taking the derivative of eq .",
    "( [ eq : log_pdf ] ) and using eq .",
    "( [ eq : potential ] ) provides the gradient of the log - likelihood : @xmath54 \\sum_{t_j^f \\in x_j } \\epsilon ( t - t_j^f)\\ , \\mathrm{d}t \\;,\\ ] ] where @xmath55 .",
    "furthermore , using eq .",
    "( [ eq : exp_rate ] ) it follows that @xmath56 which , in combination with eqs .",
    "( [ eq : grad_ascent ] ) , ( [ eq : d_log_pdf ] ) and ( [ eq : d_log_pdf_frac ] ) , provides the weight update rule : @xmath57 \\sum_{t_j^f \\in x_j } \\epsilon ( t - t_j^f)\\ , \\mathrm{d}t \\;,\\ ] ] where @xmath58 is the learning rate .",
    "the above has been derived by @xcite , and has been shown to well approximate results of experimental protocols on synaptic plasticity which depend on the coincidence of pre- and postsynaptic firing times @xcite .    in our approach , however , we wish to instead consider a learning rule that depends on the intrinsic dynamics of a postsynaptic neuron , rather than artificially clamping its firing activity to its target response . to this end , we adjust the weight update rule of eq .",
    "( [ eq : w_update_stoch ] ) to the following rule : @xmath59 \\sum_{t_j^f \\in x_j } \\epsilon ( t - t_j^f)\\ , \\mathrm{d}t \\;,\\ ] ] where we have substituted @xmath60 with @xmath61 , such that the instantaneous firing rate of the postsynaptic neuron depends on its actual sequence of emitted output spikes @xmath62 rather than its target output @xmath63 .",
    "although eq .",
    "( [ eq : w_update_intrinsic ] ) is an approximation of the theoretical result of eq .",
    "( [ eq : w_update_stoch ] ) , it can be shown that it nevertheless converges towards a similar solution when certain conditions are satisfied , depending on the magnitude of @xmath44 and @xmath64 , and the relative timing displacements between target and actual output spikes ( see ) .      the weight update rule of eq .",
    "( [ eq : w_update_stoch ] ) has been derived by taking a maximum - likelihood approach based on a stochastic spiking neuron model , but can be adapted , using eq .  , to the case of a deterministically firing neuron model to allow for precise temporal encoding . specifically , if the limit @xmath45 is taken for the stochastic threshold parameter in eq .",
    "( [ eq : exp_rate ] ) , the stochastic intensity of an intrinsically spiking neuron instead assumes one of two values : @xmath65 where the term @xmath66 is the dirac delta distribution about an actual postsynaptic firing time @xmath67 , since immediately after a spike is emitted : @xmath68 as a result of the reset term in eq .",
    "( [ eq : potential ] ) . in this way ,",
    "the postsynaptic neuron s stochastic intensity can be substituted with its output spike train @xmath69 , where @xmath70 .",
    "hence , using eq .   and the result for eq .",
    ", a deterministic adaptation of eq .",
    "( [ eq : w_update_stoch ] ) is given by @xmath71 \\sum_{t_j^f \\in x_j } \\epsilon ( t - t_j^f)\\ , \\mathrm{d}t \\;,\\ ] ] where we have renormalised the above equation by redefining the learning rate to maintain a finite value : @xmath72 .",
    "strictly speaking , taking the limit @xmath45 can not be guaranteed to provide convergence towards an optimal synaptic weight solution , as is otherwise predicted for small postsynaptic timing displacements and finite @xmath44 , but suitably simplifies the learning rule for the case of a deterministic , and intrinsically spiking , neuron model .",
    "the convergence of this simplified rule shall be experimentally analysed in detail in the results section .",
    "furthermore , performing the straightforward integration of eq .",
    "( [ eq : w_update_inst ] ) provides the batch weight update rule : @xmath73 \\;,\\ ] ] which we term the synaptic plasticity rule , to reflect the discontinuous nature of the postsynaptic error signal .",
    "the rule can be summarised as a two - factor learning rule : presynaptic activity describing a stimulus ( first learning factor ) is combined with a postsynaptic error signal ( second learning factor ) to elicit a final synaptic weight change .",
    "broadly speaking , the rule falls into a class of learning rules for which depend on an instantaneous error signal to drive synaptic weight modifications .",
    "key examples include the plasticity rule proposed in @xcite , the i - learning variant of the chronotron @xcite and the algorithm @xcite . despite this ,",
    "certain differences exist between and the aforementioned examples .",
    "specifically , weight updates for both and i - learning rely on the postsynaptic current @xmath18 , rather than the @xmath12 as is used here , as a presynaptic learning factor ( compare eqs .",
    "( [ eq : lif0_alpha_kernel ] ) and ( [ eq : psp_kernel ] ) , respectively ) .",
    "the selection of @xmath18 as a presynaptic learning factor is somewhat arbitrary , while @xmath12 is theoretically supported @xcite .",
    "although and the algorithm share @xmath12 as their presynaptic learning factor , the algorithm just takes into account the first occurrence of an error due to a misplaced postsynaptic spike , rather than accumulating all postsynaptic spike errors as for .",
    "the authors decision to restrict learning to the first error in each trial was motivated by a desire to avoid non - linear accumulation of errors arising from interacting postsynaptic spikes , due to the neuron s reset term , in order that weight updates alter the future time course of the neuron s activity in a more predictable manner @xcite .",
    "here we relax this constraint for the sake of biological plausibility and ease of implementation , but still ensure that target postsynaptic spikes are sufficiently separated from each other to reduce this error accumulation effect .      as it currently stands , the time course of the synaptic weight change @xmath74 resulting from eq .",
    "( [ eq : w_update_inst ] ) depends on the instantaneous difference between two spike trains @xmath52 and @xmath75 during learning . in other words ,",
    "candidate weight updates are only effected at the precise moments in time when target or actual postsynaptic spikes are present .",
    "although this leads to the simplified batch weight update rule of eq .",
    "( [ eq : w_update_inst_batch ] ) , there are two distinct disadvantages to this approach .",
    "the first concerns the convergence of actual postsynaptic spikes towards matching their desired target timings ; as we shall show in the results section , and as previously indicated in @xcite , if the temporal proximity of postsynaptic spikes is not accounted for by the learning rule , then fluctuations in the synaptic weights can emerge as a result of unstable learning .",
    "it then becomes problematic for the network to smoothly converge towards a desired weight solution , and maintain fixed output firing activity .",
    "secondly , from a biological standpoint it is implausible to assume that synaptic weights can be effected instantaneously at the precise timings of postsynaptic spikes .",
    "more realistically , it can be supposed that postsynaptic spikes would leave some form of synaptic trace that persists on the order of the membrane time constant , which , in combination with coincident presynaptic spiking as detected via evoked , would inform more gradual synaptic weight changes .    to address these limitations of instantaneous - error based learning",
    "we convolve the target and actual output spike trains of the postsynaptic neuron of eq .",
    "( [ eq : w_update_inst ] ) with an exponential kernel , thereby providing the following learning rule : @xmath76 \\sum_{t_j^f \\in x_j } \\epsilon ( t - t_j^f)\\ , \\mathrm{d}t \\;,\\ ] ] where a convolved actual output spike train is equivalent to @xmath77 and a similar equivalence for a target output spike train @xmath78 .",
    "the decay time constant is set to @xmath79 , similar to the membrane time constant @xmath80 , which has been indicated to give optimal performance from preliminary parameter sweeps .",
    "the upper limit of @xmath81 in eq .",
    "( [ eq : w_update_conv ] ) is necessary in order to account for the entire time course of convolved postsynaptic traces . performing the integration of eq .",
    "( [ eq : w_update_conv ] ) using the kernel given by eq .",
    "( [ eq : psp_kernel ] ) yields the batch weight update rule : @xmath82 \\;,\\ ] ] where the learning window @xmath83 arises from interacting pre- and postsynaptic spikes , and is given by @xmath84 & \\text{for } s > 0 \\\\      \\epsilon_0\\ , \\left ( \\mathcal{c}_m - \\mathcal{c}_s \\right)\\ , \\exp \\left ( \\frac{s}{\\tau_q } \\right ) & \\text{for } s \\leq 0 \\;.    \\end{cases}\\ ] ] in the above equation , the membrane and synaptic coefficient terms are @xmath85 and @xmath86 , respectively .",
    "we term eq .",
    "( [ eq : w_update_filt ] ) the synaptic plasticity rule , that depends on the smoothed difference between filtered target and actual output spike trains .",
    "the rule falls into a class of learning rules for which rely on a smoothed error signal for weight updates , and which more effectively take into account the temporal proximity of neighbouring target and actual postsynaptic spikes , such as the rule @xcite and e - learning variant of the @xcite .",
    "in particular , eq .",
    "( [ eq : w_update_conv ] ) bears a similarity with , in the sense that weight updates depend on convolved pre- and postsynaptic spike trains .",
    "however , as for the rule , makes no prediction for the choice of kernel function with which to convolve presynaptic spike trains . in our analysis",
    ", presynaptic spikes are suitably convolved with the kernel of eq .",
    "( [ eq : psp_kernel ] ) .",
    "the exponential filtering of postsynaptic spike trains by the rule may appear arbitrary , but it is not unreasonable to suppose that this operation is carried out via changes in the neuron s membrane potential in response to postsynaptic spikes : especially since the filter time constant has an optimal value , as determined through preliminary parameter sweeps , that is similar to the neuron s membrane time constant , @xmath87 .    additionally , selecting an exponential filter",
    "simplifies the resulting batch weight update rule , and coincidentally provides a resemblance of to the as is used to measure the ( dis)similarity between neuronal spike trains @xcite , where the is defined by @xmath88 ^ 2 \\mathrm{d}t \\;.\\ ] ] hence , the rule might also be interpreted as an error minimisation procedure that reduces , by gradient descent , a -like error function measuring the distance between target and actual postsynaptic spike trains .",
    "we first analyse the validity of synaptic weight modifications resulting from the and rules under general learning conditions . for ease of analysis we examine just the weight change between a single pair of pre- and postsynaptic neurons : each emitting a single spike at times @xmath89 and @xmath90 , respectively . a single target output spike at time @xmath91 is also imposed , which the postsynaptic neuron must learn to match .",
    "this subsection is organised as follows .",
    "first , simplified weight update rules for and are presented based on single pre- and postsynaptic spiking .",
    "next , two distinct scenarios of weight change driven by each learning rule are examined .",
    "the first scenario examines the weight change as a function of the relative timing difference between a target postsynaptic spike and presynaptic spike .",
    "the second scenario then considers the dynamics of each learning rule by examining their weight change as a function of the current weight value , with the intent of establishing their potential for stable convergence towards a desired weight solution .",
    "[ [ synaptic - weight - updates - for - single - spikes . ] ] synaptic weight updates for single spikes .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    according to the definition of the rule in eq .",
    "( [ eq : w_update_inst_batch ] ) , the synaptic weight change triggered by single spikes is given by @xmath92 \\;,\\ ] ] that is simply the difference between two kernels .",
    "for the above equation there exist several conditions under which no weight change results , including the trivial case when both terms are equal to zero as a result of post- before presynaptic spiking ( i.e. @xmath93 ) .",
    "additionally , no weight change occurs when both terms share the same value : ideally this would take place when target and actual output spikes become aligned , i.e. when @xmath94 .",
    "however , no weight change is also possible for non - aligned output spikes , since the kernel assumes the same value for two distinct lag times ( compare the rising and falling segments of the curve in fig .",
    "[ fig1]b ) .",
    "similarly , the batch weight update rule of eq .",
    "( [ eq : w_update_filt ] ) can be simplified for single pre- and postsynaptic spikes : @xmath95 \\;,\\ ] ] that is the difference between two synaptic learning windows , @xmath83 , as defined by eq .",
    "( [ eq : psp_filt ] ) . as with the rule , there is no weight change for the above equation in the event that both @xmath83 terms share the same value : like the kernel @xmath12 there exists two distinct lag times for which @xmath83 assumes the same value ( see the form of the curve in fig .",
    "[ fig2]b ) , hence target and actual postsynaptic spikes need not necessarily be aligned to elicit a zero weight change . unlike the rule , however , a weight change for can be non - zero for post- before presynaptic spiking .",
    "in the rest of this subsection we start by simply examining the synaptic weight change as a function of the relative timing difference between a target postsynaptic spike and input presynaptic spike , in the absence of an actual postsynaptic spike , in order to establish the temporal learning window of each synaptic plasticity rule .",
    "we then graphically study the dynamics of each rule by plotting their phase space diagrams , to predict their long term temporal evolution of the synaptic weight towards a limiting value . for demonstrative purposes the learning rate of the and rule",
    "is set to unity here , although there is no qualitative change in the results for different values .",
    "[ [ temporal - window - of - the - learning - rules . ] ] temporal window of the learning rules .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    shown in fig .",
    "[ fig2 ] is the synaptic weight change for each learning rule as a function of the relative timing difference between a target postsynaptic spike and presynaptic spike , denoted by @xmath96 , including for negative relative timings .",
    "both panels in this figure correspond to the absence of an actual postsynaptic spike , to clearly illustrate the temporal locality of each synaptic plasticity rule .     on the relative timing difference between a target postsynaptic spike and input presynaptic spike : @xmath97 and @xmath98 , respectively .",
    "* ( a ) leaning window of the rule .",
    "( b ) learning window of the rule .",
    "the peak @xmath99 values for and correspond to relative timings of just under , respectively .",
    "both panels show the weight change in the absence of an actual postsynaptic spike . ]    from the top panel of fig .",
    "[ fig2]a for the rule , it is observed that the plot of the synaptic change simply follows the form of a kernel . in this case , the synaptic change is zero for negative values of the relative timing difference , demonstrating the causality of a presynaptic spike in eliciting a desired postsynaptic spike .",
    "interestingly , the top panel of fig .",
    "[ fig2]b for the rule instead demonstrates a more symmetrical dependence of synaptic change on the relative timing difference , which is centred just right of the origin .",
    "this contrasts with the rule , and can be explained by the rule instead working to minimise the _ smoothed _ difference between a target and actual spike train , rather than just their _ instantaneous _ difference ; in other words , even if an actual , emitted postsynaptic spike can not technically be aligned with its target , then a close match is deemed to be sufficient under .    [ [ dynamics - of - the - learning - rules . ] ] dynamics of the learning rules .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we now turn to examining the dynamics of synaptic weight changes elicited under the and learning rules , in order to predict their long term behaviour .    at this point",
    "it is necessary to discuss the relationship between the timing of an actual output spike fired by a postsynaptic neuron and the shape of a evoked by an input spike . in response to a single synapse",
    ", a postsynaptic neuron is constrained to firing an output spike with a lag time up to the peak value of the kernel , since this is the only region over which the neuron s membrane potential can be adjusted to cross its firing threshold from below .",
    "for this reason , we confine our analysis here to examining the dynamics of synaptic weight changes arising from postsynaptic spikes that occur over the rising segment of the curve , corresponding to lag times up to @xmath100 for our choice of parameters , as is visualised in fig .",
    "[ fig1]b .    in more detail ,",
    "if a postsynaptic neuron @xmath0 receives a single input spike at @xmath36 from a synapse @xmath11 with weight @xmath101 , then its actual output firing time @xmath90 is provided by the relation : @xmath102 where the conditional parameter",
    "@xmath103 corresponds to the maximum value @xmath12 attains after a lag time of @xmath104 . for values",
    "@xmath105 there is insufficient synaptic drive to initiate an output spike .",
    "furthermore , if we isolate @xmath12 over its sub - domain : @xmath106 $ ] , corresponding to the rising segment of the , then the actual output firing time can be explicitly written in terms of an inverse function of @xmath12 : @xmath107 and which can be determined as @xmath108 when assuming the proportionality between the membrane and synaptic time constants : @xmath109 .",
    "as described by the above equation , an increase in the synaptic weight works to shift an actual spike backwards in time , and a decrease in the synaptic weight shifts an actual spike forwards in time . by this process , a neuron can be trained to find a desirable synaptic weight value which minimises the temporal difference of an actual output spike with respect to its target .    using eqs .",
    "( [ eq : dynamics2 ] ) and ( [ eq : inst_spike ] ) , assuming @xmath36 and taking @xmath110 , the weight update rule can be summarised as follows for a single synapse : @xmath111 that is a discontinuous function of the synaptic strength @xmath10 .",
    "the above synaptic plasticity rule is plotted as a phase portrait in fig .",
    "[ fig3]a , illustrating the change in the synaptic weight as a function of its current strength .",
    "this figure displays two states of the postsynaptic neuron : the first of which is quiescence for subthreshold weight values , and the other firing activity for suprathreshold values .",
    "the sudden transition from positive to negative @xmath99 about @xmath112 ( coinciding with @xmath113 ) corresponds to a transition between these two states , whereupon the neuron first responds with an output spike .",
    "this transition point also acts as an attractor for the system , to the extent that weight values @xmath114 are drawn towards it , where @xmath115 is a desired weight solution .",
    "this point is unstable , however , due to the discontinuity in @xmath99 , and ultimately results in fluctuations of @xmath116 .",
    "this unstable attractor is detrimental to network performance for two key reasons : the first being that it potentially draws @xmath116 away from its target value of @xmath115 , and the second arising from its tendency to drive variable postsynaptic firing activity as the neuron is effectively ` switched on and off ' due to fluctuations in @xmath116 about @xmath9 .",
    "the second fixed point in fig .",
    "[ fig3]a , indicated by the second dashed line from the left , is a repeller , and , unless @xmath116 is exactly equal to @xmath115 , will work to repel @xmath116 .",
    "this point in particular leads us to predict that learning is unlikely to precisely converge under the rule , and especially for large initial values of @xmath116 for which divergence will result .     as a function of its current strength relative to threshold @xmath117 .",
    "* in this example , a postsynaptic neuron receives an input spike at time @xmath118 from a single synapse with weight @xmath116 .",
    "the postsynaptic neuron must learn to match a target output spike time @xmath119 , which corresponds to a desired synaptic weight solution @xmath115 as indicated in both panels .",
    "the actual output spike fired by the neuron is shifted backwards in time for positive @xmath99 , and vice versa for negative @xmath99 . the horizontal arrows in each panel",
    "show the direction in which @xmath116 evolves , and are separated by the vertical dashed lines .",
    "the peak value @xmath113 ( see methods ) results in an actual output spike being fired for @xmath120 . ]",
    "starting with eq .",
    "( [ eq : filt_spike ] ) and again assuming : @xmath36 and @xmath110 , the rule can be summarised as follows for a single synapse : @xmath121 where the actual output firing time @xmath90 , emitted over the s rising segment , is determined by eq .",
    "( [ eq : dynamics3 ] ) .",
    "shown in fig .",
    "[ fig3]b is a phase portrait of the rule , where the weight change is plotted as a function of its current strength .",
    "similarly as discussed before in relation to the rule , the postsynaptic neuron here exhibits two distinct states : quiescence for @xmath122 , and firing activity for @xmath123 . as for the rule",
    "there is a discontinuity in @xmath99 as the neuron crosses its firing threshold , however , unlike with , @xmath99 remains positive until the desired weight solution is reached at @xmath115 .",
    "this has the effect of shifting the system attractor to @xmath115 ( indicated by first dashed line from the left ) , as well as making it a stable point by avoiding a discontinuous change in @xmath99 .",
    "conversely , the second dashed line corresponds to an unstable fixed point , and works to repel @xmath116 . taken together",
    ", it follows that for sufficiently small initial values of @xmath116 the rule predictably leads to convergence in learning , with a stable synaptic weight solution .    ,",
    "relative to an input spike time , that can accurately be learned using the rule , plotted as a function of the filter time constant @xmath124 .",
    "* this figure makes predictions based on a single synapse with an input spike at . at @xmath125",
    "the minimum time @xmath126 is equivalent to @xmath127 , that is the lag time corresponding to the maximum value of the kernel , and becomes equivalent to . as a reference ,",
    "the value @xmath79 was selected for use in our computer simulations , which was indicated to give optimal performance on preliminary runs . ]    however , depending on the filter time constant @xmath124 , there is a limit on the minimum value of @xmath91 that can reliably be learned when using the rule . in more detail , and from using eqs .",
    "( [ eq : filt_dynamics ] ) and ( [ eq : psp_filt ] ) , it can be shown that this lower bound on @xmath91 for stable convergence of the learning rule is given by @xmath128 corresponding to the moment at which the associated weight solution @xmath115 changes from a stable to unstable fixed point .",
    "in other words , values of @xmath129 would result in diminished learning , as @xmath116 is instead repelled away from its target value of @xmath115 . from eq .",
    "( [ eq : filt_dynamics2 ] ) it is clear that the free parameter @xmath124 influences the stability of the learning rule , such that @xmath130 is mapped to a minimum target timing of @xmath131 as illustrated in fig .",
    "[ fig4 ] for @xmath132 .",
    "therefore , decreasing @xmath126 with respect to its parameter @xmath124 should predictably lead to increased temporal precision of the rule . as stated in the previous section",
    ", we select @xmath79 for use in our simulations : this corresponds to a value of @xmath126 that is just under .",
    "[ [ summary . ] ] summary .",
    "+ + + + + + + +    this subsection has analysed synaptic weight modifications driven by the and learning rules , based on single pre- and postsynaptic spiking for a single synapse . in particular , is predicted to provide convergence towards a stable and accurate solution in most cases , which depends crucially on the magnitude of its filter time constant @xmath124 .",
    "by contrast , the rule is predicted to give rise to less accurate solutions , and typically result in variable firing activity due to fluctuations in the synaptic strength close to the postsynaptic neuron s firing threshold .",
    "in fact , this instability is indicative of a key difference between the rule and pfister s learning rule as defined by eq .  :",
    "while postsynaptic spiking , post - training , under pfister s rule would fluctuate around its target timing , would instead lead to fluctuating spikes around a timing coinciding with the peak value of the , independent of the target time .",
    "finally , it is noted that , for analytical tractability , these dynamical predictions for and have been made for single , rather than multiple , synapses .",
    "hence , it shall be the aim of the next section to explore the validity of these learning rules in larger network sizes through numerical simulation .",
    "this subsection presents results from computer simulations testing the performance of the , and e - learning rules .",
    "e - learning , henceforth referred to here as , is used in our simulations , being an ideal benchmark against which our derived rules can be compared ; is ideal since it incorporates a mechanism for linking together target and actual postsynaptic spikes , analogous to the proposed rule in the sense that it accounts for the temporal proximity of neighbouring postsynaptic spikes , as well as allowing for a very high network capacity in terms of the maximum number of input patterns it can learn to memorise @xcite .",
    "it is worth noting that these three learning rules are essentially based on distinct spike train error measures : the rule simply based on a momentary spike count error , the rule based on a smoothed -like error function @xcite , and the rule based on an adaptation of the measure @xcite .",
    "[ [ network - setup . ] ] network setup .",
    "+ + + + + + + + + + + + + +    in simulations , the network consisted of either one or multiple postsynaptic neurons receiving input spikes from a variable number @xmath5 of presynaptic neurons in a feedforward manner .",
    "the dynamics of the postsynaptic neuron s membrane potential @xmath1 was governed according to the defined by eq .",
    "( [ eq : potential ] ) , and output spikes were instantly generated when the neuron s membrane potential reached the formal firing threshold @xmath9 ; hence , we implemented a deterministic adaptation of the stochastic neuron model presented in eq .",
    "( [ eq : exp_rate ] ) , as necessitated by the derived and learning rules .",
    "the internal simulation time step was taken as .",
    "the synaptic weight between each presynaptic neuron @xmath11 and the postsynaptic neuron @xmath0 was initialised randomly at the start of every simulation run , with @xmath10 values uniformly distributed between 0 and @xmath133 ; as a result , the initial firing rate of the postsynaptic neuron was driven to @xmath134 .",
    "input patterns were conveyed to the network by the collective firing activity of presynaptic neurons , where a pattern consisted of a single spike at each neuron .",
    "presynaptic spikes were uniformly distributed over the pattern duration , and selected independently for each neuron .",
    "the choice of single rather than multiple input spikes to form pattern representations proved to be more amenable to the subsequent analysis of gathered results . in all cases , an arbitrary realisation of each pattern",
    "was used at the start of each simulation run , which was then held fixed thereafter . by this method ,",
    "a total number @xmath135 of unique patterns were generated .",
    "patterns were generated with a duration @xmath136 , that is approximately the time - scale of sensory processing in the nervous system .",
    "[ [ general - learning - task . ] ] general learning task .",
    "+ + + + + + + + + + + + + + + + + + + + + +    the postsynaptic neuron was trained to reproduce an arbitrary target output spike train in response to each of the @xmath135 input patterns through synaptic weight modifications in the network , using either the , or learning rules . in this way",
    ", the network learned to perform precise temporal encoding of input patterns . during training ,",
    "all @xmath135 input patterns were sequentially presented to the network in batches , where the completion of a batch corresponded to one epoch of learning .",
    "resulting synaptic weight changes computed for each of the individually presented input patterns ( or each trial ) were accumulated , and applied at the end of an epoch .",
    "the learning rate used for the rules was by default @xmath137 , which scaled with the number of presynaptic neurons @xmath5 , target output spikes @xmath47 and patterns @xmath135 ; any exceptions to this are specified in the main text .",
    "as shall be shown in our simulation results , it was indicated that all of the learning rules shared a similar , optimal value for the learning rate .    [ [ performing - a - single - input - output - mapping . ] ] performing a single input - output mapping .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for demonstrative purposes , we first applied the and learning rules to training the network to perform a mapping between a single , fixed input spike pattern and a target output spike train containing four spikes .",
    "the network contained 200 presynaptic neurons , and the target output spikes were equally spaced out with timings of .",
    "these wide separations were selected to avoid excessive nonlinear accumulation of error due to interactions between postsynaptic spikes during learning .",
    "simulations for the learning rules were run over 200 epochs , where each epoch corresponded to one , repeated , presentation of the pattern .",
    "hence , a single simulation run reflected of biological time .    shown in fig .",
    "[ fig5]a is a spike raster of an arbitrarily generated input pattern , consisting of a single input spike at each presynaptic neuron . in this example",
    ", two postsynaptic neurons were tasked with transforming the input pattern into the target output spike train through synaptic weight modifications , as determined by either the or learning rule . from the actual output spike rasters",
    "depicted in panel b , it can be seen that both postsynaptic neurons learned to rapidly match their target responses during learning . despite this , persistent fluctuations in the timings of actual output spikes were associated with just the rule , while the displayed stability over the remaining epochs .",
    "finally , panel c shows the accuracy of each learning rule , given as the average ( see eq .  ( [ eq : vrd ] ) ) plotted as a function of the number of learning epochs . with respect to the rule",
    ", it can be seen the failed to reach zero and was subject to a high degree of variance , as reflected by the corresponding spike raster in panel b ; its final , convergent value was , that is an output spike timing error of around with respect to its target .",
    "by contrast , the filt rule s value rapidly approached zero , and was subject to much less variation during the entire course of learning ( final value was ) .        [ [ synaptic - weight - distributions . ] ] synaptic weight distributions . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    shown in fig .",
    "[ fig6 ] are the distributions of synaptic weights before and after network training for the and learning rules , corresponding to the same experiment of fig .",
    "[ fig5 ] . in plotting fig .",
    "[ fig6 ] , synaptic weights were sorted in chronological order with respect to their associated presynaptic firing times ; for example , the height of a bar at reflects the average value of a synaptic weight from a presynaptic neuron which transmitted a spike at .",
    "the gold overlaid lines correspond to the previously defined target output spike timings of .    .",
    "* the input synaptic weight values are plotted in chronological order , with respect to their associated firing time .",
    "( a ) the distribution of weights before learning .",
    "( b ) post training under the rule .",
    "( c ) post training under the the rule .",
    "the gold coloured vertical lines indicate the target postsynaptic firing times .",
    "note the different scales of a , b and c. results were averaged based on 40 independent runs .",
    "the design of this figure is inspired from @xcite . ]    from this figure , panel a illustrates the uniform distribution of synaptic weights used to initialise the network before any learning took place , which had the effect of driving the initial postsynaptic firing rate to @xmath134 .",
    "panels b and c show the distribution of synaptic weights at the end of learning , when the and rules were respectively applied . from these two panels , a rapid increase in the synaptic weight values preceding the target output spike timings can be seen , which then proceeded to fall off . comparatively , the magnitude of weight change was largest for the rule , with peak values over three times that produced by .",
    "furthermore , only the rule resulted in negatively - valued weights , which is especially noticeable for weights associated with input spikes immediately following the target output spike timings . in effect , these sharp depressions offset the relatively strong input drive received by the postsynaptic neuron just before the target output spike timings , which is indicative of the unstable nature of the learning rule .",
    "by contrast , the rule led to a ` smoother landscape ' of synaptic weight values , following a periodic pattern when plotted in chronological order .",
    "[ [ impact - of - the - learning - rate . ] ] impact of the learning rate .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this experiment we explored the dependence of each rule s performance on the learning rate parameter @xmath58 in terms of the spike - timing accuracy of a trained postsynaptic neuron , measured using the .",
    "the primary objective was to establish the relative sensitivity of each rule to large values of @xmath58 , and secondly to establish a value of @xmath58 which provided a suitable trade - off between learning speed and final convergent accuracy .",
    "here we first include the e - learning rule proposed by @xcite , to provide a benchmark for the and rules .",
    "with respect to the experimental setup , the network consisted of 200 presynaptic neurons and a single postsynaptic neuron , and was tasked with learning to map a total of 10 different input patterns to the same , single target output spike with a timing of . in this case",
    "learning took place over 500 epochs .",
    "as shown in fig .",
    "[ fig7 ] it is clear that the rule was most sensitive to changes in the learning rate , with an average value @xmath138 that of for the largest learning rate value @xmath110 .",
    "the least sensitive rule turned out to be , which still managed to maintain an average value close to zero when plotted up to the maximum value of @xmath58 .",
    "interestingly , all three distance plots displayed the same general trend over the entire range of learning rates considered : there was a rapid decrease for small @xmath58 values , followed by a plateau up to around @xmath139 , and then a noticeable increase towards the end .",
    "the large distance values for small @xmath58 related to a lack of convergence in learning by the postsynaptic neuron after being trained over 500 epochs .     for each learning rule .",
    "* the e - learning rule of @xcite is included as a benchmark for the and rules . in every instance , a network containing 200 presynaptic neurons and a single postsynaptic neuron was tasked with mapping 10 arbitrary input patterns to the same target output spike with a timing of . learning took place over 500 epochs , and results were averaged over 40 independent runs . in this case , error bars show the standard error of the mean rather than the standard deviation : the was subject to very high variance for large @xmath58 values , therefore we considered just its average value and not its distribution . ]    to summarise , these results support our choice of an identical learning rate for all three learning rules as used in the subsequent learning tasks of this section .",
    "additional , more exhaustive parameter sweeps from further simulations conclusively demonstrated that the learning rates for all three learning rules shared the same inverse proportionality with the number of presynaptic neurons , patterns and target output spikes .",
    "this corresponded to an optimal value of @xmath140 in fig .",
    "[ fig7 ] .    [",
    "[ classifying - spike - patterns . ] ] classifying spike patterns .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    an important characteristic of a neural network is the maximum number of patterns it can learn to reliably memorise , as well the time taken to train it .",
    "therefore , we tested the performance of the network on a generic classification task , where input patterns belonging to different classes were identified by the precise timings of individual postsynaptic spikes .",
    "we first determine the performance of a network when trained to identify separate classes of input patterns based on the precise timing of a _ single _ postsynaptic spike , and then later consider identifications based on _ multiple _ postsynaptic spike timings . in this experiment ,",
    "the network contained a single postsynaptic neuron , and was trained using either the , or learning rule for comparison purposes .",
    "the network was tasked with learning to classify @xmath135 arbitrarily generated input patterns into five separate classes through hetero - association : an equal number of patterns were randomly assigned to each class , and all patterns belonging to the same class were identified using a shared target output spike timing . hence , an input pattern was considered to be correctly identified if the postsynaptic neuron responded by firing just a single output spike that fell within @xmath141 of its required target timing .",
    "the value of @xmath141 was varied depending on the level of temporal precision desired , with values selected from the range @xmath142 $ ] ms corresponding to the typical level of spike timing precision as has been observed in the brain @xcite . for each input class a target output spike time was randomly generated according to a uniform distribution that ranged in value between 40 and ; the lower bound of was enforced , given previous evidence indicating that smaller values are harder to reproduce by an @xcite . to ensure input classes",
    "were uniquely identified , target output spikes were distanced from each other by a of at least 0.5 , corresponding to a minimum timing separation of .    shown in the left column of fig .",
    "[ fig8 ] is the performance of a network containing either 200 , 400 or 600 presynaptic neurons , as a function of the number of input patterns to be classified . in this case",
    ", we took @xmath143 as the required timing precision of a postsynaptic spike with respect to its target , for each input class . to quantify the classification performance of the network , we defined a measure @xmath144 which assumed a value of in the case of a correct pattern classification , and otherwise .",
    "hence , in order to determine the maximum number of patterns memorisable by the network , we took an averaged performance level @xmath145 as our cut - off point when deciding whether all of the patterns were classified with sufficient reliability ; this criterion was also used to determine the minimum number of epochs taken by the network to learn all the patterns , and is plotted in the right column of this figure .",
    "epoch values not plotted for an increased number of patterns reflected an inability of the network to learn every pattern within 500 epochs .",
    "patterns into five separate classes . *",
    "each input class was identified using a single , unique target output spike timing , which a single postsynaptic neuron had to learn to match to within .",
    "_ left : _ the averaged classification performance @xmath146 for a network containing @xmath147 and 600 presynaptic neurons .",
    "_ right : _ the corresponding number of epochs taken by the network to reach a performance level of .",
    "more than 500 epochs was considered a failure by the network to learn all the patterns at the required performance level .",
    "results were averaged over 20 independent runs , and error bars show the standard deviation . ]",
    "as expected , fig .",
    "[ fig8 ] demonstrates a decrease in the classification performance as the number of input patterns presented to the network was increased , with a clear dependence on the number of presynaptic neurons contained in the network .",
    "for example , a network trained under was able to classify 15 , 30 and 40 patterns at a performance level when containing 200 , 400 and 600 presynaptic neurons , respectively .",
    "the number of input patterns memorised by a network can be characterised by defining a load factor @xmath148 , where @xmath135 is the number of patterns presented to a network containing @xmath5 presynaptic neurons @xcite . furthermore",
    ", the _ maximum _ number of patterns memorisable by a network can be quantified by its memory capacity @xmath149 , where @xmath150 is the maximum number of patterns memorised using @xmath5 synapses .",
    "hence , using as the cut - off point for reliable pattern classifications , we found the rule had an associated memory capacity of @xmath151 . by comparison ,",
    "the memory capacities for the and rules were @xmath152 and @xmath153 , respectively , being around twice the capacity of that determined for . beyond these",
    "increased memory capacity values , networks trained under or were capable of performance levels very close to when classifying a relatively small number of patterns ; by contrast , the maximum performance level attainable under was just over , and was subject to a relatively large variance of around . finally , it is evident from this figure that both and shared roughly the same performance levels over the entire range of input patterns and network structures considered . in terms of the time taken to train the network , both and were equally fast , while was typically slower than the other rules by a factor of between three and four .",
    "this difference in the training time became more pronounced as both the number of input patterns and presynaptic neurons were increased .",
    "[ [ memory - capacity . ] ] memory capacity .",
    "+ + + + + + + + + + + + + + + +    we now explore in more detail the memory capacity @xmath154 supported under each learning rule , specifically with respect to its dependence on the output spike timing precision @xmath141 used to identify input patterns . in determining the memory capacity as a function of the timing precision",
    ", we used the same experimental setup as considered previously for @xmath143 , but extended to also consider values of @xmath141 between 0.2 and ( equally spaced in increments of ) .",
    "as before , we assumed the maximum number of patterns memorisable by the network as those that were classified with a corresponding averaged classification performance @xmath155 of at least within 500 epochs .    from fig .",
    "[ fig9 ] it can be seen that the memory capacity provided by each learning rule increased with the size of the timing precision , which eventually levelled off for values @xmath156 .",
    "it is also clear that the trend for the rule is consistent with that for over the entire range of timing precision values considered , while the rule gave rise to the lowest memory capacities . for values",
    "@xmath157 the difference in memory capacity between and was most pronounced , to the extent that was incapable of memorising any input patterns for @xmath158 . by contrast , still maintained a memory capacity close to 0.07 when classifying patterns based on ultra - precise output spike timings of within .",
    "as a validation of our method , we note that our measured memory capacity for at a timing precision of is in close agreement with that determined originally in fig .",
    "9a of @xcite : with a value close to 0.15 after 500 epochs of network training .     of each learning rule as a function of the required output spike timing precision . *",
    "the network contained a single postsynaptic neuron , and was trained to classify input patterns into five separate classes within 500 epochs .",
    "memory capacity values were determined based on networks containing @xmath159 , 400 and 600 presynaptic neurons .",
    "results were averaged over 20 independent runs . ]    [ [ multiple - target - output - spikes . ] ] multiple target output spikes .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    finally , we examine the performance of the learning rules when input patterns are identified by the timings of _ multiple _ postsynaptic spikes . in this case , the network contained 200 presynaptic neurons and a single postsynaptic neuron , and was trained to classify a total of 10 input patterns into five separate classes , with two patterns belonging to each class . both patterns belonging to a class were identified by the same target output spike train ; hence , a correct pattern classification was considered when the number of actual output spikes fired by the postsynaptic neuron matched the number of target output spikes , and every actual spike fell within @xmath141 of its respective target . for each input class ,",
    "target output spikes were randomly generated according to a uniform distribution bound between 40 and , as used previously .",
    "a minimum inter - spike interval of was enforced to minimise interactions between output spikes . to ensure input classes",
    "were uniquely represented , generated target output spike trains were distanced from one another by a of at least @xmath160 , where @xmath47 was the number of spikes contained in a target train .    shown in fig .",
    "[ fig10 ] is the performance of the network trained under each learning rule when classifying input patterns based on the precise timings of between one and five target output spikes , with a timing precision @xmath143 .",
    "because the learning rate was inversely proportional to the number of target spikes , we extended the maximum number of epochs to 1000 to ensure the convergence of each rule .",
    "as can be seen in this figure , the performance dropped as the number of output spikes increased , and most noticeably for the rule which returned a minimum performance value approaching when patterns were identified using five output spikes . by comparison ,",
    "the rule gave rise to the highest performance levels over the entire range of output spikes tested , closely followed by the rule .",
    "if we count the maximum number of output spikes learnable by the network above a performance level , we obtain one , three and four output spikes for , and , respectively , where the associated number of training epochs in each instance is plotted in the right panel of the figure . from this",
    ", it is observed that was fastest in training the network to learn multi - spike based pattern classifications , closely followed by and finally .",
    "[ [ summary.-1 ] ] summary .",
    "+ + + + + + + +    taken together , the experimental results of this section demonstrate a similarity in the performance between the and rules under most circumstances , except when applied to learning multiple target output spikes for which the rule was best suited .",
    "the rule , however , performed worst in all cases , and in particular displayed difficulties when classifying input patterns with increasingly fine temporal precision .",
    "this disparity between and the other two rules is explained by its unstable behaviour , since it essentially fails to account for the temporal proximity of neighbouring target and actual postsynaptic spikes .",
    "as was predicted in our earlier analysis , this instability gave rise to fluctuating postsynaptic spikes close to their target timings ( see fig .",
    "[ fig5 ] ) .",
    "hence , it is evident that exponentially filtering postsynaptic spikes in order to drive more gradual synaptic weight modifications confers a strong advantage when temporally precise encoding of input patterns is desired .    from the experiment concerning pattern classifications based on multiple output spike timings ,",
    "it was found for each of the learning rules that the performance decreased with the number of target output spikes .",
    "this is not surprising given that the network needed to match every one of its targets with the same level of temporal precision , effectively increasing the synaptic load of the network during learning .",
    "qualitatively , these results are consistent with those found in @xcite for the e - learning rule .",
    "we have studied the conditions under which supervised synaptic plasticity can most effectively be applied to training to learn precise temporal encoding of input patterns . for this purpose ,",
    "we have derived two supervised learning rules , termed and , and analysed the validity of their solutions on several , generic , input - output spike timing association tasks .",
    "we have also tested the proposed rules performance in terms of the maximum number of spatio - temporal input patterns that a trained network can memorise per synapse , with patterns identified based on the precise timing of an output spike emitted by a postsynaptic neuron ; this experiment was designed to reflect the experimental observations of biological neurons which utilise the relative timing of their output spike for stimulus encoding @xcite . in order to benchmark the performance of our proposed rules , we also implemented the previously established e - learning rule . from our simulations , we found approached the high performance level of : relating to its ability to smoothly converge towards stable , desired solutions by account of its exponential filtering of postsynaptic spike trains .",
    "by contrast , consistently returned the lowest performance , which was underpinned by its tendency to result in fluctuations of emitted postsynaptic spikes near their target timings .    essentially , weight changes driven by the and rules depend on a combination of two activity variables : a postsynaptic error term to signal appropriate output responses , and a presynaptic eligibility term to capture the coincidence of input spikes with the output error . and differ , however , with respect to their postsynaptic error term : while relies on the instantaneous difference between target and actual output spike trains , instead relies on the smoothed difference between exponentially filtered target and actual output spike trains .",
    "despite this , both rules share the same presynaptic eligibility term , that is the evoked due to an input spike . in our analysis ,",
    "the was determined as a suitable presynaptic factor , whereas the structurally similar and rules instead rely on an arbitrarily defined presynaptic kernel that is typically related to the neuron s postsynaptic current @xcite .",
    "interestingly , in the authors analysis of the rule an @xmath18-shaped kernel was indicated as providing the best performance during learning , which closely resembles the shape of a curve as used here .    in our analysis of single synapse dynamics ( see results section ) , we predicted the rule to provide convergence towards a stable and desired synaptic weight solution , offering an explanation for its high performance as tested through subsequent simulations in large network sizes .",
    "the key advantage of the rule is its ability to signal not just the timings of desired or erroneous postsynaptic spikes , but also their temporal proximity with other postsynaptic spikes as measured via their convolution with an exponential kernel ; in this way , the rule is able to smoothly align emitted postsynaptic spikes with their respective targets by avoiding unstable synaptic weight changes .",
    "this operation is roughly analogous to one used by the e - learning rule , which includes a distinct mechanism for carefully shifting actual postsynaptic spikes towards their neighbouring targets , making it a highly efficient spike - based neural classifier @xcite .",
    "the and rules differ , however , in terms of their implementation : while can potentially be implemented as an online - based learning method for biological realism , is restricted to offline learning , given that it depends on discrete summations over cost functions that are non - local in time as derived from the measure . comparatively , the rule was predicted to provide imperfect and unstable convergence during learning , which we attributed to its inability to effectively account for neighbouring target and actual postsynaptic spikes .",
    "computer simulations were run to test the performance of the and rules in terms of their temporal encoding precision in large network sizes , including the e - learning rule for comparison purposes .",
    "we found and were consistent with each other performance - wise , and largely outperformed .",
    "it is worth pointing out , however , that is more straightforward to implement than , since it avoids the added complexity of having to establish whether target and actual postsynaptic spikes are independent of each other or not based on the measure @xcite . by comparison , is the simplest rule to implement , but comes at the cost of significantly decreased spike timing precision .",
    "on all these learning tasks neurons were trained to classify input patterns using the precise timings of output spikes ; an alternative and more practical method for classifying patterns might instead take the minimum distance between target and actual output spike trains in order to discriminate between different input classes , which would more effectively counteract misclassifications in the case of input noise @xcite . in this work , however , we adopted a classification method based on the precise timings of output spikes for the sake of consistency with more directly related previous studies @xcite , and to more thoroughly compare the relative performance of each learning rule with respect to the precision of their temporal encoding .      in our approach , we started by taking gradient ascent on an objective function for maximising the likelihood of generating desired output spike trains , based on the statistical method of @xcite ; this method is well suited to our analysis , especially since it has been shown to have a unique global maximum that is obtainable using a standard gradient ascent procedure @xcite .",
    "next , we substituted the stochastic spiking neuron model used during the derivation with a deterministic neuron model , such that output spikes were instead restricted to being generated upon crossing a fixed firing threshold . in this way",
    ", the resulting and rules have a reasonably strong theoretical basis , rely on intrinsic neuronal dynamics , and further still allow for the efficient learning of desired sequences of precisely - timed output spikes . by comparison ,",
    "most previous approaches to formulating supervised learning rules for have relied on heuristic approximations , such as adapting the widrow - hoff rule for use with spiking neuron models @xcite , or mapping from perceptron to spike - based learning @xcite . moreover , although the well known @xcite can more rigorously be reinterpreted as a gradient descent learning procedure @xcite , assumptions are still made regarding the functional dependence of weight changes on the relative timing differences between spikes , for the purposes of mimicking a hebbian - like rule @xcite .    according to the study of @xcite , the upper limit on the number of input - output pattern transformations",
    "a spiking neuron can learn to memorise falls between 0.1 and 0.3 per synapse , based on single target output spikes . in establishing this maximal capacity estimate , the authors of this study applied an idealised learning method , such that the firing times of the trained neuron were enforced at its target timings . from fig .",
    "[ fig9 ] , we determined the and rules to approximately share the same memory capacity , with measured values between 0.15 and 0.2 for required timing precisions larger than ; hence , the capacities afforded by these two rules can be regarded as approaching maximal values . by contrast , the rule only remained competitive with and for relatively large values of the required timing precision , with values of at least @xmath161 .",
    "it is noted that our capacity measurements here do not reflect upper estimates ; in our approach , networks were trained over a maximum of 500 epochs to also test the rapidity of learning , whereas previous studies have trained networks , for instance using e - learning , over up to two orders of magnitude increased duration @xcite .",
    "the authors of @xcite also presented an learning rule that more realistically depends on intrinsic neuronal dynamics , unlike their method .",
    "as discussed previously , the rule is essentially an -like method , in the sense that weight updates depend on an instantaneously communicated error signal .",
    "however , differs by just taking into account the first error during learning in each trial .",
    "the rule has been shown to provide a high memory capacity that is comparable with , as well as having been proven to converge towards a stable solution in finite time . by comparison ,",
    "our results have demonstrated reduced performance for , and in particular for small values of the required timing precision , @xmath141 . despite this",
    ", our has the potential for being implemented as a fully online method in simulations , unlike which must be immediately shut down upon encountering the first error during a trial",
    ". it would be interesting to explore an online implementation of learning for increased biological plausibility , while maintaining its high performance by minimising nonlinear interactions between output error signals .",
    "realistically , this might be realised by introducing a refractory effect in the neuron s error signals @xcite .",
    "it is highlighted that the and rules are capable of learning _",
    "target output spikes ; this is an important feature of any spike - based learning rule , and makes them more biologically relevant considering that precise spike timings represent a more fundamental unit of computation in the nervous system than that of lengthier firing rates @xcite .",
    "multi - spike learning rules are a natural progression from single - spike rules , such as from the original spikeprop algorithm which is restricted to learning single - spike target outputs @xcite , and the tempotron which is only capable of learning to either fire or not - fire an output spike @xcite .      out of the rules studied here , we believe matches most criteria to be considered of biological relevance : first , weight updates depend on pre- and postsynaptic activity variables that are locally available at each synapse .",
    "second , its postsynaptic error term is communicated by a smoothly decaying signal that is based on the difference between filtered target and actual output spikes , which might arise from the concentration of a synaptic neuromodulator influenced by backpropagated action potentials @xcite . finally , it is implementable as an online learning method , which is important when considering how information is most likely processed continuously by the nervous system .    as with most existing learning rules for ,",
    "the proposed rules depend on the presence of a supervisory signal to guide synaptic weight modifications . a possible explanation for supervised learning might come from so termed ` referent activity templates ' , or spike patterns generated by neural circuits existing elsewhere in the brain , which are to be mimicked by circuits of interest during learning @xcite . a detailed model of supervised learning in",
    "has recently been proposed by @xcite , providing a strong mechanistic explanation for how such referent activity templates might be used to drive the learning of desired postsynaptic activity patterns .",
    "specifically , this method has utilised a compartmental model , simulating the somatic and dendritic dynamics of a stochastic spiking neuron , such that the neuron s firing activity is determined by integrating its direct input from somatic synapses with its network input via dendritic synapses . in this way",
    ", the neuron s firing activity can be directly ` nudged ' towards some desired pattern via its somatic input ( or template pattern ) , while plasticity at its dendritic synapses takes care of forming associations of this target activity pattern with input patterns that are simultaneously presented to the network .",
    "the and synaptic plasticity rules here can in principle be implemented based on this compartmental model for increased biological realism .",
    "a more recent study @xcite has also drawn inspiration from such an associative learning paradigm , culminating in a synaptic plasticity rule that works to maintain a neuron s membrane potential below its firing threshold during learning , termed .",
    "essentially , is an unsupervised learning rule , and by itself is used to train a neuron to remain quiescent in response to input activity .",
    "however , if is also combined with strong synaptic input , delivered from an external source , that is briefly injected into a trained neuron at its desired firing timings , then the rule instead functions as a supervised one . similarly to the study by @xcite , demonstrates how the supervised learning of precisely timed spikes in an can arise in a biologically meaningful way .",
    "a final possibility , and one that is gaining increasing interest , is that supervised signalling might actually reflect a form of reinforcement - based learning , but operating on a shorter time - scale .",
    "several , biologically meaningful learning rules have been proposed based on reward - modulated synaptic plasticity @xcite , including a reimplementation of elman backpropagation @xcite , and in our previous work we have successfully demonstrated how reinforcement learning can be applied to learning multiple , and precisely timed , output spikes @xcite .",
    "in this paper , we have addressed the scarcity of existing learning rules for networks of spiking neurons that have a theoretical basis , and which allow for the learning of _ multiple _ and _ precisely - timed _ output spikes . in particular , we have shown our proposed rule , which is based on exponentially filtered output spike trains , to be a highly efficient , spike - based neural classifier .",
    "classifiers based on a temporal code are of interest since they are theoretically more capable than those using a rate - based code when processing information on rapid time - scales .    in our analysis",
    ", we have restricted our attention to relatively small network sizes when testing the performance of the proposed learning rules .",
    "our main intention , though , was to explore their potential for driving accurate synaptic weight modifications , rather than the scaling of their performance with an increasing number of input synapses .",
    "however , it would be of increased biological significance to test the performance of a learning method as applied to a much larger network size : containing on the order of @xmath162 synapses per neuron as is typical in the nervous system .",
    "practically , this could well be achieved via implementation in neuromorphic hardware , such as the massively - parallel computing architecture of spinnaker @xcite . as a starting point",
    ", the simplistic rule could be implemented in spinnaker , representing an achievable , and exciting , aim for future work .",
    "this work was supported by the engineering and physical sciences research council ( epsrc , grant no . ep / j500562/1 ) , the european community s seventh framework programme ( fp7/2007 - 2013 , grant no .",
    "604102 , hbp  the human brain project ) and horizon 2020 ( grant no . 284941 , hbp ) .",
    "[ [ convergence - of - gradient - ascent - procedure - based - on - intrinsic - neuronal - dynamics . ] ] convergence of gradient ascent procedure based on intrinsic neuronal dynamics .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in our approach we wish to consider a learning rule that depends on the intrinsic dynamics of a postsynaptic neuron , rather than artificially clamping its firing activity to its target response as in eq .",
    "( [ eq : w_update_stoch ] ) .",
    "this formula is restated below : @xmath163 \\sum_{t_j^f \\in x_j } \\epsilon ( t - t_j^f)\\ , \\mathrm{d}t \\;,\\ ] ] where we have substituted @xmath60 with @xmath61 , such that the stochastic intensity of the postsynaptic neuron depends on its actual sequence of emitted output spikes @xmath62 rather than its target output @xmath63 .",
    "we show here that eq .",
    "( [ eq : w_update_intrinsic_supp ] ) yields similar weight updates to eq .",
    "( [ eq : w_update_stoch ] ) if the actual postsynaptic spike train is already close to its target .    to demonstrate this , we start by considering the absolute difference between weight updates applied using eqs .",
    "( [ eq : w_update_intrinsic_supp ] ) and ( [ eq : w_update_stoch ] ) : @xmath164 \\sum_{t_j^f \\in x_j } \\epsilon ( t - t_j^f)\\ , \\mathrm{d}t \\bigg| \\;,\\ ] ] leading to the following inequality for an absolute integrand : latexmath:[\\[\\label{eq : converge_2 }    now , for simplicity , if we assume one of the presynaptic neurons , denoted @xmath11 , contributes a single input spike at time @xmath36 , and a single target and actual output spike occur at times @xmath166 and @xmath90 for a postsynaptic neuron @xmath0 , respectively , then the above equation simplifies to @xmath167 denotes a dependence of the postsynaptic neuron s stochastic intensity at time @xmath2 on the entire set of presynaptic spikes @xmath48 , including from neuron @xmath11 , and its actual output firing time @xmath90 .    according to the definition of the kernel in eq .",
    "( [ eq : psp_kernel ] ) , the kernel assumes a maximum value , denoted @xmath103 .",
    "hence , an upper bound of eq .",
    "( [ eq : converge_3 ] ) can be given by latexmath:[\\[\\label{eq : converge_4 }    we emphasise here that although we consider just a single input spike , the above would equally be valid for multiple input spikes by simply multiplying the upper bound on the , @xmath103 , by the number of spikes contributed from neuron @xmath11 .    as defined by eq .",
    "( [ eq : exp_rate ] ) the stochastic intensity has an exponential dependence on the postsynaptic neuron s membrane potential , and the only difference between @xmath39 and @xmath169 arises from their reset term @xmath15 , hence eq .",
    "( [ eq : converge_4 ] ) can be written as latexmath:[\\[\\label{eq : converge_5 }    for a given finite set of inputs ( all predecessor neurons of @xmath0 ) on the finite interval @xmath171 $ ] , @xmath1 is smaller than a constant , irrespective of where the target and actual output spikes fall ( consider the maximum of @xmath1 in eq .",
    "for @xmath62 the empty set ) . therefore , @xmath39 is also bounded by a constant @xmath172 , and hence latexmath:[\\[\\label{eq : converge_6 }    we now show in the following that the integral on the right - hand side of the above equation , and by extension the difference in the weight change , becomes small if @xmath90 and @xmath166 are close together .",
    "if we assume @xmath174 ( an analogous argument applies also for @xmath175 ) , then the difference between the reset kernels can be expressed as @xmath176 where @xmath177 is the indicator function on the interval @xmath178 $ ] , such that @xmath179 if @xmath180 , and @xmath181 otherwise .",
    "furthermore , the following inequality applies : @xmath182 tends to zero point - wise in @xmath2 for @xmath183 and is bounded by @xmath184 . by continuity , the integrand in eq .",
    ", @xmath185 , also goes to zero pointwise and is bounded .",
    "hence by dominated convergence , the integral in , tends to zero for @xmath186 . in other words , it is continuous in @xmath187 , and we can find for each @xmath188 a @xmath187 such that weight changes based on @xmath90 do not differ by more than @xmath188 from those based on @xmath166 if target and actual output spikes are closer than @xmath187 .",
    "the proof above shows that the intrinsic weight update rule of eq .",
    "yields comparable weight changes to the rule in eq .   if the actual output spike is already close to its target .",
    "however , in the form given above it is not constructive , i.e. does not give us an explicit estimate of @xmath187 in terms of @xmath188 and other parameters .",
    "as for all learning rules , their practical feasibility has to be demonstrated in simulations ."
  ],
  "abstract_text": [
    "<S> precise spike timing as a means to encode information in neural networks is biologically supported , and is advantageous over frequency - based codes by processing input features on a much shorter time - scale . for these reasons </S>",
    "<S> , much recent attention has been focused on the development of supervised learning rules for spiking neural networks that utilise a temporal coding scheme . however , despite significant progress in this area , there still lack rules that have a theoretical basis , and yet can be considered biologically relevant . here </S>",
    "<S> we examine the general conditions under which synaptic plasticity most effectively takes place to support the supervised learning of a precise temporal code . as part of our analysis </S>",
    "<S> we examine two spike - based learning methods : one of which relies on an instantaneous error signal to modify synaptic weights in a network ( inst rule ) , and the other one relying on a filtered error signal for smoother synaptic weight modifications ( filt rule ) . </S>",
    "<S> we test the accuracy of the solutions provided by each rule with respect to their temporal encoding precision , and then measure the maximum number of input patterns they can learn to memorise using the precise timings of individual spikes as an indication of their storage capacity . </S>",
    "<S> our results demonstrate the high performance of the filt rule in most cases , underpinned by the rule s error - filtering mechanism , which is predicted to provide smooth convergence towards a desired solution during learning . </S>",
    "<S> we also find the filt rule to be most efficient at performing input pattern memorisations , and most noticeably when patterns are identified using spikes with sub - millisecond temporal precision . in comparison with existing work , </S>",
    "<S> we determine the performance of the filt rule to be consistent with that of the highly efficient e - learning chronotron rule , but with the distinct advantage that our filt rule is also implementable as an online method for increased biological realism . </S>"
  ]
}