{
  "article_text": [
    "numerical simulations in cosmology have a long history and numerous important applications .",
    "different aspects of the simulations including history of the subject were reviewed recently by @xcite ; see also @xcite .",
    "more detailed aspects of simulations were discussed by @xcite , @xcite , and @xcite .",
    "numerical simulations play a very significant role in cosmology .",
    "it all started in 60s @xcite and 70s @xcite with simple n - body problems solved using n - body codes with few hundred particles .",
    "later the particle - particle code ( direct summation of all two - body forces ) was polished and brought to the state - of - art @xcite .",
    "already those early efforts brought some very valuable fruits .",
    "@xcite studied collapse of a cloud of particles as a model of cluster formation .",
    "the model had 300 points initially distributed within a sphere with no initial velocities .",
    "after the collapse and virialization the system looked like a cluster of galaxies .",
    "those early simulations of cluster formation , though producing cluster - like objects , signaled the first problem  simple model of initially isolated cloud ( top - hat model ) results in the density profile of the cluster which is way too steep ( power - law slope -4 ) as compared with real clusters ( slope -3 ) .",
    "the problem was addressed by @xcite , who introduced a notion of secondary infall in an effort to solve the problem .",
    "another keystone work of those times is the paper by @xcite , who studied collapse of 700 particles with different masses .",
    "it was shown that if one distributes the mass of a cluster to individual galaxies , two - body scattering will result in mass segregation not compatible with observed clusters .",
    "this was another manifestation of the dark matter in clusters .",
    "this time it was shown that inside a cluster the dark matter can not reside inside individual galaxies .",
    "survival of substructures in galaxy clusters was another problem addressed in the paper .",
    "it was found that lumps of dark matter , which in real life may represent galaxies , do not survive in dense environment of galaxy clusters .",
    "@xcite argued that the real galaxies survive inside clusters because of energy dissipation by the baryonic component .",
    "that point of view was accepted for almost 20 years .",
    "only recently it was shown the energy dissipation probably does not play a dominant role in survival of galaxies and the dark matter halos are not destroyed by tidal stripping and galaxy - galaxy collisions inside clusters @xcite .",
    "the reason why early simulations came to a wrong result was pure numerical : they did not have enough resolution . but 20 years ago it was physically impossible to make a simulation with sufficient resolution .",
    "even if at that time we had present - day codes , it would have taken about 600 years to make one run .",
    "generation of initial condition with given amplitude and spectrum of fluctuations was a problem for some time .",
    "the only correctly simulated spectrum was the flat spectrum which was generated by randomly distributing particles . in order to generate",
    "fluctuations with power spectrum , say @xmath1 , @xcite placed particles along rods .",
    "formally , it generates the spectrum , but the distribution has nothing to do with cosmological fluctuations . @xcite and @xcite were the first to use the @xcite approximation to set initial conditions .",
    "since then this method is used to generate initial conditions for arbitrary initial spectrum of perturbations .",
    "starting mid 80s the field of numerical simulations is blooming : new numerical techniques are invented , old ones are perfected .",
    "the number of publications based on numerical modeling skyrocketed .",
    "to large extend , this have changed our way of doing cosmology . instead of questionable assumptions and waving - hands arguments , we have tools of testing our hypothesis and models . as an example , i mention two analytical approximations which were validated by numerical simulations .",
    "the importance of both approximations is difficult to overestimate .",
    "the first is the zeldovich approximation , which paved the way for understanding the large - scale structure of the galaxy distribution .",
    "the second is the @xcite approximation , which gives the number of objects formed at different scales at different epochs .",
    "both approximations can not be formally proved .",
    "the zeldovich approximation formally is not applicable for hierarchical clustering .",
    "it must start with smooth perturbations ( truncated spectrum ) .",
    "nevertheless , numerical simulations have shown that even for the hierarchical clustering the approximation can be used with appropriate filtering of initial spectrum ( see * ? ? ? * and references therein ) .",
    "the press - schechter approximation is also difficult to justify without numerical simulations .",
    "it operates with the initial spectrum and the linear theory , but then ( a very long jump ) it predicts the number of objects at very nonlinear stage . because it is not based on any realistic theory of nonlinear evolution , it was an ingenious , but a wild guess .",
    "if anything , the approximation is based on a simple spherical top - hat model . but",
    "simulations show that objects do not form in this way  they are formed in a complicated fashion through multiple mergers and accretion along filaments .",
    "still this a very simple and a very useful prescription gives quite accurate predictions .",
    "this lecture is organized in the following way .",
    "section 2 gives the equations which we solve to follow the evolution of initially small fluctuations .",
    "initial conditions are discussed in section 3 .",
    "a brief discussion of different methods is given in section 4 .",
    "effects of the resolution and some other technical details are also discussed in section 5 .",
    "identification of halos ( `` galaxies '' ) is discussed in section 6 .",
    "usually the problem of the formation and dynamics of cosmological objects is formulated as @xmath0-body problem : for @xmath0 point - like objects with given initial positions and velocities find their positions and velocities at any later moment .",
    "it should be remembered that this just a short - cut in our formulation  to make things simple .",
    "while it still mathematically correct in many cases , it does not give a correct explanation to what we do .",
    "if we are literally to take this approach , we should follow the motion of zillions of axions , baryons , neutrinos , and whatever else our universe is made of .",
    "so , what it has to do with the motion of those few millions of particles in our simulations ?",
    "the correct approach is to start with the vlasov equation coupled with the poisson equation and with appropriate initial and boundary conditions . if we neglect the baryonic component , which of course is very interesting , but would complicate our situation even more , the system is described by distribution functions @xmath2 which should include all different clustered components @xmath3 . for a simple cdm model we have only one component ( axions or whatever it is ) . for more complicated cold plus hot dark matter ( chdm ) with few different types of neutrinos the system includes one df for the cold component and one df for each type of neutrino @xcite . in the comoving",
    "coordinates * x * the equations for the evolution of @xmath4 are :    @xmath5    here @xmath6 is the expansion parameter , @xmath7 is the momentum , @xmath8 is the contribution of the clustered dark matter to the mean density of the universe , @xmath9 is the mass of a particle of @xmath10th component of the dark matter .",
    "the solution of the vlasov equation can be written in terms of equations for characteristics , which _",
    "look _ like equations of particle motion : @xmath11    in these equations @xmath12 is the critical density at @xmath13 ; @xmath14 , and @xmath15 , are the density of the matter and of the cosmological constant in units of the critical density at @xmath13 .    the distribution function @xmath4 is constant along each characteristic .",
    "this property should be preserved by numerical simulations .",
    "the complete set of characteristics coming through every point in the phase space is equivalent to the vlasov equation .",
    "we can not have the complete ( infinite ) set , but we can follow the evolution of the system ( with some accuracy ) , if we select a representative sample of characteristics .",
    "one way of doing this would be to split initial phase space into small domains , take only one characteristic as representative for each volume element , and to follow the evolution of the system of the `` particles '' in a self - consistent way . in models with one `` cold '' component of clustering dark matter (",
    "like cdm or @xmath16cdm ) the initial velocity is a unique function of coordinates ( only `` zeldovich '' part is present , no thermal velocities ) .",
    "this means that we need to split only coordinate space , not velocity space . for complicated models with significant thermal component",
    ", the distribution in full phase space should be taken into account .",
    "depending on what we are interested in , we might split initial space into equal - size boxes ( typical setup for pm or p@xmath17 m simulations ) or we could divide some area of interest ( say , where a cluster will form ) into smaller boxes , and use much bigger boxes outside the area ( to mimic gravitational forces of the outside material ) . in any case , the mass assigned to a `` particle '' is equal to the mass of the domain it represents .",
    "now we can think of the `` particle '' either as a small box , which moves with the flow , but does not change its original shape , or as a point - like particle .",
    "both presentations are used in simulations .",
    "none is superior to another .",
    "there are different forms of final equations .",
    "mathematically they are all equivalent , but computationally there are very significant differences .",
    "there are considerations , which may affect the choice of particular form of the equations .",
    "any numerical method gives more accurate results for a variable , which changes slowly with time .",
    "for example , for the gravitational potential we can choose either @xmath18 or @xmath19 . at early stages of evolution perturbations still grow almost linearly . in this case",
    "we expect that @xmath20 , @xmath21 , and @xmath22 .",
    "thus , @xmath18 can be a better choice because it does not change .",
    "that is especially helpful , if code uses gravitational potential from previous moment of time as initial `` guess '' for current moment , as it happens in the case of the art code . in any case , it is better to have a variable , which does not change much . for equations of motion we can choose , for example , either first equations in eqs.([eq : eqmotiona]  [ eq : eqmotionb ] ) or the second equations .",
    "if we choose `` momentum '' @xmath23 as effective velocity and take the expansion parameter @xmath24 as time variable , then for the linear growth we expect that the change of coordinates per each step is constant : @xmath25 .",
    "numerical integration schemes should not have problem with this type of growth .",
    "for the @xmath26 and @xmath27 variable , the rate of change is more complicated : @xmath28 , which may produce some errors at small expansion parameters .",
    "the choice of variables may affect the accuracy of the solution even at very nonlinear stage of evolution as was argued by @xcite .",
    "the zeldovich approximation is commonly used to set initial conditions .",
    "the approximation is valid in mildly nonlinear regime and is much superior to the linear approximation .",
    "we slightly rewrite the original version of the approximation to incorporate cases ( like chdm ) when the growth rates @xmath29 depend on the wavelength of the perturbation @xmath30 . in the zeldovich approximation the comoving and the lagrangian coordinates",
    "are related in the following way :    @xmath31    where the displacement vector @xmath32 is related to the velocity potential @xmath33 and the power spectrum of fluctuations",
    "@xmath34 :    @xmath35    where @xmath24 and @xmath36 are gaussian random numbers with the mean zero and dispersion @xmath37 :    @xmath38    the parameter @xmath39 , together with the power spectrum @xmath40 , define the normalization of the fluctuations .    in order to set the initial conditions , we choose the size of the computational box @xmath41 and the number of particles @xmath42 .",
    "the phase space is divided into small equal cubes of size @xmath43 .",
    "each cube is centered on a harmonic @xmath44 , where @xmath45 are integer numbers with limits from zero to @xmath46 .",
    "we make a realization of the spectrum of perturbations @xmath47 and @xmath48 , and find displacement and momenta of particles with @xmath49 using eq.([eq : eqzeldtwo ] ) . here",
    "@xmath50 .      there are approximations of the power spectrum @xmath40 for a wide range of cosmological models .",
    "publicly available cosmics code ( bertschinger 1996 ) gives accurate approximations for the power spectrum .",
    "here we follow @xcite who give the following fitting formula :    @xmath51    the coefficients @xmath52 are presented by @xcite for a variety of models .",
    "the comparison of some of the power spectra with the results from cosmics ( bertschinger 1996 ) indicate that the errors of the fits are smaller than 5% .",
    "table  [ tab : spectrum ] gives parameters of the fits for some popular models .",
    "rrrrrrrr 0.3 & 0.035 & 0.60 & -1.7550e+00 & 6.0379e+01 & 2.2603e+02 & 5.6423e+02 & 9.3801e-01 + 0.3 & 0.030 & 0.65 & -1.6481e+00 & 5.3669e+01 & 1.6171e+02 & 4.1616e+02 & 9.3493e-01 + 0.3 & 0.026 & 0.70 & -1.5598e+00 & 4.7986e+01 & 1.1777e+02 & 3.2192e+02 & 9.3030e-01 + 1.0 & 0.050 & 0.50 & -1.1420e+00 & 2.9507e+01 & 4.1674e+01 & 1.1704e+02 & 9.2110e-01 + 1.0 & 0.100 & 0.50 & -1.3275e+00 & 3.0152e+01 & 5.5515e+01 & 1.2193e+02 & 9.2847e-01 + [ tab : spectrum ]    the power spectrum of cosmological models is often approximated using a fitting formula given by bardeen ( 1986 , bbks ) :    @xmath53^{-1/4 } , \\label{eq : eqbbks}\\ ] ]    where @xmath54 .",
    "unfortunately , the accuracy of this approximation is not great and it should not be used for accurate simulations .",
    "we find that the following approximation , which is a combination of a slightly modified bbks fit and the hu & sugiyama ( 1996 ) scaling with the amount of baryons , provides errors in the power spectrum smaller than 5% for the range of wavenumbers @xmath55 and for @xmath56 :    @xmath57^{-1/4},\\nonumber\\\\   q     & = & { k(t_{\\rm cmb}/2.7k)^2\\over          \\omega_0h^2\\alpha^{1/2}(1-\\omega_b/\\omega_0)^{0.60}},\\qquad   \\alpha = a_1^{-\\omega_b/\\omega_0}a_2^{-(\\omega_b/\\omega_0)^3}\\nonumber\\\\   a_1 & = & ( 46.9\\omega_0h^2)^{0.670}[1+(32.1\\omega_0h^2)^{-0.532}],\\quad   a_2 = ( 12\\omega_0h^2)^{0.424}[1+(45\\omega_0h^2)^{-0.582 } ] \\label{eq : equgly}\\end{aligned}\\ ] ]      in many cases we would like to set initial conditions in such a way that inside some specific region(s ) there are more particles and the spectrum is better resolved .",
    "we need this when we want to have high resolution for a halo , but we also need the environment of the halo .",
    "this is done in a two - step process .",
    "first , we run a low resolution simulation which has a sufficiently large volume to include the effects of the environment .",
    "for this run all the particles have the same mass .",
    "a halo is picked for rerunning with high resolution .",
    "second , using particles of the halo , we identify region in the lagrangian ( initial ) space , where the resolution should be increased .",
    "we add high - frequency harmonics , which are not present in the low resolution run .",
    "we then add contributions of all the harmonics and get initial displacements and momenta ( eq .  [ eq : eqzeldtwo ] ) .",
    "let s be more specific .",
    "in order to add the new harmonics , we must specify ( 1 ) how we divide the phase space and place the harmonics , and ( 2 ) how we sum the contributions of the harmonics .",
    "the simplest way is to divide the phase space into many small boxes of size @xmath43 , where @xmath41 is the box size .",
    "this is the same devision , which we use to set the low resolution run .",
    "but now we extend it to very high frequencies up to @xmath58 , where @xmath0 is the new effective number of particles .",
    "for example , we used @xmath59 for the low resolution run . for high resolution run we may choose @xmath60 . simply replace the value and run the code again",
    "of course , we really can not do it because it would generate too many particles . instead , in some regions , where the resolution should not be high , we combine particles together ( by taking average coordinates and average velocities ) and replace many small - mass particles with fewer larger ones .",
    "left panel in figure  [ fig : gridexample ] gives an example of mass refinement .",
    "note that we try to avoid too large jumps in the mass resolution by creating layers of particles of increasing mass .",
    "this approach is correct and relatively simple .",
    "it may seem that it takes too much cpu to get the initial conditions . in practice , cpu time is not much of an issue because initial conditions are generated only once and it takes only few cpu hours even for @xmath61 mesh . for most of applications @xmath61 particles",
    "is more then enough .",
    "the problem arises when we want to have more then @xmath61 particles .",
    "we simply do not have enough computer memory to store the information for all the harmonics . in this case",
    "we must decrease the resolution in the phase space .",
    "it is a bit easier to understand the procedure , if we consider phase space diagrams like one presented in figure  [ fig : massexample ] .",
    "the low resolution run in this case was done for @xmath62 particles with harmonics up to @xmath63 ( small points ) . for the high resolution run we choose a region of size 1/8 of the original large box . inside the small box",
    "we place another box , which is twice smaller .",
    "thus , we will have three levels of mass refinement . for each level",
    "we have corresponding size of the phase space block .",
    "the size is defined by the size of real space box and is equal to @xmath64 , @xmath65 .",
    "harmonics from different refinements should not overlap : if a region in phase space is represented on lower level of resolution , it should not appear in the the higher resolution level .",
    "this is why rows of the highest resolution harmonics ( circles ) with @xmath66 and @xmath67 are absent in the figure  [ fig : massexample ] : they have been already covered by lower resolution blocks marked by stars .",
    "figure  [ fig : massexample ] clearly illustrate that matching harmonics is a complicated process : we failed to do the match because there are partially overlapping blocks and there are gaps .",
    "we can get much better results , if we assume different ratios of the sizes of the boxes .",
    "for example , if instead of box ratios @xmath68 , we chose ratios @xmath69 , the coverage of the phase space is almost perfect as shown in figure :  [ fig : gridgood ] .",
    "there are many different numerical techniques to follow the evolution of a system of many particles . for earlier reviews see @xcite , and @xcite .",
    "most of the methods for cosmological applications take some ideas from three techniques : particle mesh ( pm ) code , direct summation or particle - particle code , and the tree code . for example , the adaptive particle - particle / particle - mesh ( ap@xmath17 m ) code @xcite is a combination of the pm code and the particle - particle code .",
    "the adaptive - refinement - tree code ( art ) @xcite is an extension of the pm code with the organization of meshes in the form of a tree .",
    "all methods have their advantages and disadvantages .",
    "* pm code*. it uses a mesh to produce density and potential . as the result , its resolution is limited by the size of the mesh .",
    "there are two advantages of the method : i ) it is fast ( the smallest number of operations per particle per time step of all the other methods ) , ii ) it typically uses very large number of particles .",
    "the later can be crucial for some applications .",
    "there are few modifications of the code .",
    "`` plain - vanilla '' pm was described by @xcite .",
    "it includes cloud - in - cell density assignment and 7-point discrete analog of the laplacian operator .",
    "higher order approximations improve the accuracy on large distances , but degrades the resolution ( e.g. , * ? ? ?",
    "the pm code is available @xcite    * p@xmath17 m code * is described in detail in @xcite and @xcite .",
    "it has two parts : pm part , which takes care of large - scale forces , and pp part , which adds small - scale particle - particle contribution .",
    "because of strong clustering at late stages of evolution , pp part becomes prohibitively expensive once large objects start to form in large numbers .",
    "significant speed is achieved in modified version of the code , which introduces sub - grids ( next levels of pm ) in areas with high density @xcite . with modification",
    "the code is as fast as the tree code even for heavily clustered configurations .",
    "the code express the inter - particle force as a sum of a short range force ( computed by direct particle - particle pair force summation ) and the smoothly varying part ( approximated by the particle - mesh force calculation ) .",
    "one of the major problems for these codes is the correct splitting of the force into a short - range and a long - range part .",
    "the grid method ( pm ) is only able to produce reliable inter particle forces down to a minimum of at least two grid cells . for smaller separations",
    "the force can no longer be represented on the grid and therefore one must introduce a cut - off radius @xmath70 ( larger than two grid cells ) , where for @xmath71 the force should smoothly go to zero . the parameter @xmath70 defines the chaining - mesh and for distances smaller than this cutoff radius @xmath70 a contribution from the direct particle - particle ( pp ) summation needs to be added to the total force acting on each particle .",
    "again this pp force should smoothly go to zero for very small distances in order to avoid unphysical particle - particle scattering .",
    "this cutoff of the pp force determines the overall force resolution of a p@xmath17 m code .",
    "the most widely used version of this algorithm is currently the adaptive p@xmath17 m ( ap@xmath17 m ) code of couchman ( 1991 ) , which is available for public .. the smoothing of the force in this code is connected to a @xmath72 sphere , as described in hockney & eastwood ( 1981 ) .",
    "* tree code * is the most flexible code in the sense of the choice of boundary conditions @xcite .",
    "it is also more expensive than pm : it takes 10 - 50 times more operations . @xcite and @xcite extended the code for the periodical boundary conditions , which is important for simulating large - scale fluctuations .",
    "some variants of the tree are publicly available .",
    "there are variants of the code modified for massively parallel computers .",
    "there code variants with variable time stepping , which is vital for extremely high resolution simulations .",
    "* art code*. multigrid methods were introduced long ago , but only recently they started to show a potential to produce real results .",
    "it worth of paying attention if a `` multigrid '' code is really a fully adaptive multigrid code .",
    "an example of this type of the codes is the adaptive refinement tree code ( art ; kravtsov et al .",
    "1997 ) , which reaches high force resolution by refining all high - density regions with an automated refinement algorithm .",
    "the refinements are recursive : the refined regions can also be refined , each subsequent refinement having half of the previous level s cell size .",
    "this creates an hierarchy of refinement meshes of different resolutions covering regions of interest .",
    "the refinement is done cell - by - cell ( individual cells can be refined or de - refined ) and meshes are not constrained to have a rectangular ( or any other ) shape .",
    "this allows the code to refine the required regions in an efficient manner .",
    "the criterion for refinement is the _ local overdensity _ of particles the code refines an individual cell only if the density of particles ( smoothed with the cloud - in - cell scheme ; hockney & eastwood 1981 ) is higher than @xmath73 particles , with typical values @xmath74 .",
    "the poisson equation on the hierarchy of meshes is solved first on the base grid using fft technique and then on the subsequent refinement levels . on each refinement level",
    "the code obtains the potential by solving the dirichlet boundary problem with boundary conditions provided by the already existing solution at the previous level or from the previous moment of time .",
    "there is no particle - particle summation in the art code and the actual force resolution is equal to @xmath75 cells of the finest refinement mesh covering a particular region .",
    "figure  [ fig : artrefinement ] ( courtesy of a. kravtsov ) gives an example of mesh refinement for hydro - dynamical version of the art code .",
    "the code produced this refinement mesh for spherical strong explosion ( sedov solution ) .",
    "the refinement of the time integration mimics spatial refinement and the time step for each subsequent refinement level is two times smaller than the step on the previous level .",
    "note , however , that particles on the same refinement level move with the same step .",
    "when a particle moves from one level to another , the time step changes and its position and velocity are interpolated to appropriate time moments .",
    "this interpolation is first - order accurate in time , whereas the rest of the integration is done with the second - order accurate time centered leap - frog scheme .",
    "all equations are integrated with the expansion factor @xmath24 as a time variable and the global time step hierarchy is thus set by the step @xmath76 at the zeroth level ( uniform base grid ) .",
    "the step on level @xmath41 is then @xmath77 .",
    "what code is the best ? which one to choose ?",
    "there is no unique answer ",
    "everything depends on the problem , which we are addressing . for example , if we are interested in explanation of the large - scale structure ( filaments , voids , zeldovich approximation , and so on ) , pm code with 256@xmath17 mesh is sufficient .",
    "it takes only one night to make a simulation on a ( good ) workstation .",
    "there is a very long list of problems like that .",
    "but if you intent to look for the structure of individual galaxies in the large - scale environment , you must have a code with much better resolution with variable time stepping , and with multiple masses . in this case",
    "the tree or art codes are the choices .",
    "as the resolution of simulations improves and the range of their applications broaden , it becomes increasingly important to understand the limits of the simulations .",
    "@xcite made detailed comparison of realistic simulations done with three codes : art , ap@xmath17 m , and pm .",
    "here we present some of their results and main conclusions .",
    "the simulations were done for the standard cdm model with the dimensionless hubble constant @xmath78 and @xmath79 .",
    "the simulation box of @xmath80 had @xmath81 equal - mass particles , which gives the mass resolution ( mass per particle ) of @xmath82 .",
    "because of the low resolution of the pm runs , we show results only for the other two codes . for the art code",
    "the force resolution is practically fixed by the number of particles .",
    "the only free parameter is the number of steps on the lowest ( zero ) level of resolution . in the case of the ap@xmath17 m , besides the number of steps , one can also request the force resolution . parameters of two runs with the art code and five simulations with the ap@xmath17 m are given in table  [ tab : param ] .",
    ".parameters of the numerical simulations . [ cols=\"<,^,^,^,^ \" , ]     figure  [ fig : dmxi ] shows the correlation function for the dark matter down to the scale of @xmath83 , which is close to the force resolution of all our high - resolution simulations . the correlation function in runs ap@xmath17m@xmath84 and art@xmath85 are similar to those of ap@xmath17m@xmath86 and art@xmath84 respectively and are not shown for clarity .",
    "we can see that the ap@xmath17m@xmath86 and the art@xmath84 runs agree to @xmath87 over the whole range of scales .",
    "the correlation amplitudes of runs ap@xmath17m@xmath88 , however , are systematically lower at @xmath89 ( i.e. , the scale corresponding to @xmath90 resolutions ) , with the ap@xmath17m@xmath91 run exhibiting the lowest amplitude .",
    "the fact that the ap@xmath17m@xmath85 correlation amplitude deviates less than that of the ap@xmath17m@xmath91 run , indicates that the effect is very sensitive to the force resolution .",
    "note that the ap@xmath17m@xmath91 run has formally the best force resolution .",
    "thus , one would naively expect that it should gives the largest correlation function . at scales",
    "@xmath92 the deviations of the ap@xmath17m@xmath91 from the art@xmath84 or the ap@xmath17m@xmath86 runs are @xmath93 .",
    "we attribute these deviations to the numerical effects : high force resolution in ap@xmath17m@xmath91 was not adequately supported by the time integration . in other words ,",
    "the ap@xmath17m@xmath91 had too few time - steps .",
    "note that it had a quite large number of steps ( 6000 ) , not much smaller than the ap@xmath17m@xmath86 ( 8000 ) .",
    "but for its force resolution , it should have many more steps .",
    "the lack of the number of steps was devastating .",
    "figure  [ fig : haloprofile ] presents the density profiles of four of the most massive halos in our simulations .",
    "we have not shown the profile of the most massive halo because it appears to have undergone a recent major merger and is not very relaxed . in this figure , we present only profiles of halos in the high - resolution runs . not surprisingly , the inner density of the pm halos is much smaller than in the high - resolution runs and their profiles deviate strongly from the profiles of high - resolution halos at the scales shown in figure  [ fig : haloprofile ] .",
    "a glance at figure  [ fig : haloprofile ] shows that all profiles agree well at @xmath94 .",
    "this scales is about eight times smaller than the mean inter - particle separation . thus , despite the very different resolutions , time steps , and numerical techniques used for the simulations , the convergence is observed at a scale much lower than the mean inter - particle separation , argued by splinter et al .",
    "( 1998 ) to be the smallest trustworthy scale .",
    "nevertheless , there are systematic differences between the runs .",
    "the profiles in two art runs are identical within the errors indicating convergence ( we have run an additional simulation with time steps twice smaller than those in the art@xmath84 finding no difference in the density profiles ) . among the ap@xmath17",
    "m runs , the profiles of the ap@xmath17m@xmath84 and ap@xmath17m@xmath86 are closer to the density profiles of the art halos than the rest . the ap@xmath17m@xmath85 , ap@xmath17m@xmath91 , and ap@xmath17m@xmath95 , despite the higher force resolution , exhibit lower densities in the halo cores , the ap@xmath17m@xmath91 and ap@xmath17m@xmath95 runs being the most deviant .    these results can be interpreted , if we examine the trend of the central density as a function of the ratio of the number of time steps to the dynamic range of the simulations ( see table  [ tab : param ] ) .",
    "the ratio is smaller when either the number of steps is smaller or the force resolution is higher .",
    "the agreement in density profiles is observed when this ratio is @xmath96 .",
    "this suggests that for a fixed number of time steps , there should be a limit on the force resolution .",
    "conversely , for a given force resolution , there is a lower limit on the required number of time steps .",
    "the exact requirements would probably depend on the code type and the integration scheme .",
    "for the ap@xmath17 m code our results suggest that the ratio of the number of time steps to the dynamic range should be no less than one .",
    "it is interesting that the deviations in the density profiles are similar to and are observed at the same scales as the deviations in the dm correlation function ( fig .",
    "[ fig : dmxi ] ) suggesting that the correlation function is sensitive to the central density distribution of dark matter halos .",
    "finding halos in dense environments is a challenge . some of the problems that any halo finding algorithm faces are not numerical . they exist in the real universe .",
    "we select a few typical difficult situations .",
    "\\1 . _ a large galaxy with a small satellite . _",
    "examples : lmc and the milky way or the m51 system . assuming that the satellite is bound ,",
    "do we have to include the mass of the satellite in the mass of the large galaxy ?",
    "if we do , then we count the mass of the satellite twice : once when we find the satellite and then when we find the large galaxy .",
    "this does not seem reasonable .",
    "if we do not include the satellite , then the mass of the large galaxy is underestimated .",
    "for example , the binding energy of a particle at the distance of the satellite will be wrong .",
    "the problem arises when we try to assign particles to different halos in an effort to find masses of halos .",
    "this is very difficult to do for particles moving between halos .",
    "even if a particle at some moment has negative energy relative to one of the halos , it is not guaranteed that it belongs to the halo . the gravitational potential changes with time , and the particle may end up falling onto another halo .",
    "this is not just a precaution .",
    "this actually was found very often in real halos when we compared contents of halos at different redshifts .",
    "interacting halos exchange mass and lose mass .",
    "we try to avoid the situation : instead of assigning mass to halos , we find the maximum of the `` rotational velocity '' , @xmath97 , which is observationally a more meaningful quantity .",
    "\\2 . _ a satellite of a large galaxy . _",
    "the previous situation is now viewed from a different angle .",
    "how can we estimate the mass or the rotational velocity of the satellite ?",
    "the formal virial radius of the satellite is large : the big galaxy is within the radius .",
    "the rotational velocity may rise all the way to the center of the large galaxy . in order to find the outer radius of the satellite , we analyze the density profile . at small distances from the center of the satellite the density steeply declines , but then it flattens out and may even increase .",
    "this means that we reached the outer border of the satellite .",
    "we use the radius at which the density starts to flatten out as the first approximation for the radius of the halo .",
    "this approximation can be improved by removing unbound particles and checking the steepness of the density profile in the outer part .",
    "_ tidal stripping .",
    "_ peripheral parts of galaxies , responsible for extended flat rotation curves outside of clusters , are very likely tidally stripped and lost when the galaxies fall into a cluster .",
    "the same happens with halos : a large fraction of halo mass may be lost due to stripping in dense cluster environments .",
    "thus , if an algorithm finds that 90% of mass of a halo identified at early epoch is lost , it does not mean that the halo was destroyed .",
    "this is not a numerical effect and is not due to `` lack of physics '' .",
    "this is a normal situation .",
    "what is left of the halo , given that it still has a large enough mass and radius , is a `` galaxy '' .",
    "* friends - of - friends ( fof ) * algorithm was used a lot and still has its adepts .",
    "if we imagine that each particle is surrounded by a sphere of radius @xmath98 , then every connected group of particles is identified as a halo . here",
    "@xmath99 is the mean distance between particles , and @xmath36 is called _ linking parameter _ , which typically is 0.2 .",
    "dependence of groups on @xmath36 is extremely strong .",
    "the method stems from an old idea to use percolation theory to discriminate between cosmological models .",
    "because of that , fof is also called percolation method , which is wrong because the percolation is about groups spanning the whole box , not collapsed and compact objects .",
    "fof was criticized for failing to find separate groups in cases when those groups were obviously present @xcite .",
    "the problem originates from the tendency of fof to `` percolate '' through bridges connecting interacting galaxies or galaxies in high density backgrounds .",
    "* denmax * tried to overcome the problems of fof by dealing with density maxima @xcite .",
    "it finds maxima of density and then tries to identify particles , which belong to each maximum ( halo ) .",
    "the procedure is quite complicated .",
    "first , density field is constructed .",
    "second , the density ( with negative sign ) is treated as potential in which particles start to move as in a viscous fluid .",
    "eventually , particles sink at bottoms of the potential ( which are also maxima density ) .",
    "third , only particles with negative energy ( relative to their group ) are retained .",
    "just as in the case of fof , we can easily imagine situations when ( this time ) denmax should fail .",
    "for example , two colliding galaxies in a cluster of galaxies . because of large relative velocity they should just pass each other . in the moment of collision denmax ceases to `` see '' both galaxies because all particle have positive energies .",
    "that is probably a quite unlikely situation .",
    "the method is definitely one of the best at present .",
    "the only problem is that it seems to be too complicated for present state of simulations .",
    "denmax has two siblings  skid ( stadel et al . ) and bdm @xcite  which are frequently used .",
    "* `` overdensity 200''*. there is no name for the method , but it is often used . find density maximum , place a sphere and find radius , within which the sphere has the mean overdensity 200 ( or 177 if you really want to follow the top - hat model of nonlinear collapse ) .",
    "aarseth , s.j .",
    "1963 , , 126 , 223 aarseth s.j .",
    ", gott j.r . ,",
    "& turner e.l .",
    "1979 , , 228 , 664 aarseth s.j .",
    "1985 , in _ multiple time scales _ edited by j. w. brackbill and b. j. cohen ( new york , academic ) , p.377 appel a. 1985 , _",
    "siam j. sci .",
    "_ , 6 , 85 barnes j. & hut p. 1986 , _ nature _ 324 , 446 bertschinger e. & gelb j. 1991 , _ comp .",
    "_ , 5 , 164 bertschinger , e. 1998 , _ ann .",
    "astrophys . , _ 36 , 599 bouchet f.r . ,",
    "& hernquist l. 1988 , , 68 , 521 couchman h.m.p .",
    "1991 , , 368 , 23 doroshkevich a.g .",
    ", kotok e.v . ,",
    "novikov i.d . ,",
    "polyudov a.n . ,",
    "& sigov yu.s .",
    "1980 , , 192 , 321 efstathiou g. , davis m. , frenk c.s . , & white s.d.m .",
    "1985 , , 57 , 241 gelb j. 1992 _ ph.d .",
    "thesis , _ mit ghigna , s. , moore , b. , governato , f. , lake , g. , quinn , t. , stadel , j. 1999 , astro - ph/9910166 gross , m. 1997 _ ph.d .",
    "thesis , _ uc santa cruz gunn , j.e .",
    ", & gott j.r .",
    "1972 , , 176 , 1 hernquist l. 1987 , , 64 , 715 hernquist l. , bouchet f.r . , & suto y. 1991 , , 75 , 231 hockney r.w . and eastwood j.w . 1981 , numerical simulations using particles new york : mcgraw - hill klypin , a. , gottlber , s. , kravtsov , a. , khokhlov , a. 1999 , , 516 , 530 klypin a. , & shandarin s.f .",
    "1983 , , 204 , 891 klypin a. , holtzman j. , primack j. , & regos e. 1993 , , 416 1 klypin a. , & holtzman j.1997 , astro - ph/9712217 knebe , a. , kravtsov , a.v .",
    ", gottlber , klypin , a. 1999 , astro - ph/9912257 , accpented to mnras kravtsov a.v .",
    ", klypin a. , & khokhlov a. 1997 .",
    "j. suppl . , 111 , 73 kravtsov , a.v .",
    "1999 _ ph.d .",
    "thesis , _ new mexico state university peebles p.j.e .",
    "1970 , astron .",
    "j .. 75 , 13 press w.h . , & schechter p.l .",
    "1974 , ap . j. , 187 , 425 quinn , t. , katz , n. , & stadel , j. , lake , g. 1997 , astro = ph/9710043 sahni , v. , & coles , p. 1995",
    ", physics reports , 262 , 2 sellwood j.a . 1987 , _ ann .",
    "astrophys . , _ 25 , 151 white s.d.m .",
    "1976 , , 177 , 717 white s.d.m . ,",
    "& rees , m. j. 1978 , , 183 , 341 zeldovich ya.b .",
    "1970 ,  , 5 , 84"
  ],
  "abstract_text": [
    "<S> we give a short description of different methods used in cosmology . </S>",
    "<S> the focus is on major features of @xmath0-body simulations : equations , main numerical techniques , effects of resolution , and methods of halo identification . </S>"
  ]
}