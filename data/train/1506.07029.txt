{
  "article_text": [
    "in this paper , we consider the following optimization problem : @xmath0\\right\\|_f^2,\\end{aligned}\\ ] ] where    * @xmath1 are proper closed nonnegative functions , and @xmath2 is _ convex _ , while @xmath3 is possibly _ nonconvex _ , _ nonsmooth _ and _ non - lipschitz _ ; * @xmath4 are linear maps and @xmath5 , @xmath6 are injective",
    ".    in particular , @xmath7 and @xmath8 in can be regularizers used for inducing the desired structures .",
    "for instance , @xmath7 can be used for inducing low rank in @xmath9 .",
    "one possible choice is @xmath10 ( see next section for notation and definitions ) .",
    "alternatively , one may consider @xmath11 , where @xmath12 is a compact convex set such as @xmath13 with @xmath14 , or @xmath15 with @xmath16 ; the former choice restricts @xmath9 to have rank at most @xmath17 and makes nuclear - norm - free ( see @xcite ) . on the other hand",
    ", @xmath8 can be used for inducing sparsity . in the literature ,",
    "@xmath8 is typically separable , i.e. , taking the form @xmath18 where @xmath19 is a nonnegative continuous function with @xmath20 and @xmath21 is a regularization parameter .",
    "some concrete examples of @xmath19 include :    * bridge penalty @xcite : @xmath22 for @xmath23 ; * fraction penalty @xcite : @xmath24 for @xmath25 ; * logistic penalty @xcite : @xmath26 for @xmath25 ; * smoothly clipped absolute deviation @xcite : @xmath27 for @xmath28 ; * minimax concave penalty @xcite : @xmath29 for @xmath25 ; * hard thresholding penalty function @xcite : @xmath30 .",
    "the bridge penalty and the logistic penalty have also been considered in @xcite .",
    "finally , the linear map @xmath31 can be suitably chosen to model different scenarios .",
    "for example , @xmath31 can be chosen to be the identity map for extracting @xmath9 and @xmath32 from a noisy data @xmath33 , and the blurring map for a blurred data @xmath33 .",
    "the linear map @xmath5 can be the identity map or some  dictionary \" that spans the data space ( see , for example , @xcite ) , and @xmath6 can be chosen to be the identity map or the inverse of certain sparsifying transform ( see , for example , @xcite ) .",
    "more examples of can be found in @xcite .",
    "one representative application that is frequently modeled by via a suitable choice of @xmath3 , @xmath2 , @xmath34 , @xmath35 and @xmath36 is the background / foreground extraction problem , which is an important problem in video processing ; see @xcite for recent surveys . in this problem ,",
    "one attempts to separate the relatively static information called  background \" and the moving objects called  foreground \" in a video .",
    "the problem can be modeled by , and such models are typically referred to as rpca - based models . in these models ,",
    "each image is stacked as a column of a data matrix @xmath33 , the relatively static background is then modeled as a low rank matrix , while the moving foreground is modeled as sparse outliers .",
    "the data matrix @xmath33 is then decomposed ( approximately ) as the sum of a low rank matrix @xmath37 modeling the background and a sparse matrix @xmath38 modeling the foreground .",
    "various approximations are then used to induce low rank and sparsity , resulting in different rpca - based models , most of which take the form of .",
    "one example is to set @xmath2 to be the nuclear norm of @xmath9 , i.e. , the sum of singular values of @xmath9 , to promote low rank in @xmath9 and @xmath3 to be the @xmath39 norm of @xmath32 to promote sparsity in @xmath32 , as in @xcite . besides convex regularizers ,",
    "nonconvex models have also been widely studied recently and their performances are promising ; see @xcite for background / foreground extraction and @xcite for other problems in image processing .",
    "there are also nuclear - norm - free models that do not require matrix decomposition of the matrix variable @xmath9 when solving them , making the model more practical especially when the size of matrix is large .",
    "for instance , in @xcite , the authors set @xmath3 to be the @xmath39 norm of @xmath32 and @xmath2 to be the indicator function of @xmath40 .",
    "a similar approach was also adopted in @xcite with promising performances .",
    "clearly , for nuclear - norm - free models , one can also take @xmath3 to be some nonconvex sparsity inducing regularizers , resulting in a special case of that has not been explicitly considered in the literature before ; we will consider these models in our numerical experiments in section  [ sec5 ] .",
    "the above discussion shows that problem is flexible enough to cover a wide range of rpca - based models for background / foreground extraction .",
    "problem , though nonconvex in general , as we will show later in section  [ sec3 ] , can be reformulated into an optimization problem with three blocks of variables .",
    "this kind of problems containing several blocks of variables has been widely studied in the literature ; see , for example , @xcite .",
    "hence , it is natural to adapt the algorithm used there , namely , the alternating direction method of multipliers ( admm ) , for solving .",
    "classically , the admm can be applied to solving problems of the following form that contains 2 blocks of variables : @xmath41 where @xmath42 and @xmath43 are proper closed convex functions , @xmath44 and @xmath45 are linear operators . the iterative scheme of admm is @xmath46 where @xmath47 is the dual step - size and @xmath48 is the augmented lagrangian function for defined as @xmath49 with @xmath50 being the penalty parameter . under some mild conditions ,",
    "the sequence @xmath51 generated by the above admm can be shown to converge to an optimal solution of ; see for example , @xcite .",
    "however , the admm used in @xcite does not have a convergence guarantee ; indeed , it is shown recently in @xcite that the admm , when applied to a convex optimization problem with @xmath52 blocks of variables , can be divergent in general .",
    "this motivates the study of many provably convergent variants of the admm for convex problems with more than 2 blocks of variables ; see , for example , @xcite .",
    "recently , hong et al .",
    "@xcite established the convergence of the multi - block admm for certain types of nonconvex problems whose objective is a sum of a possibly _ nonconvex _ lipschitz differentiable function and a bunch of convex nonsmooth functions when the penalty parameter is chosen above a computable threshold .",
    "the problem they considered covers when @xmath3 is convex , or smooth and possibly nonconvex .",
    "later , wang et al .",
    "@xcite considered a more general type of nonconvex problems that contains as a special case and allows some nonconvex nonsmooth functions in the objective . to solve this type of problems",
    ", they considered a variant of the admm whose subproblems are simplified by adding a bregman proximal term .",
    "however , their results can not be applied to the direct adaptation of the admm for solving .    in this paper , following the studies in @xcite on convergence of nonconvex admm and its variant , and the recent studies in @xcite , we manage to analyze the convergence of the admm applied to solving the possibly nonconvex problem .",
    "in addition , we would like to point out that all the aforementioned nonconvex admm have a dual step - size of @xmath53 .",
    "while it is known that the classical admm converges for any @xmath54 for convex problems , and that empirically @xmath55 works best ( see , for example , @xcite ) , to our knowledge , the algorithm with a dual step - size @xmath56 has never been studied in the nonconvex scenarios .",
    "thus , we also study the admm with a general dual step - size , which will allow more flexibility in the design of algorithms .",
    "the contributions of this paper are as follows :    1 .",
    "we show that for any positive dual step - size @xmath57 less than the golden ratio , the cluster point of the sequence generated by our admm gives a stationary point of if the penalty parameter is chosen above a computable threshold depending on @xmath57 , whenever the sequence is bounded .",
    "we achieve this via a potential function specifically constructed for our admm . to the best of our knowledge , this is the first convergence result for the admm in the nonconvex scenario with a possibly nontrivial dual step - size ( @xmath58 ) .",
    "this result is also new for the convex scenario for the multi - block admm .",
    "2 .   we establish global convergence of the whole sequence generated by the admm under the additional assumption that the special potential function is a kurdyka - ojasiewicz function .",
    "following the discussions in ( * ? ? ?",
    "* section  4 ) , one can check that this condition is satisfied for all the aforementioned @xmath19 .",
    "furthermore , we discuss an initialization strategy to guarantee the boundedness of the sequence generated by the admm .",
    "we also conduct numerical experiments to evaluate the performance of our admm by using different nonconvex regularizers and real data .",
    "our computational results illustrate the efficiency of our admm with a nontrivial dual step - size .",
    "the rest of this paper is organized as follows .",
    "we present notation and preliminaries in section  [ sec2 ] .",
    "the admm for is described in section  [ sec3 ] .",
    "we analyze the convergence of the method in section  [ sec4 ] .",
    "numerical results are presented in section  [ sec5 ] , with some concluding remarks given in section  [ sec6 ] .",
    "in this paper , we use @xmath59 to denote the set of all @xmath60 matrices . for a matrix @xmath61 ,",
    "we let @xmath62 denote its @xmath63th entry and @xmath64 denote its @xmath65th column .",
    "the number of nonzero entries in @xmath66 is denoted by @xmath67 and the largest entry in magnitude is denoted by @xmath68 .",
    "moreover , the frbenius norm is denoted by @xmath69 , the nuclear norm is denoted by @xmath70 , which is the sum of singular values of @xmath66 ; and @xmath39-norm and @xmath71-quasi - norm ( @xmath72 ) are given by @xmath73 and @xmath74 , respectively .",
    "furthermore , for two matrices @xmath66 and @xmath75 of the same size , we denote their trace inner product by @xmath76 .",
    "finally , for the linear map @xmath77 in , its adjoint is denoted by @xmath78 , while the largest ( resp . , smallest ) eigenvalue of the linear map @xmath79 is denoted by @xmath80 ( resp . , @xmath81 ) .",
    "the identity map is denoted by @xmath82 .",
    "for an extended - real - valued function @xmath83 $ ] , we say that it is _ proper _ if @xmath84 for all @xmath61 and its domain @xmath85 is nonempty . for a proper function @xmath86 , we use the notation @xmath87 to denote @xmath88 and @xmath89 . our basic _",
    "( limiting-)subdifferential _ ( * ? ? ?",
    "* definition  8.3 ) of @xmath86 at @xmath90 used in this paper , denoted by @xmath91 , is defined as @xmath92 where @xmath93 denotes the frchet subdifferential of @xmath86 at @xmath94 , which is the set of all @xmath95 satisfying @xmath96 from the above definition , we can easily observe that @xmath97 we also recall that when @xmath86 is continuously differentiable or convex , the above subdifferential coincides with the classical concept of derivative or convex subdifferential of @xmath86 ; see , for example , ( * ? ? ?",
    "* exercise  8.8 ) and ( * ? ? ? * proposition  8.12 ) .",
    "moreover , from the generalized fermat s rule ( * ? ? ?",
    "* theorem  10.1 ) , we know that if @xmath98 is a local minimizer of @xmath86 , then @xmath99 .",
    "additionally , for a function @xmath86 with several groups of variables , we write @xmath100 ( resp . ,",
    "@xmath101 ) for the subdifferential ( resp . , derivative ) of @xmath86 with respect to the group of variables @xmath66 .    for a compact convex set @xmath102 ,",
    "its indicator function @xmath103 is defined by @xmath104 the normal cone of @xmath12 at the point @xmath105 is given by @xmath106 .",
    "we also use @xmath107 to denote the distance from @xmath66 to @xmath12 , i.e. , @xmath108 , and @xmath109 to denote the unique closest point to @xmath66 in @xmath12 .",
    "next , we recall the kurdyka - ojasiewicz ( kl ) property , which plays an important role in our global convergence analysis . for notational simplicity",
    ", we use @xmath110 ( @xmath111 ) to denote the class of concave functions @xmath112 satisfying : ( 1 ) @xmath113 ; ( 2 ) @xmath114 is continuously differentiable on @xmath115 and continuous at @xmath116 ; ( 3 ) @xmath117 for all @xmath118 .",
    "then the kl property can be described as follows .",
    "let @xmath86 be a proper lower semicontinuous function .    * for @xmath119",
    ", if there exist an @xmath120 $ ] , a neighborhood @xmath121 of @xmath122 and a function @xmath123 such that for all @xmath124 , it holds that @xmath125 then @xmath86 is said to have the * kurdyka - ojasiewicz ( kl ) * property at @xmath122 . *",
    "if @xmath86 satisfies the kl property at each point of @xmath126 , then @xmath86 is called a kl function .",
    "we refer the interested readers to @xcite and references therein for examples of kl functions .",
    "we also recall the following uniformized kl property , which was established in ( * ? ? ?",
    "* lemma 6 ) .",
    "[ unikl ] suppose that @xmath86 is a proper lower semicontinuous function and @xmath127 is a compact set .",
    "if @xmath128 on @xmath127 for some constant @xmath129 and satisfies the kl property at each point of @xmath127 , then there exist @xmath130 , @xmath111 and @xmath123 such that @xmath131 for all @xmath132 .",
    "before ending this section , we discuss first - order necessary conditions for .",
    "first , recall that is the same as @xmath133\\right\\|_f^2.\\ ] ] hence , from ( * ? ? ?",
    "* theorem  10.1 ) , we have @xmath134 at any local minimizer @xmath135 of . on the other hand , from ( * ? ? ?",
    "* exercise  8.8 ) and ( * ? ? ?",
    "* proposition  10.5 ) , we see that @xmath136 consequently , the first - order necessary conditions of at the local minimizer @xmath135 is given by : @xmath137 in this paper , we say that @xmath138 is a stationary point of if @xmath138 satisfies in place of @xmath139 .",
    "in this section , we present an admm for solving , which can be equivalently written as @xmath140 to describe the iterates of the admm , we first introduce the augmented lagrangian function of the above optimization problem : @xmath141 where @xmath142 is the lagrangian multiplier and @xmath143 is the penalty parameter .",
    "the admm for solving ( equivalently ) is then presented as follows :    comparing with the admm considered in @xcite , the above algorithm has an extra dual step - size parameter @xmath144 in the @xmath145-update .",
    "such a dual step - size was introduced in @xcite for the classical admm ( i.e. , for convex problems with two separate blocks of variables ) , and was further studied in @xcite for other variants of the admm .",
    "numerically , it was also demonstrated in @xcite that a larger dual step - size ( @xmath146 ) results in faster convergence for the convex problems they consider .",
    "thus , we adapt this dual step - size @xmath57 in our algorithm above . surprisingly , in our numerical experiments , a parameter choice of @xmath146 leads to the worst performance for our nonconvex problems .",
    "when @xmath53 , the above algorithm is a special case of the general algorithm studied in @xcite when @xmath2 and @xmath3 are smooth functions , or convex nonsmooth functions .",
    "the algorithm is shown to converge when @xmath147 is chosen above a computable threshold . however",
    ", their convergence result can not be directly applied when @xmath56 or when @xmath3 is nonsmooth and nonconvex . nevertheless ,",
    "following their analysis and the related studies @xcite , the above algorithm can be shown to be convergent under suitable assumptions .",
    "we will present the convergence analysis in section  [ sec4 ] .    before ending this section",
    ", we further discuss the three subproblems in algorithm 1 .",
    "first , notice that the @xmath9-update and @xmath32-update are given by @xmath148 in general , these two subproblems are not easy to solve .",
    "however , when @xmath2 and @xmath3 are chosen to be some common regularizers used in the literature , for example , @xmath149 and @xmath150 , then these subproblems can be solved efficiently via the proximal gradient method . additionally , when @xmath11 with @xmath12 being a closed convex set and @xmath151 , the @xmath9-update can be given explicitly by @xmath152 which can be computed efficiently if @xmath12 is simple , for example , when @xmath153 @xmath154 for some @xmath155 .",
    "for the @xmath32-update , when @xmath3 is given by with @xmath19 being one of the penalty functions presented in the introduction and @xmath156 , it can be solved efficiently via a simple root - finding procedure . finally , from the optimality conditions of",
    ", the @xmath157 can be obtained by solving the following linear system @xmath158 whose complexity would depend on the choice of @xmath159 in our model .",
    "for example , when @xmath159 is just the identity map , the @xmath157 is given explicitly by @xmath160.\\ ] ]",
    "in this section , we discuss the convergence of algorithm 1 for @xmath161 .",
    "we first present the first - order optimality conditions for the subproblems in algorithm 1 as follows , which will be used repeatedly in our convergence analysis below .    0 ( l^k+1)-^*(^k)+^*((l^k+1 ) + ( s^k)-z^k ) , [ lopt ] + 0 ( s^k+1)-^*(^k)+^*((l^k+1 ) + ( s^k+1)-z^k ) , [ sopt ] + 0 = ^*((z^k+1)-d ) + ^k - ( ( l^k+1)+(s^k+1)-z^k+1 ) , [ zopt ] + ^k+1-^k = -((l^k+1)+(s^k+1)-z^k+1 ) .",
    "[ lamopt ]    our convergence analysis is largely based on the following potential function : @xmath162 where @xmath163 note that @xmath164 is a convex and nonnegative function on @xmath165 . thus , for any @xmath166 , we have @xmath167 for @xmath161 , and the equality holds when @xmath53 ( so that @xmath168 ) .",
    "our convergence analysis also relies on the following assumption .",
    "[ assum ] @xmath2 , @xmath3 , @xmath5 , @xmath6 , @xmath147 and @xmath57 satisfy    * @xmath169 for some @xmath170 and @xmath171 for some @xmath172 ; * @xmath2 is continuous in its domain ; * the first iterate @xmath173 satisfies @xmath174    \\(i ) since @xmath5 and @xmath6 in are injective , ( a1 ) holds trivially ; ( ii ) ( a2 ) holds for many common regularizers ( for example , the nuclear norm ) or the indicator function of a set ; ( iii ) ( a3 ) places conditions on the _ first _ iterate of the algorithm .",
    "it is not hard to observe that this assumption holds trivially if both @xmath2 and @xmath3 are coercive , i.e. , if @xmath175 .",
    "we will discuss more sufficient conditions for this assumption after our convergence results , i.e. , after theorem  [ thm42 ] .",
    "we now start our convergence analysis by proving the following preparatory lemma , which states that the potential function is decreasing along the sequence generated from algorithm 1 if the penalty parameter @xmath147 is chosen above a computable threshold .    [ phidelemma ] suppose that @xmath176 and @xmath177 is a sequence generated by algorithm 1 . if ( a1 ) in assumption [ assum ] holds , then for @xmath178 , we have @xmath179 moreover , if @xmath180 , then the sequence @xmath181 , @xmath182 is decreasing .",
    "we start our proof by noticing that @xmath183 where the last equality follows from .",
    "we next derive an upper bound of @xmath184 .",
    "to proceed , we first note from that @xmath185 where the second equality follows from .",
    "hence , for @xmath186 , @xmath187 - [ \\tau \\mathcal{a}^*(d - \\mathcal{a}(z^{k } ) ) + ( 1-\\tau)\\lambda^{k-1 } ] \\nonumber\\\\ & = & \\tau \\mathcal{a}^*\\mathcal{a}(z^{k } - z^{k+1 } ) + ( 1-\\tau)(\\lambda^k - \\lambda^{k-1 } ) .",
    "\\label{lamdiff}\\end{aligned}\\ ] ] we now consider two separate cases : @xmath188 and @xmath189 .    * for @xmath188 , it follows from the convexity of @xmath190 that @xmath191 we further add @xmath192 to both sides of the above inequality and simplify the resulting inequality to get @xmath193 where the last equality follows from . * for @xmath194 ,",
    "dividing @xmath57 from both sides of , we have @xmath195 this together with @xmath196 and the convexity of @xmath190 , implies that @xmath197 then , adding @xmath198 to both sides of the above inequality , simplifying the resulting inequality and using the fact that @xmath199 for @xmath189 , we see that @xmath200 where the equality follows from .",
    "thus , for @xmath161 , combining , and recalling the definition of @xmath201 in , we have @xmath202 next , note that the function @xmath203 is strongly convex with modulus at least @xmath204 . using this fact and the definition of @xmath157 as a minimizer in , we see that @xmath205    moreover , using the fact that @xmath206 is a minimizer in , we have @xmath207    finally , note that @xmath208 is strongly convex with modulus at least @xmath209 from ( a1 ) in assumption [ assum ] . from this",
    ", we can similarly obtain @xmath210 thus , summing , , , and , we obtain .    now , suppose in addition that @xmath180 .",
    "then it is easy to check that @xmath211 hence we see from that @xmath212 which means that @xmath213 is decreasing .",
    "this completes the proof .",
    "we next show that the sequence generated by algorithm 1 is bounded if @xmath147 is chosen above a computable threshold , under ( a1 ) and ( a3 ) in assumption  [ assum ] . for notational simplicity , from now on",
    ", we let @xmath214    [ boundnesslemma ] suppose that @xmath176 and @xmath215 . if ( a1 ) and ( a3 ) in assumption [ assum ] hold , then a sequence @xmath216 generated by algorithm 1 is bounded .    with our choice of @xmath147 and ( a1 ) in assumption [ assum",
    "] , we see immediately from lemma [ phidelemma ] that the sequence @xmath217 is decreasing .",
    "this together with ( a3 ) in assumption [ assum ] shows that , for @xmath178 , @xmath218 where the last equality is obtained by completing the square .",
    "we next derive an upper bound for @xmath219 .",
    "we start by substituting into and rearranging terms to obtain @xmath220 we now consider two different cases :    * for @xmath188 , it follows from the convexity of @xmath190 and that @xmath221 where the equality follows from .",
    "then , we have @xmath222 * for @xmath194 , by dividing @xmath223 from both sides of , we obtain @xmath224 then , since @xmath196 , using the convexity of @xmath190 and , we have @xmath225    thus , combining and , we have @xmath226 substituting into , we have @xmath227\\cdot\\frac{\\beta}{2}\\|\\mathcal{b}(l^k)+\\mathcal{c}(s^k)-z^k\\|_f^2 . \\end{aligned}\\end{aligned}\\ ] ]    with established , we are now ready to prove the boundedness of the sequence .",
    "we start with the observation that for @xmath176 and @xmath215 , we always have @xmath228 and @xmath229 where @xmath201 is defined in .",
    "then we consider two cases :    * for @xmath230 , it follows from , , , and the nonnegativity of @xmath2 and @xmath3 that @xmath231 , @xmath232 and @xmath233 are bounded ; and moreover , @xmath234 the boundedness of @xmath235 and @xmath236 follows immediately from this last relation .",
    "furthermore , @xmath237 is bounded since @xmath238 finally , we obtain the boundedness of @xmath239 from @xmath240 * for @xmath241 , it follows from , , , and the nonnegativity of @xmath2 and @xmath3 that @xmath231 and @xmath232 are bounded ; and moreover @xmath242 , from which we see immediately that @xmath235 and @xmath236 are bounded .",
    "the boundedness of @xmath237 now follows from with @xmath241 , i.e. , @xmath243 .",
    "the boundedness of @xmath239 again follows from .",
    "this completes the proof .",
    "we are now ready to prove our first global convergence result for algorithm 1 , which also characterizes the cluster point of the sequence generated .",
    "[ convergencethe ] suppose that @xmath176 and @xmath244 .",
    "if assumption [ assum ] holds , then    1 .   @xmath245 ; 2 .",
    "any cluster point @xmath246 of a sequence @xmath177 generated by algorithm 1 is a stationary point of .",
    "the boundedness of the sequence @xmath177 follows immediately from proposition [ boundnesslemma ] and thus a cluster point exists .",
    "we now prove statement ( i ) .",
    "suppose that @xmath246 is a cluster point of the sequence @xmath177 and let @xmath247 , @xmath248 be a convergent subsequence such that @xmath249 by summing from @xmath250 to @xmath251 , we have @xmath252 where @xmath253 ( since @xmath215 ) . passing to the limit in and rearranging terms in the resulting relation ,",
    "we obtain @xmath254 where the last inequality follows from the properness of @xmath2 and @xmath3 .",
    "this together with @xmath255 and @xmath170 implies that @xmath256 hence , we have @xmath257 next , by summing both sides of from @xmath250 to @xmath258 and passing to the limit , we have @xmath259 from which we conclude that @xmath260 finally , we have @xmath261 from , , and ( a1 ) in assumption [ assum ] .",
    "this proves statement ( i ) .",
    "we next prove statement ( ii ) .",
    "from the lower semicontinuity of @xmath262 ( since @xmath2 and @xmath3 are lower semicontinuous ) , we have @xmath263 on the other hand , from the definition of @xmath264 as a minimizer in , we have @xmath265 taking limit in above equality , and invoking statement ( i ) and ( a2 ) in assumption [ assum ] , we see that @xmath266 then , combining and , we see that @xmath267 which , together with ( a2 ) in assumption [ assum ] , @xmath268 , @xmath269 and the definition of @xmath262 , implies that @xmath270    thus , passing to the limit in - along @xmath271 and invoking statement ( i ) , and , we see that @xmath272 rearranging terms in , it is not hard to obtain @xmath273 this shows that @xmath246 is a stationary point of .",
    "this completes the proof .    from the above discussions , we establish under assumption  [ assum ] the convergence of the admm with @xmath161 when the penalty parameter @xmath147 is chosen above a computable threshold @xmath274 which depends on @xmath57",
    "the existence of this kind of threshold is also obtained in the recent studies @xcite on the nonconvex admm and its variants with @xmath241 . in fig.[beta0 ] , we plot @xmath274 against @xmath57 with @xmath31 being the identity map ( hence , @xmath275 ) .",
    "it is not hard to see from fig.[beta0 ] that for a given penalty parameter @xmath276 , we can always choose a dual step - size @xmath57 from an interval containing 1 so that the corresponding admm is convergent .     for @xmath161.,height=264 ]",
    "[ rem4penalty ] in computation , for a @xmath161 , the @xmath274 in may be too large and hence fixing a @xmath147 close to it can lead to slow convergence . as in @xcite , one could possibly accelerate the algorithm by initializing the algorithm with a small @xmath147 ( less than @xmath274 ) and then increasing the @xmath147 by a constant ratio until @xmath215 if the sequence generated becomes unbounded or the successive change does not vanish sufficiently fast . clearly , after at most finitely many increases , the penalty parameter @xmath147 gets above the threshold @xmath277 and the convergence of the resulting algorithm is guaranteed by theorem  [ convergencethe ] under assumption  [ assum ] . on the other hand ,",
    "if @xmath147 is never increased , this means that the successive change goes to zero and the sequence is bounded .",
    "then it is routine to show that any cluster point is a stationary point if @xmath3 is continuous in its domain .    under the additional assumption that the potential function @xmath262 is a kl function , we show in the next theorem that the whole sequence generated by algorithm 1 is convergent if @xmath147 is greater than a computable threshold , again under assumption  [ assum ] .",
    "our proof makes use of the uniformized kl property ; see proposition  [ unikl ] .",
    "this technique was previously used in @xcite to prove the convergence of the proximal alternating linearized minimization algorithm for nonconvex and nonsmooth problems , and later in @xcite to prove the global convergence of the bregman admm with @xmath53 . our analysis ,",
    "though follows a similar line of arguments as in @xcite , is much more intricate .",
    "this is because when @xmath56 , the successive change in the dual variable can not be controlled solely by the successive changes in the primal variables .",
    "[ thm42 ] let @xmath161 and @xmath278 .",
    "suppose in addition that assumption  [ assum ] holds and the potential function @xmath279 is a kl function .",
    "then , the sequence @xmath280 , @xmath281 , @xmath282 , @xmath182 generated by algorithm 1 converges to a stationary point of .    in view of theorem",
    "[ convergencethe ] , we only need to show that the sequence is convergent .",
    "we start by noting from , and that @xmath283 , @xmath281 , @xmath282 , @xmath182 is bounded below . since this sequence is also decreasing from theorem  [ phidelemma ] ,",
    "we conclude that @xmath284 exists . in the following ,",
    "we will consider two cases .",
    "* case 1 ) * suppose first that @xmath285 for some @xmath286 .",
    "since @xmath283 , @xmath281 , @xmath282 , @xmath182 is decreasing , we must have @xmath287 for all @xmath288 .",
    "then , it follows from that @xmath289 and @xmath290 for all @xmath291 .",
    "hence , @xmath235 and @xmath239 converge finitely .",
    "moreover , from , we have @xmath292 for all @xmath293 .",
    "since @xmath161 , we have @xmath294 and hence we see further that @xmath295 which implies the convergence of @xmath237 .",
    "additionally , for all @xmath293 , we have @xmath296 where the first inequality follows from ( a1 ) in assumption [ assum ] and the equality follows from .",
    "this together with , implies that @xmath297 .",
    "thus , @xmath236 is also convergent .",
    "consequently , we see that @xmath280 , @xmath281 , @xmath282 , @xmath182 is a convergent sequence in this case .",
    "* case 2 ) * from now on , we consider the case where @xmath298 , @xmath281 , @xmath282 , @xmath299 for all @xmath300 . in this case",
    ", we will divide the proof into three steps : * 1 . *",
    "we first prove that @xmath262 is constant on the set of cluster points of the sequence @xmath280 , @xmath281 , @xmath282 , @xmath182 and then apply the uniformized kl property ; * 2 .",
    "* we bound the distance from 0 to @xmath301 ; * 3 .",
    "* we show that the sequence @xmath280 , @xmath281 , @xmath282 , @xmath182 is a cauchy sequence and hence is convergent .",
    "the complete proof is presented as follows .",
    "_ step * 1 . *",
    "_ we recall from proposition  [ boundnesslemma ] that the sequence @xmath280 , @xmath281 , @xmath282 , @xmath182 generated by algorithm 1 is bounded and hence must have at least one cluster point .",
    "let @xmath127 denote the set of cluster points of @xmath280 , @xmath281 , @xmath282 , @xmath182 .",
    "we will show that @xmath262 is constant on @xmath127 .    to this end , take any @xmath302 and consider a convergent subsequence @xmath271 with @xmath303 .",
    "then from the lower semicontinuity of @xmath262 ( since @xmath2 and @xmath3 are lower semicontinuous ) and the definition of @xmath304 , we have @xmath305 on the other hand , notice from the definition of @xmath206 as a minimizer in that @xmath306 this together with theorem [ convergencethe](i ) , the continuity of @xmath262 with respect to @xmath9 ( from ( a2 ) in assumption [ assum ] ) , @xmath307 and @xmath145 ; and the definition of @xmath304 implies that @xmath308 combining and , we conclude that @xmath309 . since @xmath302 is arbitrary , we conclude further that the potential function @xmath262 is constant on @xmath127 .",
    "the fact that @xmath310 on @xmath127 together with our assumption that @xmath279 is a kl function and proposition [ unikl ] implies that there exist @xmath130 , @xmath111 and @xmath311 , such that @xmath312 for all @xmath166 satisfying @xmath313 and @xmath314 .",
    "on the other hand , since @xmath315 by the definition of @xmath127 , and @xmath316 , @xmath317 , @xmath318 , then for such @xmath319 and @xmath320 , there exists @xmath321 such that @xmath322 and @xmath323 for all @xmath324 .",
    "thus , for @xmath324 , we have @xmath325    _ step * 2 . *",
    "_ we next consider the subdifferential @xmath301 .",
    "looking at the partial subdifferential with respect to @xmath9 , we have @xmath326 \\\\ & \\stackrel{(\\mathrm{ii})}{= } -{\\textstyle\\left(1+\\frac{2\\theta(\\tau)}{\\tau } \\right)\\mathcal{b}^*(\\lambda^{k}-\\lambda^{k-1 } ) + \\beta\\mathcal{b}^*\\left[\\left(-\\mathcal{b}(l^k)-\\frac{\\lambda^k-\\lambda^{k-1}}{\\tau\\beta}\\right)-\\left(-\\mathcal{b}(l^{k-1 } ) -\\frac{\\lambda^{k-1}-\\lambda^{k-2}}{\\tau\\beta}\\right)\\right]}\\\\ & = -{\\textstyle\\left(1+\\frac{2\\theta(\\tau)+1}{\\tau}\\right)}\\mathcal{b}^*(\\lambda^{k}-\\lambda^{k-1 } ) + { \\textstyle\\frac{1}{\\tau}}\\mathcal{b}^*(\\lambda^{k-1}-\\lambda^{k-2})-\\beta\\mathcal{b}^*\\mathcal{b}(l^{k}-l^{k-1 } ) , \\end{aligned}\\end{aligned}\\ ] ] where the inclusion follows from , and the equalities ( i ) and ( ii ) follow from .",
    "similarly , @xmath327 where the inclusion follows from and the last equality follows from .",
    "moreover , @xmath328 where the third equality follows from and the last equality follows from . finally , @xmath329 where the last equality follows from .",
    "thus , from the above relations , there exists @xmath330 so that @xmath331    _ step * 3 . *",
    "_ we now prove the convergence of the sequence by combining with .",
    "for notational simplicity , define @xmath332 since @xmath262 is decreasing and @xmath114 is monotonic , it is easy to see @xmath333 for @xmath300",
    ". then we have for all @xmath324 that @xmath334 \\\\ & \\geq \\theta_{\\tau,\\beta } ( l^k , s^k , z^k , \\lambda^k)-\\theta_{\\tau,\\beta } ( l^{k+1 } , s^{k+1 } , z^{k+1 } , \\lambda^{k+1 } ) \\\\ & \\geq b_1\\|l^{k+1 } - l^k\\|_f^2 + b_2\\|z^{k+1 } - z^k\\|_f^2    \\\\ & \\geq \\frac{1}{2}\\min\\{b_1 , b_2\\}\\cdot\\left[\\|l^{k+1}-l^k\\|_f+\\|z^{k+1}-z^k\\|_f\\right]^2 , \\end{aligned}\\end{aligned}\\ ] ] where the first inequality follows from , the second inequality follows from the concavity of @xmath114 , the third inequality follows from , the fourth inequality follows from with @xmath335 and @xmath336 .    dividing both sides of by @xmath337 , taking the square root and using the inequality @xmath338 for @xmath339 to further upper bound the left hand side of the resulting inequality",
    ", we obtain that @xmath340 where @xmath341 is an arbitrary positive constant . on the other hand",
    ", it follows from that @xmath342 adding @xmath343 to both sides of the above inequality and simplifying the resulting inequality , we obtain that @xmath344 where we write @xmath345 and @xmath346 for notational simplicity .",
    "similarly , @xmath347 then substituting and into and rearranging terms , we have @xmath348 thus , summing from @xmath349 to @xmath350 , we have @xmath351 recall that @xmath341 introduced in is an arbitrary positive constant .",
    "taking @xmath352 and hence @xmath353 , we have from the above inequality that @xmath354 hence @xmath235 and @xmath239 are convergent . additionally , summing from @xmath349 to @xmath350",
    ", we have @xmath355 which implies that @xmath237 is convergent . finally , from and ( a1 ) in assumption [ assum ] , we see that @xmath236 is also convergent . consequently , we conclude that @xmath280 , @xmath281 , @xmath282 , @xmath182 is a convergent sequence .",
    "this completes the proof .",
    "our convergence analysis relies on assumption [ assum ] .",
    "while ( a3 ) in assumption  [ assum ] appears restrictive since it makes assumptions on the first iterate of algorithm 1 , we show below that this assumption would hold upon a suitable choice of initialization .",
    "specifically , if we initialize at @xmath356 satisfying    _ , ( l^1 , s^1 , z^1 , ^1 ) _ , ( l^0 , s^0 , z^0 , ^0 ) , [ initializationb ] + _ , ( l^0 , s^0 , z^0 , ^0 ) < h_0 , [ initializationa ]    then it is easy to check that ( a3 ) in assumption  [ assum ] holds . in the next proposition",
    ", we demonstrate that can always be satisfied with a suitable initialization . after this",
    ", we will propose a specific way to initialize algorithm 1 for a wide range of problems so that both and are satisfied .",
    "[ initial_pro ] suppose that @xmath161 and @xmath357 .",
    "if the initialization @xmath356 is chosen as @xmath358 and @xmath359 then we have @xmath360    first , from , we have @xmath361 where the last equality follows from . then , @xmath362 where the second equality follows from and the fourth equality follows from . additionally , using the same arguments as in the proof of lemma [ phidelemma ] leading to , and ,",
    "it is easy to see that @xmath363 summing , , and , we obtain @xmath364 we now consider two cases :    * for @xmath365 , it is easy to see @xmath366 and @xmath367 then , we have @xmath368 * for @xmath189 , it is easy to see @xmath369 and @xmath370 then , we have @xmath371    thus , combining the above with and @xmath372 , we conclude that @xmath360 this completes the proof .    from proposition [ initial_pro ] , we see that if the initialization @xmath356 is chosen to satisfy the conditions in proposition [ initial_pro ] , then holds . based on this",
    ", we can now present one specific way to initialize algorithm 1 so that both and are satisfied for a class of problems , whose objective functions @xmath7 and @xmath8 take forms @xmath373 and , respectively ; here , @xmath12 is a compact convex set .",
    "the initialization we consider is : @xmath374 where @xmath375 is a scaling parameter .",
    "one can easily check that this initialization satisfies .",
    "moreover , @xmath376 thus , the condition is equivalent to @xmath377 we further discuss this inequality for some concrete examples of @xmath3 presented in the introduction .",
    "suppose that @xmath3 is coercive .",
    "then @xmath378 and hence holds trivially for any choice of @xmath375 .",
    "suppose that @xmath379 for @xmath25",
    ". then @xmath380 . hence holds if the parameter @xmath375 can be chosen so that @xmath381 .",
    "suppose that @xmath382 for @xmath28 .",
    "then @xmath383 . hence holds if @xmath375 can be chosen so that @xmath384 .",
    "suppose that @xmath385 for @xmath25 .",
    "then , @xmath386 . hence holds if @xmath375 can be chosen so that @xmath387 .",
    "suppose that @xmath388 .",
    "then it is not hard to show that @xmath389 . hence holds if @xmath375 can be chosen so that @xmath390 .",
    "in this section , we conduct numerical experiments to show the performances of our algorithm .",
    "all experiments are run in matlab r2014b on a 64-bit pc with an intel(r ) core(tm ) i7 - 4790 cpu ( 3.60ghz ) and 16 gb of ram equipped with windows 8.1 os .",
    "[ [ testing - model ] ] testing model + + + + + + + + + + + + +    we consider the problem of extracting background / foreground from a given video under different scenarios .",
    "specifically , we consider : @xmath391 where @xmath392 and @xmath31 is a linear map .",
    "this model corresponds to with @xmath11 and @xmath393 .",
    "we compare the performances of the admm with different choices of @xmath57 , as well as the proximal alternating linearized minimization ( palm ) proposed in @xcite , on solving . for ease of future reference , we recall that the palm for solving is given by @xmath394 where @xmath395 and @xmath396 are positive numbers .    in our experiments , we consider the following three choices of sparse regularizers @xmath8 :    * bridge regularizer : @xmath397 for @xmath398 ; * fraction regularizer : @xmath399 for @xmath25 ; * logistic regularizer : @xmath400 for @xmath25 ;    and two choices of linear map @xmath31 :    * @xmath401 : in this case , model can be applied to extracting background / foreground from a surveillance video with noise . *",
    "@xmath402 with @xmath403 being the matrix representation of a regular blurring operator ( the blurring is assumed to occur frame - wise ) : in this case , model can be applied to extracting background / foreground from a blurred and noisy surveillance video .",
    "+    noisy     +    noisy + blurred     +    ground + truth       [ [ testing - videos ] ] testing videos + + + + + + + + + + + + + +    we choose four real videos ,  hall \" ,  bootstrap \" ,  fountain \" and  shoppingmall \" , from the dataset i2r provided by li et al . @xcite .",
    "the details of these videos are as follows :    * * hall * video contains 200 @xmath404 frames ( from airport2001 to airport2200 ) ; * * bootstrap * video contains 200 @xmath405 frames ( from b01801 to b02000 ) ; * * fountain * video contains 200 @xmath406 frames ( from fountain1301 to fountain1500 ) ; * * shoppingmall * video contains 200 @xmath407 frames ( from shoppingmall1501 to shoppingmall1700 ) .",
    "we show one frame of each testing video under two different scenarios ( noisy and noisy blurred ) , and their ground - truth images of foregrounds in fig.[fig_org ] .",
    "additionally , all pixel values of the testing videos are re - scaled into @xmath408 $ ] in our numerical experiments .",
    "[ [ parameters - setting ] ] parameters setting + + + + + + + + + + + + + + + + + +    for the admm , we use the following heuristics(i ) that the successive change of each variable goes to zero as @xmath409 .",
    "thus , intuitively , it is more favorable to see a decrease in the successive change as @xmath410 increases .",
    "this heuristic is designed based on this intuition . ] to update @xmath147 : we initialize @xmath411 and @xmath412 , where @xmath274 is given in . in the @xmath410-th iteration , we compute @xmath413 then , we increase @xmath414 by 1 if @xmath415 .",
    "obviously , @xmath414 is non - decreasing in this procedure .",
    "we then update @xmath147 as @xmath416 whenever @xmath417 and the sequence satisfies either @xmath418 or @xmath419 . on the other hand , for palm , we set @xmath420 .",
    "we initialize our algorithm and the palm at the point specified in with @xmath421 .",
    "moreover , we terminate our admm by the following two - stage criterion , @xmath32 , @xmath307 , @xmath145 ) in each iteration of our algorithm because computing matrix frobenius norms can be expensive , especially for large scale problems .",
    "this strategy will help reduce the cost per iteration .",
    "we examine @xmath422 and @xmath423 in the first stage because these quantities being small intuitively implies that @xmath424 and @xmath425 are small ; see the proof of theorem  [ convergencethe ] , particularly , and the discussions that follow . ] : in each iteration , we check if @xmath426 for some @xmath427 ; if it holds , then we further check if @xmath428 for some @xmath429 .",
    "we terminate the algorithm if this latter condition is also satisfied . for the palm ,",
    "we terminate it when @xmath430 for some @xmath431 .",
    "the specific values of @xmath432 , @xmath433 and @xmath434 are given in the following experiments .",
    "in this subsection , we use the performance profile to evaluate the performances of the admm with different @xmath57 and the palm for extraction under different scenarios .",
    "the performance profile is proposed by dolan and mor @xcite as a tool for evaluating and comparing the performance of a collection of solvers @xmath435 on a set of test problems @xmath436 .    to describe this method",
    ", we assume that we have @xmath437 solvers and @xmath438 problems , and we use the iteration number as a performance measure .",
    "then , for each problem @xmath65 and solver @xmath410 , we set @xmath439 and compute the performance ratio @xmath440 the performance profile of iteration numbers is then defined as the distribution function for the performance ratio , i.e. , @xmath441 for @xmath442 . similarly , the performance profile of function values is obtained by using @xmath443 in place of @xmath444 in , where @xmath443 denotes the function value at the solution given by solver @xmath410 for solving problem @xmath65 . generally speaking , for solver @xmath445 ,",
    "the higher @xmath446 indicates a better performance within the factor @xmath447 .    in our experiments , we evaluate the following solvers : the admm with @xmath448 , the admm with @xmath241 , the admm with @xmath449 and the palm .    for @xmath450 , our test problems are described in table [ proset ] , where we use the four real videos introduced above as our input data in , with 3 choices of sparse regularizers , 10 choices of @xmath451 , and 6 choices of @xmath452 and @xmath453 .",
    "thus , we have 4 solvers and a total of @xmath454 test problems , with 240 test problems for each sparse regularizer . moreover , we set @xmath455 , @xmath456 and @xmath457 .",
    "fig.[per_pro ] shows the performance profiles of iteration numbers and function values for different regularizers under this scenario .    for @xmath458 ,",
    "our test problems are described in table [ proset_deblur ] , where we use 2 choices of @xmath452 and @xmath453 .",
    "thus , we have 4 solvers and a total of @xmath459 test problems , with 80 test problems for each sparse regularizer . in our experiments",
    ", we use the method described in @xcite to generate the blurring matrix @xmath460 , which can be represented as a kronecker product @xmath461 under the periodic boundary condition .",
    "the matlab codespcha / hno/ as a supplement to the book @xcite . ] that generate @xmath462 and @xmath463 are shown below , where  frame_size \" is the size of each frame :    ....    [ p , center ] = psfgauss(frame_size , 1 ) ;    [ hr , hc ] = krondecomp(p , center , ' periodic ' ) ; ....    moreover , we set @xmath464 , @xmath465 and @xmath466 .",
    "fig.[per_pro_deblur ] shows the performance profiles under this scenario .",
    "it is not hard to see from fig.[per_pro ] and fig.[per_pro_deblur ] that the performance profiles of iteration numbers for the admm with @xmath448 and @xmath241 usually lie above those for the palm ; and their performance profiles of function values are almost the same .",
    "this shows that the admm with @xmath448 or @xmath241 takes less iterations for solving all the test problems while giving comparable function values . for bridge regularizer in the case",
    "where @xmath450 ( see fig.[per_pro](a ) ) and in the case where @xmath458 ( see fig.[per_pro_deblur](a ) ) , we can see that the admm with @xmath467 sightly outperforms the admm with @xmath53 in terms of the number of iterations . for other regularizers ,",
    "their performances are comparable . additionally , for the admm with @xmath449",
    ", we can see from fig.[per_pro ] and fig.[per_pro_deblur ] that it always terminates with the worst function value , although it is always fastest in the case where @xmath458 ( see fig.[per_pro_deblur ] ) .",
    "to better visualize the performance of the algorithms in terms of function values , we also plot @xmath468 against the number of iterations for each algorithm , where @xmath469 denotes the objective value obtained by each algorithm at @xmath470 and @xmath471 denotes the minimum of the objective values obtained from all algorithms .",
    "we only consider the admm with @xmath467 , the admm with @xmath53 and the palm , and terminate them only after _ at least _",
    "500 iterations _ and _ the termination criteria are satisfied with @xmath472 , @xmath473 and @xmath474 . for brevity , we focus on the scenario @xmath450 and use the  hall \" video . the results are presented in fig.[fval_vs_it ] , from which we can see that the admm with @xmath241 or @xmath448 performs better than palm for those particular instances .",
    ".problem setting for @xmath450 [ cols=\"^,^,^\",options=\"header \" , ]      +    [ [ summary ] ] summary + + + + + + +    from the results above , it can be seen that the admm with @xmath448 performs better in the sense that it takes less cpu time for solving most test problems while returning comparable f - measures .",
    "the performances of our admm for extraction are also promising from fig.[fig_noisy ] and fig.[fig_blur ] .",
    "in this paper , we study a general ( possibly nonconvex and nonsmooth ) model and adapt the admm with a general dual step - size @xmath57 , which can be chosen in @xmath475 , to solve it .",
    "we establish that any cluster point of the sequence generated by our admm gives a stationary point under some assumptions ; we also give simple sufficient conditions for these assumptions . under an additional assumption that a potential function is a kurdyka - ojasiewicz function",
    ", we can further establish the global convergence of the whole sequence generated by our admm .",
    "our computational results demonstrate the efficiency of our algorithm .",
    "note that our admm may not be beneficial when @xmath5 or @xmath6 has no special structure , because the corresponding subproblems of admm may not have closed - form solutions . nonetheless , as in @xcite",
    ", it may be possible to add  proximal terms \" to simplify the subproblems of our admm .",
    "in addition , in view of the recent work @xcite , it may also be possible to study the convergence of our admm for some specially structured nonconvex @xmath2 .",
    "these are possible future research directions .",
    "the authors are grateful to the editor and the anonymous referees for their valuable suggestions and comments , which helped improve this paper .",
    "h. attouch , j. bolte , p. redont and a. soubeyran .",
    "proximal alternating minimization and projection methods for nonconvex problems : an approach based on the kurdyka - ojasiewicz inequality . , 35(2 ) : 438457 , 2010 .",
    "t. bouwmans , a. sobral , s. javed , s.k .",
    "jung and e .- h .",
    "decomposition into low ",
    "rank plus additive matrices for background / foreground separation : a review for a comparative evaluation with a large - scale dataset . , 2015 ."
  ],
  "abstract_text": [
    "<S> in this paper , we study a general optimization model , which covers a large class of existing models for many applications in imaging sciences . to solve the resulting possibly nonconvex , nonsmooth and non - lipschitz optimization problem , we adapt the alternating direction method of multipliers ( admm ) with a general dual step - size to solve a reformulation that contains three blocks of variables , and analyze its convergence . </S>",
    "<S> we show that for any dual step - size less than the golden ratio , there exists a computable threshold such that if the penalty parameter is chosen above such a threshold and the sequence thus generated by our admm is bounded , then the cluster point of the sequence gives a stationary point of the nonconvex optimization problem . </S>",
    "<S> we achieve this via a potential function specifically constructed for our admm . </S>",
    "<S> moreover , we establish the global convergence of the whole sequence if , in addition , this special potential function is a kurdyka - ojasiewicz function . </S>",
    "<S> furthermore , we present a simple strategy for initializing the algorithm to guarantee boundedness of the sequence . finally , we perform numerical experiments comparing our admm with the proximal alternating linearized minimization ( palm ) proposed in @xcite on the background / foreground extraction problem with real data . </S>",
    "<S> the numerical results show that our admm with a nontrivial dual step - size is efficient .    </S>",
    "<S> nonsmooth and nonconvex optimization ; alternating direction method of multipliers ; dual step - size ; background / foreground extraction </S>"
  ]
}