{
  "article_text": [
    "most time - consuming tasks performed on supercomputers are linear algebra operations . with the advent of multicore architectures and massive parallelism , this results in the necessity to optimize and understand their parallel executions . here",
    ", we consider the problem of the tiled cholesky factorization .",
    "the algorithm divides the initial matrix into square sub - matrices , or _",
    "tiles _ of the same size",
    ". the focus will be on large instances of the tiled cholesky factorization , that is where the number of tiles is large , which allows asymptotical analysis .",
    "to the authors knowledge , no theoretical non trivial bound on the execution time of any schedule for the tiled cholesky factorization have been found .",
    "this motivates this paper .",
    "we note that the tiled cholesky factorization algorithm has recently received plenty of attention .",
    "either as an algorithm in itself  @xcite or as a case study for task - based schedulers  @xcite .",
    "examples of task - based schedulers which have produced papers about the scheduling of tiled cholesky factorization are for example dague  @xcite , kaapi  @xcite , quark  @xcite , starpu  @xcite , smpss  @xcite , and supermatrix  @xcite .",
    "we also note that openmp since 3.1 supports task - based parallelism .",
    "the tiled cholesky factorization algorithm is also used in practice and is implemented in dense linear algebra state of the art libraries , for example dplasma , flame , and plasma .",
    "it is therefore of interest to better understand the parallel execution of the tiled cholesky factorization algorithm . in this paper",
    ", we neglect communication costs between processing units .",
    "we also ignore any memory problems a real execution would encounter .",
    "also , we assume homogeneous processing units .",
    "also , we assume ideal flop - based weights for the execution time of the tasks . while we acknowledge that this is a very unrealistic and simplistic model",
    ", we note that any practical implementations will execute slower than this model .",
    "therefore , this model provides lower bounds on the execution time of any parallel execution on any parallel machine .",
    "the lower bounds that we exhibit are not trivial and are relevant for practical applications .",
    "we can relate our work to the recent work of agullo et al .",
    "@xcite where the authors provide lower bound as well .",
    "the authors consider a more complicated model ( heterogeneous ) and solve their problem with an lp formulation .",
    "we consider a simpler model ( homogeneous ) but we provide closed - form solutions .",
    "another contribution of our paper is the alap schedule heuristic where tasks are scheduled from the end of the execution as opposed from the start .",
    "we can also relate our work to the work of cosnard , marrakchi , robert , and trystram from 1988 to 1989  @xcite . in this work ,",
    "the authors have the same model and ask similar questions as ours .",
    "a minor difference is that they are studying the gaussian elimination while we study the cholesky factorization .",
    "the main difference is that they study the level 1 blas algorithm which work on columns of the matrix .",
    "this algorithm was popular in the 1980s due to vectorization , nowadays tiled algorithms are much more relevant .",
    "also the scheduling of the level 1 blas algorithm seems to be an easier problem . in the level 1 blas algorithm ,",
    "the matrix is partitioned by columns .",
    "the number of created tasks is @xmath9 where @xmath0 is the number of columns of the problem . in our case",
    ", we partition the matrix by tiles .",
    "if we have a @xmath0-by-@xmath0 tile matrix , the number of created tasks is @xmath2 .",
    "we have tried to apply similar techniques as in the level 1 blas algorithm study papers to solve the tiled problem and we have been unsuccessful .",
    "we have tried to solve the the level 1 blas algorithm problem with our techniques and have obtained the same results .",
    "a few scheduling algorithms exist for the ( tiled ) cholesky factorization ; in practice , the alap ( as late as possible ) schedule seems to work well with the tiled cholesky factorization .",
    "this motivates our study of this heuristic in section  [ sec : alap ] .",
    "in particular , we derive an upper bound on the number of processing units necessary to reach the critical path of the algorithm . then , we present in section  [ sec : lowerbounds ] some lower bounds on the execution time of any schedule using a given number of processing units by splitting the task set into two subsets . in section  [ sec :",
    "tightness ] , we analyze the last bound found in section  [ sec : lowerbounds ] , and show that it is nearly optimal by describing a schedule with efficiency close to the efficiency bound derived from it .",
    "given a symmetric positive definite ( spd ) matrix @xmath10 , the cholesky factorization gives a ( lower ) triangular matrix @xmath11 such that @xmath12 .",
    "it is a core operation to compute the inverse of spd matrices using the cholesky inversion .",
    "note that it also allows to solve systems of the form @xmath13 by reducing it to computing solutions of @xmath14 , and then @xmath15 .    in order to compute such a factorization using many processing units",
    ", we divide the matrix @xmath10 into @xmath16 square tiles of size @xmath17 .",
    "this allows tile computations , and globally increases the amount of parallelism and the data locality .",
    "algorithm  [ alg1 ] computes the cholesky factorization of @xmath10 using these blocks .",
    "@xmath18 @xmath19 @xmath20 @xmath21    we will rename the tasks corresponding to the potrf as @xmath22 or @xmath23 with @xmath24 ( as potrf is a @xmath17-by-@xmath17 cholesky factorization ) , trsm as @xmath25 or @xmath26 with @xmath27 , syrk as @xmath28 or @xmath29 with @xmath30 , and gemm as @xmath31 or @xmath32 with @xmath33 to refer to the tasks in algorithm  [ alg1 ] .",
    "we neglect any communication cost here .",
    "also , tasks @xmath34 will be considered as _",
    "elementary _ : at most one processing unit can execute such a task at a given time ( no divisible load ) .",
    "the dependencies between the tasks are given by :    * @xmath35 ; * @xmath36 ; * @xmath37 ; * @xmath38 ; * @xmath39 ; * @xmath40 ; * @xmath41 ; * @xmath42 .    figure [ dag55 ] presents the directed acyclic graph ( dag ) of the dependencies between the tasks of a @xmath43 tiled cholesky factorization .",
    "cholesky factorization , width=288 ]    for a task @xmath44 , @xmath45 will denote the critical path of task @xmath44 , and @xmath46 its weight .",
    "the number of tasks of each kind is given in table [ numbertasks ] .",
    ".number of tasks [ cols=\"^,^\",options=\"header \" , ]     in figure  [ alapbounds ] , we plot the exact distribution of the execution of the tasks , and the upper and lower bound functions . we note that our lower and upper bounds are assymptotically close to the function . from this figure",
    ", we see that for @xmath47 tiles , we have a schedule that executes in @xmath1 ( = 530 ) for 907 processing units .",
    "our upper bound ( which is simpler to analyze ) guarantees that we need at most 913 processing units .     tiles . in black",
    "is the exact distribution .",
    "red is the upper bound .",
    "blue is the lower bound.,width=288 ]    note that the maximum height of the distribution ( that is the minimum number of processing units required to run the alap schedule optimally ) is reached in _",
    "zone 3 _ , _ i.e. _ where @xmath48 .",
    "that phenomenon is verified experimentally .",
    "it can also be deduced from the fact that the height in the two other zones is essentially a degree 2 polynomial in @xmath49 with positive leading coefficient ( which comes from the number of g executed ) ; therefore by convexity the maximum height there is reached at the extremities of the zones .",
    "the result follows by observing that the height in the third zone is increasing at its beginning .    from that",
    ", we obtain the maximum height of the distribution , which gives the minimal number of processing units required to run the alap schedule :    @xmath50    we deduce an upper bound on the number of processing units required to reach the critical path of the cholesky factorization :    [ th:1 ] the alap schedule of a @xmath16 tiled cholesky factorization completes in the critical path time ( @xmath1 ) using less than @xmath51 processing units .",
    "we note that this results is much better than what an asap schedule would give .",
    "the asap schedule of a @xmath16 tiled cholesky factorization completes in the critical path time ( @xmath1 ) critical path using @xmath52 processing units .",
    "the analysis is easy and is based on the fact that , with an asap schedule , after the first potrf and the first @xmath53 trsm , one has @xmath54 syrk and gemm tasks to execute .",
    "we now consider lower bounds on the makespan of the cholesky factorization . to that purpose",
    ", we split the task set into two parts , and combine lower bounds on the execution time of both parts .",
    "it is well known and clear that the makespan of any schedule working on a given algorithm with @xmath7 processing units is greater than @xmath55 ( where @xmath56 is the total work necessary to finish the algorithm , @xmath57 its critical path ) .",
    "indeed , @xmath58 corresponds to a schedule that achieves full parallelism during its whole execution with p processing units ; and no schedule can execute the algorithm faster than @xmath57 .",
    "the idea here will be to combine those two bounds : let @xmath49 be a critical path parameter .",
    "let @xmath59 be the set of tasks @xmath44 such that @xmath60 , and @xmath61 be its complement .",
    "note that in the general case , @xmath59 contains some tasks with critical path @xmath62 .",
    "indeed , any task of @xmath59 without parents in @xmath59 necessarily has critical path @xmath62 ( else any of its parents would have been in @xmath59 too ) .",
    "also , as long as @xmath61 and @xmath59 are not empty , some task in @xmath61 has a child in @xmath59 ( the two sets can not be disconnected ) .",
    "note also that all the tasks in @xmath61 have critical path @xmath63 .",
    "define for a set @xmath64 of tasks , the total weight of @xmath64 @xmath65 the sum of the weights of the tasks in @xmath64 .",
    "the basic idea is that one will essentially require time @xmath66 to execute @xmath61 and time @xmath62 to execute @xmath59 .",
    "this results in the following claim to prove our lower bound :    let @xmath49 be a critical path parameter .",
    "let @xmath59 be the set of tasks @xmath44 such that @xmath60 , and @xmath61 be its complement .",
    "then the makespan of any schedule for using @xmath7 processing units is greater than @xmath67    suppose by contradiction that there exists a schedule executing the algorithm in time @xmath68 . if @xmath69 , then the schedule can not execute @xmath59 ( defined above ) in time @xmath70 , as @xmath59 contains some tasks with critical path @xmath62 , which gives a contradiction .",
    "therefore @xmath71 .",
    "then , let us write @xmath72 .",
    "then @xmath73 by hypothesis .",
    "so after time @xmath74 , @xmath61 can not be fully executed , according to the naive bound .",
    "therefore , there is a task from @xmath61 that is not fully completed at time @xmath75 , and the remaining tasks can not be executed in time @xmath76 ( as if @xmath77 , @xmath78 by definition of @xmath61 , therefore @xmath44 has a son with critical path @xmath62 ) , contradiction .    a remark here : why not simply take @xmath79 as the set of tasks that have critical path @xmath80 , and @xmath81 the set of tasks that have critical path @xmath82 ?",
    "because then the argument would be erroneous , as tasks @xmath44 with @xmath83 and @xmath84 need not to be fully completed when starting the execution of @xmath81 ( for instance , their execution could be half - done ) . as a result",
    ", we remove these tasks from @xmath79 and put them in @xmath81 , obtaining the previous definition .",
    "to use this claim , we need an expression of the work @xmath85 for a some parameter @xmath49 ; we reduce this problem to computing @xmath86 , as @xmath87 .",
    "note that the problem is very similar to the calculation of the height of the alap schedule in section [ sec : alap ] , but this time , we want all the tasks with critical path lower than the ones counting for the height of the alap . as a result",
    ", we name this quantity the _ cumulative distribution _ of the tasks .    in order to simplify the calculations and the results ,",
    "we focus on the gemms tasks only , which forms the prominent set of tasks of the cholesky factorization , both in terms of number ( there are @xmath88 gemms ) , and in terms of work ( they gather work @xmath89 ) which asymptotically respectively are the global number of tasks , and the total work of the algorithm .    thus , we count , for a given critical path parameter @xmath49 , the number @xmath90 of gemm tasks @xmath44 such that : @xmath91    recall that we assumed that the gemms have a weight of 6 ( table  [ weights ] ) and that we proved that the critical path of the task @xmath32 is @xmath92 ( table  [ cps ] ) .",
    "note then that similarly to counting the distribution in section  [ sec : alap ] , the integer set @xmath93 is convex , hence some @xmath94 and @xmath95 such that @xmath96 .",
    "also , note that for any @xmath97 , the set @xmath98 is also convex , hence some @xmath99 and @xmath100 .",
    "note that the formula @xmath101 immediately gives @xmath102 and @xmath103 .",
    "remark that @xmath94 is determined by the exact same equation as the one considered in section [ sec : alap ] , so that @xmath104 .",
    "and an easy calculation leads to @xmath105 , as @xmath99 has to be positive .",
    "so , for fixed j , there are @xmath106 distinct couples @xmath107 that satisfy equation  ( [ eqlowerbound ] ) . and for each admissible couple @xmath107 , every @xmath108 satisfy equation  ( [ eqlowerbound ] ) .",
    "we obtain that the cumulative distribution of the gemms is given by :    @xmath109    the previous claim then gives lower bounds on the execution time for a fixed number @xmath7 of processing units . but as we have only considered the gemms tasks , we have to slightly modify our statement .    under the same assumptions as in the previous claim ,",
    "the makespan of any schedule for using @xmath7 processing units is greater than @xmath110 where @xmath111 denotes the total work of the gemms in @xmath61 .",
    "note that this claim gives a worse bound than the previous one , as @xmath112 .",
    "also , the previous proof still stands : it suffices to replace @xmath85 by @xmath113 ( while keeping the same @xmath59 ) .    to use this new result ,",
    "recall that the total work from the gemms is @xmath114 .",
    "then , for a fixed parameter @xmath49 , we have @xmath115 .    with that in hand , every parameter @xmath49 gives a lower bound on the makespan of any schedule using @xmath7 processing units .",
    "for instance , we find that as long as @xmath116 , that is as long as @xmath117 , and with @xmath118 processing units , the number of tasks in @xmath59 is : @xmath119    then , the lower bound associated to @xmath49 is : @xmath120    at this point , we want to pick the best lower bound possible among all the parameters @xmath49 .",
    "let us name it @xmath121 for instance .    experimentally , @xmath122 , so that the maximum is reached where @xmath123 . also , asymptotically ( that is when @xmath124 ) , the maximum is reached for @xmath125 , which is @xmath126 as long as @xmath127 .",
    "we can assume this condition , as we will see that our bound is only relevant for @xmath128 : otherwise the naive bound is better than ours .",
    "this gives us a correct lower bound , even if in practice the maximum is not reached at the exact same point as in the asymptotical case ; but due to the complexity of the formula with the exact maximum , we first simplify as : any schedule working with @xmath118 processing units on the @xmath129 tiled cholesky factorization has an execution time greater than : @xmath130 and , with some more simplifications , we get the following theorem .    any schedule working with @xmath7 processing units on the @xmath129 tiled cholesky factorization has an execution time greater than : @xmath8    as @xmath131 , the alap height in @xmath59 is a non - decreasing function of @xmath49 .",
    "thus , the maximum alap height of @xmath59 is located at @xmath132 .",
    "it turns out that the maximum alap height in @xmath59 ( obtained with tables [ lowerheightalap ] , [ upperheightalap ] in section [ sec : alap ] ) is asymptotically equal to the number of processing units available .",
    "in particular , we can assume @xmath133 for some fixed parameter @xmath134 , for this bound to be a non - negligible improvement over the naive one ( that is we assume the size of @xmath59 is not negligible ) .",
    "then , this result improves the naive bound with a term @xmath135 , that does not depend on the size @xmath0 of the problem , which is quite a surprising result at first glance .",
    "let us propose an explanation for this phenomenon .",
    "as @xmath131 , the number of tasks in @xmath59 given in equation  [ sizee2 ] only depends on @xmath121 , not on @xmath0 . as the gain from the naive bound comes from the fact that @xmath59 can not be well parallelized ( as the limiting factor",
    "is considered to be the critical path there ) , this results in a gain which asymptotically does not depend on @xmath0 ( the negligible terms are due to the fact that we only considered gemms here ) .",
    "also , with the naive bound , we knew that at least @xmath136 processing units were necessary to reach the critical path . with our new bound , we know that we need at least @xmath137 processing units to reach it .",
    "in other words , the naive bound is better when @xmath138 , which justifies the previous assumption .",
    "the bound exhibited in the previous section improves the previously known ones .",
    "this raises a question : can we still improve it ? that is , is the bound tight ?",
    "here we analyze different schedules and compare their performance with the upper bound on performance that we derive from our lower bound on time .    in this section ,",
    "we study some existing schedules . since our execution model is theoretical ( assuming that potrfs take 1 unit of time , trsms and syrks 3 units , and gemms 6 ) we simulate these schedules with the same assumptions in order to build a consistent framework for comparison .",
    "therefore this is a theoretical study .",
    "we focus on three different schedules :    * the right - looking cholesky algorithm with multithreaded blas , this schedule is implemented in lapack for example ; * a schedule from kurzak et al . described in  @xcite ; * the alap schedule mentioned earlier with a list scheduling .",
    "the lapack schedule is the right - looking cholesky algorithm with multithreaded blas .",
    "this boils down to synchronizing all processing units at the end of every loop in the cholesky factorization ( algorithm [ alg1 ] , section [ sec : assumptions ] ) .",
    "more precisely , one processing unit executes the potrf , while all the other ones wait . when it has finished , they all execute trsms if some are available , and wait otherwise . then , they execute the syrks and the gemms the same way . as a result ,",
    "the lapack schedule suffers from huge synchronization needs , and therefore should result in quite poor performance overall .",
    "this is bulk synchronous parallelism or the fork - join approach .",
    "the schedule described in  @xcite is a variant of the left - looking cholesky factorization : it assigns rows to the processing units , which execute their assigned tasks as soon as possible .",
    "we also consider a schedule based on the alap heuristic .",
    "therefore we schedule the task from the end to the start using a list schedule and priority policy based on the ( alap ) critical path of a task .",
    "a comparison of the speedups is plotted in figure  [ fig1 ] for a cholesky factorization with 40 tiles .",
    "the lapack schedule shows quite poor performance as expected , and the two others schedule performs much better .",
    "however , the gap between their performance curve and the upper bound is quite close .",
    "the horizontal black curve represent the critical path bound .",
    "no schedule can execute faster than the critical path . for @xmath139 ,",
    "the critical path is @xmath140 .",
    "we see that the green curve reaches the critical path at @xmath141 processing units .",
    "this means that any schedule which completes in the critical path time has to have at least @xmath141 processing units .",
    "we see that the red curve reaches the critical path at @xmath142 processing units .",
    "this means that the alap schedule completes in the critical path time with @xmath142 processing units .",
    "we have analyzed the tiled cholesky factorization and improved existing lower bounds on the execution time of any schedule , with a technique that benefits from the structure of dependence graph of the tiled cholesky factorization .",
    "we took advantage of our observation that the tiled cholesky factorization is much better behaved when we schedule it with an alap ( as late as possible ) heuristic than an asap ( as soon as possible ) heuristic .",
    "we believe that our theoretical results will help practical scheduling studies of the tiled cholesky factorization . indeed",
    ", our results enable to better characterize the quality of a practical schedule with respect to an optimal schedule .",
    "we also believe that our technique is generalizable to many tile algorithms , in particular lu and qr .",
    "it is clear that many linear algebra operations would benefit from the alap scheduling strategy .",
    "also , we can easily change the weight of the tasks in our study to better represent the time of the kernels ( as opposed to the number of flops of the kernels ) on a given architecture .",
    "there are two questions left open by our work .",
    "first we do not have satisfying closed form formula for schedules on @xmath7 processors .",
    "indeed , in section  [ sec : tightness ] , we relied on simulation to plot the speedup of an alap schedule and of the schedule from kurzak et al .",
    "we do not have closed form formula for these .",
    "also , while we made significant progress , there is still a gap between our lower and upper bounds and it seems worthwhile to further our work and close this gap in an asymptotic sense .",
    "j.  kurzak , a.  buttari , and j.  dongarra , `` solving systems of linear equations on the cell processor using cholesky factorization , '' _ ieee trans .",
    "parallel distrib .",
    "_ , vol .  19 , no .  9 , pp . 11751186 , 2008 .",
    "e.  chan , f.  g. van  zee , p.  bientinesi , e.  s. quintana - ort , g.  quintana - ort , and r.  van  de geijn , `` supermatrix : a multithreaded runtime scheduling system for algorithms - by - blocks , '' in _",
    "ppopp 08 : proceedings of the 13th acm sigplan symposium on principles and practice of parallel programming_.1em plus 0.5em minus 0.4emnew york ,",
    "ny , usa : acm , 2008 , pp .",
    "123132 .",
    "e.  agullo , b.  hadri , h.  ltaief , and j.  dongarra , `` comparative study of one - sided factorizations with multiple software packages on multi - core hardware , '' in _ sc09 .",
    "acm / ieee conference on supercomputing , portland , or , november _ , 2009 .",
    "f.  song , a.  yarkhan , and j.  dongarra , `` dynamic task scheduling for linear algebra algorithms on distributed - memory multicore systems , '' in _ sc09 .",
    "acm / ieee conference on supercomputing , portland , or , november _ , 2009 .",
    "e.  agullo , o.  beaumont , l.  eyraud - dubois , j.  herrmann , s.  kumar , l.  marchal , and s.  thibault , `` bridging the gap between performance and bounds of cholesky factorization on heterogeneous platforms , '' in _ heterogeneity in computing workshop 2015 _ , 2015 .",
    "j.  kurzak , h.  ltaief , j.  dongarra , and r.  m. badia , `` scheduling dense linear algebra operations on multicore processors . '' _ concurrency and computation : practice and experience _ , vol .",
    "22 , no .  1 ,",
    "pp . 1544 , 2010 .",
    "r.  m. badia , j.  r. herrero , j.  labarta , j.  m. prez , e.  s. quintana - ort , and g.  quintana - ort , `` parallelizing dense and banded linear algebra libraries using smpss , '' _ concurrency and computation : practice and experience _ , vol .",
    "21 , no .",
    "18 , pp . 24382456 , 2009 .",
    "g.  bosilca , a.  bouteiller , a.  danalis , t.  herault , p.  lemarinier , and j.  dongarra , `` dague : a generic distributed dag engine for high performance computing , '' _ parallel computing _ , vol .",
    "( 1 - 2 ) , pp . 3751 , 2012 .        c.  augonnet , s.  thibault , r.  namyst , and p .- a .",
    "wacrenier , `` starpu : a unified platform for task scheduling on heterogeneous multicore architectures , '' _ concurrency and computation : practice and experience , special issue : euro - par 2009 _ , vol .",
    "187198 , feb .",
    "j.  m. prez , r.  m. badia , and j.  labarta , `` a flexible and portable programming model for smp and multi - cores , '' barcelona supercomputing center ",
    "centro nacional de supercomputacio , tech . rep . ,",
    "jun . 2007 ."
  ],
  "abstract_text": [
    "<S> due to the advent of multicore architectures and massive parallelism , the tiled cholesky factorization algorithm has recently received plenty of attention and is often referenced by practitioners as a case study . </S>",
    "<S> it is also implemented in mainstream dense linear algebra libraries . </S>",
    "<S> however , we note that theoretical study of the parallelism of this algorithm is currently lacking . in this paper , we present new theoretical results about the tiled cholesky factorization in the context of a parallel homogeneous model without communication costs . </S>",
    "<S> we use standard flop - based weights for the tasks . for a @xmath0-by-@xmath0 matrix , we know that the critical path of the tiled cholesky algorithm is @xmath1 and that the weight of all tasks is @xmath2 . in this context </S>",
    "<S> , we prove that no schedule with less than @xmath3 processing units can finish in a time less than the critical path . in perspective , a naive bound gives @xmath4 we then give a schedule which needs less than @xmath5 processing units to complete in the time of the critical path . in perspective </S>",
    "<S> , a naive schedule gives @xmath6 in addition , given a fixed number of processing units , @xmath7 , we give a lower bound on the execution time as follows : @xmath8 the interest of the latter formula lies in the middle term . </S>",
    "<S> our results stem from the observation that the tiled cholesky factorization is much better behaved when we schedule it with an alap ( as late as possible ) heuristic than an asap ( as soon as possible ) heuristic . </S>",
    "<S> we also provide scheduling heuristics which match closely the lower bound on execution time . </S>",
    "<S> we believe that our theoretical results will help practical scheduling studies . indeed , our results enable to better characterize the quality of a practical schedule with respect to an optimal schedule .    </S>",
    "<S> scheduling ; cholesky factorization ; makespan lower bound ; </S>"
  ]
}