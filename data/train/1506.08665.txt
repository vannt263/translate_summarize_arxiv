{
  "article_text": [
    "in many fields of science the computation of the action of the matrix exponential is of great importance . as one example amongst others we highlight exponential integrators .",
    "these methods constitute a competitive tool for the numerical solution of stiff and highly oscillatory problems , see @xcite .",
    "their efficient implementation heavily relies on the fast computation of the action of certain matrix functions among those the matrix exponential is the most prominent one .    given a square matrix @xmath1 and a vector @xmath2 the action of the matrix exponential",
    "is denoted by @xmath3 . in general , the exponential of a sparse matrix @xmath1 is a full matrix .",
    "therefore , it is not appropriate to form @xmath4 and multiply by @xmath2 for large scale matrices .",
    "the aim of this paper is to define a backward stable method to compute the action of the matrix exponential based on the leja interpolation .",
    "the performed backward error analysis allows one to predict and reduce the cost of the algorithm resulting in a more robust and efficient method .    for a given matrix @xmath1 and vector @xmath2 ,",
    "one chooses a positive integer @xmath5 so that the exponential @xmath6 can be well approximated . due to the functional equation of the exponential we",
    "can then exploit the relation @xmath7 this results in an recursive approximation of @xmath8 by @xmath9 there are various possibilities to compute the stages @xmath10 .",
    "usually , this computation is based on interpolation techniques .",
    "the best studied methods comprise krylov subspace methods ( see @xcite and @xcite ) , truncated taylor series expansion @xcite , and interpolation at leja points ( see @xcite ) . in this paper",
    "we take a closer look on the leja method ( cf .   and below ) for approximating @xmath11 .",
    "below we present two different ways of performing a backward error analysis of the leja method .",
    "our analysis indicates how the scaling parameter @xmath5 , the degree of interpolation @xmath12 and the interpolation interval @xmath13 $ ] can be selected in order to obtain an appropriately bounded backward error by still keeping the cost of the algorithm at a minimum .",
    "furthermore , we discuss how the method benefits from a shift of the matrix and we show how an early termination of the leja interpolation can help to reduce the cost in an actual computation . as a last step",
    "we illustrate the stability and behavior of the method by some numerical experiments .",
    "the paper is structured in the following way . in section  [ sec : be ] we introduce the backward error analysis and draw some conclusions from it . in particular",
    "we show how this analysis helps us to select the parameters @xmath14 , and @xmath15 . in section  [ sec : petc ] we discuss some additional aspects for a successful implementation based on the leja method .",
    "section  [ sec : numexp ] presents some numerical examples dealing with different features and benchmarks for the method . in section  [ sec : discussion ]",
    "we finally give a discussion of the presented results .    for a reader not familiar with the leja method we included a brief description in section  [ sec : leja ] .",
    "like every polynomial interpolation , the leja method essentially depends on the interpolation interval and the position and number of interpolation points .",
    "the choice of these parameters directly influences the error of the interpolation and the cost . in this section",
    "we introduce the leja method based on a sequence of leja points in a real interval . the extension to a symmetrized complex sequence of points",
    "can be found in section  [ sec : complexleja ] .    given an interval @xmath16 $ ] , the leja points are commonly defined as @xmath17 } \\prod_{j=0}^{m-1}|\\zeta-\\zeta_j| , \\quad m\\geq1,\\quad \\zeta_0\\in{\\mathop{\\operatorname{arg\\,max}}}_{\\zeta\\in [ a , b ] } |\\zeta|.\\end{aligned}\\ ] ] the interpolation polynomial of the exponential function is then given by @xmath18)=\\sum_{j=0}^m \\exp[\\zeta_0,\\ldots , \\zeta_j ] \\prod_{i=0}^{j-1}\\left(x-\\zeta_i\\right),\\end{aligned}\\ ] ] where @xmath19 $ ] denotes the @xmath20th divided difference .",
    "the scalar interpolation can be extended to the matrix case and rewritten into a two term recurrence relation for the actual computation , see @xcite . due to the functional equation of the exponential",
    "it is always possible to shift the argument and perform the interpolation on a symmetric interval with zero as its center .",
    "this allows us to optimize the algorithm for symmetric intervals .",
    "let @xmath21 be the leja points in @xmath16 $ ] and @xmath22 the leja points in the symmetric interval @xmath13 $ ] with same length .",
    "then the relation @xmath23 with @xmath24 and @xmath25 is valid . in practice",
    ", we use precomputed points on the interval @xmath26 $ ] and scale them to @xmath13 $ ] . due to the functional equation of the exponential function",
    "the shift can be singled out of the divided differences and we get @xmath27)&=\\sum_{j=0}^m \\exp[\\zeta_0,\\ldots , \\zeta_j ] \\prod_{i=0}^{j-1}\\left(x-\\zeta_i\\right ) \\\\&=      \\sum_{j=0}^m { \\mathrm{e}}^{\\ell}\\exp[\\xi_0,\\ldots , \\xi_j ] \\prod_{i=0}^{j-1}\\left((x-\\ell)-\\xi_i\\right)\\\\      & = { \\mathrm{e}}^{\\ell}l_m(x-\\ell;[-{c},{c}]).\\end{aligned}\\ ] ] therefore , it is always possible to interpolate on a symmetric interval around zero and apply the appropriate shifts to the argument and solution , respectively . in the following",
    "we will always select @xmath28 and consequently we get @xmath29 and @xmath30 .",
    "we denote the leja interpolation polynomial of degree @xmath12 on the interval @xmath13 $ ] interpolating the exponential by @xmath31).\\end{aligned}\\ ] ] note that it is not necessary to shift the matrix in order to use a symmetric interval .",
    "nevertheless , a well chosen shift can lead to faster convergence and can help to avoid round - off and overflow errors .    in order to determine a possible shift we define a rectangle @xmath32+\\mathrm{i}[{\\eta},{\\beta}]$ ] in the complex plane .",
    "we do this by splitting up the matrix into its hermitian part @xmath33 and skew hermitian part @xmath34 .",
    "furthermore , we find bounds for the field of values and the eigenvalues of these matrices with the help of gerschgorin s disk theorem , i.e. @xmath35 the four real numbers @xmath36 , and @xmath37 are chosen to satisfy @xmath38 \\qquad\\text{and}\\qquad \\sigma(a_{\\mathrm{sh}})\\subseteq\\mathrm{i}[{\\eta},{\\beta}].\\end{aligned}\\ ] ] we note that in former versions of the leja method @xmath39 was always assumed nonpositive and @xmath40 .",
    "these restrictions are no longer required here . in this sense",
    ", the method is now more general than previous versions . with the help of the rectangle @xmath41",
    "the interpolation interval was chosen in @xcite as the focal interval of the ellipse with smallest capacity circumscribing @xmath41 . here",
    "@xmath41 is used to determine the type of interpolation ( real or complex conjugate leja points ) and a possible shift @xmath42 , see section  [ sec : shift ] .",
    "we further note that , as stated in @xcite , the leja ordering is of great importance for the stability of the method .",
    "@xcite suggests the interpolation interval @xmath26 $ ] .",
    "the length of the interpolation interval does not influence the numerical accuracy .",
    "@xcite suggests @xmath26 $ ] only in order to avoid over- and underflow problems which may arise for large interpolation intervals and/or with very large values of the interpolation degree . in this version of the method we deviate from this choice",
    "this is admissible since the largest interpolation interval and the largest used degree do not give rise to over- or underflow problems .",
    "this section is devoted to the backward error of the action of the matrix exponential when approximated by the leja method .",
    "we first focus on the interpolation in a real interval , see section  [ sec : complexleja ] for the extension to the complex case .",
    "the concept of backward error analysis goes back to wilkinson , see @xcite .",
    "the underlying idea is to interpret the result of the interpolation as the exact solution of a perturbed problem @xmath43 .",
    "the perturbation @xmath44 is the absolute backward error and we aim to satisfy @xmath45 for a user given tolerance @xmath46 .",
    "the here presented backward error analysis exploits a variation of the analysis given in @xcite . for this",
    ", we define the set @xmath47 where @xmath48 denotes the spectral radius and @xmath49 is the leja interpolation polynomial of degree @xmath12 on the symmetric interval @xmath13 $ ] , see and .",
    "note that @xmath50 is open in @xmath51 and contains a neighborhood of @xmath52 for @xmath53 , since @xmath30 .",
    "for @xmath54 we define the function @xmath55 here @xmath56 denotes the principal logarithm ( * ? ? ?",
    "* thm .  1.31 ) .",
    "as @xmath57 commutes with @xmath58 we get @xmath59 for @xmath54 . by introducing a scaling factor @xmath5 such that @xmath60 for @xmath61 we obtain @xmath62 where @xmath63 is the backward error resulting from the approximation of @xmath64 by the leja method @xmath65 .    on the set @xmath50 the function",
    "@xmath66 has a series expansion of the form @xmath67 in order to bound the backward error by a specified tolerance @xmath46 we need to ensure @xmath68 for a given matrix norm . as a consequence of this bound",
    ", one can select the scaling factor @xmath5 ( always a positive integer ) such that is satisfied for a chosen degree of interpolation @xmath12 .",
    "in contrast to @xcite we have the endpoint @xmath15 of the interpolation interval as an additional parameter to @xmath12 and @xmath5 for our analysis . in the following",
    ", we are going to introduce two different ways of bounding the backward error .",
    "the first one is closely related to the analysis presented in @xcite .",
    "we study how can be used to select the interpolation parameters when we perform a power - series expansion of the backward error . in the second approach",
    "we consider a contour integral formulation of @xmath66 and estimate the error along the contour .      in this section",
    "we investigate bounds on the backward error represented by @xmath66 .",
    "the analysis is based on a power - series expansion of @xmath66 .",
    "starting from we can bound @xmath57 by @xmath69 by inserting this estimate into we get @xmath70 since zero is among the interpolation points for @xmath71 we get @xmath72 this is a monotonically increasing function for @xmath73 .",
    "furthermore , for @xmath74 , the leja interpolation reduces to the truncated taylor series at zero .",
    "we thus have @xmath75 for @xmath76 .",
    "the equation @xmath77 therefore has a unique positive root for @xmath15 sufficiently small .",
    "henceforth , we will call this root @xmath78 .",
    "the number @xmath78 can be interpreted in the following way : for the interpolation of degree @xmath12 in @xmath13 $ ] the backward error fulfills @xmath79 , if the positive integer @xmath5 fulfills @xmath80 . in other words , if the norm of a matrix is smaller than @xmath78 , the interpolation of degree @xmath12 with points in @xmath13 $ ] has an error less than or equal to @xmath46 .    in the analysis up to now",
    ", we only used that zero is among the interpolation points .",
    "however , the following discussion requires the sequence of leja points . in order to compute @xmath81 in a stable manner",
    "we expand @xmath66 in the newton basis for leja points in @xmath82 $ ] as @xmath83\\prod_{j=0}^{k-1}(x-\\xi_j i),\\end{aligned}\\ ] ] where @xmath84 $ ] denotes the @xmath85th divided difference .",
    "the above series starts with @xmath86 as @xmath87 for @xmath88 . rewriting this series in the monomial basis we obtain with the according coefficients @xmath89 . in order to get reliable results for these coefficients we use @xmath90 digits in the actual computation .",
    "figure  [ fig : thetavsbeta ] displays the path of @xmath78 for the leja interpolation with respect to @xmath15 for fixed @xmath12 up to @xmath91 and @xmath92 .",
    "for an actual computation one has to truncate the series at some index @xmath93",
    ". we always used @xmath94 .",
    "the root @xmath95 as a function of the right endpoint @xmath15 of the interpolation interval for real leja points in @xmath13 $ ] .",
    "the tolerance is set to @xmath92 .",
    "along each line the interpolation degree @xmath12 is kept fixed . ]    as we now have a way of bounding the backward error we discuss the choice of the number of scaling steps in .",
    "we propose to select the integer @xmath5 depending on @xmath12 and @xmath15 in such a way that the cost of the algorithm becomes minimal .",
    "we have the limitation that @xmath12 is bounded by @xmath91 to avoid problems with over- and underflow .",
    "however , we get several possibilities to select the free parameter @xmath15 describing the interval .",
    "the value @xmath96 corresponds to the truncated taylor series expansion as described in @xcite .",
    "a second possibility is to choose , for a fixed @xmath12 , @xmath15 in such a way that @xmath78 is maximal .",
    "this corresponds to the interpolation interval that admits the largest norm of @xmath1 .",
    "a third possibility is to select the interpolation interval such that the right endpoint @xmath15 coincides with @xmath78 .",
    "these are the points on the diagonal in figure  [ fig : thetavsbeta ] .",
    "a priori none of the above choices can be seen to be optimal for an arbitrary matrix . the choice @xmath74 together with the smallest @xmath12 such that @xmath97 is a good choice for a matrix @xmath1 with all the eigenvalues clustered in a neighborhood of @xmath52",
    ". on the other hand , if the convex hull of the eigenvalues of a matrix @xmath1 , with @xmath98 , is the interval @xmath99 $ ] , the choice @xmath100 with smallest @xmath12 such that @xmath101 is preferable .",
    "we choose @xmath78 according to the third option , which favors normal matrices where all eigenvalues lie in an interval .",
    "more precisely , we select @xmath102 this means that @xmath103 is the first intersection point of the graph @xmath104 with the diagonal , cf .",
    "figure  [ fig : thetavsbeta ] .",
    "the behavior of the curves @xmath105 is not unexpected .",
    "let us consider the approximation of @xmath106 by @xmath107 for @xmath108 and @xmath109 .",
    "then @xmath110 is an overestimate of the relative backward error @xmath111 .",
    "the value @xmath78 represents the maximum value for which @xmath112 is an acceptable approximation of @xmath113 .",
    "the value @xmath96 corresponds to interpolation at a set of confluent points at @xmath74 , i.e.  the truncated taylor series approximation .",
    "if we slightly increase the interpolation interval @xmath13 $ ] , about half of the interpolation points lie in @xmath114 $ ] .",
    "therefore , it is possible to have an acceptable interpolation up to @xmath115 . if we continue to increase @xmath15 , the mutual distance between interpolation points increases as well .",
    "when the interval gets too large , the number of interpolation points is too small to achieve the desired accuracy and the value @xmath78 starts to decrease .",
    "figure  [ fig : thetavsbeta ] shows that @xmath116 for all @xmath117 .",
    "therefore , we can safely interpolate with degree @xmath12 for all intervals @xmath13 $ ] with @xmath118 .",
    "we will use this fact in section  [ sec : hump ] .",
    "we compute @xmath103 by a combination of two root finding algorithms based on newton s method .",
    "the inner equation for computing @xmath78 is solved by an exact newton iteration with an accuracy of @xmath119 .",
    "the outer equation @xmath120 is also solved by newton s method .",
    "this time , however , the necessary derivative is approximated by numerical differentiation .",
    "we compute the result up to an accuracy of @xmath121 .",
    "the resulting values are truncated to 16 digits ( double precision ) and used henceforth as the @xmath103 values . in table",
    "[ tab : thetam ] we listed selected ( rounded ) values of @xmath103 for various @xmath12 and certain tolerances .",
    ".[tab : thetam]samples of the ( rounded ) values @xmath103 for tolerances _ half _ ( @xmath122 ) , _ single _ ( @xmath123 and _ double _ ( @xmath92 ) for the real leja interpolation . [ cols= \" > , > , > , > , > , > , > , > \" , ]      the presented backward error analysis extends in a straightforward way to the @xmath0 functions which play an important role in exponential integrators , see @xcite .",
    "we illustrate this here for the @xmath124 function . for @xmath125 and @xmath126",
    "we observe that @xmath127\\exp\\left ( \\begin{bmatrix}a & w\\\\0&0 \\end{bmatrix }      \\right ) \\begin{bmatrix}0\\\\1\\end{bmatrix},\\end{aligned}\\ ] ] see @xcite . for the choice @xmath128",
    "the backward error is preserving the structure , i.e. @xmath129 thus the above analysis applies . for general @xmath0 functions we can extend our approach with the help of ( * ? ? ?",
    "* thm .  2.1 ) .",
    "by using the previously described backward error analysis to compute the values @xmath130 , @xmath131 and @xmath132 a working algorithm can be defined .",
    "nevertheless , the performance of the algorithm can be significantly improved by some preprocessing and by introducing an early termination criterion .",
    "moreover , interpolation in nonexact arithmetic will suffer from roundoff errors , in particular in combination with the hump phenomenon .",
    "we address all these issues in this section .      in the above backward error analysis",
    ", it was assumed that the rectangle @xmath41 lies symmetrically about the origin . in general , this requires a shift of @xmath1 . on the other hand ,",
    "it is clear that a well chosen shift @xmath133 satisfying @xmath134 is beneficial for the interpolation ( a lower degree or less scaling steps will be required ) . for the exponential function",
    "such a shift can easily be compensated by scaling .",
    "more precisely , if the shift @xmath133 is selected , we use @xmath135^{s}\\end{aligned}\\ ] ] as approximation of @xmath4 .    for our algorithm",
    "a straightforward shift is to center the rectangle @xmath41 at the origin , namely @xmath136 if @xmath1 is real then @xmath137 and @xmath138 .",
    "therefore , a complex shift is only applied to complex matrices .",
    "it is easy to see that for a hermitian or skew hermitian matrix @xmath1 the proposed shift coincides with the norm - minimizing shift presented in ( * ? ? ?",
    "4.21(b ) ) .",
    "for a general matrix , the shift somewhat symmetrizes the spectrum of the matrix with regard to its estimated field of values .",
    "the shift @xmath139 used in @xcite is a transformation that centers the spectrum of the matrix around the average eigenvalue . in many cases the two shifts are similar",
    "nevertheless , it is possible to find examples where one shift leads to better results than the other .",
    "the matrix ` one - sided ` of example  [ eg : longtimestep ] is one of these cases . for the trace shift a symmetrization of the rectangle @xmath41",
    "might be required , resulting in a potential increase of scaling steps for the estimate based on . for the method proposed here",
    ", we always use as shift .",
    "the estimates based on and are worst case estimates and in particular do not take @xmath2 into account . as a result ,",
    "the choice of @xmath130 is likely to be an overestimate and can be reduced in the actual computation . by limiting @xmath12 in the computation of @xmath140 in",
    "we introduce a relative forward error that again should be bounded by the tolerance @xmath46 .",
    "we propose to take @xmath141\\right|\\left\\| \\prod_{j=0}^{k-1}({s}^{-1}a-\\xi_j i)v^{(i)}\\right\\|\\leq \\frac{{\\mathrm{tol}}}{{s } } \\|l_{k,{c}}({s}^{-1}a)v^{(i)}\\|\\end{aligned}\\ ] ] as an a posteriori error estimate for the leja method in the @xmath85th step .",
    "experiments show that turns out to be a good choice .",
    "in contrast to the method described in @xcite we divide the tolerance by the amount of scaling steps .",
    "this potentially increases the number of iterations per step but in practice results in a more stable computation for normal matrices , see section  [ sec : numexp ] .",
    "nevertheless , it sometimes leads to results of higher accuracy than prescribed . in practice",
    "it is advisable to take the sum of two or three successive approximation steps for the estimate as this captures the behavior better . on the other hand",
    ", it can also be beneficial to make the error estimate only every couple of iterations rather than in each step to save computational cost , see @xcite .",
    "a second approach for an a posteriori error estimate for the leja method based on the computation of a residual can be found in @xcite .",
    "this procedure can also be used here .",
    "furthermore , it is possible to adapt the early termination criterion to complex conjugate leja points . with the help of an early termination criterion computational cost",
    "can be saved for certain matrices , see section  [ sec : numexp ] .",
    "in general , the interpolation error does not decrease monotonically with the degree of interpolation .",
    "even worse , a distinct hump can occur in certain situations , see figure  [ fig : interror ] .",
    "this hump can significantly reduce the accuracy of the interpolation due to roundoff errors .",
    "the phenomenon is linked to the distribution of the eigenvalues of a matrix with respect to interpolation interval .",
    "note that the hump we are describing here is not the same as the one described in @xcite for nonnormal matrices .",
    "figure  [ fig : interror ] illustrates the problem for the matrix @xmath142 and vector @xmath143^\\mathrm{t}$ ] .",
    "illustration of the hump phenomenon for the real and complex case .",
    "the used matrix @xmath142 and @xmath143^\\mathrm{t}$ ] . ]     +    illustration of the hump phenomenon for the real and complex case . the used matrix @xmath142 and @xmath143^\\mathrm{t}$ ] . ]",
    "figure  [ fig : interror_a ] shows the real case . for @xmath74 ( i.e.  the truncated taylor series method ) the final error is low and no hump is formed .",
    "if we increase the interpolation interval @xmath13 $ ] , we observe that the necessary degree of the interpolation gradually decreases , while the final error stays approximately constant .",
    "optimal _ interpolation interval is reached when @xmath15 approaches the largest eigenvalue . in the figure",
    "this is the case for @xmath144 .",
    "when the interval is increased further , however , a hump starts to form .",
    "this is due to the fact that the divided differences are significantly larger than the result , which has size @xmath145 .",
    "as can be seen in figure  [ fig : interror_b ] , the behavior is different for the complex case . here",
    "the divided differences have modulus one and a hump forms if the interpolation interval is too small .",
    "note that the smallest necessary degree of interpolation is again obtained by selecting the _ optimal _ interval .    in both cases the undesired behavior can be improved by obtaining a better estimate of the spectral radius and consequently reducing the interpolation interval .",
    "for this we employ the values @xmath146 which satisfy the well known relation @xmath147 as long as the sequence of @xmath148 decreases , we adjust the interpolation interval accordingly .",
    "for a general matrix @xmath1 this phenomenon will influence the computation whenever @xmath149 strongly overestimates @xmath150 . in this case",
    "our algorithm chooses an interpolation interval that is far too large .",
    "note that this happens in particular for nonnormal matrices .    for the estimate based on the reduction of the interpolation interval is possible due to the behavior of the @xmath78 curve shown in figure  [ fig : thetavsbeta ] . however ,",
    "if we use the estimate based on the reduction of the interpolation interval is not straightforward .",
    "if we fit our rectangle @xmath41 into an ellipse with semi - axis given by then , in general , @xmath41 will not fit into an ellipse with a smaller interpolation interval .",
    "we overcome this problem by adding a circle to the ellipses .",
    "we use the largest circle defined by @xmath151 for some @xmath152 that fulfills , see figure  [ fig : ellips ] for an example . in most cases the radius of the circle is not going to be @xmath103 . if the values @xmath153 indicate a reduction of the interval , we restrict the ellipse estimate to these circles and perform a reduction of the interpolation interval .",
    "the validity of this process can be checked in the same manner as for  @xmath103 .    in the current version",
    "the algorithm does not allow to reduce the number @xmath5 along with the decay of @xmath153 as in @xcite .",
    "nevertheless , if a drastic decay is indicated it is possible to transform our method into taylor interpolation by simply setting @xmath74 .",
    "in order to illustrate the behavior of our method we provide a variety of numerical examples .",
    "we use matrices resulting from the spatial discretization of time dependent partial differential equations already used in @xcite .",
    "furthermore , we also utilize examples from @xcite and certain prototypical examples to illustrate some specific behavior of the method .",
    "all our experiments are carried out with matlab 2013a . as a measure of the required computational work we use the number of matrix - vector products ( mv ) performed by the method , without taking into account preprocessing tasks .",
    "we will compare our method to the function ` expmv ` of @xcite .",
    "note that the leja method also employs divided differences .",
    "they are computed as described in @xcite .",
    "the used divided differences are precomputed as the employed interpolation intervals are fixed .    in the following we are going to compare different variations of our algorithm based on the presented ways to compute the scaling factor @xmath5 and degree of interpolation @xmath12 .",
    "` algorithm 1 ` : : :    uses @xmath130 and @xmath131 given by . `",
    "algorithm 2 ` : : :    uses @xmath130 and @xmath131 given by .    in both algorithms ,",
    "the early termination criterion is used , as well as the shift and the hump test discussed in the previous section , if not indicated otherwise . for ` alg.2 ` the hump test procedure is only employed if the estimate of the scaling step is based on circles .    in example",
    "[ eg : lesp ] we take a look at the stability of the methods with and without early termination , examples  [ eg : ade ] and [ eg : hump ] focus on the selection of the degree and the interpolation interval for the different variations of our algorithm , and example  [ eg : longtimestep ] investigates the behavior for multiple scaling steps , i.e. ,  @xmath154 .    [ eg : lesp ] this example is taken from ( * ? ? ?",
    "* exp .  2 ) to show the influence of the early termination criterion for a specific problem .",
    "we use the matrix @xmath1 as given by ` gallery(lesp,10 ) ` .",
    "this is a nonsymmetric , tridiagonal matrix with real , negative eigenvalues .",
    "we compute @xmath155 by ` alg.1 ` and ` alg.2 ` , respectively , for 50 equally spaced time steps @xmath156 $ ] .",
    "we select the tolerance @xmath92 and @xmath157 . as @xmath1 is a nonnormal matrix , ` alg.2 ` is restricted to circles to allow the hump reduction procedure .",
    "time step @xmath158 versus relative error in the @xmath159-norm for the computation of @xmath155 with tolerance @xmath92 .",
    "the @xmath160 indicates that no early termination was used .",
    "note that there is almost no visible difference between the methods with and without the early termination in place . ]",
    "the results of the experiments can be seen in figure  [ fig : fs ] where the solid line corresponds to the condition number multiplied by the tolerance .",
    "we can not expect the algorithms to perform much better than indicated by this line . as condition number we use @xmath161 as defined in (",
    "* eq .  ( 4.2 ) ) .",
    "here @xmath162 denotes the vectorization operator that converts its matrix argument to a vector by traversing the matrix column - wise .",
    "furthermore , let @xmath163 denote the frchet derivative of @xmath164 at @xmath1 in direction @xmath44 given by @xmath165 with the relation @xmath166 the frchet derivative is given in its kronecker form as @xmath167 .",
    "in addition we use the relation @xmath168 for the computation we use the function ` expm_cond ` from the matrix function toolbox , see @xcite .",
    "overall we can see that the algorithms behave in a forward stable manner for this example .",
    "the early termination criterion shows no significant increase in the error .",
    "both algorithms are well below @xmath169 for all values of @xmath158 . for this",
    "rather small matrix we used the exact norm and not a norm estimate to allow for a better comparison .",
    "[ eg : ade ] in order to show the difference between the two suggested processes for selecting the interpolation interval for our algorithms , we use an example that allows us to easily vary the spectral properties of the discretization matrix .",
    "let us consider the advection - diffusion equation @xmath170 on the domain @xmath171 ^ 2 $ ] with homogeneous dirichlet boundary conditions .",
    "this problem is discretized in space by finite differences with grid size @xmath172 , @xmath173 . as a result of the discretization we get a sparse @xmath174 matrix @xmath1 .",
    "we define the grid pclet number @xmath175 as the ratio of advection to diffusion , scaled by @xmath176 . by increasing @xmath177 the nonnormality of the discretization matrix",
    "can be controlled .",
    "in addition , @xmath177 describes the height - to - width ratio of the rectangle @xmath41 .",
    "the estimates for @xmath178 and @xmath39 stay constant but @xmath137 increases with @xmath177 . for the following computations the parameters are chosen as : @xmath179 , @xmath180 and @xmath181 . as a result , for @xmath182 we get that @xmath41 is an interval on the real axis and for @xmath183 a square . for @xmath182",
    "the matrix is equal to @xmath184 .",
    "the vector @xmath2 is given by the discretization of the initial value @xmath185 .",
    "in the following discussion we call the shifted matrix again @xmath1 .",
    "l|*10c|c & & & + & @xmath130 & @xmath12 & rel .",
    "err & @xmath15 & @xmath130 & @xmath12 & rel .",
    "err & @xmath15 & @xmath130 & @xmath12 & rel .",
    "err + @xmath182 & 54 & 32 & 3.66e-15 & 8.96 & 40 & 32 & 3.66e-15 & 8.96 & 52 & 44 & 2.88e-15 + @xmath186 & 54 & 34 & 5.47e-15 & 8.96 & 49 & 35 & 4.17e-15 & 8.28 & 52 & 44 & 3.91e-15 + @xmath187 & 54 & 35 & 2.21e-15 & 8.96 & 55 & 36 & 2.86e-15 & 9.24 & 52 & 44 & 2.34e-15 + @xmath188 & 54 & 38 & 3.63e-15 & 8.96 & 62 & 40 & 6.36e-15 & 11.08 & 52 & 44 & 4.93e-15 + @xmath189 & 54 & 41 & 2.98e-15 & 8.96 & 67 & 43 & 9.86e-15 & 11.34 & 52 & 43 & 2.59e-15 + @xmath183 & 54 & 44 & 3.24e-15 & 8.96 & 72 & 49 & 4.50e-14 & 12.99 & 52 & 39 & 1.30e-15    table  [ tab : ad_singlestep ] gives the results of an experiment where we varied the grid pclet number .",
    "we show the results of the different selection procedures .",
    "the time step @xmath190e-3 is chosen such that ` expmv ` is able to compute the result without scaling the matrix .",
    "the actual degree of interpolation @xmath12 and the relative error with respect to the method ` expm ` in the maximum norm are shown .",
    "as the maximum norm of the matrix stays the same ( for fixed @xmath191 ) the parameters of ` expmv ` and ` alg.1 ` are always the same .",
    "for @xmath183 the eigenvalues of the matrix @xmath192 are in a small circle around zero and therefore the taylor approximation requires a lower degree of interpolation .",
    "on the other hand we can see that for a small height - to - width ratio ( small @xmath177 ) the estimate based on ellipses , i.e.  ` alg.2 ` , produces a significantly smaller @xmath130 with the same actual degree @xmath12 of interpolation and comparable error .",
    "this means that less scaling is required for larger @xmath158 , cf .",
    "example  [ eg : longtimestep ] .",
    "when the rectangle @xmath41 is closer to a square the algorithm still produces reliable results but is slightly less efficient than ` alg.1 ` .",
    "[ rem : thetamax ] as mentioned in section  [ sec : cd ] it is possible to select the @xmath103 values differently depending on @xmath15 . by selecting @xmath193 the computation corresponding to @xmath182 gives the following results : @xmath194 , @xmath195 and @xmath196 .",
    "this indicates a slower convergence with a similar error , as the eigenvalues of the shifted matrix are in @xmath197 $ ] but we interpolate in @xmath198 $ ] .",
    "[ eg : hump ] in order to illustrate the potential gain of testing for a hump in our algorithm , we use the matrix @xmath1 given by ` -1/2*gallery(triw,20,4 ) ` and @xmath199 .",
    "this corresponds to ( * ? ? ?",
    "* experiment 6 ) with a single time step of size @xmath200 . in the following discussion",
    "we call the shifted matrix again @xmath1 .",
    "the @xmath201 matrix @xmath1 is an upper triangular matrix with @xmath52 in the main diagonal and @xmath202 on the strict upper triangular part .",
    "the @xmath203-norm of the matrix is @xmath204 and @xmath205 .",
    "for this example the truncated taylor series method is optimal and stagnates after @xmath206 iterations , in a single scaling step , with a final error of about @xmath207 .",
    "we use this example to illustrate the hump phenomenon and the procedure to counteract it .",
    "this will result in a better performance of the leja method , even though for this example it is not as efficient as ` expmv ` .",
    "relative error vs.  interpolation degree @xmath12 for the approximation of @xmath208 .",
    "the plot illustrates the behavior of the hump reduction procedure . here ` alg.1 ` * indicates that no hump reduction was performed .",
    "the relative error is measured with respect to @xmath209 in the first scaling step and @xmath208 in the second .",
    "no early termination criterion is used in the computation . ]    in figure  [ fig : triw_hump ] we illustrate the behavior of the hump reduction procedure , described in section  [ sec : petc ] .",
    "a first observation is that our method selects two scaling steps ( @xmath210 ) .",
    "for this nonnormal matrix the rectangle @xmath41 is a square and therefore we only show the results for ` alg.1 ` , as we can expect a better performance for this algorithm ( cf .",
    "example  [ eg : ade ] ) .",
    "we can see quite clearly that a hump of about @xmath211 digits is formed when we use the initial guess of @xmath212 .",
    "as expected we get an error of about @xmath213 in each scaling step and consequently a final error of the same size . in this experiment",
    "we deactivate the early termination criterion and therefore the algorithm uses @xmath214 in each of the two scaling steps .",
    "furthermore , for ` alg.1 ` * and ` alg.1 ` the relative error is measured with respect to @xmath209 in the first scaling step and @xmath208 in the second . for sake of comparison",
    "we run ` expmv ` without early termination and @xmath215 iterations , and we measure the relative error with respect to @xmath208 .    on the other hand ,",
    "if we reduce the interval length , the hump is reduced as well .",
    "for this ( shifted ) matrix the values @xmath153 are @xmath216 ( see section  [ sec : hump ] ) , where @xmath217 .",
    "these values suggest that the interpolation interval should be reduced .",
    "in fact , by reducing the interval to @xmath74 we would recover the truncated taylor series method and therefore the optimal choice for this example .",
    "the cost of the computation of @xmath153 can not be neglected and in a practical implementation @xmath218 is therefore limited .",
    "experiments have shown that @xmath219 is a practical choice .",
    "therefore , we use the interpolation interval @xmath220 in the reduced case .    from figure  [",
    "fig : triw_hump ] we can see that the algorithm to reduce the impact of the hump is working .",
    "the hump is significantly reduced and the error is close to the error of the truncated taylor series method .",
    "nevertheless , the method takes about three times as many iterations ( approximately @xmath221 ) than the truncated taylor series method , cf .",
    "example  [ eg : longtimestep ] .",
    "even though this procedure might not be necessary for accuracy it is still beneficial for the overall cost reduction .",
    "if we only require @xmath211 digits of accuracy we do not need to reduce the interpolation interval to achieve this .",
    "however , for this example it would still be beneficial to reduce the interpolation interval , as the necessary degree of interpolation is reduced as well .",
    "this is related to the observations of remark  [ rem : thetamax ] and section  [ sec : hump ] . here",
    "the norm of the matrix is a large overestimate of the spectral radius leading to slow convergence .",
    "[ eg : longtimestep ]    in this experiment we investigate the behavior of the methods for multiple scaling steps .",
    "we use the two matrices of examples  [ eg : ade ] and  [ eg : hump ] from above and in addition the matrices ` orani676 ` and ` bcspwr10 ` which are obtained from the university of florida sparse matrix collection @xcite , as well as several other matrices , see table  [ tab : matrixprob ] .",
    "the sparse matrix ` orani676 ` is real and nonnormal with @xmath222 nonzero entries , whereas ` bcspwr10 ` is a real and symmetric sparse matrix with @xmath223 nonzero entries .",
    "the matrix ` triu ` is an upper triangular matrix with entries uniformly distributed on @xmath224 $ ] . for the matrix `",
    "onesided ` we have a @xmath225 upper triangular matrix with one eigenvalue at @xmath226 and @xmath227 eigenvalues uniformly distributed on @xmath228 $ ] , with standard deviation of @xmath229 .",
    "the values in the strict upper triangular part are uniformly distributed on @xmath224 $ ] .",
    "furthermore , we use ` s3d ` from ( * ? ? ?",
    "* example 3 ) , a finite difference discretization of the three dimensional schrdinger equation with harmonic potential in @xmath230 ^ 3 $ ] .",
    "the matrix ` trans1d ` is a periodic , symmetric finite difference discretization of the transport equation in @xmath230 $ ] .",
    "for the matrices ` orani676 ` , ` s3d ` and ` trans1d ` complex conjugate leja points are used in the computation .    as vector @xmath2 we use @xmath231^\\mathrm{t}$ ] for ` orani676 ` , @xmath232^\\mathrm{t}$ ] for ` bcspwr10 ` , @xmath2 as specified in example  [ eg : ade ] for ` ad ` , the discretization of @xmath233 is used for ` s3d ` , the discretization of @xmath234 for ` trans1d ` , and @xmath199 for all other examples .",
    "this corresponds to ( * ? ? ?",
    "* exp .  7 ) .",
    "l|l|*4cl*3c # & & @xmath235 & @xmath158 & @xmath236 & @xmath237 & & @xmath238 & @xmath239 & @xmath240 + 1 & orani676 & 2529 & 100 & 1.0e+03 & 1.0e+03 & 0.002 & 1.0e+03 & 3.2e+01 & 9.4e+00 + 2 & bcspwr10 & 5300 & 10 & 2.6e+01 & 0.0e+00 & 0 & 1.4e+01 & 6.8e+00 & 1.4e+01 + 3 & triw & 2000 & 10 & 8.0e+03 & 8.0e+03 & 0.5 & 8.0e+03 & 5.0e+03 & 8.0e+03 + 4 & triu & 2000 & 40 & 1.0e+02 & 1.0e+03 & 0.021 & 1.0e+03 & 4.2e+01 & 1.0e+03 + 5 & ad pe=0 & 9801 & 1/4 & 8.0e+04 & 0.0e+00 & 0 & 8.0e+04 & 7.9e+04 & 8.0e+04 + 6 & ad pe=0 & 9801 & 1 & 8.0e+04 & 0.0e+00 & 0 & 8.0e+04 & 7.9e+04 & 8.0e+04 + 7 & onesided & 41 & 5 & 3.1e+01 & 1.2e+01 & 0.5 & 2.0e+01 & 1.1e+01 & 2.0e+01 + 8 & s3d & 27000 & 1/2 & 0.0e+00 & 5.7e+03 & 0 & 5.8e+03 & 5.7e+03 & 5.8e+03 + 9 & trans1d & 1000 & 2 & 0.0e+00 & 2.0e+03 & 0 & 1.0e+03 & 1.0e+03 & 1.0e+03 +     +    lr|*9c|c & & & + # & @xmath158 & @xmath5 & @xmath241 & rel.err & @xmath5 & @xmath241 & rel.err & @xmath5 & @xmath241 & rel.err + 1 & 100 & 4639 & 41751 & 1.8e-11 & 3508 & 31572 & 2.2e-11 & 21 & 526 & 4.0e-08 + 2 & 10 & 6 & 157 & 7.8e-10 & 6 & 157 & 7.8e-10 & 5 & 171 & 7.2e-07 + 3 & 10 & 3408 & 98840 & 5.4e-09 & 2423 & 87233 & 1.0e-07 & 1588 & 10425 & 1.1e-09 + 4 & 40 & 1730 & 22495 &",
    "1.6e-11 & 1268 & 17755 & 1.6e-12 & 59 & 960 & 4.3e-09 + 5 & 1/4 & 427 & 14945 & 1.0e-08 & 357 & 13923 & 1.9e-09 & 749 & 29211 & 2.2e-06 + 6 & 1 & 1705 & 59675 & 1.9e-08 & 1426 & 55614 & 3.3e-09 & 2995 & 116805 & 9.0e-06 + 7 & 5 & 5 & 129 & 3.0e-09 & 4 & 119 & 1.6e-10 & 8 & 296 & 2.1e-08 + 8 & 1/2 & 65 & 3185 & 2.2e-11 & 57 & 2793 & 8.0e-09 & 108 & 5400 & 1.3e-07 + 9 & 2 & 89 & 4539 & 9.5e-13 & 79 & 3871 & 1.4e-08 & 150 & 5135 & 3.6e-08 +    we summarize the properties of all the matrices used in this example in table  [ tab : matrixprob ] .",
    "the tolerance is chosen as @xmath242 and the relative error is computed with respect to ` expmv ` running with the highest accuracy .",
    "furthermore we use @xmath243 as an indicator for the nonnormality of the matrices . from now on we refer to the matrices by their number given in the first column of table  [ tab : matrixprob ] .",
    "we can see that for the nonnormal matrices @xmath244 the algorithm ` expmv ` is superior in terms of matrix - vector products , in comparison to both variants of our algorithm .",
    "this is largely due to the fact that for these matrices the method ` expmv ` can reduce the number of scaling steps based on the values @xmath153 ( see remark  [ rem : alphap ] ) .",
    "as the leja method is not able to do this , the only way of getting comparable results for these example is by obtaining sharper bounds for the rectangle @xmath41 in ` alg.2 ` .",
    "this could be achieved using the matlab routines eig ( based on lapack  linear algebra package and suited for full matrices ) , eigs ( based on arpack  arnoldi package and suited for sparse matrices ) , or an eigensolver of your choice fitted to the example .",
    "however , the computation can be very expensive and therefore is not practicable in a general purpose algorithm .",
    "furthermore , for these matrices the user specified norm has a relevant influence on the performance , as can be seen in tables  [ tab : longtimestep ] and  [ tab : longtimestep2 ] , respectively . if the problem is considered with the @xmath159- or the maximum norm the number of matrix - vector products is significantly reduced .",
    "lr|*6c|c & & + # & @xmath158 & @xmath5 & @xmath241 & rel.err & @xmath5 & @xmath241 & rel.err + 1 & 100 & 142 & 2434 & 1.8e-10 & 43 & 2195 & 2.2e-11 + 2 & 10 & 2 & 106 & 7.0e-08 & 6 & 154 & 1.0e-09 + 3 & 10 & 2175 & 76141 & 5.8e-08 & 3408 & 98847 & 5.4e-09 + 4 & 40 & 66 & 2122 & 7.4e-10 & 1748 & 22732 & 1.5e-11 + 7 & 5 & 3 & 91 & 5.5e-10 & 5 & 128 & 3.0e-09 +    for the matrices @xmath245 the results show a different picture . here , the leja method is clearly beneficial in terms of matrix - vector products .",
    "furthermore , we also produce a smaller error in comparison to the truncated taylor series approach .",
    "this is due to the fact that we divide the tolerance by @xmath5 in the early termination criterion , cf .  .",
    "in the case of the complex conjugate leja points this leads to a higher accuracy than required . on the other hand , for the ` ad ` problem , the errors of the leja methods increase by a factor of @xmath159 , if we increase @xmath158 by a factor of @xmath246 , whereas the error for ` expmv ` increases by a factor of @xmath246 .",
    "this is due to the fact that the used early termination criterion for the leja method takes the number of scaling steps into account whereas ` expmv ` does not .    for matrices",
    "@xmath247 conjugate complex leja points are used , see section  [ sec : complexleja ] .",
    "for the two normal matrices @xmath248 the leja method saves a lot of matrix - vector products in comparison to ` expmv ` . as here",
    "the rectangle @xmath41 is a line , ` alg.2 ` is again superior to ` alg.1 ` as it leads to fewer scaling steps and less matrix - vector products .",
    "matrix @xmath159 is normal .",
    "however , only for the @xmath159-norm we have that @xmath249 .",
    "this is the reason why the number of scaling steps is only two in the @xmath159-norm .",
    "the final error of the methods is always comparable .",
    "the more precautious approach we propose leads sometimes to an increase in accuracy .",
    "nevertheless , in the cases where the leja method is beneficial it still uses significantly less matrix - vector products than ` expmv ` .",
    "a comparison of ` alg.1 ` and ` alg.2 ` shows that none of the two approaches can be considered superior or the _ better _ overall choice . due to the construction , ` alg.2 ` provides a scaling factor and a degree of interpolation that are independent of the norm , even though the gerschgorin discs are closely related to the @xmath203- and maximum norm .",
    "nevertheless , the reduction of the interpolation interval is connected to a norm .",
    "in fact , this is also the case where the two methods do provide similar estimates for @xmath5 . in total , if we always select the method with the least ( predicted ) computational cost we always use the more efficient methods as we save matrix - vector products .",
    "this indicates that a combination of the two algorithms , where we always select the one with the least expected cost is beneficial .",
    "depending on the specified norm , ` alg.1 ` has some significant fluctuations in performance .",
    "the backward error analysis presented in this work provides a sound basis for the selection of the scaling parameter @xmath131 and the degree of interpolation @xmath130 for the leja method . with this information at hand",
    "the algorithm becomes in a sense direct , as the maximal number of matrix - vector products is known after the initial preprocessing .",
    "the cost of ` alg.1 ` is determined by the norm of the matrix , whereas the cost of ` alg.2 ` is determined by the spectral information of the matrix .",
    "the convergence is monitored by the early termination criterion .",
    "the practical use of this approach is confirmed by the numerical experiments of section  [ sec : numexp ] .",
    "the algorithm can be adapted in a similar way as the ` expmv ` method to support dense output and provides essentially the same properties as ( * ? ? ?",
    "* algorithm 5.2 ) .",
    "in particular this means that the new algorithm also has some benefits in compared to krylov subspace methods .",
    "note that in certain applications one has to compute @xmath250 for a scalar @xmath158 and a @xmath251 matrix @xmath252 .",
    "this problem , however , is not more general since the product @xmath192 can always be considered as a new matrix and the performed analysis extends to a matrix @xmath252 instead of a vector @xmath2 .",
    "this is especially interesting in comparison to krylov subspace methods as the available implementations would need to be called repeatedly for each column of @xmath252 .    in comparison to ` expmv ` the leja method is especially beneficial for matrices where the values @xmath253 do not vary much . in these cases",
    "the method saves matrix - vector products . on the other hand",
    "our method makes a higher preprocessing effort than ` expmv ` .",
    "this is due to the more complex selection procedure of @xmath130 and @xmath131 and the fact that we need an estimate of the field of values . as the overall cost is dominated by the matrix - vector products",
    ", this fact comes only into play for low - dimensional examples .",
    "the combination of the two algorithms ` alg.1 ` and ` alg.2 ` , where we select the scaling parameter and the degree of interpolation based on the minimum of the predicted cost of the two algorithms , seems to be the logical choice for a combined ( black box ) algorithm . with the changes applied to the method",
    "it can be called for any matrix @xmath1 , it is numerically stable , the costs are predictable and the effort for the implementation is manageable .    in the present version of our algorithm , the knowledge of @xmath153 can not be used to properly scale the interpolation interval .",
    "however , and this is the focus of our future work , it is possible to modify the method and repeatedly use zero as interpolation point .",
    "these so - called leja ",
    "hermite methods will then be able to make use of @xmath153 in a suitable fashion as ` expmv ` .",
    "a matlab implementation of the algorithm presented in this paper is available on the homepage https://numerical-analysis.uibk.ac.at/exponential-integrators .",
    "we thank the referees for their constructive remarks which helped us to improve the presentation of the paper considerably .",
    "davis , t.a . , hu , y. , 2011 . the university of florida sparse matrix collection .",
    "acm  trans .  math .",
    "software , 38  ( 1 ) , 125 .",
    "higham , n.j .",
    ", the matrix function toolbox .",
    "http://www.ma.man.ac.uk/~higham/mftoolbox , version 1.0 , march 6 , 2008 ."
  ],
  "abstract_text": [
    "<S> the leja method is a polynomial interpolation procedure that can be used to compute matrix functions . in particular , computing the action of the matrix exponential on a given vector is a typical application . </S>",
    "<S> this quantity is required , e.g. , in exponential integrators .    </S>",
    "<S> the leja method essentially depends on three parameters : the scaling parameter , the location of the interpolation points , and the degree of interpolation . </S>",
    "<S> we present here a backward error analysis that allows us to determine these three parameters as a function of the prescribed accuracy . </S>",
    "<S> additional aspects that are required for an efficient and reliable implementation are discussed . </S>",
    "<S> numerical examples illustrating the performance of our matlab code are included .    </S>",
    "<S> _ mathematics subject classification _ ( ): 65f60 , 65d05 , 65f30 + _ key words : _ leja interpolation , backward error analysis , action of matrix exponential , exponential integrators , @xmath0 functions , taylor series </S>"
  ]
}