{
  "article_text": [
    "consider the standard gaussian linear regression model @xmath0 where @xmath1 is a vector of the observed response variable @xmath2 , @xmath3 is the design matrix of the @xmath4 explanatory variables ( predictors ) @xmath5 , @xmath6 is a vector of unknown regression coefficients , @xmath7 and the noise variance @xmath8 is assumed to be known .",
    "a variety of statistical applications of regression models involves a vast number of potential explanatory variables that might be even large relatively to the amount of available data .",
    "it raises a severe `` curse of dimensionality '' problem .",
    "reducing dimensionality of the model becomes therefore crucial in the analysis of such large data sets .",
    "the goal of model ( or variable ) selection is to select the `` best '' , parsimonious subset of predictors .",
    "the corresponding coefficients are then usually estimated by least squares .",
    "the meaning of the `` best '' subset however depends on the particular aim at hand .",
    "one should distinguish , for example , between estimation of regression coefficients @xmath9 , estimation of the mean vector @xmath10 , model identification and predicting future observations .",
    "different aims may lead to different optimal model selection procedures especially when the number of potential predictors @xmath4 might be much larger than the sample size @xmath11 . in this paper",
    "we focus on estimating the mean vector @xmath10 and the goodness of a model ( subset of predictors ) @xmath12 is measured by the quadratic risk @xmath13 , where @xmath14 is the least squares estimate of @xmath9 and @xmath15 is the projection of @xmath10 on the span of @xmath12 .",
    "the first ( bias ) term of the risk decomposition represents the approximation error of the projection , while the second ( variance ) term is the price for estimating the projection coefficients @xmath16 by @xmath14 and is proportional to the model size .",
    "the `` best '' model then is the one with the minimal quadratic risk .",
    "note that the true underlying model in ( [ eq : model ] ) is not necessarily the best in this sense since sometimes it is possible to reduce its risk by excluding predictors with small ( but still nonzero ! ) coefficients .",
    "such a criterion for model selection is obviously impossible to implement since it depends on the unknown @xmath9 . instead",
    ", the corresponding ideal minimal risk can be used as a benchmark for any available model selection procedure .",
    "the model selection criteria are typically based on the _ empirical _ quadratic risk @xmath17 , which is essentially the least squares .",
    "however , direct minimization of the empirical risk evidently leads to a trivial ( unsatisfactory ! ) choice of the saturated model . a typical remedy is then to add a complexity penalty @xmath18 that increases with the model size , and to consider _ penalized _ least squares criterion of the form @xmath19    the properties of the resulting estimator depends on the proper choice of the complexity penalty function @xmath20 in ( [ eq : pmle ] ) .",
    "there exists a plethora of works in literature on this problem .",
    "the standard , most commonly used choice is a _ linear _ type penalty of the form @xmath21 for some fixed @xmath22 .",
    "the most known examples motivated by different ideas include aic for @xmath23 ( akaike , 1974 ) , bic for @xmath24 ( schwarz , 1978 ) and ric for @xmath25 ( foster & george , 1994 ) .",
    "a series of recent works suggested the so - called @xmath26-type nonlinear penalties of the form @xmath27 , where @xmath28 and @xmath29 is some `` negligible '' term ( see , e.g. , birg & massart , 2001 , 2007 ; johnstone , 2002 ; abramovich _ et al .",
    "_ , 2006 ; bunea , tsybakov & wegkamp , 2007 ) .",
    "in this paper we present a bayesian formalism to the model selection problem in gaussian linear regression ( [ eq : model ] ) that leads to a general penalized model selection rule ( [ eq : pmle ] ) .",
    "the proposed bayesian approach can be used , in fact , as a natural tool for obtaining a variety of penalized least squares estimators with different complexity penalties that accommodate many of the known model selection procedures as particular cases corresponding to specific choices of the prior . within bayesian framework",
    ", the penalty term in ( [ eq : pmle ] ) is interpreted as proportional to the logarithm of a prior distribution .",
    "complexity penalties @xmath18 imply placing a prior on the model size ( the number of nonzero entries of @xmath9 ) .",
    "minimization of ( [ eq : pmle ] ) corresponds to the maximum _ a posteriori _",
    "( map ) rule yielding the resulting map model selector to be the posterior mode .",
    "although there exists a large amount of literature on bayesian model selection ( see george & mcculloch , 1993 , 1997 ; chipman , george & mccullogh , 2001 ; liang _ et al . _ , 2008 for surveys ) , it mainly focuses on `` purely bayesian '' issues ( e.g. , prior specification , posterior calculations , etc . ) and does not investigate the optimality of the resulting bayesian procedures from a frequentist view . in this paper",
    "we study the optimality properties of the proposed map model selectors for estimating the mean vector @xmath10 in ( [ eq : model ] ) .",
    "first , under mild conditions on the prior we establish the oracle inequality and show that , up to a constant multiplier , they achieve the minimal possible risk among all estimators .",
    "we then investigate their asymptotic minimaxity .",
    "for `` nearly - orthogonal '' design they are proved to be simultaneously rate - optimal ( in the minimax sense ) over a wide range of sparse and dense settings and outperform various existing model selection procedures , e.g. aic , bic , ric , lasso ( tibshirani , 1996 ) and dantzig selector ( cands & tao , 2007 ) . in a way , these results extend those of abramovich , grinshtein & pensky ( 2007 ) and abramovich _ et al . _ ( 2010 ) for the normal means problem corresponding to the particular case @xmath30 .",
    "the analysis of `` multicollinear '' design , which is especially relevant for `` @xmath4 much larger than @xmath11 '' setup , is more delicate .",
    "we demonstrate that the lower bounds for the minimax rates for estimating the mean vector in this case are smaller than those for `` nearly - orthogonal '' design by the factor depending on the design properties .",
    "such `` blessing of multicollinearity '' can be explained by a possibility of exploiting correlations between predictors to reduce the size of a model ( hence , to decrease the variance ) without paying much extra price in the bias term .",
    "we show that under some additional assumptions on the design and the coefficients vector @xmath9 in ( [ eq : model ] ) , the proposed bayesian model selectors are still asymptotically rate - optimal .",
    "the paper is organized as follows .",
    "the bayesian model selection procedure that leads to a penalized least squares estimator ( [ eq : pmle ] ) is introduced in section [ sec : map ] . in section [ sec : oracle ] we derive an upper bound for the quadratic risk of the resulting map model selector , compare it with that of an oracle and find the conditions on the prior where , up to a constant multiplier , it achieves the minimal possible risk among all estimators . in section [ sec : minimax ] we obtain the upper and lower risk bounds of the map model selector in a sparse setup that allows us to investigate its asymptotic minimaxity for nearly - orthogonal and multicollinear designs in section [ sec : asymp ] .",
    "the computational aspects are discussed in section [ sec : comp ] , and the main take - away messages of the paper are summarized in section [ sec : disc ] . all the proofs are given in the appendix .",
    "consider the gaussian linear regression model ( [ eq : model ] ) , where the number of possible predictors @xmath4 might be even larger then the number of observations @xmath11 .",
    "let @xmath31 and assume that any @xmath32 columns of @xmath33 are linearly independent .",
    "for the `` standard '' linear regression setup , where all @xmath4 predictors are linearly independent and there are at least @xmath4 linearly independent design points , @xmath34 .",
    "any model @xmath12 is uniquely defined by the @xmath35 diagonal indicator matrix @xmath36 , where @xmath37 and , therefore , @xmath38 . the corresponding least square estimate @xmath39 , where `` + '' denotes the generalized inverse matrix .",
    "assume some prior on the model size @xmath40 , where @xmath41 ( @xmath42 corresponds to a null model with a single intercept ) and @xmath43 for @xmath44 since otherwise , there necessarily exists another vector @xmath45 with at most @xmath32 nonzero entries also satisfying ( [ eq : model ] ) , that is , @xmath46 .    for any @xmath47",
    "there are @xmath48 different models of a given size @xmath49 .",
    "assume all of them to be equally likely , that is , conditionally on @xmath50 , @xmath51 one should be a little bit more careful for @xmath52 . although there are @xmath53 different sets of predictors of size @xmath32 , all of them evidently result in the same estimator for the mean vector and , in this sense , are essentially undistinguishable and associated with a _ single _ ( saturated ) model . hence",
    ", in this case , we set @xmath54 finally , assume the normal prior on the unknown vector of @xmath49 coefficients of the model @xmath12 : @xmath55 .",
    "this is a well - known conventional @xmath56-prior of zellner ( 1986 ) .",
    "for the proposed hierarchical prior , straightforward calculus yields the posterior probability of a model @xmath12 of size @xmath57  : @xmath58 finding the most likely model leads therefore to the following maximum _ a posteriori _ ( map ) model selection criterion : @xmath59 or , equivalently , @xmath60 which is of the general type ( [ eq : pmle ] ) with the complexity penalty @xmath61 similarly , for @xmath62 from ( [ eq : full ] ) one has @xmath63    a specific form of the penalty ( [ eq : pen])-([eq : pen1 ] ) depends on the choice of a prior @xmath64 . in particular ,",
    "the ( truncated if @xmath65 ) binomial prior @xmath66 corresponds to the prior assumption that the indicators @xmath67 are independent .",
    "the binomial prior yields the linear penalty @xmath68 , where @xmath69 for sufficiently large variance ratio @xmath70 .",
    "the aic criterion corresponds then to @xmath71 , while @xmath72 leads to the ric criterion .",
    "these relations indicate that ric should be appropriate for sparse cases , where the size of the true ( unknown ) model is believed to be much less than the number of possible predictors , while aic is suitable for dense cases , where they are of the same order .",
    "in fact , any binomial prior or , equivalently , any linear penalty can not `` kill two birds with one stone '' . on the other hand ,",
    "the ( truncated ) geometric prior @xmath73 for some @xmath74 , implies @xmath75 which is of the @xmath26-type introduced above . for large",
    "@xmath70 it behaves similar to ric for @xmath76 and to aic for @xmath77 and is , therefore , adaptive to both sparse and dense cases .",
    "we will discuss these issues more rigorously in section [ sec : asymp ] below .",
    "in this section we derive an upper bound for the quadratic risk of the proposed map model selector and compare it with the ideal minimal quadratic risk often called in literature as an oracle risk .",
    "[ as : p ] assume that @xmath78 where @xmath79 .    assumption ( p ) is not restrictive .",
    "indeed , the obvious inequality @xmath80 implies that for @xmath81 it automatically holds for_any _ prior @xmath82 for all @xmath83 . assumption ( p )",
    "is used to establish an upper bound for the quadratic risk of the map model selector .",
    "[ th : bound ] let the model @xmath84 be the solution of ( [ eq : pmle ] ) with the complexity penalty @xmath20 given in ( [ eq : pen])-([eq : pen1 ] ) and @xmath85 be the corresponding least squares estimate .",
    "then , under assumption ( p ) @xmath86 for some @xmath87 and @xmath88 depending only on @xmath70 .    to assess the quality of the upper bound in ( [ eq : bound ] ) , we compare it with the oracle risk @xmath89 .",
    "note that the oracle risk is exactly zero when @xmath90 and , evidently , no estimator can achieve it in this case .",
    "hence , an additional , typically negligible term @xmath8 , which is , essentially , an error of estimating a single extra parameter , is usually added to the oracle risk for a proper comparison .",
    "it is known that no estimator can attain a risk smaller than within @xmath91 factor from that of an oracle ( e.g. , foster & george , 1994 ; donoho & johnstone , 1995 ; cands , 2006 ) .",
    "the following theorem shows that under certain additional conditions on the prior @xmath64 , the resulting map model selector achieves this minimal possible risk among all estimators up to a constant multiplier depending on @xmath70 :    [ th : oracle ] let @xmath82 satisfy assumption ( p ) and , in addition , @xmath92 for some constant @xmath93 .",
    "then , the resulting map model selector satisfies @xmath94 for some @xmath95 .    in particular",
    ", it can be easily shown that theorem [ th : oracle ] holds for the ( truncated ) binomial @xmath66 with @xmath96 ( ric criterion ) and geometric priors ( see section [ sec : map ] ) . more generally , all priors such that @xmath97 corresponding to the @xmath26-type penalties satisfy the conditions of theorem [ th : oracle ] .",
    "in the previous section we considered the global behavior of the map estimator without any restrictions on the model size . however , in the analysis of large data sets , it is typically reasonable to assume that the true model in ( [ eq : model ] ) is sparse in the sense that only part of coefficients in @xmath9 are different from zero .",
    "we now show that under such extra sparsity assumption , more can be said on the optimality of the map model selection .",
    "for a given @xmath98 , define the sets of models @xmath99 that have at most @xmath100 predictors , that is , @xmath101 . obviously , if a true model in ( [ eq : model ] ) belongs to @xmath99 , the @xmath102 quasi - norm of the corresponding coefficients vector @xmath103 , where @xmath104 is the number of its nonzero entries .",
    "in this section we find the upper and lower bounds for the maximal risk of the proposed map model selector over @xmath99 .",
    "[ th : upper ] let @xmath105 , @xmath64 satisfy assumption ( p ) and , in addition , @xmath106 if @xmath107 or @xmath108 if @xmath109 for some constant @xmath110 .",
    "then , there exists a constant @xmath111 depending only on @xmath70 such that @xmath112    the general upper bound ( [ eq : upper ] ) for the maximal risk of the map selector over @xmath99 in theorem [ th : upper ] holds for any design matrix @xmath33 . to assess its accuracy",
    "we establish the lower bound for the minimax risk of estimating the mean vector @xmath10 in ( [ eq : model ] ) .",
    "for any given @xmath113 , let @xmath114 $ ] and @xmath115 $ ] be the @xmath49-sparse minimal and maximal eigenvalues of the design defined as @xmath116=\\min_{{{\\mbox{\\boldmath $ \\beta$ } } } : 1 \\leq ||{{\\mbox{\\boldmath $ \\beta$}}}||_0 \\leq k }   \\frac{||x{{\\mbox{\\boldmath $ \\beta$}}}||^2}{||{{\\mbox{\\boldmath $ \\beta$}}}||^2},\\ ] ] @xmath117=\\max_{{{\\mbox{\\boldmath $ \\beta$ } } } : 1 \\leq ||{{\\mbox{\\boldmath $ \\beta$}}}||_0 \\leq k }   \\frac{||x{{\\mbox{\\boldmath $ \\beta$}}}||^2}{||{{\\mbox{\\boldmath $ \\beta$}}}||^2}\\ ] ] ( see meinshausen & yu , 2009 ; bickel , ritov & tsybakov , 2009 ) .",
    "in fact , @xmath114 $ ] and @xmath115 $ ] are respectively the minimal and maximal eigenvalues of all @xmath118 submatrices of the matrix @xmath119 generated by any @xmath49 columns of @xmath33 .",
    "let @xmath120=\\phi_{min}[k]/\\phi_{max}[k],\\;k=1, ...",
    ",r$ ] and set @xmath120=\\tau[r]$ ] for all @xmath121 . by the definition , @xmath120 $ ] is a non - increasing function of @xmath49 .",
    "obviously , @xmath120 \\leq 1 $ ] and for the orthogonal design the equality holds for all @xmath49 .",
    "[ th : lower ] consider the model ( [ eq : model ] ) and let @xmath98 .",
    "there exists a universal constant @xmath122 such that @xmath123\\ ; p_0 ( \\ln(p / p_0)+1 ) & , \\ ; 1 \\leq p_0 \\leq r/2   \\\\",
    "c_2 \\sigma^2 \\tau[p_0]\\ ; r & , \\ ; r/2 \\leq p_0 \\leq r   \\end{array } \\right.\\ ] ] where the infimum is taken over all estimates @xmath124 of the mean vector @xmath10 .",
    "theorem [ th : lower ] shows that the minimax lower bound ( [ eq : lower ] ) depends on a specific design matrix @xmath33 only through the sparse eigenvalues ratios .",
    "a computationally simpler but less accurate minimax lower bound can be obtained by replacing @xmath125 $ ] and @xmath126 $ ] in ( [ eq : lower ] ) by @xmath127 $ ] , that for the case @xmath128 is just the ratio of the minimal and maximal eigenvalues of @xmath119 .    for the orthogonal design , where @xmath129 \\equiv 1 $ ] , and @xmath130 analogous results were obtained in birg & massart ( 2001 ) . for a general design and @xmath131 similar minimax lower bounds",
    "were independently obtained in raskutti , wainwright & yu ( 2009 ) for a design matrix of a full rank and in rigollet & tsybakov ( 2010 ) for a general case within a related aggregation context .",
    "the established upper and lower bounds ( [ eq : upper ] ) , ( [ eq : lower ] ) for the risk of the map model selector allow us in the following section to investigate its asymptotic minimaxity as both @xmath11 and @xmath4 increase .",
    "in this section we consider the asymptotic properties of the map model selector as the sample size @xmath11 increases .",
    "we allow @xmath132 to increase with @xmath11 as well and look for a projection of the unknown mean vector on an expanding span of predictors .",
    "in particular , the most challenging cases intensively studied nowadays in literature are those , where @xmath133 or even @xmath134 . in such asymptotic settings",
    "one should essentially consider a _ sequence _ of design matrices @xmath135 where @xmath136 .",
    "for simplicity of exposition , in what follows we omit the index @xmath11 and denote @xmath137 by @xmath138 emphasizing the dependence on the number of predictors @xmath4 and let @xmath32 tend to infinity .",
    "similarly , we consider now sequences of coefficients vectors @xmath139 and priors @xmath140 . in these notations ,",
    "the original model ( [ eq : model ] ) is transformed into a sequence of models @xmath141 where @xmath142 and any @xmath32 columns of @xmath138 are linearly independent ( hence , @xmath143>0 $ ] ) , @xmath144 and the noise variance @xmath8 does not depend on @xmath11 and @xmath4 .",
    "one can also view a sequence of models ( [ eq : model1 ] ) in a triangular array setup ( greenshtein & ritov , 2004 ) .",
    "[ def : nearly ] consider the sequence of design matrices @xmath138 .",
    "the design is called nearly - orthogonal if the corresponding sequence of sparse eigenvalues ratios @xmath143 $ ] is bounded away from zero by some constant @xmath93 .",
    "otherwise , the design is called multicollinear .    nearly - orthogonality condition essentially means that there is no multicollinearity in the design in the sense that there are no `` too strong '' linear relationships within any set of @xmath32 columns of @xmath138 .",
    "intuitively , it is clear that in this case @xmath4 can not be `` too large '' relative to @xmath32 and , therefore , to @xmath11 .",
    "indeed , apply the upper and lower bounds ( [ eq : upper ] ) , ( [ eq : lower ] ) for @xmath145 to get @xmath146 r ( \\ln(2p / r)+1 ) \\leq c_1(\\gamma ) \\sigma^2 r$ ] that implies the following remark :    [ rem : orth ] for nearly - orthogonal design , necessarily @xmath147 and , therefore , @xmath148 .",
    "the following corollary is an immediate consequence of theorems [ th : upper ] and [ th : lower ] :    [ cor : minimax ] let the design be nearly - orthogonal .    1 .   as @xmath32 increases , the asymptotic minimax risk of estimating the mean vector @xmath149 over @xmath99 is of the order @xmath150 , that is , there exist two constants @xmath151 such that for all sufficiently large @xmath32 , @xmath152 for all @xmath98 .",
    "2 .   assume assumption ( p ) and , in addition , that @xmath153 and @xmath154 for some constants @xmath155 .",
    "then , the corresponding map model selector attains the minimax convergence rates simultaneously over all @xmath156 .",
    "one can easily verify that the conditions on the prior of corollary [ cor : minimax ] are satisfied , for example , for the truncated geometric prior ( see section [ sec : map ] ) for all @xmath113 .",
    "the resulting map model selector attains , therefore , the minimax rates simultaneously for all @xmath157 . as we have mentioned , the corresponding penalty in ( [ eq : pen ] ) is of the @xmath26-type . on the other hand ,",
    "no truncated binomial prior @xmath158 can satisfy these conditions on the entire range @xmath113 .",
    "it is easy to verify that they hold for small @xmath159 if @xmath160 but for large @xmath159 if @xmath161 .",
    "in fact , these arguments go along the lines with the similar results of foster & george ( 1994 ) and birg & massart ( 2001 , 2007 ) . recall that binomial prior corresponds to linear penalties of the type @xmath21 in ( [ eq : pmle ] ) ( see section [ sec : map ] ) .",
    "foster & george ( 1994 ) and birg & massart ( 2001 , section 5.2 ) showed that the best possible risk of such estimators over @xmath99 is only of order @xmath162 achieved for @xmath163 corresponding to the ric criterion .",
    "it is of the same order as the optimal risk @xmath164 for @xmath165 ( sparse case ) but larger for dense case ( @xmath166 ) . on the other hand ,",
    "the risk of the aic estimator ( @xmath23 ) is of the order @xmath167 , which is optimal for dense but much larger for sparse case .",
    "furthermore , under somewhat similar nearly - orthogonality conditions , bickel , ritov & tsybakov ( 2009 ) showed that the well - known lasso ( tibshirani , 1996 ) and dantzig ( cands & tao , 2007 ) estimators achieve only the same sub - optimal rate @xmath168 as ric .",
    "these results are , in fact , not so surprising since both lasso and dantzig estimators are essentially based on convex relaxations of the @xmath102-norm of regression coefficients @xmath104 in the linear complexity penalty @xmath169 in order to replace the original combinatorial problem ( [ eq : pmle ] ) by a convex program .",
    "thus , lasso approximates the @xmath102-norm @xmath104 by the the corresponding @xmath170-norm @xmath171 . in particular , for the orthogonal design , linear complexity penalties and lasso yield respectively hard and soft thresholding of components of @xmath9 with a _",
    "fixed _ threshold .",
    "ric estimator and lasso with the optimally chosen tuning parameter ( e.g. , bickel , ritov & tsybakov , 2009 ) result in this case in the well - known hard and soft universal thresholding of donoho & johnstone ( 1994 ) with a fixed threshold @xmath172 which is rate - optimal for various sparse but not dense settings . on the other hand",
    ", the nonlinear map penalty corresponds to hard thresholding with a _ data - driven _ threshold that under conditions on @xmath140 in corollary [ cor : minimax ] is simultaneously minimax for both sparse and dense cases ( abramovich , grinshstein & pensky , 2007 ; abramovich _ et al .",
    "_ , 2010 ) .",
    "finally , note that for the nearly - orthogonal design , @xmath173 , where `` @xmath174 '' means that their ratio is bounded from below and above .",
    "therefore , all the results of corollary [ cor : minimax ] for estimating the mean vector @xmath149 in ( [ eq : model1 ] ) can be straightforwardly applied for estimating the regression coefficients @xmath139 .",
    "this equivalence , however , does not hold for the multicollinear design considered below .",
    "nearly - orthogonality assumption may be reasonable in the `` classical '' setup , where @xmath4 is not too large relatively to @xmath11 but might be questionable for the analysis of high - dimensional data , where @xmath134 , due to the multicollinearity phenomenon ( see also remark [ rem : orth ] ) .",
    "when this assumption does not hold , the sparse eigenvalues ratios in ( [ eq : lower ] ) may tend to zero as @xmath4 increases and , thus , decrease the minimax lower bound rate relatively to the nearly - orthogonal design . in this case",
    "there is a gap between the rates in the lower and upper bounds ( [ eq : lower ] ) and ( [ eq : upper ] ) .",
    "intuitively , one can think of exploiting correlations between predictors to reduce the size of a model ( hence , to decrease the variance ) without paying much extra price in the bias term , and , therefore , to reduce the risk . we show that under certain additional assumptions on the design and the coefficients vector in ( [ eq : model1 ] ) , the upper risk bound ( [ eq : upper ] ) can be indeed reduced to the minimax lower bound rate ( [ eq : lower ] ) .",
    "for simplicity of exposition we consider the sparse case @xmath131 although the corresponding conditions for the dense case @xmath175 can be obtained in a similar way with necessary changes .",
    "we introduce now several definitions that will be used in the sequel ( including the proofs in the appendix ) . for a given index set @xmath176 and @xmath177 define a @xmath178 matrix @xmath179 which columns @xmath180 are the elements of the standard basis in @xmath181 .",
    "thus , for any matrix @xmath182 with @xmath183 columns , @xmath184 selects the columns of @xmath182 indexed by @xmath176 .",
    "similarly , for any @xmath185 symmetric matrix @xmath182 , @xmath186 generates a ( symmetric ) @xmath187 submatrix of @xmath182 of the corresponding columns and rows .    for all @xmath188 , define @xmath189\\cdot k \\rceil \\geq 1 $ ] .",
    "let @xmath190 be an index set of predictors included in a model @xmath12 of size @xmath191 .",
    "for any submodel @xmath192 of size @xmath193 let @xmath194 be the minimal eigenvalue of the @xmath195 matrix @xmath196 .",
    "in fact , @xmath197 is the covariance matrix of the components of the least squares estimate vector @xmath14 corresponding to a subset of predictors in @xmath198 .    finally , define @xmath199=\\min_{m : |m|=k } \\max_{m ' \\subset m :   |m'|=k ' }   \\tilde{\\phi}_{m , m ' } \\ ] ] as we show later ( see the proof of theorem [ th : nonorth ] in the appendix ) , @xmath200 $ ] measures an error of approximating mean vectors @xmath149 , where @xmath201 , by their projections on lower dimensional subspans of predictors .",
    "the stronger is multicollinearity , the better is the approximation and the larger is @xmath202 $ ] .",
    "[ th : nonorth ] let @xmath143 \\rightarrow 0 $ ] as @xmath203 ( multicollinear design ) . assume the following additional assumptions on the design matrix @xmath138 and the ( unknown ) vector of coefficients @xmath139 in ( [ eq : model1 ] ) :    ( d ) : :    for all @xmath4 there exist    @xmath204 such that    +    1",
    ".   @xmath205\\cdot k \\leq k-1,\\;k=\\kappa_{p1 } , ... ,    \\kappa_{p2}$ ]    2 .",
    "@xmath206 \\geq ( \\kappa_{p2}/(pe))^{\\tilde{c}_2}$ ]    3 .",
    "@xmath207\\cdot \\tilde{\\phi}_p[k ] \\geq     \\tilde{c}_3,\\;k=\\kappa_{p1}, ... ,\\kappa_{p2}$ ] ( b ) : :    @xmath208 \\cdot \\tilde{\\phi}_p[p_0]\\cdot(\\ln(p / p_0)+1)$ ] , where    @xmath209    for some positive constants @xmath210 and @xmath211 .",
    "then , under the above additional restrictions , if the prior @xmath140 satisfies assumption ( p ) and for all @xmath212 , @xmath213 for some positive @xmath110 , where @xmath189\\cdot k \\rceil$ ] , the corresponding map model selector is asymptotically simultaneously minimax ( up to a constant multiplier ) over all @xmath214 .",
    "note that by simple algebra one can verify that @xmath207\\cdot \\tilde{\\phi}_p[k ] \\leq 1 $ ] and , therefore , the constant @xmath215 in assumption ( d.3 ) is not larger than one .",
    "we have argued that multicollinearity typically arises when @xmath134 .",
    "one can easily verify that for @xmath216 , assumption ( d.2 ) always follows from assumption ( d.1 ) and , therefore , can be omitted in this case .",
    "as we show in the proof , assumptions ( d.1 , d.2 ) and assumption ( b ) allow one to reduce the upper bound ( [ eq : upper ] ) for the risk of the map model selector by the factor @xmath217 $ ] , while assumption ( d.3 ) is required to guarantee that the additional constraint on @xmath139 in assumption ( b ) does not affect the lower bound ( [ eq : lower ] ) .    to obtain asymptotic minimaxity of the map selector within the entire range @xmath218 similar to corollary [ cor : minimax ] for the nearly - orthogonal case , assumptions ( d ) on the design matrix are required to be satisfied for all @xmath188 that might be quite restrictive .",
    "however , the results of theorem [ th : nonorth ] are more general and show the tradeoff between relaxation of assumptions ( d ) to a smaller range of @xmath49 and the corresponding constriction of the adaptivity range for @xmath100 .",
    "in practice , minimizing ( [ eq : pmle ] ) ( and ( [ eq : map ] ) in particular ) requires generally an np - hard combinatorial search over all possible models . during the last decade",
    "there have been substantial efforts to develop various _ approximated _ algorithms for solving ( [ eq : pmle ] ) that are computationally feasible for high - dimensional data ( see , e.g. tropp & wright , 2010 for a survey and references therein ) .",
    "the common remedies involve either greedy algorithms ( e.g. , forward selection , matching pursuit ) approximating the global solution by a stepwise sequence of local ones , or convex relaxation methods replacing the original combinatorial problem by a related convex program ( e.g. , lasso and dantzig selector for linear penalties ) .",
    "the proposed bayesian formalism allows one instead to use a stochastic search variable selection ( ssvs ) techniques originated in george & mcculloch ( 1993 , 1997 ) for solving ( [ eq : map ] ) by generating a sequence of models from the posterior distribution @xmath219 in ( [ eq : post ] ) .",
    "the key point is that the relevant models with the highest posterior probabilities will appear most frequently and can be identified even for a generated sample of a relatively small size avoiding computations of the entire posterior distribution .",
    "the ssvs algorithm for the problem at hand can be basically described as follows .",
    "as we have mentioned in section [ sec : map ] , every model @xmath12 is uniquely defined by the corresponding indicator vector @xmath220 and the joint posterior distribution of @xmath220 is given by ( [ eq : post ] ) ( up to a normalizing constant ) .",
    "ssvs uses the gibbs sampler to generate a sequence of indicator vectors @xmath221 _ componentwise _ by sampling consecutively from the conditional distributions @xmath222 , where @xmath223 .",
    "the components @xmath224 can be trivially obtained as simulations of bernoulli draws , where from ( [ eq : post ] ) the corresponding posterior odds ratio @xmath225 and @xmath226 is the increment in the residual sum of squares ( rss ) after dropping the @xmath227-th predictor from the model @xmath228 .",
    "the resulting gibbs sampler is computationally efficient and , as @xmath229 increases , the empirical distribution of the generated sample converges to the actual posterior distribution of @xmath230 . after the sequence has reached approximate stationarity",
    ", one can identify the most frequently appeared vector(s ) @xmath231 as potential candidate(s ) to solve ( [ eq : map ] ) .",
    "in this paper we considered a bayesian approach to model selection in gaussian linear regression . from a frequentist view , the resulting map model selector is a penalized least squares estimator with a complexity penalty associated with a prior @xmath64 on the model size .",
    "although the proposed estimator was originated within bayesian framework , the latter was used as a natural tool to obtain a wide class of penalized least squares estimators with various complexity penalties .",
    "thus , we believe that the main take - away messages of the paper summarized below are of a more general interest .",
    "the first main take - away message is that neither linear complexity penalties ( e.g. , aic , bic and ric ) corresponding to binomial priors @xmath64 , nor closely related lasso and dantzig estimators can be simultaneously minimax for both sparse and dense cases .",
    "we specify the class of priors and associated nonlinear penalties that do yield such a wide adaptivity range .",
    "in particular , it includes @xmath26-type penalties .",
    "another important take - away message is about the effect of multicollinearity of design . unlike model identification or coefficients estimation , where multicollinearity is a `` curse '' ,",
    "it may become a `` blessing '' for estimating the mean vector allowing one to exploit correlations between predictors to reduce the size of a model ( hence , to decrease the variance ) without paying much extra price in the bias term .",
    "interestingly , a similar phenomenon occurs in a testing setup ( e.g. , hall & jin , 2010 ) .",
    "the authors would like to thank alexander samarov and yacov ritov for valuable remarks .",
    "the authors are especially grateful to an anonymous referee for an excellent constructive review of the first version of the paper .",
    "99    abramovich , f. , benjamini , y. , donoho , d.l . and johnstone , i.m .",
    "adapting to unknown sparsity by controlling the false discovery rate .",
    "statist . _",
    "* 34 * , 584653 .",
    "abramovich , f. , grinshtein , v. and pensky , m. ( 2007 ) .",
    "on optimality of bayesian testimation in the normal means problem .",
    "statist . _",
    "* 35 * , 22612286 .",
    "abramovich , f. , grinshtein , v. , petsa , a. and sapatinas , t. ( 2010 ) . on bayesian testimation and its application to wavelet thresholding .",
    "_ biometrika _ * 97 * , 181198 .",
    "akaike , h. ( 1973 ) .",
    "information theory and an extension of the maximum likelihood principle . in _",
    "second international symposium on information theory .",
    "b.n . petrov and f. czki ) .",
    "akademiai kiad , budapest , 267 - 281 .",
    "bickel , p. , ritov , y. and tsybakov , a. ( 2009 ) .",
    "simultaneous analysis of lasso and dantzig selector .",
    "statist . _",
    "* 35 * , 17051732 .",
    "birg , l. and massart , p. ( 2001 ) .",
    "gaussian model selection .",
    "soc . _ * 3 * , 203268 .",
    "birg , l. and massart , p. ( 2007 ) .",
    "minimal penalties for gaussian model selection . _",
    "theory relat .",
    "fields _ * 138 * , 3373 .",
    "bunea , f. , tsybakov , a. and wegkamp , m.h .",
    "aggregation for gaussian regression .",
    "statist . _ * 35 * , 16741697 .",
    "cands , e.j .",
    "modern statistical estimation via oracle inequalities .",
    "_ acta numerica _ , 169 .",
    "cands , e.j . and tao , t. ( 2007 ) .",
    "the dantzig selector : statistical estimation when @xmath4 is much larger than @xmath11 .",
    "statist . _",
    "* 35 * , 23132351 .",
    "chipman , h. , george , e.i . and mccullogh , r.e .",
    "( 2001 ) . _",
    "the practical implementation of bayesian model selection .",
    "_ _ ims lecture notes ",
    "monograph series _ * 38*.    donoho , d.l . and johnstone , i.m .",
    "ideal spatial adaptation via wavelet shrinkage .",
    "_ biometrika _ * 81 * , 425455 .",
    "donoho , d.l . and johnstone , i.m .",
    "( 1995 ) . empirical atomic decomposition , _",
    "unpublished manuscript_.    foster , d.p . and george , e.i .",
    "the risk inflation criterion for multiple regression .",
    "statist . _",
    "* 22 * , 19471975 .",
    "george , e.i . and mccullogh , r.e .",
    "variable selection via gibbs sampling .",
    "_ j. am . statist .",
    "assoc . _ * 88 * , 881889 .",
    "george , e.i . and mccullogh , r.e .",
    "approaches to bayesian variable selection .",
    "_ statistica sinica _ * 7 * , 339373 .",
    "greenshstein , e. and ritov , y. ( 2004 ) .",
    "persistence in high - dimensional linear predictor selection and the virtue of overparametrization .",
    "_ bernoulli _ * 10 * , 971988 .    hall , p. and jin , j. ( 2010 ) . innovated higher criticism for detecting sparse signals in correlated noise .",
    "statist . _",
    "* 38 * , 16811732 .",
    "johnstone , i.m .",
    "_ function estimation and gaussian sequence models _ , _ unpublished manuscript_.    liang , f. , paulo , r. , molina , g. , clyde , m. and berger , j.o .",
    "mixtures of @xmath56 priors for bayesian variable selection .",
    "_ j. am . statist .",
    "assoc . _ * 103 * , 410423 .",
    "meinshausen , n. and yu , b. ( 2009 ) .",
    "lasso - type recovery of sparse representations for high - dimensional data . _ ann .",
    "statist . _",
    "* 37 * , 246270 .",
    "raskutti , g. , wainwright , m.j . and",
    "yu , b. ( 2009 ) .",
    "minimax rates of estimations for high - dimensional regression over @xmath232 balls .",
    "_ technical report , uc berkeley _ , http://arxiv.org/abs/0910/2042 .",
    "rigollet , p. and tsybakov , a. ( 2010 ) .",
    "exponential screening and optimal rates of sparse estimation .",
    "http://arxiv.org/pdf/1003.2654 .",
    "schwarz , g. ( 1978 ) .",
    "estimating the dimension of a model .",
    "statist . _",
    "* 6 * , 461464 .",
    "tibshirani , r. ( 1996 ) .",
    "regression shrinkage and selection via the lasso .",
    "b _ * 58 * , 267288 .    tropp , j.a . and wright , s.j .",
    "computational methods for sparse solution of linear inverse problems .",
    "ieee , special issue `` applications of sparse representation and comprehensive sensing''_.    tsybakov , a. ( 2009 ) . _",
    "introduction to nonparametric estimation_. springer .",
    "zellner , a. ( 1986 ) . on assessing prior distributions and bayesian regression analysis with @xmath56-prior distributions .",
    "bayesian inference and decision techniques : essays in honor of bruno de finietti _ ( eds .",
    "goel , p.k . and zellner , a. ) , north - holland , amsterdam , 233243 .",
    "throughout the proofs we use @xmath233 to denote a generic positive constant , not necessarily the same each time it is used , even within a single equation .",
    "define @xmath234 and @xmath235 in terms of @xmath236 the complexity penalty ( [ eq : pen])-([eq : pen1 ] ) is @xmath237 . following the arguments of the proof of theorem 1 of abramovich _ et al . _",
    "( 2007 ) , under the assumption ( p ) one has @xmath238 and @xmath239 the proof of theorem [ th : bound ] then follows directly from theorem 2 of birg & massart ( 2001 ) . @xmath240                on the other hand , applying the general upper bound for the risk of map model selector established in theorem [ th : bound ] for models of size @xmath252 we have @xmath253 abramovich _ et al . _",
    "( 2010 , lemma 1 ) showed that @xmath254 .",
    "hence , under the conditions on @xmath255 in theorem [ th : upper ] , for @xmath256 , ( [ eq : t1.1 ] ) yields @xmath257 finally , note that for @xmath109 , as we have already established in ( [ eq : t1.0 ] ) , @xmath258 @xmath240      the core of the proof is to find a subset @xmath259 of vectors @xmath9 , where @xmath103 , and the corresponding subset of mean vectors @xmath260 such that for any @xmath261 , @xmath262 and the kullback - leibler divergence @xmath263 .",
    "lemma a.1 of bunea , tsybakov & wegkamp ( 2007 ) will imply then that @xmath264 is the minimax lower bound over @xmath99 .",
    "@xmath131 define the subset @xmath266 of all vectors @xmath6 that have @xmath100 entries equal to @xmath267 defined later , while the remaining entries are zeros : latexmath:[$\\tilde{\\cal b}_{p_0}=\\{{{\\mbox{\\boldmath $ \\beta$ } } } : { { \\mbox{\\boldmath $ \\beta$}}}\\in \\{\\{0,c_{p_0}\\}^{p}\\},\\ ;          on the other hand , by similar arguments , the kullback - leibler divergence satisfies @xmath278 c^2_{p_0 } \\rho({{\\mbox{\\boldmath $ \\beta$}}}_1,{{\\mbox{\\boldmath $ \\beta$}}}_2)}{2\\sigma^2 } \\leq \\frac{\\phi_{max}[2p_0 ] c^2_{p_0 } p_0}{\\sigma^2 } \\label{eq : t4.2}\\ ] ] set now @xmath279 $ ] and @xmath280p_0(\\ln(p / p_0)+1)$ ] .",
    "then , ( [ eq : t4.1 ] ) and ( [ eq : t4.2 ] ) yield @xmath281 , @xmath282 , and lemma a.1 of bunea , tsybakov & wegkamp ( 2007 ) completes the proof .",
    "@xmath283 in this case consider the subset @xmath284 and apply varshamov - gilbert bound ( see , e.g. tsybakov , 2009 , lemma 2.9 ) .",
    "it guarantees the existence of a subset @xmath269 such that @xmath285 and the hamming distance @xmath286 for any pair @xmath272 .",
    "note also that for any @xmath287 , @xmath288 has at most @xmath100 non - zero componens and repeating the arguments for the case 1 , one achieves the minimax lower bound @xmath289p_0 \\geq ( c/2)\\sigma^2\\tau[p_0]r$ ] .",
    "@xmath290 for this case , obviously , @xmath291 .",
    "consider a trivial subset @xmath259 containing just two vectors @xmath292 and @xmath293 that has @xmath100 nonzero entries equal to @xmath294 $ ] .",
    "for the corresponding mean vectors @xmath295 and @xmath296 , following ( [ eq : t4.1 ] ) and ( [ eq : t4.2 ] ) one has @xmath297 8 c_{p_0}^2}{2\\sigma^2 } =   ( 1/16 ) \\ln{\\rm card}({\\cal g}_{p_0})\\ ] ] and @xmath298 p_0 c^2_{p_0 } = c\\sigma^2\\tau[p_0]p_0 \\geq   ( c/2 ) \\sigma^2\\tau[p_0]r\\ ] ] applying lemma a.1 of bunea , tsybakov & wegkamp ( 2007 ) completes the proof .",
    "we want to show that under the conditions of theorem [ th : nonorth ] we can reduce the rate in the upper bound ( [ eq : upper ] ) for the risk of the map model selector established in theorem [ th : upper ] over @xmath99 by the factor @xmath217 $ ] .",
    "recall that we derived ( [ eq : upper ] ) from the general upper bound ( [ eq : bound ] ) in theorem [ th : bound ] by considering models @xmath12 of size @xmath100 .",
    "consider now @xmath299 and apply ( [ eq : bound ] ) for models @xmath300 of less size @xmath301p_0 \\rceil \\geq 1 $ ] .",
    "consider an arbitrary @xmath139 with @xmath302 . under the conditions of theorem [ th : nonorth ] on",
    "the prior @xmath140 , ( [ eq : bound ] ) implies @xmath303 where @xmath304 is the projection of the mean vector @xmath149 on the span of @xmath300 . comparing ( [ eq : upper ] ) and ( [ eq : t5.0 ] )",
    "illustrates that reduction of a model size introduces the bias . on the other hand , under assumptions ( d.1 ) and ( d.2 ) ,",
    "a straightforward calculus shows then that the variance term decreases to the desired order @xmath305(\\ln(p / p_0)+1)$ ] .",
    "the idea of the proof will be based on finding a model @xmath306 such that the resulting bias term will be at most of the same order as the reduced variance .",
    "consider the model @xmath12 of size @xmath100 corresponding to @xmath139 and any of its submodels @xmath300 of size @xmath307 defined above .",
    "then , @xmath198 is evidently a subset of predictors from @xmath12 not included in @xmath300 and @xmath308 , where diagonal indicator matrices @xmath309 s were introduced in section [ sec : map ] . by straightforward calculus one",
    "then has @xmath310 where the matrices @xmath311 and @xmath312 and the minimal eigenvalue @xmath194 were defined in section [ subsec : multi ] .    among all submodels @xmath192 of size @xmath307 , choose @xmath306 with the maximal @xmath194 .",
    "then , @xmath313 $ ] and ( [ eq : t5.1 ] ) and assumption ( b ) yield @xmath314p_0\\ln((p / p_0)+1)\\ ] ] hence , we proved that under assumptions on the prior , assumptions ( d.1 , d.2 ) and ( b ) , the upper bound for the risk of the map model selector over @xmath99 is of the minimax order @xmath217p_0\\ln(p / p_0)+1)$ ] .",
    "assumption ( d.3 ) guarantees that the `` least - favorable '' sets @xmath259 constructed in the proof of theorem [ th : lower ] satisfy the additional assumption ( b ) on @xmath139 and , therefore , the minimax lower bound ( [ eq : lower ] ) is not reduced ."
  ],
  "abstract_text": [
    "<S> we consider a bayesian approach to model selection in gaussian linear regression , where the number of predictors might be much larger than the number of observations . from a frequentist view , the proposed procedure results in the penalized least squares estimation with a complexity penalty associated with a prior on the model size . </S>",
    "<S> we investigate the optimality properties of the resulting model selector . </S>",
    "<S> we establish the oracle inequality and specify conditions on the prior that imply its asymptotic minimaxity within a wide range of sparse and dense settings for `` nearly - orthogonal '' and `` multicollinear '' designs . </S>"
  ]
}