{
  "article_text": [
    "generalized matrix functions were first introduced by hawkins and ben - israel in @xcite in order to extend the notion of a matrix function to rectangular matrices . essentially , the definition is based on replacing the spectral decomposition of @xmath0 ( or the jordan canonical form , if @xmath0 is not diagonalizable ) with the singular value decomposition , and evaluating the function at the singular values of @xmath0 , if defined . while it is likely that this definition was inspired by the analogy between the inverse and the moore ",
    "penrose generalized inverse , which is well established in numerical linear algebra , @xcite is a purely theoretical paper and does not mention any potential applications or computational aspects .",
    "the paper appears to have gone largely unnoticed , despite increasing interest in matrix functions in the numerical linear algebra community over the past several years ; for instance , it is not cited in the important monograph by higham @xcite .",
    "while it is likely that the perceived scarcity of applications is to blame ( at least in part ) for this lack of attention , it turns out that generalized matrix functions do have interesting applications and have actually occurred in the literature without being recognized as such ; see section [ sec : app ] for some examples .    in this paper",
    "we revisit the topic of generalized matrix functions , with an emphasis on numerical aspects . after reviewing the necessary background and definitions",
    ", we consider a few situations naturally leading to generalized matrix functions . moving on to numerical considerations",
    ", we develop several computational approaches based on variants of golub  kahan bidiagonalization to compute or estimate bilinear forms involving generalized matrix functions , including entries of the generalized matrix function itself and the action of a generalized matrix function on a vector .",
    "we further consider block variants of golub  kahan bidiagonalization which can be used to evaluate matrix - valued expressions involving generalized matrix functions .",
    "numerical experiments are used to illustrate the performance of the proposed techniques on problems arising in the analysis of directed networks .",
    "in this section we review a few basic concepts from linear algebra that will be used throughout the paper , mostly to set our notation , and recall the notion of generalized matrix function .",
    "let @xmath1 and let @xmath2 be the rank of @xmath0 .",
    "we can factor the matrix @xmath0 as @xmath3 using a singular value decomposition ( svd ) .",
    "the matrix @xmath4 is diagonal and its entries @xmath5 are ordered as @xmath6 , where @xmath7 .",
    "the @xmath2 positive @xmath8 are the singular values of @xmath0 .",
    "the matrices @xmath9\\in{{\\mathbb{c}}}^{m\\times m}$ ] and @xmath10\\in{{\\mathbb{c}}}^{n\\times n}$ ] are unitary and contain the left and right singular vectors of @xmath0 , respectively .",
    "it is well known that the matrix @xmath11 is uniquely determined , while @xmath12 and @xmath13 are not .",
    "if @xmath0 is real , then @xmath12 and @xmath13 can be chosen to be real . from the singular value decomposition of a matrix @xmath0",
    "it follows that @xmath14 and @xmath15 .",
    "thus , the singular values of a matrix @xmath0 are the square roots of the positive eigenvalues of the matrix @xmath16 or @xmath17 . moreover , the left singular vectors of @xmath0 are the eigenvectors of the matrix @xmath16 , while the right singular vectors are the eigenvectors of the matrix @xmath17 .",
    "the singular values of a matrix also arise ( together with their opposites ) as the eigenvalues of the hermitian matrix @xmath18 this can be easily seen for the case @xmath19 ; indeed , under this hypothesis , the spectral factorization of @xmath20 is given by  @xcite : @xmath21    consider now the matrices @xmath22 and @xmath23 which contain the first @xmath2 columns of the matrices @xmath12 and @xmath13 , respectively , and let @xmath24 be the @xmath25 leading block of @xmath11 .",
    "then a _ compact svd _",
    "( _ csvd _ ) of the matrix @xmath0 is @xmath26      there are several equivalent ways to define @xmath27 when @xmath28 is a square matrix .",
    "we recall here the definition based on the jordan canonical form . for a comprehensive study of matrix functions ,",
    "we refer to  @xcite .",
    "let @xmath29 be the set of distinct eigenvalues of @xmath0 and let @xmath30 denote the _ index _ of the @xmath31th eigenvalue , i.e. , the size of the largest jordan block associated with @xmath32 . recall that a function @xmath33 is said to be _ defined on the spectrum of @xmath0 _ if the values @xmath34 exist for all @xmath35 and for all @xmath36 , where @xmath37 is the @xmath38th derivative of the function and @xmath39 .",
    "* definition 1.2)[def : fun ] let @xmath33 be defined on the spectrum of @xmath40 and let @xmath41 be the jordan canonical form of the matrix , where @xmath42 @xmath43 , and @xmath44 is nonsingular .",
    "then @xmath45 where @xmath46    if the matrix @xmath0 is diagonalizable , then the jordan canonical form reduces to the spectral decomposition : @xmath47 with @xmath48 . in such case ,",
    "@xmath49 .",
    "if the function @xmath33 has a taylor series expansion , we can use it to describe the associated matrix function , provided that the eigenvalues of the matrix @xmath0 satisfy certain requirements .",
    "* theorem 4.7 ) let @xmath28 and suppose that @xmath33 can be expressed as @xmath50 with radius of convergence @xmath51 . then @xmath27 is defined and is given by @xmath52 if and only if each of the distinct eigenvalues of @xmath0 @xmath29 satisfies one of the following :    * @xmath53 ; * @xmath54 and the series for @xmath55 is convergent at the point @xmath56 , @xmath57 .      in @xcite the authors considered the problem of defining functions of rectangular matrices .",
    "their definition relies on the following generalization of the svd .",
    "[ thm : ci ] let @xmath58 be a matrix of rank @xmath2 and let @xmath59 be any complex numbers satisfying @xmath60 where @xmath61 are the positive eigenvalues of @xmath16 . then there exist two unitary matrices @xmath62 and @xmath63 such that @xmath64 has entries : @xmath65    from this theorem it follows that , once the non - zero entries of @xmath66 are fixed , @xmath0 can be written as @xmath67 where @xmath68 is the leading @xmath25 block of @xmath69 and the matrices @xmath70 and @xmath71 consist of the first @xmath2 columns of the matrices @xmath72 and @xmath73 , respectively .",
    "in this paper we do not make use of the extra degrees of freedom provided from this ( slightly ) more general svd of @xmath0 , and we assume that @xmath74 for all @xmath75 .",
    "this assumption ensures that the decompositions in coincide with the svd and csvd of the matrix @xmath0 , respectively .",
    "in particular , @xmath76 , @xmath77 , and @xmath78 . all the definitions and results presented in the remaining of this section and in the next one can be extended to the case when the coefficients @xmath79 do not necessarily coincide with the singular values , but satisfy the hypothesis of theorem  [ thm : ci ] .",
    "[ def : gf ] let @xmath1 be a rank @xmath2 matrix and let @xmath80 be its csvd .",
    "let @xmath81 be a scalar function such that @xmath82 is defined for all @xmath75 .",
    "the generalized matrix function @xmath83 induced by @xmath33 is defined as @xmath84 where @xmath85 is defined for the square matrix @xmath86 according to definition  [ def : fun ] as @xmath87    as already mentioned in the introduction , generalized matrix functions arise , for instance , when computing @xmath88 , where @xmath20 is the matrix defined in  .",
    "indeed , if one uses the description of matrix function in terms of power series @xmath89 , it is easy to check that , within the radius of convergence : @xmath90 where @xmath91     if @xmath92 and the matrix @xmath28 is hermitian positive semidefinite , then the generalized matrix function @xmath93 reduces to the standard matrix function @xmath27 .",
    "if the more general decomposition of theorem [ thm : ci ] is used instead , then the generalized matrix function reduces to @xmath27 if @xmath92 and the matrix @xmath28 is normal , as long as @xmath33 is defined on the @xmath94 ; see @xcite .    the moore ",
    "penrose pseudo - inverse of a matrix @xmath1 , denoted by @xmath95 , can be expressed as @xmath96 , where @xmath97 .",
    "equivalently , @xmath98 when @xmath97 .",
    "hence , there is a slight disconnect between the definition of generalized matrix function and that of generalized inverse .",
    "this small issue could be addressed by defining a generalized matrix function corresponding to the scalar function @xmath33 as @xmath99 , so that the generalized matrix function of an @xmath100 matrix is an @xmath101 matrix , and @xmath102 when @xmath97 . however , doing so would lead to the undesirable property that @xmath103 for @xmath104 , as well as other problems .",
    "in this section we review some properties of generalized matrix functions and we summarize a few new results .    letting @xmath105 and @xmath106",
    ", we can write @xmath107 and thus it follows that @xmath108    ( sums and products of functions @xcite ) . let @xmath109 be scalar functions and let @xmath110 be the corresponding generalized matrix functions . then :    * if @xmath111 , then @xmath112 ; * if @xmath113 , then @xmath114 ; * if @xmath115 , then @xmath116 ; * if @xmath117 , then @xmath118 .    in the following",
    "we prove a few properties of generalized matrix functions .",
    "[ thm : prop ] let @xmath1 be a matrix of rank @xmath2 .",
    "let @xmath81 be a scalar function and let @xmath119 be the induced generalized matrix function , assumed to be defined at @xmath0 .",
    "then the following properties hold true .    * @xmath120^ * = { f^{\\diamond}}(a^*)$ ] ; * let @xmath121 and @xmath122 be two unitary matrices , then @xmath123y$ ] ; * if @xmath124 , then @xmath125 * @xmath126 , where @xmath127 is the @xmath128 identity matrix and @xmath129 is the kronecker product ; * @xmath130 .    * from   it",
    "follows that @xmath131 , and thus @xmath132^ * = [ { f^{\\diamond}}(a)]^*.\\ ] ] * the result follows from the fact that unitary matrices form a group under multiplication and that the rank of a matrix does not change under left or right multiplication by a nonsingular matrix  @xcite .",
    "indeed , the matrix @xmath133 has rank @xmath2 and thus @xmath134^ * \\\\           & = x u_r f(\\sigma_r ) v^*_r y             = x{f^{\\diamond}}(a)y .",
    "\\end{aligned}\\ ] ] where @xmath135 and @xmath136 are the matrices containing the first @xmath2 columns of @xmath137 and @xmath138 , respectively .",
    "* let @xmath139 be the csvd of the rank-@xmath140 matrix @xmath141 for @xmath142 .",
    "then @xmath143 , where @xmath144 is a diagonal matrix whose diagonal entries are ordered ( via the permutation matrix @xmath145 ) in non - increasing order , and @xmath146 @xmath147 from the definition of generalized matrix function and from some basic properties of standard matrix functions @xcite it follows that @xmath148 * the result follows from ( iii ) and the fact that @xmath149 is a @xmath150 diagonal block matrix with @xmath151 copies of @xmath0 on the main diagonal .",
    "* it follows from ( iv ) and from the fact that for two general matrices @xmath1 and @xmath152 , there exist two permutation matrices @xmath153 and @xmath154 called _ commutation matrices _ such that @xmath155 ( see  ( * ? ? ?",
    "3 ) ) .    the following theorem provides a result for the composition of two functions .",
    "( composite functions ) let @xmath1 be a rank-@xmath2 matrix and let @xmath156 be its singular values .",
    "assume that @xmath157 and @xmath158 are two scalar functions such that @xmath159 and @xmath160 exist for all @xmath75 .",
    "let @xmath161 and @xmath162 be the induced generalized matrix functions .",
    "moreover , let @xmath81 be the composite function @xmath163 .",
    "then the induced matrix function @xmath119 satisfies @xmath164    let @xmath165 . since @xmath159 for all @xmath75 , this matrix has rank @xmath2 .",
    "we want to construct a csvd of the matrix @xmath166 .",
    "let thus @xmath167 be a permutation matrix such that the matrix @xmath168 has diagonal entries ordered in non - increasing order .",
    "then it follows that a csvd of the matrix @xmath166 is given by @xmath169 , where @xmath170 and @xmath171 have orthonormal columns .",
    "it thus follows that @xmath172    the following result describes the relationship between standard matrix functions and generalized matrix functions .",
    "[ thm : genfun ] let @xmath1 be a rank-@xmath2 matrix and let @xmath81 be a scalar function .",
    "let @xmath119 be the induced generalized matrix function .",
    "then    @xmath173    or , equivalently , @xmath174    the two identities are an easy consequence of the fact that @xmath175 and @xmath176 for @xmath75 .",
    "let @xmath1 be a rank-@xmath2 matrix and let @xmath81 and @xmath158 be two scalar functions such that @xmath93 and @xmath177 are defined .",
    "then @xmath178    from @xmath179 it follows @xmath180 and @xmath181 ; thus @xmath182",
    "as mentioned in the introduction , generalized matrix functions ( in the sense of hawkins and ben - israel ) have appeared in the literature without being recognized as such .",
    "here we discuss a few examples that we are aware of .",
    "no doubt there have been other such instances .    in @xcite ,",
    "the authors address the problem of computing functions of real skew - symmetric matrices , in particular the evaluation of the product @xmath183 for a given skew - symmetric matrix @xmath0 and vector @xmath184 using the lanczos algorithm .",
    "the authors observe that any @xmath185 with @xmath186 is orthogonally similar to a matrix of the form @xmath187 where @xmath166 is lower bidiagonal of order @xmath188 . as a consequence ,",
    "if @xmath189 is an svd of @xmath166 , the matrix exponential @xmath190 is orthogonally similar to the matrix @xmath191 where the matrix in the upper right block is precisely @xmath192 .",
    "the authors of @xcite develop computational techniques for the matrix exponential based on ( [ eq : lopez ] ) .",
    "we also mention that in the same paper the authors derive a similar expression , also found in @xcite , for the exponential of the symmetric matrix @xmath20 given in ( [ eq : cala ] ) .",
    "these expressions are extended to more general matrix functions in @xcite , where they are used to investigate the off - diagonal decay of analytic functions of large , sparse , skew - symmetric matrices .",
    "furthermore , in @xcite it is shown how these ideas can be used to develop efficient geometrical integrators for the numerical solution of certain hamiltonian differential systems .    in @xcite ,",
    "the authors consider the problem of detecting ( approximate ) directed bipartite communities in directed graphs .",
    "consideration of alternating walks in the underlying graph leads them to introducing a  non - standard matrix function \" of the form @xmath193 where @xmath0 is the adjacency matrix of the graph . using @xmath194 this expression",
    "is readily recognized to be equivalent to @xmath195 which is a  mixture \" of the standard matrix function @xmath196 and the generalized matrix function @xmath197 .",
    "as mentioned , generalized hyperbolic matrix functions were also considered in @xcite in the context of directed networks , also based on the notion of alternating walks in directed graphs . in @xcite , the action of generalized matrix functions on a vector of all ones was used to define certain centrality measures for nodes in directed graphs ; here the connection with the work of hawkins and ben - israel was explicitly made .",
    "finally , we mention that generalized matrix functions arise when _ filter factors _",
    "are used to regularize discrete ill - posed problems ; see , e.g. , @xcite .",
    "the computation of the generalized matrix functions defined as in definition [ def : gf ] requires the knowledge of the singular value decomposition of @xmath0 . when @xmath198 and @xmath188 are large , computing the svd may be unfeasible .",
    "moreover , in most applications it is not required to compute the whole matrix @xmath93 ; rather , the goal is often to estimate quantities of the form @xmath199 or to compute the action of the generalized matrix function on a set of @xmath151 vectors , i.e. , to evaluate @xmath200 , usually with @xmath201 .",
    "for example , computing selected columns of @xmath93 reduces to the evaluation of @xmath200 where @xmath202 consists of the corresponding columns of the identity matrix @xmath203 , and computing selected entries of @xmath93 requires evaluating @xmath204 where @xmath44 contains selected columns of the identity matrix @xmath205 .",
    "the problem of estimating or giving bounds on such quantities can be tackled , following  @xcite , by using gauss - type quadrature rules . as usual in the literature",
    ", we will first analyze the case @xmath206 ; the case of @xmath207 will be dealt with in section  [ sec : approach_bl ] .",
    "it is known that in certain cases gauss - type quadrature rules can be used to obtain lower and upper bounds on bilinear forms like @xmath209 , where @xmath27 is a ( standard ) matrix function and @xmath210 .",
    "this is the case when @xmath33 enjoys certain monotonicity properties .",
    "recall that a real - valued function @xmath33 is _ completely monotonic _",
    "_ ) on an interval @xmath211 $ ] if it is continuous on @xmath212 and infinitely differentiable on @xmath213 with @xmath214 where @xmath215 denotes the @xmath188th derivative of @xmath33 and @xmath39 .",
    "if @xmath33 is completely monotonic on an interval containing the spectrum of @xmath210 , then one can obtain lower and upper bounds on quadratic forms of the type @xmath216 and from these lower and upper bounds on bilinear forms like @xmath217 with @xmath218 . for a general @xmath33 ,",
    "on the other hand , gaussian quadrature can only provide estimates of these quantities .",
    "similarly , in order to obtain bounds ( rather than mere estimates ) for bilinear expressions involving generalized matrix functions , we need the scalar functions involved in the computations to be completely monotonic .",
    "we will be applying our functions to diagonal matrices that contain the singular values of the matrix of interest .",
    "thus , in our framework , the interval on which we want to study the complete monotonicity of the functions is @xmath219 .",
    "we briefly recall here a few properties of c.  m.  functions ; see , e.g. , @xcite and references therein for systematic treatments of complete monotonicity .",
    "[ lemma3 ] if @xmath220 and @xmath221 are completely monotonic functions on @xmath212 , then    * @xmath222 with @xmath223 is completely monotonic on @xmath212 ; * @xmath224 is completely monotonic on @xmath212 .",
    "* theorem  2)[lemma1 ] let @xmath220 be completely monotonic and let @xmath221 be a nonnegative function such that @xmath225 is completely monotonic .",
    "then @xmath226 is completely monotonic .    using these lemmas",
    ", we can prove the following useful result .",
    "if @xmath33 is completely monotonic on @xmath227 , then @xmath228 is completely monotonic on @xmath227 .",
    "let @xmath229 ; then by lemma  [ lemma3 ] ( ii ) we know that @xmath230 is completely monotonic on @xmath219 if both @xmath231 and @xmath232 are completely monotonic on @xmath212 .",
    "the function @xmath233 is positive on the interval @xmath227 ; moreover , it is such that its first derivative @xmath234 is completely monotonic on @xmath212 .",
    "therefore , from lemma  [ lemma1 ] it follows that if @xmath33 is c.m .  , then @xmath231 is .",
    "similarly , since @xmath229 is completely monotonic , @xmath232 is completely monotonic .",
    "this concludes the proof .    in the following ,",
    "we propose three different approaches to approximate the bilinear forms of interest .",
    "the first approach exploits the results of theorem  [ thm : genfun ] to describe @xmath235 as a bilinear form that involves standard matrix functions of a tridiagonal matrix .",
    "the second approach works directly with the generalized matrix function and the moore ",
    "penrose pseudo - inverse of a bidiagonal matrix .",
    "the third approach first approximates the action of a generalized matrix function on a vector and then derives the approximation for the bilinear form of interest .      when the function @xmath236 that defines @xmath237 is c.m .",
    ", then gauss - type quadrature rules can be used to derive upper and lower bounds for the quantities of interest .",
    "it is straightforward to see by using that a bilinear form involving a generalized matrix function can be written as @xmath238 where @xmath239 , and @xmath240 . using the equalities in one",
    "can see that these quantities can also be expressed as bilinear forms involving functions of the matrices @xmath241 and @xmath242 , respectively .",
    "more in detail , one obtains    @xmath243    @xmath244    where in both cases @xmath245 .    in the following we focus on the case described by .",
    "the discussion for the case described by follows the same lines .",
    "note that if @xmath246 are vectors such that @xmath247 , then we can use the _ polarization identity _",
    "@xcite : @xmath248   \\ ] ] to reduce the evaluation of the bilinear form of interest to the evaluation of two symmetric bilinear forms . for this reason",
    ", the theoretical description of the procedure to follow will be carried out only for the case @xmath249 .",
    "let @xmath249 be a unit vector ( i.e. , @xmath250 ) .",
    "we can rewrite the quantity as a riemann  stieltjes integral by substituting the spectral factorization of @xmath242 : @xmath251 where @xmath252 is a piecewise constant step function with jumps at the positive eigenvalues @xmath253 of @xmath242 defined as follows : @xmath254    we use partial golub ",
    "kahan bidiagonalization @xcite of the matrix @xmath0 to find upper and lower bounds for the bilinear form described in  .",
    "after @xmath255 steps , the golub ",
    "kahan bidiagonalization of the matrix @xmath0 with initial vector @xmath256 yields the decompositions @xmath257 where the matrices @xmath258\\in{{\\mathbb{r}}}^{n\\times\\ell}$ ] and @xmath259\\in{{\\mathbb{r}}}^{m\\times\\ell}$ ] have orthonormal columns , the matrix @xmath260 is upper bidiagonal , and the first column of @xmath261 is @xmath256 .",
    "all the @xmath262 and @xmath263 can be assumed to be nonzero  @xcite . with this assumption ,",
    "the csvd of the bidiagonal matrix @xmath264 coincides with its svd : @xmath265 where @xmath266\\in{{\\mathbb{r}}}^{\\ell\\times\\ell}$ ] and @xmath267\\in{{\\mathbb{r}}}^{\\ell\\times\\ell}$ ] are orthogonal , and @xmath268 .",
    "combining the equations in   leads to @xmath269 where @xmath270 denotes the lanczos vector computed at iteration @xmath271 .",
    "the matrix @xmath272 is thus symmetric and tridiagonal and coincides ( in exact arithmetic ) with the matrix obtained when the lanczos algorithm is applied to @xmath242 .    the quadratic form in",
    "can then be approximated by using an @xmath255-point gauss quadrature rule  @xcite : @xmath273    if the function @xmath274 is c.m .",
    ", then the gauss rule provides a lower bound for  , which can be shown to be strictly increasing with @xmath255 .",
    "if the recursion formulas for the golub  kahan bidiagonalization break down , that is , if @xmath275 at step @xmath255 , then the gauss quadrature rule gives the exact value ( see  @xcite ) .",
    "the following result can be easily derived from equation  .",
    "[ prop : nodes ] let @xmath276 and let @xmath277 be the bidiagonal matrix computed after @xmath255 steps of the golub ",
    "kahan bidiagonalization algorithm .",
    "let @xmath278 for @xmath279 be the singular triplets of @xmath280 .",
    "then the nodes of the @xmath255-point gauss quadrature rule @xmath281 are the singular values @xmath282 .",
    "furthermore , if @xmath283 , the weights of @xmath281 are @xmath284 for @xmath279 .    similarly ,",
    "if @xmath249 , then the weights of the rule are given by @xmath285 .    to provide an upper bound for   when @xmath33 is c.  m. , one can use a @xmath286-point gauss ",
    "radau quadrature rule with a fixed node @xmath287 ; this can be expressed in terms of the entries of the symmetric tridiagonal matrix @xmath288 as @xmath289 , where @xmath290 .",
    "the entries of this matrix , except for the last diagonal entry , are those of @xmath291 . to compute the last diagonal entry so that @xmath292 has @xmath287 among its eigenvalues",
    ", we proceeds as follows  @xcite .",
    "first , we compute @xmath293 ; then we set @xmath294 , where @xmath295 is the solution of the tridiagonal linear system @xmath296 .",
    "the arithmetic mean between the @xmath255-point gauss rule @xmath281 and the @xmath286-point gauss ",
    "radau rule @xmath297 is then used as an approximation of the quadratic form @xmath298 .      in this section",
    "we provide a second approach to the approximation of bilinear forms expressed in terms of generalized matrix functions .",
    "the following result shows how to compute the @xmath255-point gauss quadrature rule in terms of the generalized matrix function of the bidiagonal matrix @xmath264 .",
    "two expressions are derived , depending on the starting ( unit ) vector given as input to the golub ",
    "kahan algorithm .",
    "recall that , unless @xmath299 or @xmath300 , one has to use the polarization identity to estimate the bilinear forms of interest .",
    "let be @xmath276 and let @xmath277 be the bidiagonal matrix computed at step @xmath255 of the golub ",
    "kahan bidiagonalization algorithm .",
    "then , the @xmath255-point gauss quadrature rule @xmath281 is given by @xmath301 or @xmath302    let @xmath303 be a singular value decomposition of the matrix @xmath264 obtained after @xmath255 steps of the golub ",
    "kahan bidiagonalization algorithm with starting vector @xmath249 .",
    "then , from proposition  [ prop : nodes ] , it follows that @xmath304 the proof of the case when @xmath305 goes along the same lines and it is thus omitted .",
    "the @xmath306-point gauss - radau quadrature rule @xmath307 with a fixed node @xmath308 can be expressed in terms of the entries of the bidiagonal matrix @xmath309 as @xmath310 if @xmath311 or as @xmath312 when @xmath313 .",
    "the entries of @xmath314 , except for the last diagonal entry , are those of @xmath315 . to compute the last diagonal entry",
    ", one has to ensure that @xmath316 is an eigenvalue of @xmath317 .",
    "it can be easily shown that @xmath318 where @xmath319 is the solution of the tridiagonal linear system @xmath320 .",
    "assume that we have used @xmath321 steps of the golub ",
    "kahan bidiagonalization algorithm with starting vector @xmath256 ( normalized so as to have unit norm ) to derive the matrices @xmath322 , @xmath323 , and @xmath324 such that @xmath325 .",
    "the csvd of the bidiagonal matrix is @xmath326 , where @xmath86 is the same diagonal matrix appearing in the csvd of @xmath0 .",
    "since @xmath322 and @xmath324 have full column rank , we know that @xmath327 , and thus we can write @xmath328 where @xmath329 and @xmath330 .",
    "assume now that @xmath331 .",
    "we can then truncate the bidiagonalization process and approximate @xmath332 as @xmath333 and then obtain the approximation to the bilinear form of interest as @xmath334 the quality of the approximation will depend in general on the distribution of the singular values of @xmath0 and on the particular choice of @xmath33 . generally speaking ,",
    "if @xmath82 is much larger on the first few singular values of @xmath0 than for the remaining ones , then a small number of steps result in approximations with small relative errors .",
    "in this section we describe two ways to compute approximations of quantities of the form  , when @xmath207 .",
    "it is known that for this kind of problem , block algorithms are generally more efficient than the separate computation of each individual entry ( or column ) of @xmath335 .",
    "when dealing with blocks @xmath44 and @xmath202 with a number of columns @xmath336 , the complete monotonicity of the function @xmath33 does not ensure that block gauss - type quadrature rules provide bounds on the quantities of the form @xmath335 . in this case , indeed , no information about the sign of the quadrature error can be obtained from the remainder formula for the gauss quadrature rules  @xcite .",
    "therefore , we focus on the computation of approximations for the quantities of interest , rather than on bounds .",
    "we propose two different approaches to compute the quantities  .",
    "the first one exploits the connection between generalized matrix functions and standard matrix functions described in theorem  [ thm : genfun ] , while the second one first approximates the action of a generalized matrix function on @xmath151 vectors and then derives the approximation of the quantities of interest .      as a first approach , we propose the use of a pair of block gauss and anti - gauss quadrature rules  @xcite based on the nonsymmetric block lanczos algorithm  @xcite .",
    "as already pointed out , if we let @xmath290 , it holds that @xmath337 where @xmath338 and @xmath339 . in this case",
    ", there is no equivalent to the polarization identity and thus we work directly with the blocks @xmath340 and @xmath202 , @xmath341 ( the case when @xmath44 and @xmath342 are the initial blocks is similar ) .",
    "let @xmath343 and @xmath344 have all zero entries .",
    "assume moreover that @xmath345 and @xmath346 satisfy @xmath347 .",
    "then the nonsymmetric block lanczos algorithm applied to the matrix @xmath348 is described by the following recursions : @xmath349 @xmath350 . in  , @xmath351 and @xmath352 are the qr factorizations of @xmath353 and @xmath354 , respectively , and @xmath355 is a singular value decomposition of the matrix @xmath356 .",
    "the recursion formulas   ensures that @xmath357 .",
    "more succinctly , after @xmath255 steps , the nonsymmetric block lanczos algorithm applied to the matrix @xmath348 with initial blocks @xmath358 and @xmath359 yields the decompositions @xmath360                  & = \\left[\\widetilde{z}_1,\\dots,\\widetilde{z}_\\ell\\right]j_\\ell+\\widetilde{z}_{\\ell+1}\\gamma_\\ell                      \\bm{e}_\\ell^t,\\\\ x\\left[w_1,\\dots , w_\\ell\\right ]   & = \\left[w_1,\\dots , w_\\ell\\right]j_\\ell^t+w_{\\ell+1}\\delta_\\ell \\bm{e}_\\ell^t , \\end{aligned}\\ ] ] where @xmath361 is the matrix @xmath362 and @xmath363 , for @xmath279 are @xmath364 block matrices which contain @xmath128 zero blocks everywhere , except for the @xmath31th block , which coincides with the identity matrix @xmath127",
    ". we remark that if @xmath365 , the use of the symmetric block lanczos algorithm is preferable . in this case , the matrix @xmath361   is symmetric and the decompositions   can be written as @xmath366=\\left[w_1,\\dots , w_\\ell\\right]j_\\ell+w_{\\ell+1}\\gamma_\\ell \\bm{e}_\\ell^t.\\ ] ] the @xmath255-block nonsymmetric gauss quadrature rule @xmath281 can then be expressed as @xmath367    the @xmath306-block anti - gauss quadrature rule @xmath368 is defined as the @xmath306-block quadrature rule such that @xmath369 where @xmath370 , with @xmath371 $ ] , @xmath372\\in\\mathbb{r}^{k\\times n}$ ] , and @xmath373 is the set of polynomials of degree at most @xmath374 ( see  @xcite ) . as shown in @xcite , the @xmath306-block nonsymmetric anti - gauss rule can be computed in terms of the matrix @xmath375 as @xmath376 where @xmath377    a pair of block gauss and anti - gauss quadrature rules is not guaranteed to provide upper and lower bounds , not even in the case @xmath378 .",
    "however , suppose that the function @xmath379 can be written as @xmath380 where @xmath381 are the orthonormal polynomials implicitly defined by the scalar lanczos algorithm . in  @xcite , the authors show that if the coefficients @xmath382 decay rapidly to zero , then @xmath383 that is , a pair of scalar gauss and anti - gauss rules provides _ estimates _ of upper and lower bounds on the bilinear form of interest .",
    "this result has been extended to the block case in  @xcite . in this framework , if we express @xmath230 in terms of orthonormal polynomials , the coefficients in the expansion are @xmath384 matrices . to obtain good entrywise approximations for the quantities of interest",
    "it is necessary that the norm of the coefficients decays rapidly as @xmath255 increases .",
    "this condition is satisfied if @xmath230 is analytic in a simply connected domain @xmath385 enclosing the spectrum of @xmath242 , as long as the boundary @xmath386 is not close to the spectrum @xcite .",
    "if the function @xmath230 satisfies the above conditions , the arithmetic mean @xmath387 between gauss and anti - gauss quadrature rules can be used as an approximation of the matrix - valued expression @xmath388 .",
    "the second approach extends to the block case the approach described in subsection  [ ssec : approach3 ] .",
    "assume that the initial block @xmath389 satisfies @xmath390 and that the matrices @xmath391 and @xmath392 are zero matrices .",
    "the following recursions determine the first @xmath255 steps of the block golub ",
    "kahan algorithm with starting block @xmath393 : @xmath394 where @xmath395 and @xmath396 are qr factorizations of @xmath353 and @xmath354 , respectively .",
    "after @xmath255 steps , the recursions   yield the decompositions @xmath397&=\\left[p_1,\\dots , p_\\ell\\right]b_\\ell , \\\\",
    "a^t\\left[p_1,\\dots , p_\\ell\\right]&=\\left[q_1,\\dots , q_\\ell\\right]b_\\ell^t+q_{\\ell+1}\\gamma_\\ell   \\bm{e}_\\ell^t , \\end{aligned}\\ ] ] where now @xmath398 following the same reasoning as in subsection  [ ssec : approach3 ] , when @xmath399 , we can approximate the quantities of interest as @xmath400 f^{\\diamond}(b_\\ell )   \\bm{e}_1 = f_\\ell.\\ ] ]",
    "in this section we present some numerical results concerning the application of the previously introduced techniques to the computation of centrality and communicability indices in directed networks .",
    "the first set of experiments concerns the computation of the _ total hub communicability _ of nodes , which , for a node @xmath31 , is defined as the following bilinear form : @xmath401_i = { \\bf e}_i^t \\sinh^\\diamond(a ) { \\bf 1}\\,,\\ ] ] where @xmath0 is the adjacency matrix of the digraph .",
    "as shown in @xcite , this quantity can be used to rank how important node @xmath31 is when regarded as a  hub \" , i.e. , as a broadcaster of information ( analogous quantities rank the nodes in order of their importance as  authorities \" , i.e. , receivers of information ) .",
    "the second set of experiments concerns the computation of the resolvent - based communicability  @xcite between node @xmath31 , playing the role of broadcaster of information , and node @xmath38 , acting as a receiver .",
    "the quantities of interest here have the form @xmath402_{ij}$ ] , where @xmath403 and @xmath404 . in all the tests we apply the approaches previously described and we use as stopping criterion @xmath405 where @xmath406 is a fixed tolerance and @xmath407 represents the approximation to the bilinear form of interest computed at step @xmath255 by the method under study .",
    "our dataset contains the adjacency matrices associated with three real world unweighted and directed networks : , , and  @xcite .",
    "the adjacency matrix associated with is @xmath408 and has 7281 nonzeros .",
    "the graph contains information concerning the cross - references in roget s thesaurus .",
    "the adjacency matrix associated with is an @xmath409 matrix with @xmath410 nonzeros . for this network",
    ", there is a connection from node @xmath31 to node @xmath38 if user @xmath31 indicated user @xmath38 as a friend or a foe .",
    "the last network used in the tests , , represents the italian wikipedia .",
    "its adjacency matrix is @xmath411 and has @xmath412 nonzeros , and there is a link from node @xmath31 to node @xmath38 in the graph if page @xmath31 refers to page @xmath38 .      in this section",
    "we want to investigate how the three approaches defined for the case of @xmath206 perform when we want to approximate ( [ eq : tc ] ) , the total communicability of nodes in the network . for each network in the dataset , we computed the centralities of ten nodes chosen uniformly at random among all the nodes in the graph .",
    "the results for the tests are presented in tables  [ tab : roget]-[tab : itwiki2 ] . the tolerance used in the stopping criterion",
    "is set to @xmath413 .",
    "the tables display the number of iterations required to satisfy the above criterion and the relative error of the computed solution with respect to the  exact \" value of the bilinear form . the latter has been computed using the full svd for the smallest network , and using a partial svd with a sufficiently large number of terms @xmath414 for the two larger ones . the relative error is denoted by @xmath415    concerning the first approach , since @xmath416 is not completely monotonic , we have used the gauss quadrature rule as an approximation for the quantities of interest , rather than as a lower bound .",
    ".network : , @xmath417 ( @xmath418 ) .",
    "[ cols=\"^,^,^,^,^,^,^ \" , ]     tables  [ tab : block7]-[tab : block9 ] show the results concerning the approximation of the communicabilities among @xmath151 nodes using the generalized matrix function induced by @xmath419 .",
    "as above , we consider three different values for the parameter @xmath420 . as in the scalar case ,",
    "the method requires fewer iterations to reach a higher accuracy as the value of @xmath420 moves away from @xmath421 .",
    "the two approaches behave again very differently . as before , the results obtained with the second approach are very promising also in view of the fact that we did not make any assumptions on the regularity of the function .",
    "in this paper we have proposed several algorithms for the computation of certain quantities associated with generalized matrix functions .",
    "these techniques are based on gaussian quadrature rules and different variants of the lanczos and golub  kahan algorithms . in particular ,",
    "we have investigated three distinct approaches for estimating scalar quantities like @xmath422 , and two block methods for computing matrix - valued expressions like @xmath204 .",
    "the performance of the various approaches has been tested in the context of computations arising in network theory . while not all methods can be expected to always perform well in practice ,",
    "we have identified two approaches ( one scalar - based , the other block - based ) that produce fast and accurate approximations for the type of problems considered in this paper .",
    "francesca arrigo and caterina fenu would like to thank the department of mathematics and computer science of emory university for the hospitality offered in 2015 , when part of this work was completed .",
    ", _ application of anti - gauss quadrature rules in linear algebra _ , applications and computation of orthogonal polynomials , w. gautschi , g. h. golub , and g. opfer , eds . , birkhuser , basel , 1999 , pp .",
    "4156 .                    , _ preconditioned iterative regularization for ill - posed problems _ , in l.",
    "reichel , a.  ruttan , and r.  s.  varga , eds . , _ numerical linear algebra .",
    "proceedings of the conference in numerical linear algebra and scientific computation , kent , ohio , usa , march 1314 , 1992 _ , de gruyter , berlin and new york , 1993 , pp .",
    "141163 .              , _ decay behaviour of functions of skew - symmetric matrices _ , in proceedings of hercma 2005 , 7th hellenic - european conference on computer mathematics and applications , september , 22 - 24 , 2005 , athens , e.  a.  lipitakis , ed .",
    ", electronic editions lea , athens ."
  ],
  "abstract_text": [
    "<S> we develop numerical algorithms for the efficient evaluation of quantities associated with generalized matrix functions [ j.  b.  hawkins and a.  ben - israel , _ linear and multilinear algebra _ , 1(2 ) , 1973 , pp .  </S>",
    "<S> 163171 ] . </S>",
    "<S> our algorithms are based on gaussian quadrature and golub  kahan bidiagonalization . </S>",
    "<S> block variants are also investigated . </S>",
    "<S> numerical experiments are performed to illustrate the effectiveness and efficiency of our techniques in computing generalized matrix functions arising in the analysis of networks .    </S>",
    "<S> generalized matrix functions , gauss quadrature , golub  kahan bidiagonalization , network communicability    65f60 , 15a16 , 05c50 </S>"
  ]
}