{
  "article_text": [
    "big data analytics is a major topic in contemporary data management and machine learning research and practice .",
    "many platforms , e.g. , optiml  @xcite , graphlab  @xcite , systemml  @xcite , simsql  @xcite , google brain  @xcite , glade  @xcite and libraries , e.g. , madlib  @xcite , bismarck  @xcite , mllib  @xcite , vowpal wabbit  @xcite , have been proposed to provide support for distributed / parallel statistical analytics .",
    "_ model calibration _ is a fundamental problem that has to be handled by any big data analytics system .",
    "identifying the optimal model parameters is an interactive , human - in - the - loop process that requires many hours  if not days and months  even for experienced data scientists . from discussions with skilled data scientists and our own experience",
    ", we identified several reasons that make model calibration a difficult problem .",
    "the first reason is that the entire process has to be executed from scratch for every dataset / model combination .",
    "there is little to nothing that can be reused from past experience when a new model has to be trained on an existing dataset or even when the same model is applied to a new dataset .",
    "the second reason is the massive size of the parameter space  both in terms of cardinality and dimensionality .",
    "moreover , the optimal parameter configuration is dependent on the position in the model space . and",
    "third , parameter configurations are evaluated iteratively ",
    "one at a time .",
    "this is problematic because the complete evaluation of a single configuration  even sub - optimal ones  can take prohibitively long .",
    "[ [ motivating - example . ] ] motivating example .",
    "+ + + + + + + + + + + + + + + + + + +    _ gradient descent optimization _",
    "@xcite is a fundamental method for model calibration due to its generality and simplicity .",
    "it can be applied virtually to any analytics model  @xcite  including support vector machines ( svm ) , logistic regression , low - rank matrix factorization , conditional random fields , and deep neural networks  for which the gradient or sub - gradient can be computed or estimated .",
    "all the statistical analytics platforms mentioned previously implement one version or another of gradient descent optimization . although there is essentially a single parameter , i.e. , the step size , that has to be set in a gradient descent method , its impact on model calibration is tremendous .",
    "finding a good - enough step size can be a time - consuming task .",
    "more so , in the context of the massive datasets and highly - dimensional models encountered in big data applications .",
    "the standard practice of applying gradient descent to model calibration , e.g. , madlib , vowpal wabbit , mllib , bismarck , illustrates the identified problems perfectly . for a new dataset / model combination ,",
    "an arbitrary step size is chosen .",
    "model training is executed for a fixed number of iterations . since the objective function is computed only for the result model  due to the additional pass over the data it incurs ",
    "it is impossible to identify bad step sizes in a smaller number of iterations .",
    "the process is repeated with different step values , chosen based on previous iterations , until a good - enough step size is found .",
    "certain systems , e.g. , google brain , support the evaluation of multiple step sizes concurrently .",
    "this is done by executing independent jobs on a massive cluster , without any sort of sharing .",
    "[ [ problem - statement . ] ] problem statement .",
    "+ + + + + + + + + + + + + + + + + +    we consider the abstract problem of _ distributed model calibration with iterative optimization methods _ , e.g. , gradient descent .",
    "we argue that the incapacity to evaluate multiple parameter configurations simultaneously and the lack of support to quickly identify sub - optimal configurations are the principal causes that make model calibration difficult .",
    "it is important to emphasize that these problems are not specific to a particular model , but rather they are inherent to the optimization method used in training .",
    "the target of our methods is to find optimal configurations for the tunable hyper - parameters of the optimization method , e.g. , step size , which , in turn , facilitate the discovery of optimal values for the model parameters .",
    "therefore , we investigate _ speculative processing _ and _ intra - iteration approximations _ for the optimization of distributed model calibration .",
    "speculative iteration processing allows concurrent evaluation of multiple parameter configurations in a single pass over the training data  without the proportional increase in running time .",
    "intra - iteration approximation allows for faster convergence detection  and corresponding reduction in iteration time . when put together , these techniques have the potential to reduce model calibration time significantly .    [",
    "[ contributions . ] ] contributions .",
    "+ + + + + + + + + + + + + +    in this paper , we develop two database - inspired techniques for efficient model calibration .",
    "_ speculative parameter testing",
    "_ applies advanced parallel multi - query processing methods to evaluate several configurations concurrently . _",
    "online aggregation _ is applied to identify sub - optimal configurations early in the processing by incrementally sampling the training dataset and estimating the objective function corresponding to each configuration .",
    "our major contribution is the conceptual integration of parallel multi - query processing and approximation for efficient and effective large - scale model calibration with gradient descent methods .",
    "this requires novel technical solutions as well as engineering artifacts in order to bring significant improvements to the state - of - the - art .",
    "specific contributions include :    we design speculative parameter testing algorithms that evaluate multiple configurations simultaneously .",
    "the number of configurations is determined adaptively and dynamically at runtime .",
    "the configurations are drawn from a parametric distribution that is continuously updated using a bayesian statistics  @xcite approach .",
    "we design concurrent online aggregation estimators and define halting conditions to accurately and timely stop the execution .",
    "we provide efficient parallel solutions for the evaluation of the speculative estimators and of their corresponding confidence bounds that guarantee fast convergence .",
    "we apply these techniques to distributed gradient descent optimization for svm and logistic regression models . as a prerequisite",
    ", we formalize gradient descent optimization as a database aggregation problem .",
    "we provide an extensive comparison between batch and incremental gradient descent that reveals that  contrary to the generally accepted opinion ",
    "batch gradient descent methods are better suited for distributed processing over massive datasets due to their linearity properties . with the proposed techniques integrated",
    ", batch gradient descent always outperforms the incremental method both in convergence speed and execution time .",
    "we implement the proposed solutions in glade pf - ola  @xcite  a state - of - the - art big data analytics system  and evaluate the performance over terascale - size synthetic and real datasets .",
    "the results confirm that as many as 32 configurations can be evaluated concurrently almost as fast as one , while sub - optimal configurations can be detected in a @xmath0 fraction of the time .",
    "these translate in more than 100x faster convergence over mllib on spark  @xcite and 2x over vowpal wabbit  @xcite on a terascale - size real dataset .    [",
    "[ outline . ] ] outline .",
    "+ + + + + + + +    the model calibration problem is presented in section  [ sec : problem ] .",
    "gradient descent solutions are introduced in section  [ sec : grad - descent ] , while distributed gradient descent is discussed in section  [ sec : par - grad - descent ] .",
    "speculative parameter testing is presented in section  [ sec : multiple - steps ] .",
    "intra - iteration approximation with online aggregation is detailed in section  [ sec : ola - grad - descent ] .",
    "experimental results that evaluate thoroughly the efficiency and efficacy of the proposed methods and compare the glade pf - ola implementation against the state - of - the - art are presented in section  [ sec : experiments ] .",
    "section  [ sec : rel - work ] discusses relevant related work , while section  [ sec : conclusions ] concludes the paper .",
    "consider the following model calibration problem with a linearly separable objective function : @xmath1 in which a @xmath2-dimensional vector @xmath3 , @xmath4 , known as the model , has to be found such that the objective function is minimized .",
    "the constants @xmath5 and @xmath6 , @xmath7 , correspond to the feature vector of the @xmath8 data example and its scalar label , respectively .",
    "function @xmath9 is known as the loss while @xmath10 is a regularization term to prevent overfitting .",
    "@xmath11 is a constant .",
    "for example , the objective function in svm classification with -1/+1 labels and @xmath12-norm regularization is given by @xmath13 .",
    "gradient descent represents , by far , the most popular method to solve the class of optimization problems given in eq .",
    "( [ eq : optim - form ] ) .",
    "gradient descent is an iterative optimization algorithm that starts from an arbitrary point @xmath14 and computes new points @xmath15 such that the loss decreases at every step , i.e. , @xmath16 .",
    "the new points @xmath15 are determined by moving along the opposite @xmath17 gradient direction .",
    "formally , the @xmath17 gradient is a vector consisting of entries given by the partial derivative with respect to each dimension , i.e. , @xmath18 $ ] .",
    "computing the gradient for the formulation in eq .",
    "( [ eq : optim - form ] ) reduces to the gradient computation for the loss @xmath9 and the regularizer @xmath10 , respectively .",
    "the length of the move at a given iteration is known as the step size , denoted by @xmath19 . with these",
    ", we can write the recursive equation characterizing any gradient descent method : @xmath20 in order to check for convergence , the objective function @xmath17 has to be evaluated at @xmath15 after each iteration . convergence to the global minimum is guaranteed only when @xmath17 is convex .",
    "this implies that both the loss @xmath9 and the regularizer @xmath10 are convex .",
    "the specific problem we consider in this paper is how to solve the optimization formulation given in eq .",
    "( [ eq : optim - form ] ) using generic gradient descent methods when the training dataset consisting of @xmath21 ( vector , label ) pairs @xmath22 is partitioned into @xmath23 subsets .",
    "each subset is assigned to a different processing node for execution .",
    "we focus on the case when @xmath21 is extremely large and each of the @xmath23 subsets are disk - resident .",
    "gradient and loss computation can take a prohibitive amount of time in this case .",
    "moreover , gradient descent methods are highly sensitive to a series of parameters that require careful tuning , i.e. , repeated execution with different configurations , for every dataset .",
    "thus , novel techniques are required in order to scale gradient descent to the largest big data models .",
    "in this section , we introduce the basic gradient descent optimization algorithms .",
    "we start with the standard batch gradient descent algorithm which is a direct implementation of the theory .",
    "then , we present incremental or stochastic gradient descent , a popular alternative tailored for datasets containing a large number of examples .",
    "in addition to these primitive methods , we also discuss two derived algorithms ",
    "coordinate descent and l - bfgs .",
    "we conclude the section with a comparison of the two standard gradient descent approaches .",
    "the pseudo - code for gradient descent is given in algorithm  [ alg : batch - gd ] .",
    "this is also known as batch gradient descent ( bgd ) .",
    "the algorithm takes as input the data examples and their labels , the loss function @xmath9 and the gradient of the objective @xmath24 , and initial values for the model and step size .",
    "the optimal model is returned .",
    "the main stages are gradient computation and model and step size update .",
    "they are executed until convergence is achieved .",
    "convergence can be specified as a fixed number of iterations or based on the loss , e.g. , the loss difference across consecutive iterations decreases below a given threshold . in the later case , the loss has to be computed after every iteration , which incurs an additional pass over the data .",
    "[ [ model - and - step - size - update . ] ] model and step size update .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    the standard approach to compute the updated model @xmath15 , once the direction of the gradient is determined , is to use line search methods  @xcite .",
    "@xmath15 is found by iteratively trying different step sizes along the opposite gradient direction until the decrease in loss is above a user - defined threshold , i.e. , the wolfe conditions  @xcite .",
    "line search achieves a tradeoff between optimality and runtime by choosing only an approximation to the optimal step size , but in shorter time .",
    "nonetheless , line search is still iterative in nature and requires objective and gradient loss evaluation .",
    "these involve multiple passes over the entire data .",
    "a widely used alternative is to fix the step size to some arbitrary value and then decrease it as more iterations are executed , i.e. , @xmath25 as @xmath26 .",
    "the initial step size @xmath27 and the decay are highly sensitive parameters , specific to each dataset , that require intensive tuning . by fixing the step size ,",
    "the burden is essentially moved from runtime evaluation to offline tuning .",
    "* input : * @xmath28 , @xmath9 , @xmath24 , @xmath14 , @xmath27 + * output : * @xmath29    initialize @xmath14 and @xmath27 with random values if not provided let @xmath30    * if * model_convergence(@xmath31 ) * then * * break *    compute gradient : @xmath32 update model : @xmath33 update step size @xmath19    let @xmath34    [ [ sql - representation . ] ] sql representation .",
    "+ + + + + + + + + + + + + + + + + + +    we formulate the batch gradient solution to the objective function in eq .",
    "( [ eq : optim - form ] ) as sql aggregate queries .",
    "this allows us to identify database - specific optimizations .",
    "there are two dataset - wide operations in algorithm  [ alg : batch - gd]loss evaluation and gradient computation .",
    "both of them can be expressed as sql queries over a relation @xmath35 in which each tuple represents a training example .",
    "the loss at a given point @xmath3 is evaluated by the query :    select sum(@xmath36 ) from t    in which the vector operations involving the example feature vector @xmath37 and the multi - dimensional point @xmath3 can either be expressed as array functions or be mapped explicitly into arithmetic operators .",
    "gradient computation at a point @xmath3 requires one aggregate for every dimension , as shown in the following sql query :    select sum(@xmath38 ) , @xmath39 , sum(@xmath40 ) from t    it is important to emphasize that both the point @xmath3 and the function @xmath9 and its gradient @xmath41 are constants at a given iteration in the two queries given above . to make this point clear",
    ", we show the actual queries corresponding to svm classification with -1/+1 labels :    select sum(@xmath42 ) from t where @xmath43 > 0 select sum(@xmath44 ) , @xmath39 , sum(@xmath45 ) from t where @xmath43 > 0    for other model types , only the formula of the loss function and its gradient change .",
    "the rest stays the same .",
    "most importantly , the query shape is unchanged .",
    "based on these examples , we formalize the bgd computations with the following abstract sql query : @xmath46 in which @xmath47 different aggregates are computed over the training tuples in relation @xmath48 .",
    "at least two instances of this query have to be executed per iteration ",
    "one for the gradient and one for the loss .",
    "the exact number is typically much larger and is determined by the number of times the loss is computed in line search .",
    "the problem with bgd is that one pass  or many more if line search is executed to find the optimal step size  over the entire data is required in order to move a single step .",
    "this is a big issue in our target scenario  massive number of examples @xmath21  since the time per iteration is linear in @xmath21 and many iterations are typically required in order to achieve convergence .",
    "incremental or stochastic gradient descent ( igd )  @xcite addresses this issue by taking @xmath21 steps per iteration .",
    "the direction of each step is given by the gradient corresponding to a single data example @xmath49 .",
    "essentially , the entire gradient is approximated with a single term in the summation , i.e. , @xmath50 .",
    "if we ignore the regularizer @xmath10 the step recurrence becomes : @xmath51 where @xmath52 returns the @xmath53 example in a random permutation of the data .",
    "the permutation is necessary to guarantee that progress towards convergence is made inside an iteration .",
    "moreover , a different permutation should be used at each iteration to avoid stalling at a non - minimum point and increase the convergence rate .",
    "given the large number of steps taken inside a single iteration , the step size @xmath19 has to be carefully tuned to minimize extreme oscillations . executing line search after every step is out of the question .",
    "algorithm  [ alg : sgd ] summarizes the igd differences compared to bgd .",
    "lines 5 and 6 in algorithm  [ alg : batch - gd ] are replaced with the ` for ` loop given in algorithm  [ alg : sgd ] , where @xmath54 and @xmath55 , respectively .",
    "approximate gradient : @xmath56 @xmath57    [ [ mini - batch - gradient - descent . ] ] mini - batch gradient descent .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    bgd takes a single step per iteration .",
    "igd takes one step for every data example .",
    "an intermediate solution is to estimate the gradient using more than a single term in the summation eq .",
    "( [ eq : optim - form ] ) and take one step for each group of terms . if there are @xmath58 , @xmath59 , such groups then @xmath58 steps are taken in a single iteration .",
    "the step size @xmath60 has to be configured accordingly .",
    "this alternative is known as mini - batch gradient descent . while it has been shown that there are situations when the mini - batch solution outperforms both the batch and the incremental methods , the addition of another parameter ",
    "the size of a batch or the number of batches  only increases the complexity of tuning .",
    "this can not be ignored when the number of data examples @xmath21 is large .",
    "[ [ sql - representation.-1 ] ] sql representation .",
    "+ + + + + + + + + + + + + + + + + + +    the main difference between bgd and igd with respect to sql representation is that @xmath3 is not constant for an entire iteration . in pure igd , @xmath3 is updated for every example in the input , while in mini - batch gradient descent , @xmath3 is updated after a fixed number of examples .",
    "this can be expressed in sql as the following ` update ` statement :    update w set @xmath61 , @xmath39 , @xmath62    where @xmath63 is a table with a single tuple containing the model .",
    "the pair @xmath64 is a tuple from table @xmath48 corresponding to an input example .",
    "the update is executed for all the tuples in @xmath48 , extracted by a cursor in random order . as a concrete example , for svm classification we have the following sql statement :    update w set @xmath65 , @xmath39 , @xmath66    the loss is computed for the last @xmath3 generated at the end of an iteration , using the same sql query as in bgd .",
    "apart from the first - order gradient descent methods presented in this section , coordinate descent  @xcite and l - bfgs  @xcite are two alternatives that build upon bgd and igd .",
    "_ coordinate descent ( cd ) _ optimizes a multi - dimensional function by minimizing it along one dimension at a time . instead of moving along the overall gradient direction",
    ", cd takes steps along each coordinate direction sequentially . the search along",
    "each coordinate is done by line search , which requires a step size . _",
    "l - bfgs _ is a quasi - newton method that searches the model space by approximating the inverse hessian matrix of the objective function .",
    "different from gradient descent , l - bfgs maintains a history of the last @xmath67 updates to model @xmath68 and gradient @xmath24 .",
    "the direction @xmath69 at iteration @xmath70 is given by @xmath71 , where @xmath72 is the approximation to the hessian . without going into details on",
    "how @xmath72 is computed from historical models and gradients , we point out that , after computing the search direction @xmath73 , a line search is performed .",
    "the techniques we propose in this paper can be applied to any of the gradient - based optimization methods ",
    "first- and second - order  used for large scale model calibration .",
    "we emphasize that the number of tunable hyper - parameters , i.e. , parameters of the optimization method , is small in all these methods . in the case of coordinate descent",
    ", only the step size has to be tuned . or , perhaps , the number of coordinates to update at the same time , if parallel coordinate descent is conducted . in l - bfgs , only the size of the history and the step size used in line search have to be calibrated .",
    "the relatively small space of tunable hyper - parameters allows for more opportunities to apply the techniques proposed in this paper .",
    "considering the alternative gradient descent methods discussed in this section , a comparison from the perspective of our specific problem , i.e. , eq .",
    "( [ eq : optim - form ] ) with the assumption that @xmath21 is extremely large , is required to clarify the merits of each approach .",
    "according to the thorough survey by bertsekas  @xcite , two cases have to be considered .",
    "far from the minimum , igd can have a convergence rate as much as @xmath21 times faster for identical loss functions @xmath9 and large @xmath21 .",
    "close to the minimum , igd requires diminishing step sizes in order to converge .",
    "this results in sub - linear convergence rate ",
    "slower than the linear convergence of the batch method . based on these observations ,",
    "a hybrid approach  @xcite , that first executes incremental gradient followed by batch gradient when no progress is made anymore , is likely to be the optimal solution in practice .",
    "it is also important to remark that igd requires data randomization at each iteration in order to achieve the @xmath21 factor in convergence speedup . with the same cyclical order used across all iterations",
    ", igd does not provide any theoretical convergence improvement beyond bgd  @xcite .",
    "in this section , we present parallel algorithms for distributed gradient descent optimization . with almost no exception , all the existing algorithms  @xcite",
    "make exclusive use of data partitioning parallelism , i.e. , the @xmath23 subsets are processed concurrently across the execution nodes and the partial results are merged together at the end of an iteration . to the best of our knowledge ,",
    "other types of parallelism  @xcite , e.g. , pipelining , super - scalar execution , and vectorization , are not considered in the literature .",
    "the straightforward strategy to parallelize bgd is to overlap gradient computation ( line 5 in algorithm  [ alg : batch - gd ] ) across the processing nodes storing the @xmath23 data subsets  @xcite .",
    "the partial gradients are subsequently aggregated at a coordinator node holding the current model @xmath74 , where a single global update step is performed in order to generate the new model @xmath15 based on eq .",
    "( [ eq : grad - step ] ) .",
    "the update step requires completion of partial gradient computation across all the nodes , i.e. , update model is a synchronization barrier . once the new model @xmath15 is computed  using line search or a fixed step size ",
    "it is disseminated to the processing nodes for a new iteration over the data .",
    "parallel bgd works because the objective function @xmath17 is linearly separable , i.e. , the gradient of a sum is the sum of the gradient applied to each term : @xmath75 = \\sum_{i=1}^{n } \\nabla f\\left(\\vec{w } , \\vec{x_{i } } ; y_{i}\\right)$ ] .",
    "it provides linear processing speedup and logarithmic communication in the number of nodes since aggregation can be executed on a tree structure . in terms of convergence",
    "though , there is no improvement with respect to the sequential algorithm  only faster iterations for gradient computation .",
    "an alternative parallel algorithm is given in  @xcite .",
    "each node solves to convergence an independent bgd on its corresponding data subset .",
    "the joint model is produced by averaging the partial models computed at each node . as a result ,",
    "communication between nodes is reduced to minimum . from a convergence standpoint ,",
    "this ensemble solution provides only variance reduction , but does not eliminate the bias when compared to the single node sub - sample solution  @xcite . the execution time is dominated by the convergence time of the slowest node .",
    "intuitively , parallelizing igd can not be any different from the batch version since the difference between the two algorithms is minimal",
    ". it should be even simpler since no global gradient computation is required .",
    "essentially , a model update is executed for every data example based solely on its value . at a closer look",
    "though , we observe that the updates in algorithm  [ alg : sgd ] are strictly sequential , i.e. , the input of the current update is the output of the previous one .",
    "moreover , the order in which updates are executed affects the final result , i.e. , model update is not commutative .",
    "these reasons seem to preclude any type of parallelism altogether . unless we drop the requirement that all the data examples have to be used to update the model in one iteration .",
    "similar to the approach in  @xcite , the standard method to parallelize igd  @xcite is to solve a separate model for every data partition and then to average the @xmath23 models in order to generate the final model at each iteration .",
    "the resulting model is then passed as a starting point to the subsequent iteration and the process is repeated until convergence , which is theoretically achieved in a logarithmic number of iterations  @xcite .",
    "an alternative that minimizes communication is to execute igd until convergence on each partition and average only the final models .",
    "[ [ model - averaging . ] ] model averaging .",
    "+ + + + + + + + + + + + + + + +    the following tradeoff has to be considered in the case of parallel igd . when the number of data partitions @xmath23 is large , so is the degree of parallelism .",
    "this results in higher speedup .",
    "it also results in more models computed over fewer examples to be averaged .",
    "this is a potential problem since there is a lower bound on the number of examples required for a given model to achieve the asymptotic regime  @xcite .",
    "moreover , the examples seen by every model have to represent a random sample from the entire population .",
    "the larger the number of partitions @xmath23 , the higher the communication required to generate random samples .",
    "the combined effect of non - asymptotic behavior and non - random examples is higher variance across the partial models , which finally results in slower convergence  @xcite .",
    "thus , choosing the optimal @xmath23 value has deeper implications in the case of parallel igd .",
    "[ [ shared - memory - parallelization . ] ] shared - memory parallelization .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    exactly because of this reason , the preferred solution to parallelize igd in a shared - memory setting is to discard model merging completely .",
    "a single model is shared across all the threads , while concurrent updates to the model are serialized through locking .",
    "unfortunately , this solution reduces speedup dramatically even when light locking mechanisms , e.g. , ` compare&swap ` , are used .",
    "the solution proposed in  @xcite to solve this problem is to discard locking altogether and allow for contentious updates to the shared model .",
    "as expected , this results in linear speedup .",
    "what about convergence ?",
    "as long as the model is highly - dimensional and the gradients used in updates are sparse , i.e. , only a small number of model dimensions are updated , the difference in convergence compared to the serial solution is minimal .",
    "[ [ hybrid - solution . ] ] hybrid solution .",
    "+ + + + + + + + + + + + + + + +    in a distributed setting with multi - core nodes , a hybrid approach is regarded as the optimal solution  @xcite . a single model is created for each node .",
    "it is shared across all the local threads or processes and updated using one of the shared - memory strategies , chosen based on the actual model properties .",
    "model merging across nodes is executed using the standard averaging technique along an aggregation tree .",
    "the hybrid solution aims to maximize the degree of parallelism available in the system without dramatically impacting the convergence .",
    "parallelizing gradient descent methods in a distributed environment poses different challenges . for the batch method ,",
    "gradient computation is trivially parallelizable due to the objective function being linearly separable .",
    "the main difficulty is posed by model update which is a strictly sequential operation .",
    "the main issue when parallelizing igd is the tradeoff between the degree of parallelism and the convergence rate .",
    "more parallelism does not automatically result in better convergence .",
    "a hybrid approach in which a shared model is created for each node , while averaging is used to merge models across nodes , is likely to be the optimal solution in general .",
    "in this section , we address two fundamental problems specific to model calibration  convergence detection and parameter tuning . as with any iterative method ,",
    "gradient descent convergence is achieved when there is no more decrease in the objective function , i.e. , the loss , across consecutive iterations .",
    "while it is obvious that convergence detection requires loss evaluation at every iteration , the standard practice , e.g. , vowpal wabbit  @xcite , mllib  @xcite , is to discard detection altogether and execute the algorithm for a fixed number of iterations .",
    "the reason is simple : loss computation requires a complete pass over the data , which doubles the execution time .",
    "this approach suffers from at least two problems .",
    "first , it is impossible to detect convergence before the specified number of iterations finishes . and second",
    ", it is impossible to identify bad parameter configurations , i.e. , configurations that do not lead to model convergence . recall that both bgd and igd depend on a series of parameters , the most important of which is the step size . finding the optimal step size",
    "typically requires many trials . discarding loss computation increases both the number of trials as well as the duration of each trial .",
    "[ [ high - level - approach . ] ] high - level approach .",
    "+ + + + + + + + + + + + + + + + + + + +    we propose a unified solution for convergence detection and parameter tuning based on speculative processing .",
    "the main idea is to _ overlap gradient and loss computation for multiple parameter configurations across every data traversal_. this allows for timely convergence detection and early bad configuration identification since many trials are executed simultaneously .",
    "overall , faster model training .",
    "the intuition behind our approach is that modern cpu architectures provide extensive parallelization opportunities , e.g. , multi - core , hyper - threading , vectorization , that are difficult to use at full potential in disk - based workloads specific to big data analytics over massive datasets . with the right mix of tasks and judicious scheduling ,",
    "the degree of parallelism supported in hardware can be fully utilized by executing multiple tasks concurrently . moreover",
    ", the overall execution time is similar to the time it takes to execute each task separately .",
    "our contribution is to _ design speculative gradient descent algorithms that test multiple step sizes simultaneously and overlap gradient and loss computation_. the number of step sizes used at each iteration is determined adaptively and dynamically at runtime .",
    "the step sizes are drawn from a parametric distribution that is continuously updated using a bayesian statistics  @xcite approach . only the model with the minimum loss survives each iteration , while the others are discarded ( figure  [ fig : architecture ] ) .",
    "as long as the execution time does not dramatically increase when multiple step sizes are tested , speculative execution leads to faster model training . to the best of our knowledge ,",
    "this is the first solution that uses speculative execution for gradient descent parameter tuning and loss computation .",
    "speculative execution is applied in vowpal wabbit  @xcite and mllib  @xcite , but only to deal with the problem of slow nodes . not to test multiple step sizes simultaneously .",
    "vowpal wabbit also supports progressive loss estimation which overlaps gradient and loss computation .",
    "progressive loss provides only local estimates .",
    "it never computes the exact loss at the end of an iteration .",
    "this can be done only with an additional pass over the data .",
    "our solution is exact  not approximate  and works in a distributed setting .          *",
    "input : * @xmath28 , @xmath9 , @xmath24 , @xmath14 , @xmath76 , @xmath77 , @xmath78 + * output : * @xmath74    initialize @xmath14 and @xmath76 randomly if not provided let @xmath79    draw @xmath77 step sizes @xmath80 from distribution @xmath78 let @xmath81 , @xmath82    * for each model * @xmath83 * do _ in parallel _ * compute gradient : @xmath84 compute loss : @xmath85 * end for *    let @xmath86 , @xmath82 * if * model_convergence(@xmath31 ) * then * * break *    update step distrib : @xmath87 update number of steps @xmath77 based on nested loops ( lines 6 - 11 ) execution time    let @xmath34    speculative bgd works as follows . define a set of possible step sizes @xmath80 , @xmath88 .",
    "generate an updated model @xmath89 , @xmath82 , for each of these step sizes and compute the corresponding objective function value concurrently .",
    "choose the model @xmath89 with the minimum loss as the new model .",
    "repeat the procedure for every iteration until convergence .",
    "the pseudo - code for the speculative bgd algorithm is given in algorithm  [ alg : speculative - bgd ] .",
    "the statements contained inside an _ in parallel _ loop are executed concurrently across iterations .",
    "moreover , in the case of _ speculative bgd _ , the two statements inside the loop ( lines 8 and 9 ) are executed in parallel even for the same iteration . in the following ,",
    "we discuss the main components of the algorithm .",
    "[ [ concurrent - step - size - evaluation . ] ] concurrent step size evaluation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    intuitively , speculative processing replaces multiple passes over the data with a single pass that requires @xmath90 times more computation . in sql terms",
    ", this corresponds to merging @xmath77 independent aggregate queries into a single query with @xmath90 aggregates .",
    "this provides a significant reduction in execution time even for traditional i / o - bound databases .",
    "modern databases that take advantage of state - of - the - art parallel hardware architectures can do much better and achieve similar execution times as for a single aggregate query .",
    "there are two levels of parallelism that can be exploited  multi - threaded execution on multi - core cpus and vectorized instructions . instead of updating all the @xmath77 models in a single thread , each model",
    "is updated in a separate thread , initialized with a different model .",
    "all the threads perform exactly the same processing over the same example data .",
    "only their initialization models are different .",
    "this is a standard instance of simd parallelism that is directly supported by multi - core cpus .",
    "given that every example has to be processed by all the threads , scheduling example access requires discussion .",
    "one approach is to make every example available to all the threads .",
    "while the threads can process the example concurrently , since read - only access is sufficient , synchronization is still required in order to determine when all the threads finish processing .",
    "another approach is to organize the threads into a pipeline .",
    "this allows for overlapped execution across examples , i.e. , each thread processes a different example , and eliminates barrier synchronization .",
    "vectorized processing , e.g. , ` sse ` and ` avx ` instructions , can be applied in two places .",
    "a set of examples can be grouped together into a chunk and processed as a unit inside a thread . as a side effect",
    ", chunking also reduces the overhead of moving examples through the pipeline .",
    "the second instance for vectorization is dot - product computation , which represents the primitive operation in gradient and loss evaluation .",
    "[ [ gradient - and - loss - computation - overlapping . ] ] gradient and loss computation overlapping .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the model that minimizes the loss across the @xmath77 possible step sizes is taken as input by the subsequent iteration .",
    "the gradient has to be computed for this model .",
    "remember though that we have already used this model to compute the loss in the previous iteration .",
    "unfortunately , we did not know if this model is selected or not . under the assumption that data access dominates the execution time , we argue for bundling loss and gradient computation together in order to save one pass over the data . overall",
    ", the number of data traversals is determined entirely by the number of gradient computations while loss evaluation piggybacks on the data access .",
    "we have to apply the overlapping strategy for all the @xmath77 models though , since we do not know the one minimizing the loss .",
    "although we might expect this to increase the execution time of an iteration , the aggressive use of simd pipeline parallelism across the @xmath77 step sizes minimizes the impact of additional computation and higher memory usage , as the experimental results in section  [ sec : experiments ] show .",
    "[ [ how - many - step - sizes - s ] ] how many step sizes @xmath77 ?",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the larger the number of step sizes , the higher the probability to find a model with smaller loss .",
    "however , a too large degree of speculation might result in an unacceptable increase in iteration time .",
    "thus , choosing the right number of step sizes is important for optimal speculative processing .",
    "what makes the problem even more difficult is the high variance in processing time as a function of model type and size .",
    "we propose an adaptive solution that selects the number of step sizes dynamically at runtime based on available resources .",
    "moreover , the number of step sizes varies from one iteration to another .",
    "we start with a single step size in the first iteration and measure the execution time .",
    "for each subsequent iteration , we increase the number of step sizes , e.g. , exponentially , as long as the increase in execution time remains below a specified threshold .",
    "we use this number of steps for all the subsequent iterations . in order to handle resource fluctuations in the system",
    ", we continuously monitor the execution time per iteration . in the case of a significant increase in processing time , we reduce the number of steps accordingly .",
    "[ [ how - to - choose - the - step - sizes ] ] how to choose the step sizes ?",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the simple solution to choosing the step sizes at a given iteration is to have @xmath77 constants that cover a large range of values , some of which are very small . the constants can be kept the same across iterations or they can be decreased at some rate , i.e. , the decay in igd .",
    "the problem with this approach is that we discard the knowledge we gain from previous iterations .",
    "ideally , we want to set the current @xmath77 values based on the previous best - performing step sizes .",
    "bayesian statistics  @xcite provides a principled framework to accomplish this .",
    "we start with a prior parametric distribution for the step size .",
    "this can be a single distribution or a mixture .",
    "the prior can be learned from the historical workload or it can be set to some default distribution , e.g. , normal .",
    "the @xmath77 step sizes used at each iteration are sampled randomly from the current distribution .",
    "the resulting losses are normalized and converted to probabilities .",
    "the @xmath77 pairs ( step size , loss ) are combined together with the prior in order to compute the updated step size distribution  known as the posterior .",
    "this consists in determining the posterior distribution parameters and it can be done with a straightforward expectation - maximization ( em ) algorithm that identifies the maximum likelihood estimator ( mle ) based on the available samples .",
    "we do not provide the details of the em algorithm since it is dependent on the shape of the prior .",
    "the posterior distribution becomes the prior in the subsequent iteration and the entire procedure is repeated .    *",
    "input : * @xmath28 , @xmath9 , @xmath24 , @xmath91 , @xmath78 + * output : * @xmath74    initialize @xmath91",
    "randomly if not provided let @xmath79    draw @xmath77 step sizes @xmath80 from distribution @xmath78 let @xmath92 , @xmath93    * for each model * @xmath94 * do _ in parallel _ *    approximate gradient : @xmath95 update : @xmath96    * end for *    * for each original model * @xmath97 * do _ in parallel _ *    compute loss : @xmath98    let @xmath99 , @xmath82 let @xmath67 be the index of the minimum loss @xmath100    * if * model_convergence(@xmath31 ) * then * * break *    update step distrib : @xmath101 update number of steps @xmath77 based on nested loops ( lines 6 - 13 ) execution time    let @xmath102 , @xmath82 let @xmath103      at high level , the speculative techniques proposed for bgd are directly applicable to igd : compute @xmath77 models instead of one and overlap model update with loss evaluation . at closer look though , there is a significant difference . while computing the loss for a given model , the model changes continuously since multiple steps are taken during the update phase . as a result , the loss obtained at the end of an iteration corresponds to the starting model , while the final model is the updated model generated from the same starting model .",
    "notice that they are different models though .",
    "this creates problems for at least two reasons .",
    "first , it is not clear that the resulting model corresponding to the original model having the minimum loss is the optimal model to select .",
    "and second , @xmath77 step sizes for @xmath77 initial models generate @xmath104 resulting models after one iteration . since the number grows exponentially with the number of iterations , a pruning mechanism that selects only @xmath77 models at every iteration",
    "is required .",
    "we address both these issues with the following strategy ( algorithm  [ alg : speculative - igd ] ) : select the @xmath77 resulting models corresponding to the initial model having minimum loss .",
    "this guarantees that the starting model is optimally chosen .",
    "since we do not know which of the @xmath77 step sizes is optimal , we keep the models corresponding to all of them .",
    "the models generated for the sub - optimal @xmath105 initial models are all discarded .",
    "this provides the necessary pruning mechanism to support efficient processing .",
    "there are two _ in parallel _ loops in _ speculative igd_one for continuously updating models and one for the original models .",
    "while these two loops can be processed concurrently , the loop for continuous updating ( lines 8 and 9 ) is sequential inside the same iteration .",
    "speculative processing allows for multiple models to be evaluated concurrently by taking advantage of the shared access to the same example data .",
    "when this is overlapped with loss computation , the parameter search space can be explored more effectively .",
    "we discuss the mechanisms required to support efficient speculative execution for gradient descent optimization parameter tuning .",
    "specifically , we propose adaptive algorithms for choosing the number of step sizes and their values at every iteration .",
    "while these algorithms apply to both bgd and igd , additional problems are introduced by the divergence between the initial model and the resulting models in the case of igd .",
    "we present an heuristic strategy that selects optimal models and prunes the search space to allow for efficient evaluation .",
    "the speculative processing methods proposed in section  [ sec : multiple - steps ] allow for a more effective exploration of the parameter space . however , they still require complete passes over the entire data at each iteration in order to detect the sub - optimal parameter configurations . a complete pass is often not required in the case of massive datasets due to redundancy .",
    "it is quite likely that a small random sample summarizes the most representative characteristics of the dataset and allows for the identification of the sub - optimal configurations much earlier .",
    "this results in tremendous resource savings , more focused exploration , and faster convergence .",
    "[ [ high - level - approach.-1 ] ] high - level approach .",
    "+ + + + + + + + + + + + + + + + + + + +    we present a _ novel solution for using online aggregation sampling in parallel gradient descent optimization to speed - up the execution of a speculative iteration_. we generate samples with progressively larger sizes dynamically at runtime and execute gradient descent optimization incrementally , until the approximation error drops below a user - defined threshold @xmath106 .",
    "relative to figure  [ fig : architecture ] , the entire process is executed multiple times during an iteration , on samples with increasing size .",
    "this is completely different from static sub - sampling methods that first extract a fixed - size random sample and then execute gradient descent on the sample , expecting that the optimal solution over the sample is also optimal for the entire dataset .",
    "moreover , online aggregation avoids the expensive re - sampling required in sub - sampling whenever the approximation error level is not satisfied .",
    "while parallel online aggregation has been studied before for sql aggregates  @xcite , we are the first to consider online aggregation for complex analytics such as gradient descent optimization .",
    "the most important challenge we have to address is how to _ design and compute multiple concurrent sampling estimators _ corresponding to speculative gradient and loss computation .",
    "these estimators arise in the components of multi - dimensional gradients and in the speculative objective function evaluation .",
    "our main contributions can be summarized as follows .",
    "we formalize gradient descent optimization as aggregate estimation .",
    "we provide efficient parallel solutions for the evaluation of the speculative estimators and of their corresponding confidence bounds that guarantee fast convergence .",
    "we design halting mechanisms that allow for the speculative query execution to be stopped as early as the user - defined accuracy threshold @xmath106 is achieved .",
    "exact evaluation of query  ( [ eq : abstract - query ] ) can be a lengthy process when the size of data @xmath107 is large or when the computation of a particular aggregate @xmath108 is complicated even when parallel solutions are used .",
    "the only alternative to speed - up computation further is to resort to approximation .",
    "instead of computing the aggregates exactly , they are only estimated .",
    "the estimators have to come with sound accuracy guarantees though in order to guarantee verifiable results . out of the many approximation techniques proposed in the literature",
    ", we argue that sampling is best suited for the aggregate estimation in query  ( [ eq : abstract - query ] ) .",
    "this is because other methods , e.g. , sketches  @xcite , have to be constructed separately for every aggregate .",
    "this is infeasible in gradient descent optimization since a different set of aggregates have to be computed at each iteration .",
    "only sampling maintains the identity of the data items and supports the computation of any aggregate as it would be computed over the entire data .",
    "sampling works as follows .",
    "a small dataset @xmath109 is randomly sampled from @xmath48 .",
    "query  ( [ eq : abstract - query ] ) is executed over the sample @xmath109 and the result is scaled - up to compensate for the difference in size between @xmath48 and @xmath109 .",
    "it is straightforward to show that @xmath110 is an unbiased estimator for the summation corresponding to function @xmath108 , where @xmath111 is the result of query  ( [ eq : abstract - query ] ) executed over @xmath109 .",
    "moreover , the accuracy , i.e. , confidence bounds , of the estimator can be derived by estimating the variance over the same random sample @xmath109 .",
    "all this requires is the addition of another summation , i.e. , @xmath112 , to query  ( [ eq : abstract - query ] ) for every aggregate function .",
    "while this is standard sampling - based single aggregate estimation treated extensively both in statistics as well as in databases  @xcite , what is specific to our problem is the concurrent computation of multiple aggregates .",
    "as far as we know , this is a novel problem that has not been considered extensively in the literature .",
    "we discuss aspects specific to the concurrent sampling - based estimation of multiple aggregates in the following .",
    "[ [ how - to - avoid - correlation - between - estimators ] ] how to avoid correlation between estimators ?",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the same sample @xmath109 is used to estimate all the aggregates in query  ( [ eq : abstract - query ] ) . in order to avoid correlations between estimators ,",
    "care has to be taken when extracting the sample .",
    "for example , if there is correlation between two dimensions @xmath113 and @xmath114 in the feature vector , e.g. , @xmath113 functionally determines @xmath114 , sampling based on @xmath113 determines the values of @xmath114 , thus it affects the accuracy of the estimators containing @xmath114 .",
    "this type of issues can be avoided if sampling takes into consideration the value of either all the tuple attributes or none of them .",
    "for example , a bernoulli sampling process in which a coin is tossed independently for every tuple does not depend on any of the attribute values . a random hash function that takes as parameter an entire tuple falls in the other category .",
    "both choices are guaranteed to avoid correlation between the aggregate estimators .",
    "[ [ how - to - choose - the - optimal - sample - size - t ] ] how to choose the optimal sample size @xmath115 ?",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to guarantee a given accuracy , the sample size has to be above a certain threshold .",
    "since the threshold is dependent on the actual estimator and its corresponding variance , it can not be computed directly .",
    "it can only be estimated from the historical workload .",
    "if the chosen sample size is too small and can not guarantee the required accuracy , a larger sample has to be extracted from the data .",
    "this is a complicated process that takes significant time , e.g. , @xmath116 for sampling without replacement , and requires random disk access .",
    "the situation is even more complicated for our particular problem since there are @xmath47 estimators in query  ( [ eq : abstract - query ] ) and the formula of these estimators is different from one iteration to another .",
    "thus , it is likely that a sample taken a priori is not able to guarantee the required accuracy , unless it is considerably large .",
    "* for each _ active _ model * @xmath83 * do _ in parallel _ * estimate gradient @xmath117 $ ] : @xmath118 estimate loss @xmath119 $ ] : @xmath120 * end for *    * if * test_est_convergence(@xmath121 ) * then * prune out models : @xmath122\\}_{i\\leq s } , .05)$ ] let @xmath123 be the number of remaining models , i.e. , _ active _    * if * ( @xmath124 ) * and * @xmath125\\}_{l\\leq d } , .05)$ ] * then * * break * let @xmath126 * end if *      the main idea in online aggregation ( ola )  @xcite is to sample at runtime during normal query processing .",
    "sampling and estimation are essentially overlapped with query execution . as more data are processed towards computing the final aggregates , the size of the sample increases and the accuracy of the estimators  as reflected by the width of the confidence bounds ",
    "improves accordingly . whenever the targeted accuracy is achieved , the query , i.e. , gradient iteration , can be stopped and a new iteration can be started . in the worst case",
    ", the aggregation is executed over the entire data and the exact results are obtained .",
    "algorithm  [ alg : approximate - bgd ] depicts the pseudo - code for bgd with online aggregation .",
    "we present only the nested loops portion of the algorithm .",
    "they replace the corresponding nested loops in _ speculative bgd _ ( lines 6 - 11 ) . `",
    "test_est_convergence ` can be triggered either based on the number of examples processed , or externally .",
    "ola is beneficial for at least two reasons .",
    "first , the sample size does not have to be determined before query execution .",
    "this eliminates the need for re - sampling altogether . and second",
    ", a different sample can be generated at each iteration by simply changing the order in which the examples are processed .",
    "this eliminates correlations between estimators across iterations and improves convergence .",
    "the main drawback of ola is that the same query is executed twice  once for computing the correct result and once for computing the estimator .",
    "we argue  and show experimentally in section  [ sec : experiments ]  that this is not a problem since both queries access the same data .",
    "[ [ how - to - sample - efficiently - at - runtime ] ] how to sample efficiently at runtime ?",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    according to the literature  @xcite , there are two methods to generate samples from a database at runtime .",
    "the first method uses an index that provides the random order in which to access the data .",
    "this method is highly inefficient due to the large number of random disk accesses . the second method stores data in random order on disk such that a sequential scan returns random samples . data",
    "randomization is executed as a pre - processing step during loading , thus it is a single time cost . in order to generate different samples at each iteration",
    ", we choose the starting block in sequential scanning randomly .",
    "this guarantees minimal  if at all  interference with the actual query processing .",
    "moreover , larger samples can be obtained by running the scan longer .",
    "a full scan inspects all data and computes the exact result .    *",
    "input : * @xmath127\\}_{i\\leq d}$ ] , @xmath106    let @xmath128 let @xmath129    * if * ( @xmath130 ) * then * * return * * true * * else * * return * * false *    [ [ when - to - stop - gradient - computation ] ] when to stop gradient computation ? + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in the case of gradient computation , there are @xmath2 aggregates in query  ( [ eq : abstract - query ] ) , one for every dimension in the feature vector . with online aggregation",
    "active , there is an estimator and corresponding confidence bounds for every aggregate .",
    "given a desired level of accuracy , we have to determine when to stop the computation and move to a new iteration .",
    "we measure the accuracy of an estimator by the _ relative error _ , defined as the ratio between the confidence bounds width and the estimate , i.e. , @xmath131 . for example ,",
    "if we aim for 95% accuracy , the relative error has to be below the threshold @xmath132 .",
    "what makes our problem difficult is that we have @xmath2 independent relative errors , one for each estimator .",
    "how do we determine when the desired accuracy level is reached in this situation ?",
    "the simple solution is to wait until all the errors drop below the threshold @xmath106 .",
    "this might require processing the entire dataset even when only a few estimators do not satisfy the accuracy requirement , thus defeating the purpose of online aggregation .",
    "an alternative that eliminates this problem is to require that only a percentage of the estimators , e.g. , 90% , achieve the desired accuracy .",
    "another alternative is to define a single convergence threshold across the @xmath2 estimators .",
    "for example , we can define @xmath133 and require that the sum of the relative errors across estimators is below @xmath134 ( algorithm  [ alg : stop - gradient ] ) . note that none of these alternatives outperforms the others in all the cases . while we can quantify the effect of each choice on gradient estimation , it is not clear what is the effect on convergence .",
    "* input : * @xmath127\\}_{i\\leq s}$ ] , @xmath106 + * output : * estimators with considerable overlap , i.e. , overlap @xmath135    let @xmath136 ; @xmath137    discard all estimators @xmath121 s.t .",
    "@xmath138 with @xmath139    discard all estimators @xmath121 s.t .",
    "@xmath138 with @xmath140    remaining estimators    [ [ when - to - stop - loss - computation ] ] when to stop loss computation ?",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    while the estimators in gradient computation are independent  in the sense that there is no interaction between their confidence bounds with respect to the stopping criterion  the loss estimators corresponding to different step sizes are dependent .",
    "our goal is to choose only the estimator generating the minimum loss . whenever we determine this estimator with high accuracy",
    ", we can stop the execution and start a new iteration .",
    "notice that finding the actual loss  or an accurate approximation of it  is not required if gradient descent is executed for a fixed number of iterations .        figure  [ fig : ola - loss - stop ] depicts a possible confidence bounds configuration for five loss estimators .",
    "although spread horizontally , this configuration is generated at the same time instant during processing .",
    "lower vertical points correspond to lower loss values .",
    "it is clear from the figure that estimator ` c ` has no chance to generate the minimum loss , thus it can be discarded .",
    "estimator ` a ` overlaps with all of ` b ` , ` d ` , and ` e ` , thus it can still generate the minimum loss . at closer look",
    ", we observe that the confidence bounds for ` e ` are quite tight and the overlap with ` a ` is minimal .",
    "thus , with high probability , ` e ` has smaller loss than ` a ` , which can be also discarded , without a significant impact on finding the minimum loss . while ` e ` is contained entirely inside ` d ` , it is at the upper - end of ` d ` , i.e. , the lower bound of ` e ` is close to the upper bound of ` d ` . with high probability ,",
    "` e ` does not have the minimum loss and it can also be discarded .",
    "based on the configuration in figure  [ fig : ola - loss - stop ] , it is impossible to determine , exactly or approximately , which one of ` b ` and ` d ` has minimum loss .",
    "this is because ` b ` contains ` d ` and ` d ` is positioned almost in the center of ` b ` . while we have a good idea on the loss corresponding to ` d ` , the ` b ` loss is not stable yet , as reflected by the wide confidence bounds .",
    "more data have to be seen before identifying the minimum loss with high accuracy .",
    "we design the following algorithm for stopping loss computation ( algorithm  [ alg : stop - loss ] ) .",
    "the idea is to prune as many estimators as possible early in the execution .",
    "it is important to emphasize that pruning impacts only the convergence rate  not the correctness of the proposed method .",
    "pruning conditions are exact and approximate  @xcite .",
    "all the estimators for which there exists an estimator with confidence bounds completely below their confidence bounds , can be pruned .",
    "the remaining estimators overlap . in this situation , we resort to approximate pruning .",
    "we consider three cases .",
    "first , if the overlap between the upper bound of one estimator and the lower bound of another is below a user - specified threshold , the upper estimator can be discarded with high accuracy ( ` a ` in the example ) .",
    "this is a straightforward extension of the exact pruning condition .",
    "second , if an estimator is contained inside another at the upper - end , the contained estimator can be discarded ( ` e ` in the example ) .",
    "the third case is symmetric , with the inner estimator contained at the lower - end .",
    "the encompassing estimator can be discarded in this case .",
    "the algorithm is executed every time a new series of estimators are generated .",
    "execution can be stopped when a single estimator survives the pruning process .",
    "if the process is executed until the estimators achieve the desired accuracy , we choose the estimator with the lowest expected value .    [",
    "[ when - to - stop - gradient - loss - computation ] ] when to stop gradient & loss computation ?",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    remember that gradient and loss computation are overlapped in the speculative solution we propose .",
    "when we move from a given point , we compute both the loss and the gradient at all the step sizes considered .",
    "gradient computation is speculative since the only gradient we keep is the one corresponding to the minimum loss .",
    "the others are discarded . in the online aggregation solution",
    ", we compute estimators and confidence bounds for each of these quantities .",
    "the goal is to stop the overall computation as early as possible .",
    "how do we achieve this ?",
    "we have to combine the stopping criteria for gradient and loss computation .",
    "the driving factor is loss computation .",
    "whenever a step size can be discarded based on the exact pruning condition , the corresponding gradient estimation can be also discarded . instead of applying the approximate pruning conditions directly , we have to consider the interaction with gradient estimation .",
    "while gradient estimation for the minimum loss does not converge , we can continue the estimation for all the step sizes that can not be discarded based on the exact pruning condition .",
    "this allows us to identify the minimum loss and its corresponding gradient with higher accuracy .      in order to extend online aggregation to a parallel environment where data are partitioned across multiple processing nodes ,",
    "a series of issues have to be addressed .",
    "first , how do we generate a random sample over partitioned data ?",
    "the solution we use distributes data randomly across nodes at loading .",
    "it was originally proposed in  @xcite , where the authors show that a random sample is obtained by putting together samples from each of the nodes .",
    "however , notice that we do not have to gather all the samples on the same node .",
    "the processing can be executed locally on each sample and only the partial results have to merged .",
    "this is because the gradient descent problem we consider is commutative and associative .",
    "since there is only one sample , the standard estimator applies directly .",
    "second , how do we aggregate the partial results required for estimation ? in a centralized approach , all the nodes send their local results to a designated node which merges everything together and computes the estimates . the designated node is a potential source of bottleneck when merging is time - consuming . in a distributed approach , merging is executed along an aggregation tree . while processing is faster , a delay at any of the nodes can result in delayed estimation  or even no estimation at all . and",
    "third , how and when is the online aggregation triggered ?",
    "again , two strategies are possible in a distributed environment .",
    "online aggregation is triggered synchronously by a driver application . or the nodes execute partial aggregation asynchronously whenever certain conditions are met , e.g. , a given percentage of the local data have been processed .",
    "notice that the actual processing across nodes is asynchronous in both situations .",
    "we argue for the synchronous approach because it provides better control over the execution : it produces the latest available estimators ; it detects the stopping conditions as early as possible ; and it can schedule future online aggregation requests based on the width of the confidence bounds",
    ".      recall that in igd the loss is computed only for the last @xmath3 generated at the end of an iteration .",
    "this makes it impossible to detect if convergence is achieved earlier , when only a sample of the data have been used to update the model @xmath3 .",
    "since faster convergence is expected in the case of massive datasets , the question we address is how to detect convergence as early as possible ?",
    "this allows us to stop the current iteration immediately and start a new iteration from the latest updated model .",
    "the pseudo - code for _ approximate igd _ is given in algorithm  [ alg : approximate - igd ] .",
    "whenever estimator convergence is triggered , a new snapshot is taken for all the active models .",
    "loss estimation is started for the new snapshot , executed over subsequent examples , and checked for convergence at later snapshots .",
    "a minimum number of converged loss estimators that exhibit reduced variance ( algorithm _ stop igd loss _ )",
    "is required for the process to stop .    * for each _ active _ model * @xmath94 * do _ in parallel _ *    approximate gradient : @xmath95 update : @xmath96    estimate loss @xmath141 $ ] : @xmath142 , where @xmath47 ranges over the snapshots    * end for *    * for each original _ active _ model * @xmath97 * do _ in parallel _ *    estimate loss @xmath119 $ ] : @xmath143    * if * test_est_convergence(@xmath121 ) * then *    prune out models @xmath97 , their children @xmath94 and partial snapshots @xmath144 : @xmath122\\}_{i\\leq s } , .05)$ ]    let @xmath123 be the number of remaining models , i.e. , _ active _    * if * ( @xmath124 ) * and * @xmath145 ( @xmath146\\}_{p < j}$ ] , .05 , 2 , .01 ) * then * * break *    start new snapshot @xmath147 for active models let @xmath126    * end if *    [ [ when - to - test - for - convergence ] ] when to test for convergence",
    "? + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the two extremes to test convergence are at the end of an iteration and after every model update . the first case , i.e. , standard igd , prohibits early detection .",
    "the second case requires loss computation , i.e. , a complete pass over the data , for every model update .",
    "this is impractical , unless approximation is used .",
    "we propose an intermediate solution in which convergence testing is triggered whenever certain conditions are satisfied , e.g. , at fixed time intervals or when a given number of updates are processed .",
    "convergence testing is adaptive . at the beginning of an iteration ,",
    "the time interval ( number of updates ) is larger and it decreases as the iteration progresses , based on the loss function value .",
    "lack of significant change in the loss between checks triggers more frequent convergence testing .",
    "this adaptive solution allows for early convergence detection , without the loss computation overhead after every update .",
    "[ [ approximate - loss - computation . ] ] approximate loss computation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    exact loss computation at every step convergence is tested is impractical since a complete pass over the data is required .",
    "two approximate solutions are used in practice .",
    "notice , however , that they are used only to provide an idea on how the loss evolves , not as a convergence test for early iteration termination . in the first solution ,",
    "the loss is computed only at the points where convergence is tested .",
    "the overall loss is obtained by scaling - up this loss per example value to the entire dataset size . as expected , this crude approximation is highly inaccurate and unstable .",
    "it can not be guaranteed that progressively lower loss values are generated as more updates are processed . in the second solution ,",
    "the accuracy is improved by estimating the loss over multiple data examples , situated immediately after the example where convergence is tested .",
    "the number of examples used in loss estimation is fixed . essentially , model update and loss computation are interleaved , each operating on a distinct set of examples .",
    "there are two problems with this approach .",
    "first , there is no guarantee on the accuracy of the estimation . and second , it is not clear how to choose the number of examples used in model update and in loss computation , respectively",
    ". intuitively , the larger the number of examples used for loss estimation , the better the accuracy .",
    "unfortunately , this reduces the number of examples used for model update , which results in slower convergence .",
    "we propose a solution based on online aggregation to compute the loss estimate ( algorithm  [ alg : approximate - igd ] ) . given an accuracy threshold , loss estimation uses as many data examples as required in order to achieve the desired accuracy .",
    "the details are identical to loss estimation for bgd and are not shown here for conciseness .",
    "this adaptive process guarantees that an accurate estimator with provable confidence bounds is produced .",
    "thus , convergence testing becomes theoretically sound .",
    "[ [ overlapped - model - update - and - loss - estimation . ] ] overlapped model update and loss estimation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    while online aggregation solves the accuracy issue , the number of examples derailed from model update can be too large , thus impacting convergence . instead of the interleaved ( model update - convergence testing ) approach",
    ", we propose a solution in which the two stages are overlapped .",
    "whenever convergence testing is triggered , loss estimation is started from the most recent model .",
    "the examples inspected up to the subsequent convergence testing are used to update the model and to compute the loss estimator using online aggregation .",
    "two entities are produced at every convergence testing point  an updated model and one or more estimators for previous models .",
    "the updated model triggers the creation of a new loss estimator . if an estimator achieves the specified accuracy level , it is considered stable and is eliminated from subsequent computations .",
    "stable estimators are used in model convergence testing . in case of convergence ,",
    "the iteration is stopped and the most recent model is returned . with",
    "careful parallelization over the current multi - core cpus , the overlapped execution strategy does not affect model update time almost at all .",
    "this is because the processing required in estimator computation is very light and the memory footprint of an estimator is in the order of a few bytes .    *",
    "input : * @xmath127\\}_{i\\leq p}$ ] , @xmath106 , @xmath67 , @xmath148    let the set of converged estimators be @xmath149    * if * ( @xmath150 ) * and * ( @xmath151 ) * then * * return * * true * * else * * return * * false *    [ [ parallel - intra - iteration - synchronization . ] ] parallel intra - iteration synchronization .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    convergence testing in a parallel environment with data partitioned across nodes is more complicated .",
    "while all the nodes start an iteration with the same original model , as soon as the first example is considered , the models diverge . as a result ,",
    "convergence testing requires partial model merging as a pre - requisite .",
    "although this process is identical to final model merging , several questions require consideration . what do we use the merged model for ?",
    "the obvious answer is loss estimation .",
    "parallel online aggregation applies directly in this situation .",
    "the merged model can be also used as a synchronization point to re - align the local models during an iteration .",
    "this is beneficial for convergence , especially when data exhibit variation across nodes .",
    "how do we implement model synchronization ?",
    "the straightforward approach is to stop model update at every node until the partial merged model is computed and passed back .",
    "the nodes are blocked over the entire duration of the distributed merging process .",
    "this is problematic because any delay is reflected in the overall execution time .",
    "the alternative is to allow the local updates to continue while the merged model is aggregated . when returned , the synchronized model is merged again with each local model to generate a new model that reflects both the global data as well as the local updates executed during merging .",
    "the merging weights are assigned proportional to the number of examples .",
    "this gives more importance to the synchronized model .",
    "we introduce dynamic and adaptive intra - iteration approximation methods to estimate the gradient and the loss function .",
    "the goal is to determine when convergence is achieved as early as possible such that we can terminate the current iteration and start a new one .",
    "we expect early convergence in the case of massive training data due to redundancy .",
    "we model bgd as a series of sql aggregate queries and apply sampling - based online aggregation to estimate gradient and loss .",
    "this allows us to compute sound estimates with theoretical confidence bounds that provide high accuracy in the termination decisions .",
    "the novelty of our approach consists in considering sql queries with multiple aggregates ",
    "dependent and independent  and designing adaptive termination mechanisms that are both accurate and identify convergence early .",
    "igd convergence detection is more difficult since the model is updated continuously over the duration of an iteration .",
    "we design a dynamic mechanism to overlap model update with loss estimation over the same example data .",
    "convergence is accurately and timely detected since the estimation comes with theoretical guarantees .",
    "we show how our solutions are applicable to parallel environments with data partitioned across multiple processing nodes .",
    "the only requirement is a mechanism for partial aggregation .",
    "the objective of the experimental evaluation is to investigate the efficiency and effectiveness of the speculative parameter testing and intra - iteration approximation techniques across several synthetic and real datasets .",
    "we consider model calibration with gradient descent optimization for two standard analytics tasks  svm and logistic regression ( lr ) .",
    "moreover , we compare the efficiency of our implementation against two other distributed big data analytics systems  mllib  @xcite and vowpal wabbit  @xcite .",
    "the comparison is meant to identify the overhead introduced by the proposed techniques .",
    "specifically , the experiments we design are targeted to answer the following questions :    how does speculative parameter testing improve the convergence rate of model calibration and what overhead  if there is any  does it incur ?    what is the effect of intra - iteration approximation on convergence rate and execution time ?",
    "how do speculative parameter testing and intra - iteration approximation interact ?",
    "how do the proposed techniques stack - up against state - of - the - art big data analytics systems ?",
    "how do bgd and igd compare when they integrate the proposed techniques ?      [ [ implementation . ] ] implementation .",
    "+ + + + + + + + + + + + + + +    we implement the speculative and online aggregation versions of distributed gradient descent optimization as glade pf - ola applications .",
    "glade  @xcite is a state - of - the - art parallel data processing system that executes tasks specified using the abstract user - defined aggregate ( uda ) interface .",
    "it was previously shown in  @xcite that uda is the perfect database abstraction to represent complex analytics tasks  including gradient descent optimization .",
    "our code is thus general enough to be executed by any database supporting udas . to be precise",
    ", we use the code from bismarck  @xcite with slight modifications specific to glade .",
    "glade implements two levels of parallelism  multi - node and multi - thread  and takes care automatically of all the aspects related to data partitioning , task scheduling , and resource allocation .",
    "the user has to provide only the uda code containing the model to be trained and the example data .",
    "speculative execution is supported in glade through multi - query processing .",
    "multiple instances of the same uda  with different parameters  are executed against the same example data . without going into details ,",
    "we mention only that data access is shared across udas over the entire memory hierarchy  from disk , to memory , cache , and cpu registers .",
    "online aggregation requires an extension of the uda interface with estimation functions and a pre - aggregation mechanism that allows for partial aggregate computation to be triggered during query processing .",
    "these are supported by the pf - ola framework  @xcite for parallel online aggregation implemented on top of glade .",
    "convergence and termination conditions are checked by the driver application .",
    "the code contains special function calls to harness detailed profiling data , used to generate the experimental results presented in the paper .",
    "[ [ system . ] ] system .",
    "+ + + + + + +    we execute the experiments on a 9-node cluster running ubuntu 12.04.4 smp @xmath152-bit with linux kernel 3.2.0 - 63 .",
    "one node is configured as coordinator while the other eight are workers .",
    "notice that only the workers are executing data processing tasks .",
    "each worker has 2 amd opteron 6128 series 8-core processors  16 cores ",
    "28 gb of memory , and four 1 tb 7200 rpm sas hard - drives configured raid-0 in software .",
    "each processor has 12 mb l3 cache while each core has 128 kb l1 and 512 kb l2 local caches .",
    "the storage system supports 240 , 420 and 1600 mb / second minimum , average , and maximum read rates , respectively ",
    "based on the ubuntu disk utility .",
    "there are two file systems running on each node .",
    "the storage system containing experimental data is mounted as a local file system , while the code and executables are shared across nodes through an nfs file system instance .",
    "[ [ methodology . ] ] methodology .",
    "+ + + + + + + + + + + +    we perform all experiments at least 3 times and report the average value as the result .",
    "we always enforce data to be read from disk in the first iteration by cleaning the file system buffers before execution .",
    "subsequent iterations can access cached data .",
    "when the execution time per iteration is reported , the value corresponds to iterations two and above .",
    "we execute each algorithm for a fixed number of iterations  typically 20 .",
    ".datasets used in the experiments .",
    "[ cols=\"<,>,>,>\",options=\"header \" , ]     [ [ tasks - and - datasets . ] ] tasks and datasets .",
    "+ + + + + + + + + + + + + + + + + + +    while gradient descent is a general optimization method that can be applied for training a large variety of models , in this paper , we present experiments for convex lr and svm .",
    "we use three datasets  two real and one synthetic , i.e. , ` classify50 m `  with very different characteristics , as depicted in table  [ tbl : datasets ] .",
    "it is important to emphasize that , as far as we know , ` splice ` is the largest dataset available on which gradient descent results have been published in the literature  in machine learning and databases . since ` forest ` has small size , we use it to evaluate the performance of speculative processing on a single machine .",
    "only multi - threading parallelism is used in this case .",
    "the other two datasets are evenly partitioned across the 8 worker nodes in the cluster using a random hash function .",
    "bgd and multiple versions of igd are implemented for each ( model type , dataset ) combination . `",
    "igd merge ` corresponds to model averaging , in which a separate model is created for every thread in the system . `",
    "igd lock ` uses a single model per node and synchronizes access across threads with a light locking mechanism , e.g. , ` compare&swap ` . `",
    "igd no lock ` implements the solution proposed in  @xcite that eliminates synchronization . while we run experiments for all the combinations , we include in the paper only the most representative results .",
    "[ [ convergence - rate . ] ] convergence rate .",
    "+ + + + + + + + + + + + + + + + +    to quantify the impact of speculative parameter testing on convergence rate , we execute bgd and igd with an increasing number of concurrent step sizes .",
    "we pick the step size values by mimicking a real parameter tuning procedure .",
    "we start with an arbitrary value and then add smaller and larger values .",
    "the old values are maintained when increasing the number of steps .",
    "this guarantees continuous improvement as the number of steps increases  not always the case for bayesian inference .",
    "notice though that bayesian inference can provide better values that result in faster convergence .",
    "figure  [ fig : forest : convergence ] depicts the effect of increasing the number of step sizes on convergence for the ` forest ` dataset .",
    "the general trend is to achieve faster convergence as the number of step sizes increases .",
    "while this is always true for bgd , the igd behavior is more nuanced .",
    "the bgd results are depicted in figure  [ fig : forest : bgd : svm ] .",
    "they include a comparison with ` line search `  @xcite , the standard backtracking step size search method that guarantees a certain loss decrease rate at every iteration . `",
    "line search ` adjusts the step size at every iteration , thus providing automatic step size tuning . for more than 4 step sizes ,",
    "speculative processing achieves faster convergence than ` line search ` .",
    "this is because ` line search ` limits itself to satisfying the imposed loss decrease rate .",
    "for a higher decrease rate , ` line search ` requires more passes over the example data , which results in higher time per iteration , thus slower convergence .",
    "figure  [ fig : forest : igd : lr ] depicts the convergence rate for ` igd merge ` .",
    "while a higher number of step sizes still generates a higher loss decrease per iteration , the overhead incurred is significantly higher in this case . by the time one iteration with 32 steps finishes ,",
    "8 steps has already finished execution and achieved convergence .",
    "a direct comparison between bgd and the igd versions for 16 step sizes is depicted in figure  [ fig : forest : convergence : svm ] .",
    "it is clear that bgd can handle a large number of step sizes better than igd since the number of speculative models is an order of complexity lower , i.e. , linear vs. quadratic . between the igd solutions , ` igd merge ` clearly outperforms the other two versions , even though the convergence rate per iteration is higher for ` igd lock ` and ` igd no lock ` , respectively .",
    "the hardware cache coherence mechanism coupled with the time to update a large number of models are the reason for the poor behavior of ` igd no lock ` .",
    "l|l||rrrrr|rrrrr|rr    & & & + & 1 & 2 & 4 & 16 & 32 & 1 & 2 & 4 & 16 & 32 & 1 & 2 +    & bgd & 2.57 & 2.58 & 2.57 & 2.71 & 2.83 & 95 & 95 & 96 & 97 & 99 & 625 & 639 + & igd merge & 2.58 & 2.58 & 2.74 & 8.17 & 33.2 & 96 & 96 & 99 & 363 & 1749 & 631 & 675 + & igd lock & 2.8 & 2.91 & 3.12 & 24.04 & 92.78 & 95 & 96 & 101 & 880 & 3190 & 840 & 1064 + & igd no lock & 2.5 & 2.52 & 3.06 & 23.12 & 86.73 & 95 & 96 & 101 & 726 & 2834 & 631 & 703 +    & bgd & 2.65 & 2.65 & 2.66 & 2.92 & 3.09 & 96 & 96 & 96 & 97 & 99 & 618 & 640 + & igd merge & 2.67 & 2.67 & 2.85 & 14.15 & 55.02 & 96 & 96 & 99 & 432 & 2213 & 634 & 1039 + & igd lock & 2.8 & 2.8 & 3.1 & 27.22 & 104.24 & 97 & 97 & 102 & 898 & 3301 & 2103 & 2199 + & igd no lock & 2.71 & 2.71 & 3.06 & 26.69 & 101.04 & 97 & 97 & 101 & 772 & 2975 & 636 & 1023 +    [ [ overhead . ] ] overhead . + + + + + + + + +    table  [ tbl : datasetstime ] contains the execution time per iteration for all the experimental configurations . in the case of the ` splice ` dataset , for more than two step sizes , the memory required by the model is beyond the physical capacity of the testing machine . the time per iteration changes minimally for speculative bgd when we increase the number of step sizes from 1 to 32 .",
    "the slightly higher execution time for lr is due to the more complicated gradient computation .",
    "igd incurs a considerable overhead when the number of step sizes increases since the computation is quadratic in the number of step sizes . between the igd versions , ` igd merge ` is the most efficient , especially for a large number of step sizes .",
    "this is due to complete model replication across threads which eliminates contention .",
    "overall , speculative parameter testing is able to boost the convergence rate for bgd up to 32 step sizes , without significantly increasing the execution time per iteration .",
    "we are able to achieve this because our implementation takes full advantage of the parallelism available in modern multi - core cpus , including deep pipelines and vectorized instructions .",
    "the main idea is to execute all the processing  across all the models  with minimal data movement , i.e. , whenever a data example is brought in the cpu registers , it is used to update all the gradients / models .      the combined effect of online aggregation and speculative parameter testing on convergence rate is depicted in figure  [ fig : ola : convergence ] .",
    "the execution of an iteration is halted as soon as the width of the confidence bounds corresponding to the estimator is below 5% of the estimate , i.e. , we are 95% confident on the estimator value . in the case of bgd ( figure  [ fig : ola : svm : classify ] ) , online aggregation provides a considerable boost in convergence rate across all the tested step sizes .",
    "the gradient can be estimated accurately from a small sample .",
    "the same is true for the loss .",
    "this allows for immediate detection of promising step sizes , while the sub - optimal ones can be discarded . in the best case , convergence to the same or a better loss",
    "is achieved as much as 20 times faster .",
    "the same trend can be observed for igd ( figure  [ fig : ola : lr : classify ] ) .",
    "the benefit of online aggregation is the most obvious for 8 step sizes , since convergence is achieved within one iteration . with standard igd",
    ", convergence can be detected only at the end of a complete pass over the training data .",
    "due to partial model merging , online aggregation detects convergence much earlier .",
    "figure  [ fig : ola : svm : splice ] depicts a direct comparison between bgd and igd with online aggregation enabled for the ` splice ` dataset .",
    "while igd achieves slightly faster convergence in the standard case , bgd outperforms igd by more than 50% when online aggregation is enabled .",
    "this is because gradient estimation is a considerably easier task than detecting convergence for partial models .",
    "[ [ adaptive - sampling . ] ] adaptive sampling .",
    "+ + + + + + + + + + + + + + + + + +    in order to confirm that online aggregation is the appropriate solution for intra - iteration approximation  and not sub - sampling with a pre - determined size  we measure the sample size required for the estimators to achieve convergence in each iteration .",
    "figure  [ fig : sample ] depicts the sampling ratio when an iteration is halted . while the sampling ratio is well below 5% in the first iterations , it increases drastically once we are getting close to the minimum .",
    "this happens because more data have to be seen for the estimators to converge .",
    "if a fixed - size sub - sample is taken and model calibration is executed on the sub - sample , two events can occur .",
    "if the sample size is too large , unnecessary processing is executed . for a smaller sample , convergence to the true minimum can not be achieved , no matter how long the process is executed .",
    "since online aggregation chooses the size of the sample adaptively , it avoids these problems altogether .",
    "we examine how the proposed techniques apply to a scenario where two parameters have to be calibrated .",
    "specifically , we consider mini - batch gradient descent ( section  [ ssec : grad - descent : stochastic ] ) as a variant of igd in which the gradient is approximated using a variable number of examples , rather than a single example .",
    "the number of examples , i.e. , the batch size , is the second parameter  together with the step size  to tune .",
    "since these two parameters are correlated  the larger the batch size , the larger the step size  we draw their values from a single 2-d normal distribution characterized by a covariance matrix that reflects this relationship .",
    "the normal distribution is initialized with centers at 0.1 ( step size ) and 1000 ( batch size ) , variances of 0.1 and 10000 , respectively , and positive covariance of 10 .",
    "the distribution is updated after every iteration using bayesian inference .",
    "figure  [ fig : stepsize - batchsize ] depicts the convergence behavior of mini - batch gradient descent as a function of the two parameters considered  step size and batch size . in figure",
    "[ fig : lr : classify : batchsizes ] , we use a single constant step size , while we vary the number of batch sizes .",
    "this allows us to isolate the batch size effect .",
    "we observe that there is little difference beyond two batch sizes .",
    "figure  [ fig : lr : classify : stepsize - batchsize ] illustrates the combined effect of the two parameters .",
    "as expected , a larger number of configurations results in faster convergence rate  limited only by the higher update time corresponding to ` igd lock ` . when comparing the two figures ,",
    "we conclude that the step size is the main factor impacting convergence .",
    "this confirms our approach focused on calibrating the step size .",
    "it is worth noting that step size and batch size are the most important tunable parameters for gradient descent methods applied to any type of objective functions . while our methods are directly applicable to a larger number of parameters , it is difficult to identify the effect each parameter has on convergence when bundled together with other parameters .",
    "we compare our implementation with two state - of - the - art large scale analytics systems ",
    "mllib  @xcite and vowpal wabbit  @xcite  to validate the efficiency of our solutions .",
    "we choose these systems based on the following criteria .",
    "they are disk - based distributed systems with native support for gradient descent optimization .",
    "in fact , they support only igd and not bgd .",
    "based on the complete study by cai et al .",
    "@xcite , no other open - source system satisfies these conditions .",
    "our goal is to prove that the improvement we get from speculative processing and intra - iteration approximation does not come from an inefficient implementation .",
    "in fact , as shown in table  [ tbl : comp ] , our implementation is faster than these systems . the measure we use is the time per iteration for executing a complete gradient update pass and a complete loss computation .    l|l||r|r|r    & & & +    & svm & 248 & 180 & 96 + & lr & 446 & 180 & 96 +    & svm & 1256 & 70560 & 631 + & lr & 3100 & 70560 & 634 +    [ [ mllib . ] ] mllib .",
    "+ + + + + +    we run mllib over spark on the same 9-node cluster .",
    "we set 4 workers per node and 4 cores per worker , which takes - up all the 16 cores in a node .",
    "we assign 6 gb of memory per worker , i.e. , 24 gb out of the total 28 gb of memory per node . in all the mllib results , we do not consider the data loading time for spark , which is considerably larger than the time reported in table  [ tbl : comp ] .",
    "nonetheless , we do include the data reading time from disk in our solution . for `",
    "classify50 m ` , mllib is capable to cache the entire dataset in memory , thus it has a decent execution time .",
    "this is not the case for ` splice ` and the results show it .    [",
    "[ vowpal - wabbit - vw . ] ] vowpal wabbit ( vw ) .",
    "+ + + + + + + + + + + + + + + + + + +    vw is i / o - bound if the input data are pre - loaded in its native binary format .",
    "the results reported in table  [ tbl : comp ] are for loaded data .",
    "vw requires an additional pass over the data to compute the loss exactly . the estimated loss it provides at each node during an iteration is local .",
    "the time for a single pass over the data is half of the results shown in table  [ tbl : comp ] . while closer to the results obtained by our implementation , it is important to remember that we overlap model update and loss computation .    [ [ glade - pf - ola . ] ] glade pf - ola .",
    "+ + + + + + + + + + + + +    notice that we measure the execution time of the naive igd implementation which does not include speculative processing and intra - iteration approximation .",
    "if we consider speculative processing , i.e. , multiple concurrent step sizes , the execution time for glade pf - ola stays almost the same ( table  [ tbl : datasets ] ) , while mllib and vw require @xmath58 times longer to complete , where @xmath58 is the number of steps .",
    "likewise , if we consider intra - iteration approximation , the execution time per iteration is even shorter , as shown in figure  [ fig : ola : convergence ] .",
    "we validate the effectiveness and efficiency of the proposed techniques  speculative processing and intra - iteration approximation  for large scale gradient descent optimization .",
    "we show that speculative processing speeds - up the convergence rate of both bgd and igd , and online aggregation reduces the iteration time further .",
    "we find that bgd is better suited to integrate the proposed techniques . in our experiments , bgd always outperforms igd for similar settings . by comparing with state - of - the - art systems",
    ", we confirm that our solution is able to significantly boost the model calibration time for terascale datasets .",
    "we discuss related work in two main categories : distributed gradient descent optimization and parallel online aggregation .",
    "we emphasize the novelty brought by this work when compared to our previous papers on parallel online aggregation  @xcite and incremental gradient descent in glade  @xcite .",
    "[ [ distributed - gradient - descent - optimization . ] ] distributed gradient descent optimization . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    there is a plethora of work on distributed gradient descent algorithms published in machine learning  @xcite .",
    "all these algorithms are similar in that a certain amount of model updates are performed at each node , followed by the transfer of the partial models across nodes .",
    "the differences lie in how the model communication is done  @xcite and how the model is partitioned for specific tasks , e.g. , neural networks  @xcite and matrix factorization  @xcite .",
    "many of the distributed solutions are immediately applicable to multi - core shared memory environments . the work of r et al .",
    "@xcite is representative in this sense .",
    "our work is different from all these approaches because we consider concurrent evaluation of multiple step sizes and we use adaptive intra - iteration approximation to detect convergence .",
    "moreover , igd is taken by default to be the optimal gradient descent method , while bgd is hardly ever considered .",
    "we provide a thorough comparison between bgd and igd , and show that  with our optimizations ",
    "bgd always outperforms igd .",
    "many big data analytics systems and frameworks implement gradient descent optimization .",
    "most of them target distributed applications on top of the hadoop mapreduce framework , e.g. , mahout  @xcite , mllib  @xcite , while others provide complete stacks , e.g. , madlib  @xcite , distributed graphlab  @xcite , and vowpal wabbit  @xcite . with no exception , igd is the only method implemented in all these systems . as a first step",
    ", the techniques we present in this paper can be incorporated into any of these systems , as long as multi - threading parallelism and partial aggregation are supported .",
    "more important , we provide strong evidence that bgd deserves full consideration in any big data analytics system .",
    "[ [ online - aggregation . ] ] online aggregation .",
    "+ + + + + + + + + + + + + + + + + + +    the database online aggregation literature has its origins in the seminal paper by hellerstein et al .",
    "we can broadly categorize this body of work into system design  @xcite , online join algorithms  @xcite , online algorithms for estimations other than join  @xcite , and methods to derive confidence bounds  @xcite .",
    "all of this work is targeted at single - node centralized environments .",
    "the parallel online aggregation literature is not as rich though .",
    "we identified only three lines of research that are closely related to this paper .",
    "luo et al .",
    "@xcite extend the centralized ripple join algorithm  @xcite to a parallel setting .",
    "a stratified sampling estimator  @xcite is defined to compute the result estimate while confidence bounds can not always be derived .",
    "wu et al .",
    "@xcite extend online aggregation to distributed p2p networks .",
    "they introduce a synchronized sampling estimator over partitioned data that requires data movement from storage nodes to processing nodes . in subsequent work ,",
    "wu et al .",
    "@xcite tackle online aggregation over multiple queries .",
    "the third piece of relevant work is online aggregation in mapreduce . in  @xcite",
    ", standard hadoop is extended with a mechanism to compute partial aggregates . in subsequent work",
    "@xcite , an estimation framework based on bayesian statistics is proposed .",
    "blinkdb  @xcite implements a multi - stage approximation mechanism based on pre - computed sampling synopses of multiple sizes , while earl  @xcite and abs  @xcite use bootstrapping to produce multiple estimators from the same sample .",
    "[ [ glade - pf - ola.-1 ] ] glade pf - ola .",
    "+ + + + + + + + + + + + +    the novelty of our work compared to the pf - ola framework  @xcite comes from applying online aggregation estimators to complex analytics , rather than focusing on standard sql aggregates ",
    "the case in previous literature .",
    "we are the first to model gradient descent optimization as an aggregation problem .",
    "this allows us to design multiple concurrent estimators and to define halting mechanisms that stop the execution when model update and loss computation are overlapped .",
    "moreover , the integration of online aggregation with speculative step evaluation allows for early identification of sub - optimal step sizes and directs the system resources toward the promising configurations .",
    "none of the existing systems , including glade pf - ola , support concurrent hyper - parameter evaluation or concurrent estimators .",
    "our previous work on gradient descent optimization in glade  @xcite is limited to igd . in this paper ,",
    "we also consider bgd and propose general methods applicable to distributed gradient descent optimization .",
    "in this paper , we propose two database techniques for efficient model calibration .",
    "speculative parameter testing allows for several parameter configurations to be evaluated concurrently .",
    "online aggregation identifies sub - optimal configurations early in the processing .",
    "we apply the proposed techniques to distributed gradient descent optimization  batch and incremental  and provide an extensive experimental comparison between these two methods .",
    "contrary to the general belief , bgd always outperforms igd both in convergence speed and execution time . in future work",
    ", we plan to extend the proposed techniques to other model calibration methods beyond gradient descent ."
  ],
  "abstract_text": [
    "<S> _ model calibration </S>",
    "<S> _ is a major challenge faced by the plethora of statistical analytics packages that are increasingly used in big data applications . </S>",
    "<S> identifying the optimal model parameters is a time - consuming process that has to be executed from scratch for every dataset / model combination even by experienced data scientists . </S>",
    "<S> we argue that the incapacity to evaluate multiple parameter configurations simultaneously and the lack of support to quickly identify sub - optimal configurations are the principal causes .    in this paper </S>",
    "<S> , we develop two database - inspired techniques for efficient model calibration . </S>",
    "<S> _ speculative parameter testing _ applies advanced parallel multi - query processing methods to evaluate several configurations concurrently . </S>",
    "<S> the number of configurations is determined adaptively at runtime , while the configurations themselves are extracted from a distribution that is continuously learned following a bayesian process . </S>",
    "<S> _ online aggregation _ is applied to identify sub - optimal configurations early in the processing by incrementally sampling the training dataset and estimating the objective function corresponding to each configuration . </S>",
    "<S> we design concurrent online aggregation estimators and define halting conditions to accurately and timely stop the execution .    </S>",
    "<S> we apply the proposed techniques to _ distributed gradient descent optimization _  </S>",
    "<S> batch and incremental  for support vector machines and logistic regression models . </S>",
    "<S> we implement the resulting solutions in glade pf - ola  a state - of - the - art big data analytics system  and evaluate their performance over terascale - size synthetic and real datasets . </S>",
    "<S> the results confirm that as many as 32 configurations can be evaluated concurrently almost as fast as one , while sub - optimal configurations are detected accurately in as little as a @xmath0 fraction of the time . </S>"
  ]
}