{
  "article_text": [
    "in large systems , the observed dynamics or activity of each unit can be represented by a discrete time series providing a sequence of measurements of the state of that unit .",
    "one of the main challenges researchers are faced with is that of extracting meaningful information from the high - dimensional ( multiple ) time series characterizing all the elements of a complex system . traditionally , the main object of time series analysis is the characterization of patterns in the amplitude of the increments of the quantities of interest . given a signal @xmath0 where @xmath1 denotes one of the @xmath2 units of the system and @xmath3 denotes one of the @xmath4 observed temporal snapshots , the generic increment or ` return ' @xmath5 can be defined as @xmath6 and generates a new time series .    while a time series of increments encapsulates all the relevant information about the amplitude of the fluctuations of the original signal ,",
    "a significant part of this information is encoded in the purely ` binary ' projection of @xmath5 , i.e. its sign @xmath7= \\left\\{\\begin{array}{rr } + 1&r_i(t)>0\\\\ 0&r_i(t)=0\\\\ -1&r_i(t)<0 \\end{array}\\right .",
    "\\label{eq : sign}\\ ] ] previous analyses , mainly in the field of finance , have indeed documented various forms of statistical dependency between the sign and the absolute value of fluctuations , e.g. sign - volume correlations @xcite and the leverage effect @xcite .",
    "other studies have also documented that the binary projections of various financial @xcite and neural @xcite time series exhibit nontrivial dynamical features that resemble those of the original data .",
    "all these results suggest that binary projections indeed retain a non - trivial piece of information about the original time series , and call for a deeper analysis of the problem .",
    "being binary , the sign of the increments is much more robust to noise than the increments themselves .",
    "moreover , it is scale - invariant ( i.e. independent of the chosen unit of increments ) and does not depend on whether the original data have been preliminarily rescaled or log - transformed ( as usually done e.g. for financial time series ) .",
    "binary time series can also be analyzed with the aid of much simpler mathematical models than required by non - binary data ( several examples of such models will be provided in this paper ) . finally , as we show later on , in multiple financial time series the total binary increment of a given cross section measures the instantaneous level of synchronization ( i.e. the number of stocks moving in the same direction ) of the market , while the total non - binary increment does not carry this piece of information .",
    "for all the above reasons , it is important to further investigate whether the full ` weighted ' or ` valued ' information can , in some circumstances , be somehow mapped to the binary one , thus providing a robust , highly simplified , more easily modeled , and informative representation of the system .    motivated by the above considerations , in this paper we further study , both empirically and theoretically , the relationship between weighted time series and their binary projections . we first provide robust empirical evidence of novel relationships between binary and non - binary properties of real financial time series . to this end",
    ", we use the daily closing prices of all stocks of three markets ( s@xmath8p500 , ftse100 and nikkei225 ) over the period 2001 - 2011 .",
    "we show that the average daily increment and average daily coupling of an empirical set of multiple time series are strongly and non - linearly related to the corresponding average increment of the binary projections of the same time series .",
    "these empirical relations quantify in a novel way the strong correlations existing between the increments of individual stocks and the overall level of synchronization among all stocks in the market .",
    "building on this evidence , we then introduce a formalism to analytically characterize random ensembles of single and multiple time series with desired constraints .",
    "specifically , we follow jaynes interpretation and re - derivation of statistical physics as an inference problem from partial macroscopic information to the unobservable microscopic configuration @xcite .",
    "we define statistical ensembles of matrices that maximize shannon s entropy @xcite , subject to a set of desired constraints .",
    "this maximum - entropy approach is widely used in many areas , from neuroscience @xcite to social network analysis @xcite ( and more recently network science in general @xcite ) , where it is known under the name of erg ( exponential random graph ) formalism . in the case of interest here , we introduce ensembles of maximum - entropy binary matrices that represent projections of single and multiple binary time series , subject to a set of desired constraints defined as simple empirical measurements .",
    "we discuss the main differences between our matrix ensembles and other techniques in time series analysis , including other ensembles of random matrices encountered in random matrix theory @xcite .",
    "our approach leads to a family of analytically solved null models that allow us to quantify the amount of information encoded in the chosen constraints , i.e. the selected observed properties of the binary projections of real time series .",
    "different choices of the constraints lead to different stochastic processes , a result that allows us to relate known stochastic processes to the corresponding ` target ' empirical properties defining the ensemble of time series spanned by the process itself . after applying the approach to the financial time series in our analysis",
    ", we compare the informativeness of various measured properties and show that different properties are more relevant for different time series and temporal windows .",
    "we also identify distinct regimes in the behaviour of multiple stocks and give the most likely explanation ( endogenous , exogenous , or mixed ) for the observed level of coordination or ` market mode ' , given the measured binary return at a given point in time .",
    "finally , and most importantly , we show that our approach is able to reproduce and mathematically characterize the observed non - linear relationships between binary and non - binary properties of real time series .",
    "the rest of the paper is organized as follows . in sec .",
    "[ sec : data ] we describe the data and provide empirical evidence of the relationships that motivate our work . in sec .",
    "[ sec : methods ] we introduce our theoretical formalism in its general form . in sec .",
    "[ sec : sts ] we apply the formalism to single time series , while in sec .",
    "[ sec : smts ] we apply it to single cross - sections ( temporal snapshots ) of multiple time series . finally , in sec .",
    "[ sec : full ] we consider our method in its full extent and apply it to entire spans of multiple time series , for different financial markets around the globe .",
    "we end with our conclusions in sec .",
    "[ sec : conclusions ] .",
    "we use daily closing prices , for the 10-years period ranging from 24/10/2001 to 18/10/2011 , of all stocks from the indices s@xmath8p500 , ftse100 and nikkei225 . for each index",
    ", we restrict our sample to the maximal group of stocks that are traded continuously throughout the selected period .",
    "this results in 445 stocks for the s@xmath8p500 , 78 stocks for the ftse100 and 193 stocks for the nikkei225 .",
    "we take logarithms of daily closing prices to obtain time series of _ log - prices _ that represent our original ` signal ' @xmath0 , where @xmath1 labels stocks and @xmath3 labels days in the sample .",
    "correspondingly , we construct time series of _ log - returns _ where each entry represents the increment @xmath5 as defined in eq.([eq : r ] ) .",
    "finally , we take the sign @xmath9 of each log - return @xmath5 to obtain an additional , binarized set of time series as in eq.([eq : sign ] ) .",
    "we will refer to the binarized time series as the _ binary projection _ of the original time series . in fig .",
    "[ fig : binarize ] we show a simple example of a weighted time series , along with the corresponding binary projection .",
    "the ( multiple ) time series of @xmath5 and @xmath9 are the main objects of our analysis throughout the paper .",
    "note that , while the use of log - returns rather than simple returns ( i.e. price differences ) in finance is an important step that allows to remove overall trend effects over long time spans @xcite , the binary signature is actually independent of whether the original prices have been logarithmically transformed .",
    "the main reason why for choosing the daily frequency is to achieve an optimal level of structural compatibility between the data and the models we introduce later . as we discuss in detail in sec .",
    "[ sec : methods ] , our models are binary , i.e. they only allow the two values @xmath10 depending on whether the increment of the original time series is positive or negative .",
    "an increment of @xmath11 is not admitted in the models : consistently , we chose a frequency for which zero increments are extremely rare in the data .",
    "in financial markets , this is the case for daily ( or lower ) frequency .",
    "indeed , a zero return value occurs in less than 0.2 % of the cases in our daily data ( when this happens , we randomly switch the corresponding binary increment to either @xmath12 or @xmath13 with equal probability ) .",
    "higher - frequency data feature an increasing percentage of zero returns , a property that calls for an extension of the models considered here .",
    "it should be noted that other types of binary time series , different from the @xmath10 type considered here , can also be defined .",
    "most notably , @xmath14 binary time series can indicate the occurrence of an event in a time period , i.e. whether the event happened ( @xmath15 ) or not ( @xmath11 ) .",
    "financial examples include time series of recession indicators @xcite or of ` switching points ' in stock returns @xcite .",
    "for such @xmath14 binary time series , correlations may not be very informative when measuring a dependence between the dichotomous variables . to confront this gap , in recent years",
    "new methods were introduced , like the auto - persistence function and auto - persistence graph @xcite . in these methods , the dependence structure among the observations is described in terms of conditional probabilities , rather than correlations .",
    "although throughout this paper we will be entirely focusing on @xmath16 binary time series that naturally descend from the original _ signed _ time series of fluctuations , it is interesting to notice that our approach can be extended , with slight modifications , to @xmath14 time series as well . to this end , one needs to re - express all quantities in terms of a @xmath14 binary variable @xmath17 , where @xmath18 is our @xmath10 binary variable , and adapt our approach accordingly .",
    "we now come to the main empirical findings that motivate our paper . for each index and for each day @xmath3 in the sample , we first calculate the average ( over all stocks ) weighted return , that we denote as @xmath19 and define as @xmath20 note that the above expression does not depend on the particular stock @xmath1 , but it does depend on time @xmath3 . our unconventional choice of the symbol @xmath21 to denote an average over stocks is to avoid confusion with temporal averages , that will be denoted by the more usual bar ( @xmath22 ) later in the paper . similarly , we calculate the corresponding average binary return @xmath23 , defined as @xmath24    in fig.[fig : empir1 ] we plot @xmath19 as a function of @xmath23 for all days of various 1-year intervals and for the three indices separately . we find a strong non - linear dependency between the two quantities .",
    "note that the average binary return is bound between @xmath13 and @xmath12 by construction , but the average weighted return is unbounded from both sides . while there are in principle infinite values of @xmath19 that are consistent with the same value of @xmath23 , we observe a tight relationships between the two quantities .",
    "this relationship can be fitted by a one - parameter curve of the form @xmath25=\\frac{a}{2}\\ln\\frac{1+x_i(t)}{1-x_i(t ) } \\label{eq : artan}\\ ] ] ( the theoretical justification for this functional form will be given in sec.[sec : full ] ) , where @xmath26 is in general different for different years and different indices .",
    "still , as we show later , for a given year and market the average weighted return of any day @xmath3 is to a large extent predictable ( out of sample ) from the average binary return of the same day , once @xmath26 is known ( for instance by fitting the above curve to the data for a past time window ) . in sec.[sec : full ] we will also show that the nonlinear character of the observed relations is a genuine signature of correlation in the data , as an uncorrelated null model shows a completely linear behaviour .        there is another empirical relationship , involving a higher - order quantity . for each index and for each day @xmath3 in the sample , we calculated what we will call the average ` coupling ' over the @xmath27 distinct pairs of stocks : @xmath28 ( so now the symbol @xmath21 indicates an average over _ pairs _ of stocks ) .",
    "in fig.[fig : empir2 ] we plot @xmath29 as a function of the average binary return @xmath23 , for the same data as in fig .",
    "[ fig : empir1 ] .",
    "again , we find a strong non - linear dependency , where for a given value of the average binary return of day @xmath3 there is a typical value of the average coupling among all stocks in the same day .",
    "the relationship can be fitted by a one - parameter curve that diverges at @xmath30 . as we show in sec.[sec : full ] , an uncorrelated null model would yield a different , parabolic curve with no divergences . again",
    ", this means that the empirical trend is due to genuine correlations , whose nature will be clarified later on in the paper .",
    "there are even more examples of dependencies that we can find between binary and non - binary properties in the data .",
    "however , in one way or another all these relationships , including that shown in fig.[fig : empir2 ] , ultimately derive from eq.([eq : artan ] ) .",
    "for this reason , we refrain from showing redundant results and focus on the empirical findings discussed so far .",
    "the above analysis indicates that the binary signature of financial time series contains relevant information about the original data .",
    "while the binary signature is _ a priori _ a many - to - one projection involving a significant information loss , we empirically find that there are properties ( namely the average return and average coupling ) for which the projection is virtually a one - to - one ` quasi - stationary ' transformation ( on appropriate time scales , as we show in sec.[sec : full ] ) , allowing to reconstruct the corresponding original , weighted properties to a great extent . rather than exploring the practical aspects of this possibility of reconstruction of the original signal from its binary projection , in this paper we are interested in understanding the origin of this behaviour and providing a simple data - driven model of it .",
    "this will be ultimately achieved in sec.[sec : full ] , where we also show that the binary / non - binary relations we have documented are a novel quantification of the fact that extreme price increments occur more often when most stocks move in the same direction .",
    "this is an important type of correlation between the magnitude of log - returns of individual time series and the level of synchronization ( common sign ) of the increments of all stocks in the market .",
    "having established that the binary projections of real time series contain non - trivial information , in the rest of the paper we introduce a theory of binary time series aimed , among other things , at reproducing the observed non - linear relationships showed in figs .",
    "[ fig : empir1 ] and [ fig : empir2 ] . in our approach",
    ", we regard a synchronous set of binary time series as a @xmath10 matrix and we introduce an ensemble of such matrices via the maximization of shannon s entropy , subject to the constraint that some specified properties of the ensemble match their observed values . an analogous approach is widely used e.g. in network analysis and known under the name of exponential random graphs @xcite .",
    "moreover , we provide an analytical maximum - likelihood method to find the optimal values of the paramaters governing the ensembles , which is again similar in spirit to a method that has been recently introduced for networks @xcite . finally , we describe akaike s information criterion ( aic ) @xcite , which we will use to rank and compare the performance of different null models when fitted to the same data .    being entropy - based , our approach automatically allows us to measure the amount of information encoded into the observed properties chosen as constraints , i.e. how much information is gained about the original ( set of ) time series once those properties are measured .",
    "it also allows us to identify , given a set of measured properties , which ones are more informative and which ones can be discarded , as we show on specific financial examples .",
    "our framework turns out to reproduce the observed non - linear relationships very well , thus providing a simple mathematical explanation and functional form for the plots shown in the previous section .",
    "moreover , we are able to identify , as a function of the binary return only , distinct regimes in the collective behavior of stocks , namely a ` coordinated ' regime dominated by market - wide interactions , an ` uncoordinated ' regime dominated by stock - specific noise and an ` intermediate ' regime where both market - wide and stock - specific information is relevant .",
    "we incidentally note that , despite the available variety of refined and advanced techniques in time series analysis @xcite , how one can quantify ( in the sense of statistical ensembles ) how much information is actually encoded into any given , measurable property of a time series is still not fully understood .",
    "while most studies , starting from the celebrated work by kolmogorov about the algorithmic complexity of sequences of symbols @xcite , have addressed the quantification of the information content of a single time series , much less is known about the information encoded in the measured value of a given time series property ( which , necessarily , involves the idea of an entire _ ensemble _ of time series consistent with the measured value itself ) .",
    "our approach can provide an answer to such a question , by associating an absolute level of uncertainty ( entropy ) to each observable of an empirical ( set of ) time series . in relative terms ,",
    "this also allows us to compare the information content of different properties of a time series , thereby indicating which measured property is the most informative about the original time series .    as a final consideration ,",
    "it is worth mentioning that the maximum - entropy matrix ensembles that we introduce are clearly related to ( and , depending on the specification , potentially overlapping with ) some ensembles that are well studied by random matrix theory @xcite .",
    "however , our approach is different since we generate ensembles of matrices whose probability distributions are determined by the kind of partial information ( empirically measured constraint ) about the real system . in this approach",
    "the maximization of shannon s entropy , given some real - world available information , yields the least biased probability distribution ( over the space of possible matrices ) consistent with the data .",
    "this formalism allows us to relate the probabilistic structure of each matrix ensemble with the choice of the original observed property , or constraint .",
    "similarly , since our matrices represent ( multiple ) time series , we are able to connect the various ensembles to simple stochastic processes induced by the associated matrix probabilities and , again , to the chosen empirical property specifying the ensembles themselves .",
    "we first analytically characterize the properties of families of randomized matrices .",
    "more generally , we introduce a matrix ensemble that maximizes shannon s entropy , while enforcing a set of observed constraints ( selected time series properties ) .",
    "this procedure is analogous to e.g. that leading to the definition of exponential random graphs in network theory @xcite .",
    "however , we will modify it to accommodate @xmath31 matrices , as opposed to @xmath14 or non - negative matrices that describe binary and weighted networks respectively",
    ". the resulting ensemble can thus be denoted as the ` maximum - entropy matrix ' ( mem ) ensemble or equivalently ` exponential random matrices ' ( erms ) model .",
    "let us consider the ensemble of all @xmath10 matrices with dimensions @xmath32 .",
    "each such matrix can represent @xmath2 synchronous time series , all of duration @xmath4 ( for instance , if applied to a set of multiple financial time series , @xmath2 refers to the number of stocks and @xmath4 to the number of time steps ) .",
    "let @xmath33 denote a generic matrix in the ensemble , and @xmath9 its entry ( @xmath34 , @xmath35 ) .",
    "let @xmath36 be the particular real matrix that we observe .",
    "in other words , our ensemble is composed of all possible matrices @xmath33 of the same type as @xmath36 , and includes @xmath36 itself .",
    "for any data - dependent property @xmath37 , we will consider the value @xmath38 obtained when @xmath37 is measured on the particular matrix @xmath33 .",
    "for each matrix @xmath33 in the ensemble , we will assign an occurrence probability @xmath39 .",
    "the expectation value ( ensemble average ) of a property @xmath37 can be expressed as @xmath40 where the sum runs over all matrices in the ensemble .    at this point",
    ", we introduce a set of constraints denoted by the vector @xmath41 .",
    "the constraints are meant to ensure that a given set of observed properties @xmath42 in the real matrix @xmath36 is reproduced by the ensemble itself . in our method",
    "we will enforce ` soft ' constraints by requiring that their expectation value @xmath43 equals the observed one .",
    "the resulting ensemble is a _ canonical _",
    "one where each matrix @xmath33 is assigned a probability @xmath39 that maximizes shannon s entropy @xmath44 subject to the normalization constraint @xmath45 and to the chosen vector of constraints @xmath46 that we are enforced in order to reproduce the desired set of observed quantities .",
    "the solution to the above constrained maximization problem is standard ( see for instance @xcite for a recent derivation in the context of networks ) .",
    "we first introduce the lagrange multipliers @xmath47 and @xmath48 , enforcing eqs . and respectively , and then require that the functional derivative of shannon s entropy ( plus the constraining terms ) vanishes : @xmath49 + } \\displaystyle{\\sum_i \\theta_i\\big[c_i-\\sum_\\mathbf{x } c(\\mathbf{x})p(\\mathbf{x})\\big]\\big\\}=0 \\nonumber\\end{aligned}\\ ] ] this yields @xmath50 for any matrix @xmath33 . using a notation that makes the dependence of all quantities on @xmath51 explicit , we then obtain @xmath52 where @xmath53 is the _ hamiltonian _ @xmath54 which is a linear combination of the constraints , and @xmath55 is the _ partition function _ @xmath56 which is the normalizing constant for the probability . consistently , we can rewrite eq .",
    "more explicitly as a function of @xmath48 : @xmath57 where @xmath58 indicates that the ensemble average is evaluated at the particular parameter value @xmath51 .",
    "equations to define the mem or erm model . specifically , the model yields the probability distribution over a specified ensemble of matrices , which maximizes the entropy under a set of generic constraints .",
    "the guiding principle is that the probability distribution ( over microscopic states ) which have maximum entropy , subject to observed ( macroscopic ) properties , provides the most unbiased representation of our knowledge of the state of a system@xcite . to put it in a more physical frame ,",
    "this is analogous to the gibbs - boltzmann distribution over the microstates of a large system at a well defined temperature , given the thermodynamic ( macroscopic ) observables such as the total energy .",
    "the above derivation shows that the expectation value of any property of the ensemble depends _ functionally _ on the specific enforced constraints @xmath41 through the resulting structure of @xmath59 .",
    "of course , it also depends _ numerically _ on the measured values @xmath60 of the constraints themselves , through the particular parameter value ( that we denote by @xmath61 ) required in order to enforce that the expected and observed values of @xmath41 match : @xmath62    we now show that the value @xmath61 that satisfies eq .",
    "concides with the value that maximizes the likelihood to generate the empirical data , as in the corresponding maximum likelihood ( ml ) approach to network ensembles @xcite .",
    "we start by writing the log - likelihood function of an observed matrix @xmath63 generated by the parameters @xmath48 : @xmath64 we then look for the particular value @xmath61 that maximizes @xmath65 , i.e. @xmath66_{\\vec{\\theta}=\\vec{\\theta}^*}=\\vec 0\\ ] ] ( it is easy to check that the higher - order derivative confirm that @xmath61 is a point of maximum )",
    ". this leads to @xmath67_{\\vec{\\theta}=\\vec{\\theta}^*}=\\vec 0\\ ] ] the solution for that yields the ml condition @xmath68 which coincides with eq .. thus the likelihood of the real matrix @xmath63 is maximized by the specific parameter choice such that the ensemble average of each constraint equals its empirical value measured on @xmath63 , automatically ensuring that the desired constraints are met .",
    "we finally show how we can use akaike s information criterion ( aic ) to rank the performance of different models , i.e. different choices of the constraints , in reproducing the same data .",
    "the aic is an information - theoretic measure of the relative goodness of fit of a model , as compared to a set of alternative models all used to explain the same data @xcite .",
    "it offers a relative measure of the information lost when the given model is used to describe reality .",
    "the power of aic ( and other similar criteria @xcite ) lies in the possibility to rank a set of models in terms of their achieved trade - off between accuracy ( good fit to the data ) and parsimony ( low number of free parameters ) @xcite . in general ,",
    "for the @xmath69-th model in a set of selected models , aic is defined as @xmath70 where @xmath71 is the number of free parameters in the @xmath69-th model and @xmath72 is the maximized log - likelihood of the data under the same model .",
    "the above expression effectively discounts the number @xmath71 of parameters ( complexity ) from the maximized likelihood @xmath72 ( accuracy ) . the model with the lowest value of @xmath73 (",
    "let us denote this value by @xmath74 ) is the ` best ' model in the considered set , achieving the optimal trade - off @xcite .    in the erm / mem family of models",
    "we have introduced , a model is uniquely specified by the choice of the constraints @xmath41 .",
    "given a @xmath32 data matrix @xmath63 and a set @xmath75 of @xmath76 possible choices of constraints , each of the resulting @xmath76 models has an aic value @xmath77 where @xmath71 is the dimensionality of the vector @xmath78 , @xmath79 is the maximized log - likelihood of model @xmath69 , and @xmath80 is the parameter value maximizing such log - likelihood . within our framework ,",
    "aic identifies which measured property @xmath81 is most informative about the entire time series @xmath63 .    in order to understand whether models with values of aic larger than but close to @xmath74 are still competitive , it is customary to define the so - called ` aic weights ' which provide a normalized strength of evidence for a model @xcite . for each model",
    "@xmath69 in the set of @xmath76 models , one first calculates the difference @xmath82 and then defines the aic weight @xmath83 the aic weight @xmath84 represents the probability that the @xmath69-th model is the best one among the @xmath76 selected models . for instance , an aic weight of @xmath85 indicates that , given the data , model @xmath69 has a @xmath86 chance of being the best model among the @xmath76 candidate ones . if two or more models have comparable aic weights ( e.g. @xmath87 , @xmath88 or @xmath89 , @xmath90 , @xmath91 ) , then there is no evidence that the model with the highest aic weight ( lowest aic value ) is clearly outperforming the other ones .",
    "all the models with comparable weights should be considered as competing alternatives , in principle leading to the problem of multi - model inference @xcite .",
    "in this section we consider the first family of specifications of our general approach outlined in sec.[sec : methods ] .",
    "we focus on the simple case of single time series ( @xmath92 ) , where the ensemble of @xmath32 matrices reduces to an ensemble of @xmath93 matrices , or equivalently of @xmath4-dimensional row vectors .",
    "each such vector will still be denoted by @xmath33 .",
    "we assume long time series , i.e. @xmath94 .",
    "this first specification of our abstract formalism is not meant to provide realistic models for the evolution of the binary increments of real financial time series .",
    "rather , it allows us to make different sorts of considerations . on one hand",
    ", it allows us to introduce our formalism using simpler examples first , establishing the basis for the more general cases ( leading to the main results of the paper ) that will be introduced later . on the other hand",
    ", it emphasizes that different and well known ( one - dimensional ) stochastic processes are found as particular examples of maximum - entropy ensembles defined by specific constraints that are otherwise obscure .",
    "identifying these ` driving constraints ' underlying common stochastic processes will help us interpret such processes in the light of the empirical properties being reproduced .",
    "finally , our approach allows us to identify , given the data and given a set of simple properties , which of these properties is encoding the largest amount of information about the original binary signature .",
    "let @xmath33 denote a single time series with entries @xmath95 , where @xmath35 , each representing a temporal increment .",
    "we will denote the average increment ( first moment ) as @xmath96 note that the second moment is always @xmath97 so the sample variance is @xmath98 we also define the @xmath99-delayed product ( with @xmath100 ) @xmath101 where we have introduced periodic boundary conditions : @xmath102 the above periodicity condition in inessential , since we could have used a definition avoiding its introduction , but it makes some expressions simpler in what follows .",
    "periodicity implies that the normalized ( between @xmath13 and @xmath12 ) autocorrelation function ( with delay @xmath99 ) can be defined as @xmath103    since a ( @xmath10 ) binary time series can also be regarded as a chain of classical spins pointing either up or down , it is straightforward to consider simple , analytically solved spin models as the starting point , since these models are defined in terms of a ` physical ' hamiltonian that has precisely the same structure of our ` information - theoretic ' hamiltonian defined in eq .. in what follows , we introduce various model specifications . for each model",
    ", we introduce the constraints that we enforce and the resulting hamiltonian as described in sec.[sec : erm ] .",
    "different constraints correspond to different spin models and lead to different stochastic processes .",
    "this is pictorially illustrated in fig .",
    "[ fig : singlespins ] .",
    "the free parameters conjugated to the constraints will be fitted according to the maximum likelihood principle described in sec.[sec : ml ] .",
    "different models will be ranked according to the aic weights introduced in sec.[sec : aic ] .",
    "the most trivial model is one where we enforce no constraint , i.e. there is no free parameter and the hamiltonian is @xmath104 physically , the above hamiltonian describes a gas of @xmath4 non - interacting ` spins ' in vacuum , i.e. in absence of an external magnetic field .",
    "this model is discussed in the appendix .",
    "the probability of occurrence of a time series @xmath33 is completely uniform over the ensemble of all binary time series of length @xmath4 .",
    "all the @xmath4 elements of @xmath33 are mutually independent and identically distributed .",
    "this results in a completely uniform random walk with zero expected value for each increment : @xmath105 while the ( ensemble ) variance of each increment equals @xmath106\\equiv\\langle x^2(t)\\rangle-{\\langle x(t)\\rangle}^2=1.\\ ] ]    this trivial model generates a symmetric random walk . since the expected return is zero , and the uncertainty is maximal , the variance is also maximal ( for a @xmath16 binary random variable ) .",
    "financially , the model assumes that the stock fluctuates randomly , with no memory , and with no overall ` price drift ' .",
    "this is the most basic model of price dynamics that has been considered in the financial literature since the pioneering work of bachelier , here adapted to the case of binary time series .",
    "the model can be used as a basic benchmark for checking the performance of our other models .",
    "this comparison will be studied in sec . [",
    "sec : comparing ] . since here",
    "the likelihood is independent of any parameter , the aic of the model can be calculated using eq . where the probability is given by eq .",
    "( see appendix ) and the number of parameters is @xmath107 .",
    "we now consider the total increment as the simplest non - trivial ( one - dimensional ) constraint : @xmath108 this leads to the hamiltonian @xmath109 which coincides with the physical hamiltonian for a gas of @xmath4 non - interacting ` spins ' in a common external ` magnetic field ' @xmath110 .",
    "as we show in the appendix , this model generates a _ biased _ random walk where the probability @xmath111 of a given increment @xmath112 at time @xmath3 is @xmath113 the expected return is the hyperbolic tangent @xmath114 while the variance is @xmath106=1- \\tanh^2{\\theta}.\\ ] ] financially , this model still assumes no memory in the fluctuations of a given stock , but it introduces a ` price drift ' in terms of a non - zero expected return .",
    "the maximum likelihood condition , fixing the value @xmath115 of the parameter @xmath116 given a real time series @xmath63 , leads to @xmath117 \\label{ebrw}.\\ ] ] the maximized likelihood for the model is @xmath118 which , using eq . with @xmath119",
    ", can be used to measure the aic ( see sec .  [",
    "sec : aic ] ) of the model , based on the observed data . this will be done in sec .",
    "[ sec : comparing ] .",
    "let us now explore a more complex model of collective behavior .",
    "the models considered so far were non - interacting , i.e. each return in the time series was independent of the previous outcomes .",
    "now we consider a model where , besides the constraint on the total increment specified in eq.([eq : cbiased ] ) , we enforce an additional constraint on the time - delayed ( lagged ) quantity @xmath120 , where @xmath121 is defined in eq . with @xmath122 .",
    "financially , this amounts to enforce the average return _ and _ the average one - step temporal autocorrelation of the time series . in order words , besides a price drift , we also introduce a short - term memory .",
    "the resulting 2-dimensional constraint can be written as @xmath123 if we write the corresponding lagrange multiplier as @xmath124 then the hamiltonian reads @xmath125 where we consider a periodicity condition as in eq . with @xmath122 ,",
    "i.e. @xmath126 . note that , when @xmath33 is a real binary time series of length @xmath4 , this condition can be always enforced by adding one last ( fictious ) timestep @xmath127 and a corresponding increment @xmath128 chosen equal to @xmath129 . for long time series ( large @xmath4 ) , the effects induced by this addition are negligible .",
    "the above hamiltonian coincides with that for the one - dimensional ising model with periodic boundary conditions @xcite , which is a model of interacting spins under the influence of an external ` magnetic ' field @xmath130 .",
    "the model is analytically solvable ( see appendix for the complete derivation ) , which allows us to apply it to real time series in our formalism . in our setting , each time step @xmath3 is seen as a site in an ordered chain of length @xmath4 , and each value @xmath131 is seen as the value of a spin sitting at that site .",
    "` first - neighbour interactions ' along the chain of spins are here interpreted as one - lagged memory effects . as a result of these interactions ,",
    "the model generates time series according to a markov process where the probability of an increment @xmath132 depends on the realized increment @xmath95 at the previous time step @xmath3 .",
    "this is evident from the solution of the model , see e.g. eq . in the appendix .",
    "the solution of the model yields the following expectation values @xmath133 ( see appendix ) where @xmath134 the resulting expected value of the normalized autocorrelation defined in eq .",
    "is simply @xmath135        the above expressions allow us to calculate all the relevant expected properties of the time series generated by the model , once the parameters @xmath130 and @xmath136 are set to the values @xmath137 and @xmath138 maximizing the likelihood @xmath139 of the observed time series @xmath63 .",
    "these values are the solutions of the coupled equations @xmath140 where @xmath141 and @xmath142 are the empirical values measured on the real data @xmath63 .",
    "the maximized likelihood of the model can be calculated as @xmath143 , where @xmath144 is given by eq .   in the appendix .",
    "from the maximized likelihood , the aic can be easily obtained using eq . with @xmath145 .",
    "note that the values @xmath137 and @xmath138 are such that the first point of the expected autocorrelation function , @xmath146 , is necessarily equal to the observed value @xmath147 . based on this first value alone",
    ", the model will provide the full expected autocorrelation @xmath148 as follows : @xmath149^\\tau .",
    "\\label{eq : fullauto}\\ ] ] comparing the above expression , for @xmath150 , with the observed autocorrelation function @xmath151 is an important test of the model .",
    "note that , since @xmath152 , the absolute value of the autocorrelation function @xmath153 is necessarily decreasing . if @xmath154 then @xmath153 will be positive ( and exponentially decreasing ) for all values of @xmath99 .",
    "by contrast , if @xmath155 then @xmath153 will be an oscillating function ( modulated by a decreasing exponential ) , and will take negative values when @xmath99 is odd and positive values when @xmath99 is even .    in fig .",
    "[ fig-5 ] we compare the measured autocorrelation , eq .",
    ", with the predicted one , eq .",
    ", for three different s&p500 stocks ( usb , qcom , and mjn ) over a period of 800 trading days ( approximately 3.5 years ) . as expected , we see that the first point ( one - lagged autocorrelation ) is always reproduced exactly .",
    "we also confirm that , depending on the sign of the first point , the predicted trend is either exponentially decreasing ( e.g. for the usb stock on the left ) or oscillating ( e.g. the qcom and mjn stocks ) .",
    "the dashed lines indicate the noise level , that we arbitrarily fixed at two standard deviations of the fisher - transformed independent and identically distributed pairs of random variables @xmath156 , the pearson correlation coefficient @xmath157 is distributed around zero , but in a non - gaussian way .",
    "however , the quantity @xmath158 , known as the _ fisher tranformation _ , is normally distributed around zero , with standard deviation @xmath159 . the interval @xmath160 , representing a @xmath161 confidence interval for @xmath162 ,",
    "can then be mapped back to the interval @xmath163 to obtain a @xmath161 confidence interval for @xmath157 around zero .",
    "] autocorrelation .",
    "the behaviour of the usb and qcom stocks is representative of the vast majority of stocks , with the autocorrelation within the noise level already at the minimum delay ( @xmath122 ) .",
    "this is in good agreement with what we know about financial time series ( no dependencies for daily frequency , the typical time scale for autocorrelation being of the order of minutes ) .",
    "we also found that the first point , the autocorrelation between two successive days , is small but negative for most stocks in our data set . in the rightmost panel ( mjn stock )",
    "we observe a rare dynamic , where the one - lagged autocorrelation is breaching the noise level and then rapidly oscillates to zero .    as clear from the figure , our model reproduces well the observed autocorrelation in all these different cases , and gives a single mathematical explanation for both the exponentially decaying ( from positive one - lagged autocorrelation ) and the oscillating ( from negative one - lagged autocorrelation ) behaviour .",
    "moreover , the generic feature of the one - dimensional ising model , i.e. the absence of a phase transition characterized by a diverging length ( here , time ) scale @xcite , explains why in real - world time series the memory is always found to be short - ranged .    .",
    "the latter represents the number of months elapsed backwards from october 2011 : for all stocks , all time series used to calculate the aic weights have the same endpoint @xmath16431 october 2011 , and a variable startpoint @xmath165.,title=\"fig : \" ] .",
    "the latter represents the number of months elapsed backwards from october 2011 : for all stocks , all time series used to calculate the aic weights have the same endpoint @xmath16431 october 2011 , and a variable startpoint @xmath165.,title=\"fig : \" ] .",
    "the latter represents the number of months elapsed backwards from october 2011 : for all stocks , all time series used to calculate the aic weights have the same endpoint @xmath16431 october 2011 , and a variable startpoint @xmath165.,title=\"fig : \" ]      as we illustrated in sec .",
    "[ sec : aic ] in the general case , once we have more than one model for the same data @xmath63 , we can use the aic weights to rank all models in terms of the achieved trade - off between accuray ( good fit to the data ) and parsimony ( small number of parameters ) .",
    "the aic weight @xmath84 of a specific model @xmath69 represents the probability that the model is the ` best ' one , among the candidate models .",
    "we applied this procedure to the three models discussed so far ( uniform random walk , biased random walk , one - lagged model ) . as an example , in fig .",
    "[ fig-6 ] we show the values of the aic weights for three different s@xmath8p500 stocks .",
    "we can see that the performance of the models is wildly fluctuating and different across stocks .",
    "this suggests that the informativeness of the measured properties is dependent on different factors , which are not entirely revealed to us .",
    "however , it is clear that in all cases the time horizon @xmath4 plays a key role in the performance of the models .",
    "this means that the outcome depends on how many time steps are included in the analysis .",
    "for instance , we see that in some cases ( citigroup inc .",
    "stock ) the small @xmath4 regime is oscillatory , while the large @xmath4 regime appears to set a preference for a definite model . in other cases ( united health group ) , the three models alternate over quite long periods of time .",
    "most likely , this very irregular behaviour is due to the strong non - stationarity of financial markets : extending the analysis over longer time horizons does not necessarily improve the statistics , because for large @xmath4 the underlying price ( and return ) distributions change in an uncontrolled way .",
    "we stress again that the aic weight indicates which property , among the constraints defining all models , can better characterize the stock , given the observed data .",
    "in other words , it highlights the _ measured property _ that is most informative about the original data . despite the fact that the models considered so far are extremely simplified ( and are by no means intended to be accurate models of financial time series ) , this approach can always identify , in relative terms , the most useful empirical quantity characterizing an observed time series .",
    "in the previous section we considered models for single time series , where @xmath92 and @xmath4 is large . here",
    "we consider , as a second specification of our general formalism , the somewhat ` opposite ' case of single cross - sections of @xmath2 multiple time series , which represent a daily snapshot of the market dynamics . for clarity , fig .",
    "[ fig:7 ] portrays a single cross - section of a set of multiple time series . in this case ,",
    "@xmath166 and we assume @xmath167 .",
    "so the matrix @xmath33 has dimensions @xmath168 , i.e. it is an @xmath2-dimensional column vector .",
    "the entries of a cross - section @xmath33 will be denoted by @xmath169 , where @xmath34 , each representing the daily increment of a different asset .",
    "using again the symbol @xmath21 to denote an average over stocks ( as in sec .",
    "[ sec : empirical ] ) , we now define the average increment ( first moment ) of @xmath33 as @xmath170 and the second moment as @xmath171 therefore the sample variance is @xmath172 we also define the total ` coupling ' between stocks ( for a specific cross section @xmath33 ) as @xmath173 where now , as in eq . , @xmath21 denotes an average over all pairs of stocks .    in what follows",
    ", we will consider various models for single cross - sections .",
    "the main difference with respect to the models of single time series considered in sec .",
    "[ sec : sts ] is that the interaction between time steps for a given stock is now replaced by the interaction between different stocks for a given time step . as well known , in real financial markets the interactions among stocks ( as measured e.g. via cross - correlations ) are much stronger than inter - temporal autocorrelations .",
    "this makes the cross - sectional properties significantly different from those of the dynamics of single time series , once inter - stock interactions are enforced in the model . yet ,",
    "in simple models without interaction , we recover similar expected properties .      as in sec . [",
    "sec : singleurw ] , we first consider a trivial model without constraints ( see appendix ) , defined by the hamiltonian @xmath104 the probability of occurrence of a cross section @xmath33 is completely uniform over the ensemble of all binary cross sections of @xmath2 stocks .",
    "again , this ` gas of non - interacting spins in vacuum ' model results in a uniform random walk , where all the @xmath2 elements of @xmath33 are mutually independent and identically distributed .    in the financial setting",
    ", this model assumes that all stocks fluctuate independently of each other ( where the ` fluctuations ' are intended as ensemble ones , since we are now considering a single cross section ) , and under the effect of no common factor .",
    "each stock has zero expected value @xmath174 and maximum variance @xmath175\\equiv\\langle x^2_i\\rangle-{\\langle x_i\\rangle}^2=1.\\ ] ]    in sec .",
    "[ sec : comparing2 ] , we will compare the performance of this trivial benchmark to that of the other models we are about to introduce .",
    "to this end , the aic value can be calculated from eq .",
    "choosing @xmath107 and using the ( constant ) likelihood given by eq . in the appendix .      in this model , which is analogous to",
    "that defined in sec .",
    "[ sec : singlebrw ] , the constraint is chosen as the total daily increment of the cross section @xmath33 : @xmath176 where @xmath177 is defined by eq .  .",
    "the hamiltonian is then @xmath178 similarly to its counterpart for single time series , this is a model of non - interacting spins under the effect of a common external field , and leads to a biased random walk ( see appendix ) .",
    "the financial interpretation is however different : in this model , all stocks are assumed to fluctuate ( again , in an ` ensemble ' sense ) under the effect of a common market - wide factor , but are conditionally independent of each other , given the market - wide factor itself . in the econophysics literature , the overall tendency of all stocks to move together is generally referred to as the ` market mode ' @xcite .",
    "when applied to the data , this extremely simple model interprets the observed market mode as the consequence of an external factor ( e.g. news ) , and not of direct interactions among stocks .",
    "the probability @xmath179 of a given increment @xmath112 for stock @xmath1 is @xmath180 the expected value of the @xmath1-th increment @xmath169 is @xmath181 and the variance is @xmath175 =   1- \\tanh^2{\\theta}.\\ ] ]    the maximum likelihood condition , fixing the value @xmath115 of the parameter @xmath116 given a real cross section @xmath63 , leads to @xmath182 , \\label{eq : fixtheta}\\ ] ] where @xmath183 is the measured average increment of the observed cross section @xmath63 .",
    "we will apply this model to real financial data in secs .",
    "[ sec : comparing2 ] and [ sec : full ] .",
    "the aic of the model is given by eq .",
    "where @xmath119 and where the maximized likelihood is given by @xmath184 , with @xmath185 given by eq .",
    "( see appendix ) .",
    "we now consider a more complex model , with interactions among _ all _ stocks , which is suitable for financial cross - sections .",
    "besides the constraint on the total increment , we enforce an additional constraint on the average coupling between stocks .",
    "the resulting 2-dimensional constraint can be written as @xmath186 where @xmath187 is given by eq .   and",
    "@xmath188 by eq .  .",
    "if we write the corresponding lagrange multiplier as @xmath189 then the hamiltonian reads @xmath190    like the one - lagged model for single time series ( see sec . [",
    "sec : singleolm ] ) , this model is formally analogous to an ising model of interacting spins under the influence of an external ` magnetic ' field ( here denoted by @xmath191 ) .",
    "however , the big difference is that , whereas in the one - lagged model each increment @xmath95 interacts _ only with the next temporal increment @xmath132 of the same stock _ , here each increment @xmath169 interacts _ with all the other increments @xmath192 of the same cross section @xmath33 _ , i.e. with all other stocks in the market . as a model of spin systems , the above model is generally known as the mean - field ising model @xcite . in the appendix we provide the analytical solution of the model , adapted to our setting .    in the financial setting ,",
    "this model allows us to separately consider the effects of the external field , i.e. a common factor affecting all stocks in the market , from those of the average interaction among all stocks .",
    "this market - wide interaction can also cause all stocks to correlate , but has the different interpretation of a collective effect , i.e. the tendency of stocks increments to ` align ' with each other as a result of direct interactions , rather than of a common influence .",
    "this is a sort of ` herd effect ' at the coarse - grained level of attractive ( @xmath193 ) inter - stock interactions .",
    "so , the model can generate the ` market mode ' either as the result of a common external influence such as news ( in which case all stocks are still conditionally independent given the common factor ) , or as a collective effect due to mutual interactions ( in which case all stocks are conditionally dependent given the common factor ) .    while the model can in principle simulate synthetic time series under a combination of the above two effects by varying the two parameters @xmath191 and @xmath194 independently , a problem arises when it is fitted to the data .",
    "the mathematical root of the problem is the well known fact that @xmath195 can be rewritten as a linear combination of @xmath177 and @xmath196 .",
    "as we show in the appendix , this implies that , when the maximum likelihood principle is used to fit the model to the data @xmath63 , the variance of @xmath177 becomes zero . in other words",
    ", the model degenerates to one where @xmath177 is no longer a random variable .",
    "this also implies that the two equations fixing the values of the parameters @xmath197 and @xmath198 become identical ( see appendix ) .",
    "therefore it is no longer possible to uniquely fix the values of both parameters , and the problem is over - constrained .",
    "for this reason , we need to eliminate one parameter and consider the model only in the two extreme cases @xmath199 and @xmath200 .",
    "these two cases can be treated separately .",
    "the case @xmath200 coincides with the biased random walk model already considered in sec .",
    "[ sec : crossbrw ] , where @xmath201 . using eq .",
    ", we therefore specify this model using the two parameter values @xmath202,\\quad",
    "j^*=0 \\label{eq : spec1}\\ ] ] where @xmath183 is the observed average increment of the empirical cross section @xmath63 .",
    "this model interprets the market mode as arising _ only _ from a common external factor .",
    "the case @xmath199 leads us instead to a novel model where the market mode is interpreted _ only _ as a collective effect arising from inter - stock interactions . using the analytical results reported in the appendix , and in particular eq . , we find that the parameter values are in this case @xmath203 .",
    "\\label{eq : spec2}\\ ] ] in what follows , when using the ` mean - field ' model , we will always refer to the parameter specification defined by .",
    "the other specification , eq .",
    ", will instead still be denoted as the ` biased random walk ' model .    in fig .",
    "[ fig-8 ] we plot the value of @xmath197 as a function of @xmath183 , as defined by eq ..",
    "we note however that eq .",
    "is undefined for @xmath204 and @xmath205 .",
    "the breakdown for @xmath204 simply means that , in order to align _ all _ returns ( in either direction ) , @xmath197 should diverge to @xmath206 .",
    "the breakdown for @xmath205 is instead more profound . for infinitesimal ( both positive and negative ) values of @xmath183",
    ", @xmath197 admits the finite limit @xmath207 however , at the very point @xmath205 , @xmath197 is actually indeterminate .     as a function of the measured average binary return @xmath183 ( blue curve ) for a group of @xmath208 stocks ( as in our s&p sample ) .",
    "the curve shows a one - to - one relationship for @xmath209 . while @xmath210 , for @xmath205 the value of @xmath197 is actually indeterminate , as there is an infinity of values of @xmath197 ( namely all values @xmath211 , see vertical green line ) that are possible solutions of the model .",
    "the value of @xmath212 is indicated by the horizontal red line.,scaledwidth=45.0% ]    the above effect is due to the well - known phase transition of the mean - field ising model . in the traditional physical setting",
    ", the phase transition occurs at a critical temperature ( here reabsorbed in the value of the parameters @xmath191 and @xmath194 ) .",
    "when @xmath199 , the critical value is obtained by setting @xmath213 , because for @xmath214 eq .",
    "( see appendix ) has the single solution @xmath215 , corresponding to a phase with no macroscopic magnetization , while for @xmath216 there are three solutions , one of which is still @xmath215 ( which is now unstable ) and the other two ones being the stable solutions @xmath217 ( corresponding to the onset of a macroscopic magnetization @xmath218 where most spins point in the same direction ) . in our financial setting , since the magnetization is fixed by the data through the relation @xmath219 , the condition @xmath220 implies that the phase transition occurs at the critical value @xmath221 _ of the control parameter _ @xmath197 .",
    "we can therefore rewrite eq . as @xmath222 for @xmath223",
    "we get a ` magnetized ' phase where most stock prices move in the same direction ( aligned returns ) , while for @xmath224 we get a non - magnetized phase where there is no collective alignment of stock increments , and @xmath205 .",
    "we therefore conclude that the reason why the value of @xmath197 is indeterminate for @xmath225 is because there is an infinity of values of @xmath197 ( namely all values @xmath211 ) that are possible solutions of the model .",
    "it should be noted that the case @xmath225 is never practically encountered in reality , since the empirical @xmath183 can be abritrarily small , but is generally not really zero .",
    "while this ` protects ' the model from the indeterminacy discussed above , it raises another problem of arbitrariness , which can however be solved very effectively using the information - theoretic criteria that we have introduced in sec.[sec : aic ] .",
    "the problem is that the mean - field model will always interpret even the tiniest empirical deviations from @xmath225 as the result of direct interactions among stocks , and attach a value @xmath226 to this interpretation .",
    "this will also apply to e.g. most realizations of a purely uniform random walk : even if for such a model one knows that the theoretical expected return is zero , most realizations will be such that @xmath227 is small but non - zero .",
    "so the only phase of the mean - field model that can be explored is the ` magnetized ' phase dominated by collective effects .",
    "this implies that even a pure effect of noise will be interpreted as the presence of interactions .",
    "however , this problem will be solved in the next section , where we show that an information - theoretic comparison between the mean - field model , the uniform random walk , and the biased random walk is able to discriminate the most parsimonious model , thus allowing us to trust the mean - field model only when @xmath227 is distant enough from zero .",
    "we can now combine the three models together and use the aic weights ( see sec.[sec : aic ] ) to determine which model achieves the optimal trade - off between accuracy and parsimony .",
    "this will immediately provide us with an indication of whether the observed market mode , as reflected in the empirical aggregate increment @xmath227 , should be interpreted e.g. as a common exogenous factor , as a collective endogeneous effect , or even only as the sheer outcome of chance .",
    "the fact that the likelihoods of the biased random walk and the mean - field model depend only on @xmath227 and @xmath2 , plus the fact that the likelihood of the uniform random walk is constant , allows us to obtain the aic values for the three models as functions of @xmath227 and @xmath2 only . in fig .",
    "[ fig-9 ] we show the calculated aic weights of the three models as a function of the observed value @xmath228 , for @xmath208 s&p500 stocks .",
    "each point represents a different cross section , i.e. a different day of trade , for a total of 100 randomly sampled days .",
    "it is important to note that the empirical value of the average increment only determines which point(s ) of the curves are actually visited , but the curves themselves are universal .",
    "the figure reveals us a remarkable fact , namely the presence of three distinct regimes in the behavior of the group of stocks . for @xmath229",
    ", we find that the best performing model is the uniform random walk , which displays an aic weight practically equal to one ( indicating that the model is almost surely the best one among the three models considered , see sec.[sec : aic ] ) .",
    "this means that , in this ` noisy ' regime , the most parsimonious explanation of the market mode , as reflected in the measured value of @xmath183 , is that of a pure outcome of chance .    for @xmath230 , we find that the uniform random walk is almost surely _ not _ the best model , while the biased random walk and mean field models are competing .",
    "we observe an almost equal performance of the two models for @xmath231 , and an increasing preference for the mean field model as @xmath232 increases towards @xmath233 . despite this preference",
    ", we can not reject the mean field model , meaning that in this ` mixed ' regime the most likely explanation for the market mode is a combination of exogenous and endogenous effects .",
    "finally , for @xmath230 , the mean field model achieves practically unit probability to be the best model . in this ` endogenous ' regime , the most likely explanation for the market model is uniquely in terms of a collective effect of direct influence among stocks .",
    "we can summarize the above findings as follows : @xmath234 where we recall that the values of @xmath232 delimiting the various regimes have been calculated for @xmath208 .",
    "while the qualitative finding that larger values of @xmath232 are better explained in terms of collective effects might appear intuitive , the possibility to quantitatively identify the value @xmath235 above which this intuition is fully supported by statistical evidence is a non - obvious output of the above approach .",
    "the same consideration applies to the identification of the other two regimes , and of a mixed phase where there is not enough statistical evidence in favour of a single interpretation of the market mode .",
    "moreover , the fact that the mean field model starts being statistically significant only for @xmath236 solves the aforementioned problem of an otherwise problematic interpretation of even tiny values of @xmath232 as the result of inter - stock interactions .",
    "the aic analysis shows that , for values below @xmath237 , one should not trust the mean field model , and consequently the value @xmath226 that the model itself indicates . when @xmath238 , the best model is actually the uniform random walk , which effectively corresponds to @xmath239 . this is a highly non - trivial result .",
    ", for @xmath208 s@xmath8p500 stocks , each studied for 100 days of trade . ]",
    "in this section , as our third and final specification of the abstract formalism introduced in sec.[sec : methods ] , we extend the previous results to the general case where the observed data is a full @xmath32 matrix @xmath63 representing a set of multiple binary time series for @xmath2 stocks , each extending over @xmath4 timesteps .",
    "we recall that the entries of a generic such matrix @xmath33 are denoted by @xmath9 , where @xmath1 labels the stock and @xmath3 labels the time step .",
    "we assume that @xmath2 and @xmath4 are both large , i.e. @xmath240 and @xmath241 . before introducing an explicit model ,",
    "we need to make some important considerations .",
    "we had already anticipated that the purpose of the models introduced in the previous sections was not that of introducing realistic models of financial time series .",
    "for instance , it is well known that the simple stochastic processes considered in sec.[sec : sts ] are far too simple to reproduce some key stylized facts observed in real financial time series , such as volatility clustering @xcite or a bursty behavior @xcite . moreover , being entirely binary , the above examples can not address other well established properties characterizing the amplitude of fluctuations , e.g. the ` fat ' ( power - law ) tails of the empirical distributions of price returns .    nonetheless , there is a simple argument that legitimates us to use a proper extension of the above modelling approach , especially that introduced in sec .",
    "[ sec : smts ] , provided that we adequately calibrate such extension on the observed set of multiple time series .",
    "the argument is basically the realization that we can properly model the binary signature of a time series , using temporal iterations of even the simplistic models we have introduced in sec .",
    "[ sec : smts ] , if we assume that some aggregated information measured on the original ` weighted ' time series @xmath5 ( @xmath34 ) can be used as a proxy of the driving factor defining the model itself .",
    "we will show that this simple assumption is actually verified in the data .",
    "in particular , we will show that a sequence of temporal iterations of the biased random walk model , which assumes that the binary time series is driven by an ` external ' field , can be ` bootstrapped ' on the real data by assuming that the field can be replaced by a function of the ( endogenous ) observed aggregate increment of the original weighted time series , i.e. the empirical value @xmath242 of the quantity @xmath243 defined in eq .. in such a way , we do not need a model generating a realistic dynamics of @xmath243 ( or of the individual stock - specific increments ) in order to model the behaviour of @xmath244 , because the time series of @xmath243 is taken from the data .    as a result",
    ", we will obtain an accurate model for the dynamics of the aggregate binary increment @xmath23 , given the observed dynamics of @xmath19 .",
    "this model will reproduce with great accuracy , and mathematically characterize , the empirical non - linear relation between these two quantities that we have illustrated in sec.[sec : empirical ] .",
    "we will finally test the temporal robustness and predictive power of the model , and conclude with discussion of the relatedness of our approach and more traditional ` factor models ' in finance .          in order to execute the above plan",
    ", we first analyze the correlations between single cross sections of the market .",
    "we need this preliminary analysis in order to determine whether the temporal extension of the models defined in sec . [ sec : smts ] should incorporate dependencies among different snapshots .    based on extensive financial literature",
    ", we expect no correlation ( on a daily frequency ) among the returns of different cross sections . however , most analyses focus on the auto - correlation of _ individual _ stocks , based on their _ weighted _ returns .",
    "so , to check our hypothesis we perform an explicit analysis of the temporal auto - correlation of the observed time series of the _ aggregate _ , _ binary _ return @xmath245 .",
    "this analysis is shown in fig.[fig-10 ] for the three indices , using daily data for year 2006 .",
    "we confirm that the observed autocorrelation is not statistically significant , since ( apart for a few points ) it lies within the range of random noise ( calculated by imposing a threshold of two standard deviations on the fisher - transformed autocorrelation ) .",
    "this type of uncorrelated dynamics is observed throughout our dataset .",
    "this means that , in line with other analyses of autocorrelation , the memory of the aggregate binary return of real markets , if any , is much shorter than a day .",
    "going back to the result illustrated in fig .",
    "[ fig-9 ] , we can then conclude that there is no significant correlation in the trajectories of the daily points populating the curves . in other words , given the knowledge of the position of the market in the aic curves in a given day , we can not predict where the market will move the next day , even if of course we know that it will move to another point in the curves themselves .",
    "the previous result sets the stage for our next step , where we consider an explicit extension of the models considered in sec.[sec : smts ] to an ensemble of multiple time series , as introduced in sec.[sec : methods ] in the general case .",
    "the absence of autocorrelation implies that we can define the hamiltonian of the full @xmath32 matrix @xmath33 as a sum of @xmath4 non - interacting hamiltonians , each describing a single cross section of @xmath2 stocks .",
    "next , we need to choose the model to extend .",
    "we want the final model to establish ( among other things ) an expected relationship between the binary and the weighted aggregate returns , so that we can test this prediction against the empirical relationships illustrated in sec.[sec : empirical ] .",
    "this implies that we need to input the measured weighted return @xmath246 as a driving parameter of the binary model . among the three models ,",
    "only the biased random walk and the mean field model have parameters that can be related to @xmath246 . in sec .",
    "[ sec : smts ] we treated those models as giving competing interpretations of the market model in terms of exogenous and endogenous effects respectively .",
    "however , it should be noted that this is no longer possible as soon as the parameters of these models are made dependent on the observed return .",
    "for instance , if we assume that the parameter @xmath116 of the biased random walk depends on @xmath246 ( which is a property of the data ) , we can no longer interpret @xmath116 as an external field , since it has been somehow ` endogenized ' .",
    "determining whether @xmath116 can be interpreted as endogenous or exogeneous is now entirely dependent on whether @xmath246 itself can be interpreted as endogenous or exogeneous .",
    "this tautology does not prevent us from determining a relationship between @xmath246 and @xmath183 in their full range of variation , because such relationship is independent on the optimal ( endogenous or exogenous ) interpretation of both quantities .",
    "we also note that the choice of the model to calibrate on @xmath243 is now completely independent of the relative performance of the various models that we have determined in the case of free parameters , including their aic weights shown in fig.[fig-9 ] .",
    "indeed , apart from an initial calibration , the parameters will no longer be fitted using the maximum likelihood principle , making the aic analysis no longer appropriate .",
    "in other words , ranking the ` free ' models and endogenizing their parameters are two completely different problems .",
    "in particular , the low aic weight of the biased random walk throughout most of fig.[fig-9 ] does not impede us from using this model in our next analysis . we will indeed ` bootstrap ' the biased random walk on the real data , by looking for a relationship between @xmath243 and the parameter @xmath116 .",
    "we prefer this model over the mean field one because , while it is natural to think of ( a function of ) @xmath243 as a proxy of the ` field ' @xmath116 affecting the market in the biased random walk model ( notably , @xmath243 has a definition similar to that of a market index ) , it is less natural to think of the same quantity as a proxy of the inter - stock interaction @xmath194 in the mean field model ( although , as we said before , this would be technically possible ) .    combining all the above considerations",
    ", we finally generalize the biased random walk model defined by eq . to the matrix case as follows : @xmath247 where @xmath48 it a @xmath4-dimensional vector with entries @xmath248 .",
    "note that , while the models we introduced in sec.[sec : sts ] have time - independent parameters and therefore correspond to time series at statistical equilibrium ( for example a model with constant volatility ) , we are now considering more general models with time - dependent parameters . relating @xmath248 to @xmath19 will allow us to incorporate any observed degree of non - stationarity of the data into the model itself .    as a preliminary calibration",
    ", we now look for an empirical relation between @xmath19 and @xmath248 . to this end",
    ", we first treat the latter as a free parameter and look for the optimal value @xmath249 maximizing the likelihood of the observed binary time series @xmath63 . since the hamiltonians for different timesteps are non - interacting , it is easy to show that @xmath249 is given again by eq . where @xmath227 is replaced by @xmath250 : @xmath251 .",
    "\\label{eq : fixthetat}\\ ] ]        in fig .",
    "[ fig-11 ] we compare the resulting value of @xmath249 with the corresponding observed weighted return @xmath252 , for the three indices separately .",
    "each point in the plot corresponds to a different day , and we considered 250 days ( approximately one year ) for each index .",
    "we find a strong linear relation between the two quantities .",
    "this relation can be fitted by the one - parameter curve @xmath253 where @xmath254 .",
    "this finding is very important .",
    "it confirms that the parameter @xmath249 , defined through eq . as a time - varying ` field ' driving the observed binary increment @xmath245 with maximum likelihood , is an excellent proxy for the observed non - binary ` market index ' @xmath252 .",
    "this result holds up to a negative factor @xmath255 which , on the time scale considered , is constant for each market ( in sec .",
    "[ sec : c ] we will provide a more detailed analysis of the stability of @xmath255 over different time scales ) . since @xmath252 is a property measured on the stock increments themselves , it reflects both external influences and internal dependencies . therefore @xmath249 can not be ( entirely ) interpreted as an external field .",
    "this confirms our interpretation of the biased random walk as a model agnostic to the ( endogenous or exogenous ) nature of the driving field in the present setting .    combining eqs . and",
    "together , we finally obtain a mathematical expression for the expected relationship between @xmath246 and @xmath183 in our model : @xmath256=c\\cdot\\textrm{artanh}\\{x^*_i(t)\\}. \\label{eq : endo}\\ ] ] inverting , we have @xmath257 we can now test the above expressions against the data shown previously in fig.[fig : empir1 ] .",
    "in that figure , we already showed that the observed relationship between @xmath246 and @xmath183 can be fitted very well by a curve of the form given by eq .. we have just provided a theoretical justification for the otherwise arbitrary use of such expression .",
    "moreover , now we can fit the value of @xmath255 using eq .",
    ", which is independent of eq .. once we obtain @xmath255 in this way , we can use eq . to predict @xmath252",
    "given @xmath245 , or _",
    "vice versa _ , without fitting any parameter . in fig.[fig-12 ] we show the result of this operation .",
    "we confirm that the prediction of our model matches the empirical relationship very well .",
    "we also consider a null model where we randomly shuffle the increments of each of the @xmath2 time series independently .",
    "this results in a set of randomized time series , with elements @xmath258 , where the total increment @xmath259 for each stock is preserved , but the returns of all stocks in a given day are uncorrelated . from @xmath258 , we obtain the binary signature @xmath260 as for the real data . as shown in fig.[fig-12 ] , this randomized benchmark overlaps with the empirical trend only in a very narrow , linear regime .",
    "we will now try to understand this result .",
    "the reason why the shuffled data result in a linear trend is the following .",
    "for each value of @xmath261 , there is a definite number @xmath262 of ` up ' stocks and a definite number @xmath263 of ` down ' stocks , according to the relation @xmath264 conditional on the above value of @xmath261 , the expected value of @xmath265 ( over multiple shufflings ) is @xmath266=r^*_+\\{x'_i\\}. \\label{eq : linear}\\ ] ] where @xmath267 is the average positive increment ( over all @xmath4 time steps and all @xmath2 time series ) and @xmath268 is the average negative increment .",
    "note that both values coincide with the corresponding quantities in the original data , and have been denoted by a star accordingly . assuming approximately symmetric log - return distributions for each of the @xmath2 time series as typically observed , we have set @xmath269 . given the overlap between real and shuffled data around zero returns in fig.[fig-12 ]",
    ", we can linearize eq .",
    "around zero and compare it with eq . to get @xmath270 the above expression suggests that the value of @xmath255 strongly depends on the original log - return distribution .",
    "therefore , we expect that the stability of @xmath255 is determined by that of @xmath271 . in sec . [",
    "sec : c ] we will study the stability of @xmath255 in more detail .",
    "the above simple argument shows that , for shuffled data , we indeed expect a linear relationship between @xmath272 and @xmath273 .",
    "this is a striking difference with respect to real data , where @xmath252 virtually diverges as @xmath274 approaches one .",
    "this ` divergence ' indicates that , when most stocks are aligned in real markets ( @xmath275 ) , the observed log - returns are much larger than the typical positive increment ( @xmath276 ) . in other words ,",
    "extreme log - returns are more often observed when stocks are synchronized .",
    "this means that there is a strong correlation between the magnitude of log - returns of individual time series and the degree of coordination of all stocks in the market .    while for infinite realizations of the shuffling procedure we would observe eq .",
    "extending to the full range @xmath277 , for finite realizations we observe a much narrower span of values ( see fig.[fig-12 ] ) .",
    "this is due to the absence of correlations among stocks , resulting in significantly lower values of both @xmath265 and @xmath261 with respect to the observed quantities @xmath246 and @xmath183 .",
    "interestingly enough , for the s&p500 index the randomized data span the range @xmath278 , which coincides precisely with the regime we identified in fig.[fig-9 ] for a completely noisy - driven system with the same number of stocks .",
    "this confirms that the aic analysis correctly pinpoints the boundaries outside which one should expect the observed value @xmath183 to be inconsistent with a typical realization of @xmath2 purely random variables .",
    "the above results also provide an explanation for the second empirical nonlinear relation that we had documented in sec . [",
    "sec : empirical ] , i.e. the one between @xmath279 and @xmath245 ( see fig .",
    "[ fig : empir2 ] ) . in general , we can write @xmath280 as @xmath281.\\ ] ] the term @xmath282 is of order @xmath2 , and vanishes for large markets when divided by @xmath283 .",
    "we are therefore left with @xmath284 using eq .",
    ", we get @xmath285 which theoretically justifies the fitting function we had used in fig.[fig : empir2 ] . again , rather than fitting that curve on the data , we can use the value of @xmath255 determined from the ( independent ) fit shown in fig.[fig-11 ] .",
    "this results in the non - parametric plot shown in fig .",
    "[ fig-13 ] .",
    "we confirm that , for each of the three indices , we can reproduce the observed relationship very well .    as before",
    ", we also show the relationship between @xmath286 and @xmath273 for randomly shuffled data .",
    "the linearity of eq . now translates into an expected parabolic relationship : @xmath287 again , real data strongly deviate from the above ` uncorrelated ' parabolic expectation , because extreme events make the empirical coupling @xmath288 virtually ` diverge ' when stocks are highly synchronized ( @xmath289 ) .",
    "once we have mathematically characterized the observed nonlinear relations , an unavoidable question arises : in a given market , how stable are those relations ?",
    "since @xmath255 is the only parameter in the above analysis , the question simply translates into the stability of @xmath255 .",
    "we have already noted that @xmath255 is related to the average positive return @xmath290 , which we expect to be relatively stable . in order to study the stability of @xmath255 in more detail ,",
    "we now consider several yearly and monthly time windows , and explore the time evolution of the fitted parameter for the three indices .    in fig .",
    "[ fig-14 ] ( upper panels ) we plot the values of the parameter @xmath255 ( with error bars ) for 11 yearly snapshots ( 2001 - 2010 ) .",
    "it is clear that there are periods during which the yearly values are relatively stable , and periods when they fluctuate wildly .",
    "thus , in most cases the fitted value of @xmath255 in a given year does not allow to make predictions about the value of @xmath255 int the next year .",
    "however , we can also consider a monthly frequency .",
    "in the bottom panels of fig .",
    "[ fig-14 ] we show the result of our analysis , when carried out on the 12 monthly snapshots of year 2006 .",
    "we choose this particular year because , in the yearly trends shown above , it represents very different points for different markets : the end of a stable period for the ftse100 , an exceptional jump for the s&p500 , and the middle of an increasing trend for the nikkei225 . despite these differences",
    ", we find that in all three markets the monthly dynamics is much more stable than the yearly one .",
    "in particular , the trends for ftse100 and nikkei225 are almost constant , and for the s&p500 there are only two deviating points from an otherwise stable trend ( despite the large fluctuation that 2006 represents in the yearly trend for this index ) .",
    "this implies that , in most cases , one might even use the monthly value of @xmath255 out of sample , in order to predict the future relationship between @xmath244 and @xmath243 based on a past observation .",
    "we should however stress that the aim of our method is to characterize such relationship , and not to predict it .",
    "indeed , we can not imagine any situation in which only the binary ( or only the non - binary ) information is available .",
    "the above results show that there is a trade - off between short and long periods of time . for short ( e.g. monthly ) periods",
    "there are less points to calculate @xmath255 through a fit of the type shown in fig.[fig-11 ] .",
    "this explains why the monthly trends in fig.[fig-14 ] have bigger error bars than the yearly trends in the same figure .",
    "by contrast , for longer ( e.g. yearly ) periods each individual fit is better , but there are more fluctuations in the temporal evolution of the parameter @xmath255 , because the data are less stationary . in general , we expect that in each market , and for a specific period of time , there is a different ` optimal ' frequency to consider .",
    "we would like to conclude this paper with a discussion of the relationship between some of our findings and the popular _ factor models _ in the financial literature @xcite . as a basic consideration",
    ", we stress that factor models can only be applied to the original ( non - binary ) increments ( it is impossible to decompose a binary signal into a nontrivial combination of binary signals ) , while our models only apply to the binary projections .",
    "we should bear this irreducible difference in mind in what follows .",
    "however , due to the mapping between binary and non - binary increments that we have documented , we can try to indeed relate the two approaches .",
    "first , let us consider the shuffled ( uncorrelated ) data , where the original log - returns are randomly permuted within each of the @xmath2 time series .",
    "it is well known that the total temporal increment ( over @xmath4 time steps ) of any empirical time series of price increments is generally close to zero ( due to market efficiency ) , and that the distribution of log - returns is mostly symmetric around this value .",
    "this is especially true if each of the @xmath2 original time series has been separately standardized , i.e. the @xmath1-th temporal average has been subtracted from each increment of the @xmath1-th time series , and the result has been divided by the @xmath1-th standard deviation .",
    "in such a case , the @xmath2 log - return distributions become also very similar to each other , because their support is the same and their values are comparable .",
    "this means that , after the shuffling , the time series are sequences of independent and almost identically distributed variables with zero mean .",
    "we denote the corresponding increments as @xmath291 where the @xmath292 s are random variables . in a traditional factor analysis",
    ", the above scenario takes the form of a ` zero - factor ' model . under this model , the aggregate increment over @xmath2 stocks",
    "is expected to be narrowly distributed around @xmath293 when @xmath19 takes small values around zero , we know from fig .",
    "[ fig-12 ] that @xmath23 also takes small values around zero .",
    "indeed , shuffled time series are in the linear regime that spans the range where the binary increment @xmath23 is consistent with a uniform random walk ( see fig.[fig-9 ] ) .",
    "therefore we find that the zero - factor model ( for the non - binary returns ) and the uniform random walk ( for the binary returns ) are consistent with each other in the linear regime .",
    "in other words , when in our analysis we measure a value of @xmath23 that is consistent with a uniform random walk , we know that the original log - returns are consistent with a zero - factor model .",
    "next , we consider a one - factor model , where there is one dominant underlying factor assumed to control the dynamics of all the time series . in such a case",
    ", each return can be decomposed as @xmath294 where @xmath295 is the ` factor loading ' of the @xmath1-th time series with the dominant factor @xmath296 .",
    "when referring to stocks , the factor @xmath296 is attributed to the market mode .",
    "it is known that , during crisis times when the markets are highly correlated , a one - factor model can describe the dynamics quite well . under this model ,",
    "the aggregate increment is @xmath297 where @xmath298 is the average loading , which is independent of both @xmath1 and @xmath3 .",
    "this result implies that , when the market is well described by a one - factor model , the average increment @xmath299 that we measure in our analysis is proportional to the factor @xmath296 itself .",
    "we note that the one - factor model is somehow similar to our biased random walk model , as it assumes a common drive for all the stocks .",
    "however , since @xmath296 is fitted on the data , the one - factor model can not distinguish between an endogenous or exogenous nature of the common drive .",
    "this situation is similar to when we use the observed value of @xmath299 as the driving field of the biased random walk ( see sec.[sec : reproducing ] ) .    in financial analysis",
    ", the factor model can be used to filter the original time series and remove the one - factor component from them .",
    "when the model is a good approximation to the real market , the filtered returns are @xmath300 , leading us back to eq . and the related considerations . in such a scenario , there is no correlation among the stocks , and each stock is acting as an i.i.d . variable .",
    "we therefore expect that , if we remove the market mode from the original time series , then ( in periods where the market is indeed dominated by a single factor ) we would obtain results similar to the shuffled case , and we would find the system in the uncoordinated phase of fig.[fig-9 ] .",
    "however , despite the fact that in certain conditions the one - factor model can generate the market behaviour , the model is too simplistic @xcite . in reality",
    "the dynamics is more complex and can be attributed to many factors , that sometimes overlap with industrial ( sub)sectors . generally the different factors are identified by the largest , non - random eigenvalues of the empirical cross - correlation matrix , where the market mode relates to the highest eigenvalue @xcite .",
    "the presence of many deviating eigenvalues is an indication of the fact that the one - factor model should be rejected . a more realistic , @xmath301-factor model is @xmath302 where @xmath303 denotes a common market - wide factor as above , while @xmath304 denotes sector - specific factors . in such a case ,",
    "our measured value of @xmath299 is @xmath305 which is a linear combination of the multiple factors controlling the market dynamics .",
    "it should be noted that factor models can not distinguish between an endogenous and exogenous origin for the factors @xmath306 themselves , even if we invoke some information - theoretic criterion to rank different specifications of these models . by contrast ,",
    "our binary models allow us to discriminate among these multiple scenarios , as we have shown in fig.[fig-9 ] and related discussions . moreover , while our approach allows us to relate binary and non - binary increments of real time series and replicate the observed relationships among them ( see figs.[fig-12 ] and [ fig-13 ] ) , factor models can not lead to a similar result , because they do not allow for a binary description .",
    "we presented a novel method for the analysis of single and multiple binary time series .",
    "our information - theoretic approach allowed us to extract and quantify the amount of information encoded in simple , empirically measured properties .",
    "this resulted in the possibility to associate an entropy value to a time series given its measured properties , and to compare the informativeness of different measured properties .    by employing our formalism ,",
    "we have identified distinct regimes in the collective behavior of groups of stocks , corresponding to different levels of coordination that only depend on the average return of the binary time series . in each regime",
    "the market exhibits a dominant character : the market mode can be interpreted as an exogenous factor , as pure noise , or as a combination of endogenous and exogenous components .",
    "moreover , each regime is characterized by the most informative property .",
    "finally and more importantly , we were able to replicate the observed non - linear relations between binary and non - binary aggregate increments of real multiple time series .",
    "we have mathematically characterized these relations accurately , and interpreted them as the result of the fact that very large log - returns occur more often when most stocks are synchronized , i.e. when their increments have a common sign .",
    "our findings suggest that the binary signatures carry significant information , and even allow to measure the level of coordination in a way that is unaccessible to standard non - binary analyses .",
    "we thank marc van kralingen for a thorough reading of our manuscript and for identifying some mistakes .",
    "we acknowledge support from the dutch econophysics foundation ( stichting econophysics , leiden , the netherlands ) with funds from beneficiaries of duyfken trading knowledge bv , amsterdam , the netherlands .",
    "this work was also supported by the eu project multiplex ( contract 317532 ) and the netherlands organization for scientific research ( nwo / ocw ) .",
    "we considere the case @xmath92 , i.e. when @xmath33 is a @xmath93 matrix or equivalently a @xmath4-dimensional row vector .",
    "let us denote the entries of @xmath33 as @xmath95 .",
    "the trivial model is obtained when no constraints are enforced . in this case , there is no free parameter and the hamiltonian has the form @xmath307 as a result , the partition function is @xmath308 which is nothing but the number of possible binary time series of length @xmath4 .",
    "the probability of occurrence of a time series @xmath33 is then @xmath309 and is completely uniform over the ensemble of all binary time series of length @xmath4 .",
    "all the @xmath4 elements of @xmath33 are mutually independent and identically distributed with probability @xmath310 this results in a completely uniform random walk with zero expected value for each increment : @xmath105 while the ( ensemble ) variance of each increment equals @xmath106\\equiv\\langle x^2(t)\\rangle-{\\langle x(t)\\rangle}^2=1.\\ ] ]      we now consider the total increment as the simplest non - trivial ( one - dimensional ) constraint : @xmath311 if we denote the corresponding ( scalar ) lagrange multiplier by @xmath312 , the hamiltonian has the form @xmath313 the partition function is @xmath314\\nonumber\\\\ & = & \\left [ e^{-\\theta } + e^{+\\theta } \\right]^t\\end{aligned}\\ ] ] where , when interchanging the order of the sum and product , we have replaced the sum over all time series @xmath33 with the sum over the two possible values @xmath112 of each individual entry .",
    "the probability of the occurrence of a time series @xmath33 is @xmath315^t}=   \\prod_{t=1}^t\\frac { e^{-\\theta x(t ) } } { e^{-\\theta } + e^{+\\theta}}\\nonumber\\\\ & = &   \\prod_{t=1}^t p_t\\big(x(t)|\\theta\\big)\\end{aligned}\\ ] ] where we have introduced the probability @xmath111 of a given increment @xmath112 at time @xmath3 , which we identify as @xmath113 the above expression shows that the stochastic process corresponding to this model is a biased random walk , as the two outcomes @xmath112 have a different probability , unless @xmath316 ( which leads us back to the uniform random walk model considered above ) .",
    "the expected value of the @xmath3-th increment @xmath95 ( representing the bias of the random walk ) is @xmath317 and the variance is @xmath106 = \\langle x^2(t)\\rangle_\\theta- { \\langle x(t)\\rangle_\\theta}^2=    1- \\tanh^2{\\theta}.\\ ] ]    the maximum likelihood condition , fixing the value @xmath115 of the parameter @xmath116 given a real time series @xmath63 , reads @xmath318 where @xmath319 is the measured average increment in the observed time series @xmath63 .",
    "this yields @xmath320 which gives a parameter value @xmath321=-\\frac{1}{2}\\ln\\left[\\frac{1+\\overline{x^*(t)}}{1-\\overline{x^*(t)}}\\right]\\ ] ]      we now consider a model where , besides the constraint on the total increment specified in eq.([eq : cbiased ] ) , we enforce an additional constraint on the time - delayed ( lagged ) quantity @xmath120 , where @xmath121 is defined in eq . with @xmath122 .",
    "this amounts to enforce the average one - step temporal autocorrelation of the time series .",
    "the resulting 2-dimensional constraint can be written as the column vector @xmath123 if we write the corresponding lagrange multiplier as @xmath124 then the hamiltonian reads @xmath322 where we consider a periodicity condition as in eq . with @xmath122 ,",
    "i.e. @xmath126 .",
    "note that , when @xmath33 is a real binary time series of length @xmath4 , this condition can be always enforced by adding one last ( fictious ) timestep @xmath127 and a corresponding increment @xmath128 chosen equal to @xmath129 . for long time",
    "series , this has a negligible effect .",
    "the above hamiltonian coincides with that for the one - dimensional ising model with periodic boundary conditions @xcite .",
    "each time step @xmath3 is seen as a site in an ordered chain of length @xmath4 , and each value @xmath131 is seen as the value of a spin sitting at that site .",
    "the model is analytically solvable , which allows us to apply it to real time series in our formalism . for the readers familar with time series analysis but not necessarily with the ising model , we briefly recall the standard solution of the model , adapting it from ref . @xcite .    applying the periodicity condition of eq .",
    "ensures that all sites ( time steps ) are statistically equivalent , i.e. : @xmath323 so that the system is translationally ( here , temporally ) invariant . the partition function is @xmath324\\nonumber\\ ] ] and can be rewritten as a product of terms involving only two successive time steps : @xmath325 where we have introduced the function @xmath326 defined as @xmath327    we since both @xmath18 and @xmath328 can take only the values @xmath10 , we can regard @xmath326 as the element of a @xmath329 matrix @xmath330 called the _ transfer matrix _",
    "@xcite : @xmath331 this allows us to rewrite eq . as @xmath332 let @xmath333 denote the two eigenvectors of @xmath330 , and @xmath334 the corresponding eigenvalues , so that @xmath335 the @xmath336 matrix @xmath337 ( having column vectors @xmath338 and @xmath339 ) diagonalizes @xmath330 , i.e. @xmath340 where a direct calculation of the eigenvalues and eigenvectors yields @xmath341 and @xmath342 with @xmath343 defined by @xmath344 it then follows that eq . simply reduces to @xmath345 and the probability of occurrence of a time series @xmath33 is @xmath346    the above results allow us to analytically obtain expected values .",
    "that of @xmath95 is @xmath347 where we have introduced the diagonal matrix @xmath348 having elements @xmath349 similarly , for @xmath350 the expected value of @xmath351 is @xmath352 in the limit @xmath353 ( corresponding to long time series in our case ) with @xmath354 fixed , these expressions become @xmath355 now , we note that eqs . and manifestly show the translational ( temporal ) invariance of the model , as @xmath356 is independent of @xmath3 and @xmath357 depends on @xmath3 and @xmath358 only through their difference @xmath354 .",
    "this implies that , writing @xmath359 and performing a temporal average , @xmath360 using eq .",
    "we can rewrite these expressions in terms of the model parameters , @xmath130 and @xmath136 , as @xmath361    the expected value of the autocorrelation defined in eq .",
    "can be approximated as the ratio of two expected values as follows : @xmath362",
    "for a single cross - section of a set of @xmath2 multiple time series , @xmath33 is a @xmath168 matrix or equivalently a @xmath2-dimensional column vector .",
    "we denote the entries of @xmath33 as @xmath169 .",
    "the uniform random walk is a simple modification of the same model that we considered for single time series , where @xmath95 is replaced by @xmath169 and @xmath4 is replaced by @xmath2 .",
    "this model is obtained when no constraints are enforced .",
    "the hamiltonian is @xmath307 and the partition function is simply the number of possible configurations for a single cross - section of @xmath2 stocks : @xmath363 the probability of occurrence of a cross section @xmath33 is @xmath364 and is completely uniform over the ensemble of all cross sections of @xmath2 stocks .",
    "all the @xmath2 elements of @xmath33 are mutually independent and identically distributed with probability @xmath365 this results in a completely uniform random walk with zero expected value @xmath174 and maximum variance @xmath175\\equiv\\langle x_i^2\\rangle-{\\langle x_i\\rangle}^2=1.\\ ] ]      also this model is analogous to the corresponding model for single time series .",
    "we select the total daily increment of the cross section @xmath33 as the constraint : @xmath366 let the corresponding lagrange multiplier be denoted by @xmath312 .",
    "the hamiltonian is @xmath367 and the partition function is @xmath368\\nonumber\\\\ & = & \\left [ e^{-\\theta } + e^{+\\theta } \\right]^n.\\end{aligned}\\ ] ]    the probability of the occurrence of a cross section @xmath33 is @xmath369^n}=   \\prod_{i=1}^n\\frac { e^{-\\theta x_i } } { e^{-\\theta } + e^{+\\theta}}\\nonumber\\\\ & = &   \\prod_{i=1}^n p_i\\big(x_i|\\theta\\big ) \\label{eq : apppcrossbias}\\end{aligned}\\ ] ] where we have introduced the probability @xmath179 of a given increment @xmath112 for stock @xmath1 , which we identify as @xmath370 just like the corresponding model for single time series , this model is a biased random walk , because the two outcomes @xmath112 have a different probability unless @xmath316 .",
    "the maximum likelihood condition , fixing the value @xmath115 of the parameter @xmath116 given a real cross section @xmath63 , reads @xmath372 where @xmath183 is the measured average increment of the observed cross section @xmath63 .",
    "this yields @xmath373 which gives a parameter value @xmath374=-\\frac{1}{2}\\ln\\left[\\frac{1+\\{x^*_i\\}}{1-\\{x^*_i\\}}\\right].\\ ] ]      in this model , we enforce two constraints : the total increment and the total coupling between stocks",
    ". the resulting 2-dimensional constraint can be written as @xmath375 we can write the corresponding lagrange multiplier as @xmath189 and the hamiltonian as @xmath190    note that here we are not enforcing nearest - neighbor interactions as in the one - lagged model for single time series , but market - wide interactions among all stocks for the same time step ( cross section ) .",
    "this is the result of the fact that , when considering cross sections , there is no natural notion of ` lattice sites ' induced by e.g. a temporal ordering as in the one - lagged model . in other words , pairs of stocks in a cross section",
    "are neither ` close ' nor ` distant ' . we therefore assume a common interaction strength @xmath194 among all stocks .",
    "the above model , known as the mean - field ising model , is analytically solvable .",
    "here we adapt the derivation illustrated in ref .",
    "we first note that , since @xmath376 for all @xmath1 , @xmath195 can be expressed as a function of @xmath187 alone : @xmath377 .",
    "\\label{eq : hm}\\ ] ] this implies that the sum over configurations in the partition function can be replaced by a sum over the allowed values of @xmath187 , weighted by the number of configurations for each value .",
    "if we denote by @xmath378 the number of increments that are negative @xmath379 , and by @xmath380 the number of increments that are positive @xmath381 , then we can write the hamiltonian as a function of @xmath378 alone through the expression @xmath382 the partition function can therefore be calculated as @xmath383 where @xmath384}\\ ] ] incorporates the binomial coefficient enumerating the configurations with given @xmath378 .",
    "the expected increment is therefore @xmath385    when @xmath2 is large , a traditional derivation @xcite shows that the sum at the numerator of eq . is dominated by the single addendum corresponding to the maximum of @xmath386 .",
    "the same applies to the partition function at the denominator .",
    "if @xmath387 denotes the value of @xmath378 such that @xmath386 is maximum , we then get @xmath388 a further expansion @xcite finally shows that , given @xmath191 and @xmath194 , the expected value @xmath389 is the solution of the nonlinear equation @xmath390.\\ ] ] from the above equation , one can infer the existence of a phase transition in the model , separating a regime where the expected ` magnetization ' ( here the average increment @xmath389 ) is zero from one where it is non - zero @xcite .",
    "this transition is discussed in sec.[sec : crossmfm ] .    before proceeding further",
    ", we note a peculiarity of the model , which has implications for the applicability of our maximum likelihood approach .",
    "an argument similar to that leading to eq .",
    "implies that the second moment of @xmath177 can be expressed as @xmath391 this implies that @xmath392\\equiv \\langle m_1 ^ 2\\rangle-\\langle m_1\\rangle^2=0,\\ ] ] or in other words that @xmath177 is no longer a random variable . as a consequence , something unusual happens when we apply the maximum likelihood principle . from eq . , and recalling the general result embodied by eq . in sec.[sec :",
    "ml ] , it is clear that the parameter values @xmath198 and @xmath197 maximizing the likelihood can be found as the solution to the two coupled equations @xmath393 however , eq . implies that eq .",
    "can be rewritten as @xmath394 which coincides with eq .. so eqs . and are equivalent , and they can not be used to uniquely determine the two unknown parameters @xmath198 and @xmath197 .",
    "this is the result of the fact that , when fitted to the data , the model is actually over - constrained : there are two parameters to fit the only constraint ( @xmath395 ) on which the hamiltonian depends .",
    "this aspect of the model is not manifest when @xmath395 is regarded as a function of @xmath191 and @xmath194 , as usually done when simulating spin systems .",
    "the above consideration implies that we should drop one of the two parameters and consider the two cases @xmath200 and @xmath199 separately .",
    "the former case coincides with the biased random walk model that we already discussed , and we will not discuss it any further .",
    "the latter case will instead represent our genuine specification of the ` mean - field ' model .",
    "setting @xmath199 implies @xmath396 \\label{eq : hm2}\\ ] ] and @xmath397 .",
    "\\label{eq : nonlinear}\\ ] ] applying the maximum likelihood principle to eq . tells us to select @xmath197 as the solution of eq .. however , we have seen that this condition leads to eq . ,",
    "which is actually equivalent to eq .. therefore , the value of @xmath197 can be found by replacing @xmath389 with the observed value @xmath398 in eq . , which leads to @xmath399 .",
    "\\label{eq : nonlinear2}\\ ] ] note that in the traditional situation one is interested in finding the ( expected ) magnetization given a value of @xmath194 , which implies that the transcendental eq . should be solved numerically . here",
    ", we are instead facing the inverse situation where we look for the value of @xmath197 given the ( observed ) value of the magnetization . in this quite unusual case",
    ", it turns out that eq .",
    "can be inverted to give the following analytical solution : @xmath400 .",
    "\\label{eq : fixj}\\ ] ] once this value is calculated , it can be inserted into the probability @xmath401/2 } } \\ ] ] ( where we have set @xmath199 ) to obtain the maximized likelihood of generating the observed cross section @xmath63 under the mean - field model .",
    "burnham , kenneth p. , anderson , david r. model selection an multi model inference , 2nd edition,(springer 2002 ) stanley wasserman .",
    "social network analysis : methods and applications .",
    "( cambridge university press 1994 ) m. l. metha , nucl .",
    "phys . , 18 395 ( 1960 ) ; m. l. mehta and f. j. dyson , j. math .",
    "phys . 4 , 713 ( 1963 ) ; m. l. metha , commun . math .",
    "phys . , 20 245 ( 1971 )",
    ";                              t squartini , g fagiolo , d garlaschelli randomizing world trade . i. a binary network analysis _ phys .",
    "e _ * 84 * , 046117 ( 2011 ) .",
    "t squartini , g fagiolo , d garlaschelli randomizing world trade .",
    "ii . a weighted network analysis _",
    "e _ * 84 * , 046118 ( 2011 ) ."
  ],
  "abstract_text": [
    "<S> the dynamics of complex systems , from financial markets to the brain , can be monitored in terms of multiple time series of activity of the constituent units , such as stocks or neurons respectively . while the main focus of time series analysis is on the magnitude of temporal increments , a significant piece of information </S>",
    "<S> is encoded into the binary projection ( i.e. the sign ) of such increments . in this paper </S>",
    "<S> we provide further evidence of this by showing strong nonlinear relations between binary and non - binary properties of financial time series . </S>",
    "<S> these relations are a novel quantification of the fact that extreme price increments occur more often when most stocks move in the same direction . </S>",
    "<S> we then introduce an information - theoretic approach to the analysis of the binary signature of single and multiple time series . through the definition of maximum - entropy ensembles of binary matrices and their mapping to spin models in statistical physics </S>",
    "<S> , we quantify the information encoded into the simplest binary properties of real time series and identify the most informative property given a set of measurements . </S>",
    "<S> our formalism is able to accurately replicate , and mathematically characterize , the observed binary / non - binary relations . </S>",
    "<S> we also obtain a phase diagram allowing us to identify , based only on the instantaneous aggregate return of a set of multiple time series , a regime where the so - called ` market mode ' has an optimal interpretation in terms of collective ( endogenous ) effects , a regime where it is parsimoniously explained by pure noise , and a regime where it can be regarded as a combination of endogenous and exogenous factors . </S>",
    "<S> our approach allows us to connect spin models , simple stochastic processes , and ensembles of time series inferred from partial information . </S>"
  ]
}