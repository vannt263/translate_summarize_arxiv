{
  "article_text": [
    "risk arises from the uncertainties associated with future events , and is inevitable since the consequences of actions are uncertain at the time when a decision is made .",
    "hence , risk has to be taken into account by the decision - maker , consciously or unconsciously .",
    "an economically rational decision - making rule , which is _ risk - neutral _ , is to select the alternative with the highest expected reward . in the context of sequential or multistage decision - making problems , _",
    "reinforcement learning _",
    "( rl , @xcite ) follows this line of thought .",
    "it describes how an agent ought to take actions that maximize expected cumulative rewards in an environment typically described by a _ markov decision process _",
    "( mdp , @xcite ) .",
    "rl is a well - developed model not only for human decision - making , but also for models of free choice in non - humans , because similar computational structures , such as dopaminergically mediated reward prediction errors , have been identified across species ( @xcite ) .",
    "besides risk - neutral policies , _ risk - averse _ policies , which accept a choice with a more certain but possibly lower expected reward , are also considered economically rational @xcite .",
    "for example , a risk - averse investor might choose to put money into a bank account with a low but guaranteed interest rate , rather than into a stock with possibly high expected returns but also a chance of high losses .",
    "conversely , _ risk - seeking _ policies , which prefer a choice with less certain but possibly high reward , are considered economically irrational .",
    "human agents are , however , not always economically rational @xcite .",
    "behavioral studies show that human can be risk - seeking in one situation while risk - averse in another situation @xcite .",
    "rl algorithms developed so far can not effectively model these complicated risk - preferences .",
    "risk - sensitive decision - making problems , in the context of mdps , have been investigated in various fields , e.g. , in machine learning @xcite , optimal control @xcite , operations research @xcite , finance @xcite , as well as neuroscience @xcite .",
    "note that the core of mdps consists of two sets of _ objective _ quantities describing the environment : immediate _ rewards _ obtained at states by executing actions , and _ transition probabilities _ for switching states when performing actions . facing the same environment ,",
    "however , different agents might have different policies , which indicates that risk is taken into account differently by different agents .",
    "hence , to incorporate risk , which is derived from both quantities , all existing literature applies a nonlinear transformation to either the experienced reward values or to the transition probabilities , or to both .",
    "the former is the canonical approach in classical economics , as in expected utility theory @xcite , while the latter originates from behavioral economics , as in _ subjective probability _",
    "@xcite , but is also derived from a rather recent development in mathematical finance , _ convex / coherent risk measures _ @xcite . for modeling human behaviors , prospect theory ( @xcite )",
    "suggests that we should combine both approaches , i.e. , human beings have different perceptions not only for the same objective amount of rewards but also the same value of the true probability .",
    "recently , @xcite combined both approaches by applying piecewise linear functions ( an approximation of a nonlinear transformation ) to reward prediction errors that contain the information of rewards directly and the information of transition probabilities indirectly .",
    "importantly , the reward prediction errors that incorporated experienced risk were strongly coupled to activity in the nucleus accumbens of the ventral striatum , providing a biologically based plausibility to this combined approach . in this work",
    "we show ( in section 2.1 ) that the risk - sensitive algorithm proposed by niv and colleagues is a special case of our general risk - sensitive rl framework .",
    "most of the literature in economics or engineering fields focuses on economically rational risk - averse/-neutral strategies , which are not always adopted by humans .",
    "the models proposed in behavioral economics , despite allowing economic irrationality , require knowledge of the true probability , which usually is not available at the outset of a learning task . in neuroscience , on the one hand , several works ( e.g. , @xcite ) follow the same line as in behavioral economics and require knowledge of the true probability .",
    "on the other hand , though different modified rl algorithms ( e.g. , @xcite ) are applied to model human behaviors in learning tasks , the algorithms often fail to generalize across different tasks . in our previous work @xcite",
    ", we described a general framework for incorporating risk into mdps by introducing nonlinear transformations to both rewards and transition probabilities .",
    "a risk - sensitive objective was derived and optimized by value iteration or dynamic programming .",
    "this solution , hence , does not work in learning tasks where the true transition probabilities are unknown to learning agents .",
    "for this purpose , a model - free framework for rl algorithms is to be derived in this paper , where , similar to q - learning , the knowledge of the transition and reward model is not needed .",
    "this paper is organized as follows .",
    "section [ sec : valfunc ] starts with a mathematical introduction into _ valuation functions _ for measuring risk .",
    "we then specify a sufficiently rich class of valuation functions in section [ sec : ubsf ] and provide the intuition behind our approach by applying this class to a simple example in section [ sec : toyex ] .",
    "we aslo show that key features of prospect theory can be captured by this class of valuation functions .",
    "restricted to the same class , we derive a general framework for risk - sensitive q - learning algorithms and prove its convergence in section [ sec : rsmdps ] . finally , in section [ sec :",
    "ex ] , we apply this framework to quantify human behavior .",
    "we show that the risk - sensitive variant provides a significantly better fit to the behavioral data and significant correlations are found between sequences generated by the proposed framework and changes of fmri bold signals .",
    "suppose that we are facing choices . each _ choice _ might yield different outcomes when events are generated by a random process .",
    "hence , to keep generality , we model the outcome of each choice by a real - valued random variable @xmath0 , where @xmath1 denotes an _ event space _ with a finite cardinality @xmath2 and @xmath3 is the outcome of @xmath4th event with probability @xmath5 .",
    "we say two vectors @xmath6 if @xmath7 for all @xmath8 .",
    "let @xmath9 ( resp .",
    "@xmath10 ) denote the vector with all elements equal 1 ( resp .  0 ) .",
    "let @xmath11 denote the space of all possible distributions @xmath12 .",
    "choices are made according to their outcomes .",
    "hence , we assume that there exists a mapping @xmath13 such that one prefers @xmath14 to @xmath15 whenever @xmath16 .",
    "we assume further that @xmath17 satisfies the following axioms inspired by the _ risk measure theory _ applied in mathematical finance @xcite .",
    "a mapping @xmath13 is called a * valuation function * , if it satisfies for each @xmath18 ,    * ( monotonicity ) @xmath19 , whenever @xmath20 ; * ( translation invariance ) @xmath21 , for any @xmath22 .    within the economic context ,",
    "@xmath23 and @xmath24 are outcomes of two choices .",
    "monotonicity reflects the intuition that given the same event distribution @xmath12 , if the outcome of one choice is _ always _",
    "( for all events ) higher than the outcome of another choice , the _ valuation _ of the choice must be also higher . under the axiom of translation invariance , the sure outcome @xmath25 ( equal outcome for every event ) after executing decisions , is considered as a sure outcome before making decision .",
    "this also reflects the intuition that there is no risk if there is no uncertainty .    in our setting ,",
    "valuation functions are not necessarily centralized , i.e.  @xmath26 is not necessarily 0 , since @xmath26 in fact sets a reference point , which can differ for different agents .",
    "however , we can centralize any valuation function by @xmath27 . from the two axioms",
    ", it follows that ( for the proof see lemma [ lm : inrm ] in appendix ) @xmath28 @xmath29 is the possibly largest outcome , which represents the most optimistic prediction of the future , while @xmath30 is the possibly smallest outcome and the most pessimistic estimation .",
    "the centralized valuation function @xmath31 satisfying @xmath32 can be in fact viewed as a subjective mean of the random variable @xmath23 , which varies from the best scenario @xmath29 to the worst scenario @xmath30 , covering the objective mean as a special case .    to judge the risk - preference induced by a certain type of valuation functions , we follow the rule that _ diversification _ should be preferred if the agent is _ risk - averse_. more specifically , suppose an agent has two possible choices , one of which leads to the future reward @xmath14 while the other one leads to the future reward @xmath33 . for simplicity",
    "we assume @xmath34 . if the agent _ diversifies _ ,",
    "i.e. , if one spends only a fraction @xmath35 of the resources on the first and the remaining amount on the second alternative , the future reward is given by @xmath36 .",
    "if the applied valuation function is concave , i.e. , @xmath37 for all @xmath38 $ ] and @xmath39 then the diversification should increase the ( subjective ) valuation .",
    "thus , we call the agent s behavior _ risk - averse_. conversely , if the applied valuation function is _ convex _ , the induced risk - preference should be _ risk - seeking_.      we now introduce a class of valuation functions , the utility - based shortfall , which generalizes many important special valuation functions in literature .",
    "let @xmath40 be a _ utility function _",
    ", which is continuous and strictly increasing .",
    "the shortfall @xmath41 induced by @xmath42 and an _ acceptance level _",
    "@xmath43 is then defined as @xmath44 it can be shown ( cf .",
    "@xcite ) that @xmath45 is a valid valuation function satisfying the axioms .",
    "the utility - based shortfall was first introduced in the mathematical finance literature @xcite . the class of utility functions considered here will , however , be more general than the class of utility functions typically used in finance .    comparing with the expected utility theory , the utility function in eq .",
    "is applied to the relative value @xmath46 rather than to the absolute outcome @xmath47 .",
    "this reflects the intuition that human beings judge utilities usually by comparing those outcome with a reference value which may not be zero .",
    "the property of @xmath42 being convex or concave determines the risk sensitivity of @xmath41 : given a concave function @xmath42 , @xmath17 is also concave and hence risk - averse ( see theorem 4.61 , @xcite ) .",
    "vice versa , @xmath17 is convex ( hence risk - seeking ) for convex @xmath42 .",
    "utility - based shortfalls cover a large family of valuation functions , which have been proposed in literature of various fields .    * for @xmath48 and @xmath49 ,",
    "one obtains the standard expected reward @xmath50 . * for @xmath51 and @xmath52 , one obtains @xmath53 $ ] ( the so called _ entropic map _ , see e.g.  @xcite and references therein ) . expansion w.r.t .",
    "@xmath54 leads to @xmath55 + \\lambda \\textrm{var}^\\mu[x ] + o(\\lambda^2 )   \\end{aligned}\\ ] ] where @xmath56 $ ] denotes the variance of @xmath23 under the distribution @xmath12 .",
    "hence , the entropic map is risk - averse if @xmath57 and risk - seeking if @xmath58 . in neuroscience , @xcite and @xcite applied this type of valuation function to test risk - sensitivity in human sensorimotor control .",
    "* @xcite proposed the following setting @xmath59 where @xmath60 controls the degree of risk sensitivity .",
    "its sign determines the property of the utility function @xmath42 being convex vs.  concave and , therefore , the risk - preference of @xmath17 . in a recent study",
    ", @xcite applied this type of valuation function to quantify risk - sensitive behavior of human subjects and to interpret the measured neural signals .    when quantifying human behavior , combined convex / concave utility functions , e.g. , @xmath61 are of special interest , since people tend to treat gains and losses differently and , therefore , have different risk preferences on gain and loss sides .",
    "in fact , the polynomial function in eq .   was used in the prospect theory @xcite to model human risk preferences and the results show that @xmath62 is usually below 1 , i.e. , @xmath63 is concave and thus risk - averse on gains , while @xmath64 is also below 1 and @xmath63 is therefore convex and risk - seeking on losses .      to illustrate the risk - preferences induced by different utility functions , we consider a simple example with two events .",
    "the first event has outcome @xmath65 with probability @xmath66 , while the other event has smaller outcome @xmath67 with @xmath68 .",
    "note that @xmath69 , where @xmath70 denotes the risk - neutral mean .",
    "replacing @xmath71 with the _ subjective mean _",
    "@xmath72 defined in eq .",
    ", we can define a _ subjective probability _",
    "@xcite ) as @xmath73 which measures agents subjective perception of the true probability @xmath66 .    in risk - neutral cases , @xmath74 is simply the mean and @xmath75 . in risk - averse cases",
    ", the balance moves towards the worst scenario .",
    "hence , the probability of the first event ( with larger outcome @xmath65 ) is always underestimated .",
    "on the contrary , in risk - seeking cases , the probability of the first event is always overestimated .",
    "behavioral studies show that human subjects usually overestimate low probabilities and underestimate high probabilities @xcite .",
    "this can be quantified by applying mixed valuation functions @xmath17 .",
    "if we apply utility - based shortfalls , it can be quantified by using mixed utility function @xmath42 .    ; @xmath76 as defined in eq .   with @xmath77 , @xmath78 , @xmath79 and @xmath80 ; mix2 : same as mix1 but with @xmath81 , @xmath82 , @xmath83 and @xmath84 .",
    "( right ) subjective probability functions calculated according to eq .",
    ".,scaledwidth=80.0% ]    let @xmath85 , @xmath86 and the acceptance level @xmath49 .",
    "[ fig : uf ] ( left ) shows five different utility functions , one linear function `` lin '' , one convex function `` rs '' , one concave function `` ra '' , and two mixed functions `` mix1 '' and `` mix2 '' ( for details see caption ) .",
    "the corresponding subjective probabilities are shown in fig .",
    "[ fig : uf ] ( right ) . since the function",
    "`` ra '' is concave , the corresponding valuation function is risk - averse and therefore the probability of high - reward event is always underestimated . for the case of the convex function `` rs '' , the probability of high - reward event is always overestimated .",
    "however , since the `` mix1 '' function is convex on @xmath87 but concave on @xmath88 $ ] , high probabilities are underestimated while low probabilities are overestimated , which replicates very well the probability weighting function applied in prospect theory for gains ( cf .",
    "fig .  1 , @xcite ) .",
    "conversely , the `` mix2 '' function , which is concave on @xmath87 and convex on @xmath88 $ ] , corresponds to the overestimation of high probabilities and the underestimation of low probabilities .",
    "this corresponds to the weighting function used for losses in prospect theory ( cf .",
    "fig .  2 , @xcite ) .",
    "we will see in the following section that the advantage of using the utility - based shortfall is that we can derive iterating learning algorithms for the estimation of the subjective valuations , whereas it is difficult to derive such algorithms in the framework of prospect theory .",
    "a markov decision process ( see e.g.  @xcite ) @xmath89 consists of a state space @xmath90 , admissible action spaces @xmath91 at @xmath92 , a transition kernel @xmath93 , which denotes the transition probability moving from one state @xmath94 to another state @xmath95 by executing action @xmath96 , and a reward function @xmath97 with its distribution @xmath98 . in order to model random rewards , we assume that the reward function has the form , i.e. , the mean reward at each @xmath99-pair . in risk - sensitive cases ,",
    "random rewards cause also risk and uncertainties .",
    "hence , we keep the generality by using random rewards . ]",
    "@xmath100 @xmath101 denotes the noise space with distribution @xmath102 , i.e. , given @xmath99 , @xmath103 is a random variable with values drawn from @xmath104 .",
    "let @xmath105 be the _ random _ reward gained at @xmath99 , which follows the distribution @xmath104 . the random state ( resp .",
    "action ) at time @xmath106 is denoted by @xmath107 ( resp .",
    "@xmath108 ) .",
    "finally , we assume that all sets @xmath109 are finite .    a _ markov policy _ @xmath110 $ ] consists of a sequence of single - step markov policies at times @xmath111 , where @xmath112 denotes the probability of choosing action @xmath96 at state @xmath94 .",
    "let @xmath113 be the set of all markov policies .",
    "the optimal policy within a time horizon @xmath114 is obtained by maximizing the expectation of the discounted cumulative rewards , @xmath115 .",
    "\\label{eq : mdp : obj}\\end{aligned}\\ ] ] where @xmath92 denotes the initial state and @xmath116 the discount factor .",
    "expanding the sum leads to @xmath117 \\ldots   \\right ]   \\right ] .",
    "\\label{eq : t}\\end{aligned}\\ ] ] we now generalize the conditional expectation @xmath118 to represent the valuation functions considered in section [ sec : valfunc ] .",
    "let @xmath119 be the set of all admissible state - action pairs .",
    "let @xmath120 a mapping @xmath121 is called a * valuation map * , if for each @xmath122 , @xmath123 is a valuation function on @xmath124 .",
    "let @xmath125 be a short notation of @xmath126 and let @xmath127 be the valuation map averaged over all actions . since @xmath128 for each @xmath122 , we will omit @xmath12 in @xmath129 in the following . replacing the conditional expectation @xmath130 with @xmath131 in eq .  , the risk - sensitive objective becomes @xmath132 \\ldots   ] ] .\\label{eq : t : rs}\\end{aligned}\\ ] ] the optimal policy is then given by @xmath133 .",
    "for infinite - horizon problem , we obtain @xmath134 using the same line of argument .",
    "the optimization problem for finite - stage objective function @xmath135 can be solved by a generalized _ dynamic programming _",
    "@xcite , while the one defined in eq .",
    "requires the solution to the _ risk - sensitive bellman equation _ : @xmath136 the latter is a consequence of the following theorem .",
    "[ th : vi ] @xmath137 holds for all @xmath92 , whenever @xmath138 satisfies the equation .",
    "furthermore , a deterministic policy @xmath139 is optimal , if @xmath140 .",
    "define @xmath141 .",
    ".   becomes @xmath142    to carry out value iteration algorithms , the mdp @xmath143 must be known _ a priori_. in many real - life situations , however , the transition probabilities are unknown as well as the outcome of an action before its execution . therefore , an agent has to explore the environment while gradually improving its policy .",
    "we now derive rl - type algorithms for estimating q - values of general valuation maps based on the utility - based shortfall , which do not require knowledge of the reward and transition model .",
    "[ prop : impl ] let @xmath45 be a shortfall defined in eq .  , where @xmath42 is continuous and strictly increasing .",
    "then the following statements are equivalent : ( i ) @xmath144 and ( ii ) @xmath145 = x_0 $ ]",
    ".    for proof see appendix a.    consider the valuation map induced by the utility - based shortfall and acceptance levels @xmath43 at different @xmath99-pairs .",
    "however , for simplicity , we drop their dependence on @xmath99 . ]",
    "@xmath146 \\geq x_0 \\},\\ ] ] where @xmath147 is defined in eq .  .",
    "if @xmath148 exists , proposition [ prop : impl ] assures that @xmath149 is the unique solution to equation @xmath150 = x_0.\\ ] ] let @xmath151",
    ". then @xmath149 corresponds to the optimal q - value @xmath152 defined in eq .  , which is equivalent to @xmath153 let @xmath154 be the sequence of states , chosen actions , successive states and received rewards .",
    "analogous to the standard q - learning algorithm , we consider the following iterative procedure @xmath155 , \\label{eq : ql}\\end{aligned}\\ ] ] where @xmath156 denotes learning rate function that satisfies @xmath157 only if @xmath99 is updated at time @xmath106 , i.e. , @xmath158 . in other words , for all @xmath99 that are not visited at time @xmath106 , @xmath159 and their q - values are not updated .",
    "consider utility functions @xmath42 with the following properties .",
    "[ ass:2 ] ( i ) the utility function @xmath42 is strictly increasing and there exists some @xmath160 such that @xmath161 .",
    "( ii ) there exist positive constants @xmath162 such that @xmath163 , for all @xmath164",
    ".    then the following theorem holds ( for proof see appendix [ sec : proof_rl ] ) .",
    "[ th : ql ] suppose assumption [ ass:2 ] holds .",
    "consider the generalized q - learning algorithm stated in eq .  .",
    "if the nonnegative learning rates @xmath165 satisfy @xmath166 then @xmath167 converges to @xmath152 for all @xmath122 with probability 1 .    the assumption in eq .",
    "requires in fact that all possible state - action pairs must be visited infinitely often .",
    "otherwise , the first sum in eq .   would be bounded by the setting of the learning rate function @xmath165 .",
    "it means that , similar to the standard q - learning , the agent has to explore the whole state - action space for gathering sufficient information about the environment .",
    "hence , it can not take a too greedy policy in the learning procedure before the state - action space is well explored .",
    "we call a policy * proper * if under such policy every state is visited infinitely often . a typical policy , which is widely applied in rl literature as well as in models of human reward - based learning , is given by @xmath168 where @xmath169 controls how greedy the policy should be . in appendix",
    "[ sec : softmax ] , we prove that under some technical assumptions upon the transition kernel of the underlying mdp , this policy is always proper .",
    "a widely used setting satisfying both conditions in eq .",
    "is to let @xmath170 , where @xmath171 counts the number of times of visiting the state - action pair @xmath99 up to time @xmath106 and is updated trial - by - trial .",
    "this leads to the learning procedure shown in algorithm [ alg : ql ] ( see also fig .  [",
    "fig : rl ] ) .",
    "initialize @xmath172 and @xmath173 for all @xmath174 . at state @xmath175",
    "choose action @xmath176 randomly using a proper policy ( e.g.  eq .  )",
    "; observe date @xmath177 ; @xmath178 and set learning rate : @xmath179 ; update @xmath180 as in eq .  ;    the expression @xmath181 inside the utility function of eq .   corresponds to the standard temporal difference ( td ) error . comparing eq .   with the standard q - learning algorithm",
    ", we find that the nonlinear utility function is applied to the td error ( cf .",
    "[ fig : rl ] ) .",
    "this induces nonlinear transformation not only of the true rewards but also of the true transition probabilities , as has been shown in section [ sec : ubsf ] . by applying s - shape utility function ,",
    "which is partially convex and partially concave , we can therefore replicate key effects of prospect theory without the explicit introduction of a probability - weighting function .    ) .",
    "the value function @xmath182 quantifies the current subjective evaluation of each state - action pair @xmath99 .",
    "the next action is then randomly chosen according to a proper policy ( e.g.  eq .  ) which is based on the current values of @xmath180 . after interacting with the environment ,",
    "the agent obtains the reward @xmath97 and moves to the successor @xmath95 .",
    "the value function @xmath182 is then updated by the rule given in eq .  .",
    "this procedure continues until some stopping criterion is satisfied.,scaledwidth=50.0% ]    assumption [ ass:2 ] ( ii ) seems to exclude several important types of utility functions .",
    "the exponential function @xmath183 and the polynomial function @xmath184 , @xmath185 , for example , do not satisfy the global lipschitz condition required in assumption [ ass:2 ] ( ii )",
    ". this problem can be solved by a truncation when @xmath186 is very large and by an approximation when @xmath186 is very close to 0 . for more details",
    "see appendices [ sec : truncate ] and [ sec : heuristics ] .",
    "subjects were told that they are influential stock brokers , whose task is to invest into a fictive stock market ( cf .",
    "@xcite ) . at every trial ( cf .",
    "[ fig : mdp]a ) subjects had to decide how much ( @xmath187 0 , 1 , 2 , or 3 eur ) to invest into a particular stock .",
    "after the investment , subjects first saw the change of the stock price and then were informed how much money they earned or lost .",
    "the received reward was proportional to the investment .",
    "the different trials , however , were not independent from each other ( cf .  fig .",
    "[ fig : mdp]b ) . the sequential investment game consisted of 7 states , each one coming with a different set of contingencies , and subjects",
    "were transferred from one state to the next dependent of the amount of money they invested . for high investments",
    ", transitions followed the path labeled `` risk seeking '' ( rs in fig .",
    "[ fig : mdp]b ) . for low investments",
    ", transitions followed the path labeled `` risk averse '' ( ra in fig .",
    "[ fig : mdp]b ) .",
    "after 3 decisions subjects were always transferred back to the initial state , and the reward , which was accumulated during this round , was shown .",
    "state information was available to the subjects throughout every trial ( cf .",
    "[ fig : mdp]a ) .",
    "altogether , 30 subjects ( young healthy adults ) experienced 80 rounds of the 3-decision sequence .",
    "formally , the sequential investment game can be considered as an mdp with 7 states and 4 actions ( see fig .",
    "[ fig : mdp]b ) . depending on the strategy of the subjects",
    ", there are 4 possible paths , each of which is composed of 3 states .",
    "the total expected return for each path , averaged over all policies consistent with it , are shown in the right panels of fig .",
    "[ fig : mdp]b ( `` ev '' ) .",
    "path 1 provides the largest expected return per round ( ev = 90 ) , while path 4 leads to an average loss of -9.75 .",
    "hence , to follow the on - average highest rewarded path 1 , subjects have to take `` risky '' actions ( investing 2 or 3 eur at each state ) . always taking conservative actions ( investing 0 or 1 eur ) results in path 4 and a high on - average loss . on the other hand , since the standard deviation of the return @xmath188 of each state equals @xmath189 , where @xmath96 denotes the action ( investment ) the subject takes and @xmath190 denotes the price change , the higher the investment , the higher the risk .",
    "path 1 has , therefore , the highest standard deviation ( std = 14.9 ) of the total average reward , whereas the standard deviation of path 4 is smallest ( std = 6.9 ) .",
    "path 3 provides a trade - off option : it has slightly lower expected value ( ev = 52.25 ) than path 1 but comes with a lower risk ( std = 12.3 ) . hence , the paradigm is suitable for observing and quantifying the risk - sensitive behavior of subjects .",
    "b ) they chose during the last 60 trials of the game .",
    "if a path @xmath4 is chosen in more than 60% of the trials , the subject is assigned the group `` path @xmath4 '' . otherwise , subjects are assigned the group labeled `` random '' .",
    "the vertical axis denotes the cumulative reward obtained during the last 60 trials.,scaledwidth=80.0% ]      fig .",
    "[ fig : grouping ] summarizes the strategies which were chosen by the 30 subjects .",
    "17 subjects mainly chose path 1 , which provided them high rewards .",
    "6 subjects chose path 4 , which gave very low rewards .",
    "the remaining 7 subjects show no significant preference among all 4 paths and the rewards they received are on average between the rewards received by the other 2 groups .",
    "the optimal policy for maximizing expected reward is the policy that follows path 1 .",
    "the results shown in fig .  [ fig : grouping ] , however , indicate that the standard model fails to explain the behavior of more than 40% of the subjects",
    ".    we now quantify subjects behavior by applying three classes of q - learning algorithm : ( 1 ) standard q - learning , ( 2 ) the risk - sensitive q - learning ( rsql ) method described by algorithm [ alg : ql ] , and ( 3 ) an expected utility ( eu ) algorithm with the following update rule @xmath191 where the nonlinear transformation is applied to the reward @xmath192 directly .",
    "the latter one is a straightforward extension of expected utility theory .",
    "risk - sensitivity is implemented via the nonlinear transformation of the true reward @xmath192 . for both risk - sensitive q - learning methods ( rsql and eu ) , we set the we set the reference level @xmath49 and consider the family of polynomial mixed utility functions @xmath193 the parameters @xmath194 and @xmath195 quantify the risk - preferences separately for wins and losses ( see table [ tab : lpm ] )",
    ".    .parameters for the two branches @xmath196 ( left ) and @xmath197 ( right ) of the polynomial utility function @xmath198 ( eq .  ) , its shape and the induced risk preference . [ cols=\"^,^,^,^\",options=\"header \" , ]     hence , there are 4 parameters for @xmath42 which have to be determined from the data . for all three classes ,",
    "actions are generated according to the `` softmax '' policy eq .",
    ", which is a proper policy for the paradigm ( for proof see appendix [ sec : softmax ] ) , and the learning rate @xmath35 is set constant across trials .    for rsql ,",
    "the learning rate is absorbed by the coefficients @xmath199 .",
    "hence , there are 6 parameters @xmath200 which have to be determined .",
    "standard q - learning corresponds to the choice @xmath201 and @xmath202 .",
    "the risk - sensitive model applied by @xcite is also a special case of the rsql - framework and corresponds @xmath201 . for the eu algorithm , there are 7 parameters , @xmath203 , which have to be fitted to the data .",
    "@xmath201 and @xmath204 again corresponds to the standard q - learning method .",
    "parameters were determined subject - wise by maximizing the log - likelihood of the subjects action sequences , @xmath205 where @xmath206 indicates the dependence of the q - values on the model parameters @xmath207 .",
    "since rsql / eu and the standard q - learning are nested model classes , we apply the bayesian information criterion ( bic , see e.g.  @xcite ) @xmath208 for model selection .",
    "@xmath209 denotes the log - likelihood , eq .  .",
    "@xmath210 and @xmath211 are the number of parameters and trials respectively .",
    "to compare results , we report relative bic scores , @xmath212 , where @xmath213 is the bic score of the candidate model and @xmath214 is the bic score of the standard q - learning model .",
    "we obtain @xmath215 the more negative the relative bic score is , the better the model fits data .",
    "hence , the rsql algorithm provides a significantly better explanation for the behavioral data than the eu algorithm and standard q - learning . in the following ,",
    "we only discuss the results obtained with the rsql model .",
    "( left ) and @xmath64 ( right ) for the rsql model.,scaledwidth=80.0% ]    fig .",
    "[ fig : para ] shows the distribution of best - fitting values for the two parameters @xmath216 which quantify the risk - preferences of the individual subjects .",
    "we conclude ( cf .",
    "table [ tab : lpm ] ) that most of the subjects are risk - averse for positive and risk - seeking for negative td errors .",
    "the result is consistent with previous studies from the economics literature ( see @xcite , and references therein ) .",
    "after determining the parameters @xmath217 for the utility functions , we perform an analysis similar to the analysis discussed in section [ sec : toyex ] . given an observed reward sequence @xmath218 , the empirical subjective mean @xmath219 is obtained by solving the following equation @xmath220 if subjects are risk - neutral , then @xmath48 , and @xmath221 is simply the empirical mean .",
    "following the idea of prospect theory , we define a normalized subjective probability @xmath222 , @xmath223 if @xmath222 is positive , the probability of rewards is overestimated and the induced policy is , therefore , risk - seeking .",
    "if @xmath222 is negative , the probability of rewards is underestimated and the policy is risk - averse .",
    "[ fig : subprob ] summarizes the distribution of normalized subjective probabilities for subjects from the `` path 1 '' , `` path 4 '' and `` random '' groups of fig .",
    "[ fig : grouping ] . for subjects within group `` path 1 '' ,",
    "@xmath224 is small and their behaviors are similar to those of risk - neutral agents .",
    "this is consistent with their policy , because both risk - seeking and risk - neutral agents should prefer path 1 . for subjects within groups `` path 4 '' and `` random '' , the normalized subjective probabilities are on average 10 % lower than those of risk - neutral agents .",
    "this explains why subjects in these groups adopt the conservative policies and only infrequently choose path 1 .    , eq .",
    ", for the different subject groups defined in fig .",
    "[ fig : grouping ] . , scaledwidth=80.0% ]      functional magnetic resonance imaging ( fmri ) data were simultaneously recorded while subjects played the sequential investment game .",
    "the analysis of fmri data was conducted in spm8 ( wellcome department of cognitive neurology , london , uk ; details of the magnetic resonance protocol and data processing are presented in appendix [ sec : mr ] ) .",
    "the sequence of q - values for the action chosen at each state were used as parametric modulators during the choice phase , and temporal difference ( td ) errors were used at the outcome phase ( see fig .  [",
    "fig : mdp]a ) .",
    "[ fig : fmritd ]   [ fig : fmriq ]    fig .",
    "[ fig : fmri]a shows that the sequence of td errors for the rsql model ( with best fitting parameters ) positively modulated the bold signal in the subcallosal gyrus extending into the ventral striatum ( -14 8 -16 ) ( marked by the cross in fig .",
    "[ fig : fmri]a ) , the anterior cingulate cortex ( 8 48 6 ) , and the visual cortex (-8 -92 16 ; @xmath225 ) .",
    "the modulation of the bold signal in the ventral striatum is consistent with previous experimental findings ( cf .",
    "@xcite ) , and supports the primary assertion of computational models that reward - based learning occurs when expectations ( here , expectations of `` subjective '' quantities ) are violated @xcite .",
    "[ fig : fmri]b shows the results for the sequence of q - values for the rsql model ( with best fitting parameters ) , which correspond to the subjective ( risk - sensitive ) expected value of the reward for each discrete choice .",
    "several large clusters of voxels in cortical and subcortical structures were significantly modulated by the q - values at the moment of choice .",
    "the sign of this modulation was negative .",
    "the peak of this negative modulation occurred in the left anterior insula ( -36 12 -2 , @xmath226 ) , with strong modulation also in the bilateral ventral striatum ( 14 8 -4 , marked by the cross in fig .",
    "[ fig : fmri]b ; -16 4 0 ) and the cingulate cortex ( 4 16 28 ) .",
    "the reward prediction error processed by the ventral striatum ( and other regions noted above ) would not be computable in the absence of an expectation , and as such , this activation is important for substantiating the plausibility for the rsql model of learning and choice .",
    "sequences of q - values obtained with standard q - learning ( with best fitting parameters ) , on the other hand , failed to predict any changes in brain activity even at a liberal statistical threshold of @xmath227 ( uncorrected ) .",
    "this lack of neural activity for the standard q model , in combination with the significant activation for our rsql , supports the hypothesis that some assessment of risk is induced and influences valuation . whereas the areas modulated by q - values differ from what has been reported in other studies ( i.e. , the ventromedial prefrontal cortex as in @xcite ) , it does overlap with the representation of td errors .",
    "furthermore , the opposing signs of the correlated neural activity suggests that a neural mismatch of signals in the ventral striatum between q - value and td errors may underlie the mechanism by which values are learned .",
    "we applied the risk - sensitive q - learning ( rsql ) method to quantify human behavior in a sequential investment game and investigated the correlation of the predicted td- and q - values with the neural signals measured by fmri .",
    "we first showed that the standard q - learning algorithm can not explain the behavior of a large number of subjects in the task . applying",
    "rsql generated a significantly better fit and also outperformed the expected utility algorithm .",
    "the risk - sensitivity revealed by the best fitting parameters is consistent with the studies in behavioral economics , that is , subjects are risk - averse for positive while risk - seeking for negative td errors . finally , the relative subjective probabilities provide a good explanation why some subjects take conservative policies : they underestimate the true probabilities of gaining rewards .",
    "the fmri results showed that td sequence generated by our model has a significant correlation with the activity in the subcallosal gyrus extending into the ventral striatum .",
    "the sequence of q - values has a significant correlation with the activities in the left anterior insula .",
    "previous studies ( see e.g. , chapter 23 of @xcite and @xcite ) suggest that higher order statistics of outcomes , e.g. , variance ( the 2nd order ) and skewness ( the 3rd order ) , are encoded in human brains separately and the individual integration of these risk metrics induces the corresponding risk - sensitivity .",
    "our results indicate , however , that the risk - sensitivity can be simply induced ( and therefore encoded ) by a nonlinear transformation of td errors and no additional neural representation of higher order statistics is needed .",
    "we applied a family of valuation functions , the utility - based shortfall , to the general framework of risk - sensitive markov decision processes , and we derived a risk - sensitive q - learning algorithm .",
    "we proved that the proposed algorithm converges to the optimal policy corresponding to the risk - sensitive objective . by applying s - shape utility functions",
    ", we show that key features predicted by prospect theory can be replicated using the proposed algorithm .",
    "hence , the novel q - learning algorithm provides a good candidate model for human risk - sensitive sequential decision - making procedures in learning tasks , where mixed risk - preferences are shown in behavioral studies .",
    "we applied the algorithm to model human behaviors in a sequential investment game .",
    "the results showed that the new algorithm fitted data significantly better than the standard q - learning and the expected utility model .",
    "the analysis of fmri data shows a significant correlation of the risk - sensitive td error with the bold signal change in the ventral striatum , and also a significant correlation of the risk - sensitive q - values with neural activity in the striatum , cingulate cortex and insula , which is not present if standard q - values are applied .",
    "some technical extensions are possible within our general risk - sensitive reinforcement learning ( rl ) framework : ( a ) the q - learning algorithm derived in this paper can be regarded a special type of rl algorithms , td(0 ) .",
    "it can be extended to other types of rl algorithms like sarsa and td(@xmath54 ) for @xmath228 .",
    "( b ) in our previous work @xcite , we also provided a framework for the average case .",
    "hence , rl algorithms for the average case can also be derived similar to the discounted case considered in this paper .",
    "( c ) the algorithm in its current form applies to mdps with finite state spaces only .",
    "it can be extended for mdps with continuous state spaces by applying function approximation technique .",
    "thanks to wendelin bhmer , rong guo and maziar hashemi - nezhad for useful discussions and suggestions and to the anonymous referee for helpful comments .",
    "the work of y.  shen and k.  obermayer was supported by the bmbf ( bersteinfokus lernen tp1 ) , 01gq0911 , and the work of m.j .",
    "tobia and t.  sommer was supported by the bmbf ( bernsteinfokus lernen tp2 ) , 01gq0912 .",
    "the sup - norm is defined as @xmath229 where @xmath230_{i \\in i}$ ] can be considered as a @xmath2-dimensional vector .",
    "[ lm : inrm ] let @xmath17 be valuation function on @xmath124 and @xmath231",
    ". then the following inequality holds @xmath232    by @xmath233 and monotonicity of valuation functions , we obtain @xmath234 due to the translation invariance , we have then @xmath235 which immediately imply that @xmath236    \\(ii ) @xmath237 ( i ) . by definition , @xmath238 . for any @xmath239 ,",
    "since @xmath42 is strictly increasing , we have @xmath240 , which implies @xmath241 . hence , @xmath242 .",
    "\\(i ) @xmath237 ( ii ) .",
    "by definition we have @xmath243 .",
    "assume that @xmath244 .",
    "by the continuity of @xmath42 , there exists an @xmath239 such that @xmath245 , which implies @xmath246 and hence contradicts ( i ) .",
    "thus , ( ii ) holds .      before proving the risk - sensitive q - learning",
    ", we consider a more general update rule @xmath247 .",
    "\\label{eq : qiter}\\end{aligned}\\ ] ] where @xmath248 , @xmath249 is an operator , @xmath250 denotes some random noise term and @xmath251 is learning rate with the understanding that @xmath252 if @xmath253 is not updated at time @xmath106 .",
    "denote by @xmath254 the history of the algorithm up to time @xmath106 , @xmath255 we restate the following proposition .",
    "[ prop : sapp ] let @xmath256 be the sequence generated by the iteration .",
    "we assume the following    * the learning rates @xmath257 are nonnegative and satisfy @xmath258 * the noise terms @xmath259 satisfy ( i ) for every @xmath4 and @xmath106 , @xmath260 = 0 $ ] ; ( ii ) given some norm @xmath261 on @xmath262 , there exist constants @xmath263 and @xmath213 such that @xmath264 \\leq a + b \\rvert q_t \\rvert^2 $ ] . *",
    "the mapping @xmath265 is a contraction under sup - norm .",
    "then @xmath256 converges to the unique solution @xmath266 of the equation @xmath267 with probability 1 .    to apply proposition [ prop : sapp ] , we first reformulate the q - learning rule in a different form @xmath268\\end{aligned}\\ ] ] where @xmath35 denotes an arbitrary constant such that @xmath269 $ ] . recall that @xmath209 is defined in assumption [ ass:2 ] . for simplicity , we define @xmath270 , @xmath271 and set @xmath272 more explicitly , @xmath273 is defined as @xmath274 where @xmath275 .",
    "we assume the size of the space @xmath276 is @xmath277 .",
    "[ lm : cont ] suppose that assumption [ ass:2 ] holds and @xmath278 .",
    "then there exists a real number @xmath279 such that for all @xmath280 , @xmath281 .",
    "define @xmath282 and @xmath283 .",
    "thus , @xmath284 by assumption [ ass:2 ] ( ii ) and the monotonicity of @xmath285 , there exists a @xmath286 $ ] such that @xmath287 .",
    "analogously , we obtain @xmath288   \\\\   &   + ( q(s , a ) - q'(s , a ) ) \\}\\\\   = & \\alpha \\gamma \\sum_{s ' , \\varepsilon } \\tilde{\\mathcal p}(s',\\epsilon|s , a ) \\xi_{(s , a,\\varepsilon , s',q , q ' ) } [ v(s ' ) - v'(s ' ) ] \\\\   & + ( 1- \\alpha \\sum_{s ' , \\varepsilon } \\tilde{\\mathcal p}(s',\\epsilon|s , a ) \\xi_{(s , a,\\varepsilon , s',q , q ' ) } ) [ q(s , a)-q'(s , a ) ] \\\\",
    "\\leq & \\left(1 - \\alpha(1 - \\gamma ) \\sum_{s ' , \\varepsilon } \\tilde{\\mathcal p}(s',\\epsilon|s , a ) \\xi_{(s , a,\\varepsilon , s',q , q ' ) } \\right ) \\lvert q - q'\\rvert_\\infty \\\\",
    "\\leq & \\left ( 1 - \\alpha(1 - \\gamma ) \\epsilon \\right ) \\lvert q - q'\\rvert_\\infty\\end{aligned}\\ ] ] hence , @xmath289 is the required constant .",
    "obviously , condition ( a ) in proposition [ prop : sapp ] is satisfied and condition ( c ) holds also due to lemma [ lm : cont ] .",
    "it remains to check condition ( b ) .",
    "@xmath290 = 0 $ ] holds by its definition in .",
    "next we prove ( ii ) .",
    "in fact , @xmath291 = \\alpha^2 \\mathbb e \\left[(\\tilde u(d_t))^2 | \\mathcal f_t\\right ] - \\alpha^2 ( \\mathbb e \\left[\\tilde u(d_t ) |",
    "\\mathcal f_t\\right])^2 \\leq \\alpha^2 \\mathbb e \\left[(\\tilde u(d_t))^2 | \\mathcal f_t\\right]\\ ] ] let @xmath292 be the upper bound for @xmath192 .",
    "then @xmath293 , which implies that @xmath294 due to assumption [ ass:2](ii ) .",
    "hence , @xmath295 . on the other hand , since @xmath296 we have @xmath297 \\leq 2 \\alpha^2 ( \\lvert \\tilde u(0 ) \\rvert + l \\bar r)^2 + 8 \\alpha^2 l^2 \\lvert q_t \\rvert_\\infty^2 $ ] .",
    "hence , condition ( b ) holds .      some functions like @xmath183 and @xmath184 , @xmath298 , do not satisfy the global lipschitz condition required in assumption [ ass:2 ] ( ii ) . in real applications",
    ", however , we can relax the assumption to assume that the lipschitz condition holds locally within a `` sufficiently large '' subset .",
    "lemma [ lm : bound ] states such subset provided the upper bound of absolute value of rewards is known .",
    "[ ass:1 ] the reward function @xmath299 is bounded under sup - norm , i.e. , @xmath300    [ sec : truncate ] define an operator @xmath301 as @xmath302    @xmath303 is a contracting map under sup - norm , i.e. , @xmath304 [ lm : con ]    [ lm : bound ] under assumption [ ass:2 ] ( i ) and [ ass:1 ] , applying the valuation map in , the solution @xmath305 satisfies @xmath306    by assumption , @xmath307 exists .",
    "since @xmath42 is strictly increasing , we have @xmath308 . hence , together with eq .",
    ", we obtain for all @xmath122 , @xmath309 note that lemma [ lm : con ] implies that @xmath310 for any @xmath311 . without loss of generality ,",
    "we start from @xmath312 . define @xmath313 and @xmath314 .",
    "hence , we have @xmath315 , which implies @xmath316 repeating above procedure , we obtain @xmath317 . hence , @xmath318 . by the definition of @xmath305 , above",
    "inequalities hold for @xmath305 as well .    define @xmath319 given lemma [ lm : bound ]",
    ", we can truncate the utility function @xmath42 outside the interval @xmath320 $ ] as @xmath321\\\\                   u(\\bar x ) + \\epsilon(x - \\bar x ) , & x \\in ( \\bar x , \\infty )                  \\end{array } \\right .. \\label{eq : truncate}\\end{aligned}\\ ] ]    [ th : truncate ] suppose that assumption [ ass:2 ] ( i ) and [ ass:1 ] hold . assume further that there exist positive constants @xmath322 such that @xmath163 , for all @xmath323 $ ] , where @xmath324 are defined in eq .  .",
    "then the unique solution @xmath325 to eq .   with @xmath42 and the unique solution @xmath326 to eq .",
    "with @xmath327 are identical .",
    "both uniqueness is due to theorem [ th : vi ] and proposition [ prop : impl ] . by lemma [ lm : bound ] , @xmath328 hold for all @xmath122 and @xmath329 .",
    "hence , we have for both @xmath329 and for all @xmath330 , @xmath331 since @xmath42 and @xmath327 are identical within the set @xmath332 $ ] , @xmath333 for all @xmath122 .",
    "now we state the risk - sensitive q - learing algorithm with truncation .",
    "initialize @xmath172 and @xmath173 for all @xmath174 . at state @xmath175",
    "choose action @xmath176 randomly using a proper policy ( e.g.  eq .  ) ; observe date @xmath177 ; @xmath178 and set learning rate : @xmath179 ; update @xmath180 as in eq .  ; truncate @xmath180 as in eq .",
    ", where @xmath334 and @xmath335 are defined in eq .  .",
    "so far we have relaxed the assumption for utility functions to locally lipschitz",
    ". however , some functions of interest are even not locally lipschitz . for instance , the function @xmath184 , @xmath336 is not lipschitz at the area close to 0 .",
    "we suggest two types of approximation to avoid this problem .    1",
    ".   approximate @xmath42 by @xmath337 with some positive @xmath338 .",
    "approximate @xmath42 close to 0 by a linear function , i.e. @xmath339    in both cases , @xmath338 should be set very close to 0 .",
    "the assumption in theorem and assumption [ ass:2 ] ( ii ) requires also the strictly positive lower bound @xmath340 .",
    "this causes problem when applying @xmath184 , @xmath341 at the area close to 0 .",
    "we can again apply above two approximation schemes to overcome the problem by selecting small @xmath338 . in section",
    "[ sec : ex ] , for both @xmath341 and @xmath336 , we apply the second scheme to ensure assumption [ ass:2 ] .",
    "recall that we call a policy is proper , if under such policy every state is visited infinitely often . in this subsection , we show that under some technical assumptions the softmax policy ( cf .",
    "is proper .",
    "a policy @xmath342 $ ] is deterministic if for all state @xmath94 and @xmath106 , there exists an action @xmath343 such that @xmath344 . under one policy @xmath345 ,",
    "the @xmath211-step transition probability @xmath346 for some @xmath347 can be calculated as follows @xmath348 where @xmath349 and @xmath350 is the transition kernel of the underlying mdp .",
    "assume that the state and action space are finite and the assumptions required by theorem [ th : ql ] hold .",
    "assume further that for each @xmath351 , there exist a deterministic policy @xmath352 , @xmath353 and a positive @xmath354 such that @xmath355 .",
    "then the softmax policy stated in eq .",
    "is proper .    due to the contraction property of @xmath180 ( see lemma [ lm : cont ] ) , @xmath356 is uniformly bounded w.r.t .",
    "let @xmath357 $ ] be a softmax policy associated with @xmath356 .",
    "then , by the definition of softmax policies ( see eq .  ) , there exists a positive @xmath358 such that @xmath359 holds for each @xmath122 and @xmath360 .",
    "it implies that for each @xmath361 , @xmath362 for any deterministic policy @xmath352 . then by the assumption of this proposition",
    ", we obtain that for each @xmath361 , @xmath363 .",
    "it implies that each state will be visited infinitely often .",
    "the mdp applied in the behavioral experiment in section [ sec : ex ] satisfies above assumptions , since for each @xmath351 , there exists a deterministic policy @xmath352 such that @xmath364 , @xmath365 , no matter which initial state @xmath94 we start with .",
    "magnetic resonance ( mr ) images were acquired with a 3 t whole - body mr system ( magnetom tim trio , siemens healthcare ) using a 32-channel receive - only head coil .",
    "structural mri were acquired with a t1 weighted magnetization - prepared rapid gradient - echo ( mprage ) sequence with a voxel resolution of @xmath366 , coronal orientation , phase - encoding in left - right direction , fov = @xmath367 mm , 240 slices , 1100 ms inversion time , te = 2.98 ms , tr = 2300 ms , and 90 flip angle .",
    "functional mri time series were recorded using a t2 * grappa epi sequence with tr = 2380 ms , te = 25 ms , anterior - posterior phase encode , 40 slices acquired in descending ( non- interleaved ) axial plane with @xmath368 voxels ( @xmath369 mm fov ; skip factor = .5 ) , with an acquisition time of approximately 8 minutes per scanning run .",
    "structural and functional magnetic resonance image analyzes were conducted in spm8 ( wellcome department of cognitive neurology , london , uk ) .",
    "anatomical images were segmented and transformed to montreal neurological institute ( mni ) standard space , and a group average t1 custom anatomical template image was generated using dartel .",
    "functional images were corrected for slice - timing acquisition offsets , realigned and corrected for the interaction of motion and distortion using unwarp toolbox , co - registered to anatomical images and transformed to mni space using dartel , and finally smoothed with an 8 mm fwhm isotropic gaussian kernel .",
    "functional images were analyzed using the general linear model ( glm ) implemented in spm8 .",
    "first level analyzes included onset regressors for each stimulus event excluding the anticipation phase ( see fig .  [",
    "fig : mdp]a ) , and a set of parametric modulators corresponding to trial - specific task outcome variables and computational model parameters . trial - specific task outcome variables ( and their corresponding stimulus event ) include the choice value of the investment ( choice phase ) and the total value of rewards ( gains / losses ) over each round ( corresponding to multi - trial feedback event ) .",
    "model derived parametric modulators included the time series of q values for the selected action ( choice phase ) , td ( outcome phase ) .",
    "reward value was not modeled as a parametric modulator because the td error time series and trial - by - trial reward values were strongly correlated ( all rs @xmath370 ; ps @xmath371 ) .",
    "the configuration of the first - level glm regressors for the standard q - learning model was identical to that employed in the risk - sensitive q - learning model .",
    "all regressors were convolved with a canonical hemodynamic response function . prior to model estimation",
    ", coincident parametric modulators were serially orthogonalized as implemented in spm ( i.e. , the q - value regressor was orthogonalized with respect to the choice value regressor ) .",
    "in addition , we included a set of regressors for each participant to censor epi images with large , head movement related spikes in the global mean .",
    "these first level beta values were averaged across participants and tested against zero with a t - test .",
    "monte carlo simulations determined that a cluster of more than 125 contiguous voxels with a single - voxel threshold of @xmath372 achieved a corrected @xmath66-value of @xmath373 .",
    "j.  glscher , a.n .",
    "hampton , and j.p .",
    "determining a role for ventromedial prefrontal cortex in encoding action - based value signals during reward - related decision making .",
    "_ cerebral cortex _ , 190 ( 2):0 483495 , 2009 .",
    "y.  niv , j.a .",
    "edlund , p.  dayan , and j.p .",
    "neural prediction errors reveal a risk - sensitive reinforcement - learning process in the human brain . _",
    "the journal of neuroscience _ , 320 ( 2):0 551562 , 2012 .",
    "tobia , r.  guo , u.  schwarze , w.  bhmer , j.  glscher , b.  finckh , a.  marschner , c.  bchel , k.  obermayer , and t.  sommer .",
    "neural systems for choice and valuation with counterfactual learning signals .",
    "_ to appear in neuroimage _ , 2013"
  ],
  "abstract_text": [
    "<S> we derive a family of risk - sensitive reinforcement learning methods for agents , who face sequential decision - making tasks in uncertain environments . by applying a utility function to the temporal difference ( td ) error , </S>",
    "<S> nonlinear transformations are effectively applied not only to the received rewards but also to the true transition probabilities of the underlying markov decision process . </S>",
    "<S> when appropriate utility functions are chosen , the agents behaviors express key features of human behavior as predicted by prospect theory @xcite , for example different risk - preferences for gains and losses as well as the shape of subjective probability curves . </S>",
    "<S> we derive a risk - sensitive q - learning algorithm , which is necessary for modeling human behavior when transition probabilities are unknown , and prove its convergence . as a proof of principle for the applicability of the new framework </S>",
    "<S> we apply it to quantify human behavior in a sequential investment task . </S>",
    "<S> we find , that the risk - sensitive variant provides a significantly better fit to the behavioral data and that it leads to an interpretation of the subject s responses which is indeed consistent with prospect theory . </S>",
    "<S> the analysis of simultaneously measured fmri signals show a significant correlation of the risk - sensitive td error with bold signal change in the ventral striatum . </S>",
    "<S> in addition we find a significant correlation of the risk - sensitive q - values with neural activity in the striatum , cingulate cortex and insula , which is not present if standard q - values are used . </S>"
  ]
}