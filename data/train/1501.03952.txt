{
  "article_text": [
    "while evaluating unseen test instances on a classifier trained over a set of labelled training instances , there is a standard assumption that test instances and training instances follow the same distribution .",
    "however , many real world scenarios violate this assumption .",
    "think of a case where someone wants to classify the images taken with his low quality phone camera for which he does nt have labels available .",
    "can the person classify those images using the classifier which was trained on some publicly available dataset like imagenet or flickr ?",
    "the obvious answer is no .",
    "many studies have shown that if the test instances are not sampled from the same distribution as the training instances then the performance of the classifier significantly diminishes @xcite .",
    "this problem of domain shift is also extensively studied in the field of natural language processing and speech processing @xcite . to address this challenge ,",
    "methods have been suggested to adapt a domain _ ( source domain ) _ with respect to the other domain _ ( target domain ) _ so that a classifier trained on _ source domain _ data also contains the property of _ target domain _ data .",
    "one can distinguish two settings in the domain adaptation literature : ( 1 ) _ the unsupervised setting _ when the target domain is completely unlabeled and ( 2 ) _ the semi - supervised setting _ when the target domain is partially labeled .",
    "in both settings , the source domain is fully labelled . in this work",
    "we focus on the unsupervised setting that is more challenging one",
    ". a promising line of work to solve this problem is by subspace based domain adaptation @xcite .",
    "however , none of the above approaches takes the semantic ( dis)similarity of the category classes into account .",
    "classes which are semantically similar have a very different distribution than classes which are semantically different . based on this observation",
    ", we advocate that it s better to align the subspaces separately rather than considering the whole target data distribution at once .",
    "+ to address this challenge we propose a new method of _ step - wise subspace alignment _ for domain adaptation .",
    "step - wise subspace alignment here indicates that we first align the subspaces for a set of larger clusters or group of semantically similar categories and then for the categories within the clusters .",
    "+ we evaluate the effectiveness of proposed approach on a standard dataset having classes arranged according to their semantics in the hierarchy",
    ". however , the proposed approach can also be effective for the case when the hierarchy is not available .",
    "in such scenario similar categories can be clustered together in unsupervised way .",
    "as mentioned in section [ intro ] domain adaptation is widely studied in many fields including natural language processing , speech processing and computer vision @xcite . a survey on recent advances in domain adaptation in natural language processing and computer vision",
    "can be found in @xcite .",
    "subspace based approaches are most popular for solving the visual domain shift problem @xcite .",
    "the same principal lies behind these approaches .",
    "they first determine separate subspaces for source and target data and then project the data onto these subspaces and/or a set of intermediate sampled subspaces with the aim of making the feature point domain invariant . in @xcite , a method is proposed to sample subspaces along the geodesic between source and target subspace on the grassmann manifold . once sampling is done then features are projected onto those sampled subspaces and a classifier is trained on the projected features . in @xcite ,",
    "the geodesic flow kernel is proposed to capture the incremental details in subspaces between source and target subspace along the geodesic . instead of using intermediate subspaces",
    ", @xcite proposes to learn a transformation to directly align the source subspace to the target subspace .",
    "only few works have looked at the use of hierarchies in the context of domain adaptation . in @xcite ,",
    "nguyen _ et al . _ propose to adapt a hierarchy of features to exploit the richness of visual data .",
    "the intent behind this work is similar to our work , in that semantic closeness and context information are exploited to boost domain adaptation performance .",
    "taking this idea forward a recent work on hierarchical adaptive structural svm for domain adaptation has been proposed in @xcite .",
    "they organize multiple target domains into a hierarchical structure ( tree ) and adapt the source model to them jointly .",
    "others have used statistical methods for hierarchical domain adaptation , e.g. in @xcite a hierarchical bayesian prior is used to solve the domain shift problem in natural language and speech processing .",
    "however , the previous works have assumed a single common subspace between source and target , while our approach makes use of the hierarchical structure among the different classes to learn separate subspaces .",
    "the proposed approach builds up on the previously proposed subspace based methods @xcite .",
    "one could learn the domain shift between source and target data on the original features itself .",
    "however this would be sub - optimal and involve significantly modifying the classifiers .",
    "therefore , it is more common to learn it on a more robust representation of the data by first selecting @xmath0 dominating eigenvectors obtained using _ principal _ _ component _ _ analysis_. these @xmath0 eigenvectors work as the basis vectors for the _ source _ and _ target _ subspaces .",
    "the source and target features are then projected on the subspaces .",
    "two recently proposed state - of - the - art subspace based domain adaptation methods @xcite used in this paper are discussed in [ subspace_alignment ] and [ gfk ]      subspace alignment based domain adaptation method consists of learning a transformation matrix @xmath1 that maps the source subspace to the target one @xcite .",
    "the mathematical formulation to this problem is given by @xmath2 @xmath3 and @xmath4 are matrices containing the @xmath0 most important eigenvectors for source and target respectively .",
    "@xmath1 is a transformation matrix from the source subspace @xmath5 to target subspace @xmath6 and @xmath7 is the _",
    "frobenius norm_. the solution of _ eq .",
    "_ [ equ1 ] is @xmath8 and hence for the target aligned source coordinate system we get @xmath9",
    ".      the geodesic flow kernel based domain adaptation method constructs an infinite - dimensional feature space that carries the information of incremental change from source to target domain data @xcite .",
    "a key step in this method is to determine the geodesic curve between the two subspaces and to construct the geodesic flow kernel . if @xmath5 and @xmath6 are source and target subspaces having the same dimension then these two subspaces are separate points on a grassmann manifold which is also a _",
    "riemannian manifold_. let @xmath10 be the orthogonal complement to @xmath5 . from the property of _ riemannian manifold _",
    ", flow from @xmath5 towards @xmath6 can be calculated as : @xmath11 based on the decomposition of the source subspace @xmath5 and its orthogonal complement @xmath10 , we can obtain a geodesic flow kernel matrix @xmath12 that is given by + @xmath13 = \\left [ \\begin{array}{cc } p_su_1 & r_su_2 \\end{array } \\right ]   \\begin{bmatrix } \\lambda_1 & \\lambda_2 \\\\ \\lambda_2 & \\lambda_3 \\end{bmatrix }    \\left [ \\begin{array}{c } u_1^tp_s^t \\\\",
    "u_2^tr_s^t \\end{array } \\right]$ ] where @xmath14 are diagonal matrices that depend on the principal angles .",
    "once we obtain the geodesic flow matrix @xmath12 , we can relate labeled samples @xmath15 from the source subspace @xmath16 and unlabeled samples @xmath17 from the target subspace @xmath18 by using the distance metric @xmath19 x_j$ ] .",
    "in this section we describe how the methods explained in section [ background ] are adapted for hierarchical domain adaptation . instead of using the same subspace throughout , we postulate that better results can be obtained by using different subspaces for different levels of the hierarchy .",
    "indeed , the more specific subspaces spanned by instances of categories of a certain branch of our tree ( corresponding to similar categories ) , can be expected to better fit the data and therefore better model the domain shift . for the source domain",
    ", these subspaces can easily be obtained . for the target domain , however , no class labels are available as we are working in the unsupervised setting .",
    "therefore , the exact subspaces can not be computed .",
    "we circumvent this problem by first predicting the parent class label for each instance , using the global subspaces and applying domain adaptation at the level of the root node .",
    "we then use these predicted parent class labels to compute the next level of subspaces .",
    "this results in a two step approach , as summarized in the algorithm below .",
    "@xmath20 and @xmath21 @xmath22 or @xmath23 @xmath24(target data t ) @xmath25 @xmath26 @xmath27s are labeled data points ( from @xmath28 parent ) @xmath29 @xmath30s are data points classified as @xmath28 parent by the root classifier @xmath31 or @xmath32 @xmath33(target data @xmath30 ) [ euclidendwhile ] * return * @xmath34    in hierarchical subspace alignment we learn different _ metric _ @xmath1 at different levels of hierarchy independently . without loss of generality , we consider here hierarchies with only two levels , i.e. composed of a root node , a set of parent nodes ( each corresponding to a set of similar categories ) and a set of child nodes or leaf nodes ( corresponding to the different categories ) .",
    "hence the mathematical formulation of our approach is governed by _ eq . _ [ equ2 ] and [ equ3 ] .",
    "@xmath35 @xmath36 here @xmath37 is the transformation matrix learned at the topmost level of hierarchy to differentiate between the parents .",
    "each parent category consists of several similar child categories . @xmath38 and @xmath39 are the source and target subspaces considering all the source and target data .",
    "@xmath40 is the transformation matrix learned at the second level of hierarchy to distinguish between the children of parent _",
    "i_. @xmath41 and @xmath42 are source and target subspaces for categories that belong to parent _",
    "i_. hence @xmath41 and @xmath42 are obtained using only the data points that belong to a child category of parent _",
    "i_. solutions of the _ eq . _ [ equ2 ] and [ equ3 ] are similar to _ eq . _",
    "[ equ1 ] .    in hierarchical geodesic flow kernel",
    "we compute different kernel matrices at different levels of hierarchy for categorization at a specific level . for classifying between the parent classes , kernel matrix @xmath43",
    "is computed considering the source and target subspaces generated by all the data . for classifying between the children of a specific parent category",
    "@xmath44 we compute kernel matrix @xmath45 considering the subspace obtained from the children classes of parent @xmath44 .",
    "similarity between two data points depends on the hierarchy level at which prediction is performed .",
    "0.15    0.15       0.15    0.15       0.15    0.15    in this section we evaluate our results on a part of hierarchy taken from the caltech-256 and bing - caltech @xcite .",
    "we show our experimental results on the animal hierarchy consisting of the following three parent nodes : aquatic , terrestrial and avian animals and each parent consists of several child categories . for each image in the dataset we compute @xmath46 dimensional convolutional neural network based features obtained using decaf @xcite by first resizing the full image to the desired input size .",
    "note that the dataset has not been augmented with any virtual examples by flipping or random cropping . in this paper",
    "we have used k - nn as our classifier as this has also been similarly used in @xcite .",
    "the rank of the domain is decided using the procedure given in @xcite and based on this procedure we fix the dimensionality of the subspaces for both root and parent subspaces as @xmath47 .",
    "we first show the result without applying any domain adaptation algorithm on the source and target data to show that there exists a non - negligible domain shift between these two datasets .",
    "this is shown in table [ tab2 ] in column `` base accuracy '' .",
    "the results in table [ tab2 ] show that the hierarchy based subspace alignment consistently improves the results .",
    "we also evaluate the similarity between subspaces of source and target domain by taking dot product ( @xmath48 ) at various levels of hierarchy to analyse our approach .",
    "this result is provided in table [ tab1 ] . as can be seen from the table [ tab1 ] ,",
    "the maximum similarity is observed to be between the relevant subspaces in source and target .",
    "the low values we obtain off the main diagonal indicate the subspaces for the different parent nodes are quite different from one another and different from the root subspace .",
    "+    .result for hierarchical domain adaptation on animal hierarchy of caltech-256 and bing - caltech .",
    "here gfk represents geodesic flow kernel and sa represents subspace alignment [ cols= \" < ,",
    "< , < , < , < , < \" , ]",
    "in this paper , we have considered a hierarchical subspace based domain adaptation approach . based on the evaluation",
    "we observe that considering different domain adaptation subspaces specific to the individual category level can indeed aid the domain adaptation . in future , we would like to evaluate the effect of restricting the subspaces to groups of classes which need not be obtained strictly based on hierarchy which would generalize the approach to any source and target domains that are not hierarchically labeled .",
    "fernando , basura , amaury habrard , marc sebban , and tinne tuytelaars . `` unsupervised visual domain adaptation using subspace alignment . '' in computer vision ( iccv ) , 2013 ieee international conference on , pp .",
    "2960 - 2967 .",
    "ieee , 2013 gong , boqing , yuan shi , fei sha , and kristen grauman .",
    "`` geodesic flow kernel for unsupervised domain adaptation . '' in computer vision and pattern recognition ( cvpr ) , 2012 ieee conference on , pp .",
    "2066 - 2073 .",
    "ieee , 2012 .",
    "gopalan , raghuraman , ruonan li , and rama chellappa .",
    "`` domain adaptation for object recognition : an unsupervised approach . '' in computer vision ( iccv ) , 2011 ieee international conference on , pp .",
    "999 - 1006 .",
    "ieee , 2011 .",
    "glorot , xavier , antoine bordes , and yoshua bengio .",
    "`` domain adaptation for large - scale sentiment classification : a deep learning approach . '' in proceedings of the 28th international conference on machine learning ( icml-11 ) , pp .",
    "513 - 520 . 2011 .",
    "leggetter , christopher j. , and philip c. woodland .",
    "`` maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models . ''",
    "computer speech & language 9.2 ( 1995 ) : 171 - 185 .",
    "nguyen , hien v. , huy tho ho , vishal m. patel , and rama chellappa .",
    "`` joint hierarchical domain adaptation and feature learning . ''",
    "ieee transactions on pattern analysis and machine intelligence , submitted ( 2013 ) .",
    "finkel , jenny rose , and christopher d. manning . `` hierarchical bayesian domain adaptation . ''",
    "proceedings of human language technologies : the 2009 annual conference of the north american chapter of the association for computational linguistics .",
    "association for computational linguistics , 2009 .",
    "khosla , aditya , tinghui zhou , tomasz malisiewicz , alexei a. efros , and antonio torralba .",
    "`` undoing the damage of dataset bias . '' in computer vision  eccv 2012 , pp . 158 - 171 .",
    "springer berlin heidelberg , 2012 .",
    "perronnin , florent , jorge snchez , and yan liu .",
    "`` large - scale image categorization with explicit data embedding . '' in computer vision and pattern recognition ( cvpr ) , 2010 ieee conference on , pp .",
    "2297 - 2304 .",
    "ieee , 2010 .",
    "donahue , jeff , yangqing jia , oriol vinyals , judy hoffman , ning zhang , eric tzeng , and trevor darrell .",
    "`` decaf : a deep convolutional activation feature for generic visual recognition . '' in proceedings of the 31st international conference on machine learning , pp .",
    "647 - 655 . 2014 .",
    "griffin , gregory , alex holub , and pietro perona .",
    "`` caltech-256 object category dataset . ''",
    "bergamo , alessandro , and lorenzo torresani .",
    "`` exploiting weakly - labeled web images to improve object classification : a domain adaptation approach . '' in advances in neural information processing systems , pp .",
    "181 - 189 . 2010"
  ],
  "abstract_text": [
    "<S> domain adaptation techniques aim at adapting a classifier learnt on a source domain to work on the target domain . exploiting the subspaces spanned by features of the source and target domains </S>",
    "<S> respectively is one approach that has been investigated towards solving this problem . </S>",
    "<S> these techniques normally assume the existence of a single subspace for the entire source / target domain . in this work , </S>",
    "<S> we consider the hierarchical organization of the data and consider multiple subspaces for the source and target domain based on the hierarchy . </S>",
    "<S> we evaluate different subspace based domain adaptation techniques under this setting and observe that using different subspaces based on the hierarchy yields consistent improvement over a non - hierarchical baseline . </S>"
  ]
}