{
  "article_text": [
    "local learning algorithms such as @xmath0-nn classification or @xmath0-nn regression occupy an important place in statistical learning theory , further enhanced by a surprising recent result @xcite stating that every consistent learning algorithm , @xmath1 , in the euclidean space is `` localizable '' in a suitable sense .",
    "suppose that we show to @xmath1 only data in the @xmath2-ball around each point @xmath3 , @xmath4    [ 0.25 ]    now smooth the resulting predictor function @xmath5 over the @xmath6-neighbourhood of each @xmath3 : @xmath7 zakai and ritov have shown that if @xmath8 sufficiently slowly , then the resulting `` localized '' predictor @xmath9 is consistent .",
    "it is therefore of obvious interest to examine the question of performance of local learning algorithms in high dimensional domains .",
    "provably _ affected by the curse of dimensionality ? in this article , we will concentrate on the classical @xmath0-nn classifier .",
    "the question turns out to be many - layered , and the answer is different at every layer that we peel back .",
    "perhaps the most basic consideration is that in order to run the @xmath0-nn classifier , one needs to be able to efficiently retrieve @xmath0 nearest neighbours to every input point of the domain . for smaller datasets ,",
    "this problem is solved by means of a complete sequential scan of data . however , for larger datasets this becomes impracticable , and considerable efforts of the data engineering community go towards designing various _",
    "indexing schemes _ assuring faster similarity search @xcite .    in spite of all the progress in the area",
    ", there is a considerable body of evidence in support of the so - called _ curse of dimensionality conjecture _ @xcite affecting the exact deterministic nearest neighbour search in high dimensional spaces .",
    "recall that the _ hamming cube of rank @xmath10 _ , @xmath11 , is the collection of all @xmath12-bit binary strings equipped with the _ hamming distance _ counting the number of bits where two strings differ : @xmath13 recall also that @xmath14 denotes the class of integer sequences @xmath15 that go to infinity , @xmath16 denotes the class of all bounded sequences , and @xmath17 denotes the class of integer sequences @xmath15 that are infinitely small with regard to @xmath10 , that is , @xmath18 as @xmath19 .",
    "for example , the notation @xmath20 means that @xmath12 grows faster than any finite power of @xmath10 , while @xmath21 means @xmath12 grows slower than the @xmath10-th power of any number @xmath22 .    in its simplest form ,",
    "the conjecture states that a dataset @xmath23 with @xmath12 points in the @xmath10-dimensional hamming cube @xmath11 , where @xmath24 , does not in general admit a data structure of size @xmath25 ( that is , polynomial in @xmath12 ) which supports exact deterministic similarity search in @xmath23 in time @xmath26",
    "( i.e. , polynomial in @xmath10 ) .",
    "even though the conjecture remains unproven in general , it has been established for some specific indexing schemes @xcite .",
    "should the conjecture be proved , the @xmath0-nn algorithm will be affected by the curse of dimensionality simply because of a theoretical impossibility to retrieve the @xmath0 nearest neighbours in time polynomial in the dimension @xmath10 of the domain @xmath27 without the need to store a prohibitive amount of data ( superpolynomial in the size @xmath12 of the actual dataset ) .    however , it turns out that the exact nearest neighbour search is not indispensable .",
    "approximate nearest neighbour ( ann ) search @xcite is known to admit more efficient indexing schemes than exact nn search and approximate nearest neighbours can be substitued in a classifier in place of exact ones . as our first main result , we propose a new local classification algorithm based on @xmath0 approximate nearest neighbours ( @xmath0-ann classifier ) , and prove its consistency under the assumption of absolute continuity of the data distribution . theorem [ t : kann ]",
    "is a ( partial ) extension of the classical stone consistency theorem @xcite .    at the same time",
    ", we observe that in the asymptotic setting of high dimensions , @xmath19 , the @xmath0-ann classifier is affected by what may be regarded a variant of hughes s phenomenon @xcite .",
    "the number of datapoints required to maintain the consistency of the algorithm must provably grow exponentially with the dimension of the domain , which assumption is of course unrealistic . thus , at least in an artificial theoretical setting of data sampled randomly from high dimensional distributions , switching to the ann classifier does not lift the curse of dimensionality .",
    "here comes the second main result of the article which , in spite of its simplicity , is quite interesting ( theorem [ th : main ] ) : stone s theorem is insensitive to the euclidean structure on the domain as long as the underlying borel structure remains invariant .",
    "this allows for a very simple `` borel isomorphic data reduction '' to the one - dimensional case , after which the @xmath0-nn algorithm still remains universally consistent .",
    "moreover , such a consistent reduction to the one - dimensional real case applies to functional data classification in infinite - dimensional spaces , in fact in any separable metric space .",
    "the _ borel structure _ is a subject of study of the descriptive set theory @xcite .",
    "it is a derivative of the usual topology of the euclidean or infinite - dimensional banach space , and a considerably coarser structure than the topology , preserving significantly less information . as a result , _ borel isomorphisms _ between the domains",
    " that is , bijections preserving the borel structure , possibly discontinuous at every point  are very numerous and easy to come by .",
    "every euclidean space @xmath28 , in fact every infinite - dimensional banach or even frchet linear space is borel - isomorphic to the real line , and the corresponding isomorphisms can be easily managed at an algorithmic level and implemented in code . while the borel structure is widely used in various parts of pure mathematics , including foundations of theoretical probability , we are unaware of examples of it being employed for the purposes of algorithmic data analysis .    the practical significance of this observation still remains to be seen ( it has only been tested on a few toy datasets from the uci repository , with encouraging results ) , but on a theoretical level it brings up the problem of what is `` dimension '' in the context of statistical learning .",
    "we conclude the paper with a small discussion .",
    "the presentation of results in our paper follows the order in the introduction , and is preceded by a reminder of the standard model of statistical learning and the stone consistency theorem .",
    "here we recall a fundamental result which serves as a theoretical justification for the @xmath0-nn classifier .",
    "the _ domain _ is , in the case of main interest for us , a @xmath10-dimensional euclidean space , @xmath29 .",
    "however , it can be any complete separable metric space .",
    "the _ borel structure _ on @xmath27 is the smallest family , @xmath30 , of subsets of @xmath27 which contains all open balls and is closed under countable unions and complements .",
    "a function @xmath31 is _ borel measurable _ if the inverse image of every interval @xmath32 ( equivalently , of every borel subset of the real line ) under @xmath5 is a borel subset of @xmath27 .",
    "( for a more detailed discussion , see subs .",
    "[ ss : borel ] . ) a _ borel probability measure _",
    "@xmath33 on @xmath27 is a countably - additive function on @xmath30 with values in the interval @xmath34 $ ] , satisfying @xmath35 .",
    "data pairs @xmath36 , where @xmath37 and @xmath38 , follow an unknown probability distribution @xmath33 ( a borel probability measure on @xmath39 ) . denote @xmath40 the collection of all borel measurable binary functions on the domain .",
    "given such a function @xmath41 ( a classifier ) , the _ misclassification error _ is defined by @xmath42 the _ bayes error _ is the infimal misclassification error over all possible classifiers : @xmath43 a _ learning rule _ is a family @xmath44 , where @xmath45 and the associated evalution maps @xmath46 are borel .",
    "here @xmath47 is a labelled learning sample .",
    "for example , the @xmath0-nn classifier is defined by selecting the value @xmath48 in @xmath49 by the majority vote among the values of @xmath50 corresponding to the @xmath51 nearest neighbours of @xmath3 in the learning sample @xmath52 . for even @xmath0",
    ", ties may occur , which are broken with the help of random orders on the neighbours .",
    "data is modelled by a sequence of independent identically distributed random elements @xmath53 of @xmath39 .",
    "denote @xmath54 a sample path .",
    "then the learning rule @xmath55 only gets to see the first @xmath12 labelled coordinates of @xmath54 .",
    "a learning rule @xmath1 is _ consistent _ if @xmath56 in probability as @xmath57 .",
    "if the convergence occurs almost surely , then @xmath1 is said to be _",
    "strongly consistent_. finally , @xmath1 is _ universally consistent _ if it is consistent under every probability measure @xmath33 .",
    "strong universal consistency is defined in a similar way .",
    "let @xmath58 and @xmath59 .",
    "then the @xmath0-nn classification algorithm in @xmath60 ( with regard to the euclidean distance ) is universally consistent .",
    "the conclusion was subsequently strengthened to strong universal consistency , cf .",
    "chapter 11 in @xcite and historic references .",
    "stone s theorem fails in more general metric spaces , even in an infinite - dimensional hilbert space @xmath61 .",
    "one can construct a deterministic concept in @xmath61 not learned by the @xmath0-nn classifier over a gaussian distribution ( cf .",
    "an example in @xcite , pp .",
    "351352 , based on a contruction of preiss @xcite ) .    an alternative proof of the consistency of the @xmath0-nn classifier , based on the lebesgue density theorem for the euclidean space , was given in @xcite , and in @xcite it was further shown that the @xmath0-nn classifier is universally consistent in every metric space satisfying the lebesgue  besikovitch density theorem .",
    "such metric spaces have been completely characterized by preiss @xcite ( they are the so - called sigma - finite dimensional metric spaces , cf . also @xcite ) .",
    "it would be quite interesting to give a formal proof that the universal consistency of the @xmath0-nn classifier in a metric space is equivalent to the validity of the lebesgue  besikovitch density theorem , and further to modify the original proof of stone to make it work for every sigma - finite dimensional metric space .",
    "this would in particular lead to a new proof of the density theorem of real analysis using tools of statistical learning theory .    among the factors affecting the performance of the @xmath0-nn classifier in a high - dimensional space ,",
    "the need to retrieve @xmath0 nearest neighbours of an input datapoint in an effective and efficient way is most apparent , and we will proceed to it now .",
    "let @xmath62 be a metric space , and @xmath63 a finite subset ( dataset ) .",
    "the triple @xmath64 is a _ similarity workload . _",
    "the _ @xmath0-nearest neighbour query _ is : given @xmath65 , return @xmath0 nearest neighbourhs to @xmath0 in @xmath23 . in practice ,",
    "it is often reduced by means of binary search to a sequence of _",
    "similarity queries : _ given @xmath65 and @xmath67 , return all @xmath68 , where @xmath69 denotes the @xmath66-ball around @xmath70 .",
    "see @xcite .    an _ access method _ for a workload @xmath71 is an algorithm that correctly answers every range query .",
    "principal examples of access methods are _ indexing schemes _ , in particular _ hierarchical tree - based indexing schemes_. one popular version of such a scheme as the @xmath72_-tree _ @xcite . for varying discussions of indexing schemes for similarity search in metric spaces ,",
    "see @xcite .",
    "the curse of dimensionality for access methods into high - dimensional domains is a well - known phenomenon among the practitioners , even if it is hard to pinpoint a well - documented reference ( see however @xcite ) . at a theoretical level , the following open `` curse of dimensionality conjecture '' sums up a rather commonly held belief in the curse of dimensionality being inherent in high dimensional data .",
    "@xcite ] let @xmath73 be a dataset with @xmath12 points , where the hamming cube @xmath11 is equipped with the hamming ( @xmath74 ) distance : @xmath75 suppose @xmath76 , but @xmath77 .",
    "( that is , the number of points in @xmath23 has intermediate growth with regard to the dimension @xmath10 : it is superpolynomial in @xmath10 , yet subexponential . )",
    "then any data structure for exact nearest neighbour search in @xmath23 , with @xmath26 query time , must use @xmath78 space within the _ cell probe model _ of computation .",
    "for the cell probe model of computation see @xcite ; in the context of indexing schemes it is briefly discussed in @xcite .",
    "the best lower bound presently known for polynomial space data structures is @xmath79 @xcite .",
    "( see also @xcite for some later improvements . )",
    "rigorous lower bounds superpolynomial in @xmath10 have been established for a number of concrete indexing schemes , notably the pivot - based schemes @xcite and the metric trees @xcite .",
    "the @xmath80-_approximate nearest neighbour search _",
    "problem @xcite is stated thus : given @xmath81 and @xmath82 , for a @xmath65 , if @xmath83 , then return a datapoint @xmath84 at a distance @xmath85 .",
    "( here @xmath86 denotes the distance from @xmath70 to the nearest neighbour in @xmath23 . )",
    "[ 0.25]-ann search.,title=\"fig : \" ]    the known indexing schemes for approximate nearest neighbour ( ann ) search @xcite are more efficient than those for exact nn search .    in order to be used for classification , the @xmath80-ann problem has to be modified in the following way .",
    "@xmath87 approximate nearest neighbours _",
    "( _ @xmath0-ann _ ) problem says : given @xmath70 and @xmath82 , return @xmath0 datapoints contained within the distance @xmath88 of the query point .",
    "here @xmath89 is the smallest radius of a ball around @xmath70 containing @xmath0 datapoints .",
    "[ 0.2]-ann search.,title=\"fig : \" ]    the indexing schemes based on random projections , random matrices , or locality sensitive hashing can be adapted to answer the @xmath87-ann query .    as an example , let us consider the scheme originally developed in @xcite and reformulated in @xcite , section 7.2 using random binary matrices .",
    "the core of the approach is an indexing scheme into the hamming cube @xmath11 , which is afterwards converted into an indexing scheme for @xmath60 by discretization .",
    "let @xmath90 , and denote @xmath91 the dataset with @xmath12 points .",
    "fix a _ range _ @xmath92 .",
    "the scheme for the range @xmath93 consists of a family @xmath94 of mappings from @xmath11 onto a cube of a smaller dimension @xmath95 , with the following property .",
    "if @xmath96 and a random @xmath97 is chosen ( with regard to a certain probability distribution ) , then with a constant confidence @xmath98 the mapping @xmath99 preverves distances in the set @xmath100 on the scale @xmath101 , @xmath92 , to within an additive error @xmath102 , and on a larger scale  away from it",
    ". in the scheme under consideration , the map @xmath99 is a multiplication on the right by a @xmath103 matrix with random i.i.d .",
    "bernoulli entries assuming values @xmath104 and @xmath105 with probabilities @xmath106 and @xmath107 , respectively .",
    "( the operations are carried @xmath108 . )",
    "the target cube only contains @xmath109 points , and is indexed to efficiently answer a nearest neighbour query via hashing .",
    "the indexing scheme consists of a sufficiently large family of such functions @xmath99 for every possible range @xmath93 .",
    "if we are now interested in an @xmath110-approximate nearest neighbour query , a binary search in @xmath93 finds the smallest range so that a randomly chosen @xmath99 only returns one nearest neighbour to @xmath111 at a distance @xmath93 .",
    "this neighbour is of the form @xmath112 , @xmath84 ; the point @xmath3 is returned . with confidence @xmath98 ,",
    "this is a @xmath113-approximate nearest neighbour of @xmath70 in the original hamming cube .",
    "if we want to increase the confidence , the algorithm is run repeatedly , and among the obtained points @xmath114 the nearest one to @xmath70 is returned .",
    "building the scheme takes time polynomial in @xmath12 and the algorithm answers _ every _ query @xmath70 with high confidence in time @xmath115 .",
    "a modification for the @xmath0-ann problem is now obvious .",
    "first , a binary search in @xmath93 determines the smallest value @xmath93 such that for the corresponding randomly chosen @xmath99 the @xmath93-ball around @xmath111 contains at least @xmath0 datapoints .",
    "then @xmath0 nearest neighbourhs , @xmath116 , @xmath117 , @xmath118 to @xmath111 in the cube of small dimension ( image of @xmath99 ) are retrieved , and the corresponding original points @xmath119 returned . with a constant confidence @xmath98 , they will be @xmath120 approximate nearest neighbours of @xmath70 .",
    "again , in order to make the confidence as high as desired , the procedure is repeated as many times as necessary , all returned points are put in a bucket , and the @xmath0 nearest neighbours to @xmath70 among them are returned . the only change in the indexing scheme is that the hash table now stores @xmath0 nearest neighbours instead of one .",
    "the running time of the algorithm is now @xmath121 .",
    "this section contains the first of two main new results reported in the article : an extension of the classical stone consistency theorem @xcite to an approximate nearest neighbour - based classifier .",
    "fix a @xmath82 .",
    "the value of the @xmath0-ann classifier ( more exactly , @xmath87-ann classifier ) at a point @xmath3 is determined by the majority vote among @xmath87-approximate nearest neighbourhs of @xmath3 , as returned by an indexing scheme .",
    "[ t : kann ] suppose the underlying data distribution @xmath33 on @xmath122 has density ( that is , is absolutely continuous with regard to the lebesgue measure ) .",
    "let @xmath82 be fixed , and let @xmath123 then the @xmath87-ann classifier is consistent .",
    "notice that no assumption is made about the nature of the algorithm for answering @xmath87-ann queries .",
    "the task can even be entrusted to an adversary who is aware of the underlying distribution @xmath33 : this will not affect the consistency , though possibly slow down the rate of convergence .",
    "the assumption of absolute continuity of the unknown distribution @xmath33 allows us to avoid dealing with ties in the proof below . for the moment",
    ", we do not know whether this assumption can be dropped .",
    "we also do not know how essential is the assumption that @xmath0 grows strictly faster than the logarithm of @xmath12 .",
    "here is a slightly strengthened version of stone s theorem ( @xcite , theorem 6.3 ) .",
    "[ t : stone2 ] let @xmath33 be a probability measure on @xmath124 with regard to which the datapoints are drawn as i.i.d . random variables .",
    "suppose that @xmath125 are data - dependent weights ( random measurable functions on @xmath60 ) which are nonnegative , sum up to one , @xmath126 and satisfy the properties :    1 .   for some @xmath82 and every borel subset @xmath127 , + @xmath128 2 .   for all @xmath129 , @xmath130 3",
    "@xmath131    define the classification rule @xmath132 based on the majority vote among all the values @xmath133 each given the @xmath134 share of total vote",
    ". if @xmath33 has density , then the rule @xmath132 is consistent .    by approximating a bounded measurable function with simple functions , the condition @xmath135 is seen to be equivalent to    1 .",
    "there is a constant @xmath82 such that for every bounded measurable function on @xmath60 with values in the interval , @xmath136    it follows the proof of stone s theorem ( theorem 6.3 as presented in @xcite on pages 98100 ) practically word for word .",
    "notice that the conditions @xmath137 and @xmath138 are the same , it is only the condition @xmath135 that has been relaxed .",
    "the condition @xmath135 is only used in the proof once , to obtain the last inequality in the chain of inequalites at the end of page 99 .    the functions @xmath139 and @xmath140 in the proof both take their values in the interval @xmath34 $ ] : the former is the density of @xmath33 with regard to its projection on @xmath60 , while the latter is a compactly supported uniformly continuous approximation to @xmath139 in the @xmath141-norm .",
    "therefore , the function @xmath142 takes values in @xmath34 $ ] as well .",
    "thanks to ( i@xmath143 ) , the required inequality holds approximately , to within any wanted error , if @xmath12 is large enough .",
    "thus , in the first displayed formula on top of page 100 one can replace the upper bound of @xmath144 with , for example , @xmath145 , provided @xmath12 is large enough .",
    "this will do just as well .",
    "the rest of the proof remains unchanged .",
    "we apply theorem [ t : stone2 ] with the weights @xmath146 defined as follows : @xmath147 if @xmath148 is among the @xmath87-approximate nearest neighbours of @xmath3 as returned by the oracle ( the indexing scheme ) , and @xmath105 otherwise .",
    "the interpretation of the expected value will depend on whether the indexing scheme is assumed to be randomized or deterministic , however this does not affect the proof .    clearly , the weights are non - negative . since for any given @xmath3 there are precisely @xmath0 @xmath87-approximate nearest neighbours returned , all but @xmath0 weights vanish at the point @xmath3 , and the weights add up to one almost surely .",
    "the condition @xmath137 follows from a classical observation of cover and hart @xcite that in every separable metric space equipped with a borel probability measure , the @xmath104-lipschitz function @xmath149 ( the smallest radius of a ball containing @xmath0 nearest neighbours among @xmath12 datapoints ) will converge to zero almost surely provided @xmath150 .",
    "the condition @xmath138 follows from the definition of weights and the assumption @xmath151 .",
    "it remains to verify the condition @xmath135 .",
    "denote for @xmath129 and @xmath152 @xmath153 now let @xmath154 be the density , that is , the radon  nikodm derivative of the underlying measure on @xmath60 with regard to the lebesgue measure . by the lebesgue differentiation theorem , @xmath155 where the convergence is @xmath33-almost surely and , since it is clearly dominated , also in probability . for @xmath12",
    "suitably large , @xmath156 for all @xmath3 except for a set of measure @xmath157 as @xmath57 .",
    "next we apply the uniform glivenko  cantelli theorem to estimate the number of datapoints in @xmath158 .",
    "the vc dimension of the family of all euclidean balls in @xmath60 is @xmath159 @xcite . as a consequence , for any @xmath67 , @xmath160 , if @xmath161 then with confidence @xmath98 the @xmath33-measure and the empirical measure of every ball differ between themselves by less than @xmath66 ( @xcite , theorem 7.8 ) . since for @xmath162",
    "the expression on the right hand side is of the order @xmath163 , it follows that for @xmath164 and a fixed @xmath160 the conclusion follows for @xmath12 sufficiently large .",
    "due to our assumption on @xmath0 , we can set @xmath165 and conclude that , again for @xmath12 sufficiently large , with high confidence , as @xmath57 , we have @xmath166 and besides @xmath167 thus , with confidence @xmath98 , if @xmath23 is among @xmath0-ann of @xmath168 , then the empirical measure @xmath169 of the ball of radius @xmath170 centred at @xmath168 is at most @xmath171 .    according to lemma 1.11 of @xcite , page 171 ( stone s lemma ) ,",
    "the empirical measure of the set of all such @xmath168 is at most @xmath172 , where @xmath173 is an absolute constant only depending on the dimension @xmath10 .",
    "this means that for all samples @xmath174 of measure @xmath98 and a random @xmath23 independent of @xmath148 s the number of points @xmath148 having @xmath23 as their @xmath0-ann is bounded by @xmath175 denote the set of i.i.d .",
    "samples verifying this condition by @xmath176 .",
    "one has @xmath177 .",
    "according to the `` confidence is cheap '' principle , one can assume here that @xmath178 with any rate of convergence subexponential in @xmath12 , for instance , as @xmath179 .",
    "now we proceed to verifying the condition @xmath135 .",
    "let @xmath127 be a borel subset .",
    "the quantity that we need to bound can be estimated as follows : @xmath180 @xmath181 one has @xmath182 and @xmath183    this finishes the proof .",
    "the @xmath0-ann classifier has not been tested in practice . however , within a theoretical model , it is still not free from the curse of dimensionality .",
    "namely , assuming the datapoints follow a high - dimensional distribution on @xmath60 ( such as the gaussian ) , it is not difficult to prove , as a version of the well - known `` empty space paradox , '' that for a fixed @xmath82 in the limit @xmath19 the number of datapoints must grow exponentially in dimension @xmath10 in order to maintain the consistency of the algorithm . indeed , the ball of radius @xmath184 around the query point will contain @xmath185 datapoints , so it is conceivable that the labels of @xmath0 points chosen among them by the oracle will be highly biased .    here",
    "are two examples .",
    "[ 0.45].,title=\"fig : \" ] .5 cm [ 0.45].,title=\"fig : \" ]    the first one is the segment dataset of the uci data repository , which has a relatively low intrinsic dimension in any possible sense . the second is a randomly drawn dataset from the gaussian distribution in dimension @xmath186 , whose instrinsic dimension can be described as medium .",
    "the graph of the distribution function of the average number of nearest neighbours depending on the distance to the query point is shown in black . set @xmath187 and @xmath188 .",
    "the left vertical line corresponds to the average value of the @xmath0-nn radius @xmath189 , and the second line corresponds to @xmath184 .",
    "for the segment dataset , the latter ball contains on average @xmath190 datapoints .",
    "however , for the gaussian the corresponding value is already @xmath191 .",
    "this brings us to the following definition .",
    "let us say , following @xcite , that a range query @xmath192 is _",
    "@xmath193-unstable _ if the @xmath194-ball around @xmath70 contains at least a half of all datapoints .",
    "figure [ fig : unstable ] . )",
    "[ 0.25 ]    under the subexponential data size growth assumption @xmath195 as well as a certain general assumption of intrinsic high - dimensionality of the underlying measure distribution @xcite , one can prove that asymptotically an overwhelming majority of queries are @xmath196-unstable .",
    "theorem 2.1 in @xcite . )",
    "this assumption is met by the gaussian measures on @xmath60 , the uniform measures on the cubes @xmath197 , the uniform measures on the hamming cubes @xmath11 , and so forth .",
    "in such a situation , the @xmath87-ann search problem can be essentially answered by returning @xmath0 randomly picked datapoints .",
    "the @xmath0-ann classifier becomes meaningless , because the bayes error approaches @xmath198 in the limit @xmath19 .",
    "the exponential rate of growth of dataset size @xmath12 with regard to dimension @xmath10 is of course unrealistic .",
    "this means that at least in some theoretical situations ( i.i.d .",
    "sampling from an artificial high - dimensional distribution ) even allowing for approximate nearest neighbours will not save the @xmath0-nn classifier from the curse of dimensionality .",
    "basic concepts of descriptive set theory @xcite offer a new approach to dimensionality reduction in the context of statistical learning . with this purpose ,",
    "let us re - examine the standard setting for ( non - parametric ) statistical classification as outlined in section [ s : stone ] above .",
    "recall that a family @xmath199 of subsets of a set @xmath23 is a _ sigma - algebra _ if @xmath199 contains @xmath23 and is closed under the complements and unions of countable subfamilies .",
    "a set @xmath23 equipped with a sigma - algebra @xmath199 of subsets is called a _",
    "measurable space_. let now @xmath23 be a separable metric space .",
    "the _ borel sigma - algebra _ of @xmath23 , which we will denote @xmath200 , is the smallest sigma - algebra of subsets of @xmath23 containing all open balls . in particular",
    ", @xmath200 contains all open and all closed subsets of @xmath23 , all intersections of countable families of open sets ( @xmath201-sets ) , all unions of countable families of closed sets ( @xmath202-sets ) , and so on .",
    "in fact , borel subsets are so numerous that it is not easy to exhibit a constructive example of a non - borel subset of a separable metric space such as @xmath203 .",
    "a mapping @xmath204 between two separable metric spaces is _ borel _ ( or _ borel measurable _ ) if the inverse image @xmath205 of each borel subset of @xmath206 is borel in @xmath23 .",
    "equivalently , the inverse image of every open ball in @xmath206 is a borel subset of @xmath23 .",
    "for instance , the indicator function of the rationals is a borel function . by changing the values of a lebesgue measurable function on a suitable null - set",
    ", one can obtain a borel function .",
    "this stresses how numerous borel sets and functions are .    a bijective borel mapping",
    "whose inverse mapping is also borel is called a _",
    "borel isomorphism_. it turns out that from the borel isomorphic viewpoint , metric spaces do not differ between themselves that much .",
    "more precisely , two complete metric spaces @xmath23 and @xmath206 of the same cardinality are borel isomorphic .",
    "thus , the cantor set , the closed unit interval , the euclidean space @xmath60 , the infinite - dimensional separable hilbert space @xmath61 , and in fact all separable frchet spaces different from zero space are all pairwise borel isomorphic between themselves .",
    "their borel structure is that of a _ standard borel space _ of cardinality continuum .",
    "an example of a borel isomorphism between the interval @xmath34 $ ] and the square @xmath34 ^ 2 $ ] can be obtained by interlacing between themselves the binary expansions of @xmath3 and @xmath50 of a pair @xmath207 ( subject to the usual precautions concerning infinite strings of ones ) : @xmath208 ^ 2\\ni(0.a_1a_2\\ldots , 0.b_1b_2\\ldots)\\mapsto ( 0.a_1b_1a_2b_2\\ldots)\\in [ 0,1].\\ ] ]    a geometric representation of this isomorphism can be seen in figure [ fig : borel_isomorphism ] .    [ 0.25 ]    this of course extends to any number of dimensions .",
    "usually the mappings performing the data reduction of the domain are assumed to be continuous , even lipschitz .",
    "however , if one looks at the existing theoretical model laying down a foundation for statistical learning , one can notice that stone s theorem is in fact insensitive to the _ euclidean structure _ ( that is , either metric or topological structure ) on the domain as long as the underlying _ borel structure _ remains intact .",
    "this allows for a very simple `` borel isomorphic data reduction '' to the one - dimensional case , after which the algorithm still remains universally consistent .      the following result , although straightforward , offers , in our opinion , a potentially interesting new approach to dimensionality reduction in statistical learning theory .",
    "we consider it as the central result of the work reported .",
    "recall that a _ standard borel space _ is a complete separable metric space equipped with its borel structure .",
    "[ th : main ] let @xmath27 be a standard borel space .",
    "fix a borel isomorphism @xmath209 , where @xmath210 is a metric space in which the @xmath0-nn learnig rule is universally consistent ( for instance , @xmath211 or its metric subspace ) .",
    "define a metric @xmath212 on @xmath27 by @xmath213 then the learning rule on @xmath27 given by the @xmath0-nn rule with regard to the metric @xmath212 is universally consistent .    before proving the result",
    ", we need to fix notation and terminology .",
    "a _ metric space with measure , _ or an @xmath214_-spaces , _ is a triple @xmath215 , consisting of a separable metric space @xmath62 and a borel probability measure @xmath33 on this space .",
    "this is an important notion in modern geometry and functional analysis @xcite",
    ".    the basic object of classification theory will be very similar , with the only difference that the probability measure @xmath33 is now defined on @xmath39 .",
    "equivalently , such an object can be described as a metric space @xmath62 equipped with a pair of finite measures @xmath216 , whose total mass adds up to one : @xmath217 is the restriction of @xmath33 to @xmath218 , and @xmath219 , to @xmath220 .",
    "let us call such objects @xmath221_-spaces_.    two metric spaces with measure @xmath222 , @xmath223 are _ isomorphic _ if there is an _ isomorphism _ between them , that is , a mapping @xmath224 which is an isomorphism of measure spaces and which preserves the metric almost everywhere .",
    "the concept of an isomorphism between our @xmath221-spaces is defined similarly : it is a measurable mapping @xmath225 which preserves @xmath217 , @xmath219 , and which preserves pairwise distances between points @xmath226-almost everywhere .",
    "let @xmath227 and @xmath228 be two isomorphic @xmath221-spaces , with an isomorphism @xmath229 .",
    "let @xmath1 be a learning rule in @xmath27 .",
    "then one can define a learning rule in the space @xmath228 using the isomorphism , as follows . denote @xmath230 the inverse measurable isomorphism to @xmath225 .",
    "we will denote by the same symbol @xmath231 the image of a labelled @xmath12-sample @xmath232 , that is , @xmath233 now set @xmath234 this @xmath235 is a learning rule on @xmath236 , a _ transport _ of the rule @xmath1 along the map @xmath225 .",
    "the following should now be obvious .",
    "[ l : error ] the learning rule @xmath237 in @xmath236 has the same learning error as @xmath1 does in @xmath27 .",
    "it is enough to notice that the @xmath0-nn classifier in the metric space @xmath62 is the transport along @xmath225 of the @xmath0-nn classifier in the metric space @xmath238 .",
    "for every probability distribution @xmath33 on @xmath39 , denote @xmath239 the push - forward of @xmath33 along @xmath225 .",
    "this is a borel probability distribution on @xmath240 , and clearly the bayes error of @xmath33 equals that of @xmath239 . in view of our hypothesis of the universal consistency of the @xmath0-nn classifier in @xmath23 ,",
    "the learning error of the classifier equals to bayes error . due to lemma [ l : error ] , the same conclusion holds for the @xmath0-nn classifier in the metric space @xmath62 .",
    "in particular , there is always a borel isomorphic reduction of the problem in @xmath60 to @xmath241 .",
    "moreover , the reduction even applies to functional data learning in the most general situation imaginable , when @xmath27 is an arbitrary separable metric space , for instance a separable banach space , or even a separable frchet space .",
    "the histogram learning rule in the cube @xmath197 is a borel isomorphic reduction to the cantor set ( a zero - dimensional compact metrizable space without isolated points ) equipped with a non - archimedian metric .",
    "the distance metric learning methods , such as lmnn ( see e.g. @xcite ) , are based on selecting an alternative euclidean metric in @xmath60 .",
    "this is equivalent to selecting a linear isomorphism @xmath225 from @xmath60 to itself and using the learning rule @xmath237 in the original space . here , @xmath225 is of course not just a borel isomorphism , but moreover a homeomorphism .",
    ".@xmath0-nn classification of some datasets in uci repository before and after a borel dimensionality reduction as in eq .",
    "( [ eq : borelmap ] ) , using rweka ( @xmath242 , @xmath243-fold cross - validation ) . [ cols=\"<,^,^,^,^,^,^,^,^ \" , ]",
    "in this article , we suggested two novel approaches to dimensionality reduction in the context of the @xmath0-nn classification : the @xmath0-approximate nearest neighbour rule and the borel isomorphic dimensionality reduction .",
    "the closest counterpart in the literature is an approach based on a combination of the @xmath0-nn classifier with random projections @xcite .",
    "notice , however , that this is different from our approaches : first , not every indexing scheme for ann search is based on random projections , and second , projections are not borel isomorphisms .    a more technical paper about the approximate @xmath0 nearest neighbour classifier in which the algorithm has been implemented and tested is currently in preparation @xcite .",
    "the borel isomorphic dimensionality reduction is easy to implement , and we invite the readers to try their hand at it . as a word of caution ,",
    "if the original domain is high dimensional , this may necessitate using floating - point arithmetic .",
    "the initial experiments with data from the uci machine learning repository ( table [ t : uci ] ) show that the borel isomorphic reduction succeeds at least for some datasets , and fails for others . on the one hand ,",
    "the richness of the class of borel isomorphisms is enormous , and the failure can be always attributed to a poor choice of a reduction . on the other hand ,",
    "it is definitely hard to expect such a simple idea to give a panacea of the curse of dimensionality .",
    "what is probably a realistic expectation , is a possibility to slash a high dimension by a given factor without degraing the performance , by arranging the dimensions of a domain in groups of @xmath244 and performing a borel isomorphic reduction @xmath245 on every such group separately .",
    "an interesting perspective is a theory of capacity for _ families _ of borel isomorphisms between a given domain and a fixed lower - dimensional space ( e.g. @xmath203 ) , enabling search for an optimal borel data reduction for a given dataset .    at a theoretical level , this brings up the question , what is dimension in the context of statistical learning ?",
    "it took mathematicians roughly half a century , to isolate the correct notion of dimension of a topological space and obtain the basic results of the theory ( roughly , 18731921 , see @xcite ) .",
    "will it take the same amount of time to put forward a satisfactory theory of intrinsic dimension of data in the context of statistical learning , in order to explain away the curse of dimensionality ?    what our investigation demonstrates ,",
    "is that such a notion should reflect not the dimension of the domain per se , but rather the complexity of the target classifier in the setting of a @xmath221-space consisting of a metric domain @xmath62 and a probability distribution on @xmath39 .",
    "an important factor is the isoperimetric behaviour of the unknown target concept @xmath246 , by which we mean the rate of growth of the function @xmath247 where @xmath248 is the @xmath66-neighbourhood of the concept .",
    "a fast growing isomerimetric function reflects the high complexity of the margin and a consequent difficulty of finding a classifier .",
    "this is an extended write - up of the talk of the same title given by the author on august 25 , 2011 at the dagstuhl perspective seminar 11341 `` learning in the context of very high dimensions '' ( organized by michael biehl , barbara hammer , erzsbet mernyi , alessandro sperduti , and thomas villmann ) .",
    "the original slides of the talk are available on the web page + http://www.dagstuhl.de/mat/index.en.phtml?11341    among many interesting talks at the dagstuhl seminar , i want to particularly single out those by fabrice rossi , udo seiffert , and kilian weinberger , from which i have learned a number of important results and references , including @xcite , @xcite , and @xcite , correspondingly .",
    "stimulating discussions with erzsbet mernyi are also acknowledged .",
    "nearest neighbours in high - dimensional spaces , in : j.e .",
    "goodman , j. orourke , eds .",
    ", _ handbook of discrete and computational geometry _ ,",
    "chapman and hall / crc , boca raton  london  new york ",
    "washington , d.c . 877892 , 2004 .",
    "d. preiss , _ dimension of metrics and differentiation of measures , _ in : general topology and its relations to modern analysis and algebra , v ( prague , 1981 ) , 565568 .",
    "sigma ser . pure math . * 3 * , heldermann , berlin , 1983 .",
    "p. ciaccia , m. patella and p. zezula .",
    ": an efficient access method for similarity search in metric spaces . in _ proc .",
    "23rd int . conf .",
    "on very large data bases ( vldb97 ) , ( athens , greece ) _ , 426435 , 1997 .",
    "r. weber , h .- j .",
    "schek , and s. blott , a quantatitive analysis and performance study for similarity - search methods in high - dimensional spaces . in : _ proceedings of the 24-th vldb conference , _ new york , pp . 194205 , 1998 ."
  ],
  "abstract_text": [
    "<S> there is an increasing body of evidence suggesting that exact nearest neighbour search in high - dimensional spaces is affected by the curse of dimensionality at a fundamental level . </S>",
    "<S> does it necessarily mean that the same is true for @xmath0 nearest neighbours based learning algorithms such as the @xmath0-nn classifier ? </S>",
    "<S> we analyse this question at a number of levels and show that the answer is different at each of them . as our first main observation </S>",
    "<S> , we show the consistency of a @xmath0 approximate nearest neighbour classifier . </S>",
    "<S> however , the performance of the classifier in very high dimensions is provably unstable . as our second main observation </S>",
    "<S> , we point out that the existing model for statistical learning is oblivious of dimension of the domain and so every learning problem admits a universally consistent deterministic reduction to the one - dimensional case by means of a borel isomorphism .    nearest neighbour search , the curse of dimensionality , approximate @xmath0-nn classifier , borel dimensionality reduction 62h30 , 68h05 </S>"
  ]
}