{
  "article_text": [
    "consider a spatial gaussian process ( also called a gaussian random field ) @xmath0 , where @xmath1 , @xmath2 being the model domain , and @xmath3 .",
    "nonlinear _ functional of this field : @xmath4 , where @xmath5 is a vector of length @xmath6 .",
    "suppose we have observed the value of this functional , say the vector value is @xmath7 .",
    "conditional on this observation , how can we characterize the field @xmath0 ?    this is an abstraction of statistical approaches to a wide range of scientific questions . in these questions , @xmath0 is a `` spatially - distributed '' physical attribute .",
    "the functional @xmath8 , known as the `` forward model '' or `` forward process '' , is typically embodied in a deterministic , numerical algorithm that takes the field @xmath9 as input and outputs vector - valued result @xmath10 . in nature , the forward model is a physical process whose outcome is dictated by the attribute @xmath11 in the entire domain @xmath2 .",
    "the task is , conditional on the observed outcome of the forward model , how can we characterize ( or `` back out '' ) the attribute field @xmath11 ?",
    "since we are using the outcome of the forward model to infer the quantity @xmath11 that dictates the outcome of the forward model , this task is known as an `` inverse problem '' .",
    "a few examples will make the setting clear .    _",
    "example 1_. @xcite reviews techniques for the groundwater inverse problem , in which the spatial attribute of interest is hydraulic conductivity of the subsurface medium .",
    "conductivity controls groundwater flow which is governed by well - understood partial differential equations .",
    "scientists or engineers make observations of groundwater movement to get head ( pressure ) and flux at selected locations and times , and attempt to use these data to infer the spatially distributed conductivity .    _",
    "example 2_. @xcite study a 1-d problem in exploration seismology . in this setting , an impulsive or vibrating load applied at the ground surface launches elastic waves into the earth s interior .",
    "part of the wave energy is reflected by the medium and reaches the ground surface , where it is monitored at many time instants .",
    "wave propagation in the subsurface column is described by a hyperbolic equation system with mechanic attributes of the elastic medium , including density and lam constants , as input parameters .",
    "the task is to recover the mechanic attributes , ideally everywhere in the subsurface column , using surface measurements of wave propagation including pressure and particle velocity .    _",
    "example 3_. @xcite deduce the spatial distribution of @xmath12 sources ( including sinks ) on the surface of the globe , given time - series observations of surface concentrations of @xmath12 at some 20 locations around the globe .",
    "the @xmath12 source here is treated as a spatial attribute defined for every location on the ground surface .",
    "the surface @xmath12 concentration around the globe is the result of re - distribution of the sources via a transport process , governed by a diffusion equation .",
    "these examples have three components in common : a spatial attribute of interest ( hydraulic conductivity of flow media ; mechanic properties of elastic media ; source / sink of co@xmath13 ) , a forward process ( groundwater flow ; wave propagation ; atmospheric transport ) , and observed outcomes of the forward process ( head and flux ; pressure and particle velocity ; co@xmath13 concentration ) .",
    "the attribute of interest varies in space and is possibly highly heterogeneous ; in some scientific literature it is called a `` ( spatially ) distributed parameter '' @xcite .",
    "the forward process is represented by a numerical model , which takes the spatial attribute as the main input , along with boundary and initial conditions .",
    "usually , the need for this inversion arises because the attribute @xmath0 is needed for modeling ( or simulating , or predicting ) some other process of interest .    in concept ,",
    "the unknown , @xmath9 , has infinite dimensions . in real - world applications ,",
    "it is almost always defined on a finite grid that discretizes the model domain . in either case , the unknown @xmath9 has many more dimensions than the known @xmath10 .",
    "this makes the inverse problem a severely under - determined one .",
    "it has become an established strategy to formulate the inverse problem in a statistical way @xcite , that is , take @xmath0 as a random field and try to obtain its _ conditional distribution _ given the observation @xmath7 , @xmath14 .",
    "typically , this conditional distribution is represented empirically by a large number of _ realizations _ ( simulations ) of the random field . in this context , bayesian approaches and markov chain monte carlo computations",
    "have been widely used ( see section  [ sec : anchor - motivation ] ) .    in this study , we propose a method for this inverse problem .",
    "the proposal centers on a parameterization device called `` anchors '' , hence the name of the method``anchored inversion '' . let the anchor parameters be denoted by @xmath15 .",
    "with anchor parameterization , _ we solve a reduced problem _ , namely , @xmath16 rather than @xmath14 .",
    "the dimension of @xmath15 is much lower than that of @xmath9 ( the latter could be infinite ) , and the dimension of @xmath15 is under the user s control .",
    "this dimension reduction brings advantages in computation as well as conceptualization .",
    "the proposed method is a fundamental departure from some existing approaches . in section  [ sec : anchor - motivation ] , we identify some challenges faced by existing methods , which motivate the concept of `` anchor '' .",
    "anchor parameterization is formally described in section  [ sec : anchor - parameterization ] .",
    "section  [ sec : inference ] presents an iterative algorithm that approximates the posterior distribution of the chosen anchor parameter ( @xmath15 ) by a normal mixture . in section  [ sec : choice - of - anchors ]",
    "we describe a strategy for choosing anchor parameters automatically and adaptively as the iterative algorithm proceeds .",
    "this efficient strategy makes anchor parameters evolve in iterations , adapted to the available data , complexity of the @xmath17 field , and the computational effort so far invested .",
    "this anchor - selection procedure is integrated into the algorithm described in section  [ sec : inference ] .",
    "section  [ sec : extensions ] describes two extensions to the basic framework presented in earlier sections .",
    "the first extension , geostatistical parameterization , is expected to be used in most applications .",
    "the second extension is used whenever the type of data described there are available . in section  [ sec : examples ] , the method is applied to three scientific questions using synthetic data . section  [ sec : conclusion ] concludes with a summary and highlights .",
    "we emphasize `` nonlinear '' in this study , because treatment of `` linear '' forward models is relatively mature .",
    "a mainstay of the approaches is the kalman filter and variants @xcite .",
    "however , the vast majority of forward processes in scientific applications are nonlinear , as demonstrated in section  [ sec : examples ] .",
    "linear data , if available , are accommodated as described in section  [ sec : linear - data ] .",
    "although @xmath11 is `` point - referenced '' in concept , in practice the model domain is discretized ( or aggregated ) into a numerical grid .",
    "this discretization is assumed throughout , and it enables the use of matrix notations instead of integrals .",
    "we shall use @xmath18 and @xmath11 as generic symbols for location and the spatial variable , and use @xmath19 and @xmath20 for the finite - length @xmath18 and @xmath11 vectors corresponding to the numerical grid .",
    "specific values of @xmath11 and @xmath20 are denoted by @xmath21 and @xmath22 , respectively . throughout , the symbol @xmath23 denotes a normal density or likelihood function .",
    "we use @xmath24 as a generic symbol for probability density function and @xmath25 for conditional density .",
    "when we have a specific density , say obtained by approximation , we use a specific symbol , for example @xmath26 .",
    "discretization of the model domain , along with other sources , introduces model error in the simulation of the forward process , @xmath8 . in addition",
    "there may be measurement error in @xmath7 .",
    "suppose the model and measurement errors are additive , we can write @xmath27 where @xmath7 is the forward data , @xmath8 is the forward model ( a functional of the discretized field @xmath22 ) , and @xmath28 is combined model and measurement error .",
    "a typical bayesian approach takes @xmath20 as the parameter vector and seeks its posterior distribution given the observation @xmath7 .",
    "suppose the density of the error @xmath28 is known to be @xmath29 , then the likelihood function is @xmath30 upon specification of a prior @xmath31 , the posterior is @xmath32 .",
    "this high - dimensional posterior distribution is usually represented by samples from it via markov chain monte carlo ( mcmc ) or related methods .",
    "there exist a large number of studies along this line ,  @xcite .",
    "if the error is multiplicative with known distribution , manipulations may be possible in simple cases ; see @xcite for an example .",
    "the construct above is also the cornerstone for a large class of approaches using `` regularization '' and optimization @xcite .",
    "we recognize three difficulties in this approach , namely , ( 1 ) that the same @xmath20 is both the model parameter and the input to the forward model creates conflicting requirements ; ( 2 ) the formulation is centered at `` error '' , @xmath28 , ruling out situations where error is nonexistent or negligible ; ( 3 ) distribution of the error @xmath28 , especially the model error , is hard to specify .",
    "these points are elaborated below .",
    "additional comments can be found in @xcite .    1 .   by taking the entire model grid @xmath20 as the parameter vector , the dimension of the parameter space",
    "is tied to the spatial resolution of the numerical implementation of the forward model @xmath8 .",
    "this creates conflicts between parameter identifiability and accuracy in the numerical forward model @xmath8 , which is the essential connection between the model parameter and data .",
    "one side of this paradox calls for a coarse grid ( for better identification of the model parameter ) whereas the other side prefers a fine grid ( for more accurate simulation of the forward process ) .",
    "the only randomness in the parameter - data link in this formulation arises from the model and measurement errors , @xmath28 , because the forward model @xmath8 is deterministic . while in reality error usually exists , in synthetic or theoretical studies it does not have to .",
    "clearly , the bayesian approach centered at  ( [ eq : additive - likelihood ] ) does not apply if error is nonexistent . on the other hand ,",
    "even if the magnitude of the error is not quite negligible , one may elect to ignore errors ( because , for example , knowledge about the error s distribution is very limited and unreliable ) and still have a meaningful inverse problem of inferring @xmath20 from @xmath7 .",
    "however , the formulation based on  ( [ eq : additive - error ] ) is not applicable in this situation .",
    "reliable knowledge of the error distribution , @xmath29 , is often unavailable .",
    "a major complication here is model error . while one may have a decent knowledge of the _ measurement error _ based on instrument specifications , _ model error",
    "_ is much more elusive .",
    "there are numerous sources for model error , including spatial and temporal discretization , inaccurate boundary and initial conditions , relevant physics that are omitted from consideration , and so on .",
    "another notable error stems from the so - called incommensurability @xcite , the fact that what is computed by the model and what is measured in the field are not exactly the same quantity . a common cause of incommensurability is discrepancy in the spatiotemporal resolution ( or scale ) .",
    "taken as a random vector , the model error usually has inter - correlated components . to further complicate the matter ,",
    "the distribution of this error vector may depend on the input field @xmath22 .",
    "+ @xcite point out that it is especially difficult to know the statistics of all errors when `` the theoretical predictions from a model involve approximations '' .",
    "however , all the sources of model error listed above essentially result in approximations .",
    "+ the fact that @xmath28 contains model error has been emphasized by @xcite . at this point",
    "it must be recognized that model error may not be smaller than measurement error , hence it is not feasible to assume that measurement error dominates model error and makes the latter negligible .",
    "@xcite give a telling example : `` in seismology , the theoretical error made by solving the forward travel time problem is often one order of magnitude larger than the experimental error of reading the arrival time on a seismogram . ''",
    "also see @xcite .",
    "we present an alternative approach that has the potential to alleviate these difficulties .",
    "the first intuition comes from the difficulty  ( 1 ) above . realizing that it is largely hopeless to aim for a sharp resolution of , say , a @xmath33 field @xmath20 using a data vector @xmath7 of length 100",
    ", we ask , `` instead of taking on the entire field @xmath20 , what about aiming for a sharper resolution of , say , 50 ` characteristic values ' of @xmath20 using the data @xmath7 , and then parameterizing the field with these 50 characteristic values ? ''",
    "the answer is to reduce the model parameter vector and separate it from the numerical grid .",
    "specifically , we take certain linear functionals of the field as such characteristic values and call them `` anchors '' , denoted by @xmath15 : @xmath34 where @xmath35 is a @xmath36 matrix of rank @xmath37 , satisfying @xmath38 .",
    "subsequently , our model inference involves deriving @xmath16 , and parameterizing the field @xmath20 by @xmath15 via @xmath39 . in a sense , we have proposed to solve a different problem , a `` reduced '' one , than that tackled by the standard approach ( which derives @xmath40 ) . in the reduced problem ,",
    "the anchors act as `` middleware '' between @xmath7 and @xmath20 ; the most important information in @xmath7 about @xmath20 is `` transferred '' into @xmath15 , in the form of @xmath16 , instead of into @xmath20 directly .",
    "we shall use the name `` anchored inversion '' for the proposed method .",
    "the key elements of this method are the anchor definition represented by the matrix @xmath35 , the anchor parameterization for the field , @xmath39 , and derivation of the posterior @xmath16 .",
    "these issued are discussed in the sections to come .",
    "it is apparent that the anchor concept de - couples the dimension of the model parameter ( which is now @xmath15 ) from that of the numerical grid .",
    "the former is under our control , and as such can be vastly lower than the latter .",
    "whereas the dimension of @xmath20 may be dictated by physics , required resolution and accuracy of the forward model , as well as subsequent application needs for the inferred attribute field , the dimension of @xmath15 is determined mainly by statistical and computational concerns .",
    "the user has the freedom to seek a trade - off , via the choice of anchors , between feasible derivation of @xmath16 ( calling for a shorter @xmath15 ) and sufficient parameterization of the field by @xmath39 ( calling for a longer @xmath15 ) .",
    "this spirit of dimension reduction is shared by an active line of research using `` convolutions '' @xcite .",
    "the convolution - based methods differ from anchored inversion in many ways .",
    "for one thing , the former conducts computation in a mcmc framework , whereas the latter is `` incompatible '' with mcmc , as addressed next .",
    "the second difficulty in the existing approach , errors must be present for the formulation to be applicable , is eliminated . in the existing approach , the parameter - data connection is @xmath41 in contrast , the connection in anchored inversion is @xmath42 the randomness in the parameter - data connection now comes from two sources ( or layers ) : the random field @xmath22 conditional on parameter @xmath15 , and the random error @xmath28 conditional on fixed @xmath22 .",
    "the algorithm of anchored inversion ( see section  [ sec : inference ] ) learns about the statistical relation between the model parameter @xmath15 and the data @xmath7 ( or @xmath10 , to be more accurate ) by simulating the data - generation process .",
    "note that the first layer of randomness is tractable by simulation because it is due to the _ known _",
    "parameterization @xmath39 .",
    "the second layer of randomness can be simulated if one has decent knowledge about the errors .",
    "the `` knowledge '' here does not have to be a closed - form distribution ; it may be , for example , mechanism of the occurrence of errors in just enough detail such that the ( stochastic ) mechanism can be embedded in the forward simulation .",
    "however , if the dimension of @xmath15 is much lower than that of @xmath22 , the first layer of randomness could dominate the second , making an accurate quantification of @xmath28 less critical to accurately simulating the statistical relation between @xmath15 and @xmath7 . in such situations ,",
    "inaccurate but helpful quantification of the errors may be used , or the errors may be ignored altogether . moreover ,",
    "anchored inversion allows the situation where the forward model and measurements are error free because , lacking the second layer of randomness , the parameter - data relation is still a statistical one due to the first layer of randomness .",
    "( in fact , in the case studies presented in section  [ sec : examples ] , we did not artificially add model or measurement errors to the synthetic data . )",
    "this leads to the most fundamental distinction between anchored inversion and mcmc methods : the former does not make analytical quantification of the statistical relation between the model parameter and data , but rather simulates the relation . in the simulation , a likelihood function",
    "is not known ( and not needed ) ; consequently , a standard application of the bayes theorem is not possible .",
    "related to the third difficulty in the existing approach , general forms of errors , be it additive , multiplicative , or arising in multiple stages of the forward process",
    "@xmath8 , may be accommodated in the numerical derivation of the posterior @xmath16 .",
    "this will be discussed in section  [ sec : accommodate - errors ] .",
    "section  [ sec : accommodate - errors ] also suggests that it may be possible to apply the proposed method to inverse problems where the forward model is non - deterministic .",
    "we model @xmath0 as a gaussian process . in other words",
    ", @xmath43 is a ( high dimensional ) normal random vector with mean @xmath44 and covariance matrix @xmath45 : @xmath46 where @xmath47 is the normal density function . with the anchor @xmath15 defined in  ( [ eq : anchor - def ] ) , we have the joint distribution @xmath48      { { \\boldsymbol{{y}}}}\\cr\\-      { \\ensuremath{\\theta}}\\cr      \\end{pmat } \\sim n\\biggl (          \\begin{pmat } [ { } ]              { { \\boldsymbol{\\mu}}}\\cr\\-              { { \\boldsymbol{h}}}{{\\boldsymbol{\\mu}}}\\cr              \\end{pmat }          , \\ ;          \\begin{pmat}[{| } ]              { { \\boldsymbol{\\sigma } } } & { { \\boldsymbol{\\sigma}}}{\\ensuremath{{{{\\boldsymbol{h}}}}^t}}\\cr\\-              { { \\boldsymbol{h}}}{{\\boldsymbol{\\sigma } } } & { { \\boldsymbol{h}}}{{\\boldsymbol{\\sigma}}}{\\ensuremath{{{{\\boldsymbol{h}}}}^t}}\\cr              \\end{pmat }      \\biggr ) .\\ ] ] then @xmath20 conditional on @xmath15 is normal : @xmath49    the power of the anchor parameterization lies in the basic property of the normal distribution that leads to the conditional distribution  ( [ eq : y - given - anchor ] ) .",
    "depending on the choice of anchors ( the matrix @xmath35 ) , this parameterization introduces flexible , and analytically known , mean and covariance structures into the field vector @xmath20 .",
    "we shall speak of `` sampling '' the conditional distribution @xmath50 . in actual implementations , an explicit sampling of  ( [ eq : y - given - anchor ] )",
    "is often replaced by a `` conditional simulation '' procedure that consists of two steps : +    \\(a ) sample @xmath51 from @xmath52 .",
    "+    \\(b ) update @xmath51 to @xmath53 .",
    "+ it is easily verified that the resultant @xmath54 is indeed a random draw from  ( [ eq : y - given - anchor ] ) .    a typical application of anchored inversion consists of three steps :    \\(a )",
    "define anchors @xmath15 , specify @xmath35 .",
    "\\(b ) derive the distribution of @xmath15 conditional on data @xmath7 , that is , @xmath55 .",
    "\\(c ) generate a sample of field realizations using the posterior @xmath55 and the conditional @xmath50 .",
    "we first present an iterative algorithm in section  [ sec : inference ] for approximating @xmath55 , assuming @xmath35 has been specified . in section  [ sec : choice - of - anchors ] ,",
    "we present a procedure that automatically and adaptively specifies @xmath35 in the iterations of the algorithm . therefore the anchors ( @xmath35 ) are not fixed , but rather evolve in the iterations .",
    "the third step entails sampling by `` composition '' ( * ? ? ?",
    "* sec  3.3.2 ) , that is , sampling @xmath56 from @xmath55 followed by sampling @xmath54 from @xmath57 . in these samplings , the anchor specification and their posterior distribution are taken from the final iteration of the algorithm . because @xmath57 is normal",
    ", the computational cost of the sampling is low .",
    "sampling @xmath55 is also easy because , as we shall see in section  [ sec : inference ] , the distribution is approximated by a normal mixture distribution .    in section  [ sec : geostat - extension ]",
    ", we will introduce a small number of geostatistical parameters to characterize the mean vector @xmath44 and the covariance matrix @xmath45 of the field .",
    "these geostatistical parameters are regarded as unknown and are inferred in the iterative algorithm along with the anchor parameters .",
    "we now tackle the posterior of the model parameter @xmath15 given data @xmath7 , the mean vector @xmath44 , and the covariance matrix @xmath45 of the field vector @xmath20 .",
    "let @xmath58 be a prior for @xmath15 .",
    "this is specified by @xmath59 following ( [ eq : anchor - def ] ) and  ( [ eq : field - prior ] ) . noticing the parameter - data connection depicted in  ( [ eq : anchor - data - randomness ] ) and the generality of the forward model @xmath60 , the usual route through the bayes theorem ,",
    "@xmath61 is not usable here because the likelihood function @xmath62 is unknown . in fact , it can be difficult to analyze the statistical relation between @xmath15 and @xmath10 because @xmath8 is typically defined by a complicated numerical code , involving many components and steps , including possibly ad hoc operations .    however , we can sample @xmath62 following the `` data - generating '' mechanism @xmath63 . starting with a particular @xmath56 ,",
    "this mechanism involves sampling @xmath54 from @xmath64 and subsequently evaluating @xmath65 .",
    "if @xmath56 is a random sample from the prior @xmath58 , then @xmath66 is a random sample from the joint distribution of @xmath15 and @xmath10 , namely @xmath67 .    in general ,",
    "suppose @xmath68 ( conditional on @xmath44 and @xmath45 ) has a mixture distribution with density @xmath69 where @xmath70 is the density of the @xmath71th mixture component and the weights @xmath72 sum to unit .",
    "denote the marginals of @xmath15 and @xmath10 corresponding to @xmath73 by @xmath74 and @xmath75 , respectively , and write the conditionals @xmath76 and @xmath77 .",
    "the density of @xmath15 conditional on @xmath7 as well as @xmath44 and @xmath45 is then @xmath78 where @xmath79 .",
    "this shows that the conditional ( or posterior ) @xmath55 is a mixture of the conditionals @xmath80 .",
    "take a pause here and think about the situation we are in .",
    "first , it appears possible to obtain random samples of @xmath68 .",
    "second , the joint density of @xmath68 may be approximated by a mixture distribution .",
    "third , if it is feasible to get the marginal @xmath75 and conditional @xmath80 from the mixture component @xmath73 , we are on a clear path to @xmath55 , the ultimate target .    regarding the last point , @xmath75 and @xmath80 are readily available if @xmath73 is normal . the second point suggests kernel density estimation . in view of the third point ,",
    "gaussian kernels should be used . as for the first point ,",
    "if we do not have random samples of @xmath68 , we could have `` weighted '' samples by the general importance sampling technique @xcite .",
    "in fact the algorithm below will show that importance sampling is critical for building an _ iterative _ procedure . because we are able to sample @xmath68 as needed ( via simulations ) , we sample it in iterations , each time using a manageable sample size and an `` improved '' proposal distribution . after obtaining a reasonably large sample of @xmath68 , we embark on a gaussian kernel density estimation for the joint distribution @xmath81 , with the real goal being an estimate of the posterior ( or conditional ) @xmath55 . over iterations ,",
    "the proposal distribution improves not in terms of estimating @xmath81 but in terms of estimating @xmath55 .",
    "we begin with an initial approximation to the posterior @xmath55 , denoted by @xmath82 , which is taken to be a sufficiently diffuse normal distribution .",
    "the first iteration updates @xmath83 to @xmath84 . in general",
    ", the @xmath85th iteration updates the approximation @xmath86 to @xmath87 as follows .    1 .   _ drawing a random sample of the parameters . _ +",
    "take @xmath88 random samples , denoted by @xmath89 , from @xmath86 .",
    "for @xmath90 , compute the prior density @xmath91 according to  ( [ eq : anchor - prior ] ) , and the `` proposal density '' @xmath92 ; let @xmath93 .",
    "2 .   _ evaluating the forward model . _",
    "+ for @xmath90 , sample @xmath94 from @xmath95 according to the density  ( [ eq : y - given - anchor ] ) , and evaluate @xmath96 .",
    "the sampling of @xmath94 is typically accomplished by a geostatistical simulation algorithm outlined in section  [ sec : anchor - parameterization ] , while evaluation of @xmath97 usually entails running a numerical forward model with the simulated field @xmath94 as input .",
    "3 .   _ reducing the dimension of the conditioning data by principal component analysis ( pca ) . _",
    "+ let @xmath98 { z}_1 & \\hdots & { z}_n\\cr \\end{pmat}}^t}}$ ] , where @xmath99 are column vectors of length @xmath100 . by pca , we find a @xmath101 matrix @xmath102 , where @xmath103 , such that the matrix @xmath104 `` explains '' specified proportions ( 99% ) of the variations in @xmath105 .",
    "subsequently , we transform each simulated @xmath97 , @xmath90 , to @xmath106 , and also transform the observation vector @xmath7 similarly .",
    "this transformation reduces the dimension of @xmath10 from @xmath100 to @xmath107 . to keep the notation simple",
    ", we shall continue to use the symbol @xmath7 for this transformed variable .",
    "4 .   _ estimating the joint density of @xmath108 . _",
    "+ based on the _ weighted _ sample @xmath109 with weights @xmath72 , approximate the density function of the random vector @xmath68 by a multivariate normal mixture : @xmath110 where @xmath111 and @xmath112 are the mean vector and covariance matrix of the @xmath71th mixture component .",
    "+ we use a normal - kernel density estimator with two tuning parameters : one is bandwidth , which is a standard tuning parameter ; the other is a localization parameter , which specifies how large a fraction of the entire sample is used to calculate the covariance matrix of each mixture component .",
    "these two tuning parameters are determined simultaneously by a likelihood - based optimization procedure . because this step is a completely modular component in the entire algorithm ( and alternative normal - kernel density estimators",
    "may be used without any change to other parts of the algorithm ) , we refer the reader to @xcite for details . 5 .   _ conditioning on the data . _",
    "+ for @xmath90 , partition @xmath112 as @xmath113 { \\boldsymbol{v}}_{{\\ensuremath{\\theta}}{\\ensuremath{\\theta}}i } & { \\boldsymbol{v}}_{{\\ensuremath{\\theta}}{z}i}\\cr\\- { \\boldsymbol{v}}_{{z}{\\ensuremath{\\theta}}i } & { \\boldsymbol{v}}_{{z}{z}i}\\cr \\end{pmat } , $ ] where @xmath114 is the covariance matrix of @xmath15 , @xmath115 is the covariance matrix of @xmath10 , and @xmath116 as well as @xmath117 are cross - covariance matrices . similarly , decompose @xmath111 into @xmath118 and @xmath119 , representing the @xmath15 part and @xmath10 part , respectively .",
    "use the notations @xmath74 , @xmath75 , @xmath80 , and @xmath120 as defined for relation  ( [ eq : mixture - conditional ] ) .",
    "then @xmath121 and @xmath122 where @xmath123 here @xmath7 is the observed forward data . substituting ( [ eq : mixture - z - marg])-([eq : mixture - anchor - cond ] ) into  ( [ eq : mixture - conditional ] ) , we get the conditional density @xmath124 the posterior approximation @xmath86",
    "is now updated to @xmath125 .",
    "this completes an iteration of the algorithm .",
    "note @xmath87 is a normal mixture , just like @xmath86 , and is ready to be updated in the next iteration .",
    "suppose the combined model and measurement error in @xmath7 is additive , as represented in  ( [ eq : additive - error ] ) .",
    "assume @xmath28 is independent of @xmath15 , @xmath20 , and @xmath10 .",
    "further assume the error is normal , @xmath126 .",
    "then the @xmath115 in ( [ eq : mixture - z - marg ] ) and  ( [ eq : mixture - cond - par ] ) should be replaced by @xmath127 .",
    "the algorithm is able to accommodate more general errors .",
    "for example , non - additive errors and errors that might occur in certain internal steps of the forward model @xmath8 can be built into the forward evaluation @xmath96 of step  2 by introducing randomness according to a description of the errors .",
    "this way , multiple forms of errors can be accommodated .",
    "an overall , explicit formula for the errors as a whole is not required .",
    "the passage from @xmath15 to @xmath7 in steps  12 embodies our best knowledge of the parameter - data connection : part in the anchor parameterization @xmath50 , part in the forward model @xmath60 , and part in errors in our implementation of @xmath8 and in the measurement of @xmath7 .",
    "going one step further , it is possible to relax the requirement that the forward model @xmath8 be `` deterministic '' . if the forward model is stochastic ( see an example in * ? ? ? * ) , the parameter - data connection , as depicted in  ( [ eq : anchor - data - randomness ] ) , includes one more layer of randomness .",
    "as far as the algorithm is concerned , the situation is not different from inclusion of model errors .      the overall idea in this algorithm may be summarized as `` simulate @xmath128 , then condition on the observed @xmath7 . ''",
    "the algorithm does not require one to be able to draw a random sample from the prior of @xmath15 .",
    "instead , a convenient initial approximation @xmath83 starts the procedure .",
    "one only needs to be able to evaluate the prior density at any particular value of @xmath15 .",
    "this provides flexibilities in specifying both the prior @xmath58 and the initial approximation @xmath83 .",
    "some statistics of the distribution @xmath87 may be examined semi - analytically , taking advantage of its being a normal mixture .",
    "more often , one is interested in the unknown field @xmath20 or a function thereof rather than in the parameter @xmath15 .",
    "these may be investigated based on a sample of @xmath20 based on the posterior of @xmath15 , as explained in section  [ sec : anchor - parameterization ] .",
    "typically , by far the most expensive operation in this algorithm is evaluating the forward model @xmath60 .",
    "this occurs once for each sampled value of @xmath15 .",
    "in contrast , sampling @xmath15 or generating field realizations are easy , because the distributions involved are normal or normal mixtures .",
    "the versatility of normal mixture in approximating complex densities is well documented @xcite .",
    "this approximation requires every component of @xmath68 to be a continuous variable defined on @xmath129 , hence some transformations may be necessary in the definition of @xmath15 and @xmath10 .",
    "( see section  [ sec : examples ] for examples . )",
    "the dimension reduction achieved by principal component analysis in step  3 can be significant , for example with time series data .",
    "this facilitates the use of high - dimensional ( large @xmath100 ) data without worrying ( too much ) about the correlation between the data components .",
    "the dimension reduction is also a stability feature .",
    "for example , components of @xmath10 that are almost constant in the simulations will not cause trouble .",
    "the global structure of this algorithm bears similarities to that of @xcite , with important differences .",
    "while @xcite focuses on situations where the likelihood function is known , the algorithm here completely forgoes the requirement of a ( or the ) likelihood function .",
    "this distinction has far - reaching implications for the applicability of the proposed method .",
    "this is an important feature of the proposed algorithm : it does not require a likelihood function .",
    "this feature distinguishes itself from the markov chain monte carlo ( mcmc ) methods .",
    "there is a variant of mcmc called `` approximate bayesian computation ( abc ) '' ( see * ? ? ?",
    "* ) that applies mcmc in problems without known likelihood functions .",
    "abc uses a measure of the simulation - observation mismatch in lieu of likelihood . for multivariate data ,",
    "there are significant difficulties in how to define this measure .",
    "the algorithm described here is very different from abc .",
    "in fact , this algorithm is not intrinsically tied to the anchor parameterization or anchored inversion .",
    "it is a general algorithm for approximating the posterior distribution where the likelihood is unknown but the model ( which corresponds to the likelihood ) can be simulated .",
    "the definition of anchors in  ( [ eq : anchor - def ] ) is represented by the matrix @xmath35 .",
    "there are two aspects in the choice of anchors , namely the number of anchors ( the number of rows in @xmath35 ) and the specific definition of each anchor ( the content of each row of @xmath35 ) . in this section",
    "we develop a procedure that adjusts both aspects in an automatic , adaptive manner , taking advantage of the iterative nature of the algorithm .",
    "we partition the model domain ( or numerical grid ) into a number of ( equal- or unequal - sized ) subsets and define the mean value of @xmath17 in each subset an anchor .",
    "these anchors form a _ partitioning anchorset _ , or `` anchorset '' for short .",
    "each anchorset is defined by a unique matrix @xmath35 as introduced before .",
    "this definition is instrumental in the adaptive procedure , to be presented shortly , which examines a number of anchorsets and chooses the one that has the best predicted performance in the upcoming iteration .",
    "the choice of anchorset is a model selection problem , because any anchorset represents a particular parameterization , or `` model '' , for the field @xmath0 . to facilitate comparison of models ,",
    "we measure the performance of a model by a kind of integrated predictive likelihood defined as @xmath130 where @xmath131 is the approximate posterior density of @xmath15 obtained in the @xmath85th iteration .",
    "this measure is analogous to bayes factors @xcite or , in particular , `` posterior '' bayes factors @xcite .",
    "however , the current setting differs from common ( bayesian ) model selection in several ways .",
    "for example , the likelihood @xmath62 is unknown , and is being estimated ; the ultimate subject is not really the model parameter @xmath15 , but rather the field @xmath11 ; the algorithm is iterative , which makes use of the data @xmath7 in each iteration and corrects for the repeated use of data by importance weighting .    as an approximation , we assume the predictive distribution , @xmath132 , is normal . the predictions @xmath99 generated in step  2 of the algorithm is a random sample of this distribution . based on the sample mean @xmath133 and sample covariance matrix @xmath134 of @xmath99 , an approximation of @xmath135 could be computed by @xmath136 where @xmath7 is the observation .    the measure @xmath137 is subject to a fair amount of uncertainty arising from several sources .",
    "first , it is computed based on _ random samples _ @xmath89 and @xmath99 .",
    "second , each @xmath7 is a random draw conditional on its corresponding @xmath15 ( via a random draw of @xmath22 ) .",
    "third , the normal assumption for @xmath10 may not be fully justifiable .",
    "fourth , the sample size ( @xmath88 ) is rather moderate in view of the dimensions of the random variables ( @xmath37 and @xmath100 ) . in experiments we used @xmath88 in the thousands and @xmath138 in the tens or hundreds .",
    "in fact , when @xmath138 is relatively large ( say about 100 ) , it is not rare that the empirical covariance @xmath134 is not invertible . in view of these subtleties that harm the robustness of @xmath137 , we further simplify and take @xmath139\\biggiven \\overline{z[j ] } ,",
    "s^2_{z[j]}\\bigr ) , \\ ] ] where @xmath140 $ ] indicates the @xmath141th dimension of @xmath10 .",
    "this approximation ignores correlations between the components of @xmath10 .",
    "denote the anchorset at the beginning of the @xmath85th iteration by @xmath35 . towards the end of this iteration ( somewhere in step  5 ) ,",
    "we consider whether to switch to an alternative anchorset , say @xmath142 , in the next iteration . the decision to switch or not will be made after comparing the _ predicted performances _ of the model in the next iteration using anchors @xmath35 and @xmath142 , respectively .",
    "let the two performances be denoted by @xmath143 and @xmath144 , respectively .    the model  ( [ eq : field - prior ] ) , the prior  ( [ eq : anchor - prior ] ) , and the relation  ( [ eq : y - given - anchor ] ) suggest that @xmath56 conditional on @xmath15 has a normal distribution , which we denote by @xmath145 to stress the fact that it is derived from the `` prior '' relations .",
    "analogously , we write @xmath146 and @xmath147 .    the distribution @xmath148 induces a distribution for @xmath56 , which we denote by @xmath149 .",
    "in fact , the sampling in steps  12 of the @xmath85th iteration also generates a sample of the alternative anchors , @xmath150 , and the sample is a random sample of @xmath149 by the definition of @xmath149 .",
    "alternatively , we can consider the sample @xmath151 to be obtained via @xmath152 .",
    "suppose we continue to use @xmath35 in the @xmath153th iteration .",
    "in step  5 of the @xmath85th iteration , @xmath148 is updated to @xmath131 .",
    "the distribution @xmath131 will be used in steps  12 of the @xmath153th iteration to generate a random sample of @xmath10 , which is then used to calculate the performance indicator @xmath143 . due to the high computational cost of the forward model ,",
    "however , we do not want to actually generate this sample of @xmath10 before committing to using the anchors @xmath35 in the @xmath153th iteration .",
    "the idea then is to use the sample @xmath99 created in step  2 of the @xmath85th iteration , which is based on @xmath148 , as an importance sample from the distribution of @xmath10 that would be generated in step  2 of the @xmath153th iteration , which would be based on @xmath131 .",
    "the importance weights are proportional to @xmath154 , @xmath90 . this weighted sample of @xmath10 then provides weighted sample means @xmath155}$ ] and weighted sample variances @xmath156}$ ] to be used in  ( [ eq : log - int - likely ] ) for predicting @xmath143 .",
    "now suppose we switch to @xmath142 in the @xmath153th iteration .",
    "first , at the end of step  5 of the @xmath85th iteration , we need to obtain @xmath157 instead of @xmath131 .",
    "second , the distribution @xmath157 would give rise to a sample of @xmath10 in step  2 of the @xmath153th iteration , but we again would like to use the sample @xmath99 created in step  2 of the @xmath85th iteration as a _ weighted _ replacement .",
    "we address these two steps now .    1 .",
    "we would like to view the sample @xmath151 as an importance sample from @xmath158 , then we would be able to conduct steps  45 for @xmath56 ( instead of @xmath15 ) and obtain @xmath157 .",
    "the question is : what are the importance weights ?",
    "+ if the anchorsets @xmath35 and @xmath142 have no common anchors , then @xmath159 this suggests that the importance weights are proportional to @xmath160 .",
    "+ if @xmath35 and @xmath142 share some common anchors , then write @xmath161 and @xmath162 .",
    "let @xmath163 and @xmath164 denote the marginal and conditional distributions induced by @xmath148 .",
    "we have @xmath165 noticing that the sample @xmath151 is obtained via @xmath166 , the above suggests the importance weights are proportional to @xmath167 .",
    "+ relations ( [ eq : adapt - weight-1])([eq : adapt - weight-2 ] ) show that the weights are proportional to @xmath168 when @xmath151 are taken as an importance sample from @xmath158 .",
    "these are the same weights ( the @xmath72s ) had we continued to use @xmath15 in steps  45 . in other words ,",
    "the weights do not depend on the specific choice of @xmath142 . using these weights and the sample @xmath169 , steps  45 of the @xmath85th iteration produce @xmath157 .",
    "2 .   after deriving @xmath157",
    ", we would view @xmath170 , obtained in step  2 of the @xmath85th iteration , as an importance sample of the @xmath10 that would be generated starting with @xmath157 in steps  12 of the @xmath153th iteration .",
    "the importance weights are equal to those of @xmath151 obtained in the @xmath85th iteration as an importance sample from @xmath157 .",
    "+ if the anchorsets @xmath35 and @xmath142 have no common anchors , then @xmath171 this suggests the importance weights are proportional to @xmath172 + if @xmath35 and @xmath142 share some common anchors , then , using the notation introduced above , we have @xmath173 suggesting the importance weights are proportional to @xmath174 + the weights turn @xmath175 into a weighted sample in computing @xmath176}$ ] and @xmath177}$ ] for predicting @xmath178 by  ( [ eq : log - int - likely ] ) .    next , if it turns out that @xmath179 , we take @xmath157 as the output of the @xmath85th iteration , with the anchorset @xmath142 . otherwise , the output is @xmath131 with the anchorset @xmath35 .",
    "a remaining question is , how do we identify , in a way that is flexible and efficient , candidate alternative anchorsets ( @xmath142 s ) to examine ?",
    "suppose the current anchorset , represented by @xmath35 , contains @xmath180 anchors .",
    "recall the definition of anchorset : the model domain is partitioned into @xmath180 sub - regions ; the mean value of @xmath17 in each sub - region is designated an anchor .",
    "let us call each sub - region the `` support '' of the corresponding anchor .",
    "we consider @xmath180 alternative anchorsets , denoted by @xmath181 , each containing @xmath182 anchors .",
    "the anchorset @xmath183 , @xmath184 , is identical to the original anchorset @xmath35 except that the support of the @xmath141th anchor in @xmath35 is split into two sub - regions , defining two new anchors .",
    "the @xmath185 newly - created `` small '' anchors resulting from splitting each anchor in @xmath35 form an `` umbrella '' anchorset , denoted by @xmath142 , which defines anchor vector @xmath186 .",
    "similarly defined are anchor vectors @xmath187 ,",
    "@xmath188 , ... , @xmath189",
    ". it can be seen that each of @xmath15 , @xmath190 , ... , @xmath191 is a linear function of @xmath56 . in steps  35 of the algorithm",
    ", we estimate the density of @xmath56 .",
    "( to estimate @xmath192 , extract the sample @xmath193 , then use the weights @xmath194 in steps  35 . ) because the estimate is a normal mixture , the densities of @xmath15 , @xmath190 , ... , @xmath191 follow immediately . based on these densities",
    ", we predict the @xmath195 for each of these @xmath196 anchorsets and choose the anchorset that achieves the largest @xmath195 .",
    "steps  35 of the algorithm are then repeated to estimate @xmath87 for the chosen anchorset .    by this strategy ,",
    "steps 35 of the algorithm are performed twice in order to compare and choose from @xmath182 anchorsets , including the original and the alternatives .",
    "as iterations proceed , more alternative anchorsets are considered in each iteration as the size of ( number of anchors contained in ) the currently used anchorset grows .",
    "the computational cost , however , barely increases .",
    "this efficient strategy takes advantage of the fact that the weights @xmath194 remain the same for all anchorsets .    an obvious extension to this strategy",
    "allows an alternative anchorset to split more than one anchor ( say up to two ) in the original anchorset .",
    "this extension examines more alternative anchorsets without substantially increasing the computational cost , because the umbrella anchorset @xmath142 remains unchanged .",
    "as this adaptive procedure suggests , the inversion exercise begins with a low number of anchors and gradually increases the number of anchors in iterations .",
    "in particular , we may begin with an anchorset that bisects each dimension of the space ; that is , use 2 , 4 , and 8 anchors in 1d , 2d , and 3d spaces , respectively .",
    "this completely relieves the user from the burden of specifying an initial anchorset .",
    "more anchors are introduced in the iterations as more computation has been invested in the task .",
    "new anchors tend to be introduced in regions of the model domain where a more detailed representation of the @xmath17 field is predicted to bring about most improvement to the model s capability in reproducing the observation @xmath7 .",
    "the partition of the model domain by the anchorset is not uniform ",
    "it is tailored to the unknown field @xmath17 , the forward process @xmath8 , and existing anchors in an automatic , adaptive , and evolving fashion .      in steps 35 ,",
    "the original @xmath15 is replaced by the umbrella @xmath56 ( obtained from the sample @xmath197 via @xmath198 ) , which is usually twice as long as @xmath15 . at the end of step  5",
    ", @xmath199 is obtained .",
    "step  5 now does not conclude an iteration .",
    "display  ( [ eq : mixture - updated ] ) is followed by the following new step .    1 .   _ selecting an anchorset and deriving its distribution . _",
    "+ from the estimated density @xmath200 , given by  ( [ eq : mixture - updated ] ) , derive the density of each alternative anchorset as well as the original anchorset , then predict @xmath195 for each of these anchorsets . +",
    "pick the anchorset that achieves the largest @xmath195 . use this anchorset to repeat steps  35 and",
    "derive its density @xmath55 .",
    "this density becomes @xmath131 , where @xmath15 corresponds to the picked anchorset .",
    "+ this concludes an iteration of the algorithm .",
    "the mean vector @xmath44 and covariance matrix @xmath45 of the field may be parameterized in conventional geostatistical forms . because of the existence of anchors , sophisticated trend models , such as polynomials in spatial coordinates , are usually unnecessary .",
    "one may model the mean @xmath44 by a global constant : @xmath201 the stationary covariance may be parameterized as @xmath202 where @xmath203 is variance and @xmath204 contains parameters pertaining to range , smoothness , nugget , geometric anisotropy , and so on .",
    "we leave specifics of this parameterization open .",
    "see section  [ sec : geostat - par ] for an example of a particular form of this parameterization .",
    "a requirement imposed by the proposed inversion method is that all unknown parameters are defined on supports that can be mapped onto @xmath129 .",
    "this usually means that the support of each parameter should be in one of the following forms : @xmath205 , @xmath206 , @xmath207 , and @xmath208 .",
    "once this requirement is satisfied , all parameters are transformed to be defined on @xmath129 , hence the posterior distribution can be approximated by a normal mixture .",
    "note the support of each anchor is already @xmath129 because the support of @xmath11 is @xmath129 .",
    "let us denote the geostatistical parameters @xmath209 , @xmath203 , and @xmath204 , transformed onto @xmath129 , by @xmath190 and re - label the anchor parameters @xmath210 .",
    "the complete parameter vector is now @xmath211 .",
    "the parameterization of @xmath11 described in section  [ sec : anchor - parameterization ] , or ( [ eq : y - given - anchor ] ) in particular , now is written as @xmath39 , @xmath212 , in which @xmath190 determines @xmath44 and @xmath45 , and @xmath210 plays the role of the @xmath15 in  ( [ eq : y - given - anchor ] ) .",
    "the expanded parameter vector retains the simplicity of a geostatistical description of the random field . in the meantime it enables , via  ( [ eq : y - given - anchor ] ) , sophisticated mean and covariance structures for @xmath11 beyond the expressing capability of the geostatistical parameters alone . to some extent",
    ", one may say that the geostatistical parameters capture `` global '' features , whereas the anchor parameters capture `` local '' features .    a prior for @xmath15",
    "can be specified by @xmath213 where @xmath214 is a prior for the geostatistical parameters , and @xmath215 in which @xmath44 and @xmath45 are determined by @xmath190 .",
    "the posterior is now written as @xmath16 instead of @xmath55 .    in the algorithm in section  [ sec : inference ] , all the @xmath15 s should be understood as the expanded parameter vector , @xmath211 .",
    "the overall idea of the algorithm can be summarized as `` simulate @xmath216 , then condition on the observed @xmath7 . ''    in step  1 of the algorithm , the prior @xmath217 is now given by  ( [ eq : anchor - geostat - prior ] ) . in step  2",
    ", @xmath95 is now @xmath218 , in which @xmath219 contains both anchor parameters and geostatistical parameters , the latter providing @xmath44 and @xmath45 .    similarly in section  [ sec : choice - of - anchors ] , the @xmath15 and @xmath56 should be understood as the extended parameter vectors , and their priors are given by  ( [ eq : anchor - geostat - prior ] ) . in the meantime",
    "@xmath44 and @xmath45 should be dropped because they are now determined by @xmath190 .",
    "note that the @xmath190 part is not affected by the search for alternative @xmath210 , hence @xmath190 plays the role of the @xmath220 in ( [ eq : adapt - weight-2 ] ) , ( [ eq : adapt - weight-4 ] ) , and  ( [ eq : anchorsets - weights - alt ] ) .      besides the nonlinear data @xmath7 , one may have some direct measurements of the spatial attribute @xmath11 itself or , more generally , measurements of some linear function of @xmath11 . in the groundwater example in section  [ sec : intro ] ,",
    "linear data may include direct measurements of local - scale hydraulic conductivity and covariates such as grain - size distribution and core - support geophysical properties , which provide estimates of local hydraulic conductivity via empirical relations . in the geophysical example in section  [ sec : intro ]",
    ", linear data may be mechanic properties of the elastic medium at the ground surface . in the atmospheric example in section  [ sec : intro ] ,",
    "linear data may be direct monitoring of co@xmath13 sources on the ground surface or covariates that provide estimates of co@xmath13 sources .",
    "linear data enter the proposed inversion procedure at two places : ( 1 ) the prior of anchors , @xmath58 , not only relies on the mean @xmath44 and covariance @xmath45 , but is also conditioned on the linear data ; ( 2 ) while simulating field realizations conditional on values of the anchor parameters , the simulation is in addition conditioned on the linear data .",
    "the modified prior can be alternatively viewed as the prior @xmath58 , ignoring the linear data , multiplied by the likelihood of @xmath15 with respect to the linear data .",
    "let the linear data be denoted by @xmath221 .",
    "( naturally , @xmath222 shares no common row with @xmath35 , the definition of anchors . )",
    "if the geostatistical extension described in section  [ sec : geostat - extension ] is not used , the prior of @xmath15 given by  ( [ eq : anchor - prior ] ) is replaced by @xmath223 { \\ensuremath{\\theta}}\\\\ \\cr\\- \\ell \\cr\\end{pmat }      \\ ;      \\biggm|      \\ ;      \\begin{pmat } [ { } ] { { \\boldsymbol{h}}}\\\\ \\cr\\- { \\boldsymbol{l } } \\cr\\end{pmat } { { \\boldsymbol{\\mu}}},\\ ,      \\begin{pmat}[{| } ]          { { \\boldsymbol{h}}}{{\\boldsymbol{\\sigma}}}{\\ensuremath{{{{\\boldsymbol{h}}}}^t } } & { { \\boldsymbol{h}}}{{\\boldsymbol{\\sigma}}}{\\ensuremath{{{\\boldsymbol{l}}}^t } }          \\\\",
    "\\cr\\-          { \\boldsymbol{l } } { { \\boldsymbol{\\sigma}}}{\\ensuremath{{{{\\boldsymbol{h}}}}^t } } & { \\boldsymbol{l}}{{\\boldsymbol{\\sigma}}}{\\ensuremath{{{\\boldsymbol{l}}}^t } }          \\cr\\end{pmat }      \\biggr ) .\\ ] ] the conditional distribution of the field @xmath20 given the anchor values is now @xmath224 .",
    "this can be handled by combining @xmath15 and @xmath225 into a single set of linear condition and using a formula analogous to  ( [ eq : y - given - anchor ] ) .",
    "in this situation , the target of the exercise is the posterior @xmath226 .",
    "the overall idea of the algorithm can be summarized as `` simulate @xmath227 , then condition on the observed @xmath7 . ''",
    "if the geostatistical extension is in place , let us denote @xmath211 as in section  [ sec : geostat - extension ] .",
    "the prior in  ( [ eq : anchor - geostat - prior ] ) is replaced by @xmath228 the conditional @xmath229 is normal , analogous to  ( [ eq : anchor - linear - prior ] ) except that @xmath44 and @xmath45 are determined by @xmath190 .",
    "the conditional distribution of @xmath20 given the model parameter @xmath15 is conditioned on @xmath225 in addition to @xmath15 , that is , @xmath230 , where @xmath210 and @xmath225 are linear conditions , and @xmath190 provides @xmath44 and @xmath45 .",
    "this is again similar to  ( [ eq : y - given - anchor ] ) .",
    "in this situation , the target of the exercise is the posterior @xmath231 .",
    "the overall idea of the algorithm can be summarized as `` simulate @xmath232 , then condition on the observed @xmath7 . ''",
    "we will present three examples of scientific applications using synthetic data .",
    "the first two examples are in one - dimensional space , whereas the third is in two - dimensional space . in all three examples ,",
    "the unknown physical property of interest is positive by definition , hence we take @xmath11 to be the logarithm of the physical property , and treat @xmath11 as a gaussian process .",
    "the examples demonstrate a number of aspects of the anchored inversion methodology . in example  1 , the forward data are measurements distributed in space . in example  2 , the forward data are time - series measurements at a boundary point of the model domain . in example  3 , the forward data are neither time series nor attached to specific spatial locations .",
    "in fact , all components of the forward - data in the examples are functions of the entire field , even if their measurement is associated with specific locations or times .",
    "example  1 also uses a linear datum ( , a direct measurement of @xmath11 at one location ) . in example  2",
    ", the forward function is a composition of two separate field processes , giving two datasets ( to be combined to form the forward data vector ) that are of incomparable physical natures . in example  3",
    ", the inversion algorithm is executed in a numerical grid that is coarser than the synthetic true field , introducing systematic model error . in all examples ,",
    "there exists substantial correlation between components of the forward data ( variable @xmath233 ) .    in all the examples ,",
    "the data were assumed to be error - free .",
    "( in example  3 , we knew there was model error but chose to ignore it . ) because the true field is known in these synthetic examples , field simulations compared with the true field demonstrate the performance of the inversion .",
    "another assessment comes from comparing predictions of the forward data in simulated fields with the observed forward data .    in all examples ,",
    "the algorithm was run for 20 iterations with sample sizes ( the @xmath88s ) 2400 , 1950 , 1612 , ... , 610 , 608 , totaling 19176 samples ( which is also the total number of forward model runs ) .",
    "the initial anchors are designated by evenly bisecting each spatial dimension ( hence the algorithm starts with 2 and 4 anchors in 1-d and 2-d examples , respectively ) . as iteration proceeds ,",
    "anchor designations evolve automatically as described in section  [ sec : choice - of - anchors ] . in each iteration",
    "we considered splitting one of the existing anchors , hence the number of anchors increased by 0 or 1 in any single iteration .    before",
    "delving into individual examples , in sections  [ sec : geostat - par][sec : diagnostics ] we address some general preparations that are used in all examples . in these two sections ,",
    "both @xmath11 and @xmath10 are generic symbols for the spatial variable and forward data , respectively , as the actual meanings of both depend on the specific example .",
    "for all the examples , we used a conventional geostatistical formulation for the field mean @xmath44 and covariance @xmath45 .",
    "following the notation in section  [ sec : geostat - extension ] , the geostatistical parameters , anchor parameters , and the complete parameter vector are denoted by @xmath190 , @xmath210 , and @xmath234 , respectively .",
    "the same procedures were followed in all examples to specify the prior for @xmath190 and construct the initial approximation @xmath83 . recall that the methodology is open with respect to these elements .",
    "however , the specific choices made for these examples are sensible starting points in general applications .",
    "the parameterization of the field @xmath11 ( before conditioning on anchors ) includes a global mean @xmath209 and a covariance function @xmath235 in which @xmath203 is variance , @xmath236 is `` nugget '' , @xmath237 is the indicator function assuming the value 1 if its arguments is true and 0 otherwise , @xmath238 is `` range '' or `` scale '' , and @xmath239 is an isotropic correlation function .",
    "the particular form of @xmath239 adopted above is a special case of the matrn correlation function with fixed smoothness @xmath240 .",
    "this correlation function has been used by @xcite in modeling hydrodynamics .",
    "in - depth discussion on the matrn family of correlation functions can be found in @xcite . in sum ,",
    "the geostatistical formulation uses a parameter vector @xmath241 , in the natural unit of each component .",
    "( geometric anisotropy and the option of taking @xmath242 as an unknown parameter are also implemented . )",
    "the following prior is adopted for these parameters ( see @xcite for details ) : @xmath243    to use the proposed algorithm , each parameter component is transformed onto @xmath205 , leading to the geostatistical parameter vector @xmath244 the prior of @xmath190 is determined by the prior above ( which is in terms of the parameters in their `` natural '' units ) and the transformations .",
    "this prior is used in ( [ eq : anchor - geostat - prior ] ) or  ( [ eq : anchor - geostat - linear - prior ] ) ( depending on the presence or absence of linear data ) to specify a prior for the full model parameter @xmath15 .",
    "note that a prior for the anchor parameters , @xmath210 , is determined by ( [ eq : anchor - geostat - prior ] ) or  ( [ eq : anchor - geostat - linear - prior ] ) ; no user intervention is required .",
    "the initial approximation @xmath82 was taken to be a multivariate normal distribution that is fairly diffuse .",
    "roughly speaking , the integrated likelihood @xmath195 defined by  ( [ eq : log - int - likely ] ) measures how well the model reproduces , or predicts , the observations . here",
    ", `` reproduction '' refers to the output of the forward model evaluated at @xmath17 fields simulated according to the posterior distribution of @xmath15 . in real - world applications ,",
    "a comparison of such model predictions with the actual observations provides one of the more concrete assessments of the model .",
    "an increasing @xmath195 in iterations suggests the model is improving .",
    "hence @xmath195 can be monitored as a diagnostic .",
    "another summary of this prediction - observation comparison is as follows .",
    "consider the sample @xmath245 created in step  2 of the very first iteration ( that is , based on @xmath82 ) .",
    "for dimension @xmath141 of @xmath233 , where @xmath246 , compute the median absolute difference between the sample and the observation , and denote it by @xmath247}$ ] .",
    "later in the @xmath71th iteration , compute @xmath248}$ ] and obtain the ratio @xmath249 } = \\text{\\tt mad}^{(i)}_{[j ] } / \\text{\\tt mad}^{(0)}_{[j]}$ ] .",
    "this `` mad ratio '' for each dimension of @xmath233 is expected to decrease in iterations until it reaches a stable level . in each iteration",
    ", summaries such as the median and the maximum of the @xmath100 mad ratios are reported as diagnostics .    as a diagnostic ,",
    "the `` mad ratio '' is bounded below by 0 and rarely exceeds 1 .",
    "this makes it easier to interpret than @xmath195 .",
    "consider the groundwater flow example mentioned in section  [ sec : intro ] in one - dimensional space .",
    "denote hydraulic conductivity by @xmath250 ( ) and hydraulic head by @xmath251 ( m ) .",
    "we took @xmath252 ( because @xmath250 is positive ) and modeled @xmath11 as a one - dimensional gaussian process .",
    "the synthetic log - conductivity field is shown in figure  [ fig : darcy - field ] .",
    "the field is composed of 100 grid points ; this constitutes the model domain .    the one - dimensional , steady - state groundwater flow in saturated zone is described by the following differential equation @xcite : @xmath253 where @xmath254 is spatial coordinate .",
    "here we have assumed there is no water source or sink in the model domain ( but there may be water injection or extraction at the boundaries in order to maintain certain boundary conditions ) .",
    "both @xmath250 and @xmath251 vary with @xmath254 .",
    "we view @xmath250 as the `` controlling '' physical attribute and @xmath251 as the `` outcome '' . given the 1-d conductivity field @xmath250 , we can solve this equation to find the head @xmath251 everywhere in the model domain .",
    "the scientific question is to infer @xmath250 given measured @xmath251 at a number of locations .",
    "suppose @xmath251 at 30 locations in the field were available as forward data .",
    "the forward model @xmath8 involved transforming @xmath11 to @xmath255 , solving  ( [ eq : ex1-forward ] ) for @xmath251 under specified boundary conditions ( that is , @xmath256 at the left boundary and @xmath257 at the right boundary ) , and extracting the 30 values of @xmath251 at the measurement locations .",
    "the synthetic forward data ( figure  [ fig : darcy - forward ] ) are the output of the forward model given the synthetic field as input .",
    "noticing in  ( [ eq : ex1-forward ] ) that a scaling of @xmath250 ( or a shifting of @xmath17 ) does not change the resultant @xmath251 , we need at least one direct measurement of @xmath17 . the synthetic @xmath17 value at a randomly picked location , marked in figure  [ fig : darcy - field ] ,",
    "was made available to the inversion as linear data .",
    "some summaries and diagnostics of the result are listed in table  [ tab : ex1 ] . in the table",
    "one sees decisive increase in @xmath195 and decrease in mad ratios , as desired .",
    "overall , the number of anchors started at 2 and increased to 16 . because the geostatistical parameter had 4 components , the dimension of the model parameter @xmath15 started at 6 and increased to 20 .",
    "the adaptive nature of anchor selection in the algorithm is vividly depicted in figure  [ fig : darcy - anchor - layout ] , which shows the evolution of the anchorsets in the iterations .",
    "@rrrrrr iteration & sample size & anchors & @xmath195 & median @xmath258}$ ] & max @xmath258}$ ] + 1 & 2400 & 2 & -48.6 & 1.0000 & 1.000 + 2 & 1950 & 3 & -7.4 & 0.1829 & 0.346 + 3 & 1612 & 4 & 31.7 & 0.0354 & 0.188 + 4 & 1359 & 5 & 35.2 & 0.0402 & 0.146 + 5 & 1170 & 6 & 43.3 & 0.0290 & 0.138 + 6 & 1027 & 7 & 49.1 & 0.0270 & 0.086 + 7 & 920 & 8 & 72.9 & 0.0110 & 0.089 + 8 & 840 & 9 & 85.1 & 0.0081 & 0.063 + 9 & 780 & 10 & 84.0 & 0.0095 & 0.034 + 10 & 735 & 11 & 83.6 & 0.0091 & 0.034 + 11 & 701 & 11 & 70.0 & 0.0086 & 0.040 + 12 & 676 & 11 & 85.8 & 0.0096 & 0.025 + 13 & 657 & 11 & 90.4 & 0.0063 & 0.026 + 14 & 643 & 12 & 96.0 & 0.0064 & 0.022 + 15 & 632 & 13 & 99.2 & 0.0051 & 0.023 + 16 & 624 & 14 & 98.8 & 0.0052 & 0.021 + 17 & 618 & 15 & 103.3 & 0.0040 & 0.036 + 18 & 614 & 15 & 106.0 & 0.0037 & 0.037 + 19 & 610 & 15 & 101.5 & 0.0038 & 0.038 + 20 & 608 & 16 & 97.8 & 0.0037 & 0.038 +    figure  [ fig : darcy - anchor - bw ] shows the inferred marginal distribution of each anchor , as represented by a boxplot of the sample obtained in step  1 of the algorithm . since the anchorsets evolved in iterations , we do not see the `` convergence '' of the distribution of a fixed anchorset . because the true field is known , each anchor has a `` target value '' defined by the true @xmath17 values .",
    "these target values are indicated in figure  [ fig : darcy - anchor - bw ] .",
    "it is seen that the estimated distributions homed in on the target values convincingly after a few iterations .",
    "this strength of convergence for a reasonably large number of anchors suggests that the parameterization provides a good presentation of the field .",
    "the estimated marginal distributions of the geostatistical parameters are shown in figure  [ fig : darcy - geost - sample ] .",
    "it should be stressed that the meaning of the geostatistical parameters changes during the iterations whenever the anchorset changes .",
    "in addition , interactions between the geostatistical parameters cause difficulty in their inference ( see @xcite ) . compared to the anchor parameters ,",
    "the role of geostatistical parameters is more of an intermediate modeling device .",
    "recall that the goal of anchored inversion is to characterize the field @xmath17 .",
    "we created 1000 field realizations based on the anchorset and approximate posterior distribution in the final iteration ( @xmath259 in this particular example ) .",
    "the 5th , 25th , 50th , 75th , and 95th percentiles of the realizations for each grid point are plotted in figure  [ fig : darcy - quantile ] , overlaid with the synthetic field for comparison .",
    "the comparison confirms the inversion worked well .    in real - world applications",
    ", we do not know the true field , therefore can neither compare the field simulations with the truth nor compare the anchors with their target values .",
    "perhaps the most concrete assessment is provided by the capabilities of simulated fields to `` reproduce '' the forward data . in each iteration of the algorithm ,",
    "the sample @xmath260 obtained in step  2 represents `` predictions '' of the observation based on the posterior distribution estimated in the previous iteration .",
    "these predictions are compared with the actual observations in figure  [ fig : darcy - pred - by - iter ] . in each panel ( for one iteration ) , a vertical box - plot for each component of the @xmath233 predictions is drawn on the horizontal location where the observed value is .",
    "therefore , if the predictions reproduce the observations , the box - plots should fall centered on the @xmath261 line .",
    "the figure demonstrates that the reproduction starts severely biased and outspread in the initial iteration and , after steady adjustment of the mean and reduction of the variance in subsequent iterations , achieves precise match with the actual observation in the final iterations .",
    "runoff over the land surface generated during and after a rainfall event is affected by the hydraulic roughness of the land surface ( * ? ? ?",
    "* chap  6 ) . by affecting runoff",
    ", the roughness also exerts important influence on infiltration , erosion , and vegetative growth .",
    "however , detailed measurement of the roughness coefficient in a watershed is impractical . in many hydrological and agricultural models ,",
    "surface roughness is treated as a constant or as a categorical variable taking a few values based on the type of the land cover .",
    "@xcite investigate the effect of spatially heterogeneous roughness on the runoff process .",
    "they consider a one - dimensional overland plane model described by the following equation system : @xmath262 where @xmath254 is the spatial coordinate ( m ) , @xmath263 is the time ( s ) , @xmath264 is the flow depth ( m ) , @xmath265 is the flow discharge per unit width ( ) , @xmath266 is the effective rainfall ( ) , @xmath267 is the friction slope ( m / m ) , @xmath268 is the bed slope ( m / m ) , @xmath269 is the flow velocity ( ) , and @xmath270 is the manning roughness coefficient ( ) . in this system ,",
    "the rainfall input @xmath271 and the bed slope @xmath272 are considered known .",
    "in addition , the initial condition @xmath273 and boundary condition @xmath274 are assumed .",
    "these conditions state that the surface is dry at the beginning of the rainfall event , and there is no inflow at the upstream boundary .",
    "whereas @xcite investigate the resultant @xmath251 and @xmath275 under various configurations of synthetic roughness @xmath270 , we considered the inverse problem : infer the spatial field of @xmath270 given observations of @xmath251 and @xmath275 .",
    "the synthetic log - roughness field we used is shown in figure  [ fig : runoff - field ] .",
    "the field consists of 150 grid points .    to make it more interesting , we constructed two synthetic rainfall events and corresponding observation scenarios . in the first event",
    ", rainfall lasted for 60 minutes with a uniform intensity of 5 .",
    "the discharge at the downstream boundary was measured every 10 minutes for 3 hours ( that is , during the rain and in the subsequent 120 minutes ) . in the second event ,",
    "rainfall lasted for 30  minutes with a uniform intensity of 20 .",
    "the flowdepth at the downstream boundary was measured every 5 minutes for 4 hours ( that is , during the rain and in the subsequent 210 minutes ) .",
    "these two time - series datasets were combined to form a forward data vector of length 66 .",
    "the discharge and flowdepth processes at the downstream boundary in the two rain events , computed based on the synthetic roughness field , are depicted in figure  [ fig : runoff - forward ] .",
    "we took @xmath276 and modeled @xmath11 as a gaussian process . the forward function",
    "@xmath8 involved obtaining @xmath277 , solving the equation system  ( [ eq : ex2-forward ] ) for @xmath251 and @xmath275 based on each of the two rainfall scenarios , extracting @xmath275 ( or @xmath251 ) at the specified space and time coordinates , and taking the combined vector @xmath278 as the forward data @xmath7 .",
    "note here the forward data contained some discharge measurements and some flowdepth measurements .",
    "these two physical quantities are of incomparable natures .    in the iterative algorithm ,",
    "principal component analysis reduced the dimension of the forward data from 66 down to 919 .",
    "such significant dimension reduction was expected given the high level of auto - correlation in the time series data .",
    "performance of the inversion is demonstrated by 1000 field realizations created based on the final approximation to the posterior distribution of @xmath15 .",
    "representative percentiles of the simulated realizations are shown in figure  [ fig : runoff - quantile ] with comparison to the synthetic field , confirming the potential of the inversion method .",
    "the propagation of natural or man - made seismic waves through the earth is controlled by certain physical properties of the rock media along the path of the wave propagation .",
    "it is conventional to think of the media as constituting a `` velocity field '' , that is , each location is characterized by a velocity which is determined by relevant rock properties at that location .",
    "suppose seismic p - wave emanates from a source location and is detected at a receiver location , where the `` first - arrival traveltime '' of the wave is recorded .",
    "the ray path ( route of the wave propagation ) is a complicated function of the velocity field ; in particular , it is not a straight line where the velocity field is not uniform .",
    "the traveltime is determined by the velocities on the ray path from the source to the receiver .",
    "if one knows the velocity field , then both the ray path and the traveltime can be calculated @xcite .",
    "the inverse problem , namely inference of the velocity field given measurements of traveltimes , is called `` seismic tomography '' .",
    "first developed in medical imaging @xcite , the tomography technique has seen wide applications in fields such as geophysics @xcite and hydrology @xcite .",
    "we constructed a synthetic tomography problem in two - dimensional space .",
    "define `` slowness '' @xmath279 ( ) as the reciprocal of velocity .",
    "figure  [ fig : traveltime - field ] shows a synthetic log - slowness ( @xmath17 ) field ( a vertical profile dipping down the ground surface ) , discretized into a @xmath280 grid .",
    "suppose we conducted a tomography experiment with 6 sources and 10 receivers on each vertical boundary , and 15 receivers on the top boundary .",
    "waves from each source were detected by the receivers on the opposite vertical boundary and on the top boundary .",
    "the sources and receivers are marked in figure  [ fig : traveltime - field ] .",
    "such a layout of sources and receivers is typical of `` crosshole tomography '' @xcite .",
    "the wave propagation is described approximately by the eikonal equation ( * ? ? ?",
    "* appendix  c ) : @xmath281 where @xmath282 , @xmath283 are the spatial coordinates , @xmath284 is the traveltime from a designated source to @xmath285 , and @xmath286 is the slowness at location @xmath285 . to illustrate a solution of this system , the contours of first - arrival traveltimes in the synthetic field from a source located on the left boundary",
    "are shown in figure  [ fig : traveltime - forward ] .",
    "this figure clearly illustrates the curved paths of the seismic wave in the heterogeneous media .",
    "note that the wave travels faster in regions of lower slowness ( logically enough ! ) .    solving  ( [ eq : ex3-eikonal ] ) in the synthetic slowness field and taking logarithmic transform of the resultant traveltimes , we obtained @xmath287 synthetic traveltime measurements .",
    "the solution used the algorithm described by @xcite . to reduce the computational cost",
    ", we aggregated the synthetic field into a @xmath288 grid , that is , a fourth of the resolution of the synthetic field .",
    "this introduces _ model error _ : the synthetic forward model and the forward model used in inversion are not identical  they use different numerical grids .",
    "this error is directly analogous to what discretization does to the conceptually continuous field in real - world applications",
    ". it may not be trivial to characterize this error quantitatively . in this example we chose to ignore this model error .",
    "although in this particular case the error is small , the notion of ignoring the error is important in illustrating the principles of the anchored inversion methodology .",
    "in contrast , some other methods rely on the notion of `` error '' to construct regularization criteria or likelihood functions ( see section  [ sec : anchor - motivation ] ) .    in the iterations of the algorithm ,",
    "the dimension of the forward data was reduced from 300 to 25107 , thanks to the principal component analysis in step  3 of the algorithm .",
    "the algorithm started with 4 anchors , dividing the model domain into @xmath289 sub - domains .",
    "the parameterization evolved in the 20 iterations and arrived at an anchorset with 16 anchors , as depicted in figure  [ fig : traveltime - anchor - layout ] , which once again illustrates the adaptive nature of the algorithm in terms of automatically selecting an anchorset .",
    "based on the final approximation to the posterior distribution of the model parameter @xmath15 , we created 1000 field realizations ( on the @xmath288 grid ) .",
    "the point - wise median of these realizations is shown in figure  [ fig : traveltime - median ] .",
    "comparison with the synthetic field ( aggregated to the @xmath288 grid ) confirms that the inversion captured important features of the field .",
    "the mean of the point - wise standard deviations of the field realizations is 0.015 , whereas the mean of the point - wise median absolute deviations ( from the synthetic truth ) is 0.036 .",
    "these values should be considered with reference to the range of the synthetic field , which is @xmath290 .",
    "we have proposed a general method for modeling a spatial random field , as a gaussian process , that is tailored to making use of measurements of nonlinear functionals of the field .",
    "these nonlinear functionals are `` forward processes ( or models ) '' observed in experimental or natural settings .",
    "inverse of nonlinear forward processes is a major topic with wide scientific applications , as demonstrated in sections [ sec : intro ] and  [ sec : examples ] .",
    "the method revolves around certain deliberately chosen linear functionals of the field called `` anchors '' .",
    "the anchor parameterization achieves two goals .",
    "first , it reduces the dimensionality of the parameter space from that of the spatial field vector to that of the anchor vector ( plus possibly a small number of other parameters such as geostatistical parameters ) . as a result ,",
    "the dimensionality of the statistical inversion problem is separate from the spatial resolution of the forward model .",
    "second , the parameterization establishes a statistical connection between the model parameter ( anchors ) and the data ( forward process outcome ) .",
    "this statistical connection is known in mechanism and can be studied through simulations .",
    "the existence of this connection makes the inversion task a statistical one , whether or not the data contain model and measurement errors .",
    "this makes the method a fundamental departure from some existing approaches .",
    "although the framework has a bayesian flavor , it does not go down a typical bayesian computational path , because the likelihood function is unknown .",
    "the computational core lies in an iterative algorithm ( section  [ sec : inference ] ) centered on kernel density estimation .",
    "the algorithm is general and flexible in several aspects .",
    "first , it does not require a statistical analysis of the parameter - data connection , but only needs the ability to evaluate the forward model ( usually by running a numerical code ) , which is treated as a black box operation .",
    "although this study has focused on deterministic forward models , it is possible to accommodate stochastic ones .",
    "second , the algorithm is flexible with regard to the prior specification .",
    "third , the algorithm is able to accommodate flexible forms of errors ( section  [ sec : accommodate - errors ] ) .",
    "fourth , different forward models ( hence forward data ) could be used in different iterations .",
    "the last point , not elaborated in this article , makes it possible to assimilate multiple datasets sequentially or in other flexible ways .",
    "these features make the algorithm a general procedure for approximating the posterior distribution where the likelihood is unknown but the model ( which corresponds to the likelihood ) can be simulated .    a critical component of the method concerns selecting the anchor parameterization in an adaptive , automatic fashion as iteration proceeds ( section  [ sec : choice - of - anchors ] ) .",
    "the resultant anchor configuration is tailored to features of the field @xmath17 and the computational effort already invested .",
    "for example , if more iterations are performed , the algorithm in general will introduce more anchors to achieve more detailed representation of the field .",
    "we used several synthetic examples to demonstrate a number of features of the methodology , including the capabilities of using linear data ( example  1 ) , using highly correlated data ( all examples , especially examples 2 and  3 ) , assimilating multiple datasets of different physical natures ( example  2 ) , and ignoring model or measurement errors ( example  3 ) . in view of the dimensionality of the parameter vector ( in the tens ) and that of the data ( tens or hundreds )",
    ", the algorithm showed potential in efficiency .",
    "we have implemented the method in a ` r ` package named ` anchoredinversion ` .",
    "currently the package uses the packages ` randomfields ` @xcite for simulating random fields and ` mass ` ( its function ` mvrnorm ` ) @xcite for generating multivariate normal random numbers .",
    "the graphics in this article were created using the package ` lattice ` @xcite .",
    "h.  k.  h. lee , d.  m. higdon , z.  bi , m.  a.  r. ferreira , and m.  west .",
    "random field models for high - dimensional parameters in simultaitons of fluid flow in porous media .",
    "_ technometrics _ , 440 ( 3):0 230241 , 2002 .",
    "doi : 10.1198/004017002188618419 .",
    "g.  n. newsam and i.  g. enting .",
    "inverse problems in atmospheric constituent studies : i. determination of surface sources under a diffusive transport approximation . _ inverse problems _ , 4:0 10371054 , 1988 .",
    "g.  schoups , j.  a. vrugt , f.  fenicia , and n.  c. van  de giesen .",
    "corruption of accuracy and efficiency of markov chain monte carlo simulation by inaccurate numerical implementation of conceptual hydrologic models . _ water resour .",
    "_ , 46:0 w10530 , 2010 .",
    "doi : 10.1029/2009wr008648 .",
    "z.  zhang , d.  beletsky , d.  j. schwab , and m.  l. stein .",
    "assimilation of current measurements into a circulation model of lake michigan .",
    "_ water resour .",
    "_ , 43:0 w11407 , 2007 .",
    "doi : 10.1029/2006wr005818 .                ) sampled in step  2 of selected iterations in example  1 of section  [ sec : examples ] .",
    "the horizontal location of each boxplot is at the observed value of the corresponding forward datum .",
    "the straight reference lines are @xmath261 . ]"
  ],
  "abstract_text": [
    "<S> in a broad and fundamental type of `` inverse problems '' in science , one infers a spatially distributed physical attribute based on observations of processes that are controlled by the spatial attribute in question . </S>",
    "<S> the data - generating field processes , known as `` forward processes '' , are usually nonlinear with respect to the spatial attribute , and are often defined non - analytically by a numerical model . </S>",
    "<S> the data often contain a large number of elements with significant inter - correlation . </S>",
    "<S> we propose a general statistical method to tackle this problem . </S>",
    "<S> the method is centered on a parameterization device called `` anchors '' and an iterative algorithm for deriving the distribution of anchors conditional on the observed data . </S>",
    "<S> the algorithm draws upon techniques of importance sampling and multivariate kernel density estimation with weighted samples . </S>",
    "<S> anchors are selected automatically ; the selection evolves in iterations in a way that is tailored to important features of the attribute field . </S>",
    "<S> the method and the algorithm are general with respect to the scientific nature and technical details of the forward processes . </S>",
    "<S> conceptual and technical components render the method in contrast to standard approaches that are based on regularization or optimization </S>",
    "<S> . some important features of the proposed method are demonstrated by examples from the earth sciences , including groundwater flow , rainfall - runoff , and seismic tomography .    </S>",
    "<S> * key words * : adaptive model selection , dimension reduction , inverse problem , iterative algorithm , spatial statistics . </S>"
  ]
}