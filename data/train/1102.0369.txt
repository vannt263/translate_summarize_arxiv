{
  "article_text": [
    "consider a number of dispersed sensors , each one of which observes the path of a real - valued stochastic process .",
    "the joint distribution of these processes is assumed to belong to some parametric family .",
    "the goal is to estimate the unknown parameter at a central location ( _ fusion center _ ) that receives information from all sensors .",
    "when the sensors transmit their complete observations to the fusion center , we have a classical ( _ centralized _ ) parameter estimation problem .",
    "however , the fusion center often does not have full access to the sensor observations due to practical considerations , such as limited communication bandwidth .",
    "these communication constraints are present in applications such as mobile and wireless communication , data fusion , environmental monitoring and distributed surveillance , in which it is crucial to minimize the congestion in the network and the computational burden at the fusion center ( see , e.g. , foresti et al .",
    "@xcite ) .    under this setup , which is often called",
    "_ decentralized _ , each sensor needs to transmit a small number of bits per communication to the fusion center and it is clear that the classical ( centralized ) statistical techniques are no longer applicable . as a result , there has been a great interest in decentralized formulations of statistical problems ( see , e.g. , the review papers by viswanathan and varshney @xcite , blum et al .",
    "@xcite , han and amari  @xcite and veeravalli  @xcite ) .",
    "parameter estimation under a decentralized setup has been studied extensively using information - theoretic techniques .",
    "more specifically , it is often assumed that there are two correlated sensors , each of which observes a sequence of independent and identically distributed ( i.i.d . ) , finite - valued random variables whose joint probability mass function is determined by the unknown parameter .",
    "the sensors are then required to transmit to the fusion center messages that belong to alphabets of smaller size than those of the original observations .",
    "the review paper by han and amari  @xcite describes in detail the main advances in this line of research .",
    "on the other hand , luo  @xcite and xiao and luo  @xcite considered an arbitrary number of independent sensors that take i.i.d .",
    "observations with a common mean , which is the unknown parameter . assuming that the parameter space and the support of the noise distribution are both compact intervals , they constructed decentralized estimating schemes that require the transmission of a small number of bits per communication .",
    "in all the above papers , the sensors collect i.i.d .",
    "observations at a sequence of discrete times and transmit a small number of bits to the fusion center at every such sampling time .",
    "moreover , even under an asymptotically large horizon of observations , the resulting estimators have larger mean square errors than the corresponding optimal centralized estimators , which have full access to the sensor observations .    in this paper",
    "the goal is to construct a decentralized estimating scheme that requires minimal communication activity from the sensors _ and _ achieves asymptotically the mean square error of the optimal centralized estimator , under a general statistical model for the sensor observations . in particular , we assume that the sensors observe the paths of continuous semimartingales whose drifts are linear with respect to the unknown parameter .",
    "the centralized version of this problem is well understood . for gaussian processes with independent increments , the fixed - horizon maximum likelihood estimator ( mle ) was studied by grenander  @xcite and striebel  @xcite .",
    "brown and hewitt  @xcite proved that the mle is consistent and asymptotically normal for stationary and ergodic time - homogeneous diffusions .",
    "feigin  @xcite established the same properties for more general diffusions , assuming that the score process is a martingale .",
    "liptser and shiryaev ( @xcite , pages 225236 ) studied the mle for a diffusion - type process and computed its bias and variance in the ornstein  uhlenbeck case .",
    "for a diffusion - type process with linear drift with respect to the unknown parameter , liptser and shiryaev  @xcite , pages 244248 , and earlier novikov  @xcite , suggested a  _ sequential _ version of the mle and proved that it is unbiased and that it attains a prescribed accuracy . in the particular case of a square root diffusion , brown and hewitt @xcite suggested an alternative sequential estimator with similar optimality properties .",
    "melnikov and novikov  @xcite and galtchouk and konev  @xcite studied least - squares sequential estimators that attain a prescribed accuracy in a multidimensional semimartingale regression model , generalizing in this way the results of novikov  @xcite .",
    "we refer to kutoyants @xcite and rao  @xcite for exhaustive references in the statistical inference of diffusion and diffusion - type processes .",
    "apart from the statistical model for the sensor observations , our work differs from previous approaches in some other important aspects as well .",
    "first of all , we do not assume that the frequency with which a sensor transmits its messages to the fusion center ( _ communication rate _ ) is the same as the frequency with which it collects its local observations ( _ sampling rate _ ) .",
    "instead , we assume that the sensors observe their underlying processes continuously , but communicate with the fusion center at discrete times .",
    "therefore , in our context , the incurred loss of information is not only due to the quantization of sensor observations , but also due to the discrete transmission of messages to the fusion center in comparison to the continuous flow of information at the sensors .",
    "moreover , we do not require that the sensors communicate with the fusion center at deterministic and equidistant times .",
    "instead , we allow each sensor to transmit its messages to the fusion center at random times that are triggered by its local observations .",
    "in particular , we propose a communication scheme according to which the sensors transmit only _ one - bit _ messages at first exit times of appropriate , locally - observed statistics ( see rabi et al .  @xcite and fellouris and moustakides @xcite for similar communication schemes in different decentralized problems ) .",
    "based on this communication scheme , we construct an estimator that is always consistent , even when the sensor processes are dependent .    however , the main result of this paper is that , in certain cases , the asymptotic distribution of the proposed estimator is the same as the exact distribution of the corresponding optimal centralized estimator .",
    "in particular , this holds when the sensor processes are arbitrary , orthogonal continuous semimartingales , as well as when they are correlated gaussian processes with independent increments .",
    "more importantly , these asymptotic properties are established as the horizon of observations goes to infinity and as the rate of communication between sensors and the fusion center goes to _",
    "zero_. thus , although the proposed estimator is statistically efficient , it requires minimal communication activity from the sensors , which is a very desirable property in applications with severe communication constraints .",
    "finally , we consider in more detail the special case in which the sensors observe independent brownian motions , since the tractability of this model allows us to obtain additional insight regarding the suggested estimating scheme . in this context , we also consider the case of discrete sampling , where the sensors do not observe their underlying processes continuously , but at a sequence of discrete times .",
    "it is shown that the proposed estimator remains consistent for any fixed sampling frequency , as long as the sensors have an asymptotically low rate of communication with the fusion center . however , asymptotic optimality does require a sufficiently high sampling rate , which we determine as a function of the communication rate and the observation horizon .",
    "the rest of the paper is organized as follows : in section  [ sec2 ] we formulate the problem under consideration . in section  [ sec3 ]",
    "we specify the proposed estimating scheme and analyze its asymptotic properties . in section  [ sec4 ]",
    "we focus on the special case that the sensors observe independent brownian motions .",
    "we conclude in section  [ sec5 ] .",
    "in what follows , we denote by @xmath0 the generic sensor , where @xmath1 .",
    "we assume that sensor @xmath0 observes the path of a continuous stochastic process @xmath2 and is able to compute any statistic that is adapted to the filtration generated by @xmath3 .    in this section",
    "we specify the dynamics of @xmath4 under a family of probability measures @xmath5 , we review standard results regarding the centralized estimation of the unknown parameter @xmath6 and we define the notion of an ( asymptotically optimal ) decentralized estimator .",
    "let @xmath4 be the coordinate process on the canonical space of continuous functions @xmath7 , where @xmath8 and @xmath9 is the associated borel @xmath10-algebra .",
    "we denote by @xmath11 the right - continuous version of the natural filtration generated by @xmath3 and by @xmath12 the corresponding global filtration @xmath13 let also @xmath14 be a probability measure on @xmath7 so that @xmath15 where @xmath16 is the class of continuous @xmath14-local martingales that start from 0 .    for every @xmath17 , we denote by @xmath18 the quadratic covariation of @xmath3 and @xmath19 and we assume that @xmath20 is an @xmath11-progressively measurable process so that @xmath21 then , we can define the stochastic integral @xmath22 and we denote by @xmath23 its quadratic variation , that is , @xmath24 moreover , we assume that the novikov - type condition :    @xmath25 < \\infty\\qquad\\forall0 \\leq t < \\infty\\ ] ]    is satisfied for every @xmath26 , which allows us to define for every @xmath26 the probability measure @xmath27 in the following way : @xmath28 then , if we denote by @xmath29 the class of continuous @xmath27-local martingales that start from 0 , girsanov s theorem ( see @xcite , page 331 ) implies that @xmath30 and , consequently , @xmath31 for every @xmath32 . therefore , from ( [ b ] ) and",
    "( [ n ] ) it follows that under @xmath27 @xmath33      the goal is to estimate the unknown parameter @xmath34 using the information that is being transmitted from the sensors to the fusion center",
    ". the flow of this information can be described by a sub - filtration of @xmath12 and is determined by the _ communication scheme _ that is chosen by the statistician .",
    "let @xmath35 be the fusion center filtration .",
    "we will say that :    @xmath36 is a _",
    "fixed - horizon _",
    ", @xmath37-adapted estimator of @xmath34 , if @xmath38 is a @xmath39-measurable statistic for every @xmath40 .",
    "@xmath41 is a _ sequential _ , @xmath42-adapted estimator of @xmath34 , if @xmath43 is an increasing family of @xmath37-stopping times and @xmath44 a @xmath45-measurable statistic for every @xmath46 .",
    "we will say that a @xmath42-adapted estimator , either fixed - horizon or sequential , is _ decentralized _ , when the fusion center filtration @xmath42 is of the form @xmath47 where @xmath48 is an increasing sequence of @xmath11-stopping times and each @xmath49 is an @xmath50-measurable statistic that takes values in a _",
    "finite _ set .",
    "in other words , a decentralized estimator must rely on quantized versions of the sensor observations , which may be transmitted to the fusion center at stopping times of the local sensor filtrations .",
    "if the fusion center learns the complete sensor observations at any time @xmath51 , then it can construct @xmath12-adapted estimators , which we will call _ centralized_. assuming that for every @xmath52 ,    @xmath53    the centralized , fixed - horizon mle of @xmath34 at some time @xmath40 is @xmath54 that is , the maximizer of the corresponding log - likelihood function , @xmath55 from ( [ loglikeli ] ) we also obtain the corresponding score process and ( observed ) fisher information , that is , @xmath56 and , consequently , we have @xmath57 moreover , from ( [ b ] ) , ( [ a ] ) and ( [ model ] ) it follows that @xmath58 , since @xmath59 since @xmath60 , if we also assume that for every @xmath61    @xmath62    then there exists a @xmath27-brownian motion @xmath63 ( see  @xcite , page 174 ) so that @xmath64 this representation has some important consequences , which we state in the following lemma .",
    "[ propo1 ] if @xmath65 is an increasing family of ( possibly random ) times so that @xmath66 @xmath27-a.s .",
    ", then @xmath67 @xmath68-a.s . as @xmath69 .    if @xmath70 are @xmath71-stopping times so that @xmath72",
    "< \\infty$ ] , then @xmath73 & = & { { { \\mathsf e}}_{\\lambda}}[m_{t_{2}}]=0 , \\\\ \\label{wald2 } { { { \\mathsf e}}_{\\lambda}}\\bigl[(m_{t_{2}}-m_{t_{1}})^{2}\\bigr]&= & { { { \\mathsf e}}_{\\lambda}}[a_{t_{2}}-a_{t_{1}}].\\end{aligned}\\ ] ]    if @xmath74 is deterministic , then @xmath75    part ( a ) is a consequence of ( [ deco2 ] ) , ( [ dds ] ) and the strong law of large numbers for the brownian motion . part ( b ) follows from a localization argument , optional sampling theorem and doob s maximal inequality .",
    "finally , when @xmath74 is deterministic , from ( [ dds ] ) it follows that @xmath76 for every @xmath77 . from this observation and ( [ deco2 ] ) we obtain ( [ asydet ] ) .    in the following lemma we state a version of the cramer ",
    "wolfowitz inequality .",
    "[ crw ] if @xmath78 is an @xmath12-stopping time and @xmath79 is an @xmath80-measurable statistic so that @xmath81<\\infty$ ] and @xmath82=\\lambda , { { \\mathsf v}}_{\\lambda } [ \\phi]<\\infty$ ] for every @xmath61 , then @xmath83 \\geq\\frac{1}{{{{\\mathsf e}}_{\\lambda}}[a_{t}]}.\\ ] ]    from ( [ wald1 ] ) and ( [ wald2 ] ) and the cauchy ",
    "schwarz inequality we have @xmath84 = { { { \\mathsf e}}_{\\lambda}}\\bigl[(\\phi-\\lambda ) m_{t } \\bigr ] \\leq\\sqrt { { { { \\mathsf e}}_{\\lambda}}\\bigl[(\\phi-\\lambda)^{2}\\bigr ] { { { \\mathsf e}}_{\\lambda}}\\bigl[(m_{t})^{2 } \\bigr ] } = \\sqrt { { { \\mathsf v}}_{\\lambda}[\\phi ] { { { \\mathsf e}}_{\\lambda}}[a_{t}]}.\\ ] ] thus , it suffices to show that @xmath85 = 1 $ ] .",
    "indeed , changing the measure @xmath86 and differentiating both sides in @xmath82=\\lambda$ ] with respect to @xmath34 , @xmath87 = { { \\mathsf e}}_{0 } \\bigl [ e^{\\lambda b_{t}- ( \\lambda^{2}/2 ) a_{t } } m_{t } \\phi \\bigr ] = { { { \\mathsf e}}_{\\lambda } } [ m_{t } \\phi].\\ ] ] the second equality follows from interchanging derivative and expectation , which is possible due to the ( quadratic ) form of the log - likelihood function ( [ loglikeli ] ) ( see , e.g. , @xcite , page 54 ) .",
    "lemma  [ crw ] and ( [ asydet ] ) imply that when @xmath88 is deterministic , @xmath89 is an optimal estimator of @xmath6 , in the sense that it has the smallest possible variance among @xmath90-measurable , unbiased estimators ( for any fixed @xmath77 ) . in order to obtain such an exact optimality property",
    "when @xmath74 is random , we consider the following sequential version of the centralized mle : @xmath91    [ coll ] for every @xmath92 , @xmath93 moreover , @xmath94 as @xmath95 .    assumption ( [ equa3 ] ) implies ( [ no ] ) . since @xmath23 has continuous paths , @xmath96 .",
    "thus , from ( [ dds ] ) we have @xmath97 and , consequently , from ( [ asydet ] ) we obtain ( [ asyran ] ) .",
    "finally , the strong consistency of @xmath98 as @xmath95 is implied by lemma  [ propo1](a ) .    from lemmas",
    "[ crw ] and  [ coll ] it follows that , for any given @xmath92 , @xmath98 is an optimal estimator of @xmath34 , in the sense that it has the smallest possible variance among unbiased , @xmath12-adapted estimators @xmath99 for which @xmath100 \\leq\\gamma$ ] .",
    "therefore , there is always a centralized estimator of @xmath34 that is unbiased , normally distributed and optimal in a _",
    "nonasymptotic _ sense .",
    "a decentralized estimator can not have such a strong optimality property , as it relies on less information .",
    "however , we will say that a ( decentralized ) estimator is _ asymptotically optimal _ , if it has the same distribution as the corresponding optimal centralized estimator when an asymptotically large horizon of observations is available . more specifically ,",
    "when @xmath74 is deterministic , a fixed - horizon , @xmath101-adapted estimator @xmath36 will be _ asymptotically optimal _ if @xmath102    when @xmath74 is random , a sequential , @xmath37-adapted estimator @xmath41 will be _ asymptotically optimal _",
    "if @xmath103 -\\gamma \\bigr ) \\leq0 \\quad\\mbox{and}\\quad \\sqrt{\\gamma } ( \\phi_{\\gamma}- \\lambda ) \\rightarrow{\\mathcal{n}}(0,1 ) \\qquad\\mbox{as } \\gamma\\rightarrow\\infty.\\ ] ]      we close this section with some notation that will be useful in the construction and analysis of the proposed estimating scheme .",
    "thus , for every @xmath104 we define the statistic @xmath105 and for any @xmath106 we denote by @xmath107 the quadratic covariation of @xmath108 and @xmath109 and by @xmath110 the quadratic variation of @xmath108 , that is , @xmath111 \\label{ai } a^{i}_{t } & : = & \\bigl\\langle b^{i } , b^{i } \\bigr\\rangle_{t}= \\int_{0}^{t } \\bigl(x_{s}^{i}\\bigr)^{2 } \\,{\\mathrm{d}}\\bigl\\langle y^{i } , y^{i } \\bigr\\rangle_{s},\\qquad t \\geq0.\\end{aligned}\\ ] ] then , recalling the definitions of @xmath112 and @xmath23 in ( [ b ] ) and ( [ a ] ) , we have @xmath113 moreover , we define the set @xmath114 and we have the following representation for @xmath23 : @xmath115",
    "in this section we construct and analyze the proposed decentralized estimator .",
    "more specifically , we first define the communication scheme at the sensors and then introduce the statistics and estimators that will be used by the fusion center . as in the centralized setup , we distinguish two cases and consider a fixed - horizon estimator when @xmath116 is deterministic and a sequential estimator when @xmath74 is random . in each case , we analyze the asymptotic behavior of the resulting estimator as the horizon of observations goes to infinity _ and _ the rate of communication goes to zero , assuming that conditions ( [ equa1 ] ) , ( [ equa2 ] ) , ( [ equa3 ] ) are satisfied .",
    "the main idea in the suggested communication scheme is that each sensor should inform the fusion center about the sufficient statistics for @xmath34 that it observes locally .",
    "however , instead of communicating at deterministic times , its communication times should be triggered by its local observations .",
    "in other words , each sensor @xmath0 should inform the fusion center about the evolution of the @xmath11-adapted , sufficient statistics for @xmath34 at a sequence of @xmath117-stopping times .    when @xmath23 is deterministic , @xmath118 are the only sufficient statistics for @xmath34 and it is clear that each @xmath108 is @xmath11-adapted , thus observable at sensor @xmath0 .    when @xmath23 is random , there are additional sufficient statistics , the _ random _ processes of the form @xmath110 or @xmath107 ( when @xmath110 or @xmath107 is deterministic , it is completely known to the fusion center at any time @xmath51 ) . if @xmath110 is random , it is clear that it is @xmath11-adapted , since @xmath119 . on the other hand , if @xmath107 ( with @xmath32 ) is random , it is not locally observed either at sensor @xmath0 or at sensor @xmath120 , thus , the fusion center can not be informed about its evolution ( since there is no communication between sensors ) .",
    "based on the previous discussion , we suggest that each sensor @xmath0 communicate with the fusion center at the times @xmath121 and , _ if @xmath23 and @xmath110 are random _ , also at the times @xmath122 where @xmath123 and @xmath124 are arbitrary , constant thresholds , chosen by the designer of the scheme , known both at sensor @xmath0 and the fusion center",
    ". if either @xmath110 or @xmath23 is deterministic , sensor @xmath0 does not communicate at the times @xmath125 and we set @xmath126 for every @xmath127 .    at @xmath128 , sensor @xmath0 transmits to the fusion center with one bit the outcome of the bernoulli random variable @xmath129 whereas at @xmath130 , if needed , it informs the fusion center with one bit that @xmath110 has increased by @xmath131 since @xmath132 .",
    "therefore , the induced filtration at the fusion center is @xmath133 which means that the fusion center can compute any @xmath134-adapted statistic .    for every @xmath104 we define @xmath135,\\qquad \\tau_{n}^{i ,",
    "b } \\leq t < \\tau_{n+1}^{i , b},\\qquad n \\geq1,\\end{aligned}\\ ] ] where @xmath136 for @xmath137 , with the understanding that @xmath138 when @xmath110 is deterministic . moreover , motivated by ( [ iso])([is ] )",
    ", we define @xmath139\\\\[-8pt ] & = & \\sum_{i=1}^{k } ( 1+d_{i } ) { \\tilde{a}}^{i } + \\sum_{(i , j ) \\notin{\\mathcal{d } } } a^{ij } , \\nonumber\\end{aligned}\\ ] ] where @xmath140 is the number of random terms of the form @xmath107 , that is , @xmath141 again , we set @xmath142 when @xmath23 is deterministic . finally , we define the following quantities : @xmath143 which will play an important role in the asymptotic analysis of the proposed estimating scheme .",
    "[ lemma0 ] for every @xmath104 and @xmath144 , @xmath145    if @xmath110 , @xmath23 are deterministic , then @xmath138 , @xmath142 and the corresponding inequalities hold trivially .",
    "thus , without loss of generality , we assume that both @xmath110 and @xmath23 are random .",
    "first of all , we observe that @xmath146 is exactly equal to @xmath108 at @xmath128 and @xmath147 is exactly equal to @xmath110 at @xmath130 for every @xmath148 . indeed , due to the path continuity of @xmath110 and  @xmath108 , for every @xmath148 it is @xmath149 = a^{i}_{\\tau_{n}^{i , a } } , \\\\ { \\tilde{b}}^{i}_{\\tau_{n}^{i ,",
    "b } } & = & \\sum_{j=1}^{n } \\bigl[\\overline{\\delta}{}^{i}z_{j}^{i}- \\underline { \\delta}^{i } \\bigl(1-z_{j}^{i}\\bigr)\\bigr]= \\sum _ { j=1}^{n}\\bigl[b^{i}_{\\tau_{j}^{i , b}}- b^{i}_{\\tau _ { j-1}^{i , b}}\\bigr]= b^{i}_{\\tau_{n}^{i , b}}.\\end{aligned}\\ ] ] moreover , from the definition of the communication times @xmath150 , it is clear that @xmath151 for any time @xmath51 between two jump times of @xmath146 , which proves the second inequality in ( [ dist ] ) .",
    "similarly , from the definition of @xmath152 and the fact that @xmath110 has increasing paths , it is clear that @xmath153 for any time @xmath51 between two jump times of @xmath147 , which proves the first inequality in ( [ dist ] ) .",
    "the second inequality in ( [ dist2 ] ) follows directly from the second inequality in ( [ dist ] ) and the definition of @xmath154 . finally , from the kunita ",
    "watanabe inequality ( see  @xcite , page 142 ) and the algebraic inequality @xmath155 we have @xmath156 thus , from the definitions of @xmath157 and @xmath140 [ recall ( [ set ] ) and ( [ di ] ) ] we obtain @xmath158 from the representation of @xmath23 in ( [ is ] ) and the latter inequality we have @xmath159 where the second inequality is due to ( [ dist ] ) and the equality follows from the definitions of @xmath160 and @xmath161 in ( [ ta ] ) and ( [ dc ] ) , respectively .      the proposed communication scheme requires the transmission of only one bit whenever a sensor communicates with the fusion center .",
    "thus , the overall communication activity in the network will be low as long as the communication rate of each sensor is low .",
    "therefore , we should ideally design an @xmath162-adapted estimator that is statistically efficient even under an asymptotically low communication rate as the horizon of observations goes to infinity . for this reason ,",
    "we let @xmath163 and @xmath164 as @xmath165 ( or @xmath69 ) and we determine the relative rates that guarantee consistency and asymptotic optimality .",
    "when @xmath74 is _ deterministic _ , we suggest the following estimator of @xmath34 at some arbitrary , deterministic time @xmath40 : @xmath166 in the following theorem , which is the first main result of this paper , we show that @xmath167 is consistent and asymptotically optimal under an asymptotically low communication rate .",
    "[ theo0 ] if @xmath168 so that @xmath169 , then @xmath170 converges to @xmath34 almost surely and in mean square .",
    "if additionally @xmath171 , then @xmath172 is asymptotically optimal , that is , @xmath173 .",
    "since @xmath89 converges to @xmath34 almost surely and in mean square as @xmath165 , in order to prove that @xmath172 is consistent , it suffices to show that @xmath174 and @xmath175 \\rightarrow0 $ ] as @xmath176 so that @xmath169 .",
    "moreover , since @xmath177 for any @xmath40 , in order to establish the asymptotic optimality of @xmath172 , it suffices to show that @xmath178 converges to 0 in probability as @xmath168 so that @xmath179 .",
    "indeed , from the second inequality in ( [ dist2 ] ) we have @xmath180 which proves both claims .    when @xmath74 is _ random",
    "_ , we suggest the following sequential , @xmath162-adapted estimator of @xmath34 : @xmath181    [ leme ] for any @xmath182 , @xmath161 such that @xmath183 , @xmath184 & \\leq&\\gamma.\\end{aligned}\\ ] ] moreover , if @xmath185 so that @xmath186 , then @xmath187 -\\gamma\\bigr ) \\leq0.\\ ] ]    from the first inequality in ( [ dist2 ] ) we have @xmath188 , therefore , @xmath189 from this inequality and ( [ no ] ) we obtain ( [ cs00 ] ) . moreover , since @xmath23 is the quadratic variation of @xmath112 , it has continuous and increasing paths , thus , from ( [ cs00 ] ) we obtain @xmath190",
    ". finally , from ( [ wald2 ] ) and ( [ cs0 ] ) we obtain @xmath191 = { { { \\mathsf e}}_{\\lambda}}[a_{{\\tilde{{\\mathcal{s}}}}_{\\gamma } } ] \\leq\\gamma,\\ ] ] which proves ( [ cs ] ) and implies ( [ sat ] ) .    in the following theorem",
    "we show that @xmath192 is a consistent estimator of @xmath34 , even under an asymptotically low communication rate .",
    "[ theo1 ] @xmath193 and @xmath194 \\rightarrow0 $ ] as @xmath195 so that @xmath196 .    recalling from ( [ score0 ] ) that @xmath197 , we have @xmath27-a.s .",
    "@xmath198 and , consequently , @xmath199 from the definition of @xmath200 it follows that @xmath201 , whereas from ( [ dist2 ] ) we have @xmath202 and @xmath203 .",
    "therefore , @xmath204 the first term in the right - hand side clearly goes to 0 as @xmath205 so that @xmath206 . moreover , from ( [ dds ] ) and ( [ cs0 ] ) we have @xmath27-a.s .",
    "@xmath207 if @xmath185 so that @xmath208 , @xmath209 , due to assumption ( [ equa3 ] )",
    ". therefore , the strong law of large numbers implies that the right - hand side in ( [ inemse2 ] ) converges to 0 and , consequently , @xmath193 as @xmath210 so that @xmath208 .",
    "moreover , if we square both sides in ( [ inemse ] ) , apply the algebraic inequality @xmath211 , take expectations and use ( [ cs ] ) , we obtain @xmath212 \\leq2 \\biggl ( \\frac { \\delta+ |\\lambda| c}{\\gamma - c } \\biggr)^{2 } + 2 \\frac { \\gamma}{(\\gamma - c)^{2}},\\ ] ] which implies that @xmath194 \\rightarrow0 $ ] as @xmath213 so that @xmath206 .",
    "the consistency of @xmath192 was established without any additional conditions on the dynamics of the sensor processes .",
    "however , it is clear that the suggested estimator can not be asymptotically efficient in such a general setup , since it does not have any access to sufficient statistics of the form @xmath107 with @xmath214 .    nevertheless , if every @xmath107 with @xmath32 is deterministic , then @xmath215 and the fusion center has access to all sufficient statistics for @xmath34 . in this case",
    ", we can obtain an asymptotically sharp lower bound for @xmath216 , the observed fisher information that is utilized by the proposed estimator , which allows us to establish its asymptotic optimality even under an asymptotically low communication rate .    if @xmath215 , then @xmath217 for every @xmath218 .",
    "consequently , for every @xmath219 such that @xmath220 , @xmath221 & \\leq & c.\\end{aligned}\\ ] ]    if @xmath215 , then @xmath222 for every @xmath104 , thus , from ( [ iso ] ) , ( [ ta ] ) and the first inequality in ( [ dist ] ) we obtain @xmath223 then , from the definition of @xmath200 we have @xmath224 , which proves ( [ cs20 ] ) .",
    "finally , from ( [ wald2 ] ) , ( [ path ] ) and ( [ cs20 ] ) we obtain @xmath225 = { { { \\mathsf e}}_{\\lambda}}[a_{{{\\mathcal{s}}}_{\\gamma}}- a_{{\\tilde{{\\mathcal{s}}}}_{\\gamma } } ] = { { { \\mathsf e}}_{\\lambda}}[\\gamma- a_{{\\tilde{{\\mathcal{s}}}}_{\\gamma } } ] \\leq c,\\ ] ] which completes the proof .",
    "[ theo2 ] if @xmath215 , then @xmath226 as @xmath227 so that @xmath228 .",
    "since @xmath229 for every @xmath92 , it suffices to show that @xmath230 so that @xmath231 . indeed , from ( [ deco2 ] ) and ( [ repre3 ] ) we have @xmath27-a.s .",
    "@xmath232 since @xmath233 and from ( [ dist2 ] ) we have @xmath202 and @xmath234 , @xmath235 the first term in the right - hand side of ( [ ww ] ) converges to 0 as @xmath236 so that @xmath237 .",
    "moreover , since @xmath238 and @xmath239 , @xmath240.\\nonumber\\end{aligned}\\ ] ] from the cauchy ",
    "schwarz inequality , ( [ cs ] ) and ( [ cs2 ] ) we have @xmath241 & \\leq & \\sqrt{{{{\\mathsf e}}_{\\lambda}}\\bigl[m^{2}_{{\\tilde{{\\mathcal{s}}}}_{\\gamma } } \\bigr ] } \\leq\\sqrt{\\gamma } , \\\\ { { { \\mathsf e}}_{\\lambda}}\\bigl[|m_{{\\tilde{{\\mathcal{s}}}}_{\\gamma } } - m_{{{\\mathcal{s}}}_{\\gamma}}|\\bigr ] & \\leq & \\sqrt{{{{\\mathsf e}}_{\\lambda}}\\bigl[(m_{{\\tilde{{\\mathcal{s}}}}_{\\gamma } } - m_{{{\\mathcal{s}}}_{\\gamma}})^{2}\\bigr ] } \\leq\\sqrt{c}.\\end{aligned}\\ ] ] then , taking expectations in ( [ www ] ) , we obtain @xmath242 \\leq \\frac{c}{\\gamma - c } + \\sqrt{\\frac{c}{\\gamma}}.\\ ] ] therefore , the second term in the right - hand side of ( [ ww ] ) converges to 0 in probability , due to markov s inequality , as @xmath236 so that @xmath186 .",
    "this concludes the proof .",
    "[ cor ] if @xmath215 , then @xmath243 is asymptotically optimal as @xmath244 so that @xmath228 .",
    "this is a consequence of ( [ sat ] ) and theorem  [ theo2 ] .      for the implementation of the proposed estimator",
    ", the fusion center does not need to record the values of the communication times .",
    "it simply needs to keep track of @xmath245 and  if necessary@xmath246 , and update them whenever it receives a relevant message .",
    "since these statistics are defined recursively , at most @xmath247 values need to be stored at any given time .",
    "theorems  [ theo0 ] ,  [ theo1 ] and  [ theo2 ] remain valid if @xmath161 and @xmath154 are held fixed as @xmath165 or @xmath69 .",
    "moreover , they remain valid if we use in the definitions of @xmath248 and @xmath130 time - varying , positive thresholds , @xmath249 , @xmath250 , @xmath251 , so that @xmath252 therefore , it may be possible to improve the performance of the proposed estimator by introducing linear or curved boundaries and optimizing over the additional parameters .",
    "we close this section with some examples that illustrate our main results .",
    "thus , let @xmath253 $ ] be an @xmath12-adapted , square matrix of size k , set @xmath254 , where @xmath255 is the transpose of @xmath256 , and consider the following special case of model ( [ model ] ) : @xmath257 where @xmath258 is a @xmath259-dimensional @xmath27-brownian motion .",
    "the observed fisher information @xmath74 then becomes @xmath260    in theorem  [ theo0 ] , we stated the asymptotic properties of the proposed estimator when @xmath88 is deterministic .",
    "this assumption is clearly satisfied when there are real functions @xmath261 so that @xmath262 and @xmath263 for every @xmath264 , in which case @xmath265 and @xmath4 is a gaussian process with independent increments .",
    "however , theorem  [ theo0 ] also applies when @xmath266 and @xmath267 , in which case a is still given by ( [ detfisher ] ) .    in theorem",
    "[ theo2 ] , we proved that the proposed estimator is asymptotically optimal when @xmath107 is deterministic for every @xmath268 .",
    "this condition is clearly satisfied when @xmath269 for every @xmath270 , in which case @xmath271 are independent , @xmath272 and ( [ model5 ] ) , ( [ detfisher2 ] ) become @xmath273 if , in particular , @xmath20 is a nonzero constant and @xmath274 , then @xmath3 is a square - root diffusion , whereas if @xmath275 and @xmath276 is a positive constant , then @xmath3 is an ornstein  uhlenbeck process .",
    "in this section we assume that @xmath277 , @xmath278 and @xmath279 , where @xmath280 is a known constant , for every @xmath281 and @xmath218 .",
    "thus , @xmath282 , @xmath283 , @xmath284 and ( [ model ] ) reduces to @xmath285 where @xmath286 are independent , standard brownian motions under @xmath27 .",
    "since the filtrations @xmath287 are independent , for every @xmath104 and @xmath288 we have @xmath289 we also assume , for simplicity , that @xmath290 for every @xmath291 , thus , @xmath292 and @xmath293 we denote by @xmath294 the time between the arrival of the @xmath295th and the @xmath296th message from sensor @xmath0 and by @xmath297 the number of transmitted messages by sensor @xmath0 up to time  @xmath51 , that is , @xmath298 since @xmath74 is deterministic , @xmath126 for every @xmath104 and @xmath299 and the fusion center filtration becomes @xmath300 moreover , @xmath142 and @xmath138 for every @xmath0 , however , we now define the following @xmath162-adapted statistics : @xmath301 that is , @xmath302 is an approximation of @xmath88 that relies only on the communication times @xmath303 .    since brownian motion `` restarts '' at stopping times ,",
    "each @xmath304 is a sequence of i.i.d .",
    "pairs , thus , each @xmath305 is a renewal process .",
    "moreover , it is possible to obtain a series representation for the joint density of the pair @xmath306 under  @xmath27 , @xmath307 this representation is the content of the following lemma , for which we need to define the following functions : @xmath308    [ p1 ] for every @xmath104 and @xmath40 , @xmath309    from ( [ t ] ) and ( [ renew ] ) we have @xmath310 since @xmath3 is a standard brownian motion under @xmath14 , it is well known ( see , e.g. ,  @xcite , page 99 ) that @xmath311 .",
    "then , changing the measure @xmath86 ( similarly , e.g. , to @xcite , page 196 ) , we obtain the desired result .",
    "the following lemma describes some properties of the communication scheme that remain valid in the case of discrete sampling at the sensors , which we treat in section  [ sec4.2 ] . in order to lighten the notation , we denote by @xmath312 a term that when divided by @xmath313 is asymptotically bounded from above and below as @xmath314 .",
    "[ p2 ] for any @xmath315 , @xmath316 & \\leq & \\frac{{{{\\mathsf e}}_{\\lambda}}[(\\delta _ { 1}^{i})^{2}]}{{{{\\mathsf e}}_{\\lambda}}[\\delta_{1}^{i } ] } , \\\\",
    "\\label{lor2 } { { { \\mathsf e}}_{\\lambda}}\\biggl[t- \\sum_{j=1}^{m_{t}^{i } } \\delta_{j}^{i } \\biggr ] & \\leq & \\frac{{{{\\mathsf e}}_{\\lambda}}[(\\delta_{1}^{i})^{2}]}{{{{\\mathsf e}}_{\\lambda}}[\\delta_{1}^{i}]}.\\end{aligned}\\ ] ]    as @xmath317 , @xmath318 = \\theta\\bigl ( \\delta^{i}\\bigr),\\qquad { { \\mathsf v}}_{\\lambda } \\bigl[\\delta_{1}^{i } \\bigr]= \\theta\\bigl(\\delta^{i}\\bigr ) , & \\\\ \\label{calos2 } & \\displaystyle 0 \\leq{{{\\mathsf e}}_{\\lambda}}\\bigl[a^{i}_{t}- \\check{a}^{i}_{t } \\bigr ] \\leq\\theta\\bigl(\\delta^{i}\\bigr ) , & \\\\ \\label{calos } & \\displaystyle { { { \\mathsf e}}_{\\lambda}}\\bigl[m_{t}^{i}\\bigr ] \\leq t/ \\theta\\bigl ( \\delta^{i}\\bigr ) + 1/ \\theta\\bigl(\\delta^{i}\\bigr).&\\end{aligned}\\ ] ]    \\(a ) since @xmath319 is a sequence of i.i.d .",
    "random variables , ( [ lor1 ] ) follows from theorem 1 in lorden @xcite and ( [ lor2 ] ) from lorden  @xcite , page 526 .",
    "\\(b ) recall from ( [ delt ] ) that @xmath320 is the first time a brownian motion with drift @xmath321 exits the symmetric interval @xmath322 . then , as @xmath323 , from wald s identity we have @xmath324 = \\frac{\\delta^{i}/ |x_{i}|}{|\\lambda x_{i}| } \\bigl(1+o(1)\\bigr),\\ ] ] whereas from martinsek  @xcite we have @xmath325 = \\frac{\\delta^{i}/|x_{i}|}{|\\lambda x_{i}|^{3 } } \\bigl(1+o(1)\\bigr).\\ ] ] then , from ( [ moments0 ] ) and ( [ moments ] ) we obtain ( [ rates ] ) , whereas from ( [ cai ] ) , ( [ lor2 ] ) and ( [ rates ] ) we obtain ( [ calos2 ] ) .    finally , since @xmath326 is a stopping time with respect to the filtration generated by the pairs @xmath327 , from wald s identity and ( [ lor1 ] ) we have @xmath328 { { { \\mathsf e}}_{\\lambda}}\\bigl [ \\delta_{1}^{i}\\bigr ] = { { { \\mathsf e}}_{\\lambda}}\\biggl [ \\sum _ { j=1}^{m_{t}^{i}+1 } \\delta_{j}^{i } \\biggr ] \\leq t+ \\frac{{{{\\mathsf e}}_{\\lambda}}[(\\delta^{i}_{1})^{2}]}{{{{\\mathsf e}}_{\\lambda}}[\\delta_{1}^{i}]}\\ ] ] and , consequently , @xmath329 \\leq\\frac{t}{{{{\\mathsf e}}_{\\lambda}}[\\delta_{1}^{i } ] } + \\frac { { { \\mathsf v}}_{\\lambda}[\\delta^{i}_{1}]}{({{{\\mathsf e}}_{\\lambda}}[\\delta_{1}^{i}])^{2}}.\\ ] ] from this inequality and ( [ rates ] ) we obtain ( [ calos ] ) , which completes the proof .",
    "let @xmath330 and @xmath331 be the likelihood and the log - likelihood function of @xmath34 that correspond to @xmath332 , the accumulated information at the fusion center up to time @xmath51 .",
    "the following proposition describes the structure of the corresponding score function .    for any @xmath333 , @xmath334",
    "-\\lambda a_{t } \\biggr\\ } + \\ { { \\tilde{b}}_{t } - \\lambda \\check{a}_{t } \\}.\\ ] ]    suppose that @xmath335 , that is , sensor @xmath0 has transmitted @xmath336 messages to the fusion center up to time @xmath51 , where @xmath336 is some nonnegative integer .",
    "then , since all pairs @xmath337 are independent , the fusion likelihood function has the following form : @xmath338 due to lemma  [ p1 ] , the corresponding log - likelihood function becomes @xmath339 z_{n}^{i } \\nonumber \\\\ & & { } + \\sum_{i=1}^{k } { \\mathbh{1}_{\\{m_{i}>0\\ } } } \\sum_{n=1}^{m_{i } } \\biggl [ -\\lambda \\delta^{i}- \\frac{(\\lambda x_{i } ) ^{2 } \\delta_{n}^{i}}{2 } + \\log g\\bigl ( \\delta_{n}^{i } ; \\delta^{i}/|x_{i}|\\bigr ) \\biggr ] \\bigl(1-z_{n}^{i } \\bigr).\\end{aligned}\\ ] ] then , recalling the definition of @xmath340 in ( [ tbi])([tb ] ) and of @xmath341 in ( [ cai ] ) , @xmath342 since @xmath343 , changing the measure @xmath86 , we have @xmath344\\ ] ] and , consequently , @xmath345}{{{\\mathsf p}}_{\\lambda } ( m_{t}^{i}=m_{i } ) } \\\\ & = & \\frac{{{{\\mathsf e}}_{\\lambda}}[b^{i}_{t } { \\mathbh{1}_{\\{m_{t}^{i}=m_{i}\\}}}]- \\lambda a^{i}_{t } { { \\mathsf p}}_{\\lambda } ( m_{t}^{i}=m_{i})}{{{\\mathsf p}}_{\\lambda } ( m_{t}^{i}=m_{i } ) } \\\\ & = & { { { \\mathsf e}}_{\\lambda}}\\bigl [ b^{i}_{t } | m_{t}^{i}=m_{i } \\bigr ] - \\lambda a^{i}_{t},\\end{aligned}\\ ] ] which implies ( [ score ] ) .    note that the second term in ( [ score ] ) reflects the information from the communication times and the transmitted messages , whereas the first term reflects the information between transmissions .    at time @xmath51",
    ", the fusion center should ideally estimate @xmath34 with the fusion center mle , that is , the root of the score function ( [ score ] ) .",
    "however , since @xmath346 $ ] does not admit a simple , closed - form expression as a function of @xmath34 , we can only approximate this conditional expectation and obtain an _ approximate _ fusion center mle .",
    "if we replace each @xmath347 $ ] with the corresponding unconditional expectation , @xmath348 = \\lambda a^{i}_{t}$ ] , the first term in ( [ score ] ) vanishes and we obtain the following estimator : @xmath349 on the other hand , if we approximate @xmath347 $ ] with @xmath350 , we recover the estimator @xmath167 that was defined in ( [ dbmled ] ) and whose asymptotic properties were established in theorem  [ theo0 ] . in the following proposition we show that , in the special brownian case that we consider in this section , @xmath351 has similar asymptotic behavior as @xmath170 .",
    "[ prop2 ] if @xmath168 so that @xmath352 , then @xmath353 converges to @xmath34 in probability .",
    "if additionally @xmath354 , then @xmath355 , that is , @xmath351 is an asymptotically optimal estimator of @xmath34 .    from the definition of @xmath356 in ( [ dbmled ] ) and @xmath357 in ( [ checkmle ] ) we have @xmath358 from ( [ calos2 ] ) it follows that @xmath359}{a_{t } } = \\frac { 1}{a_{t } } \\sum _ { i=1}^{k } { { { \\mathsf e}}_{\\lambda}}\\bigl[a^{i}_{t}- \\check{a}^{i}_{t}\\bigr ] \\leq\\sum_{i=1}^{k } \\frac { \\theta(\\delta^{i})}{a_{t}}= \\frac{\\theta({\\delta})}{a_{t}}.\\ ] ] therefore , markov s inequality implies that @xmath360 converges to 0 and @xmath361 converges to 1 in probability as @xmath168 so that @xmath352 , since @xmath88 is a linear function of @xmath51 .",
    "moreover , from theorem  [ theo0 ] we know that @xmath172 converges to @xmath34 in probability if @xmath352 .",
    "thus , we conclude that @xmath351 also converges to @xmath34 in probability as @xmath362 so that @xmath352 .    in order to prove that @xmath351 is asymptotically optimal , it suffices to show that @xmath363 converges to 0 in probability as @xmath364 so that @xmath354 , which also follows from ( [ rep ] ) and ( [ rep2 ] ) .",
    "we now assume that each sensor observes its underlying process only at a sequence of discrete and equidistant times @xmath365 , where @xmath366 is a common sampling period .",
    "thus , in what follows , @xmath367 .",
    "the goal is to examine the effect of discrete sampling on the proposed estimating scheme .",
    "first of all , we observe that the centralized estimator , @xmath368 is not affected by the discrete sampling of the underlying processes and ( [ asydet ] ) remains valid , that is , @xmath369 for every @xmath367 .",
    "moreover , the pairs @xmath370 remain i.i.d . and lemma  [ p2 ] still holds . on the other hand , lemma [ p1 ]",
    "is no longer valid and there is not an explicit formula for the density of the pair @xmath306 .",
    "however , the main difference in the case of discrete sampling is that at any time @xmath248 the fusion center learns whether @xmath108 increased or decreased by at least @xmath313 since @xmath371 , but does not learn by how much exactly .",
    "in other words , the fusion center does not learn the size of the realized overshoots , @xmath372 as a result , the statistic @xmath146 , defined in ( [ tbi ] ) , is no longer equal to @xmath108 at the communication times @xmath373 and the distance @xmath374 is no longer bounded by @xmath313 . therefore , theorem  [ theo0 ] , which establishes the consistency and asymptotic optimality of the proposed estimator , @xmath375 , under the assumption of continuous - time sensor observations may not hold when the sensors observe their underlying processes at discrete times .    our goal is to determine under what conditions the consistency and asymptotic optimality of @xmath172 are preserved in the context of discrete sampling at the sensors . in order to do so",
    ", we need to estimate the inflicted performance loss due to the unobserved overshoots .",
    "the following lemma is very useful in this direction .",
    "[ l ] for every @xmath104 , @xmath376 and the overshoots @xmath377 are i.i.d . with @xmath378",
    "= { \\mathcal{o}}\\bigl(\\sqrt[3]{h}\\bigr).\\ ] ]    for every @xmath218 we have @xmath379 \\bigr],\\end{aligned}\\ ] ] which implies ( [ estim1 ] ) .",
    "it is obvious that the overshoots @xmath380 are i.i.d . in order to prove ( [ estim ] ) , we write @xmath381 , where @xmath382 then , from theorem 3 of lorden  @xcite it follows that for _ any _ @xmath383 , @xmath384 & \\leq & \\max\\bigl\\ { { { { \\mathsf e}}_{\\lambda}}\\bigl [ b_{\\overline { \\delta}{}^i_1}^i - \\delta^{i } \\bigr ] , -{{{\\mathsf e}}_{\\lambda}}\\bigl[b^{i}_{\\underline{\\delta}_1^i}+ \\delta^{i}\\bigr ] \\bigr\\ } \\nonumber\\\\[-8pt]\\\\[-8pt ] & \\leq & \\sqrt[r]{\\frac{r+2}{r+1 } \\frac{{{{\\mathsf e}}_{\\lambda}}[|b^{i}_{h}|^{r+1}]}{|{{{\\mathsf e}}_{\\lambda}}[b^{i}_{h}]|}}.\\nonumber\\end{aligned}\\ ] ] since @xmath385 under @xmath27 and @xmath386 , @xmath387 & = & \\lambda ( x_{i})^{2 } h , \\\\ { { { \\mathsf e}}_{\\lambda}}\\bigl[\\bigl(b^{i}_{h}\\bigr)^{4}\\bigr ] & = & ( x_{i})^{4 } \\bigl [ ( \\lambda x_{i } h)^{4 } + 6 ( \\lambda x_{i } h)^{2 } h+ 3 h^{2 } \\bigr ] \\\\ & = & 3 ( x_{i})^{4 } h^{2 } \\bigl(1+o(1)\\bigr ) \\qquad\\mbox{as } h \\rightarrow0.\\end{aligned}\\ ] ] setting @xmath388 in ( [ lord ] ) completes the proof .    in the following theorem",
    "we show that @xmath172 remains consistent as @xmath165 for any given , fixed sampling period , @xmath366 , as long as the communication rate of every sensor is asymptotically low .    [ theo3 ] if @xmath317 so that @xmath389 for every @xmath291 , then @xmath390 \\rightarrow0 $ ] .    since @xmath391 \\rightarrow0 $ ]",
    ", it suffices to show that @xmath392 \\rightarrow0 $ ] .",
    "indeed , from the definition of the two estimators and ( [ estim1 ] ) we have @xmath393 since @xmath326 is a stopping time with respect to the filtration generated by @xmath394 , from wald s identity we obtain @xmath395= { { { \\mathsf e}}_{\\lambda}}\\bigl [ \\eta_{1}^{i } \\bigr ] { { { \\mathsf e}}_{\\lambda}}\\bigl [ m_{t}^{i}+1\\bigr].\\ ] ] taking expectations in ( [ usineq ] ) and applying ( [ estim2 ] ) , we obtain @xmath396 \\leq\\frac{{\\delta}}{a_{t } } + \\sum _ { i=1}^{k } \\frac { { { { \\mathsf e}}_{\\lambda } } [ \\eta_{1}^{i } ] { { { \\mathsf e}}_{\\lambda } } [ m_{t}^{i}+1]}{a_{t}}.\\ ] ] then , from ( [ calos ] ) , ( [ estim ] ) and the fact that @xmath88 is a linear function of @xmath51 we have @xmath397 \\leq\\frac { \\theta({\\delta})}{t } + \\sum _ { i=1}^{k } \\frac{{{{\\mathsf e}}_{\\lambda } } [ \\eta_{1}^{i } ] } { \\theta(\\delta^{i})}.\\ ] ] if some @xmath313 is fixed as @xmath165 , the second term in the right - hand side of ( [ essence00 ] ) does not go to 0 ( unless @xmath398 , in which case @xmath399 \\rightarrow0 $ ] for every @xmath104 , due to ( [ estim ] ) ) . however , if @xmath400 so that @xmath389 for every @xmath401 , then both terms in the right - hand side of ( [ essence00 ] ) go to 0 for any given sampling period , @xmath366 , which completes the proof .",
    "the proof of theorem  [ theo3 ] suggests that the proposed estimator is not consistent when both @xmath402 and @xmath403 are held fixed . in other words , it is necessary to have either a high sampling rate @xmath404 in order to reduce the size of the unobserved overshoots or a low communication rate in all sensors @xmath405 @xmath406 in order to reduce their accumulation rate .    however , an asymptotically low communication rate is not sufficient in order to preserve the asymptotic optimality of @xmath172 in the case of discrete sampling at the sensors .",
    "for this , the sampling period @xmath403 must converge to @xmath407 at an appropriate rate relative to the communication rate and the horizon of observations , which we specify in the following theorem .",
    "[ theo4 ] if @xmath408 and @xmath409 so that @xmath410{h}=o\\bigl ( \\delta^{i}/ \\sqrt{t}\\bigr ) \\qquad\\forall1 \\leq i\\leq k,\\ ] ] then @xmath411 , that is , @xmath172 is an asymptotically optimal estimator .",
    "since @xmath412 , it suffices to show that @xmath413 converges to @xmath407 in probability .",
    "indeed , from ( [ essence00 ] ) and the fact that @xmath88 is a linear function of @xmath51 , @xmath414 \\leq\\frac{\\theta({\\delta})}{\\sqrt{t } } + \\sum _ { i=1}^{k } \\frac{{{{\\mathsf e}}_{\\lambda } } [ \\eta_{1}^{i } ] } { \\theta(\\delta^{i}/\\sqrt{t})}.\\ ] ] the first term in the right - hand side goes to 0 if @xmath354 .",
    "the second term goes to @xmath407 if @xmath415=o ( \\delta^{i}/ \\sqrt{t})$ ] for every @xmath104 . for the latter",
    ", it suffices that @xmath416{h}=o ( \\delta^{i}/ \\sqrt{t})$ ] for every @xmath104 , due to ( [ estim ] ) , which completes the proof .    if each @xmath313 is fixed as @xmath417 , then theorem [ theo4 ] implies that @xmath89 is asymptotically efficient as @xmath165 and @xmath418 so that @xmath416{h } \\sqrt{t } \\rightarrow0 $ ] .",
    "in this work we considered a parameter estimation problem assuming that the statistician collects data from dispersed sensors , which observe continuous ( possibly correlated ) semimartingales with linear drifts with respect to a common , unknown parameter . motivated by sensor network applications , which are typically characterized by limited communication bandwidth , we required that the sensors must send a small number of bits per transmission and that they should avoid a high rate of communication with the fusion center .",
    "we proposed a novel methodology for this problem , according to which the sensors transmit to the fusion center one - bit messages at first exit times of appropriate statistics that they observe locally .",
    "the fusion center then combines these messages and constructs an estimator that imitates the optimal centralized estimator ( which can be computed only if there is full access to the sensor observations ) .",
    "we proved that the resulting estimator is consistent and , for a large class of processes , asymptotically optimal , in the sense that it attains the performance of the optimal centralized estimator when a sufficiently large horizon of observations is available .",
    "however , it is much more efficient from a practical point of view , as it reduces dramatically the congestion in the network and the computational burden at the fusion center .",
    "this is the case because it requires the transmission of only one - bit messages from the sensors and its statistical properties are preserved even with an asymptotically low rate of communication .",
    "it remains an open problem to design estimators with analogous optimality properties in more complicated setups , such as when there is not an explicit form for the optimal centralized estimator , the dimensionality of the parameter space is large or the sensors take non - i.i.d . , discrete - time observations .",
    "the author would like to thank dr .",
    "george v. moustakides and dr .",
    "alexandra chronopoulou for their feedback .",
    "moreover , the author is grateful to the two anonymous referees and the associate editor for their valuable remarks and suggestions that led to a significant improvement of earlier versions of this work ."
  ],
  "abstract_text": [
    "<S> a parameter estimation problem is considered , in which dispersed sensors transmit to the statistician partial information regarding their observations . </S>",
    "<S> the sensors observe the paths of continuous semimartingales , whose drifts are linear with respect to a common parameter . </S>",
    "<S> a  novel estimating scheme is suggested , according to which each sensor transmits only one - bit messages at stopping times of its local filtration . </S>",
    "<S> the proposed estimator is shown to be consistent and , for a large class of processes , asymptotically optimal , in the sense that its asymptotic distribution is the same as the exact distribution of the optimal estimator that has full access to the sensor observations . </S>",
    "<S> these properties are established under an asymptotically low rate of communication between the sensors and the statistician . </S>",
    "<S> thus , despite being asymptotically efficient , the proposed estimator requires minimal transmission activity , which is a desirable property in many applications . </S>",
    "<S> finally , the case of discrete sampling at the sensors is studied when their underlying processes are independent brownian motions . </S>"
  ]
}