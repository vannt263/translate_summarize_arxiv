{
  "article_text": [
    "cuckoo hashing @xcite is a multiple choice hashing scheme in which each item can be placed in multiple locations , and collisions are resolved by moving items to their alternative locations .",
    "this hashing scheme resembles the cuckoo s nesting habits : the cuckoo lays its eggs in other birds nests . when the cuckoo chick hatches , it pushes the other eggs out of the nest .",
    "hence the name `` cuckoo hashing . '' as ref .",
    "@xcite explains , analysis of hashing is similar to the analysis of balls and bins .",
    "hashing an item to a memory location corresponds to throwing a ball into a bin .",
    "insights from balls and bins processes led to breakthroughs in hashing methods .",
    "for example , if we throw @xmath0 balls into @xmath0 bins independently and uniformly , it is highly probable that the largest bin will get @xmath1 balls .",
    "al @xcite found that if each ball selects two bins independently and uniformly , and is placed in the bin with fewer balls , the final distribution is much more uniform .",
    "this led to hashing each item to one of two possible buckets , decreasing the load on the most - loaded bucket to @xmath2 with high probability .",
    "in general , if each item is hashed into @xmath3 buckets , the maximum load decreases to @xmath4    cuckoo hashing @xcite is an extension of two - way hashing .",
    "each item is hashed to a few possible buckets , and existing items may be moved to their alternate buckets in order to free space for a new item .",
    "there are many variants of cuckoo hashing .",
    "the goals of cuckoo hashing are to increase memory utilization ( the number of items that can be successfully hashed to a given memory size ) and to decrease insertion complexity .",
    "pagh and rodler @xcite analyzed hashing of each item to @xmath5 buckets of capacity @xmath6 , and demonstrated that moving items during inserts results in 50% space utilization with high probability .",
    "fotakis et .",
    "@xcite analyzed hashing of each item into more than two buckets .",
    "@xcite analyzed a practically - important case in which each item is hashed to @xmath5 buckets of capacity @xmath7 .",
    "@xcite found tight memory utilization thresholds for @xmath5 buckets of any size @xmath8 .",
    "specifically , they proved that the memory utilization for @xmath5 and @xmath7 is 89.7% .",
    "@xcite proved that the maximum memory utilization thresholds for @xmath9 and @xmath6 are equal to the previously known thresholds for the random k - xorsat problem .",
    "@xcite developed a tight formula for memory utilization for any @xmath9 and @xmath6 and ref .",
    "@xcite extended the formula to any @xmath9 and @xmath10 .",
    "_ comment added . _",
    "while this work was being completed , we became aware of ref .",
    "@xcite which proposed a model where the memory is divided into pages and each key has several possible locations on a single page as well as additional choices on a second backup page .",
    "they provide interesting experimental results .    in a classical implementation of two - way cuckoo hashing",
    ", the memory is partitioned into contiguous disjoint fixed - sized buckets of size @xmath11 .",
    "each item is hashed to 2 buckets and may be stored in any of the @xmath12 locations within those buckets .",
    "@xcite analyze a variation in which the buckets overlap .",
    "for example , if the bucket capacity @xmath11 is 3 , the disjoint bucket memory locations are : @xmath13 . whereas the overlapping bucket memory locations are :",
    "@xmath14 .",
    "their empirical results show that this variation increases memory utilization from 89.7% to 96.5% for @xmath5 and @xmath7",
    ". however , many systems retrieve data from secondary storage in same - size blocks called pages . fetching a page is a relatively expensive process , but",
    "once a page is fetched , its contents can be accessed orders of magnitude more quickly .",
    "we utilize this property of memory retrieval to present a variant of cuckoo hashing requiring that each bucket be fully contained in a single page but buckets are not necessarily contiguous .    in this paper",
    "we compare the following three variants of cuckoo hashing :    1 .",
    "cuckoo - choose - k- the algorithm introduced in this paper .",
    "the buckets are _ any _ @xmath11 cells in a page , not necessarily in contiguous locations .",
    "there are @xmath15 buckets in a page , where @xmath16 is the size of the page",
    "cuckoo - overlap @xcite- the buckets are contiguous and overlap . here",
    "we assume that all buckets are fully contained in a single page , so there are @xmath17 buckets in a page .",
    "this is a generalization of ref .",
    "originally ref .",
    "@xcite did not consider dividing the memory into pages .",
    "cuckoo - disjoint @xcite- the buckets are contiguous and not overlapping .",
    "there are @xmath18 buckets in a page .",
    "this is a generalization of ref .",
    "originally ref .",
    "@xcite did not consider larger buckets .",
    "note that algorithm cuckoo - disjoint is the extreme case of the cuckoo - overlap and cuckoo - choose_k algorithms when the size of the page @xmath16 equals the size of the bucket @xmath11 .",
    "we prove theoretically and present empirical evidence that our cuckoo - choose - k modification increases memory utilization .",
    "moreover , using the classical cuckoo hashing scheme , an item insertion requires multiple look - ups of candidates to displace .",
    "empirical results show that our modification dramatically decreases the number of candidate look - ups required to insert an item compared to ref .",
    "@xcite . in the overlapping buckets variant @xcite",
    ", some buckets are split between two pages , so that each item resides in up to @xmath19 pages . in our variant , each bucket is fully contained in a single page .",
    "an appealing experimental result is that cuckoo - choose - k memory utilization converges very quickly as a function of the page size @xmath16 .",
    "when @xmath7 and @xmath20 , memory utilization is 0.9763 .",
    "this value is almost identical to memory utilization when @xmath21 which equals 0.9767 .",
    "cuckoo - overlap memory utilization when @xmath20 is 0.9494 , and cuckoo - disjoint memory utilization is only 0.8970 .",
    "if we allow a tiny gap of one inside the buckets @xmath22 memory utilization increases from 0.9229 ( cuckoo - overlap ) to 0.9480 ( cuckoo - choose - k ) .",
    "table [ fig : number - of - iteration ] specifies the parameters used in our analysis .",
    "we can determine the success of cuckoo hashing by analyzing the cuckoo hyper graph .",
    "the vertices of the graph are the memory locations .",
    "the hyper - edges of the graph connect all the memory locations where each item could be placed . recall that each item can be placed in @xmath23 buckets chosen uniformly and independently of other items .",
    "each bucket is composed of any @xmath11 locations in a page .",
    "it is well known ( see , e.g. , ref .",
    "@xcite for a proof ) that a cuckoo hash _ _ fails__@xmath24 if and only if there is a sub - graph @xmath25 with @xmath26 vertices and more than @xmath26 edges .",
    "we say that a sub - graph @xmath25 has _ failed _ if it has more edges than vertices .",
    ".[tab : parameter - names - and]parameter names and descriptions [ cols=\"<,<,<\",options=\"header \" , ]     we will begin by analyzing the probability of success of cuckoo - choose - k for the case where the page size @xmath16 equals the array size @xmath0 . this simple and special case is presented here to introduce the main ideas applied in the following section , where we analyze the general case where the page size is a finite constant ( independent of @xmath0 ) .",
    "an analysis of memory utilization has been performed previously in @xcite . in their analysis , they assume @xmath6 and prove that if @xmath27 , then the hashing will be successful with a probability of at least @xmath28 . here",
    "we derive a similar constraint on memory utilization @xmath29 .",
    "we solve the constraint numerically for different values of @xmath11 and @xmath23 , and obtain a lower bound on possible memory utilization for the specified values .",
    "we perform the analysis using a modification of the method in @xcite , which we will later generalize to page sizes being equal to any given constant .",
    "we will bound the failure probability using the union bound . but",
    "first , we would like to reduce redundant summations .",
    "we observe that :    * if there exists a sub - graph @xmath30 with @xmath31 and there exists an edge that has exactly one vertex @xmath32 outside of v , then the sub - graph @xmath33 also has more edges than vertices since @xmath34 . *",
    "if there exists a sub - graph @xmath30 , such that @xmath35 and @xmath36 then there exists a sub - graph @xmath37 such that @xmath38 .",
    "we can find such a sub - graph simply by adding vertices to @xmath39 one by one until we get a sub - graph where the number of edges equals the number of vertices plus one .    for each sub - graph @xmath25",
    "we define an indicator variable @xmath40    @xmath41    if @xmath42 , then we will say that @xmath25 is a _ bad _ sub - graph , and otherwise we will say that @xmath25 is a _ good _ sub - graph . if the sum over @xmath43 of all sub - graphs is equal to zero then every sub - graph is good then the cuckoo hash _ _ succeeded__@xmath24 .",
    "we will find the the memory utilization @xmath29 such that the sum over @xmath43 of all sub - graphs is @xmath44 as @xmath45 .",
    "let @xmath46 be the probability that a random edge hits @xmath39 .",
    "let @xmath47 be the probability that a random edge connects @xmath39 to exactly one vertex from outside of v and let @xmath48 be probability that _ a given _ sub - graph @xmath30 is bad .",
    "@xmath49    immediate from the definition of @xmath43 . for s to be bad",
    ", exactly @xmath50 edges out of @xmath51 edges must hit @xmath39 and all the rest must miss @xmath39 and must not connect @xmath39 to exactly one vertex from outside of v.    the probability that a random edge hits @xmath39 is @xmath52    each item is hashed independently to @xmath23 buckets and the size of each bucket is @xmath11 .",
    "the number of buckets in @xmath39 is therefore @xmath53 and the total number of buckets is @xmath54 .",
    "the probability that a random edge connects @xmath39 to exactly one vertex from outside of v is @xmath55    @xmath56 buckets must all fall in @xmath39 and one bucket must contain any @xmath57 vertices from @xmath39 and any of the @xmath58 vertices from outside of @xmath39 .",
    "let @xmath59 be the probability that there exists a sub - graph @xmath25 with @xmath26 vertices such that @xmath25 is bad . according to the union bound , @xmath60 , where @xmath61 is the number of sub - graphs with @xmath26 vertices .",
    "we are going to analyze @xmath59 as @xmath45 . if for all @xmath26 , @xmath62 , then @xmath63 and the cuckoo hash succeeds with high probability .",
    "the analysis is similar to the analysis given in @xcite .",
    "let @xmath64 .",
    "we divide the analysis into two sections . in section [",
    "sub : x < x0 analysis ] we show that for any memory utilization @xmath29 and @xmath65 , @xmath66 is @xmath67 . in section [ sub :",
    "x > x0 analysis]we find the maximum memory utilization @xmath29 such that @xmath68 , @xmath66 is exponentially small .      in this section",
    "we show that if @xmath70 then for any memory utilization @xmath29 and @xmath65 , @xmath66 is @xmath67 .",
    "[ lem : pv < c0c1]@xmath71 ,    where @xmath72 and @xmath73    see appendix [ sec : proof - of - pv < c0c1 ] .",
    "[ lem : pv is good t=00003dn ]    @xmath74 @xmath75 be a small constant , @xmath761 . if @xmath77 then for any load @xmath29 :    \\1 .",
    "if @xmath78 , then @xmath79 .",
    "if @xmath80 , then @xmath66 decreases exponentially as @xmath45 .    for any memory utilization @xmath29 , @xmath71 ,    where @xmath72 and @xmath73",
    "note that @xmath81 and @xmath82 are independent of @xmath29 . recall that @xmath83 @xmath84 , there exist a constant @xmath85 such that @xmath86 .",
    "we obtain that :    \\1 . if @xmath87 , then @xmath88 .",
    "\\2 . if @xmath89 then @xmath86 , and @xmath71 decreases exponentially as @xmath45 .      in this section",
    "we are going to find the maximum memory utilization @xmath29 such that @xmath68 , the probability that there exists a bad sub - graph is exponentially small .",
    "[ lem : pv <",
    "c2c3c4c5]@xmath68 , @xmath91 where    @xmath92    see appendix [ sec : proof - of - pv < c2c3c4c5 ]    if @xmath93 for all @xmath90 , then @xmath66 decreases exponentially as @xmath45 .",
    "any memory utilization @xmath29 that satisfies the constraint is a lower bound on the possible memory utilization .",
    "the theorem follows directly from the inequality @xmath91 .",
    "numerical solutions to the constraint @xmath94 indicate that the memory utilization of the cuckoo - k algorithm is @xmath95 and @xmath24@xmath96 .",
    "our empirical results show that @xmath97 and @xmath98 .",
    "the memory utilization for @xmath99 rapidly approaches one .",
    "theoretical analysis performed by @xcite provided tight thresholds of the memory utilization of the cuckoo - disjoint algorithm .",
    "@xmath100 and @xmath101 . ref .",
    "@xcite do not provide a theoretical memory utilization threshold for small @xmath11 .",
    "the empirical results of ref .",
    "@xcite show that in the cuckoo - overlap algorithm @xmath102 and @xmath103 .",
    "the theoretical analysis of the cuckoo - disjoint algorithm performed in @xcite does not apply for @xmath104 and the theoretical analysis of the cuckoo - disjoint algorithm performed by @xcite does not apply for @xmath105 .      in this section",
    "we analyze the probability that the hashing fails for the case where the page size @xmath16 equals a constant .",
    "let @xmath106 be the probability that there exists a sub - graph @xmath25 with @xmath26 vertices such that @xmath25 has more edges than vertices and every vertex is in at least one edge .",
    "let @xmath107 . here",
    "again we divide the analysis into two sections . in section [ sub : x < x1 analysis ] we show that for any memory utilization @xmath29 and @xmath108 , @xmath109 is @xmath67 . in section [ sub :",
    "x > x1 analysis ] we find the maximum memory utilization @xmath29 such that @xmath110 , @xmath66 is exponentially small .      in this section",
    "we show that for any memory utilization @xmath29 and @xmath108 , @xmath109 is @xmath67 .",
    "the analysis here is similar to the case above where @xmath112 , however now we need to take into consideration the distribution of the vertices over the pages .",
    "let @xmath39 be a given set of vertices , and let @xmath113 be the number of vertices in page @xmath114 .",
    "the probability that a random edge hits @xmath39 is equal to @xmath115 .",
    "let @xmath116 be an upper bound on @xmath117 .",
    "we are going to use the following lemma which states that increasing the page size reduces @xmath118 .",
    "[ lem : low t is bad ] for any set of vertices v and any integer c,@xmath119 .",
    "since the function @xmath120 is convex , for any sequence of @xmath121 pages , @xmath122 , thus multiplying the page size by a factor @xmath121 does not increase @xmath118 . for convenience ,",
    "we restrict our analysis to pages of size @xmath123 , where c is any integer .",
    "the worst case is obtained when @xmath124 which is equivalent to the classical cuckoo - disjoint hashing .",
    "we will now analyze @xmath109 and @xmath66 as @xmath45 .",
    "recall that @xmath125 @xmath24 is the memory utilization , @xmath126 and @xmath127 .",
    "[ lem : pv < c6c7]@xmath128    where @xmath129 and @xmath130 .",
    "see appendix [ sec : proof - of pv < c6c7 ] .",
    "@xmath74 @xmath75 be a small constant , @xmath761 . if @xmath131 , then for any load @xmath29 :    \\1 .",
    "if @xmath78 , then @xmath132 .",
    "if @xmath133 , then @xmath109 decreases exponentially as @xmath45 .    for any memory utilization @xmath29 , @xmath128    where @xmath129 and @xmath130 .",
    "note that @xmath134 and @xmath135 are independent of @xmath29 .",
    "recall that @xmath107 .",
    "@xmath136 , there exist a constant @xmath85 such that @xmath137 .",
    "we obtain that :    \\1 . if @xmath87 , then @xmath138 .",
    "\\2 . if @xmath139 , then @xmath137 , and @xmath128 decreases exponentially as @xmath45 .      in this section",
    "we find the maximum memory utilization @xmath29 such that @xmath141 , the probability that there exists a bad sub - graph is exponentially small .",
    "we examine the set of sub - graphs that have a given distribution @xmath142 of vertices over the pages .",
    "@xmath143 , where @xmath144 is the number of pages that have @xmath114 vertices . for example , when the page size @xmath16 was equal to @xmath0 and the number of pages @xmath145 was 1 , the number of sub - graphs with @xmath26 vertices was @xmath146 and the corresponding @xmath142 of those sub - graphs was @xmath147 . by definition of @xmath142 ,",
    "@xmath148    when the page size @xmath16 is a given constant , the number of different possible values of @xmath142 is polynomial in @xmath0 .",
    "we denote by @xmath149 the number of different possible values of @xmath142 .    @xmath150 .",
    "since @xmath16 is constant , @xmath151 is polynomial in @xmath0 .",
    "let @xmath152 be the probability that there exists a bad sub - graph with distribution @xmath142 of vertices over the pages .",
    "if @xmath152 is exponentially small for every @xmath142 , then the union bound over a polynomial number of all possible values of @xmath153 is also exponentially small .",
    "let @xmath154 be the probability that _ a given _ sub - graph @xmath30 with @xmath143 is bad .",
    "let @xmath155 be a unit vector .",
    "when the page size @xmath16 is a constant , the probability that a random edge hits @xmath39 is :    @xmath156    and the probability that a random edge connects @xmath39 to exactly one vertex from outside of v is    @xmath157    using the union bound we obtain that    @xmath158    where @xmath159 is the number of sub - graphs with @xmath143 .",
    "@xmath160    for the asymptotic behavior of @xmath159 , we are going to use the following lemma :    [ lem : multinom_bound]@xmath161    see appendix [ sec : proof - of - lemma multinom_bound ] .",
    "[ lem : pv < c8c9]@xmath162 where    @xmath163 and    @xmath164    see appendix [ sec : proof - of - pv < c8c9 ] .    if @xmath24@xmath165 for all @xmath166 , then @xmath167 decreases exponentially as @xmath45 .",
    "any memory utilization @xmath29 that satisfies the constraint is a lower bound on the possible memory utilization .",
    "the theorem follows directly from the inequality @xmath162 .",
    "theoretical lower bounds of the memory utilization obtained from numerical solutions of the constraint @xmath168 are displayed in figure [ fig : memory - utilization vs. page size ] .",
    "]    memory utilization vs. page size .",
    "empirical cuckoo - choose - k ( green ) , empirical cuckoo - overlap ( red ) , approximation formula of empirical cuckoo - choose - k ( blue ) , and theoretical lower bound of cuckoo - choose - k ( black ) . left : @xmath7 , right : @xmath169.@xmath24    the experiments were conducted with a similar protocol to the one described in @xcite . in all experiments the number of the buckets , @xmath23 , was two .",
    "the capacity of each bucket @xmath11 was either two or three .",
    "the size of the hash tables @xmath0 was @xmath170 .",
    "the reported memory utilization @xmath29 is the mean memory utilization over twenty trials .",
    "the random hash functions were based on the matlab `` rand '' function with the twister method .",
    "items were inserted into the hash table one - by - one until an item could not be inserted .",
    "the results of both cuckoo - choose - k and cuckoo - overlap were notably stable . in each case , the standard deviation was a few hundredths of a percent , so error bars would be invisible in the figure .",
    "such strongly predictable behavior is appealing from a practical standpoint .",
    "since we added a paging constraint , our results are not comparable to previous works that do not include a paging constraint .    ]    number of lookups required to insert an item vs. memory utilization ( left ) and vs. page size ( right ) . in the left figure",
    "the page size is 8 . in the right figure",
    "the memory utilization is 92% .",
    "the left figure was smoothed with an averaging filter .",
    "the variance in the number of lookups required to insert an item was much smallar in cuckoo - choose - k .",
    "experiments show that cuckoo - choose - k improves memory utilization significantly even for a small page size @xmath16 and a small bucket capacity @xmath11 when compared to the classical cuckoo hashing cuckoo - disjoint .",
    "it outperforms cuckoo - overlap as well .",
    "recall that cuckoo - disjoint is the extreme case of cuckoo - choose - k and cuckoo - overlap when the page size is equal to bucket size @xmath11 .",
    "the memory utilization @xmath171 converges very quickly to its maximum value .",
    "for example @xmath172 is almost equal to @xmath173 . whereas @xmath174 and @xmath175 .",
    "the empirical memory utilization can be approximated very accurately by the following formulas :    @xmath176    @xmath177    the maximum approximation error is 0.0011 for @xmath7 and 0.0015 for @xmath169 .",
    "the empirical results and their approximations are displayed in figure [ fig : memory - utilization vs. page size ] together with empirical results of cuckoo - overlap . the memory utilization for @xmath99",
    "rapidly approaches one ( not displayed ) .",
    "cuckoo - choose - k outperforms cuckoo - overlap not only in memory utilization , but in the number of iterations required to insert a new item as well .",
    "figure [ fig : number - of - iteration ] illustrates the number of iterations required to insert a new item when the hash table is 92% full and we use breadth first search to search for a vacant position .",
    "@xmath17852 , whereas @xmath179 .",
    "note that in these simulations we did not limit the number of insert iterations and we continued to insert items as long as we could find free locations .",
    "most applications limit the number of inserted iterations , and maintain a low memory utilization in order to find a free location easily .",
    "if an empty position is not found within a fixed number of iterations , a rehash is performed or the item is placed outside of the cuckoo array .",
    "here we prove that @xmath24@xmath180 , where @xmath72 and @xmath73 .",
    "recall that @xmath125 @xmath24 is the memory utilization , @xmath126 and @xmath181 @xmath182 .",
    "( if @xmath183 or @xmath184 then the sub - graph @xmath25 can not fail ) .",
    "@xmath185 where    @xmath186    @xmath52 , and    @xmath55 .",
    "@xmath187 @xmath188 , @xmath189 and @xmath46 are bounded by :    @xmath190    @xmath191    @xmath192    @xmath193    and we get that    @xmath194",
    "here we prove @xmath68 , @xmath195 where    @xmath92 .    recall that : @xmath196 @xmath146 , @xmath46 and @xmath47 are bounded by:@xmath24",
    "@xmath197    @xmath198    @xmath199    since @xmath200 , we neglect the term @xmath201 in the following approximation of @xmath189 , and we neglect the term @xmath202 in the lower bounds for @xmath46 and @xmath47 . as @xmath45 , these terms contribute to @xmath59 factors which are bounded by @xmath203@xmath24 .    @xmath204    @xmath205    the proof for the left inequalities is given in @xcite and is also a special case of the more general inequality we prove later in lemma [ lem : multinom_bound ] .    @xmath24we get that :    @xmath206    where    @xmath207    and    @xmath208    since @xmath209 we get : @xmath91 .",
    "here we prove @xmath210    where @xmath129 and @xmath130 .    according to lemma [ lem : low t is bad ] , for any set of vertices v , @xmath118 decreases when the page size @xmath16 is multiplied by an integer . for simplicity , we restrict our analysis to pages of size @xmath123 , where @xmath121 is any integer .",
    "the worst case therefore is when @xmath124 which is equivalent to the classical cuckoo - disjoint hashing .",
    "we use the union bound to obtain @xmath211 , where @xmath212 is the number of sub - graphs with @xmath26 vertices , where each vertex of the sub - graph is hit by at least one edge , and @xmath213 is the probability that a given _ _",
    "sub - graph @xmath30 was hit by more than @xmath26 edges . by definition of @xmath214 :    @xmath215    when @xmath124 we get :    @xmath216    and    @xmath217    since when the page size @xmath16 equals @xmath11 , an element can be placed either in all of the locations of a page or in none of them .",
    "@xmath218    we get that :    @xmath219    where @xmath129 and @xmath130 .",
    "the proof of @xmath161 is a generalization of the proof given in @xcite . for any positive integer @xmath16 and any non - negative integer @xmath145 : @xmath220 , where the summation is taken over all sequences of non - negative integer indices @xmath221 through @xmath222 such that the sum of all @xmath223 is @xmath145 .",
    "for the special case where @xmath224 , @xmath144 is a non - negative integer and @xmath225 we get :    @xmath226    so @xmath227",
    "here we prove                                                        r. panigrahy , `` efficient hashing with lookups in two memory accesses '' , soda 05 : proceedings of the sixteenth annual acm - siam symposium on discrete algorithms , pages 830839 , philadelphia , pa , usa , 2005 .",
    "society for industrial and applied mathematics .",
    "d. fernholz and v. ramachandran .",
    "`` the k - orientability thresholds for @xmath243 '' . in soda 07 : proceedings of the eighteenth annual acm - siam symposium on discrete algorithms , pages 459468 , philadelphia , pa , usa , 2007 .",
    "society for industrial and applied mathematics .",
    "j. a. cain , p. sanders and n. wormald , `` the random graph threshold for k - orientiability and a fast algorithm for optimal multiple - choice allocation '' . in soda",
    "07 : proceedings of the eighteenth annual acm - siam symposium on discrete algorithms , pages 469476 , philadelphia , pa , usa , 2007 .",
    "society for industrial and applied mathematics .",
    "y. azar , a. z. broder , a. r. karlin , and e. upfal .",
    "balanced allocations .",
    "siam journal on computing , 29:180 - 200 , 1999.a preliminary version of this paper appeared in proceedings of the twenty - sixth annual acm symposium on the theory of computing , 1994 ."
  ],
  "abstract_text": [
    "<S> cuckoo hashing @xcite is a multiple choice hashing scheme in which each item can be placed in multiple locations , and collisions are resolved by moving items to their alternative locations . in the classical implementation of two - way cuckoo hashing , the memory is partitioned into contiguous disjoint fixed - size buckets . </S>",
    "<S> each item is hashed to two buckets , and may be stored in any of the positions within those buckets . ref . </S>",
    "<S> @xcite analyzed a variation in which the buckets are contiguous and overlap . </S>",
    "<S> however , many systems retrieve data from secondary storage in same - size blocks called pages . fetching a page is a relatively expensive process ; but once a page is fetched , its contents can be accessed orders of magnitude faster . </S>",
    "<S> we utilize this property of memory retrieval , presenting a variant of cuckoo hashing incorporating the following constraint : each bucket must be fully contained in a single page , but buckets are not necessarily contiguous . </S>",
    "<S> empirical results show that this modification increases memory utilization and decreases the number of iterations required to insert an item . </S>",
    "<S> if each item is hashed to two buckets of capacity two , the page size is 8 , and each bucket is fully contained in a single page , the memory utilization equals 89.71% in the classical contiguous disjoint bucket variant , 93.78% in the contiguous overlapping bucket variant , and increases to 97.46% in our new non - contiguous bucket variant . when the memory utilization is 92% and we use breadth first search to look for a vacant position , the number of iterations required to insert a new item is dramatically reduced from 545 in the contiguous overlapping buckets variant to 52 in our new non - contiguous bucket variant . </S>",
    "<S> in addition to the empirical results , we present a theoretical lower bound on the memory utilization of our variation as a function of the page size . </S>"
  ]
}