{
  "article_text": [
    "let @xmath17 be a separable banach space with the dual space @xmath18 for @xmath19 @xmath20 denotes the value of linear functional @xmath21 at vector @xmath22 let @xmath23 be a centered random variable in @xmath1 with @xmath24 ( that is , @xmath23 is weakly square integrable ) .",
    "let @xmath25 it is well known that this defines a bounded symmetric nonnegatively definite operator @xmath26 that is called _ the covariance operator of random variable @xmath27 moreover , if @xmath28 ( so , @xmath23 is strongly square integrable ) , then it is also well known that the covariance operator @xmath29 is nuclear . recall that a linear operator @xmath30 from a banach space @xmath31 into a banach space @xmath32 is called nuclear iff there exist sequences @xmath33 such that @xmath34 and @xmath35 the nuclear norm @xmath36 is defined as the infimum of the sums ( [ nuce_2 ] ) over all the sequences @xmath33 such that representation ( [ nuce_1 ] ) holds .",
    "_    let @xmath37 be i.i.d .",
    "copies of @xmath27 the sample ( empirical ) covariance operator based on the observations @xmath38 is defined as the operator @xmath39 such that @xmath40 clearly , this is an operator of rank at most @xmath41 and it is an unbiased estimator of the covariance operator @xmath42    in this paper , we are interested in the case when @xmath23 is a centered gaussian random vector in @xmath1 with covariance @xmath43 this implies that @xmath28 ( in fact , @xmath44 is even a random variable with a finite @xmath45-norm , see @xcite , chapter 3 ) and , as a consequence , the covariance operator @xmath29 is nuclear . for operators @xmath46 @xmath47 will denote the operator norm : @xmath48    several other definitions and notations will be used throughout the paper . in particular , the relationship @xmath49 ( for nonnegative @xmath50 ) means that there exists an absolute constant @xmath51 such that @xmath52 similarly , @xmath53 means that @xmath54 for an absolute constant @xmath55 if both @xmath49 and @xmath56 we write @xmath57 sometimes , symbols @xmath58 are provided with subscripts indicating possible dependence of constant @xmath59 on other constants ( say , @xmath60 would mean that @xmath61 with @xmath59 that might depend on @xmath62 ) .",
    "we will also use occasionally orlicz norms ( such as @xmath63- and @xmath45-norms ) in the spaces of random variables . given a convex nondecreasing function @xmath64 with @xmath65 and a random variable @xmath66 on a probability space",
    "@xmath67 define its @xmath68-norm as @xmath69 for @xmath70 @xmath71 the @xmath68-norm coincides with the @xmath72-norm . consider also @xmath73 and @xmath74 then @xmath75 means that @xmath66 has subgaussian tails and @xmath76 means that @xmath66 has subexponential tails .",
    "some well known inequalities for @xmath63 random variables will be used in what follows .",
    "for instance , for arbitrary random variables @xmath77 with @xmath78 @xmath79 if @xmath80 are i.i.d",
    ". centered random variables with @xmath81 then the sum @xmath82 satisfies the following version of bernstein s inequality : for all @xmath83 with probability at least @xmath11 @xmath84    our goal is to obtain moment bounds and concentration inequalities for the operator norm @xmath14 it turns out that both the size of the expectation of random variable @xmath6 and its concentration around its mean can be characterized in terms of the operator norm @xmath85 and another parameter defined below .",
    "assuming that @xmath23 is a centered gaussian random variable in @xmath1 with covariance operator @xmath86 define @xmath8    note that , for a gaussian vector @xmath87 @xmath88 implying that @xmath89 in the case when @xmath1 is a hilbert space , @xmath90 and @xmath91 the last quantity has been already used in the literature under the name of `` effective rank''(see @xcite ) .",
    "clearly , @xmath92    the main results of the paper include the following :    * under an assumption that @xmath93 are i.i.d . centered gaussian random variables in @xmath1 with covariance operator @xmath86 it will be shown that @xmath94 * moreover , under an additional assumption that @xmath95 the following concentration inequality holds for some constant @xmath96 and for all @xmath97 with probability at least @xmath98 @xmath99 under an assumption that @xmath100 the concentration inequality becomes @xmath101 and it holds with the same probability .",
    "the problem of bounding the operator norm @xmath6 has been intensively studied , especially , in the finite - dimensional case ( see @xcite and references therein ) .",
    "the focus has been on understanding of dependence of this norm on the dimension of the space and on the sample size @xmath41 ( that could be simultaneously large ) as well as on the tails of linear forms @xmath102 and of the norm @xmath44 of random variable @xmath27 many results that hold for gaussian random variables are also true in a slightly more general subgaussian case .    a centered random variable @xmath23 in @xmath1 will be called _ subgaussian _",
    "iff , for all @xmath103 @xmath104    we will also need the following definition .    a weakly square integrable centered random variable @xmath23 in @xmath1 with covariance operator @xmath29",
    "is called _ pregaussian _ iff there exists a centered gaussian random variable @xmath105 in @xmath1 with the same covariance operator @xmath43    suppose now that @xmath106 for some @xmath107 it will be viewed as a standard euclidean space .",
    "then , the following result is well known ( it is a slight modification of theorem 5.39 in vershynin @xcite stated there for isotropic subgaussian random variables , that is , when @xmath29 is the identity operator ) .",
    "[ classical ] there exists an absolute constant @xmath96 such that , for all @xmath10 with probability at least @xmath11 @xmath108    the proof of this theorem is based on a simple @xmath109-net argument that allows one to reduce bounding the operator norm @xmath110 to bounding the finite maximum @xmath111 where @xmath112 is a @xmath113-net of the unit sphere of cardinality @xmath114 the bounding of the finite maximum is based on a version of bernstein inequality for the sum of independent @xmath63 random variables @xmath115 combined with the union bound ( see the proof of theorem 5.39 in @xcite and the comments after this theorem ) .    in the isotropic case ( that is , when @xmath116 ) , the bound of theorem [ classical ] is sharp and it can be viewed as a non - asymptotic version of the well known bai - yin theorem from the asymptotic theory of random matrices . in the cases when the distribution of @xmath23 is far from being isotropic ,",
    "this bound is no longer sharp and it clearly can not be used in the infinite - dimensional case . if the covariance operator @xmath29 is of a small rank , it is natural to expect that the rank of @xmath29 rather than the dimension of the space @xmath1 should be involved in the bound .",
    "it turns out that one can obtain bounds on the operator norm @xmath6 in terms of the `` effective rank '' @xmath117 of the covariance operator @xmath29 ( that is always dominated by its actual rank ) .",
    "this could be done , for instance , using noncommutative bernstein type inequalities that go back to ahlswede and winter @xcite ( see also tropp @xcite , koltchinskii @xcite ) . for instance , lounici @xcite showed that with some constant @xmath96 and with probability at least @xmath11 @xmath118    another approach to bounding the operator norm @xmath6 was developed by rudelson @xcite and it is based on a noncommutative khintchine inequality due to lust - picard and pisier @xcite .",
    "this method can be used not only in subgaussian , but also in `` heavy tailed '' cases and it leads , for instance , to the following expectation bound ( see vershynin @xcite , theorem 5.48 ) : @xmath119 note that , in the subgaussian case , @xmath120 which implies that @xmath121 therefore , in this case we get @xmath122    in each of the above approaches , the bounds are not dimension free ( at least , with a straightforward application of noncommutative bernstein or khintchine inequalities ) and they could not be directly used in the infinite - dimensional case .",
    "we will use below a different approach based on recent deep results on generic chaining bounds for empirical processes .",
    "the following facts about generic chaining complexities will be needed .",
    "let @xmath123 and @xmath124 given a metric space @xmath125 an increasing sequence @xmath126 of partitions of @xmath127 is called admissible if @xmath128 for @xmath129 @xmath130 denotes the unique set of the partition @xmath126 that contains @xmath131 for @xmath132 @xmath133 denotes the diameter of set @xmath134 define @xmath135 where the infimum is taken over all admissible sequences .",
    "the following fundamental result is due to talagrand ( see @xcite ; it was initially stated it terms of majorizing measures rather than generic complexities ) .",
    "[ tal ] let @xmath136 be a centered gaussian process and suppose that @xmath137 then , there exists an absolute constant @xmath138 such that @xmath139    in what follows , generic chaining complexities are used in the case when @xmath140 is a function class on a probability space @xmath141 and @xmath142 is the metric generated by either @xmath143-norm , or by the @xmath45-norm with respect to @xmath144 we will use the following result due to mendelson @xcite ( although an earlier , simpler and weaker version , with @xmath145 instead of @xmath146 that goes back to klartag and mendelson @xcite would suffice for our purposes ) .",
    "[ men ] let @xmath93 be i.i.d .",
    "random variables in @xmath147 with common distribution @xmath148 and let @xmath149 be a class of measurable functions on @xmath150 such that @xmath151 implies @xmath152 and @xmath153 then @xmath154    assume again that @xmath1 is an arbitrary separable banach space .",
    "the next result provides a characterization of the size of @xmath155 in terms of the parameters @xmath85 and @xmath156 for gaussian random variable @xmath23 ( the upper bound also holds in the case when @xmath23 is both subgaussian and pregaussian ) .",
    "[ th_operator ] let @xmath157 be i.i.d .",
    "weakly square integrable centered random vectors in @xmath1 with covariance operator @xmath43 if @xmath23 is subgaussian and pregaussian , then @xmath158 moreover , if @xmath23 is gaussian , then @xmath159    the proof of the upper bound relies on the generic chaining bound of theorem [ men ] , while the proof of the lower bound is rather elementary .",
    "_ upper bound_. we have @xmath160 where @xmath161 @xmath162 and @xmath148 is the distribution of random variable @xmath27    since @xmath23 is subgaussian , the @xmath63- and @xmath45-norms of linear functionals @xmath163 are both equivalent to the @xmath164-norm .",
    "this implies that @xmath165 also , since @xmath23 is pregaussian , there exists a centered gaussian random variable @xmath105 in @xmath1 with the same covariance @xmath43 this means that @xmath166 using talagrand s theorem [ tal ] , we easily get that @xmath167 therefore , it follows that @xmath168 which proves the upper bound .",
    "_ lower bound .",
    "_ to prove the lower bound , note that @xmath169 for a fixed @xmath170 with @xmath171 and @xmath172 denote @xmath173 by a straightforward computation , for all @xmath174 the random variables @xmath163 and @xmath175 are uncorrelated .",
    "since they are jointly gaussian , it follows that @xmath163 and @xmath176 are independent .",
    "define @xmath177 then @xmath178 and @xmath179 are also independent .",
    "we easily get @xmath180 where we used the fact that @xmath181 note that , conditionally on @xmath182 the distribution of random variable @xmath183 is gaussian and it coincides with the distribution of the random variable @xmath184 denote now by @xmath185 the conditional expectation given @xmath186 and by @xmath187 the conditional expectation given @xmath188 then , we have @xmath189 @xmath190 @xmath191 @xmath192 also @xmath193 @xmath194 @xmath195 @xmath196 note that @xmath197 therefore , @xmath198 and @xmath199 @xmath200 where @xmath201 are i.i.d .",
    "standard normal random variables .",
    "it is easy to check that @xmath202 for a positive numerical constant @xmath203 implying that @xmath193 @xmath204 we now combine this bound with ( [ odin ] ) and ( [ dva ] ) to get @xmath205 @xmath206 we also have the following obvious bound @xmath207 @xmath208 for some numerical constant @xmath209 thus , we get @xmath210 @xmath211 provided @xmath212 is chosen to be small enough to satisfy @xmath213    this completes the proof in the case when @xmath214 since in this case @xmath215 on the other hand , under the assumption that @xmath216 @xmath217 which completes the proof in the case when @xmath218    our next goal is to prove a concentration inequality for @xmath6 around its median or around its expectation . in what follows , @xmath219 denotes a median of a random variable @xmath220    [ cor_2 ] let @xmath157 be i.i.d",
    ". centered gaussian random vectors in @xmath1 with covariance @xmath29 and let @xmath13 be either the median , or the expectation of @xmath14 then , there exists a constant @xmath96 such that the folllowing holds . if @xmath9 then for all @xmath10 with probability at least @xmath221 @xmath222 on the other hand , if @xmath15 then with the same probability @xmath223    in the case when @xmath13 is the median , this result is an immediate consequence of theorem [ th_operator ] and theorem [ spectrum_sharper ] that is given below and that provides an equivalent concentration inequality written in a somewhat implicit form .",
    "the bounds of theorem [ cor_2 ] in the case when @xmath13 is the median imply that @xmath224 when @xmath9 and @xmath225 when @xmath226 this , in turn , implies the concentration bound in the case when @xmath13 is the expectation .",
    "[ spectrum_sharper ] let @xmath157 be i.i.d",
    ". centered gaussian random vectors in @xmath1 with covariance @xmath29 and let @xmath13 be the median of @xmath14 then , there exists a constant @xmath96 such that for all @xmath97 with probability at least @xmath221 @xmath227 . \\ ] ]    the proof of theorem [ spectrum_sharper ] is somewhat long and will be given in the next section .",
    "here we will state a couple corollaries of this theorem .",
    "[ cor_1 ] under the assumptions and notations of theorem [ spectrum_sharper ] , there exists a constant @xmath96 such that , for all @xmath10 with probability at least @xmath221 @xmath228    the proof easily follows from the next simple bound : @xmath229    the following corollary can be viewed as an infinite - dimensional generalization of theorem [ classical ] .",
    "[ cor_3 ] under the assumptions and notations of theorem [ spectrum_sharper ] , there exists a constant @xmath96 such that , for all @xmath10 with probability at least @xmath221 @xmath230 this implies that for all @xmath231 @xmath232    bound ( [ sha_sha_conc_a ] ) follows immediately from corollary [ cor_1 ] and theorem [ th_operator ] .",
    "bound ( [ l_p_bound ] ) follows from ( [ sha_sha_conc_a ] ) by integrating the tail probabilities .",
    "in this section , we provide a proof of theorem [ spectrum_sharper ] .",
    "we will use the following well known fact ( see , e.g. , @xcite ) .",
    "[ kwapien ] let @xmath23 be a centered gaussian random variable in a separable banach space @xmath233 then there exists a sequence @xmath234 of vectors in @xmath1 and a sequence @xmath235 of i.i.d .",
    "standard normal random variables such that @xmath236 where the series in the right hand side converges in @xmath1 a.s . and @xmath237      it easily follows from theorem [ kwapien ] that , for @xmath239 we have @xmath240 let now @xmath241 be the covariance operator of @xmath242 and @xmath243 be the sample covariance operator based on observations @xmath244 ( with the notation @xmath245 having an obvious meaning and the sample size @xmath41 being fixed ) .",
    "then , @xmath246 thus , it is enough to prove the theorem only in the case when @xmath247 the general case would then follow by a straightforward limiting argument .",
    "[ gaussian_concentration ] let @xmath248 be a standard normal vector in @xmath249 and let @xmath250 be a function satisfying the following lipschitz condition with some @xmath251 @xmath252 then , for all @xmath253 @xmath254 where @xmath255 is the distribution function of a standard normal random variable .",
    "[ gaussian_concentration_a ] under the assumptions of lemma [ gaussian_concentration ] , suppose that for some @xmath13 and for some @xmath256 @xmath257 then , there exists a constant @xmath258 ( possibly depending on @xmath259 ) such that , for all @xmath10 with probability at least @xmath221 @xmath260    denote @xmath261 where @xmath262 where @xmath263 is an arbitrary fixed lipschitz function with constant @xmath264 on @xmath265 @xmath266 @xmath267 @xmath268 and where @xmath269 is a fixed number ( to be chosen later ) . with a little abuse of notation ,",
    "assume for now that @xmath270 @xmath271 are nonrandom vectors in @xmath272 and @xmath273 are nonrandom vectors in @xmath1 defined as follows : @xmath274        obviously , @xmath278 @xmath279 implying that @xmath280 it is enough to consider the case when @xmath281 or @xmath282 ( otherwise , the claim of the lemma is obvious ) . to be specific ,",
    "assume that @xmath283 then , using the assumption that @xmath263 is lipschitz with constant @xmath284 we get @xmath285    we will now control @xmath286 note that @xmath287 @xmath288 @xmath289 @xmath290 @xmath291 @xmath292 since @xmath293 @xmath294 therefore , @xmath295 @xmath296 which easily implies @xmath297 substituting the last bound in ( [ li_li ] ) , we get @xmath298 in view of ( [ fdelta ] ) , the left hand side is also bounded from above by @xmath299 which allows one to get from ( [ li_li  ] ) that @xmath300 in the case when @xmath301 we have @xmath302 it is also easy to check that the same bound holds in the opposite case , too . as a consequence , ( [ li_li ] ) implies that with some numerical constant @xmath303 @xmath304 we will now upper bound @xmath305 note that @xmath306 implying that @xmath307 @xmath308 @xmath309 @xmath310 combining this with bound ( [ li_li_a ] ) yields ( [ lip_lip_cc_xyz ] ) .    in what follows , denote @xmath311 it follows from lemmas [ gaussian_concentration ] and [ lipschitz_constant_xyz ] that , for all @xmath97 with probability at least @xmath221 @xmath312 where @xmath313 is a numerical constant .",
    "we will use this bound to get that , on the event where @xmath314 and , at the same time , concentration bound ( [ conc_bd ] ) holds , we have @xmath315 @xmath316 denote @xmath317 then we have @xmath318            denote @xmath328.\\ ] ] we will define @xmath329 for @xmath330 as follows : @xmath331 it is easy to see that @xmath332 ( provided that constant @xmath333 is chosen to be sufficiently large ) .",
    "note also that @xmath334 thus , by induction , @xmath335 is a nonincreasing sequence . in view of definition of @xmath336",
    "it follows from ( [ main_bou ] ) that for all @xmath330 @xmath337 also , by lemma [ very_easy ] , @xmath338    let @xmath339 then @xmath340 it is easy to check that @xmath341 in addition , @xmath342 define @xmath343 as follows : @xmath344 @xmath345 then , @xmath346 it is also easy to check that @xmath347 implying @xmath348 let @xmath349 clearly , @xmath350 where we also used ( [ bar_de ] ) . taking into account ( [ main_bou_k ] ) and ( [ main_bou_0 ] ) , we get that for some constant @xmath351 @xmath352 observe that @xmath353 and also that , for some constant @xmath354 and for @xmath97 @xmath355 using now ( [ almost_done ] ) with @xmath356 instead of @xmath357 it is easy to get that with probability at least @xmath11 @xmath358 @xmath359 } ( c_1{\\bf r}(\\sigma))}{n } } \\bigvee \\frac{\\log^{[3 ] } ( c_1{\\bf r}(\\sigma))}{n } \\bigvee \\sqrt{\\frac{\\log^{[3 ] } ( c_1 n)}{n}}\\bigvee \\frac{\\log^{[3 ] } ( c_1 n)}{n }   \\biggr],\\ ] ] where we used the notation @xmath360 } x : = \\log\\log \\log x.$ ] in the case when @xmath95 we have @xmath361 } ( c_1{\\bf r}(\\sigma))\\leq \\log^{[3 ] } ( 2c_1 n).\\ ] ] hence , doubling the value of the constant @xmath362 allows us to drop the two terms involving @xmath363 } ( c_1{\\bf r}(\\sigma))}{n}.$ ] on the other hand , assume that @xmath364 with a sufficiently large constant @xmath365 ( to be determined later ) .",
    "observe that @xmath360 } ( c_1 { \\bf r}(\\sigma))\\lesssim { \\bf r}(\\sigma)$ ] and we can use a bound for the median @xmath13 similar to ( [ very_easy_b3 ] ) : @xmath366 for some constants @xmath367 and for @xmath368 we also used the fact that for gaussian @xmath23 @xmath369 thus , we get @xmath370 } ( c_1{\\bf r}(\\sigma))}{n } } \\bigvee \\frac{\\log^{[3 ] } ( c_1{\\bf r}(\\sigma))}{n}\\biggr)\\lesssim   \\|\\sigma\\|\\frac{{\\bf r}(\\sigma)}{n}\\lesssim m.\\ ] ] since also @xmath363 } ( c_1 n)}{n}\\lesssim 1,$ ] this implies that with some constant @xmath371 and with the same probability @xmath372 } ( c_1 n)}{n}}\\biggr)\\biggr].\\end{aligned}\\ ] ] take now @xmath373 to be equal to the expression in the right hand side of bound ( [ sha_sha  ] ) and use this value of @xmath373 to do another iteration of bound ( [ main_bou ] ) .",
    "this easily yields that with some constant @xmath96 and with probability at least @xmath374 @xmath375.\\end{aligned}\\ ] ]    to complete the proof of concentration inequality ( [ sha_sha_conc ] ) , note that , for an arbitrary @xmath376 on the event where ( [ conc_bd ] ) holds and also @xmath377 @xmath378 this bound will be used with @xmath379.\\ ] ] then , in view of bound ( [ sha_sha   ] ) , @xmath380 ( provided that @xmath97 ) . note also that @xmath381 then , it follows from lemma [ gaussian_concentration_a ] that , for a sufficiently large constant @xmath313 and for all @xmath10 with probability at least @xmath221 the following bound holds : @xmath382 recall also that @xmath383 on the event where @xmath314 of probability at least @xmath384 therefore , with probability at least @xmath385 @xmath386 the result now follows by substituting @xmath373 given by ( [ del_de ] ) into bound ( [ last_bd ] ) , doing simple algebra and adjusting the value of constant @xmath313 to get the probability bound @xmath387    very recent exponential generic chaining bounds for empirical processes by dirksen @xcite ( see corollary 5.7 ) and by bednorz@xcite ( see theorem 1 ) imply the following ( earlier , mendelson @xcite , theorem 3.1 obtained another version of exponential generic chaining bounds for the same class of processes ) .",
    "[ dirksen ] let @xmath93 be i.i.d .",
    "random variables in a measurable space @xmath150 with common distribution @xmath148 and let @xmath149 be a class of measurable functions on @xmath388 there exists a constant @xmath96 such that for all @xmath97 with probability at least @xmath11 @xmath389      [ subgau ] let @xmath157 be i.i.d .",
    "weakly square integrable centered random vectors in @xmath1 with covariance operator @xmath43 if @xmath23 is subgaussian and pregaussian , then there exists a constant @xmath96 such that , for all @xmath10 with probability at least @xmath221 @xmath390    note that the proof of concentration inequality of theorem [ spectrum_sharper ] does not rely on generic chaining bounds , it relies only on the gaussian isoperimetric inequality .",
    "the bound of theorem [ subgau ] ( based on the generic chaining method ) could be used to provide a shortcut in the proof of the concentration inequality .",
    "to this end , instead of using very rough initial bound @xmath391 based on lemma [ very_easy ] one should use much more precise bound of theorem [ subgau ] . in this case",
    ", there is no need to implement an iterative argument improving the bound , the concentration inequality in its explicit form ( theorem [ cor_2 ] ) follows just by an application of the gaussian isoperimetric inequality .",
    "adamczak @xcite suggested an alternative approach to the proof of theorem [ cor_2 ] .",
    "it is based on a version of a concentration inequality for gaussian chaos and on some other tools ( such as gordon - chevet inequality ) , but it does not rely on the generic chaining bounds .",
    "the authors are especially thankful to radek adamczak for providing an alternative proof of the concentration inequality and for very helpful discussions . the initial version of theorem [ cor_2 ]",
    "was under an extra assumption that @xmath392 we improved our argument after adamczak had provided his alternative proof .",
    "99 adamczak , r. ( 2014 ) concentration for empirical covariance operators . _ personal communication .",
    "_ ahlswede , r. and winter , a. ( 2002 ) strong converse for identifications via quantum channels . _ ieee transactions on information theory _ , 48 , 3 , pp . 569679 .",
    "vershynin , r. ( 2012 ) introduction to the non - asymptotic analysis of random matrices . in : _",
    "compressed sensing , theory and applications . edited by y. eldar and g. kutyniok _ , chapter 5 , pp .",
    "210268 , cambridge university press ."
  ],
  "abstract_text": [
    "<S> let @xmath0 be i.i.d . centered gaussian random variables in a separable banach space @xmath1 with covariance operator @xmath2 @xmath3 the sample covariance operator @xmath4 is defined as @xmath5 the goal of the paper is to obtain concentration inequalities and expectation bounds for the operator norm @xmath6 of the deviation of the sample covariance operator from the true covariance operator . in particular , it is shown that @xmath7 where @xmath8 moreover , it is proved that , under the assumption that @xmath9 for all @xmath10 with probability at least @xmath11 @xmath12 where @xmath13 is either the median , or the expectation of @xmath14 on the other hand , under the assumption that @xmath15 for all @xmath10 with probability at least @xmath11 @xmath16    and </S>"
  ]
}