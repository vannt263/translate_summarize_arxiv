{
  "article_text": [
    "in the analysis of increasingly high - precision data sets , it is now common practice in cosmology to constrain cosmological parameters using sampling based methods , most notably markov chain monte carlo ( mcmc ) techniques ( christensen et al . 2001 ; knox , christensen & skordis 2001 ; lewis & bridle 2002 ) .",
    "this approach typically requires one to calculate theoretical cmb power spectra ( i.e. some subset of the tt , te , ee and bb @xmath5 spectra ) and/or the matter power spectrum @xmath6 at a large number of points ( typically @xmath7 or more ) in the cosmological parameter space .",
    "in addition , one must also evaluate at each point the corresponding ( combined ) likelihood function for the data set(s ) under consideration . as a result",
    ", the process can be computational very demanding .",
    "the purist would calculate the required power spectra at each point using codes such as cmbfast ( seljak & zaldarriaga 1996 ) or camb ( lewis , challinor & lasenby 2000 ) , which typically require around 10 secs for spatially - flat models and 50 secs for non - flat models .",
    "this approach is therefore computationally demanding , but does have the advantage that it is simple to generalise if one wishes to include new physics or change the form of the initial power spectra .",
    "mcmc parameter estimation codes such as cosmomc ( lewis & bridle 2002 ) attempt to decrease the overall computational burden by dividing the cosmological parameter space into ` fast ' parameters ( governing the initial primoridal power spectra of scalar and tensor perturbations ) and ` slow ' parameters ( governing the perturbation evolution ) and making judicious proposals for how the chain is propagated in parameter space . even with this technique , however , the total computational cost is still usually very high .",
    "if one is willing to forego the full calculation of the required power spectra at each point in parameter space , there are a number of ways in which suitably accurate spectra can be generated somewhat more rapidly .",
    "if the cosmological parameter space of interest is sufficiently small , then it is possible simply to create spectra for a regular grid of models in parameter space and interpolate between them in some way .",
    "as the number of parameters increases , however , the computational cost of constructing the grid grows exponentially .",
    "fast grid generation schemes have been proposed , such as the @xmath8-splitting scheme of tegmark & zaldarriaga ( 2000 ) that exploits analytic approximations at high-@xmath8 and insensitivity to certain parameters at low-@xmath8 .",
    "nevertheless , the pre - compute of the grid of models remains extremely time - consuming and such approaches become difficult to implement accurately when second - order effects such as gravitational lensing are important .",
    "more extensive use of analytic and semi - analytic approximations can reduce the required number of pre - computed models , but only at the cost of a loss of accuracy and/or placing restrictions on the parameters that are available as input .",
    "such approaches are usually based on a relatively sparse grid of base models in the parameter space from which the spectra of more general models are computed rapidly on - the - fly using various ( semi-)analytic approximations .",
    "the dash code of kaplinghat , knox & skordis ( 2002 ) , instead stores a sparse grid of transfer functions ( rather than @xmath5 ) , uses efficient choices for grid parameters and makes considerable use of analytic approximations . following @xmath9 hrs of computation on a typical desktop to calculate the grid",
    ", dash provides a speed - up factor of @xmath10 relative to cmbfast in calculating a @xmath11 , @xmath12 or @xmath13 spectrum .",
    "more recently , the need to pre - compute a grid of models has been removed in the cmbwarp package ( jimenez et al .",
    "2004 ) , which builds on the method introduced by kosowsky , milosavljevic & jimenez ( 2002 ) . in this approach , a new set of nearly uncorrelated ` physical parameters '",
    "are introduced upon which the cmb power spectra have a simple dependence .",
    "cmbwarp uses a modified polynomial fit in these parameters in which the coefficients are based on the spectra @xmath11 , @xmath12 or @xmath13 for just a single fiducial model in the parameter space .",
    "spectra for other models can then be calculated around @xmath14 times faster than cmbfast . by taking the fiducial model to be the best - fit model to the wmap1 data",
    ", cmbwarp gives better than 0.5 per cent accuracy for the @xmath11 spectrum throughout the entire region of parameter space lying within the wmap1 3@xmath15 confidence region , although the accuracy quickly reduces as one moves further away from the fiducial model .",
    "although the above methods have proved extremely useful in performing cosmological parameter estimation , they do exhibit a number of drawbacks , as we have outlined .",
    "most recently , this has led fendt & wandelt ( 2006 ) to propose a more flexible and robust machine - learning approach ( called pico ) to accelerating both power spectra and likelihood evaluations . in this method ,",
    "one first calculates the required spectra ( usually @xmath11 , @xmath12 or @xmath13 ) using camb and the corresponding likelihoods for the experiments of interest ( in particular wmap3 ) at @xmath16 points chosen uniformly within a box in parameter space that encompasses ( say ) the @xmath1 confidence region of the wmap3 likelihood .",
    "this constitutes the training set for the pico code ",
    "note that only power spectra values at the limited number of @xmath8-values output by camb are used ( typically @xmath17 values for @xmath18 ) . in short ,",
    "the basic algorithm used by pico consists of three major parts .",
    "first , the training set is compressed using karhunen ",
    "love eigenmodes ( essentially a principal component analysis ) which typically results in a reduction in the dimensionality of the training set by a factor of two .",
    "second , the training set is used to divide the parameter space into ( @xmath19 ) smaller regions using a @xmath20-means clustering algorithm ( see e.g. mackay 1997 ) with the goal that all clusters encompass volume of parameter space over which the power spectra vary roughly equally .",
    "finally , a ( 4th order ) polynomial is fitted within each cluster ( by minimising the squared error ) to provide a local interpolation of the power spectra within the cluster as a function of cosmological parameters .",
    "the reason for dividing up the parameter space in the second step is that the interpolation method used fails to model accurately the power spectra over the entire parameter space .",
    "the pico approach provides about the same speed - up in spectrum calculation as cmbwarp ( which is an order of magnitude faster than dash ) , but is an order of magnitude more accurate .",
    "it also has several other important advantages .",
    "first , it is very flexible and can easily be applied to the fast calculation of any observables relevant to a particular data set , such as scalar , tensor and lensed power spectra , transfer functions or even higher - order correlation functions .",
    "second , it allows the calculation of such observables from an arbitrary number of cosmological models and in any range of @xmath8 ( or @xmath20 ) values .",
    "lastly , the algorithm is sufficiently generic to allow the direct fitting of likelihood functions , thereby incorporating the functionality of the cmbfit code of sandvik et al .",
    "this last capability allows an additional order of magnitude speed - up in cosmological parameter estimation beyond that resulting from faster power spectrum calculations , and is particularly important for experiments such as wmap3 for which the likelihood calculation is very expensive .    in this letter",
    ", we present an independent approach to using machine - learning techniques for accelerating both power spectra and likelihood evaluations .",
    "our approach is based on training a neural network in the form of a 3-layer perceptron .",
    "the resulting cosmonet code shares all the advantages of pico , but we believe also has some additional benefits in terms of simplicity , computational speed , accuracy , memory requirements and ease of training . the letter is organised as follows .",
    "in section  [ sec : nn ] we give a brief introduction to neural networks and our training algorithm .",
    "the resulting network output is discussed in section  [ sec : interp ] , where we investigate the accuracy of our approach .",
    "the cosmonet code is then used to perform a cosmological parameter estimation from wmap3 data in section  [ sec : cosmo ] .",
    "our conclusions are presented in section  [ sec : conc ] .",
    "neural networks are a methodology for computing motivated by the parallel architecture of animal brains .",
    "they consist of a group of interconnected processing elements called neurons that pass simple scalar messages between them to process information .",
    "many neural networks provide feed - forward maps from a set of input neurons to a set of output neurons .",
    "for an introduction to feed - forward neural networks see bailer - jones et al .",
    "they are often used to provide empirical models for processes that are too complicated to model from theoretical principles .",
    "an astrophysical example is presented in vanzella et al .",
    "( 2004 ) , where photometric redshifts are predicted in the hdf - s from an ultra deep multicolour catalogue .",
    "the perceptron ( rosenblatt 1958 ) is the simplest type of feed - forward neuron and maps an input vector @xmath21 to a scalar output @xmath22 via f ( ; , ) = _ i w_i x_i + , [ eq : perceptron ] where @xmath23 and @xmath24 are the parameters of the perceptron , called the ` weights ' and ` bias ' respectively .",
    "multilayer perceptron neural networks ( mlps ) are a type of feed - forward network composed of a number of ordered layers of perceptron neurons that pass scalar messages from one layer to the next . in the simplest case , the network has two layers : the input layer and and the output layer .",
    "each node in the output layer is a perceptron and has an activation given by ( [ eq : perceptron ] ) . in this paper , however , we will work with a 3-layer network , which consists of an input layer , a hidden layer and an output layer , as illustrated in fig .",
    "[ fig : nn ] .",
    "an example of a 3-layer neural network with seven input nodes , 3 nodes in the hidden layer and five output nodes .",
    "each line represents one weight.,width=2 ]    in such a network , the outputs of the nodes in the hidden and output layers take the form @xmath25 where the index @xmath26 runs over input nodes , @xmath27 runs over hidden nodes and @xmath28 runs over output nodes .",
    "the functions @xmath29 and @xmath30 are called activation functions and are chosen to be bounded , smooth and monotonic . in this letter",
    ", we use @xmath31 and @xmath32 , where the non - linear nature of the former is a key ingredient in constructing a viable network .    the weights @xmath33 and biases @xmath34 are the quantities we wish to determine , which we denote collectively by @xmath35 .",
    "as these parameters vary a very wide range of non - linear mappings between the inputs and outputs are possible .",
    "in fact , according to a ` universal approximation theorem ' ( leshno et al .",
    "1993 ) , a standard multilayer feed - forward network with a locally bounded piecewise continuous activation function can approximate any continuous function to _ any _ degree of accuracy if ( and only if ) the network s activation function is not a polynomial .",
    "this result applies when activation functions are chosen apriori and held fixed as @xmath35 varies .",
    "accuracy increase with the number in the hidden layer and the above theorem tells us we can always choose sufficient hidden nodes to produce any accuracy .",
    "since the mapping from cosmological parameter space to the space of cmb power spectra ( and wmap3 likelihood ) is known to be continuous , a 3-layer mlp with an appropriate choice of activation function is an excellent candidate model for the replacement of the forward model provided by the camb package ( and wmap3 likelihood code ) .",
    "the activation functions act as basic building blocks of non - linearity in a neural network model and should be as simple as possible . additionally , the memsys routines used in training ( described below ) require derivative information and so they should be differentiable .",
    "the universal approximation theorem thus motivates us to choose a monotonic ( for simplicity ) , bounded and differentiable function that is not a polynomial and we choose the @xmath36 function .",
    "of course , this could be replaced by another such function , such as the sigmoid function , but the interpolation results will be almost identical .",
    "let us consider building an empirical model of the camb mapping using a 3-layer mlp as described above ( a model of the wmap3 likelihood code can be constructed in an analogous manner ) .",
    "the number of nodes in the input layer will correspond to the number of cosmological parameters , and the number in the output layer will be the number of uninterpolated @xmath5 values output by camb .",
    "a set of training data @xmath37 is provided by camb ( the precise form of which is described later ) and the problem now reduces to choosing the appropriate weights and biases of the neural network that best fit this training data .    as the camb mapping is exact , this is a deterministic problem , not a probabilistic one .",
    "we therefore wish to choose network parameters @xmath35 that minimise the ` error ' term @xmath38 on the training set given by ^2 ( ) = _ k _ i^2 .",
    "this is , however , a highly non - linear , multi - modal function in many dimensions whose optimisation poses a non - trivial problem . despite the deterministic nature of the problem we use an extension of a bayesian method provided by the memsys package ( gull & skilling 1999 ) .",
    "the memsys algorithm considers the parameters @xmath35 of the network to be probabilistic variables with prior probability distribution proportional to @xmath39 , where @xmath40 is the positive - negative entropy functional ( gull & skilling 1999 ; hobson & lasenby 1998 ) and @xmath41 is considered a hyperparameter of the prior .",
    "the variable @xmath41 sets the scale over which variations in @xmath35 are expected , and is chosen to maximise its marginal posterior probability .",
    "its value is inversely proportional to the standard deviation of the prior . for fixed @xmath41 ,",
    "the log - posterior is thus proportional to @xmath42 . for each choice of @xmath41",
    "there is a solution @xmath43 that maximises the posterior .",
    "as @xmath41 varies , the set of solutions @xmath43 is called the ` maximum - entropy trajectory ' .",
    "we wish to find the maximum of @xmath44 which is the solution at the end of the trajectory where @xmath45 .",
    "it is difficult to recover results for @xmath46 ( for large @xmath41 the solution is found at the maximum of the prior ) when starting with a result that lies far from the trajectory .",
    "thus for practical purposes , it is best to start from the point on the trajectory at @xmath47 and iterate @xmath41 downwards until either a bayesian @xmath41 is acheived , or in our deterministic case , @xmath41 is sufficiently small that the posterior is dominated by @xmath48 .",
    "memsys performs the algorithm using conjugate gradients at each step to converge to the maximum - entropy trajectory .",
    "the required matrix of second derivatives of @xmath48 is approximated using vector routines only .",
    "this avoids the need for the @xmath49 operations required to perform exact calculations , that would be impractical for large problems .",
    "the application of memsys to the problem of network training allows for the fast efficient training of relatively large network structures on large data sets that would otherwise be difficult to perform in a useful time - frame .",
    "the memsys algorithms are described in greater detail in ( gull & skilling 1999 ) .",
    "we demonstrate our approach by training networks to replace the camb package for the evaluation of the cmb power spectra @xmath50 , @xmath12 or @xmath13 for flat @xmath0cdm models within a box in parameter space that encompasses the @xmath1 confidence region of the wmap 1-year likelihood .",
    "we also train a network to replace the wmap 3-year likelihood code , however for reasons to be discussed later in this section , this interpolation was preformed over a slightly smaller region than @xmath51 . we train four seperate networks : one for each cmb power spectra and one for the wmap 3-year likelihood .",
    "it is possible to provide all spectra and the likelihood from a single network , but training speed is increased by keeping them separate .",
    "the training data for the spectra interpolation is produced in a similar way to that used to train the pico algorithm .",
    "we define the same box as fendt & wandelt in the 6-dimensional ` physical parameter ' space of flat @xmath0cdm models , which encompasses the @xmath1 confidence region of the likelihood determined from wmap 1-year data ( bennett et al .",
    "2003 ) and other higher resolution cmb data .",
    "this box is then sampled uniformly to select @xmath52 models .",
    "the physical parameters ( @xmath53 , @xmath54 , @xmath24 , @xmath55 , @xmath56 , @xmath57 ) are converted back to cosmological parameters ( @xmath58 , @xmath59 , @xmath60 , @xmath61 , @xmath56 , @xmath57 ) and used as input to camb to produce the training set of cmb power spectra out to @xmath62 ( which corresponds to 50 uninterpolated @xmath5 values for each spectrum ) .",
    "a further set of @xmath63 samples were generated as testing data .",
    "building a training set for the likelihood was complicated by errors in the wmap 3-year likelihood code .",
    "spuriously high likelihoods were observed for some models lying outside of roughly @xmath64 with @xmath65-likelihood = -5373 ] .",
    "these spikes in the likelihood surface prevented a reasonable interpolation in these areas and so had to be eliminated from the training set .",
    "in addition sampling uniformly from a @xmath51 region in 6 dimensions returned very few samples around the maximum likelihood point making an accurate interpolation around the peak unworkable . to correct for both of these problems we built our likelihood training set from @xmath66 samples in parameter space drawn from a gaussian distribution centered on the maximum likelihood point ( restricted to the box encompassing our parameter priors ) .",
    "the covariance matrix of the gaussian was twice that of the expected variance of the cosmological parameters and was found to provide sufficient coverage , both for the peaks of the marginalised posteriors and their tails .",
    "a small pre - processing step was used to make the variation in the training data of a similar order to the non - linearity present in the network activation functions .",
    "this involved mapping all inputs and outputs linearly so they had zero mean and a variance of one - half .",
    "appropriate scaling of the data would be performed by network training if this step were omitted , but the speed of training is increased if the initial values of the weights are closer to their likely optimal values . also , for the te and ee spectra , a small number ( 2 - 4 ) of separate neural networks were trained on separate regions of the spectra , and then combined post training to provide a single network for each spectra .",
    "this step was required to provide @xmath67 percentile error within those produced by the pico algorithm .",
    "it was found that 50 nodes in the hidden layer for the tt spectra network , 125 nodes for the te spectra network , 200 nodes for the ee spectra network , and 50 for the likelihood network , were sufficient to provide good results .",
    "the results of comparing the cosmonet output with camb over the testing set are shown in fig .",
    "[ fig : spectra ] .         for all but the very low values of @xmath26 in the ee spectrum , where the values of the spectrum and cosmic variance are small",
    ", the average error is about @xmath2 of cosmic variance .",
    "the @xmath67 percentiles are also comfortably below unit cosmic variance .",
    "a comparison of output of the cosmonet likelihood network with the wmap3 likelihood code over the testing set reveals a mean error of roughly 0.2 @xmath65 units close to the peak .",
    "to illustrate the usefulness of cosmonet in cosmological parameter estimation we perform an analysis of the wmap 3-year tt , te and ee data using cosmomc in three separate ways : ( i ) using camb power spectra and the wmap3 likelihood code ; ( ii ) using cosmonet power spectra and the wmap3 likelihood code ; and ( iii ) using the cosmonet likelihood .",
    "the resulting marginalised parameter constraints for each method are shown in fig .",
    "[ fig : wlike ] and fig .",
    "[ fig : wolike ] , and are clearly very similar .",
    "in each case 4 parallel mcmc chains were run on intel itanium 2 processors at the cosmos cluster ( sgi altix 3700 ) at damtp , cambridge .",
    "the wall - clock computational time required to gather @xmath68 post burn - in mcmc samples was @xmath69 hours for method ( i ) ( with camb further parallelised over 3 additional processors per chain , therefore totalling 16 cpus ) ,  8 hours for method ( ii ) and roughly 35@xmath70 minutes using the interpolated likelihood with method ( iii ) . for comparison , a similar run with the pico code took roughly 55@xmath70 minutes training region of both algorithms so camb was never called . ] illustrating that it is now the remaining sampling calls within cosmomc that provides the new bottleneck .",
    "the one - dimensional marginalised posteriors on the cosmological parameters within the 6-parameter flat @xmath0cdm model comparing : camb power - spectra and wmap3 likelihood ( red ) with cosmonet power spectra and wmap3 likelihood ( black).,width=2 ]     the one - dimensional marginalised posteriors on the cosmological parameters within the 6-parameter flat @xmath0cdm model comparing : camb power - spectra and wmap3 likelihood ( red ) with cosmonet likelihoods ( black).,width=2 ]",
    "we have presented a method for accelerating power spectrum and likelihood evaluations based on the training of multilayer perceptron neural networks , which we have shown to be fast , robust and accurate .",
    "our cosmonet method shares all the advantages of the pico algorithm of fendt & wandelt , achieving similar accuracies on both spectra interpolations and cosmological parameter constraints , but there are several differences between the two methods that we believe give cosmonet a number of additional benefits , which we now discuss .    _",
    "simplicity_. despite requiring the optimisation of a highly non - linear multi - dimensional function using memsys , we consider the principal advantage of our method to be the relative simplicity of the trained interpolation for the user to implement .",
    "cosmonet provides a _ single _ simple , closed - form function for each interpolation over the _",
    "whole _ of the parameter space under consideration .",
    "_ memory usage_. a neural network with @xmath71 inputs , @xmath72 nodes in the hidden layer , and @xmath73 outputs has @xmath74 parameters .",
    "this is far less than in the pico approach , where the use of clustering and individual interpolations for each @xmath5 requires far more parameters . in the case of the flat @xmath0cdm example demonstrated in section [ sec : interp ] , we require about @xmath75 kb of parameter memory for all three power spectra and the likelihood , whereas pico would require @xmath76 mb . while the memory requirements of pico will increase with the number of cosmological parameters , this should make little difference to the memory requirements of our method , as we have found the required number of nodes in the hidden layer does not increase beyond a factor of 2 for the 11 parameter non - flat model parameterised by @xmath77 , @xmath78 , @xmath79 , @xmath24 , @xmath55 , massive neutrino fraction @xmath80 , varying equation of state of dark energy @xmath81 , scalar perturbation amplitude and spectral index @xmath82 , @xmath83 and tensor modes with amplitude ratio and spectral index @xmath84 , @xmath85 ( results to appear in a forthcoming paper ) , thus representing a linear rise .",
    "_ speed_. the number of calculations required to perform the feed - forward network mapping is @xmath86 . in the example presented in section [ sec : interp ] the calculation of the 50 uninterpolated @xmath5 values for each spectrum required @xmath87 120 microseconds , whereas each wmap3 likelihood took @xmath87 10 microseconds ; this is @xmath88 times faster than pico in performing the interpolation .",
    "moreover , the cpu requirements of the pico interpolation scales as @xmath89 , for pico @xmath90 is 4 .",
    "_ ease of training_. training using the memsys package is almost totally automated and relatively quick .",
    "in fact the bottleneck in providing appropriate network weights for more complex cosmological models is the calculation of the training data using the camb package .",
    "the networks used in section [ sec : interp ] took around 100 hours of training ( on a standard pc workstation ) .",
    "additionally , memsys training time scales linearly with the number of network nodes and again linearly with the number of entries in the training set .",
    "however , networks with an accuracy of roughly @xmath91 times worse , that are sufficient to provide good parameter constraints can be trained in under an hour using just 1000 training samples .",
    "these simpler neural networks did not require the l - splitting performed in section [ sec : interp ] and had only 50 nodes in the hidden layer .",
    "ta acknowledges a studentship from epsrc .",
    "mb was supported by a benefactors scholarship at st .",
    "john s college , cambridge and an isaac newton studentship .",
    "this work was conducted in cooperation with sgi / intel utilising the altix 3700 supercomputer at damtp cambridge supported by hefce and pparc .",
    "we thank s. rankin and v. treviso for their assistance .",
    "bailer - jones c. , 2001 , automated data analysis in astronomy .",
    "narosa publishing house , new delhi bennett c. et al .",
    ", 2003 , apjs , 148 , 1 christensen n. , meyer r. , knox l. , luey b. , 2001 , class .",
    "grav . , 18 , 2677 fendt w. , wandelt b. , 2006 , apj , submitted ( astro - ph/0606709 ) gull s.f .",
    ", skilling j. , 1999 , quantified maximum entropy : memsys 5 users manual .",
    "maximum entropy data consultants ltd , royston hinshaw g. et al .",
    "2006 , apj , submitted ( astro - ph/0603451 ) hobson m.p .",
    ", lasenby a.n . , 1998 ,",
    "mnras , 298 , 905 jimenez r. , verde l. , peiris h. , kosowsky a. , 2004 , prd , 70 , 023005 kaplinghat m. , knox l. , skordis c. , 2002 , apj , 578 , 665 kosowsky a. , milosavljevic m. , jimenez r. , 2002 , prd , 66 , 063007 knox l. , christensen n. , skordis c. , 2001 , apj , 563 , l95 lewis a. , bridle s. , 2002 , prd , 66 , 103511 lewis a. , challinor a. , lasenby a. , 2000 , apj , 538 , 473    leshno m. , ya .",
    "lin v. , pinkus a. , schocken s. , 1993 , neural netw . , 1993 , 6 , 861 mackay d.j.c .",
    ", 2003 , information theory , inference and learning algorithms .",
    "cambridge university press page l. et al . , 2006 ,",
    "apj , submitted ( astro - ph/0603450 ) rosenblatt f. , 1958 , psychological review , 65 , 386 sandvik h.b . , tegmark m. , wang x. , zaldarriaga m. , 2004 , prd , 69 , 063005 seljak u. , zaldarriaga m. , 1996 , apj , 469 , 437 spergel d. et al . , 2006 ,",
    "apj , submitted ( astro - ph/0603449 ) tegmark m. , zaldarriaga m. , 2000 , apj , 544 , 30 vanzella e. et al . , 2004 , a&a , 423 , 761"
  ],
  "abstract_text": [
    "<S> we present a method for accelerating the calculation of cmb power spectra , matter power spectra and likelihood functions for use in cosmological parameter estimation . </S>",
    "<S> the algorithm , called cosmonet , is based on training a multilayer perceptron neural network and shares all the advantages of the recently released pico algorithm of fendt & wandelt , but has several additional benefits in terms of simplicity , computational speed , memory requirements and ease of training . </S>",
    "<S> we demonstrate the capabilities of cosmonet by computing cmb power spectra over a box in the parameter space of flat @xmath0cdm models containing the @xmath1 wmap1 confidence region . </S>",
    "<S> we also use cosmonet to compute the wmap3 likelihood for flat @xmath0cdm models and show that marginalised posteriors on parameters derived are very similar to those obtained using camb and the wmap3 code . </S>",
    "<S> we find that the average error in the power spectra is typically @xmath2 of cosmic variance , and that cosmonet is @xmath3 faster than camb ( for flat models ) and @xmath4 times faster than the official wmap3 likelihood code . </S>",
    "<S> cosmonet and an interface to cosmomc are publically available at www.mrao.cam.ac.uk/software/cosmonet .    </S>",
    "<S> [ firstpage ]    cosmology : cosmic microwave background  </S>",
    "<S> methods : data analysis  methods : statistical . </S>"
  ]
}