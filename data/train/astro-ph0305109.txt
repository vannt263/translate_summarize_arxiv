{
  "article_text": [
    "the olympic motto `` citius , altius , fortius '' ( `` swifter , higher , stronger '' ) succinctly describes the direction of 21st century parallel supercomputing - swifter processors , higher resolution and stronger fault - tolerant parallel algorithms . in computational astrophysics ,",
    "the need for all of these qualities is perhaps greater than most supercomputing applications . while the bulk of the physics of the formation of the planets , stars , galaxies and the large - scale structure in the universe are now largely understood with the initial conditions well posed in some cases , the challenge of computing the formation of objects and structures in the universe is difficult .",
    "the standard methods of computational fluid dynamics ( including magnetohydrodynamics ) and gravitational n - body simulation are stressed by the large dynamic range in density , pressure , and temperature that exist in nature .",
    "ever - finer computational meshes and greater numbers particles in n - body simulations are needed to capture the physics of the formation of things in the universe correctly .    in recent years , the relative low cost of commodity servers running the linux operating system and more importantly switches and network interface cards has led to the growth in performance and competitiveness of beowulf clusters versus vector machines and symmetric multi - processors .",
    "the recent list of the top 500 supercomputers  @xcite last year reveals that 55 out of the top 100 are clusters .",
    "most high performance beowulfs now use low - latency , proprietary networking with costs several times that of current commodity gigabit networking .",
    "large - scale 200 + port gigabit switches are a slightly cheaper option but still eat significantly into the cost of a cluster .",
    "networking costs alone can dominate cluster costs .",
    "the spirit of building a beowulf cluster is to get the highest performance on a limited budget so the incorporation of expensive networking solutions defeats this fundamental motivation .",
    "we describe here a cheaper networking configuration based on off - the - shelf 24-port commodity gigabit switches in a 256-node/512-cpu cluster .",
    "we achieve a total cross - sectional bandwidth of 128 gbit between 256 nodes .",
    "the innovation to achieving this performance is a networking topology involving both a stack of commodity 24-port gigabit smc switches and the configuration of each linux server as a network router through a second gigabit network port .",
    "our economical networking strategy competes directly with both large - scale 200 + port switches and proprietary low - latency networking at a fraction of the cost .",
    "the high throughput provided by linux routing capability makes this performance possible .",
    "we ran the hpl linpack standard benchmark and achieved 1.202 tflops placing it in the top 50 computers according to the top 500 list of november 2002 .",
    "we also describe the performance and results of 4 parallel applications for astrophysical problems that are currently running on the cluster spanning problems in large - scale structure formation , galactic dynamics , magnetohydrodynamic flows on blackholes and planet formation .",
    "in this section , we briefly describe the design and hardware specifications of the cluster . the hardware was assembled and installed by mynix technologies , montreal , pq at the university of toronto  @xcite .",
    "the installation of the linux operation system was done using the oscar  @xcite cluster package by our group .      the cita beowulf cluster dubbed mckenzie is comprised of 268 dual xeon 2.4 ghz rack - mounted format linux servers ( 536 processors in total ) distributed over 7 racks .",
    "all the nodes are 1u format with the exception of two 2u format head nodes .",
    "the hardware specifications for each server are given in table 1 . from the total of 268 nodes ,",
    "256 nodes are dedicated to parallel , message - passing supercomputing , 8 nodes for code development and smaller scale applications , 2 spare nodes also running to act as hot replacements in case of hardware failures on the compute nodes and 2 head nodes .",
    "the main head node  bob \" contains the home disk space as well as the root space images and linux kernels for the compute slave nodes .",
    "all slave nodes boot through the network from bob using pxelinux allowing easy kernel upgrades and maintenance for the cluster nodes .",
    "a secondary master node  doug \" mirrors bob s root and home directories on a daily basis and acts a backup in case of a bob crash .",
    "this allows quick recovery and continued operation if bob fails .",
    "the cluster is networked using 19 smc tiger 24-port configurable gigabit switches .",
    "seventeen switches are used for the main 256-node cluster network while the development cluster is connected to its own switch and the 19th switch is reserved as a spare and connected to the running spare nodes .",
    "we defer the discussion of the network topology to the section below .",
    "the nodes and switches are mounted on 7 44u high racks .",
    "the main 256 compute nodes are mounted on the first 3 and last 3 racks .",
    "the central rack is reserved for the head nodes , development nodes and spares as well as the stack of switches .",
    "the switches are mounted on the back of the racks to simplify the cabling .",
    ".mckenzie part list and cost [ cols= \" < , < \" , ]      the cost for the compute nodes plus storage and incidentals was usd$513k .",
    "the cost for the gigabit switches plus cabling was usd$48k representing only 9% of the total cluster cost of $ 561k .",
    "when considering options for networking the cluster , we decided against proprietary low - latency networking and large - scale 200 + port switches after realizing that they would consume a significant fraction of our modest budget .",
    "the prices of 24-port gigabit switches this past year had dropped significantly so we considered networking schemes that could take advantage of this inexpensive hardware .",
    "fat - tree networks that link switches together through a hierarchy are a straightforward configuration that provide connectivity but quite low cross - sectional bandwith .",
    "each node comes with a second gigabit port , however , and could be exploited in some way using point to point connections with other machines . with this in mind we came up with the following scheme .",
    "switches ( or pairs of trunked switches ) can be thought of as separate tightly coupled networks of up to a few dozen nodes .",
    "these nodes can be assigned to vertices in some more complex network topology .",
    "how can one connect these vertices to form a high - bandwidth global network linking all the compute nodes ?",
    "after loading a switch with compute nodes there are few ports left to link to other vertices .",
    "however , each compute node provides a single connection through its second port that can in principle be connected to a compute node on an adjacent network vertex .",
    "if each linux node has routing enabled , network traffic can be relayed through the second port to other nodes .",
    "this was the strategy we ultimately employed in setting up a high - bandwidth global network .      before setting up our high performance global network , we first set up a simple fat - tree that we treat as a robust but lower - performance maintenance network .",
    "this network was used to install the cluster originally but also acts as a bootstrap for configuring a high - performance network . in our final configuration ,",
    "we trunk pairs of switches together making network vertices containing 48 ports with a total of 8 vertices using 16 switches .",
    "we connect 32 compute nodes to each vertex . for 6 vertices ,",
    "we run a 4-port trunk to a 17th master switch filling it to capacity .",
    "the fat - tree networking is completed by connecting the last two vertices with 4-port trunks to 2 vertices trunked to the master switch .",
    "the head nodes bob and doug , plus the development and spare nodes are all plugged into this network as well allowing direct communication between all available nodes .",
    "this fat - tree network configuration is robust and provides connectivity to all the 256 nodes and is adequate for maintenance , installation and embarassingly parallel applications .",
    "however , the cross - sectional bandwidth is not at all ideal for heavy duty mpi applications .",
    "we now describe how we use this fat - tree network to bootstrap to our high - bandwidth network which runs predominantly through point to point connections between the second network ports of all machines .",
    "the network vertices described above can be thought of as independent networks that we can connect to each other according to some topology .",
    "the use of a master switch to build a fat tree is one such topology but it has quite low cross - sectional bandwidth . how can we use the second port on each compute node to connect these vertices to increase the network bandwidth and minimize network hops to reduce latency ?",
    "there are many possibilities but we finally settled on a cubic network topology ( figure  [ cdcc ] ) .    in our chosen topology , the 8 network vertices are arranged on the corners of a cube .",
    "communication between vertices can occur along the edges of the cube but we also connect opposite corners through diagonals that cross through the cube centroid .",
    "we call this topology a cross - diagonal connected cube ( cdcc ) .",
    "each vertex then has 4 outgoing lines of communication to its adjacent corners and opposite along the diagonal .",
    "the cdcc topology requires hard - wired routing tables between nodes which we constructed using a variety of scripts .",
    "we have 32 gigabit lines on each vertex provided by the second network port on each compute node and 4 vertex connections .",
    "we therefore assign 8-gigabit lines to each vertex connection .",
    "these lines connect 8 compute nodes on one vertex to another 8 on a neighbouring vertex through direct port to port connections with a straight - thru cable . in this way",
    ", there is an 8-gigabit pipeline connecting each vertex which must be shared between 32 compute nodes . also , the 4-port trunks connecting pairs of switches are 4- gigabit pipelines relaying traffic for 16 compute nodes per switch .    in this way ,",
    "the cdcc topology with 256 nodes approximately represents a fully - switched network running at 250 mbit , although it is slightly better since nodes on the same switch are fully - switched at 1 gbit .",
    "the full - duplex cross - sectional bandwidth is 128 gbit for 256 nodes .",
    "when constructing the routing tables , the maximum number of network hops to get from one node to another is 4 so latency is greater than a fully - switched system but we find in practice for our applications and standard benchmarks this is not a large hindrance .",
    "the performance of this configuration is about half the 500 mbit performance that is expected for large gigabit switches that typically connect 16 ports to an 8- gigabit backplane internally .",
    "latency is also about 4 times greater .",
    "however , the total cost for the 17 required 24-port switches plus cabling is much less than the cost of a single 256-port capable switch .",
    "we show below that our cluster provides competitive benchmark speeds for similar clusters with better networking and performs well for our applications .      the task of cabling the cluster also proved to be formidable but we came up with some ways to minimize confusion and tangling .",
    "connections from the first ethernet port to the switches is straightforward if the switches are centralized to one location at the centre of the rack array in a stack .",
    "network cables are run over and under the cluster in small bundles and colour coded by rack number and then plugged into their appropriate ports in the central stack of switches .",
    "although tedious , this stage of the cabling took a small team of people two days to complete .",
    "we point out that the same cabling effort is required even for more expensive multi - port switches so there is no significant overhead in manual labour .    in the second stage , it was necessary to wire the cdcc network using the point to point connections through the second network port on each node",
    ". wiring these connections could potentially lead to a tangled mess of cables if the nodes are ordered sequentially according to their vertex numbers .",
    "a simple solution to this wiring problem is to re - order the nodes on the racks in logically connected pairs so that they are also physically connected to one another . in this way , only a short 1 foot length of cable is required for each contiguous pair and wiring can be done quickly .",
    "the logic of the connectivity of the cdcc network was used to create the correct ordered list of nodes on the rack for this arrangement as well as a wiring diagram for the connecting cables to the right switches in the fat - tree diagram .",
    "the only extra level of complexity for doing things this way in comparison to 200 + port monster switches is the node ordering and the need to make sure each cable is connected to the correct port in the switch stack . in practice ,",
    "we have re - wired the network a couple of times to test the performance of different topologies and it generally only takes a few hours to do .      another essential ( and complex ) part of getting the cdcc network to perform was the configuration of the routing tables for each linux server .",
    "network packets relayed through each server on this server have to know where to go according to our defined network topology .",
    "a useful part of our design is that we have a back - up maintenance network ( the fat - tree ) that we can always use to gain access to each node in case of routing bugs or broken links in the cdcc network .",
    "this allowed us to experiment extensively with the scripts for generating the routing tables of the cdcc and led to an optimized system .",
    "our strategy is to generate purely static routing tables on each server with a simple script at startup .",
    "each of these scripts contains routing commands that establish about 50 static routes to individual nodes and sub - networks ( vertices ) in the cdcc network .",
    "each node has its own unique routing table .",
    "we find this system works very well in practice and can be taken up and down quickly if necessary .",
    "our arrangement is also very fault - tolerant in case of node failures or broken links .",
    "if one or more nodes fails in the system , it will create holes in the cdcc network but as a contingency we can route around the broken nodes simply by using the fat - tree network .",
    "we have written a simple network repair script that pings all neighbours on the cdcc - if a node fails to answer we attempt to establish a direct route to it through the fat - tree - if that fails the node is assumed to be dead . in this way , we can continue to run jobs on the cluster even when main compute nodes go down .",
    "either the hot spares , or development nodes can fill in while we replace the failed nodes .",
    "since our networking is similarly fragmented into many switches , the hardware failure of one switch will only take out those nodes connected to it so most of the cluster can still run while the broken switch is being replaced .",
    "figure  [ cdcc ] shows some final details of our routing scheme that are worth pointing out .",
    "it turned out that the network performance of the trunks was not as good as we expected .",
    "we therefore modified our routing scheme to avoid using the trunks as much as possible .",
    "network traffic from one node to another on different vertices used cross links that avoid the trunks .",
    "the only network traffic going through the trunks is between the nodes on bonded pairs of switches .      as an initial test of performance , we ran the high - performance linpack benchmark ( hpl ) , a portable implementation for distributed - memory computers to measure the speed of our cluster and compare with others on the top 500 list  @xcite .",
    "this code uses the message passing interface ( mpi ) libraries .",
    "the actual benchmark involves the inversion of the largest matrix that can be stored on your cluster . in our case , the matrix size was @xmath0 elements .",
    "we compiled the code using the intel c compiler icc version 7.0 and linked to the intel mkl math libraries and kazushige goto s optimized blas libraries  @xcite .",
    "we used the lam version 6.6b1 as the mpi library .",
    "we achieved a sustained speed of 1.202 teraflops about 48% the theoretical peak speed of 2.46 teraflops for 256 xeon 2.4 ghz chips .",
    "this ratio of sustained to theoretical peak speed is comparable to many of the machines quoted on the november 2002 top 500 list including similar sized clusters running with proprietary networking such as myrinet .",
    "our ranking on the list would currently be number 34 .",
    "of course , it remains to be seen how we will rank on the next list released in a few months since these trends tend to evolve very quickly !",
    "the beowulf cluster mckenzie was constructed for problems in computational astrophysics .",
    "we describe here several applications that are being used to investigate areas on many astronomical scales from planets to the entire universe .",
    "we also highlight the performance of the various codes we have started to use .",
    "cosmological n - body simulation is the main numerical tool used to study the formation of structure in the universe .",
    "dubinski et al .",
    "@xcite have developed a new parallel algorithm that combines the standard techniques of particle - mesh methods ( pm )  @xcite with the barnes - hut ( bh ) oct - tree algorithm  @xcite .",
    "the pm method is used to measure the long - range gravitational forces from particles within a periodic cube representing a typical volume in the universe .",
    "these forces are refined on the sub - mesh scale using the bh algorithm by building a grid of oct - trees within the simulation domain .",
    "the new code is called gotpm for grid-(of)-oct - trees - particle - mesh .",
    "the code uses slab domain decomposition to distribute the particles among the cpus . during the pm phase",
    ", poisson s equation for the gravitational potential is solved using fourier methods .",
    "we use the mpi implementation of fftw  @xcite to do the fft s during this phase which assumes slabs of equal width . a grid of bh oct - trees",
    "is then constructed within each slab and used to refine the forces at short range .",
    "force accuracies are typically 0.5% using this method . to ensure load balance ,",
    "the slab widths are allowed to vary during the tree phase with the width determined by the amount of computational work in the previous timestep .",
    "a fair amount of communication is required to move particle data at each step since they must move back and forth from an equal width slab decomposition for the pm phase and variable width slabs for tree phase . despite this high demand for bandwidth ,",
    "communication overhead is about 25% even with our inexpensive networking configuration .    figure  [ fig - cosmo ] shows one snapshot from a cosmological simulation using a standard model ( @xmath1 and @xmath2 ) in a 200 @xmath3 mpc ( 650 million light years ) box .",
    "this simulation used a @xmath4 mesh to solve poisson s equation using fourier methods in single precision .",
    "a total of @xmath5 ( 453 m ) particles are used and simulated for 5000 timesteps .",
    "an animation of a fly - through of this dark matter simulation as it structure develops is available at our website  @xcite . in a 256 processor run",
    ", the wallclock time per timestep grows from 80 seconds at the beginning of the simulation to about 200 seconds once clustering develops with cpu time being dominated by the use of trees .",
    "the spatial resolution of these simulations is set by the gravitational softening radius used to shut down the @xmath6 newtonian force at small separations . for this simulation",
    ", we set the softening radius to 1 kpc ( 3000 light years ) so we achieve a spatial resolution about 65 times greater than what could be achieved using a pm method by itself .      we have also applied a pure parallelized n - body treecode  @xcite to study galaxy interactions .",
    "the main mode by which spiral galaxies are transformed into ellipticals is through merging .",
    "when galaxies interact , the ordered energy of rotation and bulk motion of the two spiral galaxies is redistributed in the form of randomly oriented orbits .",
    "this process only requires a few dynamical times and the final structure of the merger remnant resembles and elliptical galaxy .    to study this process at high resolution , we built two models of spiral galaxies composed of a disk , a central bulge and surrounding dark halo using kuijken & dubinski s method  @xcite . to make the calculation more interesting , we used initial conditions chosen to represent a future configuration of the milky way and the andromeda galaxy which will likely merge in about 3 billion years .",
    "each model is composed of 128 m particles representing stars and 25.6 million particles representing the surrounding dark matter halos for a total of 307.2 m particles .",
    "the two galaxies are on a collision course and we computed the trajectories for all particles for 5300 timesteps representing a physical timescale of 2.3 billion years .",
    "a sequence of 9 images showing the evolution of the system over this timescale is shown in figure  [ fig - gal ] .",
    "an animation showing the galaxy collision is available here  @xcite .",
    "the code used for these calculations is partree  @xcite , a parallelized treecode based on salmon s  @xcite original algorithm and ideas of locally essential trees .",
    "the code has recently been modified to incorporate asynchronous message - passing in the construction of the locally essential trees to minimize communication overhead which becomes dominant for large numbers of cpus . with current modifications ,",
    "communication overhead only used 10% of the wallclock time of 180 seconds per step in the 307 m particle simulation .",
    "the code should scale well to thousands of processors in its current form assuming the problem size grows as well .",
    "a several thousand cpu machine with 2gb / cpu is now large enough to follow the trajectory of about 10 billion particles , the same number of stars in some galaxies and only a factor of 40 away from the number of stars in the milky way .",
    "galactic dynamics simulations will reach their ultimate resolution in terms of the number particles representing stars in less than a decade .",
    "we have undertaken mhd simulations with 1400@xmath7 zones arrayed in a uniform cartesian grid , the largest mhd simulations to date ( figure [ fig : mag ] ) . at this resolution ,",
    "each full dimensional sweep corresponding to two timesteps took 40 seconds .",
    "a series of optimizations allows the code to exploit the hardware s vector units , hyperthreading openmp parallelism , and uses the message passing interface ( mpi ) to communicate between nodes .",
    "the code  @xcite is based on a 2nd order accurate in space and time high resolution total - variation - diminishing ( tvd ) algorithm ; it explicitly conserves the sum of kinetic , thermal and magnetic energy ; hence magnetic dissipation ( at the grid scale ) heats gas directly .",
    "no explicit resistivity or viscosity is added , and reconnection and shocks occur through the solution of the flux conservation laws and the tvd constraints .",
    "magnetic flux is conserved to machine precision by storing fluxes perpendicular to each cell face .    each time step requires about 20 seconds , corresponding to 2.2 tflop sustained in single precision .",
    "the operation count was obtained by adding the number of operations in the source code by hand .",
    "this is about 44% of theoretical peak speed achievable with the using the sse2 vector units .",
    "the main parallelization overhead is the need for 16 buffer zones between computational domains .",
    "this increases the floating point count by 65% .",
    "fatter nodes with more processors and more memory would decrease the surface to volume ratio . the domain decomposition was matched to the physical three dimensional connectivity of the network .",
    "most traffic does not require any routing , and up both gigabit ports per node can be effectively exploited .",
    "all message passing latency is hidden by staggering the computation phase : the processes do not need to wait for data , and communication is asynchronous .",
    "the earliest stages of the planet formation process , in which micron sized particles of dust somehow come together to form meter and kilometer sized objects and eventually planets , is very poorly understood .",
    "humble , maddison and murray  @xcite have developed a new parallel two - phase ( dust and gas ) smoothed particle hydrodynamics  @xcite and self - gravity code , so that for the first time we can model the non - linear evolution of two coupled fluids in a 3d protoplanetary dusty disk .",
    "a typical calculation uses 125k gas and 125k dust particles and resolves the motion of the entire disk from @xmath8 to @xmath9au .",
    "we investigate how dust evolves in a so - called minimum mass solar nebula in which the total mass of the dust is @xmath10% of the gas phase , which in turn is @xmath10% the mass of the protostar .",
    "figure  [ fig - contour-1 ] shows the distribution of @xmath10 cm grains after @xmath11 years .",
    "our simulations clearly capture the expected settling and density enhancement of the dust in the midplane of the disk where grain growth is most likely to occur , and where gravitational instabilities may emerge .",
    "the simulations also reveal the new result that the pressure supported differential rotation between gas and dust is maintained only in a relatively narrow region in radius .",
    "rapid evolution of the dust occurs mostly in this narrow band , the location of which is a strong function of the dust grain size .",
    "this results in large variations of structure and dynamics between dusty disks of different characteristic grain radii .",
    "the code is based on the hashed oct tree algorithms of salmon and warren  @xcite , is written in c and mpi and uses hdf5  @xcite for i / o . the code has evolved to be a flexible parallel tree framework , being tunable for minimum bandwidth or maximum speed , and incorporating asynchronous mpi latency hiding techniques using multiple simultaneous tree traversals from each processor . the hash table allows o(@xmath10 ) random access to all data in the tree and acts as a serialisation point for remote data retrieval , whilst more traditional pointers are used for rapid traversal of all locally cached tree data .",
    "most calculations performed to date utilise around @xmath12 processors with @xmath13% parallel efficiency .",
    "this number of processors is currently adequate as this is a new field and there are large regions of parameter space to explore .",
    "a relatively inexpensive machine like the mckenzie cluster is vital so that many runs with different configurations and disk physics can be explored simultaneously in a reasonable time .",
    "we are working on further scalability and speed enhancements of the code .",
    "we have built mckenzie , 512-cpu beowulf cluster that incorporates commodity gigabit networking in a new topology that permits 128 gbit cross - sectional bandwidth .",
    "networking costs represent 9% the total cost of the cluster of $ 561k .",
    "the machine has been benchmarked with the top 500 list hpl code and achieved a sustained speed of @xmath14 teraflops or 48% of the theoretical peak speed making it competitive with clusters with more expensive networking solutions . by this measure ,",
    "our computing costs are just $ 0.47/megaflop .",
    "we note that the idea of using a linux node as a router through a secondary network or even tertiary network is a general concept that can be applied to different network topologies with larger numbers of nodes .",
    "the main limitation of this method is not so much the bandwidth but the extra latency inherent to commodity switches and perhaps multiple hops in the chosen topology .",
    "nevertheless , complex parallel algorithms with high bandwidth requirements can still run on systems like this based on our experience .",
    "we have also described 4 astrophysical parallel applications running on the cluster that span the scale of the universe from solar system scale to the size of the universe .",
    "we have been able to run some of the largest simulations to date in cosmology , galaxy dynamics and mhd accretion flows using mpi parallel codes .",
    "n - body simulations of hundreds of millions of particles in cosmology and galaxy dynamics can be done routinely .",
    "the mhd code achieved a sustained speed of 2.2 teraflops in single precision or 44% of the theoretical peak speed in @xmath15 zone calculation .",
    "our low - cost networking also justifies using the machine in a parameter survey mode on smaller sub - units of the cluster while such strategies might be considered a waste of expensive resources on a large - scale smp .",
    "the ability to run on 512 processors has allowed us to tune and optimize our codes to hide latency for large processor numbers and paves the way for porting codes to clusters of thousands of processors .",
    "we acknowledge the natural sciences and engineering research council of canada ( nserc ) and the canadian foundation for innovation ( cfi ) for funding this project .",
    "barnes , j. & hut , p. 1986 , nature , 324 , 446 dubinski , j. 1996 , new astronomy , 1 , 133 - 147 dubinski , j. , kim , j. , park , c. , humble , r.j .",
    "2003 , astro - ph/0304467 kazushige goto s optimized blas library , http://www.cs.utexas.edu/users/flame/goto/ frigo , m. and johnson s.g .",
    "1998 , fftw : an adaptive software architecture for the fft . in icassp 98 , 3 , 1381 , 1998 .",
    "http://hdf.ncsa.uiuc.edu/ hockney , r. w. , & eastwood , j. w. 1981 , computer simulation using particles ( new york : mcgraw - hill ) kuijken , k. , dubinski , j. 1995 , mnras , 277 , 1341 humble , r. j. , maddison , s. t. , murray , j. r. , 2003 , in preparation monaghan , j. j. , 1997 , jcompphys , 1997 , 138 , 801 http://www.mynix.com the oscar cluster installation package , http://oscar.sourceforge.net pen , u .- l . , matzner , c.d .",
    ", wong , s. 2003 , astro - ph/0304227 pen , u .- l . ,",
    "arras , p. , wong , s. 2003 , in prep ) salmon , j.k . , warren , m.s .",
    ", 1994 , jcompphys , 111 , 136 salmon , j.k .",
    "1990 , phd thesis , caltech http://www.top500.org link to animations , http://www.cita.utoronto.ca/@xmath16dubinski/sc2003"
  ],
  "abstract_text": [
    "<S> we describe a new 512-cpu beowulf cluster with teraflop performance dedicated to problems in computational astrophysics . </S>",
    "<S> the cluster incorporates a cubic network topology based on inexpensive commodity 24-port gigabit switches and point to point connections through the second gigabit port on each linux server . </S>",
    "<S> this configuration has network performance competitive with more expensive cluster configurations and is scaleable to much larger systems using other network topologies . </S>",
    "<S> networking represents only about 9% of our total system cost of usd$561k . </S>",
    "<S> the standard top 500 hpl linpack benchmark rating is 1.202 teraflops on 512 cpus so computing costs by this measure are $ 0.47/megaflop . </S>",
    "<S> we also describe 4 different astrophysical applications using complex parallel algorithms for studying large - scale structure formation , galaxy dynamics , magnetohydrodynamic flows onto blackholes and planet formation currently running on the cluster and achieving high parallel performance . </S>",
    "<S> the mhd code achieved a sustained speed of 2.2 teraflops in single precision or 44% of the theoretical peak . </S>"
  ]
}