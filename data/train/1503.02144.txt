{
  "article_text": [
    "sparse representation has been of significant interest over past few years and has found a variety of applications in practice as many natural signals admit a sparse or an approximate sparse representation in a certain basis @xcite . in many applications such as image denoising and interpolation ,",
    "signals are assumed to admit a sparse representation over a pre - specified non - adaptive dictionary , e.g. discrete consine / wavelet transform ( dct / dwt ) bases .",
    "nevertheless , recent research @xcite has shown that the recovery , denoising and classification performance can be considerably improved by utilizing an adaptive dictionary that is learned from the training signals @xcite .",
    "this has inspired studies on dictionary learning whose objective is to design overcompelete dictionaries that can better represent the signals .",
    "a number of algorithms , such as k - singular value decomposition ( k - svd ) @xcite , method of optimal directions ( mod ) @xcite , dictionary learning with the majorization method @xcite , and simultaneous codeword optimization ( simco ) @xcite , were developed for learning overcomplete dictionaries for sparse representation .",
    "most algorithms formulate the dictionary learning as an optimization problem and solve it via a two - stage iterative process , namely , a sparse coding stage and a dictionary update stage . the main difference between these algorithms lies in the dictionary update stage .",
    "specifically , the mod method @xcite updates the dictionary via solving a least square problem which admits a closed - form for the dictionary update .",
    "the k - svd algorithm @xcite , instead , updates atoms of the dictionary in a sequential manner and while updating each atom , the atom is updated along with the nonzero entries in the corresponding row vector of the sparse matrix .",
    "the idea of this sequential atom update was later extended to sequentially updating multiple atoms each time @xcite , and recently was generalized to parallel atom - updating in order to further accelerate the convergence of the iterative process @xcite .",
    "these methods @xcite , although delivering state - of - the - art performance , require the knowledge of the sparsity level or the noise / residual variance to define the stopping criterion for estimating the sparse codes ( e.g. @xcite ) , or select appropriate values for the regularization parameters controlling the tradeoff between the sparsity level and the data fitting error ( e.g. @xcite ) . in practice ,",
    "however , the prior information about the noise variance is usually unavailable and an inaccurate estimation may result in substantial performance degradation . to mitigate this limitation , a nonparametric bayesian dictionary learning method called as beta - bernoulli process factor analysis ( bpfa )",
    "was recently developed in @xcite .",
    "the proposed method is able to automatically infer the required number of factors ( dictionary elements ) and the noise variance from the image under test , which is deemed as an important advantage over other dictionary learning methods . for @xcite ,",
    "the posterior distributions can not be derived analytically , and a gibbs sampler was used for bayesian inference .",
    "we also note that a class of online dictionary learning algorithms were developed in @xcite .",
    "different from the above batch - based algorithms @xcite which use the whole set of training data for dictionary learning , online algorithms continuously update the dictionary using only one or a small batch of training data , which enables them to handle very large data sets .    in this paper",
    ", we propose a new hierarchical bayesian model for dictionary learning , in which a gaussian - inverse gamma hierarchical prior is used to promote the sparsity of the representation .",
    "suitable priors are also placed on the dictionary and the noise variance such that they can be reasonably inferred from the data .",
    "based on the hierarchical model , a variational bayesian method and a gibbs sampling method are developed for bayesian inference .",
    "for both inference methods , there are two different ways to update the dictionary : we can update the whole set of atoms at once , or update the atoms in a sequential manner .",
    "when updating the dictionary as a whole , the proposed variational bayesian method has a dictionary update formula similar to the mod method . nevertheless , unlike the mod method which alternates between two separate stages ( i.e. dictionary update and sparse coding ) , for our algorithm ,",
    "the dictionary and the signal are refined in an interweaved and gradual manner , which enables the algorithm to come to a reasonably nearby point as the optimization progresses , and helps avoid undesirable local minima .",
    "for the gibbs sampler , a sequential update seems able to expedite the convergence rate and helps achieve better performance .",
    "simulation results show that the proposed gibbs sampling algorithm presents uniform superiority over other state - of - the - art dictionary learning methods in a number of experiments .",
    "the rest of the paper is organized as follows . in section [ sec : model ] , we introduce a hierarchical prior model for learning dictionaries .",
    "based on this hierarchical model , a variational bayesian method and a gibbs sampler are developed in section [ sec : vb ] and section [ sec : gibbs ] for bayesian inference .",
    "simulation results are provided in section [ sec : simulation ] , followed by concluding remarks in section [ sec : conclusion ] .",
    "suppose we have @xmath0 training signals @xmath1 , where @xmath2 .",
    "dictionary learning aims at finding a common sparsifying dictionary @xmath3 such that these @xmath0 training signals admit a sparse representation over the overcomplete dictionary @xmath4 , i.e. @xmath5 where @xmath6 and @xmath7 denote the sparse vector and the residual / noise vector , respectively .",
    "define @xmath8 $ ] , @xmath9 $ ] , and @xmath10 $ ] , the model ( [ model-1 ] ) can be re - expressed as @xmath11 also , we write @xmath12 $ ] , where each column of the dictionary , @xmath13 , is called an atom .    in the following , we develop a bayesian framework for learning the overcomplete dictionary and sparse vectors . to promote sparse representations , we assign a two - layer hierarchical gaussian - inverse gamma prior to @xmath14 .",
    "the gaussian - inverse gamma prior is one of the most popular sparse - promoting priors which has been widely used in compressed sensing @xcite . in the first layer , @xmath14 is assigned a gaussian prior distribution @xmath15 where @xmath16 denotes the @xmath17th entry of @xmath14 , and @xmath18 are non - negative sparsity - controlling hyperparameters .",
    "the second layer specifies gamma distributions as hyperpriors over the hyperparameters @xmath19 , i.e. @xmath20 where @xmath21 is the gamma function , and the parameters @xmath22 and @xmath23 used to characterize the gamma distribution are usually chosen to be small values , e.g. @xmath24 .",
    "as discussed in @xcite , this hyperprior allows the posterior mean of @xmath25 to become arbitrarily large . as a consequence",
    ", the associated coefficient @xmath16 will be driven to zero , thus yielding a sparse solution . in this paper , we choose a value of @xmath26 in order to achieve a more sparsity - encouraging effect .",
    "clearly , the gamma prior with a larger @xmath22 encourages large values of the hyperparameters , and therefore promotes the sparseness of the solution since the larger the hyperparameter , the smaller the variance of the corresponding coefficient .",
    "in addition , in order to prevent the dictionary from becoming infinitely large , we assume the atoms of the dictionary @xmath27 are mutually independent and each atom is placed a gaussian prior , i.e. @xmath28 where @xmath29 is a parameter whose choice will be discussed later .",
    "the noise @xmath30 are assumed independent multivariate gaussian noise with zero mean and covariance matrix @xmath31 , where the noise variance @xmath32 is assumed unknown _ a priori_. to estimate the noise variance , we place a gamma hyperprior over @xmath33 , i.e. @xmath34 where we set @xmath35 and @xmath36 . the proposed hierarchical model ( see fig . [",
    "fig : model ] ) provides a general framework for learning the overcomplete dictionary , the sparse codes , as well as the noise variance . in the following",
    ", we will develop a variational beyesian method and a gibbs sampling method for bayesian inference .",
    "before proceeding , we firstly provide a brief review of the variational bayesian methodology . in a probabilistic model , let @xmath37 and @xmath38 denote the observed data and the hidden variables , respectively .",
    "it is straightforward to show that the marginal probability of the observed data can be decomposed into two terms @xmath39 where @xmath40 and @xmath41 where @xmath42 is any probability density function , @xmath43 is the kullback - leibler divergence between @xmath44 and @xmath42 . since @xmath45",
    ", it follows that @xmath46 is a rigorous lower bound on @xmath47 .",
    "moreover , notice that the left hand side of ( [ variational - decomposition ] ) is independent of @xmath42 .",
    "therefore maximizing @xmath46 is equivalent to minimizing @xmath43 , and thus the posterior distribution @xmath44 can be approximated by @xmath42 through maximizing @xmath46 .",
    "the significance of the above transformation is that it circumvents the difficulty of computing the posterior probability @xmath44 ( which is usually computationally intractable ) . for a suitable choice for the distribution @xmath42",
    ", the quantity @xmath46 may be more amiable to compute .",
    "specifically , we could assume some specific parameterized functional form for @xmath42 and then maximize @xmath46 with respect to the parameters of the distribution .",
    "a particular form of @xmath42 that has been widely used with great success is the factorized form over the component variables @xmath48 in @xmath38 @xcite , i.e. @xmath49 .",
    "we therefore can compute the posterior distribution approximation by finding @xmath42 of the factorized form that maximizes the lower bound @xmath46 .",
    "the maximization can be conducted in an alternating fashion for each latent variable , which leads to @xcite @xmath50 where @xmath51 denotes an expectation with respect to the distributions @xmath52 for all @xmath53 .",
    "we now proceed to perform variational bayesian inference for the proposed hierarchical model .",
    "let @xmath54 denote all hidden variables .",
    "we assume posterior independence among the variables @xmath14 , @xmath55 , @xmath4 and @xmath33 , i.e. @xmath56 with this mean field approximation , the posterior distribution of each hidden variable can be computed by maximizing @xmath46 while keeping other variables fixed using their most recent distributions , which gives @xmath57 where @xmath58 denotes the expectation with respect to ( w.r.t . )",
    "the distributions @xmath59 . in summary ,",
    "the posterior distribution approximations are computed in an alternating fashion for each hidden variable , with other variables fixed .",
    "details of this bayesian inference scheme are provided below .    * _ 1 ) .",
    "update of @xmath60 _ * : the calculation of @xmath60 can be decomposed into a set of independent tasks , with each task computing the posterior distribution approximation for each column of @xmath14 , i.e. @xmath61 .",
    "we have @xmath62 \\rangle_{q_{d}(\\boldsymbol{d } ) q_{\\alpha}(\\boldsymbol{\\alpha } ) q_{\\gamma}(\\gamma ) } \\label{eqn-2}\\end{aligned}\\ ] ] where @xmath63 is the sparsity - controlling hyperparameters associated with @xmath6 , @xmath64 and @xmath65 are respectively given by @xmath66 substituting ( [ eqn-1 ] ) into ( [ eqn-2 ] ) and after some simplifications , it can be readily verified that @xmath67 follows a gaussian distribution @xmath68 with its mean @xmath69 and covariance matrix @xmath70 given respectively as @xmath71 where @xmath72 denotes the expectation w.r.t .",
    "@xmath73 , @xmath74 and @xmath75 denote the expectation w.r.t .",
    "@xmath76 , and @xmath77 , in which @xmath78 represents the expectation w.r.t .",
    "@xmath79 .    * _ 2 ) .",
    "update of @xmath80 _ * : the approximate posterior @xmath76 can be obtained as @xmath81\\rangle_{q_x(\\boldsymbol{x } ) q_{\\gamma}(\\gamma ) } \\nonumber\\\\ \\propto & \\langle -\\gamma \\|\\boldsymbol{y}-\\boldsymbol{d}\\boldsymbol{x}\\|_{f}^2 -\\beta^{-1}\\sum_{n=1}^n",
    "\\boldsymbol{d}_n^t\\boldsymbol{d}_n\\rangle \\nonumber\\\\ \\propto & \\langle -\\gamma\\text{tr}\\{(\\boldsymbol{y}-\\boldsymbol{d}\\boldsymbol{x } ) ( \\boldsymbol{y}-\\boldsymbol{d}\\boldsymbol{x})^t\\}-\\beta^{-1}\\text{tr}\\{\\boldsymbol{d}\\boldsymbol{d}^t \\}\\rangle \\nonumber\\\\ \\propto & \\langle \\text{tr}\\{\\boldsymbol{d}(\\gamma\\boldsymbol{x}\\boldsymbol{x}^t+\\beta^{-1}\\boldsymbol{i } ) \\boldsymbol{d}^t-2\\gamma\\boldsymbol{y}\\boldsymbol{x}^t\\boldsymbol{d}^t\\}\\rangle \\nonumber\\\\ = & \\text{tr}\\{\\boldsymbol{d}(\\langle\\gamma\\rangle\\langle\\boldsymbol{x}\\boldsymbol{x}^t\\rangle+\\beta^{-1}\\boldsymbol{i } ) \\boldsymbol{d}^t-2\\langle\\gamma\\rangle\\boldsymbol{y}\\langle\\boldsymbol{x}\\rangle^t\\boldsymbol{d}^t\\}\\end{aligned}\\ ] ] where for simplicity , we have dropped the subscript of the @xmath82 operator .",
    "define @xmath83 the posterior @xmath76 can be further expressed as @xmath84 where @xmath85 and @xmath86 represents the @xmath87th row of @xmath88 and @xmath4 , respectively .",
    "it can be easily seen from ( [ eqn-3 ] ) that the posterior distribution @xmath80 has independent rows and each row follows a gaussian distribution with its mean and covariance matrix given by @xmath89 and @xmath90 , respectively , i.e. @xmath91    * _ 3 ) .",
    "update of @xmath79 _ * : the variational optimization of @xmath79 yields @xmath92 thus @xmath55 has a form of a product of gamma distributions @xmath93 in which the parameters @xmath94 and @xmath95 are respectively given as @xmath96    * _ 4 ) . update of @xmath73 _ * : the variational optimization of @xmath73 yields @xmath97 therefore @xmath73 follows a gamma distribution @xmath98 with the parameters @xmath99 and @xmath100 given respectively by @xmath101 where @xmath102    in summary , the variational bayesian inference involves updates of the approximate posterior distributions for hidden variables @xmath14 , @xmath4 , @xmath55 , and @xmath33 .",
    "some of the expectations and moments used during the update are summarized as @xmath103)^2+\\boldsymbol{\\sigma}_l^{x}[n , n ] \\nonumber\\\\ \\langle\\boldsymbol{x}\\boldsymbol{x}^t\\rangle=&\\langle\\boldsymbol{x}\\rangle\\langle\\boldsymbol{x}\\rangle^t + \\sum_{l=1}^l\\boldsymbol{\\sigma}_l^{x } \\nonumber\\\\ \\langle\\boldsymbol{d}\\rangle\\stackrel{(b)}{=}&\\boldsymbol{b}\\boldsymbol{a }",
    "\\nonumber\\\\ \\langle\\boldsymbol{d}^t\\boldsymbol{d}\\rangle=&\\langle\\boldsymbol{d}\\rangle^t \\langle\\boldsymbol{d}\\rangle+m\\langle\\boldsymbol{a}\\rangle \\nonumber\\\\ \\langle\\alpha_{nl}\\rangle=&\\tilde{a}/\\tilde{b}_{nl } \\nonumber\\\\ \\langle\\gamma\\rangle=&\\tilde{c}/\\tilde{d } \\nonumber\\end{aligned}\\ ] ] where in @xmath104 , @xmath105 $ ] denotes the @xmath106th entry of @xmath107 , @xmath108 $ ] represents the @xmath106th diagonal element of @xmath109 , and @xmath110 follows from ( [ d - update ] ) . for clarity , we summarize our algorithm as follows .",
    "* sparse bayesian dictionary learning  a variational bayesian algorithm *    [ cols= \" < , < \" , ]     [ table2 ]",
    "we developed a new bayesian hierarchical model for learning the overcomplete dictionaries based on a set of training data .",
    "this new framework can be considered as an adaptation of the conventional sparse beysian learning framework to deal with the dictionary learning problem .",
    "specifically , a gaussian - inverse gamma hierarchical prior is used to promote the sparsity of the representation .",
    "suitable priors are also placed on the dictionary and the noise variance such that they can be reasonably inferred from the data .",
    "we developed a variational bayesian method and a gibbs sampler for bayesian inference . unlike some of previous methods , the proposed methods do not need to assume knowledge of the noise variance _ a priori _ , and",
    "can infer the noise variance automatically from the data .",
    "the performance of the proposed methods is evaluated using synthetic data .",
    "numerical results show that the proposed methods are able to learn the dictionary with an accuracy considerably better than existing methods , particularly for the case where there is a limited number of training signals .",
    "the proposed methods are also applied to image denoising , where superior denoising results are achieved even compared to other state - of - the - art algorithms .",
    "our proposed hierarchical model is also flexible to incorporate additional prior information to enhance the dictionary learning performance .",
    "j.  m. duarte - carvajalino and g.  sapiro , `` learning to sense sparse signals : simultaneous sensing matrix and sparsifying dictionary optimization , '' _ ieee trans . image processing _ , vol .",
    "18 , no .  7 , pp .",
    "13951408 , july 2009 .",
    "j.  wright , a.  y. yang , a.  ganesh , s.  s. sastry , and y.  ma , `` robust face recognition vis sparse representation , '' _ ieee trans .",
    "pattern analysis and machine intelligence _ ,",
    "31 , no .  2 ,",
    "210227 , feb .",
    "m.  aharon , m.  elad , and a.  bruckstein , `` k - svd : an algorithm for designing overcomplete dictionaries for sparse representation , '' _ ieee trans . signal processing _ , vol .",
    "54 , no .  11 , pp . 43114322 , nov .",
    "2006 .",
    "k.  engan , s.  o. aase , and j.  h. hakon - husoy , `` method of optimal directions for frame design , '' in _ ieee international conference on acoustics , speech and signal processing _ , phoenix , az ,",
    "march 15 - 19 1999 .",
    "m.  zhou , h.  chen , j.  paisley , l.  ren , l.  li , z.  xing , d.  dunson , g.  sapiro , and l.  carin , `` nonparametric bayesian dictionary learning for analysis of noisy and incomplete images , '' _ ieee trans .",
    "image processing _ , vol .",
    "21 , no .  1 ,",
    "130144 , jan . 2012 .",
    "k.  labusch , e.  barth , and t.  martinetz , `` robust and fast learning of sparse codes with stochastic gradient descent , '' _ ieee journal of selected topics in signal processing _ , vol .  5 , no .  5 , pp .",
    "10481060 , 2011 ."
  ],
  "abstract_text": [
    "<S> we consider a dictionary learning problem whose objective is to design a dictionary such that the signals admits a sparse or an approximate sparse representation over the learned dictionary . </S>",
    "<S> such a problem finds a variety of applications such as image denoising , feature extraction , etc . in this paper </S>",
    "<S> , we propose a new hierarchical bayesian model for dictionary learning , in which a gaussian - inverse gamma hierarchical prior is used to promote the sparsity of the representation . </S>",
    "<S> suitable priors are also placed on the dictionary and the noise variance such that they can be reasonably inferred from the data . </S>",
    "<S> based on the hierarchical model , a variational bayesian method and a gibbs sampling method are developed for bayesian inference . </S>",
    "<S> the proposed methods have the advantage that they do not require the knowledge of the noise variance _ </S>",
    "<S> a priori_. numerical results show that the proposed methods are able to learn the dictionary with an accuracy better than existing methods , particularly for the case where there is a limited number of training signals .    </S>",
    "<S> dictionary learning , gaussian - inverse gamma prior , variational bayesian , gibbs sampling . </S>"
  ]
}