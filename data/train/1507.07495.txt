{
  "article_text": [
    "hidden markov models ( hmms ) are stochastic models for systems with a set of unobserved states between which the system hops stochastically , sometimes emitting a signal from some alphabet , with probabilities that depend upon the current state .",
    "the situation in which we are specifically interested is human mobility , partially observed , _",
    "i.e. _ , occasional signals about a person s location . for example , consider the cells of a mobile phone network , from which a user can make calls . in this case",
    "the states of a hmm are the cells , and the emitted signals are the cell itself , if a call is made by a particular user during each of a sequence of time intervals , or nothing ( 0 ) , if that user does not make a call . in the latter case , the state ( location ) of the user is ` hidden ' , and must be inferred , while in the former case , assuming no errors in the data , the ` hidden ' state is revealed by the call record . since these are data from a _ mobile _ phone network , a user can move from cell to cell .",
    "although many analyses of human mobility have estimated no more than rather crude statistics like the radius of gyration , the fraction of time spent at each location , or the entropy of the timeseries of locations  @xcite , others have used hmms to describe partially observed human mobility and have estimated their parameters  @xcite . with short time steps , however , a standard hmm ( with time - independent parameters ) is not a plausible model , since human mobility behavior changes according to , for example , the time of day  @xcite .",
    "we would like to create , therefore , a hmm with time-_dependent _ parameters .",
    "of course , allowing , for example , arbitrary transition / emission probabilities at each time step , would lead to an extremely underdetermined model .",
    "rather , we need a model with only a few additional parameters to capture the time - dependence of human mobility . since the total numbers of trips  @xcite and mobile phone calls  @xcite vary with time of day and day of week , we develop a time - dependent hmm in which the non - trivial transition and emission probabilities are proportional to _ activity levels _ , _ i.e. _ , to some given functions modeling how active humans are at different times and places .",
    "since the transition and emission probabilities in our hmm are not constant in time , it is a _ non - stationary _ hmm .",
    "many generalizations of hmms have been considered previously , of course , as more faithful models of various real systems .",
    "some of these are non - stationary : deng , for example , considers a class of models in which the emission probabilities are somewhat non - markovian , depending on a number of previous emissions , and also have polynomial - in - time trend components which are to be estimated  @xcite . _ duration _ hmms ( dhmms ) , first suggested by ferguson  @xcite , allow a sort of non - stationarity in the state transition process by including a randomly chosen duration each time the state changes , _",
    "i.e. _ , a number of time steps without a transition away from that state",
    ". this kind of model has been generalized to make the transition probabilities functions of the number of steps the system has been in the current state  @xcite . in a different direction , since one can think of transitions between the hidden states with different emission probability distributions as a kind of non - stationarity , _ triplet markov chains _ ( tmcs ) include an auxiliary set of underlying states , each of which corresponds to a different stationary regime for a hmm  @xcite .",
    "our approach is different than that of dhmms and tmcs in that the time dependence of the transition and emission probabilities is not intrinsic and random , but rather exogenous and deterministic .",
    "furthermore , unlike deng s models  @xcite , we take the `` trend '' part of the time dependence to be given , not an additional ( set of ) parameter(s ) to be estimated .    formally , our model consists of @xmath0 possible hidden states , and we denote by @xmath1^t$ ] the time series of @xmath2 hidden states ( for any @xmath3 , @xmath4=\\{1,\\ldots , n\\}$ ] ) .",
    "state transitions happen according to a sequence of matrices giving the conditional probabilities of transitions , @xmath5 at each state we observe an emission that takes a value from the set @xmath6\\cup\\{0\\}$ ] , where @xmath7 denotes `` absence of an emission '' .",
    "let @xmath8\\cup\\{0\\})^t$ ] be the series of observed emissions .",
    "the probability of emission @xmath9 at time @xmath10 , from state @xmath11 , is @xmath12    we define time varying _ activity levels _ for state @xmath11 by a pair of functions with non - negative values , @xmath13\\to [ 0,1]^2 $ ] , the _ activity functions _ , which modulate transitions and emissions from the state , respectively .",
    "given a state @xmath11 , the transition probabilities from that state are functions of _ transition parameters _",
    "@xmath14 , @xmath15 $ ] , and the activity level : @xmath16 , i\\neq j } \\tau_{ij } & \\text{if } i = j ,    \\end{array }   \\right.\\ ] ] subject to the constraints that for all @xmath17 $ ] and all @xmath18 $ ] , @xmath19 , i\\neq j}\\tau_{ij } \\leq 1.\\ ] ] in practice we may have _ a priori _ knowledge that some transitions do not occur , so that @xmath20 ( and @xmath21 ) have some entries set to @xmath7 .",
    "for example , with 10 minute time intervals and states representing cells in a mobile phone network , transitions between sufficiently distant cells are precluded .",
    "similarly , we assign emission parameters @xmath22 , @xmath23 $ ] , to each state @xmath11 .",
    "the emission probabilities are @xmath24}\\epsilon_{s j } & \\text{if } s=0 ,    \\end{array }   \\right.\\ ] ] subject to the constraints that for all @xmath17 $ ] and all @xmath18 $ ] , @xmath25}\\epsilon_{s j}\\leq1.\\ ] ]    denote the initial distribution over states by @xmath26 , subject to the constraints @xmath27 and @xmath28 } \\pi_j = 1.\\ ] ]    were this a typical hidden markov model , we could estimate its parameters using the baum - welch algorithm  @xcite . since it is not ,",
    "we develop a novel expectation maximization ( em ) algorithm  @xcite to estimate the parameters @xmath29 , given @xmath30 , @xmath31 and @xmath32 , as follows .",
    "the expectation maximization algorithm maximizes , at each iterative step , the ( expected ) log - likelihood function described below .",
    "let @xmath33 be the set of all possible time series of states and let @xmath34 be the estimate of @xmath35 at the @xmath36-th iteration of the algorithm . @xmath37",
    "the algorithm begins by initializing the parameter estimates in the first ( @xmath38 ) iteration .",
    "then the @xmath39 iteration consists of two steps :    1 .",
    "[ itr1 ] compute the _ expectation value _ of the log - likelihood , using the current ( @xmath40 ) estimate for the parameters : @xmath41                             \\pr(x\\mid y;{\\hat\\theta}^k),\\ ] ] where @xmath42 means @xmath43 in a probability distribution parametrized by @xmath35 .",
    "[ itr2 ] find the parameters that _ maximize _ the expected log - likelihood : @xmath44 subject to constraints in the inequalities  , and  .",
    "as in the regular baum - welch algorithm , we express our computations in terms of certain conditional probabilities based on the parameters estimated at the @xmath40 iteration , @xmath45 some reindexing of eq .   yields the following expression for @xmath46 in terms of these probabilities : @xmath47}\\log(\\pi_j)\\gamma^k_j(1 )       + \\sum_{i , j\\in[n]}\\sum_{t=1}^{t-1 }       \\log\\bigl(a_{ij}(t)\\bigr)\\xi^k_{i j}(t )       + \\sum_{j\\in[n]}\\sum_{t=1}^{t }       \\log\\bigl(b_{y_t j}(t)\\bigr)\\gamma^k_j(t).\\ ] ] we iterate steps  [ itr1 ] and  [ itr2 ] , for which we compute @xmath48 and @xmath49 in eqs .  , and @xmath46 in eq .   above .",
    "we continue until some standard of convergence is achieved .",
    "we then output the final @xmath34 as our estimate of @xmath35 .",
    "[ mainthm ] there is a constrained expectation maximization algorithm giving a sequence of estimates @xmath50 that converges to a critical point of the likelihood function , which is the maximum likelihood estimate @xmath51 for the observed sequence @xmath30 when the initial guess is sufficiently close .",
    "further , to achieve a precision @xmath52 in the estimates , the time complexity of the algorithm is @xmath53 . in particular , for a fixed precision @xmath52 , the time complexity is @xmath54 .",
    "suppose we have the estimates @xmath55 defined in eq .   from step @xmath36 of the algorithm .",
    "we proceed to compute @xmath56 and @xmath57 in eqs .   to begin the next iteration .",
    "just as in the regular baum - welch algorithm , we apply dynamic programming . denote the @xmath40 estimate of the transition matrix by @xmath58 , where @xmath59 similarly , @xmath60 } \\hat\\epsilon^k_{sj } & \\text{if } s=0 .",
    "\\end{array }   \\right .\\ ] ] it is convenient to define @xmath61 to be the diagonal matrix with @xmath62 entry @xmath63 . now compute two sequences of ( co)vectors , @xmath64 and @xmath65 , recursively , as follows :    @xmath66    then denotes the diagonal matrix whose @xmath62 entry is @xmath67 if @xmath68 and @xmath69 otherwise . ]",
    "@xmath70    the estimates for the initial probabilities @xmath71 are the same as in the normal baum - welch algorithm , as is clear from the expression for @xmath46 in eq .  .",
    "thus @xmath72    now notice that all the constraints in   and   necessary to define @xmath46 are implied by the strongest constraints : for all @xmath17 $ ] , @xmath73 where @xmath74 } \\ :",
    "f_{j}(t)$ ] , and @xmath75 } \\epsilon_{s j } \\leq 1,\\ ] ] where @xmath76 } \\ : g_{j}(t)$ ] .    consider the computation of @xmath77 , for @xmath15 $ ] .",
    "it should lie in the domain @xmath78 defined by constraints   and the non - negativity of the parameters .",
    "since these constraints are independent for different @xmath11s , we can consider each @xmath11 separately , and find the optimal parameters @xmath79 by computing the critical points of @xmath46 relative to @xmath80 .",
    "using eqs .   and  , @xmath81 if the left sum in eq .  , @xmath82 , the derivative is nonpositive , so @xmath46 is weakly decreasing and @xmath83 gives its largest value . if the right sum in eq .  , @xmath84 , the derivative is nonnegative , so @xmath46 is weakly increasing and takes its maximum value when @xmath79 is as large as possible , _",
    "i.e. _ , when it saturates constraint  .",
    "we will show how to handle this situation after discussing the generic case which we do next .    assuming then that neither sum in eq .",
    "is @xmath7 , to find the stationary points of @xmath46 we set eq .   to @xmath7 and",
    "solve for @xmath79 .",
    "specifically , @xmath79 must satisfy @xmath85 we note that if @xmath86 , which makes the transition probabilities , @xmath87 , time independent , then the solution to eq .",
    "is the familiar baum - welch solution : @xmath88 for all @xmath89 $ ] . for non - constant activity functions",
    ", however , the solution is more complicated .    since the right side of eq .",
    "is manifestly independent of @xmath90 , the left side must be , too .",
    "let @xmath91 where the last expression uses the antiderivative convention that for a function of @xmath10 denoted by a letter in lower case , the corresponding upper case letter is upper case @xmath92 ; @xmath93 is upper case @xmath94 ; @xmath95 is upper case @xmath96 ; @xmath0 is upper case @xmath97 ; @xmath98 is upper case @xmath99 . ] represents its sum over its domain of definition ( @xmath100 to @xmath101 in this case ) . now",
    "@xmath102 if we denote the probability of _ moving _ away from state @xmath11 by @xmath103 we can write @xmath104 . substituting in eq .  , this gives : @xmath105 or equivalently : @xmath106 we must solve this equation for @xmath107 , whence we can use eq .   to solve for each of the @xmath79 .",
    "since @xmath107 is nonnegative and constraint   must hold , we recast these conditions in terms of @xmath107 as : @xmath108 let @xmath109\\,\\mid\\,\\xi^k_{jj}(t)\\neq0 } \\ : f_{j}(t)\\,\\,\\leq f^*_{j}.\\ ] ] each of the terms in the sum on the right side of eq .   is strictly decreasing in @xmath110 when it is well - defined ( @xmath111 ) .",
    "values of @xmath110 just larger than @xmath112 make the sum arbitrarily large , and as @xmath110 increases from that value , the sum decreases monotonically to 0 , so exactly one value of @xmath113 will satisfy eq .  .",
    "we can solve the equation numerically to find this value , call it @xmath114 . if @xmath115 , splitting it proportionally to @xmath116 according to eq .",
    "gives the _ unique _ critical point @xmath80 of @xmath46 .",
    "we can compute explicitly the hessian of @xmath117 with respect to the @xmath79 ; its components are : @xmath118 thus , as a matrix the hessian can be written as the sum of two matrices : @xmath119 where @xmath120 is the vector of all @xmath69s .",
    "each of the matrices on the right is negative semi - definite , so the hessian is also .",
    "thus the ( unique ) critical point we found in this case is a global maximum of @xmath117 in @xmath121 and hence the choice for @xmath122 .    if the solution does not satisfy the original constraint  , _",
    "i.e. _ , @xmath123 , or if the right side of eq .   is @xmath7 , the maximum will be on the boundary of @xmath121 .",
    "thus we maximize @xmath117 subject to the boundary constraint @xmath124 } \\tau_{ij } = \\frac{1}{f^*_j}.\\ ] ]    this is in the form of the constraint in the regular baum - welch algorithm with 1 replaced by @xmath125 and the self - transition probability set to 0 .",
    "thus the critical @xmath79 can be computed as in the baum - welch algorithm , for all @xmath126 . ] with @xmath127 replaced by @xmath128 and the solution divided by @xmath129 : @xmath130 this is the unique critical point in this case , and the global maximum of @xmath117 in @xmath121 , by the same argument as in the baum - welch algorithm .",
    "this becomes the choice for @xmath122 .",
    "we turn to the computation of @xmath131 , @xmath132 $ ] .",
    "as before , we begin by finding the stationary points of @xmath46 , now relative to @xmath133 defined by constraints   and the non - negativity of these parameters . using eqs .   and   gives : @xmath134}\\epsilon_{lj}}\\delta_{0,y_t}.\\ ] ] as we did for eq .",
    ", we must consider the situations when either of the sums in eq .",
    "vanishes . when the left sum is @xmath7 , a extreme value is given by @xmath135 , and when the right sum is @xmath7 , constraint   is saturated . assuming neither of the sums vanishes , we find the stationary points by solving @xmath136}\\epsilon_{lj}}\\delta_{0,y_t}.\\ ] ] solution to the _ emission _ equations , eqs .  , follows using the same steps as for the _ transition _ equations , eqs .  .",
    "we first denote the probability of emission @xmath132 $ ] from state @xmath11 by : and every state @xmath11 emits either the signal @xmath11 or @xmath7 ; in other words , @xmath137 if @xmath138 $ ] .",
    "this simplifies the following expressions : for @xmath10 such that @xmath139 , @xmath140 ( the state is @xmath11 with certainty if the observed emission is @xmath11 ) , _ i.e. _ , @xmath141 , and @xmath142 if @xmath138 $ ] . ]",
    "@xmath143 in terms of which we rewrite eq .   as @xmath144}\\epsilon_{lj}}.\\",
    "] ] we define ( independent of @xmath9 ) @xmath145 we also define the probability of any _ non - zero _ emission from state @xmath11 , @xmath146}\\lambda^k_{lj}(t),\\ ] ] which , used in eq .  , gives @xmath147 let @xmath148\\,\\mid\\,\\lambda^k_{0j}(t)\\neq0}\\:g_{j}(t)\\,\\,\\leq g^*_{j}.\\ ] ] as before , there is exactly one solution @xmath149 to eq .  .",
    "we can find it numerically , and if @xmath150 , we use eq .   to find all the @xmath151 , which will then satisfy constraint  .",
    "again we can compute the hessian explicitly to confirm that this is a global maximum of @xmath117 , now in @xmath152 , and hence the choice for @xmath153 .",
    "if @xmath154 , or if the right side of eq .   is @xmath7 , we must find instead the critical point on the boundary of @xmath152 : @xmath155 } \\epsilon_{sj } = \\frac{1}{g^*_j}.\\ ] ] again , this is in the form of the constraint in the regular baum - welch algorithm .",
    "accordingly , we set the critical @xmath156 to the baum - welch estimate , rescaled by @xmath157 : @xmath158 this is the unique critical point and the global maximum of @xmath117 in @xmath152 , and therefore the choice for @xmath131 in this case .",
    "we have shown how to find @xmath159 maximizing @xmath46 in eq .  .",
    "this algorithm converges as claimed because it is an instance of expectation maximization . to understand its time complexity we must consider the numerical solution of eqs .   and  .",
    "to simplify notation we rewrite eq .   in terms of @xmath160 , and a function @xmath161 , @xmath162 as @xmath163 .",
    "as an initial estimate for the root , @xmath164 , we can use @xmath165 such that @xmath166 where @xmath167 $ ] satisfies @xmath168 and @xmath169 .",
    "@xmath170 since we found it using only one of the nonnegative terms in the sum in eq .  .",
    "now recall that @xmath171 for @xmath172 $ ] , and @xmath173 } \\mu^k_j(t )    =    \\sum_{t\\in [ t-1 ] } \\sum_{r\\neq j}\\xi^k_{rj}(t )   \\le    \\sum_{t\\in [ t-1 ] } \\gamma^k_{j}(t )    \\le    t-1.\\ ] ] thus each term in the sum in eq .",
    "is no more than @xmath174 at @xmath175 this is @xmath176 , so @xmath177 , which implies @xmath178    using newton s method , once we have an initial estimate `` sufficiently close '' to the root of @xmath163 , the time complexity to find it with error less than @xmath52 is @xmath179 , where the @xmath2 comes from the cost of evaluating @xmath161 and @xmath180 at each iteration ; in practice this is how we would find the root .",
    "since the length of the interval in is @xmath181 , however , the bisection method gets us to precision @xmath52 with @xmath182 steps , with total cost @xmath183 ; thus this is the total complexity .",
    "we need to solve eqs .",
    "and   @xmath0 and @xmath95 times , respectively , at each iteration , which thus adds @xmath184 to the @xmath185 complexity of the computations for @xmath186 and @xmath187 .",
    "thus the time complexity for the whole algorithm is @xmath188 .",
    "to demonstrate the effect of the activity functions we consider a simple model with @xmath189 states and the same number of possible emissions ( @xmath190 ) . from any state @xmath11",
    ", we only allow an emission to be either its own label @xmath11 or @xmath7 , _",
    "i.e. _ , @xmath191 for @xmath192 $ ] , so a non - zero emission uniquely identifies the state that emits it .",
    "we choose random transition and emission parameters : @xmath193 where the omitted values are the components for which @xmath194 .",
    "we generate sequences of length @xmath195 ( we may think of this as @xmath196 weeks , with an observation every @xmath197 minutes ) .",
    "we consider activity functions with variations that may approximate observed data , _",
    "i.e. _ , periodic variations with a period of @xmath198 ( one day ) .",
    "specifically , our numerical simulations use the following three functions :      we generate a random sequence of states , @xmath202 , and resulting emissions , @xmath30 , using the transition and emission parameters above , and a pair of activity functions ( a list of these pairs is shown in table  [ table2 ] ) .    before computing the sequence of parameter estimates ,",
    "we need to specify how we compute initial estimates to start the iteration .",
    "this can only depend on the observed emission sequence @xmath30 , since in any real scenario @xmath202 is unknown . as a first guess , for this simple model",
    ", we interpolate the state sequence @xmath202 as follows : for every pair of successive non - zero emissions , there is a segment of zeros ( no emission ) separating them .",
    "we divide each such segment into two subsegments : let @xmath203 $ ] be the emission immediately preceding the segment , and @xmath204 $ ] be the emission immediately following the segment .",
    "the second subsegment starts at the first time step after the one where @xmath205 first attains its maximum value on the segment ( _ i.e. _ , a time at which there is the maximum probability of hopping from state @xmath11 to state @xmath90 ) .",
    "we assign state @xmath11 to the time steps in the first subsegment and the state @xmath90 to those in the second .",
    "if the emission sequence @xmath30 starts with a segment of zeros , then that segment is assigned the value of the first non - zero emission ; similarly a terminal sequence of zeros is given the value of the last non - zero emission .",
    "denote the interpolated states by @xmath206 , @xmath18 $ ] .    from @xmath207 ,",
    "we compute the estimate @xmath208 for the initial distribution over the states @xmath209 by their frequencies of occurrence . for the initial @xmath79 estimate , @xmath210",
    ", we use the method described in the proof of theorem  [ mainthm ] to solve eq .  , using @xmath211 , including @xmath194 . for the initial @xmath212 estimate , @xmath213",
    ", we also use the method described in the proof of theorem  [ mainthm ] to solve eq .  , using @xmath214 .    to understand the performance of the algorithm in theorem  [ mainthm ]",
    ", we need a measure of the error between the estimates and the real parameter values .",
    "the _ relative entropy _ is one measure for a stationary hmm . in our case",
    "we need to account for the time variation of the transition and emission probabilities , @xmath215 and @xmath216 .",
    "hence we define a modified version of a relative entropy error criterion , the _ averaged relative entropy_.    let @xmath217 and @xmath218 , @xmath18 $ ] , be two finite sequences of discrete probability distributions on a finite set @xmath219 .",
    "the _ averaged relative entropy _ ( @xmath220 ) of @xmath221 with respect to @xmath222 is @xmath223}\\mathsf{re}(p_t , q_t),\\ ] ] where the usual relative entropy ( @xmath224 ) is given by @xmath225    thus the error function that we compute for given @xmath21 and estimate @xmath226 is @xmath227 where @xmath215 and @xmath228 are related through @xmath205 to @xmath79 and @xmath229 by eq .",
    "and eq .  , respectively",
    "similarly , for @xmath230 and estimate @xmath231 , the error function is @xmath232 where @xmath216 and @xmath233 are related through @xmath234 to @xmath235 and @xmath236 by eq .   and eq .  , respectively .",
    "( remember that we are considering the simple case in which the emission @xmath237 when the state @xmath238 . )        to - & a & b & c & d & e & f & g & h + - @xmath205 & @xmath239&@xmath239 & @xmath239 & @xmath240 & @xmath241 & @xmath242 & @xmath240 & @xmath241 + @xmath234&@xmath241&@xmath240&@xmath239&@xmath240&@xmath241&@xmath239&@xmath239&@xmath239 + -    for each set of pairs of activity functions in table  [ table1 ] we run the algorithm for @xmath243 iterations .",
    "figures  [ figtrans ] and  [ figemis ] plot the averaged relative entropy for the parameter estimates as a function of iteration step .",
    "the labels ( a)(h ) to the right of each plot appear in the order of the final error values .",
    "we do not provide a plot showing convergence of @xmath244 since the only noticeable trend is that if they converge to an exact state value , it is usually to the initial state of the interpolated sequence @xmath207 .    in each case",
    "the error for both the transitions and the emissions decreases to small values . since the @xmath220 depends on the activity functions as well as on the parameters and their estimates , we need to compute a baseline error value for each case . for the parameters @xmath21 and a specific choice of @xmath245",
    "it is : @xmath246,\\ ] ] where @xmath215 and @xmath247 are related through @xmath205 to @xmath79 and @xmath248 , respectively , by eq .  , and where @xmath249 $ ] denotes the expectation over uniformly random @xmath250 . to estimate this expectation value , we compute the average @xmath220 of the parameters with respect to 1000 independently chosen sets of random parameters ( rather than their estimates from our algorithm ) , for each case ( a)(h ) . for the emission parameters we compute baselines the same way , using 1000 uniformly random values @xmath251 to estimate @xmath252,\\ ] ] where",
    "@xmath216 and @xmath253 are related through @xmath234 to @xmath235 and @xmath254 , respectively , by eq .  , and where @xmath249 $ ] denotes the expectation value over uniformly random @xmath251 .",
    "the baseline averages thus obtained for function pairs in table  [ table1 ] are recorded in table  [ table2 ] , where the row labels indicate the parameters being baselined .      to - & a & b & c & d & e & f & g & h + - @xmath255&@xmath256 & @xmath257&@xmath258 & @xmath259&@xmath260 & @xmath261&@xmath262 & @xmath263 + @xmath264&@xmath260&@xmath265&@xmath266&@xmath267&@xmath268&@xmath269&@xmath270&@xmath271 + -    we plot these baseline errors as horizontal lines in figures  [ figtrans ] and [ figemis ] .",
    "most of these are too close to be distinguishable ; indeed they are all @xmath272 , in contrast to the estimation errors plots which are almost all smaller by at least an order of magnitude , and in most cases by 3 or 4 , indicating very good parameter estimates .",
    "furthermore , the relative quality of the estimates can be understood : case ( c ) is the standard hmm , for which our algorithm reduces to the baum - welch algorithm  @xcite .",
    "cases ( a ) and ( b ) have greater errors , which is not surprising since they have non - constant emission activity functions , oscillating in value up to 1 .",
    "this means that for each of these cases , non - zero emissions are lower probability events , so there is less information in @xmath30 . possibly surprising is the fact that when the transition activity function is non - constant , cases ( d)(h ) , the errors are _ smaller _ than in the standard hmm case .",
    "but this happens because state changing transitions are reduced , so that each non - zero emission observed provides more information . and among these cases , those with varying emission activity levels have larger errors than those without .    , for successive iterations @xmath36 .",
    "labels to the right are of activity function pairs from table  [ table1 ] , and are displayed in the order of the final error values .",
    "baseline errors , @xmath255 , from table  [ table2 ] are shown as horizontal lines . ]    , for successive iterations @xmath36 .",
    "labels to the right are of activity function pairs from table  [ table1 ] , and are displayed in the order of the final error values .",
    "baseline errors , @xmath264 , from table  [ table2 ] are shown as horizontal lines . ]"
  ],
  "abstract_text": [
    "<S> we define a hidden markov model ( hmm ) in which each hidden state has time - dependent _ activity levels _ that drive transitions and emissions , and show how to estimate its parameters . </S>",
    "<S> our construction is motivated by the problem of inferring human mobility on sub - daily time scales from , for example , mobile phone records . </S>"
  ]
}