{
  "article_text": [
    "suppose @xmath7 is a sparse vector , meaning its number of nonzero entries @xmath0 is smaller than @xmath1 .",
    "the _ support _ of @xmath8 is the locations of the nonzero entries and is sometimes called its _ sparsity pattern_. a common sparse estimation problem is to infer the sparsity pattern of @xmath8 from linear measurements of the form [ eq : yax ] = + , where @xmath9 is a known measurement matrix , @xmath10 represents a vector of measurements and @xmath11 is a vector of measurement errors ( noise ) .",
    "sparsity pattern detection and related sparse estimation problems are classical problems in nonlinear signal processing and arise in a variety of applications including wavelet - based image processing  @xcite and statistical model selection in linear regression  @xcite .",
    "there has also been considerable recent interest in sparsity pattern detection in the context of _ compressed sensing _ , which focuses on large random measurement matrices @xmath12",
    "it is this scenario with random measurements that will be analyzed here .",
    "optimal subset recovery is np - hard  @xcite and usually involves searches over all the @xmath13 possible support sets of @xmath8 .",
    "thus , most attention has focused on approximate methods .",
    "one simple and popular approximate algorithm is orthogonal matching pursuit ( omp )  @xcite .",
    "omp is a greedy method that identifies the location of one nonzero entry of @xmath8 at a time .",
    "a version of the algorithm will be described in detail below in section  [ sec : omp ] .",
    "the best known analysis of the detection performance of omp for large random matrices is due to tropp and gilbert  @xcite . among other results , tropp and gilbert",
    "show that when @xmath12 has i.i.d .",
    "gaussian entries , the measurements are noise - free ( @xmath14 ) , and the number of measurements scales as [ eq : nummeastg ] m ( 1+)4k(n ) for some @xmath15 , the omp method will recover the correct sparse pattern of @xmath8 with a probability that approaches one as @xmath1 and @xmath16 .",
    "the analysis uses a deterministic sufficient condition for success on the matrix @xmath12 based on a greedy selection ratio introduced in  @xcite .",
    "a similar deterministic condition on @xmath12 was presented in  @xcite , and a condition using the restricted isometry property was given in  @xcite .",
    "numerical experiments reported in  @xcite suggest that a smaller number of measurements than ( [ eq : nummeastg ] ) may be sufficient for asymptotic recovery with omp . specifically , the experiments suggest that the constant 4 can be reduced to 2 .    our main result , theorem  [ thm : omp ] below , does a bit better than proving this conjecture .",
    "we show that the scaling in measurements [ eq : nummeasnew ] m ( 1+)2k(n - k ) is sufficient for asymptotic reliable recovery with omp provided both @xmath17 and @xmath16 .",
    "theorem  [ thm : omp ] goes further by allowing uncertainty in the sparsity level @xmath0 .",
    "we also improve upon the tropp ",
    "gilbert analysis by accounting for the effect of the noise @xmath18 . while the tropp ",
    "gilbert analysis requires that the measurements are noise - free , we show that the scaling ( [ eq : nummeasnew ] ) is also sufficient when there is noise @xmath18 , provided the signal - to - noise ratio ( snr ) goes to infinity .",
    "the main significance of the new scaling ( [ eq : nummeasnew ] ) is that it exactly matches the conditions for sparsity pattern recovery using the well - known lasso method .",
    "the lasso method , which will be described in detail in section  [ sec : lasso ] , is based on a convex relaxation of the optimal detection problem .",
    "the best analysis of sparsity pattern recovery with lasso is due to wainwright  @xcite .",
    "he showed in  @xcite that under a similar high snr assumption , the scaling ( [ eq : nummeasnew ] ) in number of measurements is both necessary and sufficient for asymptotic reliable sparsity pattern detection .",
    "determines the sequences of regularization parameters for which asymptotic almost sure success is achieved , and the regularization parameter sequence affects the sufficient number of measurements . ]",
    "the lasso method is often more complex than omp , but it is widely believed to offset this disadvantage with superior performance  @xcite .",
    "our results show that , at least for sparsity pattern recovery under our asymptotic assumptions , omp performs at least as well as lasso .",
    "hence , the additional complexity of lasso for these problems may not be warranted .",
    "neither lasso nor omp is the best known approximate algorithm for sparsity pattern recovery .",
    "for example , where there is no noise in the measurements , the lasso minimization ( [ eq : xhatlasso ] ) can be replaced by @xmath19 a well - known analysis due to donoho and tanner  @xcite shows that , for i.i.d .",
    "gaussian measurement matrices , this minimization will recover the correct vector with [ eq : minmeasbp ] m 2k(n / m ) when @xmath20 .",
    "this scaling is fundamentally better than the scaling ( [ eq : nummeasnew ] ) achieved by omp and lasso .",
    "there are also several variants of omp that have shown improved performance .",
    "the cosamp algorithm of needell and tropp  @xcite and subspace pursuit algorithm of dai and milenkovic  @xcite achieve a scaling similar to ( [ eq : minmeasbp ] ) .",
    "other variants of omp include the stagewise omp  @xcite and regularized omp  @xcite .",
    "indeed with the recent interest in compressed sensing , there is now a wide range of promising algorithms available .",
    "we do not claim that omp achieves the best performance in any sense .",
    "rather , we simply intend to show that both omp and lasso have similar performance in certain scenarios .",
    "our proof of ( [ eq : nummeasnew ] ) follows along the same lines as tropp and gilbert s proof of ( [ eq : nummeastg ] ) , but with two key differences .",
    "first , we account for the effect of the noise by separately considering its effect in the `` true '' subspace and its orthogonal complement .",
    "second and more importantly , we address the `` nasty independence issues '' noted by tropp and gilbert  @xcite by providing a tighter bound on the maximum correlation of the incorrect vectors .",
    "specifically , in each iteration of the omp algorithm , there are @xmath17 possible incorrect vectors that the algorithm can choose .",
    "since the algorithm runs for @xmath0 iterations , there are total of @xmath21 possible error events .",
    "the tropp and gilbert proof bounds the probability of these error events with a union bound , essentially treating them as statistically independent .",
    "however , here we show that energies on any one of the incorrect vectors across the @xmath0 iterations are correlated .",
    "in fact , they are precisely described by samples of a certain normalized brownian motion . exploiting this correlation",
    "we show that the tail bound on error probability grows as @xmath17 , not @xmath21 , independent events .",
    "the outline of the remainder of this paper is as follows .",
    "section  [ sec : omp ] describes the omp algorithm .",
    "our main result , theorem  [ thm : omp ] , is stated in section  [ sec : anal ] . a comparison to lasso",
    "is provided in section  [ sec : lasso ] , and we suggest some future problems in section  [ sec : conclusions ] .",
    "the proof of the main result is somewhat long and given in the section  [ sec : proof ] .",
    "the main result was first reported in  @xcite .",
    "to describe the algorithm , suppose we wish to determine the vector @xmath8 from a vector @xmath22 of the form ( [ eq : yax ] ) .",
    "let [ eq : itrue ] = \\ {  j  :  x_j 0  } , which is the support of the vector @xmath8",
    ". the set @xmath23 will also be called the _",
    "sparsity pattern_. let @xmath24 , which is the number of nonzero entries of @xmath8 .",
    "the omp algorithm produces a sequence of estimates @xmath25 , @xmath26 , of the sparsity pattern @xmath23 , adding one index at a time . in the description",
    "below , let @xmath27 denote the @xmath28th column of @xmath12 .",
    "[ algo : omp ] given a vector @xmath10 , a measurement matrix @xmath9 , and threshold level @xmath29 , compute an estimate @xmath30 of the sparsity pattern of @xmath8 as follows :    1 .",
    "initialize @xmath31 and @xmath32 .",
    "2 .   compute @xmath33 , the projection operator onto the orthogonal complement of the span of @xmath34 .",
    "3 .   for each @xmath28 , compute @xmath35 and let [ eq : rhomax ] [ ^*(t),i^*(t ) ] = _ j=1,  ,n ( t , j ) , where @xmath36 is the value of the maximum and @xmath37 is an index that achieves the maximum .",
    "4 .   if @xmath38 , set @xmath39 .",
    "also , increment @xmath40 and return to step 2 .",
    "otherwise stop .",
    "the final estimate of the sparsity pattern is @xmath41 .",
    "note that since @xmath33 is the projection onto the orthogonal complement of the span of @xmath42 , for all @xmath43 we have @xmath44 .",
    "hence , @xmath45 for all @xmath43 , and therefore the algorithm will not select the same vector twice .",
    "the algorithm above only provides an estimate , @xmath30 , of the sparsity pattern of @xmath23 . using @xmath30",
    ", one can estimate the vector @xmath8 in a number of ways .",
    "for example , one can take the least - squares estimate , [ eq : xhatls ] = - ^2 where the minimization is over all vectors @xmath46 such @xmath47 for all @xmath48 .",
    "the estimate @xmath49 is the projection of the noisy vector @xmath22 onto the space spanned by the vectors @xmath50 with @xmath51 in the sparsity pattern estimate @xmath30 .",
    "this paper only analyzes the sparsity pattern estimate @xmath30 itself , and not the vector estimate @xmath49 .",
    "we analyze the omp algorithm in the previous section under the following assumptions .    [ as : omp ] consider a sequence of sparse recovery problems , indexed by the vector dimension @xmath1 . for each @xmath1 , let @xmath7 be a deterministic vector",
    ". also assume :    * the sparsity level @xmath52 ( i.e. , number of nonzero entries in @xmath8 ) satisfies [ eq : kbnd ] k(n ) for some deterministic sequences @xmath53 and @xmath54 with @xmath55 as @xmath3 and @xmath56 for all @xmath1 . *",
    "the number of measurements @xmath57 is a deterministic sequence satisfying [ eq : nummeas ] m ( 1 + ) 2(n- ) for some @xmath15 . *",
    "the minimum component power @xmath58 satisfies [ eq : xminlim ] _",
    "n k^2 = , where [ eq : xmin ] = _ j |x_j| is the magnitude of the smallest nonzero entry of @xmath8 . * the powers of the vectors",
    "@xmath59 satisfy [ eq : xpowbnd ] _",
    "n ( 1 + ^2 ) = 0 for all @xmath60 . *",
    "the vector @xmath22 is a random vector generated by ( [ eq : yax ] ) where @xmath12 and @xmath18 have i.i.d .",
    "gaussian entries with zero mean and variance @xmath61 .",
    "assumption  [ as : omp](a ) provides a range on the sparsity level @xmath0 .",
    "as we will see below in section  [ sec : musel ] , bounds on this range are necessary for proper selection of the threshold level @xmath29 .    assumption  [ as : omp](b ) is the scaling law on the number of measurements that we will show is sufficient for asymptotic reliable recovery . in the special case when @xmath0 is known so that @xmath62 , we obtain the simpler scaling law [ eq : nummeasnorange ] m ( 1 + ) 2k(n - k ) . we have contrasted this scaling law with the tropp ",
    "gilbert scaling law ( [ eq : nummeastg ] ) in section  [ sec : intro ] .",
    "we will also compare it to the scaling law for lasso in section  [ sec : lasso ] .",
    "assumption  [ as : omp](c ) is critical and places constraints on the smallest component magnitude .",
    "the importance of the smallest component magnitude in the detection of the sparsity pattern was first recognized by wainwright  @xcite .",
    "also , as discussed in  @xcite , the condition requires that signal - to - noise ratio ( snr ) goes to infinity .",
    "specifically , if we define the snr as @xmath63 then under assumption  [ as : omp](e ) it can be easily checked that [ eq : snrnorm ] = ^2 .",
    "since @xmath8 has @xmath0 nonzero entries , @xmath64 , and therefore condition ( [ eq : xminlim ] ) requires that @xmath65 . for this reason",
    ", we will call our analysis of omp a high - snr analysis .",
    "the analysis of omp with snr that remains bounded above is an interesting open problem .",
    "assumption ( d ) is technical and simply requires that the snr does not grow too quickly with @xmath1 .",
    "note that even if @xmath66 for any @xmath67 , assumption  [ as : omp](d ) will be satisfied .",
    "assumption  [ as : omp](e ) states that our analysis concerns large gaussian measurement matrices @xmath12 and gaussian noise @xmath18 .",
    "our main result is as follows .",
    "[ thm : omp ] under assumption  [ as : omp ] , there exists a sequence of threshold levels @xmath68 such that the omp method in algorithm  [ algo : omp ] will asymptotically detect the correct sparsity pattern in that @xmath69 moreover , the threshold levels @xmath70 can be selected simply as a function of @xmath71 , @xmath72 , @xmath1 , @xmath73 and @xmath74 .",
    "theorem  [ thm : omp ] provides our main scaling law for omp .",
    "the proof is given in section  [ sec : proof ] .",
    "it is useful to compare the scaling law ( [ eq : nummeasnorange ] ) to the number of measurements required by the widely - used lasso method described for example in  @xcite .",
    "the lasso method finds an estimate for the vector @xmath8 in ( [ eq : yax ] ) by solving the quadratic program [ eq : xhatlasso ] = _ ^n -^2 + _ 1 , where @xmath29 is an algorithm parameter that trades off the prediction error with the sparsity of the solution .",
    "lasso is sometimes referred to as basis pursuit denoising  @xcite . while the optimization ( [ eq : xhatlasso ] ) is convex , the running time of lasso is significantly longer than omp unless @xmath12 has some particular structure  @xcite .",
    "however , it is generally believed that lasso has superior performance .",
    "the best analysis of lasso for sparsity pattern recovery for large random matrices is due to wainwright  @xcite .",
    "there , it is shown that with an i.i.d.gaussian measurement matrix and white gaussian noise , the condition ( [ eq : nummeasnorange ] ) is _ necessary _ for asymptotic reliable detection of the sparsity pattern .",
    "in addition , under the condition ( [ eq : xminlim ] ) on the minimum component magnitude , the scaling ( [ eq : nummeasnorange ] ) is also sufficient .",
    "we thus conclude that omp requires an identical scaling in the number of measurements to lasso .",
    "therefore , at least for sparsity pattern recovery from measurements with large random gaussian measurement matrices and high snr , there is no additional performance improvement with the more complex lasso method over omp .",
    "in many problems , the sparsity level @xmath0 is not known _ a priori _ and must be detected as part of the estimation process . in omp ,",
    "the sparsity level of the estimate vector is precisely the number of iterations conducted before the algorithm terminates .",
    "thus , reliable sparsity level estimation requires a good stopping condition .",
    "when the measurements are noise - free and one is concerned only with exact signal recovery , the optimal stopping condition is simple : the algorithm should simply stop whenever there is no more error ; that is , @xmath75 in ( [ eq : rhomax ] )",
    ". however , with noise , selecting the correct stopping condition requires some care .",
    "the omp method as described in algorithm  [ algo : omp ] uses a stopping condition based on testing if @xmath38 for some threshold @xmath70 .",
    "one of the appealing features of theorem  [ thm : omp ] is that it provides a simple sufficient condition under which this threshold mechanism will detect the correct sparsity level .",
    "specifically , theorem  [ thm : omp ] provides a range @xmath76 $ ] under which there exists a threshold such that the omp algorithm will terminate in the correct number of iterations .",
    "the larger the number of measurements @xmath73 , the wider one can make the range @xmath77 $ ] .",
    "the formula for the threshold level is given later in ( [ eq : mudef ] ) .",
    "in practice , one may deliberately want to stop the omp algorithm with fewer iterations than the `` true '' sparsity level .",
    "as the omp method proceeds , the detection becomes less reliable and it is sometimes useful to stop the algorithm whenever there is a high chance of error .",
    "stopping early may miss some small entries , but it may result in an overall better estimate by not introducing too many erroneous entries or entries with too much noise . however , since our analysis is only concerned with exact sparsity pattern recovery , we do not consider this type of stopping condition .",
    "to verify the above analysis , we simulated the omp algorithm with fixed signal dimension @xmath78 and different sparsity levels @xmath0 , numbers of measurements @xmath73 , and randomly - generated vectors @xmath8 .    in the first experiment ,",
    "@xmath7 was generated with @xmath0 randomly placed nonzero values , with all the nonzero entries having the same magnitude @xmath79 for some @xmath80 .",
    "following assumption  [ as : omp](e ) , the measurement matrix @xmath9 and noise vector @xmath11 were generated with i.i.d .",
    "@xmath81 entries . using ( [ eq : snrnorm ] ) and the fact that @xmath8 has @xmath0 nonzero entries with power @xmath82 ,",
    "the snr is given by @xmath83 so the snr can be controlled by varying @xmath84 .",
    "[ fig : ompsim ] plots the probability that the omp algorithm incorrectly detected the sparsity pattern for different values of @xmath0 and @xmath73 .",
    "the probability is estimated with 1000 monte carlo simulations per @xmath85 pair . for each @xmath0 and @xmath73 ,",
    "the threshold level @xmath70 was selected as the one with the lowest probability of error , assuming , of course , that the same @xmath70 is used across all 1000 monte carlo runs .",
    "the solid curve in fig .  [ fig : ompsim ] is the theoretical number of measurements in ( [ eq : nummeasnorange ] ) from theorem  [ thm : omp ] that guarantees exact sparsity recovery .",
    "the formula is theoretically valid as @xmath3 and @xmath65 . at finite problem sizes , the probability of error for @xmath73 satisfying ( [ eq : nummeasnorange ] )",
    "will be nonzero",
    ". however , fig .",
    "[ fig : ompsim ] shows that for the problem size in the simulation , the probability of error for omp is indeed low for values of @xmath73 greater than the theoretical level .",
    "when there is no noise ( i.e.  @xmath86 ) , the probability of error is between 3 and 5% for most values of @xmath0 .",
    "when the snr is 20 db , the probability of error is between 15 and 20% . in either case",
    ", the formula provides a reasonable prediction of the threshold in the number of measurements at which the omp method succeeds .",
    "theorem  [ thm : omp ] is only a _",
    "sufficient condition_. it is possible that for some @xmath8 , omp could require a number of measurements less than predicted by ( [ eq : nummeasnorange ] ) .",
    "that is , the number of measurements ( [ eq : nummeasnorange ] ) may not be _",
    "necessary_.    to illustrate such a case , we consider vectors with a nonzero dynamic range of component magnitudes .",
    "[ fig : ompsimpow ] shows the probability of sparsity pattern detection as a function of @xmath73 for vectors @xmath8 with different dynamic ranges .",
    "specifically , the @xmath0 nonzero entries of @xmath8 were chosen to have powers uniformly distributed in a range of 0 , 10 and 20 db . in this simulation , we used @xmath87 and @xmath78 , so the sufficient condition predicted by ( [ eq : nummeasnorange ] ) is @xmath88 .",
    "when the dynamic range is 0 db , all the nonzero entries have equal magnitude , and the probability of error at the value @xmath89 is approximately 3% .",
    "however , with a dynamic range of 10 db , the same probability of error can be achieved with @xmath90 measurements , a value significantly below the sufficient condition in ( [ eq : nummeasnorange ] ) . with a dynamic range of 20 db ,",
    "the number of measurements decreases further to @xmath91 .",
    "this possible benefit of dynamic range in omp - like algorithms has been observed in  @xcite and in sparse bayesian learning  @xcite .",
    "a valuable line of future research would be to see if this benefit can be quantified .",
    "that is , it would be useful to develop a sufficient condition tighter than ( [ eq : nummeasnorange ] ) that accounts for the dynamic range of the signals .",
    "we have provided an improved scaling law on the number of measurements for asymptotic reliable sparsity pattern detection with omp .",
    "most importantly , the scaling law exactly matches the scaling needed by lasso under similar conditions .",
    "however , much about the performance of omp is still not fully understood .",
    "most importantly , our analysis is limited to high snr",
    ". it would be interesting to see if reasonable sufficient conditions can be derived for finite snr as well .",
    "also , our analysis has been restricted to exact sparsity pattern recovery . however , in many problems , especially with noise , it is not necessary to detect every element in the sparsity pattern",
    ". it would be useful if partial support recovery results such as those in  @xcite can be obtained for omp .",
    "the main difficulty in analyzing omp is the statistical dependencies between iterations in the omp algorithm . following along the lines of the tropp ",
    "gilbert proof in  @xcite , we avoid these difficulties by considering the following alternate `` genie '' algorithm .",
    "a similar alternate algorithm is analyzed in  @xcite as well .",
    "1 .   initialize @xmath31 and @xmath92 .",
    "2 .   compute @xmath93 , the projection operator onto the orthogonal complement of the span of @xmath94 .",
    "3 .   for all @xmath95 , compute [ eq : rhotrue ] ( t , j ) = , and let [ eq : rhomaxtrue ] [ ^*(t),i^*(t ) ] = _ j ( t , j ) .",
    "if @xmath96 , set @xmath97 . increment @xmath40 and return to step 2 .",
    "otherwise stop .",
    "the final estimate of the sparsity pattern is @xmath98 .",
    "this `` genie '' algorithm is identical to the regular omp method in algorithm  [ algo : omp ] , except that it runs for precisely @xmath0 iterations as opposed to using a threshold @xmath70 for the stopping condition .",
    "also , in the maximization in ( [ eq : rhomaxtrue ] ) , the genie algorithm searches over only the correct indices @xmath99 .",
    "hence , this genie algorithm can never select an incorrect index @xmath100 . also ,",
    "as in the regular omp algorithm , the genie algorithm will never select the same vector twice for almost all vectors @xmath22 . therefore , after @xmath0 iterations , the genie algorithm will have selected all the @xmath0 indices in @xmath23 and terminate with correct sparsity pattern estimate @xmath101 with probability one .",
    "the reason to consider the sequences @xmath93 and @xmath102 instead of @xmath33 and @xmath25 is that the quantities @xmath93 and @xmath102 depend only on the vector @xmath22 and the columns @xmath27 for @xmath99 .",
    "the vector @xmath22 also only depends on @xmath27 for @xmath99 and the noise vector @xmath18 .",
    "hence , @xmath93 and @xmath102 are statistically independent of all the columns @xmath27 , @xmath100 .",
    "this property will be essential in bounding the `` false alarm '' probability to be defined shortly .",
    "now , a simple induction argument shows that if    _ t=0,  ,k-1 _ j ( t , j ) & > & , [ eq : rhocondmd ] + _ t=0,  ,k _ j ( t , j ) & < & , [ eq : rhocondfa ]    then the regular omp algorithm , algorithm  [ algo : omp ] , will terminate in @xmath0 iterations .",
    "moreover , for all @xmath103 , the omp algorithm will output @xmath104 , @xmath105 , and @xmath106 for all @xmath103 and @xmath28",
    ". this will in turn result in the omp algorithm detecting the correct sparsity pattern @xmath107 so , we need to show that the two events in ( [ eq : rhocondmd ] ) and ( [ eq : rhocondfa ] ) occur with high probability .    to this end ,",
    "define the following two probabilities : & = & ( _ t=0,  k-1 _ j ( t , j ) ) [ eq : pmd ] + & = & ( _ t=0,  k _ j ( t , j ) ) [ eq : pfa ] both probabilities are implicitly functions of @xmath1 .",
    "the first term , @xmath108 , can be interpreted as a `` missed detection '' probability , since it corresponds to the event that the maximum correlation energy @xmath109 on the correct vectors @xmath99 falls below the threshold .",
    "we call the second term @xmath110 the `` false alarm '' probability since it corresponds to the maximum energy on one of the `` incorrect '' indices @xmath100 exceeding the threshold .",
    "the above arguments show that @xmath111 so we need to show that there exists a sequence of thresholds @xmath112 , such that @xmath113 and @xmath114 as @xmath3",
    ". we will define the threshold level in section  [ sec : mudef ] .",
    "sections  [ sec : decomp ] and  [ sec : pmd ] then prove that @xmath113 with this threshold .",
    "the difficult part of the proof is to show @xmath114 .",
    "this part is proven in section  [ sec : pfa ] after some preliminary results in sections  [ sec : proof - brownian ] and  [ sec : proof - projections ] .",
    "we will first select the threshold sequence @xmath115 . given @xmath15 in ( [ eq : nummeas ] )",
    ", let @xmath60 such that [ eq : epsdef ] 1 + .",
    "then , define the threshold level [ eq : mudef ] = ( n ) = ( n- ) .",
    "observe that since @xmath116 , ( [ eq : mudef ] ) implies that [ eq : mubndlow ] ( n - k ) .",
    "also , since @xmath117 , ( [ eq : nummeas ] ) , ( [ eq : epsdef ] ) and ( [ eq : mudef ] ) show that [ eq : mubndhi ] .      to bound the missed detection probability , it is easiest to analyze the omp algorithm in two separate subspaces : the span of the vectors @xmath118 , and its orthogonal complement .",
    "this subsection defines some notation for this orthogonal decomposition and proves some simple bounds .",
    "the actual limit of the missed detection probability will then be evaluated in the next subsection , section  [ sec : pmd ] .",
    "assume without loss of generality @xmath119 , so that the vector @xmath8 is supported on the first @xmath0 elements .",
    "let @xmath120 be the @xmath121 matrix formed by the @xmath0 correct columns : @xmath122.\\ ] ] also , let @xmath123'$ ] be the vector of the @xmath0 nonzero entries so that [ eq : phix ] = _ true .",
    "now rewrite the noise vector @xmath18 as [ eq : wdecomp ] = + ^where [ eq : vwdef ] = ( )^-1 ,   ^= - .",
    "the vectors @xmath124 and @xmath125 are , respectively , the projections of the noise vector @xmath18 onto the @xmath0-dimensional range space of @xmath120 and its orthogonal complement .",
    "combining ( [ eq : phix ] ) with ( [ eq : wdecomp ] ) , we can rewrite ( [ eq : yax ] ) as [ eq : yaxdecomp ] = + ^ , where [ eq : zdef ] = _ true+ .",
    "we begin by computing the limit of the norms of the measurement vector @xmath22 and the projected noise vector @xmath125 .",
    "[ lem : wperpnorm ] the limits _",
    "n & = & 1 , + _",
    "n ^^2 & = & 1 , hold almost surely and in probability .",
    "the vector @xmath18 is gaussian , zero mean and white with variance @xmath61 per entry .",
    "therefore , its projection , @xmath125 , will also be white in the @xmath126-dimensional orthogonal complement of the range of @xmath120 with variance @xmath61 per dimension .",
    "therefore , by the strong law of large numbers @xmath127 where the last step follows from the fact that ( [ eq : nummeas ] ) implies that @xmath128 .",
    "similarly , it is easily verified that since @xmath12 and @xmath18 have i.i.d .",
    "gaussian entries with variance @xmath61 , the vector @xmath22 is also i.i.d .",
    "gaussian with per - entry variance @xmath129 .",
    "again , the strong law of large numbers shows that @xmath130    we next need to compute the minimum singular value of @xmath120 .",
    "[ lem : svd ] let @xmath131 and @xmath132 be the minimum and maximum singular values of @xmath120 , respectively .",
    "then @xmath133 where the limits are in probability .",
    "since the matrix @xmath120 has @xmath81 i.i.d .",
    "entries , the marenko  pastur theorem  @xcite states that _",
    "n ( ) & = & _ n 1- + _ n ( ) & = & _ n 1 + where the limits are in probability .",
    "the result now follows from ( [ eq : nummeas ] ) which implies that @xmath128 as @xmath3 .",
    "we can also bound the singular values of submatrices of @xmath120 .",
    "given a subset @xmath134 , let @xmath135 be the submatrix of @xmath120 formed by the columns @xmath50 for @xmath136 . also , let @xmath137 be the projection onto the orthogonal complement of the span of the set @xmath138 .",
    "we have the following bound .",
    "[ lem : singsub ] let @xmath139 and @xmath140 be any two disjoint subsets of indices such that @xmath141 then , @xmath142    the matrix @xmath143 $ ] is identical to @xmath120 except that the columns may be permuted . in particular , @xmath144 .",
    "therefore , &= & + & & ^2()i + & = & ^2()i + & & .",
    "the schur complement ( see , for example  @xcite ) now shows that @xmath145 or equivalently , @xmath146 the result now follows from the fact that @xmath147    we also need the following tail bound on chi - squared random variables .",
    "[ lem : chisqmax ] suppose @xmath148 , @xmath149 , is a sequence of real - valued , scalar gaussian random variables with @xmath150 .",
    "the variables need not be independent .",
    "let @xmath151 be the maximum @xmath152 then @xmath153 where the limit is in probability .",
    "see for example  @xcite .",
    "this bound permits us to bound the minimum component of @xmath154 .",
    "[ lem : zmin ] let @xmath155 be the minimum component value [ eq : zmin ] = _ j = 1,  ,k |z_j| .",
    "then @xmath156 where the limit is in probability and @xmath157 is defined in ( [ eq : xmin ] ) .    since @xmath18 is zero mean and gaussian ,",
    "so is @xmath46 as defined in ( [ eq : vwdef ] ) .",
    "also , the covariance of @xmath46 is bounded above by & & ( )^-1 ( ) ()^-1 + & & ( )^-1 + & & ^-2 ( ) , where ( a ) follows from the definition of @xmath46 in ( [ eq : vwdef ] ) ; ( b ) follows from the assumption that @xmath158 = ( 1/m)i_m$ ] ; and ( c ) is a basic property of singular values .",
    "this implies that for every @xmath159 , @xmath160 applying lemma  [ lem : chisqmax ] shows that [ eq : vmaxlim ] _ k 1 , where @xmath161 therefore , + & = & _ n ( ) ( ) + & & _ n ( ) ( ) + & & _ n   + & & _",
    "n   + & & _",
    "n   + & & 0 , where all the limits are in probability and ( a ) follows from lemma  [ lem : svd ] ; ( b ) follows from ( [ eq : vmaxlim ] ) ; ( c ) follows from the fact that @xmath162 and hence @xmath163 ; ( d ) follows from ( [ eq : nummeas ] ) ; and ( e ) follows from ( [ eq : xminlim ] ) .",
    "now , for @xmath164 , @xmath165 and therefore , @xmath166 hence , @xmath167 where again the limit is in probability .      with the bounds in the previous section , we can now show that the probability of missed detection goes to zero .",
    "the proof is similar to tropp and gilbert s proof in  @xcite with some modifications to account for the noise .",
    "for any @xmath168 , let @xmath169 , which is the set of indices @xmath99 that are _ not _ yet detected in iteration @xmath103 of the genie algorithm in section  [ sec : outline ] .",
    "then [ eq : phiza ] = _ ( t)_(t ) + _ j(t)_j(t ) , where ( using the notation of the previous subsection ) , @xmath135 denotes the submatrix of @xmath120 formed by the columns with indices @xmath136 , and @xmath170 denotes the corresponding subvector .",
    "now since @xmath93 is the projection onto the orthogonal complement of the span of @xmath171 , [ eq : phizb ] ( t)_(t ) = 0 . also , since @xmath125 is orthogonal to @xmath50 for all @xmath172 and @xmath173 , [ eq : pwperp ] ( t)^= ^. therefore , ( t ) & & ( t)(+ ^ ) + & & ( t)(_j(t)_j(t ) + ^ ) + & & ( t)_j(t)_j(t ) + ^ , [ eq : pydecomp ] where ( a ) follows from ( [ eq : yaxdecomp ] ) ; ( b ) follows from ( [ eq : phiza ] ) and ( [ eq : phizb ] ) ; and ( c ) follows from ( [ eq : pwperp ] ) .    now using ( [ eq : pwperp ] ) and",
    "the fact that @xmath125 is orthogonal to @xmath50 for all @xmath172 , we have [ eq : awperp ] _ i(t)^= _",
    "i^= 0 for all @xmath172 .",
    "since the columns of @xmath174 are formed by vectors @xmath50 with @xmath172 , [ eq : pjwperp ] _",
    "j(t)(t)^= 0 . combining ( [ eq : pjwperp ] ) and ( [ eq : pydecomp ] ) , [ eq : pynorm ] ( t)^2 = ( t)_j(t)_j(t)^2 + ^^2 . now for all @xmath103 , we have that + & & _ j |_j(j)|^2 + & & _",
    "j j(t ) |_j(j)|^2 + & & _ j(t)(j)^2 _ + & & _ j(t)(j)^2_2 + & & + & & + & & + & & + & & , [ eq : rhomda ] where ( a ) follows from the definition of @xmath109 in ( [ eq : rhotrue ] ) ; ( b ) follows from the fact that @xmath175 for all @xmath176 and",
    "hence the maximum will occur on the set @xmath177 ; ( c ) follows from the fact that @xmath174 is the matrix of the columns @xmath27 with @xmath178 ; ( d ) follows the bound that @xmath179 for any @xmath180 ; ( e ) follows ( [ eq : pydecomp ] ) and ( [ eq : pjwperp ] ) ; ( f ) follows from ( [ eq : pynorm ] ) ; ( g ) follows from the fact that @xmath93 is a projection operator and hence , @xmath181 ( h ) follows from lemma  [ lem : singsub ] ; and ( i ) follows from the bound @xmath182 and @xmath183 .",
    "therefore , + & & _",
    "n + & & 1 + , [ eq : rhomdb ] where ( a ) follows from ( [ eq : rhomda ] ) , ( b ) follows from lemmas  [ lem : wperpnorm ] and  [ lem : svd ] ; ( c ) follows from lemma  [ lem : zmin ] ; ( d ) follows from the assumption of the theorem that @xmath184 ; and ( e ) follows from ( [ eq : mubndhi ] ) .",
    "the definition of @xmath108 in ( [ eq : pmd ] ) now shows that @xmath185      let @xmath186 be a standard brownian motion .",
    "define the _ normalized brownian motion _",
    "@xmath187 as the process [ eq : decaybm ] s(t ) = b(t ) , t > 0 .",
    "we call the process normalized since @xmath188 we first characterize the autocorrelation of this process .",
    "[ lem : decaycorr ] if @xmath189 , the normalized brownian motion has autocorrelation @xmath190 = \\sqrt{s / t}.\\ ] ]    write @xmath191 thus , & = & + & & + & & = , where ( a ) follows from the orthogonal increments property of brownian motions ; and ( b ) follows from the fact that @xmath192 .",
    "we now need the following standard gaussian tail bound .",
    "[ lem : erfc ] suppose @xmath193 is a real - valued , scalar gaussian random variable , @xmath194 . then , @xmath195    see for example  @xcite .",
    "we next provide a simple bound on the maximum of sample paths of @xmath187 .",
    "[ lem : smaxbnda ] for any @xmath196 , let @xmath197 } |s(t)|.\\ ] ] then , for any @xmath29 , @xmath198    since @xmath187 and @xmath199 are identically distributed , [ eq : smaxbndsingle ] ( ^2(a , b ) > ) 2(_t s(t ) > ) .",
    "so , it will suffice to bound the probability of the single - sided event @xmath200 . for @xmath201 , define @xmath202 .",
    "then , @xmath203 is a standard brownian motion independent of @xmath204 .",
    "also , + & & _ t b(t ) > + & & _ t b(t ) > + & & b(a ) + _ t b_a(t ) > .",
    "now , the reflection principle ( see , for example  @xcite ) states that for any @xmath205 , @xmath206 } b_a(t ) >",
    "y\\right )      = 2\\pr\\left ( \\sqrt{b - a}y > y \\right),\\ ] ] where @xmath207 is a unit - variance , zero - mean gaussian . also , @xmath208 , so if we define @xmath209 , then @xmath194 . since @xmath204 is independent of @xmath203 for all @xmath201 , we can write + & & 2 ( x + y > ) , [ eq : smaxbndxy ] where @xmath193 and @xmath207 are independent zero mean gaussian random variables with unit variance .",
    "now @xmath210 has variance @xmath211 = a + b - a = b.\\ ] ] applying lemma  [ lem : erfc ] shows that ( [ eq : smaxbndxy ] ) can be bounded by @xmath212 } s(t ) > \\sqrt{\\mu } \\right )     \\leq \\frac{b}{a\\mu\\sqrt{\\pi}}\\exp\\left(-\\frac{a \\mu}{2b}\\right).\\ ] ] substituting this bound in ( [ eq : smaxbndsingle ] ) proves the lemma .",
    "our next lemma improves the bound for large @xmath70 .",
    "[ lem : smaxbnd ] there exist constants @xmath213 , @xmath214 , and @xmath215 such that for any @xmath196 and @xmath216 , @xmath217    fix any integer @xmath218 , and define @xmath219 for @xmath220 .",
    "observe that @xmath221s partition the interval @xmath222 $ ] in that @xmath223 also , let @xmath224 .",
    "then , @xmath225 . applying lemma  [ lem : smaxbnda ] to each interval in the partition , + & & _ i=1^n-1 ( ^2(t_i , t_i+1 ) > ) + & & ( - ) .",
    "[ eq : probbndsmax ] now , let @xmath15 , and for @xmath226 , let [ eq : ndefsmax ] n = -",
    ". then [ eq : rnbndsmax ] r^-1/n 1 - / , and hence [ eq : expbndsmax ] ( - ) e^/2e^-/2 .",
    "also , ( [ eq : ndefsmax ] ) implies that [ eq : nbndsmax ] n 1 - 1 + ( r ) , where we have used the fact that @xmath227 for @xmath228 . combining the bounds ( [ eq : rnbndsmax ] ) and ( [ eq : nbndsmax ] ) yields [ eq : constbndsmaxa ] ( 1 + ( r ) ) .",
    "now , pick any @xmath15 and let @xmath229 . then if @xmath230 , ( [ eq : constbndsmaxa ] ) implies that [ eq : constbndsmaxb ] ( 1 + 2(r ) ) . substituting ( [ eq : expbndsmax ] ) and ( [ eq : constbndsmaxb ] ) into ( [ eq : probbndsmax ] )",
    "shows that @xmath231 where @xmath232 the result now follows from the fact that @xmath224 .",
    "we can now apply the results in the previous subsection to bound the norms of sequences of projections .",
    "let @xmath10 be any deterministic vector , and let @xmath233 , @xmath234 be a deterministic sequence of orthogonal projection operators on @xmath235 .",
    "assume that the sequence @xmath233 is _ decreasing _ in that @xmath236 for @xmath237 .",
    "[ lem : projseqbnd ] let @xmath238 be a gaussian random vector with unit variance , and define the random variable @xmath239 then there exist constants @xmath213 , @xmath214 , and @xmath240 ( all independent of the problem parameters ) such that @xmath216 implies @xmath241 where @xmath242 .",
    "define @xmath243 so that @xmath244 since each @xmath245 is the inner product of the gaussian vector @xmath246 with a fixed vector , the scalars @xmath247 are jointly gaussian .",
    "since @xmath246 has mean zero , so do the @xmath245s .    to compute the cross - correlations ,",
    "suppose that @xmath248 . then & = & + & & (i)(j ) + & & (i ) + & = & , where ( a ) uses the fact that @xmath249 = i_m$ ] ; and ( b ) uses the descending property that @xmath250 .",
    "therefore , if we let @xmath251 , we have the cross - correlations [ eq : zcorr ] = for all @xmath248 . also observe that since the projection operators are decreasing , so are the @xmath252s . that is , for @xmath248 , @xmath253 where again ( a ) uses the decreasing property ; and ( b ) uses the fact that @xmath233 is a projection operator and norm non - increasing .",
    "now let @xmath187 be the normalized brownian motion in ( [ eq : decaybm ] ) .",
    "lemma  [ lem : decaycorr ] and ( [ eq : zcorr ] ) show that the gaussian vector @xmath254 has the same covariance as the vector of samples of @xmath187 , @xmath255 since they are also both zero - mean and gaussian , they have the same distribution .",
    "hence , for all @xmath70 , ( m > ) & = & ( _ i=0,  ,k    & = & ( _ i=0,  ,k",
    "|s(t_i)|^2 > ) + & & ( _ t |s(t)|^2 > ) , where the last step follows from the fact that the @xmath221s are decreasing and hence @xmath256 for all @xmath257 .",
    "the result now follows from lemma  [ lem : smaxbnd ] .",
    "recall that all the projection operators @xmath93 and the vector @xmath22 are statistically independent of the vectors @xmath27 for @xmath100 .",
    "since the entries of the matrix @xmath12 are i.i.d.gaussian with zero mean and variance @xmath61 , the vector @xmath258 is gaussian with unit variance .",
    "hence , lemma  [ lem : projseqbnd ] shows that there exist constants @xmath213 , @xmath214 , and @xmath215 such that for any @xmath259 , [ eq : projbnda ] ( _ t=0,  ,k m ) be^-/2 , where @xmath100 and [ eq : bdef ] b = c_1 + c_2 ( ) .    therefore , & & ( _ t=1,  ,k _ j ( t , j ) > ) + & & ( n - k)_j ( _ t=1,  ,k ( t , j ) > ) + & & ( n - k)_j ( _ t=1, ",
    ",k > ) + & & ( n - k)be^-m/2 + & & ( n - k)be^-(1+)(n - k ) + & = & b , [ eq : pfabnd ] where ( a ) follows from the definition of @xmath110 in ( [ eq : pfa ] ) ; ( b ) uses the union bound and the fact that @xmath260 has @xmath17 elements ; ( c ) follows from the definition of @xmath109 in ( [ eq : rhotrue ] ) ; ( d ) follows from ( [ eq : projbnda ] ) under the condition that @xmath261 ; and ( e ) follows from ( [ eq : mubndlow ] ) . by ( [ eq : nummeas ] ) and the hypothesis of the theorem that @xmath262 , @xmath263 therefore , for sufficiently large @xmath1 , @xmath261 and ( [ eq : pfabnd ] ) holds .",
    "now , since @xmath264 , @xmath265 and therefore [ eq : p0y ] ( 0)= . also , @xmath266 and so @xmath267 is the projection onto the orthogonal complement of the range of @xmath120 .",
    "hence @xmath268 . combining this fact with ( [ eq : yaxdecomp ] ) and ( [ eq : pwperp ] ) shows [ eq : pky ] ( k)= ^. therefore , + & & _ n b + & & _ n ( c_1 + c_2 ( ) ) + & & _ n ( c_1 + c_2 ( ) ) + & & _ n ( c_1 + c_2 ( 1+^2 ) ) + & & 0 where ( a ) follows from ( [ eq : pfabnd ] ) ; ( b ) follows from ( [ eq : bdef ] ) ; ( c ) follows from ( [ eq : p0y ] ) and ( [ eq : pky ] ) ; ( d ) follows from lemma  [ lem : wperpnorm ] ; and ( e ) follows from ( [ eq : xpowbnd ] ) .",
    "this completes the proof of the theorem .",
    "the authors thank vivek goyal for comments on an earlier draft and martin vetterli for his support , wisdom , and encouragement .",
    "e.  j. cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "inform . theory _",
    "52 , no .  2 ,",
    "489509 , feb .",
    "2006 .",
    "y.  c. pati , r.  rezaiifar , and p.  s. krishnaprasad , `` orthogonal matching pursuit : recursive function approximation with applications to wavelet decomposition , '' in _ conf .",
    "27th asilomar conf .",
    ", sys . , & comput .",
    "_ , vol .  1 , pacific grove , ca , nov",
    "1993 , pp .",
    "4044 .",
    "m.  j. wainwright , `` sharp thresholds for high - dimensional and noisy recovery of sparsity , '' univ . of california , berkeley , dept . of statistics ,",
    "tech . rep .",
    ", may 2006 , arxiv : math.st/0605740 v1 30 may 2006 .",
    " , `` sharp thresholds for high - dimensional and noisy sparsity recovery using @xmath269-constrained quadratic programming ( lasso ) , '' _ ieee trans .",
    "inform . theory _",
    "55 , no .  5 , pp .",
    "21832202 , may 2009 .",
    "a.  k. fletcher and s.  rangan , `` orthogonal matching pursuit from noisy measurements : a new analysis , '' in _ proc .",
    "neural information process .",
    "_ , y.  bengio , d.  schuurmans , j.  lafferty , c.  k.  i. williams , and a.  culotta , eds . ,",
    "vancouver , canada , dec ."
  ],
  "abstract_text": [
    "<S> a well - known analysis of tropp and gilbert shows that orthogonal matching pursuit ( omp ) can recover a @xmath0-sparse @xmath1-dimensional real vector from @xmath2 noise - free linear measurements obtained through a random gaussian measurement matrix with a probability that approaches one as @xmath3 . </S>",
    "<S> this work strengthens this result by showing that a lower number of measurements , @xmath4 , is in fact sufficient for asymptotic recovery . </S>",
    "<S> more generally , when the sparsity level satisfies @xmath5 but is unknown , @xmath6 measurements is sufficient . </S>",
    "<S> furthermore , this number of measurements is also sufficient for detection of the sparsity pattern ( support ) of the vector with measurement errors provided the signal - to - noise ratio ( snr ) scales to infinity . </S>",
    "<S> the scaling @xmath4 exactly matches the number of measurements required by the more complex lasso method for signal recovery with a similar snr scaling .    </S>",
    "<S> compressed sensing , detection , lasso , orthogonal matching pursuit , random matrices , sparse approximation , sparsity , subset selection </S>"
  ]
}