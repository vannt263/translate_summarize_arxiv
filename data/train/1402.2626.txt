{
  "article_text": [
    "we investigate the application of general purpose graphics processing units ( gpus ) to solving large systems of polynomial equations with numerical methods .",
    "large systems not only lead to an increased number of operations , but also to more accumulation of numerical roundoff errors and therefore to the need to calculate in a precision that is higher than the common double precision . motivated by the need of higher numerical precision , we can formulate our goal more precisely .",
    "with massively parallel algorithms we aim to offset the extra cost of double double and quad double arithmetic  @xcite and achieve quality up  @xcite , a project we started in  @xcite .",
    "* problem statement . *",
    "our problem is to accelerate newton s method for large polynomial systems , aiming to offset the overhead cost of double double and quad double complex arithmetic .",
    "we assume the input polynomials are given in their sparse distributed form : all polynomials are fully expanded and only those monomials that have a nonzero coefficient are stored . for accuracy and application to overdetermined systems ,",
    "we solve linear systems in the least squares sense and implement the method of gauss - newton .",
    "our original massively parallel algorithms for evaluation and differentiation of polynomials  @xcite and for the modified gram - schmidt method  @xcite were written with a fine granularity , making intensive use of the shared memory .",
    "the limitations on the capacity of the shared memory led to restrictions on the dimensions on the problems we could solve .",
    "these problems worsened for higher levels of precision , in contrast to the rising need for more precision in higher dimensions .",
    "* related work .",
    "* as the qr decomposition is of fundamental importance in applied linear algebra many parallel implementations have been investigated by many authors , see e.g.  @xcite , @xcite .",
    "a high performance implementation of the qr algorithm on gpus is described in  @xcite . in  @xcite ,",
    "the performance of cpu and gpu implementations of the gram - schmidt were compared .",
    "a multicore qr factorization is compared to a gpu implementation in  @xcite .",
    "gpu algorithms for approaches related to qr and gram - schmidt are for lattice basis reduction  @xcite and singular value decomposition  @xcite . in  @xcite ,",
    "the left - looking scheme is dismissed because of its limited inherent parallelism and as in  @xcite we also prefer the right - looking algorithm for more thread - level parallelism .",
    "the application of extended precision to blas is described in  @xcite , see  @xcite for least squares solutions .",
    "the implementation of blas routines on gpus in triple precision ( double + single float ) is discussed in  @xcite . in  @xcite",
    ", double double arithmetic is described under the section of error - free transformations .",
    "an implementation of interval arithmetic on cuda gpus is presented in  @xcite .",
    "the other computationally intensive stage in the application of newton s method is the evaluation and differentiation of the system .",
    "parallel automatic differentiation techniques are described in  @xcite , @xcite , and  @xcite .    concerning the gpu acceleration of polynomial systems solving , we mention two recent works . a subresultant method with a cuda implementation of the fft to solve systems of two variables is presented in  @xcite . in  @xcite , a cuda implementation for an nvidia gpu of a multidimensional bisection algorithm is discussed .    * our contributions .",
    "* for the polynomial evaluation and differentiation we reformulate algorithms of algorithmic differentiation  @xcite applying optimized parallel reduction  @xcite to the products that appear in the reverse mode of differentiation .",
    "because our computations are geared towards extended precision arithmetic which carry a higher cost per operation , we can afford a fine granularity in our parallel algorithms . compared to our previous gpu implementations in  @xcite",
    ", we have removed the restrictions on the dimensions and are now able to solve problems involving several thousands of variables .",
    "the performance investigation involves mixing the memory - bound polynomial evaluation and differentiation with the compute - bound linear system solving .",
    "we distinguish three tasks in the evaluation and differentiation of polynomials in several variables given in their sparse distributed form .",
    "first , we separate the high degree parts into common factors and then apply algorithmic differentiation to products of variables . in the third stage , monomials are multiplied with coefficients and the terms are added up .",
    "a monomial in @xmath0 variables is defined by a sequence of natural numbers @xmath1 , for @xmath2 .",
    "we decompose a monomial as follows : @xmath3 where @xmath4 is the product of all @xmath5 variables that have a nonzero exponent .",
    "the @xmath6 variables that appear with a positive exponent occur in @xmath7 with exponent @xmath8 , for @xmath9 .",
    "we call the monomial @xmath7 a _ common factor _ , as this factor is a factor in all partial derivatives of the monomial . using tables of pure powers of the variables , the values of the common factors are products of the proper entries in those tables . the cost of evaluating monomials of high degrees is thus deferred to computing powers of the variables .",
    "the table of pure powers is computed in shared memory by each block of threads .",
    "consider a product of variables : @xmath10 .",
    "the straightforward evaluation and the computation of the gradient takes @xmath11 multiplications .",
    "recognizing the product as the example of speelpenning in algorithmic differentiation  @xcite , the number of multiplications to evaluate the product and compute all its derivatives drops to  @xmath12 .",
    "the computation of the gradient requires in total @xmath13 extra memory locations .",
    "we need @xmath14 locations for the intermediate forward products @xmath15 , @xmath16 . for the backward products",
    "@xmath17 , @xmath18 only one extra temporary memory location is needed , as this location can be reused each time for the next backward product , if the computation of the backward products is interlaced with the multiplication of the forward with the corresponding backward product .    for @xmath19 , figure  [ figcircuit1 ] displays two arithmetic circuits , one to evaluate a product of variables and another to compute its gradient .",
    "the second circuit is executed after the first one , using the same tree structure that holds intermediate products . at a node in a circuit",
    ", we write @xmath20 if the multiplication @xmath21 happens at the node and we write @xmath22 if we use the value of the product . at",
    "most one multiplication is performed at each node of the circuit .",
    "( 350,100)(0,0 ) ( 0,0 )    ( 80,80)(0,0 ) ( 0,0)@xmath23 ( 25,0)@xmath24 ( 50,0)@xmath25 ( 75,0)@xmath26 ( 2,30)@xmath20 ( 5,8)(1,2)9 ( 28,8)(-1,2)9 ( 52,30)@xmath27 ( 55,8)(1,2)9 ( 78,8)(-1,2)9 ( 17,70)@xmath28 ( 25,40)(1,2)12 ( 58,40)(-1,2)12    ( 140,0 )    ( 200,80)(0,0 ) ( 0,70)@xmath29 ( 50,70)@xmath30 ( 100,70)@xmath31 ( 150,70)@xmath32 ( 0,0)@xmath23 ( 5,10)(0,1)55 ( 50,0)@xmath24 ( 55,10)(0,1)55 ( 130,0)@xmath25 ( 135,10)(0,1)55 ( 180,0)@xmath26 ( 185,10)(0,1)55 ( 150,30)@xmath33 ( 158,38)(-3,1)78 ( 150,36)(-4,1)112 ( 20,30)@xmath22 ( 28,38)(3,1)78 ( 38,36)(4,1)112    denote by @xmath34 the product @xmath35 , for all natural numbers  @xmath5 between @xmath36 and @xmath37 .",
    "figure  [ figcircuit2 ] displays the arithmetic circuit to compute all derivatives of a product of 8 variables , after the evaluation of the product in a binary tree .",
    "( 455,160)(0,0 ) ( 0,0)@xmath23 ( 5,10)(0,1)134 ( 60,0)@xmath24 ( 65,10)(0,1)134 ( 120,0)@xmath25 ( 125,10)(0,1)134 ( 180,0)@xmath26 ( 185,10)(0,1)134 ( 240,0)@xmath38 ( 245,10)(0,1)134 ( 300,0)@xmath39 ( 305,10)(0,1)134 ( 360,0)@xmath40 ( 365,10)(0,1)134 ( 420,0)@xmath41 ( 425,10)(0,1)134 ( 0,150)@xmath42 ( 60,150)@xmath43 ( 120,150)@xmath44 ( 180,150)@xmath45 ( 240,150)@xmath46 ( 300,150)@xmath47 ( 360,150)@xmath48 ( 420,150)@xmath49 ( 85,28)@xmath50 ( 105,30)(4,1)267 ( 90,38)(3,1)170 ( 250,100)@xmath51 ( 283,110)(3,1)100 ( 295,105)(4,1)145 ( 370,100)@xmath52 ( 385,110)(-3,1)100 ( 395,110)(-2,1)60 ( 15,15)@xmath53 ( 20,25)(0,1)70 ( 135,15)@xmath54 ( 140,25)(0,1)70 ( 275,15)@xmath55 ( 280,25)(0,1)70 ( 395,15)@xmath56 ( 400,25)(0,1)70 ( 325,28)@xmath57 ( 320,30)(-4,1)267 ( 335,38)(-3,1)170 ( 15,100)@xmath58 ( 48,110)(3,1)100 ( 60,105)(4,1)145 ( 135,100)@xmath59 ( 148,110)(-3,1)105 ( 160,110)(-2,1)65    to count the number of multiplications to evaluate , we restrict to the case of a complete binary tree , i.e. : @xmath60 for some  @xmath61 and compute the sum @xmath62 .",
    "the circuit to compute all derivatives contains a tree of the same size : with @xmath13 nodes , so the number of multiplications equals @xmath13 minus 3 for the nodes closest to the root which require no computations , and plus @xmath0 for the multiplications at the leaves : @xmath63 in total .",
    "so the total number of multiplications to evaluate a product of @xmath0 variables and compute its gradient with a binary tree equals  @xmath64 .",
    "while keeping the same operational cost of  @xmath65 as the original algorithm , the organization of the multiplication in a binary tree incurs less roundoff .",
    "in particular the roundoff error for the evaluated product will be proportional to @xmath66 instead of  @xmath0 of the straightforward multiplication . for a large number of variables , such as @xmath67 , this reorganization improves the accuracy by two decimal places .",
    "the improved accuracy of the evaluated product does not cost more storage as the size of binary tree equals @xmath13 .    for the derivatives ,",
    "the roundoff error is bounded by the number of levels in the arithmetic circuit , which is @xmath68 .",
    "while this bound is still better than  @xmath13 , the improved accuracy for the gradient comes at the extra cost of  @xmath69 additional memory locations , needed as nodes in the arithmetic circuit for the gradient . in shared memory , the memory locations for the input variables @xmath70 are overwritten by the corresponding components of the gradient , e.g. : @xmath71 then occupies the location of  @xmath23 .    in the original formulation of the computation of the example of speelpenning ,",
    "only one thread performed all computation for one product and the parallelism consisted in having enough monomials in the system to occupy all threads working separately on different monomials .",
    "the reformulation of the evaluation and differentiation with a binary tree allows for several threads to collaborate on the computation of one large product .",
    "the reformulation refined the granularity of the parallel algorithm and we applied the techniques suggested in  @xcite .",
    "if @xmath0 is not a power of 2 , then for some positive @xmath5 and  @xmath6 , denote @xmath72 .",
    "the first @xmath6 threads load two variables and are in charge of the product of those two variables , while other threads load just one variable .",
    "the multiplication of values for variables of consecutive index , e.g. : @xmath20 will result in a bank conflict in shared memory as threads require data from an even and odd bank . to avoid bank conflicts , the computations are rearranged , e.g. as @xmath73 , so thread  0 operates on  @xmath74 and thread  1 on  @xmath75 .",
    "table  [ tabmoneval ] shows the results on the evaluation and differentiation of products of variables in double arithmetic , applying the techniques of  @xcite .",
    "the first gpu algorithm is the reverse mode algorithm that takes @xmath64 operations executed by one thread per monomial .",
    "when all threads in a block collaborate on one monomial in the second gpu algorithm we observe a significant speedup .",
    "speedups and memory bandwidth improve when resolving the bank conflicts in the third improvement .",
    "the best results are obtained adding unrolling techniques .",
    ".evaluation and differentiation of 65,024 monomials in 1,024 doubles .",
    "times on the k20c obtained with nvprof ( the nvidia profiler ) are in milliseconds ( ms ) . dividing the number of bytes read and written by the time gives the bandwidth .",
    "times on the cpu are on one 2.6ghz intel xeon e5 - 2670 , with code optimized with the -o2 flag .",
    "[ cols=\">,^,>,>,>\",options=\"header \" , ]     we end this paper with the application of newton s method on the cyclic @xmath0-roots problem @xmath76 for @xmath77 .",
    "the setup is as follows .",
    "we generate a random complex vector @xmath78 and consider the system @xmath79 , for @xmath80 .",
    "for @xmath81 , we have that @xmath82 is a solution and for @xmath83 sufficiently close to  1 , newton s method will converge .",
    "this setup corresponds to the start in running a newton homotopy , for @xmath83 going from one to zero . in complex double double arithmetic , with seven iterations",
    "newton s method converges to the full precision .",
    "the cpu time is 78,055.71 seconds while the gpu accelerated time is 5,930.96 seconds , reducing 21 minutes to about 1.6 minutes , giving a speedup factor of about  13 .",
    "to accurately evaluate and differentiate polynomials in several variables given in sparse distributed form we reorganized the arithmetic circuits so all threads in block can contribute to the computation .",
    "this computation is memory bound for double arithmetic and the techniques to optimize a parallel reduction are beneficial also for real double double arithmetic , but for complex double double and quad double arithmetic the problem becomes compute bound .",
    "we illustrated our cuda implementation on two benchmark problems in polynomial system solving .",
    "for the first problem , the cost of evaluation and differentiation grows linearly in the dimension and then the cost of linear system solving dominates . for systems with polynomials of high degree such as the cyclic @xmath0-roots problem ,",
    "the implementation to evaluate the system and compute its jacobian matrix achieved double digits speedups , sufficiently large enough to compensate for one extra level of precision . with gpu acceleration",
    "we obtain more accurate results faster , for larger dimensions .",
    "this material is based upon work supported by the national science foundation under grant no .",
    "the microway workstation with the nvidia tesla k20c was purchased through a uic las science award .",
    "d.  adrovic and j.  verschelde .",
    "polyhedral methods for space curves exploiting symmetry applied to the cyclic @xmath0-roots problem . in v.p .",
    "gerdt , w.  koepf , e.w .",
    "mayr , and e.v .",
    "vorozhtsov , editors , _ proceedings of casc 2013 _ , pages 1029 , 2013 .",
    "e.  agullo , c.  augonnet , j.  dongarra , m.  faverge , h.  ltaief , s.  thibault , and s.  tomov .",
    "factorization on a multicore node enhanced with multiple gpu accelerators . in _ proceedings of the 2011 ieee international parallel distributed processing symposium ( ipdps 2011 ) _ , pages 932943 .",
    "ieee computer society , 2011 .",
    "m.  anderson , g.  ballard , j.  demmel , and k.  keutzer .",
    "communication - avoiding qr decomposition for gpus . in _ proceedings of the 2011 ieee international parallel distributed processing symposium ( ipdps 2011 ) _ , pages 4858 .",
    "ieee computer society , 2011 .",
    "t.  bartkewitz and t.  gneysu . full lattice basis reduction on graphics cards . in f.",
    "armknecht and s.  lucks , editors , _",
    "weworc11 proceedings of the 4th western european conference on research in cryptology _ , volume 7242 of _ lecture notes in computer science _ , pages 3044 .",
    "springer - verlag , 2012 .    c.  bischof , n.  guertler , a.  kowartz , and a.  walther .",
    "parallel reverse mode automatic differentiation for openmp programs with adol - c . in c.",
    "bischof , h.m .",
    "bcker , p.  hovland , u.  naumann , and j.  utke , editors , _ advances in automatic differentiation _ , pages 163173 .",
    "springer - verlag , 2008 .",
    "g.  bjrck and r.  frberg .",
    "methods to `` divide out '' certain solutions from systems of algebraic equations , applied to find all cyclic 8-roots . in m.  gyllenberg and l.e .",
    "persson , editors , _ analysis , algebra and computers in math .",
    "research _ , volume 564 of _ lecture notes in mathematics _",
    ", pages 5770 .",
    "dekker , 1994 .",
    "faugre . finding all the solutions of cyclic 9 using grbner basis techniques . in _",
    "computer mathematics - proceedings of the fifth asian symposium ( ascm 2001 ) _ , volume  9 of _ lecture notes series on computing _ , pages 112 .",
    "world scientific , 2001 .",
    "b.  foster , s.  mahadevan , and r.  wang . a gpu - based approximate svd algorithm . in _ parallel processing and applied mathematics _ , volume 7203 of _ lecture notes in computer science volume _ ,",
    "pages 569578 .",
    "springer - verlag , 2012 .",
    "l.  gonzalez - vega .",
    "some examples of problem solving by using the symbolic viewpoint when dealing with polynomial systems of equations . in j.",
    "fleischer , j.  grabmeier , f.w .",
    "hehl , and w.  kchlin , editors , _ computer algebra in science and engineering _ ,",
    "pages 102116 .",
    "world scientific , 1995 .",
    "m.  grabner , t.  pock , t.  gross , and b.  kainz .",
    "automatic differentiation for gpu - accelerated 2d/3d registration . in c.  bischof , h.m .",
    "bcker , p.  hovland , u.  naumann , and j.  utke , editors , _ advances in automatic differentiation _ , pages 259269 .",
    "springer - verlag , 2008 .",
    "y.  hida , x.s .",
    "li , and d.h .",
    "algorithms for quad - double precision floating point arithmetic . in _",
    "15th ieee symposium on computer arithmetic ( arith-15 2001 ) , 11 - 17 june 2001 , vail , co , usa _ , pages 155162 .",
    "ieee computer society , 2001 . shortened version of technical report lbnl-46996 , software at http://crd.lbl.gov/@xmath84dhbailey/ mpdist .",
    "a.  kerr , d.  campbell , and m.  richards .",
    "decomposition on gpus . in d.",
    "kaeli and m.  leeser , editors , _ proceedings of 2nd workshop on general purpose processing on graphics processing units ( gpgpu09 ) _ , pages 7178 .",
    "acm , 2009 .",
    "klopotek and j.  porter - sobieraj . solving systems of polynomial equations on a gpu . in m.  ganzha , l.  maciaszek , and m.  paprzycki , editors , _ preprints of the federated conference on computer science and information systems , september 9 - 12 , 2012 , wroclaw , poland _ , pages 567572 , 2012 .",
    "x.  li , j.  demmel , d.  bailey , g.  henry , y.  hida , j.  iskandar , w.  kahan , s.  kang , a.  kapur , m.  martin , b.  thompson , t.  tung , and d.  yoo .",
    "design , implementation and testing of extended and mixed precision blas .",
    ", 28(2):152205 , 2002 . this is a shortened version of technical report lbnl-45991 .",
    "m.  lu , b.  he , and q.  luo .",
    "supporting extended precision on graphics processors . in a.  ailamaki and p.a .",
    "boncz , editors , _ proceedings of the sixth international workshop on data management on new hardware ( damon 2010 ) , june 7 , 2010 , indianapolis , indiana _ , pages 1926 , 2010 .",
    "software at http://code.google.com / p / gpuprec/.          d.  mukunoki and d.  takashashi . implementation and evaluation of triple precision blas subroutines on gpus . in _ proceedings of the 2012 ieee 26th international parallel and distributed processing symposium workshops .",
    "21 - 25 may 2012 , shanghai china _ , pages 13721380 .",
    "ieee computer society , 2012 .",
    "j.  utke , l.  hascot , p.  heimbach , c.  hill , p.  hovland , and u.  naumann . toward ajoinable mpi . in _ proceedings of the 10th ieee international workshop on parallel and",
    "distributed scientific and engineering computing ( pdsec 2009 ) _ ,",
    "pages 18 , 2009 .",
    "j.  verschelde and g.  yoffe .",
    "polynomial homotopies on multicore workstations . in m.m .",
    "maza and j .- l .",
    "roch , editors , _ proceedings of the 4th international workshop on parallel symbolic computation ( pasco 2010 ) , july 21 - 23 2010 , grenoble , france _ , pages 131140 .",
    "acm , 2010 .",
    "j.  verschelde and g.  yoffe . evaluating polynomials in several variables and their derivatives on a gpu computing processor . in _ proceedings of the",
    "2012 ieee 26th international parallel and distributed processing symposium workshops ( pdsec 2012 ) _ , pages 13911399 .",
    "ieee computer society , 2012 .",
    "j.  verschelde and g.  yoffe .",
    "orthogonalization on a general purpose graphics processing unit with double double and quad double arithmetic . in _ proceedings of the 2013 ieee 27th international parallel and distributed processing symposium workshops ( pdsec 2013 ) _ , pages 13731380 .",
    "ieee computer society , 2013 ."
  ],
  "abstract_text": [
    "<S> in order to compensate for the higher cost of double double and quad double arithmetic when solving large polynomial systems , we investigate the application of nvidia tesla k20c general purpose graphics processing unit . </S>",
    "<S> the focus on this paper is on newton s method , which requires the evaluation of the polynomials , their derivatives , and the solution of a linear system to compute the update to the current approximation for the solution . </S>",
    "<S> the reverse mode of algorithmic differentiation for a product of variables is rewritten in a binary tree fashion so all threads in a block can collaborate in the computation . for double arithmetic , </S>",
    "<S> the evaluation and differentiation problem is memory bound , whereas for complex quad double arithmetic the problem is compute bound . with acceleration we can double the dimension and get results that are twice as accurate in about the same time .    * </S>",
    "<S> key words and phrases . * </S>",
    "<S> compute unified device architecture ( cuda ) , double double arithmetic , differentiation and evaluation , general purpose graphics processing unit ( gpu ) , newton s method , least squares , massively parallel algorithm , modified gram - schmidt method , polynomial evaluation , polynomial differentiation , polynomial system , qr decomposition , quad double arithmetic , quality up . </S>"
  ]
}