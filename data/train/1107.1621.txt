{
  "article_text": [
    "multiple spatial and temporal scales occur in the dynamics of neural systems .",
    "several recent studies  @xcite have explored the consequences of having multiple built - in dynamical timescales in adaptation and learning mechanisms in particular , and its possible functional benefits .",
    "one such example is the study of @xcite , which considered short - term motor learning and forgetting involved in the phenomenon of reaching . by means of numerical simulation ,",
    "a two - state system containing a slow and a fast timescale was shown to provide a unified account of multiple behavioral phenomena observed in force - field experiments .    the motor adaptation model of ref .",
    "@xcite , while being quite successful in its application to experiments , is somewhat empirical ; the introduction of slow and fast modes does not have a clear microscopic basis , with the choice of the parameter values seeming arbitrary .",
    "moreover , the linearity of the equations might be seen as a limitation on their ability to model memory .",
    "working within the framework of activity - induced synaptic plasticity  the standard paradigm for biological learning @xcite",
    " an attempt is made here towards providing a theoretical basis for the work of ref .",
    "@xcite , with a description wherein the parameters are given meaning at a microscopic level .",
    "activity - dependent synaptic plasticity naturally introduces some form of ` competition ' and ` cooperation ' between synapses in biological neuronal networks .",
    "these synaptic interactions occur via the patterns of activation of the pre- or post - synaptic neurons shared in common @xcite .",
    "competition in interacting systems is generally modeled in the language of game theory @xcite .",
    "here , we draw upon a game - theoretic model of competitive learning @xcite , originally introduced in a sociological context , to propose a model for synaptic plasticity which incorporates a notion of synaptic competition",
    ". the rules of plasticity can well be viewed as ` decisions ' of a synapse to strengthen or weaken in response to the ` outcomes '  corresponding to activation or quiescence  of the pair of neural units that it connects .",
    "for the sake of completeness , we shall briefly summarize the strategic learning model of ref .",
    "@xcite here , as it provides the starting point for the work presented in this article .",
    "this model was originally devised to describe social phenomena like the diffusion of innovations in connected societies ; however , its essence is general enough to describe any paradigm involving decision - making in the face of competing alternatives . `",
    "agents ' are located at the sites of a regular lattice , and can be associated with one of two ` types ' : fast ( f ) or slow ( s ) ; these could represent competing strategies , for example . underpinning the dynamics of the system",
    "is the assumption that every agent revises its choice of type at regular intervals , and in this it is guided by two rules : a _ majority _ rule , reflecting the tendency of the agents to align with their local neighborhood , followed by an adaptive _ performance - based _ rule , via which the agent selects the type that it perceives to be more successful in its local neighborhood .",
    "the notion of success is gauged in terms of the random outcomes of the agents in some ` game ' , with a favorable outcome being ascribed to every f - type ( s - type ) individual with an independent probability @xmath0 ( resp .",
    "@xmath1 ) .",
    "thus , if an agent is surrounded by @xmath2 ( @xmath3 ) nearest neighbors of the f ( s ) type , @xmath4 ( resp .",
    "@xmath5 ) of which turn out to be successful in a particular trial , the agent arrives at a decision on whether or not to convert by comparing the ratios @xmath6 and @xmath7 ; if , for example , an agent is currently in the f state , then it will switch to the s type provided that @xmath8 , and remain unchanged otherwise",
    ". it should be clear that the above outcome - based updates naturally introduce stochasticity into the dynamics . in ref .",
    "@xcite , a detailed analysis of this model was carried out under the assumption of coexistence ( i.e. , when @xmath9 ) , and its collective behavior , as a function of the parameter @xmath10 , was shown to exhibit multiple dynamic phases separated by critical phase transitions .    in the following ,",
    "the above ideas will be carried over to a synaptic setting .",
    "the adaptive nature of the aforementioned update rules suggests their potential applicability to modeling plastic synapses , and it will be interesting to see what consequences they might have for learning and memory , particularly in the context of making a connection with the work of ref .",
    "@xcite .",
    "this article proceeds as follows : in the next section , we first outline our model for synaptic plasticity .",
    "next , we define a protocol for applying a signal to our system in its mean - field limit .",
    "we then analyze the learning and forgetting behavior in this set - up .",
    "the paper concludes with a discussion of the implications of our results , and further possible extensions .",
    "we consider undirected , binary synapses each of which is assumed to be of either the ` strong ' or the ` weak ' type ; these synaptic states are characterized by unequal strengths ( weights ) .",
    "although undirected , i.e. symmetric synapses are an idealized approximation , their use has figured in several previous theoretical studies of neural networks ( see e.g.  @xcite ) , so this is not an unusual assumption that we are making . as for the binary nature of our synapses , this is a natural approximation to synapses possessing a finite set of discrete states , in support of which there is some experimental evidence  @xcite ; these , again , have been used in earlier mathematical models ( e.g.  @xcite ) . for simplicity , we choose to work with a one - dimensional chain .",
    "a pair of synapses in this set - up is treated as a pair of interacting neighbors if the synapses share a connected neural unit .",
    "the basic features of the model are sketched in fig .",
    "[ fig1 ] . to begin with",
    ", we make a ` first - order ' approximation here : when a particular synapse is chosen to be considered for an update , the neural units connected to it are assumed to take on the identities of the _ neighboring _ synapses , with the effect of the synapse under consideration being omitted at this step . in other words , the firing probability of e.g. neuron a in fig .",
    "[ fig1](a ) is assumed to be determined only by the type of synapse @xmath11 .",
    "a neuron receiving input via a single strong synapse is more likely to be activated than one fed by a weak synapse ( input statistics being the same for both ) .",
    "thus , an activation probability can be associated with any neuron depending on the type of the [ single ] incoming synapse : @xmath0 for a strong synapse and @xmath1 for a weak synapse . in the context of fig .",
    "1(a ) , what this ultimately means ",
    "when it is the turn of synapse @xmath12 to be considered for updating  is that if synapse @xmath11 is in the strong ( weak ) state , then the neuron a is going to be found active with a probability @xmath0 ( resp .",
    "@xmath1 ) , _ irrespective _ of the state of synapse @xmath12 . treated this way , the probabilities @xmath13 , although being really a property of the synapses , come into play only through their effect on neuronal outcomes ( i.e. state of activity ) .",
    "we shall associate any signal ( or current ) transmitted by a synaptic connection with a ` polarity ' determined by the states as well as the types of the connected neural units .",
    "a positively ( negatively ) polarized signal is realized when the synapse connects a strong neuron with a weak neuron , _ and _ the strong ( weak ) neuron alone is activated .",
    "this event occurs with a probability @xmath14 ( resp .",
    "@xmath15 ) , and we denote such a signal by @xmath16 ( resp .",
    "@xmath17 ) .",
    "all other possible combinations of neuron types and activation states are associated with zero or unpolarized current .",
    "we now propose the following rules governing synaptic weight changes : @xmath18 ( @xmath19 ) whenever there is a positively ( negatively ) polarized current , and @xmath20 in all other cases .",
    "furthermore , to be consistent with the two - state nature of the synapses , it is assumed that a strengthening event would effect a weak @xmath21 strong conversion , while leaving an already strong synapse unchanged ( the same logic holds for a weak synapse also ) .",
    "thus , loosely speaking , the two neighboring synapses of any given synapse ` compete ' to decide its type , and this goes on repeatedly throughout the network .",
    "for the sake of clarity , implementation of the above rules is illustrated with an example .",
    "say a synapse has one neighbor of each type , as is depicted in fig .",
    "[ fig1 ] . for this configuration ,",
    "a total of four outcomes for the neuronal pair a - b are possible .",
    "there will be no weight changes if both neurons get activated ( giving an unpolarized synaptic current ) or if both remain silent ; the likelihood of this happening is @xmath22 .",
    "a depressing event ( @xmath19 ) occurs if neuron a fires but neuron b remains inactive , and this has a probability @xmath15 .",
    "the only remaining possibility is that neuron b gets activated and neuron a does not ; this occurs with a probability @xmath14 , and is accompanied by potentiation ( @xmath18 ) .    referring back to the introductory section , it is clear that the above set of rules has a close correspondence with a one - dimensional version of the outcome - based conversion rules introduced in ref .",
    "@xcite ( and this is the reason , in fact , for choosing them in the first place ) . at the same time , their biological reasonableness could be argued for by leaning on earlier work on rate - based models of synaptic plasticity . in such continuous - time models ,",
    "the firing rate of the neuron , instead of its membrane potential , is taken as the basic dynamical variable , and synaptic plasticity is a continuous process that depends on the firing rates of the pre and post - synaptic neurons .",
    "the dynamical equation describing the time evolution of the synaptic weight usually involves some non - linear function of pre / post - synaptic activities and the weight itself , and in some cases , a dependence on averages of the firing rates over some temporal window has also been motivated  @xcite . taking a cue from such approaches , we speculate that the rules for synaptic weight changes proposed in the previous paragraph might also be realizable through an iterative , discrete hebbian equation symbolically expressed as @xmath23 , where the form of the non - linear function @xmath24 is chosen to provide an appropriate ` fit ' to the plasticity rules . here ,",
    "@xmath25 and @xmath26 represent the activity states of the two connected neurons , which for simplicity are assumed to be binary variables in the present set - up , being either active ( 1 ) or inactive ( 0 ) .",
    "the symmetric form of the argument of @xmath24 is in keeping with the bi - directional nature of the synapses and ensures that the synaptic response is insensitive to the spatial direction of any current , while still being sensitive to its polarity .",
    "the inclusion of time averages of the activity of the connected neurons allows for a characterization of the strengths of the neighboring synapses in this picture , and hence allows for the determination of the polarity of any current at the synapse .",
    "the resulting model of induced synaptic plasticity is evidently stochastic , and could be simulated by using a range of updates , as in the case of the model of game - theoretic origin  @xcite .",
    "we choose however , to tackle the problem analytically , by looking at a mean - field version of the model .",
    "the idea behind the mean - field approximation is that we look at the average behavior in an infinite system .",
    "this , at one stroke , deals with two problems : first , there are no fluctuations associated with system size , and second , the ` first - order ' approximation that we have made in ignoring the self - coupling of the synapse , is better realized .",
    "the resulting mean - field equations are fully deterministic in the sense that once the probabilities @xmath13 are given , they admit of no further sources of stochasticity in their solution .    in the mean - field representation , every synapse is assigned a probability ( uniform over the lattice ) to be either strong ( @xmath27 ) or weak ( @xmath28 ) , so that spatial variation is ignored , as are fluctuations and correlations .",
    "this single effective degree of freedom allows for a description of the system in terms of its fixed point dynamics .",
    "the rate of change of the probability @xmath27 , say ( which in the limit of large system size is equivalent to the fraction of strong units ) , with time is computed by taking into account only the nearest - neighbor synaptic interactions , via the rules defined earlier .",
    "the dynamical equation for @xmath29 assumes the following form : @xmath30 with the transition probabilities being given by @xmath31 the fractions of strong and weak types are , of course , normalized by definition : @xmath32 .",
    "the [ implicitly ] time - dependent transition probabilities , which incorporate the effect of nearest - neighbor coupling , introduce non - linearity into the dynamics , an obvious departure from the linear coupled equations of @xcite .",
    "the deterministic dynamics of eq .",
    "[ eq1 ] yields stationary states ( @xmath33 ) to which the system would relax exponentially starting from an arbitrary initial state .",
    "besides the ` trivial ' unstable fixed points at 0 and 1 corresponding to homogeneous states ( all units being one or the other type ) , the algebraic equation @xmath34 also possesses a stable solution ; this is given by @xmath35 ( in the presence of fluctuations , e.g. associated with finite system sizes in mean - field , or in the full solution of the stochastic equations , we would expect the trivial fixed points to be absorbing , and the stable fixed point associated with eq .",
    "[ eq3 ] to be metastable . )",
    "the time scale for relaxation to this fixed point is the other dynamically relevant quantity , which again can be extracted from eq .",
    "[ eq1 ] and is given by @xmath36.\\ ] ]    this relaxation time is the central quantity with regard to learning and forgetting protocols ( see e.g. @xcite ) .",
    "it depends on the outcome probabilities @xmath13 , and varies with the location of the corresponding fixed point .",
    "this dependence is illustrated in fig .",
    "[ fig2 ] in the ( @xmath37 ) plane .",
    "a possible way of defining learning and retention in the coarse - grained limit is the following : we first define a general time - dependent signal as a ` perturbation ' of the system parameters ( @xmath37 ) having the following form : @xmath38 .",
    "this choice of signal definition makes sense if one thinks of @xmath39 as a ` biasing field ' , in analogy with spin models , as has been suggested before @xcite . also , this way the signal is being applied to both the parameters , rather than preferentially to only one of them .",
    "thus , the application of a signal of this form has the effect of introducing a time dependence into the system parameters .",
    "relaxation timescale ( @xmath40 ) in the ( @xmath37 ) plane.,width=297,height=268 ]    if one were to consider the case of a constant input signal , the fixed point would shift to a new location along a @xmath41 const .",
    "line in the ( @xmath37 ) plane .",
    "one can then imagine a protocol whereby a constant signal is switched on at @xmath42 and persists up to a time @xmath43 , following which the system reverts to its original state . learning and forgetting are both exponential relaxation processes in this setting , and two timescales naturally enter the picture : the [ learning ] timescale for moving to the new stable state ( after the signal is applied ) , and the [ forgetting ] timescale for reverting to the default fixed point once the signal is turned off .",
    "it may be noted that , since the relaxation timescale is a function of the parameters @xmath13 of the end state , it is in general different for the learning and the forgetting : the former depends on the values of @xmath13 in the presence of the signal , while the latter depends on the unperturbed state .",
    "one of the strengths of the preceding approach is that performance optimization can be directly related to the microscopic parameters @xmath13 ( in contrast to the approach of @xcite where optimization relied on the relative values of multiplicative constants ) .",
    "[ fig2 ] suggests an approach to optimizing the performance , i.e. achieving long forgetting times and typically short learning times : by choosing the default parameters in such a way that the unperturbed state of the system lies near the lower right corner ( or the upper left corner ) , the timescale for retention , being only a function of the unperturbed state , can be made very long , with the average timescale for learning applied signals being shorter .",
    "this limit corresponds to having a wide separation between the timescales associated with the two parameters @xmath44 and @xmath45 .",
    "( if , alternatively , one were to choose the default values of @xmath13 to lie closer to the middle of the graph , the forgetting timescale would be shortened , an undesirable feature . ) it may be noted that translating the default state along the diagonal line given by @xmath46 only modifies the retention time , while leaving the range of signals that can be absorbed ( and thus the average learning time ) unchanged .",
    "additionally , given the form of the signal as defined above , which can only produce shifts parallel to the @xmath46 diagonal , it should be clear that the range of allowed signals is maximal when the system ` lives ' on this diagonal , rather than on any other line parallel to it .",
    "[ fig3 ] illustrates the system response for two example choices of the default parameters .",
    "when the parameters are well separated , the system shows more optimal behavior .    .",
    "the signal is first turned on at @xmath47 ( not shown here ) , turned off at @xmath48 and then re - applied at @xmath49 ( indicated by dashed lines ) .",
    "the @xmath50-axis represents the variable @xmath29 relative to its value at the corresponding _ default _",
    "fixed point .",
    "the red curve corresponds to @xmath51 and the blue curve to @xmath52 .",
    "the same signal amplitude ( @xmath53 ) has been chosen in both the cases .",
    "the system that ` forgets ' faster ` relearns ' slower.,width=297,height=268 ]",
    "it may be recalled that the work of ref .",
    "@xcite was the inspiration for the present one , and we wished to enrich , from a physical point of view , a formalism that had an appealing link to experiment .",
    "a direct comparison of the effective equations in the two can not be made ( although they have some formal similarity ) .",
    "nevertheless , the basic ingredients common to both are a notion of ` memory ' and two timescales , which e.g. leads to quicker relearning in the protocols explored in @xcite .",
    "we have provided a theoretical basis for the work of @xcite , by introducing a microscopic description , where the outcome - based competition between neighboring synapses leads to our mean - field representation .",
    "the nonlinearities , related to synaptic efficacies , provide a more reasonable basis for memory to be observed .",
    "additionally , the formal similarity between our work and that of  @xcite in terms of an effective variable description suggests that our dynamical equation could also be experimentally realizable . a detailed exploration of the model presented here under different experimental protocols will be carried out in a separate study .    to summarize ,",
    "we have sketched a plausible model for competing synaptic interactions mediated by neural units , in analogy with an agent - based model of adaptive strategic learning .",
    "this model provides a microscopic basis for memory ; this is encapsulated in the parameters @xmath13 associated with the weights of the two states of a binary plastic synapse .",
    "we have , in the previous section , chosen a simple illustration of the memory manifested in our model , by devising an elementary protocol of learning and forgetting , and imposing it on our mean - field representation of synaptic dynamics .",
    "our formalism allows us to conclude that memory is optimized if the timescales corresponding to strong and weak synapses are well separated .",
    "it is also rather general , and amenable to many possible extensions : the simplest such might be the choice of different representations of signals , or an increase in complexity for the signals and protocols .",
    "also , one could extend the formalism to include stochasticity of synaptic conversions @xcite ; and of course , one could choose to look at the equations beyond mean field , including for example spatial correlations .    in conclusion ,",
    "we have devised a rather general formalism for synaptic dynamics based on multiple timescales , whose results may be viewed against the backdrop of previous work underscoring the adaptive significance of having a multiplicity of timescales in biological mechanisms of learning and memory @xcite .",
    "the authors thank dr nicolas brunel and dr jean - marc luck for helpful discussions .",
    "a.m. acknowledges a grant from the dst ( govt . of india ) under the project ",
    "generativity in cognitive networks \" , through which g.m . was supported ."
  ],
  "abstract_text": [
    "<S> competitive dynamics are thought to occur in many processes of learning involving synaptic plasticity . here </S>",
    "<S> we show , in a game theory - inspired model of synaptic interactions , that the competition between synapses in their weak and strong states gives rise to a natural framework of learning , with the prediction of memory inherent in a timescale for ` forgetting ' a learned signal . among our main results </S>",
    "<S> is the prediction that memory is optimized if the weak synapses are really weak , and the strong synapses are really strong . </S>",
    "<S> our work admits of many extensions and possible experiments to test its validity , and in particular might complement an existing model of reaching , which has strong experimental support . </S>"
  ]
}