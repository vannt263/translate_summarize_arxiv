{
  "article_text": [
    "classification is a major task in the fields of machine learning and pattern recognition . in binary classification",
    ", a hypothesis is constructed from a feasible hypothesis space based on the training set @xmath0 , where @xmath1 is a set of data points with @xmath2 sampled i.i.d . under a distribution from an input subspace , and @xmath3 with @xmath4",
    "are their corresponding labels .",
    "the obtained hypothesis , also known as classifier , is  good \" when it is able to generalize well the  knowledge \" learned from the training data to unseen instances .",
    "multiple - class cases can be analogously accomplished by a group of binary classifiers @xcite .",
    "arguably , among existing classifiers , _ support vector machine _ ( svm ) @xcite@xcite is the most popular one due to its promising performance . in general",
    ", the primal svm can be modeled as follows : @xmath5 where @xmath6 is a penalty function , @xmath7 performs as a regularizer on the learner @xmath8 and @xmath9 is the bias .",
    "the function @xmath10 is to map @xmath11 from the original @xmath12-dimensional feature space to a new @xmath13-dimensional one .",
    "moreover , @xmath14 is a non - negative coefficient that provides a trade - off between the loss term and the regularizer . if svm adopts the hinge loss as penalty , the above turns out to be : @xmath15 where the operator @xmath16 keeps the input scalar @xmath17 unchanged if @xmath17 is non - negative , returns zero otherwise , the extension of which to vectors and matrices is simply applied element - wise .",
    "furthermore , @xmath18 is a constant typically in the range @xmath19 $ ] for being meaningful . in practice ,",
    "@xmath18 is often selected to be either @xmath20 or @xmath21 for ease of computation , which correspond to @xmath22-norm and @xmath23-norm loss primal svms , respectively . as for the regularization term ,",
    "@xmath24 ( @xmath23 regularizer ) and @xmath25 ( @xmath22 regularizer ) are two classical options .",
    "as has been well recognized , a combination of various classifiers can improve predictions .",
    "ensemble approaches , with boosting @xcite and bagging @xcite as representatives , make use of this recognition and achieve strong generalization performance .",
    "the generalization error of ensemble mainly depends on two factors , formally expressed as @xmath26 , where @xmath27 is the mean - square error of the ensemble , @xmath28 represents the average mean - square error of component learners and @xmath29 stands for the average difference ( diversity ) between the ensemble and the components . _ error - ambiguity decomposition _",
    "@xcite , _ bias - variance - covariance decomposition _ @xcite and _ strength - correlation decomposition _ @xcite all confirm the above principle .",
    "this indicates that jointly minimizing the training error and maximizing the diversity of base learners is key to the ensemble performance .",
    "considering the popularity of svm and the potential of ensemble , it would be interesting and beneficial to equip svm with ensemble thoughts .",
    "this work concentrates on how to train a set of component svms and integrate them as an ensemble .",
    "more concretely , the contribution of this paper can be summarized as follows : 1 ) we define a new measurement , namely ( relaxed ) exclusivity , to manage the diversity between base learners , 2 ) 2e propose a novel ensemble , called exclusivity regularized machine ( erm ) , which concerns the training error and the diversity of components simultaneously , and 3 ) we design an augmented lagrange multiplier based algorithm to efficiently seek the solution of erm , the global optimality of which is theoretically guaranteed .",
    "it is natural to extend the traditional primal svm to the following ensemble version with @xmath30 components as : @xmath31 where @xmath32 $ ] and @xmath33^t$ ] .",
    "suppose we simply impose @xmath34 or @xmath35 on @xmath36 , all the components ( and the ensemble ) will have no difference with the hypothesis directly calculated from using the same type of regularizer .",
    "from this view , @xmath37 is critical to achieve the diversity .",
    "sub - sets and training @xmath30 classifiers separately on the sub - sets would lead to some difference between the components .",
    "but , this strategy is not so reliable since if the training data are sufficiently and i.i.d .",
    "sampled under a distribution , the difference would be very trivial . ]",
    "prior to introducing our designed regularizer , we first focus on the concept of diversity .",
    "although the diversity has no formal definition so far , the thing in common among studied measurements is that the diversity enforced in a pairwise form between members strikes a good balance between complexity and effectiveness .",
    "the evidence includes q - statistics measure @xcite , correlation coefficient measure @xcite , disagreement measure @xcite , double - fault measure @xcite , @xmath38-statistic measure @xcite and mutual angular measure @xcite .",
    "these measures somehow enhance the diversity , however , most of them are heuristic .",
    "one exception is diversity regularized machine @xcite , which attempts to seek the globally - optimal solution .",
    "unfortunately , it often fails because the condition required for the global optimality , say @xmath39 for all @xmath40 , is not always satisfied .",
    "further , li _",
    "proposed a pruning strategy to improve the performance of drm @xcite .",
    "but , drm requires too much time to converge , which limits its applicability . in this work",
    ", we define a new measure of diversity , _ i.e. _ ( relaxed ) exclusivity , as below .",
    "exclusivity between two vectors @xmath41 and @xmath42 is defined as @xmath43 , where @xmath44 designates the hadamard product , and @xmath45 is the @xmath46 norm .    from the definition",
    ", we can observe that the exclusivity encourages two vectors to be as orthogonal as possible .",
    "due to the non - convexity and discontinuity of @xmath46 norm , we have the following relaxed exclusivity .",
    "the definition of relaxed exclusivity between @xmath41 and @xmath42 is given as @xmath47 , where @xmath48 is the absolute value of @xmath17 .",
    "the relaxation is similar with that of the @xmath22 norm to the @xmath46 norm .",
    "[ def : rlxexc ]    it can be easily verified that @xmath49 , @xmath50 and @xmath51 , where @xmath52 is the vector with all of its @xmath53 entries being @xmath20 .    instead of directly using @xmath54",
    ", we employ the following : @xmath55 the main reasons of bringing @xmath34 into the regularizer are : 1 ) it essentially enhances the stability of solution , 2 ) it tends to mitigate the scale issue by penalizing large columns , and 3 ) as the relaxed exclusivity itself is non - convex , the introduction guarantees the convexity of the regularizer .",
    "finally , the proposed exclusivity regularized machine ( erm ) can be written in the following shape : @xmath56 in next sub - section , we will customize an efficient and effective augmented lagrangian multiplier algorithm ( alm ) to seek the solution to , which has rigorous convergence and global optimality guarantee as well as ( quasi ) linear complexity ( discussed in sec .",
    "[ sec : ta ] ) .    * remarks * as expressed in eq .",
    "[ eq : reg ] , we have motivated the @xmath57 regularizer from a novel perspective . it has been verified that , as one of mixed norms , the @xmath57 is in nature able to capture some structured sparsity @xcite . in general",
    ", the regression models using such mixed norms can be solved by a modified focuss algorithm @xcite .",
    "@xcite introduced the @xmath57 regularizer into a specific task , _ i.e. _ multi - task feature selection , and used the subgradient method to seek the solution of the associated optimization problem .",
    "the responsibility of the @xmath57 regularizer is to enforce the negative correlation among categories @xcite .",
    "recently , kong _ et al . _",
    "@xcite utilized @xmath57 norm to bring out sparsity at intra - group level in feature selection , and proposed an effective iteratively re - weighted algorithm to solve the corresponding optimization problem . in this work , besides the view of motivating the @xmath57 regularizer , its role in our target problem , say constructing an ensemble of svms , is also different with the previous work @xcite .",
    "the functionalities of @xcite and @xcite are the intra - exclusivity of multiple hypotheses ( tasks ) and the inter - exclusivity of a single hypothesis respectively , while our principle is the diversity of multiple components of a single ensemble hypothesis .      with the trick that @xmath58 , we introduce auxiliary variables @xmath59 .",
    "in the sequel , the minimization of can be converted into : @xmath60 where @xmath61 $ ] , @xmath62^t$ ] , @xmath63 $ ] and @xmath64^t$ ] . and",
    "each column of @xmath65 is @xmath66 .",
    "please notice that , the constraint @xmath67 is added to make the objective separable and thus solvable by the augmented lagrangian multiplier framework .",
    "it is worth mentioning that , thanks to the convexity of each term involved in the objective and the linearity of the constraints , the target problem is convex . the lagrangian function of can be written in the following form : @xmath68 with the definition @xmath69 , where @xmath70 represents matrix inner product and @xmath71 is a positive penalty scalar .",
    "in addition , @xmath72 and @xmath73 are lagrangian multipliers .",
    "the proposed solver iteratively updates one variable at a time by fixing the others .",
    "below are the solutions to sub - problems .",
    "* @xmath36 sub - problem * with the variables unrelated to @xmath36 fixed , we have the sub - problem of @xmath36 : @xmath74 as observed from the problem , it can be split into a set of smaller problems . for each row @xmath75 , instead of directly optimizing ,",
    "we resolve the following equivalent objective : @xmath76 where @xmath77 is formed by : @xmath78\\bigg ) ,   \\label{eq : g}\\ ] ] where @xmath79 ( a small constant ) is introduced to avoid zero denominators .. certainly , when @xmath79 , @xmath80 infinitely approaches to @xmath81 .",
    "] since both @xmath77 and @xmath75 depend on @xmath75 , to find out the solution to , we employ an efficient re - weighted algorithm to iteratively update @xmath77 and @xmath75 . as for @xmath75 , with @xmath77 fixed , equating the partial derivative of with respect to @xmath75 to zero yields : @xmath82 then @xmath83 is updated using @xmath84 as in . the procedure summarized in algorithm [ alg : w ]",
    "terminates when converged .    *",
    "@xmath85 sub - problem * dropping the terms independent on @xmath85 leads to a least squares regression problem : @xmath86 * @xmath87 sub - problem * similarly , picking out the terms related to @xmath87 gives the following problem : @xmath88 where @xmath89 .",
    "it can be seen that the above is a single - variable @xmath21-piece piecewise function .",
    "thus , to seek the minimum of each element in @xmath87 , we just need to pick the smaller between the minima when @xmath90 and @xmath91 .",
    "moreover , we can provide the explicit solution when @xmath92 or @xmath21 ( for arbitrary @xmath18 we will discuss it in sec .",
    "[ sec : c&d ] ) .",
    "when @xmath92 : @xmath93+{\\mathbf}{\\bar{\\omega}}\\circ{\\mathbf}{s}^{(t)}. \\label{eq : e1}\\ ] ] for @xmath94 : @xmath95 where @xmath96 is an indicator matrix , and @xmath97 is the complementary support of @xmath98 .",
    "the definition of shrinkage operator on scalars is @xmath99{\\mathrel{\\mathop:}=}{\\operatornamewithlimits{sgn}}(u)\\max(|u|-\\epsilon,0)$ ] .",
    "the extension of the shrinkage operator to vectors and matrices is simply applied element - wise .",
    "[ alg : w ]    * initial .",
    ": * @xmath100 ; @xmath101 ; @xmath102 ; @xmath103;@xmath104 ; @xmath105 ; @xmath106 ; @xmath107 ; + [ alg : erm ]    * @xmath108 sub - problem * there are two terms involve @xmath108 .",
    "the associated optimization problem reads : @xmath109 this sub - problem only contains quadratic terms , so it is easy to compute the solution in closed - form : @xmath110 where we denote @xmath111 and @xmath112 .",
    "* multipliers and @xmath71 * besides , there are two multipliers and @xmath71 to update , which are simply given by : @xmath113    for clarity , the procedure of solving is outlined in algorithm [ alg : erm ] .",
    "the algorithm should not be terminated until the change of objective value is smaller than a pre - defined threshold ( in the experiments , we use @xmath114 )",
    ". please see algorithm [ alg : erm ] for other details that we can not cover in the text .",
    "first , we come to the loss term of erm , which accesses the total penalty of base learners as : @xmath115 where @xmath116 .",
    "alternatively , the loss of the ensemble @xmath117 is as : @xmath118 based on the above , we have the relationship between the two losses as described in proposition [ pro : upb ] .",
    "let @xmath119 , ... , @xmath120 be the component learners obtained by erm ( alg .",
    "[ alg : erm ] ) , and @xmath117 the ensemble , the loss of @xmath121 is bounded by the average loss of the base learners .",
    "[ pro : upb ]    please note that , for each training instance , substituting @xmath121 with @xmath122 into yields @xmath123 . due to the convexity of the hinge loss together with @xmath116 , the relationship @xmath124 holds by applying the jensen s inequality .",
    "the proposition indicates that as we optimize erm , an upper bound of the loss of the ensemble is also minimized .",
    "thus , incorporating with our proposed regularizer , erm is able to achieve the goal of simultaneously optimizing the training error of ensemble and the diversity of components .",
    "next , we shall consider the convergence and optimality of the designed algorithms . before discussing alg .",
    "[ alg : erm ] , we have to confirm the property of alg .",
    "[ alg : w ] , which is established by theorem [ the : w ] .",
    "the updating rules and for solving the problem , _",
    "i.e. _ algorithm [ alg : w ] , converges and the obtained optimal solution is exactly the global optimal solution of the problem .",
    "[ the : w ]    algorithm [ alg : w ] is actually a special case of the algorithm proposed in @xcite . due to the limited space",
    ", we refer readers to @xcite for the detailed proof .    with theorem [ the : w ]",
    ", it is ready to analyze algorithm [ alg : erm ] . to this end",
    ", the following lemmas are required .",
    "@xcite let @xmath125 be a real hilbert space endowed with an inner product @xmath70 and a corresponding norm @xmath126 , and any @xmath127 , where @xmath128 denotes the subgradient .",
    "then @xmath129 if @xmath130 , and @xmath131 if @xmath132 , where @xmath133 is the dual norm of the norm @xmath126 .",
    "[ lemma : dual ]    both the sequences @xmath134 and @xmath135 generated by algorithm [ alg : erm ] are bounded .",
    "[ lemma : muliplers ]    according to the optimality conditions for with respect to @xmath87 and @xmath36 , and the updating rules of multipliers , we know @xmath136 . using lemma",
    "[ lemma : dual ] reaches that the sequences @xmath134 and @xmath135 are both bounded because the dual norms of @xmath137 and @xmath138 are @xmath139 and @xmath140 , respectively .",
    "now , we have come to the convergence and optimality of our proposed algorithm [ alg : erm ] .",
    "the solution consisting of the limit of the sequences @xmath141 , @xmath142 and @xmath143 generated by algorithm [ alg : erm ] , _ i.e. _ @xmath144 , is global optimal to erm , and the convergence rate is at least @xmath145 .",
    "[ the : erm ]    as the vital natural property of an alm algorithm , the following holds : @xmath146 due to @xmath147 and the boundedness of @xmath135 and @xmath134 , we can conclude that @xmath148 is upper bounded . as a consequence",
    ", @xmath149 is upper bounded , that is to say @xmath141 and @xmath143 are bounded . according to the updating rules of multipliers , the constraints are satisfied when @xmath150 . in other words , due to the boundedness of the sequences @xmath135 and @xmath134 , the right sides of @xmath151 and @xmath152 infinitely approximate to @xmath153 .",
    "this proves the feasibility of the solution by alg .",
    "[ alg : erm ] as well as the boundedness of @xmath154 and @xmath142 .",
    "because of the above and the boundedness of multipliers @xmath135 and @xmath134 , we have : @xmath155 thanks to the feasibility of solution , the last two terms in are neglectable .",
    "so alg . [ alg : erm ] converges to a global optimal solution to .",
    "the convergence rate is at least @xmath145 according to .",
    "in addition , we show the complexity of alg .",
    "[ alg : erm ] . the operations including matrix addition and subtraction are relatively cheap , and thus can be ignored .",
    "updating each row of @xmath36 takes @xmath156 and @xmath157 for and respectively , where @xmath158 is the ( inner ) iteration number of alg .",
    "[ alg : w ] .",
    "please note that , due to the diagonality of @xmath77 , the inverse of @xmath159 only needs @xmath160 .",
    "therefore , the cost of alg .",
    "[ alg : w ] is @xmath161 .",
    "the @xmath85 sub - problem requires @xmath162 .",
    "the complexity of the @xmath87 sub - problem is @xmath162 , for both @xmath92 and @xmath94 . solving @xmath108 spends @xmath163 . besides , the update of the multipliers is at @xmath162 expense . in summary , alg .",
    "[ alg : erm ] has the complexity of @xmath164 , where @xmath165 is the number of ( outer ) iterations required to converge .",
    "we use @xmath166 popularly adopted benchmark datasets from various sources for performance evaluation : including _ sonar _ @xmath167 , _ german _ @xmath168 , _ australian _ @xmath169 , _ ijcnn1 _",
    "@xmath170 , _ heart _ @xmath171 , _ ionosphere _",
    "@xmath172 , _ diabetes _",
    "@xmath173 , _ liver _ @xmath174 and _ splice _ @xmath175 .",
    "all experiments are conducted on a machine with @xmath176 ghz cpu and @xmath177 g ram .    , convergence speed and training time , title=\"fig : \" ] , convergence speed and training time , title=\"fig : \" ] , convergence speed and training time , title=\"fig : \" ] , convergence speed and training time , title=\"fig : \" ]    * parameter effect * here , we evaluate the training and testing errors of erm@xmath178 ( @xmath179 means the component number ) against varying values of @xmath14 in the range @xmath180 $ ] .",
    "all the results shown in this experiment are averaged over @xmath181 independent trials , each of which randomly samples half data from the _ sonar _ dataset for training and the other half for testing . the first picture in fig .",
    "[ fig : pe ] displays the training error and testing error plots of l@xmath182 loss erm with l@xmath182 loss psvm @xcite ( denoted as l@xmath182-psvm ) as reference . from the curves",
    ", we can observe that , as @xmath14 grows , the training errors drop , as well as composing less base learns leads to a smaller training error .",
    "this is because more and more effort is put on fitting data . as regards the testing error ,",
    "the order is reversed , which corroborates the recognition that the predication gains from the diversity of classifiers , and reveals the effectiveness of our design in comparison with l@xmath182-psvm .",
    "besides , the testing errors change very slightly in a relatively large range of @xmath14 , which implies the insensitivity of erm to @xmath14 .",
    "the second picture corresponding to @xmath92 shows an additional evidence to @xmath94 . although the performance gaps between the different cases shrink , the improvement of erm is still noticeable .",
    "based on this evaluation , we set @xmath14 to @xmath21 for both l@xmath183-erm and l@xmath182-erm in the rest experiments .    * convergence speed & training time * [ sec : c&t ] although the convergence rate and complexity of the proposed algorithm have been theoretically provided , it would be more intuitive to see its empirical behavior .",
    "thus , we here show how quick the algorithm converges , without loss of generality , on the _ ijcnn1 _ dataset . from the third picture in fig . [",
    "fig : pe ] , we can observe that , when @xmath94 , all the three cases converge with about @xmath184 iterations .",
    "the cases correspond to @xmath92 take more iterations than @xmath94 ( about @xmath185 iterations ) , but they are still very efficient .",
    "please note that , for a better view of different settings , the objective plots are normalized into the range @xmath186 $ ] .",
    "the most right picture in fig .",
    "[ fig : pe ] gives curves of how the cpu - time used for training increases with respect to the number of training samples .",
    "since the training time is too short to be accurately counted , we carry out each test for @xmath181 independent trials , and report the total training time ( in seconds ) . as can be seen , the training time for both @xmath92 and @xmath21 is quasi linear with respect to the size of training data .",
    "for all the three cases correspond to erm@xmath187 , erm@xmath188 and erm@xmath189 , the choice of @xmath18 barely brings differences in time .",
    "the gaps between the three cases dominantly come from the number of base learners .",
    "the primal svm only needs to learn one classifier while erm requires to train multiple bases .",
    "@xcite , pegasos @xcite , bmrm @xcite , and tron @xcite , pcd @xcite and dcd @xcite . ]",
    "[ tab : acc ]    [ tab : time ]    * performance comparison * this part compares our proposed erm with the classic ensemble models including adaboost and bagging , and the recently designed drm .",
    "the codes of drm are downloaded from the authors website , while those of adaboost and bagging are integrated in the matlab statistics toolbox ( _ fitensemble _ function ) .",
    "the base of drm , @xmath190-svm , is from libsvm .",
    "the sizes and distributions of the datasets are various , to avoid the effect brought by the amount of training data and test the generalization ability of the ensembles learned from different types of data , the number of training samples for all the datasets is fixed to @xmath191 .",
    "table [ tab : acc ] provides the quantitative comparison among the competitors .",
    "we report the mean testing errors over @xmath181 independent trials by randomly sampling @xmath191 data points from a dataset as its training set and the rest as the testing .",
    "adaboost and bagging are inferior to erm and drm in most cases .",
    "the exception is on the _ splice _ dataset . as for our erm",
    ", we can see that it significantly outperforms the others on the _ australian _ , _ sonar _ , _ heart _ and _ ionosphere _ , and competes very favorably on the _ german _ dataset . on each dataset",
    ", we assign ranks to methods . and",
    "the average ranks ( a.r . ) of the competitors over the involved datasets are given in the last column of tab .",
    "[ tab : acc ] .",
    "it can be observed that the top five average ranks are all lower than @xmath192 , and four of which are from erm methods .",
    "the best and the second best belong to l@xmath182-erm@xmath189 ( a.r.=2.5 ) and l@xmath182-erm@xmath188 ( a.r.=3.4 ) respectively , while the fourth and fifth places are taken by l@xmath183-erm@xmath188 ( a.r.=4.8 ) and l@xmath183-erm@xmath189 ( a.r.=4.9 ) respectively .",
    "the third goes to drm@xmath188 , the average rank of which is @xmath193 .",
    "the results on the _ ijcnn1 _ are not included in the table , because all the competitors perform very closely to each other , which may lead to an unreliable rank .",
    "another issue should be concerned is the efficiency .",
    "table [ tab : time ] lists the mean training time over all the datasets and each dataset executes @xmath181 runs . from the numbers , we can see the clear advantage of our erm . l@xmath183-erm@xmath188 and l@xmath182-erm@xmath188 only spend about @xmath194 on training , while the erms with @xmath184 components , _ i.e. _ l@xmath183-erm@xmath189 and l@xmath182-erm@xmath189 , cost less than @xmath195 .",
    "both adaboost and bagging are sufficiently efficient , which take less than @xmath196 to accomplish the task .",
    "but the training uses @xmath197 and @xmath198 by drm for the @xmath181-base and @xmath184-base cases respectively , which are about @xmath199 times expensive as the proposed erm .",
    "we would like to mention that the core of drm is implemented in c++ , while our erm is in pure matlab .",
    "moreover , as theoretically analyzed in sec .",
    "[ sec : ta ] and empirically verified in sec .",
    "[ sec : c&t ] , our algorithm is ( quasi ) linear with respect to the size of training set .",
    "in other words , the merit of erm in time would become more conspicuous as the scale of training data increases , in comparison with adaboost , bagging and drm . due to space limit",
    ", only several experiments are shown in the paper to demonstrate the efficacy of our erm . to allow more experimental verification",
    ", our code can be downloaded from http://cs.tju.edu.cn/orgs/vision/~xguo/homepage.htm",
    "the diversity of component learners is critical to the ensemble performance .",
    "this paper has defined a new measurement of diversity , _ i.e. _ exclusivity .",
    "incorporating the designed regularizer with the hinge loss function gives a birth to a novel model , namely exclusivity regularized machine .",
    "the convergence of the proposed alm - based algorithm to a global optimal solution is theoretically guaranteed .",
    "the experimental results on several benchmark datesets , compared to the state - of - the - arts , have demonstrated the clear advantages of our method in terms of accuracy and efficiency .",
    "our framework is ready to embrace more elaborate treatments for further improvement .",
    "for instance , due to the relationship @xmath50 , as discussed in sec .",
    "[ sec : d&f ] , the sparsity on @xmath36 can be promoted by extending @xmath200 to @xmath201 $ ] , where @xmath202 is a weight coefficient of the sparsity .",
    "in addition , it is difficult to directly solve the @xmath87 sub - problem with arbitrary given @xmath18 . fortunately , in this work , it is always that @xmath116 .",
    "thus the partial derivative of with respect to @xmath87 is monotonically increasing .",
    "the binary search method can be employed to narrow the possible range of @xmath87 by half via each operation .",
    "it is positive that our erm can be widely applied to various classification tasks .",
    "although , for avoiding distractions , no experiments are provided to evaluate the performance of the possible variants , it is positive that our erm can be widely applied to various classification tasks .",
    "xiaojie guo would like to thank dr .",
    "ju sun with department of electrical engineering , columbia university , for his suggestions on this work ."
  ],
  "abstract_text": [
    "<S> it has been recognized that the diversity of base learners is of utmost importance to a good ensemble . </S>",
    "<S> this paper defines a novel measurement of diversity , termed as _ </S>",
    "<S> exclusivity_. with the designed exclusivity , we further propose an ensemble model , namely _ exclusivity regularized machine _ </S>",
    "<S> ( erm ) , to jointly suppress the training error of ensemble and enhance the diversity between bases . </S>",
    "<S> moreover , an augmented lagrange multiplier based algorithm is customized to effectively and efficiently seek the optimal solution of erm . </S>",
    "<S> theoretical analysis on convergence and global optimality of the proposed algorithm , as well as experiments are provided to reveal the efficacy of our method and show its superiority over state - of - the - art alternatives in terms of accuracy and efficiency . </S>"
  ]
}