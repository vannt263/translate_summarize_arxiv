{
  "article_text": [
    "graphical models are widely used to model multivariate systems .",
    "estimation of conditional independence structure ( often called  network inference \" or  structure learning \" ) is increasingly a mainstream approach , for example in computational biology .",
    "given data @xmath0 a network estimator gives an estimate @xmath1 of the conditional independence graph @xmath2 .",
    "the type of graph and its scientific interpretation depend on the model and scientific context .    in many applications ,",
    "data is collected on multiple individuals @xmath3 that may differ with respect to interplay between variables , such that corresponding conditional independence graphs @xmath4 may be individual - specific .",
    "for example , in biology , individuals may correspond to different patients or cell lines and the networks themselves to gene regulatory or protein signaling networks .",
    "interplay in such networks can depend on the genetic and epigenetic state of the individuals , such that even for a well - defined system , such as signaling downstream of a certain receptor class , or a sub - part of the transcriptional program , details may differ between even closely related samples @xcite .",
    "for example , in yeast signaling , edges in the well - understood mitogen - activated protein kinase ( mapk ) pathway can change depending on context @xcite , whilst in cancer , it is thought that individual cell lines may differ with respect to signaling network connections . continuing reduction in the unit cost of biochemical assays has led to an increase in experimental designs that include panels of potentially heterogeneous individuals @xcite . in such settings , given individual specific data",
    "@xmath5 , there is scientific interest in the individual specific networks @xmath4 and their similarities and differences .",
    "the case of multiple related individuals poses a number of statistical challenges for network inference :    * * efficiency . *",
    "if the networks share features , then individual - level estimation ( i.e. @xmath6 ) , may be inefficient , since there is no sharing of information at the population level . although individual network estimators @xmath7 may be well - behaved as the individual - specific sample size @xmath8 grows large ( e.g. @xcite ) , in practice small - to - moderate @xmath8 s and the inherently high - dimensional nature of network inference render inference challenging . * * data aggregation . * aggregating data from multiple individuals and then performing network inference offers a way to obtain larger sample sizes",
    ". however , in settings where data from individuals are inhomogeneous ( in the sense that the graphs @xmath4 may differ between individuals ) , inferences regarding conditional independence structure can not in general be obtained from aggregated data ( simpson s paradox ) and testing whether data aggregation is appropriate may be challenging @xcite . estimating sufficiently homogeneous groups using mixture models and related clustering approaches",
    "offers an alternative @xcite , but is challenging in the network setting , as we discuss further below . * * ancillary information .",
    "* ancillary information may be available both at the  global \" ( population ) and  local \" ( individual ) levels .",
    "for example , in gene regulation , the biological literature provides general information concerning gene - gene interplay , whilst patient - specific characteristics might also be available . when such ancillary information is available it may be desirable to include it in inference ( the `` conditionality principle '' ) , but doing so requires care in prior elicitation @xcite .",
    "in this paper we present a bayesian approach to joint estimation of networks .",
    "the high - level formulation we propose is general and could be applied to a wide range of graphical model formulations .",
    "we present a detailed development for the time - course setting , focusing on directed graphical models called dynamic bayesian networks ( dbns ) .",
    "these are directed acyclic graphs ( dags ) with explicit time - indices @xcite .",
    "the main features of our approach are :    * * bayesian framework . *",
    "we use a hierarchical bayesian model , summarized in fig .",
    "[ model ] .",
    "regularization is achieved using priors over both parameters and networks .",
    "we focus in particular on regularization of individual networks , introducing a latent network @xmath2 to couple inference across the population .",
    "we report posterior marginal inclusion probabilities for every possible edge , thus providing a confidence measure for the inferred network topologies and offering robustness in settings where posterior mass is not highly concentrated on a single model . * * computationally efficient estimation from time - course data .",
    "* for the time - course setting , we put forward an efficient and deterministic algorithm .",
    "this is done by exploiting modularity of the dbn likelihood @xcite coupled with a sparsity restriction and a sum - product - type algorithm . in moderate - dimensional settings this allows exact joint estimation to be carried out in seconds to minutes ( we discuss computational complexity below ) making our approach suitable for interactive use . *",
    "* incorporation of ancillary information .",
    "* we allow for the inclusion of individual - specific ancillary information .",
    "following @xcite we also allow for interventional data , in which time courses are obtained under external intervention on network nodes .",
    "joint estimation of graphical models has recently been discussed in the penalized likelihood literature , with contributions including @xcite . in these studies ,",
    "@xmath9 penalties , such as the fused graphical lasso , are used to couple together inference of gaussian graphical models ( ggms ) .",
    "our work complements these efforts by offering a bayesian formulation of joint estimation .",
    "this facilitates regularization using prior and ancillary network information .",
    "moreover , our approach provides a natural way to estimate confidence in the inferred structure , providing robustness in multi - modal problems @xcite .",
    "further , we focus on the time - course setting and dbns rather than static data and ggms .",
    "however , we note that unlike the above penalized approaches the bayesian approach we propose is not well - suited to extremely high - dimensional settings with thousands of variables .",
    "a recent paper by @xcite considers bayesian joint estimation for time - course data .",
    "our work is in the same vein but differs in two main respects .",
    "first , we allow for prior information regarding the network structure and ancillary information including individual - specific characteristics .",
    "network priors and ancillary information can usefully constrain inference , not least in biological settings .",
    "for example in the cancer signaling example we consider below , much is known concerning relevant biochemistry ( fig . [ literature ] ) and individual - specific information pertaining to e.g. mutation status and receptor expression is often available ( nowadays also in the clinical setting ) .",
    "second , for the time - course setting , the exact algorithm we propose offers massive computational gains in comparison to the approach proposed by @xcite . as we discuss in detail below the methodology of @xcite is prohibitively computationally expensive for the applications we consider here .",
    "third , the computational efficiency of our approach allows us to present a much more extensive study of joint estimation , using both simulated and real data , than has hitherto been possible .",
    "this adds to our understanding of the performance of hierarchical bayesian formulations for joint estimation .",
    "mixtures of graphical models have been used to explore heterogeneous populations @xcite .",
    "however , mixture modeling requires the strong assumption that there exist groups which are ( sufficiently ) homogeneous with respect to model parameters .",
    "otherwise , mixture components are forced to model heterogeneous populations , resulting in potentially poor fit and networks that may not be scientifically meaningful . moreover , while graphical model estimation remains non - trivial , mixtures of graphical models are still more challenging , due to a number of factors relating to the hidden nature ( and number ) of the mixture components .",
    "further related work includes @xcite , who propose a bayesian approach to network inference based on multiple , steady - state datasets where in each dataset only a subset of the ( shared ) underlying network is identifiable .",
    "@xcite extend the information sharing scheme from @xcite in the context of inference for time - varying networks .",
    "@xcite considers covariance estimation from a heterogeneous population , treating individual covariance matrices as samples from a matrix - valued probability distribution .",
    "network priors have been discussed in the literature , including @xcite .",
    "our work differs from these efforts by focusing on joint estimation ; as we describe below , this leads to a different model structure and prior specification .",
    "the remainder of the paper is organized as follows . in section",
    "[ sec : methods ] we lay out a hierarchical bayesian formulation and in section [ sec : computers ] we discuss computationally efficient exact inference .",
    "empirical results are presented in section [ sec : results ] using simulated ( section [ silico ] ) datasets .",
    "finally we close with a discussion of our findings in section [ sec : discussion ] .",
    "we carry out joint network inference using the hierarchical model shown in fig . [ model ] that includes a prior network ( @xmath10 ) as well as a latent network ( @xmath2 ) ; each individual network ( @xmath4 ; we use superscript notation when referring to a particular individual ) is conceptually viewed as a variation upon the latter .",
    "individual data @xmath11 are then conditional upon individual networks .",
    "estimates of the individual networks @xmath4 are regularized by shrinkage towards the common latent network @xmath2 which in turn may be constrained by an informative network prior .",
    "since the latent network is itself estimated , this allows for adaptive regularization .",
    "consider the space @xmath12 of ( directed ) networks ( not necessarily acyclic ) on the vertex set @xmath13 .",
    "a network @xmath14 decomposes over parent sets as @xmath15 where @xmath16 are the network parents of @xmath17 .",
    "write @xmath18 for the set of possible parent sets for variable @xmath19 , such that formally @xmath20 .",
    "write @xmath21 for the set of individuals in the population .    as shown in fig",
    ". [ model ] , each individual network @xmath4 is conditional on a latent network @xmath2 which in turn depends on a prior network @xmath10 ( section [ priors ] ) . as in any graphical model , data",
    "@xmath11 is conditional on graph @xmath4 and parameters @xmath22 ; @xmath23 denotes any ancillary information available on individual @xmath24 . in this section",
    "we describe our general model and network priors , while in section [ sec : computers ] we discuss the special case of inference for time - course data , giving full details of the likelihood for that case .",
    "the model is specified by @xmath25 where the functionals @xmath26 and hyperparameters @xmath27 must be specified ( section [ priors ] ) .",
    "this formulation is borrowed from statistical mechanics , where @xmath28 may be interpreted as energy terms , @xmath27 as inverse temperature parameters and eqns .",
    "[ gibbs2],[gibbs1 ] as boltzmann ( or gibbs ) distributions . taken together with a suitable graphical model likelihood @xmath29",
    ", we obtain the data - generating model .",
    "jni performs inference jointly over @xmath30 , with information sharing occurring via the latent network @xmath2 .",
    "the use of a latent network follows @xcite . in some biological settings",
    ", it may be natural to think of the latent network as describing a `` wild type network '' , however such an interpretation is not required .",
    "we refer to this general formulation as joint network inference ( jni ) .      specifying a network prior ( eqn .",
    "[ gibbs2 ] ) requires a penalty functional @xmath31 and a prior network @xmath32 , with the former capturing how close a candidate network @xmath14 is to the latter @xcite .",
    "we discuss choice of @xmath10 below . given @xmath10 , a simple choice of penalty function @xmath33 is the structural hamming distance @xmath34 . here @xmath35 denotes the symmetric difference of sets @xmath36 and @xmath37 and @xmath38 denotes cardinality of the set @xmath36 .",
    "the hyperparameter @xmath39 controls the strength of the prior network @xmath10 ( eqn .",
    "[ gibbs2 ] ) . for brevity",
    "we follow @xcite by restricting attention to shd priors , however our formulation is general ( see below ) and compatible with other penalty functionals . for their work on joint estimation of inverse covariance matrices , @xcite employed the fused graphical lasso ( fgl ) penalty , which may be interpreted as a real - valued extension of shd ( strictly speaking , there is no analogue of the latent network @xmath2 here ; fgl directly penalizes the difference between individual networks @xmath40 ) . another interesting extension due to @xcite distinguishes @xmath41 ( `` false prior positives '' ) and @xmath42 ( `` false prior negatives '' ) by allocating a separate inverse temperature hyperparameter for each case .",
    "alternatively , one could employ a binomial prior as described in @xcite , which provides the same distinction , but allows for the hyperparameters of the binomial to be integrated out .",
    "conditional on a latent network @xmath2 , individual networks @xmath4 are regularized in a similar way , as @xmath43 . in their work on combining multiple data sources , @xcite",
    "allow the @xmath44 to vary over individuals ( data sources ) @xmath3 , with @xmath44 reflecting the quality of dataset @xmath24 .",
    "likewise @xcite learn the @xmath44 on an individual by individual basis .",
    "however , in both studies , hyperparameter elicitation is non - trivial ( see section [ elicit ] ) . to further limit scope ,",
    "we consider only the special case where @xmath45 .    when ancillary information @xmath23 is available regarding a specific individual network @xmath4 , it is desirable to augment the prior specification in such a way as to condition upon @xmath23 . in general such modification",
    "will be application specific .",
    "although we focus on shd priors , the inference procedures presented in this paper apply to the more general class of modular priors , which may be written in the form @xmath46 for some functionals @xmath47 .",
    "modularity here refers to a factorization over variables @xmath17 , implying that only local information is available _ a priori_. the shd priors are clearly modular .      up to inclusion of ancillary information ,",
    "prior strength is fully determined , in this simplified setting , by the parameter pair @xmath48 .",
    "taking @xmath49 requires that the latent network @xmath2 is ( almost surely ) identical to the prior network @xmath10 ; in the limit this corresponds to treating network inference for each individual separately , i.e. the estimator @xmath6 .",
    "we call this approach `` independent network inference '' ( ini ) .",
    "conversely , taking @xmath50 requires that ( almost surely ) individual networks @xmath4 do not deviate from the latent network @xmath2 ; this corresponds to assuming individuals have identical ( unknown ) network structure , but allowing parameter values @xmath22 to vary between individials , possibly becoming equal to zero .",
    "we call this approach `` aggregated network inference '' ( ani ) .",
    "taking @xmath51 together corresponds to using only the prior .",
    "a further , cruder , approach would be to simply combine all data in order to estimate a single network and parameter set , an approach which @xcite call `` monolithic '' .",
    "we compare these approaches empirically in section [ sec : results ] .",
    "elicitation of hyperparameters for network priors is an important and non - trivial issue .",
    "hyperparameters can be set using the data , but this poses a number of challenges , as reported in @xcite . in the context of sequential hierarchical network priors , @xcite observed that when there is limited data available , hyperparameters inferred from the data may be biased towards imposing too much agreement with the prior .",
    "@xcite used an improper hyperprior over the individual inverse temperature parameters @xmath44 , reporting that for most individuals posterior marginals did not differ greatly from the prior ( possibly due to uninformative data ) .",
    "similarly @xcite assigned improper flat prior distributions over the hyperparameters , reporting that estimation was rather difficult . due to such weak identifiability of hyperparameters , we chose instead to specify the hyperparameters @xmath52 in a subjective manner .    for subjective elicitation of network hyperparameters ,",
    "interpretable criteria are important .",
    "we present three criteria below which , for the special case of shd which we consider , are simple to implement and can be used for expert elicitation .",
    "these heuristics seek to relate the hyperparameters to more directly interpretable measures of the similarity and difference which they induce between prior , latent and individual networks .",
    "firstly , we note the following formula for the probability of maintaining edge status ( present / absent ) between the latent network @xmath2 and an individual network @xmath4 : @xmath53 this probability provides an interpretable way to consider the influence of @xmath54 . for example",
    "a prior confidence of @xmath55 that a given edge status in @xmath2 is preserved in a particular individual @xmath4 translates into a hyperparameter @xmath56 ( see sfig .",
    "1 ) . an analogous equation relates @xmath39 and @xmath57 , allowing prior strength to be set in terms of the probability that an edge status in the prior network @xmath10 is maintained in the latent network @xmath2 .",
    "a second , related approach is to consider the expected total shd between an individual network @xmath4 and the wild type network @xmath2 : @xmath58 this can be interpreted as the average number of edge changes needed to obtain @xmath4 from @xmath2 .",
    "an analogous equation holds for @xmath39 and @xmath59 .",
    "thirdly , in certain applications , the latent network @xmath2 may not have a direct scientific interpretation , in which case the criteria presented above may be unintuitive .",
    "then , hyperparameters could be elicited by consideration of ( a ) similarity between individual networks @xmath40 , and ( b ) concordance of individual networks @xmath4 with the prior network @xmath10 . specifically , we suggest the following two - step procedure : ( a ) exploit the fact that ( for an uniform prior on @xmath2 ) we have @xmath60 , which facilitates selection of @xmath61 via the formula @xmath62 .",
    "( b ) elicit @xmath59 using the observation that @xmath63 , so that @xmath64 .",
    "this two - step procedure uniquely determines a pair @xmath65 and hence unique hyperparameters @xmath66 .",
    "one drawback of this approach is that @xmath54 is selected under an assumption of a uniform prior on @xmath2 ; that is , @xmath67 .",
    "the quality of this procedure will therefore depend on the actual informativeness @xmath39 of the prior network @xmath10 on @xmath2 selected in step ( b ) .",
    "this approach to hyperparameter selection has an analogous interpretation using expected total shd",
    ".    the above heuristics may be useful in setting hyperparameters in practice .",
    "however , these heuristics are certainly no panacea and should be accompanied by checks of sensitivity to hyperparameters , as we report below .",
    "the jni model and network priors , as described above , are general . to apply the jni framework in a particular context",
    "requires an appropriate likelihood at the individual level , that is , to specify the distribution @xmath29 of data @xmath5 conditional on graph @xmath4 and parameters @xmath22 . in this section",
    "we focus on time - course data , using dbns to provide the likelihood .",
    "a dbn is a graphical model based on a dag whose vertices have explicit time indices ; see @xcite for details . here ,",
    "following @xcite and others , we use stationary dbns and permit only edges forwards in time .",
    "background and assumptions for dbns are described in appendix a. further assuming a modular network prior , structural inference for dbns can be carried out efficiently , as described in detail in @xcite .",
    "a novel contribution of this paper is to extend these results to allow for efficient and exact _ joint _ estimation . in order to simplify notation",
    ", we define a data - dependent functional @xmath68 which implicitly conditions upon observed history .",
    "let @xmath69 denote the observed value of variable @xmath19 in individual @xmath24 at time @xmath70 .",
    "the above notation allows us to conveniently summarize the product @xmath71 as @xmath72 .",
    "thus , we have that , for dbns , the full likelihood also satisfies modularity : @xmath73 in other words , the parent sets @xmath74 ( @xmath17 , @xmath3 ) are mutually orthogonal in the fisher sense , so that inference for each may be performed separately .    for this paper",
    ", the local bayesian score @xmath72 corresponds to the marginal likelihood for a linear autoregressive formulation described in appendix b. we consider an extension to facilitate the analysis of datasets which contain interventions ; this is described in appendix c. for this choice of model it is possible to construct a fully conjugate set of priors , delivering a closed form expression for the local score , contained in appendix d.      previous studies have used mcmc to generate samples from the posterior distribution over networks @xcite . however , ensuring mixing has proven to be extremely challenging for joint estimation , with both studies reporting extremely slow convergence .",
    "advances in mcmc and parallel computing may in the future ameliorate these issues @xcite , but at present it remains the case that fast , interactive joint estimation is currently challenging or prohibitively demanding using mcmc .",
    "we therefore propose an exact approach , using an in - degree restriction coupled with prior modularity and a sum - product - type algorithm , to facilitate efficient estimation .",
    "for example , the dream4 problem ( @xmath75 variables , @xmath76 individuals ) considered by @xcite was reported to require  several hours per node \" for mcmc convergence ; our approach solves the entire problem in @xmath77 seconds .",
    "our approach therefore complements mcmc - based inference , allowing fast , interactive investigation in moderate - dimensional settings .    specifically , we use exact model averaging to marginalize over graphs and report posterior marginal inclusion probabilities .",
    "we begin by computing and caching the marginal likelihoods @xmath72 for all parent sets @xmath78 , all variables @xmath17 and all individuals @xmath3 ; these could be obtained using essentially any suitable likelihood . the posterior marginal probability for an edge @xmath79 belonging to the latent network @xmath2",
    "is computed as @xmath80 where eqn .",
    "[ latent sp ] uses the sum - product lemma @xcite to interchange operators ( see appendix e ) .",
    "this final step has important consequences for algorithmic complexity ( see section [ computers ] ) .",
    "note that , whilst this derivation can made without the explicit marginalization of eqn .",
    "[ marginalisation ] , the approach is quite general and may be used analogously to facilitate estimation of individual networks @xmath4 : @xmath81 where again the sum - product lemma justifies the exchange of operators .",
    "following @xcite we reduced the space of parent sets @xmath18 using an in - degree restriction of the form @xmath82 for all @xmath78 , @xmath17 , @xmath3 .",
    "thus the cardinality of the space of parent sets @xmath83 is polynomial in @xmath84 , where it was previously exponential .",
    "this reduces summation over an exponential number of terms to a more manageable sum over polynomially many terms .",
    "moreover , in the protein signaling example to follow , bounded in - degree is a reasonable biological assumption .",
    "sensitivity to choice of @xmath85 is discussed in section [ metrics ] .",
    "caching of selected probabilities is used to avoid redundant recalculation .",
    "pseudocode is provided in appendix f , which consists of three phases of computation .",
    "storage costs are dominated by phases i and ii , which each requiring the caching of @xmath86 real numbers .",
    "phase ii dominates computational effort , with total ( serial ) algorithmic complexity @xmath87 . however , within - phase computation is `` embarrassingly parallel '' in the sense that all calculations are independent ( indicated by square parentheses notation in the pseudocode ) .",
    "thus an ideal implementation requires @xmath88 computational time .",
    "we provide a matlab implementation in supplement b.",
    "[ silico ]    we tested our joint estimation procedure on simulated time - course data .",
    "we compare our approach to the special cases of ( i ) inferring each network separately ( ini ) ; ( ii ) allowing parameters ( but not networks ) to change between individuals ( ani ) ; ( iii ) the naive approach of aggregating all data ( monolithic ) and ( iv ) simple temporal correlations ( absolute pearson coefficient ) . for a fair comparison , all methods , with the exception of ( iv ) ,",
    "were implemented so as to take account of the interventional nature of the data .",
    "we note that it is not possible to directly compare our results with @xcite since these methods do not apply to time - course data .",
    "the method of @xcite applies to time - course data , but the computational demands of the approach precluded application in this setting .",
    "specifically , in the simulated data example we report below , over 3000 rounds of inference were performed in total , on problems larger than dream4 ( @xmath75 , @xmath76 ) . using the approach of @xcite",
    ", these experiments would have required more than 10 years computational time ; in contrast our approach required less than 24 hours serial computation on a standard laptop .",
    "the proposed methodology addresses three questions , some or all of which may be of scientific interest depending on application ; ( i ) estimation of the latent network @xmath2 , ( ii ) estimation of individual networks @xmath89 , and ( iii ) estimation of differences between individual networks .",
    "we quantify performance for tasks ( i ) and ( ii ) using the area under the receiver operating characteristic ( roc ) curve ( aur ) .",
    "this metric , equivalent to the probability that a randomly chosen true edge is preferred by the inference scheme to a randomly chosen false edge , summarizes , across a range of thresholds , the ability to select edges in the data - generating network .",
    "aur may be computed relative to the true latent network @xmath2 , or relative to the true individual networks @xmath4 , quantifying performance on tasks ( i ) and ( ii ) respectively .",
    "both sets of results are presented below , in the latter case averaging aur over all individual networks . for ( iii ) , in order to assess ability to estimate individual heterogeneity , we computed aur scores based on the statistics @xmath90 which should be close to one if @xmath91 , otherwise @xmath92 should be close to zero .",
    "it is easy to show that inference for the latent network , under only the prior , attains mean aur equal to @xmath59 .",
    "similarly , prior inference for the individual networks attains mean aur equal to @xmath93 .",
    "this provides a baseline for the proposed methodology at tasks ( i ) and ( ii ) and allows performance to be decomposed into aur due to prior knowledge and aur contributed through inference . using a systematic variation of data - generating parameters , we defined 15 distinct data generating regimes .",
    "for all 15 regimes we considered 50 independent datasets ; standard errors accompany average aur scores .",
    "results presented below use a computationally favorable in - degree restriction @xmath94 .",
    "note that when the maximum in - degree of any of the true networks exceeds the computational restriction @xmath85 , estimator consistency will not be guaranteed . in order to check robustness to @xmath85",
    ", a subset of experiments were repeated using @xmath95 , with close agreement observed ( sfig .",
    "a latent network @xmath2 on @xmath84 vertices was drawn from the erds distribution with edge density @xmath96 . in order to simulate heterogeneity , the individual networks @xmath4 were obtained from @xmath2 by maintaining the status ( present / absent ) of each edge independently with probability @xmath61 .",
    "a parameter @xmath97 for each parent @xmath98 was independently drawn from the mixture normal distribution @xmath99 ( the mixture distribution ensures that parameters are not vanishingly small , so that the structural inference problem is well - defined ) .",
    "collecting together parameters produces matrices @xmath100 , corresponding to networks @xmath4 via @xmath98 if and only if @xmath101 .",
    "we also generate , for each individual @xmath24 , intercept parameters @xmath102 representing baseline expression levels .",
    "initial conditions were sampled as @xmath103 .",
    "data were then generated from the autoregressive model @xmath104 , where @xmath105 are independent for @xmath106 . in this way @xmath107",
    "such time courses were obtained ; that is , from @xmath107 distinct initial conditions , so the total number of data for individual @xmath24 is @xmath108 . in order to avoid issues of blow - up and to generate plausible datasets , the matrices @xmath100 were normalized by their spectral radii prior to data generation .    in order to investigate the effect of using a prior network @xmath10",
    ", we do not simply want to set @xmath10 equal to the latent network @xmath2 , since in practice this network is unknown .",
    "we therefore generated a prior network @xmath10 by correctly specifying each potential edge as either present or absent with probability @xmath59 . in this way",
    "we mimic partial prior knowledge of the networks under study .",
    "we augmented the above data - generating scheme to mimic interventional experiments . in this case , for each time course , a randomly chosen variable is marked as the target of an interventional treatment .",
    "data are then generated according to the augmented likelihood described in appendix c ( fixed effects were taken to be zero ) .",
    "furthermore , in order to investigate the impact of model misspecification , we also considered time series data generated from a computational model of protein signaling , based on nonlinear odes @xcite . in order to extend this model , which is for a single cell type , to simulate a heterogeneous population , we randomly selected three protein species per individual and deleted their outgoing edges in the data - generating network ( see supplement a ) .",
    "firstly we investigated ability to recover the latent network @xmath2 .",
    "initially all estimators are assigned approximately optimal hyperparameter values @xmath109 ( for @xcite , @xmath110 ) based on the heuristic of eqn .",
    "[ prob interpret ] ; prior misspecification is investigated later in section [ misspecify ] .",
    "we found little difference in the ability of jni and ani to recover the latent network structure across a wide range of regimes ( stable 1 ) .",
    "since ani enjoys favorable computational complexity , this estimator may be preferred for this task in practice .",
    "however , both approaches clearly outperformed monolithic inference , which was no better than inference based on the prior alone , demonstrating the importance of accounting for variation in parameter values .",
    "correlations barely outperformed random sampling .",
    "in practice , one could also estimate @xmath2 using independent network inference ( ini ) , via the _ ad hoc _ estimator @xmath111 which performs an unweighted average of @xmath112 independent network inferences .",
    "however we found that ini offered no advantage over jni and ani , performing worse than both in 14 out of 15 regimes .",
    "we obtained qualitatively similar results for both alternative data - generating schemes ( stables 3,6 ) .",
    "secondly we investigated the ability to recover individual networks @xmath4 . at this task ,",
    "jni outperformed ini in all 15 regimes ( table [ table ar cl 1 ] ) .",
    "this demonstrates a substantial increase in statistical power resulting from the hierarchical bayesian approach .",
    "jni also outperformed monolithic estimation and inference using temporal correlations in all 15 regimes , with the latter demonstrating substantial bias .",
    "one may try to improve upon ini by firstly estimating the wild type network @xmath2 , and then taking this estimate as a prior network @xmath10 within a second round of ini .",
    "informed by section [ latent ] , we consider the approach whereby @xmath2 is first estimated using ani , referring to this two - step procedure as `` empirical network inference '' ( eni ) .",
    "we found that the performance of eni consistently matched that of jni over a wide range of regimes .",
    "since eni avoids all joint computation , this may provide a practical estimator of individual networks in higher dimensional settings .",
    "similar results were observed using the alternative data - generating schemes , although jni slightly outperformed eni on the @xcite datasets ( stables 4,7 ) .",
    "thirdly , we assessed ability to pinpoint sources of variation within the population .",
    "interest is often directed toward individual - specific heterogeneity , or _",
    "features_. informally ,",
    "writing @xmath113 , features correspond to @xmath114 .",
    "jni regularizes between individuals ; it therefore ought to eliminate spurious differences , leaving only features which are strongly supported by data .",
    "equivalently , since jni offers improved estimation of the latent network @xmath2 , the features @xmath115 ought also to be better estimated .    feature detection may also be performed using ini or eni , comparing an latent network estimator ( see _ ad hoc _ estimator in section [ latent ] ) with individual networks .",
    "the performance of jni was compared to the performance of ini and eni ( stable 2 ) .",
    "we found that , whilst feature detection is much more challenging that previous tasks , jni mostly outperformed both ini and eni , with exceptions occurring whenever the underlying dataset was highly informative ( in which case ini was often superior ) .",
    "this suggests that coherence of the jni analysis aids in suppressing spurious features in the small sample setting .",
    "alternative data - generating schemes produced qualitatively similar results , although jni outperformed eni on the @xcite datasets ( stables 5,8 ) .",
    "for the above investigation we used eqn .",
    "[ prob interpret ] to elicit hyperparameters @xmath52 .",
    "this was possible because the data - generating parameters @xmath116 were known by design ; however in general this will not be the case .",
    "it is therefore important that estimator performance does not deteriorate heavily when alternative hyperparameter values are employed . by fixing @xmath117 in the data generating process , we are able to investigate the robustness of jni estimator to hyperparameter misspecification .",
    "in particular , when finite values are ascribed to data - generating parameters @xmath118 , ani and ini may be interpreted as inference using misspecified prior distributions ( see section [ limits ] ) .    sfig .",
    "3 displays how performance of the jni estimator for latent networks depends on the choice of hyperparameters @xmath119 .",
    "we notice that aur remains close to that obtained for optimal @xmath52 over a fairly large interval , so that performance is not exquisitely dependent on prior elicitation .",
    ".assessment of estimators for inference of individual networks @xmath4 ; autoregressive dataset with interventions .",
    "[ values shown are average aur @xmath120 standard error , over 50 realizations .",
    "green / red is used to indicate the highest / lowest scoring estimators .",
    "@xmath121 number of individuals , @xmath122 number of time points per time course , @xmath123 number of time courses , @xmath124 number of variables , @xmath125 noise magnitude , @xmath126 data generating hyperparameters .",
    "`` jni '' @xmath127 joint network inference , `` ani '' @xmath127 aggregate data but control for parameter confounding , `` ini '' @xmath127 average @xmath112 independent network inferences , `` monolithic '' @xmath127 aggregate data without controlling for parameter confounding , `` correl . ''",
    "@xmath127 estimation using the absolute pearson temporal correlation coefficient , `` prior '' @xmath127 estimation using only the prior network @xmath10 . ]",
    "[ cols=\"^,^,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]      the biological datasets which motivate this study often contain outliers . at the same time",
    ", experimental design may lead to platform - specific batch effects . in order to probe estimator robustness , we generated data as previously described , with the addition of outliers and certain batch effects . specifically , gaussian noise from the contamination model @xmath128 was added to all data prior to inference .",
    "at the same time , one individual s data were replaced entirely by gaussian white noise to simulate a batch effect that could arise if preparation of a specific biological sample was incorrect . the relative decrease in performance at feature detection",
    "is reported in sfig .",
    "we found that jni remained the optimal estimator for all three estimation problems , in spite of these heavy violations to the modeling assumptions .",
    "however , the actual decrease in performance was more pronounced for jni than for ini , suggesting that decoupled estimation ( ini ) may confer robustness to batch effects which affect single individuals .",
    "there are three distinct , though related , structure learning problems which may be addressed in the context of an heterogeneous population of individuals :    1 .   recovering a shared or `` wild type '' network from the heterogeneous data .",
    "2 .   recovering networks for specific individuals . 3 .   pinpointing network variation within the population .",
    "each problem may be of independent scientific interest , and the joint approaches investigated here address all three problems simultaneously within a coherent framework .",
    "we considered simulated data , with and without model misspecification .",
    "for all three problems we demonstrated that a joint analysis performs at least as well as independent or aggregate analyses .    our analysis , based on exact bayesian model averaging , was massively faster then the sampling - based schemes of @xcite .",
    "moreover , our estimators are deterministic , so that difficulties pertaining to mcmc convergence were avoided .",
    "indeed , attaining convergence on joint models of this kind appears to be challenging @xcite .",
    "the proposed methodology is scalable , with an embarrassingly parallel algorithm provided in section [ computers ] .",
    "furthermore , we described approximations to a joint analysis which enjoy further reduced computational complexity whilst providing almost equal estimator performance across a wide range of data - generating regimes .    whilst we considered the simplest form of regularization , based on prior modularity",
    ", there is potential to integrate richer knowledge into inference .",
    "one possibility would be hierarchical regularization that allows entire pathways to be either active or inactive .",
    "however , in this setting it would be important to revisit hyperparameter elicitation ; the procedures which we have described are specific to shd priors .",
    "in particular we restricted the joint model to have equal inverse temperatures @xmath129 . relaxing",
    "this assumption may improve robustness to batch effects which target single individuals , since then weak informativeness ( @xmath130 ) may be learned from data .",
    "it would also be interesting to distinguish between @xmath131 ( `` loss of function '' ) and @xmath132 ( `` gain of function '' ) features . in this work",
    "we did not explore information sharing through parameter values @xmath22 , yet this may yield more powerful estimators of network structure in settings where individuals parameters @xmath133 are not independent .",
    "the jni model could be formulated as a penalized ( log-)likelihood @xmath134 the frequentist approaches described by @xcite enjoy favorable computational complexity ( esp .",
    "@xcite who provide an example with @xmath135 variables and @xmath136 individuals ) .",
    "however , in small to moderate dimensional settings , the bayesian methods proposed here are complementary in several respects : ( i ) bayesian approaches provide a confidence measure for inferred topology , dealing with non - identifiable and multi - modal problems ; ( ii ) no convexity assumptions are required on the form of the penalty functions @xmath33 , @xmath137 in the bayesian setting , which may assist with integration of ancillary information ; ( iii )",
    "the above penalized likelihood methods do not apply directly to time course data ( but could be extended to do so ) .",
    "these experiments employed a promising formulation of likelihood under intervention due to @xcite .",
    "there are a number of interesting extensions which may be considered in future work : ( i ) in high dimensions , bayesian variable selection requires multiplicity correction in order to avoid degeneracy @xcite .",
    "such correction is required to control the false discovery rate and is independent to the penalty on model complexity provided by the marginal likelihood . in this moderate - dimensional work , in order to simplify the presentation , we did not employ a multiplicity correction ; this should be an avenue for future development .",
    "( ii ) inference was based upon a local score borrowed from bayesian linear regression .",
    "we chose to employ the @xmath138-prior due to @xcite , where following @xcite we used ( conditional ) empirical bayes to select the @xmath138 hyperparameter .",
    "others have suggested setting @xmath139 ( unit information prior ; * ? ? ?",
    "* ) , whilst @xcite and @xcite propose prior distributions over @xmath138 with attractive theoretical properties .",
    "our empirical investigation suggested that the choice of hyperparameter elicitation is influential , but a thorough comparison of linear model specifications is beyond the scope of this paper .",
    "( iii ) as discussed in @xcite , linear autoregressive formulations may be inadequate in realistic settings ; in particular , samples which are obtained unevenly in time can be problematic .",
    "recent advances which incorporate mechanistic detail into the likelihood may prove advantageous @xcite .",
    "since the jni approach decouples the marginal likelihood and model averaging computations , it may be applied directly to the output of more sophisticated models .",
    "( iv ) in the case of linear models , @xcite showed that the median probability model ( i.e. model averaging ) provides superior predictive performance over the maximum _ a posteori _ ( map ) model .",
    "however we are unaware of an analogous result for causal inference in the bayesian setting .",
    "techniques for modeling heterogeneous data are clearly widely applicable .",
    "the methodology presented here may be applicable in other disciplines .",
    "for example , our approach is suited to meta - analyses of network analyses @xcite , integration of multiple data sources @xcite or data arising from context dependent networks @xcite .",
    "the ideas discussed here share many connections with time - heterogeneous dbns which , for brevity , we did not discuss in this paper @xcite .",
    "we would like to thank j.d .",
    "aston , f. dondelinger , c.a .",
    "penfold , s.e.f .",
    "spencer and s.m .",
    "hill for helpful discussion and comments .",
    "financial support was provided by nci u54 ca112970 , uk epsrc ep / e501311/1 and the cancer systems center grant from the netherlands organisation for scientific research .",
    "dbns have emerged as popular tools for the analysis of multivariate time course data due to ( i ) the fact that no acyclicity assumption is required on the ( static ) network , and ( ii ) computational tractability resulting from a factorization of the likelihood function over variables @xmath17 @xcite . for the dbns",
    "used here , an edge @xmath140 from @xmath17 to @xmath141 in @xmath142 means that @xmath143 , the observed value of variable @xmath144 in individual @xmath24 at time @xmath70 , depends directly upon @xmath145 , the observed value of @xmath19 in individual @xmath24 at time @xmath146 ( fig .",
    "[ time slice ] ; note that @xmath70 indexes sample index , rather than actual sampling time ) .",
    "let @xmath11 denote a vector containing all observations for individual @xmath24 .",
    "then @xmath147 is conditionally independent of @xmath148 given @xmath149 and @xmath4 ( first - order markov assumption ) .",
    "these conditional independence relations are conveniently summarized as a ( static ) network @xmath4 with exactly @xmath84 vertices ( fig .",
    "[ static ] ) ; note that this latter network need not be acyclic .",
    "we follow @xcite in formulating inference in dbns as a regression problem .",
    "we entertain models for the response @xmath69 as predicted by covariates @xmath149 . in many cases",
    "multiple time series will be available . in this case",
    "the vector @xmath150 contains the concatenated time series .",
    "the dbn formulation gives rise to the following linear regression likelihood @xmath151 where @xmath152 .",
    "the matrix @xmath153_{n \\times 2}$ ] contains a term for the initial time point in each experiment .",
    "the elements of @xmath154 corresponding to initial observations @xmath155 are simply set to zero .",
    "parameters @xmath156 are specific to model @xmath74 , variable @xmath19 and cell line @xmath24 . in the simplest case",
    "the model - specific component @xmath154 of the design matrix consists of the raw predictors @xmath157 where @xmath158 denotes the elements of the vector @xmath149 belonging to the set @xmath36 , though more complex basis functions may be used .",
    "following @xcite we model interventional data by modification to the dag in line with a causal calculus @xcite .",
    "we mention briefly some of the key ideas and refer the interested reader to the references for full details .",
    "a `` perfect intervention '' corresponds to 100% removal of the target s activity with 100% specificity . in the context of protein phosphorylation",
    ", kinases may be intervened upon using agents such as monoclonal antibodies , small molecule inhibitors or even si - rna @xcite .",
    "we make the simplifying assumptions that these interventions are perfect and use the `` perfect out fixed effects '' ( pofe ) approach recommended by @xcite .",
    "we refer the reader to @xcite for an extended discussion of pofe .",
    "this changes the dag structure to model the intervention and also estimates a fixed effect parameter to model the change under intervention in the log - transformed data .",
    "we employed a jeffreys prior @xmath159 for @xmath160 over the common parameters .",
    "prior to inference , the non - interventional components of the design matrix were orthogonalized using the transformation @xmath161 , where @xmath162 @xcite .",
    "we then assumed a @xmath138-prior for regression coefficients @xcite , given by @xmath163 where @xmath164 . using these priors for the dbns with intervention",
    "as outlined above , the marginal likelihood can be obtained in closed - form : @xmath165 where @xmath166 , @xmath167 and @xmath164 .",
    "empirical investigations have previously demonstrated good results for network inference based on the above marginal likelihood @xcite . following @xcite we used the ( conditional ) empirical bayes approach to determine the @xmath168 hyperparameter ( details in supplement a ) .",
    "the `` sum - product '' lemma , which forms the basis for several exact inference procedures in graphical models , can be expressed in its most basic form as follows : for a finite set of functionals @xmath169 on finite domains @xmath170 indexed by @xmath171 we have @xmath172 the proof is straight forward ( induction on @xmath173 ) and can be found in e.g. @xcite .",
    "the sum - product lemma is typically used to reduce algorithmic complexity , replacing the @xmath174 expression on the left hand side by the @xmath175 expression on the right hand side .",
    "this appendix contains pseudocode for exact joint model averaging . [ computational complexity of calculating marginal likelihoods @xmath72 will scale with sample size @xmath176 ; scaling exponents shown here assume @xmath177 . ]",
    "below we provide pseudocode for computation of posterior marginal inclusion probabilities for edges in individual networks @xmath4 :                        aliferis , c.f .",
    "_ et al . _",
    "( 2010 ) local causal and markov blanket induction for causal discovery and feature selection for classification , part i : algorithms and empirical evaluation .",
    "_ j. mach .",
    "res . _ * 11*:171 - 234 .",
    "dondelinger , f. , lebre , s. , husmeier , d. ( 2010 ) heterogeneous continuous dynamic bayesian networks with flexible structure and inter - time segment information sharing .",
    "_ proceedings of the 27th international conference on machine learning _ , 303 - 310 .",
    "dondelinger , f. , lebre , s. , husmeier , d. ( 2012 ) non - homogeneous dynamic bayesian networks with bayesian regularization for inferring gene regulatory networks with gradually time - varying structure .",
    "_ * 90*(2):191 - 230 .",
    "grzegorczyk , m. , husmeier , d. ( 2011 ) improvements in the reconstruction of time - varying gene regulatory networks : dynamic programming and regularization by information sharing among genes .",
    "_ bioinformatics _ * 27*(5):693 - 699",
    ".      hennessey , b.t .",
    "_ et al . _",
    "( 2010 ) a technical assessment of the utility of reverse phase protein arrays for the study of the functional proteome in nonmicrodissected human breast cancer .",
    "proteom . _ * 6*:129 - 151 .",
    "werhli , a.v . , husmeier , d. ( 2008 ) gene regulatory network reconstruction by bayesian integration of prior knowledge and/or different experimental conditions . _",
    "journal of bioinformatics and computational biology _",
    "* 6*(3):543 - 572 .",
    "zellner , a. ( 1986 ) on assessing prior distributions and bayesian regression analysis with g - prior distributions , _ bayesian inference and decision techniques - essays in honor of bruno de finetti , eds . p. k. goel and a. zellner _",
    ", 233 - 243 ."
  ],
  "abstract_text": [
    "<S> graphical models are widely used to make inferences concerning interplay in multivariate systems , as modeled by a conditional independence graph or network . in many applications , </S>",
    "<S> data are collected from multiple individuals whose networks may differ but are likely to share many features . here </S>",
    "<S> we present a hierarchical bayesian formulation for joint estimation of such networks . </S>",
    "<S> the formulation is general and can be applied to a number of specific graphical models . </S>",
    "<S> motivated by applications in biology , we focus on time - course data with interventions and introduce a computationally efficient , deterministic algorithm for exact inference in this setting . </S>",
    "<S> application of the proposed method to simulated data demonstrates that joint estimation can improve ability to infer individual networks as well as differences between them . </S>",
    "<S> finally , we describe approximations which are still more computationally efficient than the exact algorithm and demonstrate good empirical performance . </S>"
  ]
}