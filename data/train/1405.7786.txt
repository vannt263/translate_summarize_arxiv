{
  "article_text": [
    "multi - dimensional or multi - way data is prevalent nowadays , which can be represented by tensors .",
    "an @xmath0th - order tensor is a multi - way array of size @xmath1 , where the @xmath2th dimension or mode is of size @xmath3 .",
    "for example , a tensor can be induced by the discretization of a multivariate function @xcite . given a multivariate function @xmath4 defined on a domain @xmath5^n$ ]",
    ", we can get a tensor with entries containing the function values at grid points .",
    "for another example , we can obtain tensors based on observed data @xcite .",
    "we can collect and integrate measurements from different modalities by neuroimaging technologies such as functional magnetic resonance imaging ( fmri ) and electroencephalography ( eeg ) : subjects , time , frequency , electrodes , task conditions , trials , and so on .",
    "furthermore , high - order tensors can be created by a process called tensorization or quantization @xcite , by which a large - scale vectors and matrices are reshaped into higher - order tensors .    however , it is impossible to store a high - order tensor because the number of entries , @xmath6 when @xmath7 , grows exponentially as the order @xmath0 increases .",
    "this is called the `` curse - of - dimensionality '' . even for @xmath8 , with @xmath9",
    "we obtain @xmath10 entries .",
    "such a huge storage and computational costs required for high dimensional problems prohibit the use of standard numerical algorithms . to make high dimensional problems tractable , there were developed approximation methods including sparse grids @xcite and low - rank tensor approximations @xcite . in this paper , we focus on the latter approach , where computational operations are performed on tensor formats , i.e. , low - parametric representations of tensors .    in this paper , we consider several tensor formats , especially the tensor train ( tt ) format , which is one of the simplest tensor networks developed with the aim of overcoming the curse - of - dimensionality .",
    "extensive overviews of the modern low - rank tensor approximation techniques are presented in @xcite .",
    "the tt format is equivalent to the matrix product states ( mps ) for open boundary conditions proposed in computational physics , and it has taken a key role in density matrix renormalization group ( dmrg ) methods for simulating quantum many - body systems @xcite .",
    "it was later re - discovered in numerical analysis community @xcite .",
    "the tt - based numerical algorithms can accomplish algorithmic stability and adaptive determination of ranks by employing the singular value decomposition ( svd ) @xcite .",
    "its scope of application is quickly expanding for addressing high - dimensional problems such as multi - dimensional integrals , stochastic and parametric pdes , computational finance , and machine learning @xcite . on the other hand , a comprehensive survey on traditional low - rank tensor approximation techniques for cp and tucker formats is presented in @xcite .    despite the large interest in high - order tensors in tt format , mathematical representations of the tt tensors",
    "are usually limited to the representations based on scalar operations on matrices and vectors , which leads to complex and tedious index notation in the tensor calculus .",
    "for example , a tt tensor is defined by each entry represented as products of matrices @xcite . on the other hand , representations of traditional low - rank tensor formats",
    "have been developed based on multilinear operations such as the kronecker product , khatri - rao product , hadamard product , and mode-@xmath2 multilinear product @xcite , which enables coordinate - free notation . through the utilization of the multilinear operations ,",
    "the traditional tensor formats expanded the area of application to chemometrics , signal processing , numerical linear algebra , computer vision , data mining , graph analysis , and neuroscience @xcite .    in this work ,",
    "we develop extended definitions of multilinear operations on tensors .",
    "based on the tensor operations , we provide a number of new and useful representations of the tt format .",
    "we also provide graphical representations of the tt format , motivated by @xcite , which are helpful in understanding the underlying principles and tt - based numerical algorithms .",
    "based on the tt representations of large - scale vectors and matrices , we show that the basic numerical operations such as the addition , contraction , matrix - vector product , and quadratic form are conveniently described by the suggested representations .",
    "we demonstrate the usefulness of the proposed tensor operations in tensor calculus by giving a proof of orthonormality of the so - called frame matrices .",
    "moreover , we derive explicit representations of localized linear maps in tt format that have been implicitly presented in matrix forms in the literature in the context of alternating linear scheme ( als ) for solving various optimization problems .",
    "the suggested mathematical operations and tt representations can be further applied to describing tt - based numerical methods such as the solutions to large - scale systems of linear equations and eigenvalue problems @xcite .",
    "this paper is organized as follows . in section 2",
    ", we introduce notations for tensors and definitions for tensor operations . in section 3",
    ", we provide the mathematical and graphical representations of the tt format .",
    "we also review mathematical properties the tt format as a low - rank approximation . in section 4 , we describe basic numerical operations on tensors in tt format such as the addition , hadamard product , matrix - vector multiplication , and quadratic form in terms of the multilinear operations and tt representations .",
    "discussion and conclusions are given in section 5 .",
    "the notations in this paper follow the convention provided by @xcite .",
    "table [ table : notation_ten ] summarizes the notations for tensors .",
    "scalars , vectors , and matrices are denoted by lowercase , lowercase bold , and uppercase bold letters @xmath11 , @xmath12 , and @xmath13 , respectively .",
    "tensors are denoted by underlined uppercase bold letters @xmath14 .",
    "the @xmath15th entry of @xmath14 of size @xmath16 is denoted by @xmath17 or @xmath18 .",
    "a subtensor of @xmath14 obtained by fixing the indices @xmath19 is denoted by @xmath20 or @xmath21 .",
    "we may omit ` : ' as @xmath22 if the rest of the indices are clear to readers .",
    "the mode-@xmath2 matricization of @xmath23 is denoted by @xmath24 .",
    "we denote the mode-@xmath25 matricization of @xmath14 by @xmath26)}\\in{\\mathbb{r}}^{i_1i_2\\cdots i_n\\times i_{n+1}\\cdots i_n}$ ] in the sense that @xmath27\\equiv \\{1,2,\\ldots , n\\}$ ] is the set of integers from 1 to @xmath2 .",
    "in addition , we define the multi - index notation by @xmath28 for @xmath29 @xmath30 . by using this notation ,",
    "we can write an entry of a kronecker product as @xmath31 . moreover , it is important to note that in this paper the vectorization and matricization are defined in accordance with the multi - index notation .",
    "that is , for @xmath23 , we have @xmath32 ) }      \\in{\\mathbb{r}}^{i_1\\cdots i_n \\times i_{n+1}\\cdots i_n }      \\quad      &       \\leftrightarrow      \\quad      { \\mathbf{x}}(\\overline{i_1\\cdots i_n},\\overline{i_{n+1}\\cdots i_n } )       = { \\underline{\\mathbf{x}}}(i_1,i_2,\\ldots , i_n ) ,       \\end{split}\\ ] ] for @xmath30 .",
    ".[table : notation_ten]notations for tensors [ cols=\"<,<\",options=\"header \" , ]      let @xmath33 and @xmath34 be tt tensors .",
    "the sum @xmath35 can be expressed in the tt format @xmath36 that is , each tt - core of the sum is written as the direct sum of the tt - cores .",
    "alternatively , each entry of @xmath37 can be represented as products of matrices @xmath38 the tt - ranks for @xmath39 are the sums , @xmath40 .    on the other hand , multiplication of @xmath14 with a scalar @xmath41",
    "can be obtained by simply multiplying one core , e.g. , @xmath42 , with @xmath43 as @xmath44 .",
    "this does not increase the tt - ranks .",
    "we note that that the set of tensors with tt - ranks bounded by @xmath45 is not convex , since a linear combination @xmath46 generally increases the tt - ranks , which may exceed @xmath45 .",
    "the hadamard ( elementwise ) product @xmath47 of @xmath48 and @xmath49 can be written in the tt format as @xmath50 that is , each tt - core is written as the mode-@xmath51 kronecker product . as an alternative representation , each entry is written as products of matrices @xmath52 the tt - ranks for the hadamard product are the multiplications , @xmath53 .",
    "the contraction of two tensors @xmath54 and @xmath55 is defined by @xmath56 the contraction of a tt tensor @xmath33 with a rank - one tensor @xmath57 can be simiplified as @xmath58 the contraction of two tt tensors @xmath59 and @xmath60 can be calculated by combining the hadamard product and the contraction with the rank one tensor @xmath61 as @xmath62 where @xmath63 for @xmath30 .",
    "we remark that @xmath64 and @xmath65 are row and column vectors .",
    "we define a generalized contraction operator of two tt - cores as follows .",
    "the core contraction of two tt - cores @xmath66 and @xmath67 is defined by @xmath68    we can express the contraction of two tt - tensors @xmath33 and @xmath34 by @xmath69    the computational cost for calculating the contraction @xmath70 is @xmath71 , which is linear in @xmath0 .      the matrix - vector product , or the linear mapping can also be efficiently represented by the tt format .",
    "the computational cost for computing a matrix - vector product in tt format is @xmath72 .    1 .",
    "suppose that a vector @xmath73 is in tt format and a matrix @xmath74 is in matrix tt format , and both are represented as kronecker products @xmath75 and @xmath76 then the matrix - vector product is represented in tt format @xmath77 where @xmath78 and @xmath79 + we can get the same expression in the case that @xmath80 and @xmath81 are represented as outer products .",
    "the linear mapping @xmath82 can also be represented by its entries written as products of matrices .",
    "suppose that @xmath80 and @xmath83 are in the tt formats @xmath84 and @xmath85 then the entries of the linear mapping @xmath86 is calculated by the contraction @xmath87 where the lateral slices @xmath88 of tt - cores @xmath89 are expressed as @xmath90 note that @xmath91 and @xmath92 are row and column vectors .",
    "3 .   we can further simplify the notation ( [ eqn : core_contraction_entry ] ) by considering the tt - core @xmath93 as an operator .",
    "let @xmath94 be a linear map defined by @xmath95 with each @xmath96th slice @xmath97 we can represent @xmath98 in a simplified form as follows .",
    "let @xmath99 be in matrix tt format and @xmath100 be in tt format .",
    "the linear mapping @xmath98 is represented by @xmath101    on the other hand , we recall that @xmath102 where @xmath103 is the frame matrix and @xmath104 .",
    "a large - scale matrix - vector multiplication reduces to a smaller matrix - vector multiplication as @xmath105 , where @xmath106 but we can not calculate @xmath107 by matrix - matrix multiplication for a large matrix @xmath108 .",
    "however , by using the representation ( [ eqn : linearmap_tensor_operation ] ) , we can show that @xmath107 can be calculated by recursive core contractions as follows .",
    "let @xmath109 be a linear map defined by @xmath110 for any @xmath111 .",
    "let @xmath107 be the matrix defined by .",
    "then @xmath112 for any @xmath111 .",
    "that is , the matrix - vector product @xmath113 , where @xmath114 , can be computed efficiently via the recursive core contractions in @xmath115 .",
    "figure [ fig : loc_lin_op ] illustrates the graph for the linear map @xmath116 . for each @xmath117 core tensor @xmath118",
    "is connected to the core tensor @xmath119 , which is represented as @xmath120 in ( [ eqn : linearmap_local_operation ] ) .",
    "graph for the matrix - vector product @xmath121 represented by the linear mapping @xmath122 , where @xmath104 , width=264 ]      the quadratic form @xmath123 for a symmetric and very large - scale matrix @xmath124 can be represented in tt format as follows .    1 .",
    "let @xmath108 and @xmath12 are represented as kronecker products @xmath75 and @xmath76 then the quadratic form is represented as the products @xmath125 where @xmath126 and @xmath127 for @xmath128 and @xmath129 for @xmath130 .",
    "the outer product representation leads to the same expression .",
    "recall that the @xmath2th core tensor of @xmath131 can be represented by the @xmath96th slices @xmath132 , @xmath133 . the quadratic form @xmath134 is computed by contraction of the hadamard product with the rank - one tensor as @xmath135 where @xmath136 we have @xmath137 and @xmath138 .",
    "3 .   instead , by using the core contraction defined in ( [ eqn : define_core_contraction ] ) , we can simplify the expression ( [ eqn : quadratic_mps_core ] ) by @xmath139 where @xmath140 is defined in the previous subsection .",
    "finally , we can express the quadratic form efficiently as @xmath141    on the other hand , from @xmath102 the quadratic form @xmath142 reduces to @xmath143 , where @xmath144 is a much smaller matrix than @xmath108 when tt - ranks @xmath145 and @xmath146 are moderate .",
    "since @xmath147 can not be calculated by matrix - matrix multiplication for a large matrix @xmath108 , we calculate it iteratively by recursive core contractions based on the distributed representation ( [ eqn : quadratic_tensor_represn ] ) as follows .",
    "let @xmath148 be a bilinear form defined by @xmath149 for any @xmath150 .",
    "let @xmath151 be the matrix defined by .",
    "then @xmath152 for any @xmath150 .",
    "that is , the bilinear form @xmath153 , where @xmath154 and @xmath114 , can be computed efficiently via the recursive core contractions in @xmath155 .",
    "figure [ fig : loc_quad_op ] illustrates the graph for the bilinear form @xmath156 .",
    "it is clear that each core tensor @xmath157 is connected to the core tensor @xmath119 , which is represented as @xmath158 in ( [ eqn : quadratic_local_represn ] ) .",
    "graph for the quadratic form @xmath159 represented by the bilinear form @xmath160 , where @xmath104 , width=283 ]",
    "in this paper , we proposed several new mathematical operations on tensors and developed novel representations of the tt formats .",
    "we generalized the standard matrix - based operations such as the kronecker product , hadamard product , and direct sum , and proposed tensor - based operations such as the self - contraction and core contraction .",
    "we have shown that the tensor - based operations are able to not only simplify traditional index notation for tt representations but also describe important basic operations which are very useful for computational algorithms .",
    "the self - contraction operator can be used for defining the tensor chain ( tc ) @xcite representations , and its properties should be more investigated in the future work . moreover",
    ", the definition of core contraction can also be generalized to any tensor network formats such as hierarchical tucker ( ht ) format @xcite .",
    "the partial contracted products of either the left or right core tensors are matricized and used as a building block of the frame matrices . we have shown that the suggested tensor operations can be used to prove the orthonormality of the frame matrices , which have been proved only by using index notation in the literature .",
    "the developed relationships may also play a key role in the alternating linear scheme ( als ) and modified alternating linear scheme ( mals ) algorithms @xcite for reducing the large - scale optimizations to iterative smaller scale problems .",
    "recent studies adjust the frame matrices in order to incorporate rank adaptivity and improve convergence for the als @xcite . in this work ,",
    "we have derived the explicit representations of the localized linear map @xmath116 and bilinear form @xmath156 by the proposed tensor operations , which are important for tt - based iterative algorithms for breaking the curse - of - dimensionality @xcite .",
    "the global convergence of the iterative methods remains as a future work .",
    "in addition , it is important to keep the tt - ranks moderate for a feasible computational cost .",
    "a. cichocki , r. zdunek , a .-",
    "phan , and s. amari ( 2009 ) .",
    "_ nonnegative matrix and tensor factorizations : applications to exploratory multi - way data analysis and blind source separation _ , chichester : wiley .",
    "a. cichocki ( 2013 ) .",
    "era of big data processing : a new approach via tensor networks and tensor decompositions . invited talk at _ international workshop on sisa-2013 _ , nagoya , 1 oct .",
    "arxiv:1403.2048 .",
    "s. v. dolgov , b. n. khoromskij , i. v. oseledets , and d. v. savostyanov ( 2014 ) .",
    "computation of extreme eigenvalues in higher dimensions using block tensor train format .",
    "_ computer physics communications _ ,",
    "185(4 ) , 12071216 .",
    "s. v. dolgov and i. v. oseledets ( 2012 ) solution of linear systems and matrix inversion in the tt - format .",
    "_ siam j. sci .",
    "_ , 34 , a27182739 . s. v. dolgov and d. v. savostyanov ( 2013 ) . alternating minimal energy methods for linear systems in higher dimensions",
    "part i : spd systems .",
    "arxiv:1301.6068 .",
    "s. v. dolgov and d. v. savostyanov ( 2013 ) . alternating minimal energy methods for linear systems in higher dimensions .",
    "part ii : faster algorithm and application to nonsymmetric systems .",
    "arxiv:1304.1222 .",
    "a. falc and w. hackbusch ( 2012 ) . on minimal subspaces in tensor representations .",
    "_ , 12 , 765803 . l. grasedyck ( 2010 ) .",
    "hierarchical singular value decomposition of tensors .",
    "_ siam j. matrix anal .",
    "_ , 31(4 ) , 20292054 .",
    "s. holtz , t. rohwedder , and r. schneider ( 2012 ) .",
    "the alternating linear scheme for tensor optimization in the tensor train format .",
    "_ siam j. sci .",
    "_ , 34(2 ) , a683a713 .",
    "v. a. kazeev and b. n. khoromskij ( 2012 ) .",
    "low - rank explicit qtt representation of the laplace operator and its inverse .",
    "_ siam j. matrix analysis applications _ , 33(3 ) , 742758 .",
    "v. a. kazeev , b. n. khoromskij , and e. e. tyrtyshnikov ( 2013 ) .",
    "multilevel toepliz matrices generated by tensor - structured vectors and convolution with logarithmic complexity .",
    "_ siam j. sci .",
    "_ , 35(3 ) , a1511a1536",
    ". t. g. kolda ( 2006 ) .",
    "multilinear operators for higher - order decompositions .",
    "technical report sand2006 - 2081 , sandia national laboratories .",
    "b. n. khoromskij and i. v. oseledets ( 2010 ) .",
    "dmrg+qtt approach to computation of the ground state for the molecular schrdinger operator .",
    "preprint 69 , mpi mis , leipzig .",
    "d. kressner , m. steinlechner , a. uschmajew ( 2013 ) .",
    "low - rank tensor methods with subspace correction for symmetric eigenvalue problems .",
    "mathicse technical report 40.2013 , epfl , lausanne .",
    "w. d. launey and j. seberry ( 1994 ) . the strong kronecker product . _",
    "journal of combinatorial theory , series a _ , 66(2 ) , 192213 .",
    "doi:10.1016/0097 - 3165(94)90062 - 0 .",
    "i. v. oseledets and e. e. tyrtyshnikov ( 2009 ) .",
    "breaking the curse of dimensionality , or how to use svd in many dimensions .",
    "_ siam j. sci .",
    "_ , 31(5 ) , 37443759 ."
  ],
  "abstract_text": [
    "<S> we review and introduce new representations of tensor train decompositions for large - scale vectors , matrices , or low - order tensors . </S>",
    "<S> we provide extended definitions of mathematical multilinear operations such as kronecker , hadamard , and contracted products , with their properties for tensor calculus . </S>",
    "<S> then we introduce an effective low - rank tensor approximation technique called the tensor train ( tt ) format with a number of mathematical and graphical representations . </S>",
    "<S> we also provide a brief review of mathematical properties of the tt format as a low - rank approximation technique . with the aim of breaking the curse - of - dimensionality in large - scale numerical analysis </S>",
    "<S> , we describe basic operations on large - scale vectors and matrices in tt format . </S>",
    "<S> the suggested representations can be used for describing numerical methods based on the tt format for solving large - scale optimization problems such as the system of linear equations and eigenvalue problems . </S>"
  ]
}