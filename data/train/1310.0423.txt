{
  "article_text": [
    "driven by applications in the natural , physical and information sciences , statistical models , methods and theory for analyzing network data have received increasing attention in the past decade @xcite . arguably , one of the most common paradigms in network analysis consists of the following steps : ( i ) collecting measurements on a complex system of interest , ( ii ) constructing a network representation using these measurements , and ( iii ) characterizing the structure of the network through various network summary statistics .",
    "importantly , errors in the original measurements induce structured errors in the network , and hence uncertainty associated with summaries of the network . in this paper , we explore the effects of error propagation from raw measurements to network representation , to summary statistics .",
    "we develop a nonparametric strategy to control inferential errors for a number of summary statistics , we characterize the way in which error levels and the underlying network structure impact the performance of the proposed strategy , and we illustrate how our theory can inform applications to gene regulatory networks and protein - protein interaction networks .    in a number of applications ,",
    "the step of network construction is frequently approached as an inferential task , in which with the goal is making inference on the edges ( and possibly the edge weights ) in a true , underlying network .",
    "importantly , in the process of inferring such a network , errors of both type i , i.e. , declaring an edge where none exists , and type ii , i.e. , omitting an edge when it exists , can be expected to happen .",
    "applications where network data are expected to contain errors due to errors in the raw measurements include the analysis of gene regulatory networks using microarray data ( e.g. , @xcite ) , protein - protein interaction networks using mass spectrometry or tandem affinity purification data ( e.g. , @xcite ) , and various related `` -omics '' networks constructed from multiple simultaneous measurements in systems biology ( e.g. , @xcite ) , as well as functional connectivity networks using fmri data ( e.g. , @xcite ) .    after constructing the network , scientists are typically most interested in characterizing it , quantitatively .",
    "numerical summaries of all sorts are used to this end , which range from vertex degrees to more general notions of vertex ` importance ' , often called centrality measures , to more global measures of network coherence , such as the density of edges or the extent to which ` flows ' can move throughout the network , often summarized through the so - called conductance .",
    "see , for example , ( * ? ? ?",
    "* ch 4 ) , for an overview .",
    "however , such network summaries are almost always presented without any quantification , nor acknowledgement , of the uncertainty they inherit from the inferential procedure in the network construction step .",
    "the most likely reason for this omission would appear to be the lack of tools and techniques for understanding the propagation of error , from measurements to network summary statistics . to the best of our knowledge , the inferential issues we have raised have not even received a formal treatment to date .    the primary contribution of this paper is to study a general formulation of this problem and , within this context , to characterize the performance of certain estimators of network summaries in terms of bounds on their statistical risk .",
    "here we are concerned with undirected , @xmath1-valued , networks represented by means of the corresponding adjacency matrix , @xmath2 .",
    "we assume that we are given a single noisy instance @xmath3 of some true network @xmath4 . for",
    "a given a statistic @xmath5 on the network , our goal is to construct an estimator @xmath6 of @xmath7 such that @xmath8 \\ll \\mathbb{e}\\left[\\ , ( g\\left(w_{obs}\\right)-g\\left(w_{true}\\right))^2\\ , \\right].\\ ] ]    the approach we develop to find @xmath6 is rooted in the literature on a related problem in signal processing  that of nonparametric function estimation , or ` denoising ' .",
    "there , given a single instance of @xmath9 the goal is to construct an estimator @xmath10 so that @xmath11 where @xmath12 refers to the @xmath0 norm measuring the mean - square error .",
    "a classical approach to this problem is to assume that the true signal @xmath13 is compressible in some set of basis functions , e.g. , a fourier or wavelet orthonormal basis ( e.g. , see * ? ? ?",
    "the estimate @xmath10 is then constructed by projecting @xmath14 onto this basis and thresholding the corresponding coefficients , discarding higher ` frequency ' components , and typically obtaining a much better reconstruction of @xmath13 than @xmath14 , under suitable assumptions on @xmath13 .    we adopt a similar strategy here , based on spectral analysis of the network , through its adjacency matrix @xmath15 @xcite . specifically ,",
    "in our approach , we construct an estimator @xmath16 of @xmath4 by projecting the noisy observation @xmath15 onto a carefully chosen basis containing most of the energy of the original signal , @xmath4 .",
    "the compression of information of @xmath4 in this basis allows one to keep some of the projections while discarding the remaining ones , resulting in an improved reconstruction of the original signal with minimal error .",
    "we find that for sufficiently smooth summaries @xmath17 , the accuracy of estimation of @xmath4 by @xmath16 translates to accuracy in estimation of @xmath7 by @xmath18 .",
    "the organization of this paper is as follows . in section",
    "[ 2 ] , we provide background and notation , discussing assumptions on the noise @xmath19 and sketching our overall approach . in section",
    "[ three ] , we present our main results : ( i ) characterization of @xmath15 and @xmath20 as naive estimators , ( ii ) characterization of our proposed @xmath16 and @xmath18 , in comparison , and ( iii ) a partial characterization of common choices of network summary statistics to which our method does and does not pertain . finally , in section [ 4 ] , we evaluate our methods on simulated data and we demonstrate how our theory can inform data analysis in applications to protein interaction networks and gene regulatory networks .",
    "let @xmath21 be a ( fixed ) graph representing an underlying network of interest , defined on @xmath22 vertices with @xmath23 edges .",
    "we assume that @xmath24 is undirected , and has no multi - edges or self - loops .",
    "this last condition is not necessary , but makes certain computations such as the moments of the random spectral radius , easier .",
    "see section [ three ] for details .",
    "denote by @xmath4 the @xmath25 adjacency matrix for @xmath24 , where @xmath26 if @xmath27 , and zero otherwise .",
    "similarly , let @xmath28 represent a version of @xmath24 observed with noise , and @xmath15 , the corresponding @xmath25 adjacency matrix .",
    "we specify the precise nature of this noise through the form of the adjacency matrix , writing @xmath29 where @xmath19 is an additive noise such that    1 .",
    "@xmath30 , if @xmath31 ; 2 .",
    "@xmath32 , if @xmath33 .    here",
    "@xmath34 can be interpreted as the probability of a type i error ( false positive ) , and @xmath35 , as the probability of a type ii error ( false negative ) , which are the same fixed values across all edges .",
    "these assumptions of homogeneous type i and ii error probabilities ( i.e. across edges ) allow for the construction of an unbiased version of @xmath15 , denoted @xmath36 ( see theorem [ raw data ] ) , which in turn will prove useful in producing interpretable bounds on the accuracy of estimation of @xmath4 by @xmath16 .",
    "these assumptions are not necessary , but rather are a simplifying feature of our analysis .",
    "we note that we have made no assumptions on the dependency of the noise .",
    "indeed , our work can accommodate dependency under suitable assumptions on its effect on the observed maximum degree squared and on the correlation coefficients of edge observations ( see section [ 3.3 ] for details ) . to date , however , the precise nature of this and similar dependencies arising in practice is largely unknown , and we therefore leave the exploration of that topic for future work .",
    "we will represent a ( real - valued ) summary statistic of a network as a function @xmath17 applied to an adjacency matrix @xmath2 .",
    "since our approach is one of spectral - based estimation of @xmath4 , we are interested in continuous , particularly ( locally ) lipschitz continuous , statistics since it allows us to control the accuracy in estimating @xmath37 by the accuracy in estimating @xmath4 . more formally , we will restrict our attention to the class of lipschitz statistics @xmath5 , for which @xmath38 where @xmath39 denotes the @xmath40 norm , i.e. , @xmath41 and the constant @xmath42 can depend on @xmath43 . hence ,",
    "for sufficiently close @xmath44 and @xmath45 , the values @xmath46 and @xmath47 will be close as well .    as a result of this last fact",
    ", we will approach the study of estimators @xmath48 of @xmath17 through the study of estimators @xmath16 of @xmath4 , with @xmath49 we will evaluate the loss in estimating @xmath7 by @xmath50 using squared - error loss , and analogously , we will evaluate the loss in estimating @xmath4 by @xmath16 using the frobenius norm , i.e. , @xmath51 as shall be seen , our rationale for choosing this norm is that it equals the sum square of eigenvalues for symmetric matrices , which upon truncation at a certain eigenvalue we will use as a complexity parameter in a bias - variance tradeoff .",
    "we normalize the corresponding risks by appropriate powers of the order @xmath43 of the network graph @xmath24 . because @xcite @xmath52 we have that @xmath53 ^ 2 \\right\\}^{\\frac{1}{2}}}{n^2 }    \\le",
    "c\\ ,   \\frac { \\left\\{\\mathbb{e}\\left [ ||\\widehat{w } - w_{true } ||_f^2\\right ] \\right\\}^{\\frac{1}{2 } } } { n}\\enskip , \\ ] ] which suggests normalization by a factor of @xmath54 on the scale of @xmath17 , and of @xmath55 , on the scale of @xmath2 . for notational convenience ,",
    "we write for two ( random ) @xmath25 matrices @xmath56 and @xmath57 , @xmath58\\right)^{1/2}$ ] .",
    "it might seem strange that while the natural setting for lipchitz continuity of statistics @xmath17 is given with respect to the @xmath40 norm on @xmath2 in ( [ eq : lip ] ) , we measure the mean - squared error of @xmath17 with respect to the mean - squared error of @xmath2 . indeed , comparing ( [ lipl1l2 ] ) and ( [ mean - squared ] ) , one could argue that our scaling could be improved by keeping the @xmath40 norm without passing to the frobenius norm . however , since our approach is spectral with symmetric matrices , the frobenius norm is invariant under orthogonal transformations while the @xmath40 norm is not . under an orthogonal base change",
    "then , errors of estimators using the frobenius norm are unchanged , while they undergo distortions with respect to the @xmath40 norm .",
    "we now outline our approach of construction of @xmath16 mentioned in the previous section .",
    "first , recall that the adjacency matrix @xmath59 $ ] can be thought of as a linear operator on @xmath60 represented in the natural basis @xmath61 , consisting of indicator functions on vertices . to uniquely define this operator , however , we can choose any basis .    if @xmath62 is an @xmath25 matrix whose columns form a new orthonormal basis for @xmath60 , then in the notation of section [ 2.1 ] , @xmath63 is the matrix representation of @xmath36 in the basis @xmath64 and similarly for @xmath65 .",
    "so , in the norm @xmath66\\right)^{1/2}$ ] , if we choose to keep entries @xmath67 of @xmath68 and transform back , obtaining the estimator @xmath69 , then    @xmath70\\\\ & = \\sum_{(i , j)\\in s } \\mathbb{e } \\left\\ { \\left(\\widetilde{w}_{obs , p}(i , j)-w_{true , p}(i , j)\\right)^2\\right\\}\\\\ & \\hspace{1.5 in } + \\sum_{(i , j)\\in s^c } \\left < p(\\cdot , i),w_{true } p(\\cdot , j)\\right>^2\\\\ \\end{aligned}\\ ] ]    note three things about this representation :    1 .",
    "it holds in the case of independent and dependent noise .",
    "2 .   taking @xmath71 to be all pairs of indices @xmath72 , @xmath73 since the frobenius norm is invariant under orthogonal transformations .",
    "thus , we achieve no improvement over @xmath36 .",
    "the form of ( [ base change ] ) reflects a trade off between the variance and bias terms , the first and second terms in ( [ base change ] ) , respectively .    in choosing a basis then",
    ", we want to simultaneously minimize both terms in ( [ base change ] ) .",
    "thus , we want :    1 .",
    "@xmath74 to be large for @xmath75 , so that the projection of the noise onto these subspaces is large , so we can discard them ; 2 .",
    "@xmath76 for @xmath75 to be as small as possible , so that the bulk of the signal lives on the subspace corresponding to projections @xmath67 ; 3 .   for practical considerations ,",
    "we want to minimize the number of such projections to compute , i.e. , @xmath77 .",
    "ideally , a natural way to achieve these goals is by choosing the columns of @xmath62 to be the first few dominant eigenvectors of @xmath4 , assuming that its squared eigenvalues are ordered descending . that way , both ( 2 ) and ( 3 ) in the above list are satisfied , and we need only verify ( 1 ) .",
    "in addition , we show that , while this basis will not be known in practice , one can instead use the first few dominant eigenvectors of @xmath36 to approximate the corresponding dominant eigenvectors of @xmath4 , at the cost of incurring an additional error term that under suitable conditions on the noise and the size of the network tends to zero as @xmath78",
    ". in particular , we will be interested in the limit of large @xmath43 with @xmath79 , but our results allow for more general considerations on @xmath35 and @xmath34 .    in the context of statistical estimation , we have in comparison to ( [ mean - squared ] ) that    @xmath80 ^ 2 \\right\\}^{\\frac{1}{2}}}{n^2 }    & \\leq   c   \\frac{d\\left(\\widehat{w}_{p , s},w_{true}\\right)}{n}\\\\    & = c \\frac{d\\left(\\widehat{w}_{p , s},w_{true}\\right)}{d\\left(\\widetilde{w}_{obs},w_{true}\\right)}\\cdot \\frac{d\\left(\\widetilde{w}_{obs},w_{true}\\right)}{n } \\enskip .",
    "\\end{aligned}\\ ] ]    thus , the quantity @xmath81 defines the error of our estimator relative to the raw data , and when less than 1 , determines how much better our estimator is guaranteed to perform in estimating statistics @xmath82 over the raw data @xmath15 . as we will see in theorem [ raw data ] , @xmath83 , and so the focus of the paper will be on bounding the relative error .",
    "in this section , we present our main results .",
    "first , we define our estimator @xmath16 of @xmath4 .",
    "we then outline our program in bounding the relative error in estimating @xmath4 , explaining its rationale .",
    "finally , we proceed to a systematic analysis .    as a preliminary step in defining our estimator @xmath16",
    ", we define @xmath84 where @xmath85 is a matrix of ones with zero diagonals .",
    "the matrix @xmath36 is a centered and rescaled version of the original @xmath15 , and will be seen ( i.e. , theorem  [ raw data ] below ) to be an unbiased estimator of @xmath4 .",
    "this step may be viewed as a pre - processing step in the overall definition of our estimator and assumes , in principle , knowledge of the type i and ii error probabilities @xmath34 and @xmath35 .",
    "this assumption is made largely for theoretical convenience and improved interpretability of results , and is roughly analogous to assuming that one knows the noise level in a standard regression model , a not - uncommon assumption in theoretical calculations for regression .    in practice , of course , the values of @xmath34 and @xmath35 must be obtained from context . in section",
    "[ 4 ] we present examples of how this may be done in two real - world settings , through ( i ) the use of experimentally reported error probabilities in the context of protein - protein interaction networks , and ( ii ) the estimation of error probabilities , using empirical null principles , in the context of gene regulatory networks inferred from microarray expression profiles .",
    "now let @xmath86 be the eigensystem of @xmath36 , ordered descending in @xmath87 .",
    "define our estimator of @xmath4 , using the first @xmath88 modes of @xmath36 , to be @xmath89    our program in bounding the relative error of @xmath90 in estimating @xmath4 is as follows .    1 .   _",
    "@xmath91 order spectral projection : ideal basis _",
    "+ let @xmath92 be the eigensystem of @xmath4 ordered decreasing in @xmath93 , and define @xmath94 we will bound @xmath95 where @xmath96 is the norm on @xmath25 matrices , @xmath97}.$ ] + 2 .   _",
    "@xmath91 order spectral projection : empirical basis _",
    "+ then , we will proceed to bound @xmath98 so that the final bound on the relative error of @xmath90 then becomes + @xmath99    the rationale for this program is as follows .",
    "we call @xmath100 the @xmath91 order ideal estimator of @xmath4 since it assumes that we have access to the true eigenbasis of @xmath4 .",
    "as mentioned in section [ 2 ] , there is a bias - variance tradeoff due to the choice of the parameter @xmath88 , and up to this choice , we show in theorem [ ideal estimator ] that the relative error of this ideal estimator is small .    in practice , however , we do not have the eigenbasis of @xmath4 .",
    "but since the relative error in estimating @xmath4 by @xmath101 is small up to the choice of @xmath88 , the remaining part of the relative error of @xmath90 to @xmath4 is determined by estimating @xmath100 by @xmath90 . both of these estimators use the same @xmath36 , but project them in directions determined by the eigenbases of @xmath36 and @xmath4 .",
    "this part of the relative error then is completely determined by the alignment of the eigenspaces of @xmath36 and @xmath4 , and theorem [ empirical estimator ] implies that under suitable assumptions on @xmath102 and the maximum degree of @xmath4 , this error is small .    the next few sections state our results in the above program .",
    "first , in section  [ 3.1 ] , we characterize the performance of the naive estimator @xmath103 in estimating @xmath37 .",
    "specifically , we provide exact expressions for how well we can do in mean - square error using just the raw data @xmath15 .",
    "second , in section  [ 3.2 ] , we bound @xmath104 and show that it is possible to do substantially better than the naive estimator , depending on the structure of the underlying network graph @xmath24 , and discuss situations where this could occur .",
    "third , in section  [ 3.3 ] , we bound @xmath105 .",
    "finally , in section  [ 3.4 ] , we present a list of various network summary statistics , characterized as to whether their corresponding functions @xmath17 are or are not lipshitz , thus indicating to which choices of summary in practice our results pertain .      in practice ,",
    "network summary statistics @xmath7 are frequently estimated through a simple plug - in estimator , i.e. , using @xmath20 .",
    "the following theorem characterizes the mean - square error performance of this estimator , appropriately renormalized .",
    "[ raw data ] under the model @xmath106 defined in ( [ eq : the.model ] ) , define @xmath107\\ ] ] and @xmath108\\ ] ] as the type i and type ii errors , respectively , for all @xmath109 .",
    "furthermore , let @xmath110 where @xmath111 is the all - ones matrix with zero diagonals .",
    "then ,    1",
    ".   @xmath112=\\mathbb{e}\\left[w_{true}\\right]$ ] in any fixed basis .",
    "2 .   the expected mean square error of @xmath36 is , @xmath113\\right)}{(1-(p+q))^2}\\\\\\enskip , \\ ] ] so that for any lipschitz continuous statistic @xmath17 ( see equation [ mean - squared ] ) @xmath114 ^ 2\\right\\}\\\\ & \\leq 2\\frac{c^2}{n^2 } \\frac{\\left(p(1-p)m+q(1-q)\\left[{n\\choose 2}-m\\right ] \\right)}{(1-(p+q))^2}\\enskip .",
    "\\end{aligned}\\ ] ]    the proof of this theorem is given in appendix [ a ] .",
    "note that since @xmath115 , the right - hand side of ( [ eq : mse.g.naive ] ) behaves like @xmath116 where @xmath117 is the density of the graph @xmath28 .",
    "hence , the performance of the naive estimator is driven by a combination of ( i ) the type i and ii error rates , @xmath34 and @xmath35 respectively , and ii ) the network structure , i.e. via the edge density .",
    "it has been observed in practice that frequently network graphs are sparse , meaning that @xmath118 . in such cases",
    ", we can then expect the performance of the naive estimator be dominated by the behavior of the type i and ii error rates @xmath34 and @xmath35 , respectively .",
    "importantly , however , whether sparse or non - sparse , we note that there is no advantage in larger networks , i.e. , as @xmath43 tends to infinity , if the error rates @xmath35 and @xmath34 are fixed relative to @xmath43 , the accuracy in estimation of @xmath37 in terms of its lipschitz continuity is @xmath119 .",
    "this observation motivates our construction of a estimator @xmath16 based on principles of denoising .",
    "we now present the ideal estimator , i.e. in the case where we know the true eigenbasis @xmath120 of @xmath4 , and characterize its performance .",
    "we give several forms of the error bound , which vary with respect to what information about @xmath4 is assumed .",
    "[ ideal estimator ] let @xmath92 be the orthonormal eigenvectors and associated eigenvalues of @xmath4 ordered decreasing according to @xmath121 .",
    "furthermore , define the @xmath122 covariance matrix @xmath123 by    @xmath124 \\enskip , \\end{aligned}\\ ] ] and define @xmath125 then ,    1 .",
    "( spectral moment error bound ) + @xmath126 + where @xmath127 is the spectral radius of @xmath42 .",
    "furthermore , the minimum of the bound is given by the value of @xmath88 such that @xmath128 if it exists ; otherwise , the function is monotone decreasing and @xmath129 .",
    "2 .   ( degree error bound )",
    "@xmath130 where @xmath131 is the degree sequence ordered descending .",
    "thus , + @xmath132 + where again , the minimum of the bound ( if it exists ) is attained at the value of @xmath88 for which @xmath133 3 .",
    "( spectral radius for independent noise ) if @xmath19 is independent across edges , @xmath134 4 .",
    "( asymptotic relative error for power law networks ) let @xmath135 be the index where equation ( [ ideal estimator mc ] ) holds .",
    "if @xmath79 , and @xmath4 has a power law degree distribution with exponent @xmath136 , then @xmath137 + in particular , if the noise is independent , this quantity is @xmath138 .",
    "the proof of these results are given in appendix [ b.1 ] . intuitively , the theorem says that the projection of the noise onto each eigenspace of the true eigenbasis is approximately the same value @xmath127 , so that one can achieve a minimum mean squared error , provided the cost of projecting the noise onto the first @xmath88 modes offsets the bias incurred by ignoring the last @xmath139 modes",
    ".    for numerical simulations and validation of these results , see section  [ 4 ] .",
    "the ideal estimator is useful in helping provide important insight into our estimation problem .",
    "however , clearly it is of limited practical use , since we typically do not know the eigenfunctions of @xmath4 .",
    "if we instead try to use the empirical basis of @xmath15 , then ( [ triangle estimate ] ) implies    @xmath140    in the previous section , we bounded @xmath104 . in this section ,",
    "we proceed to bound @xmath141 , and in the same spirit as the previous section , give several forms of the bound depending on what information is available .",
    "[ empirical estimator ] let @xmath142 be the orthonormal eigenvectors and associated eigenvalues of @xmath36 , ordered decreasing according to @xmath87 .",
    "define @xmath143 then ,    1 .   ( expected sum of spectral moments & maximum degree bounds ) + @xmath144\\leq   2s\\mathbb{e}\\left[\\widetilde{d}_{max}^2 \\right]\\ ] ] + where @xmath145 and @xmath146",
    "( bound on the expected maximum degree squared for independent noise ) let @xmath147 denote the maximum degree of @xmath4 .",
    "if the noise is independent , @xmath148 and @xmath149 + then + @xmath150 \\leq \\left ( \\delta+\\frac{1+\\sqrt{7}}{1-(p+q ) } \\sqrt{\\log(n ) \\cdot [ \\delta(1-p)+q(n-1-\\delta ) ] } \\right)^2 \\\\ & \\hspace{3in}+ \\left[\\frac{\\max\\{q,1-q\\ } } { 1-(p+q)}\\right]^2 . \\end{aligned}\\ ] ] 3 .",
    "( asymptotic relative error for power law networks and independent noise ) combining the bound above with that of theorem  2 , and applying ( [ triangle inequality of relative error ] ) , the smallest resulting error bound for @xmath151 is achieved if @xmath152 .",
    "furthermore , if @xmath79 , and @xmath4 has a power law degree distribution with exponent @xmath136 , then @xmath153 + in particular , if @xmath154 this becomes @xmath155    the proof of this theorem is given in appendix [ b.2 ] .",
    "a few things are worth pointing out in this theorem :    1 .",
    "( concentration of degree and noise ) the first part holds in full generality , while the second part holds for independent noise only .",
    "however , one can straightforwardly extend these latter results to general noise models .",
    "the key ingredient to bounding @xmath156 $ ] in any extension is a concentration inequality for the degree @xmath157 of @xmath36 to the degree @xmath157 of @xmath4 . in our work , we have used the concentration inequality for sums of independent bernoulli random variables found in @xcite .",
    "2 .   ( comparison of the ideal and empirical estimators ) when @xmath79",
    ", @xmath4 has a power law degree distribution with exponent @xmath136 , and in addition , the noise is independent , then equations ( [ ideal estimator power law ] ) and ( [ empirical estimator power law ] ) imply @xmath158 and @xmath159 where @xmath147 is the maximum degree of @xmath4 . combining this with equation ( [ statistical estimation ] ) , we have with respect to estimation of the statistic @xmath17 , that @xmath160 ^ 2 \\right\\}^{\\frac{1}{2}}}{n^2 }      & \\sim o\\left(\\frac{1}{n}\\right )   \\end{aligned}\\ ] ] + and + @xmath161 ^ 2 \\right\\}^{\\frac{1}{2}}}{n^2 }    & \\sim o\\left ( \\left(\\frac{\\delta}{n}\\right)^2 + \\frac{\\log(n)}{n }   \\right ) \\enskip .    \\end{aligned}\\ ] ] + we see that in the case of independent noise , the optimal rate for the ideal estimator in this case is @xmath162 . in the empirical estimator , we find that if @xmath163 then the rate becomes @xmath164 and @xmath165 otherwise .",
    "+ the dependence on @xmath147 is not surprising since the bound depends on the expected maximum degree squared .",
    "the presence of the factor @xmath164 is due to the concentration inequality for the independent noise and is related to the window in the concentration in which tail probabilities scale like @xmath166 .",
    "it is because of this reason that we are forced to choose @xmath152 . for different types of dependencies",
    ", one can expect this to change .",
    "it is interesting to note that in interpreting theorem  [ empirical estimator ] , the expected frobenius norm squared between the projection of a matrix onto its first @xmath88-dimensional largest eigenspaces and its projection onto the first @xmath88-dimensional largest eigenspaces of its expected value is dominated by the @xmath91 partial second spectral moment . to date , the only estimate to be found on this quantity is in the form of the extreme partial trace formula of @xcite , but more useful bounds akin to theorem [ ideal estimator ] , where one can bound this quantity by , say , the more interpretable first few dominant degrees , are unknown .",
    "+ one can expect to obtain a more refined bound akin to this partial second spectral moment bound that includes the angles between the true eigenvectors of @xmath4 and the empirical ones of @xmath36 , so that in the limit as @xmath167 , the error in approximating the true eigenbasis by the empirical one tends to zero . for our purposes , however , since @xmath79 and @xmath43 is growing , the dominant term in the error is those of the expected square eigenvalues which we find in the bound .",
    "in improving the bound by including eigenvectors angles , one would find @xmath168 for any kind of condition on @xmath169 , however we leave such improvements for future work .",
    "+ we should note , however , that in the case of the ideal estimator @xmath170 , we do have @xmath171 since by theorem [ ideal estimator ] , the smallest error occurs when @xmath172    for numerical simulations and validation of these results , see section [ 4 ] .",
    "the results of the previous sections establish that it is possible to estimate @xmath37 substantially better when the argument to @xmath17 , rather than being simply the observation @xmath15 ( as is common in practice ) , is instead an appropriately denoised version of @xmath15 , i.e. , @xmath173 .",
    "there are many such network summary statistics @xmath17 used in applications .",
    "as mentioned earlier , however , our results apply only to those that are sufficiently smooth in @xmath2 . while we have not attempted to provide an exhaustive survey of the numerous ( and still growing ) list of network summary statistics available , the following result characterizes a representative collection .",
    "[ contstatpf ] consider the class of adjacency matrices @xmath2 for all connected , undirected ( possibly weighted ) graphs @xmath174 .    1",
    ".   the following network summary statistics are not continuous functions of the adjacency matrix in any norm : 1 .",
    "geodesic distances ; 2 .",
    "betweenness centrality ; 3 .",
    "closeness centrality .",
    "the following network summary statistics are continuous functions of @xmath2 in any norm : 1 .",
    "degree centrality ; 2 .",
    "the number of @xmath175 paths between vertices ; 3 .",
    "conductance of a set @xmath176 on the set of graphs for which @xmath71 contains at least one edge with weight @xmath177 ; 4 .",
    "eigenvector centrality in the metric @xmath178 5",
    ".   density .    in particular in ( 2 ) , ( a ) , ( c ) and ( e ) are lipschitz while @xmath179 and @xmath180 are locally lipschitz .",
    "the proof of these results is given in appendix [ c ] .",
    "note that for the three summary statistics where continuity fails , all are functions of shortest paths .",
    "the geodesic distance between pairs of nodes is the length of the ( not necessarily unique ) shortest path from one to the other .",
    "betweenness centrality is a vertex - specific quantity , which effectively counts the number of shortest paths between vertex pairs that pass through a given vertex . finally , closeness centrality is simply the inverse of the total distance of a vertex to all others in the graph . in all cases ,",
    "essentially , continuity fails due to the sensitivity of shortest paths to perturbations .",
    "in contrast , the summary statistics for which continuity holds are representative of a number of different types of quantities .",
    "degree centrality is in fact just vertex degree , while density was defined earlier , as @xmath181 , which is proportional to the average vertex degree over the entire graph .",
    "hence , our results on estimation pertain to degree , a fundamental quantity in graph theory and network analysis , on both local and global scales . on the other hand ,",
    "the number of @xmath175-paths between vertices and the conductance of a set @xmath71 are both relevant to the study of flows in networks .",
    "finally , eigenvector centrality of a given vertex refers to the corresponding entry of the eigenvector of @xmath2 associated with the largest eigenvalue .",
    "it is intimately related to the stationary probabilities of a random walk on the graph @xmath174 , and is a measure of vertex ` importance ' that , among other uses , plays a role in the ranking of results from a google search .",
    "in this section we present three sets of numerical illustrations of our work .",
    "section [ 4.1 ] addresses the case where one can use the ideal estimator @xmath100 in circumstances where one knows the true eigenbasis of @xmath4 .",
    "although artificial , from a practical point of view , these results both provide additional insight into the theory developed in section  [ 3.2 ] for the ideal estimator and establish a useful baseline . in sections  [ 4.2 ] and  [ 4.3 ]",
    "we then give examples using real - world data sets .",
    "the protein - protein interaction network of  @xcite ( gleaned from biogrid ) is used in section  [ 4.2 ] to illustrate the performance that might be expected of our estimator when knowledge of the type i and ii error probabilities @xmath34 and @xmath35 are taken from the experimental literature @xcite . in particular , using simulation with a range of realistic error values , we demonstrate a strong robustness of the superiority of our proposed estimator @xmath90 over the observed @xmath15 to misspecification of the true @xmath34 and @xmath35 . in contrast , a gene regulatory network for the pheromone response pathway of @xcite , constructed using gene expression data of  @xcite , is used in section  [ 4.3 ] to illustrate the application of our proposed method in the situation where there is no other recourse but to estimate @xmath34 and @xmath35 from the same data used to infer the network @xmath28 ( and hence @xmath15 ) itself .      motivated by the success in using @xmath100 over the raw data @xmath36 in section [ 3.2 ]",
    ", we confine ourselves to this choice of the optimal basis and ask how well we can denoise the observation @xmath182 note that this choice represents an ideal situation , in which we already know a basis defined explicitly in terms of the object we seek to recover  but we do not know the values of the coefficients to assign to the elements of this basis . as such , our results below",
    "should be understood as providing insight into the best we might hope to do in our estimation problem .",
    "we begin by providing examples of constructions of networks from simpler ones where this information is known .",
    "let @xmath183 denote a family of graphs on the vertices @xmath184 with @xmath185 and adjacency matrices @xmath186 .",
    "there exist very general cartesian type operations @xcite on these graphs to obtain graphs on the vertex set @xmath187    let @xmath188 .",
    "the non - complete extended p - sum , ( neps ) of @xmath189 , @xmath190 , on @xmath191 is defined by ,    @xmath192    intuitively , the nontrivial elements of the hypercube @xmath193 indicate in which components two points @xmath194 are joined . of particular interest",
    "are :    1 .   the cartesian product , or sum , @xmath195 with @xmath193 consisting of the standard basis vectors in @xmath60 .",
    "this guarantees that for each @xmath43-tuple , if we travel in each direction @xmath196 ( fixing all vertices with the same component except @xmath197 ) , we trace out @xmath198 .",
    "+ by taking @xmath199 , a path graph on @xmath175 vertices , this include @xmath175-lattices in @xmath60 . by taking @xmath200 , the cycle on @xmath201 vertices , this also includes toroidal lattices .",
    "2 .   the tensor product , or product , @xmath202 with @xmath203 .",
    "in contrast to the cartesian product , this guarantees that an n - tuple @xmath204 is connected to all other n - tuples provided the corresponding component vertices are connecting in their graphs .",
    "3 .   the strong product , @xmath205 with @xmath206 , which is a union of the two previous graphs .",
    "we have the following results @xcite    [ svetadjprop ] the neps @xmath190 has adjacency matrix @xmath207 where @xmath208 is the identity matrix of the same size as @xmath209 and @xmath210 .",
    "[ cartlap ] using the same notation as in proposition [ svetadjprop ] , let @xmath190 be a neps graph with    @xmath211    if @xmath212 are the eigenvalues of @xmath213 for @xmath214 , then the spectrum of the neps of @xmath215 with basis @xmath193 consists of all possible values @xmath216    if @xmath217 , @xmath218 are the linearly independent eigenvectors of @xmath213 with @xmath219 , then @xmath220 is an eigenvector of @xmath221 with eigenvalue @xmath222 .",
    "we now present applications of these results relevant to when we can assume knowledge of the true eigenfunctions of an underlying graph .    1 .   for the toroidal @xmath175-lattice in @xmath60 , @xmath223 .",
    "for @xmath224 , note that each vertex has degree 2 .",
    "note that @xmath225 where @xmath62 is a permutation matrix determined by a cyclic permutation of length @xmath201 .",
    "if @xmath226 is a @xmath201 root of unity , then @xmath227 is an eigenvector of @xmath62 with eigenvalue @xmath226 .",
    "from this , we immediately see that the eigenvalues of @xmath228 are @xmath229 with eigenvectors + @xmath230 where @xmath231 is a normalization constant .",
    "we now apply proposition [ cartlap ] for the eigenvectors and eigenvalues of the toroidal lattice .",
    "one should note that most of the eigenvalues have multiplicity 2 , so that the eigenspaces are actually two - dimensional .",
    "2 .   for a @xmath175-lattice in @xmath60 , @xmath232 . from @xcite ,",
    "the eigenvalues of @xmath233 are given by + @xmath234 + the nontrivial eigenvectors are given by + @xmath235 where @xmath236 is a normalization constant .",
    "we can now apply proposition [ cartlap ] for the eigenvectors and eigenvalues of the lattice .    using these results ,",
    "we consider the simple case of a genus five toroidal lattice obtained from an undirected 5 cycle , @xmath237 , using independent noise and @xmath238 , @xmath239 .",
    "this graph has @xmath240 vertices with @xmath241 edges .    in figure [ 5x5tdensity ]",
    "we plot the histogram of the @xmath242-squared deviation of the density of edges in the network using @xmath15 from that of @xmath4 obtained over 500 samples in the top panel . in the bottom panel ,",
    "we plot a histogram of the @xmath242-squared deviation of the density of edges using our estimator @xmath243 from that of @xmath4 for the same 100 samples . as we will see in the next figure , the relative error for @xmath100",
    "is minimized at @xmath244 which motivates this choice , and we choose to plot the @xmath242-squared errors for visual clarity .",
    "we note that our estimator has a mean - squared error of @xmath245 with standard deviation @xmath246 in measuring the true density while @xmath15 has a mean - squared error of @xmath247 with standard deviation @xmath248 in measuring the true density . in other words ,",
    "our estimator yields , on average , an improvement of six orders of magnitude over a naive estimator that simply uses the observed network .",
    "-squared deviations of densities for the genus five toroidal lattice using an undirected five cycle .",
    "top : histogram of @xmath242-squared deviations of the densities for @xmath15 to @xmath4 .",
    "bottom : histogram of @xmath242-squared deviations of the densities for @xmath243 to @xmath4 .",
    "simulations are over @xmath249 samples with @xmath238 and @xmath239 using independent noise.,width=453,height=302 ]    in figure [ 5x5tre ] , we plot the relative error curves from section [ three ] , theorem [ ideal estimator ] for @xmath170 in this data set using the eigenvectors given above for @xmath237 across all @xmath250 modes .",
    "we note that when using the ideal estimator , the true relative error and the relative error through the spectral bound are nearly identical , and achieve a minimum relative error at @xmath244 of approximately @xmath251 . the degree bound on the relative error , however , while more interpretable , is worse , with an approximate error of @xmath252 , since it linearly interpolates between the relative error at @xmath152 and @xmath253 since @xmath237 has constant degree @xmath254 everywhere .     from theorems [ ideal estimator ] and [ ideal estimator ] for @xmath4 taken from a genus 5 toroidal lattice using an undirected five - cycle using @xmath238 and @xmath239 with independent noise .",
    "we note that the blue curve is underneath the red curve , indicating the estimate on the true value of the relative error is very precise.,width=453,height=377 ]      here we present an illustration of our method using a version of the yeast protein - protein interaction ( ppi ) network , as extracted from biogrid and analyzed in  @xcite .",
    "the declaration of ` edge ' or ` non - edge ' status in such networks is typically based on experiments aimed at assessing the affinity of protein pairs for binding to each other .",
    "such measurement of interactions among proteins is widely recognized to be extremely noisy .",
    "@xcite recently have summarized the fairly substantial literature on quantifying the type i and type ii error probabilities @xmath34 and @xmath35 that can be expected in practice for yeast ppi under standard experimental regimes .",
    "we use this network and the error rates drawn from this literature in a quasi - simulation study that will illustrate the robustness that can be expected of our method to misspecification of @xmath34 and @xmath35 , since such misspecification is nearly certain in practice .",
    "( we present a full application to real data in the next section , in the context of gene regulatory networks . )    the original ppi network has @xmath255 vertices and @xmath256 edges . in their table  1 , @xcite summarize ( average ) false - positive rates found in the literature . using what appears to be their preferred value , we set @xmath257 .",
    "similarly , in their table  2 is found a summary of various projections of the overall network size ( i.e. , number of edges ) in the true yeast interactome .",
    "given their mean projected size of @xmath258 edges , and the observed size of @xmath259 in our own network , we are led to use a false - negative rate of @xmath260 ( i.e. , @xmath261 ) .",
    "we then simulated noisy versions of the protein - protein interaction network of  @xcite in the same manner as described in section  [ 4.1 ] , using the values @xmath257 and @xmath260 .",
    "equipped with knowledge of these true values of @xmath34 and @xmath35 , we find that our estimator @xmath262 has an average mean - squared error of @xmath263 with standard deviation @xmath264 in measuring the true density , while @xmath15 has an average mean - squared error of @xmath265 with standard deviation @xmath266 .",
    "that is , our estimator yields , on average , an improvement of roughly five orders of magnitude .    motivated by the success of @xmath262 over @xmath267 for the true values of @xmath268 and @xmath257",
    ", we may ask how robust it is with respect to these parameters since they will not be known precisely in practice .",
    "rather , an experimentalist typically will be more comfortable stating that these parameters lie within a certain range of values . in table",
    "[ ppi w1 summary ] , therefore , we give the mean - squared error and the standard deviation of squared errors in estimating the density using @xmath262 , for values of @xmath35 and @xmath34 that deviate from their true values by @xmath269 , using the same @xmath270 samples for each choice .",
    "examing the results , we see that performance seems to be driven most strongly by the accuracy to which we know the type i error probability @xmath34 , and that when this value is known accurately , we maintain our five orders of magnitude improvement .",
    "this result is encouraging , because the type i rate will likely be more laboratory - dependent and therefore should be known with some reasonable accuracy .",
    "on the other hand , even when we misspecify @xmath34 by as much as @xmath269 ( i.e , more than @xmath271 ) , we generally still observe an order of magnitude improvement , except in the case when both @xmath34 and @xmath35 are specified @xmath272 too high .",
    ".mean - squared errors and standard deviation summaries for @xmath273 for the protein - protein interaction network of @xcite from biogrid for different values of misspecified type i and type ii errors using 500 samples [ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ gw1 summary statistics ]",
    "while it is now common across the various scientific disciplines to construct network - based representations of complex systems and to then summarize those representations with various statistics ( e.g. density , betweenness , conductance , etc . )",
    "there has to date been little to no acknowledgement of the necessary propagation of error from network construction to network summary .",
    "effectively , in simply reporting the statistics of their  noisy \" networks , investigators are using plug - in estimators .    in this paper , we formalize this problem and lay an initial formulation for its solution .",
    "specifically , we have constructed several spectral - based estimators of network adjacency matrices in order to more accurately estimate lipschitz continuous network summary statistics in mean square error    @xmath274 ^ 2 \\right\\}^{\\frac{1}{2}}}{n^2 }    \\le   c   \\frac { d\\left(w_1,w_2\\right)}{n}.\\enskip\\ ] ] where @xmath275 \\right\\}^{\\frac{1}{2 } } /n$ ] .    under the additive noise model @xmath106 that has @xmath4 fixed and type i and ii errors the same across edges , we first construct an unbiased estimator , @xmath36 that can be computed directly from the observed data , and use spectral projection onto its first @xmath88 dominant modes in absolute value to obtain our empirical estimator , @xmath90 , treating @xmath88 as a model selection parameter that controls a bias / variance tradeoff .",
    "we bound the performance of @xmath90 to @xmath4 by comparing it through the performance of another estimator , the oracle estimator @xmath100 , in which the unbiased estimator is projected onto the eigenbasis of the true network . by bounding the performance of both the oracle and empirical estimator",
    ", we show how to choose @xmath88 , and obtain asymptotic results on the relative error , @xmath276 .",
    "we conclude by giving several network summary statistics that are and are not continuous with respect to any norm , and give numerical examples employing @xmath100 and @xmath90 , to both synthetic and real - world data .",
    "our work is significant in laying a critical new foundation stone in the general area of network inference . at the same time",
    ", it should be remembered that our theoretical work makes the important assumption that the type i and ii error probabilities ( i.e. , @xmath34 and @xmath35 ) are known .",
    "while this assumption , analogous to claiming knowledge of the noise level in traditional regression modeling , is undoubtedly false in practice , it is particularly useful in allowing us to work with centered versions of the network adjacency matrix which , in turn , allows for a noticeably greater level of interpretation of the various results we produce .",
    "furthermore , we have demonstrated , through our numerical work that not only does knowledge of exact type i and ii error rates yield substantial improvements over naive plug - estimation ( i.e. , by multiple orders of magnitude ) , but in addition , these improvements appear to be rather robust to misspecification ( with our examples involving misspecification by more than @xmath271 at particularly high error rates ) .",
    "finally , we have shown that it is possible , using techniques and tools standard to the inference of association networks in computational biology , to estimate the type i and ii error rates .",
    "an important next step , building upon our work here , would be to develop an analogous set of theoretical bounds on the risk of estimators like ours that explicitly take into account the uncertainty due to any estimation of @xmath34 and @xmath35 .",
    "we note , however , that to date the development of methods for network inference ( which spans the literatures of numerous fields ) far outstrips the formal assessment of the corresponding theoretical properties , i.e. , including , in particular , a formal characterization of resulting type i and type ii error rates .",
    "for example , even in the context of regression - based methods of network inference based on principles of @xmath277 penalization , which are recently popular in the statistics literature , even the accurate estimation of @xmath35-values ( which could , for instance , be used as input to something like _ fdrtool _ in a manner similar to our analysis in section  [ 4.3 ] ) is still an open challenge , with methods like that of  @xcite beginning to make some early progress .",
    "our work is also significant in using the spectral theory of both the eigenvectors and eigenvalues of @xmath2 to solve real - world problems . to the best knowledge of the authors , eigenvectors of @xmath2 for most classes of graphs of practical interest",
    "are poorly understood to date .",
    "this is the first work in which these bases are given an applied interpretation in statistical estimation of network summary statistics .",
    "furthermore , it also shows that the so - called eigenvector centrality vector ( the dominant eigenfunction of a network ) not only can be used to rank vertices ( as is done ) , but contains substantially more information when combined with its corresponding eigenvalue ; indeed , we see this in our work using the empirical estimator where the minimal relative error is achieved with @xmath152 .",
    "the histograms of density , for example , illustrate that by simply projecting the observed signal onto this centrality vector , several orders of magnitude of improved accuracy can be achieved .",
    "some additional , technical comments are in order for future extensions of this work .",
    "first , it is straightforward to remove the assumption of the same type i and type ii errors ( @xmath35 and @xmath34 ) across all edges , at the cost of increasing all relative error bounds .",
    "however , one would still require knowledge of the number of edges since this controls how many type ii errors are being committed . in this spirit , it is the belief of the authors that much of this can be extended straightforwardly to directed networks using svd in lieu of an eigendecomposition .",
    "second , we have shown that @xmath100 is substantially better than @xmath90 in estimating @xmath4 and hence all lipschitz continuous statistics .",
    "this raises the question as to whether one can estimate the eigenbasis of @xmath4 better than just using the eigenbasis of @xmath36 .",
    "for example , if one uses a spanning set of vectors that still parametrize the eigenspaces that are linearly dependent , redundancy can make projection onto certain subspaces more stable and hence might make estimation of the eigenspaces more tenable .",
    "further considerations are so - called star bases in @xcite .",
    "third , when bounding the relative error of @xmath90 to @xmath100 , we neglected the analysis of the angles between the eigenspaces of @xmath36 , and @xmath4 since in the limit of large @xmath43 the dominating effect is in the eigenvalues . indeed ,",
    "if one were to consider the limit of @xmath167 for fixed @xmath43 , then these angles must be taken into account , and as a result , @xmath168 . in the general case",
    "then , we can expect @xmath171 and a more careful analysis might reduce all relative error bounds , even in the case we have been considering , @xmath79 and @xmath78 .",
    "such an analysis , however , requires a detailed understanding of the concentration of the angles @xmath278 where @xmath279 are the eigenbasis of @xmath36 and @xmath120 are the eigenbasis of @xmath4 both sorted descending according to their respective squared eigenvalues .",
    "fourth , and perhaps most interesting , would be a further detailed analysis of the covariance matrix @xmath42 appearing in theorem [ ideal estimator ] for dependencies in the noise across edges and its spectral properties , since most noise in many real - world applications are in fact dependent .",
    "indeed , at the time of this writing , dependencies in additive noise has been poorly understood in the context of @xmath42 and this object has appeared nowhere explicitly in the literature .",
    "once the nature of this dependency is understood , it would also allow for an extension of the relative error of @xmath100 to @xmath90 and would yield a more nontrivial bias / variance tradeoff than simply taking @xmath152 in the empirical estimator .",
    "33 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    e.  m. airoldi , d.  m. blei , s.  e. fienberg , and e.  p. xing .",
    "mixed membership stochastic block models for relational data with application to protein - protein interactions . in _ annual meeting of the international biometrics society ( enar ) _ , tampa , fl , 2006 .",
    "_ an introduction to systems biology : design principles of biological circuits_. chapman & hall , 2006 .",
    "z.  bai and j.  w. silverstein .",
    "_ spectral analysis of large dimensional random matrices_. springer , 2nd edition edition , 2010 .",
    "r.  b. brem and l.  kruglyak .",
    "the landscape of genetic complexity across 5,700 gene expression traits in yeast . _ proceedings of the national academy of sciences _ , 1020 ( 5):0 15721577 , 2005 .",
    "rachel  b brem and leonid kruglyak .",
    "the landscape of genetic complexity across 5,700 gene expression traits in yeast .",
    "_ proc natl acad sci u s a _ , 1020 ( 5):0 15721577 , feb 2005 .",
    "doi : 10.1073/pnas.0408709102 .",
    "f.  chung and l.  lu .",
    "_ complex graphs and networks_. american mathematical society , 2006",
    ".    f.  r.  k. chung .",
    "_ spectral graph theory_. number  92 in cbms regional conference series in mathematics .",
    "american mathematical society , 1996 .",
    "d.  cvetkovic , p.  rowlinson , and s.  simic . _ an introduction to the theory of graph spectra_. cambridge university press , 2012 .",
    "bradley efron .",
    "_ large - scale inference : empirical bayes methods for estimation , testing , and prediction _ , volume  1 .",
    "cambridge university press , 2010 .",
    "frank emmert - streib , galina  v glazko , gkmen altay , and ricardo de  matos  simoes . statistical inference and reverse engineering of gene regulatory networks from observational expression data . _ frontiers in genetics _",
    ", 3 , 2012 .",
    "m.  finegold and m.  drton .",
    "robust graphical modeling of gene networks using classical and alternative t - distributions . _ annals of applied statistics _ , 50 ( 2a):0 10571080 , 2011 .    l.  c. freeman .",
    "centrality in social networks : conceptual clarification .",
    "_ social networks _ , 10 ( 3 ) , 1979 .",
    "a.  goldenberg , a.x .",
    "zheng , s.e .",
    "fienberg , and e.m .",
    "a survey of statistical network models .",
    "_ foundations and trends in machine learning _ , 2:0 129233 , 2009 .",
    "r.  horn and c.  johnson .",
    "_ matrix analysis_. cambridge university press , 1985 .",
    "m.  o. jackson .",
    "_ social and economic networks_. princeton university press , 2008 .",
    "x.  jiang , n.  nariai , m.  steffen , s.  kasif , and e.  kolaczyk .",
    "integration of relational and hierarchical network information for protein function prediction .",
    "_ bmc bioinformatics _ , 90 ( 350 ) , 2008 .",
    "n.  katenka and e.  d. kolaczyk .",
    "multi - attribute networks and the impact of partial information on inference and characterization . _",
    "annals of applied statistics _ , 60 ( 3):0 10681094 , 2012 .",
    "e.  d. kolaczyk .",
    "_ statistical analysis of network data : methods and models_. springer , 2009 .",
    "wei - po lee and wen - shyong tzou .",
    "computational methods for discovering gene networks from expression data . _",
    "briefings in bioinformatics _ , 100 ( 4):0 408423 , 2009 .    han liu , fang han , ming yuan , john lafferty , and larry wasserman . high - dimensional semiparametric gaussian copula graphical models . _",
    "the annals of statistics _ , 400 ( 4):0 22932326 , 2012 .",
    "p.  mieghem .",
    "_ graph spectra for complex networks_. cambridge university press , 2011 .",
    "mark newman .",
    "_ networks : an introduction_. oxford university press , 2010 .",
    "l.  pham , l.  christadore , s.  schaus , and e.d .",
    "network - based prediction for sources of transcriptional dysregulation using latent pathway identification analysis .",
    "_ proceedings of the national academy of sciences _ , 1080 ( 32):0 1334713352 , 2011 .    c.  e. priebe , d.  l. sussman , m.  tang , and j.  t. vogelstein .",
    "statistical inference on errorfully observed graphs .",
    "arxiv 1211.3601 ,",
    "november 2012 .",
    "c.  j. roberts , b.  nelson , m.  j. marton , r.  stoughton , m.  r. meyer , h.  a. bennett , y.  d. he , h.  dai , w.  l. walker , t.  r. hughes , m.  tyers , c.  boone , and s.  h. friend . signaling and circuitry of multiple mapk pathways revealed by a matrix of global gene expression profiles .",
    "_ science _ , 2870 ( 5454):0 873880 , 2000 .",
    "stephen  m smith , karla  l miller , gholamreza salimi - khorshidi , matthew webster , christian  f beckmann , thomas  e nichols , joseph  d ramsey , and mark  w woolrich .",
    "network modelling methods for fmri .",
    "_ neuroimage _ , 540 ( 2):0 875891 , 2011 .    g.  stewart and j.  sun .",
    "_ matrix perturbation theory_. academic press , 1990 .",
    "g.  strang and t.  nguyen .",
    "_ wavelets and filter banks_. wellesley college , 2nd edition edition , 1996 .",
    "korbinian strimmer .",
    "fdrtool : a versatile r package for estimating local and tail area - based false discovery rates .",
    "_ bioinformatics _ , 240 ( 12):0 14611462 , 2008",
    ".    t.  tao .",
    "254a , notes 3a : eigenvalues and sums of hermitian matrices .",
    "terrytao.wordpress.com , january 2012 .",
    "d.  telesca , p.  mller , s.  m. kornblau , m.  a. suchard , and y.  ji .",
    "modeling protein expression and protein signaling pathways .",
    "_ journal american statistical association _ ,",
    "1070 ( 500):0 13721384 , 2012 .",
    "g.  traver hart , a.  k. ramani , and e.  m. marcotte .",
    "how complete are current yeast and human protein - interaction networks ?",
    "_ genome biology _ , 70 ( 120 ) , 2006",
    ".    u.  von luxburg . a tutorial on spectral clustering . _ statistics and computing _ , 17:0 395416 , 2007 .",
    "1 .   in the original basis , @xmath280\\\\ & = \\mathbb{e}\\left[\\frac{w_{obs}(i ,",
    "j)-qw_{k_n}(i , j)}{1-(p+q)}-w_{true}(i , j)\\right]\\\\ & = \\frac{1}{1-(p+q)}\\mathbb{e}\\left [ w_{noise}(i , j)-q+(p+q)w_{true}(i , j)\\right]\\\\ & = 0 .",
    "\\end{aligned}\\ ] ] + changing bases using a deterministic base change proves the result , as the corresponding entries are linear combinations of mean zero random variables .",
    "2 .   by the previous result , + @xmath281&=\\sum_{x",
    ", y=1}^n \\mathbb{e}\\left[\\ \\left|\\widetilde{w}_{obs}(x , y)-w_{true}(x , y)\\right|^2\\right]\\\\ & = \\frac{p(1-p)2m+q(1-q)\\left[n(n-1)-2m\\right]}{(1-(p+q))^2}\\\\ & = \\frac{2\\left(p(1-p)m+q(1-q)\\left[{n\\choose 2}-m\\right]\\right)}{(1-(p+q))^2}. \\end{aligned}\\ ] ] + the inequality ( [ eq : mse.g.naive ] ) follows in turn from the lipschitz property , @xmath282 since @xmath283    @xmath284",
    "1 .   using that + @xmath285 and @xmath286",
    "we collect the first @xmath88 terms and use orthonormality of @xmath287 to obtain , + @xmath288 \\\\ & = \\mathbb{e}\\left [ \\left| \\left| \\sum_{j=1}^s \\left<\\psi_j,\\widetilde{w}_{obs}\\psi_j\\right > \\psi_j \\psi_j^t - \\sum_{j=1}^n \\left<\\psi_j , w_{true}\\psi_j\\right > \\psi_j\\psi_j^t\\right| \\right|_f^2\\right]\\\\ & = \\mathbb{e}\\left [ \\left| \\left| \\sum_{j=1}^s \\left<\\psi_j,\\left(\\widetilde{w}_{obs}-w_{true}\\right)\\psi_j\\right > \\psi_j \\psi_j^t - \\sum_{j = s+1}^n \\left<\\psi_j , w_{true}\\psi_j\\right > \\psi_j\\psi_j^t\\right| \\right|_f^2\\right]\\\\ & = \\sum_{j=1}^s \\mathbb{e}\\left [ \\left| \\left<\\psi_j , ( \\widetilde{w}_{obs}-w_{true } ) \\psi_j\\right>\\right|^2\\right]+\\sum_{j = s+1}^n \\lambda_j^2\\\\ & = \\frac{1}{(1-(p+q))^2}\\sum_{j=1}^s \\mathbb{e}\\left [ \\left|\\left<\\psi_j,(w_{noise}-qw_{k_n}+(p+q)w_{true})\\psi_j\\right>\\right|^2\\right]\\\\ & \\hspace{2.5in}+\\sum_{j = s+1}^n \\lambda_j^2\\\\ \\end{aligned}\\ ] ] + where in the last equality , we have used the definition of @xmath289 now , @xmath290\\\\ & = \\frac{1}{(1-(p+q))^2 } \\sum_{j=1}^s \\mathbb{e}\\left [ \\left| \\sum_{x , y=1}^n \\left(w_{noise}(x , y)-q \\right .",
    "\\\\ & \\hspace{2in}\\left .",
    "\\left.+(p+q)w_{true}(x , y ) \\right)\\psi_j(x)\\psi_j(y)\\right|^2\\right]\\\\ & = \\frac { \\sum_{j=1}^s \\sum_{x , y , z , w=1}^n c(x , y , z , w)\\psi_j(x)\\psi_j(y)\\psi_j(z)\\psi_j(w ) } { ( 1-(p+q))^2}\\\\ & = \\frac { \\sum_{j=1}^s \\left<\\psi_j\\psi_j^t , c \\psi_j\\psi_j^t\\right>}{(1-(p+q))^2 } \\\\ & \\leq \\frac{\\sigma_{max}}{(1-(p+q))^2 } s\\\\ \\end{aligned}\\ ] ] + where in the last line , @xmath127 is the spectral radius of @xmath42 .",
    "thus , + @xmath288 \\leq \\frac{\\sigma_{max}}{(1-(p+q))^2 } s + \\sum_{j = s+1}^n \\lambda_j^2.\\\\ \\end{aligned}\\ ] ] + if we call the bound @xmath291 , notice that + @xmath292 since @xmath293 are ordered descending , it is clear that the minimum value will occur when @xmath294 .",
    "we use the following result from @xcite .",
    "+ let @xmath56 be a real symmetric @xmath25 matrix with eigenvalues @xmath295 and ordered diagonal elements @xmath296 then , for any @xmath297 , it holds that @xmath298 + now , consider the matrix @xmath299 with diagonal entries @xmath300 . by invariance of the trace , @xmath301 ,",
    "so that applying the above proposition , + @xmath302 + the result now follows immediately from the bound in ( 1 ) .",
    "the statement about the minimum follows the same argument as that in ( 1 ) .",
    "if @xmath19 is independent across edges , the definition of @xmath42 implies that @xmath42 is diagonal , with diagonal entries either @xmath303 or @xmath304 .",
    "thus , @xmath305 4 .",
    "using theorem [ raw data ] , and the above results ( 2 ) + @xmath306\\\\   & = \\frac{(1-(p+q))^2}{2 } \\left [ \\frac{s\\left(\\frac{\\sigma_{max}}{(1-(p+q))^2}\\right)+\\sum_{j = s+1}^n \\bar{d}(j)}{p(1-p)m+q(1-q)\\left[{n\\choose 2}-m\\right]}\\right]\\\\ \\end{aligned}\\ ] ] + the minimization condition + @xmath307 + implies that this occurs precisely when we choose @xmath88 to be the value when the @xmath308 largest degree grows as fast as @xmath309 , implying that @xmath88 is the number of vertices in the network whose degree grow faster than @xmath310 .",
    "we take @xmath88 to be the first index , @xmath135 , for which this occurs .",
    "+ if @xmath4 has a power - law degree distribution , i.e. @xmath311 with @xmath136 where @xmath312 is the fraction of vertices in @xmath4 with degree @xmath313 , the fraction of vertices whose degree is larger than @xmath314 is given by + @xmath315 \\end{aligned}\\ ] ] + so that + @xmath316\\ ] ] + similarly , + @xmath317 \\end{aligned}\\ ] ] + so that + @xmath318\\ ] ] + substituting these into equation [ ideal comp ] , + @xmath319\\\\   & = \\frac{\\frac{(1-(p+q))^2}{2 } \\cdot n\\pi}{p(1-p)m+q(1-q)\\left[{n\\choose 2}-m\\right]}\\\\   & \\hspace{.5in}\\cdot   \\left [ \\frac{1}{\\gamma-1 } \\cdot \\left[\\frac{1}{\\bar{d}(s_0 + 1)^{\\gamma-1}}-\\frac{1}{\\bar{d}(1)^{\\gamma-1}}\\right]\\left(\\frac{\\sigma_{max}}{(1-(p+q))^2}\\right)\\right.\\\\   & \\hspace{1in}\\left.+\\frac{1}{\\bar{d}(n)^{\\gamma-1}}+\\frac{1}{\\gamma-2 } \\left ( \\frac{1}{\\bar{d}(n)^{\\gamma-2 } } - \\frac{1}{\\bar{d}(s_0 + 1)^{\\gamma-2}}\\right ) \\right]\\\\   & \\leq \\frac{\\frac{(1-(p+q))^2}{2 } \\cdot \\frac{n\\pi}{\\gamma-2}}{p(1-p)m+q(1-q)\\left[{n\\choose 2}-m\\right ] } \\\\   & \\hspace{1in}\\cdot   \\left [ \\frac{3}{\\bar{d}(n)^{\\gamma-2 } } - \\frac{\\sigma_{max}}{(1-(p+q))^2 \\bar{d}(1)^{\\gamma-1 } } \\right ] \\\\   & \\sim o\\left(\\frac{1}{n}\\cdot \\left ( \\frac{1}{\\bar{d}(n)^{\\gamma-2 } } - \\frac{\\sigma_{max } } { \\bar{d}(1)^{\\gamma-1}}\\right ) \\right)\\\\ \\end{aligned}\\ ] ] + when @xmath79 .",
    "+ if the noise if independent , @xmath320 since @xmath79 , so that the asymptotic becomes @xmath321 ."
  ],
  "abstract_text": [
    "<S> consider observing an undirected network that is ` noisy ' in the sense that there are type i and type ii errors in the observation of edges . </S>",
    "<S> such errors can arise , for example , in the context of inferring gene regulatory networks in genomics or functional connectivity networks in neuroscience . given a single observed network then , to what extent </S>",
    "<S> are summary statistics for that network representative of their analogues for the true underlying network ? </S>",
    "<S> can we infer such statistics more accurately by taking into account the noise in the observed network edges ? in this paper , we answer both of these questions . in particular , we develop a spectral - based methodology using the adjacency matrix to ` denoise ' the observed network data and produce more accurate inference of the summary statistics of the true network . </S>",
    "<S> we characterize performance of our methodology through bounds on appropriate notions of risk in the @xmath0 sense , and conclude by illustrating the practical impact of this work on synthetic and real - world data .    ,     +     + </S>"
  ]
}