{
  "article_text": [
    "in recent years , convex programs have become increasingly popular for solving a wide range of problems in machine learning and other fields , ranging from theoretical modeling , e.g. , latent variable graphical model selection @xcite , low - rank feature extraction ( e.g. , matrix decomposition @xcite and matrix completion @xcite ) , subspace clustering @xcite , and kernel discriminant analysis @xcite , to real - world applications , e.g. , face recognition @xcite , saliency detection @xcite , and video denoising @xcite .",
    "most of the problems can be ( re)formulated as the following linearly constrained separable convex program , @xmath0 , where @xmath1 s are convex sets , the program can be transformed into ( [ eq : model_problem_multivar ] ) by introducing auxiliary variables , c.f .",
    "( [ eq : model_problem_multivar_convex_sets])-([eq : redefine_equiv ] ) . ] : @xmath2 where @xmath3 and @xmath4 could be either vectors or matrices a  block \" of variables because it may consist of multiple scalar variables .",
    "we will use bold capital letters if a block is known to be a matrix .",
    "] , @xmath5 is a closed proper convex function , and @xmath6 is a linear mapping . without loss of generality , we may assume that none of the @xmath7 s is a zero mapping , the solution to @xmath8 is non - unique , and the mapping @xmath9 is onto is not full column rank but full row rank , where @xmath10 is the matrix representation of @xmath7 . ] .      in this subsection",
    ", we present some examples of machine learning problems that can be formulated as the model problem .",
    "low - rank representation ( lrr )  @xcite is a recently proposed technique for robust subspace clustering and has been applied to many machine learning and computer vision problems . however , lrr works well only when the number of samples is more than the dimension of the samples , which may not be satisfied when the data dimension is high .",
    "so liu et al .",
    "@xcite proposed latent lrr to overcome this difficulty .",
    "the mathematical model of latent lrr is as follows : @xmath11 where @xmath12 is the data matrix , each column being a sample vector , @xmath13 is the nuclear norm  @xcite , i.e. , the sum of singular values , and @xmath14 is the @xmath15 norm  @xcite , i.e. , the sum of absolute values of all entries .",
    "latent lrr is to decompose data into principal feature @xmath16 and salient feature @xmath17 , up to sparse noise @xmath18 .",
    "nonnegative matrix completion ( nmc )  @xcite is a novel technique for dimensionality reduction , text mining , collaborative filtering , and clustering , etc .",
    "it can be formulated as : @xmath19 where @xmath20 is the observed data in the matrix @xmath12 contaminated by noise @xmath21 , @xmath22 is an index set , @xmath23 is a linear mapping that selects those elements whose indices are in @xmath22 , and @xmath24 is the frobenius norm .",
    "nmc is to recover the nonnegative low - rank matrix @xmath12 from the observed noisy data @xmath20 .    to see that the nmc problem can be reformulated as ( [ eq : model_problem_multivar ] ) , we introduce an auxiliary variable @xmath25 and rewrite as @xmath26 where @xmath27 is the characteristic function of the set of nonegative matrices .      besides unsupervised learning models shown above , many supervised machine learning problems can also be written in the form of .",
    "for example , using logistic function as the loss function in the group lasso with overlap  @xcite , one obtains the following model : @xmath28 where @xmath3 and @xmath29 , @xmath30 , are the training data and labels , respectively , and @xmath31 and @xmath32 parameterize the linear classifier .",
    "@xmath33 , @xmath34 , are the selection matrices , with only one 1 at each row and the rest entries are all zeros .",
    "the groups of entries , @xmath35 , @xmath34 , may overlap each other .",
    "this model can also be considered as an extension of the group sparse logistic regression problem  @xcite to the case of overlapped groups .    introducing @xmath36 , @xmath37 , @xmath38 , and @xmath39 , where @xmath40 , can be rewritten as @xmath41 which is a special case of .",
    "although general theories on convex programs are fairly complete nowadays , e.g. , most of them can be solved by the interior point method  @xcite , when faced with large scale problems , which are typical in machine learning , the general theory may not lead to efficient algorithms .",
    "for example , when using cvx , an interior point based toolbox , to solve nuclear norm minimization problems ( i.e. , one of the @xmath5 s is the nuclear norm of a matrix , e.g. , and ) , such as matrix completion  @xcite , robust principal component analysis  @xcite , and low - rank representation  @xcite , the complexity of each iteration is @xmath42 , where @xmath43 is the matrix size .",
    "such a complexity is unbearable for large scale computing .    to address the scalability issue ,",
    "first order methods are often preferred . the accelerated proximal gradient ( apg )",
    "algorithm  @xcite is popular due to its guaranteed @xmath44 convergence rate , where @xmath45 is the iteration number .",
    "however , apg is basically for unconstrained optimization . for constrained optimization",
    ", the constraints have to be added to the objective function as penalties , resulting in approximated solutions only . the alternating direction method ( adm )  @xcite has regained a lot of attention recently and",
    "is also widely used .",
    "it is especially suitable for separable convex programs like ( [ eq : model_problem_multivar ] ) because it fully utilizes the separable structure of the objective function . unlike apg",
    ", adm can solve ( [ eq : model_problem_multivar ] ) exactly .",
    "another first order method is the split bregman method @xcite , which is closely related to adm  @xcite and is influential in image processing .",
    "an important reason that first order methods are popular for solving large scale convex programs in machine learning is that the convex functions @xmath5 s are often matrix or vector norms or characteristic functions of convex sets , which enables the following subproblems ( called the proximal operation of @xmath5  @xcite ) @xmath46 to have closed form solutions .",
    "for example , when @xmath5 is the @xmath15 norm , @xmath47 , where @xmath48 is the soft - thresholding operator  @xcite ; when @xmath5 is the nuclear norm , the optimal solution is : @xmath49 , where @xmath50 is the singular value decomposition ( svd ) of @xmath51  @xcite ; and when @xmath5 is the characteristic function of the nonnegative cone , the optimal solution is @xmath52 .",
    "since subproblems like ( [ eq : proxy ] ) have to be solved in each iteration when using first order methods to solve separable convex programs , that they have closed form solutions greatly facilitates the optimization .",
    "however , when applying adm to solve ( [ eq : model_problem_multivar ] ) with non - unitary linear mappings ( i.e. , @xmath53 is not the identity mapping , where @xmath54 is the adjoint operator of @xmath7 ) , the resulting subproblems may not have closed form solutions in ( [ eq : proxy ] ) becomes @xmath55 , which can not be reduced to @xmath56 .",
    "] , hence need to be solved iteratively , making the optimization process awkward .",
    "some work @xcite has considered this issue by linearizing the quadratic term @xmath55 in the subproblems , hence such a variant of adm is called the linearized adm ( ladm ) . @xcite",
    "further propose the generalized adm that makes both adm and ladm as its special cases and prove its globally linear convergence by imposing strong convexity on the objective function or full - rankness on some linear operators .",
    "nonetheless , most of the existing theories on adm and ladm are for the _ two - block _ case , i.e. , @xmath57 in ( [ eq : model_problem_multivar ] )  @xcite .",
    "the number of blocks is restricted to two because the proofs of convergence for the two - block case are not applicable for the multi - block case , i.e. , @xmath58 in ( [ eq : model_problem_multivar ] ) .",
    "actually , a naive generalization of adm or ladm to the multi - block case may diverge ( see ( [ eq : parallel_bp ] ) and @xcite ) . unfortunately , in practice multi - block convex programs often occur , e.g. , robust principal component analysis with dense noise  @xcite , latent low - rank representation  @xcite ( see ) , and when there are extra convex set constraints ( see ( [ eq : nmc ] ) and ( [ eq : model_problem_multivar_convex_sets])-([eq : model_problem_multivar_equiv ] ) ) .",
    "so it is desirable to design practical algorithms for the multi - block case .    recently @xcite and @xcite considered the multi - block ladm and adm , respectively . to safeguard convergence , @xcite proposed ladm with gaussian back substitution ( ladmgb ) , which destroys the sparsity or low - rankness of the iterates during iterations when dealing with sparse representation and low - rank recovery problems , while @xcite proposed adm with parallel splitting , whose subproblems may not be easily solvable .",
    "moreover , they all developed their theories with the penalty parameter being fixed , resulting in difficulty of tuning an optimal penalty parameter that fits for different data and data sizes .",
    "this has been identified as an important issue  @xcite .      to propose an algorithm that is more suitable for convex programs in machine learning , in this paper we aim at combining the advantages of @xcite , @xcite , and @xcite , i.e. , combining ladm , parallel splitting , and adaptive penalty .",
    "hence we call our method ladm with parallel splitting and adaptive penalty ( ladmpsap ) .",
    "with ladm , the subproblems will have forms like ( [ eq : proxy ] ) and hence can be easily solved . with parallel splitting ,",
    "the sparsity and low - rankness of iterates can be preserved during iterations when dealing with sparse representation and low - rank recovery problems , saving both the storage and the computation load . with adaptive penalty ,",
    "the convergence can be faster and it is unnecessary to tune an optimal penalty parameter .",
    "parallel splitting also makes the algorithm highly parallelizable , making ladmpsap suitable for parallel or distributed computing , which is important for large scale machine learning . when all the component objective functions have bounded subgradients , we prove convergence results that are stronger than the existing theories on adm and ladm .",
    "for example , the penalty parameter can be _ unbounded _ and the _ sufficient and necessary _ conditions of the global convergence of ladmpsap can be obtained as well .",
    "we also propose a simple optimality measure and prove the convergence rate of ladmpsap in an ergodic sense under this measure . our proof is simpler than those in @xcite and  @xcite which relied on a complex optimality measure .",
    "when a convex program has extra convex set constraints , we further devise a practical version of ladmpsap that converges faster thanks to better parameter analysis .",
    "finally , we generalize ladmpsap to cope with more difficult @xmath5 s , whose proximal operation is not easily solvable , by further linearizing the smooth components of @xmath5 s .",
    "experiments testify to the advantage of ladmpsap in speed and numerical accuracy .",
    "note that @xcite also proposed a multiple splitting algorithm for convex optimization .",
    "however , they only considered a special case of our model problem ( [ eq : model_problem_multivar ] ) , i.e. , all the linear mappings @xmath7 s are identity mappings . with their simpler model problem ,",
    "linearization is unnecessary and a faster convergence rate , @xmath44 , can be achieved .",
    "in contrast , in this paper we aim at proposing a practical algorithm for efficiently solving more general problems like ( [ eq : model_problem_multivar ] ) .",
    "we also note that @xcite used the same linearization technique for the smooth components of @xmath5 s as well , but they only considered a special class of @xmath5 s .",
    "namely , the non - smooth component of @xmath5 is a sum of @xmath15 and @xmath59 norms or its epigraph is polyhedral .",
    "moreover , for parallel splitting ( jacobi update ) @xcite has to incorporate a postprocessing to guarantee convergence , by interpolating between an intermediate iterate and the previous iterate .",
    "third , @xcite still focused on a fixed penalty parameter .",
    "again , our method can handle more general @xmath5 s , does not require postprocessing , and allows for an adaptive penalty parameter .",
    "a more general splitting / linearization technique can be founded in @xcite . however , the authors only proved that any accumulation point of the iteration is a kuhn - karush - tucker ( kkt ) point and did not investigate the convergence rate .",
    "there was no evidence that the iteration could converge to a unique point .",
    "moreover , the authors only studied the case of fixed penalty parameter .",
    "although dual ascent with dual decomposition  @xcite can also solve ( [ eq : model_problem_multivar ] ) in a parallel way , it may break down when some @xmath5 s are not strictly convex  @xcite , which typically happens in sparse or low - rank recovery problems where @xmath15 norm or nuclear norm are used .",
    "even if it works , since @xmath5 is not strictly convex , dual ascent becomes dual _ subgradient _ ascent  @xcite , which is known to converge at a rate of @xmath60  slower than our @xmath61 rate .",
    "moreover , dual ascent requires choosing a good step size for each iteration , which is less convenient than adm based methods .",
    "the remainder of this paper is organized as follows .",
    "we first review ladm with adaptive penalty ( ladmap ) for the two - block case in section  [ sec : ladmap-2var ] .",
    "then we present ladmpsap for the multi - block case in section  [ sec : ladmpsap ] .",
    "next , we propose a practical version of ladmpsap for separable convex programs with convex set constraints in section  [ sec : pplamdap ] .",
    "we further extend ladmpsap to proximal ladmpsap for programs with more difficult objective functions in section  [ sec : g - ladmpsap ] .",
    "we compare the advantage of ladmpsap in speed and numerical accuracy with other first order methods in section  [ sec : exp ] .",
    "finally , we conclude the paper in section  [ sec : con ] .",
    "we first review ladmap  @xcite for the two - block case of ( [ eq : model_problem_multivar ] ) .",
    "it consists of four steps :    1 .",
    "update @xmath62 : @xmath63 2 .",
    "update @xmath64 : @xmath65 3 .",
    "update @xmath66 : @xmath67 4 .",
    "update @xmath68 : @xmath69    where @xmath70 is the lagrange multiplier , @xmath71 is the penalty parameter , @xmath72 with @xmath73 ( @xmath74 is the operator norm of @xmath7 ) , @xmath75 and @xmath76 is an adaptively updated parameter ( see ) .",
    "please refer to  @xcite for details .",
    "note that the latest @xmath77 is immediately used to compute @xmath78 ( see  ) .",
    "so @xmath62 and @xmath64 have to be updated alternately , hence the name alternating direction method .",
    "in this section , we extend ladmap for multi - block separable convex programs ( [ eq : model_problem_multivar ] ) .",
    "we also provide the _ sufficient and necessary conditions _ for global convergence when subgradients of the objective functions are all bounded .",
    "we further prove the convergence rate in an ergodic sense .",
    "contrary to our intuition , the multi - block case is actually fundamentally different from the two - block one . for the multi - block case , it is very natural to generalize ladmap for the two - block case in a straightforward way , with @xmath79 unfortunately , we were unable to prove the convergence of such a naive ladmap using the same proof for the two - block case .",
    "this is because their fejr monotone inequalities ( see remark  [ rem : different ] ) can not be the same .",
    "that is why he et al .",
    "has to introduce an extra gaussian back substitution  @xcite for correcting the iterates .",
    "actually , the above naive generalization of ladmap may be divergent ( which is even worse than converging to a wrong solution ) , e.g. , when applied to the following problem : @xmath80 where @xmath81 and @xmath10 and @xmath20 are gaussian random matrix and vector , respectively , whose entries fulfil the standard gaussian distribution independently .",
    "@xcite also analyzed the naively generalized adm for the multi - block case and showed that even for three blocks the iteration could still be divergent .",
    "they also provided sufficient conditions , which basically require that the linear mappings @xmath7 should be orthogonal to each other ( @xmath82 , @xmath83 ) , to ensure the convergence of naive adm .",
    "fortunately , by modifying @xmath84 slightly we are able to prove the convergence of the corresponding algorithm . more specifically , our algorithm for solving ( [ eq : model_problem_multivar ] ) consists of the following steps :    1 .",
    "update @xmath3 s in parallel : + @xmath85 2 .",
    "update @xmath66 : + @xmath86 3 .",
    "update @xmath68 : + @xmath87    where @xmath72 , @xmath88 and @xmath89 with @xmath90 being a constant and @xmath91 being a threshold .",
    "indeed , we replace @xmath84 with @xmath92 as ( [ eq : hat_lambda ] ) , which is independent of @xmath93 , and the rest procedures of the algorithm , including the scheme and to update the penalty parameter , are all inherited from  @xcite , except that @xmath94 s have to be made larger ( see theorem  [ thm : converge_multivar ] ) . as now @xmath3 s are updated in parallel and @xmath71 changes adaptively",
    ", we call the new algorithm ladm with _",
    "parallel splitting _ and _ adaptive penalty _ ( ladmpsap ) .",
    "some existing work ( e.g. , @xcite ) proposed stopping criteria out of intuition only , which may not guarantee that the correct solution is approached .",
    "recently , @xcite and @xcite suggested that the stopping criteria can be derived from the kkt conditions of a problem .",
    "here we also adopt such a strategy .",
    "specifically , the iteration terminates when the following two conditions are met : @xmath95 @xmath96 the first condition measures the feasibility error .",
    "the second condition is derived by comparing the kkt conditions of problem and the optimality condition of subproblem  .",
    "the rules ( [ eq : update_beta_multivar ] ) and ( [ eq : update_rho ] ) for updating @xmath68 are actually hinted by the above stopping criteria such that the two errors are well balanced .    for better reference , we summarize the proposed ladmpsap algorithm in algorithm  [ alg : ladmpsap - multivar ] . for fast convergence , we suggest that @xmath97 and @xmath98 and @xmath90 should be chosen such that @xmath71 increases steadily along with iterations .",
    "set @xmath90 , @xmath99 , @xmath100 , @xmath101 , @xmath102 , @xmath103 , @xmath104 , @xmath0 .",
    "compute @xmath92 as ( [ eq : hat_lambda ] ) .",
    "update @xmath105 s in parallel by solving @xmath106 update @xmath66 by ( [ eq : update_lambda_multivar ] ) and @xmath68 by ( [ eq : update_beta_multivar ] ) and ( [ eq : update_rho ] ) .      in the following",
    ", we always use @xmath107 to denote the kkt point of problem ( [ eq : model_problem_multivar ] ) . for the global convergence of ladmpsap",
    ", we have the following theorem , where we denote @xmath108 for simplicity .    *",
    "( convergence of ladmpsap ) * if @xmath109 is non - decreasing and upper bounded , @xmath110 , @xmath0 , then @xmath111 generated by ladmpsap converge to a kkt point of problem ( [ eq : model_problem_multivar]).[thm : converge_multivar ]      theorem  [ thm : converge_multivar ] is a convergence result for general convex programs  ( [ eq : model_problem_multivar ] ) , where @xmath5 s are general convex functions and hence @xmath109 needs to be bounded . actually , almost all the existing theories on adm and ladm even assumed a fixed @xmath68 .",
    "for adaptive @xmath71 , it will be more convenient if a user needs not to specify an upper bound on @xmath109 because imposing a large upper bound essentially equals to allowing @xmath109 to be unbounded . since many machine learning problems choose @xmath5 s as matrix / vector norms , which result in bounded subgradients , we find that the boundedness assumption can be removed .",
    "moreover , we can further prove the _ sufficient and necessary _ condition for global convergence .",
    "[ thm : convergence_unbounded]*(sufficient condition for global convergence ) * if @xmath109 is non - decreasing and @xmath112 , @xmath113 , @xmath114 is bounded , @xmath0 , then the sequence @xmath115 generated by ladmpsap converges to an optimal solution to ( [ eq : model_problem_multivar ] ) .",
    "[ rem : convergence_of_lambda ] theorem  [ thm : convergence_unbounded ] does not claim that @xmath116 converges to a point @xmath117 .",
    "however , as we are more interested in @xmath115 , such a weakening is harmless .",
    "we also have the following result on the necessity of @xmath112 .",
    "[ thm : convergence_unbounded_necessary]*(necessary condition for global convergence ) * if @xmath109 is non - decreasing , @xmath118 , @xmath114 is bounded , @xmath0 , then @xmath112 is also a necessary condition for the global convergence of @xmath115 generated by ladmpsap to an optimal solution to ( [ eq : model_problem_multivar ] ) .    with the above analysis , when all the subgradients of the component objective functions are bounded we can remove @xmath119 in algorithm  [ alg : ladmpsap - multivar ] .",
    "the convergence rate of adm and ladm in the traditional sense is an open problem  @xcite . although @xcite claimed that they proved the linear convergence rate of adm , their assumptions are actually quite strong .",
    "they assumed that the non - smooth part of @xmath5 is a sum of @xmath15 and @xmath59 norms or its epigraph is polyhedral .",
    "moreover , the convex constraint sets should all be polyhedral and bounded .",
    "so although their results are encouraging , for general convex programs the convergence rate is still a mystery .",
    "recently , @xcite and @xcite proved an @xmath120 convergence rate of adm and adm with parallel splitting in an ergodic sense , respectively .",
    "namely @xmath121 violates an optimality measure in @xmath120 .",
    "their proof is lengthy and is for fixed penalty parameter only .    in this subsection ,",
    "based on a simple optimality measure we give a simple proof for the convergence rate of ladmpsap .",
    "for simplicity , we denote @xmath122 , @xmath123 , and @xmath124 .",
    "we first have the following proposition .",
    "[ prop : optimality ] @xmath125 is an optimal solution to ( [ eq : model_problem_multivar ] ) if and only if there exists @xmath126 , such that @xmath127    since the left hand side of ( [ eq : constrained_optimality ] ) is always nonnegative and it becomes zero only when @xmath125 is an optimal solution , we may use its magnitude to measure how far a point @xmath125 is from an optimal solution .",
    "note that in the unconstrained case , as in apg  @xcite , one may simply use @xmath128 to measure the optimality .",
    "but here we have to deal with the constraints .",
    "our criterion is simpler than that in @xcite , which has to compare @xmath129 with all @xmath130 .",
    "then we have the following convergence rate theorem for ladmpsap in an ergodic sense .",
    "[ thm : convergence_rate_2var]*(convergence rate of ladmpsap ) * define @xmath131 , where @xmath132 .",
    "then the following inequality holds for @xmath133 : @xmath134 where @xmath135 @xmath136 and @xmath137 @xmath138 .",
    "theorem  [ thm : convergence_rate_2var ] means that @xmath133 is by @xmath139 from being an optimal solution .",
    "this theorem holds for both bounded and unbounded @xmath109 . in the bounded case , @xmath139 is simply @xmath120 .",
    "theorem  [ thm : convergence_rate_2var ] also hints that @xmath140 should approach infinity to guarantee the convergence of ladmpsap , which is consistent with theorem  [ thm : convergence_unbounded_necessary ] .",
    "in real applications , we are often faced with convex programs with convex set constraints : @xmath141 where @xmath142 is a closed convex set . in this section",
    ", we consider to extend ladmpsap to solve the more complex convex set constraint model .",
    "we assume that the projections onto @xmath1 s are all easily computable . for many convex sets used in machine learning ,",
    "such an assumption is valid , e.g. , when @xmath1 s are nonnegative cones or positive semi - definite cones . in the following ,",
    "we discuss how to solve ( [ eq : model_problem_multivar_convex_sets ] ) efficiently . for simplicity ,",
    "we assume @xmath143 , @xmath144 .",
    "finally , we assume that @xmath20 is an interior point of @xmath145 .",
    "we introduce auxiliary variables @xmath146 to convert @xmath147 into @xmath148 and @xmath149 , @xmath0 .",
    "then ( [ eq : model_problem_multivar_convex_sets ] ) can be reformulated as : @xmath150 where @xmath151 is the characteristic function of @xmath1 , @xmath152 where @xmath0 .",
    "the adjoint operator @xmath153 is @xmath154 where @xmath155 is the @xmath93-th sub - vector of @xmath156 , partitioned according to the sizes of @xmath4 and @xmath3 , @xmath0 .",
    "then ladmpsap can be applied to solve problem ( [ eq : model_problem_multivar_equiv ] ) .",
    "the lagrange multiplier @xmath66 and the auxiliary multiplier @xmath157 are respectively updated as @xmath158 and @xmath3 is updated as ( see ( [ eq : update_xi ] ) ) @xmath159/(\\eta_i\\beta_k)\\right\\|^2,\\ \\label{eq : ladmpsap_update_xi1}\\\\ { \\mathbf{x}}_{n+i}^{k+1}&=&\\operatorname*{argmin}\\limits_{{\\mathbf{x}}\\in x_i } { \\frac{\\displaystyle \\eta_{n+i}\\beta_k}{\\displaystyle   2}}\\left\\|{\\mathbf{x}}-{\\mathbf{x}}_{n+i}^k-\\hat{{\\mathbf{\\lambda}}}^k_{i+1}/(\\eta_{n+i}\\beta_k)\\right\\|^2\\nonumber\\\\ & = & \\pi_{x_i } \\left({\\mathbf{x}}_{n+i}^k+\\hat{{\\mathbf{\\lambda}}}^k_{i+1}/(\\eta_{n+i}\\beta_k)\\right),\\label{eq : ladmpsap_update_xi2}\\end{aligned}\\ ] ] where @xmath160 is the projection onto @xmath1 and @xmath0 .    as for the choice of @xmath94 s , although we can simply apply theorem  [ thm : converge_multivar ] to assign their values as @xmath161 and @xmath162 , @xmath0 , such choices are too pessimistic . as @xmath94 s are related to the magnitudes of the differences in @xmath163 from @xmath164 , we had better provide tighter estimate on @xmath94 s in order to achieve faster convergence .",
    "actually , we have the following better result .",
    "[ thm : better_eta ] for problem ( [ eq : model_problem_multivar_equiv ] ) , if @xmath109 is non - decreasing and upper bounded and @xmath94 s are chosen as @xmath165 and @xmath166 , @xmath0 , then the sequence @xmath111 generated by ladmpsap converge to a kkt point of problem ( [ eq : model_problem_multivar_equiv ] ) .    finally , we summarize ladmpsap for problem ( [ eq : model_problem_multivar_equiv ] ) in algorithm  [ alg : ladmpsap - multivar_equiv ] , which is a practical algorithm for solving ( [ eq : model_problem_multivar_convex_sets ] ) .",
    "set @xmath167 , @xmath99 , @xmath100 , @xmath101 , @xmath168 , @xmath169 , @xmath166 , @xmath104 , @xmath170 , @xmath0 .",
    "compute @xmath92 as ( [ eq : update_hatlambda_multivar - equiv ] ) .",
    "update @xmath105 , @xmath171 , in parallel as ( [ eq : ladmpsap_update_xi1])-([eq : ladmpsap_update_xi2 ] ) .",
    "update @xmath66 by ( [ eq : update_lambda_multivar - equiv ] ) and @xmath68 by ( [ eq : update_beta_multivar ] ) and ( [ eq : update_rho ] ) .",
    "( note that in ( [ eq : update_rho ] ) , ( [ eq : stopping1 ] ) , and ( [ eq : stopping2 ] ) , @xmath172 and @xmath173 should be replaced by @xmath174 and @xmath175 , respectively . )",
    "analogs of theorems  [ thm : convergence_unbounded ] and [ thm : convergence_unbounded_necessary ] are also true for algorithm  [ alg : ladmpsap - multivar_equiv ] although @xmath176 s are unbounded , thanks to our assumptions that all @xmath177 , @xmath0 , are bounded and @xmath20 is an interior point of @xmath145 , which result in an analog of proposition  [ prop : unbounded_beta ] .",
    "consequently , @xmath119 can also be removed if all @xmath177 , @xmath0 , are bounded .",
    "since algorithm  [ alg : ladmpsap - multivar_equiv ] is an application of algorithm  [ alg : ladmpsap - multivar ] to problem ( [ eq : model_problem_multivar_equiv ] ) , only with refined parameter estimation , its convergence rate in an ergodic sense is also @xmath178 , where @xmath45 is the number of iterations .",
    "in ladmpsap we have assumed that the subproblems ( [ eq : update_xi ] ) are easily solvable . in many machine learning problems , the functions @xmath5 s are often matrix or vector norms or characteristic functions of convex sets .",
    "so this assumption often holds .",
    "nonetheless , this assumption is not always true , e.g. , when @xmath5 is the logistic loss function ( see  ) .",
    "so in this section we aim at generalizing ladmpsap to solve even more general convex programs .",
    "we are interested in the case that @xmath5 can be decomposed into two components : @xmath179 where both @xmath180 and @xmath181 are convex , @xmath180 is @xmath182 : @xmath183 and @xmath181 may not be differentiable but its proximal operation is easily solvable . for brevity , we call @xmath184 the lipschitz constant of @xmath185",
    ".    recall that in each iteration of ladmpsap , we have to solve subproblem .",
    "since now we do not assume that the proximal operation of @xmath5 is easily solvable , we may have difficulty in solving subproblem . by",
    ", we write down as @xmath186 since @xmath187 is @xmath182 , we may also linearize it at @xmath164 and add a proximal term .",
    "such an idea leads to the following updating scheme of @xmath188 : @xmath189 { \\right\\|}^2 , \\end{array}\\label{eq : update_xi_proximal}\\end{aligned}\\ ] ] where @xmath0 .",
    "the choice of @xmath190 is presented in theorem  [ thm : general_convergence_unbounded ] , i.e. @xmath191 , where @xmath192 and @xmath193 are both positive constants .    by our assumption on @xmath181 , the above subproblems are easily solvable .",
    "the update of lagrange multiplier @xmath70 and @xmath68 are still respectively goes as and but with @xmath194 the iteration terminates when the following two conditions are met : @xmath195 @xmath196 these two conditions are also deduced from the kkt conditions .",
    "we call the above algorithm as proximal ladmpsap and summarize it in algorithm  [ alg : general - ladmpsap - multivar ] .",
    "set @xmath167 , @xmath197 , @xmath102 , @xmath192 , @xmath118 , @xmath104 , @xmath0 .",
    "compute @xmath92 as ( [ eq : hat_lambda ] ) .",
    "update @xmath105 s in parallel by solving @xmath198 { \\right\\|}^2,\\ i=1,\\cdots , n,\\label{eq : general_ladmpsap_update_xi}\\ ] ] where @xmath191 .",
    "update @xmath66 by ( [ eq : update_lambda_multivar ] ) and @xmath68 by with @xmath76 defined in .    as for the convergence of proximal ladmpsap",
    ", we have the following theorem .",
    "[ thm : general_convergence_unbounded]*(convergence of proximal ladmpsap ) * if @xmath71 is non - decreasing and upper bounded , @xmath191 , where @xmath192 and @xmath193 are both positive constants , @xmath0 , then @xmath111 generated by proximal ladmpsap converge to a kkt point of problem ( [ eq : model_problem_multivar ] ) .",
    "we further have the following convergence rate theorem for proximal ladmpsap in an ergodic sense .",
    "[ thm : general_convergence_unbounded_rate]*(convergence rate of proximal ladmpsap ) * define @xmath199 , where @xmath132 .",
    "then the following inequality holds for @xmath200 : @xmath201 where @xmath202 and @xmath203 .    when there are extra convex set constraints , @xmath204 , @xmath0 , we can also introduce auxiliary variables as in section  [ sec : pplamdap ] and have an analogy of theorems  [ thm : better_eta ] and [ thm : convergence_rate_2var ] .",
    "[ thm : better_eta_proximal ] for problem ( [ eq : model_problem_multivar_equiv ] ) , where @xmath5 is described at the beginning of section  [ sec : g - ladmpsap ] , if @xmath71 is non - decreasing and upper bounded and @xmath205 , where @xmath192 , @xmath206 , @xmath207 , and @xmath208 , @xmath0 , then @xmath111 generated by proximal ladmpsap converge to a kkt point of problem ( [ eq : model_problem_multivar_equiv ] ) . the convergence rate in an ergodic sense is also @xmath209 , where @xmath45 is the number of iterations .",
    "in this section , we test the performance of ladmpsap on three specific examples of problem ( [ eq : model_problem_multivar ] ) , i.e. , latent low - rank representation ( see ) , nonnegative matrix completion ( see ) , and group sparse logistic regression with overlap ( see ) .",
    "we first solve the latent lrr problem  @xcite  . in order to test ladmpsap and related algorithms with data whose characteristics are controllable",
    ", we follow @xcite to generate synthetic data , which are parameterized as ( @xmath210 , @xmath211 , @xmath212 , @xmath213 ) , where @xmath210 , @xmath211 , @xmath212 , and @xmath213 are the number of independent subspaces , points in each subspace , and ambient and intrinsic dimensions , respectively .",
    "the number of scale variables and constraints is @xmath214 .    as first order methods are popular for solving convex programs in machine learning  @xcite , here we compare ladmpsap with several conceivable first order algorithms , including apg  @xcite , naive adm , naive ladm , ladmgb , and ladmps .",
    "naive adm and naive ladm are generalizations of adm and ladm , respectively , which are straightforwardly generalized from two variables to multiple variables , as discussed in section  [ sec : ladm+psap ] .",
    "naive adm is applied to solve ( [ eq : llrr ] ) after rewriting the constraint of ( [ eq : llrr ] ) as @xmath215 . for ladmps , @xmath71 is fixed in order to show the effectiveness of adaptive penalty .",
    "the parameters of apg and adm are the same as those in  @xcite and  @xcite , respectively . for ladm , we follow the suggestions in @xcite to fix its penalty parameter @xmath68 at @xmath216 , where @xmath217 is the size of @xmath12 . for ladmgb ,",
    "as there is no suggestion in  @xcite on how to choose a fixed @xmath68 , we simply set it the same as that in ladm .",
    "the rest of the parameters are the same as those suggested in @xcite .",
    "we fix @xmath218 in ladmps and set @xmath219 and @xmath220 in ladmpsap . for ladmpsap , we also set @xmath221 , where @xmath222 and @xmath223 are the parameters @xmath94 s in algorithm  [ alg : ladmpsap - multivar ] for @xmath224 and @xmath225 , respectively . for the stopping criteria , @xmath226 and @xmath227 , with @xmath228 and @xmath229",
    "are used for all the algorithms . for the parameter @xmath230 in ( [ eq : llrr ] ) , we empirically set it as @xmath231 . to measure the relative errors in the solutions we run ladmpsap 2000 iterations with @xmath232 to obtain the estimated ground truth solution ( @xmath233 ) .",
    "the experiments are run and timed on a notebook computer with an intel core i7 2.00 ghz cpu and 6 gb memory , running windows 7 and matlab 7.13 .",
    "table  [ tab : latlrr ] shows the results of related algorithms .",
    "we can see that ladmps and ladmpsap are faster and more numerically accurate than ladmgb , and ladmpsap is even faster than ladmps thanks to the adaptive penalty .",
    "moreover , naive adm and naive ladm have relatively poorer numerical accuracy , possibly due to converging to wrong solutions .",
    "the numerical accuracy of apg is also worse than those of ladmps and ladmpsap because it only solves an approximate problem by adding the constraint to the objective function as penalty .",
    "note that although we do not require @xmath109 to be bounded , this does not imply that @xmath71 will grow infinitely . as a matter of fact ,",
    "when ladmpsap terminates the final values of @xmath71 are @xmath234 , @xmath235 , and @xmath236 for the three data settings , respectively .",
    "we then test the performance of the above six algorithms on the hopkins155 database @xcite , which consists of 156 sequences , each having 39 to 550 data vectors drawn from two or three motions . for computational efficiency , we preprocess the data by projecting them to be 5-dimensional using pca .",
    "we test all algorithms with @xmath237 , which is the best parameter for lrr on this database @xcite .",
    "table [ tab : latlrr_rel ] shows the results on the hopkins155 database .",
    "we can also see that ladmpsap is faster than other methods in comparison .",
    "in particular , ladmpsap is faster than ladmps , which uses a fixed @xmath68 .",
    "this testify to the advantage of using an adaptive penalty .    [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]      then we consider the pathway analysis problem using the breast cancer gene expression data set  @xcite , which consists of 8141 genes in 295 breast cancer tumors ( 78 metastatic and 217 non - metastatic ) .",
    "we follow  @xcite and use the canonical pathways from msigdb  @xcite to generate the overlapping gene sets , which contains 639 groups of genes , 637 of which involve genes from our study .",
    "the statistics of the 637 gene groups are summarized as follows : the average number of genes in each group is 23.7 , the largest gene group has 213 genes , and 3510 genes appear in these 637 groups with an average appearance frequency of about four .",
    "we follow  @xcite to restrict the analysis to the 3510 genes and balance the data set by using three replicates of each metastasis patient in the training set .",
    "we use model ( [ eq : logit ] ) to select genes , where @xmath238 .",
    "we want to predict whether a tumor is metastatic ( @xmath239 ) or non - metastatic ( @xmath240 ) .",
    "we compare proximal ladmpsap with the active set method , which was adopted in  @xcite , ladm , and ladmpsap . in ladmpsap and proximal ladmpsap , we both set @xmath241 and @xmath242 . for ladm",
    ", we try multiple choices of @xmath68 and choose the one that results in the fastest convergence . in ladm and ladmpsap",
    ", we terminate the inner loop by apg when the norm of gradient of the objective function of the subproblem is less than @xmath243 .",
    "the thresholds for terminating the outer loop are all chosen as @xmath228 and @xmath244 .",
    "for the three ladm based methods , we first solve to select genes .",
    "then we use the selected genes to re - train a traditional logistic regression model and use the model to predict the test samples . as in  @xcite we partition the whole data set into three subsets to do the experiment three times .",
    "each time we select one subset as the test set and the other two as the training set ( i.e. , there are @xmath245 samples for training ) .",
    "it is worth mentioning that  @xcite only kept the 300 genes that are the most correlated with the output in the pre - processing step .",
    "in contrast , we use all the 3510 genes in the training phase .",
    "table [ table - gene ] shows that proximal ladmpsap is more than ten times faster than the active set method used in  @xcite , although it computes with a more than ten times larger training set .",
    "proximal ladmpsap is also much faster than ladm and ladmpsap due to the lack of inner loop to solve subproblems .",
    "the prediction error and the sparseness at the pathway level by proximal ladmpsap is also competitive with those of other methods in comparison .",
    "in this paper , we propose linearized alternating direction method with parallel splitting and adaptive penalty ( ladmpsap ) for efficiently solving linearly constrained multi - block separable convex programs , which are abundant in machine learning .",
    "ladmpsap fully utilizes the properties that the proximal operations of the component objective functions and the projections onto convex sets are easily solvable , which are usually satisfied by machine learning problems , making each of its iterations cheap .",
    "it is also highly parallel , making it appealing for parallel or distributed computing .",
    "numerical experiments testify to the advantages of ladmpsap over other possible first order methods .",
    "although ladmpsap is inherently parallel , when solving the proximal operations of component objective functions we will still face basic numerical algebraic computations . so for particular large scale machine learning problems , it will be interesting to integrate the existing distributed computing techniques ( e.g. , parallel incomplete cholesky factorization  @xcite and caching factorization techniques  @xcite ) with our ladmpsap in order to effectively address the scalability issues .",
    "z. lin is supported by nsfc ( nos .",
    "61272341 , 61231002 , 61121002 ) .",
    "r. liu is supported by nsfc ( no .",
    "61300086 ) , the china postdoctoral science foundation ( no .",
    "2013m530917 ) , the fundamental research funds for the central universities ( no .",
    "dut12rc(3)67 ) and the open project program of the state key lab of cad&cg ( no.a1404 ) , zhejiang university .",
    "z. lin also thanks xiaoming yuan , wotao yin , and edward chang for valuable discussions and htc for financial support .",
    "to prove this theorem , we first have the following lemmas and propositions .    [",
    "lem : kkt]*(kkt condition ) * the kuhn - karush - tucker ( kkt ) condition of problem ( [ eq : model_problem_multivar ] ) is that there exists @xmath107 , such that @xmath246 where @xmath177 is the subgradient of @xmath5 .",
    "the first is the feasibility condition and the second is the duality condition .",
    "such @xmath107 is called a kkt point of problem ( [ eq : model_problem_multivar ] ) .",
    "[ lem : subgradients ] for @xmath247 generated by algorithm  [ alg : ladmpsap - multivar ] , we have that @xmath248 where @xmath249 .",
    "this can be easily proved by checking the optimality conditions of ( [ eq : update_xi ] ) .",
    "[ lem : weak - monotonicity ] for @xmath247 generated by algorithm  [ alg : ladmpsap - multivar ] and a kkt point @xmath107 of problem ( [ eq : model_problem_multivar ] ) , the following inequality holds : @xmath250    this can be deduced by the monotonicity of subgradient mapping  @xcite .",
    "[ lem : basic - identity ] for @xmath247 generated by algorithm  [ alg : ladmpsap - multivar ] and a kkt point @xmath107 of problem ( [ eq : model_problem_multivar ] ) , we have that @xmath251    this can be easily checked .",
    "first , we add ( [ eq : basic_id_line3 ] ) and ( [ eq : basic_id_line5 ] ) to have @xmath252 where we have used ( [ eq : kkt2 ] ) in ( [ eq : basic_id_proof_line3 ] ) .",
    "then we apply the identity @xmath253 to see that ( [ eq : basic_id_line1])-([eq : basic - identity ] ) holds .",
    "[ prop : basic - identity - multivar ] for @xmath247 generated by algorithm  [ alg : ladmpsap - multivar ] and a kkt point @xmath107 of problem ( [ eq : model_problem_multivar ] ) , the following inequality holds : @xmath254    we continue from ( [ eq : basic_id_line5])-([eq : basic - identity ] ) . as @xmath255 ,",
    "we have @xmath256 plugging the above into ( [ eq : basic_id_line5])-([eq : basic - identity ] ) , we have ( [ eq : basic - identity - multivar - line1])-([eq : basic - identity - multivar ] ) .",
    "[ rem : different ] proposition  [ prop : basic - identity - multivar ] shows that the sequence @xmath257 is fejr monotone . proposition  [ prop : basic - identity - multivar ] is different from lemma 1 in supplementary material of  @xcite because for @xmath58 we can not obtain an ( in)equality that is similar to lemma 1 in supplementary material of  @xcite such that each term with minus sign could be made non - positive . such fejr monotone ( in)equalities are the corner stones for proving the convergence of lagrange multiplier based optimization algorithms .",
    "as a result , we can not prove the convergence of the naively generalized ladm for the multi - block case .",
    "then we have the following proposition .",
    "[ prop : converge_multivar ] let @xmath72 , @xmath0 .",
    "if @xmath109 is non - decreasing , @xmath258 , @xmath0 , @xmath247 is generated by algorithm  [ alg : ladmpsap - multivar ] , and @xmath107 is any kkt point of problem ( [ eq : model_problem_multivar ] ) , then    1 .",
    "@xmath259 is nonnegative and non - increasing .",
    "2 .   @xmath260 , @xmath0 , and @xmath261 .",
    "3 .   @xmath262 , @xmath0 .",
    "we divide both sides of ( [ eq : basic - identity - multivar - line1])-([eq : basic - identity - multivar ] ) by @xmath263 to have @xmath264 then by ( [ eq : weak - monotonicity ] ) , @xmath265 and the non - decrement of @xmath109 , we can easily obtain 1 ) .",
    "second , we sum both sides of ( [ eq : basic - identity - multivar - line1])-([eq : basic - identity - multivar ] ) over @xmath266 to have @xmath267 then 2 ) and 3 ) can be easily deduced .",
    "now we are ready to prove theorem  [ thm : converge_multivar ] .",
    "the proof resembles that in @xcite .    *",
    "( of theorem  [ thm : converge_multivar ] ) * by proposition  [ prop : converge_multivar]-1 ) and the boundedness of @xmath109 , @xmath268 is bounded , hence has an accumulation point , say @xmath269 .",
    "we accomplish the proof in two steps .",
    "we first prove that @xmath270 is a kkt point of problem ( [ eq : model_problem_multivar ] ) .",
    "by proposition  [ prop : converge_multivar]-2 ) , @xmath271 so any accumulation point of @xmath272 is a feasible solution .",
    "since @xmath273 , we have @xmath274 let @xmath275 . by observing proposition  [ prop : converge_multivar]-2 ) and",
    "the boundedness of @xmath109 , we have @xmath276 so we conclude that @xmath277 is an optimal solution to ( [ eq : model_problem_multivar ] ) .",
    "again by @xmath273 we have @xmath278 fixing @xmath279 and letting @xmath275 , we see that @xmath280 so @xmath281 , @xmath0 .",
    "thus @xmath282 is a kkt point of problem ( [ eq : model_problem_multivar ] ) .",
    "we next prove that the whole sequence @xmath268 converges to @xmath270 .    by choosing @xmath283 in proposition  [ prop :",
    "converge_multivar ] , we have @xmath284 by proposition  [ prop : converge_multivar]-1 ) , we readily have @xmath285 so @xmath286 .",
    "as @xmath270 can be an arbitrary accumulation point of @xmath268 , we conclude that @xmath268 converge to a kkt point of problem ( [ eq : model_problem_multivar ] ) .",
    "we first have the following proposition .",
    "[ prop : unbounded_beta ] if @xmath109 is non - decreasing and unbounded , @xmath113 and @xmath114 is bounded for @xmath0 , then proposition  [ prop : converge_multivar ] holds and @xmath287    as the conditions here are stricter than those in proposition  [ prop : converge_multivar ] , proposition  [ prop : converge_multivar ] holds .",
    "then we have that @xmath288 is bounded due to proposition  [ prop : converge_multivar]-1 ) .",
    "so @xmath289 is bounded due to @xmath290 .",
    "@xmath291 is also bounded thanks to proposition  [ prop : converge_multivar]-2 ) .",
    "we rewrite lemma  [ lem : subgradients ] as @xmath292 then by the boundedness of @xmath293 , the unboundedness of @xmath109 and proposition  [ prop : converge_multivar]-2 ) , letting @xmath294 , we have that @xmath295 where @xmath296 is any accumulation point of @xmath291 , which is the same as that of @xmath289 due to proposition  [ prop : converge_multivar]-2 ) .",
    "recall that we have assumed that the mapping @xmath9 is onto .",
    "so @xmath297 .",
    "therefore by ( [ eq : subproblem_optimality2 ] ) , @xmath298 .    based on proposition  [ prop :",
    "unbounded_beta ] , we can prove theorem  [ thm : convergence_unbounded ] as follows .    * ( of theorem  [ thm : convergence_unbounded ] ) * when @xmath109 is bounded , the convergence has been proven in theorem 1 . in the following , we only focus on the case that @xmath109 is unbounded .    by proposition  [ prop : converge_multivar]-1 )",
    ", @xmath299 is bounded , hence has at least one accumulation point @xmath300 . by proposition  [ prop : converge_multivar]-2 ) , @xmath300 is a feasible solution .",
    "since @xmath112 and proposition  [ prop : converge_multivar]-3 ) , there exists a subsequence @xmath301 such that @xmath302 as @xmath303 and @xmath177 is bounded , we may assume that @xmath304 it can be easily proven that @xmath305 then letting @xmath306 in ( [ eq : subsequence_to_zero ] ) , we have @xmath307 then by @xmath308 , @xmath309 letting @xmath306 and making use of ( [ eq : equal_zero ] ) , we have @xmath310 so together with the feasibility of @xmath311 we have that @xmath301 converges to an optimal solution @xmath311 to ( [ eq : model_problem_multivar ] ) .    finally , we set @xmath312 and @xmath313 be the corresponding lagrange multiplier @xmath117 in proposition  [ prop : converge_multivar ] . by proposition",
    "[ prop : unbounded_beta ] , we have that @xmath284 by proposition  [ prop : converge_multivar]-1 ) , we readily have @xmath285 so @xmath314 .",
    "* ( of theorem  [ thm : convergence_unbounded_necessary ] ) * we first prove that there exist linear mappings @xmath315 , @xmath0 , such that @xmath315 s are not all zeros and @xmath316 .",
    "indeed , @xmath316 is equivalent to @xmath317 where @xmath10 and @xmath318 are the matrix representations of @xmath7 and @xmath315 , respectively .",
    "( [ eq : linear_mappings_equiv ] ) can be further written as @xmath319 recall that we have assumed that the solution to @xmath320 is non - unique .",
    "so @xmath321 is not full column rank hence ( [ eq : linear_mappings_equiv ] ) has nonzero solutions .",
    "thus there exist @xmath315 s such that they are not all zeros and @xmath316 .    by lemma  [ lem : subgradients ] ,",
    "@xmath322 as @xmath177 is bounded , @xmath0 , so is @xmath323 where @xmath324 and @xmath325 in ( [ eq : cancel_lambda ] ) we have utilized @xmath316 to cancel @xmath326 , whose boundedness is uncertain .",
    "then we have that there exists a constant @xmath327 such that @xmath328    if @xmath329 , then @xmath330 is a cauchy sequence , hence has a limit @xmath331 .",
    "define @xmath332 where @xmath333 is any optimal solution .",
    "then @xmath334 so if @xmath335 is initialized badly such that @xmath336 then @xmath337 , which implies that @xmath338 can not converge to @xmath333 .",
    "note that ( [ eq : bad_init ] ) is possible because @xmath339 is not a zero mapping given the conditions on @xmath315 .",
    "* ( of proposition  [ prop : optimality ] ) * if @xmath125 is optimal , it is easy to check that holds .    since @xmath340",
    ", we have @xmath341 so if ( [ eq : constrained_optimality ] ) holds , we have @xmath342 with ( [ eq : constrained_optimality2 ] ) , we have @xmath343 so ( [ eq : constrained_optimality1 ] ) reduces to @xmath344 . as @xmath125 satisfies the feasibility condition , it is an optimal solution to ( [ eq : model_problem_multivar ] ) .    *",
    "( of theorem  [ thm : convergence_rate_2var ] ) * we first deduce @xmath345 by proposition  [ prop : basic - identity - multivar ] , we have @xmath346 so by lemma  [ lem : subgradients ] and combining the above inequalities , we have @xmath347 here we use the fact that @xmath348 , which is guaranteed by and .",
    "summing the above inequalities from @xmath349 to @xmath45 , and dividing both sides with @xmath350 , we have @xmath351 next , by the convexity of @xmath352 and the squared frobenius norm @xmath353 , we have @xmath354 combining ( [ eq : recursive4-line1])-([eq : recursive4 ] ) and ( [ eq : recursive5-line1])-([eq : recursive5 ] ) , we have @xmath355",
    "we only need to prove the following proposition . then by the same technique for proving theorem  [ thm : converge_multivar ] , we can prove theorem  [ thm : better_eta ] .    [",
    "prop : basic - identity - multivar - equiv ] for @xmath356 generated by algorithm  [ alg : ladmpsap - multivar_equiv ] and a kkt point @xmath357 of problem ( [ eq : model_problem_multivar_equiv ] ) , we have that @xmath358    we continue from ( [ eq : basic - identity - multivar - line7 ] ) : @xmath359 then we can have ( [ eq : basic - identity - multivar - equiv - line1])-([eq : basic - identity - multivar - equiv ] ) .",
    "to prove theorem  [ thm : general_convergence_unbounded ] , we need the following proposition :    [ prop : inequ_para ] for @xmath247 generated by algorithm  [ alg : general - ladmpsap - multivar ] and a kkt point @xmath360 of problem ( [ eq : model_problem_multivar ] ) with @xmath5 described in section  [ sec : g - ladmpsap ] , we have that @xmath361    it can be observed that @xmath362 so we have @xmath363 and @xmath364 on the one hand , @xmath365\\notag\\\\ & & -\\frac{1}{2\\beta_k}\\left(\\|\\lambda^{k+1}-\\lambda\\|^2-\\|\\lambda^{k}-\\lambda\\|^2+\\|\\hat{\\lambda}^{k}-\\lambda^k\\|^2-\\|\\lambda^{k+1}-\\hat{\\lambda}^k\\|^2\\right)\\notag\\\\ & = & \\sum\\limits_{i=1}^n \\left[\\frac{\\tau_i^{(k)}}{2}\\left(\\|{\\mathbf{x}}_i^k-{\\mathbf{x}}_i\\|^2-\\|{\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i\\|^2-\\|{\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i^k\\|^2\\right)+\\frac{l_i}{2}\\|{\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i^k\\|^2\\right]\\notag\\\\ & & -\\frac{1}{2\\beta_k}\\left(\\|\\lambda^{k+1}-\\lambda\\|^2-\\|\\lambda^{k}-\\lambda\\|^2+\\|\\hat{\\lambda}^{k}-\\lambda^k\\|^2-\\beta_k^2{\\left\\|}\\sum\\limits_{i=1}^n{\\mathcal{a}}_i({\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i^k){\\right\\|}^2\\right)\\notag\\\\ & \\leq&\\frac{1}{2}\\sum\\limits_{i=1}^n \\tau_i^{(k)}\\left(\\|{\\mathbf{x}}_i^{k}-{\\mathbf{x}}_i\\|^2-\\|{\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i\\|^2\\right)-\\frac{1}{2}\\sum\\limits_{i=1}^n\\left(\\tau_i^{(k ) } -l_i - n\\beta_k\\|{\\mathcal{a}}_i\\|^2\\right)\\|{\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i^k\\|^2\\notag\\\\ & & + \\frac{1}{2\\beta_k}\\left(\\|\\lambda^{k}-\\lambda\\|^2-\\|\\lambda^{k+1}-\\lambda\\|^2-\\|\\hat\\lambda^{k}-\\lambda^k\\|^2\\right )",
    ". \\label{pop1_line_end}\\end{aligned}\\ ] ] on the other hand ,",
    "@xmath366 so we have @xmath367 let @xmath368 and @xmath369 , we have @xmath370+\\frac{1}{2\\beta_k}\\left(\\|\\lambda^{k}-\\lambda^*\\|^2-\\|\\lambda^{k+1}-\\lambda^*\\|^2\\right)\\notag\\\\ & & -\\frac{1}{2}\\sum\\limits_{i=1}^n\\left(\\tau_i^{(k)}-l_i - n\\beta_k\\|{\\mathcal{a}}_i\\|^2\\right)\\|{\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i^k\\|^2-\\frac{1}{2\\beta_k}\\|\\hat\\lambda^{k}-\\lambda^k\\|^2.\\notag\\end{aligned}\\ ] ]    * ( of theorem  [ thm : general_convergence_unbounded ] ) * as @xmath371 minimizes @xmath372 , we have @xmath373 by proposition [ prop : inequ_para ] , we have @xmath374 dividing both sides by @xmath71 and using @xmath375 , the non - decrement of @xmath71 and the non - increment of @xmath376 , we have @xmath377 it can be easily seen that @xmath378 is bounded , hence has an accumulation point , say @xmath379 .    summing - over @xmath380 , we have @xmath381 so @xmath382 and @xmath383 as @xmath384 .",
    "hence @xmath385 , which means that @xmath386 is a feasible solution .    from ( [ pop1_line1])-([pop1_line_end ] )",
    ", we have @xmath387 let @xmath306 . by the boundedness of @xmath388 we have",
    "@xmath389 together with the feasibility of @xmath300 , we can see that @xmath390 is a kkt point .    by choosing @xmath283",
    "we have @xmath391 using ( [ the_line1])-([the_line2 ] ) , we have @xmath392 so @xmath393 .",
    "* ( of theorem  [ thm : general_convergence_unbounded_rate ] ) * by the definition of @xmath394 and @xmath190 , @xmath395\\hspace*{2cm}\\\\ & \\geq&{\\frac{\\displaystyle \\beta_k}{\\displaystyle   2}}\\left[\\sum\\limits_{i=1}^n\\left(\\eta_i - n\\|{\\mathcal{a}}_i\\|^2\\right)\\|{\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i^k\\|^2+\\frac{1}{\\beta_k^2}\\|\\hat\\lambda^{k}-\\lambda^k\\|^2\\right]\\notag\\\\ & \\geq&{\\frac{\\displaystyle \\alpha\\beta_k}{\\displaystyle   2}}(n+1)\\left(\\sum\\limits_{i=1}^n\\|{\\mathcal{a}}_i\\|^2\\|{\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i^k\\|^2+\\frac{1}{\\beta_k^2}\\|\\hat\\lambda^{k}-\\lambda^k\\|^2\\right)\\notag\\\\ & \\geq&{\\frac{\\displaystyle \\alpha\\beta_k}{\\displaystyle   2}}(n+1)\\left(\\sum\\limits_{i=1}^n\\|{\\mathcal{a}}_i({\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i^k)\\|^2+\\frac{1}{\\beta_k^2}\\|\\hat\\lambda^{k}-\\lambda^k\\|^2\\right)\\notag\\\\ & = & { \\frac{\\displaystyle \\alpha\\beta_k}{\\displaystyle   2}}(n+1)\\left(\\sum\\limits_{i=1}^n\\|{\\mathcal{a}}_i({\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i^k)\\|^2+{\\left\\|}\\sum\\limits_{i=1}^n{\\mathcal{a}}_i({\\mathbf{x}}_i^k)-{\\mathbf{b}}{\\right\\|}^2\\right)\\notag\\\\ & \\geq&{\\frac{\\displaystyle \\alpha\\beta_k}{\\displaystyle   2}}{\\left\\|}\\sum\\limits_{i=1}^n{\\mathcal{a}}_i({\\mathbf{x}}_i^{k+1})-{\\mathbf{b}}{\\right\\|}^2.\\end{aligned}\\ ] ] so by ( [ line_1])-([line_end ] ) and the non - decrement of @xmath71 , we have @xmath396 dividing both sides by @xmath71 and using the non - decrement of @xmath71 and the non - increment of @xmath376 , we have @xmath397\\hspace*{1cm}\\\\ & \\leq & \\frac{1}{2}\\sum\\limits_{i=1}^n \\beta_k^{-1}\\tau_i^{(k)}\\left(\\|{\\mathbf{x}}_i^{k}-{\\mathbf{x}}_i^*\\|^2-\\|{\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i^*\\|^2\\right)+\\frac{1}{2\\beta_k^2}\\left(\\|\\lambda^{k}-\\lambda^*\\|^2-\\|\\lambda^{k+1}-\\lambda^*\\|^2\\right)\\notag\\\\ & \\leq&\\frac{1}{2}\\sum\\limits_{i=1}^n \\left(\\beta_k^{-1}\\tau_i^{(k)}\\|{\\mathbf{x}}_i^{k}-{\\mathbf{x}}_i^*\\|^2-\\beta_{k+1}^{-1}\\tau_i^{(k+1)}\\|{\\mathbf{x}}_i^{k+1}-{\\mathbf{x}}_i^*\\|^2\\right)\\notag\\\\ & & + \\left(\\frac{1}{2\\beta_k^2}\\|\\lambda^{k}-\\lambda^*\\|^2-\\frac{1}{2\\beta_{k+1}^2}\\|\\lambda^{k+1}-\\lambda^*\\|^2\\right).\\end{aligned}\\ ] ] summing over @xmath398 and dividing both sides by @xmath350 , we have @xmath399 using the convexity of @xmath5 and @xmath353 , we have @xmath400 so we have @xmath401        boyd s , parikh n , chu e , peleato b , eckstein j ( 2011 ) distributed optimization and statistical learning via the alternating direction method of multipliers . in : jordan m ( ed ) foundations and trends in machine learning                                                                  subramanian a , tamayo p , mootha v , mukherjee s , et al ( 2005 ) gene set enrichment analysis : a knowledge - based approach for interpreting genome - wide expression profiles .",
    "proceedings of the national academy of sciences 102(43):267288"
  ],
  "abstract_text": [
    "<S> many problems in machine learning and other fields can be ( re)for - mulated as linearly constrained separable convex programs . in most of the cases , </S>",
    "<S> there are multiple blocks of variables . however , the traditional alternating direction method ( adm ) and its linearized version ( ladm , obtained by linearizing the quadratic penalty term ) are for the two - block case and can not be naively generalized to solve the multi - block case . </S>",
    "<S> so there is great demand on extending the adm based methods for the multi - block case . in this paper , we propose ladm with parallel splitting and adaptive penalty ( ladmpsap ) to solve multi - block separable convex programs efficiently . </S>",
    "<S> when all the component objective functions have bounded subgradients , we obtain convergence results that are stronger than those of adm and ladm , e.g. , allowing the penalty parameter to be _ unbounded _ and proving the _ sufficient and necessary conditions _ for global convergence . </S>",
    "<S> we further propose a simple optimality measure and reveal the convergence rate of ladmpsap in an ergodic sense . for programs with extra convex set constraints , with refined parameter estimation </S>",
    "<S> we devise a practical version of ladmpsap for faster convergence . </S>",
    "<S> finally , we generalize ladmpsap to handle programs with more difficult objective functions by linearizing part of the objective function as well . </S>",
    "<S> ladmpsap is particularly suitable for sparse representation and low - rank recovery problems because its subproblems have closed form solutions and the sparsity and low - rankness of the iterates can be preserved during the iteration . </S>",
    "<S> it is also highly parallelizable and hence fits for parallel or distributed computing . </S>",
    "<S> numerical experiments testify to the advantages of ladmpsap in speed and numerical accuracy . </S>"
  ]
}