{
  "article_text": [
    "technology improvements over the last twenty years have allowed scientists to acquire and retain an ever - increasing amount of data from their experiments . this trend has the potential to greatly increase the reliability of experimental results , to allow the detection of physical effects which are extremely weak or rare , and to provide a library of data for testing new results and theories .",
    "an increase in the amount of data can provide a lower statistical uncertainty in results and allow a better evaluation of potential systematic effects .",
    "additionally , an increase in stored data allows scientists to search for physical effects which may occur very rarely , since the chance of seeing the effect is better with many observations .",
    "finally , scientists do not have to save only that data which is most interesting now  data which might be interesting in the future can also be stored .",
    "the rapid decline in data storage costs has prompted many experimental groups to store vastly larger quantities of data than before .",
    "for some types of experiment , the ability to analyze data has not kept pace with the ability to record data .",
    "consider the problem facing particle physicists at the new large hadron collider ( lhc ) : studies of the cms experiment predict that the experiment will generate 1 petabyte of data per year@xcite .",
    "this is an amount of data which large laboratories such as cern and fermilab struggle to manage and store , and which individual universities have no hope of storing locally .",
    "however , the majority of researchers in these experiments will wish to remain at their local universities where they can fulfill their teaching responsibilities and involve undergraduates in the research process .",
    "this challenge is already confronting researchers in the cdf and d  experiments at fermilab , where the tevatron is currently generating large volumes of data with even higher data rates to come .",
    "an alternative to bringing the data to the analysis program at the local university is to carry the analysis program to the data .",
    "previously , this was usually done by having several powerful machines at the experiment site which all researchers could use .",
    "a researcher would compile and run his or her analysis on one of these central machines and the access to the data would occur at high rates from the central store .",
    "this technique works well , but it is hard to scale up to the hundreds or thousands of physicists in today s largest collaborations",
    ". it also ignores the substantial computing resources which are available at other national and university sites around the world .",
    "most experimental collaborations keep archives of the data at a central location but also distribute the data to many worldwide locations , from national laboratories to regional centers to university group machines .",
    "such a data distribution carries many advantages .",
    "one advantage is increased system robustness .",
    "for example , all work does not have to stop for remote users if there is trouble with the network link between the united states and geneva , switzerland , where cern is located .",
    "physicists can simply access the data from another source .",
    "another advantage is the speed of response and scalability . for most requests",
    ", the data should be available locally or regionally , and the central servers will only be burdened by special requests . despite its advantages ,",
    "a wide distribution of data breaks the traditional technique of bringing the program to the data .",
    "now , for many analyses the relevant data is not `` local '' anywhere , but rather is spread over many different locations",
    ". the challenge is figuring out how to break up the program so that it can follow the data in its distribution around the world , while providing debugging and monitoring tools which are easy to use .",
    "we have addressed this challenge by designing an analysis framework which aims to allow a physicist to design a nearly - arbitrary analysis program and have the program executed efficiently and transparently on a large distributed set of data .",
    "we based our design on the needs of a physicist who wants to analyze the data . usually , such a physicist is attempting to achieve a pure sample of some interesting process and either study a feature of the process or simply demonstrate the process s existence .",
    "the analysis contains a set of criteria which select preferentially the events which match the signature of the desired process , generally designated as the signal process , and remove all the other events from data , which are designated as background .",
    "the separation techniques vary from the simple to the complex . sometimes the separation is done by setting upper and/or lower limits on quantities in the events .",
    "other analyses might train or use a neural network , build a multi - dimensional probability distribution , or even implement an entirely new analysis technique which was not conceived of when the framework was designed .",
    "we have designed our framework to support both kinds of analysis as completely as possible .",
    "we have dubbed our project `` blueox '' after paul bunyan s companion , babe the blue ox .",
    "blueox is based on a set of autonomous agents which represent the physics users and which interact with a set of servers to provide execution for physics analysis jobs .",
    "our system is a framework , which means that a physics analysis code is not a complete program but rather plugs into the framework .",
    "the framework is responsible for ensuring that the analysis code is presented with the right data at the right time and that the results are all gathered together correctly .",
    "the role of the blueox framework is to provide a transparent service where an analysis job which requires access to many data sets is broken into many sub - jobs and each runs on a separate server .",
    "the blueox framework is written and based on the java@xmath0 programming language .",
    "we selected java specifically for two major advantages in distributed computing .",
    "first , the java compiler produces machine code which is not specific to any processor or operating system , but rather runs within a `` virtual machine '' ( vm)@xcite . the vm can be easily emulated on any real computer , and compiled `` binaries '' will run without being recompiled .",
    "native code and libraries can be used , through the java - native interface ( jni ) , but such usage limits the portability of the code .",
    "second , the java vm provides a `` sandbox '' for the analysis code .",
    "the server classes run outside the sandbox , so that they have full access to the system . inside the sandbox , analysis code and any classes downloaded from the user s computer will not be able to read or write files , open network connections , or carry out any other dangerous behaviors .",
    "the sandbox will protect the server against both malicious attacks and poorly written analysis code .",
    "the execution of a job proceeds through three distinct phases : discovery , brokering , and execution .",
    "each of these phases is represented by a separate set of abstract interfaces in blueox .",
    "\\1 . * discovery *  in the discovery phase , the agent obtains a list of data sets available on any of the servers on the network .",
    "this list may be a complete list of all data sets or it may be based on a query submitted by the user .",
    "the list of data sets is provided without reference to which server or servers may actually host the data sets .",
    "the discovery interface also allows access to data set descriptor objects , which provide a textual description of the data set , a count of events contained in the data set , and a map of additional parameters defined by the data set .",
    "the agent may be configured to use any number of different discovery implementations .",
    "the implementations written to date are discussed below .",
    "\\2 . * brokering *  in the brokering phase , each data set the user wishes to consider in the analysis is assigned to a server .",
    "the brokering phase begins when the user gives the list of chosen data sets and the analysis code to the agent . as with discovery",
    ", the agent may be configured to use any given implementation of the brokerage interface .",
    "this interface accepts a list of data sets or data set descriptors and returns a list of contracts .",
    "each contract specifies a server and a list of data sets which are assigned to that server for analysis .",
    "\\3 . * job execution *  using the contracts obtained from the brokering phase , the agent splits the job and distributes it to the servers .",
    "the servers can download any required analysis classes from the agent .",
    "the agent is responsible for monitoring the job execution and attempting to re - broker parts of jobs which fail due to a server or network malfunction .",
    "once the job is complete , the agent retrieves the split copies of the job and merges them together before returning the results to the user .",
    "blueox operates through sets of abstract interfaces . for each phase of blueox operation ,",
    "one or more abstract interfaces comes into operation , allowing many different data stores , communication schemes , and user authentication techniques to be used with the framework .",
    "the data handling interfaces allow the blueox server to open and access events from data stores .",
    "each data store may contain any number of data sets , which are identified by unique names .",
    "the dataset abstract interface allows the server to iterate over the events in a data set or to access individual events as identified by sixty - four bit numbers which are required to be unique within the data set .",
    "each data set must be able to produce a datasetdescriptor object which contains the unique name of the data set , the number of events , and a list of descriptive parameters which can contain anything additional the data set wants to provide . in all",
    ", the data handling interfaces define thirteen methods sufficient to integrate a data source into the blueox framework .",
    "examples of data sets which have been interfaced to blueox include the l3 new particle ntuple , the cms hcal testbeam data format ( in root ) , and the high energy electron - positron collidor format used for demonstrations .    in the discovery phase",
    ", the agent determines the available data sets using one more more implementations of the discovery interface .",
    "an implementation can make this determination in many ways , from reading a local file to querying a central remote database or directly contacting servers to request lists of data sets .",
    "this flexibility allows an experiment to use a previously - developed file / run database or to use a standard implementation such as the ldap - based discovery classes provided with blueox . the user can either request all the data sets , or can request those which match a query on data set name or parameters specified in the data set descriptor .",
    "the brokering phase is an extremely important phase in the execution of a job .",
    "brokering assigns a server for each data set which the user requests for the analysis .",
    "brokering is handled by the agent through the brokerage interface .",
    "the brokerage interface accepts a list of data sets and returns a list of contracts specifying the servers to be used and the data sets to handled by each .",
    "the contracts also specify a validity time period for the contract and a level of service , including an amount of cpu horsepower to be made available and an estimated delay before job execution begins .",
    "the contracts are crypographically signed by the issuing authority and the servers are configured to accept only contracts which can be cryptographically verified .",
    "the communication between the agent and the server during job execution is handled through a communicationscheme .",
    "each scheme implements a pair of interfaces , one of which defines access to the server for the client and the other which defines access to the client for the server . from the client side ,",
    "the interface allows the submission of the job , monitoring of the execution , reception of messages and exceptions , and access to the completed job fragment . from the server side ,",
    "the interface allows the server to check the authentication of the user , report messages and exceptions , and return the completed job .",
    "servers can offer multiple communication schemes and the clients can select the scheme to use . for example , a client behind a firewall might have to use a polling protocol , while a directly - connected client might use a protocol allowing a server to directly contact the client .",
    "one important interface which is not used for communication or data management is the mergeable interface .",
    "the agent splits jobs for execution using the standard serialization operations of java .",
    "the mergeable interface supports the reversal of the process , adding together objects to create a merged or summed object . in general , the merge operation is quite easy to define . for histograms",
    ", the merge simply adds each histogram to its matching histogram from the other split objects , bin by bin . for lists ,",
    "merging requires simply concatenation ( and perhaps sorting ) of the lists from the partial analyses . for compound objects ,",
    "each member variable must be merged with its analogue in the other fragments .",
    "every analysis must use mergable objects if it wishes to return results from the analysis . to ease the complexity for the analysis writer",
    ", blueox provides both a group of mergeable utility classes and an automerging facility .",
    "if a class contains only mergeable , static , or transient member variables , it can be merged automatically using the java reflection capabilities . any object tagged with the automergeable interface",
    "is considered to fulfill these requirements and no further work is required by the author of the class to support the merge operation .",
    "the mergeable classes which blueox provides include both container classes ( trees and sets ) and physics - oriented classes including histograms and ntuples . for sets",
    "the union operation is used , while in the maps values with identical keys are merged themselves .",
    "physics analyses depend heavily on histograms and distributions .",
    "blueox does not provide these objects directly , but rather it provides a interface to the standard aida ( abstract interfaces for data analysis ) histograms , functions , and ntuples@xcite .",
    "this interface allows any implementation of aida ( such as jaida@xcite ) to be used with blueox .",
    "the transport of the aida tree is handled through xml , so a different aida implementation can be used on the server and client without difficulty .",
    "the blueox framework , which was first described in prototype form at chep01@xcite , has been rewritten to use abstract interfaces and more thoroughly tested than was done for the previous version .",
    "a `` dummy '' data source was written to allow testing of reasonably large - scale blueox systems in terms of server and client process counts without requiring hundreds of physical machines .",
    "for example , figure  [ fig : test ] shows a test with forty server processes running on four physical machines and one hundred client processes each submitting ten jobs with a random delay between job submissions . the brokering process keeps the load on each server reasonably near to one . in this test ,",
    "the jobs execute much faster ( i.e. in milliseconds ) than a real analysis job would , which reduces somewhat the ability of the servers to load - balance correctly .        for testing and demonstration purposes ,",
    "we have also written a data source to access linear - collider - like data generated by pythia@xcite then smeared and reconstructed into physics objects by a custom program .",
    "this data source allows the study and demonstration of blueox without involving the release of any experiment s data . using the blueox framework",
    ", we developed a simple analysis to isolate the @xmath1 process from background using several analysis techniques including neural networks .",
    "the blueox system is an active object of research and development , as we attempt to improve both the operation of the system and our understanding of distributed analysis .    in the near term , one important center of future study",
    "is the brokering system . in the current default implementation ,",
    "the brokerage uses seed information stored in an ldap server about server performance to determine which servers to contact directly to request contracts .",
    "the agent starts with the server which the seed data indicates would provide the highest horsepower for the largest data set , and it requests a contract from that server .",
    "if the server s offer indicates an available horsepower below that of the next server on the list , the agent will contact that second server , otherwise it will keep the contract and proceed to the next data set . the horsepower offer in the contract",
    "is kept and used as the new estimate of the server s availablity .",
    "this process suffers from several problems , including an `` optimistic agent '' problem .",
    "since the agent uses only data about the server s maximum potential horsepower and not recent load data or user - dependent privileges , the agent may attempt to contact every single server in the system in search of an improved deal which may not exist .",
    "one solution to the problem is to provide the agent with more information about the servers recent load and the amount of horsepower which would be available to a given user .",
    "another approach to control contract - related traffic is for a single computer to be responsible for brokering contacts for a cluster of servers , for example in a farm .    in the longer term ,",
    "our study targets include the data movement which will be required to support an active analysis community . in an active experiment ,",
    "new data and monte carlo are constantly produced , and the focus of analysis may shift through time . for blueox to be most effective , the analysis should be performed using a local cache of the data , if possible . in an dynamic experiment",
    ", the distribution of data among the servers will have to shift over time as new data and monte carlo arrive and analysis groups change interests and priorities .",
    "while blueox is not a data - management system , it can function as an important source of information about what data users are requesting and how well the distribution of data matches the distribution of requests .",
    "9 koen holtman .",
    "`` cms data grid system overview and requirements . ''",
    "cms node 2001/037 .",
    "sun java homepage .",
    "java is a registered trademark of sun corporation .",
    "aida homepage .",
    "http://aida.freehep.org/[http://aida.freehep.org ] .",
    "jaida homepage .",
    "http://java.freehep.org/lib/freehep/doc/aida/index.html[http://java.freehep.org/lib/freehep/doc/aida ] .",
    "jeremiah mans .",
    "`` the blueox companion . ''",
    "jeremiah mans .",
    "`` distributed analysis with java and objectivity . ''",
    "* proceedings of chep01 . * science press , 2001 .",
    "t. sjstrand , p. eden , c. friberg , l. lnnblad , g. miu , s. mrenna and e. norrbin , computer phys .",
    "135 ( 2001 ) 238 ( lu tp 00 - 30 , hep - ph/0010017 ) ."
  ],
  "abstract_text": [
    "<S> high energy physics experiments including those at the tevatron and the upcoming lhc require analysis of large data sets which are best handled by distributed computation . </S>",
    "<S> we present the design and development of a distributed data analysis framework based on java . </S>",
    "<S> analysis jobs run through three phases : discovery of data sets available , brokering / assignment of data sets to analysis servers , and job execution . </S>",
    "<S> each phase is represented by a set of abstract interfaces . </S>",
    "<S> these interfaces allow different techniques to be used without modification to the framework . </S>",
    "<S> for example , the communications interface has been implemented by both a packet protocol and a soap - based scheme . </S>",
    "<S> user authentication can be provided either through simple passwords or through a gsi certificates system . </S>",
    "<S> data from cms hcal testbeams , the l3 lep experiment , and a hypothetical high - energy linear collider experiment have been interfaced with the framework . </S>"
  ]
}