{
  "article_text": [
    "there are many problems in science which are too complex for numerical solution as they stand .",
    "examples include turbulence and other problems where multiple scales must be taken into account .",
    "such problems must be reduced to more amenable forms before one computes . in the present paper",
    "we would like to summarize some reduction methods that have been developed in recent years , together with an account of what was learned in the process .",
    "it is obvious that the problem has not been fully solved , but we think that the examples and the conclusions reached so far are useful .    in general terms , a reduction to a more amenable form is a renormalization group transformation , as in physics  a transformation of a problem into a more tractable form while keeping quantities of interest invariant .",
    "a renormalization group transformation involves an incomplete similarity transformation ( see below for definitions ) , and thus a reduction method is a search for hidden similarities .",
    "this a general feature of reduction methods , and it will be illustrated in the examples .",
    "a successful problem reduction produces a new problem which must in some asymptotic sense be similar to the original problem . for general backgound on renormalization , see e.g.@xcite .    in problems with strong time dependence ,",
    "reduction methods resemble methods for the analysis of thermodynamic systems not in equilibrium ; indeed , those aspects of the problem that are ignored in a reduced description conspire to destroy order and increase entropy .",
    "problem reduction for time - dependent problems is basically renormalization group theory for non - equilibrium statistical mechanics . for background on such theory , see e.g. @xcite .",
    "the content of the paper is as follows : in section [ ave ] we consider hamiltonian systems and their conditional expectations . in section [ block ]",
    "we narrow the discussion to statistically stationary hamiltonian systems and recover kadanoff real - space renormalization groups and an interesting block monte - carlo method . in section [ kdv ]",
    "we display an example that exhibits and also extends the main features of this analysis in simple form .    in section [ mz ]",
    "we explain the mori - zwanzig formalism for the reduction of statistically time - dependent problems .",
    "the analysis shows that averaging the equations is in general not enough ; one must take into account noise and a temporal memory .",
    "the mori - zwazig formalism is rather dense , and in the sections that follow we present various special cases in which it can be simplified , in particular when the memory is very short or very long .    for the sake of readability , we remind the reader of the rudiments of similarity theory @xcite .",
    "suppose a variable @xmath0 is a function of variables @xmath1 , @xmath2 , where @xmath3 have independent units , for example units of length and mass , while the units of @xmath4 , can be formed from the units of @xmath1 .",
    "then there exist dimensionless variables @xmath5 , @xmath6 , @xmath7 , where the @xmath8 are simple fractions , such that @xmath9 is a function of the @xmath10 : @xmath11 this is just a consequence of the requirement that a physical relationship be independent of the size of the units of measurement . at this stage nothing",
    "can be said about the function @xmath12 .",
    "now suppose the variables @xmath13 are small or large , and assume that the function @xmath12 has a non - zero finite limit as its arguments tend to zero or to infinity ; then @xmath14 constant , and one finds a power monomial relation between @xmath0 and the @xmath15 .",
    "this is a complete similarity relation . if the function @xmath12 does not have the assumed limit",
    ", it may happen that for @xmath16 small or large , @xmath17 , where the dots denote lower order terms , @xmath18 is a constant , the other arguments of @xmath12 have been omitted and @xmath19 has a finite non - zero limit .",
    "one can then obtain a scaling ( power monomial ) expression for @xmath0 in terms of the @xmath15 and @xmath20 , with undetermined powers which must be found by means other than dimensional analysis .",
    "the resulting power relation is an _",
    "incomplete _ similarity relation .",
    "of course one may well have functions @xmath12 with neither kind of similarity .",
    "incomplete similarity expresses what is invariant under a renormalization group ; all renormalization group transformations involve incomplete similarity , see the books already cited as well as @xcite written before the notion of incomplete similarity was formalized .",
    "the exponent @xmath18 is called an anomalous exponent .",
    "the paper @xcite is a survey of reduction methods organized along different lines and can be profitably read in tandem with the present paper .",
    "we begin by examining what happens when one tries to reduce the complexity of a hamiltonian system by averaging ( see also @xcite ) .",
    "consider a system of nonlinear ordinary differential equations , @xmath21 where @xmath22 and @xmath23 are @xmath24-dimensional vectors with components @xmath25 and @xmath26 , and @xmath27 is a vector - valued function with components @xmath28 ; @xmath29 is time . to each initial value @xmath23 in ( [ eq : system ] )",
    "corresponds a trajectory @xmath30 .",
    "suppose that we only want to find @xmath31 of the @xmath24 components of the solution vector @xmath32 without finding the @xmath33 others .",
    "one has to assume something about the variables that are not evaluated , and we assume that at time t=0 we have a a joint probability density @xmath34 for all the variables .",
    "the variables we keep will have definite initial values @xmath35 , and the rest of variables will then have a conditional probability density @xmath36 , where @xmath37 is a normalization constant . without some assumption about the missing variables the problem is meaningless ; this particular assumption is reasonable because in practice @xmath38 can often be estimated from previous experience or from general considerations of statistical mechanics .",
    "the question is how to use this prior knowledge in the evaluation of @xmath32 .",
    "partition the vector @xmath23 so that @xmath39 , @xmath40 and @xmath41 , and similarly @xmath42 . in general",
    "the first @xmath31 components of @xmath27 depend on all the components of @xmath43 , @xmath44 ; if they do not we have a system of @xmath31 equations in @xmath31 variables and nothing further needs to be done .",
    "we want to calculate only the variables @xmath45 ; then @xmath46 where the right hand side depends on the variables @xmath47 which are unknown at time @xmath29 .",
    "we shall call the variables @xmath45 the  resolved variables \" and the remaining variables @xmath47 the  unresolved variables \" .",
    "consider in particular a hamiltonian system as in @xcite,@xcite .",
    "there exists then a hamiltonian function @xmath48 such that for @xmath49 odd @xmath28 , the @xmath49-th component of the vector @xmath27 in ( [ eq : system ] ) satisfies @xmath50 while for @xmath49 even one has @xmath51 , with @xmath24 , the size of the system , even .",
    "assume furthermore that @xmath38 , the initial probability density , is @xmath52 where @xmath53 is a parameter , known in physics as the  temperature \" , which will be set equal to one in much , but not all , of the discussion below . in physics",
    "this density appears naturally and is known as the  canonical \" density ; the normalizing constant @xmath54 is the  partition function \" .",
    "this density @xmath38 is invariant , i.e. sampling it and evolving the system in time commute .",
    "a numerical analyst who wants to approximate the solution of an equation usually starts by approximating the equation .",
    "if one solves for the resolved variables one has values for the variables @xmath45 available at each instant @xmath29 and the best approximation should be a function of these variables ; it is natural to seek a best approximation in the mean square sense with respect to the invariant density @xmath38 at each time ; the best approximation in this sense is the conditional expectation @xmath55=\\int e^{-h}d\\tilde\\varphi\\bigl/\\int e^{-h}d\\tilde\\vp$ ] ( note that we set @xmath56 ) .",
    "this conditional expectation is the orthogonal projection of @xmath27 onto the space of functions of @xmath57 with respect to the inner product @xmath58=\\int u(\\varphi)v(\\varphi)f(\\varphi)d\\varphi$ ] , where @xmath59 denotes integration over all the components of @xmath22 .",
    "we then try to approximate the system ( @xmath60 ) by : @xmath61,\\nonumber \\\\",
    "\\hat\\varphi(0)&=&\\hat x. \\label{foop}\\end{aligned}\\ ] ]    we have shown in @xcite that : ( i ) the new system ( [ foop ] ) is also hamiltonian : @xmath62=\\int\\frac{\\ph}{\\partial \\varphi_i}\\exp(-h)d\\tilde\\varphi \\bigl/   \\int \\exp(-h)d\\tilde\\varphi = \\frac{\\partial \\hat h}{\\partial\\varphi_i } , \\label{hald1}\\ ] ] where @xmath63 the dimension of @xmath45 , and @xmath64 is the new hamiltonian .",
    "\\(ii ) the new canonical density @xmath65 is invariant in the evolution of the new , reduced , system .",
    "\\(iii ) when the data are sampled from the canonical distribution , the distribution of @xmath66 in the new system is its marginal distribution in the old system ; equivalently , the partition function @xmath67 is the same for the old system and for the new system .",
    "now the question is , what does the solution @xmath68 of ( [ foop ] ) represent ? it does not approximate the first @xmath31 components of the solution @xmath32 of ( [ eq : system])- the components of @xmath57 and the components of @xmath22 live in spaces of different dimension and in general the components of the latter in those higher @xmath33 dimensions are not small .",
    "one could hope that what the solution of ( [ foop ] ) approximates is the vector @xmath69 $ ] , the best estimate of the first components of the solution at time @xmath29 given the partial initial information @xmath70 .",
    "this is the case for linear systems ( where averaging and time integration commute ) , and is approximately the case for limited time in some other special situations- nearly linear systems , some systems where the ",
    "unresolved variables \" are fast .",
    "however , in general this is not the case .",
    "we shall see below that a reduced description of the solution of nonlinear systems in time requires in general  noise \" and a  memory \" .",
    "the lack of convergence can be understood by the following physics argument . in physics",
    "a system in which the values of all the variables are drawn from a canonical distribution is a system in thermal equilibrium .",
    "the assignment of definite values @xmath71 to the variables @xmath66 at time @xmath72 amounts to taking the system out of equilibrium at @xmath72 ; if the system is ergodic it will then decay to equilibrium in time , so that all the variables become randomized and acquire the joint density @xmath38 .",
    "thus the predictive value of the partial initial data @xmath71 decreases in time ; all averages of the @xmath66 approach equilibrium averages .",
    "however , the reduced system ( [ foop ] ) is hamiltonian , and the solutions it produces oscillate forever .    in figure 1",
    "we consider the hald hamiltonian system ( @xcite ) with @xmath73 ( physically , two linear oscillators with a nonlinear coupling ) .",
    "we assume that @xmath74 are given and sample the two other initial data from the canonical distribution with @xmath56 .    in figure 1",
    "are displayed ( 1 ) the result for @xmath75 of a  galerkin \" calculation in which the unresolved variables are set to zero ( this is what is implicitly done in many unresolved computations ) ; ( 2 ) the result of the averaging procedure just described , and ( 3 ) the true @xmath76 $ ] , calculated by repeatedly sampling the initial data , solving the full system , and averaging .",
    "as one can see , averaging is initially better than the null  galerkin \" method , but in the long run the truth decays but the solution of the averaged system oscillates for ever . for more detail , see @xcite .",
    "the procedure we have just described resembles sufficiently the averaging methods used in some areas of engineering , for example the large - eddy simulation methods in turbulence ( see e.g. @xcite ) and in some multiscale problems ( see e.g. @xcite ) , to cast a very serious doubt on the broad validity of the latter . for a description of special cases , with small fluctuations and particular structures , where this procedure is legitimate , see @xcite .",
    "there is however a case where the construction of the preceding section can be very useful when @xmath77 , i.e. , when one tries to predict the future with no initial information .",
    "equations ( [ eq : system ] ) then sample the canonical distribution and the reduced system samples a subset of variables without sampling the others , and , as we have seen , keep the statistics of the resolved variables unchanged ( see @xcite for an application to molecular dynamics ) .    to see what is happening , suppose the variables @xmath25 are associated with nodes on a regular lattice , for example , they may represent spins in a solid , or originate in the spatial discretization of a partial differential equation .",
    "divide the lattice into blocks of some fixed shape ( for example , divide a regular one - dimensional lattice into groups of two contiguous nodes ) .",
    "we had not yet specified how the variables are to be divided into resolved and unresolved .",
    "now decide to  resolve \" one variable per block , and leave the others in the same block unresolved .",
    "the transformation between the old variables and the smaller set of resolved variables is a kadanoff renormalization group transformation @xcite ; the hamiltonian @xmath78 defined above in equation ( [ renham ] ) is the renormalized hamiltonian .",
    "we will now explain what this means .",
    "suppose the system described by the hamiltonian is translation invariant .",
    "the equations of motion for any at any one point , say at the location labeled by @xmath79 , have the same form as the equations of motion at any any other point .",
    "the relation between the right hand side of the reduced system and the right hand side of the old system can be rewritten as : @xmath80 , \\label{start}\\ ] ] where the expected value is with respect to the invariant density as before .",
    "this relation is the starting point for the actual evaluation of @xmath78 .",
    "hamiltonians are functions of the variables @xmath22 .",
    "they can be expanded in the form : @xmath81 where the @xmath82 are  elementary hamiltonians \" . in a translation invariant system , where each equation has the same form as any other , the hamiltonian is made up of sums over @xmath49 of terms of the form @xmath83 for various values of @xmath84 , where @xmath85 is some function ; these terms represent  couplings \" between variables @xmath84 apart ; one can then choose the elementary hamiltonians to be polynomials in @xmath86 with a fixed @xmath84 in each @xmath82 , i.e. , one segregates the couplings between variables @xmath84 apart into separate terms .    in a homogeneous system where there is only one variable per site",
    "it is enough to satisfy ( [ start ] ) for one variable , say for @xmath75 .",
    "define @xmath87 , noting that though @xmath88 is necessarily a function with at least as many arguments as there are components on @xmath22 , @xmath89 can be sparse .",
    "equation ( [ start ] ) reduces to @xmath90 with the projection @xmath91 defined as before by @xmath92 $ ] for any function @xmath93 of @xmath22 .",
    "now we re almost done .",
    "one can pick a basis in @xmath94 , the subspace of square integrable functions that depend only on the variables @xmath45 , which consists of a subset of the set of functions @xmath89 . the right - hand of equation ( [ more1 ] ) is then a linear combination of @xmath95 ; integration with respect to @xmath75 requires only the erasure of the primes and yields a series for @xmath78 .",
    "the elements of @xmath47 are now gone , and one can relabel the remaining variables @xmath45 so that the terms in the series have exactly the same form as before ; the calculation can then be repeated , yielding a sequence of hamiltonians with ever fewer variables : @xmath96 , @xmath97 . the corresponding densities",
    "@xmath98 can in principle be sampled by any sampling scheme , for example by metropolis sampling ( but there are caveats , see e.g. @xcite ) .    at this point",
    "we have reduced the number of variables by a factor @xmath99 equal to the number of variables in each block , but this may well seem to be a pyrrhic victory .",
    "the hamiltonians one usually encounters are simple , in the sense that they involve few couplings- finite differences typically link a few neighboring variables , and so do the usual spin hamiltonians in physics . as one reduces the number of variables , the new hamiltonians become more complex , with more terms in the series ( [ expandh ] ) ; the cost per time step of solving the equations in time or of the cost per move in a metropolis sampling typically increases fast as well . to see",
    "what has been gained one must turn to the physics literature ( see e.g. @xcite.@xcite ) .",
    "consider the spatial correlation length @xmath100 which measures the range of values of @xmath101 over which the spatial covariances @xmath102 $ ] are non negligible , and the correlation time @xmath103 for which the temporal covariances @xmath104 $ ] are non - negligible .",
    "for very large and very small values of the temperature @xmath53 ( the variance parameter in the density @xmath38 ) both the correlation time and the correlation length are small ; the properties of the system can then be found from calculations with a small number of variables and it is not urgent to reduce the number of variables .",
    "there is a range of intermediate values of @xmath53 for which the correlation length and time for are large and then the reduction is worthwhile .",
    "there often is a value @xmath105 of @xmath53 , the  critical value \" , for which @xmath106 .",
    "values of @xmath53 around @xmath105 are often of great interest .",
    "now we can see what the reduction can accomplish .",
    "if one tries to compute averages with @xmath53 near @xmath105 one finds that the cost of computation is proportional to @xmath103- one has to compute long enough to obtain independent samples of @xmath22 , and a new independent sample will not appear until a time @xmath107 has passed .",
    "the reductions above produce a system with smaller @xmath100 and @xmath103 and therefore computation takes less time . though we started with the declared goal of reducing the number of variables ,",
    "what has been produced is more interesting : a new system with shorter correlations which is more amenable to computation .",
    "it is not the raw number of variables that matters .",
    "the renormalization can be used with a multigrid scheme , in which one runs up and down on different levels of renormalization , on the finer ones to achieve accuracy and the cruder ones to move fast from one macroscopic configuration to another .",
    "a comparison with other multigrid sampling schemes ( see e.g. @xcite ) reveals that we have derived a reasonably standard scheme , with however a particularly effective way to store conditional expectations .",
    "for details see @xcite .",
    "an alternative method for obtaining the expansion coefficients for the renormalized hamiltonians was proposed in @xcite .",
    "the method is based on the maximization of the likelihood of the renormalized density .",
    "the maximization of the likelihood leads to a moment - matching problem .",
    "the moments in this case are the expectation values of the `` elementary hamiltonians '' ( see above ) with respect to the renormalized density .",
    "the solution of the moment matching problem yields the expansion of the renormalized hamiltonian .",
    "the recognition of the links of probability with renormalization is largely due to jona - lasinio ( see e.g. @xcite ) .",
    "the connection of renormalization with incomplete similarity is too well known ( see @xcite ) to require further comment here .",
    "as an illustration of the ideas in the previous section , consider the equation @xmath108 with boundary conditions @xmath109 where the subscripts denote differentiation , @xmath23 is the spatial variable , @xmath29 is time , @xmath110 is a diffusion coefficient , @xmath111 is a dispersion coefficient and @xmath112 is a given constant .",
    "the boundary conditions create a traveling wave solution moving to the right ( towards @xmath113 ) with velocity @xmath114 which becomes steady in a moving framework as @xmath115 . in nondimensional form",
    "the equation can be written as : @xmath116 with @xmath117 , @xmath118 , @xmath119 ; @xmath120 is a  reynolds number \" . for @xmath121",
    "the traveling wave has a monotonic profile , while for @xmath122 the profile is oscillatory , with oscillations whose wave length is of order 1 @xcite . at zero diffusion @xmath123",
    "the stationary asymptotic wave train extends to infinity on the left . for finite @xmath27",
    "the wave train is damped and the solution tends to 1 as @xmath23 decreases .    the steady wave profile can be found by noting that it satisfies an ordinary differential equation , whose solution connects a spiral singularity at @xmath124 to a saddle point at @xmath125 . at the steady state",
    "we average the solution at each point @xmath23 over the region @xmath126 and call the result @xmath127 .",
    "now look for an effective equation @xmath128 whose solution @xmath129 approximates @xmath127 ; @xmath129 can be expected to be smoother than the solution of ( [ kdvb ] ) and thus require fewer mesh points for an accurate numerical solution .",
    "we now make an analogy between the conditional expectations which define the renormalized variables in the previous sections and an averaging in space which defines  renormalized \" variables for solutions of the kdvb equations that are stationary in a moving frame .",
    "averaging over an increasing length scale corresponds either to more renormalization steps or , equivalently , to renormalization with a greater number of variables grouped together .",
    "we pick a class of equations in which to seek the  effective \" equation , the one whose solutions best approximate the averages of the true solution in the mean square sense ; the choice of mean - square approximation in the kdvb case corresponds to the use of @xmath130 norms implied by the use of conditional expectations in the previous sections , and the choice of a class of equations in which to look for the effective equation is analogous to the choice of a basis for the representation of the hamiltonian ; the calculation of the best coefficients in the chosen class of  effective \" equations corresponds to the evaluation of the coefficients in the series for the renormalized hamiltonians . in the hamiltonian case we average the right - hand - sides of the equations and in the analogous kdvb case we attempt to average the solutions ; this must be so because in the kdvb case we do not have theorems which guarantee that averaging the right - hand - sides produces the correct statistics for the solutions .",
    "we can look for an effective equation in the class of equations of the form @xmath131 where @xmath132 are constants and @xmath133 is the velocity of propagation of the steady wave ( see also @xcite ) .",
    "the problem is to find the value of the parameters in the effective equation which minimizes @xmath134 one finds numerically that that the last terms have little effect on the minimum if @xmath135 when @xmath136 ( in the physics terminology , they are  irrelevant \" ) .",
    "the effective equation is thus a burgers equation with a value of the dimensionless diffusion coefficient @xmath137 different from @xmath138 .",
    "the minimization in ( [ min ] ) was carried out in @xcite , and it showed that the mimimun was achieved when @xmath139 , with the exponent @xmath140 . note that when the diffusion coefficient @xmath141 , then @xmath142 .",
    "this is an incomplete similarity relation , as advertised , relating a  bare \" reynolds number @xmath27 to a  dressed \" reynolds @xmath143 .",
    "the form of the effective equation could conceivably have been found by averaging the original equation , but the relation between the original @xmath144 and @xmath137 requires some form of renormalization - like reasoning .",
    "we now return to the problem we started investigating in section [ ave ] : how to determine the evolution of a subset @xmath45 of components of a vector @xmath22 described by a nonlinear set of equations of the form ( [ eq : system ] ) .",
    "this is a nonlinear closure problem of a type much studied in physics , and a variety of formalisms is available for the job .",
    "we choose the mori - zwanzig formalism of irreversible statistical mechanics @xcite , because it homes in on the basic difficulty , which is the description of the memory in the system ; the relation of this formalism to other nonlinear formalisms is described in @xcite .",
    "that a reduced description of a nonlinear system involves a memory should be intuitively obvious : suppose you have @xmath145 billiard balls moving about on top of a table and are trying to describe the motion of just three ; the second ball may strike the seventh ball at a time @xmath146 and the seventh ball may then strike the third ball at a later time .",
    "the third ball then  remembers \" the state of the system at time @xmath146 , and if this memory is not encoded in the explicit knowledge of where the seventh ball is at all times , then it has to be encoded in some other way .",
    "we are no longer assuming that the system is hamiltonian nor that we know an invariant density .",
    "it is much easier to work with linear equations , and we start by finding a linear equation equivalent to ( not approximating ! ) the system ( [ eq : system ] ) . introduce the linear liouville operator @xmath147 , and the liouville equation : @xmath148 with initial data @xmath149 .",
    "this is the partial differential equation for which ( [ eq : system ] ) is the set of characteristic equations .",
    "one can verify that the solution of the liouville equation is @xmath150 ( see e.g @xcite ) .",
    "in particular , if @xmath151 , the solution is @xmath152 the i - th component of the solution of ( [ eq : system ] ) .",
    "this linear partial differential equation is thus equivalent to the nonlinear system ( [ eq : system ] ) .",
    "the linearity of equation ( [ liouville ] ) greatly facilitates the analysis .",
    "introduce the semigroup notation @xmath153 , where @xmath154 is the evolution operator associated with the operator @xmath99 ; therefore @xmath155 , and one can also verify that @xmath156 ( this can be seen to be a change of variables formula ) . equation ( [ liouville ] ) becomes @xmath157 we suppose that as before we are given the initial values of the @xmath31 coordinates @xmath70 , and that the distribution of the remaining @xmath33 coordinates @xmath158 is the conditional density , @xmath38 conditioned by @xmath70 , where @xmath38 is initially given .",
    "we define a projection operator @xmath91 by @xmath159 $ ] .",
    "the conditioning variables are the initial values of @xmath66 ; in section [ ave ] the conditioning variables were the values of @xmath68 , which are unusable here when we do not know the probability density at time @xmath29 .",
    "quantities such as @xmath160 $ ] are by definition the best estimates of the future values of the variables @xmath45 given the partial data @xmath70 and are often the quantities of greatest interest .",
    "consider a resolved coordinate @xmath161 ( @xmath162 ) , and split its time derivative , @xmath163 as follows : @xmath164 where @xmath165 .",
    "define @xmath166 ; the first term is @xmath167 and is a function of the resolved components only ( but it is a function of the whole vector of initial data ) .",
    "note that if @xmath168 were zero we would recover something that looks like the crude approximation of the previous section ; however the conditioning variables are not the same .",
    "we shall see that the term in @xmath168 is essential .",
    "we further split the remaining term @xmath169 .",
    "this splitting will bring it into a very useful form : a noise term , and a memory term whose kernel depends on the correlations of the noise term .",
    "the fact that such a splitting is possible is the essence of  fluctuation - dissipation \" theorems ( see e.g @xcite ) .",
    "let @xmath170 , i.e. , let @xmath171 be a solution of the initial value problem : @xmath172 if for some function h(x ) , @xmath173 then @xmath174 for all time @xmath29 , i.e. , @xmath175 maps the null space of @xmath176 into itself .",
    "the evolution operators @xmath154 and @xmath177 satisfy the duhamel relation @xmath178 hence , @xmath179    collecting terms , we find @xmath180    the first term on the right hand side is the markovian contribution to @xmath181it depends only on the instantaneous value of the resolved @xmath182 .",
    "the second term depends on @xmath23 through the values of @xmath183 at times @xmath184 between @xmath185 and @xmath29 , and embodies a memory  a dependence on the past values of the resolved variables .",
    "finally , the third term , which depends on full knowledge of the initial conditions @xmath23 , lies in the null space of @xmath176 and can be viewed as noise with statistics determined by the initial conditions .",
    "it is important to see that equation ( [ eq : langevin ] ) is an identity .",
    "the memory and noise terms have not been added artificially , their presence is a direct consequence of the original equations of motion . however tempting it may be to average equations by taking one - time averages",
    ", the results will in general be wrong ; one must add a memory and a noise as well .    if what is desired is @xmath186 , the conditional expectation of @xmath187 given @xmath71 ( the best approximation in the sense of @xmath130 to @xmath188 given the partial data @xmath71 )",
    ", then one can premultiply equation ( [ eq : langevin ] ) by p ; the noise term then drops out and we find @xmath189 even if the system we start with is hamiltonian , the langevin equation ( [ eq : langevin ] ) is not ; the memory and the noise allow the system to forget its initial values and decay to  thermal equilibrium \" as it should ( see section [ ave ] ) .",
    "we now show that the memory term is a functional of the temporal correlations of the noise . to save on writing",
    "we restrict ourselves to cases where the operator @xmath99 is skew - symmetric , i.e , @xmath190 , ( remember @xmath58 $ ] ) .",
    "the skew - symmetry holds in particular for hamiltonian systems with canonical data , see @xcite,@xcite ; however , here the the assumption is skew - symmetry is only an excuse to reduce the number of symbols , not a return to the hamiltonian case .",
    "pick an orthonormal basis @xmath191 in the range of @xmath91 , which is the space of functions of @xmath71 ( for example , the @xmath192 could be hermite polynomials in the variables @xmath70 ) .",
    "any function @xmath193 , can be expanded as @xmath194 , and in particular , @xmath195 where a factor @xmath168 has been inserted before the exponentials , harmlessly because the operators that follow it all live in the null space of @xmath91 .",
    "the memory term now becomes @xmath196 in the last identity we used the fact that the parenthesis is independent of time and therefore commutes with the time evolution operator @xmath197 , and also the fact that @xmath198 by definition .",
    "now @xmath199 by the symmetry of @xmath168 and the assumed skew - symmetry of @xmath99 ; each term on the right hand side of equation ( [ expand ] ) is the ensemble average of the product of the value of the stochastic process @xmath200 at time @xmath201 with the value of the stochastic process @xmath202 evaluated at time @xmath203 , i.e. , it is a temporal correlation .",
    "all these stochastic processes are in the range of @xmath168 for all @xmath29 , they are therefore components of the noise . remember that by definition @xmath204 ( a right - hand side in equations ( [ eq : system ] ) ) .",
    "@xmath205 is then an average of the right - hand side of ( [ eq : system ] ) and @xmath206 $ ] is the initial fluctuation in that right - hand side .",
    "the first ,  markovian \" , term in equations ( [ eq : langevin ] ) looks straightforward , but perils lurk there as well . in general @xmath207 in equations ( [ eq : system ] ) is nonlinear , and so is @xmath208 $ ] .",
    "@xmath209 is a nonlinear function of the functions @xmath68 which depends on all the components of @xmath23 , not only on @xmath71",
    ". some way of approximating this function must be found .",
    "if one looks for conditional expectations , one must find a way to commute @xmath91 with a nonlinear function ; for a discussion , see @xcite .",
    "this bullet was dodged in section [ ave ] when the conditioning variables were chosen to be @xmath68 which change in time , but it may be hard to dodge here .",
    "the task now at hand is to extract something usable from these rather cumbersome formulas .",
    "a very detailed presentation of the analysis in this section can be found in @xcite .",
    "we have established a relation between kernels in the memory term and the noise ( the former is made up of covariances of the latter ) .",
    "this is the mathematical content of what are known as  fluctuation - dissipation theorems \" in physics .",
    "however , under some specific restricted circumstances , the relation between noise and memory takes on more intuitively appealing forms , which we now briefly describe . in physics one",
    "often takes a restricted basis in the range of @xmath91 consisting of the coordinate functions @xmath210 ( the components of @xmath211 ) .",
    "the resulting projection is called there the  linear projection \" as if @xmath91 as defined above were not linear .",
    "the use of this projection is appropriate when the amplitude of the functions @xmath212 is small .",
    "one then has @xmath213 for @xmath214",
    ". the correlations in equation ( [ expand ] ) are then simply the temporal correlations of the noise ( not of the full solutions of the system ! ) .",
    "this is known as the fluctuation - dissipation theorem of the second kind .",
    "specialize further to a situation where there is a single resolved variable , say @xmath215 , so that @xmath216 and @xmath217 has a single component .",
    "the mori - zwanzig equation becomes :    @xmath218    or , @xmath219 where we have again inserted a harmless factor @xmath168 in front of @xmath220 , assumed that @xmath99 was skew - symmetric as above , and for the sake of simplicity also assumed @xmath221 ( if the last statement is not true the formulas can be adjusted appropriately ) .",
    "take the inner product of equation ( [ lmz ] ) with @xmath222 , you find : @xmath223 because @xmath224 and hence @xmath225 multiply equation ( [ clmz ] ) by @xmath222 , and remember that @xmath226 you find : @xmath227 you observe that the covariance @xmath228 and the projection of @xmath215 on @xmath222 obey the same homogenous linear integral equation .",
    "this is the fluctuation - dissipation theorem of the first kind , which embodies the onsager principle , according to which spontaneous fluctations in a system decay at the same rate as perturbations imposed by external means , when both are small ( so that the linear projection is adequate ) .",
    "this reasoning can be extended to cases where there are multiple resolved variables , and this is usually done with the added simplifying assumption that @xmath229 when @xmath230 .",
    "we omit the details .",
    "the approximation we shall examine is some detail is : @xmath231 and we will consider under what conditions it is reasonable",
    ". we will find that it is reasonable both when memory is very short and when it is very long .",
    "the fact that the same approximation works for two opposite cases is not a paradox .",
    "the approximation ( [ qlel ] ) states that the orthogonal dynamics operator is very close to the full dynamics operator .",
    "in other words , the orthogonal dynamics , which evolve in a space orthogonal to that of the resolved variables , are insensitive to the coupling between resolved and unresolved variables .",
    "this can happen in particular when the orthogonal dynamics are very fast or when the orthogonal dynamics are very slow .",
    "the ansatz above should work when there is an effective decoupling of the equations for the resolved and unresolved variables .",
    "this raises the question of what determines the range of the memory .",
    "is it possible to have a reduced model with very short or very long memory , depending on how one coarse - grains a particular system at hand ? in @xcite evidence was presented that , fo ! r the kuramoto - sivashinsky equation , the range of the memory of a reduced model can vary dramatically , depending on whether all the unstable modes in the system are resolved or not .",
    "the construction of a reduced model corresponds to renormalization , and the two extreme cases can be interpreted as two fixed points of a renormalization scheme . in which one a reduced model will end up depends on how one renormalizes .",
    "finally , note that the duhamel formula can be used for an iterative solution of the orthogonal dynamics equation .",
    "the term @xmath154 is the zero - th order term of an iterative solution for @xmath232 this construction can be based on the use of feynman diagrams .",
    "first we examine the case when the memory is short , i.e. , when the various terms in the series ( [ expand_fin ] ) vanish for @xmath184 beyond a small value ; see @xcite for a different approach to short - memory reduced model construction and @xcite for comparison with the present short - memory approximation , as well as @xcite and the references therein .    the memory term in the mori - zwanzig equations ( [ eq : langevin ] ) can be rewritten as @xmath233 where the insertion of the extra @xmath234 is harmless . adding and subtracting equal quantities , we find : @xmath235 a taylor series yields : @xmath236 and therefore , using @xmath237 , we find : @xmath238 if @xmath91 is a finite rank projection then @xmath239 where , as before , one can write @xmath240 as @xmath241 when @xmath99 is skew - symmetric . if the correlations @xmath242 and also the correlations @xmath243 are significant only over short times @xmath184 , the approximation ( [ qlel ] ) provides an acceptable approximation without requiring the solution of the orthogonal dynamics equation ( see @xcite for an application to the dimensional reduction of the kuramoto - sivashinsky equation and @xcite for an application to molecular dynamics ) .",
    "the limiting case of the short - memory approximation is when the correlations are delta functions .",
    "there is a large literature on solving equations ( [ eq : langevin ] ) with the assumption of delta function memory ; usually this is done without explicit mention , as if it were an obvious property of stochastic systems- an astonishing state of affairs nearly 40 years after alder and wainwright demonstrated the long memory in a typical physical system @xcite .",
    "all the dynamic ( i.e. , time - dependent ) renormalization group methods we can find depend on this assumption @xcite , and this remark goes a long way towards explaining their relative lack of success in applications .",
    "we will no longer bother making detailed comparisons with this dynamic renormalization literature ; the point of view here is that reduction on the basis of equations ( [ eq : langevin ] ) is the right kind of renormalization , and anything with added drastic assumptions must be justified by appeal to that right kind .",
    "nevertheless , there are important circumstances where the very short memory assumption can be justified , in particular in problems with separation of time scales , where the components of @xmath244 , the unresolved variables , vary on much faster scales than the resolved variables ( see e.g. @xcite,@xcite ) .",
    "one can then set @xmath245 where the prime denotes a derivative , the @xmath246 are independent unit brownian motions , and the @xmath247 constants that must be derived from some prior knowledge .",
    "assume further that the projection @xmath91 is well represented by the physicists  linear \" projection and that the density used to perform the projections is invariant .",
    "the memory term becomes @xmath248 , equations ( [ eq : langevin ] ) become stochastic ordinary differential equations of the usual kind . as usual",
    "( see e.g. @xcite ) , the corresponding probability densities can be found via fokker - planck formalisms ( or kolmogorov equations , in mathematicians language ) .",
    "everything is easier .",
    "there is a big literature on these methods which we recoil from surveying .",
    "it is often the case that the quantities of interest are the components of @xmath249 $ ] , and the corresponding projection @xmath91 is in general poorly approximated by the  linear \" projection .",
    "the formalism above readily extends to more general projections , with more terms in the basis chosen in the range of @xmath91 ( see e.g. @xcite ) , as long as one assumes that the temporal correlations of the new terms are fast decaying functions .",
    "terms that have long correlation times violate the ansatz ( [ qlel ] ) and can hamper rather than enhance accuracy ( see e.g. @xcite ) .",
    "a way to pick the fast decaying terms in the projection of the memory kernel for problems that exhibit separation of time scales was presented in @xcite .",
    "we should note here that projections which include higher than linear terms are at the heart of mode - coupling theory ( see e.g. @xcite ) , which has proved very effective in tackling problems in condensed matter physics .",
    "we examine now the validity of the ansatz @xmath250 for cases with slowly decaying memory .",
    "write the memory term in the mori - zwanzig equation ( [ eq : langevin ] ) as @xmath251 where we have used the commutation of @xmath99 and @xmath252 with @xmath154 and @xmath253 respectively .",
    "at this point , make the approximation ( [ qlel ] ) , which eliminates the @xmath184 dependence of both integrands and we have @xmath254 all that remains of the integration in time is the coefficient @xmath29 .",
    "one can get rid of the noise term by premultiplying equations ( [ eq : langevin ] ) by a projection @xmath176 , as in equation ( [ eq : langevin_pro ] ) , and obtain a reduced non - autonomous set of differential equations .",
    "this approximation was named the @xmath29-model in @xcite ( see @xcite for an application to the dimensional reduction of a nonlinear schrdinger equation ) .",
    "other cases where non - markovian models can be approximated by markovian equations with time - dependent coefficients can be found in @xcite .",
    "we proceed to examine the order of accuracy of this approximation .",
    "we have    @xmath255qlx_jds.\\end{gathered}\\ ] ]    adding and subtracting equal quantities we find    @xmath256,\\ ] ] and a taylor series around @xmath203 gives @xmath257 this implies @xmath258 the @xmath259 error estimate can be put into perspective by examining an alternate derivation of the @xmath29-model .",
    "if we expand the integrand of the memory term of the mori - zwanzig equation around @xmath203 and retain only the leading term , we find @xmath260ds\\\\ & = t e^{tl } plqlx_j + o(t^2).\\end{aligned}\\ ] ] if we retain only the leading term , we do not keep any information about the time evolution of the integrand , which in turn means no information about the evolution of the resolved component and of the coupling to the orthogonal dynamics ( through the term ( @xmath261 ) .",
    "such a drastic approximation is expected to be appropriate in cases where the memory term integrand is slowly decaying , so that information about its initial value is enough .",
    "as an example , consider again the hald model whose hamiltonian is @xmath262 the resulting equations of motion are : @xmath263 suppose one wants to solve only for @xmath264 , with initial data @xmath265 . assume the initial data @xmath266 are sampled from a canonical density with temperature @xmath56 . a quick calculation yields",
    "@xmath267=1/(1+x_1 ^ 2)$ ] .",
    "the advance in time described by the multiplication by @xmath154 requires just the substitution @xmath268 .",
    "if one commutes the nonlinear function evaluation and the conditional averaging , i.e. , writes @xmath269 ( a  mean - field approximation \" ) , and writes furthemore @xmath270 $ ] one finds @xmath271 ; one can calculate @xmath272 for @xmath273 and finally one finds :    @xmath274    the last term represents the damping due to the loss of predictive power of partial data ; the coefficient of the last term increases in time and one may worry that this last term eventually overpowers the equations and leads to some odd behavior .",
    "this is not the case .",
    "indeed , one can prove the following .",
    "if the system one starts from , equation ( [ eq : system ] ) is hamiltonian with hamiltonian @xmath275 , and if the initial data are sampled from an initial canonical density conditioned by partial data @xmath71 , and if @xmath78 is the renormalized hamiltonian ( in the sense of section [ ave ] ) , then @xmath276 , showing that the components of @xmath217 decay as they should . the proof requires a technical assumption ( that the hamiltonian @xmath275 can be written as the sum of a function of @xmath277 and a function of @xmath278 , a condition commonly satisfied ) and we omit it ( see @xcite ) . the reduced system ( [ eq : hald_t ] ) was solved numerically in @xcite with gratifying results .",
    "the @xmath29-model is the zero - th order term in a taylor expansion ( around @xmath203 ) of the integrand of the memory term in ( [ eq : langevin ] ) .",
    "however , nothing prevents us from keeping more terms in this expansion .",
    "let @xmath279 and expand @xmath280 around @xmath203 , i.e. @xmath281 in the case when @xmath91 is the finite - rank projection and the density used to define the projection is invariant , the derivatives of @xmath280 at @xmath203 are equal - time ( static ) correlations . in mode - coupling theory ,",
    "such expressions are known as sum rules .",
    "one can assume a functional form for the memory term integrand around @xmath203 , e.g. a gaussian @xmath282 and use the derivatives of @xmath280 at @xmath203 to estimate @xmath283 ( see @xcite for more on sum rules and mode - coupling theory ) .",
    "there are intermediate cases where the memory is sufficiently long - range for the short - memory approximation to break down , yet not so slowly decaying that the @xmath29-model can give accurate results . at present",
    ", it is not known how to deal effectively with such cases . in a series of papers",
    "@xcite-@xcite we presented special cases and their solutions .",
    "in particular in @xcite we presented a detailed analysis of the hald system .",
    "we showed that the memory decays roughly at the same rate as the solution itself ( this is the general case in the absence of separation of scales ) .",
    "we expanded the various correlation functions at equilibrium ( i.e. , when there are no resolved variables ) in hermite polynomials , evaluated the coefficients in the expansions by monte - carlo once and for all , and then obtained a system of integro - differential approximations to equations ( [ eq : langevin ] ) which we then solved in various cases .",
    "this is a legitimate procedure which may be useful when the same system of equations has to be solved repeatedly .",
    "these calculations do exhibit a salient feature of model reduction in time - dependent problems , which is that its set - up costs are often very high . the future remedy ,",
    "if there is one , will surely lie in a deeper understanding of dynamical renormalization and in particular of the way memory depends on scale .",
    "we would like to thank prof .",
    "barenblatt , prof .",
    "o. hald and prof .",
    "r. kupferman for many helpful discussions and comments .",
    "this work was supported in part by the national science foundation under grant dms 04 - 32710 , and by the director , office of science , computational and technology research , u.s .",
    "department of energy under contract no .",
    "de - ac03 - 76sf000098 .",
    "g. benettin , c. di castro , g. jona - lasinio , l. peliti and a. stella , on the equivalence of different renormalization groups , in `` new developements in quantum theory and statistical mechanics '' , cargese conf .",
    "physics , m. levy and p. mitter ( eds ) , springer , ny , ( 1976 ) .",
    "l. chen , p. debenedetti , c. gear and i. kevrekidis , from molecular dynamics to coarse self - similar solutions : a simple example using equation - free computation , j. non - newt .",
    "fluid . mech .",
    "( 2004 ) , 120 , 215 ."
  ],
  "abstract_text": [
    "<S> methods for the reduction of the complexity of computational problems are presented , as well as their connections to renormalization , scaling , and irreversible statistical mechanics . </S>",
    "<S> several statistically stationary cases are analyzed ; for time dependent problem averaging usually fails , and averaged equations must be augmented by appropriate memory and random forcing terms . </S>",
    "<S> approximations are described and examples are given .    * </S>",
    "<S> problem reduction , renormalization , and memory *    * alexandre j.  chorin and panagiotis stinis *    department of mathematics , university of california    and    lawrence berkeley national laboratory    berkeley , ca 94720 </S>"
  ]
}