{
  "article_text": [
    "let @xmath3 , with @xmath4 being an unknown parameter , be a family of density functions .",
    "sampling under selection bias involves observations being drawn not from @xmath3 directly , but rather from a distribution which is a biased version of @xmath3 , given by the density function @xmath5 where the @xmath6 is the weight function .",
    "we observe a sample @xmath7 , independently taken from @xmath8 .",
    "in particular , when the weight function is linear ; i.e. @xmath9 , the samples are known as length biased .",
    "there are many situations where weighted data arise ; for example , in survival analysis ( asgharian et al . , 2002 ) ; quality control problems for estimating fiber length distributions ( cox , 1969 ) ; models with clustered or over  dispersed data ( efron , 1986 ) ; visibility bias in aerial data ; sampling from queues or telephone networks . for",
    "further examples of length biased sampling see , for example , patil and rao ( 1978 ) and patil ( 2002 ) .    in the nonparametric setting @xmath3",
    "is replaced by the more general @xmath10 , so the likelihood function for @xmath11 data points becomes , @xmath12 a classical nonparametric maximum likelihood estimator ( npmle ) for @xmath13 ( the disribution function corresponding to @xmath14 ) exists for this problem and is discrete , with atoms located at the observed data points .",
    "in particular , vardi ( 1982 ) finds an explicit form for the estimator in the presence of two independent samples , one from @xmath14 and the other from the length biased density @xmath15 .",
    "our work focuses on length biased sampling and from the bayesian nonparametric setting we work in , the aim is to obtain a density estimator for @xmath14 .",
    "there has been no work done on this problem in the bayesian nonparametric framework due to the issue of the intractable likelihood function , particularly when @xmath14 is modeled nonparametrically using , for example , the mixture of dirichlet process ( mdp ) model ; see lo ( 1984 ) and escobar and west ( 1995 ) . while some ideas do exist on how to deal with intractable normalizing constants ; see murray et al .",
    "( 2006 ) ; tokdar , ( 2007 ) ; adams et al .",
    "( 2009 ) ; and walker , ( 2011 ) , these ideas fail here for two reasons : the infinite dimensional model and the unbounded @xmath9 when the space of observations is the positive reals .",
    "we by - pass the intractable normalizing constant by modeling @xmath15 nonparametrically .",
    "we argue that modeling @xmath14 or @xmath15 nonparametrically is providing the same flexibility to either ; i.e. modeling @xmath16 nonparametrically and defining @xmath17 is essentially equivalent to modeling @xmath18 nonparametrically and defining @xmath19 .",
    "we adopt the latter style , obtain samples from the predictive density of @xmath15 and then  convert \" these samples from @xmath15 into samples from @xmath14 , which forms the basis of the density estimator of @xmath14 .",
    "the layout of the paper is as follows : in section 2 we provide supporting theory for the model idea which avoids the need to deal with the intractable likelihood function .",
    "section 3 describes the model and the mcmc algorithm for estimating it and section 4 describes some numerical illustrations . in section 5",
    "are the concluding remarks and in section 6 asymptotic results are provided .",
    "our aim is to avoid computing the intractable normalizing constant .",
    "the strategy for that would be to model the density @xmath18 directly and then make inference about @xmath10 by exploiting the fact that @xmath20    in the parametric case if a family @xmath3 is known then so is @xmath21 , except its normalizing constant may not be tractable .",
    "there is a reluctance to avoid the problem of the normalizing constant in the parametric case by modeling the data directly with a tractable @xmath21 since the incorrect model would be employed .",
    "however , in the nonparametric setting it is not regarded as relevant whether one models @xmath22 or @xmath8 directly .",
    "a clear motivation to model @xmath8 directly is that this is where the data are coming from .    for a general weight function @xmath23 , an essential condition to model @xmath13 through @xmath24 ( @xmath13 and",
    "@xmath24 denote the corresponding distribution functions of @xmath14 and @xmath15 , respectively ) is the finiteness of @xmath25 .",
    "this , through invertibility , enables us to reconstruct @xmath13 from @xmath24 and occurs when @xmath13 is absolutely continuous with respect to @xmath24 , with the radon - nikodym derivative being proportional to @xmath26 .    for absolute continuity to hold we need that @xmath6 in the support of @xmath13 ie @xmath27 . in the length biased case examined here @xmath28 and the densities have support on the positive real line , so this condition is automatically satisfied . a case , for instance ,",
    "when this does not hold and invertibility fails is in a truncated model where @xmath29 , @xmath30 is a borel set and @xmath13 is a distribution which could be positive outside of @xmath30 .    a bayesian model is thus constructed by assigning an appropriate nonparametric prior distribution to @xmath15 , provided that @xmath31 this in turn specifies a prior for @xmath14 .",
    "the question that now arises is how the posterior structures obtained after modelling @xmath15 directly can be converted to posterior structures from @xmath14 .",
    "the first step in this process would be to devise a method to convert a biased sample from a density @xmath15 to one from its debiased version @xmath14 .",
    "this algorithm is then incorporated to our model building process so that posterior inference becomes possible .    specifically , assume that a sample @xmath32 , comes from a biased density @xmath15 .",
    "this can be converted into a sample from @xmath33 using a metropolis ",
    "hastings algorithm .",
    "if we denote the current sample from @xmath10 as @xmath34 , then @xmath35 otherwise @xmath36 . here , we have the transition density for this process as @xmath37 where @xmath38 this transition density satisfies detailed balance with respect to @xmath10 since @xmath39 and thus the transition density has stationary density given by @xmath10 .",
    "this algorithm was first tested on a toy example , i.e. @xmath18 is ga@xmath40 so that @xmath10 is ga@xmath41 .",
    "a sample of @xmath42 of the @xmath43 was taken independently from the @xmath8 and the metropolis algorithm run to generate the @xmath44 , starting with @xmath45 .",
    "sample values for the sequence of @xmath44 yield @xmath46 which are compatible outcomes with the @xmath44 sample coming from @xmath10 . a similar example will be elaborated on in the numerical illustration section .",
    "applying this idea to our model would amount to turning a sample from the biased posterior predictive density to an unbiased one using a mh step .",
    "an outline of the inferential methodology is now described .    1 .",
    "once data @xmath47 from a biased distribution @xmath15 become avalaible a model for @xmath15 is assumed and a nonparametric prior is assigned .",
    "2 .   using mcmc methods , after a sensible burn - in period , at each iteration , posterior values of the random measure @xmath48 and the relevant parameters are obtained .",
    "subsequently , conditionally on those values , a sequence @xmath49 , from the posterior predictive density @xmath50 is generated .",
    "3 .   the @xmath51 will then form a sequence of proposal values of a metropolis - hastings chain with stationary density the debiased version of the posterior predictive , i.e. @xmath52 . specifically , at the @xmath53-th iteration of the algorithm applying a rejection criterion a value @xmath54 is generated such that @xmath55 with probability @xmath56 , otherwise @xmath57 .",
    "these @xmath58 values form a sample from the posterior predictive of @xmath14 .",
    "we want the model for @xmath18 to have large support and the standard bayesian nonparametric idea for achieving this is based on infinite mixture models ( lo , 1984 ) of the type @xmath59 where @xmath60 is a discrete probability measure and @xmath61 is a density on @xmath62 for all @xmath63 .",
    "since we require @xmath18 to be such that @xmath64 or , equivalently , for a kernel @xmath61 @xmath65 we find it most appropriate to take the kernel to be a log  normal distribution .",
    "so , assuming a constant precision parameter @xmath66 for each component , we have @xmath67 where @xmath60 is a discrete random probability measure defined in @xmath68 and @xmath69 , where @xmath70 denotes the dirichlet process ( ferguson , 1973 ) with precision parameter @xmath71 and base measure @xmath72 . interpreting the parameters",
    ", we have that @xmath73 , and @xmath74 for appropriate sets @xmath75 .",
    "this dirichlet process mixture model implies the hierarchical model for @xmath76 : for @xmath77 @xmath78 to complete the model we choose @xmath79 ga@xmath80 and for the base measure , @xmath72 is @xmath81 .",
    "a useful representation of the dirichlet process , introduced by sethuraman and tiwari ( 1982 ) and sethuraman ( 1994 ) , is the stick  breaking constructive representation given by @xmath82 where the @xmath83 are i.i.d . from @xmath72 ,",
    "i.e. @xmath81 . the @xmath84 are constructed via a stick  breaking process ; so that @xmath85 and , for @xmath86 , @xmath87 where the @xmath88 are i.i.d . from the beta@xmath89 distribution , for some @xmath71 , and @xmath90 almost surely .",
    "let @xmath91 and @xmath92 ; then we can then write @xmath93 this is a standard bayesian nonparametric model .",
    "the mcmc algorithm is implemented using latent variable techniques , despite the infinite dimensional model .",
    "the basis of this sampler is in walker ( 2007 ) and kalli et al .",
    "( 2011 ) .    for @xmath77",
    "we introduce latent variables @xmath94 which make the sum finite .",
    "the @xmath94 augmented density then becomes , @xmath95 this has a finite representation and @xmath96 denotes the almost surely finite @xmath94 slice set @xmath97 .",
    "now we introduce latent variables @xmath98 which allocate the component that @xmath99 are sampled from . conditionally on the weights @xmath23 these are sampled independently with @xmath100 .",
    "hence , we consider the @xmath101 augmented random density @xmath102 therefore , the complete data likelihood based on a sample of size @xmath11 is seen to be @xmath103 this will form the basis of our gibbs sampler . at each iteration we sample from the associated full conditional densities of the following variables : @xmath104 where @xmath105 is a random variable , such that @xmath106 , and @xmath107 almost surely .",
    "these distributions are , by now , quite standard so we proceed directly to the last two steps of the algorithm .",
    "the upshot is that after a sensible burn  in time period given the current selection of parameters , at each iteration , we can sample values @xmath108 from the posterior predictive density @xmath50 and subsequently , using a metropolis step , draw a @xmath109 value from its debiased version @xmath110 .",
    "* once stationarity is reached then at each iteration we have points generated by the posterior measure of the variables . these points are represented by @xmath111 given @xmath112 a value @xmath113 is generated .",
    "this is done by sampling a @xmath114 uniformly in the unit interval and then take @xmath115 if @xmath116 or @xmath117 if @xmath118 the appropriate @xmath119 is then assigned , with probability @xmath120 . even though we have not sampled all the weights , if we  run out \" of weights , in essence the indices",
    "\\{1,  ,n } , we merely draw a @xmath121 from @xmath122 . finally , the predictive value @xmath108 comes from @xmath123 . *",
    "the metropolis step for the posterior predictive of @xmath14 : let @xmath124 be the state of the chain from the previous gibbs iteration . accept the sample @xmath108 , from the @xmath15-predictive , as coming from the @xmath14-predictive , that is @xmath125 , with probability @xmath126 ; otherwise the chain remains in its current state i.e. @xmath127 .",
    "we illustrate the model with two simulated data sets and a real data example . in each of the assumed models , for a given realisation @xmath47",
    ", we report on the results and compare them with the following density estimators :    * the classical kernel density estimate given by @xmath128 * the kernel density estimate for indirect data , see jones ( 1991 ) , is given by @xmath129 where @xmath130 is the harmonic mean of @xmath47 .",
    "here @xmath131 is the bandwidth and in all cases an estimate of it has been calculated as the average of the plug  in and solve  the  equation versions of it , ( sheather and jones @xmath132 ) .",
    "the gibbs sampler iterates @xmath133 times with a burn  in period of @xmath134 .      here",
    "we use non informative prior specifications : @xmath135 the value of the concentration parameter has been set to @xmath136 .    * example 1 . * the length biased distribution is @xmath137 and we simulate @xmath138 of size @xmath139 .",
    "the following results are presented figure 1 :    * 1(a ) : ( i ) a histogram of the simulated length biased data set @xmath140 , ii ) the true biased density ga@xmath141 ( the solid line ) and iii ) the kernel density estimate @xmath142 ( the dashed line ) . *",
    "1(b ) : ( i ) a histogram of a sample from the posterior predictive density @xmath143 , ( ii ) the true biased density ga@xmath141 ( the solid line ) and iii ) the kernel density estimate @xmath142 ( the dashed line ) . *",
    "1(c ) : ( i ) a histogram of the debiased data associated with the application of the metropolis step , ii ) the true debiased density @xmath144 ( the solid line ) and iii ) jones kernel density estimate @xmath145 ( the dashed line ) .    for both estimators",
    "@xmath142 and @xmath146 the bandwidth parameter is set at @xmath147 .",
    "the average number of clusters @xmath148 is @xmath149 . as it can be seen from the graphs we are hitting the right distributions with the metropolis step .",
    "* example 2 . * here the length biased distribution is the mixture @xmath150 we simulate a sample @xmath151 of size @xmath152 . similar results , as in the first example , are shown in figure @xmath153 , ( a)(c ) . for both estimators @xmath154 and @xmath155 the bandwidth parameter has been calculated to @xmath156 . for the average number of clusters",
    ", we estimate @xmath157 . it is noted that the metropolis sampler produces samples that are very close to the debiased mixture @xmath158 depicted with a solid line in @xmath153(c ) .",
    "the data can be found in muttlak and mcdonald ( 1990 ) and consist of @xmath159 , @xmath160 , measurements representing the widths of shrubs obtained by line ",
    "transect sampling . in this sampling method",
    "the probability of inclusion in the sample is proportional to the width of the shrub making it a case of length biased sampling .",
    "a noninformative estimation is shown in figure @xmath161 ( a)-(c ) with the same specifications as in ( [ noninformative ] ) while in @xmath161(d ) , 3(e ) we perform a highly informative estimation with @xmath162 .",
    "the following results are presented in figures @xmath161 and @xmath163 :    * @xmath161(a ) , @xmath161(b ) : histograms of the length biased data set @xmath164 and of a sample from the posterior predictive @xmath165 , respectively . in both subfigures",
    "the associated classical estimator @xmath166 is depicted with a dashed line , for @xmath167 .",
    "* @xmath161(c ) : a histogram of the debiased data associated with the metropolis chain estimator .",
    "jones estimator @xmath168 is shown in dashed line , for the same bandwidth value .",
    "* @xmath161(d ) , @xmath161(e ) : histograms of the posterior predictive and the metropolis sample , respectively , under the highly informative prior @xmath162 , with superimposed classical density estimators . * @xmath163(a ) : the running acceptance rate of the metropolis with jump distribution the posterior predictive values from @xmath165 with an estimated value of about @xmath169 .",
    "* @xmath163(b ) , @xmath163(c ) : running averages of the predictive and metropolis samples respectively .",
    "finally , in figure 5 we provide the autocorrelation function as a function of lag , among the values of the posterior predictive sample for the synthetic and real data sets , after a reasonable burn - in period .",
    "* estimation for the simulated data is nearly perfect and we get the best results for @xmath170 . as it is evident from subfigure @xmath171(c ) , for the @xmath144 , the estimator @xmath172 does not properly capture the distributional features near the origin .",
    "the same holds true for the debiased mixture density @xmath173 , subfigure @xmath153(c ) .",
    "* for the real data set the @xmath174 prior gives again the best results .",
    "such a prior gives the largest average number of clusters among all noninformative specifications that were examined .",
    "the debiased @xmath14 density is close to @xmath175 though not exactly the same .",
    "the difference comes from a small area where the biased data have the group of observations @xmath176 that causes @xmath175 to produce an intense second mode . excluding these @xmath161 data points",
    "jones estimator @xmath175 becomes identical with ours .",
    "* the highly informative specification @xmath177 increases the average number of clusters from @xmath178 ( noninformative estimation ) to about @xmath179 , thus the appearance of a second mode between @xmath180 and @xmath181 , in @xmath161(d ) . from our numerical experiments",
    "it seems that @xmath175 is `` data hunting '' in the sense that it overestimates data sets and produces spurious modes .",
    "our method performs better as it does not tend to overestimate , and at the same time has better properties near the origin .",
    "* when informative prior specifications are used they increase the average number of realized clusters and the nonparametric estimates tend to look more like jones type estimates .",
    "for example choices of @xmath66 priors like ga@xmath182 with @xmath183 increase considerably the average number of clusters and our real data estimates in subfigures @xmath161(d ) and 3(e ) become nearly identical to @xmath175 .",
    "in this paper we have described a novel approach to the bayesian nonparametric modeling of a length bias sampling model .",
    "we directly tackle the length bias sampling distribution , from where the data arise , and this technique avoids the impossible situation of the normalizing constant if one decides to model the density of interest directly .",
    "this is legitimate modeling since only mild assumptions are made on both densities , so we are free to model @xmath15 directly and choose an appropriate kernel with the only condition that @xmath184 .    in a parametric set - up since",
    "@xmath14 is known up to a parameter @xmath63 modeling @xmath15 directly is not recommended , since to avoid a normalizing constant problem a model for @xmath15 would not result from the correct family for @xmath14 .",
    "we have also as part of the solution presented a metropolis step to  turn \" the samples from @xmath8 into samples from @xmath22 .",
    "a rejection sampler here would not work as the @xmath185 is unbounded .",
    "the method we have proposed here should also be applicable to an arbitrary weight function @xmath186 , whereby samples are obtained from @xmath18 and yet interest focuses on the density function @xmath16 , where the connection is provided by @xmath187    our estimator , besides being the first bayesian kernel density estimator for length biased data , it was demonstrated that it performs at least as well and in some cases even better than its frequentist counterpart .",
    "in this section we assume that the posterior predictive sequence @xmath188 is consistent in the sense that @xmath189 a.s . as @xmath190 , where @xmath191 is the true density function generating the data and",
    "@xmath192 denotes the @xmath193 distance .",
    "this would be a standard result in bayesian nonparametric consistency involving mixture of dirichlet process models : see , for example , lijoi et al .",
    "( 2005 ) , where sufficient conditions for the @xmath193 consistency are given .",
    "the following theorem establishes a similar consistency result for the debiased density .",
    "* theorem . *",
    "let @xmath194 and @xmath195 denote the sequence of posterior predictive estimates for the debiased density and the true debiased density , respectively .",
    "then , @xmath196 a.s .",
    "* let @xmath197 where @xmath198 is the posterior expectation of @xmath199 , and for some @xmath72 it is that @xmath200 the assumption of consistency also implies that @xmath198 converges weakly to @xmath72 with probability one .",
    "this means for any continuous and bounded function @xmath201 of @xmath202 we have the a.s .",
    "weak consistency of @xmath198 implies @xmath203 and note that @xmath204    we now aim to show that these results imply the a.s .",
    "@xmath193 convergence of @xmath205 to @xmath206 .",
    "to this end , if we construct the prior so that for some constants @xmath207 and @xmath208 it is that @xmath209 and @xmath210 , assuming @xmath72 puts all the mass on @xmath211\\times ( \\underline{\\sigma}^2,\\overline{\\sigma}^{2})$ ] , then from the definition of weak convergence we have that , with probability one , @xmath212    also , with the conditions on @xmath202 , we have @xmath213 is a bounded and continuous function of @xmath202 for all @xmath214 .",
    "hence @xmath215    pointwise for all @xmath214 .",
    "consequently , by scheff s theorem , we have @xmath216 now @xmath217 and so @xmath218 as required .",
    "adams , r. p. , murray , i. and mackay , d.j.c . the gaussian process density sampler . _ advances in neural information processing systems ( nips ) _ * 21*(2009 ) .",
    "lijoi , a. , pruenster , i. and walker , s.g .",
    "( 2005 ) . on consistency of nonparametric normal mixtures for bayesian density estimation .",
    "_ journal of the american statistical association _ * 100 * , 12921296(2005 ) ."
  ],
  "abstract_text": [
    "<S> a density estimation method in a bayesian nonparametric framework is presented when recorded data are not coming directly from the distribution of interest , but from a length biased version . from a bayesian perspective , efforts to computationally evaluate posterior quantities conditionally on length biased data were hindered by the inability to circumvent the problem of a normalizing constant . in this paper </S>",
    "<S> we present a novel bayesian nonparametric approach to the length bias sampling problem which circumvents the issue of the normalizing constant . </S>",
    "<S> numerical illustrations as well as a real data example are presented and the estimator is compared against its frequentist counterpart , the kernel density estimator for indirect data of jones ( 1991 ) .    _ </S>",
    "<S> keywords : _ bayesian nonparametric inference ; length biased sampling ; metropolis algorithm .    * bayesian nonparametric density estimation under length bias *   +   +    @xmath0 department of mathematics , university of the aegean ,    karlovassi , samos , gr-832 00 , greece .    </S>",
    "<S> @xmath1 department of economics , national and kapodistrian university of athens ,    athens , gr-105 59 , greece .    </S>",
    "<S> @xmath2department of mathematics , university of texas at austin ,    austin , texas 7812 , usa . </S>"
  ]
}