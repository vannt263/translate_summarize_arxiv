{
  "article_text": [
    "in this article we consider prediction problems that can be seen as classification problems on graphs .",
    "these arise in several applied settings , for instance in the prediction of the biological function of a protein in a protein - protein interaction graph ( e.g. @xcite ) , or in graph - based semi - supervised learning ( e.g.  @xcite ) .",
    "we have problems in mind in which the graph is given by the application context and the graph has vertices of different types , coded by vertex labels that can have two possible values , say .",
    "the available data are noisy observations of some of the labels .",
    "the goal of the statistical procedure is to classify the vertices correctly , including those for which there is no observation available .",
    "the idea is that typically , the location of a given vertex in the graph , in combination with ( noisy ) information about the labels of vertices close to it , should have predictive power for the label of the vertex of interest .",
    "hence , successful prediction of labels should be possible to some degree .",
    "several approaches for graph - based prediction have been considered in the literature . in this paper",
    "we investigate a nonparametric full bayes procedure that was recently considered in @xcite and that has so far only been studied theoretically .",
    "concretely , we consider a connected , simple graph @xmath0 , with @xmath1 vertices which we denote simply by @xmath2 .",
    "we formalize the notion of nonparametric prediction in this setting by postulating the existence of a _ soft label function _ @xmath3 that determines the hard labels @xmath4 that we ( partly ) observe . the label @xmath5 corresponding to vertex @xmath6 is a bernoulli random variable with @xmath7 and the @xmath5 s are assumed to be independent .",
    "the underlying idea is that the `` real '' , hard labels @xmath8 of the vertices are obtained by thresholding the soft labels at level @xmath9 , i.e.  @xmath10 .",
    "the @xmath5 are noisy versions of the real hard labels @xmath8 , in the sense that they can be wrong with some positive probability .",
    "specifically , it holds in this setup that @xmath11 . the bayesian approach proposed in @xcite",
    "consists in endowing the soft label function @xmath12 with a prior distribution and determining the corresponding posterior .",
    "the priors we consider are described in detail in the next section .",
    "the posterior distribution for @xmath12 that results from a bayesian analysis can be used for instance for prediction . for the priors",
    "we consider , the computation of the posterior mode is closely related to the computation of a kernel - based regression estimate with kernel based on the laplacian matrix associated with the graph ( e.g. @xcite ) . in that sense",
    "the method we consider is close to those in the cited papers .",
    "a benefit of the full bayesian framework is that the spread of the posterior may be used to produce a quantification of the uncertainty in the predictions .",
    "moreover , we specifically consider a full , hierarchical bayes procedure because of its capability , when properly designed at least , to automatically let the data determine the appropriate value of crucial tuning parameters .",
    "as is well known , the choice of bandwidths , smoothness , or regularization parameters in nonparametric methods is a delicate issue in general .",
    "the graph context is no exception in this regard and it is recognized that it would be beneficial to have a better understanding of how to choose the regularization parameters ( e.g. @xcite ) .",
    "the theoretical results in @xcite indicate how the performance of nonparametric bayesian prediction on graphs depends on both the geometry of the underlying graph and on the `` smoothness '' of the ( unobserved ) soft label function @xmath12 .",
    "moreover , for a certain family of gaussian process priors @xcite gives guidelines for choosing the hyperparameters in such a way that asymptotically optimal performance is obtained .",
    "the aim of the present paper is to provide an implementation of nonparametric bayesian prediction on graphs using gaussian priors based on the laplacian on the graph .",
    "moreover , we investigate numerically the influence of the geometry of the graph and the smoothness of the soft label function , motivated by the asymptotics given in @xcite . in this manner",
    "we arrive at recommended choices for tuning parameters and hyper priors that are in line with the theoretical guarantees and that are also computationally convenient .",
    "the rest of this paper is organized as follows . in the next section a more precise description of the problem setting and of the priors we consider are given .",
    "algorithms to sample from the posterior are given in section 3 and computational aspects are discussed in section 4 . in section 5",
    "we present numerical experiments .",
    "we first apply and test the procedure on two simulated datasets , one involving the path graph and one a small - world graph , respectively .",
    "next we study the performance on real data , considering the problems of predicting the function of a protein in a protein - protein interaction graph , and classifying hand - written digits using a nearest - neighbour graph .",
    "concluding remarks are given in section 6 .",
    "again , we start with a connected , simple undirected graph @xmath0 , with @xmath1 vertices denoted by @xmath2 . associated to every vertex @xmath6",
    "is a noisy hard label @xmath5 .",
    "we assume the @xmath5 s are independent bernoulli variables , with @xmath13 where @xmath14 is an unobserved function on the vertices of the graph , the so - called soft label function .",
    "we observe only a subset @xmath15 of all the noisy labels .",
    "this can be a random subset of all the @xmath16 , generated in an arbitrary way , but independent of the values of the labels .",
    "our prediction method consists in first inferring the soft label function @xmath12 from @xmath17 and subsequently predicting the hard labels by thresholding .",
    "we take a bayesian approach which is nonparametric , in the sense that we do not assume that @xmath12 belongs to some low - dimensional , for instance generalized linear family of functions .      to put a prior on @xmath12 we first use the probit link @xmath18 ( i.e.  the cdf of the standard normal distribution ) to write @xmath19 for some function @xmath20 and then put a prior on @xmath21 . to achieve a form of laplacian regularization , which takes the geometry of the graph into account ( e.g. @xcite )",
    "we employ a gaussian prior with a covariance structure that is based on the laplacian @xmath22 of the graph @xmath23 . recall that this is the matrix defined as @xmath24 , where @xmath25 is the diagonal matrix of vertex degrees and @xmath26 is the adjacency matrix of the graph .",
    "( see for instance @xcite for background information . )",
    "we want to consider a gaussian prior on @xmath21 with a fixed power of the laplacian matrix as precision ( inverse covariance ) matrix . as the laplacian matrix has eigenvalue @xmath27 however , it is not invertible",
    ". therefore we add a small number @xmath28 to all eigenvalues of @xmath22 to overcome this problem . by theorem 4.2 of @xcite , we know that the smallest positive eigenvalue @xmath29 of @xmath23 satisfies @xmath30 , which motivates this choice . to make the prior flexible enough we add a multiplicative scale parameter @xmath31 as well .",
    "together , this results in a gaussian prior for @xmath21 with zero mean and precision matrix @xmath32 , where @xmath33 .",
    "we then have @xmath34    to make the connection with kernel - based learning we note that in the corresponding regularized kernel - based regression model , the matrix @xmath35 corresponds to the kernel and the scale parameter @xmath36 to the regularization parameter controlling the trade - off between fitting the observed data and the `` smoothness '' of the function estimate , as measured by the squared `` smoothness '' norm @xmath37 .      as with all nonparametric methods ,",
    "the performance of our procedure will depend crucially on the choice of the hyperparameters @xmath36 and @xmath38 , which control the bias - variance trade - off .",
    "the `` correct '' choices of these parameters depends in principle on properties of the unobserved function @xmath21 ( or equivalently , the function @xmath12 ) .",
    "theoretical considerations in @xcite have shown that good performance can obtained across a range of regularities of @xmath21 by fixing @xmath38 at an appropriate level , and putting a prior on the hyperparameter @xmath12 , so that we obtain a hierarchical bayes procedure .",
    "the choices for the prior on @xmath36 and for @xmath38 that were shown to work well in @xcite depend on the geometry of the graph @xmath23 .",
    "a main goal of the present paper is to investigate numerically whether this dependence is indeed visible when the method is implemented and to investigate choices for @xmath38 and @xmath36 that yield good performance and are computationally convenient as well .",
    "the geometry of the graph @xmath23 enters through the eigenvalues of the laplacian , which we denote by @xmath39 .",
    "( see , e.g. , chapter 7 of @xcite for the main properties of the spectrum of @xmath22 . ) in @xcite the theoretical performance of nonparametric bayes methods on graphs is studied under the assumption that for some parameter @xmath40 , there exist @xmath41 , @xmath42 $ ] and @xmath43 such that for all @xmath44 large enough , @xmath45 this condition can be verified numerically for a given graph ( as is done for instance for certain propotein - protein interaction and small world graphs in @xcite ) and can be shown to hold theoretically for instance for graphs that look like regular grids or torii of arbitrary dimensions .    in our notation ,",
    "the hyper prior for the scaling parameter @xmath36 that was shown to have good theoretical properties in @xcite ( under the geometry condition ) is the prior with density @xmath46 given by @xmath47 if the true ( unknown ) soft label function @xmath12 has regularity @xmath48 , defined in an appropriate , sobolev - type sense , then this choice for @xmath36 guarantees that the posterior contracts around the truth at the optimal rate , provided the hyper parameter @xmath38 has been chosen such that @xmath49 .",
    "below we investigate the effect of choosing @xmath38 or @xmath50 too high or too low relative to these `` optimal '' choices .    to understand better how crucial it is to use the prior and",
    "to set its hyperparameters just right , we compare it to a slightly simpler choice that is natural here , which is a gamma prior on @xmath36 with density @xmath51 for certain @xmath52 .",
    "this choice is computationally convenient due to the usual normal - inverse gamma partial conjugacy ( see e.g.  @xcite in the context of our setting ) .",
    "it introduces two more hyperparameters @xmath53 and @xmath54 .",
    "the authors of @xcite and @xcite mention @xmath55 , corresponding to jeffreys prior , which is improper , but does not result in any computational restrictions .",
    "we will see in the numerical experiments that this choice is a reasonable one in our setting as well .",
    "to distinguish between the two priors and in the paper we always call the _ ordinary gamma _",
    "prior for @xmath36 , and the _ generalized gamma _",
    "prior for @xmath36 .",
    "combining what we have so far , we obtain a hierarchical model that can be described as follows : @xmath56 as is well known ( cf .",
    "@xcite ) an equivalent formulation using an additional layer of latent variables @xmath57 is given by @xmath58 this is a more convenient representation which we will use in our computations .",
    "we consider the situation in which we do not observe all the labels @xmath5 , but only a certain subset @xmath17 .",
    "the precise mechanism that determines which @xmath5 s we observe and which ones are missing is not important for the algorithm we propose .",
    "we only assume that it is independent of the other elements of the model .",
    "specifically , we assume that for some arbitrary distribution @xmath59 on the collection @xmath60 of subsets vertices , a set of vertices @xmath61 is drawn and that we see which vertices were selected and what the corresponding noisy labels are . in other words , the observed data is @xmath62 .",
    "all in all , the full hierarchical scheme we will work with is the following : @xmath63 our goal is to compute the posterior @xmath64 and to use it to predict the unobserved labels .",
    "we use the latent variable approach as in @xcite , for instance , and implement a gibbs sampler to sample from the posterior @xmath64 in the setup .",
    "this involves sampling repeatedly from the conditionals @xmath65 , @xmath66 and @xmath67 .",
    "we detail these three steps in the following subsections .      by construction ,",
    "we have given @xmath25 that the latent variables @xmath68 , @xmath69 corresponding to the missing observations are independent of those corresponding to the observed labels .",
    "we simply have that given @xmath70 and @xmath25 , the variables @xmath68 , @xmath69 , are independent @xmath71-variables .    as for the latent variables corresponding to the observed labels",
    ", we have by that the @xmath68 are independent given @xmath70 and @xmath25 and @xmath72 where we say that @xmath68 matches @xmath5 if @xmath73 . for @xmath74 ,",
    "this describes the @xmath71-distribution , conditioned to be positive .",
    "we denote this distribution by @xmath75 .",
    "for @xmath76 it corresponds to the @xmath71-distribution , conditioned to be negative .",
    "we denote this distribution by @xmath77 .",
    "put together , we see that given @xmath70 , and @xmath25 , the @xmath68 are independent and @xmath78 we note that generating normal random variables conditioned to be positive or negative can for example be done by a simple rejection algorithm or inversion ( e.g. @xcite ) .      since given @xmath79 we know all the @xmath5 s , and @xmath80 is independent of all other elements of the model , we have @xmath81 .",
    "next , we have @xmath82 by plugging in what we know from we get @xmath83 completing the square , it follows that @xmath84      again we use that given @xmath79 we know all the @xmath5 s , and that @xmath80 is independent of everything else , which gives @xmath85 . since given @xmath21 , @xmath79 is independent of @xmath36 , we have @xmath86 now the method for ( approximate ) sampling from @xmath87 depends on the choice of the prior for @xmath36 .      if the prior density for @xmath36 is the ordinary gamma density given by we have the usual gaussian - inverse gamma conjugacy",
    "indeed , then we have @xmath88 in other words , in this case we have @xmath89      if the prior density for @xmath36 is the generalized gamma density given by we do not have conjugacy and we replace drawing from the exact conditional @xmath90 , as done in the preceding subsection , by a metropolis - hastings step .",
    "to this end we choose a proposal density @xmath91 . to generate a new draw for @xmath36",
    "we follow the usual steps :    * draw a proposal @xmath92 ; * draw an independent uniform variable @xmath93 on @xmath94 $ ] ; * if @xmath95 where @xmath96 then accept the proposal @xmath97 as new draw , else retain the old draw @xmath36 .    note that @xmath98 is indeed proportional to @xmath99 , as required ( cf .  ) .",
    "we have considered different proposal distributions @xmath91 in our experiments .",
    "our experiments indicate that a random walk proposal works well .      for convenience",
    "we summarize our sampling scheme , which depends on the prior for @xmath36 that we use .    for the generalised gamma",
    "prior we have the following :    data @xmath100 , initial values @xmath101 and @xmath102 .",
    "mcmc sample from the joint posterior @xmath103 . for @xmath104 ,",
    "draw independent @xmath105 draw @xmath106 draw a proposal @xmath107 and a uniform @xmath93 on @xmath94 $ ] .",
    "set @xmath108 .",
    "retain @xmath36 .",
    "for the ordinary gamma prior the algorithm looks as follows :    data @xmath100 , initial values @xmath101 and @xmath102 .",
    "mcmc sample from the joint posterior @xmath103 . for @xmath104 ,",
    "draw independent @xmath105 draw @xmath106 draw @xmath109",
    "in every iteration of either algorithm [ alg : mcmc1 ] or [ alg : mcmc2 ] the matrix @xmath110 has to be inverted .",
    "doing this naively can in general be computationally demanding , taking @xmath111 computations , in particular if @xmath22 is not very sparse , i.e.  if @xmath23 is a dense graph with many vertices with relatively large degree . to relax the computational burden",
    "it can be advantageous to change coordinates and to work relative to a basis of eigenvectors of the graph laplacian @xmath22 .    to make this concrete , suppose we have the eigendecomposition of the laplacian matrix @xmath112 , where @xmath113 is",
    "the diagonal matrix of eigenvalues of @xmath22 and @xmath114 is an orthogonal matrix of eigenvectors . computing this decomposition costs us one time @xmath111 computations .",
    "we can then parametrise the model by the vector @xmath115 instead of @xmath21 .",
    "the corresponding equivalent formulation of is given by @xmath116    making the appropriate , straightforward adaptations in the posterior computations , the sampling schemes take the following form in this parametrisation :    data @xmath100 , initial values @xmath117 and @xmath102 .",
    "mcmc sample from the joint posterior @xmath118 .",
    "compute @xmath119 and for @xmath104 , draw independent @xmath105 for @xmath120 , draw independent @xmath121 draw a proposal @xmath122 and a uniform @xmath93 on @xmath94 $ ] . set @xmath108 .",
    "retain @xmath36 .",
    "for the ordinary gamma prior the algorithm looks as follows :    data @xmath100 , initial values @xmath117 and @xmath102 .",
    "mcmc sample from the joint posterior @xmath118 .",
    "compute @xmath119 and for @xmath104 , draw independent @xmath105 for @xmath120 , draw independent @xmath121 draw @xmath123    we note that in this approach we need to invest once in the computation of the spectral decomposition of the laplacian @xmath22 .",
    "this removes the @xmath111 matrix inversions in each iteration of the algorithms .",
    "we do remark however that there is a matrix multiplication @xmath124 in line 2 of the algorithms , which in principle is an @xmath125 operation .",
    "moreover , algorithms [ alg : mcmc3 ] and [ alg : mcmc4 ] produce samples of @xmath126 , that is , we obtain the posterior samples as vectors of coordinates relative to the eigenbasis of the laplacian .",
    "if we want the samples in the original basis , which is what we need for prediction , we need to multiply all these vectors by @xmath114 .      instead of sampling the gaussian distribution in @xmath127 at once as in",
    ", it can be advantageous to sample this vector one coordinate at the time .    denoting by @xmath128 the vector @xmath129 with coordinate @xmath6 removed ,",
    "standard gaussian computations show that for every coordinate @xmath6 , @xmath130 where @xmath131 and @xmath132 this method for sampling from the conditional @xmath133 does not require the eigendecomposition of @xmath22 . in case",
    "the power of the laplacian matrix @xmath38 is an integer , the computations for a fixed @xmath6 only involve the @xmath38-step neighbors of vertex @xmath6 , which might be computationally attractive in graphs where the number of @xmath38-step neighbors of each vertex is low .",
    "to explore some of the issues involved in implementing bayesian prediction prediction on graphs we first consider the basic example of simulated data on the path graph with @xmath134 vertices . in this case , it is known that the laplacian eigenvalues are @xmath135 for @xmath136 , with corresponding eigenvectors @xmath137 this graph satisfies the geometry condition with @xmath138 . to simulate data",
    "we construct a function @xmath139 on the graph by setting @xmath140 where we choose @xmath141 for @xmath136 .",
    "this function has sobolev - type smoothness @xmath142 , as defined precisely in @xcite .",
    "we simulate noisy labels @xmath143 on the graph vertices satisfiying @xmath144 , where @xmath18 is the standard normal cdf .",
    "finally we remove @xmath145 of the labels at random to generate the set of observed labels @xmath17 .",
    "the left panel of figure [ fig : p1 ] shows the soft label function @xmath146 and simulated noisy hard labels @xmath143 on the path graph with @xmath134 vertices . in the right panel @xmath147",
    "is plotted against @xmath148 to illustrate that for this graph the geometry condition indeed holds with @xmath149 .    in figure",
    "[ fig : pbasic ] we visualise the posterior for the soft label function @xmath12 for various graph sizes @xmath44 . here",
    "we used the generalised gamma prior on @xmath36 with @xmath138 , @xmath150 and @xmath151 .",
    "these values are suggested by the theory in @xcite .",
    "the blue line is the posterior mean and the gray area depicts point - wise @xmath152 credible intervals .    at a first glance it appears that the procedure might be slightly oversmoothing , which could be due to the fact that the posterior for @xmath36 is concentrated at too large values . to get more insight into this issue we compare to posteriors computed with a fixed tuning parameter @xmath36 , set at the `` oracle value '' which minimises the mse of the posterior mean , which we determined numerically .",
    "the results are given in figure [ fig : popt ] .",
    "the posteriors have slightly better coverage than those in figure [ fig : pbasic ] .",
    "posterior histograms of the tuning parameter show if we use the generalised gamma prior for @xmath36 , the posterior indeed favours too high values of @xmath36 , compared to the oracle choice .",
    "this results in the oversmoothing we observe in figure [ fig : pbasic ] .",
    "see the first two rows of figure [ fig : preg ] .",
    "when instead of the theoretically optimal generalised gamma prior on @xmath36 we use the ordinary gamma prior , we can use the hyper parameters @xmath53 and @xmath54 to ensure that the posterior for @xmath36 assigns more mass close to the oracle tuning parameter . in practice , we do not know the true underlying function , so it is natural to spread the prior mass as much as possible .",
    "we can for example choose @xmath153 , corresponding to an improper prior @xmath154 ( as in @xcite ) , or @xmath155 and @xmath156 such that @xmath157 . in figure",
    "[ fig : dens ] we plot the ordinary gamma prior density corresponding to @xmath55 ( blue dashed line ) and the generalised gamma prior densities for various @xmath44 ( black lines ) . since the ordinary gamma assigns more mass to smaller values of @xmath36",
    ", we might hope that if we use that prior on @xmath36 , we get a posterior closer to the oracle and hence reduce the oversmoothing problem .     in black .",
    "blue dashed line the ordinary gamma prior with @xmath55 .",
    "]    figure [ fig : preg2 ] visualises the posteriors that we get for the soft label function when using the ordinary gamma prior with @xmath55 .",
    "we see that indeed we get better posterior coverage than in figure [ fig : pbasic ] .",
    "the third row in figure [ fig : preg ] confirms that when using the ordinary gamma prior on @xmath36 , the posterior for @xmath36 puts more mass around the optimal value .      in the paper @xcite it",
    "was suggested to take the power of the laplacian equal to @xmath151 , where @xmath50 is the number appearing in the geometry condition and @xmath158 is a tuning parameter that quantifies the smoothness of the prior in some sense . it was proved that when combining this with the generalised gamma prior on @xmath36 , we get good convergence rates if the sobolev - type smoothness of the true soft - label function is less than @xmath38 .",
    "this might suggest that it is advantageous to set @xmath38 high , since then the theory says that we get good rates across a large range of regularities of the true function . on the other hand ,",
    "setting @xmath38 higher means we favour smooth functions more .",
    "this could potentially lead to oversmoothing and hence to poor posterior coverage . in this section",
    "we investigate this issue numerically .    in figure",
    "[ fig : q1 ] we use the generalised gamma prior on @xmath36 .",
    "we plot the posterior for @xmath12 , varying @xmath44 from left to right and @xmath38 from top to bottom . in the top row @xmath159 .",
    "since this is less than @xmath142 , the theory suggests that we are undersmoothing too much and will get sub - optimal convergence rates .",
    "the figure seems to confirms this . in the middle row",
    "we have @xmath160 .",
    "this means the prior smoothness matches the true smoothness , which is asymptotically a good choice according to the theory .",
    "this is the same picture as in figure [ fig : pbasic ] . in the bottom row @xmath161 . in this case",
    "the prior smoothness is larger than the true smoothness @xmath162 .",
    "however , the theory says that we should still get a good convergence rates , because to compensate for the smoothness mismatch the posterior for @xmath36 will automatically charge smaller values of @xmath36 more .",
    "in the simulations we see that the result is indeed not dramatic , but that the procedure is in actual fact oversmoothing somewhat , resulting in worse posterior coverage .",
    "figure [ fig : q2 ] gives the same plots when using the ordinary gamma prior on @xmath36 , with @xmath55 .",
    "we see essentially the same effects , but the effect of choosing @xmath38 too small is a bit more pronounced . in terms of posterior coverage",
    "the ordinary gamma prior does a bit better , although it gives too conservative credible sets when @xmath38 is set too low .      in this section",
    "we consider simulated data on a small - world graph obtained as a realization of the watts - strogatz model ( @xcite ) .",
    "the graph is obtained by first considering a ring graph of @xmath163 nodes .",
    "then we loop through the nodes and uniformly rewire each edge with probability @xmath164 .",
    "we keep the largest connected component and delete multiple edges and loops resulting in the graph in figure [ fig : watts ] with @xmath165 nodes .",
    "we numerically determine the eigenvalues @xmath166 and eigenfunctions @xmath167 of the graph laplacian and define a function @xmath139 on the graph by @xmath140 where we choose @xmath168 for @xmath169 to have sobolev - type smoothness @xmath170 ( cf .",
    "@xcite ) . as before we assign labels to the graph according to probabilities @xmath144 , where @xmath18 is the distribution function of the standard normal distribution .",
    "we remove the label of @xmath171 of the nodes .",
    "the aim is to predict these using the observed labels .    in this case",
    "it is hard to visualise smooth functions on the graph and hence to visualise the entire posterior distribution of the soft label function .",
    "instead we analyse the quality of the procedure by plotting @xmath152 credible intervals for the soft label function at @xmath172 randomly selected vertices of which we have not observed the noisy labels .",
    "figure [ fig : sw1 ] gives these plots for the procedure with the generalised gamma prior on @xmath36 ( left ) , the ordinary gamma prior on @xmath36 with @xmath55 ( middle ) , and the fixed oracle choice of @xmath36 ( right ) . at the bottom left and middle the posteriors for @xmath36",
    "are shown .",
    "the bottom right is a plot of the mse of the posterior mean corresponding to a fixed @xmath36 as a function of that @xmath36 .",
    "the point where it is minimal is the oracle choice of @xmath36 .",
    "vertices with missing labels .",
    "blue dots are the posterior means , black dots are the true function values . from left to",
    "right the plots correspond to the procedure with the generalised gamma prior on @xmath36 , the ordinary gamma prior with @xmath55 , and with the fixed , oracle choice of @xmath36 , respectively .",
    "bottom row : histograms of posterior for @xmath36 ( scaled).,title=\"fig : \" ]   vertices with missing labels .",
    "blue dots are the posterior means , black dots are the true function values . from left to right the plots correspond to the procedure with the generalised gamma prior on @xmath36 , the ordinary gamma prior with @xmath55 , and with the fixed , oracle choice of @xmath36 , respectively . bottom row : histograms of posterior for @xmath36 ( scaled).,title=\"fig : \" ]     vertices with missing labels .",
    "blue dots are the posterior means , black dots are the true function values .",
    "from left to right the plots correspond to the procedure with the generalised gamma prior on @xmath36 , the ordinary gamma prior with @xmath55 , and with the fixed , oracle choice of @xmath36 , respectively . bottom row : histograms of posterior for @xmath36 ( scaled).,title=\"fig : \" ]   vertices with missing labels .",
    "blue dots are the posterior means , black dots are the true function values . from left to right the plots correspond to the procedure with the generalised gamma prior on @xmath36 , the ordinary gamma prior with @xmath55 , and with the fixed , oracle choice of @xmath36 , respectively .",
    "bottom row : histograms of posterior for @xmath36 ( scaled).,title=\"fig : \" ]     vertices with missing labels .",
    "blue dots are the posterior means , black dots are the true function values . from left to right the plots correspond to the procedure with the generalised gamma prior on @xmath36 , the ordinary gamma prior with @xmath55 , and with the fixed , oracle choice of @xmath36 , respectively . bottom row : histograms of posterior for @xmath36 ( scaled).,title=\"fig : \" ]   vertices with missing labels .",
    "blue dots are the posterior means , black dots are the true function values . from left to right the plots correspond to the procedure with the generalised gamma prior on @xmath36 , the ordinary gamma prior with @xmath55 , and with the fixed , oracle choice of @xmath36 , respectively . bottom row : histograms of posterior for @xmath36 ( scaled).,title=\"fig : \" ]    also in this example we observe that with the theoretically optimal generalised gamma prior on @xmath36 we are shrinking a bit too much , resulting in particular in credible intervals not containing the true soft label . when using the ordinary gamma prior the performance is closer to the oracle procedure .",
    "the bottom row of figure [ fig : sw1 ] confirms that with the ordinary gamma prior the posterior for @xmath36 is closer to the oracle choice .",
    "we have determined numerically that the graph under consideration satisfies the geometry condition with @xmath173 , see the right panel of figure [ fig : watts ] .",
    "the results of @xcite thus suggest to use as prior on @xmath21 the laplacian to the power @xmath174 for parameter @xmath158 that determines the prior smoothness .",
    "also for this example we have investigated the impact of different choices .",
    "in figure [ fig : ws1 ] illustrate what happens if @xmath50 is chosen too low . on the left",
    "we see that the procedure with the generalised gamma prior on @xmath36 oversmooths quite dramatically .",
    "the bottom row of the figure shows that the posterior for @xmath36 puts too little mass around the oracle @xmath36 in that case .",
    "the plots in the middle corresponds to ordinary gamma prior on @xmath36 with @xmath55 .",
    "this performs much better , close to procedure with oracle choice of @xmath36 shown on the right . in figure",
    "[ fig : ws2 ] the parameter @xmath50 is chosen too high . here",
    "all three procedures have comparable performance . all oversmoothing a but too much due to the fact that the power of the laplacian becomes too large .",
    "this is in line with what we say for the path graph .    , but now with @xmath149.,title=\"fig : \" ] , but now with @xmath149.,title=\"fig : \" ]    , but now with @xmath149.,title=\"fig : \" ] , but now with @xmath149.,title=\"fig : \" ]    , but now with @xmath149.,title=\"fig : \" ] , but now with @xmath149.,title=\"fig : \" ]    , but now with @xmath175.,title=\"fig : \" ] , but now with @xmath175.,title=\"fig : \" ]    , but now with @xmath175.,title=\"fig : \" ] , but now with @xmath175.,title=\"fig : \" ]    , but now with @xmath175.,title=\"fig : \" ] , but now with @xmath175.,title=\"fig : \" ]    if we choose the parameter @xmath50 correctly , i.e.  @xmath173 is this case , and only choose different values for the hyper parameter @xmath158 we only change the prior smoothness of @xmath21 , without changing the prior on @xmath36 .",
    "figures [ fig : a1 ] and [ fig : a2 ] illustrate the effect . in the first we set @xmath176 , which is too low relative to the smoothness of the true soft label function , in the second one @xmath177 , which is too high .",
    "we clearly see the effect on the width of the credible intervals .",
    "the effect on coverage is not very large .    , but now with @xmath178.,title=\"fig : \" ] , but now with @xmath178.,title=\"fig : \" ]    , but now with @xmath178.,title=\"fig : \" ] , but now with @xmath178.,title=\"fig : \" ]    , but now with @xmath178.,title=\"fig : \" ] , but now with @xmath178.,title=\"fig : \" ]    , but now with @xmath179.,title=\"fig : \" ] , but now with @xmath179.,title=\"fig : \" ]    , but now with @xmath179.,title=\"fig : \" ] , but now with @xmath179.,title=\"fig : \" ]    , but now with @xmath179.,title=\"fig : \" ] , but now with @xmath179.,title=\"fig : \" ]      to test the nonparametric bayes procedure on real data we adapt the case study from @xcite , section 8.5 .",
    "the example is about the prediction of protein function from a network of interactions among proteins that are responsible for cell communication in yeast . for more information about the background of the experiment set - up ,",
    "see @xcite .",
    "the protein interaction graph is shown on the left in figure [ fig : ppi ] .",
    "a vertex in the graph is labelled according to whether or not the corresponding protein is involved in so - called intracellular signaling cascades ( icsc ) , which is a specific form of communication .",
    "we have randomly removed @xmath180 of the labels and we apply our bayesian prediction procedure to try and recover them from the observed labels .    in view of the findings in the preceding sections",
    "we apply the procedure with the ordinary gamma prior on @xmath36 with @xmath55 .",
    "numerical computation of the laplacian eigenvalues shows that in this case the geometry condition is fulfilled with @xmath181 , see the right panel of figure [ fig : ppi ] .",
    "we use this value in the procedure .",
    "the parameter @xmath158 that determines the prior smoothness of @xmath21 is set to @xmath182 .",
    "this is a conservative choice , in order to avoid oversmoothing .",
    "we now run our algorithm to produce credible intervals for the soft label function @xmath12 at the vertices of which we did not observe the labels .     in vertices @xmath183 and @xmath184 . ]",
    "figure [ fig : res ] shows the results , together with the true labels that we removed .",
    "we see that if we predict the missing labels by thresholding the posterior means at @xmath9 , we have a misclassification rate of @xmath185 . if we repeat the procedure @xmath186 times , every time removing @xmath180 different labels at random , we obtain an average misclassification rate of @xmath187 . to assess this",
    ", we also computed @xmath188-nearest neighbour ( @xmath188-nn ) predictions for various @xmath188 .",
    "we found average missclassification rates of @xmath189 for one @xmath183-nn , @xmath190 for @xmath191-nn and @xmath192 for @xmath193-nn .",
    "hence in terms of prediction performance our procedure is comparable to @xmath188-nn with the oracle choice of @xmath188 .",
    "this illustrates that in line with the theory , our procedure succeeds in automatically tuning the appropriate degree of smoothing .",
    "moreover , the bayes procedure has the advantage that in addition to predictions , we obtain credible intervals as an indication of the uncertainty in the predictions .",
    "so far we have considered examples with graphs that satisfy the geometry condition . for such graph",
    "we have theoretical results that provide some guidelines for the construction of the prior and choices of the hyper parameters . in principle",
    "however , we can also apply our procedure to graphs that do not satisfy for some @xmath50 .",
    "it is intuitively clear that a condition like should not always be necessary for good performance .",
    "if we use the ordinary gamma prior on @xmath36 and set @xmath38 at a conservative ( not too high ) value , we can just apply our procedure and should still get reasonable results if the graph geometry and the distribution of the labels are sufficiently related . in this section",
    "we briefly investigate such as case .",
    "the mnist dataset of handwritten digits has a training set of @xmath194 thousand examples and a test set of @xmath195 thousand .",
    "the digits are size - normalized and centered in a fixed - size image .",
    "the dataset is publicly available at ` http://yann.lecun.com/exbd/mnist ` .",
    "we have randomly selected a subsample of @xmath196 consisting of only the digits @xmath27 and @xmath183 of which @xmath197 in are the training set and @xmath186 are from the test set .",
    "our goal is to classify the @xmath186 images from the test set . to turn this into a label prediction problem on a graph we construct a graph with @xmath196 nodes , corresponding to the images . for each image we determined the @xmath195 closest images in terms of pixel - distance and connected the corresponding nodes in the graph by an edge .",
    "the resulting graph in shown in figure [ fig : mnist ] the eigenvalue plot suggests that the graph does not satisfy the geometry condition to the extent that the preceding graphs did .",
    "the picture indicates that predicting the missing labels in this graph is not a very hard problem . and",
    "indeed , our procedure performs well in this case .",
    "we classify all missing labels correctly , with very high certainties .",
    "see figure [ fig : res2 ] .",
    "we have described a nonparametric bayes procedure to perform binary classification on graphs .",
    "we have considered a hierarchical bayesian approach with a randomly scaled gaussian prior as in the theoretical framework in @xcite .",
    "we have implemented the procedure with the theoretically optimal prior from @xcite and a variant with a different prior on the scale , which exploits partial conjugacy and has some more flexibility .",
    "our numerical experiments suggest that good results are obtained when using algorithm [ alg : mcmc2 ] , i.e.  using the ordinary gamma prior on the scale . suggested choices for the hyper parameters are @xmath55 and @xmath198 . here",
    "@xmath50 is the geometry parameter appearing in the geometry condition and can be determined numerically from the spectrum of the graph laplacian .",
    "the parameter @xmath158 reflects prior smoothness and should not be set too high ( e.g.  @xmath182 or @xmath191 ) , to avoid oversmoothing .",
    "in view of computational complexity it might be more advantageous to consider other methods to adaptively find the tuning parameter , such as empirical bayes methods .",
    "also , it might be sensible to modify the prior by truncating the @xmath44-dimensional gaussian prior on @xmath21 to a lower dimensional one by writing a series expansion for @xmath21 and truncating the sum at a random point , similar to the approach in @xcite for instance .",
    "it is conceivable that in this way the procedure becomes both more flexible in terms of adaptation to smoothness and will also computationally scale better to large sample size @xmath44 .",
    "we intend to investigate this in future work ."
  ],
  "abstract_text": [
    "<S> this article describes an implementation of a nonparametric bayesian approach to solving binary classification problems on graphs . </S>",
    "<S> we consider a hierarchical bayesian approach with a randomly scaled gaussian prior . </S>",
    "<S> we have two simulated data examples and two examples using real data to illustrate our proposed methods . </S>"
  ]
}