{
  "article_text": [
    "supervised classification is still one of the hot topics for high dimensional and functional data due to the importance of their applications and the intrinsic difficulty in a general setup . in this context",
    ", there is a vast literature on classification methods which include : linear classification , @xmath1-nearest neighbors and kernel rules , classification based on partial least squares , reproducing kernels or depth measures .",
    "complete surveys of the literature are the works by ballo et al .",
    "@xcite , cuevas @xcite and delaigle and hall @xcite . in the book _ contributions in infinite - dimensional statistics and related topics _",
    "@xcite , there are also several recent advances in supervised and unsupervised classification .",
    "see for instance , chapters 2 , 5 , 22 or 48 , or directly , chapter 1 of this issue ( bongiorno et al .",
    "@xcite ) . in this context ,",
    "very recently there have been of great interest to develop aggregation methods . in particular , there is a large list of linear aggregation methods like boosting ( breiman @xcite , breiman @xcite ) , random forest ( breiman @xcite , biau et al .",
    "@xcite , biau @xcite ) , among others .",
    "all these methods exhibit an important improvement when combining a subset of classifiers to produce a new one .",
    "most of the contributions to the aggregation literature have been proposed for nonparametric regression , a problem closely related to classification rules , which can be obtained just by plugging in the estimate of the regression function into the bayes rule ( see for instance , yang @xcite and bunea et al .",
    "@xcite ) . model selection ( select the optimal single model from a list of models ) , convex aggregation ( search for the optimal convex combination of a given set of estimators ) , and linear aggregation ( select the optimal linear combination of estimators ) are important contributions among a large list .    in the finite dimensional setup , mojirsheibani @xcite and @xcite introduced a combined classifier showing strong consistency under someway hard to verify assumptions involving the vapnik chervonenkis dimension of the random partitions of the set of classifiers , which are non  valid in the functional setup .",
    "very recently biau et al .",
    "@xcite introduced a new nonlinear aggregation strategy for the regression problem called cobra , extending the ideas in mojirsheibani @xcite to the more general setup of nonparametric regression in @xmath2 . in the same direction but for the classification problem in the infinite dimensional setup",
    ", we extend the ideas in mojirsheibani @xcite to construct a classification rule which combines , in a nonlinear way , several classifiers to construct an optimal one .",
    "we point out that our rule allows to combine methods of very different nature , taking advantage of the abilities of each expert and allowing to adapt the method to different class of datasets .",
    "even though our classifier allows aggregate experts of the same nature , the possibility of combine classifiers of different character , improves the use of existing rules as the bagged nearest neighbors classifier ( see for instance hall and samworth @xcite ) . as in biau",
    "@xcite , we also introduce a more flexible form of the rule which discards a small percentage @xmath3 of those preliminary experts that behaves differently from the rest . under very mild assumptions ,",
    "we prove consistency , obtain rates of convergence and show some optimality properties of the aggregated rule . to build up this classifier",
    ", we use the inverse function ( see also fraiman et al .",
    "@xcite ) of each preliminary experts which makes the proposal particularly well designed for high dimensional data avoiding the curse of dimensionality .",
    "it also performs well in functional data settings .    in section [ setup ]",
    "we introduce the new classifier in the general context of a separable and complete metric space which combines , in a nonlinear way , the decision of @xmath0 experts ( classifiers ) . a more flexible rule is also considered . in section [ resultados ]",
    "we state our two main results regarding consistency , rates of convergence and asymptotic optimality of the classifier .",
    "asymptotically , the new rule performs as the best of the @xmath0 classifiers used to build it up .",
    "section [ simus ] is devoted to show through some simulations the performance of the new classifier in high dimensional and functional data for moderate sample sizes .",
    "a real data example is also considered .",
    "all proofs are given in the appendix .",
    "throughout the manuscript @xmath4 will denote a separable and complete metric space , @xmath5 a random pair taking values in @xmath6 and @xmath7 the probability measure of @xmath8 .",
    "the elements of the training sample @xmath9 , are iid random elements with the same distribution as the pair @xmath5 .",
    "the regression function is denoted by @xmath10 , the bayes rule by @xmath11 and the optimal bayes risk by @xmath12 .    in order to define our classifier , we split the sample @xmath13 into two subsamples @xmath14 and @xmath15 with @xmath16 . with @xmath17",
    "we build up @xmath0 classifiers @xmath18 , @xmath19 which we place in the vector @xmath20 and , following some ideas in @xcite , with @xmath21 we construct our aggregate classifier as , @xmath22 where @xmath23 with weights @xmath24 given by @xmath25 here , @xmath26 is assumed to be @xmath27 . like in @xcite , for @xmath28 a more flexible version of the classifier , called @xmath29 , can be defined replacing the weights in ( [ pesos ] ) by @xmath30 more precisely , the more flexible version of the classifier ( [ clasificador ] ) is given by @xmath31 where @xmath32 is defined as in ( [ agregados ] ) but with the weights given by ( [ pesos2 ] ) .",
    "observe that if we choose @xmath33 in ( [ pesos2 ] ) and ( [ clasifgen ] ) we obtain the weights given in ( [ pesos ] ) and the classifier ( [ clasificador ] ) respectively .    * the type of nonlinear aggregation used to define our classifiers turns out to be quite natural .",
    "indeed , we give a weight different from zero to those @xmath34 which classify @xmath35 in the same group as the whole set of classifiers @xmath36 ( or @xmath37 of them ) . *",
    "since we are using the inverse functions of the classifiers @xmath38 , observations which are far from @xmath35 for which the condition mentioned in a ) is fulfilled are involved in the definition of the classification rule .",
    "this may be very important in the case of high dimensional data to avoid the curse of dimensionality .",
    "this is illustrated in figure [ fig : example ] , where we show two samples of points : one uniformly distributed in the square @xmath39\\times[-2,2]$ ] ( filled black points ) and another uniformly distributed in the @xmath40-ring @xmath39\\times[-1,1]$ ] ( empty black points ) .",
    "we also show two points to classify , the empty red and the filled magenta triangles together with their corresponding voters , empty green squares and filled blue squares , respectively .",
    "as we can see , observations that are far from the triangles are also involved in the classification .",
    "in this section we show two asymptotic results for the nonlinear aggregation classifier the first one shows that the classifier @xmath41 is consistent if , for @xmath42 , at least @xmath43 of them are consistent .",
    "moreover , rates of convergence for @xmath41 ( and @xmath44 ) are obtained assuming we know the rates of convergence of the @xmath45 consistent experts .",
    "the second result , shows that @xmath44 behaves asymptotically as the best of the @xmath0 classifiers used to build it up .",
    "both results are proved under mild conditions . throughout this section",
    "we will use the notation @xmath46 .",
    "[ consistencia ] [ cor1 ] assume that , for every @xmath47 , the classifier @xmath38 converges in probability to @xmath48 as @xmath49 , with @xmath50 and @xmath51 .",
    "let us assume that @xmath52 and @xmath53 , then    * @xmath54 * let @xmath55 as @xmath49 , for @xmath56 and @xmath57 .",
    "if @xmath58 , then , for @xmath1 large enough , @xmath59 for some constant @xmath60 .    *",
    "the assumption @xmath61 is really mild .",
    "it just requires that if the bayes rule @xmath62 takes the value @xmath63 ( or 0 ) the probability that @xmath64 is greater than the probability that @xmath65 ( the probability that @xmath65 is greater than the probability that @xmath64 ) .",
    "moreover since the bayes risk @xmath66 one of the conditions in ( [ condteo1 ] ) is always fulfilled .",
    "* it is well known that in the finite dimensional case , if the regression function @xmath67 verifies a lipschitz condition and @xmath8 is bounded supported , the accuracy of classical classification rules is @xmath68 .",
    "therefore the right hand side of ( [ ordteo1 ] ) is @xmath69 and the optimal rate for @xmath70 is attained for @xmath71 . *",
    "the choice of the parameters @xmath3 and @xmath72 is an important issue . from a practical point of view , we suggest to perform a cross validation procedure to select the values of the corresponding parameters . see section [ realdata ] for an implementation in a real data example .    in order to state the optimality result",
    "we introduce some additional notation .",
    "let @xmath73 and let us call @xmath74 .",
    "calling @xmath75 the @xmath76-th entry of the vector @xmath77 , we define the following subsets @xmath78 @xmath79 for each @xmath74 , we consider the assumption : @xmath80    [ optimalidad ]    * for each @xmath81 , @xmath82 which implies that , @xmath83 * under assumption ( @xmath84 ) we obtain a better approximation rate , @xmath85",
    "in this section we present the performance of the aggregated classifier in two different scenarios .",
    "the first one corresponds to high dimensional data while , in the second one , we consider two simulated models for functional data analyzed in delaigle and hall @xcite . +      in this setting we show the performance of our method by analyzing data generated in @xmath86 in the following way : we generate @xmath87 iid uniform random variables in @xmath88 $ ] , say @xmath89 . for each @xmath90 , if @xmath91 , we generate a random variable @xmath92 with uniform distribution in @xmath39^{150}$ ] and set @xmath93 .",
    "if @xmath94 , we generate a random variable @xmath95 with uniform distribution in @xmath96^{150})$ ] where @xmath97 is the translation along the direction @xmath98 for @xmath99 and set @xmath100",
    ". then we split the sample into two subsamples : with the first @xmath101 pairs @xmath102 , we build the training sample , with the remaining @xmath103 we build the testing sample .",
    "we consider two cases : the homogeneous case , where we aggregate classifiers of the same nature and in the heterogeneous case , where we aggregate experts of different nature .    * : @xmath0 @xmath1-nearest neighbor classifiers with the number of neighbors taken as follows : 1 .",
    "[ 1 ] we fix @xmath104 consecutive odd numbers ; 2 .   [ 2 ] we choose at random @xmath105 different odd integers between @xmath63 and + .    in table",
    "[ tabla11 ] , we report the mean and standard deviation ( in brackets ) of the misclassification error rate for case [ 1 ] , when compared with the nearest neighbor rules build up with a sample size @xmath101 taking @xmath106 nearest neighbors ( these classifiers are denoted by @xmath107 for @xmath108 ) . in table",
    "[ tabla12 ] we report the median and mad ( in brackets ) of the misclassification error rate for this case .    in table",
    "[ tabla21 ] we report the mean of the misclassification error rate and standard deviation for case [ 2 ] , with the original aggregated classifier and the two more flexible versions : @xmath109 and @xmath110 . in this table",
    "we compare the performance of our rules with the ( optimal ) cross validated nearest neighbor classifier computed with @xmath1 and also with @xmath101 . in table",
    "[ tabla22 ] we report the median and mad of the misclassification error rate for this case .",
    "* : @xmath111 classifiers : 3 @xmath1-nearest neighbor rules with fixed values of @xmath1 , the fisher and the random forest classifiers .    here",
    "we take @xmath112 nearest neighbors ( denoted by @xmath107 for @xmath113 ) , the fisher classifier ( denoted by @xmath114 ) and the random forest classifier ( denoted by @xmath115 ) . in table [ tabla31 ] we report the averaged misclassification error rates and standard deviation and in table [ tabla32 ] we report the median and mad for this case .          in this",
    "setting we show the performance of our method by analyzing the following two models considered in delaigle and hall @xcite :    * model i : we generate two samples of size @xmath116 from different populations following the model @xmath117 where @xmath118 , @xmath119 and @xmath120 are , respectively , the j - th coordinate of the mean vectors @xmath121 , and @xmath122 while the errors are given by @xmath123 with @xmath124 and @xmath125 .",
    "* model ii : we generate two samples of size @xmath116 from different populations following the model @xmath126 where @xmath127 and @xmath120 the j - th coordinate of @xmath128 , @xmath125 and the errors are given by @xmath123 with @xmath124 and @xmath129 .",
    "+ this second model looks more challenging since although the means of the two populations are quite different , the error process is very wiggly , concentrated in high frequencies ( as shown in figure [ figmodiimean ] left and right panel , respectively ) .",
    "so in this case , in order to apply our classification method , we have first performed the nadaraya - watson kernel smoother ( taking a normal kernel ) to the training sample with different values of the bandwidths for each of the two populations .",
    "the values for the bandwidths were chosen via cross - validation with our classifier , varying the bandwidths between @xmath130 and @xmath131 ( in intervals of length @xmath132 ) .",
    "the optimal values , over 200 replicates , were @xmath133 for the first population ( with mean @xmath134 ) and @xmath135 for the second one .",
    "finally , we apply the classification method to the raw ( non - smoothed ) curves of the testing sample .    in table [ tabla51 ] we report the averaged misclassification error rate and the standard deviation over @xmath103 replications for models i and ii , taking @xmath136 , @xmath137 , @xmath138 , and @xmath139 . in the whole training sample ( of @xmath101 functions ) the @xmath116 labels for every population were chosen at random . the test sample consist of @xmath103 data , taking @xmath140 of every population . here , @xmath141-nearest neighbor rule for @xmath142 . in table",
    "[ tabla52 ] we report the median of the misclassification error rate and the mad . for model",
    "i we get a better performance than the pls - centroid classifier proposed by delaigle and hall @xcite . for model",
    "ii pls - centroid classifier clearly outperforms our classifier although we get a quite small missclassification error , just using a combination of five nearest neighbor estimates .",
    "the data to be analyzed in this section consists in the mass spectra from blood samples of 216 women of which , 121 suffer from an ovarian cancer condition and the remaining 95 are healthy women which were taken as control group .",
    "we refer to @xcite for a previous analysis of these data with a detailed discussion of their medical aspects , see also @xcite for further statistical analysis of these data .",
    "a spectrogram is a curve showing the number of molecules ( or fragments ) found for every mass / charge ratio and , the idea behind spectrograms , is to control the amount of proteins produced in cells since , when cancer starts to grow , its cells produce a different kind of proteins than those produced by healthy cells",
    ". moreover , the amount of common produced proteins may be different .",
    "proteomics , broadly speaking , consists of a family of procedures allowing researchers to analyze proteins .",
    "in particular , here we are interested in some techniques which allow to separate mixtures of complex molecules according to the rate mass / charge ( observe that , molecules with the same mass / charge ratio are indistinguishable with a spectrogram ) .",
    "we have processed the data as follows : we have restricted ourselves to the interval mass charge ( horizontal axis ) @xmath143 $ ] .",
    "then , in order to have all the spectra defined in a common equi - spaced grid , we have smoothed them via a nadaraya - watson smoother . finally , every function has been divided by its maximum , in order to have all the values scaled in the common interval @xmath88 $ ] .",
    "observe that our interest is to find the location of maxima amount of molecules more than the corresponding heights .    to build the classifier introduced in ( [ clasifgen ] ) we have taken @xmath144 nearest neighbor classifiers , with @xmath145 neighbors .",
    "we have implemented the cross validation method in a grid for @xmath146 , with @xmath3 taking the values @xmath147 and @xmath72 taking @xmath148 values @xmath149 .",
    "the minimum of the misclassification error was attained for @xmath150 and @xmath151 in whose case the accuracy obtained was 95% .",
    "* we introduce a new nonlinear aggregating method for supervised classification in a general setup built up from a family of classifiers @xmath152 .",
    "it combines the decision of the m experts according to a  coincidence opinion \" with respect to the new data we want to classify . * the new method , besides being easy to implement , is particularly well designed for high dimensional and functional data .",
    "the method is not local , and the use of the inverse functions prevent from the curse of dimensionality that suffers all local methods .",
    "* we obtain consistency and rates of convergence under very mild conditions on a general metric space setup . *",
    "an optimality result is obtained in the sense that the nonlinear aggregation rule behaves asymptotically as well as the best one among the @xmath0 classifiers ( experts ) @xmath152 . *",
    "a small simulation study confirms the asymptotic results for moderate sample sizes .",
    "in particular it is very well behaved for high  dimensional and functional data .",
    "* in a well known spectrogram curves dataset , we obtain a very good performance , classifying 95% , very close to the best known results for these data . * although we have implemented cross validation to choose the parameters @xmath146 in section [ realdata ] , conditions for the validity of this procedure remains as an open problem .",
    "to prove theorem [ consistencia ] we will need the following lemma .",
    "[ lema ] let @xmath153 be a classifier built up from the training sample @xmath17 such that @xmath154 when @xmath155 .",
    "then , @xmath156 .",
    "[ proof of lemma [ lema ] ] first we write , @xmath157 where in the last equality we have used that @xmath158 implies @xmath159 therefore , replacing in ( [ error ] ) we get that @xmath160 which by hypothesis converges to zero as @xmath49 and the lemma is proved .",
    "[ proof of theorem [ consistencia ] ] we will prove part b ) of the theorem since part a ) is a direct consequence of it . by ( [ boundl3 ] )",
    ", it suffices to prove that , for @xmath1 large enough : @xmath161 we first split @xmath162 into two terms , @xmath163 then we will prove that , for @xmath1 large enough , @xmath164 for some arbitrary constant @xmath165 .",
    "the proof that @xmath166 for some arbitrary constant @xmath167 is completely analogous and we omit it . finally , taking @xmath168 , the proof will be completed . in order to deal with term @xmath169 , let us define the vectors @xmath170 then , @xmath171 observe that , conditioning to @xmath172 and defining @xmath173 we can rewrite @xmath174 as @xmath175 therefore , @xmath176 in order to use a concentration inequality to bound this probability , we need to compute the expectation of @xmath177 . to do this , observe that @xmath178 and @xmath179 since @xmath180 we have , @xmath181\\\\ & \\geq \\mathbb{p}_{\\mathcal{d}_k}\\big(\\mathbf{g_{rk}}(x)=1\\big)\\big[\\mathbb p_{\\mathcal{d}_k}(y=1|a_\\alpha)-1/2\\big].\\nonumber\\end{aligned}\\ ] ] now , since for @xmath182 , @xmath183 in probability as @xmath49 , @xmath184 on the other hand , we have that , for @xmath1 large enough , @xmath185 . indeed , for @xmath56 ,",
    "let us consider the events @xmath186 which , by hypothesis , for @xmath1 large enough verify @xmath187 for all @xmath188 . in particular , we can take @xmath188 such that @xmath189 .",
    "this implies that @xmath190 conditioning to @xmath191 the event @xmath192 equals @xmath193 given by @xmath194 however , @xmath195 imply that @xmath196 .",
    "indeed , from the inequality @xmath197 , it is clear that @xmath198 . on the other hand , @xmath199 and @xmath195 imply that @xmath200 , and so the sum in the second term of ( [ condaalpha ] ) is at most @xmath201 and consequently , @xmath202 .",
    "then , combining this fact with ( [ eq ] ) we have that , for @xmath1 large enough    @xmath203    therefore , from ( [ lim1 ] ) and ( [ otraeq ] ) in ( [ esp2 ] ) we get @xmath204 going back to ( [ proba ] ) , conditioning to @xmath205 and using the hoeffding inequality for @xmath206 , for @xmath1 large enough we have    @xmath207    with @xmath208 .",
    "on the other hand , by hypothesis we have @xmath209 which concludes the proof .",
    "first we write , @xmath210    let us take @xmath77 fixed . observe that in this case , @xmath211 depends only on the subsample @xmath21 , therefore the events @xmath212 and @xmath213 are independent for all @xmath214 , @xmath215",
    ". then , @xmath216 let us define @xmath217 @xmath218 @xmath219 to bound term @xmath169 , we will consider 3 cases , @xmath220 , @xmath221 and @xmath222 .",
    "let us first assume that @xmath220 . in this case",
    ", using the hoeffding inequality we have ,    @xmath223    if @xmath221 , using hoeffding inequality again we get @xmath224    if @xmath222 , since for all @xmath77 and @xmath225 , @xmath226 , using the berry - esseen inequality we get @xmath227\\mathbb{i}_{\\{\\sigma_\\nu^2>0\\}}.\\end{aligned}\\ ] ] observe that , since @xmath228 and @xmath229 , there exists @xmath77 such that @xmath230 .",
    "then , from ( [ ia ] ) , ( [ ib ] ) and ( [ ib1 ] ) we get    @xmath231    analogously , it is easy to prove that @xmath232 where in the last term we have used that @xmath222 implies @xmath233 . therefore , with ( [ eqi ] ) and ( [ eqii ] ) in ( [ primera ] ) we get ,    @xmath234    on the other hand , for each @xmath76 we have ,    @xmath235    where in the last equality we used again that @xmath222 implies @xmath233 to joint @xmath236    therefore , from ( [ i+ii ] ) and ( [ unosolo ] ) we get @xmath237 observe that , if @xmath238 for all @xmath77 we get @xmath239 .",
    "we would like to thank gerard biau and james malley for helpful suggestions .",
    "we also thanks to the referees for their helpful suggestions which improved the presentation of this final version .",
    "9    ballo , a. , cuevas , a. , and fraiman , r. ( 2011 ) classification methods for functional data . in : ferraty ,",
    "f. and romain , y. ( eds . ) , the oxford handbook of functional data analysis .",
    "osford university press , oxford , 259297 .",
    "bongiorno , e. , salinelli , e. , goia , a. and vieu , p. ( 2014 ) an overview of iwfos2014 .",
    "contributions in infinite - dimensional statistics and related topics . in : enea g. bongiorno , ernesto salinelli , aldo goia , philippe vieu ( eds . ) , societ editrice esculapio , 16 ."
  ],
  "abstract_text": [
    "<S> we introduce a nonlinear aggregation type classifier for functional data defined on a separable and complete metric space . </S>",
    "<S> the new rule is built up from a collection of @xmath0 arbitrary training classifiers . </S>",
    "<S> if the classifiers are consistent , then so is the aggregation rule . </S>",
    "<S> moreover , asymptotically the aggregation rule behaves as well as the best of the @xmath0 classifiers . </S>",
    "<S> the results of a small simulation are reported both , for high dimensional and functional data , and a real data example is analyzed .    _ </S>",
    "<S> keywords : _ functional data ; supervised classification ; non - linear aggregation . </S>"
  ]
}