{
  "article_text": [
    "we consider message passing algorithms in factor graphs @xcite , @xcite . if the factor graph has no cycles , the messages computed by the basic sum - product and max - product algorithms are _ exact _ summaries of the subgraph behind the corresponding edge .",
    "however , in many applications ( especially with continuous variables ) , complexity considerations suggest , or even dictate , the use of _ approximate _ or _ lossy _ summaries . for example , it is customary to use gaussian messages even in cases where the `` true '' ( sum - product or max - product ) messages are not gaussian , or to use scalar ( i.e. , single - variable ) messages instead of multi - dimensional ( i.e. , multi - variable ) messages . in this paper",
    ", we first formulate a general message update rule for lossy summaries / messages that is a nontrivial generalization of the standard sum - product or max - product rules .",
    "this rule was in essence proposed by minka @xcite , @xcite , but our general formulation of it may not be obvious from minka s work .",
    "we then focus on one particular application : the conversion of binary ( `` soft - bit '' ) messages into gaussian messages , which has many uses in communications . for our numerical examples ,",
    "we then further focus on equalization : we give simulation results for an iterative kalman equalizer both for a linear fir ( finite impulse response ) channel and for a linear iir ( infinite impulse response ) channel . for uncoded transmission",
    ", the new algorithm almost closes the gap between the bjcr algorithm and the lmmse ( linear minimum mean squared error ) equalizer ; for coded transmission , the new algorithm improves the performance of the iterative kalman equalizer at very little additional cost .",
    "it should be noted that the new message computation rule yields iterative algorithms even for cycle - free graphs .",
    "we also note that some sort of damping is usually required to stabilize the algorithm .    in this paper",
    ", we will use forney - style factor graphs as in @xcite where edges represent variables and nodes represent factors .",
    "consider the messages along a general edge ( variable ) @xmath0 in some factor graph as illustrated in fig .",
    "[ fig : genrulesetup ] .",
    "let @xmath1 be the `` true '' sum - product or max - product message which we want ( or need ) to replace by a message @xmath2 in some prescribed family of functions ( e.g. , gaussians ) .",
    "in such cases , most writers ( including these authors ) used to compute @xmath3 as some approximation of @xmath4 .",
    "however , the semantics of factor graphs suggests another approach .",
    "note that the factor graph of fig .",
    "[ fig : genrulesetup ] represents the function @xmath5 which the replacement of @xmath4 by @xmath3 will change into @xmath6 it is thus natural to first compute @xmath7 and then to compute @xmath3 from ( [ eqn : approxgraph ] ) .",
    "the approximation in ( [ eqn : genruleapprox ] ) must be chosen so that solving ( [ eqn : approxgraph ] ) for @xmath3 yields a function in the prescribed family .",
    "( 55,15)(0,0 ) ( 0,5)(10,10 ) ( 10,10)(1,0)35 ( 27.5,13)(0,0)@xmath0 ( 16.5,7.5)(1,0)7 ( 20,3)(0,0)@xmath4 ( 20,-3)(0,0)@xmath3 ( 38.5,7.5)(-1,0)7 ( 35,3)(0,0)@xmath8 ( 45,5)(10,10 )    important special cases of this general approach ( including the gaussian case ) were proposed as `` expectation propagation '' in @xcite and @xcite .",
    "the choice of a suitable approximation in ( [ eqn : genruleapprox ] ) will , in general , depend on the application .",
    "for many applications , a natural approach ( proposed and pursued by minka ) is to minimize the kullback - leibler divergence : @xmath9    in this paper , the approximate messages will always be gaussian .",
    "however , other families of functions can be used .",
    "for example , multivariable messages with a prescribed markov - chain structure were used in @xcite ; with hindsight , the update rule for such messages that was proposed in @xcite is indeed an example of the general scheme described here .",
    "a related idea was proposed in @xcite .",
    "we will now apply the general scheme of the previous section to the conversion of messages defined on the finite alphabet @xmath10 into gaussian messages .",
    "the setup is shown in fig .",
    "[ fig : bintogaussian ] , which is ( a part of ) a factor graph with an equality constraint between the real variable @xmath0 and the @xmath11-valued variable @xmath12 .",
    "( the equality constraint node in fig .",
    "[ fig : bintogaussian ] may formally be viewed as representing the factor @xmath13 , which is to be understood as a dirac delta in @xmath14 and a kronecker delta in  @xmath15 . ) the messages @xmath16 and @xmath17 are defined on the finite alphabet @xmath10 and the messages @xmath18 , @xmath19 , and @xmath20 are gaussians ; @xmath19 denotes the standard gaussian approximation and @xmath20 denotes the alternative gaussian approximation due to minka , as will be detailed below .",
    "( 75,35)(0,0 ) ( 0,10)(10,10 ) ( 10,15)(1,0)25 ( 23,17)(-1,0)6 ( 20,21)(0,0)@xmath17 ( 17,13)(1,0)6 ( 20,9)(0,0)@xmath16 ( 30,18)(0,0)@xmath12 ( 37.5,17.5)(37.5,35 ) ( 33,32)(0,0)[r]binary ( 42,32)(0,0)[l]gaussian ( 35,12.5)(5,5)@xmath21 ( 37.5,0)(37.5,12.5 ) ( 40,15)(1,0)25 ( 45,18)(0,0)@xmath0 ( 58,17)(-1,0)6 ( 55,21)(0,0)@xmath18 ( 52,13)(1,0)6 ( 55,9)(0,0)@xmath19 ( 55,3)(0,0)@xmath20 ( 65,10)(10,10 )    let us first recall the conversion of gaussian messages into soft - bit messages . let @xmath22 and @xmath23 be the mean and the variance , respectively , of @xmath18 . the ( lossless ) conversion from @xmath18 to @xmath17 is an immediate and standard application of the sum - product ( or max - product ) rule @xcite , @xcite : @xmath24 in the standard logarithmic representation , this becomes @xmath25    we now turn to the more interesting lossy conversion of @xmath16 into a gaussian",
    ". let @xmath26 and @xmath27 be the mean and the variance , respectively , of @xmath16 , which are given by @xmath28 the traditional approach forms the gaussian message @xmath19 ( with mean @xmath29 and variance @xmath30 ) from the mean and the variance of @xmath16 : @xmath31 the approach of section  [ sec : generalminkarule ] yields another gaussian message @xmath20 ( with mean @xmath32 and variance @xmath33 ) as follows . in fig .",
    "[ fig : bintogaussian ] , the true global function corresponding to ( [ eqn : trueglobalfunction ] ) is @xmath34 which ( when properly normalized ) has mean @xmath35 and variance @xmath36 where @xmath37 is the mean of @xmath17 ( [ eqn : muleftb ] ) , which is formed as in ( [ eqn : meanofmrightb ] ) .",
    "the approximate global function ( corresponding to ( [ eqn : approxgraph ] ) ) is the gaussian @xmath38 with mean @xmath39 and variance @xmath40 given by @xmath41    now a natural choice for the approximation ( [ eqn : genruleapprox ] ) is to equate the mean and the variance of the gaussian approximation with the corresponding moments of the true global function : @xmath42 ( as pointed out by minka , this choice may be derived from  ( [ eqn : mindivergence ] ) . )",
    "the desired gaussian message @xmath20 is thus obtained by first evaluating ( [ eqn : mtrue ] ) and ( [ eqn : sigmatrue ] ) and then computing @xmath33 and @xmath32 from ( [ eqn : sigmag ] ) and ( [ eqn : mg ] ) .",
    "note that , in general , the message @xmath20 is not trivial even if @xmath16 is neutral ( @xmath43 and @xmath44 ) .",
    "solving ( [ eqn : sigmag ] ) for @xmath33 may result in a negative value for @xmath33 .",
    "( this indeed happens in the examples to be described in section  [ sec : simulationresults ] . ) in such cases , @xmath20 is a correction factor ( not itself a probability mass function ) that tries to compensate for an overly confident @xmath18 .",
    "the product ( [ eqn : murightg_muleftg ] ) usually remains a valid probability mass function , up to a scale factor .      in our numerical experiments ( section  [ sec : simulationresults ] ) , simply replacing the standard gaussian message @xmath19 by @xmath20 yielded unstable algorithms .",
    "good results were obtained , however , by geometric mixtures of the form @xmath45 with @xmath46 . the mean and the variance of the resulting gaussian @xmath47",
    "are given by @xmath48 and @xmath49",
    "consider the transmission of binary ( @xmath11-valued ) symbols @xmath50 over a linear channel with transfer function @xmath51 and additive white gaussian noise @xmath52 .",
    "the received channel output symbols are @xmath53 with @xmath54 where we assume @xmath55 for @xmath56 .",
    "the binary symbols @xmath57 may or may not be coded .    ( 75,45)(0,0 ) ( 0,35)(60,10)code ( -5,29)(78,30 ) ( 65,33)binary ( 65,25)gaussian ( 5,20)(0,1)15 ( 4,25)(0,0)[r]@xmath58 ( 20,20)(0,1)15 ( 19,25)(0,0)[r]@xmath59 ( 37.5,25)(0,0 ) ",
    "( 55,20)(0,1)15 ( 54,25)(0,0)[r]@xmath60 ( 0,10)(60,10)channel model ( 5,0)(0,1)10 ( 4,3)(0,0)[r]@xmath61 ( 20,0)(0,1)10 ( 19,3)(0,0)[r]@xmath62 ( 37.5,3)(0,0 ) ",
    "( 55,0)(0,1)10 ( 54,3)(0,0)[r]@xmath63    ( 75,70)(0,0 ) ( -4,37)(0,0 )  ( 0,42.5)(1,0)15 ( 15,40)(5,5)@xmath64 ( 20,42.5)(1,0)15 ( 35,40)(5,5)@xmath65 ( 37.5,70)(0,-1)10 ( 36.5,66)(0,0)[r]@xmath57 ( 35,55)(5,5)@xmath66 ( 37.5,55)(0,-1)10 ( 40,42.5)(1,0)15 ( 55,40)(5,5)@xmath21 ( 60,42.5)(1,0)15 ( 79,37)(0,0 ) ",
    "( 57.5,40)(0,-1)10 ( 55,25)(5,5)@xmath67 ( 57.5,25)(0,-1)10 ( 55,10)(5,5)@xmath65 ( 57.5,10)(0,-1)10 ( 59,3)@xmath68 ( 35,10)(5,5 ) ( 40,12.5)(1,0)15 ( 47.5,15.5)(0,0)@xmath69    the joint code / channel factor graph is shown in fig .  [",
    "fig : codechannelgraph ] with channel - model details as in fig .",
    "[ fig : channelstatespacegraph ] .",
    "( in the uncoded case , the code graph is missing . )",
    "the factor graph shown in fig .",
    "[ fig : channelstatespacegraph ] results from writing ( [ eqn : transmissionmodel ] ) in state space form with suitable matrices @xmath64 , @xmath66 , and @xmath67 , where @xmath66 is a column vector and @xmath67 is a row vector , cf .",
    "@xcite .",
    "equalization is achieved by forward - backward gaussian message passing ( i.e. , kalman smoothing ) in the factor graph of fig .",
    "[ fig : channelstatespacegraph ] according to the recipes stated in @xcite .",
    "( see @xcite for a more detailed discussion . )    in this paper , we are only concerned with the messages along the edges @xmath57 ( towards the channel model ) in fig .",
    "[ fig : codechannelgraph ] . using the standard messages ( [ eqn : standardgaussianapprox ] ) results in an lmmse equalizer",
    "; in the uncoded case , this algorithm terminates after a single forward - backward sweep since the factor graph of fig .",
    "[ fig : channelstatespacegraph ] has no cycles .",
    "however , using the ( damped ) minka messages ( [ eqn : murightmixed])([eqn : mixedmean ] ) results in an iterative algorithm even in the uncoded case .",
    "simulation results for two different channels are given in figures [ fig : fir5_uncoded][fig : iir1_uncoded ] .",
    "figures [ fig : fir5_uncoded ] and  [ fig : fir5_coded ] show the bit error rate vs.  the signal - to - noise ratio ( snr ) for an fir channel with transfer function @xmath70 ; fig .",
    "[ fig : iir1_uncoded ] shows the bit error rate vs.  the snr for an iir channel with transfer function @xmath71 .",
    "the fir channel was used as an example in @xcite ; because this channel has a spectral null , the difference between a lmmse equalizer and the optimal bcjr equalizer is large .",
    "the iir channel was used as an example in @xcite .",
    "two different message update schedules are used : in schedule  a , the output messages ( along edge @xmath57 out of the channel model ) are initialized to `` infinite '' variance and are updated only after a complete forward - backward kalman sweep ; in schedule  b , these messages are updated ( and immediately used for the corresponding incoming minka message ) both during the forward kalman sweep and the backward kalman sweep . from our simulations ,",
    "schedule  b is clearly superior .",
    "it is obvious from figures [ fig : fir5_uncoded ] and  [ fig : iir1_uncoded ] that , for uncoded transmission , the minka messages provide a very marked improvement over the standard messages ( i.e. , over the lmmse equalizer ) . in fig .",
    "[ fig : fir5_uncoded ] , we almost achieve the performance of the bcjr ( or viterbi ) equalizer ( and we also outperform the decision - feedback equalizer @xcite ) . as for fig .",
    "[ fig : iir1_uncoded ] , we almost achieve the performance of the quasi - viterbi algorithm reported in @xcite .    for the coded example of fig .",
    "[ fig : fir5_coded ] , a rate 1/2 convolutional codes with constraint length  7 was used . in this case",
    ", the iterative kalman equalizer does quite well already with the standard input messages ( [ eqn : standardgaussianapprox ] ) , but the minka messages do give a further improvement at very small cost .",
    "a key issue with all these simulations is the choice of the damping / mixing factor @xmath72 in ( [ eqn : murightmixed])([eqn : mixedmean ] ) .",
    "the best results were obtained by changing @xmath72 in every iteration .",
    "typical good sequences of values of @xmath72 are plotted in fig .",
    "[ fig : alpha ] .",
    "we note the following observations :    * the initial values of @xmath72 are very small . * after a moderate number of iterations ( typically 10  20 ) , the bit error rate stops decreasing . at this point",
    ", @xmath72 is still very small .",
    "* many more iterations with slowly increasing @xmath72 are required to reach a fixed point with @xmath73 .",
    "* at such a fixed point with @xmath73 , the approximation  ( [ eqn : genruleapprox ] ) holds everywhere .",
    "bit error rate vs.  snr for uncoded binary transmission over fir channel with transfer function @xmath70.,width=321 ]    bit error rate vs.  snr for coded binary transmission over fir channel.,width=321 ]    bit error rate vs.  snr for uncoded binary transmission over iir channel with transfer function @xmath71.,width=321 ]    good sequences for @xmath72 vs.  the iteration number @xmath74.,width=302 ]",
    "elaborating on minka s work , we have formulated a general computation rule for lossy messages .",
    "an important special case is the conversion of `` soft - bit '' messages to gaussian messages . in this case",
    ", the resulting gaussian message is non - trivial even if the `` soft - bit '' message is neutral . by this method",
    ", the performance of a kalman equalizer is significantly improved .",
    "t.  p.  minka , `` expectation propagation for approximate bayesian inference , '' in _ proceedings of the 17th annual conference on uncertainty in artificial intelligence ( uai-01 ) , _ vol .",
    "17 , pp .  362369 , 2001 .",
    "j.  dauwels , h .- a .",
    "loeliger , p.  merkli , and m.  ostojic , `` on markov structured summary propagation and lfsr synchronization , '' _ proc .",
    "42nd allerton conf .  on communication , control , and computing , _",
    "( allerton house , monticello , illinois ) , sept .  29oct .  1 , 2004 , pp .  451460 ."
  ],
  "abstract_text": [
    "<S> elaborating on prior work by minka , we formulate a general computation rule for lossy messages . an important special case ( with many applications in communications ) </S>",
    "<S> is the conversion of `` soft - bit '' messages to gaussian messages . by this method , </S>",
    "<S> the performance of a kalman equalizer is improved , both for uncoded and coded transmission . </S>"
  ]
}