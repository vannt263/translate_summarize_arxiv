{
  "article_text": [
    "the k - means clustering problem is one of the most important problems in data mining and machine learning that has been widely studied .",
    "the problem is defined as follows :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * ( k - means problem ) * : given a set of @xmath9 points @xmath10 in a @xmath11-dimensional space , find a set of @xmath0 points @xmath12 such that the cost function @xmath13 is minimized . here",
    "@xmath14 denotes the square of the euclidean distance between points @xmath15 and @xmath8 . in the _ discrete _ version of this problem",
    "the centers are constrained to be a subset of the given points @xmath16 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the problem is known to be np - hard even for small values of the parameters such as when @xmath17  @xcite and when @xmath18  @xcite .",
    "there are various approximation algorithm for solving the problem .",
    "however , in practice , a heuristic known as the k - means algorithm ( also known as lloyd s algorithm ) is used because of its excellent performance on real datasets even though it does not given any performance guarantees .",
    "this algorithm is simple and can be described as follows :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * ( k - means algorithm ) * : ( i ) arbitrarily , pick @xmath0 points @xmath19 as centers .",
    "( ii ) cluster the given points based on the nearest distance of points to centers in @xmath19 .",
    "( iii ) for all clusters , find the mean of all points within a cluster and replace the corresponding member of @xmath19 with this mean .",
    "repeat steps ( ii ) and ( iii ) until convergence . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    even though the above algorithm performs very well on real datasets , it does not have any performance guarantees .",
    "this means that this _ local search _ algorithm may either converge to a local optimum solution or may take a large amount of time to converge  @xcite .",
    "poor choice of the initial @xmath0 centers ( step ( i ) ) is one of the main reasons for its bad performance with respect to approximation factor .",
    "a number of _ seeding _ heuristics have been suggested for picking the initial centers .",
    "one such seeding algorithm that has become popular is the k - means++ seeding algorithm .",
    "the algorithm is extremely simple and runs very fast in practice .",
    "moreover , this simple randomized algorithm also gives an approximation factor of @xmath4 in expectation  @xcite . in practice",
    ", this seeding technique is used for find the initial @xmath0 centers to be used with the k - means algorithm and this guarantees a theoretical approximation guarantee .",
    "the simplicity of the algorithm can be seen by its simple description below :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * ( k - means++ seeding ) * : pick the first center randomly from among the given points . pick a point to be the @xmath2 center ( @xmath20 ) with probability proportional to the square of the euclidean distance of this point to the previously @xmath3 chosen centers . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    a lot of recent work has been done in understanding the power of this simple sampling based approach for clustering .",
    "we discuss these in the following paragraph .",
    "arthur and vassilvitskii  @xcite showed that the sampling algorithm gives an approximation guarantee of @xmath4 in expectation .",
    "they also give an example dataset on which this approximation guarantee is best possible .",
    "et al . _",
    "@xcite and aggarwal _ et al . _",
    "@xcite show that sampling more than @xmath0 centers in the manner described above gives a constant pseudo - approximation .",
    "ackermann and blmer  @xcite showed that the results of arthur and vassilvitskii  @xcite may be extended to a large class of other distance measures .",
    "et al . _",
    "@xcite showed that the seeding algorithm may be modified appropriately to give a @xmath21-approximation algorithm for the k - means problem .",
    "jaiswal and garg  @xcite and agarwal _ et al . _",
    "@xcite showed that if the dataset satisfies certain separation conditions , then the seeding algorithm gives constant approximation with probability @xmath22 .",
    "et al . _",
    "@xcite showed that the seeding algorithm performs well even when fewer than @xmath0 sampling iterations are executed provided that more than one center is chosen in a sampling iteration .",
    "we now discuss our main results .",
    "the lower - bound examples of arthur and vassilvitskii  @xcite and aggarwal _ et al . _",
    "@xcite have the following two properties : ( a ) the examples are high dimensional and ( b ) the examples lower - bound the _ expected _ approximation factor .",
    "brunsch and rglin  @xcite discussed whether the k - means++ seeding gives better than @xmath4 approximation with probability @xmath23 .",
    "they constructed a high dimensional example where this is not true and a @xmath4 approximation is achieved only with probability exponentially small in @xmath0 .",
    "an important open problem mentioned in their work is to understand the behavior of the seeding algorithm on low - dimensional examples .",
    "this problem is also mentioned as an open problem by mahajan _",
    "et al . _",
    "@xcite who examined the hardness of the k - means problem on @xmath24-dimensional datasets . in this work ,",
    "we construct a two dimensional dataset on which the k - means++ seeding algorithm achieves an approximation ratio of @xmath8 ( for some universal constant @xmath8 ) with probability exponentially small in @xmath0 .",
    "following is the main theorem that we prove in this work .",
    "let @xmath25 and @xmath26 .",
    "consider the discrete version of the k - means problem .",
    "there exists a two dimensional dataset @xmath16 such that the probability that the k - means++ algorithm gives an approximation factor better than @xmath27 on @xmath16 is at most @xmath28 .    for the non - discrete version ,",
    "we get the above statement with approximation factor @xmath29 .      all the known lower - bound examples  @xcite have the following general properties :    1 .",
    "all optimal clusters have equal number of points .",
    "the optimal clusters are high dimensional simplices .    in order to construct a counterexample for the two dimensional case",
    ", we consider datasets that have different number of points in different optimal clusters .",
    "our counterexample is shown in figure  [ fig : example ] .",
    "note that the optimal clusters are points at the end of the vertical bars and the cluster sizes decreases exponentially going from left to right .",
    "we say that the seeding algorithm _ covers _ the @xmath2 optimal clusters if the algorithm picks a point from either of the ends of the @xmath2 vertical bar ( recall that this is the @xmath2 optimal cluster ) .",
    "the proof follows from the following three observations about this dataset :    * * observation 1 * : once the @xmath2 cluster gets covered , the probability of covering any cluster @xmath30 in subsequent rounds is roughly the same for any @xmath31 .",
    "moreover , there is a good chance that after the first few iterations , a cluster @xmath32 for some small @xmath32 gets covered . *",
    "* observation 2 * : the algorithm needs to cover more than some constant fraction of clusters to achieve good ( another constant ) approximation . * * observation 3 * : given that a small numbered cluster ( clusters are numbered from left to right ) is covered in the initial few iterations , the probability of covering more than certain constant fraction of clusters is exponentially small in @xmath0 .    in the next section ,",
    "we give the details of this proof .",
    "the figure means that there are @xmath33 points each located at @xmath34 and @xmath35 .",
    "there are @xmath36 points each located at @xmath37 and @xmath38 and so on . here",
    "are some simple observations regarding this example : the total number of points is @xmath39 .",
    "note that the optimal cost in the discrete version of the problem is @xmath40 .",
    "moreover , this is when one center from each of the @xmath0 clusters ( vertical bars ) is chosen .",
    "we denote the optimal clusters by @xmath41 from left to right .",
    "note that the number of points in these clusters drops exponentially .",
    "we say that an optimal cluster @xmath42 is _ covered _ by the k - means++ algorithm if the algorithm picks a point as a center from @xmath42 .",
    "since the number of points in the initial few clusters are large , there is a good chance that in the initial few iterations of the k - means++ seeding algorithm , one center from these initial few clusters are chosen .",
    "the next lemma shows this more formally .",
    "[ lemma-1 ] let @xmath43and let @xmath44 .",
    "let @xmath45 be the set of centers chosen by the k - means++ algorithm in the first @xmath46 iterations .",
    "then @xmath47 \\leq e^{-(\\alpha \\beta/3 ) k}\\ ] ]    for @xmath48 , let @xmath49 denote the event that @xmath50 does not cover any cluster in @xmath51 .",
    "we make the following observations : @xmath52 = 1 - \\frac{m + m/2 ^ 2 + ... + m/2^{2 \\beta k - 2}}{m + m/2 ^ 2 +   ... + m/2^{2k-2 } } = 1 - \\frac{1 - 1/2^{2\\beta k}}{1 - 1/2^{2k } } < 1/2^{2\\beta k}\\ ] ] we can also prove the following simple lemma .",
    "@xmath53 \\leq ( 1 - \\beta/3)$ ]    consider centers @xmath54 . let @xmath30 be the smallest integer such that @xmath55 covers @xmath42 .",
    "conditioned on the event @xmath56 , we have that @xmath57 .",
    "let us partition the optimal clusters into the following 3 parts : the first partition is @xmath51 , the second partition is @xmath58 and the third partition is @xmath59 .",
    "we note that @xmath60 = \\frac{\\phi_{c_{i-1}}(l)}{\\phi_{c_{i-1}}(l ) + \\phi_{c_{i-1}}(m ) + \\phi_{c_{i-1}}(r)}\\ ] ] note that @xmath61 , @xmath62 , and @xmath63 . using these inequalities",
    "we get the following : @xmath64 \\geq \\frac{1}{2 + ( 1/2)\\frac{k}{2^{2\\beta k } } } \\geq \\frac{\\beta}{3}\\\\ \\rightarrow & & { { \\bf pr}}[e_i | e_1 , ... , e_{i-1 } ] \\leq 1-\\frac{\\beta}{3}\\end{aligned}\\ ] ]    so we get that @xmath65 \\leq ( 1 - \\beta/3)^{\\alpha k } \\leq e^{-(\\alpha \\beta/3 ) k}$ ] .",
    "this completes the proof of lemma  [ lemma-1 ] .",
    "the next lemma shows that unless a large number of optimal clusters get covered , the approximation factor is bad .",
    "[ lemma-2 ] let @xmath19 denote the centers chosen by the k - means++ algorithm .",
    "if @xmath19 covers @xmath66 clusters , then @xmath67    note that @xmath68 .",
    "this is when all the optimal clusters are covered .",
    "suppose @xmath19 is such that @xmath69 clusters are covered by @xmath19 .",
    "let @xmath70 be an optimal cluster that is not covered with respect to @xmath19 .",
    "then we have : @xmath71 using this , we have : @xmath72 so , we get @xmath73 .",
    "we now need to show that the probability that k - means++ algorithm covers more than @xmath74 ( for some constant @xmath75 ) clusters is exponentially small in @xmath0 . to prove this , we define and analyze a random sampling procedure which may be of independent interest .      in this section",
    ", we analyze a sampling procedure that will help in the analysis of the behavior of k - means++ for our counterexample .",
    "this might be of independent interest .",
    "the procedure is defined as follows :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * sampball * : there are @xmath76 balls each colored with one of @xmath0 colors such that for each color there are exactly two balls with that color .",
    "@xmath0 balls are sampled randomly without replacement out of these @xmath76 balls . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    let @xmath77 be the random variable denoting the sampled set of of size @xmath0 .",
    "we are interested in bounding the probability that @xmath77 contains balls with more than @xmath78 different colors .",
    "let @xmath49 be the probability that @xmath77 contains of exactly @xmath32 distinct colors .",
    "the next lemma bounds the probability of the event @xmath49 .    for any @xmath79 ,",
    "@xmath80 \\leq \\frac{5}{\\sqrt{k } } \\cdot 2^{-k/16}$ ] .",
    "let us consider the following alternative procedure @xmath81 : whenever a new colored ball is sampled , the procedure outputs the letter `` n '' and if a ball with the same color has already been sampled , then @xmath81 outputs `` o '' . note that @xmath80 $ ] is equal to the probability that @xmath81 outputs a string in @xmath82 such that there are exactly @xmath32",
    "let @xmath83 be the random variable denoting the string output of @xmath81 .",
    "then we have : @xmath84 & = & { { \\bf pr}}[s \\textrm { has exactly } i \\textrm { n 's } ] \\\\ &",
    "\\leq & \\binom{k}{i } \\cdot { { \\bf pr}}[s = \\underbrace{nn ... n}_{i\\ terms } \\underbrace{oo ... o}_{(k - i)\\ terms } ] \\\\ & = & \\binom{k}{i } \\cdot \\frac{2k}{2k } \\cdot \\frac{2(k-1)}{2(k-1 ) + 1 } ...",
    "\\frac{2(k - i+1)}{2(k - i+1 ) + i-1 } \\cdot \\frac{i}{i+2(k - i ) } ...",
    "\\frac{2i - k+1}{k+1}\\\\ & = & \\binom{k}{i } \\cdot \\frac{2^i \\cdot ( k!)^2 \\cdot i!}{(k - i ) !",
    "\\cdot ( 2k ) ! \\cdot ( 2i - k)!}\\\\ & = & \\frac{2^i \\cdot ( k!)^3}{((k - i)!)^2 \\cdot ( 2k ) ! \\cdot ( 2i - k ) ! } \\\\ & \\leq & \\frac{2^{7k/8 } \\cdot ( k / e)^{3k } \\cdot e^3 \\cdot k^{3/2}}{2\\pi \\cdot ( k/8 ) \\cdot ( k/8e)^{k/4 } \\cdot \\sqrt{2\\pi } \\cdot \\sqrt{2k } \\cdot ( 2k / e)^{2k } \\cdot \\sqrt{2\\pi } \\cdot \\sqrt{k/2 } \\cdot ( 3k/4e)^{3k/4 } } \\\\ & = & \\frac{2 \\cdot e^3 \\cdot 2^{3k/4 + 7k/8 } \\cdot ( 4/3)^{3k/4}}{\\pi^2 \\cdot \\sqrt{k } \\cdot 2^{2k } } \\\\ & \\leq & \\frac{2 \\cdot e^3 \\cdot 2^{3k/4 + 7k/8 + 5k/16}}{\\pi^2 \\cdot \\sqrt{k } \\cdot 2^{2k } } \\\\ & \\leq & \\frac{5}{\\sqrt{k } } \\cdot 2^{-k/16}\\end{aligned}\\ ] ]    this gives us the following useful corollary .",
    "@xmath85 \\leq 5 \\sqrt{k } \\cdot 2^{-k/16}$ ] .",
    "the relationship of the sampling procedure * sampball * with the counter - example should not be very difficult to see .",
    "sampling a ball with new color corresponds to sampling a center from an uncovered cluster and so on .",
    "the main difference is that sampling a center from a new cluster is more likely than sampling a ball with new color .",
    "we modify our sampling procedure to be able to use the analysis for analyzing k - means++ over our counterexample .",
    "here is our new sampling procedure :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * biasedsampball * : there are @xmath76 balls each colored with one of @xmath0 colors such that for each color there are two balls with that color .",
    "@xmath0 balls are sampled randomly without replacement out of these @xmath76 balls .",
    "there is a bias towards sampling balls of new color . when sampling a ball the probability of sampling a ball of new color is at most @xmath86 times more than the probability of sampling a ball with color that has already been picked .",
    "_ _ _ _ _ _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    any value of @xmath87 will work for our purposes .",
    "we are interested in the probability that the above randomized procedure picks balls of at least @xmath88 different colors .",
    "let @xmath77 be the random variable denoting the sample of @xmath0 balls .",
    "let @xmath49 be the probability that @xmath77 contains of exactly @xmath32 distinct colors .",
    "next , we bound the probability of the event @xmath49 .",
    "first , we need the following simple lemma bounding a quantity we will later need .",
    "[ lemma : calc ] @xmath89 .",
    "we show the above lemma using the following calculations : @xmath90    for any @xmath91 and @xmath87 , @xmath80 \\leq \\frac{1}{\\sqrt{k } } \\cdot 2^{-k/64}$ ] .",
    "let us consider the following alternative procedure @xmath81 : whenever a new colored ball is sampled , the procedure outputs the letter `` n '' and if a ball with the same color has already been sampled , then @xmath81 outputs `` o '' . note that @xmath80 $ ] is equal to the probability that @xmath81 outputs a string in @xmath82 such that there are exactly @xmath32",
    "let @xmath83 be the random variable denoting the string output of @xmath81 .",
    "then we have : @xmath84 & = & { { \\bf pr}}[s \\textrm { has exactly } i \\textrm { n 's } ] \\\\ &",
    "\\leq & \\binom{k}{i } \\cdot { { \\bf pr}}[s = \\underbrace{nn ... n}_{i\\ terms } \\underbrace{oo ... o}_{(k - i)\\ terms } ] \\\\ & = & \\binom{k}{i } \\cdot \\frac{2\\gamma k}{2\\gamma k } \\cdot \\frac{2\\gamma(k-1)}{2\\gamma(k-1 ) + 1 } ...",
    "\\frac{2\\gamma(k - i+1)}{2\\gamma(k - i+1 ) + i-1 } \\cdot \\frac{i}{i+2\\gamma(k - i ) } ... \\frac{2i - k+1}{2i - k+1 + 2\\gamma(k - i)}\\\\ & \\leq & \\binom{k}{i } \\cdot \\frac{2k}{2k } \\cdot \\frac{2(k-1)}{(2k-1 ) - 1 \\cdot ( 1 - 1/\\gamma ) } ... \\frac{2(k - i+1)}{(2k - i+1 ) - ( i-1)\\cdot ( 1 - 1/\\gamma ) } \\cdot \\\\ & & \\qquad \\frac{i}{i+2\\gamma(k - i ) } ... \\frac{2i - k+1}{2i - k+1 + 2\\gamma(k - i)}\\\\ & \\leq & \\binom{k}{i } \\cdot \\frac{2k}{2k } \\cdot \\frac{2(k-1)}{(2k-1 ) - 1 \\cdot ( 4/5 ) } \\cdot \\frac{2(k-2)}{(2k-2 ) - 2 \\cdot ( 4/5 ) } ... \\frac{2(k - i+1)}{(2k - i+1 ) - ( i-1)\\cdot ( 4/5 ) } \\cdot \\\\ & & \\qquad \\frac{i}{i+2(k - i ) } ... \\frac{2i - k+1}{2i - k+1 + 2(k - i ) } \\quad \\textrm{(since $ 1 \\leq \\gamma \\leq 5$)}\\\\ & \\leq & \\binom{k}{i } \\cdot \\frac{2^i \\cdot k\\cdot ( k-1 ) ... ( k - i+1)}{(2k)\\cdot ( 2k - 1\\cdot ( 9/5 ) ) \\cdot ( 2k - 2\\cdot ( 9/5 ) ) ...",
    "( 2k - ( i-1)\\cdot ( 9/5 ) ) } \\cdot \\frac{i \\cdot ( i-1) ...",
    "(2i - k+1)}{(2k - i ) \\cdot ( 2k - i-1) ... (k+1 ) }   \\\\ & = & \\binom{k}{i } \\cdot   \\frac{k!}{(k - i ) ! } \\cdot \\frac{(k - ( 9/10)i)!}{k ! } \\cdot \\frac{1}{(k - ( 9/10)i)^{i/10 } } \\cdot \\frac{i!}{(2i - k ) ! } \\cdot \\frac{k!}{(2k - i ) ! } \\textrm{(using lemma~\\ref{lemma : calc})}\\\\ & = & \\binom{k}{i } \\cdot   \\frac{1}{(k - i ) ! }",
    "\\cdot \\frac{(k - ( 9/10)i)!}{1 } \\cdot \\frac{1}{(k - ( 9/10)i)^{i/10 } } \\cdot \\frac{i!}{(2i - k ) ! } \\cdot \\frac{k!}{(2k - i ) ! } \\\\ & = & \\frac{k ! \\cdot ( k - ( 9/10)i)!}{((k - i)!)^2 } \\cdot \\frac{1}{(k - ( 9/10)i)^{i/10 } } \\cdot \\frac{1}{(2i - k ) ! } \\cdot \\frac{k!}{(2k - i ) ! } \\\\ & \\leq & \\frac{1}{\\sqrt{k } } \\cdot 2^{-k/64}\\end{aligned}\\ ] ] note that the last step is obtained by using sterling s approximation and plotting the resulting function .",
    "this gives us the following useful corollary .",
    "@xmath92 \\leq \\sqrt{k } \\cdot 2^{-k/64}$ ] .      in this section",
    ", we will prove that the k - means++ algorithm covers at most @xmath74 clusters for some universal constant @xmath75 .",
    "this in conjunction with lemma  [ lemma-2 ] gives the main theorem .",
    "[ lemma : cover ] let @xmath93 and @xmath94 .",
    "then we have : @xmath95 \\leq ( 2\\sqrt{k } ) \\cdot 2^{-k/300}.\\ ] ]    let @xmath96 denote the event that after the first @xmath97 iterations , the first @xmath97 cluster are uncovered . note that from lemma  [ lemma-1 ] , we have : @xmath98 \\leq e^{-k/300}\\ ] ] note that conditioned on the event @xmath99 , we have that in the remaining @xmath100 iterations the probability of sampling a center from either end of any uncovered cluster from the @xmath100 rightmost clusters is at most @xmath101 times the probability of sampling from the uncovered end of a covered cluster .",
    "this is because the potential of the points at the uncovered end of a covered cluster is @xmath102 and the that of points in either end of an uncovered cluster is at most @xmath103 given that event @xmath99 happens . ]",
    "so , we can use the analysis of the previous section .",
    "we get that conditioned on the event @xmath99 , the probability that more than @xmath104 from the @xmath105 rightmost clusters will be covered is at most @xmath106 .",
    "so we have : @xmath107 & \\leq &   { { \\bf pr}}[e ] + \\\\ & & { { \\bf pr}}[\\textrm{k - means++ covers more than } \\eta k \\textrm { clusters } | \\neg e ] \\\\ & \\leq & e^{-k/300 } + ( \\sqrt{k } ) \\cdot 2^{-k/128}\\\\ & \\leq & ( 2\\sqrt{k } ) \\cdot 2^{-k/300}\\end{aligned}\\ ] ]    now the proof of the main theorem follows from the above lemma and lemma  [ lemma-2 ] .    from lemmas  [ lemma-2 ] and [ lemma :",
    "cover ] , we get that : @xmath108 \\leq ( 2\\sqrt{k})\\cdot 2^{-k/300}.\\ ] ]",
    "the k - median problem is similar to the k - means problem . here , the objective is to minimize the sum of euclidean distances rather than the sum of squares of the euclidean distances as in the k - means problem .",
    "that is , the objective function to be minimized is @xmath109 , where @xmath110 denotes the euclidean distance between points @xmath15 and @xmath8 . for the k - median problem , we make appropriate changes to the seeding algorithm .",
    "more specifically , we consider the following algorithm :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * ( smpalg ) * : pick the first center randomly from among the given points .",
    "pick a point to be the @xmath2 center ( @xmath20 ) with probability proportional to the euclidean distance of this point to the previously @xmath3 chosen centers . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the counterexample showing that the above algorithm achieves a fixed constant approximation with probability only exponentially small in @xmath0 is similar to the example in figure  1 . instead of the @xmath2 optimal cluster ( @xmath2 vertical bar ) containing @xmath111 points at either end , it contains @xmath112 points at either end . given this , note that the cost of the optimal clustering is @xmath113 .",
    "note that the analysis of the previous section can be easily extended for this case .",
    "in this work , we give a two dimensional example dataset on which the k - means++ seeding algorithm achieves a constant factor approximation ( for some universal constant ) with probability exponentially small in @xmath0 .",
    "this is only the first step towards understanding the behavior of k - means++ seeding algorithm on low - dimensional datasets .",
    "this addresses the open question of brunsch and rglin  @xcite .",
    "brunsch and rglin gave a @xmath114-dimensional instance where the k - means++ seeding algorithm achieves @xmath4 approximation factor with exponentially small probability and ask whether similar instances can be constructed in small dimension . an interesting open question is whether we can show that the seeding algorithm gives better than @xmath4 approximation factor on instances in small dimension .",
    "ragesh jaiswal would like to thank nitin garg and abhishek gupta who were involved in the initial stages of this project .",
    "meena mahajan , prajakta nimbhorkar , and kasturi varadarajan .",
    "the planar k - means problem is np - hard .",
    "theoretical computer science , volume 442 , pp .",
    "1321 , 13 july 2012 ."
  ],
  "abstract_text": [
    "<S> the k - means++ seeding algorithm is one of the most popular algorithms that is used for finding the initial @xmath0 centers when using the k - means heuristic . </S>",
    "<S> the algorithm is a simple sampling procedure and can be described as follows :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ pick the first center randomly from among the given points . for @xmath1 , pick a point to be the @xmath2 center with probability proportional to the square of the euclidean distance of this point to the previously @xmath3 chosen centers . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the k - means++ seeding algorithm is not only simple and fast but gives an @xmath4 approximation in expectation as shown by arthur and vassilvitskii  @xcite . </S>",
    "<S> there are datasets  @xcite on which this seeding algorithm gives an approximation factor @xmath5 in expectation . </S>",
    "<S> however , it is not clear from these results if the algorithm achieves good approximation factor with reasonably large probability ( say @xmath6 ) . </S>",
    "<S> brunsch and rglin  @xcite gave a dataset where the k - means++ seeding algorithm achieves an approximation ratio of @xmath7 only with probability that is exponentially small in @xmath0 . </S>",
    "<S> however , this and all other known _ lower - bound examples _ </S>",
    "<S> @xcite are high dimensional . </S>",
    "<S> so , an open problem is to understand the behavior of the algorithm on low dimensional datasets . in this work , </S>",
    "<S> we give a simple two dimensional dataset on which the seeding algorithm achieves an approximation ratio @xmath8 ( for some universal constant @xmath8 ) only with probability exponentially small in @xmath0 . </S>",
    "<S> this is the first step towards solving open problems posed by mahajan _ </S>",
    "<S> et al . _  </S>",
    "<S> @xcite and by brunsch and rglin  @xcite . </S>"
  ]
}