{
  "article_text": [
    "the standard quickest change detection problem is set to detect some unknown time point at which certain signal probability distribution changes over a sequence of observations .",
    "recently , with the development of wireless sensor networks , multiple sensors can be deployed to execute the quickest change detection , and the sensors can send quantized or unquantized observations or certain local decisions to a control center , who then makes a final decision  @xcite . most of the existing work is based on an assumption that the statistical properties of observations at all sensors change simultaneously . however , in certain scenarios , this assumption may not hold well .",
    "for instance , when multiple sensors are used to detect the occurrence of the chemical leakage , the sensors that are closer to the leakage source usually observe the change earlier than those far away from the source . in such cases , two interesting problems arise : one is to detect the change as soon as possible ; the other is to identify which sensor is the closest to the source , such that we could have a first - order inference over the leakage source location .",
    "as far as we know , currently there are few work studying the case of change occurring non - simultaneously . in the related work , the authors in @xcite proposed a scheme that each sensor makes a local decision with the computing burden at the local sensors , where they did not consider the identification problem . in @xcite , the authors modeled the change propagation process as a markov process to derive the optimal stopping rule and assumed that the change pattern always first reaches a predetermined sensor , such that the identification problem is ignored . in @xcite ,",
    "the identification problem for the special case of two sensors was studied , where the sufficient statistic is proven as a markov process and a joint optimal stopping rule and terminal decision rule are proposed .    in this paper",
    ", we study the joint change point detection and identification problem over a linear array of @xmath0 sensors , where the change first occurs near an unknown sensor , then propagates to sensors further away .",
    "we assume that all sensors send their observations to a control center . with the sequential observation signals , the control center first operates a stopping rule to decide when to alarm that the change has occurred",
    "; then the control center deploys a terminal decision rule to determine which sensor that the change pattern reaches first . in our setup , three performance metrics are of interest : i ) detection delay , which is the time interval between the moment that the change occurs and the moment that an alarm is raised ; ii ) false alarm probability , which is the probability that an alarm is raised before the actual change occurs ; and iii ) false identification probability , which is the probability that the control center does not correctly identify the sensor that the change pattern first reaches .",
    "we apply the markov optimal stopping time theory to design the optimal decision rules to minimize a weighted sum of the above three metrics .",
    "furthermore , we derive a scheme with a much simpler structure and certain performance guarantee .    the rest of this paper is organized as follows . in section  [ sec :",
    "model ] , we introduce the system model . in section  [ sec : solution ] , we derive the optimal decision rules . in section  [ sec : approx ] , we propose a scheme approximate to the optimal decision rules with a much lower complexity . in section",
    "[ sec : num ] , we present some numerical results , with conclusions in section  [ sec : con ] .",
    "we consider a scenario with @xmath0 sensors constructing a linear array to monitor the environment , as shown in fig .",
    "[ array ] . at an unknown time point ,",
    "a change occurs at an unknown location and propagates , where we use change point time @xmath1 to denote the time that the change pattern reaches sensor @xmath2 .",
    "we further use @xmath3 to denote the index of the sensor that the change pattern first reaches .",
    "we focus on the bayesian setup and use @xmath4 to denote the prior probability of @xmath5 , simply with @xmath6 . conditioned on the event that the change pattern first reaches sensor @xmath2 ,",
    "@xmath1 is assumed to bear a geometric distribution @xcite with parameter @xmath7 , @xmath8 i.e. , @xmath9=\\rho(1-\\rho)^k , k\\geq0,\\ ] ] where @xmath10 denotes the discretized time and takes integer values .",
    "we consider the practical factors in the environment , such as the wind or the blockers , which will affect the propagation speed of the change .",
    "for instance , see in fig .",
    "[ array ] , if the direction of the wind is from the left to the right side in the monitored scenario , then the propagation of the air pollution will be much faster at the right side of sensor @xmath11 than that of the left side . and at the same side",
    ", the propagation follows the deterministic order shown as @xmath12    we further assume that after the change patten reaches the first sensor @xmath2 , for the right side of sensor @xmath2 , it will propagate from one sensor to another sensor following the geometric propagation models as @xmath13=\\rho_1(1-\\rho_1)^{k_{2}},j > i , k_2\\geq0,\\ ] ] while for the left side of sensor @xmath2 , the propagation follows @xmath14=\\rho_2(1-\\rho_2)^{k_{2}},j",
    "< i , k_2\\geq0,\\ ] ] where @xmath15 and @xmath16 are used to model possibly different propagation speed along each direction , e.g. , @xmath17 means the propagation speed is higher at the right side that that of the left side .",
    "taking above assumption , for @xmath11 , we define all possible @xmath18 events at time @xmath10 as follows : @xmath19 where @xmath20 denote the events that after the change pattern first reaching sensor @xmath2 , it propagates across the sensors sequentially at the right side of sensor @xmath2 .",
    "the number of events equals to the number of sensors at the right side plus 1 , i.e. , @xmath21 . @xmath22",
    "denote the events that after the change pattern reaching sensor @xmath2 and @xmath23 , it propagates across the sensors sequentially at the right side of sensor @xmath2 , and the number of events is also @xmath21 , which is the same for the case that after the change pattern reaching sensor @xmath2 , @xmath23 , @xmath24 and so on .",
    "since there are @xmath23 sensors at the left side of sensor @xmath2 and the event of no change pattern reaches any sensor is @xmath25 , the total number of possible events is @xmath18 .    at each time @xmath10",
    ", we assume that the observations @xmath26 $ ] from all sensors are available at a control center . for each sensor @xmath2 , conditioned on @xmath1 ,",
    "@xmath27 is identically and independently distributed ( iid ) according to @xmath28 before @xmath1 , and iid according to @xmath29 after @xmath1 , i.e. , @xmath30    the observation sequence @xmath31 generates a filtration @xmath32 with @xmath33 where @xmath34 denotes the smallest @xmath35-field in which @xmath36 is measurable and @xmath37 .",
    "we use @xmath38 to denote the probability measure that specifies the prior distribution of @xmath5 , the distribution of change point time , and the distribution of @xmath31 .",
    "we also use @xmath39 to denote the expectation under the probability measure @xmath38 .",
    "specifically , we use @xmath40 to denote the probability measure when @xmath11 .    with above setups , the control center needs to detect the earliest change point time @xmath37 as soon as it occurs .",
    "a stopping time @xmath41 will be decided for when to stop sampling and alarm that a change has occurred , where a false alarm may happen if @xmath42 .",
    "we target to minimize the averaged detection delay @xmath43 with keeping the false alarm probability @xmath44 $ ] small .",
    "in addition , we also require the control center to identify which sensor the change pattern reaches first .",
    "we adopt @xmath45 to denote the @xmath46-measurable terminal decision rule used by the control center to make the identification , and @xmath47 to denote the index of the sensor identified , i.e. , @xmath48 .",
    "a false identification occurs if @xmath49 , such that we also want to keep @xmath50 $ ] small .",
    "we use @xmath51 to denote the sequence of terminal decision rules . summarizing above ,",
    "our goal is to design a stopping time @xmath41 and a terminal decision rule @xmath52 that minimize the aggregated risk function defined as @xmath53+c_1\\mathbb{e}\\{(\\tau-\\gamma)^{+}\\}+c_2p[\\hat{b}_1\\neq b_1],\\ ] ] where @xmath54 and @xmath55 are appropriate constants that balance the three costs .",
    ", scaledwidth=50.0% ]",
    "with the posterior probabilities defined above , we denote @xmath63 .",
    "we first have the following theorem regarding the optimal terminal decision rule @xmath52 .    for any stopping time @xmath41 ,",
    "the optimal terminal decision rule is @xmath64 and we have @xmath65=\\mathbb{e}\\left\\{1-\\max\\left\\{p_\\tau^{1}, ... ,p_\\tau^{n}\\right\\}\\right\\}.\\end{aligned}\\ ] ]    the proof follows from proposition 4.1 of @xcite .",
    "theorem 1 implies that the optimal terminal decision rule is simply to choose the sensor that has the largest posterior probability .",
    "a similar situation also arises in the multiple hypothesis testing problem considered in @xcite .    using above optimal terminal decision rule",
    ", we can further express the optimization objective in as a function of the posterior probabilities defined in ( [ def_pi ] ) and ( [ def_p ] ) , as shown below .",
    "[ lem : cost ] for any stopping time @xmath41 , can be written as @xmath66    based on the bayesian s rule , we have @xmath67 = p[t_{0,k}\\left| { { { \\cal f}_k } } \\right . ] & = \\sum\\limits_{i = 1}^n { p[t_{0,k}\\left| { { { \\cal f}_k } } \\right.,b = i ] } p_k^i \\nonumber\\\\ & = \\sum\\limits_{i = 1}^n { { \\pi _ { 0,k\\left| i \\right . } } } p_k^i.\\end{aligned}\\ ] ]    further , according to proposition 5.1 in @xcite , @xmath68+c_1\\mathbb{e}\\{(\\tau-\\gamma)^{+}\\}\\nonumber\\\\ & = \\mathbb{e}\\left\\{p[\\tau<\\gamma|\\mathcal{f}_\\tau]+c_1\\sum \\limits_{k=0}\\limits^{\\tau-1}p[\\gamma\\leq k|\\mathcal{f}_k]\\right\\}\\nonumber\\\\ & = \\mathbb{e } \\left\\ { \\sum\\limits_{i = 1}^n { { \\pi _ { 0,\\tau \\left| i \\right . } } } p_\\tau ^i+ { c_1}\\sum\\limits_{k = 0}^{\\tau - 1 } \\left(1 - \\sum\\limits_{i = 1}^n { { \\pi _ { 0,k\\left| i \\right . } } } p_k^i \\right)\\right\\}.\\end{aligned}\\ ] ] by combining ( [ decisionrule1 ] ) , we complete the proof .",
    "furthermore , we have the following lemma regarding @xmath69 .",
    "there is a time - invariant function @xmath70 such that @xmath71 .",
    "we have @xmath72\\nonumber\\\\ & = \\frac{{f({{\\bf{z}}_k}\\left| { { { \\cal f}_{k - 1}},t_{j , k } } \\right.,{s } = i)p[t_{j , k}\\left| { { { \\cal f}_{k - 1}},s = i } \\right.]}}{{\\sum\\limits_{j = 0}^{i(n -i+ 1 ) } { f({{\\bf{z}}_k}\\left| { { { \\cal f}_{k - 1}},t_{j , k } } \\right.,{s } = i)p[t_{j , k}\\left| { { { \\cal f}_{k - 1}},s = i } \\right . ] } } } , \\end{aligned}\\ ] ] in which @xmath73          for another element @xmath78 in @xmath69 , we have @xmath79 \\\\ & = \\frac{{f({{\\bf{z}}_k}\\left| { { { \\cal f}_{k - 1}},{s } = i } \\right.)p[{s } = i\\left| { { { \\cal f}_{k - 1 } } } \\right.]}}{{\\sum\\limits_{n = 0}^{m-1 } { f({{\\bf{z}}_k}\\left| { { { \\cal f}_{k - 1}},{s } = n } \\right.)p[{s } = n\\left| { { { \\cal f}_{k - 1 } } } \\right . ] } } } , \\end{split}\\ ] ] where @xmath80,\\end{aligned}\\ ] ] which can be calculated by using ( [ direcltywrite ] ) and ( [ transition ] ) .",
    "hence , @xmath78 can also be computed by @xmath76 and @xmath77 .",
    "this lemma implies that the posterior probabilities @xmath69 can be recursively computed from @xmath76 and @xmath77 .",
    "combined with lemma  [ lem : cost ] , we know that @xmath69 is a sufficient statistic for the problem of minimizing ( [ formulation ] ) . thus , the problem at the hand is a markov stopping time problem .",
    "therefore , we could borrow results from the optimal stopping time theory to design the optimal decision rules for our problem .",
    "we first consider a finite time horizon case , in which one has to make a decision before a deadline @xmath81 , i.e. , @xmath82 .",
    "it is easy to check that the cost - to - go functions are @xmath83 where @xmath84    applying the optimal stopping time theory @xcite , we have the following theorem for the optimal decision rules .",
    "[ thm2 ] the optimal stopping time is obtained as @xmath85 with the optimal terminal decision rule is given in ( [ decisonrule ] ) .",
    "in the infinite time horizon case when @xmath86 , we have @xmath87 defined as @xmath88 since we have @xmath89 , @xmath90 , and the fact that all strategies allowed with deadline @xmath81 are also allowed with deadline @xmath91 .",
    "since the observations are memoryless and conditionally iid , @xmath87 is the same for all @xmath10 ; we then use @xmath92 to denote @xmath87 .",
    "thus , @xmath93 is derived as @xmath94 in which the interchange of @xmath95 and @xmath96 is allowed due to the dominated convergence theorem .",
    "therefore , when the deadline is infinite , the optimal stopping rule becomes @xmath97 with the optimal terminal decision rule is given in ( [ decisonrule ] ) .",
    "when @xmath0 is large , the optimal stopping rule does not have a simple structure , which makes the implementation highly costly . in this section ,",
    "we propose a much simpler rule which approximates to the optimal stopping rule .",
    "[ lemma3 ] the sequence @xmath98 is a supermartingale , i.e. , @xmath99    the proof follows from page 477 of @xcite , by using fatou s lemma .",
    "we can use lemma  [ lemma3 ] to derive the following approximation of the optimal stopping rule .    in the asymptotic case of the rare change occurring with @xmath100 , one approximation of the optimal stopping rule has the following simple structure @xmath101 where @xmath102 .",
    "and we use the optimal terminal decision rule specified in ( [ decisonrule ] ) .",
    "for the second part of ( [ a(t-1 ) ] ) , according to lemma  [ lemma3 ] , @xmath106f(\\mathbf{z}_t|\\mathcal{f}_{t-1})d\\mathbf{z}_t \\leq c_2(1-p_{t-1}^{i_{t-1}}).\\ ] ]    plugging the above two results ( [ firstpart ] ) and ( [ p2 ] ) into ( [ a(t-1 ) ] ) , we have @xmath107    in the sequel , we assume that @xmath108 equals to the right side of ( [ inequ ] ) . according to ( [ cost - to - go ] ) , we have if @xmath109 , @xmath110 if @xmath111 , @xmath112 we define the following transformation as @xmath113 then @xmath114 further we have @xmath115 and @xmath116 then , @xmath117 can be rewritten as @xmath118 we define @xmath119 and @xmath120 as @xmath121 @xmath122 then straightly we see that @xmath123 , @xmath124 , and @xmath125\\mathbb{i}\\left(\\left\\{\\sum\\limits_{j = 1}^{m-1 } { v_{t-1,j } } \\leq \\frac{1}{c_1}\\right\\}\\right).\\ ] ]    for the next steps , we follow the proof of theorem 2 of @xcite , which is skipped here . and",
    "additionally we use lemma  [ lemma3 ] .",
    "finally , it can be derived that @xmath126    and the test structure reduces to stopping when @xmath127    therefore , we have the structure of the stopping rule as stated in theorem 3 .",
    "regarding to theorem 3 , we have several notes as follows .",
    "\\1 ) from lemma  [ lemma3 ] and theorem  [ thm2 ] , we see that @xmath128 is a lower bound of the optimal stopping time , i.e. @xmath129 , in the case of @xmath100 .",
    "the supermartingale property shown in lemma  [ lemma3 ] plays an important role in deriving @xmath128 .",
    "the tightness of this lower bound is related to the relationship between @xmath130 and @xmath131 .",
    "the simulation results in section  [ sec : num ] show that @xmath132 and @xmath131 are quite close , which indicates that @xmath128 would be close to @xmath133 .",
    "\\2 ) from ( [ eq1_lm1 ] ) and ( [ v_kl ] ) , we have the testing statistic @xmath134 as @xmath135}{\\rho p[\\gamma > k\\left| { { { \\cal f}_k } } \\right.]}.\\end{aligned}\\ ] ] this structure conforms to the well - known shiryaev s procedure  @xcite , which is the optimal stopping rule for single sensor with iid observations and bayesian setting .",
    "given that it is hard to efficiently compute the solution structure in ( [ stoprule ] ) , we compute the approximate optimal stopping rule in ( [ appro_stoppingrule ] ) and simulate its performance .",
    "we assign 5 nodes constructing a linear sensor array and assume that @xmath136 and @xmath137 .",
    "the change point time is generated according to the geometric distribution with @xmath138 , @xmath139 and @xmath140 , respectively . according to ( [ eq1_lm1 ] ) , the false alarm probability with @xmath128 is @xmath141=\\mathbb{e}\\left\\{{\\sum\\limits_{i = 1}^n{{\\pi _ { n+1,\\tau_{app}\\left| i \\right . } } } p_{\\tau_{app}}^i}\\right\\}\\leq\\frac{c_1}{c_1+\\rho}=\\alpha.\\ ] ] thus we have @xmath142 , where @xmath143 is the maximum allowance for the false alarm probability , which could determine the required select @xmath54 value .        in fig .",
    "[ pfavsadd ] , we illustrate the relationships among the false alarm probability , the false identification probability , and the averaged detection delay .",
    "we see that as the averaged detection delay increases , the false alarm probability decreases .",
    "when the averaged detection delay becomes large , the false identification probability does not decrease much and a probability floor appears , which is due to the fact that only the samples between the time when the change pattern reaches the first sensor and the time when it reaches the second sensor can be used to effectively distinguish the sensor that the change pattern first reaches . since this part of the samples is limited , which will not increase with the detection delay , a false identification probability floor exists . in fig .",
    "[ p_figure ] , we draw the posterior probability @xmath144 over time , where we assume that the change pattern first reaches node 3 , and then propagates to node 4 . we see that as time goes , @xmath145 gradually becomes larger than the others , which indicates that node 3 should be identified . in fig .",
    "[ bound_figure ] , we show the relation between @xmath146 and @xmath147 in ( [ p2 ] ) .",
    "since ( [ p2 ] ) is the key in deriving the our simplified rule , the fact that these two curves are close suggests that the performance of our low - complexity rule might be close to that of the optimal stopping rule in ( [ stoprule ] ) and ( [ stoprule_infi ] ) .    vs.  @xmath148",
    "we have studied the quickest change point detection problem and the closest - node identification problem over a sensor array .",
    "we have proposed an optimal decision scheme combing the stopping rule and the identification rule to alarm the change happening and to determine the sensor closest to the change source .",
    "since the structure the obtained optimal scheme is complex and impractical to implement , we have further proposed a scheme with a much simpler structure ."
  ],
  "abstract_text": [
    "<S> in this paper , we consider the problem of quickest change point detection and identification over a linear array of @xmath0 sensors , where the change pattern could first reach any of these sensors , and then propagate to the other sensors . </S>",
    "<S> our goal is not only to detect the presence of such a change as quickly as possible , but also to identify which sensor that the change pattern first reaches . </S>",
    "<S> we jointly design two decision rules : a stopping rule , which determines when we should stop sampling and claim a change occurred , and a terminal decision rule , which decides which sensor that the change pattern reaches first , with the objective to strike a balance among the detection delay , the false alarm probability , and the false identification probability . </S>",
    "<S> we show that this problem can be converted to a markov optimal stopping time problem , from which some technical tools could be borrowed . furthermore , to avoid the high implementation complexity issue of the optimal rules </S>",
    "<S> , we develop a scheme with a much simpler structure and certain performance guarantee . </S>"
  ]
}