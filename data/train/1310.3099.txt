{
  "article_text": [
    "robust automatic speech recognition ( asr ) still represents a challenging research topic . the underlying obstacle , namely the mismatch of test and training data ,",
    "can be tackled by enhancing the observed speech signals or features in order to meet the usually _",
    "clean _ training conditions or by compensating for the _ distorted _ test conditions in the acoustic model of the asr system .",
    "methods that modify the acoustic model are in general termed as _ ( acoustic ) model - based _ or _ model compensation _ approaches and comprise inter alia the following sub - categories : so - called _ model adaptation _ techniques mostly update the parameters of the acoustic model , i.e. , of the hidden markov models ( hmms ) , prior to the recognition of a set of observed feature vectors .",
    "in contrast , _ decoder - based _ approaches re - adapt the hmm parameters for each observed feature vector .",
    "the most common decoder - based approaches are _ missing feature _ and _ uncertainty decoding _ that incorporate additional time - varying uncertainty information into the evaluation of hmms probability density functions ( pdfs ) .",
    "various model compensation techniques exhibit two ( more or less ) distinct steps that can occur interchangeably : first , the compensation parameters need to be estimated and , second , the actual compensation rule is applied to the acoustic model .",
    "the compensation rules can often be motivated based on an observation model that relates the clean and distorted feature vectors .    in this article",
    ", we show how the compensation rules can arise from the bayesian network representations of the observation models of several uncertainty decoding @xcite , missing feature @xcite , and model adaptation techniques @xcite .",
    "in addition , we give a bayesian network description of the generic uncertainty decoding approach of @xcite , of the maximum a posteriori ( map ) adaptation technique @xcite , and of some alternative hmm topologies @xcite .",
    "while bayesian networks have been sporadically employed before in this context @xcite , we give novel derivations for certain of the considered algorithms in order to fill some gaps with this article .    throughout this article , feature vectors",
    "are denoted by bold - face letters @xmath0 with time index @xmath1 .",
    "feature vector sequences are written as . without distinguishing a random variable from its realization , a pdf over a random variable @xmath2",
    "is denoted by @xmath3 . for a normally distributed random vector @xmath4 with mean vector @xmath5 and covariance matrix @xmath6 ,",
    "we write or @xmath7 to express that a family of random variables @xmath8 share the same statistics , we write @xmath9 or , in the gaussian case , @xmath10 with time - invariant mean vector @xmath11 and covariance matrix @xmath12 . finally , for a gaussian random variable @xmath4 conditioned on another random variable @xmath13",
    ", we write @xmath14 if the statistics of @xmath4 depend only on time through @xmath13 , i.e. , if @xmath15 and @xmath16 for @xmath17 and @xmath18 .",
    "the remainder of the article is organized as follows : after summarizing the employed bayesian network view in section  [ sec : bayes ] , this perspective is applied to uncertainty decoding in section  [ sec : uncdec ] , missing feature techniques in section  [ sec : missfea ] , and other model - based approaches in section  [ sec : model_adapt ] .",
    "finally , conclusions are drawn in section  [ sec : conclusion ] .",
    "we start by recalling the bayesian network perspective on acoustic model - based techniques that we use in sections  [ sec : uncdec ] , [ sec : missfea ] , and [ sec : model_adapt ] to compare different algorithms . given a sequence of observed feature vectors @xmath19 , the acoustic score @xmath20 of a sequence @xmath21 of conventional hmms , as depicted in fig .",
    "[ fig : uncdec ] a ) , is given by @xcite @xmath22 where @xmath23 and @xmath24 denotes a state sequence through @xmath21 superseding the explicit dependency on @xmath21 in the right - hand side of ( [ eqn : hmm ] ) and ( [ eqn : hmm_totalsplit ] ) .",
    "note that the pdf @xmath25 can be scaled by @xmath26 without influencing the discrimination capability of the acoustic score w.r.t . changing word sequences @xmath21 .",
    "we thus define @xmath27 for later use .",
    "[ c][c]@xmath28 [ l][l]@xmath29 [ l][l]@xmath30 [ l][l]@xmath31 [ l][l]@xmath32 [ l][l]@xmath33 [ l][l]@xmath34 [ l][l]@xmath33 [ l][l]@xmath34 [ l][l]@xmath35 [ l][l]@xmath36 [ l][l]a ) [ l][l]b )     the compensation rules of a wide range of model adaptation , missing feature and uncertainty decoding approaches can be expressed by modifying the bayesian network structure of a conventional hmm and applying the inference rules of bayesian networks @xcite  potentially followed by suitable approximations to ensure mathematical tractability .",
    "while some approaches postulate a certain bayesian network structure , others indirectly define a modified bayesian network by assuming an observed feature vector @xmath34 to be a _ distorted _ version of an underlying _ clean _ feature vector @xmath32 that is introduced as latent variable in the hmm as , e.g. , in fig .",
    "[ fig : uncdec ] b ) . in the latter case ,",
    "the relation of @xmath34 and @xmath32 can be expressed by an analytical observation model @xmath37 that incorporates certain compensation parameters @xmath36 : @xmath38 note that here it is not distinguished whether @xmath34 is the output of a front - end enhancement process or a noisy or reverberant observation that is directly fed into the recognizer . by converting the observation model to a bayesian network representation , the pdf @xmath39 in ( [ eqn : hmm_totalsplit ] ) can be derived exploiting the inference rules of bayesian networks @xcite . for the case of fig .",
    "[ fig : uncdec ] b ) , the observation likelihood in ( [ eqn : hmm_totalsplit ] ) becomes : @xmath40 where the precise functional form of @xmath41 depends on the assumptions on @xmath37 and the statistics @xmath42 of @xmath36 .    the abstract perspective",
    "taken in this contribution reveals a fundamental difference between model adaptation approaches on the one hand and missing feature and uncertainty decoding approaches on the other hand : model adaptation techniques usually assume @xmath36 to have constant statistics over time , i.e. , @xmath43 or to be a deterministic parameter of value @xmath44 , i.e. , @xmath45 where @xmath46 denotes the dirac distribution . missing feature and uncertainty decoding approaches in contrast typically assume @xmath42 to be a time - varying pdf . as exemplified in the following sections , the bayesian network view conveniently illustrates the underlying statistical dependencies of model - based approaches .",
    "if two approaches share the same bayesian network , their underlying joint pdfs over all involved random variables share the same decomposition properties .",
    "in contrast , major aspects are not reflected by a bayesian network : the particular functional form of the joint pdf , potential approximations to arrive at a tractable algorithm , as well as the estimation procedure for the compensation parameters .",
    "while some approaches estimate these parameters through an acoustic front - end , others derive them from clean or distorted data . in this article",
    ", we entirely focus on the compensation rules while ignoring the parameter estimation step .",
    "we also neglect approaches that apply a modified training method to conventional hmms without exhibiting a distinct compensation step , as it is , e.g. , the case for discriminative @xcite , multi - condition @xcite or reverberant training @xcite .",
    "in the following , we consider the compensation rules of several uncertainty decoding techniques from a bayesian network view . while the formulations in subsections [ ssec : arrow ] , [ ssec : dvc ] , [ ssec : jud ] , and [ ssec : remos ]",
    "are similarly given in the original papers , those in subsections [ ssec : splice ] , [ ssec : ion ] have not been presented before from this perspective .",
    "[ c][c]@xmath28 [ l][l]a ) [ l][l]b ) [ l][l]c ) [ l][l]d ) [ l][l]@xmath30 [ l][l]@xmath47 [ l][l]@xmath32 [ l][l]@xmath34 [ l][l]@xmath36 [ l][l]@xmath48 [ l][l]@xmath49      a quite general example of the perspective described in section  [ sec : bayes ] can be extracted from @xcite .",
    "the underlying observation model @xmath50 can be represented by the bayesian network in fig .",
    "[ fig : vertall ] a ) . according to the conditional independence properties of bayesian networks @xcite , the observation likelihood in ( [ eqn : hmm_totalsplit ] ) becomes : @xmath51 without loss of generality , a single gaussian pdf @xmath52 is assumed since , in the case of a gaussian mixture model ( gmm ) , the linear mismatch function can be applied to each gaussian component separately . in practice ,",
    "the covariance matrix @xmath53 could , e.g. , reflect the observation uncertainty of @xmath34 resulting from acoustic front - end processing .",
    "the concept of dynamic variance compensation @xcite is based on the algonquin model of speech distortion @xcite : @xmath54 with @xmath55 being a noise estimate of any noise tracking algorithm and @xmath56 a residual error term .",
    "since the analytical derivation of @xmath25 is intractable , an approximate pdf is evaluated based on the assumption of @xmath57 being gaussian and that the compensation can be applied to each gaussian component of the gmm separately such that the observation likelihood in ( [ eqn : hmm_totalsplit ] ) becomes according to fig .",
    "[ fig : vertall ] a ) : @xmath58 where the first approximation is all the more accurate , as @xmath59 is flat .",
    "the estimation of the moments @xmath60 , @xmath61 of @xmath57 represents the core of @xcite .      in order to derive a bayesian network representation of the uncertainty decoding version of splice ( stereo piecewise linear compensation for environment ) as proposed in @xcite , we first note from @xcite that one fundamental assumption is : @xmath62 where @xmath63 denotes a discrete region index . exploiting the symmetry of the gaussian pdf @xmath64 and defining @xmath65 , we identify the observation model to be @xmath66 given a certain region index @xmath48 .",
    "if we assume the region index @xmath48 to be independent of the clean speech feature vector @xmath32  which has not been explicitly stated in @xcite but seems reasonable to us since @xmath48 is a parameter of the distortion model  the observation model can be expressed by the bayesian network in fig .",
    "[ fig : vertall ] b ) with @xmath42 being a gmm : @xmath67 noting that then @xmath68 a possible compensation rule is @xmath69 an alternative compensation rule is used in @xcite by introducing a separate prior model @xmath70 and rewriting ( [ eqn : splice1 ] ) to become    &",
    "p(_n|q_n ) = p(_n|q_n ) p(_n|_n ) d_n &    @xmath71\\end{aligned}\\ ] ]      model - based joint uncertainty decoding @xcite assumes an affine observation model in the cepstral domain @xmath72 with @xmath73 depending on the considered gaussian component @xmath47 of the gmm @xmath74 the bayesian network is depicted in fig .",
    "[ fig : vertall ] c ) implying the following compensation rule : @xmath75 which can be analytically derived analogously to ( [ eqn : arrowood ] ) . in practice ,",
    "the compensation parameters @xmath76 are not estimated for each gaussian component @xmath47 but for each regression class comprising a set of gaussian components .",
    "[ l][l]a ) [ l][l]b ) [ c][c]@xmath28 [ c][c]@xmath28 [ l][l]@xmath77 [ l][l]@xmath29 [ l][l]@xmath30 [ l][l]@xmath78 [ l][l]@xmath31 [ l][l]@xmath32 [ l][l]@xmath79 [ l][l]@xmath80 [ l][l]@xmath33 [ l][l]@xmath34 [ l][l]@xmath81 [ l][l]@xmath35 [ l][l]@xmath36 [ l][l ] [ l][l ] ) . the figure is based on @xcite.,title=\"fig : \" ]    as many other techniques , the remos ( reverberation modeling for speech recognition ) concept assumes the environmental distortion to be additive in the melspectral domain",
    ". however , remos also considers influence of the @xmath82 previous clean speech feature vectors @xmath83 in order to model the dispersive effect of reverberation and to relax the conditional independence assumption of conventional hmms .",
    "the observation model reads in the logarithmic melspectral domain : @xmath84 where the normally distributed random variables @xmath85 model the additive noise components , the early part of the room impulse response ( rir ) , and the weighting of the late part of the rir , respectively , and the parameters @xmath86 represent a deterministic description of the late part of the rir .",
    "the bayesian network is depicted in fig .",
    "[ fig : remos ] with @xmath87 $ ] .",
    "in contrast to most of the other compensation rules discussed in this article , the remos concept necessitates a modification of the viterbi decoder due to the introduced cross - connections .",
    "we refer to @xcite for the derivation of the corresponding decoding routine .",
    "[ c][c]@xmath28 [ l][l]a ) [ l][l]b ) [ l][l]@xmath29 [ l][l]@xmath30 [ l][l]@xmath31 [ l][l]@xmath32 [ l][l]@xmath33 [ l][l]@xmath34 ) .,title=\"fig : \" ]    similar to remos , the generic uncertainty decoding approach proposed by @xcite considers cross - connections in the bayesian network in order to relax the conditional independence assumption of hmms .",
    "the concept at hand is an example of uncertainty decoding , where the compensation rule can be defined by a modified bayesian network structure  given in fig .",
    "[ fig : ion ] a )  without fixing a particular functional form of the involved pdfs via an analytical observation model . in order to derive the compensation rule , we start by introducing the sequence @xmath88 of latent clean speech vectors in each summand of ( [ eqn : hmm ] ) :    & p(_1:n , q_1:n ) = p(_1:n,_1:n , q_1:n ) d_1:n &    @xmath89 where we exploited the conditional independence properties defined by fig .  [ fig : ion ] a ) ( respecting the dashed links ) and dropped @xmath90 in the last line of ( [ eqn : ion1 ] ) as it represents a constant factor w.r.t . a varying state sequence @xmath24 . omitting the dashed links in fig .",
    "[ fig : ion ] a ) for each variable @xmath32 respectively turns the pdf in the numerator of ( [ eqn : ion1 ] ) into @xmath91 the denominator in ( [ eqn : ion1 ] ) can also be further decomposed if the dashed links in fig .",
    "[ fig : ion ] b ) are disregarded : @xmath92 which finally turns ( [ eqn : ion1 ] ) into the following simplified form : @xmath93 that is given in @xcite . due to the approximations in fig .",
    "[ fig : ion ] a ) and b ) , the compensation rule defined by ( [ eqn : ion2 ] ) exhibits the same decoupling as ( [ eqn : hmm_totalsplit ] ) and can thus be carried out without modifying the underlying decoder . in practice",
    ", @xmath59 is modeled as separate single gaussian density and @xmath94 as a separate markov process  which could again be represented in separate bayesian networks .",
    "in the following , we consider the compensation rules of several missing feature techniques from a bayesian network view . while the formulations in subsections  [ ssec : imputation ] and [ ssec : marginalization ]",
    "are already given in @xcite , those in subsections  [ ssec : mod_imp ] and [ ssec : sig_dec ] have not been presented before from this perspective .      a major subcategory of missing feature approaches is called _ feature vector imputation _ where each feature vector component @xmath95 is either classified as reliable ( @xmath96 ) or unreliable ( @xmath97 ) , where @xmath98 and @xmath99 denote the set of reliable and unreliable components of the @xmath100-th feature vector , respectively @xcite . while unreliable components are withdrawn and replaced by an estimate @xmath101 of the original observation @xmath102 , reliable components",
    "are directly `` plugged '' in the pdf . the score calculation in ( [ eqn : hmm_totalsplit ] )",
    "therefore becomes : @xmath103 with @xmath104 and @xcite @xmath105 with the general bayesian network in fig .",
    "[ fig : vertdet ] a ) .",
    "the second major subcategory of missing feature techniques is called _ marginalization _ where unreliable components are `` replaced '' by marginalizing over a clean - speech distribution @xmath106 that is usually not derived from the hmm but separately modeled .",
    "the posterior likelihood in ( [ eqn : imputation ] ) thus becomes @xcite : @xmath107 with the general bayesian network in fig .",
    "[ fig : vertdet ] a ) .",
    "the approach presented in @xcite again implicitly assumes the basic observation model @xmath108 , where @xmath56 denotes the uncertainty of the enhancement algorithm .",
    "the corresponding bayesian network is depicted in fig .",
    "[ fig : vertall ] a ) implying that the observation likelihood in ( [ eqn : nodenom ] ) becomes : @xmath109 where @xmath110 .",
    "note that if @xmath111 were determined to be the maximum of @xmath112 , it could be considered as a maximum a posteriori @xcite estimate .",
    "the recently proposed concept of significance decoding arises directly from ( [ eqn : mi ] ) by approximating the integral by its maximum integrand : @xmath113 both concepts of modified imputation and significance decoding are often used to model the observation uncertainty introduced by an acoustic front - end .",
    "in the following , we consider the compensation rules of several acoustic model adaptation and other model - based approaches from a bayesian network view . while the formulations in subsections  [ ssec : pmc ] , [ ssec : vts ] , [ ssec : rev_vts ] , and [ ssec : hirsch_raut_sehr ] are similarly given in related papers , those in subsections  [ ssec : cmllr ] , [ ssec : mllr ] , [ ssec : map ] , [ ssec : bayes_mllr ] , [ ssec : takiguchi ] , and [ ssec : cond_comb_hmm ] have not ( or only partly ) been presented before from this perspective .",
    "[ c][c]@xmath28 [ l][l]a ) [ l][l]b ) [ l][l]c ) [ l][l]@xmath29 [ l][l]@xmath30 [ l][l]@xmath32 [ l][l]@xmath31 [ l][l]@xmath33 [ l][l]@xmath34 [ l][l]@xmath36 [ l][l]@xmath48 [ l][l]@xmath49 [ l][l]@xmath29 [ l][l]@xmath30 [ l][l]@xmath47 [ l][l]@xmath33 [ l][l]@xmath34 [ l][l]@xmath114 ) and mllr ( subsection  [ ssec : mllr ] ) , and c ) map adaptation ( subsection  [ ssec : map ] ) .",
    "a detailed description is given in the text.,title=\"fig : \" ]      the concept of parallel model combination ( pmc ) assumes inter alia a linear relation of static clean and corrupted features in the melspectral domain , which reads in the logarithmic melspectral domain : @xmath115 where the deterministic parameter @xmath116 accounts for level differences between the clean speech @xmath32 and the distortion @xmath36 . under the assumption of stationary distortion ,",
    "i.e. , @xmath117 the underlying bayesian network corresponds to fig .",
    "[ fig : vertall ] a ) .",
    "since the resulting adapted pdf @xmath118 can not be derived in an analytical closed - form , a variety of approximations to the true pdf @xmath39 have been investigated @xcite . for nonstationary distortion , @xcite proposes to employ a separate noise hmm leading to the bayesian network representation of fig .",
    "[ fig : vertall ] d ) . marginalizing over the distortion state sequence @xmath119 as in ( [ eqn : hmm_totalsplit ] ) reveals the acoustic score to become    & p(_1:n | ) = _ p(_1:n , q_1:n,_1:n ) &    @xmath120 where @xmath121 \\label{eqn : noisehmm}\\end{aligned}\\ ] ] the overall acoustic score can be approximated by a 3d viterbi decoder , which can in turn be mapped onto a conventional 2d viterbi decoder @xcite .",
    "the vector taylor series ( vts ) approach of @xcite is based on the following observation model in the logarithmic melspectral domain : @xmath122 where @xmath123 captures short convolutive distortion and @xmath124 models additive noise components .",
    "the bayesian network is represented by fig .",
    "[ fig : vertall ] a ) with @xmath125 $ ] . note that in contrast to uncertainty decoding , @xmath42 is constant over time .",
    "as the adapted pdf is again of the form of equation ( [ eqn : pmc ] ) and thus analytically intractable , it is assumed that ( [ eqn : vts ] ) can , firstly , be applied to each gaussian component @xmath126 of the gmm @xmath127 individually and , secondly , be approximated by a taylor series around @xmath128 $ ] , where @xmath129 denotes the mean of the component @xmath126 .",
    "constrained maximum likelihood linear regression ( cmllr ) can be seen as the deterministic counterpart of joint uncertainty decoding ( subsection  [ ssec : jud ] ) with observation model @xmath130 and deterministic parameters @xmath131 .",
    "the adaptation rule of @xmath132 has the same form as ( [ eqn : jud ] ) with @xmath133 the bayesian network corresponds to fig .",
    "[ fig : vertdet ] b ) , where the use of regression classes is again reflected by the dependency of the observation model parameters on the gaussian component @xmath47 ( cf .",
    "subsection  [ ssec : jud ] ) .",
    "the concept of maximum likelihood linear regression ( mllr ) applied to the mean vector @xmath129 of each gaussian component @xmath134 can be considered as a simplified version of cmllr , fig .",
    "[ fig : vertdet ] b ) , that neglects the adaptation of variances : @xmath135 this principle is also known from other approaches that can be applied to both means and variances but in practice are often only carried out on the means ( e.g. , for the sake of robustness ) @xcite .",
    "we next describe the map adaptation applied to any parameters @xmath114 of the pdfs of an hmm . in map ,",
    "these parameters are considered as bayesian parameters , i.e. , random variables that are drawn once for all times as depicted in fig .",
    "[ fig : vertdet ] c ) @xcite . as a direct consequence",
    ", any two observation vectors @xmath136 are conditionally dependent given the state sequence .",
    "the predictive pdf in ( [ eqn : hmm ] ) therefore explicitly depends on the adaptation data that we denote as @xmath137 , @xmath138 , and becomes : @xmath139 where the posterior @xmath140 is approximated as dirac distribution @xmath141 at the mode @xmath142 : @xmath143 an iterative ( local ) solution to ( [ eqn : map2 ] ) is obtained by the em algorithm .",
    "note that due to the map approximation of the posterior @xmath144 , the conditional independence assumption is again fulfilled such that a conventional decoder can be employed .",
    "[ c][c]@xmath28 [ l][l]a ) [ l][l]b ) [ l][l]@xmath29 [ l][l]@xmath30 [ l][l]@xmath33 [ l][l]@xmath34 [ l][l]@xmath31 [ l][l]@xmath32 [ l][l]@xmath35 [ l][l]@xmath36 [ l][l]@xmath44   and b ) bayesian model adaptation.,title=\"fig : \" ]      as mentioned in section  [ sec : bayes ] , uncertainty decoding techniques allow for a time - varying pdf @xmath42 , while model adaptation approaches , such as in subsections  [ ssec : pmc ] , [ ssec : vts ] , and [ ssec : rev_vts ] , mostly set @xmath42 to be constant over time . in both cases , however , the `` randomized '' model parameter @xmath36 is assumed to be redrawn in each time step @xmath100 as in fig .",
    "[ fig : bayes_vs_uncdec ] a ) .",
    "in contrast , _",
    "bayesian estimation _  as mentioned in the previous subsection  usually refers to inference problems , where the random model parameters are drawn once for all times @xcite as in fig .",
    "[ fig : bayes_vs_uncdec ] b ) .",
    "another example of bayesian model adaptation , besides map , is bayesian mllr applied to the mean vector @xmath145 of each pdf @xmath146 : @xmath147 with @xmath148 $ ] being usually drawn from a gaussian distribution .",
    "here , we do not consider different regression classes and assume @xmath52 to be a single gaussian pdf since , in the case of a gmm , the linear mismatch function ( [ eqn : bayesianmllr ] ) can be applied to each gaussian component separately .",
    "the likelihood score in ( [ eqn : hmm ] ) thus becomes ) , we do not explicitly mention the dependency on the adaptation data @xmath137 . ] : @xmath149 this score can , e.g. , be approximated by a frame - synchronous viterbi search @xcite .",
    "another approach is to apply the bayesian integral in a frame - wise manner and use a standard decoder @xcite . in this case , the integral in ( [ eqn : bayesianint ] ) becomes : @xmath150 which in turn corresponds to the case depicted in fig .",
    "[ fig : bayes_vs_uncdec ] a ) with constant pdf @xmath151 = const ..    [ c][c]@xmath28 [ l][l]a ) [ l][l]b ) [ l][l]@xmath29 [ l][l]@xmath30 [ l][l]@xmath33 [ l][l]@xmath34 [ l][l]@xmath31 [ l][l]@xmath32 [ l][l]@xmath152 [ l][l]@xmath79 [ l][l]@xmath35 [ l][l]@xmath36 [ l][l]@xmath153 ) a ) before and b ) after approximation via an extended observation vector with @xmath154 .",
    "the figure is based on @xcite .",
    "see fig .",
    "[ fig : remos ] for a generalized version of subfig .",
    "a).,title=\"fig : \" ]      reverberant vts is an extension of conventional vts ( subsection  [ ssec : vts ] ) to capture the dispersive effect of reverberation .",
    "its observation model reads for static features in the logmelspec domain : @xmath155 with @xmath36 being an additive noise component modeled as normally distributed random variable and @xmath156 being a deterministic description of the reverberant distortion . for the sake of tractability , the observation model is approximated in a similar manner as in the vts approach .",
    "the concept at hand can be seen as an alternative to remos ( subsection  [ ssec : remos ] ) : while remos tailors the viterbi decoder to the modified bayesian network , reverberant vts avoids the computationally expensive marginalization over all previous clean - speech vectors by averaging the clean - speech statistics over all possible previous states and their gaussian components .",
    "thus , @xmath34 is assumed to depend on the extended clean - speech vector @xmath79 = @xmath157 $ ] , cf .",
    "[ fig : rev_adapt ] a ) vs. b ) .",
    "all three concepts @xcite , @xcite , and @xcite are tailored to model the dispersive effect of reverberation via an observation model that is similar to ( [ eqn : remos ] ) and ( [ eqn : revvts ] ) and reads in the logarithmic melspectral domain : @xmath158 where @xmath156 denotes a deterministic description of the reverberant distortion that are differently determined by the three approaches . the observation model ( [ eqn : hirsch_raut_sehr ] ) can be represented by the bayesian network in fig .",
    "[ fig : remos ] without the random component @xmath36 . both @xcite and @xcite",
    "use the `` log - add approximation '' @xcite to derive @xmath39 , i.e. , @xmath159 where @xmath160 and @xmath129 denote the mean of the @xmath47-th gaussian component of @xmath25 and @xmath146 , respectively .",
    "the previous means @xmath161 , @xmath162 , are averaged over all means of the corresponding gmm @xmath163 . on the other hand",
    ", @xcite employs the `` log - normal approximation '' @xcite to adapt @xmath25 according to ( [ eqn : hirsch_raut_sehr ] ) . while @xcite and @xcite perform the adaptation once prior to recognition and then use a standard decoder , the concept proposed in @xcite performs an online adaptation based on the best partial path @xcite .",
    "we like to recall at this point that there are a variety of other approximations to the statistics of the log - sum of ( mixtures of ) gaussian random variables ( as seen in subsections  [ ssec : dvc ] , [ ssec : remos ] , [ ssec : pmc ] , [ ssec : vts ] , [ ssec : rev_vts ] ) , ranging from different pmc methods @xcite to maximum @xcite , piecewise linear @xcite , and other analytical approximations @xcite .",
    "[ c][c]@xmath28 [ l][l]@xmath29 [ l][l]@xmath30 [ l][l]@xmath31 [ l][l]@xmath32 [ l][l]@xmath33 [ l][l]@xmath34 ) .,title=\"fig : \" ]      in contrast to the approaches of subsections  [ ssec : rev_vts ] and [ ssec : hirsch_raut_sehr ] , the concept proposed in @xcite assumes the reverberant observation vector @xmath164 at time @xmath165 to be an approximation to the reverberation tail at time @xmath100 : @xmath166 where @xmath167 and @xmath168 are deterministic parameters modeling short convolutive distortion and the weighting of the reverberation tail , respectively .",
    "thus , each summand in ( [ eqn : hmm ] ) becomes : @xmath169 with the bayesian network of fig .",
    "[ fig : taki ] .",
    "it seems interesting to note that ( [ eqn : takiguchi ] ) can be analytically evaluated as @xmath164 is observed and , thus , ( [ eqn : takiguchi ] ) represents a non - linear mapping @xmath37 of one random vector @xmath32 : @xmath170 with @xmath171 where @xmath172 denotes the jacobian w.r.t . @xmath34 .",
    "[ c][c]@xmath28 [ l][l]a ) [ l][l]b ) [ l][l]@xmath29 [ l][l]@xmath30 [ l][l]@xmath33 [ l][l]@xmath34 ) .",
    "a ) is based on @xcite.,title=\"fig : \" ]      we close this section by turning to two model - based approaches that can not be classified as `` model adaptation '' as they postulate different hmm topologies rather than adapting a conventional hmm . both approaches aim at relaxing the conditional independence assumption of conventional hmms in order to improve the modeling of the inter - frame correlation .",
    "the concept of conditional hmms thus models the observation @xmath34 as depending on the previous observations at time shifts @xmath173 .",
    "each summand in ( [ eqn : hmm ] ) therefore becomes : @xmath174 according to fig .",
    "[ fig : hmm_co_cond ] a ) .",
    "such hmms are also known as _ autoregressive hmms _",
    "@xcite .",
    "in contrast to conditional hmms , combined - order hmms assume the current observation @xmath34 to additionally depend on the previous hmm state @xmath175 : @xmath176 according to fig .  [",
    "fig : hmm_co_cond ] b ) .",
    "in this article , we described the compensation rules of several acoustic model - based techniques employing bayesian network representations in duality with their underlying observation models .",
    "the presented formulations aim both at providing a comprehensive overview of different concepts as well as to relate the various algorithms in order to reveal their similarities .",
    "we hereby hope to offer other researchers the possibility to more easily exploit or combine existing techniques and to link their own algorithms to the presented ones .",
    "while the bayesian network description does not reflect all sophistications related to the considered concepts , it generally seems to be a suitable language for illustrating and comparing a broad class of algorithms .",
    "l.  deng , j.  droppo , and a.  acero , `` dynamic compensation of hmm variances using the feature enhancement uncertainty computed from a parametric model of speech distortion , '' _ ieee transactions on speech and audio processing _",
    ", vol .  13 , no .  3 , pp .",
    "412421 , 2005 .",
    "j.  droppo , a.  acero , and l.  deng , `` uncertainty decoding with splice for noise robust speech recognition , '' in _ proceedings ieee international conference on acoustics , speech , and signal processing ( icassp ) _ , vol .  1 , 2002 , pp . 5760 .",
    "r.  maas , a.  sehr , t.  yoshioka , m.  delcroix , k.  kinoshita , t.  nakatani , and w.  kellermann , `` formulation of the remos concept from an uncertainty decoding perspective , '' in _ proceedings international conference on digital signal processing ( dsp ) _ , 2013 .",
    "d.  kolossa , a.  klimas , and r.  orglmeister , `` separation and robust recognition of noisy , convolutive speech mixtures using time - frequency masking and missing data techniques , '' in _ proceedings ieee workshop on applications of signal processing to audio and acoustics ( waspaa ) _ , 2005 , pp .",
    "8285 .",
    "a.  acero , l.  deng , t.  kristjansson , and j.  zhang , `` hmm adaptation using vector taylor series for noisy speech recognition , '' in _ proceedings international conference on spoken language processing ( icslp ) _ , vol .  3 , 2000 , pp . 869872 .",
    "v.  v. digalakis , d.  rtischev , and l.  g. neumeyer , `` speaker adaptation using constrained estimation of gaussian mixtures , '' _ ieee transactions on speech and audio processing _ , vol .  3 , no .  5 ,",
    "pp . 357366 , 1995 .",
    "c.  j. leggetter and p.  c. woodland , `` maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models , '' _ computer speech and language _ , vol .  9 , no .  2 , pp .",
    "171185 , 1995 .",
    "y.  q. wang and m.  j.  f. gales , `` improving reverberant vts for hands - free robust speech recognition , '' in _ proceedings ieee workshop on automatic speech recognition and understanding ( asru ) _ , 2011 , pp .",
    "113118 .",
    "c.  k. raut , t.  nishimoto , and s.  sagayama , `` model adaptation for long convolutional distortion by maximum likelihood based state filtering approach , '' in _ proceedings ieee international conference on acoustics , speech , and signal processing ( icassp ) _ , vol .  1 , 2006 .    a.  sehr , r.  maas , and w.  kellermann , `` frame - wise hmm adaptation using state - dependent reverberation estimates , '' in _ proceedings ieee international conference on acoustics , speech , and signal processing ( icassp ) _ , 2011 , pp . 54845487",
    ".    t.  takiguchi , m.  nishimura , and y.  ariki , `` acoustic model adaptation using first - order linear prediction for reverberant speech , '' _ ieice transactions on information and systems _ , vol .",
    "e89-d , no .  3 , pp .",
    "908914 , 2006 .",
    "v.  ion and r.  haeb - umbach , `` a novel uncertainty decoding rule with applications to transmission error robust speech recognition , '' _ ieee transactions on audio , speech , and language processing _ , vol .",
    "16 , no .  5 , pp . 10471060 , 2008 .",
    "r.  maas , s.  r. kotha , a.  sehr , and w.  kellermann , `` combined - order hidden markov models for reverberation - robust speech recognition , '' in _ proceedings international workshop on cognitive information processing ( cip ) _ , 2012 , pp .",
    "167171 .",
    "r.  haeb - umbach , `` uncertainty decoding and conditional bayesian estimation , '' in _ robust speech recognition of uncertain or missing data _ , d.  kolossa and r.  haeb - umbach , eds.1em plus 0.5em minus 0.4emspringer , 2011 , pp .",
    "933 .",
    "g.  heigold , h.  ney , r.  schluter , and s.  wiesler , `` discriminative training for automatic speech recognition : modeling , criteria , optimization , implementation , and performance , '' _ ieee signal processing magazine _ , vol .",
    "29 , no .  6 , pp .",
    "5869 , 2012 .",
    "m.  matassoni , m.  omologo , d.  giuliani , and p.  svaizer , `` hidden markov model training with contaminated speech material for distant - talking speech recognition , '' _ computer speech and language _ , vol .",
    "16 , no .  2 ,",
    "pp . 205223 , apr .",
    "2002 .",
    "b.  frey , l.  deng , a.  acero , and t.  kristjansson , `` algonquin : iterating laplace s method to remove multiple types of acoustic distortion for robust speech recognition , '' in _ proceedings eurospeech _ , vol .  2 , 2001 , pp",
    ". 901904 .",
    "a.  sehr , r.  maas , and w.  kellermann , `` reverberation model - based decoding in the logmelspec domain for robust distant - talking speech recognition , '' _ ieee transactions on audio , speech , and language processing _ ,",
    "18 , no .  7 , pp . 16761691 , 2010 .",
    "r.  maas , a.  sehr , m.  gugat , and w.  kellermann , `` a highly efficient optimization scheme for remos - based distant - talking speech recognition , '' in _",
    "proceedings european signal processing conference ( eusipco ) _ , 2010 , pp . 19831987 .",
    "j.  r. hershey , s.  j. rennie , and j.  l. roux , `` factorial models for noise robust speech recognition , '' in _ techniques for noise robustness in automatic speech recognition _",
    ", t.  virtanen , r.  singh , and b.  raj , eds . 1em plus 0.5em minus 0.4emjohn wiley & sons , 2013 , pp . 311345 ."
  ],
  "abstract_text": [
    "<S> this article provides a bayesian network view on several acoustic model adaptation , missing feature , and uncertainty decoding approaches that are well - known in the literature of robust automatic speech recognition . </S>",
    "<S> the representatives of these classes can often be deduced from a bayesian network that extends the conventional hidden markov models used in speech recognition . </S>",
    "<S> the extensions , in turn , can in many cases be motivated from an underlying observation model that relates clean and distorted feature vectors . by converting the observation models into a bayesian network representation , </S>",
    "<S> we formulate the corresponding compensation rules leading to a unified view on known derivations as well as to novel derivations for certain approaches . the generic bayesian perspective provided in this contribution </S>",
    "<S> thus highlights structural differences and similarities between the analyzed approaches .    robust automatic speech recognition , bayesian network , model adaptation , missing feature , uncertainty decoding </S>"
  ]
}