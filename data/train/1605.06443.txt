{
  "article_text": [
    "structured prediction covers a broad family of important learning problems . these include key tasks in natural language processing such as part - of - speech tagging , parsing , machine translation , and named - entity recognition , important areas in computer vision such as image segmentation and object recognition , and also crucial areas in speech processing such as pronunciation modeling and speech recognition .    in all these problems ,",
    "the output space admits some structure .",
    "this may be a sequence of tags as in part - of - speech tagging , a parse tree as in context - free parsing , an acyclic graph as in dependency parsing , or labels of image segments as in object detection .",
    "another property common to these tasks is that , in each case , the natural loss function admits a decomposition along the output substructures . as an example , the loss function may be the hamming loss as in part - of - speech tagging , or it may be the edit - distance , which is widely used in natural language and speech processing .",
    "the output structure and corresponding loss function make these problems significantly different from the ( unstructured ) binary classification problems extensively studied in learning theory . in recent years",
    ", a number of different algorithms have been designed for structured prediction , including conditional random field ( crf ) @xcite , structsvm @xcite , maximum - margin markov network ( m3n ) @xcite , a kernel - regression algorithm @xcite , and search - based approaches such as @xcite .",
    "more recently , deep learning techniques have also been developed for tasks including part - of - speech tagging @xcite , named - entity recognition @xcite , machine translation @xcite , image segmentation @xcite , and image annotation @xcite .",
    "however , in contrast to the plethora of algorithms , there have been relatively few studies devoted to the theoretical understanding of structured prediction @xcite .",
    "existing learning guarantees hold primarily for simple losses such as the hamming loss @xcite and do not cover other natural losses such as the edit - distance .",
    "they also typically only apply to specific factor graph models .",
    "the main exception is the work of @xcite , which provides pac - bayesian guarantees for arbitrary losses , though only in the special case of randomized algorithms using linear ( count - based ) hypotheses .",
    "this paper presents a general theoretical analysis of structured prediction with a series of new results .",
    "we give new data - dependent margin guarantees for structured prediction for a broad family of loss functions and a general family of hypotheses , with an arbitrary factor graph decomposition .",
    "these are the tightest margin bounds known for both standard multi - class and general structured prediction problems . for special cases studied in the past ,",
    "our learning bounds match or improve upon the previously best bounds ( see section  [ sec : special ] ) .",
    "in particular , our bounds improve upon those of @xcite .",
    "our guarantees are expressed in terms of a data - dependent complexity measure , _ factor graph complexity _ , which we show can be estimated from data and bounded in terms of familiar quantities for several commonly used hypothesis sets along with a sparsity measure for features and graphs .",
    "we further extend our theory by leveraging the principle of voted risk minimization ( vrm ) and show that learning is possible even with complex factor graphs .",
    "we present new learning bounds for this advanced setting , which we use to design two new algorithms , _ voted conditional random field _ ( vcrf ) and _ voted structured boosting _ ( structboost ) .",
    "these algorithms can make use of complex features and factor graphs and yet benefit from favorable learning guarantees . as a proof of concept validating our theory",
    ", we also report the results of experiments with vcrf on several datasets .",
    "the paper is organized as follows . in section",
    "[ sec : scenario ] we introduce the notation and definitions relevant to our discussion of structured prediction . in section  [ sec : bounds ] , we derive a series of new learning guarantees for structured prediction , which are then used to prove the vrm principle in section  [ sec : vrm ] .",
    "section  [ sec : algo ] develops the algorithmic framework which is directly based on our theory . in section  [ sec : experiments ] , we provide some preliminary experimental results that serve as a proof of concept for our theory .",
    "let @xmath0 denote the input space and @xmath1 the output space . in structured prediction , the output space may be a set of sequences , images , graphs , parse trees , lists , or some other ( typically discrete ) objects admitting some possibly overlapping structure .",
    "thus , we assume that the output structure can be decomposed into @xmath2 substructures .",
    "for example , this may be positions along a sequence , so that the output space @xmath1 is decomposable along these substructures : @xmath3 . here , @xmath4 is the set of possible labels ( or classes ) that can be assigned to substructure @xmath5 .",
    "* loss functions*. we denote by @xmath6 a loss function measuring the dissimilarity of two elements of the output space @xmath1 .",
    "we will assume that the loss function @xmath7 is _ definite _ , that is @xmath8 iff @xmath9 .",
    "this assumption holds for all loss functions commonly used in structured prediction .",
    "a key aspect of structured prediction is that the loss function can be decomposed along the substructures @xmath4 . as an example",
    ", @xmath7 may be the hamming loss defined by @xmath10 for all @xmath11 and @xmath12 , with @xmath13 . in the common case where @xmath1 is a set of sequences defined over a finite alphabet",
    ", @xmath7 may be the edit - distance , which is widely used in natural language and speech processing applications , with possibly different costs associated to insertions , deletions and substitutions .",
    "@xmath7 may also be a loss based on the negative inner product of the vectors of @xmath14-gram counts of two sequences , or its negative logarithm .",
    "such losses have been used to approximate the bleu score loss in machine translation .",
    "there are other losses defined in computational biology based on various string - similarity measures .",
    "our theoretical analysis is general and applies to arbitrary bounded and definite loss functions .    * scoring functions and factor graphs*. we will adopt the common approach in structured prediction where predictions are based on a _ scoring function _ mapping @xmath15 to @xmath16 .",
    "let @xmath17 be a family of scoring functions .",
    "for any @xmath18 , we denote by @xmath19 the predictor defined by @xmath20 : for any @xmath21 , @xmath22 .",
    "furthermore , we will assume , as is standard in structured prediction , that each function @xmath18 can be decomposed as a sum .",
    "we will consider the most general case for such decompositions , which can be made explicit using the notion of _ factor graphs_. would then be @xmath23 of a probability . ]",
    "a factor graph @xmath24 is a tuple @xmath25 , where @xmath26 is a set of variable nodes , @xmath27 a set of factor nodes , and @xmath28 a set of undirected edges between a variable node and a factor node . in our context , @xmath26 can be identified with the set of substructure indices , that is @xmath29 .    for any factor node @xmath30 ,",
    "denote by @xmath31 the set of variable nodes connected to @xmath30 via an edge and define @xmath32 as the substructure set cross - product @xmath33 .",
    "then , @xmath20 admits the following decomposition as a sum of functions @xmath34 , each taking as argument an element of the input space @xmath21 and an element of @xmath32 , @xmath35 : @xmath36 figure  [ fig : factor_graph ] illustrates this definition with two different decompositions .",
    "more generally , we will consider the setting in which a factor graph may depend on a particular example @xmath37 : @xmath38 , f_i , e_i)$ ] .",
    "a special case of this setting is for example when the size @xmath39 ( or length ) of each example is allowed to vary and where the number of possible labels @xmath40 is potentially infinite .    [",
    "cols=\"^,^ \" , ]     -.2 in",
    "in this section , we consider algorithms based on the structboost  surrogate loss , where we choose @xmath41 .",
    "let @xmath42 .",
    "this then leads to the following optimization problem : @xmath43 one disadvantage of this formulation is that the first term of the objective is not differentiable .",
    "upper bounding the maximum by a sum leads to the following optimization problem : @xmath44 we refer to the learning algorithm based on the optimization problem   as vstructboost . to the best of our knowledge ,",
    "the formulations and are new , even with the standard @xmath45- or @xmath46-regularization .",
    "here , we show how the optimization problems in and can be solved efficiently when the feature vectors admit a particular factor graph decomposition that we refer to as markov property .",
    "we will consider in what follows the common case where @xmath1 is a set of sequences of length @xmath2 over a finite alphabet @xmath47 of size @xmath48 .",
    "other structured problems can be treated in similar ways .",
    "we will denote by @xmath49 the empty string and for any sequence @xmath50 , we will denote by @xmath51 the substring of @xmath52 starting at index @xmath53 and ending at @xmath54 . for convenience , for @xmath55 , we define @xmath56 by @xmath57 .",
    "one common assumption that we shall adopt here is that the feature vector @xmath58 admits a _",
    "markovian property of order @xmath59_. by this , we mean that it can be decomposed as follows for any @xmath60 : @xmath61 for some position - dependent feature vector function @xmath62 defined over @xmath63 $ ] .",
    "this also suggests a natural decomposition of the family of feature vectors @xmath64 for the application of vrm principle where @xmath65 is a markovian feature vector of order @xmath5 .",
    "thus , @xmath66 then consists of the family of markovian feature functions of order @xmath5 .",
    "we note that we can write @xmath67 with @xmath68 . in the following , abusing the notation , we will simply write @xmath65 instead of @xmath69 . thus , for any @xmath21 and @xmath70 , .",
    "] @xmath71 for any @xmath72 $ ] , let @xmath73 denote the position - dependent feature vector function corresponding to @xmath65 .",
    "also , for any @xmath21 and @xmath74 , define @xmath75 by @xmath76 .",
    "observe then that we can write @xmath77 in sections  [ sec : grad_vcrf ] and [ sec : grad_vsb ] , we describe algorithms for efficiently computing the gradient by leveraging the underlying graph structure of the problem .      in this section ,",
    "we show how gradient descent ( gd ) and stochastic gradient descent ( sgd ) can be used to solve the optimization problem of vcrf .",
    "to do so , we will show how the subgradient of the contribution to the objective function of a given point @xmath78 can be computed efficiently .",
    "since the computation of the subgradient of the regularization term presents no difficulty , it suffices to show that the gradient of @xmath79 , the contribution of point @xmath78 to the empirical loss term for an arbitrary @xmath80 $ ] , can be computed efficiently . in the special case of the hamming loss or when loss is omitted from the objective altogether ,",
    "this coincides with the standard crf training procedure .",
    "we extend this to more general families of loss function .",
    "fix @xmath80 $ ] . for the vcrf  objective ,",
    "@xmath79 can be rewritten as follows : @xmath81 the following lemma gives the expression of the gradient of @xmath79 and helps identify the key computationally challenging terms @xmath82 .",
    "[ lemma : vcrf_grad ] the gradient of @xmath79 at any @xmath83 can be expressed as follows : @xmath84 { \\widetilde}{{\\boldsymbol \\psi}}(x_i , { { { \\mathbf z } } } , s )     - \\frac{{{{\\mathbf \\psi}}}(x_i , y_i)}{m},\\ ] ] where , for all @xmath70 , @xmath85    in view of the expression of @xmath79 given above , the gradient of @xmath79 at any @xmath83 is given by @xmath86 - \\frac{{{{\\mathbf \\psi}}}(x_i , y_i)}{m}.\\end{aligned}\\ ] ] by , we can write @xmath87   = \\sum_{y \\in \\delta^l } { { { \\mathsf}q}}_{{{\\mathbf w}}}(y ) \\sum_{s = 1}^l { \\widetilde}{{\\boldsymbol \\psi}}(x_i , y_{s - p + 1}^s , s )    = \\sum_{s = 1}^l \\sum_{{{{\\mathbf z}}}\\in \\delta^p } \\bigg [ \\sum_{y\\colon y_{s - p +    1}^s = { { { \\mathbf z } } } } { { { \\mathsf}q}}_{{{\\mathbf w}}}(y ) \\bigg ] { \\widetilde}{{\\boldsymbol \\psi}}(x_i , { { { \\mathbf z } } } , s),\\end{aligned}\\ ] ] which completes the proof .",
    "the lemma implies that the key computation in the gradient is @xmath88 for all @xmath89 $ ] and @xmath90 .",
    "the sum defining these terms is over a number of sequences @xmath52 that is exponential in @xmath91 .",
    "however , we will show in the following sections how to efficiently compute @xmath92 for any @xmath89 $ ] and @xmath90 in several important cases : ( 0 ) in the absence of a loss ; ( 1 ) when @xmath7 is markovian ; ( 2 ) when @xmath7 is a _ rational loss _ ; and ( 3 ) when @xmath7 is the edit - distance or any other _",
    "tropical loss_.      in that case , it suffices to show how to compute @xmath93 and the following term , ignoring the loss factors : @xmath94 for all @xmath89 $ ] and @xmath90 .",
    "we will show that @xmath95 coincides with the flow through an edge of a weighted graph we will define , which leads to an efficient computation .",
    "we will use for any @xmath74 , the convention @xmath57 if @xmath55 .",
    "now , let @xmath96 be the weighted finite automaton ( wfa ) with the following set of states : @xmath97 with @xmath98 its single initial state , @xmath99 its set of final states , and a transition from state @xmath100 to state @xmath101 with label @xmath102 and weight @xmath103 , that is the following set of transitions : @xmath104 \\big\\}.\\end{aligned}\\ ] ] figure  [ fig : wfa ] illustrates this construction in the case @xmath105 .",
    "the wfa @xmath96 is deterministic by construction .",
    "the weight of a path in @xmath96 is obtained by multiplying the weights of its constituent transitions . in view of that",
    ", @xmath95 can be seen as the sum of the weights of all paths in @xmath96 going through the transition from state @xmath106 to @xmath107 with label @xmath108 .      for any state @xmath109 ,",
    "let @xmath110 denote the sum of the weights of all paths in @xmath96 from @xmath111 to @xmath112 and @xmath113 the sum of the weights of all paths from @xmath112 to a final state .",
    "then , @xmath95 is given by @xmath114 note also that @xmath115 is simply the sum of the weights of all paths in @xmath96 , that is @xmath116 .",
    "since @xmath96 is acyclic , @xmath117 and @xmath118 can be computed for all states in linear time in the size of @xmath96 using a single - source shortest - distance algorithm over the @xmath119 semiring or the so - called forward - backward algorithm .",
    "thus , since @xmath96 admits @xmath120 transitions , we can compute all of the quantities @xmath95 , @xmath89 $ ] and @xmath121 and @xmath115 , in time @xmath120 .",
    "we will say that a _ loss function @xmath7 is markovian _ if it admits a decomposition similar to the features , that is for all @xmath122 , @xmath123 in that case , we can absorb the losses in the transition weights and define new transition weights @xmath124 as follows : @xmath125 using the resulting wfa @xmath126 and precisely the same techniques as those described in the previous section , we can compute all @xmath92 in time @xmath120 . in particular , we can compute efficiently these quantities in the case of the hamming loss which is a markovian loss for @xmath127 .",
    "fix @xmath80 $ ] and let @xmath79 denote the contribution of point @xmath78 to the empirical loss in vstructboost . using the equality @xmath128 , @xmath79 can be rewritten as @xmath129 the gradient of @xmath79 can therefore be expressed as follows : @xmath130 efficient computation of these terms is not straightforward , since the sums run over exponentially many sequences @xmath52 . however , by leveraging the markovian property of the features , we can reduce the calculation to flow computations over a weighted directed graph , in a manner analogous to what we demonstrated for vcrf .        in this section",
    ", we describe an efficient algorithm for inference when using markovian features .",
    "the algorithm consists of a standard single - source shortest - path algorithm applied to a wfa @xmath126 differs from the wfa @xmath96 only by the weight of each transition , defined as follows : @xmath131 \\big\\}.\\end{aligned}\\ ] ] furthermore , here , the weight of a path is obtained by adding the weights of its constituent transitions .",
    "figure  [ fig : inference ] shows @xmath126 in the special case of @xmath105 . by construction ,",
    "the weight of the unique accepting path in @xmath126 labeled with @xmath74 is @xmath132 .",
    "thus , the label of the single - source shortest path , @xmath133 , is the desired predicted label .",
    "since @xmath126 is acyclic , the running - time complexity of the algorithm is linear in the size of @xmath126 , that is @xmath134 ."
  ],
  "abstract_text": [
    "<S> we present a general theoretical analysis of structured prediction with a series of new results . </S>",
    "<S> we give new data - dependent margin guarantees for structured prediction for a very wide family of loss functions and a general family of hypotheses , with an arbitrary factor graph decomposition . </S>",
    "<S> these are the tightest margin bounds known for both standard multi - class and general structured prediction problems . </S>",
    "<S> our guarantees are expressed in terms of a data - dependent complexity measure , _ factor graph complexity _ , which we show can be estimated from data and bounded in terms of familiar quantities for several commonly used hypothesis sets along with a sparsity measure for features and graphs . </S>",
    "<S> our proof techniques include generalizations of talagrand s contraction lemma that can be of independent interest .    </S>",
    "<S> we further extend our theory by leveraging the principle of voted risk minimization ( vrm ) and show that learning is possible even with complex factor graphs . </S>",
    "<S> we present new learning bounds for this advanced setting , which we use to design two new algorithms , _ voted conditional random field _ ( vcrf ) and _ voted structured boosting _ </S>",
    "<S> ( structboost ) . </S>",
    "<S> these algorithms can make use of complex features and factor graphs and yet benefit from favorable learning guarantees . </S>",
    "<S> we also report the results of experiments with vcrf on several datasets to validate our theory . </S>"
  ]
}