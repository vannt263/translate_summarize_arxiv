{
  "article_text": [
    "let @xmath0 be a distribution on @xmath1 , where @xmath2 is an arbitrary set equipped with a @xmath3-algebra .",
    "the goal of quantile regression is to estimate the conditional quantile , that is , the set - valued function @xmath4 | x ) \\geq \\tau     \\mbox { and } \\mathrm{p}([t,\\infty ) | x)\\geq     1-\\tau\\ }   , \\qquad x\\in   x , \\ ] ] where @xmath5 is a fixed constant specifying the desired quantile level and @xmath6 , @xmath7 , is the regular conditional probability of @xmath0 . throughout this paper , we assume that @xmath6 has its support in @xmath8 $ ] for @xmath9-almost all @xmath7 , where @xmath9 denotes the marginal distribution of @xmath0 on @xmath2 .",
    "( by a simple scaling argument , all our results can be generalized to distributions living on @xmath10 $ ] for some @xmath11 .",
    "the uniform boundedness of the conditionals @xmath6 is , however , crucial . )",
    "let us additionally assume for a moment that @xmath12 consists of singletons , that is , there exists an @xmath13 , called the conditional @xmath14-quantile function , such that @xmath15 for @xmath9-almost all @xmath7 .",
    "( most of our main results do not require this assumption , but here , in the introduction , it makes the exposition more transparent . )",
    "then one approach to estimate the conditional @xmath14-quantile function is based on the so - called _ @xmath14-pinball loss _ @xmath16 , which is defined by @xmath17 with the help of this loss function we define the @xmath18-risk of a function @xmath19 by @xmath20 recall that @xmath21 is up to @xmath9-zero sets the _ only _ function satisfying @xmath22 , where the infimum is taken over _ all _ measurable functions @xmath23 .",
    "based on this observation , several estimators minimizing a ( modified ) empirical @xmath18-risk were proposed ( see @xcite for a survey on both parametric and nonparametric methods ) for situations where @xmath0 is unknown , but i.i.d .",
    "samples @xmath24 drawn from @xmath0 are given .",
    "empirical methods estimating quantile functions with the help of the pinball loss typically obtain functions @xmath25 for which @xmath26 is close to @xmath27 with high probability . in general",
    ", however , this only implies that @xmath25 is close to @xmath21 in a very weak sense ( see @xcite , remark 3.18 ) but recently , @xcite , theorem 2.5 , established _ self - calibration inequalities _ of the form @xmath28 which hold under mild assumptions on @xmath0 described by the parameter @xmath29 $ ] .",
    "the first goal of this paper is to generalize and to improve these inequalities .",
    "moreover , we will use these new self - calibration inequalities to establish _ variance bounds _ for the pinball risk , which in turn are known to improve the statistical analysis of empirical risk minimization ( erm ) approaches .",
    "the second goal of this paper is to apply the self - calibration inequalities and the variance bounds to support vector machines ( svms ) for quantile regression .",
    "recall that @xcite proposed an svm that finds a solution @xmath30 of @xmath31 where @xmath32 is a regularization parameter , @xmath33 is a reproducing kernel hilbert space ( rkhs ) over @xmath2 , and @xmath34 denotes the empirical risk of @xmath35 , that is , @xmath36 . in @xcite robustness properties and consistency for _ all _ distributions @xmath0 on @xmath1 were established for this svm , while @xcite worked out how to solve this optimization problem with standard techniques from machine learning .",
    "moreover , @xcite also provided an exhaustive empirical study , which shows the excellent performance of this svm .",
    "we have recently established an oracle inequality for these svms in @xcite , which was based on ( [ m1-ineq ] ) and the resulting variance bounds . in this paper , we improve this oracle inequality with the help of the new self - calibration inequalities and variance bounds .",
    "it turns out that the resulting learning rates are substantially faster than those of @xcite .",
    "finally , we briefly discuss an adaptive parameter selection strategy .",
    "the rest of this paper is organized as follows . in section",
    "[ results ] , we present both our new self - calibration inequality and the new variance bound .",
    "we also introduce the assumptions on @xmath0 that lead to these inequalities and discuss how these inequalities improve our former results in @xcite . in section [ rates ] , we use these new inequalities to establish an oracle inequality for the svm approach above . in addition , we discuss the resulting learning rates and how these can be achieved in an adaptive way . finally , all proofs are contained in section [ proofs ] .",
    "in order to formulate the main results of this section , we need to introduce some assumptions on the data - generating distribution @xmath0 . to this end , let @xmath37 be a distribution on @xmath38 and @xmath39 be its support . for @xmath40 , the _ @xmath14-quantile _ of @xmath37 is the set @xmath41 ) \\geq",
    "\\tau   \\mbox { and }   \\mathrm{q}([t,\\infty))\\geq     1-\\tau\\ }   .\\ ] ] it is well known that @xmath42 is a bounded and closed interval .",
    "we write @xmath43 which implies @xmath44 $ ] . moreover , it is easy to check that the interior of @xmath42 is a @xmath37-zero set , that is , @xmath45 . to avoid notational overload ,",
    "we usually omit the argument @xmath37 if the considered distribution is clearly determined from the context .",
    "[ distribut - type - q ] a distribution @xmath37 with @xmath46 $ ] is said to have a @xmath14-quantile of type @xmath47 if there exist constants @xmath48 $ ] and @xmath49 such that @xmath50 for all @xmath51 $ ] .",
    "moreover , @xmath37 has a @xmath14-quantile of type @xmath52 , if @xmath53 and . in this case , we define @xmath54 and @xmath55 ) - \\tau\\ } ,           & \\quad\\mbox{if } $     t^*_{\\mathrm{min } } = t^*_{\\mathrm{max}}$ , } \\ ] ] where we note that @xmath49 in both cases . for all @xmath56",
    ", we finally write @xmath57 .",
    "since @xmath14-quantiles of type @xmath58 are the central concept of this work , let us illustrate this notion by a few examples .",
    "we begin with an example for which all quantiles are of type @xmath59 .",
    "let @xmath60 be a distribution with @xmath61 $ ] , @xmath62 be a distribution with @xmath63 $ ] that has a density @xmath64 with respect to the lebesgue measure and @xmath65 for some @xmath66 .",
    "if @xmath64 is bounded away from @xmath67 , that is , @xmath68 for some @xmath69 and lebesgue - almost all @xmath70 $ ] , then @xmath37 has a @xmath14-quantile of type @xmath71 for all @xmath40 as simple integration shows . in this case , we set @xmath72 and @xmath73 .",
    "again , let @xmath60 be a distribution with @xmath61 $ ] , @xmath62 be a distribution with @xmath63 $ ] that has a lebesgue density @xmath64 , and @xmath65 for some @xmath66 . if , for a fixed @xmath40 , there exist constants @xmath69 and @xmath74 such that @xmath75 ,   \\\\   h(y ) & \\geq & b   \\bigl(y - t^*_{\\mathrm{max}}(\\mathrm{q } )   \\bigr)^p   , \\qquad   y\\in   [ t^*_{\\mathrm{max}}(\\mathrm{q}),1].\\end{aligned}\\ ] ] lebesgue - almost surely , then simple integration shows that @xmath37 has a @xmath14-quantile of type @xmath76 and we may set @xmath77 and @xmath78 .",
    "let @xmath60 be a distribution with @xmath61 $ ] and @xmath79 for some @xmath66 , where @xmath80 denotes the dirac measure at @xmath81 .",
    "if @xmath82 , we then have @xmath83 and @xmath84 ) = \\alpha \\nu   ( ( -\\infty , t^ * ) ) + 1-\\alpha$ ] , and hence @xmath85 is a @xmath14-quantile of type @xmath52 for all @xmath14 satisfying @xmath86 .",
    "let @xmath60 be a distribution with @xmath61 $ ] and @xmath87 for some @xmath88 $ ] with @xmath89 . if @xmath90 ) = 0 $ ] , we have @xmath91 )",
    "= ( 1-\\alpha-\\beta ) \\nu   ( ( -\\infty , t_{\\mathrm{min } } ] ) + \\alpha$ ] and @xmath92 ) )   + \\beta$ ] .",
    "consequently , @xmath93 $ ] is the @xmath94 )   + \\alpha$ ] quantile of @xmath37 and this quantile is of type @xmath52 .    as outlined in the introduction , we are not interested in a single distribution @xmath37 on @xmath38 but in distributions @xmath0 on @xmath1 .",
    "the following definition extends the previous definition to such @xmath0",
    ".    let @xmath95 $ ] , @xmath96 , and @xmath0 be a distribution on @xmath1 with @xmath97 $ ] for @xmath9-almost all @xmath7 .",
    "then @xmath0 is said to have a @xmath14-quantile of @xmath98-average type @xmath58 , if @xmath99 has a @xmath14-quantile of type @xmath58 for @xmath9-almost all @xmath7 , and the function @xmath100 $ ] defined , for @xmath9-almost all @xmath7 , by @xmath101 where @xmath102 is defined in definition [ distribut - type - q ] , satisfies @xmath103 .    to establish the announced self - calibration inequality",
    ", we finally need the distance @xmath104 between an element @xmath105 and an @xmath106 .",
    "moreover , @xmath107 denotes the function @xmath108 . with these preparations the self - calibration inequality reads as follows .",
    "[ main1 ] let @xmath18 be the @xmath14-pinball loss , @xmath95 $ ] and @xmath96 be real numbers , and @xmath109",
    ". moreover , let @xmath0 be a distribution that has a @xmath14-quantile of @xmath98-average type @xmath96 .",
    "then , for all @xmath110 $ ] , we have @xmath111    let us briefly compare the self - calibration inequality above with the one established in @xcite . to this end , we can solely focus on the case @xmath71 , since this was the only case considered in @xcite .",
    "for the same reason , we can restrict our considerations to distributions @xmath0 that have a unique conditional @xmath14-quantile @xmath112 for @xmath9-almost all @xmath7 . then theorem [ main1 ] yields @xmath113 for @xmath114 .",
    "on the other hand , it was shown in @xcite , theorem 2.5 , that @xmath115 under the _ additional _ assumption that the conditional widths @xmath116 considered in definition [ distribut - type - q ] are _ independent _ of @xmath117 .",
    "consequently , our new self - calibration inequality is more general and , modulo the constant @xmath118 , also sharper .",
    "it is well known that self - calibration inequalities for lipschitz continuous losses lead to variance bounds , which in turn are important for the statistical analysis of erm approaches ; see @xcite . for the pinball loss ,",
    "we obtain the following variance bound .",
    "[ main2 ] let @xmath18 be the @xmath14-pinball loss , @xmath95 $ ] and @xmath96 be real numbers , and @xmath119 let @xmath0 be a distribution that has a @xmath14-quantile of @xmath98-average type @xmath58 .",
    "then , for all @xmath110 $ ] , there exists an @xmath120 $ ] with @xmath121 for @xmath9-almost all @xmath7 such that @xmath122 where we used the shorthand @xmath123 for the function @xmath124 .",
    "again , it is straightforward to show that the variance bound above is both more general and stronger than the variance bound established in @xcite , theorem 2.6 .",
    "the goal of this section is to establish an oracle inequality for the svm defined in ( [ svm1 ] ) .",
    "the use of this oracle inequality is then illustrated by some learning rates we derive from it .",
    "let us begin by recalling some rkhs theory ( see , e.g. , @xcite , chapter 4 , for a more detailed account ) . to this end , let @xmath125 be a measurable kernel , that is , a measurable function that is symmetric and positive definite .",
    "then the associated rkhs @xmath33 consists of measurable functions .",
    "let us additionally assume that @xmath126 is bounded with @xmath127 , which in turn implies that @xmath33 consists of bounded functions and @xmath128 for all @xmath129 .",
    "suppose now that we have a distribution @xmath0 on @xmath130 . to describe the approximation error of svms we use the _ approximation error function _",
    "@xmath131 where @xmath18 is the @xmath14-pinball loss . recall that @xcite , lemma 5.15 and theorem 5.31 , showed that @xmath132 , if the rkhs @xmath33 is dense in @xmath133 and the speed of this convergence describes how well @xmath33 approximates the bayes @xmath18-risk @xmath27 .",
    "in particular , @xcite , corollary 5.18 , shows that @xmath134 for some constant @xmath135 and all @xmath32 if and only if there exists an @xmath129 such that @xmath136 for @xmath9-almost all @xmath7 .",
    "we further need the integral operator @xmath137 defined by @xmath138 it is well known that @xmath139 is self - adjoint and nuclear ; see , for example , @xcite , theorem 4.27 .",
    "consequently , it has at most countably many eigenvalues ( including geometric multiplicities ) , which are all non - negative and summable .",
    "let us order these eigenvalues @xmath140 .",
    "moreover , if we only have finitely many eigenvalues , we extend this finite sequence by zeros . as a result , we can always deal with a decreasing , non - negative sequence @xmath141 , which satisfies @xmath142 .",
    "the finiteness of this sum can already be used to establish oracle inequalities ; see  @xcite , theorem 7.22 .",
    "but in the following we assume that the eigenvalues converge even faster to zero , since ( a ) this case is satisfied for many rkhss and ( b ) it leads to better oracle inequalities . to be more precise",
    ", we assume that there exist constants @xmath143 and @xmath144 such that @xmath145 recall that ( [ eigenvalues - assump ] ) was first used in @xcite to establish an oracle inequality for svms using the hinge loss , while @xcite consider ( [ eigenvalues - assump ] ) for svms using the least - squares loss .",
    "furthermore , one can show ( see @xcite ) that ( [ eigenvalues - assump ] ) is equivalent ( modulo a constant only depending on @xmath146 ) to @xmath147 where @xmath148 denotes the @xmath149th ( dyadic ) entropy number @xcite of the inclusion map from @xmath33 into @xmath150 .",
    "in addition , @xcite shows that ( [ en ] ) implies a bound on expectations of random entropy numbers , which in turn are used in @xcite , chapter 7.4 , to establish general oracle inequalities for svms . on the other hand , ( [ en ] ) has been extensively studied in the literature .",
    "for example , for @xmath151-times differentiable kernels on euclidean balls @xmath2 of @xmath152 , it is known that ( [ en ] ) holds for @xmath153 .",
    "we refer to  @xcite , chapter 5 , and @xcite , theorem 6.26 , for a precise statement .",
    "analogously , if @xmath154 is some integer , then the sobolev space @xmath155 is an rkhs that satisfies ( [ en ] ) for @xmath156 , and this estimate is also asymptotically sharp ; see @xcite .",
    "we finally need the clipping operation defined by @xmath157 for all @xmath105 .",
    "we can now state the following oracle inequality for svms using the pinball loss .",
    "[ oracle - general ] let @xmath18 be the @xmath14-pinball loss and @xmath0 be a distribution on @xmath1 with @xmath97 $ ] for @xmath9-almost all @xmath7 . assume that there exists a function @xmath13 with @xmath158 for @xmath9-almost all @xmath7 and constants @xmath159 and @xmath160 $ ] such that @xmath161 for all @xmath110 $ ] .",
    "moreover , let @xmath33 be a separable rkhs over @xmath2 with a bounded measurable kernel satisfying @xmath162 .",
    "in addition , assume that ( [ eigenvalues - assump ] ) is satisfied for some @xmath143 and @xmath144 .",
    "then there exists a constant @xmath163 depending only on @xmath146 , @xmath164 , and @xmath165 such that , for all @xmath166 , @xmath167 and @xmath168 , we have with probability @xmath169 not less than @xmath170 that @xmath171    let us now discuss the learning rates obtained from this oracle inequality . to this end , we assume in the following that there exist constants @xmath135 and @xmath172 $ ] such that @xmath173 recall from @xcite , corollary 5.18 , that , for @xmath174 , this assumption holds if and only if there exists a @xmath14-quantile function @xmath21 with @xmath175 .",
    "moreover , for @xmath176 , there is a tight relationship between ( [ a2 ] ) and the behavior of the approximation error of the balls @xmath177 ; see @xcite , theorem  5.25 .",
    "in addition , one can show ( see @xcite , chapter 5.6 ) that if @xmath21 is contained in the real interpolation space @xmath178 , see @xcite , then ( [ a2 ] ) is satisfied for @xmath179 .",
    "for example , if @xmath155 is a sobolev space over a euclidean ball @xmath180 of order @xmath154 and @xmath9 has a lebesgue density that is bounded away from @xmath67 and @xmath181 , then @xmath182 for some @xmath183 $ ] implies ( [ a2 ] ) for @xmath184 .",
    "now assume that ( [ a2 ] ) holds .",
    "we further assume that @xmath185 is determined by @xmath186 , where @xmath187 then theorem [ oracle - general ] shows that @xmath188 converges to @xmath27 with rate @xmath189 ; see @xcite , lemma  a.1.7 , for calculating the value of @xmath190 .",
    "note that this choice of @xmath185 yields the best learning rates from theorem [ oracle - general ] .",
    "unfortunately , however , this choice requires knowledge of the usually unknown parameters @xmath191 , @xmath165 and @xmath146 . to address this issue ,",
    "let us consider the following scheme that is close to approaches taken in practice ( see @xcite for a similar technique that has a fast implementation based on regularization paths ) .",
    "[ conc - bas : tv - svm ] let @xmath33 be an rkhs over @xmath2 and @xmath192 be a sequence of finite subsets @xmath193 $ ] .",
    "given a data set @xmath194 , we define @xmath195 where @xmath196 and @xmath197",
    ". then we use @xmath198 to compute the svm decision functions @xmath199 and @xmath200 to determine @xmath185 by choosing a @xmath201 such that @xmath202 in the following , we call this learning method , which produces the decision functions @xmath203 , a training validation svm with respect to @xmath204 .",
    "training validation svms have been extensively studied in @xcite , chapter 7.4 . in particular , @xcite , theorem 7.24 , gives the following result that shows that the learning rate @xmath189 can be achieved without knowing of the existence of the parameters @xmath191 , @xmath165 and @xmath146 or their particular values .",
    "[ tv - svm - rate ] let @xmath205 be a sequence of @xmath206-nets @xmath207 of @xmath208 $ ] such that the cardinality @xmath209 of @xmath207 grows polynomially in @xmath210 .",
    "furthermore , consider the situation of theorem [ oracle - general ] and assume that ( [ a2 ] ) is satisfied for some @xmath172 $ ] .",
    "then the training validation svm with respect to @xmath192 learns with rate @xmath211 , where @xmath190 is defined by ( [ def - gamma ] ) .",
    "let us now consider how these learning rates in terms of risks translate into rates for @xmath212 to this end , we assume that @xmath0 has a @xmath14-quantile of @xmath98-average type @xmath58 , where we additionally assume for the sake of simplicity that @xmath213 . note that the latter is satisfied for all @xmath98 if @xmath214 , that is , if all conditional distributions are concentrated around the quantile at least as much as the uniform distribution ; see the discussion following definition [ distribut - type - q ] .",
    "we further assume that the conditional quantiles @xmath215 are singletons for @xmath9-almost all @xmath7 .",
    "then theorem [ main2 ] provides a variance bound of the form ( [ var - bound ] ) for @xmath216 , and hence @xmath190 defined in ( [ def - gamma ] ) becomes @xmath217 by theorem [ main1 ] we consequently see that ( [ rnorm ] ) converges with rate @xmath218 , where @xmath219 . to illustrate this learning rate ,",
    "let us assume that we have picked an rkhs @xmath33 with @xmath220 .",
    "then we have @xmath174 , and hence it is easy to check that the latter learning rate reduces  to @xmath221 for the sake of simplicity , let us further assume that the conditional distributions do not change too much in the sense that @xmath222 .",
    "then we have @xmath223 , and hence @xmath224 converges to zero with rate @xmath225 .",
    "the latter shows that the value of @xmath58 does not change the learning rate for ( [ excess ] ) , but only the exponent in ( [ excess ] ) .",
    "now note that by our assumption on @xmath0 and the definition of the clipping operation we have @xmath226 and consequently small values of @xmath58 emphasize the discrepancy of @xmath227 to @xmath21 more than large values of @xmath58 do . in this sense ,",
    "a stronger average concentration around the quantile is helpful for the learning process .",
    "let us now have a closer look at the special case @xmath71 , which is probably the most interesting case for applications .",
    "then we have the learning rate @xmath228 for @xmath229 now recall that the conditional median equals the conditional mean for _ symmetric _ conditional distributions @xmath230 . moreover ,",
    "if @xmath33 is a sobolev space @xmath231 , where @xmath154 denotes the smoothness index and @xmath2 is a euclidean ball in @xmath152 , then @xmath33 consists of continuous functions , and @xcite shows that @xmath33 satisfies ( [ eigenvalues - assump ] ) for @xmath232 .",
    "consequently , we see that in this case the latter convergence rate is optimal in a min",
    " max sense @xcite if @xmath9 is the uniform distribution .",
    "finally , recall that in the case @xmath174 , @xmath71 and @xmath222 discussed so far , the results derived in @xcite only yield a learning rate of @xmath233 for @xmath234 in other words , the earlier rates from @xcite are not only worse by a factor of @xmath235 in the exponent but also are stated in terms of the weaker @xmath133-norm .",
    "in addition , @xcite only considered the case @xmath71 , and hence we see that our new results are also more general .",
    "since the proofs of theorems [ main1 ] and [ main2 ] use some notation developed in @xcite and @xcite , chapter  3 , let us begin by recalling these . to this end , let @xmath18 be the @xmath14-pinball loss for some fixed @xmath40 and @xmath37 be a distribution on @xmath38 with @xmath46 $ ]",
    ". then @xcite defined the _",
    "inner @xmath18-risks _ by @xmath236 and the _ minimal inner @xmath18-risk _ was denoted by @xmath237 .",
    "moreover , we write @xmath238 for the set of exact minimizers .    our first goal is to compute the excess inner risks and the set of exact minimizers for the pinball loss . to this end recall that ( see @xcite , theorem 23.8 ) , given a distribution @xmath37 on @xmath38 and a measurable function @xmath239 we have @xmath240 with these preparations we can now show the following generalization of @xcite , proposition 3.9 .    [",
    "loss : pin - ball - more ] let @xmath18 be the @xmath14-pinball loss and @xmath37 be a distribution on @xmath38 with @xmath241 . then there exist @xmath242 $ ] with @xmath243)$ ] , and , for all @xmath244 , we have @xmath245 moreover , if @xmath246 , then we have @xmath247 and @xmath248 . finally , @xmath249 equals the @xmath14-quantile , that is , @xmath250 $ ] .",
    "obviously , we have @xmath251 ) + \\mathrm{q }   ( [ t^*_{\\mathrm{max}},\\infty ) ) = 1 + \\mathrm{q}(\\{t^*_{\\mathrm{max}}\\})$ ] , and hence we obtain @xmath252 ) \\leq \\tau + \\mathrm{q}(\\{t^*_{\\mathrm{max}}\\})$ ] . in other words , there exists a @xmath253 $ ] satisfying @xmath254 and @xmath255 )   = \\tau + q_+   . \\ ] ] let us consider the distribution @xmath256 defined by @xmath257 for all measurable .",
    "then it is not hard to see that @xmath258 .",
    "moreover , we obviously have @xmath259 for all @xmath105 .",
    "let us now compute the inner risks of @xmath18 with respect to @xmath256 . to this end , we fix a @xmath244 .",
    "then we have @xmath260 and @xmath261 and hence we obtain @xmath262 moreover , using ( [ bauer - h1 ] ) we find @xmath263 and since ( [ loss : pin - ball - more - h1 ] ) implies @xmath264 ) = \\tau + q_+$ ] , we thus obtain @xmath265 by considering the pinball loss with parameter @xmath266 and the distribution @xmath267 defined by @xmath268 , @xmath106 measurable , we further see that ( [ loss : pin - ball - more - a1-h1 ] ) implies @xmath269 where @xmath270 satisfies @xmath271 and @xmath272 . by ( [ loss : pin - ball - more - h1 ] ) we then find @xmath273)$ ] .",
    "moreover , if @xmath246 , the fact @xmath274 yields @xmath275 ) = \\mathrm{q}(\\{t^*_{\\mathrm{min}}\\ } ) + \\mathrm{q}(\\{t^*_{\\mathrm{max}}\\ } )   .\\ ] ] using the earlier established @xmath276 and @xmath277 , we then find both @xmath247 and @xmath248 .    to prove ( [ loss : pin - ball - more - a1 ] ) and ( [ loss : pin - ball - more - a2 ] ) , we first consider the case @xmath278 . then ( [ loss : pin - ball - more - a1-h1 ] ) and ( [ loss : pin - ball - more - a1-h2 ] ) yield @xmath279 , @xmath105 .",
    "this implies @xmath280 , and hence we conclude that ( [ loss : pin - ball - more - a1-h1 ] ) and ( [ loss : pin - ball - more - a1-h2 ] ) are equivalent to ( [ loss : pin - ball - more - a1 ] ) and ( [ loss : pin - ball - more - a2 ] ) , respectively .",
    "moreover , in the case @xmath281 , we have @xmath45 , which in turn implies @xmath282 ) = \\tau$ ] and @xmath283 . for @xmath284 $ ]",
    ", we consequently find @xmath285\\\\[-8pt ] & = & ( \\tau-1)\\int_{y < t^*_{\\mathrm{max}}}y   \\ , \\mathrm{d }   \\mathrm{q}(y ) + \\tau \\int_{y\\geq t^*_{\\mathrm{max}}}y   \\ , \\mathrm{d }   \\mathrm{q}(y )   , \\nonumber\\end{aligned}\\ ] ] where we used @xmath286 ) = \\tau$ ] and @xmath287 .",
    "since the right - hand side of ( [ h1 ] ) is independent of @xmath288 , we thus conclude @xmath289 for all @xmath284 $ ] .",
    "analogously , we find @xmath290 for all @xmath291 , and hence we can , again , conclude @xmath279 for all @xmath105 .",
    "as in the case @xmath278 , the latter implies that ( [ loss : pin - ball - more - a1-h1 ] ) and ( [ loss : pin - ball - more - a1-h2 ] ) are equivalent to ( [ loss : pin - ball - more - a1 ] ) and ( [ loss : pin - ball - more - a2 ] ) , respectively .    for the proof of @xmath250",
    "$ ] , we first note that the previous discussion has already shown @xmath292 $ ] .",
    "let us assume that @xmath293 $ ] . by a symmetry argument",
    ", we then may assume without loss of generality that there exists a @xmath294 with @xmath295 .",
    "from ( [ loss : pin - ball - more - a1 ] ) we then conclude that @xmath296 and @xmath297 .",
    "now , @xmath296 together with ( [ loss : pin - ball - more - h1 ] ) shows @xmath298 )   = \\tau$ ] , which in turn implies @xmath299 )   \\geq \\tau$ ] .",
    "moreover , @xmath297 yields @xmath300 ) = 1-\\tau    .\\ ] ] in other words , @xmath288 is a @xmath14-quantile , which contradicts @xmath295 .    for the proof of theorem [ main1 ]",
    "we further need the _ self - calibration loss _ of @xmath18 that is defined by @xmath301 where @xmath37 is a distribution with @xmath46 $ ] .",
    "let us define the _ self - calibration function _",
    "by @xmath302 note that if , for @xmath105 , we write @xmath303 , then we have @xmath304 , and hence the definition of the self - calibration function yields @xmath305 in other words , the self - calibration function measures how well an @xmath306-approximate @xmath18-risk minimizer @xmath288 approximates the set of exact @xmath18-risk minimizers .      [ lower - pol ] for @xmath307 $ ] and @xmath96 consider the function @xmath308\\to [ 0,\\infty)$ ] defined by @xmath309 $ , \\cr q\\alpha^{q-1 } \\varepsilon - \\alpha^q(q-1 ) , & \\quad \\mbox{if } $   \\varepsilon\\in [ \\alpha,2]$. } \\ ] ] then , for all @xmath310 $ ] , we have @xmath311    since @xmath312 and @xmath56 we easily see by the definition of @xmath313 that the assertion is true for @xmath314 $ ] . now consider the function @xmath315\\to \\mathbb{r}$ ] defined by @xmath316.\\ ] ] it suffices to show that @xmath317 for all @xmath318 $ ] . to show the latter we first check that @xmath319\\ ] ] and hence we have @xmath320 for all @xmath318 $ ] .",
    "now we obtain the assertion from this , @xmath321 $ ] and @xmath322    [ self - cal - pinball - lower ] let @xmath18 be the @xmath14-pinball loss and @xmath37 be a distribution on @xmath38 with @xmath46 $ ] that has a @xmath14-quantile of type @xmath96 .",
    "moreover , let @xmath48 $ ] and @xmath49 denote the corresponding constants .",
    "then , for all @xmath310 $ ] , we have @xmath323    since @xmath18 is convex , the map @xmath324 is convex , and thus it is decreasing on @xmath325 $ ] and increasing on @xmath326 . using @xmath327 $ ] , we thus find @xmath328 for all @xmath329 . since this gives @xmath330 , we obtain @xmath331 let us first consider the case @xmath47 .",
    "for @xmath332 $ ] , ( [ loss : pin - ball - more - a1 ] ) and ( [ q - type-2 ] ) then yield @xmath333 and , for @xmath334 $ ] , ( [ loss : pin - ball - more - a1 ] ) and ( [ q - type-2 ] ) yield @xmath335 for @xmath310 $ ] , we have thus shown @xmath336 , where @xmath313 is the function defined in lemma [ lower - pol ] for @xmath337 .",
    "furthermore , in the case @xmath52 and @xmath246 , proposition [ loss : pin - ball - more ] shows @xmath248 , and hence ( [ loss : pin - ball - more - a1 ] ) yields @xmath338 for all @xmath310 = [ 0,\\alpha_\\mathrm{q}]$ ] by the definition of @xmath339 and @xmath340 . in the case",
    "@xmath52 and @xmath278 , ( [ loss : pin - ball - more - h1 ] ) yields @xmath341 ) - \\tau \\geq b_\\mathrm{q}$ ] by the definition of @xmath339 , and hence ( [ loss : pin - ball - more - a1 ] ) again gives @xmath342 for all @xmath310 $ ] .",
    "finally , using ( [ loss : pin - ball - more - a2 ] ) instead of ( [ loss : pin - ball - more - a1 ] ) , we can analogously show @xmath343 for all @xmath344 $ ] and @xmath56 .",
    "by ( [ loss : self - cal - conv - ineq ] ) we thus conclude that @xmath345 for all @xmath310 $ ] .",
    "now the assertion follows from lemma [ lower - pol ] .",
    "proof of theorem [ main1 ] for fixed @xmath7 we write @xmath346 . by lemma  [ self - cal - pinball - lower ] and ( [ m1 ] )",
    "we obtain , for @xmath9-almost all @xmath7 , @xmath347 by taking the @xmath348th power on both sides , integrating and finally applying hlder s inequality , we then obtain the assertion .",
    "proof of theorem [ main2 ] let @xmath110 $ ] be a function .",
    "since @xmath215 is closed , there then exists a @xmath9-almost surely uniquely determined function @xmath120 $ ] that satisfies both latexmath:[\\[\\begin{aligned } f^*_{\\tau,\\mathrm{p}}(x ) & \\in & f^*_{\\tau,\\mathrm{p}}(x ) , \\\\    for @xmath9-almost all @xmath7 .",
    "let us write @xmath109 .",
    "we first consider the case @xmath350 , that is , @xmath351 .",
    "using the lipschitz continuity of the pinball loss @xmath18 and theorem [ main1 ] we then obtain @xmath352 since @xmath353 , we thus obtain the assertion in this case .",
    "let us now consider the case @xmath354 .",
    "the lipschitz continuity of @xmath18 and theorem [ main1 ] yield @xmath355 since for @xmath354 we have @xmath356 , we again obtain the assertion .",
    "proof of theorem [ oracle - general ] as shown in @xcite , lemma 2.2 , ( [ eigenvalues - assump ] ) is equivalent to the entropy assumption ( [ en ] ) , which in turn implies ( see @xcite , theorem 2.1 , and @xcite , corollary 7.31 ) @xmath357 where @xmath358 denotes the empirical measure with respect to @xmath359 and @xmath360 is a constant only depending on @xmath146 .",
    "now the assertion follows from @xcite , theorem 7.23 , by considering the function @xmath361 that achieves @xmath362 .",
    "mendelson , s. ( 2001 ) .",
    "geometric methods in the analysis of glivenko  cantelli classes . in _ proceedings of the 14th annual conference on computational learning theory _ ( d. helmbold and b. williamson , eds . ) 256272 .",
    "new york : springer .",
    "mendelson , s. ( 2001 ) .",
    "learning relatively small classes . in _ proceedings of the 14th annual conference on computational learning theory _",
    "( d. helmbold and b. williamson , eds . ) 273288 .",
    "new york : springer .",
    "steinwart , i. and christmann , a. ( 2008 ) . how svms can estimate quantiles and the median . in _ advances in neural information processing systems 20 _",
    "platt , d. koller , y. singer and s. roweis , eds . ) 305312 .",
    "cambridge , ma : mit press .",
    "steinwart , i. , hush , d. and scovel , c. ( 2009 ) .",
    "optimal rates for regularized least squares regression . in _ proceedings of the 22nd annual conference on learning theory _",
    "( s. dasgupta and a. klivans , eds . ) 7993 .",
    "available at http://www.cs.mcgill.ca/\\textasciitilde colt2009/papers/038.pdf#page=1[http://www.cs.mcgill.ca/~colt2009/papers/038.pdf#page=1 ] ."
  ],
  "abstract_text": [
    "<S> the so - called pinball loss for estimating conditional quantiles is a well - known tool in both statistics and machine learning . so far , </S>",
    "<S> however , only little work has been done to quantify the efficiency of this tool for nonparametric approaches . </S>",
    "<S> we fill this gap by establishing inequalities that describe how close approximate pinball risk minimizers are to the corresponding conditional quantile . </S>",
    "<S> these inequalities , which hold under mild assumptions on the data - generating distribution , are then used to establish so - called variance bounds , which recently turned out to play an important role in the statistical analysis of ( regularized ) empirical risk minimization approaches . </S>",
    "<S> finally , we use both types of inequalities to establish an oracle inequality for support vector machines that use the pinball loss . the resulting learning rates are min  max optimal under some standard regularity assumptions on the conditional quantile . </S>"
  ]
}