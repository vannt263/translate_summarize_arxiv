{
  "article_text": [
    "many machine learning problems boil down to optimizing specific objective functions . in this paper , we consider the following generic optimization problem associated with @xmath0-regularized loss : @xmath3 where @xmath4 represent @xmath5 training examples of the task , each with feature vector @xmath6 and label / response @xmath7 , and @xmath8 is a smooth convex loss function with respect to its first argument .",
    "this objective function is the heart of several important machine learning problems , including lasso @xcite where @xmath9 , and sparse logistic regression @xcite where @xmath10 .",
    "much effort has been put into developing optimization methods for this model , ranging from coordinate minimization @xcite , randomized coordinate descent @xcite , stochastic gradient descent @xcite , dual coordinate ascent @xcite , to higher order methods such as interior point methods @xcite , l - bfgs @xcite , to name a few . however , the need for faster and more scalable algorithms is still growing due to the emergence of applications that make use of massive amount of high dimensional data ( e.g. @xcite ) .",
    "one direction to design faster algorithms is to utilize parallel computations on shared memory multi - processors or on clusters .",
    "some methods parallelize over examples @xcite , while others parallelize over features @xcite .",
    "as the references of the latter approach argue , it is sometimes more preferable to parallelize over features for @xmath0-regularized loss , which will thus be the focus of this work .",
    "another direction to design more efficient algorithms is to make use of the curvature of the objective function to obtain faster theoretical convergence rate .",
    "it is well known that for smooth objective functions , vanilla gradient descent only converges at a suboptimal rate @xmath1 , while nesterov s acceleration technique would allow the optimal rate @xmath2 @xcite .",
    "recent progress along this line includes generalizing nesterov s acceleration to randomized coordinate descent @xcite .",
    "even if the objective is not smooth , algorithms that still enjoy the same fast convergence rate have been proposed for functions that are a sum of a smooth part and a simple separable non - smooth part , such as the @xmath0-regularized loss we consider here ( see for instance the fista algorithm @xcite ) .    in this work",
    "we aim to combine both of the techniques mentioned above , that is , to design parallelizable accelerated optimization algorithms .",
    "similar work includes @xcite .",
    "we start by revisiting and improving boom @xcite , a parallelizable variant of accelerated gradient descent that tries to utilize the sparsity and elliptical geometry of the data .",
    "we first give a simplified form of boom which allows one to clearly see the the connection between boom and fista , and also to study these algorithms in a unified framework .",
    "surprisingly , we show that boom is actually provably slower than fista when data is normalized , which is an equivalent way of utilizing the elliptical geometry .",
    "moreover , we also propose a refined measurement of sparsity that improves the one used in boom .    moving on to parallel coordinate descent algorithms",
    ", we then propose an accelerated version of the shotgun algorithm @xcite .",
    "shotgun converges as fast as vanilla gradient descent while only updating a small subset of coordinates per iteration ( in parallel ) .",
    "our accelerated version even improves the convergence rate from @xmath1 to @xmath2 , that is , the same convergence rate as fista while updating much fewer coordinates per iteration .",
    "our algorithm is a unified framework of accelerated single coordinate descent @xcite , multiple coordinate descent and full gradient descent .",
    "however , instead of directly generalizing @xcite or the very recent work @xcite , we take a different route to present nesterov s acceleration technique so that our method enjoys a simpler form that makes use of only one auxiliary sequence and our analysis is also much more concise .",
    "we discuss how these algorithms are connected and which one is optimal under different circumstances .",
    "we finally mention several computational tricks to allow highly efficient implementation of our algorithm .",
    "in this section , we investigate algorithms that make use of a full gradient at each round . specifically , we revisit , simplify and improve the boom algorithm @xcite .",
    "there are essentially two forms of nesterov s acceleration technique , one which follows the original presentation of nesterov and uses two auxiliary sequences of points , and the other which follows the presentation of fista and uses only one auxiliary sequence .",
    "boom falls into the first category . to make the algorithm more clear and the connection to other algorithms more explicit",
    ", we will first translate boom into the second form . before doing so , to make things even more concise we assume that the data is _",
    "normalized_. specifically , let @xmath11 be an @xmath5 by @xmath12 matrix such that the @xmath13 row is @xmath14 .",
    "we assume each column of @xmath11 is normalized such that @xmath15 for all @xmath16 .",
    "it is clear that this is without loss of generality ( see further discussion at the end of this section ) .",
    "now we are ready to present our simplified version of boom ( see algorithm [ alg : agd ] , option 2 ) . here , @xmath17 is the smoothness parameter of the loss function such that its second derivative @xmath18 ( with respect to the first argument ) is upper bounded by @xmath17 for all @xmath19 and @xmath20 .",
    "for instance , @xmath21 for square loss used in lasso and @xmath22 for logistic loss .",
    "@xmath23 is the usual coefficient for nesterov s technique which satisfies : @xmath24 , with @xmath25 finally , the shrinkage function @xmath26 is defined as @xmath27 .",
    "one can see that boom explicitly makes use of the sparsity of the data , and uses @xmath28 , a measurement of sparsity , to scale the gradient .",
    "set step size @xmath29 , where    * option 1 ( fista ) : @xmath30 , where @xmath31 is the spectral radius of @xmath32 .",
    "* option 2 ( boom ) : @xmath33 , where @xmath34 and @xmath35 . * option 3 ( our method ) : @xmath36 , where @xmath37 .",
    "initialize @xmath38 + * for * @xmath39 * do * @xmath40(in parallel for each coordinate ) + @xmath41 + @xmath42 +    when written in this form , it is clear that boom is very close to fista ( see algorithm [ alg : agd ] , option 1 ) .",
    "the only difference is that boom replaces @xmath31 with the sparsity @xmath28 . the questions are , whether this slight modification results in a faster algorithm ?",
    "does boom converges faster by utilizing the sparsity of the data ?",
    "the following lemma and theorem answer these questions _ in the negative_.    [ lem : kappa ] let @xmath43 and @xmath28 be defined as in algorithm [ alg : agd ] .",
    "then we have @xmath44    let @xmath45 be a unit eigenvector of @xmath32 with respect to the eigenvalue @xmath31 .",
    "one has @xmath46 by cauchy - schwarz inequality , we continue @xmath47 where the last two equalities make use of the fact that @xmath48 and @xmath49 respectively .",
    "[ thm : agd ] suppose @xmath50 admits a minimizer @xmath51 .",
    "as long as @xmath52 , algorithm [ alg : agd ] insures @xmath53 after @xmath54 iterations . in other words , to reach @xmath55 accuracy , algorithm [ alg : agd ] needs @xmath56 iterations .",
    "we omit the proof of theorem [ thm : agd ] since it is a direct generalization of the original proof for fista .",
    "together with lemma [ lem : kappa ] , this theorem suggests that we should always choose fista over boom if we know @xmath31 , since it uses a larger step size and converges faster . in practice , computing @xmath31 might be time consuming . however , a useful side product of lemma [ lem : kappa ] is that it proposes a refined measurement of sparsity @xmath57 , which is clearly also easy to compute at the same time .",
    "we include this improved variant in option 3 of algorithm [ alg : agd ] .",
    "note that for any fixed @xmath58 , @xmath59 forms a distribution , and thus @xmath57 should be interpreted as the maximum weighted average sparsity of the data .",
    "* generality of feature normalizing .",
    "* one might doubt that our results hold merely because of the normalization assumption of the data .",
    "indeed , authors of @xcite emphasize that one of the advantages of boom is that it utilizes the elliptical geometry of the feature space , which does not exist any more if each feature is normalized .",
    "however , one can easily verify that the outputs of the following two methods are completely identical : 1 ) apply boom directly on the original data ; 2 ) scale each feature first so that the data is normalized , apply boom , and at the end scale back each coordinate of the output accordingly .",
    "therefore , feature normalization does not affect the behavior of boom at all .",
    "put it differently , feature normalization is an equivalent way to utilize the elliptical geometry of the data .",
    "note that the same argument does not hold for fista . indeed , while our results show that fista is provably faster than boom , experiments in @xcite show the opposite on unnormalized data .",
    "this suggests that we should always normalize the data before applying fista .",
    "updating all coordinates in parallel is not realistic , either because of a limited number of cores in multi - processors or communication bottlenecks in clusters .",
    "therefore in this section , we shift gears and consider algorithms that only update a subset of the coordinates at each iterations . to simplify presentation , we assume there is no regularization ( i.e. @xmath60 ) and again data is normalized .",
    "we propose a generalized and accelerated version of the shotgun algorithm @xcite ( see algorithm [ alg : ashotgun ] )",
    ".      what shotgun does is to perform coordinate descent on several randomly selected coordinates ( in parallel ) , with the _ same step size _ as in the usual ( single ) coordinate descent .",
    "if the number of updates per iteration @xmath62 is well tuned , shotgun converges as fast as doing a full gradient descent , even if it updates much fewer coordinates .",
    "the main difference between shotgun and our accelerated version is that as usual accelerated methods , our algorithm maintains an auxiliary sequence @xmath63 at which we compute gradients .",
    "however , @xmath64 is not just @xmath65 as in algorithm [ alg : agd ] . instead , a small _ step back _ has to be taken , which is reflected in the extra term @xmath66 where constant @xmath67 will be specified later in theorem [ thm : ashotgun ] . intuitively , this step back is to reduce the momentum due to the fact that we are not updating all coordinates",
    ". another important generalization of shotgun is that we introduce an extra step size coefficient @xmath68 , which allows us to unify several algorithms ( see further discussion below ) .",
    "note that @xmath68 is fixed to @xmath69 in the original shotgun .",
    "we now state the convergence rate of our algorithm in the following theorem .",
    "[ thm : ashotgun ] suppose @xmath50 admits a minimizer @xmath51 . if @xmath62 and @xmath70 are such that @xmath71 where @xmath72 ( recall @xmath31 is the spectral radius of @xmath32 ) , constant @xmath23 is defined as in section [ sec : boom ] , and @xmath73 then the following holds for any @xmath74 , @xmath75 - f({{\\bf w}}^ * ) \\leq   \\frac{\\beta d^2 \\|{{\\bf w}}_1-{{\\bf w}}^*\\|^2}{t^2p^2\\eta{\\left(}1-\\frac{\\eta}{2}(1+\\sigma){\\right)}},\\ ] ] where the expectation is with respect to the random choices of subsets @xmath76 .    for any iteration @xmath77",
    ", we first consider the expectation of @xmath78 conditioning on @xmath79 , which we denote by @xmath80 $ ] .",
    "staring from lemma 3.3 in @xcite , we have @xmath81   & \\leq \\frac{p}{d}\\sum_{j=1}^d { \\left(}-\\frac{\\eta}{\\beta } \\nabla f({{\\bf u}}_k)_j^2 + \\frac{\\beta}{2}(1+\\sigma ) ( \\frac{\\eta}{\\beta } \\nabla f({{\\bf u}}_k)_j ) ^2 { \\right)}\\\\ & = -\\frac{p\\eta}{d\\beta}{\\left(}1 - \\frac{\\eta}{2}(1+\\sigma){\\right)}\\|\\nabla f({{\\bf u}}_k)\\|^2.\\end{aligned}\\ ] ]",
    "let @xmath82 be an arbitrary point , by the above inequality and the convexity of @xmath83 we have @xmath84   & = { \\mathbb{e}}_k[f({{\\bf w}}_{k+1 } ) - f({{\\bf u}}_k ) + f({{\\bf",
    "u}}_k ) - f({{\\bf w } } ) ] \\\\ & \\leq -\\frac{p\\eta}{d\\beta}{\\left(}1 - \\frac{\\eta}{2}(1+\\sigma){\\right)}\\|\\nabla f({{\\bf",
    "u}}_k)\\|^2 + \\nabla f({{\\bf u}}_k)^t({{\\bf u}}_k - { { \\bf w}}).\\end{aligned}\\ ] ] also , direct calculations show @xmath85 = -\\frac{p\\eta}{d\\beta}\\nabla f({{\\bf u}}_k ) , \\quad    { \\mathbb{e}}_k\\left[\\|{{\\bf w}}_{k+1}-{{\\bf u}}_k\\|^2\\right ] = \\frac{p\\eta^2}{d\\beta^2 } \\|\\nabla f({{\\bf u}}_k)\\|^2,\\ ] ] which with @xmath86 and @xmath87 leads to @xmath88   \\leq a { \\mathbb{e}}_k \\left [   \\|{{\\bf w}}_{k+1}-{{\\bf u}}_k\\|^2 + 2b({{\\bf w}}_{k+1}-{{\\bf u}}_k)^t({{\\bf u}}_k-{{\\bf w } } )   \\right].\\ ] ]    by the law of total expectation , taking the expectation with respect to @xmath79 on both sides and plugging @xmath89 and @xmath90 respectively gives @xmath91,\\ ] ] and @xmath92,\\ ] ] where we define @xmath93 . since @xmath94 for any @xmath95 , we now multiply both sides of eq . by @xmath96 , and add the result to eq . , arriving at @xmath97.\\ ] ] multiplying both sides by @xmath98 and using the fact that @xmath99 , we obtain @xmath100.\\ ] ] let @xmath101 be such that @xmath102 we continue with @xmath103 \\\\ & =   ab^2   { \\mathbb{e}}_{s_{1:k } } \\left [   \\|\\theta_k{{\\bf w } } ' - ( \\theta_k-1){{\\bf w}}_k - { { \\bf w}}^ * \\|^2 - \\| \\theta_k { { \\bf u}}_k - ( \\theta_k-1){{\\bf w}}_k   - { { \\bf w}}^ * \\|^2 \\right].\\end{aligned}\\ ] ] note that @xmath104 so we finally arrive at @xmath105 where @xmath106 $ ] . summing these inequalities from @xmath107 to @xmath108 , and noting that @xmath109 by the assumption @xmath71",
    ", we have @xmath110 by induction one can verify that @xmath111 for any @xmath112 , and thus eq . follows .",
    "we now explain how to interpret this result and how to choose parameters @xmath62 and @xmath68 .",
    "first , note that to reach @xmath55 accuracy ( i.e. @xmath113 - f({{\\bf w}}^ * ) \\leq \\epsilon$ ] ) , algorithm [ alg : ashotgun ] requires @xmath114 iterations . for any fixed @xmath62",
    ", one can verify that @xmath115 is the optimal choice for @xmath68 to minimize @xmath116 . plugging @xmath117 back in @xmath116 and minimizing over @xmath62",
    ", one can check that @xmath118 is the best choice . in this case , since @xmath119 and @xmath120 , algorithm [ alg : ashotgun ] actually degenerates to fista and @xmath121 , recovering the results in theorem [ thm : agd ] exactly .    of course",
    ", at the end it is not the number of iterations , but the total computational complexity that we care about most .",
    "suppose we implement the algorithm without using any parallel computation .",
    "then as we will discuss later , the time complexity for each iteration would be @xmath122 ( recall @xmath5 is the number of examples ) , and the total complexity @xmath123 is minimized when @xmath124 and @xmath125 , leading to @xmath126 . in this case , our algorithm degenerates to an accelerated version of randomized ( single ) coordinate descent .",
    "note that this essentially recovers the algorithms and results in @xcite , but in a much simpler form ( and analysis ) .",
    "finally , we consider implementing the algorithm using parallel computation .",
    "note that @xmath115 is always at most 1 .",
    "so we fix @xmath68 to be @xmath69 ( as in the original shotgun algorithm ) , leading to the largest possible step size @xmath127 and a potentially small @xmath62 . ignoring for now hardware or communication limits",
    ", we assume in this case updating @xmath62 coordinates in parallel costs approximately the same time no matter what @xmath62 is .",
    "in other words , we are again only interested in minimizing @xmath116 .",
    "one can verify that the optimal choice here for @xmath62 is @xmath128 and @xmath129 .",
    "this improves the convergence rate from @xmath130 to @xmath131 compared to shotgun , and is almost as good as fista ( with much less computation per iteration )",
    ".    * efficient implementation .",
    "* we discuss how to efficiently implement algorithm [ alg : ashotgun ] .",
    "1 ) at first glance it seems that computing vector @xmath132 needs to go over all @xmath12 coordinates .",
    "however , one can easily generalize the trick introduced in @xcite to update only @xmath62 coordinates and compute @xmath133 and @xmath132 implicitly .",
    "2 ) another widely used trick is to maintain inner products between examples and weight vectors ( i.e. @xmath134 and @xmath135 ) , so that computing a single coordinate of the gradient can be done in @xmath136 , or even faster in the case of sparse data .",
    "3 ) instead of choosing @xmath137 uniformly at random , one can also do the following : arbitrarily divide the set @xmath138 into @xmath62 disjoint subsets of equal size in advanced ( assuming @xmath12 is a multiple of @xmath62 for simplicity ) , then at each iteration , select one and only one element from each of the @xmath62 subsets uniformly at random to form @xmath137 . this would only lead to a minor change of the convergence results ( indeed , one only needs to redefine @xmath139 to be @xmath140 ) .",
    "the advantage of this approach is that it suggests that we can separate the data matrix @xmath11 by columns and store these subsets on @xmath62 machines separately to naturally allow parallel updates at each iteration ."
  ],
  "abstract_text": [
    "<S> the growing amount of high dimensional data in different machine learning applications requires more efficient and scalable optimization algorithms . in this work </S>",
    "<S> , we consider combining two techniques , parallelism and nesterov s acceleration , to design faster algorithms for @xmath0-regularized loss . </S>",
    "<S> we first simplify boom @xcite , a variant of gradient descent , and study it in a unified framework , which allows us to not only propose a refined measurement of sparsity to improve boom , but also show that boom is provably slower than fista @xcite . moving on to parallel </S>",
    "<S> coordinate descent methods , we then propose an efficient accelerated version of shotgun @xcite , improving the convergence rate from @xmath1 to @xmath2 . </S>",
    "<S> our algorithm enjoys a concise form and analysis compared to previous work , and also allows one to study several connected work in a unified way . </S>"
  ]
}