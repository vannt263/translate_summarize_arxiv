{
  "article_text": [
    "current remote - sensing technology provides high - resolution terrain data for geographic information systems ( ) . in particular , with technology one can get detailed elevation models of the earth surface .",
    "these are usually made available in the form of a triangulated irregular network ( ) or a grid : a matrix with elevation values for points in a regular grid on the surface of the earth .",
    "one important application of such models are hydrological studies : analysing the flow of water on a terrain , for example to study the effects of possible human intervention or to estimate risks of erosion .",
    "the grid models may be as large as several dozen gigabytes so that they do not fit in the main memory of a computer at once .",
    "therefore hydrological analysis requires efficient algorithms that scale well and are designed to minimise the swapping of data between main memory and disk . throughout the current decade arge et al .",
    "have designed and published such algorithms for the following pipeline of computations on terrain data :    constructing an elevation model from raw elevation samples  @xcite ;    ( partial ) _ flooding : _ eliminating spurious depressions from the model , so that streams do not appear to be interrupted by virtual dams that result from sampling errors  @xcite ;    _ flow routing : _ determining in what direction water could flow on each point of the terrain , in such a way that from every point of the terrain water would follow a non - ascending path to the lowest point of the watershed that contains that point  @xcite ;    _ flow accumulation : _ computing for every point of the terrain the size of the region from which water flows to that point  @xcite ;    _ hierarchical watershed labelling : _ giving each point of the terrain a label that indicates its position in the watershed hierarchy  @xcite .",
    "arge et al .  implemented these algorithms in the form of a system called  @xcite .",
    "while the published algorithms have been shown to be very effective , in this paper we will show that for some of these algorithms alternatives exist that may process grid models ( but not ) up to an order of magnitude faster .",
    "[ [ analysing - io - efficiency ] ] analysing i / o - efficiency + + + + + + + + + + + + + + + + + + + + + + + +    in this paper we analyse the efficiency of algorithms with the standard model that was defined by aggarwal and vitter  @xcite . in this model ,",
    "a computer has a memory of size @xmath4 and a disk of unbounded size .",
    "the disk is divided into blocks of size @xmath5 .",
    "data is transferred between memory and disk by transferring complete blocks : transferring one block is called an `` '' .",
    "algorithms can only operate on data that is currently in main memory ; to access the data in any block that is not in main memory , it first has to be copied from disk .",
    "if data in the block is modified , it has to be copied back to disk later , at the latest when it is evicted from memory to make room for another block . in this paper",
    "we will be concerned with the -efficiency of algorithms : the number of they need as a function of the input size @xmath2 , the memory size @xmath4 , and the block size @xmath5 . as a point of reference , scanning @xmath2 consecutive",
    "records from disk takes @xmath6 ; sorting takes @xmath7 in the worst case  @xcite .",
    "it is sometimes assumed that @xmath8 .",
    "we distinguish _ cache - aware _ algorithms and _ cache - oblivious _ algorithms .",
    "cache - aware algorithms may use knowledge of @xmath4 and @xmath5 ( and to some extent even control @xmath5 ) and they may use this to control which blocks are kept in memory and which blocks are evicted .",
    "cache - oblivious algorithms do not know @xmath4 and @xmath5 and can not control which blocks are kept in memory : the caching policy is left to the hardware and the operating system . nevertheless cache - oblivious algorithms can often be designed and proven to be -efficient  @xcite .",
    "the idea is to design the algorithm s pattern of access to locations in input and output files such that effective caching is achieved by any reasonable general - purpose caching policy ( such as least - recently - used replacement ) for any values of @xmath4 and @xmath5 . as a result ,",
    "any bounds that can be proven on the -efficiency of a cache - oblivious algorithm do not only apply to the transfer of data between disk and main memory , but also to the transfer of data between main memory and the various levels of smaller caches .",
    "however , in practice cache - oblivious algorithms can not always match the performance of cache - aware algorithms that are tuned to specific values of @xmath4 and @xmath5 .",
    "[ [ our - results ] ] our results + + + + + + + + + + +    we present and analyse several algorithms for flow accumulation on grid terrains :    an `` -nave '' algorithm that processes the grid row by row ;    a variant of the above that processes the row - by - row data in so - called z - order ;    a variant of the above that processes data that is stored in z - order ;    a cache - aware -efficient algorithm based on separators ;    a variant of the above that processes data that is stored in z - order ;    a cache - oblivious variant of the above ;    and for comparison : the algorithm based on time - forward processing from arge et al .",
    "@xcite    our results are summarised in table  [ tab : results ] .",
    "we find that the last algorithm induces a significant overhead from sorting the input cells into topological order .",
    "this is not only because the sorting itself takes , but also because sorting the cells makes it necessary to store the coordinates in the grid for each cell : the coordinates are needed to sort the cells back into row - by - row order when the computation is complete .",
    "our new algorithms avoid this overhead : they do not sort .",
    "the best of our algorithms are asymptotically more efficient than time - forward processing ( under mild assumptions ) , and because the coordinates of a grid cell can always be deduced from its location in the file , there is no overhead from storing coordinates . in practice",
    "the difference in performance may be up to an order of magnitude , and preliminary experimental results indicate that our algorithms are fast indeed . for comparison : the authors of reported 455 minutes for flow accumulation by time - forward processing , working on a grid of similar size and hardware that appeared to be slightly faster than ours .",
    "& & worst case & realistic & & +   + `` nave '' row - by - row scan & & @xmath9 & @xmath10 @xmath11 & & 111 min .",
    "+ `` nave '' z - order - scan & & @xmath9 & @xmath1 @xmath12 & & + `` nave '' z - order - scan & & @xmath10 & @xmath1 @xmath11 & & 41 min .",
    "+ cache - aware separators & & & 2.06.6 @xmath13 & 39 min .",
    "+ cache - aware separators & & & 1.1 @xmath13 & + cache - oblivious separators & & & & + cache - oblivious separators & & & & 118 min .",
    "+ time - forward proc .",
    "@xcite & & & 7.832.0 @xmath13 & +   + & & & 88 min .",
    "+ & & 2.0 & + & & 4.06.0 @xmath13 & +    this suggests that , while the time - forward processing algorithms are more flexible ( they are easier to adapt to triangulated terrains and to so - called multiple - flow - direction models ) , their flexibility comes at a price when processing grid terrains .",
    "simple alternatives such as our algorithms based on z - order or our cache - aware algorithm are therefore worth to be considered for practical applications .    in the following section we discuss some preliminaries :",
    "we introduce the _ confluence assumption _ which is helpful when analysing the -efficiency of algorithms on realistic inputs , and we discuss how to traverse a grid in z - order and how to convert a grid file from row - by - row order to z - order and back . in the next section we present and analyse our flow accumulation algorithms and experimental results . in section  [ sec :",
    "otherstages ] we discuss possible applications of our ideas to other stages of the hydrological analysis pipeline .",
    "sometimes we will require the _ tall - cache assumption _ ( @xmath14 for some constant @xmath0 ) ; when this is the case we will indicate this explicitly .",
    "when estimating -volumes , we will explicitly consider the scenarios @xmath15 bytes , @xmath16 bytes , and @xmath15 bytes , @xmath17 bytes , in both cases with an input grid of size @xmath18 cells ( this is roughly the size of the largest grid we experimented with ) .      in the next section",
    "we will see that some algorithms would perform very poorly when given a grid of @xmath19 cells in which a river flows from the right to the left , making @xmath20 meanders with an amplitude of @xmath21 cells ( see figure  [ fig : meanders ] , left ) , for some constants @xmath22 and @xmath23 .",
    "however , it does not seem to make all that much sense to take such cases into account in asymptotic analysis : that would suggest that with growing resolution and size of our grids , rivers would get more and larger meanders .",
    "but the major features of real drainage networks would never change just because our grids get bigger . to capture this intuition",
    ", we pose the _ confluence assumption _ , defined as follows .    for any square @xmath24 of @xmath25 cells , let @xmath26 be the square of @xmath27 cells centered on @xmath24 .",
    "let the _ first far cells _ of @xmath24",
    "be the cells @xmath28 on the boundary of @xmath26 such that from at least one cell @xmath0 of @xmath24 water reaches @xmath28 before leaving @xmath26 .",
    "the _ confluence assumption _ states that there is a constant @xmath29 , independent of @xmath2 and @xmath30 , such that any square @xmath24 has at most @xmath29 different first far cells ( see figure  [ fig : confluence ] , middle ) .",
    "we call @xmath29 the _ confluence constant_. informally , the confluence assumption says that the water that flows from any square region will collect into a small number of river valleys before it gets very far .",
    "we will use the confluence assumption in the analysis of some of our algorithms .",
    "meanders with an amplitude of @xmath31 .",
    "middle : a realistic grid .",
    "the white encircled cells are the first far cells of @xmath24 ; it has only few first far cells .",
    "right : z - order . ]",
    "[ fig : confluence][fig : zorder ]      some of our algorithms will benefit from processing or storing the grid cells in z - order .",
    "this order is defined as follows .",
    "say our input is a grid @xmath32 of height @xmath33 and width @xmath34 .",
    "this grid is contained in a matrix @xmath35 of @xmath36 cells , where @xmath37 is the smallest power of 2 such that @xmath38 and @xmath39 .",
    "let the first cell of @xmath32 ( in the upper left corner ) also be the first cell of @xmath35 .",
    "the matrix @xmath35 has four quadrants of @xmath40 cells . when storing @xmath32 in z - order , we first store the part of @xmath32 in the upper left quadrant of @xmath35 , then the part of @xmath32 in the upper right quadrant of @xmath35 , then the part of @xmath32 in the lower left quadrant of @xmath35 , and finally the part of @xmath32 in the lower right quadrant of @xmath35 . within each quadrant ,",
    "the cells of @xmath32 are ordered recursively in the same way ( see figure  [ fig : zorder ] , right ) .",
    "[ [ computing - z - order - coordinates - from - rowcolumn - coordinates - and - vice - versa ] ] computing z - order coordinates from row / column coordinates and vice versa + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    assume , for now , that @xmath41 , and consider the cell @xmath0 in row @xmath42 and column @xmath43 of the grid .",
    "let @xmath44 and @xmath45 be the binary representations of @xmath42 and @xmath43 ( where @xmath46 ) .",
    "then the position of @xmath0 in a row - by - row file is @xmath47 , and its position in a z - order file is @xmath48 .",
    "it is therefore quite easy to obtain , for any cell , its position in a row - by - row file from its position in a z - order file and vice versa .",
    "when the input grid is not square or when its height and width are not integral powers of two , the conversion is slightly more difficult , but in practice it can still be done efficiently as follows .",
    "let @xmath32 be a grid of size @xmath49 stored in z - order .",
    "then @xmath35 has size @xmath50 , where @xmath51 .",
    "the sequence of cells of the matrix @xmath35 in z - order can now be divided into @xmath52 subsequences @xmath53 , such that each sequence @xmath54 consists of cells in @xmath32 and each sequence @xmath55 consists of cells outside  @xmath32 .",
    "we can now construct two arrays @xmath56 $ ] and @xmath57 $ ] , where @xmath58 = \\sum_{j=1}^{i-1 } |d_j|$ ] and @xmath59 = \\sum_{j=1}^{i-1 } ( |d_j| + |n_j|)$ ] .",
    "thus @xmath58 $ ] and @xmath59 $ ] store , for each segment @xmath54 , its offset in the file ( which only stores the cells in @xmath32 ) and its offset in the z - order traversal of @xmath35 , respectively .",
    "an offset @xmath60 in the z - order file can now be converted to row and column coordinates as follows : find the highest @xmath61 such that @xmath58 \\leq p$ ] , compute @xmath62 + ( p - f[i])$ ] , and extract the row and column coordinates @xmath44 and @xmath45 .",
    "row and column coordinates can be converted to an offset in the z - order file in a symmetric way .",
    "note that @xmath63 and @xmath64 can be computed without reading the input grid .",
    "for all reasonable grid sizes @xmath63  and @xmath64 easily fit in main memory .",
    "when this is not the case , the number of incurred by swapping blocks of @xmath63 and @xmath64 is always dominated by the number of incurred by accessing the cells whose offsets or coordinates are being computed .",
    "the costs of converting coordinates can therefore be ignored in the -efficiency analysis of the algorithms in this paper .    from a practical point of view one",
    "should also consider the effort involved in the bit manipulations that are needed to extract @xmath44 and @xmath45 from @xmath48 , and to find the z - order offsets of the neighbours of a given cell @xmath48 .",
    "this can also be done efficiently with a method called _ dilated arithmetic _ and another set of look - up tables of size @xmath65  @xcite .",
    "[ [ converting - grids - fromto - z - order - efficiently ] ] converting grids from / to z - order efficiently + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to convert a grid from row - by - row order to z - order , we consider two very simple cache - oblivious algorithms : ( a ) read each cell from the row - by - row file and write it to the z - order file , going through the grid in z - order , and ( b ) read each cell from the row - by - row file and write it to the z - order file , going through the grid row by row .",
    "algorithm ( a ) , the z - order scan , is efficient when we assume a tall cache .",
    "then there is a @xmath66 such that @xmath67 is a power of two , a square of @xmath68 cells occupies @xmath69 blocks of the row - by - row file on disk , and these blocks fit in memory .",
    "the algorithm processes roughly @xmath70 such squares that each form a contiguous section of the z - order file . for each such section",
    "the necessary blocks from the row - by - row file can be read and kept in memory until the section is completely processed .",
    "the total number of for the conversion thus becomes at most @xmath71 for reading plus @xmath1 for writing , making a total of @xmath1 .",
    "if @xmath72 rows of the grid fit in memory , where @xmath73 is the number of bytes per cell , then algorithm ( b ) is even more efficient . algorithm ( b ) , the row - by - row scan , would read every input block ( from the row - by - row file ) once and it would be able to keep every output block ( which is @xmath72 rows high ) in memory until all of its cells have been read .",
    "thus every output - block only needs to be written to disk once . in practice",
    "the assumption that @xmath72 rows of the grid fit in memory seems reasonable ; therefore we assume that the -volume of the conversion from row - by - row to z - order is twice the input size .",
    "when @xmath72 rows do not fit in memory , algorithm ( b ) may cause every output block to be reloaded @xmath72 times , so that the algorithm uses @xmath74 .",
    "alternatively , one may adapt merge sort to convert a row - by - row file to z - order in @xmath75 , making two or three read / write passes over the input in practice .    to convert a grid from z - order to row - by - row order",
    ", the same algorithms can be used while substituting reading for writing and vice versa .",
    "we assume that the input to the flow accumulation problem consists of a file @xmath76 that contains one byte for each cell , stored row - by - row .",
    "the byte for cell @xmath0 indicates the _ flow direction _ of @xmath0 : to which of the eight neighbours of @xmath0 ( if any ) any water that arrives at @xmath0 will flow .",
    "this neighbour is called the _ out - neighbour _ of @xmath0 .",
    "the required output is a file @xmath77 with eight bytes per cell , stored row - by - row , that specify each cell s _ flow accumulation_. the flow accumulation of a cell @xmath0 is the number of cells that lie upstream of @xmath0 , including @xmath0 itself . we can picture the flow accumulation of @xmath0 as the total number of units of rain that arrive at or pass through @xmath0 when each cell receives one unit of rain from the sky , which then flows down over the surface from cell to cell , following the flow directions , until it arrives at a cell that does not have an out - neighbour .    in the following we describe several algorithms for the flow accumulation problem and",
    "also discuss their -volume ( number of times block size ) divided by the total size of the input and the output files . with input and output",
    "as defined above , the input+output size is 9 bytes times the number of cells in the grid .",
    "our first algorithm is just clever enough to run in linear time , but utterly nave from an point of view . in this algorithm , let @xmath0 be the index ( offset in the input file ) of a cell in @xmath77 , and let @xmath78 be the index of the neighbour of cell @xmath0 to which the incoming water of @xmath0 flows .",
    "note that @xmath78 can be computed from @xmath0 and the flow direction stored for @xmath0 in @xmath76 .",
    "create a file @xmath77 with flow value ( initially 1 ) and marking bit ( initially not set ) for each cell each cell @xmath0 ( in row - by - row order ) @xmath0 is not marked @xmath79 all in - neighbours of @xmath30 are marked and @xmath30 has an out - neighbour @xmath80 mark @xmath30 @xmath81 \\gets \\id{flowacc}[\\id{out}(d ) ] + \\id{flowacc}[d]$ ] @xmath82    the algorithm goes through all cells and gives each of them one unit of water ; this water is then forwarded downstream until a cell is reached that is still waiting for incoming water from some of its neighbours .",
    "the accumulated flow then waits there until the cell has received the incoming flow from all neighbours , so that for each cell , all incoming flow is forwarded downstream in a single operation .",
    "thus the algorithm runs in linear time",
    ".    however , the -behaviour of this simple algorithm can be quite bad : consider processing the terrain shown in figure  [ fig : meanders ] ( left ) in row - by - row order .",
    "none of the cells on the meandering river can have their flow forwarded downstream until the scan reaches the last cell in the lower right corner .",
    "at that point the while - loop of lines 5 to  8 will follow the whole river back to the upper left corner , possibly requiring one almost every step of the way .",
    "the worst - case -efficiency is therefore @xmath83 .",
    "fortunately , under the confluence assumption the situation does not look so grim :    [ naiveaccumulationronr ] under the confluence assumption algorithm naiveaccumulation uses @xmath10 i / o s .    consider the grid to consist of square subgrids of @xmath84 cells . while running the algorithm",
    ", we maintain that at line 3 , the subgrid that contains @xmath0 and the eight subgrids around it are in memory .",
    "when in line 58 we access a cell @xmath30 that is currently not in memory , we load the subgrid that contains @xmath30 and the subgrids around it into memory ; upon termination of this loop we reload the subgrids that were in memory before entering the loop .    ignoring the loop in line 58 , each disk block of the row - by - row files",
    "is loaded into memory at most @xmath85 times , causing @xmath10 in total . in line 58 ,",
    "the nine subgrids currently in memory are replaced ( and possibly reloaded afterwards ) only when we reach and finish a cell on the boundary of the nine subgrids after following a path that started from the subgrid in the middle . by the confluence assumption",
    "this happens at most a constant number of times for each group of @xmath86 subgrids .",
    "since there are @xmath87 such groups ( one centered on each subgrid ) and each is stored across at most @xmath88 blocks on disk , the algorithm takes @xmath10 in total .",
    "we can make algorithm naiveaccumulation slightly smarter by changing the loop in line 2 to go through all cells in z - order .",
    "this does not change the worst - case -efficiency of the algorithm ( extreme rivers could still cause @xmath83 ) . however",
    ", we would expect significantly better performance in practice :    [ naiveaccumulationzonrrealistic ] under the confluence assumption and with a tall cache of size @xmath14 , algorithm naiveaccumulation , running the loop on line 2 in z - order , needs only @xmath1 i / o s .    by the tall - cache assumption",
    ", there is a @xmath89 such that @xmath67 is a power of two and the blocks containing a section of @xmath90 cells fit in memory .",
    "now consider the grid to consist of square subgrids of @xmath91 cells .",
    "thus each subgrid contains @xmath92 cells and is stored in @xmath93 blocks in the row - by - row files .",
    "the idea is again to keep the subgrid of the current cell in memory , together with the eight subgrids around it .    ignoring the loop in line 58 , each group of @xmath86 subgrids",
    "is loaded into memory once : each group is loaded when the z - order scan of line 2 enters the subgrid @xmath24 in the middle of the group , and the z - order scan then traverses @xmath24 completely before proceeding to the next subgrid .",
    "thus each group is loaded at most once and each subgrid is loaded at most nine times , causing @xmath94 .",
    "following the same analysis as before , lines 58 cause each group of @xmath86 subgrids to be evicted from memory at most a constant number of times , causing @xmath1 as well .",
    "algorithm naiveaccumulation can be improved further by not only processing the cells in z - order , but also using input in z - order and producing output in z - order .",
    "we now get the following :    algorithm naiveaccumulation , running the loop on line 2 in z - order and working on files in z - order , needs only @xmath10 i / o s in the worst case . under the confluence assumption ,",
    "the required number of i / o s is @xmath1 .    as in the proof of theorem  [ naiveaccumulationronr ]",
    ", we consider the grid to be divided into subgrids of size @xmath95 . in z - order files , these occupy only a constant number of blocks each .",
    "such a subgrid @xmath24 is loaded into memory when we access a cell on its boundary , or when it is reloaded after completing the loop in lines 58 of the algorithm . in the latter case",
    "we can charge the reloading of block @xmath24 to the access to the boundary cell of @xmath24 that was last accessed before @xmath24 was evicted from memory . since each subgrid has only @xmath88 cells on its boundary and each cell of the grid",
    "is accessed only a constant number of times , each of the @xmath87 subgrids is loaded only @xmath88 times , causing @xmath10 in total .    for the -efficiency under the confluence assumption ,",
    "see the proof of theorem  [ naiveaccumulationzonrrealistic ] .",
    "we will now describe a slightly more involved algorithm that achieves an -efficiency of @xmath1 even in the worst case .",
    "unlike the previous algorithms , this algorithm is cache - aware : it needs to know @xmath4 and  @xmath5 .",
    "the algorithm is based on separators much in the same way as the single - source shortest paths algorithm of arge et al .",
    "@xcite ( but with a much smaller `` reduced '' graph ) .",
    "we present a cache - oblivious variant of our algorithm in section  [ sec : cacheobliviousaccumulation ] .",
    "let @xmath96 be chosen such that a subgrid of size @xmath91 fits in memory ( while leaving a bit of space to store additional information about its boundary cells ) .",
    "our cache - aware algorithm processes the grid in subgrids of size @xmath91 , where the boundary cells of each subgrid are shared with the neighbouring subgrids ( except on the outer boundary of the grid ) .",
    "let @xmath97 be the set of boundary cells of all subgrids .",
    "for any such subgrid @xmath24 , let @xmath98 be the set of cells of @xmath24 that do not lie on the boundary of @xmath24 , and let @xmath99 be the union of all subgrid interiors , that is , all cells except those in @xmath97 . given a subgrid @xmath24 and a cell @xmath0 on its boundary",
    ", we define its local destination @xmath100 as follows : if the out - neighbour of @xmath0 lies outside @xmath24 , then @xmath100 is undefined , otherwise it is the first boundary cell of @xmath24 that lies downstream of @xmath0 ( if it exists ) .",
    "the algorithm now proceeds in three phases .    in the first phase , the rain that falls on each cell of @xmath99",
    "is forwarded downstream to the first cell of @xmath97 that is reached ( if any ) .",
    "the rain is only stored at that cell of @xmath97 , not at the cells on the way . in the second phase , the rain that arrived at each cell @xmath0 of @xmath97 , together with the rain that falls on @xmath0 ,",
    "is forwarded to all cells of @xmath97 downstream of @xmath0 . in the third phase , the rain that falls on each cell of @xmath99 and the rain that arrived at each cell of @xmath97",
    "is forwarded to all cells of @xmath99 that lie downstream of it , until another cell of @xmath97 is reached ( or a cell without an out - neighbour ) .",
    "the result of these three phases is that the rain that falls on any grid cell @xmath0 is forwarded to all cells downstream of @xmath0 ( see figure  [ fig : cacheawarephases ] for an illustration , figure  [ fig : cacheawarecode ] for pseudocode ) .    .",
    "from left to right : input ; contents of and after the first phase ; contents of after the second phase ; contents of after the third phase . ]",
    "create file @xmath77 with flow value ( initially 1 ) and marking bit ( initially not set ) for each cell create file @xmath101 with out - neighbour ( initially undefined ) for each cell of @xmath97 create file @xmath102 with flow value ( initially 1 ) and marking bit ( not set ) for each cell of @xmath97 phase one : push flow from subgrid interiors to @xmath97 each subgrid @xmath24 run on @xmath98 in memory , starting with zero flow on the cells of @xmath103 , and leaving @xmath80 undefined for each cell of @xmath103 each cell @xmath0 on the boundary of @xmath24 store flow accumulation of @xmath0 in @xmath104 $ ] ( adding it to any previously stored value ) @xmath100 is defined @xmath105 \\gets \\id{dest}(q , c)$ ] phase two : compute flow accumulation for @xmath97 run line 28 of on @xmath102 , using neighbour relations in @xmath101 phase three : recompute flow in subgrid interiors , now including flow that comes in from @xmath97 each subgrid @xmath24 each cell @xmath0 on the boundary of @xmath24 @xmath106 \\gets \\id{sflowacc}[c]$ ] @xmath78 lies in @xmath98 @xmath107 \\gets",
    "\\id{flowacc}[\\id{out}(c ) ] + \\id{sflowacc}[c]$ ] run line 28 of on @xmath98 , using the file @xmath77 , leaving @xmath80 undefined whenever @xmath80 lies on the boundary of @xmath24    [ cacheawareaccumulation ] with a tall cache of size @xmath14 , algorithm cacheawareaccumulation needs only @xmath1 i / o s .    in the first and the third phase",
    ", we process @xmath108 blocks that each take @xmath109 to read , can be processed completely in main memory , and  in the third phase  may take another @xmath109 to write .",
    "the second phase runs a linear - time algorithm on a file of size @xmath110 , of which each record is accessed a constant number of times in the first and the third phase . the total number of is therefore @xmath111 . by the tall - cache assumption , this is @xmath1 .",
    "[ [ io - volume - estimate - for - realistic - values - of - n - m - and - b ] ] i / o - volume estimate for realistic values of @xmath2 , @xmath4 and @xmath5 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    cacheawareaccumulation does not depend on the interplay between @xmath5 , @xmath4 and @xmath29 as much as the previous algorithms .",
    "therefore it is possible to give a good estimate of the -volume .",
    "the bottleneck is the third phase .",
    "assume we set up and such that they first store , row by row , all rows that completely consist of cells of @xmath97 , and then , column by column , the remaining cells of @xmath97 .",
    "then we need to choose @xmath67 such that the blocks containing @xmath67 rows of @xmath67 cells from and fit in memory , plus 4 rows of @xmath67 cells from and .",
    "in we use 1 byte per cell , in the other files we use 8 bytes per cell .",
    "so @xmath67 must satisfy:@xmath112 a sufficient ( but not necessary ) condition is that @xmath67 is at most roughly @xmath113 .",
    "this means that @xmath67 will typically be several thousands : for @xmath15 and @xmath17 we would get @xmath114 ; for @xmath15 and @xmath16 we would get @xmath115 .",
    "taking the number of subgrids times the number of rows times the number of blocks per row times the block size times two ( for reading and writing ) , we find that the -volume for accessing in the third phase is now roughly @xmath116 . from this",
    "we may subtract @xmath117 , since the blocks of do not need to be read from disk the first time they are accessed . for accessing @xmath76 in the first and third phase ( reading only )",
    "we get roughly @xmath118 .",
    "the remaining is neglectible : for accessing @xmath119 in the first phase we get roughly @xmath120 , which is roughly three orders of magnitude smaller than the amount of for accessing .",
    "the same is true for accessing in the first and the third phase , and for accessing these files in the second phase : in practice @xmath97 will be small enough that the second phase can be done in main memory .",
    "the -volume thus adds up to roughly @xmath121 , dividing this by @xmath122 ( the input+output size ) we get an `` overhead '' of factor @xmath123 . for the example values of @xmath4 and @xmath5",
    "just mentioned , the result ranges from 2.0 to 6.6 .",
    "the algorithm explained above depends on the tall - cache assumption .",
    "this is because the boundary of a square of @xmath91 cells may cross @xmath124 blocks on disk , and without the tall - cache assumption , there is no guarantee that these will fit in memory for any @xmath125 .",
    "the need for the tall - cache assumption can be eliminated by using files in z - order .",
    "then any square @xmath24 of @xmath91 cells is contained in at most four squares of size @xmath126 , where @xmath127 , that are each stored contiguously on disk .",
    "the square @xmath24 is thus stored in @xmath128 blocks , which will always fit for some @xmath125 .",
    "thus we get :    [ cacheawareaccumulation ] algorithm cacheawareaccumulation on files in z - order needs only @xmath1 i / o s .",
    "[ [ io - volume - estimate - for - realistic - values - of - n - m - and - b-1 ] ] i / o - volume estimate for realistic values of @xmath2 , @xmath4 and @xmath5 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to get a good -volume in practice , we will now make sure that each subgrid @xmath24 is stored consecutively in the z - order file . to achieve this ,",
    "we do not let neighbouring subgrids share rows and columns anymore , but make sure the subgrids are disjoint and their height is a power of two .",
    "furthermore , we let the loops of line 5 and line 16 go through the subgrids in z - order .    as a result",
    ", the first phase only scans each block of the file once , and so does the third phase ; in addition the third phase writes each block of once ( reading is not necessary )",
    ". there may be up to eight times more cells in @xmath97 ( two times more because subgrids do not share boundary cells anymore , and four times more because we need to round the subgrid width down to a power of two ) .",
    "however , as explained above , the accesses to and are so few that they will still be neglectible .",
    "thus the total -volume becomes roughly n +",
    "n + 8n bytes , which is 1.1 times the input+output size .",
    "we can make a cache - oblivious version of the separator - based algorithm by using a hierarchy of subgrids and separators as follows .",
    "we consider @xmath129 levels , numbered 0 to @xmath37 , where @xmath37 is the smallest integer such that the input grid is contained in a @xmath130 grid . on level @xmath61",
    ", we consider subgrids of size @xmath131 , that each share their boundary cells with the neighbouring subgrid . let @xmath132 be the set of boundary cells of all subgrids on level @xmath61 , and let @xmath133 be the union of all subgrid interiors on level @xmath61 . note that @xmath134 , @xmath135 is the full grid , and @xmath136 is the boundary of the full grid .",
    "we can think of the subgrids as being organised in a tree  @xmath137 , where the leaves correspond to subgrids of size @xmath138 , and each internal node on level @xmath61 corresponds to a subgrid @xmath24 on level @xmath61 with four children , namely the subgrids on level @xmath139 that lie inside @xmath24 .",
    "the root of  @xmath137 corresponds to the full grid .",
    "the algorithm now proceeds in two phases . in the first phase we do a post - order traversal of  @xmath137 .",
    "when processing a subgrid @xmath24 at level @xmath61 , we forward the rain that falls on each cell @xmath0 of @xmath140 to the first cell of @xmath132 downstream of @xmath0 , that is , to the boundary of @xmath24 . in the second phase we traverse @xmath137 in reverse order ( which is therefore a pre - order traversal ) .",
    "when processing a subgrid @xmath24 at level @xmath61 , we forward the rain that arrived at each cell @xmath0 on the boundary of @xmath24 to all cells of @xmath141 that lie on the path that leads downstream from @xmath0 until it reaches another cell on the boundary of @xmath24 ( or a cell without an out - neighbour ) .",
    "these two phases together result in all rain that falls on any grid cell @xmath0 to be forwarded to all cells downstream of  @xmath0 .    to implement this approach efficiently",
    ", we use the post - order traversal to create a stack @xmath142 of pointers from boundary cells to internal cells that can be retrieved during the reverse traversal .",
    "more precisely , when processing a subgrid @xmath24 at level @xmath61 in the post - order traversal , we store on @xmath142 the coordinates of the first cell of @xmath143 that lies downstream of @xmath0 , for each cell @xmath0 on the boundary of @xmath24 that has an out - neighbour inside @xmath24 .",
    "[ cacheawareaccumulation ] with a tall cache of size @xmath14 , separator - based cache - oblivious flow accumulation needs only @xmath1 i / o s .    for simplicity",
    "we assume that the input grid is square .",
    "we first analyse the number of pointers @xmath144 that are stored on @xmath142 for a subtree of @xmath137 rooted at a subgrid @xmath24 of size @xmath145 ( where a cell on the boundary of two / four subgrids counts for 1/2 or 1/4 to each of them ) . since pointers are stored only for cells on the boundary of @xmath24 , we get @xmath146 , which solves to @xmath147 .",
    "next we analyse the number of @xmath148 for the post - order traversal of a subtree of @xmath137 rooted at a subgrid @xmath24 of size @xmath145 . for a sufficiently small constant @xmath0",
    ", we have @xmath149 , since we would simply load @xmath24 into memory , do all the necessary processing in memory , and then write @xmath24 back to disk while pushing @xmath150 pointers for @xmath24 and its subgrids on the stack . by the tall - cache assumption",
    ", we thus have @xmath151 . for subgrids @xmath24 of size @xmath152",
    ", we need to make the recursive calls , read the flow values for @xmath153 cells and the @xmath153 pointers stored on @xmath142 for the four subgrids of @xmath24 , and then use this information to forward flow to @xmath153 cells , compute @xmath153 pointers for @xmath24 and push these onto @xmath142 .",
    "forwarding the flow and computing the pointers can easily be done in @xmath153 time and , so that we get : @xmath154 . with base case",
    "@xmath151 , this solves to @xmath155 .",
    "the analysis of the reverse traversal is similar to the analysis of the post - order traversal , so that we get @xmath1 in total .",
    "although the cache - oblivious algorithms needs only @xmath1 , it is not as efficient as the cache - aware algorithm of the previous subsection .",
    "the cache - oblivious algorithm as just described is expensive in two ways .",
    "first , the number of pointers over all levels sums up to approximately @xmath156 . except a small number on the highest levels , all of these pointers are written to and read from disk at least once .",
    "if a pointer is stored in 16 bytes ( coordinates of source and destination ) , this amounts to an -volume of almost 10 times the input+output size . to alleviate this problem ,",
    "an efficient implementation should use larger subgrids as the base case ( for example @xmath157 instead of @xmath138 ) .",
    "second , we need to maintain flow accumulation values for boundary cells . doing so directly in the output file is expensive , because on a vertical subgrid boundary , every cell will be in a different block .",
    "although in the asymptotic analysis this works out , it causes a significant constant - factor overhead . storing the grid in z - order helps to some extent ( as confirmed by our experiments ,",
    "see section  [ sec : evaluation ] ) .",
    "alternatively one could use the stack with pointers to store intermediate flow accumulation values ( similar to @xmath102 in ) , but this would further increase the -volume for stack operations .      in and  @xcite , the following algorithm is used to compute flow accumulation .",
    "we first create a stream of cells in topological order . in principle",
    "it would suffice to sort the cells by decreasing elevation , but this would not work for flat areas .",
    "therefore we will assume that after flooding , a flow routing phase has produced an additional file with a topological number for each cell .",
    "we now create a stream of cells that lists for each cell its location , its topological number and the topological number of its out - neighbour .",
    "we sort this stream by the first topological number .",
    "we now apply a technique called time - forward processing : we maintain a priority queue in which flow values are stored with the topological number of the cell to which the flow is going .",
    "the algorithm processes all cells one by one in topological order ; when processing a cell , it extracts its incoming flow from the priority queue , adds one unit of rain , writes the resulting total to an output stream together with the coordinates of the cell , and finally forwards the total flow to the out - neighbour by entering it in the priority queue with the out - neighbour s topological number as key .",
    "the algorithm finishes by sorting the flow accumulation values from the output stream into a row - by - row grid .    using an -efficient priority queue",
    ", the algorithm clearly runs in @xmath3  @xcite    [ [ io - volume - estimate - for - realistic - values - of - n - m - and - b-2 ] ] i / o - volume estimate for realistic values of @xmath2 , @xmath4 and @xmath5 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for an estimate of the -volume in a practical setting with a grid of size @xmath18 and a memory size of @xmath15 , we consider two scenarios . in the optimistic scenario ,",
    "we have block size @xmath17 , only 1/3 of the cells contain data , and the priority queue is completely maintained in memory at all times . in the pessimistic ( but nevertheless quite realistic ) scenario , we have block size @xmath16 , all cells contain data , and the records that pass through the priority queue are written to and read from disk once on average .    in the optimistic scenario ,",
    "we start with scanning the input file with flow directions and the file with topological numbers , scanning the latter with a window of @xmath86 cells to be able to retrieve topological numbers of neighbours . assuming three rows of the grid fit in memory",
    ", this amounts to scanning 9 bytes per cell .",
    "while doing so , we generate a stream of cells with location , topological number , and topological number of the out - neighbour , which is fed to the first pass of a sorting algorithm .",
    "the sorting algorithm writes the partially sorted stream to disk ( 24 bytes per actual data cell = 8 bytes per grid cell ) .",
    "since @xmath158 , the sorting algorithm needs only two passes ; the second pass results in 16 bytes of per cell ( 24 for reading , 24 for writing , for 1/3 of the cells ) .",
    "the time - forward processing phase reads the sorted stream ( 24 bytes @xmath159 1/3 of the cells ) and eventually outputs locations and flow accumulations for all data cells ( 16 bytes @xmath159 1/3 of the cells ) .",
    "the output then needs to be sorted : the first pass results in @xmath160 bytes of per cell ( 32 bytes per data cell ) .",
    "the second pass reads @xmath161 bytes per cell and writes 8 bytes per grid cell ( flow accumulations only , but also for non - data cells ) . in total , we transfer @xmath162 bytes per grid cell . for a fair comparison ,",
    "we do not count the input file with topological numbers towards the size of the input and output ",
    "after all , all other algorithms presented in this paper do not even need such a file .",
    "so @xmath162 bytes per grid cell amounts to 7.8 times the input+output size .    in the pessimistic scenario , we need three sorting passes , and the priority queue needs the disk .",
    "filling in the details of the computation above , we get an -volume of 289 bytes per cell , which is 32 times the input+output size .",
    "we implemented some of our algorithms and tested them on elevation models of the netherlands and of the neuse watershed in north carolina , using a dell optiplex gx260 computer , equipped with a 3 ghz pentium 4 processor and 1 gb of ram , ubuntu 7.04 ( kernel 2.60.20 - 16 ) ( installed on a 80 gb samsung hd080hj hard disk ) , and a 250 gb seagate st320506as hard disk .",
    "we report the results for the largest data set : a grid of @xmath163 cells modelling the neuse basin ; 35% of the grid cells contain data .",
    "our results are shown in table  [ tab : runningtimes ] .",
    "for comparison , the authors of reported 455 minutes for flow accumulation of a grid of similar size , on hardware that appeared to be faster than ours  @xcite .",
    "our cache - aware flow accumulation algorithm thus seems to be an order of magnitude faster .",
    "note that the -volume estimates given in section  [ sec : cacheawareaccumulation ] and section  [ sec : tfpaccumulation ] would predict a difference of a factor four in the optimistic setting , where we assume that s time - forward processing would manage to do with sorting in two passes and keeping the priority queue in main memory .",
    "although we have not run direct comparisons with the latest version of , our analysis indicates that the performance difference must remain significant because it is inherent to the characteristics of the different algorithms .",
    ".running times in minutes and cpu usage for flow accumulation of a grid of @xmath164 cells .",
    "[ cols= \" < , > , > , > , > \" , ]     our results also indicate that our -nave algorithms perform surprisingly well , especially when working on data in z - order . at first sight",
    "our theoretical analysis seems to explain this : under the confluence assumption the -nave algorithm on z - order files needs only @xmath1 .",
    "however , if we try to estimate the -volume by filling in the constant factors in the computation of the asymptotic bound , then we end up with an -volume bound of dozens ( using z - order ) or thousands ( using row - by - row order ) times the input+output size .",
    "it seems that the surprisingly good performance in practice must be due to the fact that for modest values of @xmath0 we can fit @xmath165 rows of the grid in memory .",
    "thus the main scan can keep blocks in memory long enough so that each block only needs to be loaded once , and apparently the inner loop of the algorithm does not cause as many disruptions as the theoretical analysis might suggest .",
    "all things considered this means that these nave algorithms are surprisingly fast , but we can not rule out that their efficiency may depend on the ratio of @xmath2 and @xmath4 and on the characteristics of the terrain .",
    "in contrast , the efficiency of our cache - aware algorithm is supported firmly by our theoretical analysis .",
    "note that our theoretical analysis also indicates that the algorithm should be much faster still when working on z - order files , requiring little more than two scans of the input file and a single scan of the output file ( further experiments should confirm this ) .",
    "however , converting to z - order takes time too ( converting a grid of this size with 8 bytes per cell took 88 minutes ) . whether it pays off to use files in z - order",
    "may therefore depend on the context .",
    "storing temporary files in the pipeline in z - order seems to be a good idea ; files that need to be processed by other software may better be kept in row - by - row order .",
    "we also implemented the cache - oblivious algorithm , but until now this was significantly slower than the cache - aware algorithm ( even when the subgrid size of the latter was not tuned optimally ) so the cache - oblivious implementation would need to be optimised further to be competitive .",
    "the flooding problem takes as input a file @xmath166 that stores the elevation of each cell .",
    "a path in the grid is a sequence of cells such that each pair of consecutive cells on the path are neighbours of each other ( each cell that is not on the boundary has eight neighbours ) .",
    "the flooding problem in its basic form is to compute to what elevation each cell should be raised , so that from each cell there is a non - ascending path to a cell on the boundary of the grid .",
    "if we define the height of a path as the elevation of the highest cell on the path , the problem is equivalent to determining , for each cell @xmath0 , the height of the lowest path from @xmath0 to the boundary . the required output is a file @xmath167 that stores these heights for each cell @xmath0 .",
    "[ [ time - forward - processing - and - io - nave - flooding ] ] time - forward processing and i / o - nave flooding + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in 2003 arge et al .  described two algorithms for the flooding problem  @xcite .",
    "the first algorithm proceeds in three phases .",
    "first we follow the path from each cell downhill until a sink , a cell without an out - neighbour , is reached .",
    "each cell @xmath0 is labelled with a pointer @xmath168 to the sink at the end of the path that goes downhill from @xmath0 .",
    "this labelling constitutes a decomposition of the terrain into watersheds and is used to build a _ watershed graph _",
    "the watershed graph has one node for every sink , and for every pair of adjacent watersheds with sinks @xmath170 and @xmath171 , the graph contains an edge @xmath172 with height equal to the lowest pass between @xmath170 and @xmath171 .",
    "in other words , the elevation of @xmath172 is the minimum of @xmath173 over all pairs of neighbouring cells @xmath174 such that @xmath175 and @xmath176 . in the second phase an algorithm",
    "is run on @xmath169 to determine to what height each watershed should be flooded . in the third phase , we scan the complete terrain and replace each cell s elevation by the maximum of its original elevation and its watershed s flood height .",
    "arge et al .",
    "use time - forward processing for the first phase , so that the complete algorithm needs @xmath3 , assuming @xmath169 can be kept in main memory .",
    "instead we could consider using an -nave algorithm similar to the one described in section  [ sec : naiveacc ] for the first phase , and get @xmath1 under the confluence assumption .",
    "we found that in practice , this did not work well with row - by - row - ordered files .",
    "using files in z - order we could flood the neuse grid mentioned before in 435 minutes : a running time of roughly the same order of magnitude as terrastream s  @xcite .",
    "[ [ separator - based - algorithms ] ] separator - based algorithms + + + + + + + + + + + + + + + + + + + + + + + + + +    the second algorithm sketched by arge et al",
    ".  seems quite similar to our cache - aware separator - based flow accumulation algorithm . as in section",
    "[ sec : cacheawareaccumulation ] , the idea is to first process subgrids of @xmath177 cells that fit in memory . for each subgrid",
    "@xmath24 we compute a ` substitute ' graph that encodes the lowest - path heights between each pair of cells on the boundary of @xmath24 . in the second phase of the algorithm we would combine the substitute graphs for all subgrids into one graph , which is used to compute the lowest - path heights from each cell of @xmath97 to the boundary of the grid , where @xmath97 is the set of boundary cells of all subgrids .",
    "finally , in the third phase each subgrid @xmath24 is processed again , now to compute the lowest - path height from each cell of @xmath24 to the boundary of the grid , using the previously computed lowest - path heights for the cells on the boundary of @xmath24 .",
    "the key to success is the size of the substitute graphs .",
    "the algorithm by arge et al .",
    "@xcite for single - source _ shortest _ paths works in a similar way , but needs substitute graphs of size @xmath178 to encode the shortest - path distances between all pairs of cells on the boundary of a @xmath91 subgrid .",
    "however , for _ lowest _ paths substitute graphs of size @xmath124 suffice . such a graph can be created from the elevations and neighbour relations in a subgrid @xmath24 as follows .",
    "consider the subgrid @xmath24 as a graph , whose edges represent the neighbour relations in the grid , where each edge has elevation equal to its highest vertex .",
    "compute the lowest paths to the boundary of @xmath24 for all cells in @xmath179 .",
    "next , contract all directed edges @xmath172 of those lowest paths one by one , replacing each undirected edge @xmath180 of the graph with an edge @xmath181 with elevation @xmath182 . whenever there are multiple edges between the same pair of vertices , keep only the edge with the lowest elevation .",
    "this results in a substitute graph whose vertices are the boundary vertices of @xmath24 and which preserves the lowest - path heights between each pair of vertices . since the substitute graph thus constructed is planar , it has size @xmath183 .",
    "we implemented a separator - based flooding algorithm as described above , and found that it could process the neuse grid in 146 minutes , using row - by - row - ordered files on one disk , or in 132 minutes on two disks .",
    "as with our flow accumulation algorithm , we believe further efficiency gains could be achieved by using files in z - order .    [ [ partial - flooding ] ] partial flooding ? + + + + + + + + + + + + + + + + +    it should be noted that a direct comparison between our algorithms and terra - stream can not be made .",
    "offers the functionality of _ partial flooding _ : eliminating only insignificant depressions while not flooding major depressions .",
    "our algorithms do not do this .",
    "a major open question is therefore how the grid structure could be exploited to design an algorithm that can do very fast _ partial _ flooding .",
    "s hierarchical watershed labelling algorithm  @xcite uses time - forward processing to pass labels upwards into the river network .",
    "this is not very different from how time - forward processing is used for flooding or for flow accumulation , and one may expect that grid - based algorithms ( -nave or cache - aware ) may help here too .",
    "open questions include whether grid - based algorithms , maybe together with assumptions on realistic terrains , could help to simplify and speed up flow routing on flat areas and to do flow accumulation with multiple - flow directions , an approach where each cell sends water to all of its lower neighbours instead of just one of them .",
    "we have shown that certain hydrological computations on terrain data may be sped up by an order of magnitude by exploiting the grid structure of the data and/or by storing grids in z - order rather than row - by - row order .",
    "a striking result is that one of the algorithms with the best performance is actually so simple that it is almost nave .",
    "some of the most prominent questions that remain unanswered at this point are the following .    _ what would be typical values for the confluence constant ? _ it would be interesting to design an algorithm that can compute the confluence constant for any given grid terrain .",
    "then we may investigate to what extent the confluence constant is indeed independent of the sampling density of a terrain , and what are typical values for the confluence constant for different types of terrains .",
    "_ how would the cache - aware separator - based algorithm perform on files in z - order ? _ unfortunately we did not have time to implement and run these tests yet , and we hope to be able to do so some time in the future .",
    "_ can we exploit the grid structure to design equally efficient flow routing algorithms ?",
    "_ if yes , then the complete part of the pipeline from elevation model to hierarchical watershed labels could probably be sped up tremendously . in that case , even if files in row - by - row order would be desired at the input and output end of the pipeline , it could pay off to convert them to z - order and to use files in z - order for the intermediate stages .",
    "a disadvantage of our current algorithm is that multiple - flow direction models can not be handled , but for hierarchical watershed labelling such models can not be used and single - flow direction models  those handled by our algorithms  are exactly what is needed .",
    "the authors thank andrew danner for providing test data and assistance with .",
    "we thank laura toma and the students of the 2006 class of -efficient algorithms at the tu eindhoven for inspiring discussions .",
    "l. arge , a. danner , h. haverkort , and n. zeh .",
    "i / o - efficient hierarchical watershed decomposition of grid terrain models . in _ proc .",
    "spatial data handling _",
    "( sdh 2006 ) , pages 825844 , springer , 2006 .",
    "a.  danner , t.  mlhave , k.  yi , p.  k.  agarwal , l.  arge , and h.  mitasova .",
    "terrastream : from elevation data to watershed hierarchies . in _ proc .",
    "advances in geographic information systems .",
    "_ ( gis 2007 ) ."
  ],
  "abstract_text": [
    "<S> the _ flow accumulation _ problem for grid terrains takes as input a matrix of flow directions , that specifies for each cell of the grid to which of its eight neighbours any incoming water would flow . </S>",
    "<S> the problem is to compute , for each cell @xmath0 , from how many cells of the terrain water would reach @xmath0 . </S>",
    "<S> we show that this problem can be solved in @xmath1 for a terrain of @xmath2 cells . taking constant factors in the -efficiency into account </S>",
    "<S> , our algorithm may be an order of magnitude faster than the previously known algorithm that is based on time - forward processing and needs @xmath3 . </S>"
  ]
}