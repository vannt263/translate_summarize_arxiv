{
  "article_text": [
    "when one analyzes data that arrive sequentially over time , it is important to detect changes in the underlying model which can then be adjusted accordingly .",
    "such problems arise in many engineering ( signal processing , speech recognition , communication systems ) , econometric and biomedical applications and can be found in an extensive literature widely scattered in these fields .",
    "inference on time - varying parameters in stochastic systems is therefore of fundamental interest in sequential analysis .",
    "consider an @xmath0-valued time series @xmath1 , @xmath2 , @xmath3 , @xmath4 , such that at time moment @xmath5 the first observation @xmath6 and subsequently at each time moment @xmath7 a new observation @xmath8 arrives according to the model @xmath9 , where @xmath10 .",
    "suppose we are interested in certain characteristics of the conditional distribution of @xmath8 given the past @xmath11 : @xmath12 .",
    "here @xmath13 is an operator mapping conditional distributions @xmath14 into measurable @xmath15-valued functions @xmath16 , @xmath17 , @xmath15 is a compact subset of @xmath18 .",
    "the goal is to estimate ( or to track ) @xmath19 at time instant @xmath20 , based on the data @xmath21 ( and prior information ) available by that time moment .",
    "the traditional parametric formulation is the most simple particular case of the above setting : the observations are independent and the parameter @xmath22 is a constant vector .",
    "the simplest nonparametric formulation deals again with independent observations and time - varying parameter @xmath23 , @xmath20 ( cf .  .",
    "modeling observations by a markov chain with a time varying parameter of the transition law would add a next level of complexity ( cf .",
    "for the autoregressive model in @xcite ) .",
    "the proposed time series formulation admits an arbitrary dependence structure between the observations . another important and peculiar feature of our approach is that the multidimensional parameter @xmath24 , @xmath7 , besides being time - varying , is also allowed to depend on the past of the time series .",
    "it is thus a predictable process with respect to the natural filtration : @xmath25 .",
    "an example of such characteristics is the conditional expectation @xmath26 $ ] .",
    "this time series formulation , with time - varying parameter of interest which is also allowed to depend on the past , represents the most general sequential framework , independent observations and markov chains are typical examples of models that fit into this framework .",
    "since the data arrives in a successive manner , conventional methods based on samples of fixed size are not easy to use .",
    "a more appropriate approach is based on sequential methods , stochastic recursive algorithms , which allow fast updating of parameter or state estimates at each instant as new data arrive and therefore can be used to produce an `` online '' inference , that is , during the operation of the system .",
    "stochastic recursive algorithms , also known as stochastic approximation , take many forms and have numerous applications in the biomedical , socio - economic and engineering sciences , which highlights the interdisciplinary nature of the subject .",
    "there is a vast literature on stochastic approximation beginning with the seminal papers of robbins and monro @xcite and of kiefer and wolfowitz @xcite .",
    "there is a big variety of techniques in the area of stochastic approximation which have been developed and inspired by the applications from other fields .",
    "we mention here the books @xcite .    a classical topic in adaptive control concerns the problem of tracking drifting parameters of a linear regression model , or somewhat equivalently , tracking the best linear fit when the parameters change slowly .",
    "this problem also occurs in communication theory for adaptive equalizers and noise cancellation , etc . , where the signal , noise , and channel properties change with time .",
    "successful stochastic approximation schemes for tracking in the time - varying case were given by @xcite ( see further references therein ) .",
    "coming back to our time series model , the problem of tracking a time - varying parameter that is a functional of conditional distribution of the current observation given the past , is clearly unfeasible , especially in such general formulation , without some conditions on the model . in general , some knowledge about the structure of underlying time seres and some control over the variability of the parameter of interest over time are needed .",
    "interestingly , in this seemingly very general time series framework , we actually do not require the ( full ) knowledge of the observational model . instead , all we do need is to be able to compute a so called _ gain vector _ at each time moment @xmath7 , which is a certain ( vector ) function of the previous estimate of the parameter @xmath19 , new observation @xmath8 and prehistory @xmath27 .",
    "the essential property of such gain vector is that it , roughly speaking , `` pushes '' in the right direction of the current value of true parameter to track .",
    "although the assumption about the existence of that gain vector seems to be rather strong , we demonstrate on a number of interesting examples when such an assumption indeed holds .",
    "basically , in case of markov chain observations , if the form of transition density is known as function of the underlying parameter and it satisfies certain regularity assumptions , then the gain vector can always be constructed , for example , as a score function corresponding to the conditional maximum likelihood method . under appropriate regularity conditions ( the existence of the conditional fisher information and @xmath28-differentiability of the conditional log likelihood ) ,",
    "such a score function has always the property of gain vector at least locally .",
    "a gain function , together with a step sequence and new observations from the model , can be used to adjust the current approximation of the drifting parameter , resulting in a tracking algorithm . to ease the verification of our assumptions on the gain function ,",
    "we formulate them in two equivalent forms . under some assumptions on the gain vectors ,",
    "we establish a uniform non - asymptotic bound the @xmath29 error of the resulting tracking algorithm , in terms of the variation of the drifting parameter . under the extra assumption",
    "that the gain function is bounded , we can strengthen this result to a uniform bound on the @xmath30 error ( and then an almost sure bound ) .",
    "these error bounds constitute our main result and they also guide us in the choice of the step size for the algorithm .",
    "some extensions are also presented where we allow for approximation terms and approximate gains .",
    "based on our main result , we specify the appropriate choice for the step sequence in three different variational setups for the drifting parameter .",
    "we treat first the simple case of a constant parameter . although we are mainly concerned with tracking time - varying parameters , our algorithm is still of interest in the constant parameter case since it should result in an algorithm which is both recursive and robust .",
    "we also consider a setup where the parameter is stabilizing .",
    "this covers both the case where the parameter is converging and where we sample the signal with increasing frequency .",
    "the third variational setup covers the important case of tracking smooth signals .",
    "this setup is somewhat different in that we make observations with a certain frequency from an underlying continuous time process which is indexed by a parameter changing like a lipschitz function .",
    "our result can then either be interpreted as a uniform , non - asymptotic result for each fixed sampling frequency or as an asymptotic statement in the observation frequency .",
    "examples are also given for different possible gain functions .",
    "these fall into two categories : general , score based gain functions for tracking multidimensional parameters in regular models and specialized gains for tracking more specific quantities .",
    "the latter include gains to track level sets or maxima of drifting functions ( extending the classical robbins - monro and kiefer - wolfowitz algorithms ) and gains to track drifting conditional quantiles .",
    "we also propose modifications for a given gain function ( rescaling , truncation , projection ) which can be used to design gains tailored specifically to verify our assumptions .",
    "we illustrate our method by treating some concrete applications of the proposed algorithm , in particular , we elaborate on the problem of tracking drifting parameters in autoregressive models . results on tracking algorithms for these models already exist in the literature ( cf.@xcite ) and we can derive similar results by choosing an appropriate gain function .",
    "using our approach , obtaining error bounds on the resulting tracking algorithm reduces to verifying our assumptions for the chosen gain function which considerably simplifies the derivation of results .",
    "this paper is structured as follows . in section  [ sec : preliminaries ] we summarize the notation that will be used thought the paper , as well as our model and two equivalent formulations for our assumptions .",
    "section  [ sec : main_result ] contains our main result and respective proof as well as some straightforward extensions of the main result .",
    "the construction and modification of gain functions for different models and different parameters of interest is explained in section  [ sec : gains ] .",
    "section  [ sec : variational_setups ] contains three examples of variational setups for the time - varying parameter for which we specify the tracking error implied by our main result .",
    "we collect in section  [ sec : examples ] some examples of applications .",
    "section  [ sec : proofs ] contains the proofs for our lemmas .",
    "first we introduce some notation that we are going to use throughout the paper .",
    "all vectors are always column vectors .",
    "we use bold letters to represent matrices and families of vectors , uppercase letters for families of random vectors and matrices ; denote @xmath31 for a set of vectors @xmath32 . for @xmath33 , @xmath34 ( of course , @xmath35 ) , @xmath36 .",
    "we use interchangeably @xmath37 for @xmath38 . for vectors",
    "@xmath39 , denote by @xmath40 and @xmath41 the usual euclidean norm and the inner product in @xmath18 , respectively , and by @xmath42 the @xmath43 norm ( with @xmath44 ) on vectors in @xmath18 . for an event @xmath45 , we denote by @xmath46 the indicator of the event @xmath45 . for a symmetric @xmath47-matrix @xmath48 ,",
    "let @xmath49 and @xmath50 be the smallest and the largest eigenvalues of @xmath48 respectively .",
    "denote by @xmath51 denote the zero matrix and by @xmath52 the identity matrix , whose dimensions will be determined by the context . besides , we adopt the convention that @xmath53 and @xmath54 for matrices @xmath55 and @xmath56 . when applied to matrices",
    ", @xmath57 represents the matrix norm induced by the @xmath43 vector norm : @xmath58 .",
    "assume that by time @xmath59 , time series data @xmath60 ( which we may not be fully observable ) occur according to the following model : @xmath61 here vector @xmath8 takes value in some set @xmath62 with @xmath4 , i.e. , @xmath63 , @xmath20 . by @xmath64",
    "we denote the conditional distribution of @xmath8 given @xmath27 and @xmath65 for @xmath20 , with @xmath66 .",
    "thus , @xmath67 takes values in @xmath68 .",
    "clearly , the distribution of @xmath69 , @xmath59 , is given by @xmath70 where @xmath71 should be understood as @xmath72 . for the sake of consistent notation , @xmath73 ( also @xmath74 etc . )",
    "means void variables from now on .",
    "introduce an increasing sequence of @xmath75-algebras @xmath76 ( i.e. , @xmath77 if @xmath78 ) such that @xmath79 , @xmath20 , where @xmath80 denote the @xmath75-algebra generated by @xmath67 and @xmath81 is a @xmath75-algebra such that @xmath82 . unless otherwise specified , we assume that @xmath83 .",
    "as is discussed in remark [ rem : filtration ] below , the case @xmath84 is not conceptually new because it can be reduced to the situation of the time series @xmath85 with @xmath86 for some @xmath87-measurable functions @xmath88 s such that @xmath89 , @xmath90 .",
    "now we describe the statistical model which is a family probability measures for the observed data . for each @xmath20 , let @xmath91 be a given family of conditional probability measures of @xmath8 given @xmath92",
    ". then the underlying statistical model is determined by imposing @xmath93 , @xmath20 .",
    "thus , at time @xmath94 , the underlying ( growing ) statistical model is @xmath95 .    for some compact subset @xmath15 of @xmath96 , denote by @xmath97 the borel @xmath75-algebra on @xmath15 and by @xmath98 the set of @xmath15-valued @xmath99-measurable functions on @xmath68 , @xmath20 .",
    "consider a sequence of operators @xmath100 , @xmath20 , so that for a @xmath93 @xmath101 with @xmath102 .",
    "we will often abbreviate @xmath103 , remembering that this is a measurable function of @xmath104 . for @xmath5 , since @xmath73 is void , @xmath105 means a constant .",
    "our goal is to design an online algorithm for tracking the drifting _",
    "parameter of interest _ @xmath19 at time moment @xmath90 on the basis of the data @xmath67 observed by that time moment .",
    "the time - varying parameter @xmath25 , @xmath20 , is thus allowed to depend on the past of the time series , i.e. , it is a predictable process with respect to the filtration @xmath106 . recall further that @xmath107 is assumed to take values in some compact subset @xmath15 of @xmath18 , to be precise , @xmath108 for all @xmath90 .",
    "denote @xmath109    at time @xmath110 , given @xmath67 , the model @xmath111 contains all the relevant information about the next observation .",
    "actually , we do not consider the model to be ( completely ) known .",
    "instead , we assume that our prior knowledge about the model is formalized as follows : for each @xmath20 , we have certain @xmath112-measurable functions @xmath113 at our disposal ( which we call _ gain functions _ or _",
    "gain vectors _ or just _ gains _ ) , @xmath114 .",
    "we use these gain functions to construct a recursive algorithm for tracking the sequence @xmath115 from the observations ( [ model ] ) : @xmath116 for some positive sequence @xmath117 and some ( arbitrary ) initial value @xmath118 , measurable with respect to @xmath81 . since @xmath119 is @xmath120-measurable , then @xmath121 is predictable with respect to the filtration @xmath106 .",
    "notice that @xmath122 can be a random vector if @xmath81 is not the trivial @xmath75-algebra .",
    "of course , it is not to be expected that the tracking algorithm ( [ eq : algorithm_main ] ) performs well for arbitrary gains .",
    "intuition suggests that the gain @xmath113 should `` push '' @xmath123 in the direction of @xmath107 .",
    "the following conditions formalize this requirement .",
    "* for all @xmath20 , the quantity , which we call _ ( conditional ) average gain _ , @xmath124\\",
    "] ] is well defined ( recall that @xmath125 and @xmath123 is defined by ( [ eq : algorithm_main ] ) ) and there exist a @xmath120-measurable symmetric positive definite matrix @xmath126 ( its entries are @xmath120-measurable functions ) such that , almost surely @xmath127 @xmath128,\\quad   \\lambda_1 \\le\\lambda_{(d)}(\\bm{m}_k ) \\le \\lambda_2,\\ ] ] for some fixed constants @xmath129 .",
    "* there exists a constant @xmath130 such that @xmath131    as one can see from ( [ eq : lambda ] ) , a @xmath75-algebra @xmath132 , such that @xmath133 , is also needed . without loss of generality , assume that @xmath134 , the trivial @xmath75-algebra on @xmath135 .",
    "we will often use shorthand notation : @xmath136 and @xmath137 .",
    "condition ( a1 ) means , in a way , that the gain @xmath138 shifts any @xmath139 , on average , towards the `` true '' value @xmath140 .",
    "this elucidates the idea of algorithm ( [ eq : algorithm_main ] ) .",
    "suppose at time instant @xmath141 an observer had a reasonable estimator @xmath142 of the `` old '' value of the parameter of interest @xmath143 , and a new data vector @xmath8 arrives .",
    "then the available data is @xmath144 and the observer can construct an estimator @xmath145 of the new value of the parameter @xmath25 by calculating the gain @xmath146 and using a rescaled ( by a step size @xmath147 ) version of it to update the `` old '' estimator @xmath148 towards @xmath25 .",
    "the upper bound in relation ( [ eq : lambda ] ) means that the gain is of a bounded magnitude , and the lower bound has the meaning of the so called _ persistence of excitation _ as it is termed in control theory literature .",
    "assumption ( a2 ) is trivially satisfied if the gain vectors @xmath138 are almost surely uniformly bounded .",
    "this is not so difficult to arrange , for example , by dividing the gain vector by a multiple of its length or by truncating . in doing so , we make the resulting gain vector bounded , whereas retaining its direction .",
    "we discuss these approaches in more detail in section [ sec : gains ] .",
    "assumption ( a2 ) is also satisfied if @xmath149 indeed , for a random vector @xmath150 with a finite second moment of its norm and any @xmath75-algebra @xmath151 , @xmath152\\le { { \\mathbb{e}}}\\|x\\|^2 $ ] . combining this with ( [ g_bounded ] ) yields @xmath153 , @xmath90 .    on the other hand , from ( [ c_theta ] ) , ( a1 ) and ( a2 )",
    "it follows that @xmath154 this relation and lemma [ lemma_bound ] below ( thus the conditions of lemma [ lemma_bound ] must hold ) will in turn imply the uniform bound ( [ g_bounded ] ) .    generally , there is no universal way to find gain vectors which satisfy conditions ( a1 ) and ( a2 ) . in many practical situations ,",
    "the model @xmath155 is typically specified and it is an art to find gain vectors which satisfy ( a1 ) and ( a2 ) ; we discuss this issue in more detail in section [ sec : gains ] .",
    "the assumptions above look somewhat unnatural and cumbersome because they are assumed to hold for all @xmath156 , whereas functions involved in the conditions depend in general on @xmath27 whose dimension increases unlimitedly as @xmath110 increases .",
    "however , the assumptions become reasonable in the important case of markov chain observations @xmath157 of order , say , @xmath158 . in this case , for any @xmath141 we can use vector of bounded dimension @xmath159 instead of @xmath27 ( of growing dimension ) in all the quantities from conditions ( a1 ) and ( a2 ) .",
    "independent observations is a next simplification , also important in many practical applications . in this case",
    "there is no past involved in the function @xmath107 , @xmath160 , it will only be a function of time .",
    "[ rem : filtration ] suppose that conditions ( a1 ) and ( a2 ) hold for the filtration @xmath161 and for some measurable gain functions @xmath162 , @xmath20 , but the parameter sequence @xmath163 is predictable with respect to a coarser filtration @xmath76 , i.e. , @xmath164 , @xmath20 , where @xmath165 for some measurable @xmath88 s .",
    "for example , the vector @xmath8 consist of two subvectors @xmath166 and @xmath167 ( i.e. , @xmath168 ) and @xmath169 with @xmath170 ( think of @xmath171 as unobservable part of @xmath67 and @xmath172 as observable ) .",
    "then , by the tower property of the conditional expectation , conditions ( a1 ) and ( a2 ) hold for the filtration @xmath76 as well if we take the new gain function @xmath173 $ ] , @xmath20 .",
    "in fact , this means that the case of a coarser filtration @xmath76 , with respect to which the parameter sequence @xmath163 is predictable , can be reduced to the above described setup in terms of `` new time series '' @xmath172 , @xmath90 , and the filtration @xmath76 generated by this new time series ( or other way around ) .",
    "thus , if the parameter sequence @xmath163 is known to be predictable with respect to a coarser filtration @xmath76 , we could have considered this coarser filtration and the corresponding time series @xmath174 from the very beginning , and impose conditions ( a1 ) and ( a2 ) in terms of @xmath174 and @xmath76 .",
    "in fact , the coarser the filtration @xmath76 , the weaker the conditions .",
    "on the other hand , if the parameter sequence @xmath163 is known to be predictable with respect to a finer filtration @xmath76 ( i.e. , @xmath175 , @xmath20 ) , then the key relation ( [ eq : d ] ) will most likely not hold since the expression on the left hand side is @xmath80-measurable and the expression on the right hand side is @xmath176-measurable .",
    "however , in such situation ( [ eq : d ] ) may still hold with some ( small ) error @xmath177 , we address this issue in remark [ rem : close_parameter ] below .",
    "intuitively , the information of what we observe should match ( or , at least , not be less than ) the information of what we want to track .",
    "[ rem : parametrized ] consider one particular case of our general setting . at time @xmath94",
    ", we observe @xmath60 such that @xmath178 the model in this case is @xmath179 and the operator @xmath180 .",
    "this is a convenient formulation when the time series model is parametrized by a time - varying parameter which we would like to recover by using an online tracking algorithm .",
    "also in this case we can actually allow the parameter @xmath107 to depend on the past of the time series , i.e. , @xmath25 so that the sequence @xmath181 is predictable with respect to the filtration @xmath76 .",
    "condition ( a1 ) can be reformulated as condition ( 1 ) below , which gives some intuition about the role of the average gain @xmath182 defined in ( a1 ) and which may , in certain situations , be easier to verify .    * for @xmath160 , the average gain @xmath183 defined in ( a1 )",
    "satisfies , almost surely , the following conditions : there exist random variables @xmath184 , @xmath185 and constants @xmath186 , @xmath187 such that @xmath188 and @xmath189 with @xmath190 $ ] and @xmath191 .    in view of the lemma below , if ( a1 ) holds , then ( 1 ) will also hold ( and vice versa ) ; the values of the constants @xmath192 and @xmath193 appearing in the assumptions are different , though .",
    "the proof of this lemma is deferred to section  [ sec : proofs ] .",
    "[ lemma1 ] let @xmath194 . if there exists a symmetric positive definite matrix @xmath48 such that @xmath195 and @xmath196 for some @xmath197 , then @xmath198 and @xmath199 for some @xmath200 ( depending only on @xmath201 ) such that @xmath202 and @xmath203 .",
    "conversely , if @xmath198 and @xmath199 for some @xmath200 such that @xmath202 and @xmath203 , then there exists a symmetric positive definite matrix @xmath48 such that @xmath195 and @xmath196 for some constants @xmath197 depending only on @xmath204 and @xmath205 .",
    "we start with a lemma which we will need in the proof of the main result .",
    "heuristically , since the gain vector @xmath146 moves , on average , @xmath123 towards @xmath107 and the sequence @xmath206 is bounded ( since @xmath15 is compact ) , the resulting estimating sequence @xmath123 should also be well - behaved .",
    "the following lemma states that the second moment of @xmath123 is uniformly bounded in @xmath20 for sufficiently small @xmath207 s .    [ lemma_bound ]",
    "let assumptions ( a1 ) and ( a2 ) hold .",
    "then for sufficiently small @xmath207 there exists a constant @xmath208 such that @xmath209    the proof of this lemma is given in section  [ sec : proofs ] .",
    "in fact , it is enough to assume that @xmath207 is sufficiently small for all @xmath210 for some fixed @xmath211 .",
    "this is the case if @xmath212 as @xmath213 , which is typically assumed .",
    "this lemma will be used in the proof of the main theorem below . from now on we assume that the sequence @xmath207 is such that lemma  [ lemma_bound ] holds .",
    "the following theorem is our main result , it provides a non - asymptotic upper bound on the quality of the tracking algorithm ( [ eq : algorithm_main ] ) in terms of of the algorithm step sequence @xmath214 and oscillation of the process to track @xmath215 between arbitrary time moments @xmath216 , @xmath217 .",
    "[ theo : bound ] let assumptions ( a1 ) and ( a2 ) hold , the tracking sequence @xmath123 be defined by ( [ eq : algorithm_main ] ) and @xmath218 , @xmath90 . then for any @xmath219 and sequence @xmath220 ( satisfying the conditions of lemma [ lemma_bound ] ) such that @xmath221 and @xmath222 for all @xmath223 , the following relation holds : @xmath224^{1/2 } \\!\\!+ c_3\\max_{k_0\\le i\\le k } { { \\mathbb{e}}}\\|\\theta_{i+1}-\\theta_{k_0}\\|,\\ ] ] where @xmath225 , @xmath226 , @xmath227 , constants @xmath228 are from assumptions ( a1 ) and ( a2 ) , @xmath229 is defined by ( [ c_theta ] ) and @xmath208 is from lemma  [ lemma_bound ] .",
    "[ rem : norms ] by using ( [ eq : bounde ] ) , one can derive a bound for @xmath230 , with @xmath44 .",
    "indeed , as @xmath231 for any @xmath232 and @xmath233 , we obtain that @xmath234 for @xmath235 and @xmath236 for @xmath237",
    ".    for the sake of brevity , denote @xmath238 , @xmath239 , @xmath240 and @xmath241 , @xmath20 .",
    "we have @xmath242= { { \\mathbb{e}}}[(g_k - g_k)|\\mathcal{f}_{k-1}]=g_k - g_k=0 , \\quad k \\in \\mathbb{n}_0.\\ ] ] it follows that @xmath243 , is a ( vector ) martingale difference sequence with respect to the filtration @xmath106 .",
    "rewrite the algorithm equation ( [ eq : algorithm_main ] ) as @xmath244 in view of ( a1 ) , the decomposition @xmath245 holds almost surely , with an @xmath120-measurable symmetric positive definite matrix @xmath246 , so that @xmath247 by iterating the above relation , we obtain that for any @xmath248 @xmath249 \\delta_{k_0 } + \\sum_{i = k_0}^k \\big[\\prod_{j = i+1}^k(\\bm{i}- \\gamma_j \\bm{m}_j)\\big ] ( \\delta\\theta_i+\\gamma_i d_i).\\end{aligned}\\ ] ] denote @xmath250 , @xmath251 and @xmath252 . applying the abel transformation ( lemma [ lemma : abel ] ) to the second term of the right hand side of ( [ eq : recursed ] ) yields @xmath253 ( \\delta\\theta_i+\\gamma_i d_i ) = h_k - \\sum_{i = k_0}^{k-1}\\ !",
    "\\gamma_{i+1}\\bm{m}_{i+1 } \\big[\\!\\prod_{j = i+2}^k(\\bm{i}-\\gamma_j \\bm{m}_j)\\big]h_i.\\ ] ] in particular , note that if we take @xmath254 , @xmath255 and @xmath256 for @xmath257 , @xmath258 and @xmath259 for @xmath260 , we derive that ( since @xmath261 for @xmath262 ) @xmath263 which we will use later .",
    "using ( [ abel_transform ] ) , we can rewrite our expansion of @xmath264 in ( [ eq : recursed ] ) as follows : @xmath265 \\delta_{k_0 } + h_k - \\sum_{i = k_0}^{k-1}\\gamma_{i+1}\\bm{m}_{i+1 } \\big[\\prod_{j = i+2}^k(\\bm{i}-\\gamma_j\\bm{m}_j)\\big]h_i.\\end{aligned}\\ ] ]    the previous display , the minkowski inequality and the sub - multiplicative property of the operator norm ( @xmath266 ) imply that @xmath267    in view of ( a1 ) and the condition @xmath222 for @xmath268 , @xmath269 , @xmath268 , almost surely .",
    "hence @xmath270 , @xmath268 , almost surely .",
    "this , lemma  [ lemma : eig ] and the fact ( see ( [ eq : lambda ] ) from ( a1 ) ) that @xmath271 $ ] , @xmath268 , almost surely , imply that @xmath272\\notag\\\\ & = { { \\mathbb{e}}}\\big [ { { \\mathbb{e}}}\\big [ \\big(1-\\gamma_k \\lambda_{(1)}(\\bm{m}_k)\\big)\\big|\\mathcal{f}_{k-2}\\big ] \\prod_{i = k_0}^{k-1 }   \\big(1- \\gamma_i \\lambda_{(1)}(\\bm{m}_i)\\big ) \\big ] \\notag\\\\ & \\label{eq : m_i } \\le ( 1-\\gamma_k \\lambda_1 ) { { \\mathbb{e}}}\\prod_{i = k_0}^{k-1}\\big(i-\\gamma_i \\lambda_{(1)}(\\bm{m}_i)\\big ) \\le\\ldots \\le   \\prod_{i = k_0}^k ( 1- \\gamma_i \\lambda_1 ) .\\end{aligned}\\ ] ] by ( [ c_theta ] ) and lemma  [ lemma_bound ] , we have @xmath273 .",
    "using this fact , the cauchy - schwartz inequality , ( [ eq : m_i ] ) and the elementary inequality @xmath274 , @xmath275 , leads to @xmath276   & \\le \\big[{{\\mathbb{e}}}\\|\\delta_{k_0}\\|^2\\ , { { \\mathbb{e}}}\\prod_{i = k_0}^k \\|\\bm{i}- \\gamma_i \\bm{m}_i\\|^2\\big]^{1/2 } \\notag\\\\ & \\label{eq : products } \\le   c_1\\prod_{i = k_0}^k ( 1- \\gamma_i \\lambda_1 ) ^{1/2 } \\le c_1\\exp\\big\\{-\\frac{\\lambda_1}{2}\\sum_{i = k_0}^k\\gamma_i\\big\\}.\\end{aligned}\\ ] ]    let @xmath277 denote the @xmath278-th coordinate of the vector @xmath279 .",
    "clearly , for each @xmath280 , @xmath281 is a martingale difference with respect to the filtration @xmath106 . using ( [ eq : g2 ] ) from ( a2 ) and",
    "the fact that martingale difference terms are uncorrelated , we derive that for all @xmath282 @xmath283 denote for brevity @xmath284 , so that @xmath285 , @xmath282 .",
    "the obtained relation for @xmath286 , together with the minkowski and hlder inequalities , imply that for all @xmath282 @xmath287    notice that @xmath288 from ( [ eq : after_triangle_lp ] ) is @xmath289-measurable for all @xmath290 .",
    "therefore , by ( [ eq : lambda ] ) from ( a1 ) , ( [ eq : eh_i ] ) and lemma [ lemma : eig ] , @xmath291 = { { \\mathbb{e}}}{{\\mathbb{e}}}\\big[\\|\\bm{m}_{i+1}\\|\\|h_i\\| \\prod_{j = i+2}^k \\|\\bm{i}- \\gamma_j \\bm{m}_j\\| \\big| \\mathcal{f}_{k-2 } \\big]\\notag\\\\ & = { { \\mathbb{e}}}\\big [ { { \\mathbb{e}}}\\big[1-\\gamma_k \\lambda_{(1)}(\\bm{m}_k ) \\big| \\mathcal{f}_{k-2 } \\big ] \\|\\bm{m}_{i+1}\\|\\|h_i\\|   \\prod_{j = i+2}^{k-1 } \\big(1- \\gamma_j \\lambda_{(1)}(\\bm{m}_j)\\big ) \\big ] \\notag\\\\ & \\le(1-\\gamma_k\\lambda_1 )   { { \\mathbb{e}}}\\big[\\|\\bm{m}_{i+1}\\|\\|h_i\\| \\prod_{j = i+2}^{k-1 } \\big(1- \\gamma_j \\lambda_{(1)}(\\bm{m}_j)\\big)\\big ] \\le \\ldots \\notag\\\\ & \\le { { \\mathbb{e}}}\\big(\\|\\bm{m}_{i+1}\\| \\|h_i\\|\\big ) \\prod_{j = i+2}^k(1- \\gamma_j \\lambda_1 ) \\le \\lambda_2 { { \\mathbb{e}}}\\|h_i\\| \\prod_{j = i+2}^k(1- \\gamma_j \\lambda_1 ) \\notag\\\\ &   \\label{eq : c_i } \\le \\lambda_2 \\big[c_g^{1/2 } \\gamma_{k_0,k } + { { \\mathbb{e}}}\\|b_i\\| \\big ] \\prod_{j = i+2}^k(1- \\gamma_j \\lambda_1).\\end{aligned}\\ ] ]",
    "now we take the expectation of relation ( [ eq : after_triangle_lp ] ) and use relations ( [ eq : products ] ) , ( [ eq : eh_i ] ) , ( [ eq : c_i ] ) and ( [ eq : bound_on_coefs ] ) to derive that @xmath292 is bounded from above by @xmath293 + { { \\mathbb{e}}}\\|h_k\\| + \\sum_{i = k_0}^{k-1 } \\gamma_{i+1 } { { \\mathbb{e}}}\\big [ \\|\\bm{m}_{i+1}\\| \\|h_i\\| \\prod_{j = i+2}^k \\|\\bm{i}-\\gamma_j\\bm{m}_j\\| \\big]\\\\ & \\le   c_1\\exp\\big\\{-\\frac{\\lambda_1}{2}\\sum_{i = k_0}^k\\gamma_i\\big\\ }    + { { \\mathbb{e}}}\\|h_k\\| + \\lambda_2 \\sum_{i = k_0}^{k-1 } \\gamma_{i+1 }    { { \\mathbb{e}}}\\|h_i\\| \\prod_{j = i+2}^k ( 1- \\gamma_j \\lambda_1 )   \\\\ & \\le c_1\\exp\\big\\{-\\frac{\\lambda_1}{2}\\sum_{i = k_0}^k\\gamma_i\\big\\}+ { { \\mathbb{e}}}\\|h_i\\| \\big[1+\\sum_{i = k_0}^{k-1}\\lambda_2 \\gamma_{i+1 } \\prod_{j = i+2}^k \\big(1-\\gamma_j \\lambda_1\\big)\\big ] \\\\ & \\le    c_1\\exp\\big\\{-\\frac{\\lambda_1}{2}\\sum_{i = k_0}^k\\gamma_i\\big\\}+ \\big[c_g^{1/2 } \\gamma_{k_0,k}+\\max_{k_0\\le i\\le k } { { \\mathbb{e}}}\\|\\theta_{i+1}-\\theta_{k_0}\\| \\big ] \\big(1+\\frac{\\lambda_2}{\\lambda_1}\\big),\\end{aligned}\\ ] ] where we also used in the last bound that @xmath294 , @xmath295 , is a telescopic sum .",
    "this completes the proof of the the theorem .    at this stage",
    ", it may not be clear how the non - asymptotic bound from theorem [ theo : bound ] can be utilized .",
    "the obtained result is not useful unless we assume some sort of damping of the oscillations of the parameter process @xmath181 .",
    "looking ahead , in section [ sec : variational_setups ] we impose certain settings for damping of the parameter process oscillations ( either `` stabilizing '' in time or increasing the observation frequency ) and derive results in various asymptotic regimes by using our main theorem [ theo : bound ] .",
    "if we assume a slightly stronger version of ( [ eq : lambda ] ) in ( a1 ) , @xmath296 almost surely , then a slightly better version of bound ( [ eq : products ] ) holds : @xmath297 \\le   \\bar{c}_1 \\prod_{i = k_0 + 1}^k ( 1- \\gamma_i \\lambda_1 ) \\le   \\bar{c}_1\\exp\\big\\{-\\lambda_1\\sum_{i = k_0}^k\\gamma_i\\big\\},\\ ] ] since @xmath298 , by ( [ c_theta ] ) and lemma  [ lemma_bound ] .",
    "we can derive a bound alternative to ( [ eq : products ] ) , which leads to slightly better constants in the first term of the right hand side of ( [ eq : bounde ] ) .",
    "indeed , @xmath299 is @xmath120 -measurable , and , instead of ( [ eq : products ] ) , we derive @xmath300 = { { \\mathbb{e}}}{{\\mathbb{e}}}\\big[\\|\\delta_{k_0}\\| \\prod_{i = k_0}^k \\big(1- \\gamma_i \\lambda_{(1)}(\\bm{m}_i)\\big)\\big|\\mathcal{f}_{k-2}\\big ] \\\\ &",
    "\\le{{\\mathbb{e}}}\\big [ \\|\\delta_{k_0}\\|    \\prod_{i = k_0}^{k-1}\\big(1-\\gamma_i \\lambda_{(1)}(\\bm{m}_i)\\big)\\big ] ( 1-\\gamma_k \\lambda_1 ) \\le\\ldots \\\\ & \\le { { \\mathbb{e}}}\\big[\\|\\delta_{k_0}\\|\\big(1-\\gamma_{k_0 } \\lambda_{(1)}(\\bm{m}_{k_0})\\big ) \\big ] \\prod_{i = k_0 + 1}^k ( 1- \\gamma_i \\lambda_1 )   \\le { { \\mathbb{e}}}\\big [ \\|\\delta_{k_0}\\|\\big ] \\prod_{i = k_0 + 1}^k ( 1- \\gamma_i \\lambda_1 ) \\\\ & \\le ( \\bar{c}_\\theta^{1/2}+c_\\theta^{1/2 } ) \\prod_{i = k_0 + 1}^k ( 1- \\gamma_i \\lambda_1 ) .\\end{aligned}\\ ] ]    if we assume that @xmath301 for all @xmath302 and @xmath303 , then we can prove lemma  [ lemma_bound ] in another way : first take the expectation of the second power of the relation ( [ eq : after_triangle_lp ] ) with @xmath304 to establish that @xmath305 is uniformly bounded in @xmath20 , and then @xmath306 , by ( [ c_theta ] ) .",
    "[ rem_version_theorem ] one can try to establish a version of theorem [ theo : bound ] where , instead of ( [ eq : lambda ] ) , one assumes @xmath307\\big ) \\le",
    "\\lambda_{(d ) } \\big(\\mathbb{e } [ \\bm{m}_k |\\mathcal{f}_{k-2}]\\big ) \\le \\lambda_2",
    "\\quad \\mbox{almost surely}.\\ ] ] the point is that there may be situations with certain gain functions when ( [ eq : lambda ] ) does not hold but ( [ different_bound ] ) does ; see remark [ rem_mouilines ] below .",
    "the idea of the proof would be to first introduce @xmath308 $ ] , @xmath309 , and then , beginning with the relation ( [ eq : recursed ] ) , work with the representation @xmath310 instead of just @xmath311 , using the relation ( [ different_bound ] ) for @xmath312 and the fact that @xmath313 , is a ( matrix ) martingale difference sequence with respect to the filtration @xmath314 .",
    "we will not pursue this here .",
    "imposing somewhat stronger versions of conditions ( a1 ) and ( a2 ) enables us to derive a similar non - asymptotic bound for the expectation of @xmath315 for all @xmath44 . of course",
    ", the bigger @xmath158 , the bigger the constants involved in the bound .",
    "the next theorem is a strengthened version of the previous result .",
    "[ theo : bound2 ] suppose that the conditions of theorem [ theo : bound ] are fulfilled .",
    "if , in addition ( to assumption ( a1 ) ) , @xmath316 and @xmath317 ( instead of ( a2 ) ) almost surely for all @xmath318 , then for any @xmath44 @xmath319^{p/2 } + c'_3\\max_{k_0\\le i\\le k}{{\\mathbb{e}}}\\|\\theta_{i+1}-\\theta_{k_0}\\|_p^p,\\end{aligned}\\ ] ] where @xmath320 , @xmath321 , @xmath322 and @xmath323 is the constant from lemma [ lemma : eig ] .",
    "now we have stronger versions of assumptions ( a1 ) and ( a2 ) : @xmath324 hold almost surely . along the same lines as for ( [ eq : after_triangle_lp ] ) , by using lemma [ lemma : eig ] , ( [ eq : a1_stronger ] ) , ( [ eq : bound_on_coefs ] ) and the elementary inequality @xmath325 , we obtain that @xmath326\\\\ & \\le k_p \\|\\delta_{k_0}\\|_p \\exp\\big\\{-\\lambda_1\\sum_{i = k_0}^k\\gamma_i\\big\\ } + \\big[1+\\frac{k_p^2\\lambda_2}{\\lambda_1}\\big ] \\big(\\max_{k_0\\le i\\le k } \\|a_i\\|_p+\\max_{k_0\\le i\\le k } \\|b_i\\|_p\\big)\\end{aligned}\\ ] ] almost surely , where constant @xmath323 is from lemma [ lemma : eig ] .",
    "take now the @xmath158-th power of both sides of the inequality and apply the hlder inequality @xmath327 for @xmath328 to get @xmath329    recall that the sequence @xmath330 is a martingale with respect to the filtration @xmath331 and that the coordinates of @xmath332 verify @xmath333 almost surely , @xmath334 , @xmath262 . applying the maximal burkholder inequlity for @xmath335 and the davis inequality for @xmath336 ( cf .",
    "@xcite ) yields @xmath337^{p/2 } \\le d b_p 2^p \\bar{g}^p \\bigg[\\sum_{j = k_0}^k \\gamma_j^2 \\bigg]^{p/2},\\end{aligned}\\ ] ] for some constant @xmath338 .",
    "one can take @xmath339 for @xmath335 , cf .",
    "the second inequality of the theorem now follows by taking expectations on both sides of the bound on @xmath315 above and by using the last inequality .",
    "one can derive a similar result for the @xmath340 , by simply taking the @xmath158-th power of the inequality ( [ eq : after_triangle_lp ] ) and then proceeding in the same way as in the proof of theorem [ theo : bound2 ] , with minor modifications in the argument for the martingale @xmath341 .",
    "once a bound on @xmath340 is established , one can use it for proving theorem [ theo : bound2 ] in another way .",
    "namely , since @xmath231 for any @xmath232 and @xmath233 , @xmath342 , with @xmath343 if @xmath235 and @xmath344 if @xmath237 . thus , a bound for @xmath345 will immediately follow from the obtained bound for @xmath340",
    ". the bound will be of the same form as in theorem [ theo : bound2 ] , but with different constants @xmath346 .",
    "[ rem : close_parameter ] consider the following situation , which we will call case i. suppose we are not interested in tracking the , say , _ natural _ parameter @xmath107 of the model , but rather some other time - varying parameter @xmath347 , which is also assumed to be predicable with respect to the filtration @xmath76 .",
    "denote @xmath348 , @xmath20 .",
    "the difference @xmath349 , @xmath160 , can be seen as an approximation error .",
    "similar to ( [ eq : recursed ] ) , the following expansion can be derived for the quantity @xmath350 : @xmath351 \\delta_{k_0}^ *   + \\sum_{i = k_0}^k \\big[\\prod_{j = i+1}^k(\\bm{i}- \\gamma_j \\bm{m}_j)\\big ]   ( \\delta\\theta^*_i + \\gamma_i\\bm{m}_i \\varepsilon_i+\\gamma_i d_i).\\end{aligned}\\ ] ]    now consider case ii : we want to track the natural parameter @xmath107 but the average gain makes an error @xmath177 , i.e. , @xmath352 , @xmath90 .",
    "the error term @xmath177 may be random but must be measurable with respect to @xmath120 . again , similar to ( [ eq : recursed ] ) , we can derive @xmath353 \\delta_{k_0 }   + \\sum_{i = k_0}^k \\big[\\prod_{j = i+1}^k(\\bm{i}- \\gamma_j \\bm{m}_j)\\big ]   ( \\delta\\theta_i + \\gamma_i\\eta_i + \\gamma_i d_i).\\ ] ] now notice that case i can actually be reduced to case ii by putting in the last relation @xmath354 and @xmath355 ( where @xmath356 ) , @xmath309 .",
    "therefore , consider only case ii from now on .    under the conditions of theorem [ theo : bound ] , in the same way as for ( [ eq : bounde ] ) , we can derive the following bound : @xmath357^{1/2}\\\\ & + c_3{{\\mathbb{e}}}\\max_{k_0\\le i\\le k } \\|\\theta_{i+1}-\\theta_{k_0}\\| + c_3 { { \\mathbb{e}}}\\sum_{i = k_0}^k \\gamma_i\\|\\eta_i\\| . \\end{aligned}\\ ] ] similarly , under the conditions of theorem [ theo : bound2 ] , @xmath358^{p/2}\\\\ & + c'_3 { { \\mathbb{e}}}\\big[\\max_{k_0\\le i\\le k } \\|\\theta_{i+1}-\\theta_{k_0}\\|_p   + \\sum_{i = k_0}^k\\gamma_i\\|\\eta_i\\|_p\\big]^p . \\end{aligned}\\ ] ] clearly , ( [ eq : bounde2 ] ) and ( [ eq : boundas2 ] ) generalize the bounds of theorems [ theo : bound ] and [ theo : bound2 ] , where we had @xmath359 , @xmath309 .    in case i , we have @xmath354 and @xmath360 with @xmath349 , @xmath309 , in relations ( [ eq : bounde2 ] ) and ( [ eq : boundas2 ] ) . noting that @xmath361 for all @xmath44 and @xmath309",
    ", we can rewrite bounds ( [ eq : bounde2 ] ) and ( [ eq : boundas2 ] ) in terms of @xmath362 instead of @xmath363 with appropriate adjustments of corresponding constants .",
    "any gain function for which conditions ( a1 ) and ( a2 ) hold may be used with our algorithm , and whether a particular gain function is suitable or not depends on the model under study and the quantity that we wish to track . for certain types of models and quantities to track ,",
    "there are natural choices for the gain function .",
    "many different settings are investigated in the literature . in this section",
    "we consider the construction of appropriate gain functions to be used in the algorithm ( [ eq : algorithm_main ] ) in several traditional settings .",
    "in particular , we relate our general approach to well known classical procedures such as robbins - monro and kiefer - wolfowitz algorithms and outline possible extensions .",
    "the traditional ` signal+noise ' situation can be represented by the following observation model : @xmath364 where @xmath365 , @xmath163 is a predictable process ( @xmath25 ) we are interested in tracking , @xmath366 is a martingale difference noise , with respect to the filtration @xmath76 .",
    "we use the algorithm ( [ eq : algorithm_main ] ) for tracking @xmath107 , and in this case we can simply take the following gain function @xmath367 since @xmath368 = - ( \\hat{\\theta}_k - \\theta_k ) , \\quad k \\in \\mathbb{n}_0,\\ ] ] i.e. , @xmath369 .",
    "clearly , condition ( a1 ) holds and condition ( a2 ) follows as well if we assume @xmath370 , @xmath20 . indeed , according to ( [ g_bounded ] ) , it is enough to show the boundedness of the second moment of @xmath113 : @xmath371 \\le c , \\quad   k \\in \\mathbb{n}_0,\\ ] ] by virtue of the hlder inequality , lemma [ lemma_bound ] and ( [ c_theta ] ) .",
    "the classical nonparametric regression model fits into this framework so that our results can be applied .",
    "for example , the simplest nonparametric regression model with an equidistant design on @xmath372 $ ] is as follows : @xmath373 , @xmath374 , with independent noises @xmath375 s , @xmath376 , @xmath377 ; we will return to this issue in subsection [ sec : variational_setups : lipschitz ] .",
    "[ gain_autoregression ] possibly , @xmath378 = { \\varphi}(\\theta_k)$ ] for some smooth function @xmath379 . in this case",
    ", one should consider @xmath380 , @xmath381 , so that @xmath382 , @xmath160 , which should be comparable to @xmath383 .",
    "autoregressive models , for example , fall into this category ( cf",
    ".  section  [ sec : examples : ard ] ) .",
    "let us turn to more dynamical situations where the observations themselves depend on our tracking sequence . in their seminal paper ,",
    "@xcite studied the problem of finding the unique @xmath384-root @xmath385 of a monotone function @xmath386 , i.e. , the equation @xmath387 has a unique solution at @xmath388 .",
    "the function @xmath386 can be observed at any point @xmath389 but with noise @xmath390 : @xmath391 so that @xmath392 .",
    "a stochastic approximation algorithm of design points converging to @xmath385 is known as classical robbins - monro procedure .",
    "we now illustrate how this also fits into our general tracking algorithm scheme .",
    "in fact , the following model essentially extends the original setup of @xcite .",
    "suppose there is a time series @xmath393 ( with @xmath171 taking values in @xmath394 ) running at the background , which is not ( fully ) observable . instead",
    ", some other @xmath395-dimensional ( related ) time series @xmath396 is observed , which we introduce below . as usual , let @xmath397 , @xmath398 .",
    "further , for a sequence of functions @xmath399 , let a @xmath395-dimensional measurable function @xmath25 be the unique solution of the equation @xmath400 , where @xmath401 $ ] , for some measurable function @xmath402 , @xmath90 .",
    "( here @xmath171 may contain @xmath27 . )",
    "the goal is to track the sequence @xmath403 . at a time moment @xmath90 ,",
    "we observe the noise corrupted value of @xmath404 at some design point @xmath123 ( which can be picked on the basis of the previous observations @xmath27 , i.e. , @xmath119 ) : @xmath405 where @xmath406 is a martingale difference noise sequence with respect to the filtration @xmath106 ( indeed , let simply @xmath407 ) .",
    "of course , we could assume a more general model @xmath408 , @xmath160 , but this would not have made any principal difference , since variable @xmath375 can be incorporated into the vector @xmath167 .",
    "let the design points @xmath409 in ( [ robbins_monro ] ) be determined by the algorithm ( [ eq : algorithm_main ] ) and we want this algorithm to track @xmath107 . theorem [ theo : bound ] is applicable if the gain @xmath113 in ( [ eq : algorithm_main ] ) satisfies ( a1 ) and ( a2 ) .",
    "as in the robbins - monro algorithm , the gain is taken to be @xmath410",
    "then @xmath411 , @xmath160 , and ( a2 ) is fulfilled if , for example , @xmath412 and @xmath413 , @xmath160 .",
    "condition ( a1 ) , or equivalently ( 1 ) , is fulfilled if , for some @xmath414 , @xmath415 almost surely . in the last display",
    ", one should recognizes the usual regularity requirements for the function @xmath386 in the classical robbins - monro setting : @xmath254 , @xmath416 and @xmath417 ( so that @xmath418 is the non - random solution of the equation @xmath419 ) : @xmath420 . in multidimensional case ,",
    "this can be seen as a generalized identifiability requirement for the sequence @xmath181 .",
    "for example , if @xmath421 is a differentiable mapping in @xmath422 for each @xmath423 , then a sufficient condition for ( a1 ) is positive definiteness of the jacobian matrix of @xmath421 ( with respect to @xmath424 ) , uniformly in @xmath423 and over the support of @xmath123 .",
    "one can possibly relax this to a vicinity of the root @xmath107 under other appropriate conditions which guarantee that @xmath123 eventually gets into a neighborhood of @xmath107 .    a particular example is @xmath425 $ ] , where @xmath166 is a subvector of @xmath167 , independent of @xmath120 .",
    "another classical example is the algorithm of @xcite for successive estimating the maximum of a function @xmath386 which can be observed at any point , but gets corrupted with a martingale difference noise ( similarly , one can formulate the problem of tracking minima of a sequence of functions ) .",
    "the algorithm is based on a gradient - like method , the gradient of @xmath386 being approximated by using finite differences .",
    "there are many modifications of the procedure , including multivariate extensions , and they are all based on estimates of the gradient of @xmath386 .",
    "the following scheme essentially contains many such procedures considered in the literature and even extends them to a time - varying predictable maxima process @xmath181 .    as in the previous subsection , suppose there is a time series @xmath393 , with @xmath171 taking values in @xmath394 , running in the background , which is not ( fully ) observable . instead , some other related time series @xmath396 is observed , which we introduce below . let @xmath397 , @xmath398 .",
    "suppose we are given a sequence of measurable functions @xmath426 , @xmath427 , @xmath20 , such that the function @xmath428 $ ] has a unique maximum @xmath25 on @xmath15 , i.e. , @xmath429 again , @xmath171 may contain @xmath27 .",
    "we want to track the sequence @xmath403 . for",
    "that we use the sequence of design points @xmath409 defined by the tracking algorithm ( [ eq : algorithm_main ] ) , with @xmath430-measurable gain functions @xmath113 to be specified later . at a time moment @xmath90",
    ", we observe the so called `` noisy approximate gradients '' at those design points : @xmath431 where @xmath432 and @xmath413 for all @xmath160 , and @xmath406 is a martingale difference noise sequence with respect to the filtration @xmath76 .",
    "the @xmath395-dimensional approximate gradient @xmath433 is not necessarily the exact pathwise @xmath424-derivative of @xmath434 at @xmath123 ( i.e. , @xmath435 ) but such that @xmath436 = \\bar{f}_k(\\hat{\\theta}_k,\\bm{x}_{k-1})=-\\bm{m}_k(\\hat{\\theta}_k-\\theta_k ) + \\eta_k , \\quad k \\in \\mathbb{n}_0,\\ ] ] almost surely , where a symmetric positive definite matrix @xmath126 satisfies conditions ( [ eq : lambda ] ) and @xmath437 is some predictable approximation error .",
    "of course , such a representation ( [ appr_gradient ] ) is always possible : simply take @xmath438 for some symmetric positive definite matrix @xmath246 satisfying ( [ eq : lambda ] ) ; useful ones are those for which the @xmath177 s are under control  basically , the @xmath177 s should be small .    as a choice for the gain , take now @xmath439 , so that @xmath440 = \\bar{f}_k(\\hat{\\theta}_k,\\bm{x}_{k-1})=-\\bm{m}_k(\\hat{\\theta}_k-\\theta_k ) + \\eta_k$ ] , @xmath160 .",
    "clearly , ( a2 ) holds in view of moment conditions on the quantities in ( [ kiefer_wolfowitz ] ) , however ( a1 ) is not satisfied in general since there is an approximation ( possibly nonzero ) term @xmath177 involved .",
    "yet , we are in the position of remark [ rem : close_parameter ] and thus the bound ( [ eq : bounde2 ] ) for the tracking error holds in this case .",
    "this bound is however useful only if the approximation errors @xmath177 s get sufficiently small as @xmath110 gets bigger .",
    "the most desirable situation is when @xmath441 , @xmath160 .    for each particular model of form ( [ kiefer_wolfowitz ] )",
    ", one needs to determine conditions that should be imposed on the approximate gradients @xmath442 s in order to be able to claim a reasonable quality of the tracking algorithm by using our general result .",
    "conditions on approximate gradients @xmath442 s from ( [ kiefer_wolfowitz ] ) which provide control on the magnitude of the approximation errors @xmath177 s are comparable to the ones proposed in many papers .",
    "examples can be found in ; see further references therein .",
    "commonly , a finite difference form of the gradient estimate is used as noisy approximate gradient .",
    "below we outline two settings .",
    "first consider the following situation which is very close to the classical kiefer - wolfowitz setting : @xmath443 for some subvector @xmath166 of @xmath167 , independent of @xmath27 defined below and we wish to maximize the function @xmath444 . for simplicity ,",
    "let @xmath445 and all @xmath166 s are identically distributed ( although the generalization to the time - varying case is straightforward ) so that @xmath446 is to be maximized : @xmath447",
    ". let @xmath448 be a positive sequence , @xmath449 be the standard orthonormal basis vectors in @xmath18 , @xmath450 en @xmath451 have the same distribution as @xmath166 , @xmath452 .",
    "denote @xmath453 , @xmath454 , likewise for @xmath455 and @xmath456 .",
    "the observations are the noisy finite difference estimates of the gradient : @xmath457 here @xmath458 is a martingale difference noise sequence with respect to the filtration @xmath76 , @xmath123 denotes the @xmath110th estimate of the maximum point @xmath385 according to the algorithm ( [ eq : algorithm_main ] ) with the gain @xmath459 . then , under some regularity conditions , @xmath460 where the magnitude of @xmath177 is controlled by @xmath461 . usually @xmath462 as @xmath463 in an appropriate way . to ensure that @xmath464 ( possibly with a small approximation error ) for some positive definite matrix @xmath246 satisfying ( [ eq : lambda ] ) , concavity of @xmath465 is typically required , either global or over a compact set which is known to include the maximum location @xmath385 . for example , if function @xmath465 is sufficiently smooth and strongly concave , then by taylor s expansion @xmath466 , the hessian matrix of @xmath465 at some point @xmath347 between @xmath123 and @xmath385 , the relations ( [ eq : lambda ] ) are fulfilled and the approximation error @xmath177 is small if @xmath461 is small .",
    "another approach ( due to @xcite ) is based on random direction instead of the unit basis vectors .",
    "we use the same notations as in the previous setting with one simplification : assume now that there are no vectors @xmath166 s involved in the model so that @xmath467 .",
    "let @xmath468 denote a sequence of independent ( @xmath279 is also assumed to be independent of @xmath27 ) random unit vectors in @xmath18 . at time moment @xmath160 we observe @xmath469 where the tracking sequence @xmath123 is defined by the algorithm ( [ eq : algorithm_main ] ) with the gain function @xmath470 .",
    "notice that one step in the previous ( classical kiefer - wolfowitz ) observation scheme requires in essence @xmath471 observations in design points @xmath472 , @xmath452 , whereas only two measurements must be made in the case of the above random direction observation scheme .",
    "this property was the main motivation for the random direction method introduced by @xcite .",
    "then , under some regularity conditions , @xmath473 \\\\ & = { { \\mathbb{e}}}\\big [ d_kd_k^t \\big]\\nabla f(\\hat{\\theta}_k)+ \\eta_k = -\\bm{m}_k(\\hat{\\theta}_k-\\theta_k ) + \\eta_k,\\end{aligned}\\ ] ] where @xmath474h(f)(\\theta_k^*)$ ] and again the magnitude of @xmath177 is controlled by @xmath461 . the relations ( [ eq : lambda ] ) hold if , for example , we assume that the random directions were chosen in such a way that @xmath475 $ ] are positive definite matrices and the hessian @xmath476 is negative definite .",
    "a particular choice of function @xmath477 is @xmath478 , @xmath20 , where @xmath479 s is a sequence of observations with values on a measurable space @xmath480 and @xmath481 is a loss function .",
    "then @xmath482 is the prediction risk of the predictor given by @xmath424 .",
    "classical examples are least squares and logistic regression ( cf .  ):",
    "@xmath483 or @xmath484 $ ] , where @xmath485 , @xmath486 and @xmath487 , or @xmath488 for logistic regression .",
    "consider one more example .",
    "suppose @xmath489 and we would like to track the conditional quantile of the distribution of our observed time series @xmath490 , i.e. , @xmath25 such that @xmath491 , where the levels @xmath492 are of our choice and @xmath493 is the conditional distribution function of @xmath8 given the past @xmath27 .",
    "assume that this conditional distribution posses a density @xmath494 .",
    "in this case it makes sense to use @xmath495 in the algorithm ( [ eq : algorithm_main ] ) for tracking @xmath107 , since @xmath496 for some @xmath497 between @xmath123 and @xmath107 . under some mild conditions theorem [ theo : bound ]",
    "is applicable .",
    "note also that the algorithm based on this gain function only requires knowledge of the values of the indicators @xmath498 which means that we may still track the required quantiles without explicitly observing @xmath8 .",
    "this problem is treated in detail for the case of independent observations in .      for certain models",
    "it may not be obvious how gain functions can be constructed , especially when tracking multi - dimensional parameters .",
    "it is therefore important to have a general procedure that can be used to construct candidate gain functions that can either be used directly or , if needed , modified to verify ( a1 ) and ( a2 ) .    in this subsection",
    "we assume that we are in the framework of remark [ rem : parametrized ] , i.e. , we are dealing with a parameterized model @xmath179 .",
    "assume further that for each @xmath156 , each distribution from the family of conditional distributions @xmath91 has a density with respect to some @xmath75-finite dominating measure and denote this conditional density by @xmath499 , @xmath500 .",
    "assume also that there is a common support @xmath0 for these densities , and that for any @xmath501 and @xmath502 , the partial derivatives @xmath503 , @xmath504 , exist and are finite , almost surely .",
    "as before , the `` true '' value of the time - varying parameter at time moment @xmath20 is denoted by @xmath25 . under these assumptions , the _ conditional _ gradient vector @xmath505 and the random matrices @xmath506 , @xmath160 , with entries @xmath507 , \\quad i , j=1,\\dots , d,\\ ] ] can be defined , almost surely . a possible gain function for the algorithm ( [ eq : algorithm_main ] )",
    "is simply the conditional score of the model , i.e. , the gradient vector @xmath508 if @xmath506 is almost surely non - singular in point @xmath123 , then one might also consider @xmath509    we now outline some heuristic arguments why these choices are reasonable .",
    "take @xmath510 .",
    "it is not uncommon for the kullback - leibler divergence @xmath511 to be a quadratic form in the distance between the parameters @xmath385 and @xmath424 , i.e. , equal to a multiple of @xmath512 for some ( eventually random ) positive semi - definite matrix @xmath48 . actually , this is also in general true under some regularity conditions , at least locally , in a vicinity of the `` true '' @xmath385 .",
    "for example , suppose that we can interchange integration and differentiation and that @xmath48 does not depend on @xmath424 , then @xmath513    the score in principle depends on the past of the time series @xmath27 and the previous argument might only be valid for a certain subset of values @xmath27 in @xmath514 .",
    "this dependence could prevent ( a1 ) from holding . in such cases , using a gain of the form ( [ eq : gain_canonical_2 ] ) might be a good alternative since the matrix @xmath515 acts as an appropriate scaling factor .",
    "the dependence of the gain function on the past of the time series is in fact one of the main issues one has to deal with when checking ( a1 ) and ( a2 ) . on one hand , to ensure that the gain function has , on average , the right direction , as required by ( [ eq : g ] ) , the gain will often need to depend on previous observations .",
    "this might , however , affect either the range or the variance of the gain .",
    "gain functions , such as ( [ eq : gain_canonical_1 ] ) and ( [ eq : gain_canonical_2 ] ) , can be modified , or rescaled , to ensure that the respective conditional expectation @xmath516 verifies the assumptions of theorem [ theo : bound ] .",
    "one can for example truncate certain entries or factors in both @xmath138 and @xmath506 to ensure that the resulting @xmath516 meets the required assumptions .",
    "another possibility is to rescale , or directly truncate , the length of a given gain vector and consider , for example , one of the following gains @xmath517,\\\\ { \\bar g}_k(\\vartheta,\\bm{x}_k ) & = g_k(\\vartheta,\\bm{x}_k ) \\frac{\\min\\{s_k(\\bm{x}_{k-1 } ) , \\kappa \\ } } { s_k(\\bm{x}_{k-1})},\\end{aligned}\\ ] ] for @xmath113 an arbitrary gain function , @xmath518 and some functions @xmath519 .",
    "note that @xmath520 , @xmath521 and @xmath522 all preserve the direction of @xmath113 and have norm bounded by respectively @xmath523 , @xmath524 and the norm of @xmath113 , almost surely .",
    "the gain @xmath522 is a rescaling of @xmath113 for situations when the corresponding conditional gain @xmath182 is of the form @xmath525 , where @xmath246 has eigenvalues as prescribed by ( a1 ) .",
    "consequently the conditional rescaled gain is @xmath526 so that the largest eigenvalue of the matrix @xmath527 is almost surely upper bounded . as to the lower bound , in certain situations it will be possible to show that @xmath528\\ge c\\lambda_1 $ ] almost surely , for some @xmath529 and sufficiently large @xmath524 , by using the fact that @xmath530\\ge\\lambda_1 $ ] almost surely",
    ". this would establish condition ( a1 ) for the rescaled gain @xmath522 . since @xmath531 for all @xmath532 , then @xmath533\\notag\\\\ & \\quad\\quad\\quad   \\label{eq : scaled_gain } = { { \\mathbb{e}}}\\big [ \\big(\\frac{\\min\\big ( s(\\bm{x}_{k-1 } ) , \\kappa \\big)}{s(\\bm{x}_{k-1})}\\big)^2   { { \\mathbb{e}}}\\big[\\|g_k - g_k\\|^2\\big|\\bm{x}_{k-1 } \\big ] \\big ] \\le { { \\mathbb{e}}}\\|g_k - g_k\\|^2.\\end{aligned}\\ ] ] thus , if @xmath113 verifies ( a2 ) , then so does @xmath522 .",
    "another possible modification one might consider is to truncate the iterates of the our algorithm ( [ eq : algorithm_main ] ) .",
    "this might be motivated by practical considerations in the case where the parameter being tracked has some physical meaning and is bounded for that reason .",
    "the algorithm should be restricted as well .",
    "we can then consider an algorithm of the form @xmath534 where @xmath535 acts as a projection on a convex compact set @xmath536 : @xmath535 is an identity on @xmath537 and maps any point from @xmath538 to the closest point in @xmath537 .",
    "we provide concrete examples of gain functions later in section  [ sec : examples ] . in section  [ sec : variational_setups ] , we present some examples of different types of parameter variation such that our algorithm is capable of adequately tracking the time - varying parameter .",
    "it is clear  and in fact explicit in ( [ eq : bounde ] ) and ( [ eq : boundas ] )  that the changes in the parameter have a non - negligible contribution to the accuracy of our tracking algorithm .",
    "this is reasonable since , if the parameter changes arbitrarily in - between observations , we should not expect it to the  trackable \" .",
    "we should then specify how the parameter is allowed to vary and , based on that assumption , pick an appropriate sequence @xmath207 which minimizes the general bounds in ( [ eq : bounde ] ) or ( [ eq : boundas ] ) . in this section",
    ", we specify different settings for the variation of the parameter to be tracked .",
    "these settings refer only to how the parameter is assumed to change and are unrelated to the actual model in question ; examples of specific models can be found in section  [ sec : examples ] .    to avoid overloaded notations , we use letters @xmath205 and @xmath539 for constants whose values are not important to us and which can be different in different expressions .",
    "we assume in this section that @xmath540 , @xmath541 , almost surely , for some unknown @xmath542 so that @xmath543 ( zero vector ) for all @xmath541 , almost surely , and we are actually in a parametric setup . in this case the second terms in both ( [ eq : bounde ] ) and ( [ eq : boundas ] ) obviously vanish .",
    "take then @xmath544 and for @xmath545 , @xmath546 $ ] , where @xmath547 $ ] is the whole part of @xmath548 .",
    "let @xmath549 such that @xmath550 .",
    "for any @xmath551 there is a large enough @xmath552 such that , for all @xmath553 , @xmath554 = c_\\gamma \\log n_0 \\log\\big[\\frac{n+1}{n_0}\\big ] \\ge c \\log n. \\ ] ] moreover , it is easy to see that , under the conditions of theorem [ theo : bound2 ] , @xmath555 .",
    "thus , in both ( [ eq : bounde ] ) and ( [ eq : boundas ] ) the first term can be upper bounded by @xmath556 for any @xmath551 by taking sufficiently large @xmath552 .",
    "next note that @xmath557^{p/2}\\le c ( n^{-1/2 } \\log n)^p$ ] , we conclude that , for a sufficiently large @xmath552 , we can rewrite ( [ eq : bounde ] ) and ( [ eq : boundas ] ) as respectively , @xmath558^p \\le c , \\quad p\\ge1.\\ ] ] if we let @xmath559 , this is almost ( up to a log factor ) parametric convergence rate , the @xmath560-factor in the rate can not be avoided and is in some sense a price for the recursiveness of the algorithm .    if we are in the situation of theorem [ theo : bound2 ] , then by taking @xmath561 ( where @xmath562 is some small fixed number ) and by using markov s inequality and the second bound in the previous display , we derive that @xmath563 in view of the borel - cantelli lemma , it follows that @xmath564 as @xmath565 with probability 1 at a rate @xmath566",
    ".    the particular setup presented in this section , where the parameter is fixed , might seem out of place since we are mainly concerned with tracking time - changing parameters .",
    "we would like to point out that recursive algorithms in parametric situation can also be useful ; for example , the classical robbins - monro and kiefer - wolfowitz algorithms deal with the parametric case .",
    "recursive procedures often produce estimates in a fast , straightforward fashion .",
    "this is an advantage especially over  offline \" estimators obtained , say , as solutions to a certain system , which require iterative likelihood or least squares optimization or are obtained via other indirect methods , a situation which is common when dealing with markov models ( cf . section  [ sec : examples : ard ] . )",
    "suppose now that the parameter we want to track is stabilizing .",
    "this situation might arise if the expectation of the sequence of values that the parameter takes is converging to some limiting value .",
    "it could also be the case that the data is being sampled with increasing frequency from an underlying , continuous time process which depends on a parameter varying continuously ; in this case , the parameter varies less because it has less time to change .",
    "regardless , we assume that @xmath567 verifies @xmath568 for @xmath569 and some positive sequence @xmath570 .",
    "assume that @xmath571 for some @xmath572 and @xmath573 .",
    "consider first the case @xmath574 . in this case",
    ", the variation of the parameter vanishes so quickly that we are essentially in the setup of the previous section , i.e. , as if the parameter is constant .",
    "indeed , take @xmath575 and @xmath576 as in the previous section .",
    "the first and second terms in both ( [ eq : bounde ] ) and ( [ eq : boundas ] ) can be bounded in the same way as in the previous section . using the relations between norms from remark [ rem : norms ] , we upper",
    "bound the third term in ( [ eq : bounde ] ) by a multiple of @xmath577 using the hlder inequality , we upper bound the third term in ( [ eq : boundas ] ) by a multiple of @xmath578^p \\le c n^{-(\\beta-1)p } \\le c n^{-p/2}.\\end{aligned}\\ ] ] clearly , in both ( [ eq : bounde ] ) and ( [ eq : boundas ] ) the third term is of a smaller order than the second term .",
    "thus , the relations ( [ constant_parameter ] ) remain valid for the case @xmath574 .",
    "consider now the case @xmath579 .",
    "let @xmath580 , @xmath581 . by using the elementary inequality @xmath582 for @xmath583 and @xmath584",
    ", we obtain that for any @xmath551 there is a sufficiently large constant @xmath585 such that @xmath586\\\\   & \\ge\\frac{c_\\gamma ( \\log n_0)^{1/3}}{1 - 2\\beta/3 } \\big[n^{1 - 2\\beta/3 } -n^{1 - 2\\beta/3 } \\big(1-n^{2\\beta/3 - 1 } ( \\log n)^{2/3}(1 - 2\\beta/3)\\big)\\big]\\\\ & = c_\\gamma ( \\log n_0)^{1/3 } ( \\log n)^{2/3 } \\ge c \\log n \\ ] ] for sufficiently large @xmath587 , i.e. , @xmath588 .",
    "this yields the same upper bound for the first term in ( [ eq : bounde ] ) and ( [ eq : boundas ] ) as for the static parameter , namely , @xmath556 for any @xmath551 by taking sufficiently large @xmath552 .",
    "let us bound now the second term in ( [ eq : bounde ] ) and ( [ eq : boundas ] ) : @xmath589 for @xmath590 . for sufficiently large @xmath587 ( i.e. , @xmath591 ) the third terms in ( [ eq : bounde ] ) and ( [ eq : boundas ] ) are bounded similarly to ( [ eq : bound00 ] ) and ( [ eq : bound01 ] ) by , respectively , @xmath592 and @xmath593 finally we obtain that for @xmath594 and sufficiently large constant @xmath552 in the algorithm step @xmath580 , ( [ eq : bounde ] ) and ( [ eq : boundas ] ) can be rewritten as respectively @xmath595^p \\le c,\\ ] ] where @xmath596 is the burn - in period of the algorithm .    if we choose @xmath597 and @xmath598 , @xmath599 , @xmath600 , @xmath601 in case @xmath594 , then we get the following bound of the convergence rate : for sufficiently large @xmath587 and sufficiently large constant @xmath552 @xmath602 thus , the choices @xmath603 , @xmath604 , @xmath605 are optimal in the sense of the minimum of the right - hand side of the above inequality .",
    "much in the same way as for ( [ eq : almost_sure ] ) , we can establish that for any @xmath562 , @xmath606 with probability 1 .",
    "finally , consider the case @xmath607 , i.e. , we assume the following weak requirement : @xmath608 , @xmath309 , for some uniform constant @xmath539 .",
    "take @xmath609 , @xmath610 for some @xmath611 , @xmath612 .",
    "then theorem [ theo : bound ] implies that @xmath613 we thus have that the algorithm will track down the parameter in the proximity of size @xmath614 , which we can try to minimize by choosing appropriate constants @xmath615 and @xmath616 .",
    "we consider now a different setup where we assume that the parameter is changing , on average , like a lipschitz function . in this setup",
    "we let the time series ( [ model ] ) be sampled from a continuous time process @xmath617 , @xmath618 $ ] , which we observe with frequency @xmath587 .",
    "this means that we deal with a triangular sequence of models , i.e. , for each @xmath619 we have a different model , namely , @xmath620 where the parameter @xmath621 verifies , for some @xmath44 , @xmath622 , @xmath623 , @xmath624 assume for example that @xmath625 , where @xmath626\\}$ ] for some @xmath623 and @xmath187 , a space of vector valued lipschitz functions .",
    "let @xmath627 ( constant in @xmath110 ) for @xmath628 , and @xmath629 for @xmath630 .",
    "note that for @xmath631 as @xmath632 for any @xmath623 .",
    "we have @xmath633 so that once again the first term in ( [ eq : bounde ] ) and ( [ eq : boundas ] ) can be upper bounded by @xmath556 for any @xmath551 by taking sufficiently large @xmath552 . as to the second term , we evaluate @xmath634 from our assumption on the variation of the parameter , we have @xmath635^p.\\end{aligned}\\ ] ] combining these three bounds , we get that ( [ eq : bounde ] ) ( we also need the relations between norms from remark [ rem : norms ] ) and ( [ eq : boundas ] ) imply @xmath636^p.\\end{aligned}\\ ] ]    if we consider step sizes of the form @xmath637 , the above proposed choices of @xmath638 and @xmath639 are optimal in the sense of tracking error minimum .",
    "note that the obtained convergence rate ( the asymptotic regime : the observation frequency @xmath559 ) coincides , up to a log factor , with the minimax rate of convergence in the problem of estimating nonparametric regression function over lipschitz functional class @xmath640 .",
    "in this section we present some examples of particular models to which our algorithm may be applied .",
    "we start with two toy examples and present thereafter some more involved examples .",
    "the toy examples illustrate the type of results that can be obtained from our main result and its extensions , how a gain function can be picked and modified , and how conditions ( a1 ) and ( a2 ) are checked .",
    "suppose we are monitoring @xmath619 independent poisson processes on @xmath372 $ ] with unknown intensity function @xmath641 .",
    "this is equivalent to observing @xmath642 , a poisson process with intensity @xmath643 , @xmath644 .",
    "we would like to track the intensity function @xmath641 which is assumed to be upper bounded by @xmath645 .",
    "assume that we observe the process with frequency @xmath587 , in that our observations are @xmath646 , so that for each @xmath619 we have a markov model @xmath647 where @xmath648 represents a poisson law with parameter @xmath649 .",
    "from now on , we will skip the dependence on @xmath587 for notational simplicity : write @xmath8 instead of @xmath650 , @xmath107 instead of @xmath651 etc . introduce the conditional , shifted poisson mass function given by @xmath652 the moving parameter is given by @xmath653 , @xmath654 , which is the average of function @xmath655 over the interval @xmath656 $ ] .",
    "assume that @xmath655 is continuous , then @xmath657 for @xmath587 large enough .",
    "consider now the gain function @xmath113 of the type ( [ eq : gain_canonical_2 ] ) for the algorithm ( [ eq : algorithm_main ] ) so that @xmath658 = - ( \\hat{\\theta}_k-\\theta_k ) , \\notag\\end{aligned}\\ ] ] it follows that @xmath659 since @xmath660}\\lambda(t)\\le l$ ] .",
    "we thus conclude that the gain function ( [ eq : gain_poisson ] ) satisfies both ( a1 ) and ( a2 )",
    ".    this gain function can now be used for the three setups outlined in section  [ sec : variational_setups ] and we can attain the rates indicated there . for a constant intensity function @xmath661 , @xmath662 , the algorithm will simply estimate the parameter of the underlying homogeneous poisson process @xmath385 , because we matched the sampling frequency @xmath663 with the sample size @xmath587 .",
    "if we had sampled the process with frequency , say , @xmath664 , then @xmath665 and the algorithm would track @xmath666 and not @xmath385 .",
    "the tracking sequence would then have to be rescaled by a factor @xmath667 to obtain a tracking sequence for @xmath385 itself .    in the setup where we assume that the parameter is stabilizing ,",
    "take @xmath668 so that @xmath669 is the mean number of events per time unit @xmath656 $ ] .",
    "note that @xmath670 and the average number of events per time unit will stabilize in time if , for example , @xmath671 as @xmath672 .",
    "the algorithm will then track the mean number of events per time unit .",
    "we can also assume that the intensity function @xmath641 belongs to @xmath673 for some @xmath623 and @xmath187 .",
    "let @xmath674 , @xmath675 .",
    "it follows that @xmath676 the tracking sequence based on the gain ( [ eq : gain_poisson ] ) will then track the sequence @xmath677 , @xmath675 ( as well as @xmath107 ) with the asymptotics seen in section [ sec : variational_setups ] ( cf .",
    "remark  [ rem : close_parameter ] ) .",
    "assume that we observe , with fixed frequency @xmath619 , a process @xmath678 , @xmath679 , taking values on @xmath680 , @xmath681 .",
    "the observations available up to time moment @xmath682 is a random vector @xmath683 , with @xmath684 .",
    "we again skip the dependence on @xmath587 , although all the quantities below do depend on @xmath587 .",
    "the increments @xmath685 are assumed to be conditionally gaussian in the sense that given the past of the process , each increment has a multivariate normal distribution : @xmath686 the dependence on the past in the model comes from the fact that both the mean and the covariance processes of the above conditional distributions are predictable , i.e. , @xmath25 and @xmath687 , @xmath20 , with respect to the filtration @xmath106 .",
    "if the covariance structure of the process is known , we can use the gain ( [ eq : gain_canonical_1 ] ) which verifies @xmath688 for this gain , we assume that almost surely @xmath689 for some positive @xmath690 . we then obtain that @xmath691 and assumptions ( a1 ) and ( a2 ) are thus met for the gain from ( [ eq : gain_gaussian1 ] ) .",
    "now suppose that the covariance matrix of the process is unknown or difficult to invert .",
    "then we can use the gain ( [ eq : gain_canonical_2 ] ) , so that @xmath692 clearly , assumptions ( a1 ) and ( a2 ) are again met for the gain from ( [ eq : gain_gaussian2 ] ) if @xmath693 for some @xmath203 , @xmath160 , almost surely .",
    "the results of section  [ sec : variational_setups ] can be applied to the algorithm based on the gain functions presented above for all three considered asymptotic regimes : constant parameter process , stabilizing ( on average ) process and lipschitz on average .",
    "although designed for different frameworks , it is interesting to compare the above resulting tracking algorithm with the famous _ kalman filter_. for simplicity , consider the one dimensional situation .",
    "suppose we observe @xmath694 where the parameter of interest @xmath107 , evolves according to @xmath695 with @xmath696 .",
    "at each step , the initial state and the noises @xmath697 are assumed to be mutually independent .",
    "one can show ( by combining both prediction and update steps ) that the kalman filter in this case reduces to @xmath698 we also derive the exact expression for the mean squared error of the algorithm : @xmath699    coming back to our framework , suppose we have observations ( [ model_kalman ] ) with predictable process @xmath403 such that @xmath700 , @xmath156 ; cf .",
    "section [ sec : variational_setups : stabilizing ] .",
    "then the kalman filter ( [ kalman_algorithm ] ) coincides with our tracking algorithm with the gain ( [ eq : gain_gaussian1 ] ) and a particular choice of the step sequence @xmath207 given by ( [ kalman_step ] ) .",
    "one should keep in mind that the two frameworks are different , but it would still be interesting to compare the convergence rates for some particular settings for stabilizing the parameter @xmath107 .",
    "for example , one can consider @xmath701 , @xmath702 , as in section [ sec : variational_setups : stabilizing ] .",
    "the above kalman filter setting has more structure and we expect therefore that the rate in this case ( which is of order @xmath703 , with @xmath704 defined by ( [ kalman_step ] ) ) should be faster than the rate @xmath705 obtained in section [ sec : variational_setups : stabilizing ] for our general framework .",
    "we were however unable to solve the recursive rational difference equation ( [ kalman_step ] ) for @xmath701 .",
    "note that the trivial case @xmath706 leads to the situation of a constant parameter @xmath707 and the sample mean @xmath708 as an estimator for that parameter .",
    "consider the following arch(@xmath523 ) model with drifting parameter @xmath709 where @xmath710 almost surely , @xmath711 is predictable and @xmath712 is a martingale difference noise with respect to the filtration @xmath713 , @xmath714=\\sigma_k^2 $ ] for some known @xmath715 , @xmath7 . without loss of generality",
    "assume @xmath716 .",
    "assume further that @xmath717 and @xmath718 \\le \\rho$ ] , @xmath156 , for some @xmath719 .",
    "consider the gain function @xmath720 for some truncating constant @xmath721 . since @xmath722 and @xmath723 = 1+\\theta_k x_{k-1}^2 $ ] , @xmath724   = - \\min(x_{k-1}^2 , t)(\\vartheta-\\theta_k).\\ ] ] we have that @xmath725 almost surely .",
    "besides , @xmath726 & = { { \\mathbb{e}}}\\big[\\min\\{(1+\\theta_{k-1}x_{k-2}^2)\\epsilon_{k-1}^2 , t\\ } \\big|\\bm{x}_{k-2}\\big ] \\\\ & \\ge{{\\mathbb{e}}}\\big[\\min(\\epsilon_{k-1}^2 , t)|\\bm{x}_{k-2}\\big].\\end{aligned}\\ ] ] using the hlder inequality and the facts that @xmath727 and @xmath728 , it is straightforward to check that @xmath729   & = { { \\mathbb{e}}}\\big [ t+\\epsilon_{k-1}^2 - |\\epsilon_{k-1}^2-t| \\big|\\bm{x}_{k-2}\\big ]   \\\\ & \\ge   t+1- \\big({{\\mathbb{e}}}[(\\epsilon_{k-1}^2-t)^2|\\bm{x}_{k-2}]\\big)^{1/2 } \\\\ & \\ge t+1 + 2 t - \\big({{\\mathbb{e}}}[\\epsilon_{k-1}^4 |\\bm{x}_{k-2}]+t^2\\big)^{1/2 } \\\\ & \\ge1 + 2 t - \\big({{\\mathbb{e}}}[\\epsilon_{k-1}^4 |\\bm{x}_{k-2}]\\big)^{1/2 }   \\ge 1,\\end{aligned}\\ ] ] as long as @xmath730   /4 $ ] , @xmath156 . for example , we can take @xmath731 .",
    "we conclude that ( a1 ) holds for the gain ( [ eq : gain_arch_1 ] ) .    to ensure ( a2 ) ,",
    "we evaluate @xmath732 \\\\ & \\le 3{{\\mathbb{e}}}\\big[\\big(\\frac{\\min(x_{k-1}^2 , t)}{x_{k-1}^2}\\big)^2 \\big ( ( 2 + 2\\theta_k^2x_{k-1}^4 ) \\epsilon_k^4 + 1+\\theta_k^2 x_{k-1}^4\\big ) \\big ] \\\\ & \\le   9 +   3 { { \\mathbb{e}}}\\big[\\big(\\frac{\\min(x_{k-1}^2 , t)}{x_{k-1}^2}\\big)^2   ( 2c_\\theta \\rho + c_\\theta ) x_{k-1}^4\\big ] \\\\ & \\le 9 + 3c_\\theta t^2(2\\rho+1 ) . \\ ] ]      in this section we use the notation @xmath733 for the vector of the @xmath395 consecutive observations ending with @xmath8 .    consider an autoregressive model with @xmath395 time varying parameters : @xmath734 where @xmath735 is @xmath736-measurable , @xmath737 is a martingale difference noise with respect to the filtration @xmath713 such that @xmath738 , @xmath156 , starting random vector @xmath739 is given and such that @xmath740 , for some @xmath741 .",
    "for @xmath742 , associate with the ar(d ) model its polynomial @xmath743 it is well know that an ar(d ) model with autoregressive parameters @xmath424 is stationary if , and only if , the ( complex ) zeros of the polynomial @xmath744 are outside the unit circle .",
    "this motivates the definition of the parameter sets @xmath745 for some @xmath746 : @xmath747 cf .",
    "@xcite who also showed that the following embeddings hold : @xmath748 where @xmath749 is a uniform ball around zero in @xmath18 with radius @xmath750 .",
    "this gives some feeling about the size of the parameter set @xmath745 and implies in particular that the set @xmath745 is non - empty and bounded for all @xmath751 .",
    "the ar(d ) model can also be described by the following inhomogeneous difference equation @xmath752 where @xmath753 and , for any @xmath754 , @xmath755 is the square matrix of order @xmath395 @xmath756 this matrix is usually called the _ companion matrix _ to the autoregressive polynomial @xmath757 ; it is also sometimes called the _ state transition matrix_. one can show that the eigenvalues of @xmath755 are exactly the reciprocals of the zeros of @xmath744 .",
    "this means that the absolute values of the eigenvalues of @xmath755 for @xmath758 are all at most @xmath759 .",
    "this in turn implies that for any sequence of vectors @xmath760 , the pair of sequences @xmath761 forms a so called _ exponentially stable _ pair ( cf .",
    "@xcite ) . among other things ,",
    "this gives us that so long as the @xmath158-th moments of both the initial @xmath739 and the noise terms @xmath375 are bounded , then the @xmath158-th moments of all @xmath8 , @xmath7 , will be bounded as well ( cf .  proposition 10 of @xcite ) .",
    "in @xcite the model ( [ eq : ar_p_model ] ) is considered with nonrandom but time varying @xmath762 for some smooth function @xmath763 , @xmath764 $ ] .",
    "one has a triangular array of models and the studied asymptotics is in sampling frequency @xmath559 ; cf .",
    "section [ sec : variational_setups : lipschitz ] .",
    "the considered gain function is an appropriately rescaled version of the gain from remark [ gain_autoregression ] , namely , @xmath765 for an appropriately chosen @xmath766 depending on the observation frequency @xmath587 .",
    "one should mind the difference in indexing in our algorithm ( [ eq : algorithm_main ] ) and algorithm ( 3 ) from @xcite .",
    "this is not an issue since we can make the correspondence between the algorithms exact by treating @xmath767 as an estimate of @xmath107 rather than of @xmath768 , the error can be absorbed into the third term of the right hand side of ( [ eq : bounde ] ) .",
    "although assumption ( a2 ) is trivially satisfied , our general theorem [ theo : bound ] can not be applied for @xmath769 because assumption ( a1 ) does not hold .",
    "indeed , @xmath770 and the matrix @xmath246 is of the form @xmath771 for some @xmath772 and column vectors @xmath39 .",
    "but the matrix @xmath773 has @xmath774 zero eigenvalues and one eigenvalue @xmath775 , so that always @xmath776 and thus ( [ eq : lambda ] ) does not hold .    [ rem_mouilines ] on the other hand , the authors of @xcite do manage to establish a convergence results for the gain function ( [ gain_moulines ] ) .",
    "it is instructive to understand where the difference in the two approaches is .",
    "careful inspection of the proofs in @xcite reveals that the analogue of the lower bound ( [ eq : lambda ] ) , the persistence of excitation condition , is established in lemma 17 ( p.  2627",
    "of @xcite ) .",
    "the basic difference is that the quantity to bound from below in our case is the conditional expectation of the smallest eigenvalue of the matrix @xmath246 , whereas in @xcite ) it is the smallest eigenvalue of the conditional expectation of the matrix @xmath246 . for the gain function ( [ gain_moulines ] ) ,",
    "the lower bound for the former is zero ( as is demonstrated above ) and it is positive for the latter ( cf .",
    "lemma 17 of @xcite ) . a way to fix",
    "this would be to establish a version of the general theorem , where ( [ different_bound ] ) is assumed instead of ( [ eq : lambda ] ) , see also remark [ rem_version_theorem ] .",
    "we do not consider this here .",
    "consider the case @xmath254 and gain ( [ gain_moulines ] ) for which theorem [ theo : bound ] can be applied .",
    "assume that @xmath777 is bounded , @xmath778 and @xmath779 , @xmath156 .",
    "we have @xmath780 where @xmath781 . besides , if @xmath782 , then @xmath783 , and if @xmath784 , then @xmath785 condition ( a1 ) is fulfilled . as to condition ( a2 ) ,",
    "@xmath786 \\le   \\sigma^2 \\max_{u>0 } \\big\\{\\frac{u}{(1+\\mu u)^2}\\big\\ } = \\frac{\\sigma^2}{4\\mu}.\\end{aligned}\\ ] ] both ( a1 ) and ( a2 ) are thus satisfied .",
    "interestingly , there is no issue of stability in this case : we do not have to assume that @xmath787 , @xmath90 , almost surely .",
    "just almost sure boundedness @xmath788 , @xmath90 , for a constant @xmath229 , is sufficient .",
    "consider now another gain function for the case @xmath254 .",
    "this time we assume that @xmath789=\\mathbb{e } [ \\xi_k^3|\\bm{x}_{k-1}]=0 $ ] , @xmath790=\\sigma^2>0 $ ] , and @xmath791 = c \\sigma^4 $ ] , @xmath156 , for some constant @xmath792 .",
    "the proposed gain and the corresponding average gain are as follows : @xmath793 with some @xmath794 .",
    "note that this is a rescaled gain function of type @xmath795 from section  [ sec : gains ] .",
    "clearly , @xmath796 and , according to lemma  [ lemma : truncated_conditional ] , @xmath797 = \\mathbb{e}\\big[\\min\\{x_{k-1}^2,t\\}|\\bm{x}_{k-2}\\big ] \\ge \\frac{(5-c)\\sigma^2}{4},\\ ] ] so that ( a1 ) holds . assumption ( a2 ) also holds since @xmath798   \\le \\max\\{t^2,1\\ } \\sigma^2.\\end{aligned}\\ ] ]    finally consider a version of general ar(d ) model .",
    "we will only outline the main steps , leaving out the details .",
    "assume that the noise terms @xmath375 in ( [ eq : ar_p_model ] ) form a gaussian white noise sequence with mean zero and variance @xmath799 and that the parameter process @xmath800 is constant within the batch of @xmath395 consecutive observations .",
    "for a @xmath801-dimensional vector @xmath802 , introduce the toeplitz matrix @xmath803 associated with that vector whose entries are @xmath804 , @xmath805 , so that this matrix has constant ( from left to right ) diagonals .",
    "thus , @xmath806 is the column vector formed by starting at the top right element of @xmath807 , going backwards along the top row of @xmath807 and then down the left column of @xmath807 .",
    "denote @xmath808 and introduce @xmath809 and @xmath810 , the toeplitz matrices created from the vectors @xmath811 and @xmath812 respectively . under the imposed assumptions , we can rewrite the model ( [ eq : ar_p_model ] ) as follows : @xmath813 the matrix @xmath814 is upper triangular with a diagonal consisting of ones , whence invertible . from this point on ,",
    "we regard vector @xmath815 , @xmath156 , as an observation at time moment @xmath110 so that we can specify our observation model in terms of conditional distribution of @xmath816 given @xmath817 : @xmath818 where @xmath711 is a predictable process with respect to the filtration @xmath819 .",
    "notice that the observation process is of a markov structure .    even if the normality of the noise is assumed in the model ( [ eq : ar_p_model ] ) , the models ( [ eq : ar_p_model ] ) and ( [ eq : ar_d_kernel ] ) still differ since in general the parameter process @xmath711 varies also within the batches of @xmath395 observations in the model ( [ eq : ar_p_model ] ) . however , this is not an issue . indeed , even though the parameter is allowed to vary within each batch of @xmath395 observation , we still can use the gain function ( which we derive below ) as if the parameter process is constant within the batches and establish an upper bound of type ( [ theo : bound ] ) for the quality of such a procedure .",
    "the error that is made by pretending that the parameter is constant within the batches can be absorbed into the third term of the right hand side of ( [ theo : bound ] ) .    in this case",
    "we propose a gain of the type ( [ eq : gain_canonical_1 ] ) : @xmath820 where @xmath821 is the conditional density of ( [ eq : ar_d_kernel ] ) .",
    "thus , the tracking sequence is updated with batches of @xmath395 observations from the autoregressive process .",
    "below , to ease the notation , we will often write @xmath150 and @xmath822 instead of @xmath815 and @xmath823 , respectively . as explained in section  [ sec : gains ] , the corresponding average gain @xmath824 can be found as minus the gradient of the kullback - leibler divergence between the two conditional distributions with two different parameters .",
    "this observation is particularly useful if we are able to write this kullback - leibler divergence as an appropriate quadratic form .",
    "the kullback - leibler divergence between two @xmath395-dimensional multivariate normal distributions @xmath825 and @xmath826 is given by @xmath827    let @xmath828 , i.e. , @xmath829 ( not to be confused with the vectors @xmath107 , @xmath20 ) and @xmath830 . according to ( [ eq : ar_d_kernel ] ) , @xmath831 and @xmath832 .",
    "now we compute @xmath833 .",
    "let @xmath834 be the toeplitz matrix associated with the vector @xmath835 where @xmath523 is in the @xmath836-th position .",
    "matrix @xmath837 has ones above the main diagonal and zeros elsewhere and it is sometimes called _ upper shift matrix_. for @xmath838 , the powers @xmath839 are the toeplitz matrices associated with the vectors @xmath840 where @xmath523 occupies the @xmath841-th position , @xmath842 , the zero matrix of order @xmath395 , and @xmath843 should be read as @xmath52 , the identity matrix of order @xmath395 .",
    "it follows that @xmath844 , so that @xmath845 and @xmath846    for all @xmath754 , the matrices @xmath847 have all eigenvalues equal to one ( so do their inverses ) , hence @xmath848 and we conclude that the logarithm in ( [ eq : kl_normals ] ) is zero . also , using basic properties properties of the trace and the representation for @xmath849 derived above , @xmath850- d   = \\operatorname{tr}\\big[\\big(\\bm{a}^{-1}(\\vartheta)\\bm{a}^{-t}(\\vartheta)\\big)^{-1}\\big(\\bm{a}^{-1}(\\theta)\\bm{a}^{-t}(\\theta)\\big ) \\big]- d\\\\ & = \\operatorname{tr}\\big[\\bm{a}^t(\\vartheta)\\bm{a}(\\vartheta)\\bm{a}^{-1}(\\theta)\\bm{a}^{-t}(\\theta)\\big]- d   = \\operatorname{tr}\\big[\\big(\\bm{a}(\\vartheta)\\bm{a}^{-1}(\\theta)\\big)^t\\bm{a}(\\vartheta)\\bm{a}^{-1}(\\theta)\\big]-d\\\\ & = 2\\sum_{i=1}^d \\operatorname{tr}\\big[\\bm{s}^i\\bm{a}^{-1}(\\theta)\\big](\\theta_i-\\vartheta_i)+ \\sum_{i=1}^d\\sum_{j=1}^d   \\operatorname{tr}\\big[\\bm{a}^{-t}(\\theta)(\\bm{s}^i)^t\\bm{s}^j\\bm{a}^{-1}(\\theta ) \\big ] ( \\theta_i-\\vartheta_i)(\\theta_j-\\vartheta_j).\\end{aligned}\\ ] ] since the inverse of an upper - triangular matrix is upper - triangular , @xmath851=0 $ ] , for all @xmath504 and @xmath852 . for any @xmath853-matrix @xmath48 , denote by @xmath854 the column vector containing the @xmath855 entries of @xmath48 in any ( fixed ) order .",
    "let @xmath856 , @xmath857 , and note that @xmath858 is always a zero vector .",
    "note that @xmath859 = v_i^t(\\theta)v_j(\\theta)$ ] , @xmath860 .",
    "we conclude that the previous display can be written as @xmath861- d = ( \\vartheta-\\theta)^t\\big[v_1(\\theta ) v_2(\\theta ) \\ldots v_d(\\theta)\\big]^t \\big[v_1(\\theta ) v_2(\\theta ) \\ldots v_d(\\theta)\\big ] ( \\vartheta-\\theta),\\ ] ] where the matrices on the right are comprised by columns .",
    "consider now the quadratic form in the kullback - leibler divergence ( [ eq : kl_normals ] ) .",
    "for any @xmath862 , @xmath863 since @xmath864 , we have @xmath865 which , together with the representation for @xmath866 derived above , imply @xmath867 where @xmath868 , @xmath452 ; notice also that @xmath869 . then @xmath870(\\vartheta-\\theta).\\ ] ]    summarizing , we obtained that @xmath871 with @xmath872^t \\big[v_1(\\theta ) v_2(\\theta ) \\ldots v_d(\\theta)\\big ] \\notag\\\\ \\label{matrix_m } & \\quad + \\sigma^{-2 } \\big[\\bm{c}_1(\\theta)y \\ldots",
    "\\bm{c}_d(\\theta)y \\big]^t \\big[\\bm{c}_1(\\theta)y \\ldots \\bm{c}_d(\\theta)y\\big].\\end{aligned}\\ ] ]    according to ( [ kl - gain ] ) , we can derive the expression for the average gain : @xmath873 where @xmath48 is given by ( [ matrix_m ] ) .",
    "note that the matrix @xmath48 does not depend on @xmath424 and is clearly positive semidefinite .",
    "we evaluate now its eigenvalues . in the representation ( [ matrix_m ] ) ,",
    "the first matrix in the sum is positive semi - definite but has at least one zero eigenvalue .",
    "it is also clear that the entries of this matrix are polynomials of the coordinates of @xmath385 , so that , if @xmath22 for a bounded set @xmath15 , then the largest eigenvalue of this matrix is upper bounded , uniformly over @xmath15 , by some constant , say , @xmath874 . as to the second (",
    "also positive semidefinite ) matrix in the sum of matrices from ( [ matrix_m ] ) , note that @xmath875 the entries of the matrices @xmath876 , @xmath504 , are polynomials in @xmath877 which are bounded uniformly over a bounded set @xmath15 .",
    "recall also that the trace of a matrix is equal to the sum of its eigenvalues .",
    "we conclude that @xmath878 uniformly in @xmath879 for any bounded @xmath880 .    to derive a lower bound on the smallest eigenvalue of the matrix @xmath881 , note that this matrix can be rewritten in the form @xmath882 + \\left [ \\begin{array}{ccc|c } c_{1,1}(\\theta ) &    \\cdots   &    c_{1,d-1}(\\theta )    &    c_{1,d}(\\theta)\\\\ \\vdots           &    \\ddots   &    \\vdots           &    \\vdots\\\\ c_{d-1,1}(\\theta )    &    \\cdots   &    c_{d-1,d-1}(\\theta ) &    c_{d-1,d}(\\theta)\\\\ \\hline",
    "c_{d,1}(\\theta ) &    \\cdots   &    c_{d , d-1}(\\theta ) &    0 \\end{array } \\right]\\ ] ] for @xmath883 and @xmath884 , where we swapped the @xmath885-th entries of the matrices in the sum from ( [ matrix_m ] ) .",
    "we used also that @xmath886 and @xmath887 .",
    "note that the top left matrices in the block matrices above are gram matrices and therefore positive semidefinite .",
    "the matrix @xmath888_{i , j=1,\\dots , d-1}$ ] is the gram matrix associated with the vectors @xmath889 .",
    "since @xmath890 is a triangular matrix with @xmath523 s in its main diagonal , it follows that these vectors are linearly independent .",
    "hence the associated gramian is actually positive definite for each @xmath385 .",
    "the determinant of this gramian is a polynomial in the entries of the matrix which in turn are polynomials in @xmath877 .",
    "if @xmath385 lies in a compact set @xmath15 , the infimum of the determinant of this matrix over @xmath879 must be lower bounded by some positive constant , say , @xmath891 . using the same reasoning",
    ", we conclude that its determinant is upper bounded by some constant @xmath892 . a lower bound on the smallest eigenvalue",
    "can then be obtained by noting that for any positive definite matrix @xmath48 of order @xmath395 , @xmath893 we conclude that the smallest eigenvalue of the block matrix on the left is at least @xmath894 .",
    "the block matrix on the right is clearly positive semidefinite .",
    "we conclude that the smallest eigenvalue of the matrix @xmath895 by using weyl s monotonicity theorem , see for example @xcite .",
    "this means that @xmath896 for all @xmath897 .",
    "swapping back the @xmath885-th entries of the matrices in the sum from ( [ matrix_m ] ) , we see that there must exist an @xmath822 such that @xmath897 and for which the smallest eigenvalue of the second matrix in the sum from ( [ matrix_m ] ) is bounded from below by @xmath898 for some @xmath899 . by using renormalization arguments ,",
    "we conclude that @xmath900 for some @xmath901 .",
    "condition ( a2 ) is not difficult to check .",
    "the gain ( [ eq : gain_ar_d ] ) can be written in the following form @xmath902 where @xmath903 represents the jacobian operator . to verify ( a2 ) , it suffices to check that the expectation of the norm of @xmath904 is bounded .",
    "we omit the details but it is clear from the expression derived above that the norm of the gain function squared is a polynomial of degree four in the coordinates of @xmath905 . we have already mentioned that if the initial values for the autoregressive process and the noise terms have uniformly bounded second moments , then this transfers to the each observation @xmath8 , provided that the sequence of parameters of the model , @xmath107 , lives in the parameter set @xmath745 for some @xmath759 .    for the matrix @xmath48 defined by ( [ matrix_m ] ) , we established above that almost surely @xmath906 uniformly in @xmath879 for any bounded @xmath880 .",
    "we can get rid of the dependence on @xmath907 ( recall that @xmath908 ) by using the rescaled gain @xmath909 defined in section  [ sec : gains ] : @xmath910 then @xmath911 , where @xmath912 and @xmath48 is defined by ( [ matrix_m ] ) .",
    "the argument in shows that ( a2 ) still holds for this rescaled gain .",
    "next , @xmath913 almost surely by construction . thus to establish ( a1 ) , we need to verify that @xmath914 \\ge c$ ] .",
    "one can then proceed as in ( [ m_bound_1 ] ) ( and lemma  [ lemma : truncated_conditional ] ) to show that for an appropriately large @xmath524 , @xmath915 > c$ ] , we omit this derivation .",
    "one could drop the requirement for the errors to be gaussian and still use the same gain @xmath904 .",
    "we expect the same results to hold , under appropriate moment assumptions . instead of using the kullback - leibler representation",
    ", one has to work with quantities @xmath904 and @xmath824 directly and assure the validity of ( a1 ) and ( a2 ) based on moment assumptions on the error terms ( and possibly the initial conditions ) as we did in the one dimensional case .",
    "first suppose that @xmath195 for some symmetric positive definite matrix @xmath48 such that @xmath916 .",
    "then @xmath917 and therefore @xmath918 and @xmath919    now we prove the converse assertion .",
    "suppose @xmath39 and @xmath920 for some @xmath921 such that @xmath202 and that @xmath922 .",
    "let @xmath923 be the linear space spanned by @xmath389 and @xmath924 .",
    "first consider the case @xmath925 , i.e. , @xmath926 for some @xmath927 .",
    "then @xmath928 so that @xmath929",
    ". thus @xmath930 with symmetric and positive @xmath931 so that @xmath932 .",
    "let @xmath941 be chosen in such a way that @xmath942 ( which is always possible . )",
    "now , we change the basis of @xmath936 as follows : @xmath943 we thus rotate the basis @xmath935 by the angle @xmath385 . in these new basis",
    "we have @xmath944      let @xmath952 be the orthonormal basis of @xmath953 , so that @xmath954 is an orthonormal basis of @xmath18 .",
    "take @xmath955 \\quad \\hbox { with } \\quad \\bm{d}=\\left[\\begin{array}{cc } \\alpha_y/\\alpha_x & 0\\\\ 0 & \\beta_y/\\beta_x \\end{array}\\right]\\ ] ] where the @xmath51 s indicate null matrices of the appropriate dimensions .",
    "we then have @xmath956 in the basis @xmath957 and @xmath958 .",
    "we can finally obtain @xmath48 by using the orthogonal matrix @xmath959 to change the basis @xmath960 to the canonical basis of @xmath18 as @xmath961 .",
    "clearly , @xmath48 has the same eigenvalues as @xmath962 and is symmetric .    for the sake of brevity , we use the notations @xmath25 , @xmath963 and @xmath240 , @xmath20 .",
    "recall that @xmath15 is compact and @xmath964 . by iterating ( [ eq : algorithm_main ] ) ,",
    "it is easy to see that @xmath965 for each @xmath160 .",
    "first assume @xmath966 , for some @xmath967 to be chosen later . by ( [ bound_g_k ] )",
    ", we obtain @xmath968 , which implies , in view of ( [ eq : algorithm_main ] ) and the fact that @xmath117 , @xmath969    next , consider the case @xmath970 , which , together with ( [ c_theta ] ) , implies that @xmath971 .",
    "recall that , in view of ( a1 ) , @xmath246 is a symmetric positive definite matrix such that @xmath972 almost surely .",
    "therefore we obtain that , almost surely , @xmath973 and , by the cauchy - schwarz inequality , @xmath974 by using the last two relation , ( [ c_theta ] ) , ( [ eq : d ] ) , ( [ eq : g2 ] ) and ( [ eq : algorithm_main ] ) , we evaluate @xmath975 : @xmath976 + \\gamma^2_k { { \\mathbb{e}}}\\|g_k\\|^2\\\\ \\le & { { \\mathbb{e}}}\\|\\hat{\\theta}_k\\|^2 - 2 \\gamma_k { { \\mathbb{e}}}\\big(\\hat{\\theta}_k^t \\bm{m}_k ( \\hat{\\theta}_k - \\theta_k)\\big ) + \\gamma_k^2 \\bar{c}_g\\\\ \\le & { { \\mathbb{e}}}\\|\\hat{\\theta}_k\\|^2 - 2 \\gamma_k \\big[\\lambda_1 { { \\mathbb{e}}}\\|\\hat{\\theta}_k\\|^2   - { { \\mathbb{e}}}\\big(\\hat{\\theta}_k^t\\bm{m}_k \\theta_k ) \\big]+\\gamma_k^2 \\bar{c}_g(kc_\\theta)^{-1 } { { \\mathbb{e}}}\\|\\hat{\\theta}_k\\|^2\\\\ \\le & { { \\mathbb{e}}}\\|\\hat{\\theta}_k\\|^2 - 2 \\gamma_k \\big[\\lambda_1 { { \\mathbb{e}}}\\|\\hat{\\theta}_k\\|^2 -   \\lambda_2 { { \\mathbb{e}}}\\big(\\|\\hat{\\theta}_k\\| \\|\\theta_k\\|\\big)\\big ] + \\gamma_k^2 c_3{{\\mathbb{e}}}\\|\\hat{\\theta}_k\\|^2,\\end{aligned}\\ ] ] with @xmath977 .",
    "since @xmath978 , it follows that @xmath979 . using this ,",
    "we proceed by bounding the previous display as follows : @xmath980 for sufficiently large @xmath981 and sufficiently small @xmath207 . thus , for sufficiently large @xmath981 and sufficiently small @xmath207 , @xmath982 with @xmath983 as defined above .",
    "[ lemma : eig ] let @xmath48 be a symmetric positive definite matrix of order @xmath395 , @xmath569 and a constant @xmath612 be such that @xmath984",
    ". then @xmath985 and @xmath986 besides , @xmath987 for some constant @xmath988 .",
    "let @xmath989 s be the eigenvalues of @xmath48 , so that the matrix @xmath990 has eigenvalues @xmath991 , @xmath992 . since @xmath984 , then , for all @xmath992 , @xmath993 , implying @xmath994 , so that @xmath995 .",
    "the first two assertions follow .",
    "it remains to prove the last assertion .",
    "for @xmath996 , let @xmath997 and @xmath998 . according to theorem 5.6.18 from ,",
    "@xmath999 recall that @xmath1000 and @xmath231 for any @xmath232 and @xmath233 . from the last relation it is easy to get the following bounds : @xmath1001 if @xmath235 , @xmath1002 if @xmath1003 ; @xmath1004 if @xmath235 , @xmath1005 if @xmath1003 .",
    "these bounds imply that @xmath1006 if @xmath235 and @xmath1007 if @xmath1003 .",
    "this completes the proof of the lemma .",
    "we prove this by induction in @xmath110 .",
    "for @xmath1013 we simply have @xmath1014 and the assertion holds true .",
    "assume that the equality holds for @xmath1015 and let us prove the result for @xmath1016 .",
    "we have @xmath1017    [ lemma : truncated_conditional ] consider an ar(1)-model with a measurable @xmath25 : @xmath1018 where @xmath789=\\mathbb{e } [ \\xi_k^3|\\bm{x}_{k-1}]=0 $ ] , @xmath790=\\sigma^2>0 $ ] , and @xmath791=c \\sigma^4 $ ] , @xmath156 , for some constant @xmath792 .",
    "then , for any @xmath1019 such that @xmath1020 , @xmath1021 \\ge \\frac{(5-c)\\sigma^2}{4},\\quad k \\in \\mathbb{n}.\\ ] ]    we compute @xmath1022 & = x_{k-1}^2 \\theta_k^2 + 2x_{k-1}\\theta_k   \\mathbb{e}[\\xi_k|\\bm{x}_{k-1 } ] + \\mathbb{e}[\\xi_k^2|\\bm{x}_{k-1 } ] = x_{k-1}^2 \\theta_k^2+\\sigma^2,\\\\ \\mathbb{e}[x_k^4 |\\bm{x}_{k-1 } ]   & = x_{k-1}^4\\theta_k^4 - 4x_{k-1}^3\\theta_k^3 \\mathbb{e}[\\xi_k|\\bm{x}_{k-1 } ]   + 6 x_{k-1}^2\\theta_k^2 \\mathbb{e}[\\xi_k^2|\\bm{x}_{k-1 } ] \\\\ & \\;\\;\\ ; -   4x_{k-1}\\theta_k \\mathbb{e}[\\xi_k^3|\\bm{x}_{k-1}]+\\mathbb{e}[\\xi_k^4|\\bm{x}_{k-1 } ]   = x_{k-1}^4\\theta_k^4 + 6x_{k-1}^2\\theta_k^2\\sigma^2+c\\,\\sigma^4.\\end{aligned}\\ ] ] for @xmath1023 we have @xmath1024 . using this relation , conditional version of jensen s inequality and the last display ,",
    "we derive : @xmath1025 & =   \\frac{1}{2}\\mathbb{e}\\big[x_k^2 + \\rho\\sigma^2-|x_k^2-\\rho\\sigma^2| \\big|\\bm{x}_{k-1}\\big]\\\\ & \\ge \\frac{1}{2 } \\big[x_{k-1}^2\\theta_k^2 + ( \\rho+1)\\sigma^2 -\\big(\\mathbb{e}\\big[\\big(x_k^2-\\rho\\sigma^2\\big)^2|\\bm{x}_{k-1}\\big]\\big)^{1/2}\\big],\\end{aligned}\\ ] ] for @xmath719 .",
    "we now have , by plugging in the expressions derived above and simplifying , @xmath1026 = \\mathbb{e}\\big[x_k^4|\\bm{x}_{k-1}\\big ] - 2\\rho\\sigma^2\\mathbb{e}\\big[x_k^2|\\bm{x}_{k-1}\\big ] + \\rho^2\\sigma^4\\\\ & = x_{k-1}^4\\theta_k^4 + 2(3-\\rho)x_{k-1}^2\\theta_k^2\\sigma^2 + ( c-2\\rho+\\rho^2)\\sigma^4 = \\big(x_{k-1}^2\\theta_k^2+\\frac{c+3}4\\sigma^2\\big)^2,\\end{aligned}\\ ] ] if we pick @xmath1027 . combining the previous two displays ,",
    "we conclude that @xmath1028 \\ge   \\frac{(5-c)\\sigma^2}{4 } , \\quad k \\in \\mathbb{n},\\ ] ] and the statement of the lemma follows ."
  ],
  "abstract_text": [
    "<S> we propose an online algorithm for tracking a multidimensional time - varying parameter of a time series , which is also allowed to be a predictable process with respect to the underlying time series . </S>",
    "<S> the algorithm is driven by a gain function . under assumptions on the gain , </S>",
    "<S> we derive uniform non - asymptotic error bounds on the tracking algorithm in terms of chosen step size for the algorithm and the variation of the parameter of interest . </S>",
    "<S> we also outline how appropriate gain functions can be constructed . </S>",
    "<S> we give several examples of different variational setups for the parameter process where our result can be applied . </S>",
    "<S> the proposed approach covers many frameworks and models ( including the classical robbins - monro and kiefer - wolfowitz procedures ) where stochastic approximation algorithms comprise the main inference tool for the data analysis . </S>",
    "<S> we treat in some detail a couple of specific models .    </S>",
    "<S> * keywords : * on - line tracking ; predictable drifting parameter ; recursive algorithm ; stochastic approximation procedure ; time series ; time - varying parameter . </S>"
  ]
}