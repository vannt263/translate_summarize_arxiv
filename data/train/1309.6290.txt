{
  "article_text": [
    "structural vector autoregressive ( svar ) model @xcite is widely used for multi - branch signal modelling in the fields of wireless communication , econometrics , physics , and multi - dimensional audio signal processing .",
    "given @xmath0 , the input matrix with m signal branches and length n , we would like to express @xmath1 in the k - th order vector auto - regressive form in terms of time - invariant mxm matrices @xmath2 , @xmath3 , and intercept vector @xmath4 , collectively referred to as coefficient matrices , such that the residual @xmath5 has an identity covariance matrix @xmath6 .",
    "the model for svar is given as follows : @xmath7    in this paper we present an efficient method for computation of the model coefficient matrices . in section [ sec : ls ] we review a widely used method based on least - squares . in section [ sec : lic ] we propose the large inverse cholesky ( lic ) method . in section",
    "[ sec : sc ] we compare the computational complexity of these methods , followed by concluding remarks in [ sec : con ] .",
    "least - squares based method uses the reduced - form var ( rvar ) as an intermediate step in computing the coefficient matrices of svar .",
    "first , the input matrix @xmath1 is modelled as an rvar and the coefficient matrices of rvar are computed using least - squares .",
    "the rvar model is as follows : @xmath8 in the above equation , the covariance matrix of @xmath9 is a positive - definite symmetric matrix .    equation ( [ eqn : rvar ] ) may be rewritten in the matrix - form as follows : @xmath10 where , the matrices @xmath11 , @xmath12 , @xmath13 , and @xmath14 are given as : @xmath15 \\\\ { a } = [ & { c } , { a}_1 , { a}_2 , \\ldots , { a}_k ] \\\\ { s } = [ & \\mathbf{1}_{{\\mathrm{1x(n - k ) } } } ;   \\nonumber \\\\          &   { x}(k ) , { x}(k+1 ) , \\ldots , { x}(n-1 ) ; \\nonumber \\\\          & { x}(k-1 ) , { x}(k ) , \\ldots , { x}(n-2 ) ; \\nonumber \\\\           & \\vdots \\nonumber \\\\          & { x}(1 ) , { x}(2 ) , \\ldots , { x}(n - k ) ] \\label{eqn : sdef } \\\\ { v } = [ & { v}(k+1 ) , { v}(k+2 ) , \\ldots , { v}(n)]\\end{aligned}\\ ] ]    in the definition of @xmath16 , a comma indicates that the next term is concatenated horizontally , thereby increasing the number of columns , while a semicolon indicates that the next term is concatenated vertically , thereby increasing the number of rows .",
    "the parameter matrix of the rvar model @xmath17 , and the matrix @xmath18 may be found using the least - squares method as : @xmath19    next , the rvar model is converted to its equivalent svar model by multiplying the rvar model equation ( [ eqn : rvar ] ) with the inverse - cholesky based whitening filter of @xmath18 . from comparison of equations ( [ eqn : svar ] ) and ( [ eqn : rvar ] ) we note that the inverse - cholesky filter is @xmath2 , i.e. @xmath20",
    "therefore , the coefficient matrices of svar may be computed from the coefficient matrices of rvar by multiplying them with @xmath2 .",
    "large inverse cholesky method computes the coefficient matrices of svar directly using an inverse - cholesky of @xmath21 , where @xmath22 defined as follows : @xmath23 \\label{eqn : tdef}\\end{aligned}\\ ] ]    let @xmath24 be the lower - triangular inverse - cholesky of @xmath21 , then we have : @xmath25    let index variables : @xmath26 then , the coefficient matrices of the svar model are given by : @xmath27 with @xmath28 .",
    "we can easily verify that the coefficient matrices computed by the least - squares method and the lic method are equivalent .",
    "let @xmath29 $ ] , and let the last m rows of @xmath24 be the matrix @xmath30 , then we have @xmath31 $ ] and @xmath32 $ ] . from equation ( [ eqn : licm ] )",
    "we have : @xmath33 upon simplification : @xmath34 which is readily identified as the equivalent of least - squares based method .",
    "a matlab compatible algorithm without optimization is given below , the full program may be downloaded from the matlab central file exchange from @xcite .",
    "for computing the complexity , we consider the number of multiplies for each operation .",
    "it is assumed that on modern dsp processors , the multiply - accumulate ( mac ) instruction hides the additions that are performed in each step .",
    "further , where the output of matrix multiplies result in symmetric matrices , the number of multiplies is chosen as half of the conventional value . the computational complexity for cholesky decomposition and inversion is chosen for an nxn matrix at the most efficient value of @xmath35 .",
    "[ sec : sc ]      [ cols=\"<,<\",options=\"header \" , ]     lic is efficient for @xmath36 . for practical cases , lic can be upto 30% efficient than least - squares method for finding the coefficient matrices of an svar .",
    "in this paper we presented the large inverse cholesky method for computing the coefficient matrices of a structural autoregressive model which is upto 30% efficient compared to the conventional least - squares based method .",
    "9 helmut ltkepohl , new introduction to multiple time series analysis , springer , october 4 , 2007 .",
    "david s. watkins , `` fundamentals of matrix computations '' , second edition , wiley , 2002 . gene h. golub , charles f. van loan , matrix computations , third edition , the johns hopkins university press , 1996 . aravindh krishnamoorthy , large inverse cholesky ( http://www.mathworks.de/matlabcentral/fileexchange/44106 ) matlab central file exchange , 2013 ."
  ],
  "abstract_text": [
    "<S> in this paper we present the large inverse cholesky ( lic ) method , an efficient method for computing the coefficient matrices of a structural vector autoregressive ( svar ) model . </S>"
  ]
}