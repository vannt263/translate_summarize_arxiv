{
  "article_text": [
    "there are many problems in science in which the state of a system must be identified from an uncertain equation supplemented by a stream of noisy data ( see e.g. @xcite ) .",
    "a natural model of this situation consists of a stochastic differential equation ( sde ) : @xmath1 where @xmath2 is an @xmath3-dimensional vector , @xmath4 is @xmath3-dimensional brownian motion , @xmath5 is an @xmath3-dimensional vector function , and @xmath6 is a scalar ( i.e. , an @xmath3 by @xmath3 diagonal matrix of the form @xmath7 , where @xmath6 is a scalar and @xmath8 is the identity matrix ) .",
    "the brownian motion encapsulates all the uncertainty in this equation .",
    "the initial state @xmath9 is assumed given and may be random as well .",
    "as the experiment unfolds , it is observed , and the values @xmath10 of a measurement process are recorded at times @xmath11 ; for simplicity assume @xmath12 , where @xmath13 is a fixed time interval and @xmath14 is an integer .",
    "the measurements are related to the evolving state @xmath15 by @xmath16 where @xmath17 is a @xmath18-dimensional , generally nonlinear , vector function with @xmath19 , @xmath20 is a diagonal matrix , @xmath21 , and @xmath22 is a vector whose components are independent gaussian variables of mean 0 and variance 1 , independent also of the brownian motion in equation ( [ eq : datass ] ) . the task is to estimate @xmath23 on the basis of equation ( [ eq : datass ] ) and the observations ( [ eq : observe ] ) .",
    "if the system  ( [ eq : datass ] ) is linear and the data are gaussian , the solution can be found via the kalman - bucy filter . in the general case ,",
    "it is natural to try to estimate @xmath23 as the mean of its evolving probability density .",
    "the initial state @xmath23 is known and so is its probability density ; all one has to do is evaluate sequentially the density @xmath24 of @xmath25 given the probability density @xmath26 of @xmath27 and the data @xmath28 .",
    "this can be done by following  particles \" ( replicas of the system ) whose empirical distribution approximates @xmath26 . in a bayesian filter",
    "( see e.g @xcite , one uses the pdf @xmath26 and equation ( [ eq : datass ] ) to generate a prior density , and then one uses the new data @xmath28 to generate a posterior density @xmath24 .",
    "in addition , one may have to sample backward to take into account the information each measurement provides about the past and avoid having too many identical particles .",
    "evolving particles is typically expensive , and the backward sampling , usually done by markov chain monte carlo ( mcmc ) , can be expensive as well , because the number of particles needed can grow catastrophically ( see e.g. @xcite ) .    in this paper",
    "we offer an alternative to the standard approach , in which @xmath24 is sampled directly without recourse to bayes theorem and backward sampling , if needed , is done by chainless monte carlo @xcite .",
    "our direct sampling is based on a representation of a variable with density @xmath24 by a collection of functions of gaussian variables parametrized by the support of @xmath26 , with parameters found by iteration .",
    "the construction is related to chainless sampling as described in @xcite .",
    "the idea in chainless sampling is to produce a sample of a large set of variables by sequentially sampling a growing sequence of nested conditionally independent subsets .",
    "as observed in @xcite , chainless sampling for a sde reduces to interpolatory sampling , as explained below",
    ". our construction will be explained in the following sections through an example where the position of a ship is deduced from the measurements of an azimuth , already used as a test bed in @xcite .",
    "first we explain how to sample via interpolation and iteration in a simple example , related to the example and the construction in @xcite .",
    "consider the scalar sde @xmath29 we want to find sample paths @xmath30 , subject to the conditions @xmath31 .",
    "let @xmath32 denote a gaussian variable with mean @xmath33 and variance @xmath34 .",
    "we first discretize equation ( [ scalar ] ) on a regular mesh @xmath35 , where @xmath12 , @xmath36 , @xmath37 , with @xmath38 , and , following @xcite , use a balanced implicit discretization @xcite : @xmath39 where @xmath40 and @xmath41 is @xmath42 .",
    "the joint probability density of the variables @xmath43 is @xmath44 , where @xmath45 is the normalization constant and @xmath46 where @xmath47 are functions of the @xmath48 , and @xmath49 ( see @xcite ) .",
    "one can obtain sample solutions by sampling this density , e.g. by mcmc , or one can obtain them by interpolation ( chainless sampling ) , as follows .",
    "consider first the special case @xmath50 , so that in particular @xmath51 .",
    "each increment @xmath52 is now a @xmath53 variable , with the @xmath54 known explicitly .",
    "let @xmath55 be a power of @xmath56 .",
    "consider the variable @xmath57 . on one hand , @xmath58 where @xmath59 . on the other hand",
    ", @xmath60 so that @xmath61 with @xmath62 the pdf of @xmath57 is the product of the two pdfs ; one can check that @xmath63 where @xmath64 , @xmath65 , and @xmath66 ; @xmath67 is the probability of getting from the origin to @xmath68 , up to a normalization constant .    pick a sample @xmath69 from the @xmath70 density ; one obtains a sample of @xmath57 by setting @xmath71 . given a sample of @xmath57 one can similarly sample @xmath72 , then @xmath73 , @xmath74 , etc .",
    ", until all the @xmath48 have been sampled .",
    "if we define @xmath75 , then for each choice of @xmath76 we find a sample @xmath77 such that @xmath78 where the factor @xmath79 on the left is the probability of the fixed end value @xmath68 up to a normalization constant . in this linear problem",
    ", this factor is the same for all the samples and therefore harmless .",
    "one can repeat this sampling process for multiple choices of the variables @xmath80 ; each sample of the corresponding set of @xmath81 is independent of any previous samples of this set .",
    "now return to the general case .",
    "the functions @xmath82 , @xmath83 are now functions of the @xmath48 .",
    "we obtain a sample of the probability density we want by iteration .",
    "first pick @xmath84 , where each @xmath80 is drawn independently from the @xmath70 density ( this vector remains fixed during the iteration ) .",
    "make a first guess @xmath85 ( for example , if @xmath86 , pick @xmath87 ) .",
    "evaluate the functions @xmath47 at @xmath88 ( note that now @xmath89 , and therefore the variances of the various increments are no longer constants ) .",
    "we are back in previous case , and can find values of the increments @xmath90 corresponding to the values of @xmath47 we have . repeat the process starting with the new iterate . if the vectors @xmath88 converge to a vector @xmath91 , we obtain , in the limit , equation ( [ palim ] ) , where now on the right side @xmath92 depends on @xmath14 so that @xmath93 , and both @xmath94 are functions of the final @xmath23 .",
    "the left hand side of ( [ palim ] ) becomes : @xmath95 note that now the factor @xmath96 is different from sample to sample , and changes the relative weights of the different samples . in averaging , one should take this factor as weight , or resample as described at the end of the following section . in order to obtain more uniform weights ,",
    "one also can use the strategies in @xcite .",
    "one can readily see that the iteration converges if @xmath97 , where @xmath98 is the lipshitz constant of @xmath82 , @xmath99 is the length of the interval on which one works ( here @xmath100 ) , and @xmath101 is the maximum norm of the vectors @xmath102 .",
    "if this inequality is not satisfied for the iteration above , it can be re - established by a suitable underrelaxation .",
    "one should course choose @xmath55 large enough so that the results are converged in @xmath55 .",
    "we do not provide more details here because they are extraneous to our purpose , which is to explain chainless / interpolatory sampling and the use of reference variables in a simple context .",
    "the problem we focus on is discussed in @xcite , where it is used to demonstrate the capabilities of particular bayesian filters .",
    "a ship sets out from a point @xmath103 in the plane and undergoes a random walk , @xmath104 for @xmath105 , and with @xmath106 given , and @xmath107 , @xmath108 , i.e. , each displacement is a sample of a gaussian random variable whose variance @xmath92 does not change from step to step and whose mean is the value of the previous displacement .",
    "an observer makes noisy measurements of the azimuth @xmath109 , recording @xmath110 where the variance @xmath111 is also fixed ; here the observed quantity @xmath112 is scalar and is not be denoted by a boldfaced letter .",
    "the problem is to reconstruct the positions @xmath113 from equations ( [ eq1],[eq2 ] ) .",
    "we take the same parameters as @xcite : @xmath114 , @xmath115 , @xmath116 , @xmath117 .",
    "we follow numerically @xmath101 particles , all starting from @xmath118 , as described in the following sections , and we estimate the ship s position at time @xmath119 as the mean of the locations @xmath120 of the particles at that time .",
    "the authors of @xcite also show numerical results for runs with varying data and constants ; we discuss those refinements in section 6 below .",
    "assume we have a collection of @xmath101 particles @xmath121 at time @xmath12 whose empirical density approximates @xmath26 ; now we find increments @xmath122 such that the empirical density of @xmath123 approximates @xmath24 .",
    "@xmath24 is known implicitly : it is the product of the density that can be deduced from the sde and the one that comes from the observations , with the appropriate normalization .",
    "if the increments were known , their probability @xmath124 ( the density @xmath24 evaluated at the resulting positions @xmath125 ) would be known , so @xmath124 is a function of @xmath122 ,  @xmath126 . for each particle",
    "@xmath127 , we are going to sample a gaussian reference density , obtain a sample of probability @xmath128 , then solve ( by iteration ) the equation @xmath129 to obtain @xmath130 .",
    "define @xmath131 and @xmath132 .",
    "we are working on one particle at a time , so the index @xmath127 can be temporarily suppressed .",
    "pick two independent samples @xmath133 , @xmath134 from a @xmath70 density ( the reference density in the present calculation ) , and set @xmath135 ; the variables @xmath133 , @xmath134 remain unchanged until the end of the iteration .",
    "we are looking for displacements @xmath136 , @xmath137 , and parameters @xmath138 , such that : @xmath139 the first equality states what we wish to accomplish : find increments @xmath136 , @xmath137 , functions respectively of @xmath140 , whose probability with respect to @xmath24 is @xmath128 .",
    "the factor @xmath141 is needed to normalize this term ( @xmath142 is called below a  phase \" ) . the second equality says how the goal is reached : we are looking for parameters @xmath143 ( all functions of @xmath121 ) such that the increments are samples of gaussian variables with these parameters , with the assumed probability .",
    "one should remember that in our example the mean of @xmath136 is @xmath144 , and similarly for @xmath137 .",
    "we are not representing @xmath24 as a function of a single gaussian- there is a different gaussian for every value of @xmath121 . to satisfy the second equality we set up an iteration for vectors @xmath145 for brevity ) that converges to @xmath122 .",
    "start with @xmath146 .",
    "we now explain how to compute @xmath147 given @xmath148 .",
    "approximate the observation equation ( [ eq2 ] ) by @xmath149 where the derivatives @xmath150 are , like @xmath82 , evaluated at @xmath151 , i.e. , approximate the observation equation by its taylor series expansion around the previous iterate .",
    "define a variable @xmath152 .",
    "the approximate observation equation says that @xmath153 is a @xmath154 variable , with @xmath155 on the other hand , from the equations of motion one finds that @xmath153 is @xmath156 , with @xmath157 and @xmath158 .",
    "hence the pdf of @xmath153 is , up to normalization factors , @xmath159 where @xmath160 , @xmath161 , @xmath162 .",
    "we can also define a variable @xmath163 that is a linear combination of @xmath164 , @xmath165 and is uncorrelated with @xmath153 : @xmath166 the observations do not affect @xmath167 , so its mean and variance are known .",
    "given the means and variances of @xmath153 , @xmath168 one can easily invert the orthogonal matrix that connects them to @xmath164 , @xmath165 and find the means and variances @xmath169 of @xmath164 and @xmath170 of @xmath165 after their modification by the observation ( the subscripts on @xmath171 are labels , not differentiations ) .",
    "now one can produce values for @xmath172 : @xmath173 where @xmath133 , @xmath134 are the samples from @xmath70 chosen at the beginning of the iteration .",
    "this completes the iteration .",
    "this iteration converges to @xmath125 such that @xmath174 , and the phases @xmath175 converge to a limit @xmath176 , where the particle index @xmath127 has been restored .",
    "the time interval over which the solution is updated in each step is short , and we do not expect any problem with convergence , either here or in the next section , and indeed there is none ; in all cases the iteration converges in a small number of steps .",
    "note that after the iteration the variables @xmath177 are no longer independent- the observation creates a relation between them .",
    "do this for all the particles .",
    "the particles are now samples of @xmath24 , but they have been obtained by sampling different densities ( remember that the parameters in the gaussians in equation ( [ forward ] ) vary ) .",
    "one can get rid of this heterogeneity by viewing the factors @xmath178 as weights and resampling , i.e. , for each of @xmath101 random numbers @xmath179 drawn from the uniform distribution on @xmath180 $ ] , choose a new @xmath181 such that @xmath182 ( where @xmath183 ) , and then suppress the hat .",
    "we have traded the resampling of bayesian filters for a resampling based on the normalizing factors of the several gaussian densities ; this is a worthwhile trade because in a bayesian filter one gets a set of samples many of which may have low probability with respect to @xmath24 , and here we have a set of samples each one of which has high probability with respect to a pdf close to @xmath24 .",
    "note also that the resampling does not have to be done at every step- for example , one can add up the phases for a given particle and resample only when the ratio of the largest cumulative weight @xmath184 to the smallest such weight exceeds some limit @xmath185 ( the summation is over the weights accrued to a particular particle @xmath127 since the last resampling ) . if one is worried by too many particles being close to each other ( `` depletion '' in the bayesian terminology )",
    ", one can divide the set of particles into subsets of small size and resample only inside those subsets , creating a greater diversity . as will be seen in section 6",
    ", none of these strategies will be used here and we will resample fully at every step .",
    "the algorithm of the previous section is sufficient to create a filter , but accuracy may require an additional refinement .",
    "every observation provides information not only about the future but also about the past- it may , for example , tag as improbable earlier states that had seemed probable before the observation was made ; one may have to go back and correct the past after every observation ( this backward sampling is often misleadingly motivated solely by the need to create greater diversity among the particles in a bayesian filter ) . as will be seen below",
    ", this backward sampling does not provide a significant boost to accuracy in the present problem , but it is described here for the sake a completeness .    given a set of particles at time @xmath186 , after a forward step and maybe a subsequent resampling , one can figure out where each particle @xmath127 was in the previous two steps , and have a partial history for each particle @xmath127 : @xmath187 ( if resamples had occurred , some parts of that history may be shared among several current particles ) .",
    "knowing the first and the last member of this sequence , one can interpolate for the middle term as in section 2 , thus projecting information backward .",
    "this requires that one recompute @xmath188 .    let @xmath189 ; in the present section this quantity is assumed known and remains fixed . in the azimuth problem discussed here , one has to deal with the slight complication due to the fact that the mean of each increment is the value of the previous one , so that two successive increments are related in a slightly more complicated way than usual .",
    "the displacement @xmath144 is a @xmath190 variable , and @xmath136 is a @xmath191 variable , so that one goes from @xmath192 to @xmath193 by sampling first a @xmath194 variable that takes us from @xmath195 to an intermediate point @xmath196 , with a correction by the observation half way up this first leg , and then one samples a @xmath197 variable to reach @xmath193 , and similarly for @xmath198 . let the variable that connects @xmath199 to @xmath196 be @xmath200 , so that what replaces @xmath188 is @xmath201 .",
    "accordingly , we are looking for a new displacement @xmath202 , and for parameters @xmath203 such that @xmath204 where @xmath205 and @xmath133 , @xmath134 are independent @xmath70 gaussian variables . as in equation ( [ forward ] ) , the first equality embodies what we wish to accomplish- find increments , functions of the reference variables , that sample the new pdf at time @xmath119 defined by the forward motion , the constraint imposed by the observation , and by knowledge of the position at time @xmath206 .",
    "the second equality states that this is done by finding particle - dependent parameters for a gaussian density .",
    "we again find these parameters as well as the increments by iteration .",
    "much of the work is separate for the @xmath68 and @xmath198 components of the equations of motion , so we write some of the equations for the @xmath68 component only .",
    "again set up an iteration for variables @xmath207 which converge to @xmath208 .",
    "start with @xmath209 . to find @xmath164 given @xmath210 ,",
    "approximate the observation equation ( [ eq2 ] ) , as before , by equation ( [ obs ] ) ; define again variables @xmath211 , one in the direction of the approximate constraint and one orthogonal to it ; in the direction of the constraint multiply the pdfs as in the previous section ; construct new means @xmath212 and new variances @xmath213 for @xmath214 at time @xmath14 , taking into account the observation at time @xmath14 , again as before .",
    "this also produces a phase @xmath215 .",
    "now take into account that the location of the boat at time @xmath216 is known ; this creates a new mean @xmath217 , a new variance @xmath218 , and a new phase @xmath219 , by @xmath160 , @xmath220 , @xmath221 , where @xmath222 . finally , find a new interpolated position @xmath223 ( the calculation for @xmath165 is similar , with a phase @xmath224 ) , and we are done .",
    "the total phase for in this iteration is @xmath225 . as",
    "the iterates @xmath210 converge to @xmath208 , the phases converge to a limit @xmath176 .",
    "the probability of a particle arriving at the given position at time @xmath206 having been determined in the forward step , there is no need to resample before comparing samples . once one has the values of @xmath226 , a forward step gives corrected values of @xmath125 ; one can use this interpolation process to correct estimates of @xmath227 by subsequent observations for @xmath228 , as many as are useful .",
    "before presenting examples of numerical results for the azimuth problem , we discuss the accuracy one can expect .",
    "a single set of observations for our problem relies on 160 samples of a @xmath229 variable .",
    "the maximum likelihood estimate of @xmath92 given these samples is a random variable with mean @xmath92 and standard deviation @xmath230 .",
    "we estimate the uncertainty in the position of the boat by picking a set of observations , then making multiple runs of the boat where the random components of the motion in the direction of the constraint are frozen while the ones orthogonal to it are sampled over and over from the suitable gaussian density , then computing the distances to the fixed observations , estimating the standard deviation of these differences , and accepting the trajectory if the estimated standard deviation is within one standard deviation of the nominal value of @xmath111 .",
    "this process generates a family of boat trajectories compatible with the given observations . in table",
    "i we display the standard deviations of the differences between the resulting paths and the original path that produced the observations after the number of steps indicated there ( the means of these differences are statistically indistinguishable from zero ) .",
    "this table provides an estimate of the accuracy we can expect .",
    "it is fair to assume that these standard deviations are underestimates of the uncertainty- a variation of a single standard deviation in @xmath111 is a strict constraint , and we allowed no variation in @xmath92 .",
    "table i + intrinsic uncertainty in the azimuth problem    [ cols=\"^,^,^\",options=\"header \" , ]",
    "we have exhibited a non - bayesian filtering method , related to recent work on chainless sampling , designed to focus particle paths more sharply and thus require fewer of them , at the cost of an added complexity in the evaluation of each path .",
    "the main features of the algorithm are a representation of a new pdf by means of a set of functions of gaussian variables and a resampling based on normalization factors .",
    "the construction was demonstrated on a standard ill - conditioned test problem .",
    "further applications will be published elsewhere .",
    "we would like to thank prof .",
    "r. kupferman , prof .",
    "r. miller , and dr .",
    "j. weare for asking searching questions and providing good advice .",
    "this work was supported in part by the director , office of science , computational and technology research , u.s .",
    "department of energy under contract no .",
    "de - ac02 - 05ch11231 , and by the national science foundation under grant dms-0705910 ."
  ],
  "abstract_text": [
    "<S> particle filters for data assimilation in nonlinear problems use  particles \" ( replicas of the underlying system ) to generate a sequence of probability density functions ( pdfs ) through a bayesian process . </S>",
    "<S> this can be expensive because a significant number of particles has to be used to maintain accuracy . </S>",
    "<S> we offer here an alternative , in which the relevant pdfs are sampled directly by an iteration . </S>",
    "<S> an example is discussed in detail .    * non - bayesian particle filters * @xmath0 + @xmath0 + * alexandre j.  chorin and xuemin tu * + @xmath0 + department of mathematics , university of california at berkeley + and + lawrence berkeley national laboratory + berkeley , ca , 94720    * keywords * particle filter , chainless sampling , normalization factor , iteration , non - bayesian </S>"
  ]
}