{
  "article_text": [
    "technological developments in science and engineering lead to collections of massive amounts of high - dimensional data .",
    "scientific advances have become more and more data - driven , and researchers have been making efforts to understand the contemporary large - scale and complex data . among these efforts , variable selection plays a pivotal role in high - dimensional statistical modeling , where the goal is to extract a small set of explanatory variables that are associated with given responses such as biological , clinical , and societal outcomes . toward this end , in the past two decades , statisticians have developed many data learning methods and algorithms , and have applied them to solve problems arising from diverse fields of sciences , engineering and humanities , ranging from genomics , neurosciences and health sciences to economics , finance and machine learning . for an overview , see @xcite and @xcite .",
    "linear regression is often used to investigate the relationship between a response variable @xmath1 and explanatory variables @xmath2 . in the high - dimensional linear model @xmath3 ,",
    "the coefficient @xmath4 is assumed to be sparse with support @xmath5 .",
    "variable selection techniques such as the forward stepwise regression , the lasso @xcite and folded concave penalized least squares @xcite are frequently used .",
    "however , it has been recently noted in @xcite that high dimensionality introduces large spurious correlations between response and unrelated covariates , which may lead to wrong statistical inference and false scientific discoveries . as an illustration",
    ", @xcite considered a real data example using the gene expression data from the international ` hapmap ' project @xcite .",
    "there , the sample correlation between the observed and post - lasso fitted responses is as large as @xmath6 . while conventionally it is a common belief that a correlation of 0.92 between the response and a fit is noteworthy , in high - dimensional scenarios , this intuition may no longer be true .",
    "in fact , even if the response and all the covariates are scientifically independent in the sense that @xmath7 , simply by chance , some covariates will appear to be highly correlated with the response . as a result , the findings obtained via any variable selection techniques are hardly impressive unless they are proven to be better than by chance . to simplify terminology , in this paper",
    "we say that the discovery ( by a variable selection method ) is spurious if it is no better than by chance .    to guard against spurious discoveries",
    ", one naturally asks how good a response can be fitted by optimally selected subsets of covariates , even when the response variable and the covariates are not causally related to each other , that is , when they are independent .",
    "such a measure of the goodness of spurious fit ( gosf ) is a random variable whose distribution can provide a benchmark to gauge whether the discoveries by statistical machine learning methods any better than a spurious fit ( chance ) .",
    "measuring such a goodness of spurious fit and estimating its theoretical distributions are the aims of this paper .",
    "this problem arises from not only high - dimensional linear models and generalized linear models , but also robust regression and other statistical model fitting . to formally measure the degree of spurious fit",
    ", @xcite derived the distributions of maximum spurious correlations , which provide a benchmark to assess the strength of the spurious associations ( between response and independent covariates ) and to judge whether discoveries by a certain variable selection technique are any better than by chance .",
    "the response , however , is not always a quantitative value .",
    "instead , it is often binary ; for example , positive or negative , presence or absence and success or failure . in this",
    "regard , generalized linear models ( glim ) serve as a flexible parametric approach to modeling the relationship between explanatory and response variables @xcite .",
    "prototypical examples include linear , logistic and poisson regression models which are frequently encountered in practice .    in glim ,",
    "the relationship between the response and covariates is more complicated and can not be effectively measured via pearson correlation coefficient , which is essentially a measure of the linear correlation between two variables .",
    "we need to extend the concept of spurious correlation or the measure of goodness of spurious fit to more general models and study its null distribution .",
    "a natural measure of goodness of fit is the likelihood ratio statistic , denoted by @xmath8 , where @xmath9 is the sample size and @xmath10 is size of optimally fitted model .",
    "it measures the goodness of spurious fit when @xmath11 and @xmath1 are independent .",
    "this generalization is consistent with the spurious correlation studied in @xcite , that is , applying @xmath8 to linear regression yields the maximum spurious correlation .",
    "we plan to study the limiting null distribution of @xmath12 under various scenarios .",
    "this reference distribution then serves as a benchmark to determine whether the discoveries are spurious .    to gain further insights ,",
    "let us illustrate the issue by using the gene expression profiles for @xmath13 genes from @xmath14 patients in the german neuroblastoma trials nb90-nb2004 @xcite .",
    "the response labeled as `` 3-year event - free survival '' ( 3-year efs ) is a binary outcome indicating whether each patient survived 3 years after the diagnosis of neuroblastoma . excluding five outlier arrays , there are @xmath15 subjects ( 101 females and 145 males ) with 3-year efs information available . among them , 56 are positives and 190 are negatives .",
    "we apply lasso using the logistic regression model with tuning parameter selected via ten - fold cross validation ( 40 genes are selected ) . the fitted likelihood ratio @xmath16 .",
    "to judge the credibility of the finding of these 40 genes , we should compare the value @xmath17 with the distribution of the goodness of spurious fit ( gosf ) @xmath18 when @xmath11 and @xmath1 are indeed independent , where @xmath19 , @xmath20 and @xmath21 .",
    "this requires some new methodology and technical work .",
    "figure  [ fig0 ] shows the distribution of the gosf estimated by our proposed method below and indicates how abnormal the value @xmath17 is .",
    "it can be concluded that the goodness of fit to the binary outcome is not statistically significantly better than gosf .     in comparison to the distribution of gosf @xmath18 with @xmath19 , @xmath20 and @xmath21 .",
    "[ fig0 ] ]    the above result shows that the 10-fold cross - validation chooses a too large model with 40 variables .",
    "this prompts us to reduce the model sizes along the lasso path such that their fits are better than gosf .",
    "the results are reported in table  [ tab2 ] .",
    "the largest model along the lasso path that fits better than gosf has model size 17 .",
    "we can use the cross - validation to select a model with model size no more than 17 or to select a best model among all models that fit better than gosf .",
    "this is another important application of our method .      in section  [ sec2 ]",
    ", we introduce a general measure of spurious fit via generalized likelihood ratios , which extends the concept of spurious correlation in the linear model to more general models , including generalized linear models and robust linear regression .",
    "we also introduce a local adaptive majorization - minimization ( lamm ) algorithm to compute the gosf .",
    "section  [ sec3 ] presents the main results on the limiting laws of goodness of spurious fit and their bootstrap approximations .",
    "for conducting inference , we use the proposed lamm algorithm to compute the bootstrap statistic . in section  [ sec4 ]",
    ", we discuss an application of our theoretical findings to high - dimensional statistical inference and model selection .",
    "section  [ sec5 ] presents numerical studies .",
    "proofs of the main results , theorems  [ thm3.1 ] and [ thm3.3 ] , are provided in section  [ sec6 ] ; in each case , we break down the key steps in a series of lemmas with proofs deferred to the appendix .",
    "we collect standard pieces of notation here for readers convenience . for two sequences @xmath22 and @xmath23 of positive numbers , we write @xmath24 or @xmath25 if there exists a constant @xmath26 such that @xmath27 for all sufficiently large @xmath9 ; we write @xmath28 if there exist constants @xmath29 such that , for all @xmath9 large enough , @xmath30 ; and we write @xmath31 if @xmath32 , respectively . for @xmath33 , we write @xmath34 .    for every positive integer @xmath35 , we write @xmath36= \\{1 , 2 , \\ldots , \\ell\\}$ ] , and for any set @xmath37 , we use @xmath38 to denote its complement and @xmath39 for its cardinality .",
    "for any real - valued random variable @xmath40 , its sub - gaussian norm is defined by @xmath41 .",
    "we say that a random variable @xmath40 is sub - gaussian if @xmath42 .",
    "let @xmath43 be two positive integers .",
    "for every @xmath44-vector @xmath45 , we define its @xmath46-norm to be @xmath47 , and set @xmath48 .",
    "let @xmath49 be the unit sphere in @xmath50 .",
    "moreover , for each subset @xmath51 $ ] with @xmath52 $ ] , we denote by @xmath53 the @xmath10-variate sub - vector of @xmath54 containing only the coordinates indexed by @xmath37 .",
    "we use @xmath55 to denote the spectral norm of a matrix @xmath56 .",
    "let @xmath57 be independent and identically distributed ( i.i.d . )",
    "random variables with mean zero and variance @xmath58 , and @xmath59 be i.i.d .",
    "@xmath44-dimensional random vectors .",
    "we write @xmath60 for @xmath61 $ ] , the maximum @xmath10-multiple correlation between @xmath1 and @xmath11 is given by @xmath62 where @xmath63 denotes the sample pearson correlation coefficient .",
    "when @xmath1 and @xmath11 are independent , we regard @xmath64 as the maximum spurious ( multiple ) correlation .",
    "the limiting distribution of @xmath64 is studied in @xcite and @xcite when @xmath65 and @xmath66 ( the standard normal distribution in @xmath50 ) , and later in @xcite under a general setting where @xmath67 and @xmath11 is sub - gaussian with an arbitrary covariance matrix .    for binary data , the sample pearson correlation is not effective for measuring the regression effect .",
    "we need a new metric . in classical regression analysis ,",
    "the multiple correlation coefficient , also known as the @xmath68 , is the proportion of variance explained by the regression model . for each submodel",
    "@xmath69 $ ] , its @xmath68 statistic can be computed as @xmath70 then , the maximum @xmath10-multiple correlation @xmath64 can be expressed as the maximum @xmath68 statistic : @xmath71 : |s| = s } r_s^2 .\\end{aligned}\\ ] ]    the concept of @xmath68 can be extended to more general models . for binary response models , @xcite suggested the following generalization : @xmath72 where @xmath73 and @xmath74 denote the log - likelihoods of the fitted and the null model , respectively .",
    "this motivates us to use the likelihood ratio as a generalization of the goodness of fit beyond the linear model .",
    "let @xmath75 , @xmath76 be the negative logarithm of a quasi - likelihood process of the sample @xmath77 . for a given model size",
    "@xmath78 $ ] , the best subset fit is @xmath79 .",
    "the goodness of such a fit , in comparison with the baseline fit @xmath80 , can be measured by @xmath81 when @xmath11 and @xmath1 are independent , it becomes the goodness of spurious fit ( gosf ) .",
    "according to and , this definition is consistent with the maximum spurious correlation when it is applied to the linear model with gaussian quasi - likelihood , where @xmath82 and @xmath83 .    throughout ,",
    "we refer to @xmath84 as the loss function which is assumed to be convex .",
    "this setup encompasses the generalized linear models @xcite with @xmath85 under the canonical link where @xmath86 is a model - dependent convex function ( we take the dispersion parameter as one , as we do nt consider the dispersion issue ) , robust regression with @xmath87 , the hinge loss @xmath88 in the support vector machine @xcite and exponential loss @xmath89 in adaboost @xcite in classification with @xmath1 taking values @xmath90 .",
    "the prime goal of this paper is to derive the limiting laws of gosf @xmath8 in the null setting where the response @xmath1 and the explanatory variables @xmath11 are independent . here",
    ", both @xmath10 and @xmath44 can depend on @xmath9 , as we shall use double - array asymptotics .",
    "we will mainly focus on the glim and robust linear regression that are of particular interest in statistics .",
    "recall that @xmath91 are i.i.d .",
    "copies of @xmath92 .",
    "assume that the conditional distribution of @xmath93 given @xmath94 belongs to the canonical exponential family with the probability density function taking the form @xcite @xmath95 , \\end{aligned}\\ ] ] where @xmath96 is the unknown @xmath44-dimensional vector of regression coefficients , and @xmath97 is the dispersion parameter .",
    "the log - likelihood function with respect to the given data @xmath98 is @xmath99 . for simplicity , we take @xmath100 with the exception that in the linear model with gaussian noise , @xmath101 is the variance .",
    "two other showcases are    1 .   logistic regression : @xmath102 , @xmath103 and @xmath100 .",
    "poisson regression : @xmath104 , @xmath105 and @xmath100 .    in glim ,",
    "the loss function is @xmath106 .",
    "by , the generalized measure of goodness of fit for glim is @xmath107 in section  [ sec3 ] , we derive under mild regularity conditions the limiting distribution of gosf @xmath108 in the null model .",
    "this extends the classical wilks theorem @xcite . here",
    ", we interpret @xmath109 as the degree of spuriousness caused by the high - dimensionality .      in this section",
    ", we revisit the high - dimensional linear model @xmath110 where @xmath111 is the response vector and @xmath112 is the @xmath9-vector of measurement errors .",
    "robustness considerations lead to least absolute deviation ( lad ) regression and more generally quantile regression @xcite . for simplicity , we consider the @xmath113-loss @xmath114 , @xmath115 .",
    "the generalized measure of goodness of fit now becomes @xmath116 the limiting distribution of gosf @xmath117 is studied in section  [ sec3.4 ] .    in particular , if @xmath118 in are i.i.d .",
    "from the double exponential distribution with the density @xmath119 , @xmath105 , the @xmath113-loss @xmath120 corresponds to the negative log - likelihood function . in general",
    ", we assume that the regression error @xmath121 has median zero , that is , @xmath122 .",
    "hence , the conditional median of @xmath123 given @xmath124 is @xmath125 for @xmath126 $ ] , and @xmath127 , where @xmath128 denotes the conditional expectation given @xmath129 .",
    "the computation of the best subset regression coefficient @xmath130 in requires solving a combinatorial optimization problem with a cardinality constraint , and therefore is np - hard . in the following ,",
    "we suggest a fast and easily implementable method , which combines the forward selection ( stepwise addition ) algorithm and a local adaptive majorization - minimization ( lamm ) algorithm @xcite to provide an approximate solution .    our optimization problem is @xmath131 , where @xmath132 .",
    "we say that a function @xmath133 majorizes @xmath134 at the point @xmath135 if @xmath136 and @xmath137 for all @xmath115 .",
    "an majorization - minimization ( mm ) algorithm initializes at @xmath138 and then iteratively computes @xmath139 . the target value of such an algorithm is non - increasing since @xmath140    we now majorize @xmath134 at @xmath141 by an isotropic quadratic function @xmath142 this is a valid majorization as long as @xmath143 ( this will be relaxed below ) .",
    "the isotropic form on the right - hand side of ( [ eq2.10 ] ) allows a simple analytic solution given by @xmath144}. { \\nonumber}\\end{aligned}\\ ] ] here , we used the notation that for any @xmath115 , @xmath145}\\in \\bbr^p$ ] retains the @xmath10 largest ( in magnitude ) entries of @xmath146 and assigns the rest to zero .",
    "[ rmk1 ]    to implement the mm algorithm , we need to compute the gradient of the objective function of interest . in the @xmath0 regression ,",
    "the loss function @xmath114 , @xmath115 is not differentiable everywhere .",
    "recall that the subdifferential of the absolute function @xmath147 , @xmath148 is given by @xmath149 h(x ) = \\ {    ll \\ { 1 } , & x>0 , + @xmath150 -1 , 1 @xmath151 $ ] , & x=0 , + \\ { -1 } , & x < 0 .    .",
    "@xmath149 with slight abuse of notation , we suggest a randomized algorithm using the stochastic subgradient @xmath152 where @xmath153 are i.i.d .",
    "random variables uniformly distributed on @xmath154 $ ] .",
    "we propose to use the stepwise forward selection algorithm to compute an initial estimator @xmath155 . as the mm algorithm decreases the target value as shown in ( [ eq2.9 ] ) , the resulting target value is no larger than that produced by the stepwise forward selection algorithm .    to properly choose the isotropic parameter @xmath156 without computing the maximum eigenvalue",
    ", we use the local adaptive procedure as in @xcite . note that , in order to have a non - increasing target value , the majorization is not actually required .",
    "as long as @xmath157 , arguments in ( [ eq2.9 ] ) hold . starting from a prespecified value @xmath158 ,",
    "we successfully inflate @xmath159 by a factor @xmath160 .",
    "after the @xmath35th iteration , @xmath161 .",
    "we take the first @xmath35 such that @xmath162 and set @xmath163 .",
    "such an @xmath35 always exists as a large @xmath35 will major the function @xmath164 .",
    "we then continue with the iteration in the mm part .",
    "a simple criteria for stopping the iteration is that @xmath165 for a sufficiently small @xmath166 , say @xmath167 .",
    "we refer to @xcite for a detailed computational complexity analysis of the lamm algorithm .",
    "while the lamm algorithm can be applied to compute @xmath130 in a general setting , in our application , the algorithm is mainly applied to compute gosf under the null model ( see figure  [ fig0 ] and section  [ sec3.5 ] ) . from our simulation experiences , our algorithm delivers a good enough solution under the null model .",
    "it always provides an upper certificate @xmath168 to the problem @xmath169 , where @xmath170 is the output of the lamm algorithm . as in @xcite ,",
    "if needed to verify the accuracy of our method , a lower certificate is @xmath171 , where @xmath172 is the solution to the convex problem @xmath173 , and @xmath174 is a sufficient large constant so that the @xmath175-solution satisfies @xmath176 . for example , under the null model , it is well known that @xmath177 .",
    "therefore , we can take @xmath178 for a sufficiently large constant @xmath179 .",
    "a data - driven heuristic approach is to take @xmath180 along the lasso path such that @xmath181 .",
    "note that the minimum target value falls in the interval @xmath182 $ ] .",
    "if this interval is very tight , we have certified that @xmath170 is an accurate solution .",
    "define @xmath183 covariance matrices @xmath184 for @xmath61 $ ] , we say that @xmath51 $ ] is an @xmath10-subset if @xmath185 . for every @xmath10-subset",
    "@xmath186 $ ] , let @xmath187 and @xmath188 be the @xmath189 sub - matrices of @xmath190 and @xmath191 containing the entries indexed by @xmath192 , that is , @xmath193    [ cond3.1 ] the covariates are standardized to have unit second moment , that is , @xmath194 for @xmath195",
    ". there exits a random vector @xmath196 satisfying @xmath197 , such that @xmath198 and @xmath199 .    for @xmath200 ,",
    "the @xmath10-sparse condition number of @xmath190 is given by @xmath201 where @xmath202 and @xmath203 denote the @xmath10-sparse largest and smallest eigenvalues of @xmath190 , respectively .",
    "let @xmath204 be a centered gaussian random vector with covariance matrix @xmath190 .",
    "for any @xmath10-subset @xmath69 $ ] , @xmath205 . define the random variable @xmath206 : |s|=s } \\|   \\bsigma_{ss}^{-1/2 } \\bg_s \\|_2 , \\label{eq3.4}\\end{aligned}\\ ] ] which is the maximum of the @xmath207-norms of a sequence of dependent chi - squared random variables with @xmath10 degrees of freedom .",
    "the distribution of @xmath208 depends on the unknown @xmath190 and can be estimated by the multiplier bootstrap in section  [ sec3.5 ] .",
    "it will be shown that this distribution is the asymptotic distribution of gosf .",
    "in particular , for the isotropic case where @xmath209 , @xmath210 , the sum of the largest @xmath10 order statistics of @xmath44 independent @xmath211 random variables .      for i.i.d .",
    "observations @xmath212 from the distribution in , define individual residuals @xmath213 with conditional variance @xmath214 , where @xmath215 . in particular , under the null model , @xmath1 is independent of @xmath11 with mean @xmath216 and variance @xmath217 .",
    "[ cond3.3 ] there exists @xmath218 such that @xmath219 holds for all @xmath220 .",
    "the function @xmath86 in satisfies @xmath221 for some constants @xmath222 .",
    "condition  [ cond3.3 ] is satisfied by a wide class of glims , including the logistic and poisson regression models .",
    "the following theorem shows that , under certain moment and regularity conditions , the distribution of the generalized likelihood ratio statistic @xmath12 can be consistently approximated by that of @xmath223 given in .",
    "[ thm3.1 ] let conditions  [ cond3.1 ] and [ cond3.3 ] be satisfied .",
    "assume that @xmath100 in , @xmath224 and @xmath225 .",
    "then , under the null model with @xmath226 , @xmath227 , \\label{eq3.6}\\end{aligned}\\ ] ] where @xmath26 is a constant depending only on @xmath228 in conditions  [ cond3.1 ] and [ cond3.3 ] .    we regard theorem  [ thm3.1 ] as a nonasymptotic , high - dimensional version of the celebrated wilks theorem . in the low - dimensional setting",
    "where @xmath229 is fixed , theorem  [ thm3.1 ] reduces to the conventional wilks theorem , which asserts that the generalized likelihood ratio statistic converges in distribution to @xmath230 .",
    "in addition , we also provide a berry - esseen bound in .      as a specific case of glim",
    ", we consider the linear regression model with the loss function @xmath231 .",
    "the corresponding likelihood ratio statistic @xmath232 then coincides with that in with @xmath233 .",
    "we state the null limiting distribution of @xmath8 in a general case , where @xmath118 are i.i.d .",
    "copies of a sub - gaussian random variable @xmath234 .",
    "specifically , we assume that    [ cond3.2 ] @xmath234 is a centered , sub - gaussian random variable with @xmath235 and @xmath236 .",
    "moreover , write @xmath237 for @xmath238 .",
    "the following corollary is a particular case of the general result theorem  [ thm3.1 ] with @xmath233 , @xmath105 and @xmath101 . by examining the proof of theorem  [ thm3.1 ] and noting that @xmath239 , it can be easily shown that the second term on the right - side of vanishes .",
    "hence , the proof is omitted .",
    "[ cor3.2 ] let conditions  [ cond3.1 ] and [ cond3.2 ] hold .",
    "assume that @xmath240 and @xmath241 .",
    "then , under the null model with @xmath7 , @xmath242 where @xmath26 is a constant depending only on @xmath243 and @xmath244 in conditions  [ cond3.1 ] and [ cond3.2 ] .    [ rmk3.3 ] _ under the null model , the variance @xmath245 can be consistently estimated by @xmath246 , where @xmath247 . under the same conditions of corollary  [ cor3.2 ] , it can be proved that @xmath248 which is in line with theorem  3.1 in @xcite . to see this , note that @xmath249 : |s| = s } \\min_{\\btheta \\in \\bbr^s } \\| \\by - { \\mathbb{x}}_s \\btheta \\|_2 ^ 2 { \\nonumber}\\\\ & =   \\max_{s\\subseteq [ p ] : |s| = s }    \\by^\\t   { \\mathbb{x}}_s ( { \\mathbb{x}}_s^\\t { \\mathbb{x}}_s)^{-1 } { \\mathbb{x}}_s^{\\t } \\by =     \\max_{\\balpha \\in \\bbr^p : \\| \\balpha\\|_0 \\leq s }   (   \\by^\\t { \\mathbb{x}}\\balpha ) ^2 / \\|   { \\mathbb{x}}\\balpha \\|_2 ^ 2 .   { \\nonumber}\\end{aligned}\\ ] ] the estimator @xmath250 , used in computing the maximum spurious correlation , can be seriously biased beyond the null model and hence adversely affect the power .",
    "thus , we suggest using either the refitted cross - validation procedure @xcite or the scaled lasso estimator @xcite to estimate @xmath245 .",
    "_      we now state an analogous result to theorem  [ thm3.1 ] regarding the @xmath113-loss considered in section  [ sec2.2 ] .",
    "[ cond3.4 ] the noise @xmath251 in are i.i.d .",
    "copies of a random variable @xmath234 satisfying @xmath252 for some @xmath253 .",
    "there exist positive constants @xmath254 , @xmath255 and @xmath256 such that the distribution function @xmath257 and the density function @xmath258 of @xmath234 satisfy @xmath259    [ thm3.3 ] if @xmath224 and @xmath225 , then under the null model with @xmath226 and conditions  [ cond3.1 ] and [ cond3.4 ] , we have @xmath260 ,    \\label{eq3.10}\\end{aligned}\\ ] ] where @xmath8 is given by , @xmath261 is a constant depending on @xmath262 , @xmath263 , @xmath264 , @xmath265 and @xmath266 is a constant depending on @xmath267 and @xmath256 in conditions  [ cond3.1 ] and [ cond3.4 ] .    [ rmk3.2 ] under the null model , the unknown parameter @xmath268 can be consistently estimated by the kernel density estimator @xmath269 , where @xmath270 is a kernel function and @xmath271 is the bandwidth . for simplicity , we may use the epanechnikov kernel function @xmath272 along with the rule - of - thumb bandwidth @xmath273 , where @xmath274 .      the distribution of the random variable @xmath208 given by depends on the unknown covariance matrix @xmath190 .",
    "in practice , it is natural to replace @xmath190 by @xmath275 and @xmath276 by @xmath277 in the definition of @xmath208 . with this substitution",
    ", the distribution of @xmath208 can be simulated .",
    "in particular , @xmath278 can be simulated as @xmath279 , where @xmath280 are i.i.d .",
    "standard normal random variables that are independent of @xmath281 .",
    "the resulting estimator is @xmath282 : |s| = s } \\| { \\widehat}{\\bsigma}^{-1/2}_{ss } { \\widehat}{\\bg}_s \\|_2 ,   \\label{eq3.11}\\end{aligned}\\ ] ] which is a multiplier bootstrap version of @xmath208 .",
    "the following proposition follows directly from theorem  3.2 in @xcite .",
    "[ prop3.1 ] assume that condition   holds , @xmath225 and @xmath283 as @xmath284 .",
    "then @xmath285 in probability .",
    "the computation of @xmath286 requires solving a combinatorial optimization .",
    "this can be alleviated by using the lamm algorithm in section  [ sec2.3 ] . to begin with , by remark  [ rmk3.3 ]",
    ", we write @xmath286 in as @xmath287 : |s|=s }   \\mathbf{e}^\\t { \\mathbb{x}}_s    ( { \\mathbb{x}}_s^\\t { \\mathbb{x}}_s)^{-1 } { \\mathbb{x}}_s^\\t \\mathbf{e } =   \\| \\be \\|_2 ^ 2 - \\min _ { \\bbeta    \\in \\bbr^p : \\| \\bbeta \\|_0\\leq s } \\| \\be - { \\mathbb{x}}\\bbeta \\|_2 ^ 2 ,   { \\nonumber}\\ ] ] where @xmath288 and @xmath289 for every subset @xmath69 $ ] .",
    "this can be computed approximately by the lamm algorithm in section  [ sec2.3 ] , resulting in the solution @xmath290 . finally , we set @xmath291",
    ".    the numerical performance may be improved by employing mixed integer optimization formulations @xcite .",
    "such an attempt , however , is beyond the scope of the paper and we leave it for future research .",
    "based on the theoretical developments in section  [ sec3 ] , here we address the question whether discoveries by machine learning and data mining techniques for glim are any better than by chance . for simplicity , we focus on the lasso .",
    "let @xmath292 be the upper @xmath293-quantile of the random variable @xmath208 defined by .",
    "assume that the dispersion parameter @xmath294 in equals 1 . by theorem  [ thm3.1 ]",
    ", we see that for any prespecified @xmath295 , @xmath296 where @xmath8 is as in .",
    "let @xmath297 be the @xmath113-penalized maximum likelihood estimator with @xmath298 , where @xmath156 is the regularization parameter .",
    "the goodness of fit is likelihood ratio @xmath299 .",
    "since @xmath300 covariates are selected , it should be compared with the distribution of gosf @xmath8 by taking @xmath301 . in view of ,",
    "if @xmath302 then we may regard the discovery of variables @xmath303 as unimpressive , no better than fitting by chance , or simply spurious .    in practice",
    ", the unknown quantile @xmath304 should be replaced by its bootstrap version @xmath305 , the upper @xmath293-quantile of @xmath286 defined by .",
    "this leads to the following data - driven criteria for judging where the discovery @xmath306 is spurious : @xmath307 the theoretical justification is given by theorem  [ thm3.1 ] and proposition  [ prop3.1 ] .",
    "in particular , when the loss is quadratic , this reduces to the case studied by @xcite .",
    "the concept of gosf and its theoretical quantile provide important guidelines for model selection .",
    "let @xmath308 be a cross - validated lasso estimator , which selects @xmath309 important variables .",
    "due to the bias of the @xmath113 penalty , the lasso typically selects far larger model size since the visible bias in lasso forces the cross - validation procedure to choose a smaller value of @xmath159 .",
    "this phenomenon is documented in the simulations studies .",
    "see table  [ tab1 ] in section  [ sec5.2 ] .",
    "with an over - selected model , both the goodness of fit @xmath310 and the spurious fit can be very large , and so is the finite sample wilks approximation error . to avoid over - selecting ,",
    "we suggest an alternative procedure that uses the quantity @xmath305 as a guidance to choose the tuning parameter , which guards us from spurious discoveries .",
    "more specifically , for each @xmath159 in the lasso solution path , we compute @xmath311 and @xmath312 with a prespecified @xmath293 . starting from the largest @xmath159",
    ", we stop the lasso path the first time that the sign of @xmath313 is changed from positive to negative , and let @xmath314 be the smallest @xmath159 satisfying @xmath315 . denote by @xmath316 the corresponding selected model size .",
    "this value can be regarded as the maximum model size for lasso ( or any other variable selection technique such as scad ) to choose from .",
    "another viable alternative is to only select the best cross - validated model among those whose fit are better than gosf .",
    "we will show in section  [ sec5.2 ] by simulation studies that this procedure selects much smaller model size which is closer to the truth .",
    "first we ran a simulation study to examine how accurate the gaussian approximation @xmath317 is to the generalized likelihood ratio statistic @xmath318 in the null model . to illustrate the method , we focus on the logistic regression model : @xmath319 . under the null model @xmath320 , @xmath321 are i.i.d .",
    "bernoulli random variables with success probability @xmath322 .",
    "independent of @xmath123 s , we generate @xmath323 with two different covariance matrices : @xmath324 and @xmath325 , where @xmath326 the first design has an ar(1 ) correlation structure ( a short - memory process ) , whereas the second design reflects strong long memory dependence .",
    "we take @xmath327 in both cases .",
    ", @xmath328 and @xmath329 when @xmath190 is equal to @xmath330 ( upper panel ) or @xmath331 ( lower panel ) .",
    "[ fig1 ] ]    figure  [ fig1 ] reports the distributions of generalized likelihood ratios ( glrs ) and their gaussian approximations ( gars ) when @xmath332 , @xmath333 and @xmath334 .",
    "the results show that the accuracy of gaussian approximation is fairly reasonable and is affected by the size of @xmath10 as well as the dependence between the coordinates of @xmath11 .      in this section , we conduct a moderate scale simulation study to examine how effective the multiplier bootstrap quantile @xmath305 serves as a benchmark for judging whether the discovery is spurious . to illustrate the main idea , again we restrict our attention to the logistic regression model and the lasso procedure .",
    "the results reported here are based on 200 simulations with the ambient dimension @xmath335 and the sample size @xmath9 taken values in @xmath336 .",
    "the true regression coefficient vector @xmath337 is @xmath338 .",
    "we consider two random designs : @xmath339 ( independent ) and @xmath340 ( dependent ) .",
    "let @xmath308 be the five - fold cross - validated lasso estimator , which selects a model of size @xmath309 . for a given @xmath341 ,",
    "consider the spurious discovery probability ( sdp ) @xmath342 which is basically the probability of the type ii error since the simulated model is not null .",
    "we take @xmath343 and compute the empirical sdp based on 200 simulations . for each simulated data set ,",
    "@xmath344 is computed based on 1000 bootstrap replications .",
    "the results are depicted in table  [ tab1 ] below .    as reflected by table  [ tab1 ] , the empirical power , which is one minus the empirical sdp , increases rapidly as the sample size @xmath9 grows .",
    "this is in line with our intuition that the more data we have , the less likely that the discovery by a variable selection method is spurious .",
    "when the sample size is small , the sdp can be high and hence the discovery @xmath345 should be interpreted with caution .",
    "we need either more samples or more powerful variable selection methods .",
    "we see from table  [ tab1 ] that the lasso with cross - validation selects far larger model size than the true one , which is 5 .",
    "this is because the intrinsic bias in lasso forces the cross - validation procedure to choose a smaller value of @xmath159 .",
    "we now use our procedure in section  [ sec4 ] to choose the tuning parameter from the lasso solution path . as before ,",
    "we take @xmath343 in @xmath346 to provide an upper bound on the model size from perspective of guarding against spurious discoveries .",
    "the empirical median of @xmath316 and its robust standard deviation are 9 and 1.87 over 200 simulations when @xmath347 and @xmath348 . the feature over - selection phenomenon is considerably alleviated .      in this section ,",
    "we apply the idea of detecting spurious discoveries to the neuroblastoma data reported in @xcite .",
    "this data set consists of 251 patients of the german neuroblastoma trials nb90-nb2004 , diagnosed between 1989 and 2004 .",
    "the complete data set , obtained via the microarray quality control phase - ii ( maqc - ii ) project @xcite , includes gene expression over 10,707 probe sites .",
    "there are 246 subjects with 3-year event - free survival information available ( 56 positive and 190 negative ) .",
    "see @xcite for more details about the data sets .    for each @xmath156 , we apply lasso using the logistic regression model to select @xmath300 genes .",
    "in particular , ten - fold cross - validated lasso selects @xmath349 genes .",
    "then we calculate the goodness of fit @xmath350 . along the lasso path , we record in table  [ tab2 ] the number of selected probes , the corresponding square - root the goodness of fit @xmath351 and upper @xmath293-quantiles of the multiplier bootstrap approximations @xmath352 with @xmath353 and @xmath354 based on 2000 bootstrap replications .",
    "for illustrative purposes , we only display partial lasso solutions with selected model size @xmath300 lying between 20 and 40 . from table",
    "[ tab2 ] , we observe that only the discovery of 17 probes has a generalized measure of the goodness of fit better than gosf at @xmath355 , whereas the finding ( of the 40 probes ) via the cross - validation procedure is likely to over - select .",
    "we now turn to the proofs of theorems  [ thm3.1 ] and [ thm3.3 ] . in each proof",
    ", we provide the primary steps , with more technical details stated as lemmas and proved in the appendix .      throughout",
    ", we work with the quasi - likelihood @xmath356 and consider the general case where the dispersion parameter @xmath294 in is specified ( not necessarily equals 1 to facilitate the derivations for the normal case ) . for",
    "a given @xmath78 $ ] , define @xmath357 we divide the proof into three steps . first , for each @xmath10-subset @xmath69 $ ] , we prove wilks s result for the @xmath37-restricted model where only a subset of the covariates indexed by @xmath37 are included . specifically , we show that the square root deviation of the @xmath37-restricted maximum log - likelihood from its baseline value under the null model can be well approximated by the @xmath207-norm of the normalized score vector .",
    "second , based on a high - dimensional invariance principle , we prove the gaussian / chi - squared approximation for the maximum of the @xmath207-norms of normalized score vectors .",
    "finally , we apply an anti - concentration argument to construct non - asymptotic wilks approximation for @xmath358 .    .",
    "in the null model where @xmath1 and @xmath11 are independent , the true parameter @xmath4 in is zero , and thus the density function of @xmath1 has the form @xmath359 .",
    "moreover , we have @xmath360 to this see , note that in model with @xmath361 , @xmath362 and @xmath363 .",
    "this implies that @xmath364 .",
    "this function is strictly concave with respect to @xmath146 and @xmath365 satisfies its first order condition , and hence is its maximizer .    for each @xmath10-subset",
    "@xmath51 $ ] , define the @xmath37-restricted log - likelihood @xmath366 and the score function @xmath367 , @xmath368 . in this notation",
    ", it can be seen from that @xmath369 : |s|=s } \\max _ {   \\btheta\\in   \\bbr^s } { \\cal l}_n^s ( \\btheta ) =   \\max_{s\\subseteq[p ] : |s|=s } { \\cal l}_n^s ( { \\widehat}{\\btheta}_s )   ,   \\label{qn.equiv}\\end{aligned}\\ ] ] where @xmath370 denotes the maximum likelihood estimate of the target parameter for the @xmath37-restricted model , which is given by @xmath371 .",
    "given the i.i.d .",
    "observations @xmath77 , @xmath372 and @xmath373 for @xmath374 . in particular ,",
    "write @xmath375 for @xmath376 as in .",
    "further , define the @xmath37-restricted normalized score @xmath377    the following result is a conditional analogue of corollary  1.12 in the supplement of @xcite , which provides an exponential inequality for the @xmath207-norm of @xmath378 given @xmath281 .",
    "the proofs of this lemma and other lemmas can be found in the appendix .",
    "[ quadratic.concentration.lemma ] assume that conditions  [ cond3.1 ] and [ cond3.3 ] hold .",
    "then , for every @xmath379 , @xmath380 holds almost surely on the event @xmath381 , where @xmath382    the following lemma characterizes the wilks phenomenon from a non - asymptotic perspective .",
    "recall that @xmath383 at is the @xmath37-restricted maximum likelihood estimator , and in the null model , @xmath384 , @xmath385 . for every @xmath386 ,",
    "define the event @xmath387 : |s|=s }   \\bigg\\ { { \\widehat}{\\bsigma}_{ss }   \\succ { \\mathbf{0 } } ,   \\,\\max_{1\\leq i\\leq n }   \\bx_{i s}^\\t { \\widehat}{\\bsigma}_{ss}^{-1 } \\bx_{i s }   \\leq   \\tau   \\bigg\\ } .",
    "\\label{event1.def}\\end{aligned}\\ ] ]    [ wilks.approximation ] assume that conditions  [ cond3.1 ] and [ cond3.3 ] hold .",
    "then , on the event @xmath388 , for any @xmath389 , @xmath390 : |s| = s }   \\big| [ 2 \\ { { \\cal l}_n^s({\\widehat}{\\btheta}_s ) - { \\cal l}_n({\\mathbf{0 } } ) \\ }   ] ^{1/2 }   -    \\|   { \\widehat}\\bxi_s \\|_2    \\big|   \\leq c_1 \\ , \\phi \\tau^{1/2 } \\frac{s\\log(pn)}{\\sqrt{n } }    \\ , \\bigg ) \\leq 5 n^{-1 }       \\label{wilks.concentration}\\end{aligned}\\ ] ] whenever @xmath391 , where @xmath392 and @xmath393 are positive constants depending only on @xmath394 and @xmath395 .",
    "to apply lemma  [ wilks.approximation ] , we need to show first that for properly chosen @xmath396 , the event @xmath397 occurs with high probability .",
    "first , applying theorem 5.39 in @xcite to the random vectors @xmath398 yields that , for every @xmath399 , @xmath400 holds with probability at least @xmath401 , where @xmath402 , and @xmath403 is a constant depending only on @xmath243 .",
    "this , together with boole s inequality implies by taking @xmath404 that , with probability at least @xmath405 , @xmath406 : |s|=s } \\big\\| \\bsigma_{ss}^{-1/2 } { \\widehat}\\bsigma_{ss } \\bsigma_{ss}^{-1/2 } - \\bi_{s } \\big\\| \\leq c_3 \\bigg ( \\frac{s\\log\\frac{ep}{s } + \\log n}{n } \\bigg)^{1/2 } \\leq \\frac{1}{2 } \\label{uniform.hats.positivity}\\end{aligned}\\ ] ] whenever @xmath407 .",
    "providing holds , the smallest eigenvalue of @xmath408 is bounded from below by @xmath409 so that @xmath410",
    ". moreover , @xmath411 for the last term on the right - hand side of , let @xmath412 be the unit vector in @xmath50 with 1 at the @xmath413th position and note that @xmath414 with @xmath415 , where @xmath416 are i.i.d .",
    "@xmath44-dimensional random vectors with covariance matrix @xmath417 . by condition  [ cond3.1 ] , @xmath418 and hence for every @xmath399 , @xmath419 where @xmath420 is a constant depending only on @xmath243 .",
    "this , together with implies by taking @xmath421 that , with probability at least @xmath422 , @xmath423 : |s|=s } \\bx_{is}^\\t { \\widehat}\\bsigma_{ss}^{-1 } \\bx_{is }   \\leq 2 \\lambda^{-1}_{\\min}(s )     \\ { 1 + 2c_4 \\ , s\\log(pn )   \\}. \\label{quadratic.ubd2}\\end{aligned}\\ ] ]    now , by and , we take @xmath424 such that the event @xmath425 occurs with probability greater than @xmath426 as long as @xmath427 .",
    "this , together with lemma  [ wilks.approximation ] yields that with probability at least @xmath428 , @xmath429 : |s|=s } \\big|   [ 2   \\ { { \\cal l}_n^s({\\widehat}{\\btheta}_s ) - { \\cal l}_n({\\mathbf{0 } } ) \\ }    ] ^{1/2 }   -   \\| { \\widehat}\\bxi_s \\|_2 \\big|   \\leq c_5 \\ ,   \\phi \\lambda^{-1/2}_{\\min}(s )      \\{s\\log(pn)\\}^{3/2 }   n^{-1/2 }    \\label{uniform.wilks.approxi}\\end{aligned}\\ ] ] whenever @xmath430 , where @xmath431 are constants depending only on @xmath432 and @xmath395 .    .",
    "for any @xmath433 and @xmath51 $ ] , define @xmath434 and @xmath435 such that @xmath436 .",
    "moreover , define @xmath437 the following result shows that for each @xmath10-subset @xmath69 $ ] , the @xmath207-norm of the @xmath37-restricted normalized score @xmath438 is close to that of @xmath439 with overwhelmingly high probability .",
    "[ gar.lem1 ] assume that condition  [ cond3.1 ] holds .",
    "then , for every @xmath10-subset @xmath69 $ ] and for every @xmath440 ,",
    "@xmath441 \\leq 12.4\\,e^{-t } , \\label{quadratic.approxi}\\end{aligned}\\ ] ] provided that @xmath442 , where @xmath443 is as in and @xmath444 are constants depending only on @xmath445 and @xmath243 .    using the union bound and taking @xmath446 in lemma  [ gar.lem1 ]",
    ", we see that with probability at least @xmath447 , @xmath448 : |s|=s } \\big|   \\| { \\widehat}{\\bxi}_s \\|_2   - \\|   \\bxi_s \\|_2   \\big|   \\leq c_7 \\ ,",
    "\\phi^{1/2 }   (   s\\log \\tfrac{ep}{s } + \\log n )   \\ , n^{-1/2 }   \\label{unif.quadratic.approxi}\\end{aligned}\\ ] ] whenever @xmath449 .    note that , the random vectors @xmath450 and @xmath451 $ ] defined in satisfy @xmath452 , @xmath453 , @xmath454 and @xmath455 .",
    "the following lemma provides a coupling inequality , showing that the random variable @xmath456 : |s|=s }   \\| \\phi^{-1/2}\\bxi_s \\|_2 $ ] can be well approximated , with high probability , by some random variable which is distributed as the maximum of the @xmath207-norms of a sequence of normalized gaussian random vectors , that is , @xmath457 , |s| = s \\}$ ] .",
    "[ gar.lem2 ] assume that condition  [ cond3.1 ] holds .",
    "then , there exists a random variable @xmath458 such that for any @xmath459 $ ] , @xmath460 : |s| = s }   \\| \\phi^{-1/2 } \\bxi_s \\|_2 - t_0 \\bigg| \\leq c_{10 }   \\big [ \\delta +   \\ { s\\log(\\gamma_s pn ) \\}^{1/2 }   n^{-1/2 } +   \\{s\\log(\\gamma_s pn ) \\}^2 n^{- 3/2 }   \\big ]              \\label{gar.be}\\end{aligned}\\ ] ] holds with probability greater than @xmath461 $ ] , where @xmath462 are constants depending only on @xmath445 and @xmath243    _ * step  3 : completion of the proof*_. we now apply an anti - concentration argument to construct the berry - esseen bound for the square root of the excess @xmath463 . to this end , taking @xmath464 in lemma  [ gar.lem2 ] leads to that , with probability at least @xmath465 , @xmath466 : |s| = s }   \\| \\phi^{-1/2 } \\bxi_s \\|_2 - t_0 \\bigg|   \\leq c_{12 }   \\ { s\\log(\\gamma_s pn ) \\}^{3/8 } n^{-1/8 }   \\label{uniform.coupling}\\end{aligned}\\ ] ] whenever @xmath467 .",
    "further , for @xmath208 in , note that @xmath468 : |s| = s } \\max_{\\bu \\in \\mathbb{s}^{s-1 } } \\frac { ( \\bu^\\t \\bg_s ) ^2 } { \\bu^\\t \\bsigma_{ss } \\bu }      = \\max_{\\bu \\in \\mathcal{f}(s , p ) } \\frac { ( \\bu^\\t \\bg)^2 } { \\bu^\\t \\bsigma   \\bu } , { \\nonumber}\\end{aligned}\\ ] ] where @xmath469 and @xmath470 is a class of linear functions @xmath471 .",
    "hence , it follows from lemma  7.3 in @xcite with slight modification and lemma  a.1 in the supplement of @xcite that , for every @xmath472 , @xmath473 where @xmath474 is an absolute constant . combining with the preceding results , and proves .      the main strategy of the proof is similar to that of theorem  [ thm3.1 ] but technical details are substantially different .",
    "as before , we define the quasi - likelihood @xmath475 , @xmath115 , and observe that @xmath476 : |s|=s } \\max_{\\btheta \\in \\bbr^s } { \\cal l}_n^s(\\btheta)$ ] , where @xmath477 . in the null model with @xmath7 , we have for each @xmath10-subset @xmath51 $ ] , @xmath478 by the first order condition and concavity , and the @xmath37-restricted least absolute deviation estimator can be written as @xmath479    we first establish in lemma  [ lad.concentration ] an upper bound for the maximum @xmath207-risks of @xmath383 .",
    "[ lad.concentration ] assume that holds and that @xmath480 for some @xmath481",
    ". then , on the event @xmath388 for @xmath389 , the sequence of lad estimators @xmath482 , |s| = s\\}$ ] satisfies @xmath483 : |s| = s } \\| { \\widehat}{\\bsigma}_{ss}^{1/2 } { \\widehat}{\\btheta}_s \\|_2   \\leq   c_1 \\ ,   a_2^{-1 }   \\ {   s\\log(pn ) \\}^{1/2 }    n^{-1/2}\\end{aligned}\\ ] ] with conditional probability ( over the randomness of @xmath484 ) greater than @xmath485 , where @xmath486 are absolute constants and @xmath487 is a constant depending only on @xmath262 , @xmath263 , @xmath264 and @xmath265 .    based on lemma  [ lad.concentration ] ,",
    "we further study the concentration property of the wilks expansion for the excess @xmath488 .",
    "since the function @xmath489 is concave , we use @xmath490 to denote its subgradient . for @xmath368 , let @xmath491 be the stochastic component of @xmath492 .",
    "then , it is easy to see that @xmath493 where @xmath494 . in particular",
    ", we have @xmath495 . recall that @xmath496 and @xmath497 denote , respectively , the density function and the cumulative distribution function of @xmath234 . by the second expression in , @xmath498 and @xmath499 in line with , we have @xmath500 , which is the negative hessian of @xmath501 . as in ,",
    "define the normalized score @xmath502    the following result is a non - asymptotic , conditional version of the wilks theorem , saying that with high probability , the square root of the excess @xmath503 and the @xmath207-norm of the normalized score @xmath504 are sufficiently close uniformly over all @xmath10-subsets @xmath69 $ ] .",
    "[ wilks.approximation.2 ] assume that conditions  [ cond3.1 ] and [ cond3.4 ] are satisfied .",
    "then @xmath505 : |s|=s } \\big|   [ 2 \\ { { \\cal l}_n^s({\\widehat}{\\btheta}_s )   - { \\cal l}_n^s({\\mathbf{0 } } ) \\ } ] ^{1/2 } -   \\| { \\widehat}{\\bxi}_s \\|_2 \\big| { \\nonumber}\\\\   & \\qquad \\qquad   \\leq   c_2   \\ { f_{\\varepsilon}(0)\\}^{-1/2 } \\big [ \\lambda_{\\min}^{-1/2}(s )   \\ { s\\log(pn ) \\}^{3/2 }   n^{-1/2 }    + \\lambda_{\\min}^{-1/4}(s ) s\\log ( pn ) n^{-1/4 }   \\big ] \\label{wilks.approx.2}\\end{aligned}\\ ] ] holds with probability greater than @xmath506 whenever @xmath507 , where @xmath508 is a constant depending only on @xmath509 and @xmath256 , @xmath510 is as in lemma  [ lad.concentration ] , @xmath511 is an absolute constant and @xmath512 is a constant depending only on @xmath262 and @xmath255 .",
    "further , write @xmath513 and @xmath514 . note that @xmath515 are i.i.d .",
    "rademacher random variables and thus @xmath516 are sub - exponential random vectors . in this notation , we have @xmath517 . for each @xmath69 $ ] , define @xmath518 then , applying lemma  [ gar.lem1 ] with slight modification and the union bound we obtain that , with probability at least @xmath519 , @xmath520 : |s| = s } \\big| \\| { \\widehat}{\\bxi}_s \\|_2 - \\| \\bxi_s \\|_2 \\big| \\leq   c_4 \\",
    "{    f_\\varepsilon(0 )   \\}^{-1/2 }     s\\log(pn)\\ , n^{-1/2}\\end{aligned}\\ ] ] for all @xmath521 , where @xmath522 is an absolute constant and @xmath523 are constants depending only on @xmath243 .",
    "observe that @xmath524 = 0 $ ] and @xmath525 .",
    "hence , it follows from lemma  [ gar.lem2 ] that there exists a random variable @xmath526 such that for any @xmath459 $ ] , @xmath527 : |s| = s } \\|",
    "\\bxi_s \\|_2 - t_0 \\bigg| \\leq c_6   \\big [ \\delta +   \\ {   s\\log(\\gamma_s pn ) \\}^{1/2 } n^{-1/2 } +   \\{s\\log(\\gamma_s pn ) \\}^2 n^{-3/2 }   \\big ] \\label{gar.be.2}\\end{aligned}\\ ] ] holds with probability at least @xmath528 $ ] , where @xmath529 are constants depending only on @xmath243 .    finally , combining , , and proves .    0.2 in    39 [ 1]#1 [ 1]`#1 `",
    "urlstyle[1]doi : # 1    dimitris bertsimas , angela king , and rahul mazumder .",
    "best subset selection via a modern optimization lens . _",
    "the annals of statistics _ , 44(2):813852 , 2016 .",
    "peter bhlmann and sara van de geer .",
    "_ statistics for high - dimensional data : methods , theory and applications .",
    "_ springer - verlag , berlin heidelberg , 2011 .",
    "t. tony cai and tiefeng jiang .",
    "phase transition in limiting distributions of coherence of high - dimensional random matrices .",
    "_ journal of multivariate analysis _",
    ", 107:2439 , 2012 .",
    "t. tony cai , jianqing fan , and tiefeng , jiang .",
    "distributions of angles in random packing on spheres .",
    "_ journal of machine learning research _",
    ", 14:18371864 , 2013 .",
    "victor chernozhukov , denis chetverikov , and kengo kato .",
    "gaussian approximation of suprema of empirical processes .",
    "_ the annals of statistics _ , 42(4):15641597 , 2014 .    jianqing fan , shaojun guo , and ning hao . variance estimation using refitted cross - validation in ultrahigh dimensional regression .",
    "_ journal of the royal statistical society : series b ( statistical methodology ) _ , 74(1):3765 , 2012 .",
    "jianqing fan and runze li .",
    "variable selection via nonconcave penalized likelihood and its oracle properties .",
    "_ journal of the american statistical association _ , 96(456):13481360 , 2001 .",
    "jianqing fan , han liu , qiang sun , and tong zhang .",
    "tac for sparse learning : simultaneous control of algorithmic complexity and statistical error .",
    "_ arxiv preprint arxiv:1507.01037 _ , 2015 .",
    "jianqing fan , qi - man shao , and wen - xin zhou .",
    "are discoveries spurious ?",
    "distributions of maximum spurious correlations and their applications .",
    "_ arxiv preprint arxiv:1502.04237 _ , 2015 .",
    "yoav freund and robert e schapire",
    ". a decision - theoretic generalization of on - line learning and an application to boosting .",
    "_ journal of computer and system sciences _",
    ", 55(1):119139 , 1997 .",
    "trevor hastie , robert tibshirani , and martin wainwright _ statistical learning with sparsity : the lasso and generalizations . _ crc press , 2015 .",
    "roger koenker .",
    "_ quantile regression",
    ". _ cambridge university press , cambridge , 2005 .",
    "kenneth lange , david r. hunter , and ilsoon yang .",
    "optimization transfer using surrogate objective functions .",
    "_ journal of computational and graphical statistics _",
    ", 9(1):120 , 2000 .    gangadharrao s. maddala . _ limited - dependent and qualitative variables in econometrics . _ cambridge university press , cambridge , 1983 .",
    "peter mccullagh and john a. nelder .",
    "_ generalized linear models . _ chapman & hall / crc , london , 1989 .",
    "andr oberthuer , frank berthold , patrick warnat , barbara hero , yvonne kahlert , rdiger spitz , karen ernestus , rainer knig , stefan haas , roland eils , manfred schwab , benedikt brors , frank westermann , and matthias fischer .",
    "customized oligonucleotide microarray gene expression based classification of neuroblastoma patients outperforms current clinical risk stratification .",
    "_ journal of clinical oncology _",
    ", 24(31):50705078 , 2006 .    leming shi , et al .",
    "( maqc consortium ) . the microarray quality control ( maqc)-ii study of common practices for the development and validation of microarray - based predictive models . _ nature biotechnology _",
    ", 28(8):827841 , 2010 .",
    "vladimir spokoiny .",
    "parametric estimation .",
    "finite sample theory .",
    "_ the annals of statistics _ , 40(6):28772909 , 2012 .",
    "vladimir spokoiny .",
    "bernstein - von mises theorem for growing parameter dimension .",
    "_ arxiv preprint arxiv:1302.3430 _ , 2013 .",
    "vladimir spokoiny and mayya zhilova .",
    "bootstrap confidence sets under model misspecification . _",
    "the annals of statistics _ , 43(6):26532675 , 2015 .",
    "tingni sun and cun - hui zhang . scaled sparse linear regression .",
    "_ biometrika _ , 99(4):879898 , 2012 .",
    "gudmundur a. thorisson , albert v. smith , lalitha krishnan , and lincoln d. stein . the international hapmap project web site .",
    "_ genome research _ , 15:15921593 , 2005 .",
    "robert tibshirani .",
    "regression shrinkage and selection via the lasso .",
    "_ journal of the royal statistical society : series b ( statistical methodology ) _ , 58(1):267288 , 1996 .",
    "vladimir n. vapnik . _",
    "the nature of statistical learning theory . _ springer - verlag , new york , 1995 .",
    "roman vershynin .",
    "introduction to the non - asymptotic analysis of random matrices . in _",
    "compressed sensing : theory and applications _ , pages 210268 , cambridge university press , cambridge , 2012 .",
    "the @xmath0 penalized lad estimator for high dimensional linear regression .",
    "_ journal of multivariate analysis _",
    ", 120:135151 , 2013 .",
    "samuel s. wilks .",
    "the large - sample distribution of the likelihood ratio for testing composite hypotheses . _ the annals of mathematical statistics _",
    ", 9(1):6062 , 1938 .",
    "hui zou and runze li .",
    "one - step sparse estimates in nonconcave penalized likelihood models . _",
    "the annals of statistics _ , 36(4):15091533 , 2008 .",
    "in this appendix we prove the technical lemmas appeared in section  [ sec6 ] .        for every @xmath538 and @xmath103 , @xmath539 this verifies condition ( @xmath540 ) with @xmath541 in theorem  b.3 from the supplement of @xcite .",
    "consequently , taking @xmath542 and @xmath543 for some @xmath544 there , we have @xmath545 , @xmath546 , @xmath547 and @xmath548 .",
    "this implies that almost surely on the event @xmath381 , with conditional probability at least @xmath549 , @xmath550 finally , letting @xmath551 proves .",
    "we prove this lemma by applying the conditional version of theorem  2.3 in @xcite . to this end",
    ", we need to verify conditions ( @xmath540 ) , ( @xmath552 ) , ( @xmath553 ) , ( @xmath554 ) and ( @xmath555 ) . in line with",
    "the notation used therein , we fix @xmath69 $ ] and write @xmath556    the validity of ( @xmath540 ) is guaranteed from the proof of lemma  [ quadratic.concentration.lemma ] , and ( @xmath552 ) is automatically satisfied with @xmath557 since @xmath558 vanishes for all @xmath368 . turning to ( @xmath553 ) , observe that @xmath559 where @xmath560 lies between @xmath561 and @xmath562 . for @xmath563 , define @xmath564 . on the event @xmath388 for some @xmath389 and for @xmath565 , @xmath566 this together with implies that @xmath567      to verify ( @xmath570 ) , define @xmath571 so that @xmath572 and @xmath573",
    ". then , for any @xmath368 satisfying @xmath574 , it follows from the second - order taylor expansion that @xmath575 where @xmath560 is a point lying between @xmath561 and @xmath562 . on the event @xmath388",
    ", the right - hand side of is further bounded from below by @xmath576 when @xmath577 , @xmath578 is bounded from below by @xmath579 for @xmath580 as in .",
    "further , from the convexity of the function @xmath581 , we see that @xmath582 , for all @xmath583 satisfying @xmath584 .",
    "define the function @xmath585 as @xmath586 by definition , @xmath587 is non - decreasing in @xmath588 and for @xmath368 satisfying @xmath589 , @xmath590    with the above preparations , we apply theorem  2.3 in @xcite with slight modification on the constant . in view of and , set @xmath591^{1/2 } ,   \\label{def.r0.glim}\\end{aligned}\\ ] ] such that condition  2.3 there is satisfied on @xmath388 whenever @xmath592 .",
    "hence , it follows from theorem  2.3 in @xcite and the union bound that , conditional on the event @xmath388 , @xmath593 : |s|=s } \\big|   [   2\\ { { \\cal l}_n^s({\\widehat}{\\btheta}_s ) - { \\cal l}_n^s({\\mathbf{0 } } ) \\ }   ] ^{1/2 }   - \\| { \\widehat}{\\bxi}_s \\|_2 \\big| \\leq 5    \\delta(\\tau , r_0 ) r_0 \\bigg ) \\leq 5 n^{-1 } , \\end{aligned}\\ ] ] where @xmath594 and @xmath595 are as in and , respectively .",
    "this proves by properly choosing @xmath392 and @xmath393 .      to begin with , note that for each @xmath10-subset @xmath69 $ ] , @xmath596 are i.i.d .",
    "@xmath10-dimensional random vectors with mean zero and covariance matrix @xmath597 . by and",
    ", @xmath598 write @xmath599 , then @xmath600 is an @xmath601 matrix whose rows are independent sub - gaussian random vectors in @xmath602 .",
    "further , observe that @xmath603 and @xmath604 , where @xmath605 is a projection matrix . under condition",
    "[ cond3.1 ] , @xmath606 for @xmath607 .",
    "then , it follows from that for all sufficient large @xmath9 so that @xmath608 , @xmath609 and hence , @xmath610    next we upper bound the quadratic term @xmath611",
    ". first we show that @xmath612 are sub - exponential random vectors , where @xmath613 .",
    "in fact , for every @xmath614 , @xmath615 , where @xmath616 is a constant depending only on @xmath445 in condition  [ cond3.1 ] .",
    "following the proof of lemma  5.15 in @xcite , we derive that for every @xmath617 satisfying @xmath618 , @xmath619 consequently , applying corollary  1.12 in the supplement of @xcite with @xmath620 , @xmath621 and @xmath622 to the random vector @xmath623 yields that , for every @xmath624 , @xmath625 \\leq 2 e^{-t } + 8.4 \\,e^{-{\\rm x}_c}. \\label{quadratic.deviation}\\end{aligned}\\ ] ]        first , observe that @xmath429 : |s|=s } \\| \\bxi_s \\|_2 = \\max_{\\bu \\in",
    "\\mathcal{f}(s , p ) }   n^{-1/2 } { \\sum_{i=1}^n}\\frac{\\bu^\\t \\bz_i } { ( \\bu^\\t \\bsigma \\bu ) ^{1/2 } } , { \\nonumber}\\end{aligned}\\ ] ] where @xmath626 . recall that @xmath627 are i.i.d .",
    "@xmath44-dimensional centered random vectors with covariance matrix @xmath628 . as in the proof of lemma  [ gar.lem1 ] , we have for any @xmath629 , @xmath630 consequently , it follows from lemma  7.5 in @xcite that there exists a random variable @xmath631 for @xmath632 such that , for any @xmath633 $ ] , @xmath634 : |s|=s } \\| \\phi^{-1/2 } \\bxi_s \\|_2 -   t_0 \\bigg|   \\geq c_1   a'_0 a_0 \\bigg (   \\delta +   \\frac{\\gamma^{1/2}_{s , p , n}}{\\sqrt{n } }     +   \\frac{\\gamma_{s , p , n}^2 } { n^{3/2 } } \\bigg ) \\bigg\\ } { \\nonumber}\\\\      & \\qquad \\qquad \\qquad \\qquad   \\qquad \\qquad \\qquad   \\qquad \\leq c_2 \\bigg [     \\frac{\\{s \\log(\\gamma_s pn)\\}^2}{\\delta^3 \\sqrt{n } } +    \\frac{\\{s\\log(\\gamma_s pn ) \\}^5}{\\delta^4 n } \\bigg ] , { \\nonumber}\\end{aligned}\\ ] ] where @xmath635 and @xmath636 are absolute constants .",
    "this proves .",
    "the proof employs techniques from empirical process theory which modify the arguments used in @xcite . to begin with ,",
    "note that @xmath637 under the null model , @xmath638 with @xmath639 .",
    "then the sub - differential of @xmath640 at @xmath641 can be written as @xmath642 , where @xmath643 with @xmath644 .",
    "define @xmath645 , and note that @xmath646 are i.i.d .",
    "random variables satisfying @xmath647 .",
    "since @xmath383 minimizes @xmath648 over @xmath602 , we have the following basic inequality @xmath649 further , define a random process @xmath650 indexed by @xmath368 : @xmath651 in what follows , we prove that with overwhelmingly high probability , @xmath652 is concentrated around its expectation @xmath653 uniformly over @xmath368 via a straightforward adaptation of the peeling argument .    for @xmath654 and @xmath655 ,",
    "consider the following sequence of events @xmath656 where @xmath657 . here",
    ", @xmath658 can be regarded as a tolerance parameter , and it is easy to see that @xmath659 . for @xmath660 ,",
    "set @xmath661 and let @xmath662 be the maximum deviation over the elliptic vicinity @xmath663 : @xmath664 for every @xmath368 , define the rescaled vector @xmath665 such that @xmath666 for every @xmath667 , there exists an @xmath166-net @xmath668 of the euclidean ball @xmath669 with cardinality bounded by @xmath670 . for @xmath671 satisfying @xmath672 ,",
    "observe that @xmath673 then , it is easy to see that @xmath674 for each @xmath675 fixed , @xmath676 is a sum of independent random variables with zero means and for @xmath433 , @xmath677 .",
    "therefore , it follows from hoeffding s inequality that for every @xmath472 , @xmath678 in other words , for every @xmath675 and @xmath679 , @xmath680 holds with probability at least @xmath681 .",
    "this , together with the union bound yields @xmath682 in particular , by taking @xmath683 in and @xmath684 in we conclude that @xmath685 holds almost surely on the event @xmath388 for any @xmath389 .    in particular , by taking @xmath686 in for some @xmath687 to be specified below and the union bound , we have @xmath688 { \\nonumber}\\\\      & \\leq   \\sum_{\\ell=1}^\\infty    \\p_{\\bx } \\big [   \\exists \\ , \\btheta \\in",
    "\\mathcal{g}_\\ell(\\delta_1 ) , { \\rm s.t . } \\ ,   | q(\\btheta ) - q_{{\\mathbb{x}}}(\\btheta ) | \\geq       ( \\alpha^\\ell \\delta_1)^2 ( 2c n ) ^{1/2 } + 2\\alpha^\\ell \\delta_1   \\big\\ { ( s\\log n)^{1/2 } + n^{-1/2 } \\big\\ } \\big ] { \\nonumber}\\\\      & \\leq    \\sum_{\\ell=1}^\\infty \\p_{\\bx}\\big [   \\delta(\\alpha^\\ell \\delta_1 ) \\geq ( \\alpha^\\ell \\delta_1)^2 ( 2c n ) ^{1/2 }   + 2\\alpha^\\ell \\delta_1 \\big\\ { ( s\\log n)^{1/2 } + n^{-1/2 } \\big\\ } \\big ]   { \\nonumber}\\\\      & \\leq 2 \\sum_{\\ell=1}^\\infty \\exp\\ {   - c n(\\alpha^\\ell \\delta_1)^2\\ }   \\leq 2 \\sum_{\\ell=1}^\\infty   \\exp\\ {   -2 c",
    "\\ell    \\log(\\alpha ) n \\delta_1 ^ 2 \\ } \\leq \\frac{2\\exp(-c_0 n \\delta_1 ^ 2)}{1- \\exp(-c_0 n \\delta_1 ^ 2 ) } , { \\nonumber}\\end{aligned}\\ ] ] where @xmath689 .",
    "this implies that with probability at least @xmath690 , @xmath691 holds for all @xmath692 whenever @xmath693 .",
    "for the ( conditional ) expectation @xmath694 applying lemmas  5 and 6 in @xcite with slight modifications gives @xmath695 where @xmath262 is as in condition  [ cond3.4 ] . for the sequence of lad estimators @xmath696 , |s| = s \\}$ ] , from it can be seen that @xmath697 , and hence @xmath698 : |s|=s }   \\| n^{-1 } { \\mathbb{x}}_s { \\widehat}{\\btheta}_s \\|_1   \\leq   2   \\bigg\\ { { \\mathbb{e}}|\\varepsilon|+ n^{-1 } { \\sum_{i=1}^n } ( |\\varepsilon_i|-{\\mathbb{e}}|\\varepsilon_i| ) \\bigg\\}.\\ ] ] for every @xmath472 and @xmath481 , by markov s inequality we have @xmath699 where we used the inequality @xmath700 for @xmath481 and @xmath148 .",
    "the last two displays together imply that , with probability at least @xmath701 , @xmath702 : |s|=s }   \\| n^{-1 } { \\mathbb{x}}_s { \\widehat}{\\btheta}_s \\|_1 \\leq 2 { \\mathbb{e}}|\\varepsilon |    \\big\\ { 1   + 4^{(2-\\kappa)/\\kappa }   ( { \\mathbb{e}}|\\varepsilon| ) ^{-1 }   (    { \\mathbb{e}}|\\varepsilon|^\\kappa   ) ^{1/\\kappa }   \\delta_2^{-1/\\kappa }   n^{-1 + 1/\\kappa } \\big\\ } .",
    "{ \\nonumber}\\end{aligned}\\ ] ] by condition  [ cond3.4 ] , we have @xmath703 .",
    "therefore , as long as the sample size @xmath9 satisfies @xmath704 the event @xmath705 : |s|=s }   \\| n^{-1 } { \\mathbb{x}}_s { \\widehat}{\\btheta}_s \\|_1 \\leq 2 a_2^{-1 } \\bigg\\}\\end{aligned}\\ ] ] occurs with probability at least @xmath701 .",
    "now , by , we have @xmath706 and thus @xmath707 holds for every @xmath10-subset @xmath69 $ ] .",
    "together with  and the union bound , this implies that on the event @xmath708 for any @xmath386 , @xmath406 : |s| = s } \\| { \\widehat}{\\bsigma}_{ss}^{1/2 } { \\widehat}{\\btheta}_s \\|_2 \\leq \\min\\bigg [ \\delta_1 ,    32 \\sqrt{2 } \\ , a_2^{-1 }    \\bigg\\ { \\bigg ( \\frac{s\\log n}{n } \\bigg)^{1/2 } + \\frac{1}{n } \\bigg\\ } \\bigg ]   \\label{concentration.lad}\\end{aligned}\\ ] ] holds with ( conditional ) probability @xmath709 , provided that the sample size @xmath9 satisfies @xmath710 and .",
    "we prove this lemma by employing the arguments similar to those used in @xcite , where the likelihood function @xmath712 is assumed to be twice differentiable with respect to @xmath583 .",
    "it is worth noticing that both conditions  ( @xmath555 ) and ( @xmath552 ) in @xcite are not satisfied in the current situation .",
    "we provide here a self - contained proof in which lemma  [ lad.concentration ] also plays an important role .",
    "_ * step  1 : local linear approximation of @xmath713*_. let @xmath714 be the normalized residual of the local linear approximation of @xmath713 given by @xmath715 where @xmath716 and @xmath717 .",
    "then it follows from the mean value theorem that @xmath718 where @xmath719 and @xmath720 for some @xmath721 . as before , for every @xmath588 , define the local elliptic neighborhood of @xmath722 as @xmath723 on the event @xmath724 for some @xmath386 , @xmath725 for all @xmath565 .",
    "thus it follows from the taylor expansion that for @xmath726 , @xmath727 together , and imply that under the same constraint for , @xmath728    turning to the stochastic component @xmath729 , we aim to bound @xmath730 , which can be written as @xmath731 note that @xmath732 is a bivariate process indexed by @xmath733 .",
    "define @xmath734 in this notation , from and the identity @xmath735 , it is easy to see that @xmath736      * conditional on @xmath742 , @xmath743 with probability @xmath744 and @xmath745 with probability @xmath746 ; * conditional on @xmath747 , @xmath748 with probability @xmath749 and @xmath745 with probability @xmath750 ,    where @xmath751 . in this notation , @xmath752 . for every @xmath753 and @xmath617",
    ", we have @xmath754 { \\nonumber}\\\\   & = \\prod_{i=1}^n \\big [ { \\mathbb{e}}_{\\bx } \\ { e^ {   -2 \\lambda \\bu^\\t \\bx_{is }   ( i-{\\mathbb{e}}_{\\bx})\\varepsilon_{i,\\btheta } } \\ } i (    \\bx_{is}^\\t \\btheta \\geq 0 ) + { \\mathbb{e}}_{\\bx } \\ { e^ {   -2 \\lambda \\bu^\\t \\bx_{is }   ( i-{\\mathbb{e}}_{\\bx } ) \\varepsilon_{i,\\btheta } } \\ }   i ( \\bx_{is}^\\t \\btheta < 0 )   \\big ] { \\nonumber}\\\\   & = \\prod_{i=1}^n   \\big [   \\big\\ { e^{-2\\lambda \\bu^\\t \\bx_{is } ( 3/2-p_{i,\\btheta } ) } ( p_{i,\\btheta}-1/2 ) + e^{2\\lambda \\bu^\\t \\bx_{is}(p_{i,\\btheta } -1/2)}(3/2-p_{i,\\btheta } ) \\big\\ } i ( \\bx_{is}^\\t \\btheta \\geq 0 )   { \\nonumber}\\\\   & \\qquad \\qquad +   \\big\\ { e^{2\\lambda \\bu^\\t \\bx_{is } ( 1/2 + p_{i,\\btheta } ) } ( 1/2 - p_{i,\\btheta } ) + e^{2\\lambda \\bu^\\t \\bx_{is}(p_{i,\\btheta } -1/2)}(1/2 + p_{i,\\btheta } )   \\big\\ }",
    "i ( \\bx_{is}^\\t \\btheta < 0 ) \\big ] .",
    "{ \\nonumber}\\end{aligned}\\ ] ] further , using the inequalities @xmath755 and @xmath756 which hold for all @xmath103 , the last term above can be bounded by @xmath757   { \\nonumber}\\\\   & \\leq \\prod_{i=1}^n \\big\\ {   1 + 2 \\lambda^2   ( \\bu^\\t \\bx_{is})^2 |p_{i,\\btheta } -1/2| e^{2\\lambda |\\bu^\\t \\bx_{is}| } \\big\\ }   { \\nonumber}\\\\   & \\leq   \\prod_{i=1}^n \\exp\\big\\ { 2 \\lambda^2   ( \\bu^\\t \\bx_{is})^2 |p_{i,\\btheta } -1/2| e^{2\\lambda |\\bu^\\t \\bx_{is}| } \\big\\ } .",
    "{ \\nonumber}\\end{aligned}\\ ] ] consequently , for every @xmath758 , @xmath759 on the event @xmath724 for some @xmath386 , we have @xmath760 and @xmath761 . together with , this yields that for all @xmath762 , @xmath763 in view of , define @xmath764 for some @xmath765 to be specified ( see below ) , such that for any @xmath758 with @xmath766 , @xmath767 holds almost surely on @xmath724 for all @xmath768 by , it follows from corollary  2.2 in the supplement of @xcite and that , for any @xmath386 , @xmath766 and @xmath769 , @xmath770 holds almost surely on @xmath724 , where @xmath771 is given at .",
    "_ * step  2 : fisher approximation*_. by lemma  [ lad.concentration ] , @xmath777 : |s|=s } \\| \\bd_0 { \\widehat}{\\btheta}_s \\|_2 { \\nonumber}\\\\      & = \\ { 2 n f_\\varepsilon(0 ) \\}^{1/2 } \\max_{s\\subseteq [ p ] : |s|=s } \\| { \\widehat}\\bsigma_{ss}^{1/2 }   { \\widehat}{\\btheta}_s \\|_2 \\leq    c_1 \\ , a_2^{-1 } \\ { 2 f_\\varepsilon(0 ) s\\log(pn ) \\}^{1/2 } : = r_0 \\label{def.r0}\\end{aligned}\\ ] ] holds with probability at least @xmath778 .",
    "moreover , since @xmath383 maximizes @xmath492 over @xmath374 for each @xmath10-subset @xmath69 $ ] , we have @xmath779 and @xmath780 . this , together with implies that on the event @xmath781 , @xmath782 whenever @xmath783 .    _ * step  3 : wilks approximation*_. for @xmath784 , define @xmath785 noting that @xmath786 , we have @xmath787 where @xmath788 for some @xmath789 .",
    "let @xmath790 be as in .",
    "then , it follows from that on @xmath791 with @xmath792 , @xmath793    in view of , @xmath794 .",
    "therefore , on the event @xmath795 we have @xmath796^{1/2 }   - \\| \\bd_0 { \\widehat}{\\btheta}_s \\|_2   \\big| { \\nonumber}\\\\      & \\leq \\frac{|2\\ { { \\cal l}_n^s({\\widehat}{\\btheta}_s ) - { \\cal l}_n^s({\\mathbf{0 } } ) \\ } -   \\|   \\bd_0 { \\widehat}{\\btheta}_s\\|_2 ^ 2   |}{\\| \\bd_0 { \\widehat}{\\btheta}_s \\|_2 } \\leq \\frac{2|\\chi_2^s({\\mathbf{0 } } , { \\widehat}{\\btheta}_s)| } { \\| \\bd_0 { \\widehat}{\\btheta}_s \\|_2 } \\leq 4   \\big\\ { \\delta(\\tau , r_0 ) r_0 + 6 w_0(\\tau )   ( 2 t + 4s ) ^{1/2 }   \\big\\ } ,   { \\nonumber}\\end{aligned}\\ ] ] provided that @xmath797 .",
    "together with , this implies that conditional on the event @xmath798 : |s| = s } \\ { { \\widehat}{\\btheta}_s \\in \\theta_0(r_0 ) \\ } \\cap \\omega^s_0(\\tau , r_0,t)$ ] , @xmath799 : |s| = s }   \\big|   [ 2\\ { { \\cal l}_n^s({\\widehat}{\\btheta}_s ) - { \\cal l}_n^s({\\mathbf{0 } } ) \\ }    ] ^{1/2 }   - \\| { \\widehat}{\\bxi}_s \\|_2   \\big| \\leq   5 \\big\\ { \\delta(\\tau , r_0 ) r_0 + 6 w_0(\\tau ) ( 2 t + 4s ) ^{1/2 } \\big\\ }    \\label{sqrt.approxi}\\end{aligned}\\ ] ] whenever @xmath800 , where @xmath594 , @xmath595 and @xmath801 are as in , and ."
  ],
  "abstract_text": [
    "<S> many data mining and statistical machine learning algorithms have been developed to select a subset of covariates to associate with a response variable . </S>",
    "<S> spurious discoveries can easily arise in high - dimensional data analysis due to enormous possibilities of such selections . </S>",
    "<S> how can we know statistically our discoveries better than those by chance ? </S>",
    "<S> in this paper , we define a measure of goodness of spurious fit , which shows how good a response variable can be fitted by an optimally selected subset of covariates under the null model , and propose a simple and effective lamm algorithm to compute it . </S>",
    "<S> it coincides with the maximum spurious correlation for linear models and can be regarded as a generalized maximum spurious correlation . </S>",
    "<S> we derive the asymptotic distribution of such goodness of spurious fit for generalized linear models and @xmath0 regression . </S>",
    "<S> such an asymptotic distribution depends on the sample size , ambient dimension , the number of variables used in the fit , and the covariance information . </S>",
    "<S> it can be consistently estimated by multiplier bootstrapping and used as a benchmark to guard against spurious discoveries . </S>",
    "<S> it can also be applied to model selection , which considers only candidate models with goodness of fits better than those by spurious fits . </S>",
    "<S> the theory and method are convincingly illustrated by simulated examples and an application to the binary outcomes from german neuroblastoma trials .    _ key words _ : bootstrap , gaussian approximation , generalized linear models , @xmath0 regression , model selection , sparsity , spurious correlation , spurious fit </S>"
  ]
}