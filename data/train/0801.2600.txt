{
  "article_text": [
    "let @xmath2 be i.i.d .  observations , where @xmath3 and the @xmath4 s and @xmath5 s are independent .",
    "assume that the @xmath4 s are unobservable and that they have the density @xmath6 and also that the @xmath5 s have a known density @xmath7 the deconvolution problem consists in estimation of the density @xmath6 based on the sample @xmath8    a popular estimator of @xmath6 is the deconvolution kernel density estimator , which is constructed via fourier inversion and kernel smoothing .",
    "let @xmath9 be a kernel function and @xmath10 a bandwidth .",
    "the kernel deconvolution density estimator @xmath11 is defined as @xmath12 where @xmath13 denotes the empirical characteristic function of the sample , i.e. @xmath14 @xmath15 and @xmath16 are fourier transforms of the functions @xmath9 and @xmath17 respectively , and @xmath18 the estimator was proposed in @xcite and @xcite and there is a vast amount of literature dedicated to it ( for additional bibliographic information see e.g.  @xcite and @xcite ) .",
    "depending on the rate of decay of the characteristic function @xmath16 at plus and minus infinity , deconvolution problems are usually divided into two groups , ordinary smooth deconvolution problems and supersmooth deconvolution problems . in the first case",
    "it is assumed that @xmath16 decays algebraically and in the second case the decay is essentially exponential .",
    "this rate of decay , and consequently the smoothness of the density @xmath17 has a decisive influence on the performance of .",
    "the general picture that one sees is that smoother @xmath19 is , the harder the estimation of @xmath6 becomes , see e.g.  @xcite .",
    "asymptotic normality of in the ordinary smooth case was established in @xcite , see also @xcite .",
    "the limit behaviour in this case is essentially the same as that of a kernel estimator of a higher order derivative of a density .",
    "this is obvious in certain relatively simple cases where the estimator is actually equal to the sum of derivatives of a kernel density estimator , cf .",
    "@xcite .",
    "our main interest , however , lies in asymptotic normality of in the supersmooth case . in this case under certain conditions on the kernel @xmath9 and the unknown density @xmath20 the following theorem was proved in @xcite .",
    "[ thmanfan ] let @xmath11 be defined by .",
    "then @xmath21)\\convd { \\mathcal n}(0,1)\\ ] ] as @xmath1 here either @xmath22 or @xmath23 is the sample variance of @xmath24 with @xmath25    the asymptotic variance of @xmath11 itself does not follow from this result . on the other hand @xcite ,",
    "see also @xcite , derived a central limit theorem for where the normalisation is deterministic and the asymptotic variance is given .    for the purposes of the present work it is sufficient to use the result of @xcite .",
    "however , before recalling the corresponding theorem , we first formulate conditions on the kernel @xmath9 and the density @xmath7    [ condw ] let @xmath15 be real - valued , symmetric and have support @xmath26.$ ] let @xmath27 and assume @xmath28 as @xmath29 for some constants @xmath30 and @xmath31    the simplest example of such a kernel is the sinc kernel @xmath32 its characteristic function equals @xmath33}(t).$ ] in this case @xmath34 and @xmath35    another kernel satisfying condition [ condw ] is @xmath36 its corresponding fourier transform is given by @xmath37}(t).$ ] here @xmath38 and @xmath39 the kernel was used for simulations in @xcite and its good performance in deconvolution context was established in @xcite .",
    "yet another example is @xmath40 the corresponding fourier transform equals @xmath41}(|t|)+(6|t|^3 - 6t^2 + 1)1_{[-1/2,1/2]}(t).\\ ] ] here @xmath42 and @xmath39 this kernel was considered in @xcite and @xcite .",
    "now we formulate the condition on the density @xmath7    [ condk ] assume that @xmath43 for some @xmath44 and some constant @xmath45 furthermore , let @xmath46 for all @xmath47    the following theorem holds true , see @xcite .    [ thman ] assume conditions [ condw ] and [ condk ] and let @xmath48<\\infty.$ ] then , as @xmath49 and @xmath50 @xmath51)\\convd { \\mathcal n}\\left(0,\\frac{a^2}{2\\pi^2}\\left(\\frac{\\mu}{\\lambda}\\right)^{2 + 2\\alpha}(\\gamma(\\alpha+1))^2\\right).\\ ] ] here @xmath52 denotes the gamma function .",
    "the goal of the present note is to compare the theoretical behaviour of the estimator predicted by theorem [ thman ] to its behaviour in practice , which will be done via a limited simulation study .",
    "the obtained results can be used to compare theorem [ thmanfan ] to theorem [ thman ] , e.g.whether it is preferable to use the sample standard deviation @xmath53 in the construction of pointwise confidence intervals ( computation of @xmath53 is more involved ) or to use the normalisation of theorem [ thman ] ( this involves evaluation of a simpler expression ) .",
    "the rest of the paper is organised as follows : in section [ simulations ] we present some simulation results , while in section [ conclusions ] we discuss the obtained results and draw conclusions .",
    "all the simulations in this section were done in mathematica .",
    "we considered three target densities .",
    "these densities are :    1 .   density # 1 : @xmath54 2 .",
    "density # 2 : @xmath55 3 .",
    "density # 3 : @xmath56    the density # 2 was chosen because it is skewed , while the density # 3 was selected because it has two unequal modes .",
    "we also assumed that the noise term @xmath5 was @xmath57 distributed . notice that the noise - to - signal ratio @xmath58/\\var[y ] 100\\%$ ] for the density # 1 equals @xmath59 for the density # 2 it is equal to @xmath60 and for the density # 3 it is given by @xmath61 we have chosen the sample size @xmath62 and generated @xmath63 samples from the density @xmath64 notice that such @xmath65 was also used in simulations in e.g.  @xcite . even though at the first sight @xmath62 might look too small for normal deconvolution , for the low noise level that we have the deconvolution kernel density estimator will still perform well , cf .",
    "@xcite . as a kernel we took the kernel . for each model that we considered ,",
    "the theoretically optimal bandwidth , i.e.  the bandwidth minimising @xmath66=\\ex \\left[{\\int_{-\\infty}^{\\infty}}(f_{nh}(x)-f(x))^2dx \\right],\\ ] ] the mean - squared error of the estimator @xmath67 was selected by evaluating for a grid of values of @xmath68 and selecting the @xmath69 that minimised @xmath70 $ ] on that grid .",
    "notice that it is easier to evaluate by rewriting it in terms of the characteristic functions , which can be done via parseval s identity , cf .",
    "@xcite . for real data",
    "of course the above method does not work , because depends on the unknown @xmath71 we refer to @xcite for data - dependent bandwidth selection methods in kernel deconvolution .",
    "following the recommendation of @xcite , in order to avoid possible numerical issues , the fast fourier transform was used to evaluate the estimate .",
    "several outcomes for two sample sizes , @xmath62 and @xmath72 are given in figure [ fig1 ] .",
    "we see that the fit in general is quite reasonable .",
    "this is in line with results in @xcite , where it was shown by finite sample calculations that the deconvolution kernel density estimator performs well even in the supersmooth noise distribution case , if the noise level is not too high .",
    "( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    in figure [ fig2 ] we provide histograms of estimates @xmath73 that we obtained from our simulations for @xmath74 and @xmath75 ( the densities # 1 and # 2 ) and for @xmath74 and @xmath76 ( the density # 3 ) . for the density",
    "# 1 points @xmath74 and @xmath75 were selected because the first corresponds to its mode , while the second comes from the region where the value of the density is moderately high .",
    "notice that @xmath74 is a boundary point for the support of density # 2 and that the derivative of density # 2 is infinite there . for the density # 3",
    "the point @xmath74 corresponds to the region between its two modes , while @xmath76 is close to where it has one of its modes .",
    "the histograms look satisfactory and indicate that the asymptotic normality is not an issue .",
    "( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    our main interest , however , is in comparison of the sample standard deviation of at a fixed point @xmath77 to the theoretical standard deviation computed using theorem [ thman ] .",
    "this is of practical importance e.g.  for construction of confidence intervals .",
    "the theoretical standard deviation can be evaluated as @xmath78 upon noticing that in our case , i.e.  when using kernel and the error distribution @xmath79 we have @xmath80 after comparing this theoretical value to the sample standard deviation of the estimator @xmath11 at points @xmath74 and @xmath75 ( the densities # 1 and # 2 ) and at points @xmath74 and @xmath76 ( the density # 3 ) , see table [ table1 ] , we notice a considerable discrepancy ( by a factor @xmath81 for the density # 1 and even larger discrepancy for densities # 2 and # 3 ) . at the same time the sample means",
    "evaluated at these two points are close to the true values of the target density and broadly correspond to the expected theoretical value @xmath82 note here that the bias of @xmath73 is equal to the bias of an ordinary kernel density estimator based on a sample from @xmath20 see e.g.  @xcite .",
    ".[table1 ] sample means @xmath83 and @xmath84 and sample standard deviations @xmath85 and @xmath86 evaluated at @xmath74 and @xmath75 ( densities # 1 and # 2 ) and @xmath74 and @xmath76 ( the density # 3 ) together with the theoretical standard deviation @xmath87 and the corrected theoretical standard deviation @xmath88 .",
    "the bandwidth is given by @xmath89 [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     finally , we mention that results qualitatively similar to the ones presented in this section were obtained for the kernel as well .",
    "these are not reported here because of space restrictions .",
    "in the simulation examples considered in section [ simulations ] for theorem [ thman ] , we notice that the corrected theoretical asymptotic standard deviation is always considerably larger than the sample standard deviation given the fact that the noise level is not high .",
    "we conjecture , that this might be true for the densities other than # 1 , # 2 and # 3 as well in case when the noise level is low .",
    "this possibly is one more explanation of the fact of a reasonably good performance of deconvolution kernel density estimators in the supersmooth error case for relatively small sample sizes which was noted in @xcite . on the other hand",
    "the match between the sample standard deviation and the corrected theoretical standard deviation is much better for higher levels of noise .",
    "these observations suggest studying the asymptotic distribution of the deconvolution kernel density estimator under the assumption @xmath90 as @xmath91 cf.@xcite , where @xmath87 denotes the standard deviation of the noise term .",
    "our simulation examples suggest that the asymptotic standard deviation evaluated via theorem [ thman ] in general will not lead to an accurate approximation of the sample standard deviation , unless the bandwidth is small enough , which implies that the corresponding sample size must be rather large .",
    "the latter is hardly ever the case in practice .",
    "on the other hand , we have seen that in certain cases this poor approximation can be improved by using the left - hand side of instead of the right - hand side .",
    "a perfect match is impossible to obtain given that we still neglect the remainder term in .",
    "however , even after the correction step , the corrected theoretical standard deviation still differs from the sample standard deviation considerably for small sample sizes and lower levels of noise .",
    "moreover , in some cases the corrected theoretical standard deviation is even farther from the sample standard deviation than the original uncorrected version .",
    "the latter fact can be explained as follows :    1 .",
    "it seems that both the theoretical and corrected theoretical standard deviation overestimate the sample standard deviation .",
    "the value of the bandwidth @xmath92 for which the match between the corrected theoretical standard deviation and the sample standard deviation become worse , belongs to the range where the corrected theoretical standard deviation is larger than the theoretical standard deviation . in view of item 1 above , it is not surprising that in this case the theoretical value turns out to be closer to the sample standard deviation than the corrected theoretical value .",
    "the consequence of the above observations is that a naive attempt to directly use theorem [ thman ] , e.g.  in the construction of pointwise confidence intervals , will lead to largely inaccurate results .",
    "an indication of how large the contribution of the remainder term in can be can be obtained only after a thorough simulation study for various distributions and sample sizes , a goal which is not pursued in the present note . from the three simulation examples that we considered",
    ", it appears that the contribution of the remainder term in is quite noticeable for small sample sizes .",
    "for now we would advise to use theorem [ thman ] for small sample sizes and lower noise levels with caution .",
    "it seems that the similar cautious approach is needed in case of theorem [ thmanfan ] as well , at least for some values of @xmath93    unlike for the ordinary smooth case , see @xcite , there is no study dealing with the construction of uniform confidence intervals in the supersmooth case . in the latter paper a better performance of the bootstrap confidence intervals",
    "was demonstrated in the ordinary smooth case compared to the asymptotic confidence bands obtained from the expression for the asymptotic variance in the central limit theorem .",
    "the main difficulty in the supersmooth case is that the asymptotic distribution of the supremum distance between the estimator @xmath11 and the true density @xmath6 is unknown .",
    "our simulation results seem to indicate that the bootstrap approach is more promising for the construction of pointwise confidence intervals than e.g.  the direct use of theorems [ thmanfan ] or [ thman ] .",
    "moreover , the simulations suggest that at least theorem [ thman ] is not appropriate when the noise level is low ."
  ],
  "abstract_text": [
    "<S> via a simulation study we compare the finite sample performance of the deconvolution kernel density estimator in the supersmooth deconvolution problem to its asymptotic behaviour predicted by two asymptotic normality theorems . </S>",
    "<S> our results indicate that for lower noise levels and moderate sample sizes the match between the asymptotic theory and the finite sample performance of the estimator is not satisfactory . </S>",
    "<S> on the other hand we show that the two approaches produce reasonably close results for higher noise levels . these observations in turn provide additional motivation for the study of deconvolution problems under the assumption that the error term variance @xmath0 as the sample size @xmath1 + _ keywords : _ finite sample behavior , asymptotic normality , deconvolution kernel density estimator , fast fourier transform . + _ ams subject classification : _ 62g07 + </S>"
  ]
}