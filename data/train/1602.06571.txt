{
  "article_text": [
    "we consider a model of nomadic agents exploring and competing for time - varying stochastic location - specific resources .",
    "such multi - agent systems arise in crowdsourced transportation services like uber and lyft where drivers position themselves to be close to demand ; in online communities like twitch and reddit where webizens choose in which subcommunities to participate ; and in location - specific activity in the traditional economy , such as food trucks choosing where to position themselves , fisherman choosing where to fish , and pastoralists choosing where to graze their animals . in each of these examples ,",
    "overall social welfare is determined both by agents willingness to explore their domain to find and exploit resource - rich locations , the level of antagonism or synergy inherent to having multiple agents at the same location , and the equilibrium distribution of agents across locations that these factors induce .",
    "the model we study comprises a set of locations and a group of agents .",
    "each location has a resource level that varies randomly with time .",
    "each agent periodically derives resource from the location at which she currently resides , whose amount is determined by the number of other agents currently residing there , and the location s current overall resource level .",
    "based on these quantities , the agent then decides whether to stay at the same location or switch to another .",
    "the agents are fully strategic and seek to maximize their total rewards over their lifetime .",
    "we study the equilibrium behavior of the agents in this system as a function of dynamics of the spatio - temporal resource process and the level of synergy or antagonism in the agents sharing of resources .",
    "we analyze the system under the limit where the number of agents and locations both increase proportionally , using the methodology of a mean field equilibrium .",
    "we show that an equilibrium exists , and the agents optimal strategy has a simple threshold structure , in which it is optimal to leave a location when the number of other agents exceeds a threshold that depends on the resource level at that location . in the limit",
    "as the system grows large , this induces a joint probability distribution over the number of agents and level of resource at each location .",
    "our results allow us to obtain economic insights into how the nature of the externality agents impose on others at the same location affects the exploration of the locations for resources , and consequently the overall welfare of the economy .",
    "in particular , our methodology allows us to analyze settings where the overall reward at a location either increases or decreases with the number of agents at the location , and how these two settings affect the equilibrium exploration .",
    "furthermore , our methodology allows us to evaluate engineering interventions , such as providing subsidies to or imposing costs on agents to promote or discourage exploration to improve welfare .",
    "[ [ examples ] ] examples + + + + + + + +    the model we study is a simplified version of systems appearing in real - world settings .",
    "it arises in the shared economy , in crowdsourced transportation services such as uber and lyft , in which drivers choose neighborhoods , and then earn money ( reward ) based on the number of riders requesting service within that neighborhood ( the overall resource level ) , and the number of other drivers working there .",
    "this overall resource level varies stochastically over time in a neighborhood - specific way as demand rises and falls , and the resource derived by a driver decreases with the number of other drivers working in her neighborhood .",
    "this model also arises in the internet economy , in online communities such as reddit and twitch , in which participants choose sub - communities , and then derive enjoyment depending both on some underlying but transitory societal interest in the sub - community s topic of focus ( the overall resource ) and the number of other participants in the sub - community .",
    "when the number of other participants is too small , lack of social interaction prevents enjoyment ; when the number of other participants is too large , crowding diminishes the sense of community .",
    "this model also arises in the traditional economy , for example in food trucks deciding in which neighborhoods to locate , in pastoralists deciding where to graze their livestock , and in fishermen deciding where to fish . in these examples",
    ", the level of resource derived by each agent from their location ( whether profit from hungry passers by ; or food for livestock provided by the range - land ; or profit from the catch ) depends both on the number of other agents at the location , and on the location s overall and stochastically varying resource level .",
    "this model even arises among scientific researchers , who must choose a research area in which to work , and derive value from this choice based both on the underlying level of societal interest and funding in their chosen area , and in the number of other researchers working in it . as with online communities",
    ", the number of other researchers should be neither too large nor too small too maximize the value derived .",
    "[ [ related - work ] ] related work + + + + + + + + + + + +    our paper adds to the growing literature on mean field equilibrium @xcite , that studies complex systems under a large system limit and obtains insights about agent behavior that are hard to obtain from analyzing finite models . the main insight behind this line of literature , that in the large system limit , agents behavior are characterized by their ( private ) state and an aggregate distribution of the rest of system , has been used to study settings that include industry dynamics and oligopoly models @xcite , repeated dynamics auctions @xcite , online labor markets @xcite , and queueing systems @xcite .",
    "our model can be seen as an extension of the kolkata paise restaurant problem @xcite . in this game , each agent chooses ( simultaneously ) a restaurant to visit , and earns a reward that depends both on the restaurant s rank , which is common across agents , and the number of other agents at that restaurant .",
    "this reward is inversely proportional to the number of agents visiting the restaurant .",
    "the kolkata paise restaurant problem is itself a generalization of the el farol bar problem @xcite .",
    "the kolkata paise restaurant problem is studied both in the one - shot and repeated settings , with results on the limiting behavior of myopic @xcite and other strategies @xcite , although we are not aware of existing results on mean - field equilibria in this model .",
    "our model is both more general , in that we allow general reward functions and allow location s resource to vary stochastically , and more specific , in that our locations are homogeneous .",
    "our model also differs in that our agents decisions are made asynchronously .",
    "our model is also related to congestion games @xcite , in which agents choose paths on which to travel , and then incur costs that depend on the number of other agents that have chosen the same path .",
    "one may view paths as being synonymous with locations in our model , and observe that in both cases the utility / cost derived from a path / location depends on the number of other agents using that path , or portion thereof .",
    "the main difference between our model and congestion games is the stochastic time - varying nature of our overall level of resource ( making our model more complex ) , and the lack of interaction between locations contrasting with the interaction between paths ( making our model simpler ) .",
    "our model has within it an exploration vs. exploitation tradeoff , in which an agent faces the decision of whether to stay at his current node , exploiting its resource and obtaining a known reward , or to leave and go to another randomly chosen location with unknown reward .",
    "visiting this new location provides information upon which future decisions may be based .",
    "similar tradeoffs between exploration and exploitation appear widely , and have been studied extensively in the single - agent setting @xcite .",
    "exploration and learning in multi - agent settings has been considered by @xcite .",
    "we consider a setting with @xmath0 agents , each situated at each time @xmath1 in one of @xmath2 locations .",
    "each location @xmath3 has a stochastic dynamic resource process , denoted by @xmath4 , that determines the reward obtained by each agent at that location , as we describe below .",
    "we assume that each process @xmath4 is a finite state continuous time markov chain , that is distributed identically and independently from the rest of the system . for the purpose of analysis , we assume that each @xmath5 takes values in @xmath6 , with holding time at state @xmath7 distributed as @xmath8 for some fixed @xmath9 .",
    "agents may switch between locations to explore for resources .",
    "formally , each agent @xmath10 has an associated independent poisson process @xmath11 with rate @xmath12 , at whose jump times @xmath13 the agent makes the decision to either stay in her current location or switch instantaneously to a different location that is chosen uniformly at random .",
    "let @xmath14 denote the location of agent @xmath10 at time @xmath1 and let @xmath15 denote the number of agents at location @xmath3 at time @xmath16 .",
    "the agents in the model are short - lived , and at each jump time @xmath17 , the agent @xmath10 departs the system with probability @xmath18 .",
    "thus , each agent @xmath10 lives in the system for a random time @xmath19 which is distributed according to @xmath20 . for each agent",
    "@xmath10 that leaves the system , a new agent ( with the same label @xmath10 ) arrives at a location chosen uniformly at random .",
    "we make this modeling assumption to ensure that the number of agents in the system is always positive ; this assumption can be relaxed to allow for random arrivals and departures , with the arrival rate equal to the departure rate .",
    "we now describe the decision problem faced by an agent @xmath10 in more detail . at each jump time",
    "@xmath21 , an agent @xmath10 at location @xmath22 receives a reward @xmath23 that depends on the state of the resource process @xmath5 and the overall number of agents @xmath24 at the location @xmath3 . in the following ,",
    "we assume that the function @xmath25 governing the reward at each location is given by @xmath26 for some function @xmath27 that is non - increasing in @xmath28 , with @xmath29 .",
    "essentially , this implies that the reward at a location at any time is zero if the resource process is in state @xmath30 , and it is equal to @xmath31 if the resource process is in state @xmath32 , if there are @xmath28 agents at that location .",
    "furthermore , this reward @xmath31 decreases to zero as the number of agents at a location increases . given this setting , each agent @xmath10 at any time prefers to be at a location with resource process state equal to @xmath32 , and where the number of other agents is small .    within this setting",
    ", we will focus on three cases : ( 1 ) for each @xmath33 , we have @xmath34 . in this case",
    "the ( unit ) resource , if available at a location , is shared equally among the agents at that location ; ( 2 ) @xmath35 is non - decreasing in @xmath28 . in this case adding agents to a location increases the total reward earned , either through synergy , or because a small number of agents can not fully utilize a location s resource ; and ( 3 ) @xmath35 is non - increasing in the number of agents @xmath28 at that location . in this case antagonism or overutilization",
    "causes the total reward earned to decrease as agents are added .",
    "next , we discuss the information each agent @xmath10 has while making their decision to stay or switch .",
    "we assume that an agent @xmath10 has access to the states of the resource process @xmath36 and the number of agents @xmath15 of a location @xmath3 during the time she is present at the location @xmath3 , i.e. , when @xmath37 .",
    "we further assume that agents have perfect recall , and hence , at a jump time @xmath38 , each agent @xmath10 bases her decision to switch or stay on the entire history @xmath39 she has observed until time @xmath16 , namely the resource process states and the number of agents at each location she has visited during the time period she visited that location : @xmath40 thus , a strategy @xmath41 for an agent @xmath10 , specifies a ( mixed ) action between stay and switch at each jump time @xmath42 of her associated poisson process @xmath11 , based on her history @xmath39 .    given this informational assumption , each agent @xmath10 seeks to maximize the total expected reward accrued over her lifetime , given by @xmath43.\\end{aligned}\\ ] ] observe that since the agent departs the system with probability @xmath44 at each jump time independently , the total expected reward can be equivalently written as @xmath45.\\end{aligned}\\ ] ] thus , each agent s decision problem is equivalent to maximizing her total discounted expected reward assuming she persists in the system .",
    "since the reward at any location is determined by the number of agents at that location , each agent s decision to stay in her current location or to switch to a new one depends on all the other agents behavior .",
    "consequently , the interaction among the agents is a dynamic game , and analyzing the agents behavior requires an equilibrium analysis .",
    "the standard equilibrium concept to analyze the induced dynamic game is a perfect bayesian equilibrium ( pbe ) .",
    "a pbe consists of a strategy @xmath41 and a belief system @xmath46 for each player @xmath10 .",
    "a belief system @xmath46 for agent @xmath10 specifies a belief @xmath47 after any history @xmath39 over all aspects of the system that she is uncertain of and that influence her expected payoff .",
    "a pbe then requires two conditions to hold : ( 1 ) each agent @xmath10 s strategy @xmath41 is a best response after any history @xmath39 , given their belief system and given all other agents strategies ; and ( 2 ) each agent @xmath10 s beliefs @xmath47 are updated via bayes rule whenever possible ( see @xcite for more details ) .",
    "observe that a pbe supposes a complex model of agent behavior .",
    "it requires each player @xmath10 to keep track of her entire history , and maintain complex beliefs about the rest of the system .",
    "while this may be plausible in small settings , this behavioral model seems implausible for large systems . on the contrary , in such settings ,",
    "it is more plausible that each agent would base her decision to stay or switch solely on the current state of the location she is in  specifically on its level of resource , and the number of other agents there and on the aggregate features of the entire system .",
    "moreover , we expect that if an agent were to base her decision only on this information , then she would pursue a `` threshold '' strategy : she would stay in her current location if the number of agents at that location is low , and switch to a different location if that number is high , with the threshold used depending on that location s level of resource .",
    "below , we seek to uncover this intuitive behavioral model as an equilibrium in large systems by letting the number of agents and the number of location both increase proportionally to infinity , and studying the limiting infinite system .",
    "in this section , we consider an infinite system that is obtained as the limit of the finite system as the number of location @xmath2 and the number of agents @xmath0 both tend to infinity , with @xmath48 , for some fixed @xmath49 . in the limiting system , there are infinite number of locations and agents , with the expected number of agents per location fixed at @xmath50 . in such a limit ,",
    "given certain consistency conditions that bind the mean dynamics of all the locations , the dynamics of each location essentially decouples from the rest of the system .",
    "under such a decoupling , instead of focusing on the entire limiting system , it suffices to focus on the dynamics of a single location , as well as the empirical distribution of the states of all the locations .",
    "we begin with the description of the dynamics of a single location in such an infinite system .      to analyze the agents behavior in the infinite system , we fix a location @xmath3 and focus on the decision problem faced by an agent @xmath10 at location @xmath3 about when to switch to a different location .",
    "let @xmath11 denote the poisson process with rate @xmath51 associated with agent @xmath10 , with jump times @xmath17 for @xmath52 .",
    "as before , let @xmath36 denote the state of the resource process at location @xmath3 and let @xmath15 denote the number of agents at location @xmath3 at time @xmath16 .",
    "we assume that agents arrive at location @xmath3 according to a poisson arrival process with rate @xmath53 .",
    "note that these arriving include new agents arriving to the system ( following a departure ) , as well as existing agents who have chosen to switch from their current location .",
    "inspired by the discussion at the end of the preceding section , we focus on a family of threshold strategies for the agents .",
    "a threshold strategy is characterized by a pair @xmath54 . in a threshold strategy @xmath55 , an agent at a location with resource level @xmath7 chooses to stay at her current location if the number of agents is strictly below @xmath56 ; chooses to switch her location if the number of agents at her current location is strictly above @xmath57 ; and stays with probability @xmath58 and switches with the remaining probability if the number of agents is equal to @xmath59 .",
    "our eventual goal is to show that there exists an equilibrium for agents behavior where all agents follow ( the same ) threshold strategy . for",
    "now , we assume that all agents except agent @xmath10 adopt a threshold strategy @xmath60 , and seek an optimal strategy over the class of all history - dependent strategies ( not just the class of threshold strategies ) for agent @xmath10 .",
    "note that given the arrival rate @xmath53 , and the threshold policy @xmath55 , the process @xmath61 evolves as a continuous time markov chain on the state space @xmath62 with the following transition rate matrix : for each @xmath7 , and for all @xmath33 , we have @xmath63 here , the first equation represents the transitions in @xmath36 , which is an independent markov chain on @xmath64 with holding times @xmath65 and @xmath66 .",
    "the second equation follows from the assumption that agents arrive at location @xmath3 according to a poisson process with rate @xmath53 .",
    "the third equation represents a transition where an agent at location @xmath3 leaves .",
    "this transition can occur in two ways : first , the agent could leave the system with probability @xmath67 ; second , the agent could survive , with probability @xmath68 , but choose to switch to a different location , which happens with probability @xmath32 if @xmath69 , with probability @xmath70 if @xmath71 , and zero otherwise .",
    "since there are @xmath72 other agents that make this decision to stay or switch at rate @xmath73 , these transitions occur at rate @xmath74 .",
    "we denote this continuous time markov chain describing the dynamics of a single location , where all agents adopt the threshold policy @xmath55 and the rate of arrival of agents is @xmath75 , by @xmath76 .",
    "we are now ready to describe the decision problem faced by the agent @xmath10 regarding when to switch from her current location . at each jump time @xmath38 of @xmath11",
    ", the agent @xmath10 receives an immediate payoff of @xmath77 and may leave the system with probability @xmath67 .",
    "if she does not leave the system , then she has to decide between two actions `` stay '' or `` switch '' .",
    "on choosing `` stay '' continues until the next jump time @xmath78 ; on choosing `` switch '' , the decision problem terminates with an immediate payoff of @xmath79 , that does not depend on the state of the location @xmath3 .    before proceeding ,",
    "we provide a brief interpretation of the termination payoff @xmath80 .",
    "observe that in a finite system , an agent on switching from a location , moves on to a different location that is chosen uniformly at random , and continues to accrue payoffs until she leaves the system .",
    "this suggests that one may interpret the termination payoff @xmath80 as capturing the notion of a continuation payoff on switching in the finite system in the context of the limiting infinite system .",
    "subsequently , we impose conditions on our equilibrium notion that ensure that indeed @xmath80 denotes the continuation payoff in the infinite system .    = .5em given these payoffs and actions for agent @xmath10 , it follows that the decision problem facing agent @xmath10 is an optimal stopping problem , which we denote by @xmath81 .",
    "we next specify the dynamic programming formulation of @xmath81 .",
    "note that in the decision problem , when the markov chain @xmath76 is in state @xmath82 , events occur at rate @xmath83 : with rate @xmath75 a new agent arrives , with rate @xmath84 the resource level changes , with rate @xmath74 one of the other agents either leaves the system or survives and makes the decision to stay or switch , and finally with rate @xmath73 , agent @xmath10 arrives at a jump time to make a decision herself .",
    "thus , we define the following transition probabilities for the state transition : @xmath85 here , @xmath86 denotes the probability that the next event corresponds to agent @xmath10 s decision epoch , @xmath87 denotes the probability the next event corresponds to one of the other agents exiting the system , @xmath88 denotes the probability the next event corresponds to one of the other agents persisting in the system , @xmath89 denotes the probability the next event corresponds to change in the resource level , and finally , @xmath90 denotes the probability that the next event corresponds to a new arrival .",
    "given @xmath91 with @xmath92 , let @xmath93 denote the optimal expected total reward of agent @xmath10 just after her associated poisson process @xmath11 has undergone a jump , but prior to her making a decision or receiving any reward .",
    "similarly , let @xmath94 denote the optimal expected total reward of agent @xmath10 after a jump time , conditional on the decision problem not terminating either due to the agent leaving the system or choosing to switch to a different location .",
    "then , we have the following bellman equation : @xmath95 here , the first equation follows from the fact that subsequent to a jump time , the agent receives an immediate payoff equal to @xmath96 . following this",
    ", she continues with survival probability @xmath68 , and has to make a decision to stay , which gets her expected payoff equal to @xmath94 or switch , which gets her an expected payoff equal to @xmath80 .",
    "the second equation relates @xmath94 to the agent s expected payoff subsequent to various events that can occur at the next transition . for a solution @xmath97 and @xmath98 to the bellman s equation , an optimal strategy @xmath41 for the agent @xmath10 requires agent @xmath10 to stay if @xmath99 , and to switch if @xmath100 ( any mixed action is optimal if @xmath101 ) .",
    "let @xmath102 denote the set of all optimal strategies ( not necessarily threshold strategies ) for the agent s decision problem @xmath103",
    ".      given the markov chain @xmath76 and an agent s decision problem @xmath81 , we are now ready to state the equilibrium conditions on the limiting system .",
    "first , in the infinite system , we require the agents strategies to be in equilibrium .",
    "since @xmath76 describes the dynamics of a location where all agents other than agent @xmath10 use the threshold policy @xmath55 , for equilibrium we must impose the condition that @xmath55 is an optimal strategy for the agent s decision problem .",
    "this leads to the following condition : @xmath104    if all agents at location @xmath3 , including agent @xmath10 , follow the threshold policy @xmath55 , then the transitions in @xmath91 follow a markov chain with transition rate matrix @xmath105 that is equal to @xmath106 except for the transition @xmath107 which is equal to @xmath108 .",
    "this is because the arrival and the changes in the resource level occur at the same rate , but now any one of the agents at location @xmath3 might choose to leave the location , as opposed to any one of the agents other than agent @xmath10 as defined in @xmath106 . denote this markov chain by @xmath109 and let @xmath110 denote an invariant distribution of this chain : @xmath111 in a large system , a natural requirement to impose is that the invariant distribution of a single location @xmath3 equals the steady state empirical distribution of the resource level and the number of agents across all locations . requiring this condition to hold leads to two consequences .",
    "first , because in the infinite system the `` agent density '' , i.e. , mean number of agent across all locations , is equal to @xmath112 , this implies that the expected number of agents at location @xmath3 must equal @xmath112 : @xmath113 note that this equation imposes a restriction on the arrival rate @xmath75 of the markov chain @xmath76 . in particular , it requires the arrival rate to be such that in steady state the expected number of agents at each location is equal to @xmath112 .",
    "the second condition imposes a restriction on the immediate termination reward on switching @xmath80 .",
    "recall that we interpret @xmath80 as modeling the optimal continuation payoff on switching in the finite system . since the empirical distribution of the states of other locations is given by @xmath110 , the optimal expected reward an agent can obtain on switching",
    "is given by @xmath114 .",
    "this is because , after the agent moves to a location in state @xmath82 , which happens with probability @xmath115 , the number of agents at that location becomes @xmath116 , and the expected payoff to that agent is @xmath117 .",
    "we require that this quantity equals the immediate reward @xmath80 : @xmath118    given these consistency conditions , we are now ready to define a mean field equilibrium for the infinite system :    a mean field equilibrium is characterized by a threshold strategy @xmath55 , an arrival rate @xmath53 , an distribution @xmath110 over @xmath119 , and an immediate reward @xmath120 , such that the set of equations , , , and hold .",
    "note that as opposed to a pbe , a mean field equilibrium adopts a fairly natural and simple model of agent behavior , where each agent needs to keep track only of current state and the number of agents at the location she is in , along with the immediate payoff for switching .",
    "having defined the equilibrium concept , we now consider the problem of existence of a mean field equilibrium in the infinite system .",
    "we begin with the following lemma that shows that for any level of resource at a location , the value function @xmath94 is non - increasing with the number of agents at that location .",
    "the proof may be found in appendix  [ ap : few - proofs ] .",
    "[ lem : value - decreasing ] for each @xmath121 , the value function @xmath122 is non - increasing in @xmath28 .    using this lemma , it is straightforward to show that for any level of resource @xmath7 , if it is optimal to switch when the number of agents at the location is @xmath28 , then it is still optimal to switch when the number of agents is greater than @xmath28 . from this , we obtain the first result of this section .",
    "[ thm : threshold ] for any @xmath123 , @xmath124 and @xmath79 , there always exists an optimal strategy with a threshold structure for the decision problem @xmath81 .",
    "the preceding theorem suggests that set of threshold strategies is closed under best - responses : if all agents other than agent @xmath10 adopt the same threshold strategy , then it is optimal for agent @xmath10 to also follow a threshold strategy .",
    "thus , it suffices to focus on the set of optimal threshold strategies for the agent decision problem , which we denote by @xmath125 .",
    "note that @xmath125 can be characterized as a subset of @xmath126 corresponding to the values of the thresholds of the optimal threshold strategies . in lemma  [ lem : thresholds - convex ] in the appendix  [ ap : few - proofs ] , we show that @xmath125 is a convex set .",
    "building on this result , we obtain the main theorem of our paper .",
    "[ thm : existence ] for any @xmath127 , @xmath50 and @xmath128 , there exists a mean field equilibrium for the infinite system .",
    "we emphasize that the existence of a mean field equilibrium is obtained under very general conditions , requiring only that the reward function @xmath31 is non - increasing in the number of agents @xmath28 at a location , and converging to zero as @xmath28 tends to infinity .",
    "our proof technique can be extended to cases where the resource level @xmath36 at any location can take values in any finite subset of @xmath129 .",
    "the full proof of theorem  [ thm : existence ] is technical and is omitted due to space constraints . instead ,",
    "in the next section , we sketch the main ideas behind the proof and provide a brief outline . selected portions of the proof may be found in appendix  [ ap : few - proofs ] , and a full proof will appear later in a longer version of the paper .",
    "the proof of theorem  [ thm : existence ] follows by applying kakutani s fixed point theorem on carefully defined map @xmath130 , whose fixed points correspond to the mean field equilibria of the infinite system . to define the map @xmath130 requires a number of intermediate steps , which we outline below .",
    "the first step of the proof involves showing that given any @xmath123 and @xmath120 , there exists a unique @xmath75 and distribution @xmath110 such that @xmath110 is the unique invariant distribution of the markov chain @xmath109 ( i.e. , @xmath110 satisfies equation ) , and for which condition holds .",
    "this step itself involves first showing that for any @xmath53 , the markov chain @xmath131 is irreducible and positive recurrent , and hence has a unique stationary distribution @xmath132 . to show this , we use coupling arguments to bound the markov chain @xmath133 between two @xmath134 queues",
    "then , we show that the quantity @xmath135 is strictly increasing and continuous over an interval of values of @xmath75 , which suffices to show that there exists a value of @xmath75 satisfying .",
    "we then compute the value function @xmath98 satisfying the bellman s equation for the decision problem @xmath136 , where @xmath75 is the value of the arrival rate obtained in the first step . using this value function",
    ", we identify the set @xmath125 of optimal threshold strategies .    finally , using the value function @xmath98 and the invariant distribution @xmath110 from the first step , we compute the total expected payoff of an agent subsequent to switching to a different location chosen uniformly at random , defined as @xmath137 .",
    "the map @xmath130 is then defined as follows : for each @xmath123 and @xmath120 , we define @xmath138 .",
    "we depict the map pictorially in fig .",
    "[ fig : map - t ] .",
    "( originalthreshold ) @xmath55 ; ( kappa ) [ right=1.5 cm of originalthreshold ] @xmath75 ; ( pi ) [ right=1.5 cm of kappa ] @xmath110 ; ( c ) [ below=1.5 cm of originalthreshold ] @xmath80 ; ( vhat ) [ below=1.5 cm of kappa ] @xmath139 ; ( ctilde ) [ below=1.5 cm of pi ] @xmath140 ; ( newthreshold ) [ below=1.5 cm of vhat ] @xmath141 ;    ( originalthreshold.east )  ( kappa.west ) ; ( originalthreshold.north ) edge[out=15 , in=165 , singlearrow ] ( pi.north west ) ; ( c.east )  ( vhat.west ) ; ( vhat.east )  ( ctilde.west ) ; ( kappa.south )  ( vhat.north ) ; ( pi.south )  ( ctilde.north ) ; ( originalthreshold.south )  ( vhat.north west ) ; ( vhat.south )  ( newthreshold.north ) ; ( c.south )  ( newthreshold.north ) ;    by definition , any fixed point @xmath142 of the map @xmath130 must satisfy @xmath143 , and @xmath144 .",
    "this implies that @xmath55 is an optimal threshold strategy for the decision problem @xmath81 , and hence is also an optimal strategy and equation holds .",
    "recall that the arrival rate @xmath75 and the invariant distribution @xmath110 satisfy equations and .",
    "finally , @xmath145 implies that equation holds . from this",
    "we conclude that @xmath55 , @xmath80 and resulting @xmath75 and the invariant distribution @xmath110 together constitute a mean field equilibrium .",
    "thus each fixed point of the map @xmath130 corresponds to a mean field equilibrium .    to show that the map @xmath130 has a fixed point , we apply kakutani s fixed point theorem . for this",
    ", we first identify a compact set @xmath146 ^ 2\\times [ { { \\underaccent{\\bar}{c } } } , { \\bar{c}}]$ ] of @xmath147 , with @xmath148 and @xmath149 , and show that @xmath150 .",
    "we include the proof of this step in appendix  [ ap : compactness - map ] .",
    "second , we show that the map is upper hemicontinuous under the euclidean topology .",
    "to show this , we need to show that each of the intermediate maps in fig .",
    "[ fig : map - t ] is continuous ( or upper hemicontinuous if the map is a correspondence ) under appropriate topologies .",
    "we apply berge s maximum theorem @xcite to show the map from @xmath151 to @xmath152 is continuous , and the continuity of fixed points of continuous contraction mappings @xcite to show the map from @xmath153 to the value function @xmath98 is continuous .",
    "finally , we show that the image of @xmath130 is convex and non - empty for all values of @xmath151 .",
    "then , it follows by a direct application of kakutani s fixed point theorem that the map @xmath130 has a fixed point , and consequently a mean field equilibrium exist in the infinite system .",
    "having shown the existence of a mean field equilibrium for the infinite system , we now study how the model parameters and the reward function @xmath154 affect the equilibrium agent behavior .",
    "we perform this investigation numerically by computing the mean field equilibrium for a range of parameter values , and studying how the equilibrium thresholds @xmath55 and the stationary distribution vary .",
    "as our model is invariant to the proportional scaling of the decision epoch rates @xmath73 , and the holding rates @xmath155 and @xmath156 , we assume for our computations that @xmath157 and vary the holding rates .",
    "further , we restrict our attention to the symmetric case where the holding rates are equal : @xmath158 . in all our computation",
    ", we set the agent density @xmath159 , and the survival probability @xmath160 .",
    "for this , we study the mean field equilibrium in three reward settings : ( 1 ) @xmath161 for all @xmath28 ; ( 2 ) @xmath162 for all @xmath28 ; and ( 3 ) @xmath163 for all @xmath28 .",
    "before we discuss the results of our numerical investigation , we briefly describe our approach to compute an ( approximate ) mean field equilibrium of the infinite system .",
    "recall that the mean field equilibria of our model are the fixed points of the correspondence @xmath130 .",
    "we thus seek to find ( approximate ) fixed points of this map .",
    "to do this , we adopt a brute - force approach .",
    "we first truncate the state space @xmath119 of the agent decision problem to @xmath164 .",
    "we restrict the thresholds @xmath60 to grid of values in @xmath165 ^ 2 $ ] , where we set the grid resolution @xmath166 adaptively over different runs .",
    "we do a similar adaptive meshing of the set of values of the immediate payoff @xmath80 . having restricted the set of values of @xmath60 and @xmath80 thus , we search over all values to find lie in the image of @xmath130 , within some pre - specified tolerance @xmath167 .",
    "we describe this process in detail :    1 .   for each value of @xmath55",
    ", we perform a binary search on @xmath75 to find a value for which the stationary distribution @xmath110 of the markov chain @xmath76 restricted to @xmath168 satisfies equation @xmath169 with a tolerance @xmath167 .",
    "( the stationary distribution @xmath110 is obtained by solving the set of linear equations . ) 2 .   for this value of @xmath75 , @xmath60 and each value of @xmath80 ,",
    "we perform value iteration to compute the value function @xmath98 , again with a tolerance of @xmath167 , and compute the set of optimal thresholds @xmath170 . let @xmath171 denote the distance between @xmath60 and the @xmath141 under the euclidean norm .",
    "( note that the latter set is convex and the distance is well defined . )",
    "3 .   next , using the stationary distribution @xmath110 and the value function @xmath98 , we compute the immediate payoff @xmath172 .",
    "4 .   for each value of @xmath173 and @xmath80 ,",
    "we compute @xmath174 .",
    "we output the value of @xmath175 that minimizes @xmath176 over all values .",
    "to make the brute - force search efficient , we run this algorithm sequentially and adaptively by first identifying candidate regions where equilibria might exist , and restricting the search to those regions with lower tolerance and finer grid values .      in table",
    "[ table ] we report computationally determined values for three different reward functions , obtained over five different rates for the underlying resource process .",
    ".computationally determined approximate equilibrium values of the payoff for switching @xmath80 , and the thresholds @xmath177 and @xmath178 used to decide whether to switch or not .",
    "values are reported in the table as @xmath179 , for each value for reward function @xmath154 and the rate @xmath180 at which the resource level changes .",
    "[ table ] [ cols=\"^,^,^,^\",options=\"header \" , ]     for large values of @xmath180 , we see @xmath177 and @xmath178 are close .",
    "this is natural because large values of @xmath180 imply that the resource level is changing very quickly , and so the level of resource at the time of the decision has little impact on what the resource level will be at the next time the agent receives a reward .",
    "thus , the current level of resource @xmath181 has little impact on the threshold @xmath57 . on the other hand , for small values of @xmath180 ,",
    "the thresholds differ significantly with the resource level .",
    "we also observe that , when comparing reward functions @xmath163 , @xmath182 , and @xmath183 , when @xmath154 decreases more quickly , agents are more willing to switch ( the threshold for switching is lower ) , and the payoff for switching is also lower .",
    "we have studied a multi - agent location - specific resource - sharing game , and have established the existence of an equilibrium in this game , and have characterized each agent s policy in this equilibrium as being a threshold policy .",
    "this result provides economic insight into such multi - agent resource - sharing games , and also allows evaluating the effects of designing and modifying these games , through subsidies or penalties added to natural occurring rewards and costs .",
    "this work also sets the stage for studying information sharing in multi - agent resource - sharing systems .",
    "our current analysis assumes that agents observe only the number of other agents and resource level at their current location , and the locations they have visited in the past .",
    "one may extend this model to allow for information sharing among the agents as well as between the agents and a central planner who has access to the current state(s ) of the location(s ) that the agent mights switch to . a first question of interest",
    "would then be whether such information sharing necessarily improves social welfare , or whether it can in fact degrade it .",
    "a second question is how this information sharing mechanism should be designed to maximize social welfare .",
    "in this appendix we provide proofs of selected results discussed in the main text .",
    "a full proof of the main theorem is technical , and is omitted due to space constraints .",
    "this full proof will appear in a future version of the paper .      by markov property",
    "we may assume at @xmath184 the agent makes his current decision , and let @xmath185 be the time his next decision epoch starts .",
    "we denote @xmath186 as the distribution of @xmath187 given @xmath188 , then we can write @xmath189    let @xmath190 for all @xmath181 and @xmath28 .",
    "compute @xmath191 and @xmath192 using value iteration for the bellman s equation .",
    "@xmath193 by convergence of value iteration we have @xmath194 and @xmath195 as @xmath196 .",
    "therefore , it suffices to show @xmath197 , for @xmath198 , @xmath199 and @xmath200 are non - increasing in @xmath28 .",
    "we can prove this by induction on @xmath201 .",
    "the base case follows trivially .",
    "now assume @xmath200 is non - increasing in @xmath28 for @xmath198 , for some @xmath202 . from this",
    "it is straightforward to conclude that @xmath203 must be non - increasing in @xmath28 since both @xmath31 and @xmath200 are non - increasing in @xmath28 .",
    "thus , we only need to show that @xmath192 is also non - increasing in @xmath28 .    observe showing @xmath204 is equivalent to showing @xmath205 @xmath206 .",
    "we show this using a sample path argument by considering two processes .",
    "let @xmath207 be a copy of @xmath76 that starts at @xmath208 and let @xmath209 be a copy of @xmath76 that starts at @xmath210 . by carefully coupling the two processes , the details",
    "of which we omit due to space restrictions , it can be shown that for all @xmath211 , @xmath212 and @xmath213 . since @xmath214 and @xmath215",
    ", we have @xmath216,\\end{aligned}\\ ] ] and @xmath217.\\end{aligned}\\ ] ] since @xmath199 is non - increasing in @xmath28 for both @xmath218 , @xmath219 in all sample paths , and therefore @xmath220 \\geq { \\ensuremath{\\mathbf{e}}}[v^{(m)}(z_t^2 , n_t^2)]$ ] , which completes the proof .        from lemma  [ lem : value - decreasing ] , we know that the value function @xmath122 is non - increasing in @xmath28 for each @xmath7 . now for @xmath7 , denote @xmath222 and @xmath223 . for all @xmath224 ,",
    "the optimal action is to stay at the current location and for all @xmath225 , the optimal action is to switch to a different location . for any integer @xmath226 $ ] , we have @xmath227 , meaning the agent is indifferent between staying in the current location or switching to a different location when the state at the current location is @xmath82 .",
    "let @xmath228 and @xmath229 be two threshold strategies that are both optimal : @xmath230 for @xmath231 . by the definition of the threshold strategy",
    ", this implies that @xmath232 , and @xmath233 for each @xmath234 .",
    "this is because in the threshold strategy @xmath235 , at state @xmath82 , the agent stays at her current location for all @xmath236 , switches to a different location for all @xmath237 and and stays with probability @xmath238 and switches with the remaining probability if the number of agents @xmath28 is equal to @xmath239 .",
    "since this is true for each @xmath234 , we have for any @xmath240 , @xmath241 , and @xmath242 .",
    "this implies that a threshold strategy that at state @xmath82 stays in the current location if @xmath243 , switches to a different location if @xmath244 , and stays with probability @xmath245 and switches otherwise if @xmath246 is also optimal .",
    "this implies that @xmath247 also lies in the set @xmath221 , and hence the latter set is convex .        towards that goal ,",
    "we define @xmath251 , @xmath252 as follows : @xmath253 it is straightforward that @xmath254 . also , for @xmath255 , define @xmath256\\notag\\\\         & \\quad + \\frac{\\gamma^{\\lfloor ( \\log n)^{1/2 } \\rfloor}}{1-\\gamma}f(1).\\end{aligned}\\ ] ] note @xmath257 is decreasing in @xmath28 and @xmath258 as @xmath259 .",
    "we pick @xmath260 such that @xmath261        we prove this theorem in two steps .",
    "first , in lemma  [ lem : compactness - of - c ] , we show that for all values of @xmath265 in @xmath248 , the value of @xmath140 lies in @xmath266 $ ] .",
    "then , we show in lemma  [ lem : compactness - threshold ] , that for all values of @xmath265 in @xmath248 , the optimal thresholds must always be less than @xmath260 .",
    "we begin with the first lemma .",
    "[ lem : compactness - of - c ] for any @xmath267 $ ] and @xmath268 $ ] , let @xmath139 be the solution of the bellman equation with given parameters @xmath269 .",
    "let @xmath110 be the unique stationary distribution of @xmath109 .",
    "let @xmath270 .",
    "then @xmath271 $ ] .",
    "we first show @xmath272 .",
    "we have @xmath273 where the last equality is implied by lemma  [ lem : value - decreasing ] .",
    "also note @xmath274 for @xmath198 , therefore @xmath275 from lemma  [ lem : value - decreasing ] we have @xmath276 for @xmath198 .",
    "further , assume @xmath277 attains @xmath278 , then @xmath279 , and becomes @xmath280 we have @xmath281 , along with this gives @xmath282 .",
    "thus , we have @xmath283        let @xmath294 be the number of agents in our system at time @xmath16 , and @xmath295 be that of an @xmath134 queue with arrival rate @xmath75 and service rate @xmath296 .",
    "assume @xmath297 . using a coupling argument , which we omit due to space limitations",
    ", it can be shown that @xmath295 stochastically dominates @xmath294 for all @xmath1",
    ". therefore , @xmath298 where the last equality follows from the steady state distribution of the @xmath134 queue .",
    "in lemma  [ lem : bound - expec - agents ] we will show @xmath312 .",
    "we have assumed @xmath313 $ ] , hence @xmath314 .",
    "since @xmath154 is monotonically decreasing , becomes : @xmath315 which gives us the desired result .",
    "suppose at @xmath184 the @xmath321 decision epoch the agent starts , and there are @xmath28 agents at the node .",
    "we show for large enough @xmath28 , the total expected payoff the agent receives if he chooses to stay at @xmath184 will be less than what he would receive if he chooses to switch to a different location .",
    "let @xmath322 be the first decision epoch the agent chooses to leave .",
    "we seek to show that @xmath323 .",
    "suppose , for the sake of arriving at a contradiction , @xmath324 .",
    "let @xmath325 be the immediate reward the agent receives at his @xmath326 decision epoch if he is still at the current node at the time he makes his @xmath326 decision , and @xmath327 be the total reward he receives on choosing to stay at @xmath184 .",
    "we have : @xmath328 & = \\sum_{i=0}^{+\\infty } \\gamma^i { \\ensuremath{\\mathbf{e}}}[r_i{\\ensuremath{\\mathbf{1}}}{\\{\\tau > i\\ } } ] + { \\ensuremath{\\mathbf{e}}}[\\gamma^\\tau c ] \\notag \\\\                & \\leq \\sum_{i=0}^{+\\infty } \\gamma^i { \\ensuremath{\\mathbf{e}}}[r_i{\\ensuremath{\\mathbf{1}}}{\\{\\tau > i\\ } } ] + \\gamma c.    \\end{aligned}\\ ] ] the first term in the first equation of is the expected total reward until the agent chooses to leave , and the second term is the aggregated expected payoff on leaving .",
    "the inequality follows from the assumption that @xmath324 .",
    "our goal is to show @xmath329 $ ] vanishes as @xmath259 , hence @xmath330 \\rightarrow \\gamma c < c$ ] as @xmath259 , and since @xmath80 is the aggregated expected payoff on leaving at @xmath184 , we would have @xmath323 .",
    "let @xmath331 be the time the @xmath332 decision epoch of the agent starts .",
    "we have @xmath333 since the interval between two consecutive decision epochs are i.i.d . @xmath334 and @xmath331 is the sum of @xmath10 such intervals .",
    "let @xmath335 .",
    "we have for all @xmath336 , @xmath337 & = { \\ensuremath{\\mathbf{e}}}[r_i{\\ensuremath{\\mathbf{1}}}{\\{\\tau\\geq i\\ } } | n_{t_i } \\geq n']{\\ensuremath{\\mathbf{p}}}(n_{t_i } \\geq n')\\notag \\\\ & \\quad + { \\ensuremath{\\mathbf{e}}}[r_i{\\ensuremath{\\mathbf{1}}}{\\{\\tau \\geq i \\ } } | n_{t_i } < n']{\\ensuremath{\\mathbf{p}}}(n_{t_i } < n ' ) \\notag \\\\              & \\leq f(n'){\\ensuremath{\\mathbf{p}}}(n_{t_i } \\geq n ' ) + { \\ensuremath{\\mathbf{p}}}(n_{t_i } < n')f(1 ) \\notag \\\\              & \\leq f(n ' ) + { \\ensuremath{\\mathbf{p}}}(n_{t_i } <",
    "n')f(1 ) .      \\end{aligned}\\ ] ] we first show @xmath338 vanishes as @xmath259 . consider an alternative system with @xmath28 agents at @xmath184 where each agent stays an @xmath334 time and then leaves the system .",
    "let @xmath339 be the number of agents in the system at time @xmath16 .",
    "for any agent in this alternative system , the probability he is still in the system at time @xmath331 is @xmath340 , therefore we have @xmath341 .    using a similar argument as in the proof of lemma  [ lem : value - decreasing ] , we can show for all @xmath211 , @xmath342 is no less than @xmath339 in all sample paths , i.e. , @xmath342 stochastically dominates @xmath339 in the first order .",
    "let @xmath343 .",
    "note @xmath344 , for all @xmath345 .",
    "thus , pick @xmath346 .",
    "we have @xmath347    we have @xmath348 .",
    "given @xmath349 , we can apply the chernoff bound to obtain , @xmath350 also , by markov s inequality we have @xmath351}{t_k }   = \\frac{k}{\\lambda t_k }   = \\frac{2\\lfloor ( \\log n)^{\\frac{1}{2 } } \\rfloor}{\\log n }            \\leq \\frac{2}{\\sqrt{\\log n}}.      \\end{aligned}\\ ] ] therefore , @xmath352 by and using the fact that @xmath353 , we have @xmath354 \\leq f(\\frac{\\sqrt{n}}{2 } ) + \\exp(-\\frac{\\sqrt{n}}{8 } ) + \\frac{2}{\\sqrt{\\log n}},\\ ] ] for all @xmath355 .",
    "therefore , we have @xmath356 & =   \\sum_{i=0}^{k - 1 } \\gamma^i { \\ensuremath{\\mathbf{e}}}[r_i { \\ensuremath{\\mathbf{1}}}{\\{\\tau > i\\ } } ] +   \\sum_{i = k}^{\\infty } \\gamma^i { \\ensuremath{\\mathbf{e}}}[r_i { \\ensuremath{\\mathbf{1}}}{\\{\\tau > i\\ } } ] \\\\          & \\leq \\sum_{i=0}^{k-1 } \\gamma^i \\left [ f(\\frac{\\sqrt{n}}{2 } ) + \\exp(-\\frac{\\sqrt{n}}{8 } ) + \\frac{2}{\\sqrt{\\log n } } \\right]+   \\sum_{i = k}^{\\infty } \\gamma^i f(1 ) \\\\          & \\leq \\frac{1}{1-\\gamma }",
    "\\left [ f(\\frac{\\sqrt{n}}{2 } ) + \\exp(-\\frac{\\sqrt{n}}{8 } ) + \\frac{2}{\\sqrt{\\log n } } \\right ] + \\frac{\\gamma^k}{1-\\gamma}f(1 ) .      \\end{aligned}\\ ] ]    the righthand side is denoted as @xmath257 in . as @xmath357 , @xmath257 goes to 0 .",
    "hence by picking @xmath260 such that @xmath358 we have @xmath359 < ( 1-\\gamma){{\\underaccent{\\bar}{c}}}+ \\gamma c < c,\\ ] ] for all @xmath360 .",
    "thus we have proved choosing to stay at @xmath184 when there are more than @xmath260 agents at the location is suboptimal no matter what the resource level is .",
    "this contradicts the assumption that @xmath324 ."
  ],
  "abstract_text": [
    "<S> we consider a model of nomadic agents exploring and competing for time - varying location - specific resources , arising in crowdsourced transportation services , online communities , and in traditional location - based economic activity . </S>",
    "<S> this model comprises a group of agents , and a set of locations each endowed with a dynamic stochastic resource process . </S>",
    "<S> each agent derives a periodic reward determined by the overall resource level at her location , and the number of other agents there . </S>",
    "<S> each agent is strategic and free to move between locations , and at each time decides whether to stay at the same node or switch to another one . </S>",
    "<S> we study the equilibrium behavior of the agents as a function of dynamics of the stochastic resource process and the nature of the externality each agent imposes on others at the same location . in the asymptotic limit with the number of agents and locations increasing proportionally , </S>",
    "<S> we show that an equilibrium exists and has a threshold structure , where each agent decides to switch to a different location based only on their current location s resource level and the number of other agents at that location . </S>",
    "<S> this result provides insight into how system structure affects the agents collective ability to explore their domain to find and effectively utilize resource - rich areas . </S>",
    "<S> it also allows assessing the impact of changing the reward structure through penalties or subsidies . </S>"
  ]
}