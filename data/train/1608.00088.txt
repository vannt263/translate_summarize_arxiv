{
  "article_text": [
    "a linear classifier for two or more populations is presented when the data are high - dimensional and possibly non - normal .",
    "let @xmath2 be @xmath3 independent and identically distributed random vectors from @xmath4th population with distribution function @xmath5 , where @xmath6 , @xmath7 , @xmath8 , @xmath9 , are the mean vector and covariance matrix .",
    "given this set up , we are interested to construct a linear classifier for high - dimensional , low sample size settings , i.e. @xmath10 , and when @xmath5 can be non - normal .",
    "+ classification and regression are two of the most powerful tools of statistical analysis , both as main objective of analysis on their own and also as a source of further investigation . due to ever growing complexity of data",
    ", classification has particularly attracted a central place in modern statistical analysis .",
    "the wave of large - dimensional data sets in the last few decades and their associated questions of analysis have lead the researchers to substantially think and improve the classical framework of classification and discrimination .",
    "+ this paper mainly addresses the classification problem for such a complex data set up , particularly when the dimension of the multivariate vector , @xmath11 , may exceed the number of such vectors , @xmath3 , i.e. , @xmath10 ( see sec . [",
    "sec : applns ] for examples ) . as the classical theory of classification does not work in this case , mainly due to the singularity of empirical covariance matrix ( see sec .",
    "[ sec : twosampdf ] for more details ) , efforts have been made in the literature to offer some potential alternatives .",
    "@xcite propose independence rule ( ir ) , or naive bayes rule , by using only the diagonal of the empirical covariance matrix and compare it to fisher s linear discriminant function ( ldf ) for the case of two normal populations . under certain general conditions on the eigenvalues of the scaled covariance matrix ,",
    "they show that ir under independence assumption is comparable to fisher s ldf under dependence when the empirical covariance matrix is replaced with a @xmath12-inverse computed from the empirical non - zero eigenvalues and the corresponding eigenvectors .",
    "a regularized discriminant analysis using fisher s ldf is given in @xcite .",
    "for a short but useful review of this and other classification methods for high - dimensional data , see @xcite .",
    "further relevant approaches will be referred to in their appropriate context in the sequel . + we begin in sec .",
    "[ sec : twosampdf ] with the two - sample @xmath0-classifier , giving detailed explanations on the its construction and justification .",
    "an extension to the multi - sample case is given in sec .",
    "[ sec : multsampdf ] .",
    "accuracy of the classifier under different parameter settings is shown in sec .",
    "[ sec : simns ] , whereas the practical applications on real data sets are demonstrated in sec .",
    "[ sec : applns ] .",
    "all technical proofs are deferred to the appendix .",
    "this section is devoted to the construction and justification of the two - sample classifier along with its properties , asymptotic distribution and misclassification rate .",
    "these are , respectively , the subjects of the next three subsections .",
    "let @xmath13 be as defined above and @xmath14 denote the @xmath4th ( unknown ) population , @xmath15 ( @xmath16 ) .",
    "let @xmath17 be the new observation to be classified to either of the two populations where the misclassification errors are represented by the conditional probabilities @xmath18 using the information on @xmath11 characteristics in each sample , i.e. @xmath19 , we aim to construct a classifier which assigns @xmath17 to @xmath14 , @xmath15 , optimally , i.e. by keeping @xmath20 as small as possible , as @xmath21 , when ( i ) @xmath11 may arbitrarily exceed @xmath3 , @xmath10 , ( ii ) @xmath5 may not necessarily be normal , and ( iii ) @xmath22 s may be unequal , @xmath23 .",
    "note that high - dimensional or , as is frequently known , @xmath24-asymptotic framework is kept general in that it implies both @xmath25 and @xmath26 but without requiring the two indices to satisfy any specific relationship of mutual growth order .",
    "it will , however , be shown in the sequel that some of the results hold even by assuming @xmath3 fixed and any arbitrary @xmath11 . + first , to set the notations , let @xmath27 be the usual unbiased estimators of @xmath28 and @xmath22 , respectively .",
    "the classical two - sample linear classifier , assuming equal and known @xmath29 with equal costs and priors can be expressed , ignoring the constants , as ( * ? ? ?",
    "6 ) @xmath30 where @xmath17 is the point to be classified .",
    "although this classifier is usually constructed under normality assumption , using a ratio of multivariate normal density functions of the two populations and substituting @xmath31 ( which makes the classifier linear ) , it is well - known that fisher constructed the same classifier without assuming normality , and hence it is also known as fisher s linear discriminant function .",
    "it is the most frequently used classifier in practice and assuming @xmath32 and normality , the misclassification probability can be computed using the normal distribution function .",
    "obviously , with @xmath29 unknown in practice , we need to estimate @xmath33 , replacing @xmath29 with its usual pooled estimator , @xmath34 , under the homoscedasticity assumption , where @xmath35 are defined above , so that @xmath36 when the data are high - dimensional , i.e. , when @xmath37 , @xmath35 , hence , @xmath38 , are singular and can not be inverted , implying that @xmath39 can not be used in this case . to see how the situation develops in this framework ,",
    "let us first take @xmath40 out of the classifier in ( [ eqn : classtwosampdf ] ) and consider @xmath41 assuming @xmath42 , we immediately note that @xmath43&= & \\frac{1}{2}\\|{{\\boldsymbol{\\mu}}}_1 - { { \\boldsymbol{\\mu}}}_2\\|^2 - \\text{b},\\label{eqn : meanclassdfwbias}\\end{aligned}\\ ] ] where @xmath44 is the euclidean norm and b = @xmath45 .",
    "we note that , removing the covariance matrix makes the resulting classifier biased with the bias term , b , composed of the traces of the unknown covariance matrices . if @xmath31 , then b = @xmath46 , so that the classifier is positively or negatively biased given @xmath47 or @xmath48 .",
    "+ to inherently adjust the classifier for its bias and improve its accuracy , consider the second component of @xmath33 in eqn .",
    "( [ eqn : classtwosampdfwosigma ] ) , and note that @xmath49 = \\text{b } + ( { { \\boldsymbol{\\mu}}}'_1{{\\boldsymbol{\\mu}}}_1 - { { \\boldsymbol{\\mu}}}'_2{{\\boldsymbol{\\mu}}}_2)/2,\\ ] ] where @xmath50 were used to complete the squared norm in the expectation of complete classifier in ( [ eqn : meanclassdfwbias ] ) and b is the same bias term .",
    "now @xmath51 where @xmath52 , @xmath53 , @xmath54 , so that @xmath55 and @xmath56 .",
    "let @xmath57 , @xmath58 .",
    "then @xmath59 where @xmath60 .",
    "as r appears with opposite signs in the two components , adjusting each component by this amount , keeping the total expectation same , we have @xmath61 + r = @xmath62 and @xmath63 - r = 2b . adjusting the corresponding terms in @xmath33 in ( [ eqn : classtwosampdfwosigma ] )",
    "similarly , we re - write it as @xmath64 where @xmath65 , @xmath66 , is a one - sample @xmath0-statistic with symmetric kernel , @xmath67 , @xmath54 , which is a bilinear form of two independent components . now , assuming @xmath42 and independent of the elements of both samples , we have @xmath68 = { { \\boldsymbol{\\mu}}}'_1{{\\boldsymbol{\\mu}}}_1 - { { \\boldsymbol{\\mu}}}'_1{{\\boldsymbol{\\mu}}}_2 $ ] , so that @xmath69 = \\|{{\\boldsymbol{\\mu}}}_1 - { { \\boldsymbol{\\mu}}}_2\\|^2/2,\\ ] ] without any bias term .",
    "further , with @xmath42 , @xmath70 is composed of four bilinear forms , two from sample 1 , one from sample 2 , and one mixed . by symmetry ,",
    "@xmath71 = -\\|{{\\boldsymbol{\\mu}}}_1 - { { \\boldsymbol{\\mu}}}_2\\|^2/2 $ ] , and the classifier again consists of four bilinear forms , two from sample 2 , one from sample 1 and one mixed .",
    "we , therefore , define the classification rule for the proposed @xmath0-classifier as @xmath72 before we study the properties of @xmath70 in the next section , a few important remarks are in order .",
    "first , @xmath70 is composed of bilinear forms - and we call it _ bilinear classifier _ - where the bi - linearity of the @xmath0-component is expressed in the kernels of the two @xmath0-statistics and that of the @xmath1-component by the projection of the new observation with respect to the difference between the empirical centroids of the two independent samples .",
    "further , @xmath70 is entirely composed of empirical quantities , free of any unknown parameter , so that it can be directly used in practice using the decision rule stated above .",
    "note also that , @xmath70 is linear but the linearity does not require homoscedasticity assumption , i.e. , it is linear even if @xmath23 .",
    "+ moreover , the first part of @xmath70 is normalized by @xmath11 , and so are the kernels of @xmath0-statistics in the second part .",
    "this will help us derive the limiting distribution of the classifier for @xmath24-asymptotics under a general multivariate model and mild assumptions . as a final remark ,",
    "recall that the formulation of @xmath70 arises from depriving the original classifier of empirical covariance matrix . whereas , this removal of an essential ingredient has its price to be paid , the resulting classifier can still be justified from a different perspective which has its merit , particularly for high - dimensional data .",
    "+ to see this , consider @xmath73 , @xmath74 , where @xmath75 is the euclidean distance of @xmath17 from sample @xmath15 , @xmath76 .",
    "it follows that @xmath77 has same bias b as for @xmath33 . for all expressions location - invariant , write @xmath78 , @xmath79 @xcite .",
    "now @xmath80 $ ] , and since @xmath81 , it simplifies to @xmath82 + b ; compare with eqn .",
    "( [ eqn : meanclassdfwbias ] ) .",
    "+ this implies that @xmath70 can also be constructed using euclidean distances and the same bias - adjustment that lead eqn .",
    "( [ eqn : meanclassdfwbias ] ) to eqn .",
    "( [ eqn : modifiedtwosdf ] ) .",
    "this distance - based approach has been discussed in @xcite and the same is further evaluated in @xcite .",
    "our approach , however , makes the classifier not only unbiased but also more general as well as convenient to study and apply in practice .",
    "given @xmath83 , let @xmath84 with @xmath85 , @xmath86 , @xmath15 .",
    "when we relax normality , we assume the following multivariate model @xmath87 where @xmath88 has iid elements with @xmath89 , @xmath90 , and @xmath91 is a known @xmath92 matrix of constants such that @xmath93 , @xmath94 , @xmath15 . to study the properties of the classifier and its asymptotic normality , we shall supplement model ( [ eqn : modellinn ] ) with the following assumptions .",
    "[ assn:4thmomnt ] @xmath95 , @xmath96 , @xmath15 .",
    "[ assn : tracesigma ] @xmath97 , @xmath15 .",
    "[ assn : extrah1distn ] @xmath98 , @xmath99 .",
    "[ assn : tracesigmahadprod ] @xmath100 , @xmath101 , @xmath102 , @xmath103 , where @xmath104 and @xmath105 are hadamard and kronecker products .",
    "assumption [ assn:4thmomnt ] essentially replaces normality .",
    "assumptions [ assn : tracesigma ] is simple and mild , and as its consequence , @xmath106 , so that a reference to the assumption may also imply it consequence in the sequel . assumptions [ assn : extrah1distn ] and [ assn : tracesigmahadprod ] are needed only to show control of misclassification rate and consistency of the moments of classifier .",
    "assumption [ assn : tracesigmahadprod ] ensures that the moments asymptotically coincide with those under normality .",
    "this assumption is neither directly needed in practical use of the classifier , nor is it required under normality whence all terms involving the ratio vanish .",
    "the same assumptions will be extended for multi - sample case in sec .",
    "[ sec : multsampdf ] .",
    "+ now , continuing to assume @xmath17 independent of all elements of sample 1 ( where it is already independent of all elements of sample 2 ) , we get the following lemma , proved in appendix [ subsec : prooflemmamomstwosdf ] , on the moments of classifier .    [",
    "lem : momstwosdf ] let the two - sample modified classifier be as given in eqn .",
    "( [ eqn : modifiedtwosdf ] ) . then , assuming @xmath107 , we have @xmath108 & = & \\frac{1}{2p}\\|{{\\boldsymbol{\\mu}}}_i - { { \\boldsymbol{\\mu}}}_j\\|^2_{\\bf i } = \\frac{1}{2p}\\delta^2_{\\bf i}\\label{eqn : meandftwos}\\\\ \\text{var}[a_0({\\bf x})|\\pi_i ] & = & \\frac{\\delta^2_i}{p^2 } + \\frac{1}{p^2}\\|{{\\boldsymbol{\\mu}}}_1 - { { \\boldsymbol{\\mu}}}_2\\|^2_{{{\\boldsymbol{\\sigma}}}^{-1}_i } = \\frac{\\delta^2_i}{p^2 } + \\frac{1}{p^2}\\delta^2_{{{\\boldsymbol{\\sigma}}}^{-1}_i},\\label{eqn : vardftwos}\\end{aligned}\\ ] ] where @xmath109 , @xmath103 , @xmath110 , for any @xmath111 , and @xmath112    the moments in lemma [ lem : momstwosdf ] are reported using general notation for @xmath107 so that they can be easily extended to the multi - sample case later . for the present case of @xmath16 , the moments easily reduce to their specific form for @xmath113 or @xmath114 , where the mean is obviously the same for both , i.e. , @xmath115 .",
    "+ with the picture of the proposed classifier and its moments relatively clear , we can express our high - dimensional classification problem precisely as @xmath116 where the index @xmath11 implies the dependence of components @xmath117 on the dimension . note that , the second component in eqn .",
    "( [ eqn : vardftwos ] ) vanishes under assumption [ assn : extrah1distn ] .",
    "the rest of @xmath118 $ ] , and @xmath119 $ ] , are uniformly bounded in @xmath11 , for any fixed @xmath3 , under assumptions [ assn : tracesigma ] .",
    "we can thus write @xmath120 & = & \\frac{1}{2}\\delta^2_{0 , { \\bf i } } \\label{eqn : lmtgmeandf}\\\\ \\lim_{p \\rightarrow \\infty}\\text{var}[a_0({\\bf x})|\\pi_i ] & = & \\delta^2_{0 , i}\\left[o\\left(\\frac{1}{n_1 } + \\frac{1}{n_2}\\right ) + o(1)\\right],\\label{eqn : lmtgvardf}\\end{aligned}\\ ] ] where @xmath121 and @xmath122",
    ". now , the variance obviously vanishes when we also allow @xmath25 along with @xmath26 , which immediately gives consistency of the classifier , formally stated in theorem [ thm : consistencytwosdf ] below and proved in appendix [ subsec : proofthmconsistwosdf ] .",
    "obviously , in practice , the consistency is expected to hold with unknown parameters replaced by their estimators .",
    "we need to estimate @xmath123 and non - vanishing traces in @xmath124 to estimate the limiting moments of the classifier . in the following ,",
    "we define unbiased and consistent plug - in estimators of these components .",
    "+ for @xmath125 , @xmath15 , let @xmath79 , @xmath54 , be as defined in sec .",
    "[ subsec : constructdf ] with @xmath126 , @xmath127 .",
    "also let @xmath128 using differences of vectors from two independent samples with @xmath129 , @xmath130 , @xmath103 , @xmath110 .",
    "extending the strategy of sec .",
    "[ subsec : constructdf ] for the estimation of @xmath131 , we define estimators of the traces involved in @xmath124 .",
    "let @xmath132 and @xmath133 , @xmath134 , @xmath110 , using within- and between - sample independent vectors , respectively .",
    "then , by independence , the plug - in estimators of @xmath135 , @xmath136 and @xmath137 are defined , respectively , as @xmath138 where @xmath139 and @xmath140 , @xmath103 , @xmath110 , and @xmath141 implies all indices pairwise unequal .",
    "further , @xmath142 a two - sample @xmath0-statistic , so that , with @xmath143 , @xmath15 , as one - sample @xmath0-statistic defined after eqn . (",
    "[ eqn : modifiedtwosdf ] ) , @xmath144 estimates @xmath145 .",
    "@xmath144 , @xmath146 , @xmath147 are unbiased and location - invariant estimators .",
    "the following theorem , proved in appendix [ subsec : proofthmpropsestrs ] , shows further that the variances of the ratios of these estimators to the parameters they estimate are uniformly bounded in @xmath11 , so that they are consistent as @xmath148 ( @xmath3 fixed ) and also when @xmath21 .",
    "[ thm : propsestrsbounds ] @xmath149 , @xmath147 , defined in eqns .",
    "( [ eqn : locinve0])-([eqn : locinve2 ] ) , are unbiased estimators of @xmath135 , @xmath136 and @xmath150 .",
    "further , under assumptions [ assn:4thmomnt]-[assn : tracesigmahadprod ] , @xmath151 @xmath152    the bounds in theorem [ thm : propsestrsbounds ] suffice for consistency of estimators so that exact variances and covariances are not needed .",
    "these exact moments follow from theorem [ thm : basicqfbfmomsnn ] and lemma [ lem : qfbfresults ] ; see @xcite . + from theorem [ thm : propsestrsbounds ] , it immediately follow that @xmath153 which gives empirical mean , @xmath154 = \\frac{1}{2}e_0 $ ] , as a consistent estimator of the true mean of the classifier . using similar probability convergence of the other two estimators @xmath146 and @xmath147 , we obtain , by slutsky s lemma ( * ? ? ?",
    "* p 11 ) , the first component of the empirical variance , @xmath155 $ ] , i.e. @xmath156 , as a consistent estimator of @xmath124 , @xmath15 , using plug - in estimators such that @xmath157 .",
    "the limiting empirical moments , parallel to eqns .",
    "( [ eqn : lmtgmeandf])-([eqn : lmtgvardf ] ) , thus follow as @xmath158 & = & \\frac{1}{2}\\delta^2_{0 , { \\bf i}}\\label{eqn : lmtgmeandfestd}\\\\ \\lim_{p \\rightarrow \\infty}\\widehat{\\text{var}}[a_0({\\bf x})|\\pi_i ] & = & \\delta^2_{0 , i}\\left[o\\left(\\frac{1}{n_1 } + \\frac{1}{n_2}\\right ) + o_p(1)\\right]\\label{eqn : lmtgvardfestd}\\end{aligned}\\ ] ] with @xmath159 , @xmath160 . hence @xmath161 - \\text{e}[a_0({\\bf x})|\\pi_i]\\right ] & = & o_p(1)\\label{eqn : lmtgmeandfdiff}\\\\ \\lim_{p \\rightarrow \\infty}\\left[\\widehat{\\text{var}}[a_0({\\bf x})|\\pi_i ] - \\text{var}[a_0({\\bf x})|\\pi_i]\\right ] & = &   o_p(1).\\label{eqn : lmtgvardfdiff}\\end{aligned}\\ ] ] the following theorem , proved in appendix [ subsec : proofthmconsistwosdf ] , summarizes both true and empirical consistency of the classifier .",
    "[ thm : consistencytwosdf ] given @xmath70 in eqn .",
    "( [ eqn : modifiedtwosdf ] ) with its moments as in lemma [ lem : momstwosdf ] .",
    "let @xmath107 . under assumptions [ assn:4thmomnt]-[assn : extrah1distn ] ,",
    "as @xmath21 , @xmath15 , @xmath162 with @xmath163 defined above .",
    "further , the consistency holds when the moments of the classifier are replaced with their empirical estimators , given in eqns .",
    "( [ eqn : lmtgmeandfestd])-([eqn : lmtgvardfestd ] ) .",
    "the same arguments help us establish asymptotic normality of the classifier as stated in the following theorem , proved in appendix [ subsec : proofthmasympntwosdf ] .",
    "[ thm : asympntwosdf ] given @xmath70 in eqn .",
    "( [ eqn : modifiedtwosdf ] ) with its moments as in lemma [ lem : momstwosdf ] .",
    "let @xmath107 . under assumptions [ assn:4thmomnt]-[assn : extrah1distn ] , as @xmath21 , @xmath15 , @xmath164}{\\sqrt{\\text{var}[a_0({\\bf x } ) ] } } \\xrightarrow{\\mathcal{d } } n(0 , 1),\\ ] ] further , the normal limit holds when the moments are replaced with their empirical estimators , given in eqns .",
    "( [ eqn : lmtgmeandfestd])-([eqn : lmtgvardfestd ] ) .",
    "the construction of @xmath70 is of great benefit in proving theorem [ thm : asympntwosdf ] .",
    "its composition of two parts , each of which in turn a linear combination of two independent components , reduces the bulk of computational burden .",
    "moreover , the optimality property of @xmath0-statistics ensures the minimum variance ( efficiency ) of the classifier .",
    "a further verification of these properties of the classifier through simulations is demonstrated in sec .",
    "[ sec : simns ] .",
    "consider the classification problem @xmath117 in ( [ eqn : classfcnprob ] ) again .",
    "using notation introduced in the beginning of sec .",
    "[ subsec : constructdf ] , the optimality of the proposed classifier can be evaluated by the misclassification rates @xmath165 and @xmath166 with @xmath167 where @xmath168 denotes the distribution function and @xmath169 is the region of observed data from @xmath4th population with @xmath170 , @xmath171 , where @xmath172 denotes the space of observed @xmath173 and @xmath174 is the empty set . under the assumption of equal probabilities and equal costs , we are interested to minimize the total probability of misclassification , say @xmath175 , given the observed data , i.e. @xmath176/2,\\ ] ] where the subscript @xmath177 stands for _",
    ", the ideal minimum can only be achieved when the parameters are known in which case the ( ideal ) classifier takes the form @xmath178 using the fact that @xmath179 and @xmath143 are unbiased estimators of @xmath28 and @xmath50 , respectively , @xmath15 . now ,",
    "if @xmath5 are known , say multivariate normal , i.e. @xmath180 , then , under the homoscedasticity assumption @xmath181 , the error rate of @xmath182 can be expressed as @xmath183 where @xmath184 is the standard normal distribution function . assuming equal priors , the best possible performance in this ideal setting , i.e. with @xmath185 , @xmath186 , @xmath187 known , is achieved by fisher s linear classifier ( equivalently , bayes rule ) @xmath188 @xcite with the corresponding misclassification rate given by @xmath189 where @xmath190 is the mahalanobis distance .",
    "denoting @xmath191 as a benchmark , the relative performance of @xmath192 can be theoretically evaluated by using the ratio of the arguments of @xmath184 , say @xmath193 , where @xmath194^{1/2}}.\\ ] ] @xcite put forth a nice strategy to compute a bound for an expression like @xmath193 , based on kantorovich inequality @xcite .",
    "following the same idea , let @xmath195 be any positive definite symmetric @xmath196 matrix . then for any vector @xmath197 @xmath198^{2 } } , \\ ] ] where @xmath199 and @xmath200 denote the smallest and the largest eigenvalues of @xmath201 , respectively . applying this inequality to @xmath193 and denoting",
    "the ratio @xmath202 ( assuming the two extreme eigenvalues bounded away from @xmath203 and @xmath204 ) , we get @xmath205 so that the upper bound on the misclassification probability of @xmath192 is @xmath206 which essentially depends on @xmath207 , the range of non - zero eigenvalues of @xmath187 .",
    "[ fig : boundq ]    we note that , for moderate @xmath207 , the increase in the misclassification rate , induced by taking the covariance matrix away while constructing @xmath192 , is not large relative to the best possible performance , i.e. @xmath208 ( see fig .",
    "further , the upper bound in ( [ eqn : boundq ] ) represents the worst - case scenario so that the empirical results are expected to be better .",
    "+ now , for an alternative flavor of the evaluation of the classifier , while still continuing to assume normality , let us condition the classifier on the data , i.e. , @xmath209 , say , and it immediately follows that @xmath210 @xmath15 .",
    "this , using the standardized version of classifier ( theorem [ thm : asympntwosdf ] ) , leads to the actual error rate @xmath211,\\end{aligned}\\ ] ] where the subscript @xmath212 denotes the dependence on the observed sample . using theorem [ thm : consistencytwosdf ] , @xmath213 so that by slutsky s lemma ( * ? ? ?",
    "* p 11 ) it follows that , as @xmath21 , @xmath214,\\ ] ] where the convergence remains ( asymptotically ) true even for the sample based classifier @xmath215 since @xmath216 is the limiting value of @xmath217 .",
    "+ finally , we consider a similar evaluation of the classifier under non - normality which we discuss for the general class of elliptically contoured distributions ( including multivariate normal ) .",
    "let @xmath218 , @xmath219 , where @xmath220 denotes a @xmath11-dimensional elliptical distribution with density function @xmath221 is a monotone ( decreasing ) function on @xmath222 and parameters @xmath223 and @xmath29 are specified as in model ( 7 ) .",
    "assume now that @xmath224 and denote the first two conditional moments of the classifier @xmath225 by @xmath226 , \\quad v_{i}= \\text{var } \\left [ a_{0}({\\bf x})| \\overline{\\bf x}_i , u_{n_i } \\right ] , \\ ] ] respectively , @xmath15 .",
    "@xcite has discussed fisher s linear discriminant function for elliptical distributions and also its robustness particularly in the context of @xmath227-estimation .",
    "our assertion in the following closely follows his structure .",
    "+ by conditioning the classifier on the data , i.e. by considering @xmath209 and using its standardized version , we get ( see * ? ?",
    "* theorem 1.1 , p 260 ) @xmath228 for any @xmath229 , where @xmath230 denotes the gamma function , @xmath231 is defined in ( [ eqn : eldens ] ) , and @xmath232 is the distribution function whose density function is given by @xmath233 the normal distribution is a special case of @xmath234 , for if @xmath235 , then @xmath236 reduces to the standard normal .",
    "now , using ( [ eqn : qdistr ] ) , we can express the conditional ( or actual ) misclassification probability as @xmath237.\\end{aligned}\\ ] ]",
    "it is obvious from the construction of the two - sample classifier that it can be easily extended to the multi - sample case .",
    "let @xmath238 be iid vectors with @xmath6 , @xmath239 , @xmath240 . the multi - sample version of classifier in eqn .",
    "( [ eqn : modifiedtwosdf ] ) can be expressed as @xmath241 @xmath242 , @xmath243 .",
    "alternatively , to write it in a more explicit form , let @xmath244 be the discriminant function for population @xmath4 , so that the classifier is @xmath245 for any distinct pair @xmath246 .",
    "the classification rule modifies to @xmath247 to study the properties of the multi - sample classifier and its asymptotic behavior , we first extend the two - sample assumptions for the general case .",
    "[ assn:4thmomntms ] @xmath248 , @xmath96 , @xmath8 .",
    "[ assn : tracesigmams ] @xmath97 , @xmath8 .",
    "[ assn : extrah1distnms ] @xmath98 , @xmath249 .",
    "[ assn : tracesigmahadprodms ] @xmath100 , @xmath101 , @xmath102 , @xmath250 , where @xmath104 and @xmath105 are hadamard and kronecker products .",
    "we begin by the following lemma which generalizes lemma [ lem : momstwosdf ] on the moments of the two - sample classifier .",
    "[ lem : momsmultsdf ] given @xmath70 in eqn .",
    "( [ eqn : multsdfdirectext ] ) or ( [ eqn : multsdfsimplerform ] ) .",
    "let @xmath107 . then",
    "@xmath108 & = & \\|{{\\boldsymbol{\\mu}}}_i - { { \\boldsymbol{\\mu}}}_k\\|^2/2p = \\delta^2_{\\bf i}/2p\\label{eqn : meandfmults}\\\\ \\text{var}[a_0({\\bf x})|\\pi_i ] & = & \\delta^2_i / p^2 + \\|{{\\boldsymbol{\\mu}}}_i - { { \\boldsymbol{\\mu}}}_k\\|^2_{{{\\boldsymbol{\\sigma}}}^{-1}_i}/p^2 = \\delta^2_i / p^2 + \\delta^2_{{{\\boldsymbol{\\sigma}}}^{-1}_i}/p^2,\\label{eqn : vardfmults}\\end{aligned}\\ ] ] where @xmath251 , @xmath252 or @xmath253 , and @xmath254    the moment estimators follow obviously from those of the two - sample case given in eqns .",
    "( [ eqn : locinve0])-([eqn : locinve2 ] ) .",
    "likewise , the consistency of these estimators follows from lemma [ lem : momstwosdf ] .",
    "this helps us extend theorem [ thm : consistencytwosdf ] on the consistency and asymptotic normality of @xmath70 for the general case as following .",
    "[ thm : consisasympndmultsdf ] given @xmath70 in eqn .",
    "( [ eqn : multsdfdirectext ] ) or ( [ eqn : multsdfsimplerform ] ) with its moments in lemma [ lem : momstwosdf ] .",
    "let @xmath107 . under assumptions [ assn:4thmomntms]-[assn :",
    "extrah1distnms ] , as @xmath21 , @xmath8 , @xmath255}{\\sqrt{\\text{var}[a_0({\\bf x } ) ] } } & \\xrightarrow{\\mathcal{d } } & n(0 , 1),\\end{aligned}\\ ] ] further , the limits hold when the moments are replaced with their empirical estimators given in eqns .",
    "( [ eqn : lmtgmeandfestd])-([eqn : lmtgvardfestd ] ) .    as the multi - sample case is a straightforward extension of its two - sample counterpart in sec .",
    "[ sec : twosampdf ] , we skip many detailed proofs to avoid unnecessary repetitions .",
    "we use simulation results to evaluate the performance of @xmath70 under practical scenarios , mainly focusing on consistency , asymptotic normality and control of misclassification under high - dimensional framework .",
    "we consider @xmath16 case and generate data from multivariate normal and @xmath256 distributions , i.e. @xmath5 is either @xmath257 , @xmath15 , or @xmath258 , @xmath259 , @xmath15 . for each distribution , we set @xmath260 with @xmath261 elements of @xmath186 also 0 and the rest as 1 where @xmath262 denotes the smallest integer . for @xmath22 , we consider two cases : ( 1 ) both populations have ar(1 ) structure , @xmath263 , @xmath264 , with @xmath265 for @xmath113 and 2 , where @xmath266 for @xmath113 and @xmath267 for @xmath114 , to represent both low and high correlation structures ; ( 2 ) the same ar(1 ) structure for @xmath113 with @xmath265 , @xmath268 , whereas an unstructured ( un ) @xmath22 for @xmath114 , defined as @xmath269 with @xmath270 and @xmath271 , @xmath110 .",
    "+ for finite - sample performance of the classifier under arbitrarily growing dimension , emphasizing the @xmath10 , we generate samples of sizes @xmath272 , @xmath273 , and combine with @xmath274 .",
    "finally , all results are averages of 1000 simulation runs for each combination of parameters mentioned above . to additionally observe the effect of large @xmath3",
    ", the misclassification rates are also presented for @xmath275 .",
    "similarly , to assess the classifier for very different sample sizes , we also used @xmath276 and @xmath277 and observed very similar results , hence not reported here .",
    "2 shows the results of asymptotic normality of @xmath70 , where the first two rows are for normal distribution , respectively for ar - ar and ar - un structures .",
    "each row gives a histogram of @xmath70 , with empirical density added to it , for @xmath278 and 1000 ( left to right ) .",
    "the last two rows are for multivariate @xmath256 distribution with @xmath259 .",
    "as stated above , the results are carried out for several other dimensions as well , up to @xmath279 , and also for other sample sizes , but due to similarity of the graphs , only a selection is reported here . + we observe close normal approximation for @xmath3 as small as 5 or 7 , and the results for @xmath256 distribution depict small sample robustness of the classifier to non - normality . to make the results of the two distributions comparable ,",
    "the density axes are scaled to the same height , so that the heavy - tailed behavior of @xmath256 distribution can be witnessed from a slightly extended range on the x - axis . in general , it is observed that the increasing dimension does not damage the asymptotic normality of the classifier even if the data are non - normal . + a similar performance is observed for the control of misclassification rate , shown in figs . 3 for @xmath272 , @xmath273 , and 4 for @xmath275 .",
    "the thick line represents the actual error rate under asymptotic normality of the classifier , i.e. @xmath280 , where @xmath281 is the ( univariate ) normal distribution function and @xmath282 , @xmath283 are the moments of the classifier .",
    "this actual error rate is used as a reference to assess the estimated error rate shown in dashed ( dotted ) line for normal ( @xmath284 ) distribution .",
    "further , the upper ( lower ) panel in each figure is for ar - ar ( ar - un ) pair of covariances .",
    "+ the estimated error closely follows the actual error for @xmath285 , and the error rate also converges to zero , showing consistency of the classifier . for @xmath256    [",
    "fig : figasympnd ]    +      +      [ fig : figapersmalln ]    [ fig : figaperlargen ] +    distribution with @xmath285 , the estimated error rates are relatively higher than under normality , but with @xmath3 increased only by 5 , a discernable difference in the performance of the classifier is observed in fig .",
    "4 . note that , the x - axis in figs .",
    "3 - 4 is truncated at @xmath286 since the misclassification rates already converge to 0 by this value and remain so for larger @xmath11 .",
    "we apply @xmath70 on two large data sets for @xmath16 and @xmath287 . with moderate sample sizes ( 77 and 102 ) ,",
    "we use @xmath288-fold cv for evaluation ( see * ? ? ?",
    "+ let @xmath289 and @xmath290 denote the learning and test sets .",
    "we randomly divide data set into @xmath291 classes of roughly equal size where @xmath290 consists of @xmath292 classes with @xmath291th class held out as test data . the procedure is repeated @xmath291 times , each time with a different test class , and a misclassification rate is computed for each repetition . the evaluation criterion is the average misclassification rate over all repetitions . + for @xmath293th fold of cv , let @xmath294 , @xmath295 and @xmath296 be , respectively , the sample sizes for learning and test data in sample @xmath4 and the number of misclassified observations from class @xmath4 into class @xmath297 , @xmath298 or 3 , @xmath299 .",
    "let @xmath300 be the estimated misclassification rate , an estimator of @xmath20 in ( [ eqn : misclrate ] ) , for @xmath293th rotation , i.e. @xmath301 , where @xmath302 .",
    "for @xmath303 , we do the same procedure for each of three pairs and compute overall misclassification rate . for details on the used and other data sets , see @xcite and also @xcite and @xcite .",
    "[ [ example-1-dlbcl - data ] ] example 1 : dlbcl data + + + + + + + + + + + + + + + + + + + + +    the diffuse large b - cell lymphoma ( dlbcl ) data belongs to a study of lymphoid malignancy in adults .",
    "the analysis reported here consists of @xmath304 gene expressions studied on pre - treatment biopsies from two independent groups of 77 patients , one with dlbcl ( @xmath305 ) , the other with follicular lymphoma ( fl ) ( @xmath306 ) .",
    "+ for a 3-fold cv , we randomly divide the data into three groups of sizes 26 , 26 , 25 with @xmath307 , @xmath308 and @xmath309 , @xmath310 for @xmath311 . by coding the populations as 1 ( dlbcl ) and 2 ( fl ) ,",
    "the misclassifications observed from the three rotations of cv , i.e. @xmath312 and @xmath313 , for @xmath314 , are @xmath315 so that the overall misclassification rate is computed as @xmath316 .",
    "although of relatively less importance , due to randomly sampled folds for cross - validation , we also report the sample sizes for each fold as following : @xmath317    [ [ example-2-leukemia - data ] ] example 2 : leukemia data + + + + + + + + + + + + + + + + + + + + + + + +    the data set pertains to a study of patients with acute lymphoblastic leukemia ( all ) carrying a chromosomal translocation involving mixed - lineage leukemia ( mll ) gene .",
    "the analysis reported here consists of @xmath318 gene expression profiles of leukemia cells from @xmath319 patients diagnosed with b - precursor all carrying an mll translocation and compared to a group of @xmath320 individual diagnosed with conventional b - precursor without mll translocation .",
    "in addition , there is a third group of a random sample of @xmath321 with acute myelogenous leukemia ( aml ) .",
    "+ for a 3-fold cross - validation , we randomly divide the data into three equal groups of size 24 each and use @xmath322 classes of total @xmath323 observations in learning set and @xmath324 in the test set every time , @xmath314 .",
    "the rest of the procedure is the same as explained in the example 1 above .",
    "we obtain the following misclassifications for the three folds of cross - validation : @xmath325 this gives an overall misclassification rate @xmath326 .",
    "the sample sizes used in each rotation are also reported below .",
    "a @xmath0-classifier for high - dimensional and possibly non - normal data is proposed .",
    "the threshold part of the classifier , called @xmath0-component , is a linear combination of two bivariate @xmath0-statistics of computed from the two independent samples .",
    "the discriminant function part , called @xmath1-component , forms an inner product between the observation to be classified and the difference of the mean vectors of the corresponding independent samples .",
    "it results into a computationally simple classifier which is linear without requiring the underlying covariance matrices to be equal .",
    "a multi - class extension with same properties is also given .",
    "+ the classifier is unbiased , consistent and asymptotically normal , under a general multivariate model , including ( but not necessarily ) the multivariate normal distribution",
    ". rapid convergence of the misclassification rate of the classifier is shown for very small sample sizes and non - normal distributions , under mild and practically justifiable assumptions .",
    "the performance of the classifier , in terms of its consistency , asymptotic normality and control of misclassification rate , is shown through simulations for normal and non - normal distributions with sample sizes as small as 5 or 7 and for arbitrary large dimension .",
    "+ we apply the classifier to genetics and microarray data sets , some of the most popular areas for classification analysis . to emphasize the role of high - dimensionality",
    ", we demonstrate that the use , accuracy , and validity of the classifier does not rest on any form of data pre - processing as is usually shown in the literature . in other words ,",
    "a data set measured in large dimension can be directly used for the classifier without any pre - requisites of reducing the dimension through sorting or clustering or other means .",
    "for @xmath328 in ( [ eqn : modellinn ] ) , let @xmath329 and @xmath330 , @xmath331 , be a quadratic and a bilinear form of independent components with @xmath332 for @xmath333 .",
    "as all terms involving @xmath334 eventually vanish under assumption [ assn : tracesigmahadprod ] , we write @xmath335 for simplicity . theorem [ thm : basicqfbfmomsnn ] gives basic moments of quadratic and bilinear forms which we extend in lemma [ lem : qfbfresults ] .",
    "proofs of these results are tedious but simple , therefore skipped ( see * ? ? ?",
    "[ thm : basicqfbfmomsnn ] for @xmath336 and @xmath337 , as defined above , we have      with @xmath339 , @xmath340 , @xmath341 , @xmath342 and @xmath343 .",
    "moreover , @xmath344 , @xmath345 and @xmath346 .",
    "[ lem : qfbfresults ] let @xmath347 be as given above with @xmath347 , @xmath348 independent if @xmath349",
    ". then @xmath350 & = & \\text{tr}({{\\boldsymbol{\\sigma}}}^4_i)\\label{eqn : a8}\\\\ \\text{e}[{\\bf z}'_{it}{\\bf z}_{iu}{\\bf z}'_{iw}{\\bf z}_{iu}{\\bf z}'_{it}{\\bf z}_{iv}{\\bf z}'_{iw}{\\bf z}_{iv } ] & = & \\text{tr}({{\\boldsymbol{\\sigma}}}^4_i)\\label{eqn : a9}\\\\ \\text{e}[({\\bf z}'_{it}{\\bf z}_{iu})^2{\\bf z}'_{it}{{\\boldsymbol{\\sigma}}}_i{\\bf z}_{it } ] & = & 2\\text{tr}({{\\boldsymbol{\\sigma}}}^4_i ) + [ \\text{tr}({{\\boldsymbol{\\sigma}}}^2_i)]^2 + m_2\\label{eqn : a10}\\\\ \\text{var}({\\bf z}'_{it}{\\bf z}_{iu}{\\bf z}'_{iv}{\\bf z}_{iu } ) & = & 2\\text{tr}({{\\boldsymbol{\\sigma}}}^4_i ) + [ \\text{tr}({{\\boldsymbol{\\sigma}}}^2_i)]^2 + m_2\\label{eqn : a11}\\\\ \\text{cov}[({\\bf z}'_{it}{\\bf z}_{iu})^2 , ( { \\bf z}'_{it}{\\bf z}_{iv})^2 ] & = & 2\\text{tr}({{\\boldsymbol{\\sigma}}}^4_i ) + m_2\\label{eqn : a12}\\\\ \\text{e}[({\\bf z}'_{it}{\\bf z}_{ju})^2{\\bf z}'_{it}{{\\boldsymbol{\\sigma}}}_j{\\bf z}_{it } ] & = & 2\\text{tr}\\{({{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\sigma}}}_j)^2\\ } + \\big[\\text{tr}({{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\sigma}}}_j)\\big]^2 + m_2~~\\label{eqn : a13}\\\\ \\text{var}({\\bf z}'_{it}{\\bf z}_{ju}{\\bf z}'_{iv}{\\bf z}_{ju } ) & = & 2\\text{tr}\\{({{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\sigma}}}_j)^2\\ } + \\big[\\text{tr}({{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\sigma}}}_j)\\big]^2 + m_2~~\\label{eqn : a14}\\\\ \\text{cov}[({\\bf z}'_{jt}{\\bf z}_{iu})^2 , ( { \\bf z}'_{jt}{\\bf z}_{iv})^2 ] & = & 2\\text{tr}\\{({{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\sigma}}}_j)^2\\ } + m_2\\label{eqn : a15}\\\\ \\text{e}({\\bf z}'_{jt}{\\bf z}_{iu}{\\bf z}'_{jt}{\\bf z}_{iv}{\\bf z}'_{iu}{{\\boldsymbol{\\sigma}}}_j{\\bf z}_{iv } ) & = & \\text{tr}\\{({{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\sigma}}}_j)^2\\}\\label{eqn : a16}\\\\ \\text{cov}[({\\bf z}'_{it}{\\bf z}_{iu})^2 , { \\bf z}'_{it}{{\\boldsymbol{\\sigma}}}_j{\\bf z}_{it } ] & = & 2\\text{tr}({{\\boldsymbol{\\sigma}}}^3_i{{\\boldsymbol{\\sigma}}}_j ) + m_2\\label{eqn : a17}\\\\ \\text{cov}({\\bf z}'_{it}{{\\boldsymbol{\\sigma}}}_i{\\bf z}_{iu } , { \\bf z}'_{it}{{\\boldsymbol{\\sigma}}}_j{\\bf z}_{iu } ) & = & \\text{tr}\\{({{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\sigma}}}_j)^2\\}\\label{eqn : a18}\\\\ \\text{e}[({\\bf z}'_{iu}{\\bf z}_{iv})^2{\\bf z}'_{it}{{\\boldsymbol{\\sigma}}}_j{\\bf z}_{it } ] & = & \\text{tr}({{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\sigma}}}_j)\\text{tr}({{\\boldsymbol{\\sigma}}}_i)^2,\\label{eqn : a19}\\end{aligned}\\ ] ] where @xmath351 $ ] , @xmath352 $ ] , @xmath353 $ ] , + @xmath354 $ ] , @xmath355 $ ] , @xmath356 $ ] all vanish .",
    "let @xmath42 . with @xmath17 independent of both samples , @xmath357 $ ] is trivial .",
    "for variance , ignoring @xmath11 for simplicity , we begin with @xmath358 = \\text{e}[{\\bf x}'_0(\\overline{\\bf x}_1 - \\overline{\\bf x}_2)]^2 - [ { { \\boldsymbol{\\mu}}}'_1({{\\boldsymbol{\\mu}}}_1 - { { \\boldsymbol{\\mu}}}_2)]^2.\\ ] ] since @xmath359 ^ 2 = \\text{e}[{\\bf x}'_0(\\overline{\\bf x}_1 - \\overline{\\bf x}_2)(\\overline{\\bf x}_1 - \\overline{\\bf x}_2)'{\\bf x}_0]$ ] , we immediately get @xmath360 ^ 2 & = & \\text{tr}\\left[({{\\boldsymbol{\\sigma}}}_1 + { { \\boldsymbol{\\mu}}}_1{{\\boldsymbol{\\mu}}}'_1)\\left(\\frac{{{\\boldsymbol{\\sigma}}}_1}{n_1 } + \\frac{{{\\boldsymbol{\\sigma}}}_2}{n_2 } + ( { { \\boldsymbol{\\mu}}}_1 - { { \\boldsymbol{\\mu}}}_2)({{\\boldsymbol{\\mu}}}_1 - { { \\boldsymbol{\\mu}}}_2)'\\right)\\right],\\end{aligned}\\ ] ] so that @xmath358 = \\frac{\\text{tr}({{\\boldsymbol{\\sigma}}}^2_1)}{n_1 } + \\frac{\\text{tr}({{\\boldsymbol{\\sigma}}}_1{{\\boldsymbol{\\sigma}}}_2)}{n_2 } + \\frac{{{\\boldsymbol{\\mu}}}'_1{{\\boldsymbol{\\sigma}}}_1{{\\boldsymbol{\\mu}}}_1}{n_1 } + \\frac{{{\\boldsymbol{\\mu}}}'_1{{\\boldsymbol{\\sigma}}}_2{{\\boldsymbol{\\mu}}}_1}{n_2 } + ( { { \\boldsymbol{\\mu}}}_1 - { { \\boldsymbol{\\mu}}}_2)'{{\\boldsymbol{\\sigma}}}_1({{\\boldsymbol{\\mu}}}_1 - { { \\boldsymbol{\\mu}}}_2).\\ ] ] now @xmath361 . for @xmath143 with @xmath362 ( * ? ? ?",
    "5 ) ) , @xmath363 with @xmath364 = { { \\boldsymbol{\\mu}}}'_i{{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\mu}}}_i$ ] , and @xmath365 with @xmath366 , so that @xmath367 = \\frac{2\\text{tr}({{\\boldsymbol{\\sigma}}}^2_i)}{n_i(n_i - 1 ) } + \\frac{4{{\\boldsymbol{\\mu}}}'_i{{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\mu}}}_i}{n_i } , ~i = 1 , 2.\\ ] ] for @xmath368 $ ] , @xmath369 , by independence , where it immediately follows that @xmath370 , @xmath15 . combining all results and simplifying gives eqn .",
    "( [ eqn : vardftwos ] ) .",
    "the unbiasedness is trivial . for @xmath371 , @xmath372 , @xmath15 , are given in sec .",
    "[ subsec : prooflemmamomstwosdf ] . for @xmath373 , @xmath374 with @xmath375 , @xmath376 so that @xmath377 = { { \\boldsymbol{\\mu}}}'_j{{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\mu}}}_j$ ] and @xmath377 = { { \\boldsymbol{\\mu}}}'_i{{\\boldsymbol{\\sigma}}}_j{{\\boldsymbol{\\mu}}}_i$ ] . also @xmath378 with @xmath379 = { { \\boldsymbol{\\mu}}}'_i{{\\boldsymbol{\\sigma}}}_j{{\\boldsymbol{\\mu}}}_i + { { \\boldsymbol{\\mu}}}'_j{{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\mu}}}_j + \\text{tr}({{\\boldsymbol{\\sigma}}}_i{{\\boldsymbol{\\sigma}}}_j)$ ] .",
    "hence @xcite @xmath380\\ ] ] where @xmath381 , @xmath382 and @xmath383 by independence .",
    "@xmath384 can now be approximated as @xmath385 with second term vanishing under assumption [ assn : extrah1distn ] and first term bounded in @xmath11 under assumption [ assn : tracesigma ] , @xmath371 reduces to @xmath386 as @xmath26 , so that the consistency follows immediately as @xmath21 .",
    "the bound in ( [ eqn : boundvare0 ] ) also follows trivially . as @xmath146 and @xmath147 , are also one- and two - sample @xmath0-statistics with higher order kernels , we essentially follow the same strategy as for @xmath144 .",
    "first , from theorem [ thm : basicqfbfmomsnn ] and lemma [ lem : qfbfresults ] , it can be shown that ( see * ? ? ?",
    "2 ) @xmath387 ^ 2 + m_2o(n^3_i ) + m_3o(n^2_i)\\big]\\\\ \\text{var}(e_{ij } ) & = & \\frac{2}{(n_i - 1)(n_j - 1)p^4}\\big[(n - 1)\\text{tr}\\{({{\\boldsymbol{\\sigma}}}_1{{\\boldsymbol{\\sigma}}}_2)^2\\ } + [ \\text{tr}({{\\boldsymbol{\\sigma}}}_1{{\\boldsymbol{\\sigma}}}_2)]^2 \\nonumber\\\\ & & \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+~m_2o(n ) + m_3o(1)\\big]\\\\ \\text{cov}(e_i , e_{ij } ) & = & \\frac{4}{n_i(n_i - 1)p^4}\\big[n_i\\text{tr}({{\\boldsymbol{\\sigma}}}^3_i{{\\boldsymbol{\\sigma}}}_j ) + m_2o(n_i)\\big]\\end{aligned}\\ ] ] @xmath388 , @xmath103 , @xmath110 , @xmath389 , @xmath390 are as in theorem [ thm : basicqfbfmomsnn ] and @xmath391 . as terms involving",
    "@xmath227 s vanish under assumption [ assn : tracesigmahadprod ] , the consistency and the bounds ( by cauchy - schwarz inequality ) follow the same way as for @xmath144 .",
    "note also that , the terms involving @xmath227 s are exactly zero under normality in which case the same results follow even more conveniently .",
    "the proof essentially follows from that of theorem [ thm : propsestrsbounds ] without much new computations .",
    "in particular , the first part , assuming true parameters known , is trivial . for the second part with empirical estimators , the @xmath392-consistency of estimators proved in sec .",
    "[ subsec : proofthmpropsestrs ] implies that @xmath153 , and the same holds for @xmath146 , @xmath147 .",
    "plugging these estimators in the moments of @xmath70 and using slutsky s lemma , @xmath157 so that @xmath393 } = \\text{var}[a_0({\\bf x } ] + o_p(1)$ ] , and the consistency follows similarly as with known parameters .",
    "write @xmath394- \\text{e}[a_0({\\bf x})|{\\bf x}_0 \\in \\pi_1]$ ] , where @xmath395 - [ ( u_{n_1 } - { { \\boldsymbol{\\mu}}}'_1{{\\boldsymbol{\\mu}}}_1 ) - ( u_{n_2 } - { { \\boldsymbol{\\mu}}}'_2{{\\boldsymbol{\\mu}}}_2)]/2,\\ ] ] ignoring @xmath11 for simplicity .",
    "let @xmath396 be the projection of @xmath397 , @xmath15 .",
    "then ( * ? ? ?",
    "5 ) @xmath398 for @xmath399 , and similarly @xmath400 for @xmath401 , with @xmath402 = 0 $ ] in both cases , so that @xmath403 where @xmath404 , @xmath15 . with @xmath42 and independence of samples , this projection of @xmath405 results into a sum of two independent components , each an average of iid variables @xcite .",
    "taking @xmath11 into account , the asymptotic normality follows by the clt under assumptions [ assn:4thmomnt]-[assn : extrah1distn ] as @xmath21 .",
    "we are sincerely thankful to prof .",
    "thomas mikosch , department of mathematics , university of copenhagen , for careful perusal of the paper , discussion and encouragement .",
    "the research of tatjana pavlenko is partially supported by the swedish research council , grant no .",
    "c0595201 .",
    "ahmad , mr ( 2014a ) . a @xmath0-statistic approach for a high - dimensional two - sample mean testing problem under non - normality and behrens - fisher setting . _ annals of the institute of statistical mathematics _",
    ", * 66*(1 ) , 33 - 61 .",
    "bickel , p and e levina ( 2004 ) . some theory for fisher s linear discriminant function , naive bayes , and some alternatives when there are many more variables than observations .",
    "_ bernoulli _ , * 10*(6 ) , 989 - 1010 .",
    "statnikov , a , i tsamardinos , y dosbayev , and cf aliferis ( 2005 ) .",
    "gems : a system for automated cancer diagnosis and biomarker discovery from microarray gene expression data . _ international journal of medical informatics _ , * 74 * , 491 - 503 ."
  ],
  "abstract_text": [
    "<S> a classifier for two or more samples is proposed when the data are high - dimensional and the underlying distributions may be non - normal . </S>",
    "<S> the classifier is constructed as a linear combination of two easily computable and interpretable components , the @xmath0-component and the @xmath1-component . </S>",
    "<S> the @xmath0-component is a linear combination of @xmath0-statistics which are averages of bilinear forms of pairwise distinct vectors from two independent samples . </S>",
    "<S> the @xmath1-component is the discriminant score and is a function of the projection of the @xmath0-component on the observation to be classified . </S>",
    "<S> combined , the two components constitute an inherently bias - adjusted classifier valid for high - dimensional data . </S>",
    "<S> the simplicity of the classifier helps conveniently study its properties , including its asymptotic normal limit , and extend it to multi - sample case . </S>",
    "<S> the classifier is linear but its linearity does not rest on the assumption of homoscedasticity . </S>",
    "<S> probabilities of misclassification and asymptotic properties of their empirical versions are discussed in detail . </S>",
    "<S> simulation results are used to show the accuracy of the proposed classifier for sample sizes as small as 5 or 7 and any large dimensions . </S>",
    "<S> applications on real data sets are also demonstrated .    </S>",
    "<S> * keyword : * bias - adjusted classifier ; @xmath0-statistics ; discriminant analysis ; </S>"
  ]
}