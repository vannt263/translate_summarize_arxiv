{
  "article_text": [
    "boosting @xcite is a popular method that leverages simple learning models ( e.g. , decision stumps ) to generate powerful learners .",
    "boosting has been used to great effect and trump other learning algorithms in a variety of applications . in computer vision , boosting was made popular by the seminal viola - jones cascade  @xcite and is still used to generate state - of - the - art results in pedestrian detection  @xcite .",
    "boosting has also found success in domains ranging from document relevance ranking  @xcite and transportation  @xcite to medical inference  @xcite . finally , boosting yields an anytime property at test time , which allows it to work with varying computation budgets  @xcite for use in real - time applications such as controls and robotics .",
    "the advent of large - scale data - sets has driven the need for adapting boosting from the traditional batch setting , where the optimization is done over the whole dataset , to the online setting where the weak learners ( models ) can be updated with streaming data .",
    "in fact , online boosting has received tremendous attention so far . for classification , @xcite proposed online boosting algorithms along with theoretical justifications .",
    "recent work by  @xcite , addressed the regression task through the introduction of _ online gradient boosting _",
    "we build upon on the developments in  @xcite to devise a new set of algorithms presented below .    in this work",
    ", we develop streaming boosting algorithms for regression with strong theoretical guarantees under stochastic setting , where at each round the data are i.i.d sampled from some unknown fixed distribution . in particular",
    ", our algorithms are streaming extension to the classic gradient boosting  @xcite , where weak predictors are trained in a stage - wise fashion to approximate the functional gradient of the loss with respect to the previous ensemble prediction , a procedure that is shown by  @xcite to be functional gradient descent of the loss in the space of predictors .",
    "since the weak learners can not match the gradients of the loss exactly , we measure the error of approximation by redefining of _ edge _ of online weak learners  @xcite for online regression setting .    assuming a non - trivial edge",
    "can be achieved by each deployed weak online learner , we develop algorithms to handle smooth or non - smooth loss functions , and theoretically analyze the convergence rates of our streaming boosting algorithms .",
    "our first algorithm targets strongly convex and smooth loss functions and achieves exponential decay on the average regret with respect to the number of weak learners .",
    "we show the ratio of the decay depends on the edge and also the condition number of the loss function . the second algorithm , designed for strongly convex but non - smooth loss functions , extends from the batch residual gradient boosting algorithm from @xcite .",
    "we show that the algorithm achieves @xmath1 convergence rate with respect to the number of weak learners @xmath2 , which matches the online gradient descent ( ogd ) s no - regret rate for strongly convex loss @xcite .",
    "both of our algorithms promise that as @xmath3 ( the number of samples ) and @xmath2 go to infinity , the average regret converges to zero .",
    "our analysis leverages online - to - batch reduction @xcite , hence our results naturally extends to adversarial online learning setting as long as the weak online learning edge holds in adversarial setting , a harsher setting than stochastic setting .",
    "we conclude with some proof - of - concept experiments to support our analysis .",
    "we demonstrate that our algorithm significantly boosts the performance of weak learners and converges to the performance of classic gradient boosting with less computation .",
    "online boosting algorithms have been evolving since their batch counterparts are introduced .",
    "@xcite developed some of the first online boosting algorithm , and their work are applied to online feature selection  @xcite and online semi - supervised learning  @xcite . @xcite",
    "introduced online gradient boosting for the classification setting albeit without a theoretical analysis .",
    "@xcite developed the first convergence guarantees of online boosting for classification .",
    "then  @xcite presented two online classification boosting algorithms that are proved to be respectively optimal and adaptive .",
    "our work is most related to @xcite , which extends gradient boosting for regression to the online setting under a smooth loss : each weak online learner is trained by minimizing a linear loss , and weak learners are combined using frank - wolfe  @xcite fashioned updates .",
    "their analysis generalizes those of batch boosting for regression  @xcite .",
    "in particular , these proofs forgo edge assumptions of the weak learners . though frank - wolfe is a nice projection - free algorithm",
    ", it has relatively slow convergence and usually is restricted to smooth loss functions . in our work , each weak learner instead minimizes the squared loss between its prediction and the gradient , which allows us to treat weak learners as approximations of the gradients thanks to the weak learner edge assumption .",
    "hence we can mimic classic gradient boosting and use a gradient descent approach to combine the weak learners predictions .",
    "these differences enable our algorithms to handle non - smooth convex losses , such as hinge and @xmath4-losses , and result in convergence bounds that is more analogous to the bounds of classic batch boosting algorithms .",
    "this work also differs from @xcite in that we assume an online weak learner edge exists , a common assumption in the classic boosting literature  @xcite that is extended to the online boosting for classification by  @xcite . with this assumption",
    ", we analyze online gradient boosting using techniques from gradient descent for convex losses  @xcite .",
    "in the classic online learning setting , at every time step @xmath5 , the learner @xmath6 first makes a prediction ( i.e. , picks a predictor @xmath7 , where @xmath8 is a pre - defined class of predictors ) on the input @xmath9 , then receives a loss @xmath10 .",
    "the learner then updates @xmath11 to @xmath12 .",
    "the samples @xmath13 could be generated by an adversary , but this work mainly focuses on the setting where @xmath14 are i.i.d sampled from a distribution @xmath15 .",
    "the regret @xmath16 of the learner is defined as the difference between the total loss from the learner and the total loss from the best hypothesis in hindsight under the sequence of samples @xmath17 : @xmath18 we say the online learner is _ no - regret _ if and only if @xmath16 is @xmath19 .",
    "that is , time averaged , the online learner predictor @xmath11 is doing as well as the best hypothesis @xmath20 in hindsight .",
    "we define _ risk _ of a hypothesis @xmath21 as @xmath22 $ ] .",
    "our analysis of the risk leverages the classic online - to - batch reduction @xcite .",
    "the online - to - batch reduction first analyzes regret without the stochastic assumption on the sequence of loss @xmath23 , and it then relates regret to risk using concentration of measure .    throughout the paper we will use the concepts of strong convexity and smoothness .",
    "a function @xmath24 is said to be @xmath25-strongly convex and @xmath26-smooth with respect to norm @xmath27 if and only if for any pair @xmath28 and @xmath29 : @xmath30 where @xmath31 denotes the gradient of function @xmath23 with respect to @xmath32 .",
    "      our online boosting setup is similar to  @xcite and  @xcite . at each time",
    "step @xmath33 , the environment picks loss @xmath34 .",
    "the online boosting learner makes a prediction @xmath35 without knowing @xmath36 .",
    "then the learner suffers loss @xmath37 . throughout the paper",
    "we assume the loss is bounded as @xmath38 .",
    "we also assume that the gradient of the loss @xmath39 is also bounded as @xmath40 . for any finite dimension vector @xmath32 stands for the classic l2 norm . ]",
    "the online boosting learner maintains a sequence of weak online learning algorithms @xmath41 .",
    "each weak learner @xmath42 can only use hypothesis from a restricted hypothesis class @xmath43 to produce its prediction @xmath44 ( @xmath45 ) , where @xmath46 . to make a prediction @xmath47 at each iteration ,",
    "each @xmath42 will first make a prediction @xmath48 where @xmath44 .",
    "the online boosting learner combines all the weak learners predictions to produce the final prediction @xmath49 for sample @xmath50 .",
    "the online learner then suffers loss @xmath51 after the loss @xmath36 is revealed . as we will show later , with the loss @xmath36",
    ", the online learner will pass a square loss to each weak learner .",
    "each weak learner will then use its internal no - regret online update procedure to update its own weak hypothesis from @xmath52 to @xmath53 . in stochastic",
    "setting where @xmath36 and @xmath50 are i.i.d samples from a fixed distribution , the online boosting learner will output a combination of the hypothesises that were generated by weak learners as the final boosted hypothesis for future testing .    by leveraging linear combination of weak learners ,",
    "the goal of the online boosting learner is to boost the performance of a single online learner @xmath42 .",
    "additionally , we ideally want the prediction error to decrease exponentially fast in the number @xmath2 of weak learners , as is the result from classic batch gradient boosting @xcite .",
    "we specifically consider the setting where each weak learner minimizes a square loss , where @xmath54 is the regression target , and @xmath55 is in the weak - learner hypothesis class @xmath43 . at each step",
    "@xmath5 , a weak online learner @xmath6 chooses a predictor @xmath56 to predict @xmath57 , receives the target @xmath49 , @xmath58 simply stands for a regression target for the weak learner at step @xmath5 , not the final prediction of the boosted learner defined in sec .  [",
    "sec : boosting_setup ] . ] and then suffers loss @xmath59 . with this",
    ", we now introduce the definition of weak online learning edge .",
    "[ def : weak_learning](*weak online learning edge * ) given a restricted hypothesis class @xmath43 and a sequence of square losses @xmath60 , the weak online learner predicts a sequence @xmath61 that has edge @xmath62 $ ] , such that with high probability @xmath63 : @xmath64 where @xmath65 is usually known as the excess loss .",
    "the high probability @xmath63 comes from the possible randomness of the weak online learner and the sequence of examples .",
    "usually the dependence of the high probability bound on @xmath66 is poly - logarithmic in @xmath67 that is included in the term @xmath68",
    ". we will give a concrete example on this edge definition in next section where we will show what @xmath68 consists of .",
    "intuitively , a larger edge implies that the hypothesis is able to better explain the variance of the learning targets @xmath54 .",
    "our online weak learning definition is closely related to the one from  @xcite in that our definition is an result of the following two assumptions : ( 1 ) the online learning problem is agnostic - learnable ( i.e. , the weak learner has @xmath69 time - averaged regret against the best hypothesis @xmath70 ) with high probability : @xmath71 and ( 2 ) the restricted hypothesis class @xmath43 is rich enough such that for any sequence of @xmath72 with high probability : @xmath73 our definition of online weak learning directly generalizes the batch weak learning definition in  @xcite to the online setting by the additional agnostic learnability assumption as shown in eqn .",
    "[ eq : learnable ] .",
    "note that we pick square losses ( eqn .",
    "[ eq : leastsqure ] ) in our weak online learning definition .",
    "as we will show later , the goal is to enforce that the weak learners to accurately predict gradients , as was also originally used in the batch gradient boosting algorithm  @xcite .",
    "least - squares losses are also shown to be important in streaming tasks by  @xcite for their superior computational and theoretical properties .",
    "the above online weak learning edge definition immediately implies the following result , which is used in later proofs :    [ lemma : from_weak_learning ] given the sequence of losses @xmath74 , @xmath75 , the online weak learner generates a sequence of predictors @xmath76 , such that : @xmath77.\\end{aligned}\\ ] ]    the above lemma can be proved by expanding the square on the lhs of eqn .",
    "[ eq : weak_learner_eq ] , cancelling common terms and rearranging terms .",
    "we demonstrate here that the weak online learning edge assumption is reasonable .",
    "let us consider the case that the hypothesis class @xmath43 is closed under scaling ( meaning if @xmath70 , then for all @xmath78 , @xmath79 ) and let us assume @xmath80 , and @xmath81 for some unknown function @xmath20 .",
    "we define the inner product @xmath82 of any two functions @xmath83 as @xmath84 $ ] and the squared norm @xmath85 of any function @xmath55 as @xmath86 .",
    "we assume @xmath20 is bounded in a sense @xmath87 .",
    "the following proposition shows that as long as @xmath20 is not perpendicular to the span of @xmath43 ( @xmath88 ) , i.e. , @xmath89 such that @xmath90 , then we can achieve a non - zero edge :    [ prop : edge_example ] consider any sequence of pairs @xmath91 , where @xmath50 is i.i.d sampled from @xmath15 , @xmath92 and @xmath88 .",
    "run any no - regret online algorithm @xmath6 on sequence of losses @xmath60 and output a sequence of predictions @xmath76 .",
    "with probability at least @xmath63 , there exists a weak online learning edge @xmath93 $ ] , such that : @xmath94 where @xmath16 is the regret of online algorithm @xmath6 .",
    "the proof of the above proposition can be found in appendix .",
    "matching to eq .",
    "[ eq : weak_learner_eq ] , we have @xmath95 . in addition , the contrapositive of the proposition implies that without a positive edge , @xmath96 is orthogonal to @xmath20 so that no linear boosted ensemble can approximate @xmath20 . hence having a positive online weak learner edge",
    "is necessary for online boosted algorithms .",
    "we first present streaming gradient boosting(sgb ) , an algorithm ( alg .",
    "[ alg : online_gradient_boost ] ) that is designed for loss functions @xmath97 that are @xmath25-strongly convex and @xmath26-smooth .    a restricted class @xmath43 . @xmath2 online weak learners @xmath98 . learning rate @xmath99 .",
    "each weak learner initlizes a hypothesis @xmath100 .",
    "receive @xmath50 and initialize @xmath101 ( e.g. , @xmath102 ) .",
    "[ line : gd ] set the partial sum @xmath103 .",
    "predict @xmath104 .",
    "@xmath36 is revealed and learner suffers loss @xmath51 .",
    "compute gradient w.r.t partial sum : @xmath105 .",
    "[ line : gradient ] feed loss @xmath106 to @xmath107 .",
    "[ line : feed ] weak learner @xmath107 computes @xmath108 using its no - regret update procedure .",
    "[ line : weak_learner_update ] set @xmath109 .",
    "[ line : average ] .",
    "[ line : stoch_return ]    alg .",
    "[ alg : online_gradient_boost ] is the online version of the classic batch gradient boosting algorithms  @xcite .",
    "[ alg : online_gradient_boost ] maintains @xmath2 weak learners . at each time",
    "step @xmath5 , given example @xmath50 , the algorithm predicts @xmath49 by linearly combining the weak learners predictions ( line  [ line : gd ] ) .",
    "then after receiving loss @xmath36 , for each weak learner , the algorithm computes the gradient of @xmath36 with respect to @xmath54 evaluated at the _ partial _ sum @xmath110 ( line  [ line : gradient ] ) and feeds the square loss @xmath111 with the computed gradient as the regression target to weak learner @xmath107 ( line  [ line : feed ] ) .",
    "the weak learner @xmath107 then performs its own no - regret online update to compute @xmath112 ( line  [ line : weak_learner_update ] ) .    line  [ line : average ] and  [ line : stoch_return ] are needed for stochastic setting .",
    "we compute the average @xmath113 for every weak learner @xmath42 in line  [ line : average ] . in testing time , given @xmath114 , we predict @xmath54 as : @xmath115    since we penalize the weak learners by the squared deviation of its own prediction and the gradient from the previous partial sum , we essentially force weak learners to produce predictions that are close to the gradients ( in a no - regret perspective ) . with this perspective , sgbcan be understood as using the weak learners predictions as @xmath2 gradient descent steps where the gradient of each step @xmath116 is approximated by a weak learner s prediction ( line  [ line : gd ] ) .",
    "let us define @xmath117 , for any @xmath118 .",
    "namely @xmath119 measures the performance of the initialization @xmath120 . under our assumption",
    "that the loss is bounded , @xmath121 , we can simply upper bound @xmath119 as @xmath122 . alg .",
    "[ alg : online_gradient_boost ] has the following performance guarantee :    [ them : smooth_strongly_convex ] assume weak learner @xmath123 has weak online learning edge @xmath93 $ ] .",
    "let @xmath124 .",
    "there exists a @xmath125 , for @xmath25-strongly convex and @xmath26-smooth loss functions , @xmath126 , such that when @xmath127 , alg .",
    "[ alg : online_gradient_boost ] generates a sequence of predictions @xmath128 where : @xmath129 \\leq 2b(1 - \\frac{\\gamma^2\\lambda}{16\\beta})^n .",
    "\\ ] ] for stochastic setting where @xmath130 independently , we have when @xmath127 : @xmath131\\leq 2b(1-\\frac{\\gamma^2\\lambda}{16\\beta})^n.\\end{aligned}\\ ] ]    the expectation in eqn .",
    "[ eq : smooth_result_stoch ] of the above theorem is taken over the randomness of the sequence of pairs of loss and samples @xmath132 ( note that @xmath113 is dependent on @xmath133 ) and @xmath134 .",
    "theorem  [ them : smooth_strongly_convex ] shows that with infinite amount samples the average regret decreases exponentially as we increase the number of weak learners .",
    "this performance guarantee is very similar to classic batch boosting algorithms @xcite , where the empirical risk decreases exponentially with the number of algorithm iterations , i.e. , the number of weak learners .",
    "theorem  [ them : smooth_strongly_convex ] mirrors that of theorem 1 in @xcite , which bounds the regret of the frank - wolfe - based online gradient boosting algorithm .",
    "our results utilize the additional assumptions that the losses @xmath36 are strongly convex and that the weak learners have edge , allowing us to shrink the average regret exponentially with respect to n , while the average regret in @xcite shrinks in the order of @xmath135 ( though this dependency on @xmath2 is optimal under their setting ) .",
    "proof of theorem  [ them : smooth_strongly_convex ] , detailed in appendix  [ sec : proof_smooth_strongly_convex ] , weaves our additional assumptions into the proof framework of gradient descent on smooth losses .",
    "in particular , using weak learner edge assumption , we derive lemma  [ lemma : from_weak_learning ] and the lemma  [ lemma : helper_1 ] to relate parts of the strong smoothness expansion of the losses to the norm - squared of the gradients @xmath136 , which is an upper bound of due to strong convexity . using this observation",
    ", we can relate the total regret of the ensemble of the first @xmath116 learners , , with the regret from using @xmath137 learners , @xmath138 , and show that @xmath138 shrinks @xmath139 by a constant fraction while only adding a small term @xmath140 . solving the recursion on the sequence of @xmath141 , we arrive at the final exponentially decaying regret bound in the number of learners .",
    "[ [ remark ] ] remark + + + + + +    due to the weak online learning edge assumption , the regret bound shown in eqn .",
    "[ eq : regret_bound_smooth ] and the risk bound shown in eqn .",
    "[ eq : smooth_result_stoch ] are stronger than typical bounds in classic online learning , in a sense that we are competing against @xmath20 that could potentially be much more powerful than any hypothesis from @xmath43 . for instance",
    "when the loss function is square loss @xmath142 , theorem  [ them : smooth_strongly_convex ] essentially shows that the risk of the boosted hypothesis @xmath143 $ ] approaches to zero as @xmath2 approaches to infinity , under the assumption that @xmath123 have no - zero weak learning edge ( e.g.,@xmath144 ) . note that this is analogous to the results of classification based batch boosting @xcite and online boosting @xcite : as number of weak learners increase , the average number of prediction mistakes approaches to zero . in other words , with the corresponding edge assumptions , these batch / online boosting classification algorithms can compete against any arbitrarily powerful classifier that always makes zero mistakes on any given training data .",
    "the regret bound shown in theorem  [ them : smooth_strongly_convex ] only applies for strongly convex and smooth loss functions .",
    "in fact , one can show that alg .",
    "[ alg : online_gradient_boost ] will fail for general non - smooth loss functions .",
    "we can construct a sequence of non - smooth loss functions and a special weak hypothesis class @xmath43 , which together show that the regret of alg .",
    "[ alg : online_gradient_boost ] grows linearly in the number of samples , regardless of the number of weak learners .",
    "we refer readers to appendix  [ sec : counter_example ] for more details .    a restricted class @xmath43 . @xmath2",
    "online weak learners @xmath98 . learning rate schedule @xmath145 .",
    "@xmath146 initializes a hypothesis @xmath147 .",
    "receive @xmath50 and initialize @xmath101 ( e.g. , @xmath148 ) .",
    "[ line : gd ] set the projected partial sum @xmath149 ) .",
    "predict @xmath150 the loss @xmath36 is revealed and compute loss @xmath51 .",
    "set initial residual @xmath151 .",
    "compute subgradient w.r.t .",
    "partial sum : @xmath105 .",
    "feed loss @xmath152 to @xmath107 .",
    "[ line : feed_residual_loss ] update residual : @xmath153 .",
    "[ line : residual_update ] weak learner @xmath107 computes @xmath108 using its no - regret update procedure .",
    "test sample @xmath32 and @xmath154 from the output of alg .",
    "[ alg : online_gradient_boost_non_smooth_residual ] .",
    "[ line : for_loop ] @xmath155 ) .",
    "@xmath156 . .",
    "[ line : stoch_return_non_smooth ]    our next algorithm , alg .",
    "[ alg : online_gradient_boost_non_smooth_residual ] , extends sgb(alg .",
    "[ alg : online_gradient_boost ] ) to handle strongly convex but non - smooth losses . instead of training each weak learner to fit the subgradients of non - smooth loss with respect to current prediction",
    ", we instead keep track of a residual @xmath139 ) , @xmath139 does not refer to the regret of the ensemble s regret with the @xmath116-th as used in the analysis of alg .",
    "[ alg : online_gradient_boost ] ] that accumulates the difference between the subgradients , @xmath157 , and the fitted prediction @xmath158 , from @xmath159 up to @xmath160 . instead of fitting the predictor @xmath161 to match the subgradient @xmath162 , we fit it to match the sum of the subgradient and the residuals , @xmath163 .",
    "more specifically , in line  [ line : feed_residual_loss ] of alg .",
    "[ alg : online_gradient_boost_non_smooth_residual ] , for each weak learner @xmath107 , we feed a square loss with the sum of residual and the gradient as the regression target",
    ". then line  [ line : residual_update ] sets the new the residual @xmath164 as the difference between the target @xmath165 and the weak learner @xmath107 s prediction @xmath166 .    the last line of alg .",
    "[ alg : online_gradient_boost_non_smooth_residual ] is needed for stochastic setting where @xmath167 i.i.d . in test , given sample @xmath114 , we predict @xmath54 using @xmath168 in procedure shown in alg .",
    "[ alg : online_gradient_boost_non_smooth_residual_test ] . for notation simplicity",
    ", we denote the testing procedure shown in alg .",
    "[ alg : online_gradient_boost_non_smooth_residual_test ] as @xmath169 , which @xmath170 explicitly depends on the returns @xmath154 from sgb(residual projection ) .",
    "since it s impractical to store and apply all @xmath171 models , we follow a common stochastic learning technique which uses the final predictor at time @xmath3 for testing ( e.g. , @xcite ) in the experiment section ( i.e. , simply set @xmath172 in line  [ line : for_loop ] in alg .",
    "[ alg : online_gradient_boost_non_smooth_residual_test ] ) . in practice ,",
    "if the learners converge and @xmath3 is large , the average and final predictions are close .",
    "intuitively , this approach prevents the weak learners from consistently failing to match a certain direction of the subgradient as the net error in the direction is stored in residual . by the assumption of weak learner edge , the directions will be approximated .",
    "we also note that if we assume the subgradients are bounded , then the residual magnitudes increase at most linearly in the number of weak learners .",
    "simultaneously , each weak learner shrinks the residual by at least a constant factor due to the assumption of edge .",
    "hence , we expect the residual to shrink exponentially in the number of learners .",
    "utilizing this observation , we arrive at the following performance guarantee :    [ them : non_smooth_strongly_convex ] assume the loss @xmath36 is @xmath25-strongly convex for all @xmath5 with bounded gradients , @xmath173 for all @xmath54 , and each weak learner @xmath42 has edge @xmath93 $ ] .",
    "let @xmath8 be a function space , and @xmath174 be a restriction of @xmath8 let @xmath175 be the optimal predictor in @xmath8 in hindsight .",
    "let @xmath176 .",
    "let step size be @xmath177 .",
    "when @xmath178 , we have : @xmath179 for stochastic setting where @xmath130 independently , when @xmath127 we have : @xmath180\\leq \\frac{4c^2g^2}{\\lambda n } ( 1 + \\ln n + \\frac{1}{8n } ) .   \\nonumber\\end{aligned}\\ ] ]    the above theorem shows that the average regret of alg .",
    "[ alg : online_gradient_boost_non_smooth_residual ] is @xmath1 with respect to the number @xmath2 of weak learners , which matches the regret bounds of online gradient descent for strongly convex loss . the key idea for proving theorem  [ them : non_smooth_strongly_convex ] is to combine our online weak learning edge definition with the proof framework of online gradient descent for strongly convex loss functions from  @xcite .",
    "the detailed proof can be found in appendix  [ sec : proof_non_smooth_strongly_convex ] .",
    "0.42      0.42      we demonstrate the performance of our streaming gradient boostingusing the following uci datasets  @xcite : year , abalone , slice , and a9a  @xcite as well as the mnist  @xcite dataset . if available , we use the given train - test split of each data - set .",
    "otherwise , we create a random 90%-10% train - test split .",
    "we first demonstrate the relationships between the regret bounds shown in eqn .",
    "[ eq : regret_bound_smooth ] and the parameters including the number of weak learners , the number of samples and edge @xmath181 .",
    "we compute the regret of sgbwith respect to a deep regression tree ( depth@xmath182 15 ) , which plays the @xmath20 in eqn .",
    "[ eq : regret_bound_smooth ] .",
    "we use regression trees as the weak learners .",
    "we assume that deeper trees have higher edges @xmath181 because they empirically fit training data better .",
    "we show how the regret relates to the trees depth , the number of weak learners @xmath2 ( fig .",
    "[ fig : regret_learners ] ) and the number of samples @xmath3 ( fig .",
    "[ fig : regret_samps ] ) .    for the experimental results shown in fig .",
    "[ fig : regret ] , we used smooth loss functions with @xmath183 regularization ( see appendix  [ sec : implementation ] for more details ) .",
    "we use logistic loss and square loss for binary classification ( a9a ) and regression task ( slice ) , respectively . for each regression tree weak learner , follow the regularized leader ( ftrl )  @xcite was used as the no - regret online update algorithm with regularization posed as the depth of the tree .",
    "[ fig : regret_learners ] shows the relationship between the number of weak learners and the average regret given a fixed total number of samples .",
    "the average regret decreases as we increase the number of weak learners .",
    "we note that the curves are close to linear at the beginning , matching our theoretical analysis that the average regret decays exponentially ( note the y - axis is log scale ) with respect to the number of weak learners .",
    "this shows that sgbcan significantly boost the performance of a single weak learner . to investigate the effect of the edge parameter @xmath181",
    ", we additionally compute the average regret in fig .",
    "[ fig : regret ] as the depth of the regression tree is increased .",
    "the tree depth increases the model complexity of the base learner and should relate to a larger @xmath181 edge parameter . from this experiment",
    ", we see that the average regret shrinks as the depth of the trees increases .    finally , fig .",
    "[ fig : regret_samps ] shows the convergence of the average regret with respect to the number of samples .",
    "we see that more powerful weak learners ( deeper regression trees ) results in faster convergence of our algorithm .",
    "we ran alg .",
    "[ alg : online_gradient_boost_non_smooth_residual ] on a9a with hinge loss and slice with @xmath184 ( least absolute deviation ) loss and observed very similar results as shown in fig .",
    "[ fig : regret ] .",
    "0.28    0.28    0.28    0.28    0.29    0.27    we next compare batch boosting to sgbusing two - layer neural networks as weak learners and see that sgbreaches similar final performance as the batch boosting algorithm albeit with less training computation . as stated in sec  [ subsec : non - smooth ] , we report @xmath185 instead of @xmath113 for sgb , since at convergence the average prediction is close to the final prediction , and the latter is impractical to compute .",
    "we implement our baseline , the classic batch gradient boosting ( * gb * )  @xcite , by optimizing each weak learner until convergence in order . in both gb and sgb ,",
    "we train weak learners using adam  @xcite optimization and use the default random parameter initialization for nn .",
    "we analyze the complexity of training sgband gb .",
    "we define the prediction complexity of one weak learner as the _ unit cost _",
    ", since the training run - time complexity almost equates the total complexity of weak learner predictions and updates .",
    "our choice of weak learner and update method ( two - layer networks and adam ) determines that updating a weak learner is about two units cost . in training",
    "using sgb , each of the @xmath3 data samples triggers predictions and updates with all @xmath2 of the weak learners .",
    "this results in a training computational complexity of @xmath186 . for gb ,",
    "let @xmath187 be the samples needed for each weak learner to converge .",
    "then the complexity of training gb is @xmath188 , because when training weak learner @xmath116 , all previous @xmath160 weak learners must also predict for each data point .",
    "hence , sgband gb will have the same training complexity if @xmath189 . in our experiments",
    "we observe weak learners typically converge less than @xmath190 samples , but our following experiment shows that sgbstill can converge faster overall .    fig .",
    "[ fig : convg ] plots the test - time loss versus training computation , measured by the unit cost . blue dots highlights when the weak learners are added in gb .",
    "we first note that sgbsuccessfully converges to the results of gb in all cases , supporting that sgbis a truly a streaming conversion of gb .",
    "as it takes many weak learners to achieve good performance on abalone and year , we observe that sgbconverges with less computation than gb . on a9a",
    ", however , gb is more computationally efficient than sgb , because the first weak learner in gb already performs well and learning a single weak learner for gb is faster than simultaneously optimizing all @xmath191 weak learners with sgb .",
    "this suggests that if we initially set @xmath2 too big , sgbcould be less computationally efficient . in fact fig .",
    "[ fig : abalone_vary_n ] shows that very larger @xmath2 causes slower convergence to the same final error plateau . on the other hand , small @xmath2 ( @xmath192 ) results in worse performance .",
    "we specify the chosen @xmath2 for sgbin fig .",
    "[ fig : convg ] , and they are around the number of weak learners that gb requires to converge and achieve good performance .",
    "we also note that sgbhas slower initial progress compared to gb on slice in fig .",
    "[ fig : slice ] and mnist in fig .",
    "[ fig : mnist ] .",
    "this is an understandable result as sgbhas a much larger pool of parameters to optimize . despite this initial disadvantage",
    ", sgbsurpasses gb and converges faster overall , suggesting the advantage of updating all the weak learners together . in practice , if we do not have a good guess of @xmath2 , we can still use sgb to add multiple weak learners at a time in gb to speed up convergence .",
    "table  [ table : validation ] records the test error ( square error for regression and error ratio for classification ) of the neural network base learner , gb , and sgb .",
    "we observe that sgbachieves test errors that are competitive with gb in all cases .",
    "in this paper , we present sgbfor online convex programming . by introducing an online weak learning edge definition that naturally extends the edge definition from batch boosting to the online setting and by using square loss , we are able to boost the predictions from weak learners in a gradient descent fashion .",
    "our sgbalgorithm guarantees exponential regret shrinkage in the number @xmath2 of weak learners for strongly convex and smooth loss functions .",
    "we additionally extend sgbfor optimizing non - smooth loss function , which achieves @xmath1 no - regret rate . finally , experimental results support the theoretical analysis .",
    "though our sgbalgorithm currently utilizes the procedure of gradient descent to combine the weak learners predictions , our online weak learning definition and the design of square loss for weak learners leave open the possibility to leverage other gradient - based update procedures such as accelerated gradient descent , mirror descent , and adaptive gradient descent for combining the weak learners predictions .",
    "this material is based upon work supported in part by : echo s grant name , darpa alias contract number hr0011 - 15-c-0027 , and national science foundation graduate research fellowship grant no .",
    "dge-1252522 .    * supplementary material for gradient boosting on stochastic data streams *",
    "given that a no - regret online learning algorithm @xmath6 running on sequence of loss @xmath193 , we have can easily see that eqn .",
    "[ eq : learnable ] holds as : @xmath194where @xmath16 is the regret of @xmath6 and is @xmath19 . to prove proposition  [ prop : edge_example ]",
    ", we only need to show that eqn .",
    "[ eq : leastsqure ] holds for some @xmath93 $ ] .",
    "this is equivalent to showing that there exist a hypothesis @xmath195 ( @xmath196 ) , such that @xmath197 . to see this equivalence ,",
    "let us assume that @xmath198 .",
    "let us set @xmath199 .",
    "using pythagorean theorem , we can see that @xmath200 .",
    "hence we get @xmath181 is at least @xmath201 , which is in @xmath202 $ ] .",
    "now since we assume that @xmath88 , then there must exist @xmath203 , such that @xmath204 , otherwise @xmath205 .",
    "consider the hypothesis @xmath206 and @xmath207 ( we assume @xmath43 is closed under scale ) , we have that either @xmath208 or @xmath209 .",
    "namely , we find at least one hypothesis @xmath55 such that @xmath210 and @xmath211 .",
    "hence if we pick @xmath212 , we must have @xmath213 .",
    "namely we can find a hypothesis @xmath214 , which is @xmath215 , such that there is non - zero @xmath93 $ ] : @xmath216      applying hoeffding inequality , we get with probability at least @xmath217 , @xmath218 is bounded as @xmath219 . similarly , we have with probability at least @xmath217 : latexmath:[\\ ] ] where @xmath329 . in this setting , we let weak learner @xmath116 pick hypothesis @xmath55 from @xmath43 that takes feature @xmath50 as input , and output @xmath330 .",
    "the online boosting algorithm then linearly combines the weak learners prediction to predict @xmath54 ."
  ],
  "abstract_text": [
    "<S> boosting is a popular ensemble algorithm that generates more powerful learners by linearly combining base models from a simpler hypothesis class . in this work </S>",
    "<S> , we investigate the problem of adapting batch gradient boosting for minimizing convex loss functions to online setting where the loss at each iteration is i.i.d sampled from an unknown distribution . to generalize from batch to online </S>",
    "<S> , we first introduce the definition of online weak learning edge with which for strongly convex and smooth loss functions , we present an algorithm , streaming gradient boosting(sgb ) with exponential shrinkage guarantees in the number of weak learners . </S>",
    "<S> we further present an adaptation of sgbto optimize non - smooth loss functions , for which we derive a @xmath0 convergence rate . we also show that our analysis can extend to adversarial online learning setting under a stronger assumption that the online weak learning edge will hold in adversarial setting . </S>",
    "<S> we finally demonstrate experimental results showing that in practice our algorithms can achieve competitive results as classic gradient boosting while using less computation . </S>"
  ]
}